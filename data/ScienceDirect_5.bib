@article{KEDIR2022104498,
title = {Hybridization of reinforcement learning and agent-based modeling to optimize construction planning and scheduling},
journal = {Automation in Construction},
volume = {142},
pages = {104498},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104498},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003715},
author = {Nebiyu Siraj Kedir and Sahand Somi and Aminah Robinson Fayek and Phuong H.D. Nguyen},
keywords = {R, A, G, O, P, D},
abstract = {Decision-making in construction planning and scheduling is complex because of budget and resource constraints, uncertainty, and the dynamic nature of construction environments. A knowledge gap in the construction literature exists regarding decision-making frameworks with the ability to learn and propose an optimal set of solutions for construction scheduling problems, such as activity sequencing and work breakdown structure formulations under uncertainty. The objective of this paper is to propose a hybrid reinforcement learning–graph embedding network model that 1) simulates complex construction planning environments using agent-based modeling and 2) minimizes computational burdens in establishing activity sequences and work breakdown formations. Three case studies with practical construction scheduling problems were used to demonstrate applicability of the developed model. This paper contributes to the body of knowledge by proposing the hybridization of reinforcement learning and simulation approaches to optimize project durations with resource constraints and support construction practitioners in making project planning decision-making.}
}
@article{CHEN2022109,
title = {Fault-tolerant adaptive tracking control of Euler-Lagrange systems – An echo state network approach driven by reinforcement learning},
journal = {Neurocomputing},
volume = {484},
pages = {109-116},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.10.083},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221015824},
author = {Qing Chen and Yaochu Jin and Yongduan Song},
keywords = {Echo state network, Reinforcement learning, Euler-Lagrange systems, Actuation faults},
abstract = {Reinforcement learning (RL) has enjoyed considerable success in application to nonlinear systems. However, very few RL-based works that explicitly address the control problem of MIMO nonlinear systems with subject to actuator failures. In this work, we develop a fault-tolerant adaptive tracking control method fused with an echo state network (ESN) driven by reinforcement learning for Euler-Lagrange systems subject to actuation faults. The proposed control includes an associative search network (ASN), a control gain network (CGN), and an adaptive critic network (ACN), with ASN to estimate the unknown items of the control system, CGN to deal with the time-varying and unknown control gains matrix, and ACN to generate the reinforcement signal, all together ensuring stable tracking and accommodate modeling uncertainties and actuation failures. Different from traditional reinforcement learning controllers that utilizes radial basis function neural networks (RBFNN) or fuzzy systems, the proposed one adopts an echo state network, a paradigm of recurrent neural networks, to implement the ASN, ACN and CGN, resulting in enhanced learning capabilities and stronger robustness against external uncertainties and disturbances, thus better control performance.}
}
@article{LI2021126451,
title = {Optimal consensus control for unknown second-order multi-agent systems: Using model-free reinforcement learning method},
journal = {Applied Mathematics and Computation},
volume = {410},
pages = {126451},
year = {2021},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2021.126451},
url = {https://www.sciencedirect.com/science/article/pii/S0096300321005403},
author = {Jun Li and Lianghao Ji and Huaqing Li},
keywords = {Reinforcement learning, Optimal consensus control, Online policy iteration, Neural networks, Second-order multi-agent systems},
abstract = {In this paper, the optimal consensus control problem with second-order dynamics consisting of leader and follower agents is discussed. For optimal consensus problem, the optimal control policies rely on algebraic Riccati equations (AREs) equation, which are difficult to solve. Furthermore, both the follower agents’ and the leader agent’s dynamics are assumed to be completely unknown. As the consensus problem based on feedback control, the second-order discrete-time multi-agent systems (DT-MASs) model with directed topology is formulated to the optimal tracking control problem via online deep reinforcement learning method. Based on graph theory, matrix analysis, Lyapunov stability, deep learning and optimal control, the optimality of value function and the stability of the consensus error systems for the unknown second-order systems are guaranteed for each agent. The results show that the designed policy iteration algorithm not only stabilizes the distributed dynamic systems, but also makes all agents’ position and velocity states reach consensus, respectively. Finally, the correctness of our theoretical results is illustrated under two numerical simulations based on the designing model-free actor-critic networks.}
}
@article{OH2021107280,
title = {Actor-critic reinforcement learning to estimate the optimal operating conditions of the hydrocracking process},
journal = {Computers & Chemical Engineering},
volume = {149},
pages = {107280},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107280},
url = {https://www.sciencedirect.com/science/article/pii/S0098135421000582},
author = {Dong-Hoon Oh and Derrick Adams and Nguyen Dat Vo and Dela Quarme Gbadago and Chang-Ha Lee and Min Oh},
keywords = {Hydrocracking process, Mathematical modeling, Deep neural network, Surrogate model, Actor-critic reinforcement learning, Optimization of operating conditions},
abstract = {Determining the optimal operating conditions for hydrocracking units is imperative due to the changing nature of production requirements. However, it is expensive to optimize the hydrocracking process with mathematical models because hydrocracking units have a limited capacity for quick response and customization. This study proposes an actor-critic reinforcement learning optimization strategy using a DNN surrogate model, which was developed from a validated mathematical model with a marginal error of less than 2%. The surrogate model interacted with the A2C algorithm and the optimal operating conditions were determined with an accuracy of 97.86% and 98.5%. To demonstrate the reliability, case studies were executed; the strategy was found to be consistent, with an average efficiency of 98%. The proposed approach offers the advantages of quick response time, low computational burden and customizability for online implementation, which are essential for practical optimization problems. It can be extended beyond hydrocracking to other chemical industries.}
}
@article{XU202014960,
title = {Vehicle emission control on road with temporal traffic information using deep reinforcement learning⁎⁎This work was supported in part by the National Key R&D Program of China under Grant (2018AAA0100800, 2018YFE0106800, 2018YFC0213104), National Natural Science Foundation of China (61725304, 61673361), Major Special Science and Technology Project of Anhui, China (912198698036), as well as the Fundamental Research Funds for the Central Universities under Grant WK2380000001.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {14960-14965},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1988},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320326197},
author = {Zhenyi Xu and Yang Cao and Yu Kang and Zhenyi Zhao},
keywords = {Urban air pollution, traffic emission control, deep reinforcement learning},
abstract = {The increased vehicle usage significantly aggravate the urban air pollution, which have great impact on the public health. Therefore, it is necessary to make proper traffic control policies and reduce traffic emissions. However, it is difficult to establish control strategies based on modeling methods, and carry out online control based on historical traffic information for the complex time-varying characteristics of emissions. In this paper, we present a deep reinforcement learning emission control strategy, which automatically learns the optimal traffic flow and speed limits to reduce traffic emission on the target road segment based on the temporal traffic information. The proposed approach is evaluated on real world vehicle emission data in Hefei. And the results demonstrate the effectiveness of the proposed approach against baseline methods.}
}
@article{CHEN202162,
title = {Deep reinforcement learning based moving object grasping},
journal = {Information Sciences},
volume = {565},
pages = {62-76},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.01.077},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521001158},
author = {Pengzhan Chen and Weiqing Lu},
keywords = {Moving object, Grasping planning, Object detection, Soft-Actor-Critic algorithm},
abstract = {Traditional grasping methods for locating unpredictable positions of moving objects under an unstructured environment cannot achieve good performance. This paper studies the utilization of deep reinforcement learning (DRL) with a Kinect depth sensor to resolve this challenging problem. The proposed grasping system integrates the DRL algorithm, Soft-Actor-Critic, and object detection techniques to implement an approaching-tracking-grasping scheme. Considering the state and action space for the high-degree-of-freedom manipulator, we employ an improved Soft-Actor-Critic algorithm to speed up the learning process. The proposed system can decouple object detection from the DRL control, which allows us to generalize the framework from a simulation environment to a real robot. Experimental results demonstrate that the developed system can autonomously grasp a moving object with different moving trajectories.}
}
@article{XUE202225,
title = {Neural network-based event-triggered integral reinforcement learning for constrained H∞ tracking control with experience replay},
journal = {Neurocomputing},
volume = {513},
pages = {25-35},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.09.119},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222012127},
author = {Shan Xue and Biao Luo and Derong Liu and Ying Gao},
keywords = {Adaptive dynamic programming, Neural networks, Integral reinforcement learning,  tracking control, Event-triggered mechanism},
abstract = {Since input constraints and external disturbances are unavoidable in tracking control problems, how to obtain a controller in this case to save communication and data resources at the same time is very challenging. Aiming at these challenges, this paper develops a novel neural network (NN)-based event-triggered integral reinforcement learning (IRL) algorithm for constrained H∞ tracking control problems. First, the constrained H∞ tracking control problem is transformed into a regulation problem. Second, an event-triggered optimal controller is designed to reduce network transmission burden and improve resource utilization, where a novel threshold is proposed and its non-negativity can be guaranteed. Third, for implementation purpose, a novel NN-based event-triggered IRL algorithm is developed. In order to improve data utilization, the experience replay technique with an easy-to-verify condition is employed in the learning process. Theoretical analysis proves that the tracking error and weight estimation error are uniformly ultimately bounded. Finally, simulation verification shows the effectiveness of the present method.}
}
@article{FANG2023144,
title = {Deep reinforcement learning assisted reticle floorplanning with rectilinear polygon modules for multiple-project wafer},
journal = {Integration},
volume = {91},
pages = {144-152},
year = {2023},
issn = {0167-9260},
doi = {https://doi.org/10.1016/j.vlsi.2023.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167926023000561},
author = {Zehua Fang and Jinglin Han and Huaxinyu Wang},
keywords = {Multiple-project wafer, Reticle floorplan, Deep reinforcement learning},
abstract = {In multiple-project wafer (MPW), the cost of mask tooling can be significantly reduced by optimizing the floorplan of the reticle. However, the generation of an ideal reticle floorplan can be very challenging due to the unusual constraints and high complexity in the placement of the dies. In this paper, we propose a low-complexity method to extract the placement of dies in a reticle. The proposed algorithms are capable of placing die modules in rectilinear polygon shapes and achieving a high reticle area usage. The aspect ratio of the reticle is also optimized for fabrication consistency. We first design a heuristic method to decide the placement of all modules with given width and height of the reticle. Then based on this method, we propose a searching framework to minimize the area of the reticle while keeping its aspect ratio close to 1. Finally a deep reinforcement learning based recursive neural network (RNN) model is developed to optimize the sequence of modules so that the area usage of the reticle can be further improved. The experimental results demonstrate an up to 88.61% area usage obtained solely from our heuristic method, and an additional 9.15% usage is improved after implementing the RNN model.}
}
@article{ZHANG2023108995,
title = {Coordinated voltage regulation of high renewable-penetrated distribution networks: An evolutionary curriculum-based deep reinforcement learning approach},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {149},
pages = {108995},
year = {2023},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2023.108995},
url = {https://www.sciencedirect.com/science/article/pii/S0142061523000522},
author = {Tingjun Zhang and Liang Yu and Dong Yue and Chunxia Dou and Xiangpeng Xie and Lei Chen},
keywords = {Active distribution networks, Voltage control, Multi-agent deep reinforcement learning, Multi-stage parallel training, Evolutionary mechanism},
abstract = {With the increasing penetration of renewable energy in active distribution networks (ADNs), voltage regulation problem is becoming more and more challenging. In this article, we focus on providing a scalable data-driven approach to ensure the voltage security of ADNs with high penetration of PVs. To this end, we first formulate an optimization problem for real-time voltage control considering source-load-storage collaboration while minimizing the total active power curtailment of PVs. Then, we reformulate the above problem as a Markov game and propose a novel voltage regulation algorithm based on evolutionary curriculum-based multi-agent deep reinforcement learning (EC-MADRL) to solve it. The key idea of the proposed algorithm is to adopt a multi-stage parallel training framework based on attention multi-agent deep deterministic policy gradient algorithm (Attention-MADDPG) and use an evolutionary mechanism to select voltage regulation models with high fitnesses in the previous stage, which means that good agents with best adaptability could be utilized for learning in the environment with a larger number of PVs. Simulation results show the effectiveness and scalability of the proposed algorithm.}
}
@article{WANG2022101773,
title = {Multi-label fault recognition framework using deep reinforcement learning and curriculum learning mechanism},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101773},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101773},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002312},
author = {Zisheng Wang and Jianping Xuan and Tielin Shi},
keywords = {Fault recognition, Deep reinforcement learning, Multi-label learning, Curriculum learning mechanism, Proximal policy optimization},
abstract = {In the actual working site, the equipment often works in different working conditions while the manufacturing system is rather complicated. However, traditional multi-label learning methods need to use the pre-defined label sequence or synchronously predict all labels of the input sample in the fault diagnosis domain. Deep reinforcement learning (DRL) combines the perception ability of deep learning and the decision-making ability of reinforcement learning. Moreover, the curriculum learning mechanism follows the learning approach of humans from easy to complex. Consequently, an improved proximal policy optimization (PPO) method, which is a typical algorithm in DRL, is proposed as a novel method on multi-label classification in this paper. The improved PPO method could build a relationship between several predicted labels of input sample because of designing an action history vector, which encodes all history actions selected by the agent at current time step. In two rolling bearing experiments, the diagnostic results demonstrate that the proposed method provides a higher accuracy than traditional multi-label methods on fault recognition under complicated working conditions. Besides, the proposed method could distinguish the multiple labels of input samples following the curriculum mechanism from easy to complex, compared with the same network using the pre-defined label sequence.}
}
@article{ZOU2020106535,
title = {Towards optimal control of air handling units using deep reinforcement learning and recurrent neural network},
journal = {Building and Environment},
volume = {168},
pages = {106535},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2019.106535},
url = {https://www.sciencedirect.com/science/article/pii/S0360132319307474},
author = {Zhengbo Zou and Xinran Yu and Semiha Ergan},
keywords = {HVAC control, Energy consumption, Thermal comfort, Deep reinforcement learning, Long-short-term-memory network},
abstract = {Optimal control of heating, ventilation and air conditioning systems (HVACs) aims to minimize the energy consumption of equipment while maintaining the thermal comfort of occupants. Traditional rule-based control methods are not optimized for HVAC systems with continuous sensor readings and actuator controls. Recent developments in deep reinforcement learning (DRL) enabled control of HVACs with continuous sensor inputs and actions, while eliminating the need of building complex thermodynamic models. DRL control includes an environment, which approximates real-world HVAC operations; and an agent, that aims to achieve optimal control over the HVAC. Existing DRL control frameworks use simulation tools (e.g., EnergyPlus) to build DRL training environments with HVAC systems information, but oversimplify building geometrics. This study proposes a framework aiming to achieve optimal control over Air Handling Units (AHUs) by implementing long-short-term-memory (LSTM) networks to approximate real-world HVAC operations to build DRL training environments. The framework also implements state-of-the-art DRL algorithms (e.g., deep deterministic policy gradient) for optimal control over the AHUs. Three AHUs, each with two-years of building automation system (BAS) data, were used as testbeds for evaluation. Our LSTM-based DRL training environments, built using the first year's BAS data, achieved an average mean square error of 0.0015 across 16 normalized AHU parameters. When deployed in the testing environments, which were built using the second year's BAS data of the same AHUs, the DRL agents achieved 27%–30% energy saving comparing to the actual energy consumption, while maintaining the predicted percentage of discomfort (PPD) at 10%.}
}
@article{GAO2023105572,
title = {Frequency matching optimization model of ultrasonic scalpel transducer based on neural network and reinforcement learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {117},
pages = {105572},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105572},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622005620},
author = {Li Gao and Sheng-long Yang and Bin Meng and Guo-xiang Tong and Hai-Ping Fan and Gui-Song Yang},
keywords = {Ultrasonic scalpel transducer, RBF neural network, Strengthen learning, Frequency matching},
abstract = {Aiming at the problem that the excitation frequency and resonant frequency of the transducer cannot keep synchronous, the output amplitude decreases and the vibration is unstable. In this study, the working principle of piezoelectric transducers is firstly analyzed by the equivalent circuit method and the instantaneous characteristic variables (installing preload, assembly preload, load, and other factors) that affect the frequency matching obtained to establish the equivalent relationship with the resonant frequency. Secondly, in order to make the synchronization between excitation frequency and resonance frequency, the key instantaneous characteristic variables are extracted based on Pearson correlation coefficient. Thirdly, the mathematical model of the mapping relationships between instantaneous characteristic variables and resonance frequency is established with the radial basis function neural network (RBFNN). Fourthly, for the purpose of the adaptation to the characteristics of dynamic load and real-time frequency modulation in the operation of ultrasonic scalpels, the reinforcement learning (Q-learning algorithm) and the weight vector of RBFNN are used to define the eligibility trace, which is used to dynamically adjust the RBFNN frequency matching optimization model in real time and maintain the “constant” amplitude output and stable harmonic response process. Finally, the experimental results show that, compared with the traditional methods, the frequency matching optimization model of ultrasonic scalpel transducer based on RBF neural network and Q-Learning reinforcement learning is effective, and the vibration amplitude of the transducer is increased by 15.25μm. The amplitude fluctuation is stable at 0.92μm. It can provide decision-making guidance for relevant engineering fields.}
}
@incollection{BRANDNER2023595,
title = {Reinforcement learning combined with model predictive control to optimally operate a flash separation unit},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {595-600},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50094-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152740500949},
author = {Dean Brandner and Torben Talis and Erik Esche and Jens-Uwe Repke and Sergio Lucia},
keywords = {Reinforcement Learning, Model Predictive Control, Flash Separation Unit},
abstract = {Model predictive control (MPC) and reinforcement learning (RL) are two powerful optimal control methods. However, the performance of MPC depends mainly on the accuracy of the underlying model and the prediction horizon. Classic RL needs an excessive amount of data and cannot consider constraints explicitly. This work combines both approaches and uses Q-learning to improve the closed-loop performance of a parameterized MPC structure with a surrogate model and a short prediction horizon. The parameterized MPC structure provides a suitable starting point for RL training, which keeps the required data in a reasonable amount. Moreover, constraints are considered explicitly. The solution can be obtained in real-time due to the surrogate model and the short prediction horizon. The method is applied for control of a flash separation unit and compared to a MPC structure that uses a rigorous model and a large prediction horizon.}
}
@article{BLUHER2022113,
title = {Model Building for better Transfer of AI Systems using Reinforcement Learning from Simulation to the Physical World},
journal = {Procedia CIRP},
volume = {109},
pages = {113-118},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.223},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122006710},
author = {Till Blüher and Harold Billiet and Rainer Stark},
keywords = {Modelling, Reinforcement Learning, Simulation, Modular Design, PSS},
abstract = {Autonomous product service systems (PSS) provide services by autonomously controlling products to deliver value. This requires powerful intelligences embedded in products. The development of powerful intelligences can be carried out by means of reinforcement learning in simulations, in which products act as agents and independently learn optimal behaviors via trial-and-error procedures. These simulations can refer to different system levels (e.g. car as an agent in a city, microcontroller as an agent in a car) and can vary in terms of visualization (e.g. 3D, 2D, block diagrams, no visualization). However, they always involve an agent, that can choose from a set of possible actions that cause changes to the environment, which can be evaluated against an objective based on a reward function, that needs to be defined by engineers before. Within this process, a neural network gradually learns an optimal behavioral model through a reinforcement learning algorithm that makes use of the data on selected actions by the agent, outcomes and yielded reward. However, the models required for the agent, environment, artificial intelligence (e.g. deep neural networks), reinforcement learning algorithm and reward function are highly interdependent and their design can have a significant influence on the training result. When training results are poor, rapid adaptation of these models is thereby desirable to improve the training process, but difficult to implement due to the interdependencies between models. In this paper, we discuss the challenges of model building for reinforcement learning in simulation and propose a general approach.}
}
@article{YUAN2023398,
title = {Digital Twin-Based economic assessment of solar energy in smart microgrids using reinforcement learning technique},
journal = {Solar Energy},
volume = {250},
pages = {398-408},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2022.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X22009021},
author = {Guanghui Yuan and Fei Xie},
keywords = {Smart microgrid, Reinforcement learning, Load scheduling, Demand response, Renewable energy},
abstract = {Utility companies recognize the importance and necessity of demand response (DR) programs for reducing the increased production costs associated with rising energy demand. The advent of smart information and communication systems has made DR programs on the basis of cost a viable option to control load in smart microgrids. Small domestic consumers are rapidly using stochastic renewable energy resources such as photovoltaic (PV). The study examines an integrated layout for residential load scheduling or load commitment problems (LCP) with renewable energy resources no matter what kind of tariff is applied. Uncertainty-based decision-making problems are effectively solved using reinforcement learning (RL). The paper proposes an RL-enabled solution to the LCP in smart microgrids. An innovative aspect of the study is the development of an integrated layout containing an implementable solution that takes into account user satisfaction, stochastic renewable power, and tariffs. In simulation tests, the suggested layout is tested for its effectiveness and flexibility. An analysis of the algorithm's efficiency using a household user with schedule-able and non-schedulable devices, together with a PV resource, has been presented.}
}
@article{NAJAFI2023109179,
title = {A deep reinforcement learning approach for repair-based maintenance of multi-unit systems using proportional hazards model},
journal = {Reliability Engineering & System Safety},
volume = {234},
pages = {109179},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109179},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023000947},
author = {Seyedvahid Najafi and Chi-Guhn Lee},
keywords = {Condition-based maintenance, Deep reinforcement learning, Sequential decision making},
abstract = {Condition-based maintenance (CBM) optimization may turn intractable when a complex system with multiple units becomes an asset of interest. This paper aims to find a CBM policy for a multi-unit series system subject to stochastic degradation, where a new inspection is scheduled based on age and condition monitoring data upon each inspection. The novelty of this study lies in proposing a modified deep reinforcement learning (DRL) algorithm for the semi-Markov decision processes (SMDP) to find an opportunistic CBM policy for a multi-unit system with economic dependency over an infinite horizon, where a range of repair actions are allowed under an aperiodic inspection scheme. We also suggested a novel environment simulator that considers the simultaneous impact of age and covariates using the proportional hazards (PH) model and the system's reliability characteristics. DRL acts as not only a learning algorithm obviating the full specification of the model but also an approximate scheme producing a solution in a limited computation. The proposed algorithm is applied to a multi-unit hydroelectric power system with the damage self-healing property to demonstrate the higher performance of the DRL algorithm in cost reduction than alternative policies and explain how enhancing system reliability reduces costs during the learning process.}
}
@article{WANG2023854,
title = {Multi-objective deep reinforcement learning for optimal design of wind turbine blade},
journal = {Renewable Energy},
volume = {203},
pages = {854-869},
year = {2023},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2023.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0960148123000034},
author = {Zheng Wang and Tiansheng Zeng and Xuening Chu and Deyi Xue},
keywords = {Wind turbine design, Multi-objective optimization, Deep reinforcement learning, Deterministic policy gradient, Stochastic policy gradient},
abstract = {The design of a wind turbine blade is a typical complex multi-objective optimization problem, mostly solved by evolutionary algorithms. However, these methods are not effective due to limitations such as inaccurate solutions on Pareto fronts for high-dimensional problems, numerous iterations and low adaptability to problems with similar conditions. To address these issues, two multi-objective deep reinforcement learning models are introduced in this paper from an entirely different perspective. The first model, namely the multi-objective deep deterministic policy gradient (MO-DDPG), extends the existing popular reinforcement learning algorithm DDPG to multi-objective optimization problems by integrating various techniques including modeling of constraints on high-dimensional spaces and generation of Pareto solutions. The second model, namely the multi-objective deep stochastic policy gradient (MO-DSPG), further improves the MO-DDPG by incorporating a random neural network called restricted Boltzmann machine (RBM). An adaptive random agent is trained to transform multiple deterministic policies into an optimal stochastic policy. In addition, neighborhood-based parameter transfer strategy is applied to MO-DSPG in the model training phase to reduce the computation time. Experiments showed that the aerodynamic performance of the blades is improved by both the MO-DDPG and the MO-DSPG models with the hypervolume increasing an average of 6.67% and 9.25% respectively, compared with the state-of-art models. The computational efficiency of MO-DSPG is improved by using the parameter transfer strategy, with its runtime reduced to 72.52% compared with state-of-art models.}
}
@article{TIAN2023104351,
title = {Reinforcement learning under temporal logic constraints as a sequence modeling problem},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104351},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104351},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002408},
author = {Daiying Tian and Hao Fang and Qingkai Yang and Haoyong Yu and Wenyu Liang and Yan Wu},
keywords = {Temporal logic, Reinforcement learning, Trajectory transformer, Sparse attention},
abstract = {Reinforcement learning (RL) under temporal logic typically suffers from slow propagation for credit assignment. Inspired by recent advancements called trajectory transformer in machine learning, the reinforcement learning under Temporal Logic (TL) is modeled as a sequence modeling problem in this paper, where an agent utilizes the transformer to fit the optimal policy satisfying the Finite Linear Temporal Logic (LTLf) tasks. To combat the sparse reward issue, dense reward functions for LTLf are designed. For the sake of reducing the computational complexity, a sparse transformer with local and global attention is constructed to automatically conduct credit assignment, which removes the time-consuming value iteration process. The optimal action is found by the beam search performed in transformers. The proposed method generates a series of policies fitted by sparse transformers, which has sustainably high accuracy in fitting the demonstrations. At last, the effectiveness of the proposed method is demonstrated by simulations in Mini-Grid environments.}
}
@article{LI2023103358,
title = {A hierarchical deep reinforcement learning model with expert prior knowledge for intelligent penetration testing},
journal = {Computers & Security},
volume = {132},
pages = {103358},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103358},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823002687},
author = {Qianyu Li and Min Zhang and Yi Shen and Ruipeng Wang and Miao Hu and Yang Li and Hao Hao},
keywords = {Network security assessment, Penetration testing, Automated penetration testing, Reinforcement learning, Expert prior knowledge},
abstract = {Penetration testing (PT) is an effective method to assess the security of a network, mainly carried out by experienced human experts, and is widely applied in practice. It is urgent to develop automated tools to alleviate the pressure of talent shortages. Reinforcement learning (RL) is a promising approach to achieving automated PT. However, the high complexity of PT scenarios and the low sample efficiency of RL hinder its applications in practice. Specifically, it faces two dilemmas: (1) vast state and action spaces and (2) highly ineffective exploration. We propose a hierarchical deep reinforcement learning (HDRL) model with expert prior knowledge to overcome the above dilemmas. The HDRL model mitigates the first dilemma. According to the characteristics of PT, we design the model as a hierarchical structure containing two layers of agents, and the agents as a deep neural network to decompose PT tasks and reduce their complexity. Expert prior knowledge mitigates the second dilemma. It is used as rules and knowledge graphs, carries out action constraints according to the rules, and obtains action advice according to knowledge graphs. The two jointly guide the decision-making of agents to reduce invalid exploration. To verify the effectiveness of the proposed method, we design scenarios based on actual network environments. The experimental results show that our model significantly improves the sample efficiency, greatly reduces the learning time of the agents, and shows good performance on large-scale network scenarios, which has the potential to promote the practical application of intelligent PT based on RL.}
}
@article{QIN2021102822,
title = {A real-time tracking controller for piezoelectric actuators based on reinforcement learning and inverse compensation},
journal = {Sustainable Cities and Society},
volume = {69},
pages = {102822},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102822},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721001128},
author = {Shijie Qin and Long Cheng},
keywords = {Piezoelectric actuators, Real-time tracking control, Reinforcement learning, Adaptive dynamic programming, Hysteresis compensation},
abstract = {Nanotechnology is a promising technology and has been widely applied for sustainable smart cities. As the fundamental devices for nanotechnology, piezoelectric actuators (PEAs) have gained wide attention in precision manufacturing because of the advantages of rapid response, large mechanical force and high resolution. However, the inherent nonlinearities of PEAs hinder wide applications for nano-positioning and high-precision manipulation. To eliminate these nonlinearities, various control methods have been proposed, while the optimal control of PEAs is considered rarely. Inspired by the reinforcement learning, adaptive dynamic programming (ADP) is proposed to solve the optimal tracking control problem of PEAs. In this paper, a controller based on reinforcement learning and inverse compensation is designed for the tracking control of PEAs. The experiments on the PEA platform are designed to verify the effectiveness of the proposed method. Comparisons with some representative controllers have demonstrated that the proposed controller has a better control performance.}
}
@article{CHEN20221051,
title = {Emergency load-shedding optimization control method based on reinforcement learning assistance},
journal = {Energy Reports},
volume = {8},
pages = {1051-1061},
year = {2022},
note = {ICPE 2021 - The 2nd International Conference on Power Engineering},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2022.02.140},
url = {https://www.sciencedirect.com/science/article/pii/S2352484722003870},
author = {Yilin Chen and Siyang Liao and Jian Xu},
keywords = {Safety control strategy, Emergency load control, Load-shedding cost, Reinforcement learning},
abstract = {At present, on-line pre-decision and real-time matching control forms are the most widely used emergency control forms in the power system. The safety control system predicts the accidents according to the current operation state of the power grid and sets the emergency control strategy for the predicted accidents to ensure the safety and stability of the power grid when the fault occurs. However, the voltage level controlled by the safety control system is relatively high, which leads to a large granularity of load control. Because load nodes are mainly mixed users of various types, the strategy under large-scale centralized control may include important load nodes. It is necessary to replace the important load with other controllable low voltage nodes in the area to ensure that the load-shedding quantity set by the safety control system can be implemented sufficiently. Most emergency control optimization problems regard the load-shedding cost as a definite and constant value, but in fact, the load-shedding cost will change with the environmental factors. Based on the analysis of the influence of environmental factors on the cost of load shedding, this paper uses the reinforcement learning method to learn historical operation data and adapt to the variability of load-shedding cost to decide the best alternatives. The results of an example show that the proposed method can make replacement decisions in any scene based on adaptive environmental variability and can effectively prevent over-cutting through the design of a precise final reward.}
}
@article{ZHANG2022105302,
title = {Online reinforcement learning with passivity-based stabilizing term for real time overhead crane control without knowledge of the system model},
journal = {Control Engineering Practice},
volume = {127},
pages = {105302},
year = {2022},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2022.105302},
url = {https://www.sciencedirect.com/science/article/pii/S0967066122001496},
author = {Haoran Zhang and Chunhui Zhao and Jinliang Ding},
keywords = {Overhead cranes, Online reinforcement learning, Passivity, Lyapunov stability, Uniformly ultimately boundedness},
abstract = {Due to the existing uncertainties such as the payload mass and unmodeled dynamics in the overhead crane system, classical model-based control methods yielding fixed control gain can exhibit certain limitations. In this study, a novel model-free online Reinforcement Learning (RL) control method is proposed for the real-time overhead crane position regulation and anti-swing control problem, combining the benefits of adaptive control and optimal control. The crane control problem is first formulated as an optimal regulation problem with a user-specified objective function. Then, an improved neural-network updating rule with an additional passivity-based stabilizing term is developed to ensure the system stability during learning based on the overhead crane system passivity analysis. The proposed method, unlike other online RL algorithms, does not require the initial stabilizing control policy or prior knowledge of the crane mathematical model. Lyapunov approach is used to prove the closed-loop system stability, and the learned controller is shown to be near-optimal within a finite bound. Finally, simulation studies are conducted to demonstrate the effectiveness of the proposed method in the presence of system parameter variations and external disturbances, exhibiting satisfactory performance when compared to LQR and passivity-based control.}
}
@article{YAO2022105345,
title = {Data-driven constrained reinforcement learning for optimal control of a multistage evaporation process},
journal = {Control Engineering Practice},
volume = {129},
pages = {105345},
year = {2022},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2022.105345},
url = {https://www.sciencedirect.com/science/article/pii/S0967066122001769},
author = {Yao Yao and Jinliang Ding and Chunhui Zhao and Yonggang Wang and Tianyou Chai},
keywords = {Constrained reinforcement learning, Data-driven, Evaporation process, Optimal control},
abstract = {It is challenging for reinforcement learning to solve the optimal control problem of industrial processes with constraints under uncertain operating conditions. In this context, this paper proposes a novel data-driven constrained reinforcement learning algorithm for the optimal control of a multistage evaporation process consisting of multiple evaporators in series with coupled liquid levels. We first formulate the optimal control problem as a constrained Markov decision process. Then, with the cumulative tracking error of the outlet liquor density taken as the cumulative constraint, a Lagrangian-based constrained policy optimization is developed. The fast setpoint tracking is achieved by gradient iteration of the policy and the dual variable. An action correction layer based on the online sequential version of random vector functional-link networks is built on the output of the policy network to address the instantaneous constraints of the liquid levels. The infeasible action is corrected in real-time so as to keep the liquid levels in each evaporator within operating range. Finally, we utilize both on-policy and off-policy data generated by the interaction between the constrained policy and the evaporation environment to update our algorithm, which is more data-efficient. Experiments have been carried out on a multistage evaporation system, and the results validate the effectiveness of the proposed algorithm.}
}
@article{JIANG2022108278,
title = {A collaborative optimization strategy for computing offloading and resource allocation based on multi-agent deep reinforcement learning},
journal = {Computers and Electrical Engineering},
volume = {103},
pages = {108278},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108278},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622005067},
author = {Yingying Jiang and Yuxuan Mao and Gaoxiang Wu and Zhenhua Cai and Yixue Hao},
keywords = {MEC, Task offloading, Resource allocation, MADRL},
abstract = {With the emergence of mobile edge computing (MEC), the edge cloud with certain computing power is deployed closer to the mobile device, which can well solve the computing and delay requirements of the mobile device. In 5G ultra-dense heterogeneous networks, where the macro base station (MBS) and multiple dense small base stations (SBS) are deployed in the region, the offloading decision faces multiple choices. In order to solve the problem of computing offloading and resource allocation in 5G ultra-dense heterogeneous networks, we propose a collaborative optimization strategy based on multi-agent deep reinforcement learning (MADRL). At each time, the mobile device only needs to make the optimal offloading decision according to its own historical offloading decision, the allocated bandwidth and computing resources at the past time, as well as the service response delay and energy consumption at the past time, without knowing other user information and dynamic network environment information. Simulation results show that the proposed collaborative optimization strategy is better than the other three baseline schemes in terms of service response delay and energy consumption performance.}
}
@article{ZHUANG2023120936,
title = {Data-driven predictive control for smart HVAC system in IoT-integrated buildings with time-series forecasting and reinforcement learning},
journal = {Applied Energy},
volume = {338},
pages = {120936},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.120936},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923003008},
author = {Dian Zhuang and Vincent J.L. Gan and Zeynep {Duygu Tekler} and Adrian Chong and Shuai Tian and Xing Shi},
keywords = {Smart Facilities Management, Reinforcement Learning, Data-driven Control, Building Automation, Recursive Prediction, Time-Series Forecasting},
abstract = {Optimising HVAC operations towards human wellness and energy efficiency is a major challenge for smart facilities management, especially amid COVID situations. Although IoT sensors and deep learning were applied to support HVAC operations, the loss of forecasting accuracy in recursive prediction largely hinders their applications. This study presents a data-driven predictive control method with time-series forecasting (TSF) and reinforcement learning (RL), to examine various sensor metadata for HVAC system optimisation. This involves the development and validation of 16 Long Short-Term Memory (LSTM) based architectures with bi-directional processing, convolution, and attention mechanisms. The TSF models are comprehensively evaluated under independent, short-term recursive, and long-term recursive prediction scenarios. The optimal TSF models are integrated with a Soft Actor-Critic RL agent to analyse sensor metadata and optimise HVAC operations, achieving 17.4% energy savings and 16.9% thermal comfort improvement in the surrogate environment. The results show that recursive prediction leads to a significant reduction in model accuracy, and the effect is more pronounced in the temperature-humidity prediction model. The attention mechanism significantly improves prediction performance in both recursive and independent prediction scenarios. This study contributes new data-driven methods for smart HVAC operations in IoT-enabled intelligent buildings towards a human-centric built environment.}
}
@article{DAPOLITO2022281,
title = {Reinforcement Learning Training Environment for Fixed Wing UAV Collision Avoidance},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {39},
pages = {281-285},
year = {2022},
note = {21st IFAC Conference on Technology, Culture and International Stability TECIS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.12.035},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322030750},
author = {Francesco d'Apolito},
keywords = {Reinforcement Learning, Unmanned Aerial Vehicles, Collision Avoidance, Artificial Intelligence, Applications, Intelligent Systems, Models, Simulation},
abstract = {In recent years, Unmanned Aerial Vehicles (UAVs) are emerging as a key technology with possible uses for several applications. To fully exploit their potential, UAVs needs to be able to be fully automatized and they need to be allowed to go Beyond Visual Line of Sight (BVLOS). Nowadays, autopilots and Detect and Avoid (DAA) systems, essential components for BVLOS operations, are developed with classical control approaches and classical algorithm for identification and avoidance of collision in the airspace. However, new methods based on Artificial Intelligence and, in particular, on Reinforcement Learning are attracting scientific interest. In order to train an agent with Reinforcement Learning methods, the development of the training environment is of utmost importance. This paper aims to present the development of a training environment to train an agent to avoid collisions in the airspace by setting desired values of roll, pitch and throttle. It will describe the developed training environment and the PID controllers implemented for allowing the own aircraft to receive the roll, pitch and throttle commands and to control the intruders’ trajectory.}
}
@article{FENG2020679,
title = {Optimal trajectory tracking control based on reinforcement learning for the deployment process of space tether system},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {1},
pages = {679-684},
year = {2020},
note = {6th Conference on Advances in Control and Optimization of Dynamical Systems ACODS 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.06.113},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301324},
author = {Yiting Feng and Changqing Wang and Aijun Li},
keywords = {space tether system, deployment, trajectory tracking, neural network, adaptive dynamic programming, reinforcement learning},
abstract = {Space tether system has a wide application prospect in space mission. Due to the characteristics of strong non-linearity and under-actuation, as well as the interference of complex space environment, it is difficult to model the tethered system accurately. Hence, the controller based on the parameters of the system model will cause large errors in the process of control. In this paper, an adaptive dynamic programming algorithm based on reinforcement learning theory is adopted. By training two Back Propagation (BP) neural networks, namely critic neural network (NN) and actor NN, the performance index function and control law of the system approach approximate optimal values respectively. The controller design is independent of the system model, so model-free control of the system is realized by implementing this control method. First, assuming that the out-of-plane motion of the system is stable, the optimal deployment trajectory of the tethered system is obtained by parameter optimization based on Nelder-Mead method. The optimal trajectory is taken as the nominal trajectory and the trajectory tracking is carried out by reinforcement learning controller. The simulation results show that the reinforcement learning algorithm has a good control effect on the in-plane trajectory tracking of the tethered system, which proves the feasibility and robustness of the control method.}
}
@article{SIERRAGARCIA202478,
title = {Federated Discrete Reinforcement Learning for Automatic Guided Vehicle Control},
journal = {Future Generation Computer Systems},
volume = {150},
pages = {78-89},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.08.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003217},
author = {J. Enrique Sierra-Garcia and Matilde Santos},
keywords = {Automated guided vehicle (AGV), Federated learning, Industry 4.0, Intelligent control, Path following, Reinforcement learning},
abstract = {Under the federated learning paradigm, the agents learn in parallel and combine their knowledge to build a global knowledge model. This new machine learning strategy increases privacy and reduces communication costs, some benefits that can be very useful for industry applications deployed in the edge. Automatic Guided Vehicles (AGVs) can take advantage of this approach since they can be considered intelligent agents, operate in fleets, and are normally managed by a central system that can run in the edge and handles the knowledge of each of them to obtain a global emerging behavioral model. Furthermore, this idea can be combined with the concept of reinforcement learning (RL). This way, the AGVs can interact with the system to learn according to the policy implemented by the RL algorithm in order to follow specified routes, and send their findings to the main system. The centralized system collects this information in a group policy to turn it over to the AGVs. In this work, a novel Federated Discrete Reinforcement Learning (FDRL) approach is implemented to control the trajectories of a fleet of AGVs. Each industrial AGV runs the modules that correspond to an RL system: a state estimator, a rewards calculator, an action selector, and a policy update algorithm. AGVs share their policy variation with the federated server, which combines them into a group policy with a learning aggregation function. To validate the proposal, simulation results of the FDRL control for five hybrid tricycle-differential AGVs and four different trajectories (ellipse, lemniscate, octagon, and a closed 16-polyline) have been obtained and compared with a Proportional Integral Derivative (PID) controller optimized with genetic algorithms. The intelligent control approach shows an average improvement of 78% in mean absolute error, 75% in root mean square error, and 73% in terms of standard deviation. It has been shown that this approach also accelerates the learning up to a 50 % depending on the trajectory, with an average of 36% speed up while allowing precise tracking. The suggested federated-learning based technique outperforms an optimized fuzzy logic controller (FLC) for all of the measured trajectories as well. In addition, different learning aggregation functions have been proposed and evaluated. The influence of the number of vehicles (from 2 to 10) on the path following performance and on network transmission has been analyzed too.}
}
@article{LU2021140,
title = {MGRL: Graph neural network based inference in a Markov network with reinforcement learning for visual navigation},
journal = {Neurocomputing},
volume = {421},
pages = {140-150},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.07.091},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220312583},
author = {Yi Lu and Yaran Chen and Dongbin Zhao and Dong Li},
keywords = {Visual navigation, Graph neural network, Markov network, Reinforcement learning, Probabilistic graph model, Knowledge graph},
abstract = {Visual navigation is an essential task for indoor robots and usually uses the map as assistance to providing global information for the agent. Because the traditional maps match the environments, the map-based and map-building-based navigation methods are limited in the new environments for obtaining maps. Although the deep reinforcement learning navigation method, utilizing the non-map-based navigation technique, achieves satisfactory performance, it lacks the interpretability and the global view of the environment. Therefore, we propose a novel abstract map for the deep reinforcement learning navigation method with better global relative position information and more reasonable interpretability. The abstract map is modeled as a Markov network which is used for explicitly representing the regularity of objects arrangement, influenced by people activities in different environments. Besides, a knowledge graph is utilized to initialize the structure of the Markov network, as providing the prior structure for the model and reducing the difficulty of model learning. Then, a graph neural network is adopted for probability inference in the Markov network. Furthermore, the update of the abstract map, including the knowledge graph structure and the parameters of the graph neural network, are combined into an end-to-end learning process trained by a reinforcement learning method. Finally, experiments in the AI2THOR framework and the physical environment indicate that our algorithm greatly improves the success rate of navigation in case of new environments, thus confirming the good generalization.}
}
@article{ZHU2023126847,
title = {Integrating reinforcement learning with deterministic learning for fault diagnosis of nonlinear systems},
journal = {Neurocomputing},
volume = {562},
pages = {126847},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126847},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223009700},
author = {Zejian Zhu and Weiming Wu and Tianrui Chen and Jingtao Hu and Cong Wang},
keywords = {Reinforcement learning, Deterministic learning, Fault diagnosis, Nonlinear systems, Rotating stall warning},
abstract = {Reliable fault diagnosis (FD) is important to ensure safety in nonlinear engineering systems. Modern engineering systems are often subject to unknown complex nonlinearities and varying operation conditions, therefore, one of the main challenges for FD of nonlinear systems is the robustness against these uncertainties. In this paper, a novel robust FD approach combining the reinforcement learning (RL) and the deterministic learning theory (DLT) is developed for a class of discrete-time nonlinear systems. The DLT is employed to pre-train the neural network (NN) aiming at approximating the unknown nonlinear complexity and obtaining dynamical fault models, then RL techniques are employed to adapt the NN parameters to improve the robustness of fault models. The stability of the learning process is rigorously analyzed using Lyapunov-based methods, and the effectiveness of the presented method is validated by a rotating stall warning experiment based on the data from Beihang University compressor test rig. Experiment results demonstrate that compared with other methods, the proposed method can achieve better performance in lead warning time and robustness.}
}
@incollection{GOVINDAN20191507,
title = {Simulation-based reinforcement learning for delivery fleet optimisation in CO2 fertilisation networks to enhance food production systems},
editor = {Anton A. Kiss and Edwin Zondervan and Richard Lakerveld and Leyla Özkan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {46},
pages = {1507-1512},
year = {2019},
booktitle = {29th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818634-3.50252-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186343502526},
author = {Rajesh Govindan and Tareq Al-Ansari},
keywords = {CO fertilisation, Simulation, Logistics, Reinforcement Learning},
abstract = {As part of the drive for global food security, all nations will need to intensify food production, including those situated in hyper arid climates. The State of Qatar is one such example of a national system that whilst it is presented with environmental challenges, seeks to enhance food security. There is a consensus that CO2 fertilisation of agricultural systems has the potential to enhance their productivity. In this paper, the authors present a novel study that involves the development of a simulation model of a GIS-based CO2 fertilisation network comprising of power plants equipped with CO2 capture systems, transportation network, including pipeline and roadways, and agricultural sinks, such as greenhouses. The simulation model is used to specifically train the CO2 distribution agent in order to optimise the logistical performance objectives of the network, namely delivery fulfilment and network utilisation rates. The Pareto non-dominating solutions correspond to an optimal CO2 delivery fleet size of around 1-2 trucks for an average year in the simulation example considered.}
}
@article{WAUBERTDEPUISEAU2023101383,
title = {schlably: A Python framework for deep reinforcement learning based scheduling experiments},
journal = {SoftwareX},
volume = {22},
pages = {101383},
year = {2023},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2023.101383},
url = {https://www.sciencedirect.com/science/article/pii/S2352711023000791},
author = {Constantin {Waubert de Puiseau} and Jannik Peters and Christian Dörpelkus and Hasan Tercan and Tobias Meisen},
keywords = {Production scheduling, Deep reinforcement learning, Python, Framework},
abstract = {Research on deep reinforcement learning (DRL) based production scheduling (PS) has gained a lot of attention in recent years, primarily due to the high demand for optimizing scheduling problems in diverse industry settings. Numerous studies are carried out and published as stand-alone experiments that often vary only slightly with respect to problem setups and solution approaches. The programmatic core of these experiments is typically very similar. Despite this fact, no standardized and resilient framework for experimentation on PS problems with DRL algorithms could be established so far. In this paper, we introduce schlably, a Python-based framework that provides researchers a comprehensive toolset to facilitate the development of PS solution strategies based on DRL. schlably eliminates the redundant overhead work that the creation of a sturdy and flexible backbone requires and increases the comparability and reusability of conducted research work.}
}
@incollection{BENYAHIA20211371,
title = {Control of Batch and Continuous Crystallization Processes using Reinforcement Learning},
editor = {Metin Türkay and Rafiqul Gani},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {50},
pages = {1371-1376},
year = {2021},
booktitle = {31st European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-88506-5.50211-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323885065502114},
author = {Brahim Benyahia and Paul Danny Anandan and Chris Rielly},
keywords = {Reinforcement learning, Trajectory tracking control, Batch crystallization, Continuous crystallization, Supersaturation control, Crystal size distribution},
abstract = {In crystallization processes, the control of particle size distribution, shape and purity are crucial to achieve the targeted critical quality attributes of the final drug product and meet the pharmaceutical regulatory requirements. This work presents novel optimal trajectory tracking control strategies for batch and continuous cooling crystallization processes using reinforcement learning (RL). The cooling crystallization of paracetamol in water was used as a case study. A model-based reinforcement learning technique is implemented to achieve large crystal size by reducing the deviation from targeted reference trajectories namely process temperature, supersaturation and particle size. This multioutput tracking control strategy was development to address quality and performance challenges commonly encountered in batch and continuous crystallization processes. Various training strategies and reward functions were investigated to enhance the learning capabilities and robustness of the reinforcement-learning-based control. Despite the computational costs inherent to reinforcement learning, the later demonstrated robust control capabilities compared the benchmark control strategies such as model predictive control.}
}
@article{ZHOU2023280,
title = {Safe reinforcement learning method integrating process knowledge for real-time scheduling of gas supply network},
journal = {Information Sciences},
volume = {633},
pages = {280-304},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.02.084},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523002815},
author = {Pengwei Zhou and Zuhua Xu and Xiaoping Zhu and Jun Zhao and Chunyue Song and Zhijiang Shao},
keywords = {Real-time scheduling, Gas supply network, Safe reinforcement learning, Process knowledge},
abstract = {Gas supply networks play a crucial role in steel enterprises because they provide downstream customers with the gas required for production. In this paper, the real-time scheduling problem of a multi-product gas supply network is first modeled under the framework of reinforcement learning (RL). A safe RL method with prediction and safeguard modules is further developed by utilizing the process knowledge from the gas supply network. The prediction module is designed to predict state changes, and the safeguard module is developed to judge and replace dangerous actions according to current and predicted states. In order to avoid repeated dangerous actions, the safeguard module will provide a negative reward to the agent as feedback whenever a dangerous action is replaced. This active prediction-safeguard strategy is beneficial for reducing trial-and-error costs, speeding up training, and running online. Finally, case studies are implemented on an actual gas supply network to demonstrate the advantages of the proposed method.}
}
@article{DAI2023120650,
title = {A reinforcement learning-enabled iterative learning control strategy of air-conditioning systems for building energy saving by shortening the morning start period},
journal = {Applied Energy},
volume = {334},
pages = {120650},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.120650},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923000144},
author = {Mingkun Dai and Hangxin Li and Shengwei Wang},
keywords = {Iterative learning control, Precooling control, Reinforcement learning, Building energy efficiency, Indoor environment control},
abstract = {Air-conditioning systems in commercial buildings are usually switched on in advance to precool the indoor spaces to create an acceptable working environment upon the office hour. However, the central cooling systems often fail to provide enough cooling supply capacity due to the high cooling demand at the morning start period especially in hot seasons. In this situation, the imbalanced cooling distribution in the air-conditioning systems often results in large difference of cooling-down speed among different building zones, so that the precooling time has to be extended, leading to significant energy waste. This study proposes a new iterative learning control strategy to properly manage the cooling distribution (i.e., water valve openings of air-handling units) for achieving uniform cooling (i.e., synchronously reaching the indoor dry-blub temperature setpoint) among building zones during the morning start period. A reinforcement learning method (Q-learning) is adopted for the control parameter setting of the developed iterative learning controller. Validation tests are conducted and results show that the proposed control strategy could reduce the daily precooling time up to 12.1% during typical days in Hong Kong by achieving uniform cooling. The daily energy consumption could be reduced between 5.1% and 17.8% by shortening morning start period, corresponding a weekly electrical energy saving between 1,376 kWh and 2,916 kWh in the test building.}
}
@article{GOBY2023109165,
title = {Deep reinforcement learning with combinatorial actions spaces: An application to prescriptive maintenance},
journal = {Computers & Industrial Engineering},
volume = {179},
pages = {109165},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109165},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223001894},
author = {Niklas Goby and Tobias Brandt and Dirk Neumann},
keywords = {Maintenance, Prescriptive analytics, Deep reinforcement learning, Combinatorial action space, Prescriptive maintenance},
abstract = {In this paper, we leverage a prescriptive analytics approach based on deep reinforcement learning and adapt it for sequential decision-problems with large, noisy state spaces and combinatorial actions spaces. We implement a novel mechanism that uses deep learning to reduce the action space and apply the approach to the context of maintenance management. We show that our method substantially outperforms established baseline methods from practice and research, closing more than 90 percent of the cost gap between the next-best solution and the optimum under perfect information. In addition to reducing costs, the specifically-designed reward function incentivizes bundling maintenance actions in a way that fully utilizes the available number of workers. Thereby, the number of time steps in which any maintenance action occurs is reduced. This decreases the organizational and operational impact of maintenance in real-world settings as disruptions can be limited to a few days. Beyond this context, our work illustrates the potential of prescriptive approaches based on deep reinforcement learning in other applications that face similarly challenging problem settings.}
}
@article{XIONG202112,
title = {Safety robustness of reinforcement learning policies: A view from robust control},
journal = {Neurocomputing},
volume = {422},
pages = {12-21},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.09.055},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220314636},
author = {Hao Xiong and Xiumin Diao},
keywords = {Safety robustness, Reinforcement learning, Robust control, Deep deterministic policy gradient, Cable-driven parallel robot},
abstract = {For a reinforcement learning (RL) problem without a specified reward function, one may specify different reward functions to better guide an agent to learn. With different reward functions, the agent can learn different policies that generally have different robustness. Both the achieved reward and the success rate have been commonly used to evaluate the robustness of policies. Safety is a concern when using RL to solve problems in many safety–critical applications (e.g., robotic manipulation). However, evaluating the robustness of policies from the perspective of safety has not been discussed in the literature. The major contributions of this paper are the proposal of a novel concept of safety robustness to evaluate the robustness of policies from the perspective of safety and an algorithm to approximate the safety robustness of policies. To demonstrate how to implement the proposed algorithm, illustrative experiments are conducted and the safety robustness of three policies for controlling the manipulation of a cable-driven parallel robot is analyzed. Experiment results show that the proposed algorithm can approximate the safety robustness of policies using the ratio of the number of safe episodes to the number of total episodes and identify the best policy from multiple policies in terms of the safety of policies.}
}
@article{ADAMS2021125915,
title = {Deep reinforcement learning optimization framework for a power generation plant considering performance and environmental issues},
journal = {Journal of Cleaner Production},
volume = {291},
pages = {125915},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.125915},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621001359},
author = {Derrick Adams and Dong-Hoon Oh and Dong-Won Kim and Chang-Ha Lee and Min Oh},
keywords = {Circulating fluidized bed, Coal power generation, NOx emission, Multi-objective optimization, Deep reinforcement learning},
abstract = {In the electric power generation sector, striking a balance between maximum power production and acceptable emission limits is a challenging task that requires sophisticated techniques. With traditional methods, this is an extremely complex issue due to the large number of process variables that are involved. In this paper, a deep reinforcement learning optimization framework (DRLOF) is proposed to determine the optimal operating conditions for a commercial circulating fluidized bed (CFB) power plant that strikes a good balance between performance and environmental issues. The DRLOF included the CFB as an environment created from over 1.5 years of plant data with a 1 min sampling time which interacted with an advantage actor-critic (A2C) agent of two architectures named ‘separate-A2CN’ and ‘shared-A2CN’. The framework was optimized by maximizing electric power generation within the constraints of the plant’s capacity and environmental emission standards, taking into consideration the cost of operations. After training, the framework of the separate-A2CN architecture achieved a 1.97% increase in electricity generation and 1.59% emission reduction for NOx at 14.3 times lower computational cost. Furthermore, we demonstrated the framework’s flexibility, adaptability and lower computational burden by carrying out different test scenarios which demonstrated the effectiveness of the DRLOF. The findings of this study are not limited to the CFB power plant but can be extended to other chemical processes and industries. This approach minimizes the need for costly experiments, online optimization challenges and associated customizations.}
}
@article{LIU2022132,
title = {Quantum reinforcement learning method and application based on value function},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {11},
pages = {132-137},
year = {2022},
note = {IFAC Workshop on Control for Smart Cities CSC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.08.061},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322011508},
author = {Yi-Pei Liu and Qing-Shan Jia and Xu Wang},
keywords = {Quantum Reinforcement Learning, Quantum Computation, Quantum Amplitude Amplification, Bandit, Grid},
abstract = {The Multi-Arm bandit(MAB) problem is a classical problem in the field of reinforcement learning with only one state. The Grid problem is a multi-state problem for reinforcement learning. In this work, we focus on how to combine the classical value function method to quantum computation, and we propose three novel quantum reinforcement learning(QRL) algorithms for the MAB problem and one novel QRL algorithm, which is combined with the quantum random walk and Grover algorithm, for the Grid problem. From the experiments, the learning process is speed-up by combining the value function with quantum computation.}
}
@article{ZHANG20209465,
title = {Robot Navigation among External Autonomous Agents through Deep Reinforcement Learning using Graph Attention Network},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {9465-9470},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2419},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320331013},
author = {Tianle Zhang and Tenghai Qiu and Zhiqiang Pu and Zhen Liu and Jianqiang Yi},
keywords = {Robot navigation, deep reinforcement learning (DRL), graph attention network},
abstract = {Finding collision-free and efficient paths in an uncertain dynamic environment is a challenge for robot navigation tasks, especially when there are external autonomous agents that also have decision-making abilities in the same environment. This paper develops a novel method based on DRL with graph attention network (GAT) to solve the problem of robot navigation among external autonomous agents (other agents). Specifically, GAT is adopted to describe the robot and other agents as a specific graph, and extract the spatial structural influence features of other agents on the robot from the graph. Multi-head attention mechanism is utilized to calculate the weights of interactions between the robot and other agents. This GAT uses observations of an arbitrary number of other agents in dynamic environments. Furthermore, the proposed method is combined with optimal reciprocal collision avoidance to improve its safety in new environments. Various simulations demonstrate that our method has good performance and robustness in different environments.}
}
@article{YANG2021102630,
title = {Humanoid motion planning of robotic arm based on human arm action feature and reinforcement learning},
journal = {Mechatronics},
volume = {78},
pages = {102630},
year = {2021},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2021.102630},
url = {https://www.sciencedirect.com/science/article/pii/S0957415821001082},
author = {Aolei Yang and Yanling Chen and Wasif Naeem and Minrui Fei and Ling Chen},
keywords = {Human arm action feature, Humanoid motion, Reward function, Reinforcement learning},
abstract = {The use and application of robotic arms in helping the aged and vulnerable persons are increasing gradually. In order to achieve safer and reliable human-robot interaction and its wider adoption, the requirements for the humanoid motion of robotic arms are becoming more stringent. This paper presents a humanoid motion planning method for a robotic arm based on the physics of human arm and reinforcement learning. Firstly, the humanoid motion rules are extracted by analyzing and learning the action data of human arm, which is collected using the VICON optical motion capture system. Then, according to the acquired features and rules, the corresponding reward functions are proposed and the humanoid motion training of the robotic arm is carried out by using the reinforcement learning based on Deep Deterministic Policy Gradient (DDPG) and Hindsight Experience Replay (HER) algorithm. Finally, the experiments are carried out to verify whether the robotic arm motions planned by the proposed approach are humanoid, and the observed results show its feasibility and effectiveness in planning the humanoid motion of the robotic arm.}
}
@article{PRADEEP2018218,
title = {A Finite Horizon Markov Decision Process Based Reinforcement Learning Control of a Rapid Thermal Processing system},
journal = {Journal of Process Control},
volume = {68},
pages = {218-225},
year = {2018},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0959152418301161},
author = {D. John Pradeep and Mathew Mithra Noel},
keywords = {Reinforcement Learning, Rapid Thermal Processing, Nonlinear control, Markov Decision Process, Process control, Multivariable control},
abstract = {Manufacture of ultra large-scale integrated circuits involves accurate control of a challenging nonlinear Rapid Thermal Processing (RTP) system. Precise control of temperature profile and rapid ramp-up and ramp-down rates demanded by a RTP system cannot be achieved with conventional control strategies due to nonlinear and multi time-scale effects. In this paper the control of a RTP system is reformulated as an optimal multi-step sequential decision problem using the framework of finite horizon Markov decision processes and solved using a Reinforcement Learning (RL) algorithm. Three increasingly complex RL based control strategies are explored and compared with the existing state-of-the-art approach for controlling RTPs. Simulation results indicate that the approach proposed in this paper achieves superior control of the temperature profile and ramp-up and ramp-down rates for the RTP system.}
}
@article{LEVINSON2023104136,
title = {Connecting planning horizons in mining complexes with reinforcement learning and stochastic programming},
journal = {Resources Policy},
volume = {86},
pages = {104136},
year = {2023},
issn = {0301-4207},
doi = {https://doi.org/10.1016/j.resourpol.2023.104136},
url = {https://www.sciencedirect.com/science/article/pii/S0301420723008474},
author = {Zachary Levinson and Roussos Dimitrakopoulos},
keywords = {Mining complex, Short-term mine planning, Long-term mine planning, Stochastic programming, Reinforcement learning, Metaheuristics},
abstract = {Connecting short- and long-term production schedules in mining complexes is essential to ensure that the long-term production schedule is achievable at shorter timescales. Previous research that addresses optimizing mining complexes under uncertainty focus on simultaneously optimizing different components in the mining complex to capitalize on advantageous synergies. Typically, short- and long-term production schedules are optimized separately in a number of stages. This poses risk of schedule misalignment, which can adversely affect the economic outcome of a mining complex and the ability to meet long-term production forecasts at shorter timescales. A framework is proposed to jointly optimize short- and long-term production schedules by connecting planning horizons with stochastic mathematical programming and reinforcement learning. The solution approach is tested in a large operating copper mining complex and demonstrates significant improvements in the resulting production and financial forecasts.}
}
@article{HAQ2022560,
title = {Implementation of home energy management system based on reinforcement learning},
journal = {Energy Reports},
volume = {8},
pages = {560-566},
year = {2022},
note = {2021 The 8th International Conference on Power and Energy Systems Engineering},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.11.170},
url = {https://www.sciencedirect.com/science/article/pii/S2352484721013172},
author = {Ejaz Ul Haq and Cheng Lyu and Peng Xie and Shuo Yan and Fiaz Ahmad and Youwei Jia},
keywords = {Home energy management system, Reinforcement learning, Energy cost, Thermal comfort, Energy storage systems},
abstract = {The implementation of machine learning methods in home energy management have been shown to be a feasible alternative in the minimization of electricity cost. These methods regulate the home electric appliance systems, which contribute to the most critical loads in a household, thus enabling consumers to save electricity while still enhancing their comfort. Furthermore, renewable energy supplies are continuously integrating with other electricity resources in number of homes that is an important component to optimize energy consumption which result in the reduction of peak load and can bring economic benefits. In this paper, a reinforcement learning algorithm is explored for monitoring household electric appliances with the intention of lowering energy consumption through properly optimizing and addressing the best use renewable energy resources. The proposed method does not necessitate any previous information or knowledge of the uncertain dynamics and parameters of different household electric appliances. Simulation-based findings using real-time data validate the efficiency and reliability of the proposed method.}
}
@article{AJAGEKAR2023100119,
title = {Energy-efficient AI-based Control of Semi-closed Greenhouses Leveraging Robust Optimization in Deep Reinforcement Learning},
journal = {Advances in Applied Energy},
volume = {9},
pages = {100119},
year = {2023},
issn = {2666-7924},
doi = {https://doi.org/10.1016/j.adapen.2022.100119},
url = {https://www.sciencedirect.com/science/article/pii/S2666792422000373},
author = {Akshay Ajagekar and Neil S. Mattson and Fengqi You},
keywords = {Artificial intelligence, climate control, deep reinforcement learning, robust optimization, energy efficiency, greenhouse},
abstract = {As greenhouses are being widely adopted worldwide, it is important to improve the energy efficiency of the control systems while accurately regulating their indoor climate to realize sustainable agricultural practices for food production. In this work, we propose an artificial intelligence (AI)-based control framework that combines deep reinforcement learning techniques to generate insights into greenhouse operation combined with robust optimization to produce energy-efficient controls by hedging against associated uncertainties. The proposed control strategy is capable of learning from historical greenhouse climate trajectories while adapting to current climatic conditions and disturbances like time-varying crop growth and outdoor weather. We evaluate the performance of the proposed AI-based control strategy against state-of-the-art model-based and model-free approaches like certainty-equivalent model predictive control, robust model predictive control (RMPC), and deep deterministic policy gradient. Based on the computational results obtained for the tomato crop's greenhouse climate control case study, the proposed control technique demonstrates a significant reduction in energy consumption of 57% over traditional control techniques. The AI-based control framework also produces robust controls that are not overly conservative, with an improvement in deviation from setpoints of over 26.8% as compared to the baseline control approach RMPC.}
}
@article{SHA2022110092,
title = {Fully asynchronous policy evaluation in distributed reinforcement learning over networks},
journal = {Automatica},
volume = {136},
pages = {110092},
year = {2022},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2021.110092},
url = {https://www.sciencedirect.com/science/article/pii/S000510982100621X},
author = {Xingyu Sha and Jiaqi Zhang and Keyou You and Kaiqing Zhang and Tamer Başar},
keywords = {Distributed reinforcement learning, Multi-agent networks, Fully asynchronous updates, Policy evaluation},
abstract = {This paper proposes a fully asynchronous scheme for the policy evaluation problem of distributed reinforcement learning (DisRL) over directed peer-to-peer networks. Without waiting for any other node of the network, each node can locally update its value function at any time using (possibly delayed) information from its neighbors. This is in sharp contrast to the gossip-based scheme where a pair of nodes concurrently update. Even though the fully asynchronous setting involves a difficult multi-timescale decision problem, we design a novel incremental aggregated gradient (IAG) based distributed algorithm and develop a push–pull augmented graph approach to prove its exact convergence at a linear rate of O(ck) where c∈(0,1) and k is the total number of updates within the entire network. Finally, numerical experiments validate that our method speeds up linearly with respect to the number of nodes, and is robust to straggler nodes.}
}
@article{WANG202131,
title = {Integral reinforcement learning-based optimal output feedback control for linear continuous-time systems with input delay},
journal = {Neurocomputing},
volume = {460},
pages = {31-38},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.06.073},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221010031},
author = {Gao Wang and Biao Luo and Shan Xue},
keywords = {Integral reinforcement learning, Output-feedback, Optimal control, Input delay},
abstract = {In this paper, an integral reinforcement learning (IRL)-based model-free optimal output-feedback (OPFB) control scheme is developed for linear continuous-time systems with input delay, where the input and past output data are employed rather than the system dynamic model. First, the equivalence between the delayed optimal control and delay-free case is analyzed. Subsequently, the system state is constructed with output signal and the Bellman equation is written in the form of past outputs. Therefore, the IRL algorithm is developed to learn the OPFB control policy, where the iterative policy is evaluated and improved simultaneously. It is proved that the obtained optimal OPFB controller gives the same solution as the optimal state-feedback. Finally, the presented simulation results illustrate the effectiveness of the developed control method.}
}
@article{MIZUYAMA202320,
title = {Reinforcement learning approach for characterizing a suitable cognitive framework of a dynamic slab-yard control decision-making process},
journal = {Procedia CIRP},
volume = {118},
pages = {20-25},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123002275},
author = {Hajime Mizuyama},
keywords = {Dynamic decision-making process, production-control decisions, reinforcement learning, serious games, slab-yard control},
abstract = {A slab yard upstream of a heating furnace in a steel factory is controlled by a human operator with a crane in a dynamic environment, where new slabs arrive at the yard, and heated slabs depart from the furnace stochastically. The performance of this dynamic slab-yard control decision-making process depends, at least partly, on how the operator cognizes the decision-making problem. Thus, it is essential to characterize the suitable cognitive framework of the problem not only to enhance and stabilize the performance but also to support the decision-making process effectively. This paper proposes a reinforcement learning approach built around a serious game model that mimics the production control task for this challenge. The interface aspect of the suitable cognitive framework for the task is characterized by conducting numerical experiments using the models. The experiments show that the suitable interface depends on the congestion of the yard.}
}
@article{ZHAO2023119672,
title = {A multi-agent reinforcement learning driven artificial bee colony algorithm with the central controller},
journal = {Expert Systems with Applications},
volume = {219},
pages = {119672},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119672},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423001732},
author = {Fuqing Zhao and Zhenyu Wang and Ling Wang and Tianpeng Xu and Ningning Zhu and  Jonrinaldi},
keywords = {Reinforcement learning, Artificial bee colony algorithm, Multi-agent central controller, Q-learning mechanism, Large-scale},
abstract = {The large-scale real value problem in continuous and discrete optimization problems is a challenging issue for researchers and practitioners in the manufacturing domain. An improved artificial bee colony algorithm (ABC) combined with multi-agent reinforcement learning (called MARLABC) is presented for addressing the large-scale real value optimization problem in this study. Two stages including the training and the testing are introduced in the MARLABC via the multi-agent central controller to improve the convergence speed and local exploitation capability of the algorithm. The optimal strategy pool is constructed by training procedures via the multi-agent central controller with Q-learning mechanism. The effective strategy is selected from the optimal strategy pool for each agent during the testing process in the multi-agent central controller. The elite agents in the training population are reserved to generate the testing population to guide the search. The MARLABC algorithm is compared with the ABC variants and state-of-art algorithms on CEC 2017 benchmark problems. The stability and effectiveness of the MARLABC algorithm are confirmed by the experimental results.}
}
@article{CHEN2022104116,
title = {Reinforcement learning control for the swimming motions of a beaver-like, single-legged robot based on biological inspiration},
journal = {Robotics and Autonomous Systems},
volume = {154},
pages = {104116},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104116},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022000653},
author = {Gang Chen and Yuwang Lu and Xin Yang and Huosheng Hu},
keywords = {Reinforcement learning control, Q-learning, Beaver-like, Swimming, Underwater bionic robots},
abstract = {Complex hydrodynamic modeling and analysis are considered as stumbling blocks in the motion study of underwater bionic robots. In recent years, reinforcement learning techniques have been applied for robot motion control in unknown environments. However, robots may act in an unconventional or dangerous manner during the learning process. These actions increase the training difficulty and decrease the training efficiency. In this study, a biological-inspired reinforcement learning control method is proposed. It realizes the self-learning movement policy of the robot with discretized swimming motions of a beaver without the need to establish motion models, such as hydrodynamics, of underwater robots. The biological-inspired model further reduces the robot’s ineffective movements during the reinforcement learning and improves training efficiency. The experiment results verify the environmental adaptation and self-learning ability of the proposed robot platform and proves the effectiveness of the reinforcement learning control method for robotic swimming based on biological inspiration. This study’s findings provide new ideas for the motion control of underwater bionic robots and further promote the application of artificial intelligence in underwater robots.}
}
@article{LIN2020135,
title = {Event-triggered reinforcement learning control for the quadrotor UAV with actuator saturation},
journal = {Neurocomputing},
volume = {415},
pages = {135-145},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.07.042},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220311504},
author = {Xiaobo Lin and Jian Liu and Yao Yu and Changyin Sun},
keywords = {Quadrotor, UAV, Reinforcement learning, Flight control, Event-triggered control, Actuator saturation},
abstract = {This paper proposes an event-triggered reinforcement learning (RL) control strategy to stabilize the quadrotor unmanned aerial vehicle (UAV) with actuator saturation. As the quadrotor UAV equips with a complex dynamic is difficult to be model accurately, a model free reinforcement learning scheme is designed. Due to the practical limitation of actuators, the end of controller is constrained with a bounded function. In order to reduce the calculation consumption for the onboard computer, an event-triggered mechanism is developed, which only update the controller when the triggered condition is satisfied. The proposed controller is implemented with two neural networks which are called critic and actor. Some advanced RL technologies are utilized for speeding up the train process, e.g. off-policy training, experience replay, etc. The stability of closed-loop system is proved by the Lyapunov analysis. The simulation results including a stability task and a tracking task verify the theoretical analysis, in which we find the updating frequency of controller is decreased greatly.}
}
@article{BROWN2022110672,
title = {“Deep reinforcement learning for engineering design through topology optimization of elementally discretized design domains”},
journal = {Materials & Design},
volume = {218},
pages = {110672},
year = {2022},
issn = {0264-1275},
doi = {https://doi.org/10.1016/j.matdes.2022.110672},
url = {https://www.sciencedirect.com/science/article/pii/S0264127522002933},
author = {Nathan K. Brown and Anthony P. Garland and Georges M. Fadel and Gang Li},
keywords = {Reinforcement learning, Topology optimization, Deep learning, Engineering design, Structural design, Data-driven},
abstract = {Advances in machine learning algorithms and increased computational efficiencies give engineers new capabilities and tools to apply to engineering design. Machine learning models can approximate complex functions and, therefore, can be useful for various tasks in the engineering design workflow. This paper investigates using reinforcement learning (RL), a subset of machine learning that teaches an agent to complete a task through accumulating experiences in an interactive environment, to automate the designing of 2D discretized topologies. RL agents use past experiences to learn sequential sets of actions to best achieve some objective. In the proposed environment, an RL agent can make sequential decisions to design a topology by removing elements to best satisfy compliance minimization objectives. After each action, the agent receives feedback by evaluating how well the current topology satisfies the design objectives. After training, the agent was tasked with designing optimal topologies under various load cases. The agent's proposed designs had similar or better compliance minimization performance to those produced by traditional gradient-based topology optimization methods. These results show that a deep RL agent can learn generalized design strategies to satisfy multi-objective design tasks and, therefore, shows promise as a tool for arbitrarily complex design problems across many domains.}
}
@article{ERHARTER2021103701,
title = {Reinforcement learning based process optimization and strategy development in conventional tunneling},
journal = {Automation in Construction},
volume = {127},
pages = {103701},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103701},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521001527},
author = {Georg H. Erharter and Tom F. Hansen and Zhongqiang Liu and Thomas Marcher},
keywords = {Conventional tunneling, Reinforcement learning, Tunnel excavation strategy, Machine learning, Excavation sequences},
abstract = {Reinforcement learning (RL) - a branch of machine learning - refers to the process of an agent learning to achieve a certain goal by interaction with its environment. The process of conventional tunneling shows many similarities, where a geotechnician (agent) tries to achieve a breakthrough (goal) by excavating the rockmass (environment) in an optimum way. In this paper we present a novel RL based framework for strategy development for conventional tunneling. We developed a virtual environment with the goal of a tunnel breakthrough and with a deep Q-network as the agent's architecture. It can choose from different excavation sequences to reach that goal and learns to do so in an economical and safe way by getting feedback from a specially designed reward system. Result analyses show that the optimal policies have great similarities to current practices of sequential tunneling and the framework has the potential to discover new tunneling strategies.}
}
@article{YANG2022279,
title = {Optimal sensor scheduling for remote state estimation with limited bandwidth: a deep reinforcement learning approach},
journal = {Information Sciences},
volume = {588},
pages = {279-292},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.12.043},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521012652},
author = {Lixin Yang and Hongxia Rao and Ming Lin and Yong Xu and Peng Shi},
keywords = {Sensor scheduling, State estimation, Markov decision process, Deep reinforcement learning},
abstract = {This paper considers co-scheduling for multiple sensors to observe multiple dynamical systems. The measurements obtained by the sensors need to be transmitted to the remote estimator over a shared network with packet dropouts, where only a part of sensors can access the network due to its limited bandwidth. To improve the estimation performance, a scheduler is adopted to determine which systems to be observed and which sensors to access the network based on the real-time information of each system. For this issue, the co-scheduling protocol is formulated as an associated Markov decision process (MDP) and the existence of an optimal deterministic and stationary policy (DSP) is proved. Then a Deep Q-Network (DQN) is developed to solve the MDP in a scalable and model-free manner. A practical example of vehicle moving is presented to compare the DQN method with some other existing scheduling protocols, and the results show that the developed approach significantly outperforms other protocols in all kinds of situations.}
}
@article{REN201996,
title = {Integral reinforcement learning off-policy method for solving nonlinear multi-player nonzero-sum games with saturated actuator},
journal = {Neurocomputing},
volume = {335},
pages = {96-104},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.01.033},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219300451},
author = {He Ren and Huaguang Zhang and Yinlei Wen and Chong Liu},
keywords = {Adaptive dynamic programming, Nonzero-sum game, Reinforcement learning, Off-policy, Constrained optimal control},
abstract = {In this paper, an effective off-policy algorithm is proposed to solve the continuous time nonzero-sum (NZS) control problem for unknown nonlinear systems with saturated actuator. A class of nonquadratic function is used to construct the performance functions to deal with constrained inputs. Utilizing the integral reinforcement learning (IRL) technique, the off-policy learning mechanism is introduced to design an iterative method for the continuous-time NZS constrained control problem without requiring the knowledge of system dynamics. To show the convergence of the proposed method, the traditional policy iteration (PI) method is discussed for the continuous-time NZS control problem with saturated actuator at first. Then, the equivalence of the proposed method with the traditional PI method is proved. Neural networks are introduced to construct the actor-critic structure, where the critic neural networks are aimed at approximating the iterative value functions and the actor neural networks are aimed at approximating the iterative control policies. Finally, two cases are simulated to verify the effectiveness of the proposed method.}
}
@article{CHEN2020106778,
title = {A self-learning genetic algorithm based on reinforcement learning for flexible job-shop scheduling problem},
journal = {Computers & Industrial Engineering},
volume = {149},
pages = {106778},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106778},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220304885},
author = {Ronghua Chen and Bo Yang and Shi Li and Shilong Wang},
keywords = {Flexible job-shop scheduling problem (FJSP), Self-learning genetic algorithm (SLGA), Genetic algorithm (GA), Reinforcement learning (RL)},
abstract = {As an important branch of production scheduling, flexible job-shop scheduling problem (FJSP) is difficult to solve and is proven to be NP-hard. Many intelligent algorithms have been proposed to solve FJSP, but their key parameters cannot be dynamically adjusted effectively during the calculation process, which causes the solution efficiency and quality not being able to meet the production requirements. Therefore, a self-learning genetic algorithm (SLGA) is proposed in this paper, in which genetic algorithm (GA) is adopted as the basic optimization method and its key parameters are intelligently adjusted based on reinforcement learning (RL). Firstly, the self-learning model is analyzed and constructed in SLGA, SARSA algorithm and Q-Learning algorithm are applied as the learning methods at initial and later stages of optimization, respectively, and the conversion condition is designed. Secondly, the state determination method and reward method are designed for RL in GA environment. Finally, the learning effect and performance of SLGA in solving FJSP are compared with other algorithms using two groups of benchmark data instances with different scales. Experiment results show that the proposed SLGA significantly outperforms its competitors in solving FJSP.}
}
@article{XIA2013413,
title = {Application of Reinforcement Learning to Switched Control of Hybrid Systems},
journal = {IFAC Proceedings Volumes},
volume = {46},
number = {13},
pages = {413-418},
year = {2013},
note = {13th IFAC Symposium on Large Scale Complex Systems: Theory and Applications},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20130708-3-CN-2036.00024},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016302920},
author = {Jiajun Xia and Jianhua Zhang},
abstract = {In this paper, we try to apply learning automata with reinforcement learning (RL) algorithm to realize switched control of hybrid systems. The learning automata were used to optimize the generation probabilities of behaviors in order to achieve a satisfactory result. At first we apply RL to an experiment proposed in other's previous work, then we do some reproduction in order to compare two different methods. Since there are differences in targets, we generalized former RL algorithm. Simulation results demonstrate the feasibility and advantage of applying RL to switched control of hybrid systems.}
}
@article{GUNTHER2014474,
title = {First Steps Towards an Intelligent Laser Welding Architecture Using Deep Neural Networks and Reinforcement Learning},
journal = {Procedia Technology},
volume = {15},
pages = {474-483},
year = {2014},
note = {2nd International Conference on System-Integrated Intelligence: Challenges for Product and Production Engineering},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314001224},
author = {Johannes Günther and Patrick M. Pilarski and Gerhard Helfrich and Hao Shen and Klaus Diepold},
keywords = {Deep learning, reinforcement learning, prediction, control, laser welding},
abstract = {To address control difficulties in laser welding, we propose the idea of a self-learning and self-improving laser welding system that combines three modern machine learning techniques. We first show the ability of a deep neural network to extract meaningful, low-dimensional features from high-dimensional laser-welding camera data. These features are then used by a temporal-difference learning algorithm to predict and anticipate important aspects of the system's sensor data. The third part of our proposed architecture suggests using these features and predictions to learn to deliver situation-appropriate welding power; preliminary control results are demonstrated using a laser-welding simulator. The intelligent laser-welding architecture introduced in this work has the capacity to improve its performance without further human assistance and therefore addresses key requirements of modern industry. To our knowledge, it is the first demonstrated combination of deep learning and Nexting with general value functions and also the first usage of deep learning for laser welding specifically and production engineering in general. This work also provides a unique example of how predictions can be explicitly learned using reinforcement learning to support laser welding. We believe that it would be straightforward to adapt our approach to other production engineering applications.}
}
@article{HERNANDEZDELOLMO20122355,
title = {An emergent approach for the control of wastewater treatment plants by means of reinforcement learning techniques},
journal = {Expert Systems with Applications},
volume = {39},
number = {3},
pages = {2355-2360},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2011.08.062},
url = {https://www.sciencedirect.com/science/article/pii/S0957417411011742},
author = {Félix Hernández-del-Olmo and Félix H. Llanes and Elena Gaudioso},
keywords = {Artificial intelligence, Emergent approach, Reinforcement learning, Control of wastewater treatment plants},
abstract = {One of the main problems in the automation of the control of wastewater treatment plants (WWTPs) appears when the control system does not respond as it should because of changes on influent load or flow. To tackle this difficult task, the application of Artificial Intelligence is not new, and in fact, currently Expert Systems may supervise the plant 24h/day assisting the plant operators in their daily work. However, the knowledge of the Expert System must be elicited previously from interviews to plant operators and/or extracted from data previously stored in databases. Although this approach still has a place in the control of wastewater treatment plants, it should aim to develop autonomous systems that learn from the direct interaction with the WWTP and that can operate taking into account changing environmental circumstances. In this paper we present an approach based on an agent with learning capabilities. In this approach, the agent’s knowledge emerges from the interaction with the plant. In order to show the validity of our assertions, we have implemented such an emergent approach for the N-Ammonia removal process in a well established simulated WWTP known as Benchmark Simulation Model No.1 (BSM1).}
}
@article{NADIM2023106853,
title = {Learn-to-supervise: Causal reinforcement learning for high-level control in industrial processes},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106853},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106853},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623010370},
author = {Karim Nadim and Mohamed-Salah Ouali and Hakim Ghezzaz and Ahmed Ragab},
keywords = {Supervisory control, Causality analysis, Deep reinforcement learning, Process mining, Discrete event systems, Energy efficiency},
abstract = {Possessing efficient supervisory control systems is crucial for maintaining the desired operational performance of complex industrial processes. Several challenges face the developers of these systems, such as requiring accurate physical models, dealing with the variability and uncertainty of process operating conditions and coordinating between local controllers to reach desired global performance. This paper proposes an intelligent supervisory control approach based on causal reinforcement learning (CRL) to effectively manipulate the controllers’ setpoints of the process in a way that optimizes its key performance indicators (KPIs), thereby improving the energy efficiency of the process. The approach adopts deep reinforcement learning (DRL) to develop an efficient control policy through interaction with a process simulation. The DRL training history is then exploited using interpretable machine learning and process mining to build a discrete event system (DES) model, in the form of a state-event graph. The DES model identifies causal relationships between events and provides interpretability to the control policy developed by the DRL method. The DES discovered is exploited as a Markov decision process to apply the Q-learning algorithm as a CRL supervisor. The supervisor incorporates causal knowledge into its training process, thus improving the DRL control policy developed and identifying the event paths that optimize the process’s KPIs. The proposed approach is validated using two heat recovery systems in a pulp & paper mill. It successfully achieves a control policy that reduces energy consumption by up to 15.6% for the first system and 5.02% for the second, compared to the expert’s baseline methods.}
}
@article{DEY2023100255,
title = {Reinforcement learning building control approach harnessing imitation learning},
journal = {Energy and AI},
volume = {14},
pages = {100255},
year = {2023},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2023.100255},
url = {https://www.sciencedirect.com/science/article/pii/S2666546823000277},
author = {Sourav Dey and Thibault Marzullo and Xiangyu Zhang and Gregor Henze},
keywords = {Reinforcement learning, Building controls, Imitation learning, Artificial intelligence},
abstract = {Reinforcement learning (RL) has shown significant success in sequential decision making in fields like autonomous vehicles, robotics, marketing and gaming industries. This success has attracted the attention to the RL control approach for building energy systems which are becoming complicated due to the need to optimize for multiple, potentially conflicting, goals like occupant comfort, energy use and grid interactivity. However, for real world applications, RL has several drawbacks like requiring large training data and time, and unstable control behavior during the early exploration process making it infeasible for an application directly to building control tasks. To address these issues, an imitation learning approach is utilized herein where the RL agents starts with a policy transferred from accepted rule based policies and heuristic policies. This approach is successful in reducing the training time, preventing the unstable early exploration behavior and improving upon an accepted rule-based policy — all of these make RL a more practical control approach for real world applications in the domain of building controls.}
}
@article{MAEDA2021102108,
title = {Automating post-exploitation with deep reinforcement learning},
journal = {Computers & Security},
volume = {100},
pages = {102108},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.102108},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820303813},
author = {Ryusei Maeda and Mamoru Mimura},
keywords = {Reinforcement learning, Post-exploitation, A2C, Q-Learning, SARSA, Deep reinforcement learning, Lateral movement},
abstract = {In order to assess the risk of information systems, it is important to investigate the behavior of the attacker after successful exploitation (post-exploitation). However, the audit requires the experts, and to the best of our knowledge, there are no solutions to automate this process. This paper proposes a method of automating post-exploitation by combining deep reinforcement learning and the PowerShell Empire, which is famous as a post-exploitation framework. Our reinforcement learning agents select one of the PowerShell Empire modules as an action. The state of the agents is defined by 10 parameters such as type of account that was compromised by the agents. In the learning phase, we compared the learning progress of the 3 reinforcement learning models: A2C, Q-Learning, and SARSA. The result shows that the A2C could gain reward most efficiently. Moreover, the behavior of the trained agents are evaluated in a test domain network. The results show that the trained agent using A2C could obtain the administrative privileges to the domain controller.}
}
@article{JUNG2023108979,
title = {On the value of operational flexibility in the trailer shipment and assignment problem: Data-driven approaches and reinforcement learning},
journal = {International Journal of Production Economics},
volume = {264},
pages = {108979},
year = {2023},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2023.108979},
url = {https://www.sciencedirect.com/science/article/pii/S0925527323002116},
author = {Seung Hwan Jung and Yunsi Yang},
keywords = {Data analytics, Reinforcement learning, Trailer shipment, Trailer allocation, Third-party logistics},
abstract = {This paper addresses the trailer shipment problem—the task of managing the optimal weight of products in a trailer, taking into consideration the uncertain weight of tractors provided by Third-Party Logistics (3PL) providers, and abiding by the gross weight regulation. We propose a series of data-analytics methodologies, including Sample Average Approximation (SAA), Empirical Risk Minimization (ERM), and a dynamic trailer assignment methodology using Reinforcement Learning (RL), to optimize the trailer shipment process. The introduction of operational flexibility and the dynamic utilization of tractor weight information upon arrival are pivotal to the effectiveness of the RL-based methodology. To validate our approaches, we apply them to transaction-level shipping data from a real company. The results demonstrate significant cost reductions in the logistics process, driven by the dynamic assignment methodology which allows efficient selection of trailers to suit varying tractor weights. This research proposes an innovative approach to the prevalent trailer shipment problem, applicable to a wide range of industries using 3PL outsourcing. Through this work, we demonstrate the transformative potential of data-analytics methodologies to enhance efficiency and profitability in logistics operations.}
}
@article{HESSE201815,
title = {A Reinforcement Learning Strategy for the Swing-Up of the Double Pendulum on a Cart},
journal = {Procedia Manufacturing},
volume = {24},
pages = {15-20},
year = {2018},
note = {4th International Conference on System-Integrated Intelligence: Intelligent, Flexible and Connected Systems in Products and Production},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918305134},
author = {Michael Hesse and Julia Timmermann and Eyke Hüllermeier and Ansgar Trächtler},
keywords = {Reinforcement Learning, PILCO, double pendulum, experimental validation},
abstract = {The effective control design of a dynamical system traditionally relies on a high level of system understanding, usually expressed in terms of an exact physical model. In contrast to this, reinforcement learning adopts a data-driven approach and constructs an optimal control strategy by interacting with the underlying system. To keep the wear of real-world systems as low as possible, the learning process should be short. In our research, we used the state-of-the-art reinforcement learning method PILCO to design a feedback control strategy for the swing-up of the double pendulum on a cart with remarkably few test iterations at the test bench. PILCO stands for “probabilistic inference for learning control” and requires only few expert knowledge for learning. To achieve the swing-up of a double pendulum on a cart to its upper unstable equilibrium position, we introduce additional state restrictions to PILCO, so that the limited cart distance can be taken into account. Thanks to these measures, we were able to learn the swing up at the real test bench for the first time and in only 27 learning iterations.}
}
@article{JIN2023106075,
title = {Controlling fracture propagation using deep reinforcement learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {122},
pages = {106075},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106075},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623002592},
author = {Yuteng Jin and Siddharth Misra},
keywords = {Reinforcement learning, Policy gradient, Discontinuity, Propagation, Control},
abstract = {Mechanical discontinuity embedded in a material plays an essential role in determining the bulk mechanical, physical, and chemical properties. The ability to control mechanical discontinuity is relevant for industries dependent on natural, synthetic and composite materials, e.g. construction, aerospace, oil and gas, ceramics, metal, and geothermal industries, to name a few. The paper is a proof-of-concept development of a reinforcement learning framework to control the propagation of mechanical discontinuity. The reinforcement learning framework is coupled with an OpenAI-Gym-based environment that uses the mechanistic equation governing the propagation of mechanical discontinuity. Learning agent does not explicitly know about the underlying physics of propagation of discontinuity; nonetheless, the learning agent can infer the control strategy by continuously interacting the environment. The design of Markov decision process, which includes state, action and reward, is crucial for robust control. The deep deterministic policy gradient (DDPG) algorithm is implemented for learning continuous actions. It is also observed that the training efficiency is strongly determined by the formulation of reward function. The reward function that forces the learning agent to stay on the shortest linear path between crack tip and goal point performs much better than the reward function that aims to reach closest to the goal point in minimum number of steps. After close to 500 training episodes, the reinforcement learning framework successfully controlled the propagation of discontinuity in a material despite the complexity of the propagation pathway determined by multiple goal points.}
}
@article{ZHANG2018803,
title = {Trajectory tracking control for rotary steerable systems using interval type-2 fuzzy logic and reinforcement learning},
journal = {Journal of the Franklin Institute},
volume = {355},
number = {2},
pages = {803-826},
year = {2018},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2017.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0016003217306324},
author = {Chi Zhang and Wei Zou and Ningbo Cheng and Junshan Gao},
abstract = {Rotary steerable system (RSS) is a directional drilling technique which has been applied in oil and gas exploration under complex environment for the requirements of fossil energy and geological prospecting. The nonlinearities and uncertainties which are caused by dynamical device, mechanical structure, extreme downhole environment and requirements of complex trajectory design in the actual drilling work increase the difficulties of accurate trajectory tracking. This paper proposes a model-based dual-loop feedback cooperative control method based on interval type-2 fuzzy logic control (IT2FLC) and actor-critic reinforcement learning (RL) algorithms with one-order digital low-pass filters (LPF) for three-dimensional trajectory tracking of RSS. In the proposed RSS trajectory tracking control architecture, an IT2FLC is utilized to deal with system nonlinearities and uncertainties, and an online iterative actor-critic RL controller structured by radial basis function neural networks (RBFNN) and adaptive dynamic programming (ADP) is exploited to eliminate the stick–slip oscillations relying on its approximate properties both in action function (actor) and value function (critic). The two control effects are fused to constitute cooperative controller to realize accurate trajectory tracking of RSS. The effectiveness of our controller is validated by simulations on designed function tests for angle building hole rate and complete downhole trajectory tracking, and by comparisons with other control methods.}
}
@article{XIAO2017114,
title = {General value iteration based reinforcement learning for solving optimal tracking control problem of continuous–time affine nonlinear systems},
journal = {Neurocomputing},
volume = {245},
pages = {114-123},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.03.038},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217305532},
author = {Geyang Xiao and Huaguang Zhang and Yanhong Luo and Qiuxia Qu},
keywords = {Adaptive dynamic programming, Optimal control, Reinforcement learning, Continuous–time systems},
abstract = {In this paper, a novel reinforcement learning (RL) based approach is proposed to solve the optimal tracking control problem (OTCP) for continuous–time (CT) affine nonlinear systems using general value iteration (VI). First, the tracking performance criterion is described in a total-cost manner without a discount term which can ensure the asymptotic stability of the tracking error. Then, some mild assumptions are assumed to relax the restriction of the initial admissible control in most existing references. Based on the proposed assumptions, the general VI method is proposed and three situations are considered to show the convergence with any initial positive performance function. To validate the theoretical results, the proposed general VI method is implemented by two neural networks on a nonlinear spring–mass–damper system and two situations are considered to show the effectiveness.}
}
@article{KOPACZ2021104316,
title = {Deep replacement: Reinforcement learning based constellation management and autonomous replacement},
journal = {Engineering Applications of Artificial Intelligence},
volume = {104},
pages = {104316},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104316},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621001640},
author = {Joseph Kopacz and Jason Roney and Roman Herschitz},
abstract = {The Deep Reinforcement Learning (DRL) algorithm, Proximal Policy Optimization (PPO2), is deployed on a custom spacecraft (S/C) build and loss model to determine if an Artificial Intelligence (AI) can learn to monitor satellite constellation health and determine an optimal replacement strategy. A custom environment is created to simulate how S/C are built, launched, generate revenue, and finally decay. The reinforcement learning agent successfully learned an optimal policy for two models: a Simplified Model where the financial cost of actions is ignored; and an Advanced Model where the financial cost of actions is a major element. In both models the AI monitors the constellations and takes multiple strategic and tactical actions to replace satellites to maintain constellation performance. The Simplified Model showed that the PPO2 algorithm was able to converge on an optimal solution after ∼200,000 simulations. The Advanced Model was much more difficult for the AI to learn, and thus, the performance drops during the early episodes, but eventually converges to an optimal policy at ∼25,000,000 simulations. With the Advanced Model, the AI is taking actions that are successfully providing strategies for constellation management and satellite replacements which include these actions’ financial implications. Thus, the methods in this paper provide initial research developments towards a real-world tool and an AI application that can aid various Aerospace businesses in managing Low Earth Orbit (LEO) constellations. This type of AI application may become imperative for deploying and maintaining small satellite mega-constellations.}
}
@incollection{SU2023451,
title = {A Reinforcement Learning Development for The Exact Guillotine with Flexibility on Cutting Stock Problem},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {451-456},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50072-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044315274050072X},
author = {Jie-Ying Su and Chia-Hsiang Liu and Cian-Shan Syu and Jia-Lin Kang and Shi-Shang Jang},
keywords = {Reinforcement learning, machine learning, cutting stock problem},
abstract = {The two-dimensional cutting stock problem (CSP) is critical in several industries. Reinforcement learning (RL) is a novel method to obtain a quality solution of two-dimensional CSP in a short computation time. In this research, we applied a model-free off-policy RL algorithm to an industrial example of exact guillotine two-dimensional CSP, and compared the results with mixed-integer programming (MIP), which is a common traditional mathematical method for optimization. The results showed that RL had a much lower computation time than MIP with a solution closed to optima, and the ability to make a trade-off between waste, inventory level, and back order.}
}
@article{YUAN2023108858,
title = {Deep reinforcement learning-based controller for dynamic positioning of an unmanned surface vehicle},
journal = {Computers and Electrical Engineering},
volume = {110},
pages = {108858},
year = {2023},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2023.108858},
url = {https://www.sciencedirect.com/science/article/pii/S0045790623002823},
author = {Wei Yuan and Xingwen Rui},
keywords = {Unmanned surface vehicles, Dynamic positioning, Deep reinforcement learning, Soft Actor-Critic algorithm, Prioritized experience replay},
abstract = {Dynamic positioning (DP) system is of great significance for the unmanned surface vehicle (USV) to achieve fully autonomous navigation. Traditional control schemes have problems such as model accuracy, parameter tuning, and complex design. In addition, although the deep reinforcement learning (DRL) is widely used in the field of vessel motion control, the learning efficiency is not high, and insufficient robustness in the face of changing environmental. In order to improve the anti-disturbance ability, robustness and convergence speed of the controller during training, a deep reinforcement learning control method based on priority experience replay (PER) is proposed for dynamic positioning of the USV. The mathematical models are established based on the kinematic and dynamic of the USV. Markov decision process (MDP) models are constructed according to the DP tasks. The simulation results show that compared with other DRL algorithms, the proposed method has higher reward value, faster convergence speed, higher control precision and smoother control output.}
}
@article{SHARBAF2022101123,
title = {Automatic resolution of model merging conflicts using quality-based reinforcement learning},
journal = {Journal of Computer Languages},
volume = {71},
pages = {101123},
year = {2022},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2022.101123},
url = {https://www.sciencedirect.com/science/article/pii/S2590118422000260},
author = {Mohammadreza Sharbaf and Bahman Zamani and Gerson Sunyé},
keywords = {Collaborative modeling, Model merging conflict, Conflict resolution, Reinforcement learning, Quality evaluation},
abstract = {Modeling is an activity in the software development life cycle in which different experts and stakeholders collaborate as a team. In collaborative modeling, adhering to the optimistic versioning paradigm allows users to apply concurrent changes to the same model. In such a situation, conflicts may arise. To have an integrated yet consistent merged model, conflicts have to be resolved. To this end, automation is currently at its limit or is not supported at all, and user interaction is often required. To alleviate this flaw, there is an opportunity to apply Artificial Intelligence techniques in a collaborative modeling environment to empower the provisioning of automated and intelligent decision-making. In this paper, we propose the use of reinforcement learning algorithms to achieve merging conflict resolution with a high degree of automation. This enables the personalized and quality-based integration of model versions. To evaluate our idea, we demonstrate the resolution of UML class diagram conflicts using a learning process in an illustrative modeling scenario. We also show the applicability of our approach through a proof of concept implementation and assess its accuracy compared to the greedy and search-based algorithms. Moreover, we conducted an experience with five experts to evaluate the satisfaction of actual users with the selection of resolution actions for different conflicts. The result of the assessment validates our proposal with various syntactic and semantic conflicts.}
}
@article{HOURFAR201998,
title = {A reinforcement learning approach for waterflooding optimization in petroleum reservoirs},
journal = {Engineering Applications of Artificial Intelligence},
volume = {77},
pages = {98-116},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2018.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0952197618302057},
author = {Farzad Hourfar and Hamed Jalaly Bidgoly and Behzad Moshiri and Karim Salahshoor and Ali Elkamel},
keywords = {Waterflooding process, Reinforcement learning, Production optimization, Closed-loop reservoir management, Derivative-free optimization},
abstract = {Waterflooding optimization in closed-loop management of the oil reservoirs is always considered as a challenging issue due to the complicated and unpredicted dynamics of the process. The main goal in waterflooding is to adjust the manipulated variables such that the total oil production or a defined objective function, which has a strong correlation with the gained financial profit, is maximized. Fortunately, due to the recent progresses in the computational tools and also expansion of the calculating facilities, utilization of non-conventional optimization methods is feasible to achieve the desired goals. In this paper, waterflooding optimization problem has been defined and formulated in the framework of Reinforcement Learning (RL) methodology, which is known as a derivative-free and also model-free optimization approach. This technique prevents from the challenges corresponding with the complex gradient calculations for handling the objective functions. So, availability of explicit dynamic models of the reservoir for gradient computations is not mandatory to apply the proposed method. The developed algorithm provides the facility to achieve the desired operational targets, by appropriately defining the learning problem and the necessary variables. The fundamental learning elements such as actions, states, and rewards have been delineated both in discrete and continuous domain. The proposed methodology has been implemented and assessed on the Egg-model which is a popular and well-known reservoir case study. Different configurations for active injection and production wells have been taken into account to simulate Single-Input-Multi-Output (SIMO) as well as Multi-Input-Multi-Output (MIMO) optimization scenarios. The results demonstrate that the “agent” is able to gradually, but successfully learn the most appropriate sequence of actions tailored for each practical scenario. Consequently, the manipulated variables (actions) are set optimally to satisfy the defined production objectives which are generally dictated by the management level or even contractual obligations. Moreover, it has been shown that by properly adjustment of the rewarding policies in the learning process, diverse forms of multi-objective optimization problems can be formulated, analyzed and solved.}
}
@article{SHEHAWY2023104506,
title = {Flattening and folding towels with a single-arm robot based on reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {169},
pages = {104506},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104506},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023001458},
author = {Hassan Shehawy and Daniele Pareyson and Virginia Caruso and Stefano {De Bernardi} and Andrea Maria Zanchettin and Paolo Rocco},
keywords = {Reinforcement learning, Robotics, Deformable objects},
abstract = {Robots can learn how to complete a variety of tasks without explicit instructions thanks to reinforcement learning. In this work, a piece of cloth is placed on a table and manipulated using a single-arm robot. We consider 2 forms of manipulation: flattening a crumpled towel and folding a flat one. To learn a policy that will allow the robot to select the optimum course of action based on observations of the environment, we construct a simulation environment using a gripper and a piece of cloth. After that, the policy is applied to a real robot and put to the test. Additionally, we present our method for identifying the corners of a garment using computer vision, which includes a comparison between a traditional computer vision approach with a deep learning one. We use an ABB robot and a 2D camera for the experiments and PyBullet software for the simulation.}
}
@article{DONG2021107229,
title = {A Strategic Day-ahead bidding strategy and operation for battery energy storage system by reinforcement learning},
journal = {Electric Power Systems Research},
volume = {196},
pages = {107229},
year = {2021},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2021.107229},
url = {https://www.sciencedirect.com/science/article/pii/S0378779621002108},
author = {Yi Dong and Zhen Dong and Tianqiao Zhao and Zhengtao Ding},
keywords = {Battery energy storage system (BESS), Power market bidding, Reinforcement learning},
abstract = {The Battery Energy Storage System (BESS) plays an essential role in the smart grid, and the ancillary market offers a high revenue. It is important for BESS owners to maximise their profit by deciding how to balance between the different offers and bidding with the rivals. Therefore, this paper formulates the BESS bidding problem as a Markov Decision Process(MDP) to maximise the total profit from the e Automation Generation Control (AGC) market and the energy market, considering the factors such as charging/discharging losses and the lifetime of the BESS. In the proposed algorithm, function approximation technology is introduced to handle the continuous massive bidding scales and avoid the dimension curse. As a model-free approach, the proposed algorithm can learn from the stochastic and dynamic environment of a power market, so as to help the BESS owners to decide their bidding and operational schedules profitably. Several case studies illustrate the effectiveness and validity of the proposed algorithm.}
}
@article{ZHU2022118636,
title = {Energy management based on multi-agent deep reinforcement learning for a multi-energy industrial park},
journal = {Applied Energy},
volume = {311},
pages = {118636},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.118636},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922001064},
author = {Dafeng Zhu and Bo Yang and Yuxiang Liu and Zhaojian Wang and Kai Ma and Xinping Guan},
keywords = {Multi-energy management, Industrial park, Multi-agent, Counterfactual baseline, Soft actor–critic, Attention mechanism},
abstract = {Owing to large industrial energy consumption, industrial production has brought a huge burden to the grid in terms of renewable energy access and power supply. Due to the coupling of multiple energy sources and the uncertainty of renewable energy and demand, centralized methods require large calculation and coordination overhead. Thus, this paper proposes a multi-energy management framework achieved by decentralized execution and centralized training for an industrial park. The energy management problem is formulated as a partially-observable Markov decision process, which is intractable by dynamic programming due to the lack of the prior knowledge of the underlying stochastic process. The objective is to minimize long-term energy costs while ensuring the demand of users. To solve this issue and improve the calculation speed, a novel multi-agent deep reinforcement learning algorithm is proposed, which contains the following key points: counterfactual baseline for facilitating contributing agents to learn better policies, soft actor–critic for improving robustness and exploring optimal solutions. A novel reward is designed by Lagrange multiplier method to ensure the capacity constraints of energy storage. In addition, considering that the increase in the number of agents leads to performance degradation due to large observation spaces, an attention mechanism is introduced to enhance the stability of policy and enable agents to focus on important energy-related information, which improves the exploration efficiency of soft actor–critic. Numerical results based on actual data verify the performance of the proposed algorithm with high scalability, indicating that the industrial park can minimize energy costs under different demands.}
}
@article{BLAD2022125290,
title = {Data-driven Offline Reinforcement Learning for HVAC-systems},
journal = {Energy},
volume = {261},
pages = {125290},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.125290},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222021739},
author = {Christian Blad and Simon Bøgh and Carsten Skovmose Kallesøe},
keywords = {Reinforcement learning, Energy optimization, Black-box models, HVAC-systems, Optimal control},
abstract = {This paper presents a novel framework for Offline Reinforcement Learning (RL) with online fine tuning for Heating Ventilation and Air-conditioning (HVAC) systems. The framework presents a method to do pre-training in a black box model environment, where the black box models are built on data acquired under a traditional control policy. The paper focuses on the application of Underfloor Heating (UFH) with an air-to-water-based heat pump. However, the framework should also generalize to other HVAC control applications. Because Black box methods are used is there little to no commissioning time when applying this framework to other buildings/simulations beyond the one presented in this study. This paper explores and deploys Artificial Neural Network (ANN) based methods to design efficient controllers. Two ANN methods are tested and presented in this paper; a Multilayer Perceptron (MLP) method and a Long Short Term Memory (LSTM) based method. It is found that the LSTM-based method reduces the prediction error by 45% when compared with a MLP model. Additionally, different network architectures are tested. It is found that by creating a new model for each time step, performance can be improved additionally 19%. By using these models in the framework presented in this paper, it is shown that a Multi-Agent RL algorithm can be deployed without ever performing worse than an industrial controller. Furthermore, it is shown that if building data from a Building Management System (BMS) is available, an RL agent can be deployed which performs close to optimally from the first day of deployment. An optimal control policy reduces the cost of heating by 19.4 % when compared to a traditional control policy in the simulation presented in this paper.}
}
@article{HUANG20232361,
title = {Off-policy reinforcement learning for tracking control of discrete-time Markov jump linear systems with completely unknown dynamics},
journal = {Journal of the Franklin Institute},
volume = {360},
number = {3},
pages = {2361-2378},
year = {2023},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2022.10.052},
url = {https://www.sciencedirect.com/science/article/pii/S0016003222007967},
author = {Zhen Huang and Yidong Tu and Haiyang Fang and Hai Wang and Liang Zhang and Kaibo Shi and Shuping He},
abstract = {In this paper, a model-free off-policy reinforcement learning (RL) algorithm is proposed to address the optimal tracking control (OTC) problem for discrete-time Markov jump linear systems (MJLSs). The tracking reference signal is firstly augmented into discrete-time MJLSs, whereby the original tracking control problem is converted to the optimal control problem of the augmented system. The corresponding augmented coupled game algebraic Riccati equation (ACGARE) is then derived. On this basis, an online RL algorithm is developed to solve the OTC problem by using the policy iteration (PI) technique. Then, a novel model-free method is proposed, which eliminates the requirement of the system dynamics and transition probability. Finally, a simulation example is provided to prove the convergence and validate the effectiveness of the proposed algorithm.}
}
@article{GUNTHER20161,
title = {Intelligent laser welding through representation, prediction, and control learning: An architecture with deep neural networks and reinforcement learning},
journal = {Mechatronics},
volume = {34},
pages = {1-11},
year = {2016},
note = {System-Integrated Intelligence: New Challenges for Product and Production Engineering},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2015.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0957415815001555},
author = {Johannes Günther and Patrick M. Pilarski and Gerhard Helfrich and Hao Shen and Klaus Diepold},
keywords = {Deep learning, Reinforcement learning, Prediction, Control, Laser welding},
abstract = {Laser welding is a widely used but complex industrial process. In this work, we propose the use of an integrated machine intelligence architecture to help address the significant control difficulties that prevent laser welding from seeing its full potential in process engineering and production. This architecture combines three contemporary machine learning techniques to allow a laser welding controller to learn and improve in a self-directed manner. As a first contribution of this work, we show how a deep, auto-encoding neural network is capable of extracting salient, low-dimensional features from real high-dimensional laser welding data. As a second contribution and novel integration step, these features are then used as input to a temporal-difference learning algorithm (in this case a general-value-function learner) to acquire important real-time information about the process of laser welding; temporally extended predictions are used in combination with deep learning to directly map sensor data to the final quality of a welding seam. As a third contribution and final part of our proposed architecture, we suggest that deep learning features and general-value-function predictions can be beneficially combined with actor–critic reinforcement learning to learn context-appropriate control policies to govern welding power in real time. Preliminary control results are demonstrated using multiple runs with a laser-welding simulator. The proposed intelligent laser-welding architecture combines representation, prediction, and control learning: three of the main hallmarks of an intelligent system. As such, we suggest that an integration approach like the one described in this work has the capacity to improve laser welding performance without ongoing and time-intensive human assistance. Our architecture therefore promises to address several key requirements of modern industry. To our knowledge, this architecture is the first demonstrated combination of deep learning and general value functions. It also represents the first use of deep learning for laser welding specifically and production engineering in general. We believe that it would be straightforward to adapt our architecture for use in other industrial and production engineering settings.}
}
@article{DOBRIBORSCI20221545,
title = {An experimental study of two predictive reinforcement learning methods and comparison with model-predictive control},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1545-1550},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.610},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322019206},
author = {Dmitrii Dobriborsci and Pavel Osinenko and Wolfgang Aumer},
keywords = {reinforcement learning, predictive control, experimental study, mobile robots, model predictive control},
abstract = {Reinforcement learning (RL) has been successfully used in various simulations and computer games. Industry-related applications, such as autonomous mobile robot motion control, are somewhat challenging for RL up to date though. This paper presents an experimental evaluation of predictive RL controllers for optimal mobile robot motion control. As a baseline for comparison, model-predictive control (MPC) is used. Two RL methods are tested: a rollout Q-learning, which may be considered as MPC with terminal cost being a Q-function approximation, and a so-called stacked Q-learning, which in turn is like MPC with the running cost substituted for a Q-function approximation. The experimental foundation is a mobile robot with a differential drive (Robotis Turtlebot3). Experimental results showed that both RL methods beat the baseline in terms of the accumulated cost, whereas the stacked variant performed best. Provided the series of previous works on stacked Q-learning, this particular study supports the idea that MPC with a running cost adaptation inspired by Q-learning possesses potential of performance boost while retaining the nice properties of MPC.}
}
@article{KHADER2021102132,
title = {Adaptive optimal control of stencil printing process using reinforcement learning},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {71},
pages = {102132},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102132},
url = {https://www.sciencedirect.com/science/article/pii/S073658452100017X},
author = {Nourma Khader and Sang Won Yoon},
keywords = {Surface mount technology, Stencil printing process, Reinforcement learning, -table, -network, Clustering},
abstract = {The stencil printing process (SPP) is a critical operation in surface mount technology (SMT) because it contributes to 60% of soldering defects. The complex relationships between solder paste volume transfer efficiency (TE) and the SPP variables make the control of the solder paste volume TE during production a challenging problem. This research aims to optimize the stencil printing parameters in real time to control the solder paste volume TE and increase the first-pass yields of printed circuit boards (PCBs). A Reinforcement learning (RL) approach, specifically Q-learning, is used to control and maintain the volume TE within the spec limits in an optimal adaptive control system. RL deals with the problem of building a control system or an autonomous agent that can learn how to take the proper actions to reach its objectives through interaction with its environment. The application of RL in SPP is not yet fully explored; therefore, this study investigates the impacts of applying Q-learning to control the volume TE in real time. The proposed control systems capture the induced variations in the SPP for two printing directions and consequently adjust the significant and easy-to-change printing parameters in real time. Two types of Q-learning are explored: Q-table that uses a tabular format to store the Q-values and Q-network that uses an artificial neural network (ANN) to approximate the Q-value function. Moreover, a new heterogeneous reward function-based clustering is proposed, which is integrated into the Q-network to enhance its performance. The results show that the developed control systems can learn the optimal policy and take the proper actions to transit from initial states to terminal states. The proposed control systems using Q-network with a function approximator and heterogeneous reward function converge fully much faster than Q-table using continuous state space. Moreover, Q-network control systems are capable to transit more states to terminal states with a lower number of actions when compared to Q-table control systems.}
}
@article{KOZJEK2020425,
title = {Multi-objective adjustment of remaining useful life predictions based on reinforcement learning},
journal = {Procedia CIRP},
volume = {93},
pages = {425-430},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.03.051},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120306582},
author = {Dominik Kozjek and Andreja Malus and Rok Vrabič},
keywords = {predictive maintainance, remaining useful life, reinforcement learning},
abstract = {Effective tracking of degradation in machine tools or vehicle, ship, and aircraft engines is key to ensure their high utilization, effective maintenance, and safety. Data from the built-in sensors can be used to build models that accurately predict the remaining useful life (RUL) of the observed system. However, existing approaches often lack the ability to incorporate domain-specific knowledge in form of degradation models. This paper proposes a reinforcement-learning based approach for encoding the degradation model used for multi-objective adjustment of RUL predictions. The approach is demonstrated with a case of RUL prediction for aircraft engines.}
}
@article{ZOU2022305,
title = {Deep Reinforcement Learning Control of Wave Energy Converters},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {27},
pages = {305-310},
year = {2022},
note = {9th IFAC Symposium on Mechatronic Systems MECHATRONICS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.530},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322025836},
author = {Shangyan Zou and Xiang Zhou and Wayne Weaver and Ossama Abdelkhalik},
keywords = {Dynamics, control, Renewable Energy System Modeling, Integration, Control of renewable energy resources, Reinforcement learning control, Data-Driven Decision Making},
abstract = {One major challenge of converting wave energy to useful electricity is the development of highly efficient Power Take-off (PTO) control algorithms. Conventional model-based controls are typically developed based on reduced-order models and neglecting the dynamics of other subsystems. Although showing promising performance in idealized conditions, it may be misleading in practice. On the other hand, it is nearly impossible to derive a model-based control for a highly nonlinear/complex system (e.g., a wave-to-wire model). Therefore, in this study, we propose a Deep Reinforcement Learning (DRL, model-free) control that aims at/capable of optimizing the performance of WECs from wave to wire. This wave-to-wire model is composed of a heaving point absorber and a direct-drive PTO unit. The numerical simulations are first conducted on comparing the performance of the proposed DRL and conventional model-based controls. The results show maximumly a 152% improvement in terms of the electricity generation and an 84% improvement in terms of the power quality. Moreover, the robustness of the proposed control is also validated under varied real ocean conditions at PacWave. The results indicate a consistent improvement of power production and quality of the proposed DRL control compared to model-based controls.}
}
@article{XIAO201851,
title = {Value iteration based integral reinforcement learning approach for H∞ controller design of continuous-time nonlinear systems},
journal = {Neurocomputing},
volume = {285},
pages = {51-59},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S092523121830047X},
author = {Geyang Xiao and Huaguang Zhang and Kun Zhang and Yinlei Wen},
keywords = {Value iteration,  control, Reinforcement learning, Continuous-time systems},
abstract = {In this paper, a novel integral reinforcement learning approach is developed based on value iteration (VI) for designing the H∞ controller of continuous-time (CT) nonlinear systems. First, the VI learning mechanism is introduced to solve the zero-sum game problems, which is equivalent to the Hamilton–Jacobi–Isaacs (HJI) equation arising in H∞ control problems. Since the proposed method is based on VI learning mechanism, it does not require the admissible control for the implementation, and thus satisfies a more general initial condition than the works based on policy iteration (PI). The iterative property of the value function is analysed with an arbitrary initial positive function, and the H∞ controller can be derived as the iteration converges. For the implementation of the proposed method, three neural networks are introduced to approximate the iterative value function, the iterative control policy and the iterative disturbance policy, respectively. To verify the effectiveness of the VI based method, a linear case and a nonlinear case are presented, respectively.}
}
@article{WANG2021101339,
title = {Integrated scheduling and flexible maintenance in deteriorating multi-state single machine system using a reinforcement learning approach},
journal = {Advanced Engineering Informatics},
volume = {49},
pages = {101339},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101339},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000926},
author = {Hongfeng Wang and Qi Yan and Shuzhu Zhang},
keywords = {Production scheduling, Time-based preventive maintenance, Condition-based preventive maintenance, Multi-state machine deterioration, Reinforcement learning},
abstract = {This paper studies an integrated optimization problem of production scheduling and flexible preventive maintenance (PM) in a multi-state single machine system with deteriorating effects. A flexible PM strategy is proposed to proactively cope with machine failures while ensuring relatively regular PM intervals, which is composed of time-based PM (TBPM) and condition-based PM (CBPM). TBPM is conducted within every flexible time window and CBPM is implemented immediately after the most deteriorated yet still functional state. An illustrative case is presented using the enumeration approach to demonstrate the integration of production scheduling and machine maintenance. Then, Q-learning-based solution framework (QLSF) is further designed with proper state and action sets and reward functions to facilitate the determination of appropriate production scheduling rule under the constraint of the flexible maintenance. Numerical experiments show that the proposed QLSF outperforms the other four state-of-the-art scheduling rules in different scenarios. Moreover, the performance of the proposed flexible PM strategy is also examined and validated in comparison with three candidate maintenance strategies, i.e., run-to-failure corrective maintenance (CM), combination of TBPM and CM, and CBPM. The proposed flexible maintenance and solution approach can enrich the relevant academic knowledge base, and provide managerial insights and guidance in practical production systems.}
}
@article{SU2020243,
title = {Integral reinforcement learning-based online adaptive event-triggered control for non-zero-sum games of partially unknown nonlinear systems},
journal = {Neurocomputing},
volume = {377},
pages = {243-255},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.09.088},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219314109},
author = {Hanguang Su and Huaguang Zhang and Shaoxin Sun and Yuliang Cai},
keywords = {Event-triggered control (ETC), Integral reinforcement learning (IRL), Adaptive dynamic programming (ADP), Adaptive critic design, Non-zero-sum (NZS) games},
abstract = {This paper develops an integral reinforcement learning (IRL)-based adaptive control method for the multi-player non-zero-sum (NZS) games of the nonlinear continuous-time systems with partially unknown dynamics, in the context of event-triggered mechanism. With the principle of IRL method, the requirement for the system drift dynamics is relaxed in the controller design. Moreover, different from the conventional iteration computation methods, the algorithm developed in this work is implemented in an online adaptive fashion, which provides a new way to combine the IRL algorithm and the event-triggered control framework in solving the NZS game issues. In the event-based algorithm, a state-dependent triggering condition is presented, which not only guarantees the closed-loop system stability, but also reduces the computation and communication loads of the controlled plant. By means of Lyapunov theorem, the uniform ultimate boundedness (UUB) properties of the system states and the critic weight estimation errors have been proved. Finally, two numerical examples are utilized to demonstrate the efficacy of the proposed method.}
}
@article{ELMAZ2023108310,
title = {Reinforcement learning-based approach for optimizing solvent-switch processes},
journal = {Computers & Chemical Engineering},
volume = {176},
pages = {108310},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108310},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423001801},
author = {Furkan Elmaz and Ulderico {Di Caprio} and Min Wu and Yentl Wouters and Geert {Van Der Vorst} and Niels Vandervoort and Ali Anwar and M. Enis Leblebici and Peter Hellinckx and Siegfried Mercelis},
keywords = {Reinforcement learning, Process control, Solvent-switch optimization, Separation process},
abstract = {In chemical and pharmaceutical industries, process control optimization is a crucial step to improve economical efficiency and the environmental impact. The current state-of-practice heavily relies on expert knowledge and extensive lab experiments. This not only increases the development time but also limits the discovery of new strategies. In this study, we propose Reinforcement Learning-based optimization approach for solvent-switch processes. We utilize a digital twin as the environment for a process designed to switch the THF to 1-propanol. A reward function is created for minimizing the process time and constraints are implemented using logarithmic barrier functions. A PPO agent is trained on the environment. The agent proposed a novel strategy that combines two conventionally separate phases, evaporation and constant volume distillation. This strategy resulted in an overall cost decrease of 24.9% compared to the baseline strategy. Moreover, results were verified experimentally on a pilot plant of Johnson & Johnson (J&J).}
}
@article{DEY2023112941,
title = {Inverse reinforcement learning control for building energy management},
journal = {Energy and Buildings},
volume = {286},
pages = {112941},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2023.112941},
url = {https://www.sciencedirect.com/science/article/pii/S0378778823001718},
author = {Sourav Dey and Thibault Marzullo and Gregor Henze},
keywords = {Inverse reinforcement learning, Building controls, Artificial intelligence, Practical reinforcement learning},
abstract = {Reinforcement learning (RL) based control is widely considered a promising approach in building automation and control as it has demonstrated the potential to deal with complex objectives in adjacent domains like robotics, autonomous vehicles, gaming applications, and advertisement recommendations. When applied to any environment, model-free RL learns to improve its control performance over time without requiring a control model, by receiving and then analyzing feedback from the building environment after each control action. Operational objectives are becoming increasingly complex through the simultaneous consideration of thermal comfort, carbon emissions, grid services, and indoor air quality. In this context, conventional rule-based control approaches are proving sub-optimal, mostly heuristic, and inadequate. The model-free and self learning nature of RL appears promising and attractive as it may address the scalability issues associated with advanced control approaches. However, it suffers from long training times and unstable control behavior during the early stages of its learning process, which makes it unsuitable to be applied directly to buildings. This paper addresses these issues using an inverse reinforcement learning approach (IRL), a technique utilized to learn the objective of a controller agent which is considered an expert in its respective domain. Here, we consider a rule-based control as the expert demonstrator. IRL is different from a direct imitation (i.e., direct mapping of states to actions) of control actions as it tries to find the underlying intent of an expert’s policy, providing the controller with a better-generalized policy for unseen states or environments with slightly different dynamics. This approach propels the RL controller’s policy to levels similar to or better than that of a rule-based policy before it starts learning by interacting with the building. This makes RL for building energy management applications more practical as it prevents the erratic and exploratory behavior in the initial training period, simultaneously speeding up the learning process when compared to applying an untrained RL agent directly to a building environment.}
}
@article{WANG2023101321,
title = {Bi-objective scenario-guided swarm intelligent algorithms based on reinforcement learning for robust unrelated parallel machines scheduling with setup times},
journal = {Swarm and Evolutionary Computation},
volume = {80},
pages = {101321},
year = {2023},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2023.101321},
url = {https://www.sciencedirect.com/science/article/pii/S2210650223000949},
author = {Bing Wang and Kai Feng and Xiaozhi Wang},
keywords = {Unrelated parallel machine scheduling problem, Bi-objective robust optimization, Discrete scenarios, Reinforcement learning, Swarm intelligent algorithm},
abstract = {This paper addresses an uncertain unrelated parallel machine scheduling problem (UPMSP) with setup times, which is referred to the scenario UPMSP since uncertain processing times are described by a set of discrete scenarios. The bi-objective robust optimization formulation is established. Two objectives are to minimize the mean makespan and the worst-case makespan across all scenarios, which reflect solution optimality and solution robustness respectively. The contribution of this paper is three-fold. First, we propose the bi-objective robust optimization formulation under discrete scenarios for uncertain UPMSP. Second, two versions of swarm intelligent algorithms are developed by combining fruit fly optimization algorithm (FOA) framework and scenario-guided local search, which are performed based on two problem-specific neighborhood structures. The learning-scenario neighborhood structure is constructed by selecting single scenario using reinforcement learning. The united-scenario neighborhood structure is constructed by collecting all discrete scenarios. Third, an experiment was conducted to compare two developed algorithms with the state-of-the-art alternative algorithms. The computational results show that the developed algorithms are identically better than possible alternatives in terms of multi-objective metrics. Moreover, it is shown that the FOA algorithm with learning-scenario-neighborhood smell search is advantageous for the discussed problem.}
}
@article{WIJAYA2023213,
title = {Long short-term memory (LSTM) model-based reinforcement learning for nonlinear mass spring damper system control},
journal = {Procedia Computer Science},
volume = {216},
pages = {213-220},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.129},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022074},
author = {Santo Wijaya and Yaya Heryadi and Yulyani Arifin and Wayan Suparta and  Lukas},
keywords = {Model-based Reinforcement Learning, Random-Shooting Policy, LSTM, MPC, Nonlinear System},
abstract = {The Neural Networks (NN) model which is incorporated in the control system design has been studied, and the results show better performance than the mathematical model approach. However, some studies consider that only offline NN model learning and does not use the online NN model learning directly on the control system. As a result, the controller's performance decreases due to changes in the system environment from time to time. The Reinforcement Learning (RL) method has been investigated intensively, especially Model-based RL (Mb-RL) to predict system dynamics. It has been investigated and performs well in making the system more robust to environmental changes by enabling online learning. This paper proposes online learning of local dynamics using the Mb-RL method by utilizing Long Short-Term Memory (LSTM) model. We consider Model Predictive Control (MPC) scheme as an agent of the Mb-RL method to control the regulatory trajectory objectives with a random shooting policy to search for the minimum objective function. A nonlinear Mass Spring Damper (NMSD) system with parameter-varying linear inertia is used to demonstrate the effectiveness of the proposed method. The simulation results show that the system can effectively control high-oscillating nonlinear systems with good performance.}
}
@article{DONG2021327,
title = {Variance aware reward smoothing for deep reinforcement learning},
journal = {Neurocomputing},
volume = {458},
pages = {327-335},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221009139},
author = {Yunlong Dong and Shengjun Zhang and Xing Liu and Yu Zhang and Tan Shen},
keywords = {Deep reinforcement learning, Rewards drop, Variance reduction, Rewards smoothing},
abstract = {A Reinforcement Learning (RL) agent interacts with the environment to learn a policy with high accumulated rewards through attempts and failures. However, RL suffers from its own trial-and-error learning nature, which results in an unstable learning process. In this paper, we investigate a common phenomenon called rewards drop at the late-stage RL training session, where the rewards trajectory oscillates dramatically. In order to solve such a problem, we propose a novel rewards shaping technique named Variance Aware Rewards Smoothing (VAR). We show that the proposed method reduces the variance of rewards and mitigates the rewards drop problem without changing the formulation of the value function. Furthermore, the theoretical analysis of convergence of VAR is provided, which is derived from the γ-contraction operator and the fixed point attribute of the value function. Finally, the theoretical results are illustrated by extensive results on various benchmarks and advanced algorithms across different random seeds to demonstrate the effectiveness and the compatibility of VAR.}
}
@article{LIANG2023,
title = {Reinforcement learning based adaptive control for uncertain mechanical systems with asymptotic tracking},
journal = {Defence Technology},
year = {2023},
issn = {2214-9147},
doi = {https://doi.org/10.1016/j.dt.2023.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S2214914723001393},
author = {Xiang-long Liang and Zhi-kai Yao and Yao-wen Ge and Jian-yong Yao},
keywords = {Adaptive control, Reinforcement learning, Uncertain mechanical systems, Asymptotic tracking},
abstract = {This paper mainly focuses on the development of a learning-based controller for a class of uncertain mechanical systems modeled by the Euler-Lagrange formulation. The considered system can depict the behavior of a large class of engineering systems, such as vehicular systems, robot manipulators and satellites. All these systems are often characterized by highly nonlinear characteristics, heavy modeling uncertainties and unknown perturbations, therefore, accurate-model-based nonlinear control approaches become unavailable. Motivated by the challenge, a reinforcement learning (RL) adaptive control methodology based on the actor-critic framework is investigated to compensate the uncertain mechanical dynamics. The approximation inaccuracies caused by RL and the exogenous unknown disturbances are circumvented via a continuous robust integral of the sign of the error (RISE) control approach. Different from a classical RISE control law, a tanh(·) function is utilized instead of a sign(·) function to acquire a more smooth control signal. The developed controller requires very little prior knowledge of the dynamic model, is robust to unknown dynamics and exogenous disturbances, and can achieve asymptotic output tracking. Eventually, co-simulations through ADAMS and MATLAB/Simulink on a three degrees-of-freedom (3-DOF) manipulator and experiments on a real-time electromechanical servo system are performed to verify the performance of the proposed approach.}
}
@article{YAN2020103594,
title = {Fixed-Wing UAVs flocking in continuous spaces: A deep reinforcement learning approach},
journal = {Robotics and Autonomous Systems},
volume = {131},
pages = {103594},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103594},
url = {https://www.sciencedirect.com/science/article/pii/S0921889020304346},
author = {Chao Yan and Xiaojia Xiang and Chang Wang},
keywords = {Fixed-wing UAV, Flocking, Reinforcement learning, Actor–critic},
abstract = {Fixed-Wing UAVs (Unmanned Aerial Vehicles) flocking is still a challenging problem due to the kinematics complexity and environmental dynamics. In this paper, we solve the leader–followers flocking problem using a novel deep reinforcement learning algorithm that can generate roll angle and velocity commands by training an end-to-end controller in continuous state and action spaces. Specifically, we choose CACLA (Continuous Actor–Critic Learning Automation) as the base algorithm and we use the multi-layer perceptron to represent both the actor and the critic. Besides, we further improve the learning efficiency by using the experience replay technique that stores the training data in the experience memory and samples from the memory as needed. We have compared the performance of the proposed CACER (Continuous Actor–Critic with Experience Replay) algorithm with benchmark algorithms such as DDPG and double DQN in numerical simulation, and we have demonstrated the performance of the learned optimal policy in semi-physical simulation without any parameter tuning.}
}
@article{KIM2023160,
title = {Look-ahead based reinforcement learning for robotic flow shop scheduling},
journal = {Journal of Manufacturing Systems},
volume = {68},
pages = {160-175},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523000262},
author = {Hyun-Jung Kim and Jun-Ho Lee},
keywords = {Deep reinforcement learning, Look-ahead search, Petri net, Scheduling, Robotic flow shop},
abstract = {Scheduling of a robotic flow shop, where a dual-gripper robot transports parts between machines, is addressed with a makespan measure. Most previous studies on robotic flow shop scheduling have focused on cyclic scheduling where the robot repeats a certain sequence to process identical parts. Recently, noncyclic scheduling of robotic flow shops is strongly required due to the need for producing customized products and smaller order sizes efficiently. This study therefore considers noncyclic scheduling of a dual-gripper robotic flow shop with a given part sequence and proposes a novel solution approach, look-ahead based reinforcement learning (LARL). The LARL consists of deep Q-learning for training a Q-network based on a given set of instances and the look-ahead search used for testing new instances. The look-ahead search in the LARL is efficient, especially for robotic flow shop scheduling where future state information can be used for determining the current robot task. The experimental results comparing the LARL with an optimal algorithm and the well-known robot task sequence show the effectiveness of the LARL.}
}
@article{LI2012743,
title = {Optimal control in microgrid using multi-agent reinforcement learning},
journal = {ISA Transactions},
volume = {51},
number = {6},
pages = {743-751},
year = {2012},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2012.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0019057812000894},
author = {Fu-Dong Li and Min Wu and Yong He and Xin Chen},
keywords = {Distributed generation, Microgrid, Multi-agent system, Reinforcement learning, MAXQ},
abstract = {This paper presents an improved reinforcement learning method to minimize electricity costs on the premise of satisfying the power balance and generation limit of units in a microgrid with grid-connected mode. Firstly, the microgrid control requirements are analyzed and the objective function of optimal control for microgrid is proposed. Then, a state variable “Average Electricity Price Trend” which is used to express the most possible transitions of the system is developed so as to reduce the complexity and randomicity of the microgrid, and a multi-agent architecture including agents, state variables, action variables and reward function is formulated. Furthermore, dynamic hierarchical reinforcement learning, based on change rate of key state variable, is established to carry out optimal policy exploration. The analysis shows that the proposed method is beneficial to handle the problem of “curse of dimensionality” and speed up learning in the unknown large-scale world. Finally, the simulation results under JADE (Java Agent Development Framework) demonstrate the validity of the presented method in optimal control for a microgrid with grid-connected mode.}
}
@article{HWANGBO2020106910,
title = {Design of control framework based on deep reinforcement learning and Monte-Carlo sampling in downstream separation},
journal = {Computers & Chemical Engineering},
volume = {140},
pages = {106910},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106910},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419310750},
author = {Soonho Hwangbo and Gürkan Sin},
keywords = {Liquid-liquid extraction column, Deep reinforcement learning, Monte-Carlo sampling, Control system, API production, Biopharmaceuticals},
abstract = {This paper proposes a systematic framework to develop deep reinforcement learning (RL)-based algorithms for control system of downstream separation in biopharmaceutical process as follows. First, a simulation model as a digital twin is built and Monte-Carlo sampling generates substantial amounts of samples considering disturbances. Second, the deep RL-based control system is designed and the optimization subject to sample datasets is conducted. The methodology is implemented in a prototype software and relevant codes are shared by Mendeley Data. The proposed model is successfully applied to control the liquid-liquid extraction column for the recovery of fusidic acid as part of downstream processing. The resulting deep RL algorithm provides an operation performance with a better API recovery yield (32 % higher than open loop operation) and lower deviations (23 % lower than open loop operation) against disturbances.}
}
@article{ZHAO2020542,
title = {A Deep Reinforcement Learning Approach for Autonomous Highway Driving},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {5},
pages = {542-546},
year = {2020},
note = {3rd IFAC Workshop on Cyber-Physical & Human Systems CPHS 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.04.142},
url = {https://www.sciencedirect.com/science/article/pii/S240589632100272X},
author = {Junwu Zhao and Ting Qu and Fang Xu},
keywords = {Autonomous Driving, Deep Reinforcement Learning, Vehicle Control, Traffic Modeling, Traffic Simulator},
abstract = {Autonomous driving has been the trend. In this paper, a Deep Reinforcement Learning (DRL) method is exploited to model the decision making and interaction between vehicles on highway driving. To avoid the overestimate action values induced by Q-learning, we use the Double Deep Q-Network (DDQN) for the training of the host vehicle. The agent learns from trial and interactions with the environment. A simulation platform based on the Simulation of Urban Mobility (SUMO) is also established, it helps facilitate the variation of control algorithms. The results show that the proposed framework can simulate highway driving, and the trained agent can accomplish the driving task with ease after training and can approximate the highest safe driving speed as defined without collision.}
}
@article{WANG2022694,
title = {Independent double DQN-based multi-agent reinforcement learning approach for online two-stage hybrid flow shop scheduling with batch machines},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {694-708},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001923},
author = {Ming Wang and Jie Zhang and Peng Zhang and Li Cui and Guoqing Zhang},
keywords = {Two-stage hybrid flow shop, Online scheduling, Batch machines, Jobs arriving over time, Multi-agent reinforcement learning},
abstract = {Two-stage hybrid flow shop scheduling with batch machines and jobs arriving over time is complex and challenging in various real-world production scenarios. For the online scheduling problem, traditional heuristic rules can quickly respond to dynamically arrived jobs, while with poor and unstable performance. To close the research gap in the problem, this paper proposes an independent double deep-q-network-based multi-agent reinforcement learning (MA-IDDQN) approach to produce an adaptive rule for batch forming and scheduling. Specifically, the online scheduling problem is transformed into a cooperative Markov decision process by defining state space, action space, and reward function for different agents. Then, two agents are constructed and trained via double DQN to address the batch forming task and scheduling task respectively. Meanwhile, multi-agent cooperates through the behavior analysis mechanism among agents. Moreover, we designed a ε-greedy policy considering waiting in batch forming to make a reasonable decision through historical data. To validate the proposed approach, 27 instances with different scales are settled and contrasted. By comparing with frequently-used heuristic rules and other deep reinforcement learning methods, the experimental results demonstrate that the MA-IDDQN can integrate online batch forming and scheduling to minimize the total tardiness time effectively.}
}
@article{YUAN2023110436,
title = {Solving job shop scheduling problems via deep reinforcement learning},
journal = {Applied Soft Computing},
volume = {143},
pages = {110436},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110436},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623004544},
author = {Erdong Yuan and Shuli Cheng and Liejun Wang and Shiji Song and Fang Wu},
keywords = {Job shop scheduling, State representation, Invalid action masking, Deep reinforcement learning, Generalization},
abstract = {Deep reinforcement learning (DRL), as a promising technique, is a new approach to solve the job shop scheduling problem (JSSP). Although DRL method is effective for solving JSSP, there are still deficiencies in state representation, action space definition, and reward function design, which make it difficult for the agent to learn effective policy. In this paper, we model JSSP as a Markov decision process (MDP) and design a new state representation using the state features of bidirectional scheduling, which can not only enable the agent to capture more effective state information, improve its decision-making ability, but also effectively avoid the phenomenon of multiple optimal action selections in candidate action set. Invalid action masking (IAM) technique is employed to narrow the search space, which helps the agent avoid exploring suboptimal solutions. We evaluate the performance of the policy model on eight public test datasets: ABZ, FT, ORB, YN, SWV, LA, TA, and DMU. Extensive experimental results show that the proposed method on the whole has better optimization ability than the existing state-of-the-art models and priority dispatching rules.}
}
@article{MA201940,
title = {Continuous control of a polymerization system with deep reinforcement learning},
journal = {Journal of Process Control},
volume = {75},
pages = {40-47},
year = {2019},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959152418304876},
author = {Yan Ma and Wenbo Zhu and Michael G. Benton and José Romagnoli},
keywords = {Deep reinforcement learning, Polymerization, Process control},
abstract = {Reinforcement learning is a branch of machine learning, where the machines gradually learn control behaviors via self-exploration of the environment. In this paper, we present a controller using deep reinforcement learning (DRL) with Deep Deterministic Policy Gradient (DDPG) for a non-linear semi-batch polymerization reaction. Several adaptations to apply DRL for chemical process control are addressed in this paper including the Markov state assumption, action boundaries and reward definition. This work illustrates that a DRL controller is capable of handling complicated control tasks for chemical processes with multiple inputs, non-linearity, large time delay and noise tolerance. The application of this AI-based framework, using DRL, is a promising direction in the field of chemical process control towards the goal of smart manufacturing.}
}
@article{LIU20191,
title = {Integral reinforcement learning based decentralized optimal tracking control of unknown nonlinear large-scale interconnected systems with constrained-input},
journal = {Neurocomputing},
volume = {323},
pages = {1-11},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218310725},
author = {Chong Liu and Huaguang Zhang and Geyang Xiao and Shaoxin Sun},
keywords = {Decentralized optimal tracking control, Large-scale interconnected systems, Constrained-input, Data-based, Integral reinforcement learning (IRL), Neural networks (NN)},
abstract = {This paper deals with the decentralized optimal tracking control problem of large-scale interconnected systems with constrained-input. The large-scale interconnected systems are firstly transformed to several nominal isolated subsystems. Then, nominal isolated subsystems tracking problem is solved via integral reinforcement learning (IRL) method. It is proved that the solved optimal controllers ensure the boundedness of the original systems tracking error. The actor-critic neural network (NN) technique is used to approximate the critic cost and control policy to implement the IRL algorithm. The least squares approach is employed to solve the weights of actor-critic NN by using only system data. A simulation example is provided to verify the effectiveness of the controllers by comparing with the controllers without considering constrained-input.}
}