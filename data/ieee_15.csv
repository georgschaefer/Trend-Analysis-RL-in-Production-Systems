"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Graphical Minimax Game and On-Policy Reinforcement Learning for Consensus of Leaderless Multi-Agent Systems","W. Dong; C. Wang; J. Li; J. Wang","Key Laboratory of Dynamics and Control of Flight Vehicle within Ministry of Education, School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China; Key Laboratory of Dynamics and Control of Flight Vehicle within Ministry of Education, School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China; School of Information and Control Engineering, Liaoning Shihua University, Fushun, China; Key Laboratory of Dynamics and Control of Flight Vehicle within Ministry of Education, School of Aerospace Engineering, Beijing Institute of Technology, Beijing, China","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","606","611","In this paper, we study the adaptive optimal consensus control of leaderless multi-agent systems (MASs) with heterogeneous dynamics. First, the consensus control problem is converted into a graphical minimax game problem and the corresponding algebraic Riccati equation (ARE) for each agent is obtained. Then, an on-policy reinforcement learning algorithm is proposed to online learn the optimal control policy without requiring the system dynamics. A certain rank condition is established to guarantee the convergence of the proposed online learning algorithm to the unique solution of the ARE. Finally, the effectiveness of the proposed algorithm is demonstrated through a numerical simulation.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264527","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264527","","Games;Heuristic algorithms;Reinforcement learning;Mathematical model;Vehicle dynamics;Performance analysis;Optimal control","adaptive control;game theory;graph theory;learning (artificial intelligence);minimax techniques;multi-agent systems;optimal control;Riccati equations","ARE;online learning;convergence;rank condition;graphical minimax game;optimal control policy;on-policy reinforcement learning;algebraic Riccati equation;consensus control problem;heterogeneous dynamics;adaptive optimal consensus control;leaderless multiagent systems","","2","","23","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning Solution for Cyber-Physical Systems Security Against Replay Attacks","Y. Yu; W. Yang; W. Ding; J. Zhou","Key Laboratory of Smart Manufacturing in Energy Chemical Process, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, East China University of Science and Technology, Shanghai, China","IEEE Transactions on Information Forensics and Security","28 Apr 2023","2023","18","","2583","2595","The security problem of state estimation plays a critical role in monitoring and managing operation of cyber-physical systems (CPS). This paper considers the problem of network security under replay attacks and formulates a novel attack detection method. More specifically, we design a model-free reinforcement learning-based replay attack detection framework that can automatically learn and recognize the evolving attacks with more effectiveness. Attackers in some situations are more like intelligent agents with initiative, who can transform their attack strategies purposefully according to the actions of defenders. Thus, we propose a new defense strategy against the interaction between the attacker and the defender which is solved by optimization learning. The proposed analytical procedure concerning reinforcement learning technology can also be extended to the study of other control applications. Finally, the numerical examples are provided to illustrate the effectiveness of the detection method.","1556-6021","","10.1109/TIFS.2023.3268532","National Natural Science Foundation of China(grant numbers:62122026,61973123); Program of Introducing Talents of Discipline to Universities (the 111 Project)(grant numbers:B17017); Shuguang Program through the Shanghai Education Development Foundation and the Shanghai Municipal Education Commission; Special Project of Military Civilian Integration Development in Shanghai(grant numbers:2019-jmrh1-kj25); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10105656","Cyber-physical systems;distributed Kalman filtering;replay attack;reinforcement learning","Reinforcement learning;Detectors;Games;Watermarking;Technological innovation;State estimation;Noise measurement","computer network security;cyber-physical systems;learning (artificial intelligence);reinforcement learning;security of data;state estimation","attack strategies;cyber-physical systems security against replay attacks;evolving attacks;formulates;managing operation;model-free reinforcement learning-based replay attack detection framework;network security;novel attack detection method;optimization learning;reinforcement learning solution;reinforcement learning technology;security problem;state estimation","","1","","30","IEEE","19 Apr 2023","","","IEEE","IEEE Journals"
"Meta-Reinforcement Learning-Based Transferable Scheduling Strategy for Energy Management","L. Xiong; Y. Tang; C. Liu; S. Mao; K. Meng; Z. Dong; F. Qian","Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, and the Engineering Research Center of Process System Engineering, Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, and the Engineering Research Center of Process System Engineering, Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, and the Engineering Research Center of Process System Engineering, Ministry of Education, East China University of Science and Technology, Shanghai, China; Department of Electrical Engineering, Nantong University, Nantong, China; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney NSW, Australia; School of Electrical and Electronics Engineering, Nanyang Technological University, Jurong West, Singapore; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, and the Engineering Research Center of Process System Engineering, Ministry of Education, East China University of Science and Technology, Shanghai, China","IEEE Transactions on Circuits and Systems I: Regular Papers","31 Mar 2023","2023","70","4","1685","1695","In Home Energy Management System (HEMS), the scheduling of energy storage equipment and shiftable loads has been widely studied to reduce home energy costs. However, existing data-driven methods can hardly ensure the transferability amongst different tasks, such as customers with diverse preferences, appliances, and fluctuations of renewable energy in different seasons. This paper designs a transferable scheduling strategy for HEMS with different tasks utilizing a Meta-Reinforcement Learning (Meta-RL) framework, which can alleviate data dependence and massive training time for other data-driven methods. Specifically, a more practical and complete demand response scenario of HEMS is considered in the proposed Meta-RL framework, where customers with distinct electricity preferences, as well as fluctuating renewable energy in different seasons are taken into consideration. An inner level and an outer level are integrated in the proposed Meta-RL-based transferable scheduling strategy, where the inner and the outer level ensure the learning speed and appropriate initial model parameters, respectively. Moreover, Long Short-Term Memory (LSTM) is presented to extract the features from historical actions and rewards, which can overcome the challenges brought by the uncertainties of renewable energy and the customers’ loads, and enhance the robustness of scheduling strategies. A set of experiments conducted on practical data of Australia’s electricity network verify the performance of the transferable scheduling strategy.","1558-0806","","10.1109/TCSI.2023.3240702","National Natural Science Foundation of China(grant numbers:61988101,62293502,62293504); Program of Shanghai Academic Research Leader(grant numbers:20XD1401300); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10081065","Home energy management system;transferable scheduling strategies;meta-reinforcement learning;long short-term memory","Costs;Optimal scheduling;Task analysis;Demand response;Renewable energy sources;Batteries;Load modeling","building management systems;demand side management;energy management systems;learning (artificial intelligence);power engineering computing;recurrent neural nets;reinforcement learning;scheduling","complete demand response scenario;customers;data dependence;data-driven methods;distinct electricity preferences;diverse preferences;energy storage equipment;HEMS;home energy costs;Home Energy Management System;learning speed;Meta-Reinforcement Learning framework;Meta-Reinforcement Learning-based transferable scheduling strategy;Meta-RL framework;Meta-RL-based transferable scheduling strategy;outer level;practical data;practical demand response scenario;renewable energy;scheduling strategies;shiftable loads","","1","","54","IEEE","24 Mar 2023","","","IEEE","IEEE Journals"
"Three-Dimensional Waypoint Navigation of Multicopters by Attitude and Throttle Commands using Off-Policy Reinforcement Learning","F. d’Apolito; C. Sulzbachner","Center for Vision, Automation and Control, AIT - Austrian Institute of Technology, Vienna, Austria; Center for Vision, Automation and Control, AIT - Austrian Institute of Technology, Vienna, Austria","2022 International Conference on Unmanned Aircraft Systems (ICUAS)","26 Jul 2022","2022","","","1359","1366","Artificial intelligence, in particular machine learning, is becoming increasingly important in automation and robotics. Machine learning approaches are also becoming more and more accepted in aviation. In particular, Reinforcement Learning is gaining more attention in navigation and control problems, for example in training flight manoeuvres. This paper aims to investigate the use of Off-Policy Reinforcement Learning techniques for three-dimensional waypoint navigation of multicopters by providing roll, pitch and throttle commands. It describes and compare the trainings performed using two well-known Off-Policy algorithms, namely the Deep Deterministic Policy Gradient (DDPG) and the Soft Actor Critic (SAC). Furthermore, we investigate the impact of the reward definition on the training outcome. For each of the used algorithm, two agents are trained with two different reward definitions. Finally, the paper shows the validations performed to evaluate the performance of the four trained agents under different known and unknown conditions. Their performances are evaluated and compared with respect to the training algorithm and the reward definition used.","2575-7296","978-1-6654-0593-5","10.1109/ICUAS54217.2022.9836078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9836078","","Training;Automation;Navigation;Reinforcement learning;Aircraft;Robots","helicopters;path planning;reinforcement learning","artificial intelligence;deep deterministic policy gradient;known conditions;machine learning approaches;off-policy algorithms;off-policy reinforcement;reinforcement learning;reward definition;reward definitions;soft actor critic;throttle commands;trained agents;training algorithm;training flight manoeuvres;training outcome;unknown conditions","","","","21","IEEE","26 Jul 2022","","","IEEE","IEEE Conferences"
"Multi-objective energy management for we-energy in Energy Internet using reinforcement learning","Q. Sun; D. Wang; D. Ma; B. Huang","School of Information Science and Engineering, Northeastern University, Shenyang, China; School of Information Science and Engineering, Northeastern University, Shenyang, China; School of Information Science and Engineering, Northeastern University, Shenyang, China; School of Information Science and Engineering, Northeastern University, Shenyang, China","2017 IEEE Symposium Series on Computational Intelligence (SSCI)","8 Feb 2018","2017","","","1","6","We-Energy is a novel energy production-storage-consumption mode proposed for Energy Internet, where more renewable energy can be utilized. This paper mainly focuses on the energy management of We-Energy in Energy Internet consisting of combined heat and power unit (CHP), photovoltaic unit, heating only unit and storage device. To construct an environmental-friendly and low-operating cost energy consumption structure, a multi-objective optimization model is proposed in this paper. Furthermore, in order to satisfy the power and heat demands of the We-Energy simultaneously as well as realizing minimum operating cost and pollutant emission, an intelligent energy management system (IEMS) is presented. In particular, reinforcement learning method has been implemented to formulate the optimal operating strategy. Eligibility trace theory is also been introduced to accelerate the computational process. Finally, simulation results are given to prove the effectiveness of the proposed optimization strategy.","","978-1-5386-2726-6","10.1109/SSCI.2017.8285243","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8285243","Energy management;reinforcement learning;We-Energy;multi-objective optimization","Cogeneration;Internet;Energy management;Optimization;Learning (artificial intelligence);Resistance heating","cogeneration;energy conservation;energy consumption;energy management systems;energy storage;learning (artificial intelligence);optimisation;photovoltaic power systems;power engineering computing;renewable energy sources","intelligent energy management system;multiobjective energy management;energy production-storage-consumption mode;renewable energy;photovoltaic unit;storage device;environmental-friendly operating cost energy consumption structure;low-operating cost energy consumption structure;multiobjective optimization model;energy Internet;combined heat and power unit;CHP;heating only unit;IEMS;reinforcement learning method;eligibility trace theory","","5","","11","IEEE","8 Feb 2018","","","IEEE","IEEE Conferences"
"An Automatic Chord Progression Generator Based On Reinforcement Learning","S. Shukla; H. Banka","Department of Computer Science and Engineering, Indian Institute of Technology (ISM), Dhanbad, India; Department of Computer Science and Engineering, Indian Institute of Technology (ISM), Dhanbad, India","2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","2 Dec 2018","2018","","","55","59","Rapidly developing technology has led convincing improvements in the music production domain and generation of chord progression is one of the affected areas. Chords are the most significant part of music to create harmony. An arrangement of two or more chords in a music portion makes chord progressions. Generation of chord progression is not just a random selection of notes; it requires as much knowledge of grammatical rules as we need for selecting letters to write a sentence. Some note combinations may sound good together while others may sound harsh. A novice musician has to spend significant amount of time to achieve perfection in music composition. To make the task of musicians less time consuming, we delve into the usefulness of automatic chord progression generation through RL algorithm. RL has already been used in numerous fields, but researchers are still investigating its performance in creative tasks. The proposed work uses music theory concepts to define the rewards and Q-learning algorithm to train the RL agent. The fundamental objective of this paper is twofold. Firstly, the goal is to test this approach as an alternative to generate chord progression. Secondly, the aim is to generate tuneful chord progression that the composers can utilize in their work. Results are validated by comparing it with the chords used in the database of 1300 songs.","","978-1-5386-5314-2","10.1109/ICACCI.2018.8554901","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554901","Chord progression;Music;Reinforcement learning;Q-learning","Music;Generators;Genetic algorithms;Task analysis;Markov processes","audio signal processing;learning (artificial intelligence);music","music production domain;music portion;automatic chord progression generation;tuneful chord progression;automatic chord progression generator;reinforcement learning;RL algorithm;Q-learning algorithm;music composition","","5","","15","IEEE","2 Dec 2018","","","IEEE","IEEE Conferences"
"Using reinforcement learning to make smart energy storage sources in microgrid","S. Etemad; N. Mozayani","School of Computer Engineering, Iran University of Science and Technology, Tehran, Iran; School of Computer Engineering, Iran University of Science and Technology, Tehran, Iran","2015 30th International Power System Conference (PSC)","23 Jan 2017","2015","","","345","350","The use of renewable energy in power generation, sudden changes in load and fault in power transmission lines, causing a voltage drop in the system and challenge the reliability of the system. One way to compensate the changing nature of renewable energies in the short term without the need to disconnect loads or turn on other plants, use of renewable energies. The use of energy storage improved electrical stability, power quality and improve the peak power load. In this paper, using reinforcement learning to present optimal method for charge and discharge the consumer battery and Uncertainty of production could be due to the random nature of wind energy improved. Simulation results indicate not only the use of renewable energy and battery enhanced but also the cost of annual payments and peak consumption times reduced.","","978-1-5090-2705-7","10.1109/IPSC.2015.7827771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827771","reinforcement learning;microgrid;renewable energy;battery;Q learning;intelligence agent","Batteries;Learning (artificial intelligence);Microgrids;Wind turbines;Load modeling;Renewable energy sources","distributed power generation;learning (artificial intelligence);power engineering computing;power generation faults;power generation reliability;power supply quality;power system stability;power transmission lines;wind power","smart energy storage source;reinforcement learning;microgrid;power transmission line;power generation fault;voltage drop;power system reliability;electrical stability;power quality;peak power load improvement;production uncertainty;consumer battery;wind energy random nature","","4","","14","IEEE","23 Jan 2017","","","IEEE","IEEE Conferences"
"Introducing Deep Reinforcement Learning to Nlu Ranking Tasks","G. Yu; E. Barut; C. Su",NA; NA; NA,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","3465","3469","Natural language understanding (NLU) models in production systems rely heavily on human annotated data, which involves an expensive, time-consuming, and error-prone process. Moreover, the model release process requires offline supervised learning and human in-the-loop. Together, these factors prolong the model update cycle and result in sub-standard model performance in situations where usage behavior is non-stationary. In this paper, we address these issues with a deep reinforcement learning approach that ranks suggestions from multiple experts in an online fashion. Our proposed method removes the reliance on annotated data, and can effectively adapt to recent changes in the data distribution. The efficiency of the new approach is demonstrated through simulation experiments using logged data from voice-based virtual assistants. Our results show that our algorithm, without any reliance on annotation, outperforms offline supervised learning methods.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414475","ranking;natural language understanding;reinforcement learning","Adaptation models;Uncertainty;Friction;Computational modeling;Virtual assistants;Supervised learning;Reinforcement learning","deep learning (artificial intelligence);natural language processing","natural language understanding model;production systems;human annotated data;offline supervised learning;model update cycle;usage behavior;data distribution;logged data;deep reinforcement learning;NLU ranking task;voice-based virtual assistant","","1","","13","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Autonomous Car Parking System using Deep Reinforcement Learning","R. Takehara; T. Gonsalves","Dept. of Information & Communication Sciences, Faculty of Science & Technology, Sophia University, Tokyo, Japan; Dept. of Information & Communication Sciences, Faculty of Science & Technology, Sophia University, Tokyo, Japan","2021 2nd International Conference on Innovative and Creative Information Technology (ICITech)","9 Nov 2021","2021","","","85","89","In recent years, technologies based on deep learning have been useful in various aspects of our daily lives. In the field of automated driving, which is attracting particular attention, image recognition technology is used to detect roads, white lines, and vehicles ahead. However, since automated vehicles are controlled by acquiring information about the vehicle's position and surrounding environment mainly from image sensors and cameras, the production cost is very high. The goal of this research is to develop autonomous driving technology using only an on-board visual camera, without any image sensors. Automatic parking is implemented using reinforcement learning in the virtual environment of Unity. Autonomous parking with high accuracy is achieved by using the input image as a segmentation image.","","978-1-7281-9747-0","10.1109/ICITech50181.2021.9590169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9590169","autonomous cars;self-parking;deep learning;segmentation","Image sensors;Image segmentation;Visualization;Virtual environments;Reinforcement learning;Cameras;Turning","automobiles;cameras;deep learning (artificial intelligence);image segmentation;image sensors;mobile robots;reinforcement learning;road traffic control;road vehicles","reinforcement learning;virtual environment;image segmentation;autonomous car parking system;deep learning;automated driving;image recognition technology;roads;automated vehicles;image sensors;production cost;autonomous driving technology;on-board visual camera","","","","13","IEEE","9 Nov 2021","","","IEEE","IEEE Conferences"
"Distributed Data Center Cooling control based on Multi-Agent Reinforcement Learning","D. Chen; J. Wan; L. Li; C. Liu","Inner Mongolia University of Technology, Hohhot, China; Inner Mongolia University of Technology, Hohhot, China; Inner Mongolia University of Technology, Hohhot, China; Inner Mongolia University of Technology, Hohhot, China","2022 4th International Conference on Frontiers Technology of Information and Computer (ICFTIC)","27 Mar 2023","2022","","","456","461","The data center cooling system is a complex-structured system integrated with a large number of components. Traditional centralized control methods are hard to apply in production data centers due to scalability issues. This paper studies the distributed cooling control problem where the data center is cooled by multiple Computer Room Air Conditioners (CRACs) to minimize the energy consumption. To characterize the interactions of CRACs, we formulate the distributed data center cooling control problem as a stochastic game, where each CRAC acts as an agent to learn the optimal local cooling policy. We propose a Multi-Agent Reinforcement Learning (MARL) framework based on the Counterfactual Multi-Agent (COMA). Specifically, we policies based on the agents' observations of the global state. After the implementation of these policies, they will receive their own rewards, then evaluate the quality of the policies. Simulation results show that compared with the Independent Q-Learning (IQL) method, this algorithm reduces the power consumption of CRACs by 6.2%.","","979-8-3503-2195-1","10.1109/ICFTIC57696.2022.10075260","National Natural Science Foundation of China(grant numbers:61862048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10075260","data center cooling control;multi-agent reinforcement learning;energy optimization","Data centers;Q-learning;Cooling;Simulation;Scalability;Software algorithms;Stochastic processes","air conditioning;computer centres;cooling;energy consumption;learning (artificial intelligence);multi-agent systems;reinforcement learning;stochastic games","complex-structured system;control methods;Counterfactual MultiAgent;CRAC;data center cooling system;distributed cooling;distributed data center cooling;MultiAgent Reinforcement Learning framework;multiple Computer Room Air Conditioners;optimal local cooling policy;production data centers","","","","15","IEEE","27 Mar 2023","","","IEEE","IEEE Conferences"
"Application of reinforcement learning-based algorithms in CO2 allowance and electricity markets","V. Nanduri","Department of Industrial & Manufacturing Engineering, University of Wisconsin, Milwaukee, Milwaukee, WI, USA","2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)","28 Jul 2011","2011","","","164","169","Climate change is one of the most important challenges faced by the world this century. In the U.S., the electric power industry is the largest emitter of CO2, contributing to the climate crisis. Federal emissions control bills in the form of cap-and-trade programs are currently idling in the U.S. Congress. In the mean time, ten states in the northeastern U.S. have adopted a regional cap-and-trade program to reduce CO2 levels and also to increase investments in cleaner technologies. Many of the states in which the cap-and-trade programs are active operate under a restructured market paradigm, where generators compete to supply power. This research presents a bi-level game-theoretic model to capture competition between generators in cap-and-trade markets and restructured electricity markets. The solution to the game-theoretic model is obtained using a reinforcement learning based algorithm.","2325-1867","978-1-4244-9888-8","10.1109/ADPRL.2011.5967367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967367","reinforcement learning;cap-and-trade programs;game-theoretic models;climate change;restructured electricity markets","Generators;Electricity supply industry;Games;Electricity;Companies;Meteorology;Power systems","climatology;environmental science computing;game theory;geophysics computing;learning (artificial intelligence);pollution control;power engineering computing;power markets","reinforcement learning-based algorithms;CO2 allowance;restructured electricity markets;climate change;electric power industry;climate crisis;federal emissions control bills;cap-and-trade programs;U.S. Congress;bi-level game-theoretic model;cap-and-trade markets","","4","","25","IEEE","28 Jul 2011","","","IEEE","IEEE Conferences"
"Learning Automata-Based Multiagent Reinforcement Learning for Optimization of Cooperative Tasks","Z. Zhang; D. Wang; J. Gao","School of Automation, Qingdao University, Qingdao, China; School of Electrical Engineering, Qingdao University, Qingdao, China; School of Automation, Qingdao University, Qingdao, China","IEEE Transactions on Neural Networks and Learning Systems","5 Oct 2021","2021","32","10","4639","4652","Multiagent reinforcement learning (MARL) has been extensively used in many applications for its tractable implementation and task distribution. Learning automata, which can be classified under MARL in the category of independent learner, are used to obtain the optimal joint action or some type of equilibrium. Learning automata have the following advantages. First, learning automata do not require any agent to observe the action of any other agent. Second, learning automata are simple in structure and easy to be implemented. Learning automata have been applied to function optimization, image processing, data clustering, recommender systems, and wireless sensor networks. However, a few learning automata-based algorithms have been proposed for optimization of cooperative repeated games and stochastic games. We propose an algorithm known as learning automata for optimization of cooperative agents (LA-OCA). To make learning automata applicable to cooperative tasks, we transform the environment to a P-model by introducing an indicator variable whose value is one when the maximal reward is obtained and is zero otherwise. Theoretical analysis shows that all the strict optimal joint actions are stable critical points of the model of LA-OCA in cooperative repeated games with an arbitrary finite number of players and actions. Simulation results show that LA-OCA obtains the pure optimal joint strategy with a success rate of 100% in all of the three cooperative tasks and outperforms the other algorithms in terms of learning speed.","2162-2388","","10.1109/TNNLS.2020.3025711","National Natural Science Foundation of China(grant numbers:61903209,61873138,61573205); Qingdao Postdoctoral Applied Research Project with Name (AGV Road Network Design and Path Planning Method Based on Multi-Agent Reinforcement Learning); Shandong Provincial Natural Science Foundation of China(grant numbers:ZR2019MF063); Science and Technology Support Plan for Youth Innovation of Universities in Shandong Province(grant numbers:2019KJN033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9216601","Learning automata;multiagent reinforcement learning (MARL);multiagent system;reinforcement learning (RL)","Games;Learning automata;Task analysis;Stochastic processes;Optimization;Learning (artificial intelligence);Clustering algorithms","learning (artificial intelligence);learning automata;multi-agent systems;optimisation;stochastic games","learning automata-based multiagent reinforcement learning;cooperative task optimization;MARL;independent learner;optimal joint action;cooperative agent optimization;LA-OCA model;P-model;indicator variable;maximal reward;cooperative repeated games;pure optimal joint strategy;stochastic games","","32","","55","IEEE","7 Oct 2020","","","IEEE","IEEE Journals"
"Probabilistic Charging Power Forecast of EVCS: Reinforcement Learning Assisted Deep Learning Approach","Y. Li; S. He; Y. Li; L. Ge; S. Lou; Z. Zeng","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; China-EU Institute for Clean and Renewable Energy, Huazhong University of Science and Technology, Wuhan, China; School of Electrical Engineering, Northeast Electric Power University, Jilin, China; Key Laboratory of Smart Grid of Ministry of Education, Tianjin University, Tianjin, China; School of Electrical and Electronic Engineering, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Intelligent Vehicles","23 Jan 2023","2023","8","1","344","357","The electric vehicle (EV) and electric vehicle charging station (EVCS) have been widely deployed with the development of large-scale transportation electrifications. However, since charging behaviors of EVs show large uncertainties, the forecasting of EVCS charging power is non-trivial. This paper tackles this issue by proposing a reinforcement learning assisted deep learning framework for the probabilistic EVCS charging power forecasting to capture its uncertainties. Since the EVCS charging power data are not standard time-series data like electricity load, they are first converted to the time-series format. On this basis, one of the most popular deep learning models, the long short-term memory (LSTM) is used and trained to obtain the point forecast of EVCS charging power. To further capture the forecast uncertainty, a Markov decision process (MDP) is employed to model the change of LSTM cell states, which is solved by our proposed adaptive exploration proximal policy optimization (AePPO) algorithm based on reinforcement learning. Finally, experiments are carried out on the real EVCSs charging data from Caltech, and Jet Propulsion Laboratory, USA, respectively. The results and comparative analysis verify the effectiveness and outperformance of our proposed framework.","2379-8904","","10.1109/TIV.2022.3168577","National Natural Science Foundation of China(grant numbers:62073148); Tencent Rhinoceros Foundation of China(grant numbers:CCF-Tencent RAGR20210102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9760076","Deep learning;electric vehicle charging station;forecast uncertainty;probabilistic charging power forecasting;reinforcement learning","Probabilistic logic;Deep learning;Predictive models;Reinforcement learning;Forecast uncertainty;Electric vehicle charging;Data models","battery powered vehicles;deep learning (artificial intelligence);electric vehicle charging;load forecasting;Markov processes;optimisation;power engineering computing;recurrent neural nets;reinforcement learning;time series","adaptive exploration proximal policy optimization algorithm;AePPO;Caltech;electric vehicle charging station;electricity load;EVCS;forecast uncertainty;Jet Propulsion Laboratory;long short-term memory;LSTM;Markov decision process;MDP;probabilistic charging power forecast;reinforcement learning assisted deep learning approach;standard time-series data;transportation electrification;USA","","13","","31","IEEE","19 Apr 2022","","","IEEE","IEEE Journals"
"Inter-Cell Network Slicing With Transfer Learning Empowered Multi-Agent Deep Reinforcement Learning","T. Hu; Q. Liao; Q. Liu; G. Carle","Network Automation Department, NSSR Lab, Nokia Bell Labs, Stuttgart, Germany; Network Automation Department, NSSR Lab, Nokia Bell Labs, Stuttgart, Germany; School of Computing, University of Nebraska–Lincoln, Lincoln, NE, USA; Department of Computer Engineering, Technical University of Munich, Munich, Germany","IEEE Open Journal of the Communications Society","19 May 2023","2023","4","","1141","1155","Network slicing enables operators to cost-efficiently support diverse applications on a common physical infrastructure. The ever-increasing densification of network deployment leads to complex and non-trivial inter-cell interference, which requires more than inaccurate analytic models to dynamically optimize resource management for network slices. In this paper, we develop a DRIP algorithm with multiple deep reinforcement learning (DRL) agents to cooperatively optimize resource partition in individual cells to fulfill the requirements of each slice, based on two alternative reward functions with max-min fairness and logarithmic utility. Nevertheless, existing DRL approaches usually tie the pretrained model parameters to specific network environments with poor transferability, which raises practical deployment concerns in large-scale mobile networks. Hence, we design a novel transfer learning-aided DIRP (TL-DIRP) algorithm to ease the transfer of DRIP agents across different network environments in terms of sample efficiency, model reproducibility, and algorithm scalability. The TL-DIRP algorithm first centrally trains a generalized model and then transfers the “generalist” to each local agent (a.k.a., the “specialist”) with distributed finetuning and execution. TL-DIRP consists of two steps: 1) centralized training of a generalized distributed model, and 2) transferring the “generalist” to each local agent with distributed finetuning and execution. We comprehensively investigate different types of transferable knowledge: model transfer, instance transfer, and combined model and instance transfer. We evaluate the proposed algorithms in a system-level network simulator with 12 cells. The numerical results show that not only DIRP outperforms existing baseline approaches in terms of faster convergence and higher reward, but more importantly, TL-DIRP significantly improves the service performance, with reduced exploration cost, accelerated convergence rate, and enhanced model reproducibility. As compared to a traffic-aware baseline, TL-DIRP provides about 15% less violation ratio of the quality of service (QoS) for the worst slice service and 8.8% less violation on the average service QoS.","2644-125X","","10.1109/OJCOMS.2023.3273310","German Federal Ministry of Education and Research (BMBF) Project KICK(grant numbers:16KIS1102K); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10121811","Transfer learning;deep reinforcement learning;multi-agent coordination;network slicing;resource allocation","Resource management;Network slicing;Heuristic algorithms;Optimization;Convergence;Dynamic scheduling;Numerical models","5G mobile communication;deep learning (artificial intelligence);densification;multi-agent systems;optimisation;quality of service;reinforcement learning;resource allocation;telecommunication computing;telecommunication scheduling;telecommunication traffic","algorithm scalability;alternative reward functions;common physical infrastructure;cooperatively optimize resource partition;distributed finetuning;DRIP agents;enhanced model reproducibility;generalized distributed model;generalized model;inaccurate analytic models;individual cells;instance transfer;inter-cell network;large-scale mobile networks;local agent;max-min fairness;model transfer;multiple deep reinforcement learning agents;network deployment;network environments;network slicing;nontrivial inter-cell interference;poor transferability;pretrained model parameters;resource management;sample efficiency;specific network environments;system-level network simulator;TL-DIRP algorithm;transfer learning-aided DIRP algorithm;transferable knowledge","","","","44","CCBYNCND","9 May 2023","","","IEEE","IEEE Journals"
"Efficient Reinforcement Learning for Autonomous Ship Collision Avoidance under Learning Experience Reuse","C. Wang; X. Zhang; H. Gao; H. Su; K. Zheng; W. Wang","Navigation College, Dalian Maritime University, Dalian, China; Shenzhen Research Institute, Dalian Maritime University, Shenzhen, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Navigation College, Dalian Maritime University, Dalian, China; Navigation College, Dalian Maritime University, Dalian, China","2022 IEEE International Conference on Unmanned Systems (ICUS)","29 Dec 2022","2022","","","1563","1568","In this paper, a learning experience reuse - reinforcement learning collision avoidance (LER-RLCA) method is proposed, which can synthesize near-optimal collision avoidance policy with efficient sampling and good seamanship, to solve the local safety sailing of autonomous ship in a multi-obstacle environment. Lying on the general reinforcement learning (RL), using learning experience reuse, the hidden features of historical training data were mined. Meanwhile, a new reward function combining external revenue signal with internal incentive signal was designed to encourage search the environment with a low probability of state transition. We further applied LER-RLCA algorithm to the simulation of autonomous ship collision avoidance. The results show that the proposed LER-RLCA algorithm can well realize the collision-free and safe navigation of autonomous ships, to avoid falling into local iteration, greatly improve the convergence speed of the algorithm, and improve the performance of online collision avoidance decision-making.","2771-7372","978-1-6654-8456-5","10.1109/ICUS55513.2022.9986793","National Natural Science Foundation of China(grant numbers:U20A20225,U2013601); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9986793","maritime safety guarantee;learning experience reuse (LER);deep reinforcement learning (DRL);autonomous ships;intelligent collision avoidance","Navigation;Training data;Reinforcement learning;Real-time systems;Iterative algorithms;Safety;Collision avoidance","collision avoidance;learning (artificial intelligence);probability;ships","autonomous ship collision avoidance;efficient reinforcement;efficient sampling;general reinforcement learning;learning experience reuse - reinforcement learning collision avoidance;LER-RLCA algorithm;multiobstacle environment;near-optimal collision avoidance policy;online collision avoidance decision-making","","","","23","IEEE","29 Dec 2022","","","IEEE","IEEE Conferences"
"Barrier Certified Safety Learning Control: When Sum-of-Square Programming Meets Reinforcement Learning","H. Huang; Z. Li; D. Han","Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong; Department of Electrical and Electronic Engineering, The University of Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong","2022 IEEE Conference on Control Technology and Applications (CCTA)","8 Dec 2022","2022","","","1190","1195","Safety guarantee is essential in many engineering implementations. Reinforcement learning provides a useful way to strengthen safety. However, reinforcement learning algorithms cannot completely guarantee safety over realistic op-erations. To address this issue, this work adopts control barrier functions over reinforcement learning, and proposes a compen-sated algorithm to completely maintain safety. Specifically, a sum-of-squares programming has been exploited to search for the optimal controller, and tune the learning hyperparameters simultaneously. Thus, the control actions are pledged to be always within the safe region. The effectiveness of proposed method is demonstrated via an inverted pendulum model. Compared to quadratic programming based reinforcement learning methods, our sum-of-squares programming based reinforcement learning has shown its superiority.","2768-0770","978-1-6654-7338-5","10.1109/CCTA49430.2022.9966192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9966192","","Heuristic algorithms;Computational modeling;Reinforcement learning;Programming;Safety;Numerical models;Control theory","control engineering computing;control system synthesis;learning systems;nonlinear control systems;optimal control;pendulums;reinforcement learning","barrier certified safety learning control;control actions;control barrier functions;inverted pendulum;learning hyperparameters;optimal controller;reinforcement learning;safety guarantee;sum-of-square programming","","","","35","IEEE","8 Dec 2022","","","IEEE","IEEE Conferences"
"Dynamic Dispatching for Large-Scale Heterogeneous Fleet via Multi-agent Deep Reinforcement Learning","C. Zhang; P. Odonkor; S. Zheng; H. Khorasgani; S. Serita; C. Gupta; H. Wang","Industrial AI Lab Hitachi America Ltd., Santa Clara, CA; Stevens Institute of Technology, Hoboken, NJ; Industrial AI Lab Hitachi America Ltd., Santa Clara, CA; Industrial AI Lab Hitachi America Ltd., Santa Clara, CA; Industrial AI Lab Hitachi America Ltd., Santa Clara, CA; Industrial AI Lab Hitachi America Ltd., Santa Clara, CA; Industrial AI Lab Hitachi America Ltd., Santa Clara, CA","2020 IEEE International Conference on Big Data (Big Data)","19 Mar 2021","2020","","","1436","1441","Dynamic dispatching is one of the core problems for operation optimization in traditional industries such as mining, as it is about how to smartly allocate the right resources to the right place at the right time. Conventionally, the industry relies on heuristics or even human intuitions which are often short-sighted and sub-optimal solutions. Leveraging the power of AI and Internet of Things (IoT), data-driven automation is reshaping this area. However, facing its own challenges such as large-scale and heterogenous trucks running in a highly dynamic environment, it can barely adopt methods developed in other domains (e.g., ride-sharing). In this paper, we propose a novel Deep Reinforcement Learning approach to solve the dynamic dispatching problem in mining. We first develop an event-based mining simulator with parameters calibrated in real mines. Then we propose an experience-sharing Deep Q Network with a novel abstract state/action representation to learn memories from heterogeneous agents altogether and realizes learning in a centralized way. We demonstrate that the proposed methods significantly outperform the most widely adopted approaches in the industry by 5.56% in terms of productivity. The proposed approach has great potential in a broader range of industries (e.g., manufacturing, logistics) which have a large-scale of heterogenous equipment working in a highly dynamic environment, as a general framework for dynamic resource allocation.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9378191","Hitachi; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9378191","Dispatching;Reinforcement Learning;Mining","Industries;Reinforcement learning;Big Data;Dynamic scheduling;Dispatching;Internet of Things;Optimization","data mining;learning (artificial intelligence);multi-agent systems;resource allocation","heterogenous trucks;highly dynamic environment;ride-sharing;novel Deep Reinforcement Learning approach;dynamic dispatching problem;event-based mining;experience-sharing Deep Q Network;heterogeneous agents;widely adopted approaches;heterogenous equipment;dynamic resource allocation;large-scale heterogeneous fleet;multiagent Deep Reinforcement Learning;core problems;operation optimization;human intuitions;sub-optimal solutions;data-driven automation","","7","","22","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Resource Allocation in 5G Communications","M. -L. Tham; A. Iqbal; Y. C. Chang","Department of Electrical and Electronic Engineering, Lee Kong Chian Faculty of Engineering and Science, Universiti Tunku Abdul Rahman (UTAR), Malaysia; Department of Electrical and Electronic Engineering, Lee Kong Chian Faculty of Engineering and Science, Universiti Tunku Abdul Rahman (UTAR), Malaysia; Department of Electrical and Electronic Engineering, Lee Kong Chian Faculty of Engineering and Science, Universiti Tunku Abdul Rahman (UTAR), Malaysia","2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","5 Mar 2020","2019","","","1852","1855","The rapid growth of data traffic has pushed the mobile telecommunication industry towards the adoption of fifth generation (5G) communications. Cloud radio access network (CRAN), one of the 5G key enabler, facilitates fine-grained management of network resources by separating the remote radio head (RRH) from the baseband unit (BBU) via a highspeed front-haul link. Classical resource allocation (RA) schemes rely on numerical techniques to optimize various performance metrics. Most of these works can be defined as instantaneous since the optimization decisions are derived from the current network state without considering past network states. While utility theory can incorporate long-term optimization effect into these optimization actions, the growing heterogeneity and complexity of network environments has rendered the RA issue intractable. One prospective candidate is reinforcement learning (RL), a dynamic programming framework which solves the RA problems optimally over varying network states. Still, such method cannot handle the highly dimensional state-action spaces in the context of CRAN problems. Driven by the success of machine learning, researchers begin to explore the potential of deep reinforcement learning (DRL) to address the RA problems. In this work, an overview of the major existing DRL approaches in CRAN is presented. We conclude this article by identifying current technical hurdles and potential future research directions.","2640-0103","978-1-7281-3248-8","10.1109/APSIPAASC47483.2019.9023112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9023112","Deep Reinforcement Learning;5G;Resource Allocation;Cloud RAN","5G mobile communication;Optimization;Resource management;Machine learning;Task analysis;Complexity theory;Antennas","5G mobile communication;cloud computing;dynamic programming;learning (artificial intelligence);mobility management (mobile radio);radio access networks;resource allocation;telecommunication computing;telecommunication traffic;utility theory","high-speed front-haul link;network environment complexity;high dimensional state-action spaces;current network state;optimization decisions;performance metrics;numerical techniques;classical resource allocation schemes;baseband unit;remote radio head;network resources;fine-grained management;5G key enabler;cloud radio access network;fifth generation communications;mobile telecommunication industry;data traffic;5G communications;deep reinforcement learning;machine learning;CRAN problems;network states;RA problems;dynamic programming framework;optimization actions;long-term optimization effect;utility theory","","7","","24","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"A Preview Neuro-Fuzzy Controller Based on Deep Reinforcement Learning for Backing Up a Truck-Trailer Vehicle","E. Bejar; A. Moran","Engineering Department, Pontifical Catholic University of Peru, Lima, Peru; Engineering Department, Pontifical Catholic University of Peru, Lima, Peru","2019 IEEE Canadian Conference of Electrical and Computer Engineering (CCECE)","11 Oct 2019","2019","","","1","4","Backing up truck-trailer vehicles is a required task in several industry sectors. Controllers that have been proposed to automate this process usually struggle when the vehicle is required to follow a nonlinear trajectory. This paper presents a neuro-fuzzy controller based on preview control and deep reinforcement learning for reverse parking truck-trailer vehicles. The controller consists of a deep neural network trained with reinforcement learning. A preview control signal is coupled into the trained controller to improve the control performance when tracking complex trajectories. Moreover, a fuzzy logic approach is used to avoid the jackknife state. Simulation results are presented to show that the controller is able to track circular and sinusoidal trajectories.","2576-7046","978-1-7281-0319-8","10.1109/CCECE.2019.8861534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8861534","artificial intelligence;reinforcement learning;deep learning;self-driving vehicles;robotics;control systems.","Trajectory;Reinforcement learning;Neural networks;Fuzzy logic;Training;Robots;Neurocontrollers","fuzzy control;fuzzy logic;learning (artificial intelligence);neurocontrollers;nonlinear control systems;road traffic control;trajectory control;vehicle dynamics","deep reinforcement learning;industry sectors;nonlinear trajectory;reverse parking truck-trailer vehicles;deep neural network;preview control signal;fuzzy logic approach;neuro-fuzzy controller","","6","","13","IEEE","11 Oct 2019","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Energy Management System Enhancement Using Digital Twin for Electric Vehicles","Y. Ye; B. Xu; J. Zhang; B. Lawler; B. Ayalew","Department of Automotive Engineering, Clemson University, Greenville, SC, USA; School of Aerospace and Mechanical Engineering, The University of Oklahoma, Norman, OK, USA; Department of Automotive Engineering, Clemson University, Greenville, SC, USA; Department of Automotive Engineering, Clemson University, Greenville, SC, USA; Department of Automotive Engineering, Clemson University, Greenville, SC, USA","2022 IEEE Vehicle Power and Propulsion Conference (VPPC)","5 Jan 2023","2022","","","1","6","Compared to conventional engine-based powertrains, electrified powertrain exhibit increased energy efficiency and reduced emissions, making electrification a key goal for the automotive industry. For a vehicle with hybrid energy storage system, its performance and lifespan are substantially affected by the energy management system. Reinforcement learning-based methods are gaining popularity in vehicle energy management, but most of the literature in this area focus on pure simulation while hardware implementation is still limited. This paper introduces the digital twin methodology to enhance the Q-learning-based energy management system for battery and ultracapacitor electric vehicles. The digital twin model can exploit the bilateral interdependency between the virtual model and the actual system, which improves the control performance of the energy management system. The physical model is established based on a hardware-in-the-loop simulation platform. In addition, battery degradation is also considered for prolonging the battery lifespan to reduce the operating cost. The validation results of the trained reinforcement learning agent illustrate that the digital twin-enhanced Q-learning energy management system improves the energy efficiency by 4.36% and the battery degradation is reduced by 25.28%.","1938-8756","978-1-6654-7587-7","10.1109/VPPC55846.2022.10003411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003411","Reinforcement learning;Digital twin;Energy management;Electric vehicle","Degradation;Q-learning;Costs;Mechanical power transmission;Supercapacitors;Propulsion;Energy efficiency","battery powered vehicles;digital twins;energy conservation;energy management systems;hardware-in-the-loop simulation;hybrid electric vehicles;power engineering computing;reinforcement learning;supercapacitors","automotive industry;battery degradation;battery electric vehicles;battery lifespan;bilateral interdependency;control performance;digital twin model;electrified powertrain;emission reduction;energy efficiency;energy management system;hardware-in-the-loop simulation platform;hybrid energy storage system;operating cost reduction;Q-learning;reinforcement learning-based methods;ultracapacitor electric vehicles","","3","","16","IEEE","5 Jan 2023","","","IEEE","IEEE Conferences"
"Spectrum Management in High-Speed Railway Cooperative Cognitive Radio Network Based on Multi-agent Reinforcement Learning","C. Wang; Q. Wu; Z. Tang; J. Sheng; C. Wu; Y. Wang","School of Rail Transportation, Soochow University, 8 Jixue Road, Suzhou, Jiangsu, PRC; School of Rail Transportation, Soochow University, 8 Jixue Road, Suzhou, Jiangsu, PRC; School of Rail Transportation, Soochow University, 8 Jixue Road, Suzhou, Jiangsu, PRC; School of Rail Transportation, Soochow University, 8 Jixue Road, Suzhou, Jiangsu, PRC; School of Rail Transportation, Soochow University, 8 Jixue Road, Suzhou, Jiangsu, PRC; School of Rail Transportation, Soochow University, 8 Jixue Road, Suzhou, Jiangsu, PRC","2020 International Wireless Communications and Mobile Computing (IWCMC)","27 Jul 2020","2020","","","702","707","As the high-speed railway industry matures, higher requirements are put forward for the railway wireless communication, and the demand for spectrum resources is also increasing gradually. When the train is moving at a high speed, it will bring about the frequent handover of wireless communication networks, which will lead to the deterioration of wireless communication quality and even the dropping of calls. Therefore, based on the Cognitive Radio, we established the cognitive base station model to enable the base station to have cognitive functions in this paper. We also proposed a multiple base station cooperative reinforcement learning to achieve dynamic spectrum management. Cognitive base station can select the optimal channel for communication services by fusing interaction information between different cognitive base stations, thus reducing the failure of handover when the train crosses the cells. The experimental results showed that the proposed algorithm can improve the utilization of spectrum resources effectively.","2376-6506","978-1-7281-3129-0","10.1109/IWCMC48107.2020.9148437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9148437","Cognitvie radio;High speed railway;Spectrum management;milti-agent reinforcement learning","Rail transportation;Base stations;Learning (artificial intelligence);Cognitive radio;Handover","cognitive radio;cooperative communication;learning (artificial intelligence);mobility management (mobile radio);radio spectrum management;railway communication;telecommunication computing;telecommunication network reliability;wireless channels","cognitive radio;cognitive base station model;dynamic spectrum management;spectrum resources;multiagent reinforcement learning;high-speed railway industry;handover failure;railway wireless communication networks;multiple base station cooperative reinforcement learning","","1","","15","IEEE","27 Jul 2020","","","IEEE","IEEE Conferences"
"Active Object Searching Based on Deep Reinforcement Learning","R. Mao","School of Big Data and Soitware Engineering, ChongQing University, ChongQing, China","2020 International Conference on Computing and Data Science (CDS)","9 Dec 2020","2020","","","362","366","Recently, computer vision has progressed a lot and object detection by robot has drawn interests of many researchers. By combining artificial intelligence and traditional industry together, computer vision and object detection has gained remarkable achievements. Though robot can detect the still complete images quite efficiently with a considerable accuracy, yet the active object detection and searching is a relatively new field. Since a robot is able to adjust its position to get a better view of the images, a good policy of the action selection should be payed attention to. In this paper, several DQN-like algorithms are introduced and compared. Then in a bid to weaken the estimation bias of them, a Maxmin DQN algorithm is implemented on a household dataset. After comparing the performance of these algorithms, the Maxmin DQN proves to be the best on average.","","978-1-7281-7106-7","10.1109/CDS49703.2020.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9276007","object detection;reinforcement learning;object searching","Data science","computer vision;deep learning (artificial intelligence);object detection","active object searching;deep reinforcement learning;computer vision;artificial intelligence;traditional industry;active object detection;Maxmin DQN algorithm;household dataset;action selection","","","","27","IEEE","9 Dec 2020","","","IEEE","IEEE Conferences"
"A hybrid algorithm combining ant colony optimization and large neighborhood search based on reinforcement learning for integrated container-truck scheduling problem","Y. Zhang","Department of Logistics Management, School of Economics and Management, Beijing Jiaotong University, Beijing, China","2023 8th International Conference on Intelligent Computing and Signal Processing (ICSP)","19 Sep 2023","2023","","","1354","1358","The integrated container-truck problem is a classical problem in the logistic industry, which combines the vehicle routing problem and container allocation problem. In order to solve this problem efficiently, this paper proposed a hybrid solution approach combining the merits of ant colony optimization (ACO) and large neighborhood search (LNS). In the solution process, deep reinforcement learning is used to dynamically adopt the most suitable algorithm (ACO and GA) to conduct the trial-and-error process. The pheromone trails generated by ACO are used as the space, and the current solution generated by LNS is used as the action space. Then the objective value is used as the reward signal. The reinforcement learning agent is trained such to learn the best policy. The proposed approach is suitable to solving discrete container-truck scheduling problem and improve the convergence speed of ACO algorithm. Our computational study demonstrates the effectiveness of the proposed algorithm.","","979-8-3503-0245-5","10.1109/ICSP58490.2023.10248613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10248613","container-truck scheduling problem;reinforcement learnin;ant colony optimization;large neighborhood search","Ant colony optimization;Machine learning algorithms;Job shop scheduling;Processor scheduling;Heuristic algorithms;Vehicle routing;Signal processing algorithms","ant colony optimisation;logistics;reinforcement learning;scheduling;search problems;vehicle routing","ACO algorithm;ant colony optimization;container allocation problem;deep reinforcement learning;discrete container-truck scheduling problem;hybrid algorithm;hybrid solution approach;integrated container-truck scheduling problem;large neighborhood search;LNS;logistic industry;reward signal;trial-and-error process;vehicle routing problem","","","","10","IEEE","19 Sep 2023","","","IEEE","IEEE Conferences"
"Research on Vehicle Routing Problem Based on Deep Reinforcement Learning","C. Pan","School of Information Engineering, Wuhan University of Technology, Wuhan, China","2023 8th International Conference on Intelligent Computing and Signal Processing (ICSP)","19 Sep 2023","2023","","","2116","2119","The path planning problem is a core problem in the logistics industry. In this paper, an attention-based deep reinforcement learning method is proposed to investigate the vehicle routing problem with the goal of minimizing energy consumption. The method uses a graph attention network as an encoder, uses dynamic attention to enhance feature extraction, designs a decoder with reference to Transformer, and uses both greedy sampling and random sampling strategies in decoding. In this paper, we use the improved REINFORCE algorithm with rollout baseline as the training algorithm for the model, and the trained model is better than the traditional heuristic algorithm in solving the vehicle path planning problem.","","979-8-3503-0245-5","10.1109/ICSP58490.2023.10248912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10248912","component;path planning;deep reinforcement learning;graph attention networks;energy saving","Deep learning;Training;Heuristic algorithms;Vehicle routing;Signal processing algorithms;Reinforcement learning;Transformers","deep learning (artificial intelligence);energy consumption;feature extraction;graph theory;learning (artificial intelligence);logistics;path planning;reinforcement learning;vehicle routing","attention-based deep reinforcement learning method;core problem;decoder;decoding;dynamic attention;graph attention network;improved REINFORCE algorithm;logistics industry;random sampling strategies;vehicle path planning problem;vehicle routing problem","","","","10","IEEE","19 Sep 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Optimal Stabilization for Unknown Nonlinear Systems Subject to Inputs With Uncertain Constraints","B. Zhao; D. Liu; C. Luo","School of Systems Science, Beijing Normal University, Beijing, China; School of Automation, Guangdong University of Technology, Guangzhou, China; Department of Electrical and Computer Engineering, Mississippi State University, Mississippi State, MS, USA","IEEE Transactions on Neural Networks and Learning Systems","5 Oct 2020","2020","31","10","4330","4340","This article presents a novel reinforcement learning strategy that addresses an optimal stabilizing problem for unknown nonlinear systems subject to uncertain input constraints. The control algorithm is composed of two parts, i.e., online learning optimal control for the nominal system and feedforward neural networks (NNs) compensation for handling uncertain input constraints, which are considered as the saturation nonlinearities. Integrating the input-output data and recurrent NN, a Luenberger observer is established to approximate the unknown system dynamics. For nominal systems without input constraints, the online learning optimal control policy is derived by solving Hamilton-Jacobi-Bellman equation via a critic NN alone. By transforming the uncertain input constraints to saturation nonlinearities, the uncertain input constraints can be compensated by employing a feedforward NN compensator. The convergence of the closed-loop system is guaranteed to be uniformly ultimately bounded by using the Lyapunov stability analysis. Finally, the effectiveness of the developed stabilization scheme is illustrated by simulation studies.","2162-2388","","10.1109/TNNLS.2019.2954983","National Natural Science Foundation of China(grant numbers:61973330,61603387,61773075,61533017,U1501251); Fundamental Research Funds for the Central Universities(grant numbers:2019NTST25); Early Career Development Award of SKLMCCS(grant numbers:20180201); State Key Laboratory of Synthetical Automation for Process Industries(grant numbers:2019-KF-23-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8944275","Adaptive dynamic programming (ADP);neural networks (NNs);optimal control;reinforcement learning (RL);uncertain input constraints;unknown nonlinear systems","Nonlinear systems;Optimal control;Artificial neural networks;Actuators;Observers;Feedforward systems","closed loop systems;compensation;feedforward neural nets;learning (artificial intelligence);Lyapunov methods;nonlinear control systems;observers;optimal control;stability;uncertain systems","nominal system;online learning optimal control policy;uncertain input constraints;saturation nonlinearities;reinforcement learning-based optimal stabilization;unknown nonlinear systems;optimal stabilizing problem;input-output data;unknown system dynamics;feedforward neural networks compensation;critic NN;Hamilton-Jacobi-Bellman equation;feedforward NN compensator;Lyapunov stability analysis;closed-loop system;Luenberger observer","","71","","49","IEEE","27 Dec 2019","","","IEEE","IEEE Journals"
"Optimal Management of the Peak Power Penalty for Smart Grids Using MPC-based Reinforcement Learning","W. Cai; H. N. Esfahani; A. B. Kordabad; S. Gros","Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, Norway","2021 60th IEEE Conference on Decision and Control (CDC)","1 Feb 2022","2021","","","6365","6370","The cost of the power distribution infrastructures is driven by the peak power encountered in the system. Therefore, the distribution network operators consider billing consumers behind a common transformer in the function of their peak demand and leave it to the consumers to manage their collective costs. This management problem is, however, not trivial. In this paper, we consider a multi-agent residential smart grid system, where each agent has local renewable energy production and energy storage, and all agents are connected to a local transformer. The objective is to develop an optimal policy that minimizes the economic cost consisting of both the spot-market cost for each consumer and their collective peak-power cost. We propose to use a parametric Model Predictive Control (MPC)-scheme to approximate the optimal policy. The optimality of this policy is limited by its finite horizon and inaccurate forecasts of the local power production-consumption. A Deterministic Policy Gradient (DPG) method is deployed to adjust the MPC parameters and improve the policy. Our simulations show that the proposed MPC-based Reinforcement Learning (RL) method can effectively decrease the long-term economic cost for this smart grid problem.","2576-2370","978-1-6654-3659-5","10.1109/CDC45484.2021.9683333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9683333","","Renewable energy sources;Costs;Power system management;Power distribution;Reinforcement learning;Production;Transformers","invoicing;optimisation;power consumption;power distribution control;power distribution economics;power engineering computing;power markets;power transformers;predictive control;reinforcement learning;smart power grids","MPC-based reinforcement learning method;parametric model predictive control;DPG method;billing consumers;common transformer;distribution network operators;power distribution infrastructures;optimal management;long-term economic cost;MPC parameters;deterministic policy gradient method;local power production-consumption;spot-market cost;energy storage;local renewable energy production;multiagent residential smart grid system;management problem","","5","","25","IEEE","1 Feb 2022","","","IEEE","IEEE Conferences"
"GoodFloorplan: Graph Convolutional Network and Reinforcement Learning-Based Floorplanning","Q. Xu; H. Geng; S. Chen; B. Yuan; C. Zhuo; Y. Kang; X. Wen","School of Microelectronics, University of Science and Technology of China, Hefei, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; School of Microelectronics, University of Science and Technology of China, Hefei, China; Department of Computer Science and Engineering, Guangdong Provincial Key Laboratory of Brain-inspired Intelligent Computation, Southern University of Science and Technology, Shenzhen, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; School of Microelectronics, University of Science and Technology of China, Hefei, China; Department of Computer Science and Networks, Kyushu Institute of Technology, Fukuoka, Japan","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","19 Sep 2022","2022","41","10","3492","3502","Electronic design automation (EDA) comprises a series of computationally difficult optimization problems that require substantial specialized knowledge as well as a considerable amount of trial-and-error efforts. However, open challenges, including long simulation runtime and lack of generalization, continue to restrict the applications of the existing EDA tools. Recently, learning-based algorithms, especially reinforcement learning (RL), have been successfully applied to handle various combinatorial optimization problems by automatically acquiring knowledge from the past experience. In this article, we formulate the floorplanning problem, the first stage of the physical design flow, as a Markov decision process (MDP). An end-to-end learning-based floorplanning framework GoodFloorplan is proposed to explore the design space, which combines graph convolutional network (GCN) and RL. Experimental results demonstrate that compared with state-of-the-art heuristic-based floorplanners, the proposed GoodFloorplan can provide better area and wirelength.","1937-4151","","10.1109/TCAD.2021.3131550","National Key Research and Development Program of China(grant numbers:2019YFB2204800); National Natural Science Foundation of China (NSFC)(grant numbers:61904047,61732020,61931008,U19A2074,62034007,61974133); Guangdong Provincial Key Laboratory(grant numbers:2020B121201001); CAS Project for Young Scientists in Basic Research(grant numbers:YSBR-029); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDB44000000); Zhejiang Provincial Key Research and Development Program(grant numbers:2020C01052); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9628153","Advantage actor--critic;floorplanning;graph convolutional network (GCN);reinforcement learning (RL)","Reinforcement learning;Optimization;Integrated circuit modeling;Space exploration;Routing;Mathematical models;Tools","circuit layout;convolutional neural nets;electronic design automation;integrated circuit layout;Markov processes;optimisation;reinforcement learning","heuristic-based floorplanners;GCN;GoodFloorplan;end-to-end learning-based floorplanning framework;MDP;reinforcement learning-based floorplanning;long simulation runtime;computationally difficult optimization problems;electronic design automation;graph convolutional network;design space;Markov decision process;physical design flow;floorplanning problem;combinatorial optimization problems;RL;learning-based algorithms;EDA tools","","2","","39","IEEE","30 Nov 2021","","","IEEE","IEEE Journals"
"A Graph-Based Evolutionary Algorithm: Genetic Network Programming (GNP) and Its Extension Using Reinforcement Learning","S. Mabu; K. Hirasawa; J. Hu","Graduate School of Information, Production and Systems, Waseda University, Hibikino 2-7 Wakamatsu-ku, Kitakyushu, Fukuoka, 808-0135, Japan mabu@asagi.waseda.jp; Graduate School of Information, Production and Systems, Waseda University, Hibikino 2-7, Wakamatsu-ku, Kitakyushu, Fukuoka, 808-0135, Japan hirasawa@waseda.jp; Graduate School of Information, Production and Systems, Waseda University, Hibikino 2-7, Wakamatsu-ku, Kitakyushu, Fukuoka, 808-0135, Japan jinglu@waseda.jp","Evolutionary Computation","19 May 2014","2007","15","3","369","398","This paper proposes a graph-based evolutionary algorithm called Genetic Network Programming (GNP). Our goal is to develop GNP, which can deal with dynamic environments efficiently and effectively, based on the distinguished expression ability of the graph (network) structure. The characteristics of GNP are as follows. 1) GNP programs are composed of a number of nodes which execute simple judgment/processing, and these nodes are connected by directed links to each other. 2) The graph structure enables GNP to re-use nodes, thus the structure can be very compact. 3) The node transition of GNP is executed according to its node connections without any terminal nodes, thus the past history of the node transition affects the current node to be used and this characteristic works as an implicit memory function. These structural characteristics are useful for dealing with dynamic environments. Furthermore, we propose an extended algorithm, “GNP with Reinforcement Learning (GNPRL)” which combines evolution and reinforcement learning in order to create effective graph structures and obtain better results in dynamic environments. In this paper, we applied GNP to the problem of determining agents' behavior to evaluate its effectiveness. Tileworld was used as the simulation environment. The results show some advantages for GNP over conventional methods.","1063-6560","","10.1162/evco.2007.15.3.369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6791978","Evolutionary computation;graph structure;reinforcement learning;agent;tileworld","","","","","138","2","","","19 May 2014","","","MIT Press","MIT Press Journals"
"A Novel Graph-Based Estimation of the Distribution Algorithm and its Extension Using Reinforcement Learning","X. Li; S. Mabu; K. Hirasawa","Waseda University, Graduate School of Information, Production and Systems, Fukuoka, Japan; Waseda University, Graduate School of Information, Production and Systems, Fukuoka, Japan; Waseda University, Graduate School of Information, Production and Systems, Fukuoka, Japan","IEEE Transactions on Evolutionary Computation","27 Jan 2014","2014","18","1","98","113","In recent years, numerous studies have drawn the success of estimation of distribution algorithms (EDAs) to avoid the frequent breakage of building blocks of the conventional stochastic genetic operators-based evolutionary algorithms (EAs). In this paper, a novel graph-based EDA called probabilistic model building genetic network programming (PMBGNP) is proposed. Using the distinguished graph (network) structure of a graph-based EA called genetic network programming (GNP), PMBGNP ensures higher expression ability than the conventional EDAs to solve some specific problems. Furthermore, an extended algorithm called reinforced PMBGNP is proposed to combine PMBGNP and reinforcement learning to enhance the performance in terms of fitness values, search speed, and reliability. The proposed algorithms are applied to solve the problems of controlling the agents' behavior. Two problems are selected to demonstrate the effectiveness of the proposed algorithms, including the benchmark one, i.e., the Tileworld system, and a real mobile robot control.","1941-0026","","10.1109/TEVC.2013.2238240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6408015","Agent control;estimation of distribution algorithm (EDA);genetic network programming (GNP);graph structure;reinforcement learning (RL)","Probabilistic logic;Economic indicators;Sociology;Delay effects;Genetics;Boltzmann distribution","genetic algorithms;graph theory;learning (artificial intelligence);multi-agent systems;network theory (graphs);probability","graph-based EDA;estimation-of-the-distribution algorithm;EDA extension;reinforcement learning;conventional stochastic genetic operators;evolutionary algorithms;EA;probabilistic model building genetic network programming;distinguished graph structure;reinforced PMBGNP algorithm;agent behavior;Tileworld system;mobile robot control","","26","","59","IEEE","9 Jan 2013","","","IEEE","IEEE Journals"
"Reinforcement learning methods for multi-linked manipulator obstacle avoidance and control","C. K. Tham; R. W. Prager","Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK","Proceedings 1993 Asia-Pacific Workshop on Advances in Motion Control","6 Aug 2002","1993","","","140","145","This paper treats the multi-linked manipulator obstacle avoidance and control task as the interaction between a learning agent and an unknown environment. The role of the agent is to generate actions that maximises the reward that it receives from the environment. We demonstrate how two learning algorithms common in reinforcement learning literature- adaptive heuristic critic (AHC) (Barto et al., 1983), and Q-learning (Watkins, 1989)-can be used to solve the task successfully in two different ways: 1) through the generation of position commands to a PD controller which produces torque commands to drive the manipulator, and 2) through the direct generation of torque commands, removing the need for a PD controller. During the process, the inverse kinematics problem for multi-linked manipulators is automatically solved. Fast function approximation is achieved through the use of an array of cerebellar model arithmetic computers (CMAC). The generation of both discrete and continuous actions are investigated and the performance of the algorithms in terms of learning rates, efficiency of solutions, and memory requirements are evaluated.<>","","0-7803-1223-6","10.1109/APWAM.1993.316204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=316204","","Programmable control;Adaptive control;PD control;Machine learning;Manipulator dynamics;Robots;Stochastic processes;Torque control;Kinematics;Function approximation","kinematics;function approximation;path planning;neural nets;adaptive systems;learning systems;nonlinear control systems","multilinked manipulator;obstacle avoidance;reinforcement learning;adaptive heuristic critic;Q-learning;position commands;PD controller;torque commands;inverse kinematics;function approximation;cerebellar model arithmetic computers;CMAC","","2","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Optimal Control of Iron-Removal Systems Based on Off-Policy Reinforcement Learning","N. Chen; S. Luo; J. Dai; B. Luo; W. Gui","School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China","IEEE Access","20 Aug 2020","2020","8","","149730","149740","The goethite iron-removal process is an important procedure to remove the iron ions from the zinc hydrometallurgy. However, as a coherent system with complex reaction mechanism, associated uncertainties, and interconnected adjacent reactors, it is difficult for the process to accurately control the ion concentration. Because a large amount of historical data can be obtained during the process, an optimal control algorithm based on off-policy reinforcement learning is proposed in this paper to overcome these difficulties. According to the historical data, the weights of neural network are learned offline, and the optimal control strategy is solved online. Firstly, a bounded function is introduced to define the maximum effect of the coherent system on the subsystem cost function and to extend the cost function of the nominal system, so that the decentralized guaranteed cost control problem can be expressed as the optimal control problem of the nominal system. Then, an approximate iterative control algorithm based on actor-critic structure is proposed. The actor and critic neural networks are used to approximate control strategies and cost functions respectively. To achieve complete off-line, a new neural network is added to the actor-critic structure to approximate a part of the unknown system structure, and the three neural network parameters are optimized by the state transition algorithm. Finally, the strategy update and strategy iteration operations are performed alternately to learn optimal control strategies. The effectiveness and flexibility of the proposed off-policy optimal control method is validated by data from a real industrial goethite iron-removal process.","2169-3536","","10.1109/ACCESS.2020.3015801","Program of National Natural Science Foundation of China(grant numbers:61673399); Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61621062); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9164953","Goethite iron-removal process;optimal control;off-policy;reinforcement learning","Optimal control;Inductors;Iron;Zinc;Process control;Leaching;Learning (artificial intelligence)","cost optimal control;decentralised control;iron;iterative methods;learning (artificial intelligence);metal refining;neurocontrollers;zinc","nominal system;approximate iterative control algorithm;actor-critic structure;critic neural networks;unknown system structure;neural network parameters;strategy iteration operations;industrial goethite iron-removal process;iron-removal systems;off-policy reinforcement learning;iron ions;complex reaction mechanism;interconnected adjacent reactors;ion concentration;bounded function;subsystem cost function;decentralized guaranteed cost control problem;off-policy optimal control;zinc hydrometallurgy;strategy update operations","","4","","35","CCBY","11 Aug 2020","","","IEEE","IEEE Journals"
"Online Series-Parallel Reinforcement-Learning- Based Balancing Control for Reaction Wheel Bicycle Robots on a Curved Pavement","X. Zhu; Y. Deng; X. Zheng; Q. Zheng; Z. Chen; B. Liang; Y. Liu","School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China; Department of Automation, Tsinghua University, Beijing, China; School of Automation, Beijing University of Posts and Telecommunications, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China","IEEE Access","10 Jul 2023","2023","11","","66756","66766","The reaction wheel bicycle robot is a kind of unmanned mobile robot with great potential. However, the control of such bicycle robots on a curved pavement under inaccurate model parameters, model uncertainties and disturbances is challenging due to the lateral instability and underactuated characteristic. Applying conventional control methods to this problem often results in brittle and inaccurate controllers. In this paper, an online serial-parallel combination reinforcement learning with conventional control methods is designed to achieve the path tracking and banlancing control for a reaction wheel bicycle robot on curved pavements. The parallel part of the controller refers to compensating the equilibrium point and the serial part of the controller refers to adjusting the parameters of a sliding mode controller that tracks the target roll equilibrium point. The comparison between the proposed controller and several existing controllers in experimental test built in Matlab Simscape illustrates stronger robustness and better control performances.","2169-3536","","10.1109/ACCESS.2023.3268524","National Natural Science Foundation of China(grant numbers:62203252,52205008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10105202","Reaction wheel bicycle robot;reinforcement learning;robustness;sliding model control","Wheels;Bicycles;Mobile robots;Mathematical models;Adaptation models;Reinforcement learning;Gravity;Sliding mode control","bicycles;control system synthesis;mechanical stability;mobile robots;motion control;nonlinear control systems;reinforcement learning;robust control;variable structure systems;wheels","banlancing control;conventional control methods;curved pavement;model uncertainties;online series-parallel combination reinforcement-learning based balancing control;path tracking;reaction wheel bicycle robot;sliding mode controller;unmanned mobile robot","","","","54","CCBYNCND","19 Apr 2023","","","IEEE","IEEE Journals"
"Comprehensive overview on the deployment of machine learning, deep learning, reinforcement learning algorithms in Selfish mining attack in blockchain","M. J. Jeyasheela Rakkini; K. Geetha","School of Computing, SASTRA Deemed University, Thanjavur, India; School of Computing, SASTRA Deemed University, Thanjavur, India","2022 IEEE 2nd Mysore Sub Section International Conference (MysuruCon)","13 Dec 2022","2022","","","1","5","Blockchain, a disruptive technology, has many applications in the domain of Finance, banking, real estate, insurance, supply chain, gaming industry with much more plethora of applications in near future. In spite of the decentralized, distributed, transparent, tamper-proof, data- provenance nature of the blockchain, it is subject to a lot of security attacks such as forking attacks and block withholding attacks. One such attack under the forking attack is selfish mining, which targets the reward distribution and also the difficulty adjustment algorithms(DAA). An exhaustive surve is done on the existing approaches to detect selfish mining and also on the profitability of selfish mining attacks. This survey is organized particularly around the aspects of selection or exploration of the shortest branch of the blockchain when a fork occurs. We aim to identify the implications of selecting the shorter branch of the fork in the blockchain, especially after 2016 blocks, where a difficulty adjustment occurs. Our survey focuses on the deployment of machine learning and deep learning, reinforcement methods on mitigating the selfish mining attacks in the blockchain.","","978-1-6654-9790-9","10.1109/MysuruCon55714.2022.9972484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9972484","Selfish mining;Reinforcement learning;hash rate;Difficulty adjustment algorithms;forking attack","Deep learning;Machine learning algorithms;Profitability;Supply chains;Clustering algorithms;Reinforcement learning;Prediction algorithms","blockchains;data mining;deep learning (artificial intelligence);reinforcement learning;security of data","block withholding attacks;blockchain;DAA;deep learning algorithms;deep learning methods;difficulty adjustment algorithms;disruptive technology;finance domain;forking attack;gaming industry;machine learning algorithms;machine learning methods;real estate;reinforcement learning algorithms;reinforcement methods;reward distribution;security attacks;selfish mining attacks;shortest branch exploration;shortest branch selection;supply chain","","1","","37","IEEE","13 Dec 2022","","","IEEE","IEEE Conferences"
"Adaptive and Explainable Deployment of Navigation Skills via Hierarchical Deep Reinforcement Learning","K. Lee; S. Kim; J. Choi","Department of Computer Science and Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea; Graduate School of Artificial Intelligence, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Graduate School of Artificial Intelligence, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","1673","1679","For robotic vehicles to navigate robustly and safely in unseen environments, it is crucial to decide the most suitable navigation policy. However, most existing deep reinforcement learning based navigation policies are trained with a hand-engineered curriculum and reward function which are difficult to be deployed in a wide range of real-world scenarios. In this paper, we propose a framework to learn a family of low-level navigation policies and a high-level policy for deploying them. The main idea is that, instead of learning a single navigation policy with a fixed reward function, we simultaneously learn a family of policies that exhibit different behaviors with a wide range of reward functions. We then train the high-level policy which adaptively deploys the most suitable navigation skill. We evaluate our approach in simulation and the real world and demonstrate that our method can learn diverse navigation skills and adaptively deploy them. We also illustrate that our proposed hierarchical learning framework presents explainability by providing semantics for the behavior of an autonomous agent.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160371","Ministry of Trade, Industry & Energy; Institute of Information & communications Technology Planning & Evaluation (IITP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160371","","Deep learning;Adaptation models;Automation;Navigation;Semantics;Reinforcement learning;Autonomous agents","deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;navigation;path planning;reinforcement learning","adaptively deploy;diverse navigation skills;existing deep reinforcement learning based navigation policies;fixed reward function;hand-engineered curriculum;hierarchical deep reinforcement learning;hierarchical learning framework presents explainability;high-level policy;low-level navigation policies;reward functions;robotic vehicles;single navigation policy;suitable navigation policy;suitable navigation skill;unseen environments","","","","33","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning Control of a Flexible Two-Link Manipulator: An Experimental Investigation","W. He; H. Gao; C. Zhou; C. Yang; Z. Li","Institute of Artificial Intelligence, University of Science and Technology Beijing, Beijing, China; Institute of Artificial Intelligence, University of Science and Technology Beijing, Beijing, China; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Bristol Robotics Laboratory, University of the West of England, Bristol, U.K.; Department of Automation, University of Science and Technology of China, Hefei, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","18 Nov 2021","2021","51","12","7326","7336","This article discusses the control design and experiment validation of a flexible two-link manipulator (FTLM) system represented by ordinary differential equations (ODEs). A reinforcement learning (RL) control strategy is developed that is based on actor–critic structure to enable vibration suppression while retaining trajectory tracking. Subsequently, the closed-loop system with the proposed RL control algorithm is proved to be semi-global uniform ultimate bounded (SGUUB) by Lyapunov’s direct method. In the simulations, the control approach presented has been tested on the discretized ODE dynamic model and the analytical claims have been justified under the existence of uncertainty. Eventually, a series of experiments in a Quanser laboratory platform are investigated to demonstrate the effectiveness of the presented control and its application effect is compared with PD control.","2168-2232","","10.1109/TSMC.2020.2975232","National Natural Science Foundation of China(grant numbers:61933001,61873298); National Key Research and Development Program of China(grant numbers:2019YFB1703600); Joint Funds of Equipment Preresearch and Ministry of Education of China(grant numbers:6141A02033339); Beijing Top Discipline for Artificial Intelligent Science and Engineering, University Science and Technology Beijing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025764","Flexible structure;neural networks (NNs);reinforcement learning (RL);robots;vibration control","Mathematical model;Artificial neural networks;Vibrations;Manipulators;Heuristic algorithms;Robot kinematics","adaptive control;aircraft control;closed loop systems;control system synthesis;differential equations;learning (artificial intelligence);Lyapunov methods;neurocontrollers;nonlinear control systems;PD control;tracking;uncertain systems;vibration control","ODEs;reinforcement learning control strategy;actor-critic structure;vibration suppression;trajectory tracking;closed-loop system;RL control algorithm;semiglobal uniform;Lyapunov's direct method;control approach;discretized ODE dynamic model;presented control;PD control;control design;experiment validation;two-link manipulator system;ordinary differential equations","","109","","62","IEEE","5 Mar 2020","","","IEEE","IEEE Journals"
"Adaptive Impedance Control of Human–Robot Cooperation Using Reinforcement Learning","Z. Li; J. Liu; Z. Huang; Y. Peng; H. Pu; L. Ding","State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; Research Institute of Intelligent Control and Systems, School of Astronautics, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Industrial Electronics","11 Sep 2017","2017","64","10","8013","8022","This paper presents human-robot cooperation with adaptive behavior of the robot, which helps the human operator to perform the cooperative task and optimizes its performance. A novel adaptive impedance control is proposed for the robotic manipulator, whose end-effector's motions are constrained by human arm motion limits. In order to minimized motion tracking errors and acquire an optimal impedance mode of human arms, the linear quadratic regulation (LQR) is formulated; then, integral reinforcement learning (IRL) has been proposed to solve the given LQR with little information of the human arm model. Considering human-robot interaction force during the robot performing manipulation, a novel barrier-Lyapunov-function-based adaptive impedance control incorporating adaptive parameter learning is developed for physical limits, transient perturbations, and time-varying dynamics. Experimental results validate that the proposed controller is effective in assisting the operator to perform the human-robot cooperative task.","1557-9948","","10.1109/TIE.2017.2694391","National Natural Science Foundation of China(grant numbers:61573147,91520201,61625303); Guangzhou Research Collaborative Innovation Projects(grant numbers:2014Y2-00507); Guangdong Science and Technology Research Collaborative Innovation Projects(grant numbers:2013B010102010,2014B090901056,2015B020214003); Guangdong Science and Technology Plan Project; Application Technology Research Foundation(grant numbers:2015B020233006); National High-Tech Research and Development Program of China (863 Program)(grant numbers:2015AA042303); State Key Laboratory of Robotics and System, Harbin Institute of Technology(grant numbers:SKLRS-2016-KF-04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900394","Adaptive impedance control;barrier Lyapunov function (BLF);human–robot cooperation (HRC);integral reinforcement learning (IRL);linear quadratic regulation (LQR)","Impedance;Robot kinematics;Manipulators;Robot sensing systems;Lyapunov methods;Learning (artificial intelligence)","adaptive control;control engineering computing;end effectors;human-robot interaction;learning (artificial intelligence);linear quadratic control;Lyapunov methods;minimisation;time-varying systems","time-varying dynamics;transient perturbations;physical limits;barrier-Lyapunov-function-based adaptive impedance control;robot performing manipulation;human-robot interaction force;human arm model;LQR;integral reinforcement learning;linear quadratic regulation;optimal impedance mode;motion tracking error minimization;human arm motion limits;end-effector motion;robotic manipulator;adaptive impedance control;performance optimization;cooperative task;human operator;adaptive behavior;human-robot cooperation","","101","","25","IEEE","14 Apr 2017","","","IEEE","IEEE Journals"
"Data-Driven Robust Control of Discrete-Time Uncertain Linear Systems via Off-Policy Reinforcement Learning","Y. Yang; Z. Guo; H. Xiong; D. -W. Ding; Y. Yin; D. C. Wunsch","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Department of Electrical and Computer Engineering, University of Central Florida, Orlando, USA; Big Data Laboratory, Baidu Research, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Department of Electrical and Computer Engineering, Missouri University of Science and Technology, Rolla, USA","IEEE Transactions on Neural Networks and Learning Systems","10 Dec 2019","2019","30","12","3735","3747","This paper presents a model-free solution to the robust stabilization problem of discrete-time linear dynamical systems with bounded and mismatched uncertainty. An optimal controller design method is derived to solve the robust control problem, which results in solving an algebraic Riccati equation (ARE). It is shown that the optimal controller obtained by solving the ARE can robustly stabilize the uncertain system. To develop a model-free solution to the translated ARE, off-policy reinforcement learning (RL) is employed to solve the problem in hand without the requirement of system dynamics. In addition, the comparisons between on- and off-policy RL methods are presented regarding the robustness to probing noise and the dependence on system dynamics. Finally, a simulation example is carried out to validate the efficacy of the presented off-policy RL approach.","2162-2388","","10.1109/TNNLS.2019.2897814","National Natural Science Foundation of China(grant numbers:61333002,61873028,61473032); Fundamental Research Funds for the Central Universities(grant numbers:FRF-TP-18-031A1,FRF-BD-17-002A); China Postdoctoral Science Foundation(grant numbers:2018M641197); National Science Foundation(grant numbers:CNS-1850851); Mary K. Finley Endowment; Intelligent Systems Center; Army Research Laboratory(grant numbers:W911NF-18-2-0260); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8657370","Model-free;off-policy;on-policy;reinforcement learning (RL);robust control;system uncertainty","Robust control;Optimal control;Uncertainty;Uncertain systems;System dynamics;Linear systems;Reinforcement learning","control system synthesis;discrete time systems;learning (artificial intelligence);linear systems;optimal control;Riccati equations;robust control;uncertain systems","on-and-off-policy RL methods;data driven robust control;discrete time linear dynamical systems;discrete time uncertain linear systems;system dynamics;uncertain system;algebraic Riccati equation;robust control problem;optimal controller design method;mismatched uncertainty;bounded uncertainty;robust stabilization problem;model-free solution;off-policy reinforcement learning","","52","","39","IEEE","1 Mar 2019","","","IEEE","IEEE Journals"
"QoE-Driven Edge Caching in Vehicle Networks Based on Deep Reinforcement Learning","C. Song; W. Xu; T. Wu; S. Yu; P. Zeng; N. Zhang","Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Department of Electrical, and Computer Engineering University of Windsor, Windsor, ON, Canada","IEEE Transactions on Vehicular Technology","8 Jul 2021","2021","70","6","5286","5295","The Internet of vehicles (IoV) is a large information interaction network that collects information on vehicles, roads and pedestrians. One of the important uses of vehicle networks is to meet the entertainment needs of driving users through communication between vehicles and roadside units (RSUs). Due to the limited storage space of RSUs, determining the content cached in each RSU is a key challenge. With the development of 5G and video editing technology, short video systems have become increasingly popular. Current widely used cache update methods, such as partial file precaching and content popularity- and user interest-based determination, are inefficient for such systems. To solve this problem, this paper proposes a QoE-driven edge caching method for the IoV based on deep reinforcement learning. First, a class-based user interest model is established. Compared with the traditional file popularity- and user interest distribution-based cache update methods, the proposed method is more suitable for systems with a large number of small files. Second, a quality of experience (QoE)-driven RSU cache model is established based on the proposed class-based user interest model. Third, a deep reinforcement learning method is designed to address the QoE-driven RSU cache update issue effectively. The experimental results verify the effectiveness of the proposed algorithm.","1939-9359","","10.1109/TVT.2021.3077072","National Key R&D Program of China(grant numbers:2018YFB1700200); National Natural Science Foundation of China(grant numbers:U1908212,61773368); Industrial Internet Innovation Development; “Edge computing test bed,”; Shenzhen Science and Technology Innovation Commission(grant numbers:JCYJ20190809145407809); Revitalizing Liaoning Outstanding Talents(grant numbers:XLYC1907057); State Grid Corporation Science and Technology(grant numbers:SG2NK00DWJS1800123); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9422164","Internet of vehicles;roadside units;cache update;quality of experience;deep reinforcement learning","Computational modeling;Trajectory;Roads;Reinforcement learning;Optimization;Quality of experience;Privacy","cache storage;Internet;learning (artificial intelligence);peer-to-peer computing;quality of experience;vehicular ad hoc networks","vehicle networks;IoV;information interaction network;roads;pedestrians;important uses;RSU;video editing technology;short video systems;current widely used cache update methods;partial file precaching;user interest-based determination;QoE-driven edge caching method;class-based user interest model;traditional file popularity;user interest distribution-based cache update methods;experience-driven RSU cache model;deep reinforcement learning method;QoE-driven RSU cache update issue","","37","","32","IEEE","3 May 2021","","","IEEE","IEEE Journals"
"A Vibration Control Method for Hybrid-Structured Flexible Manipulator Based on Sliding Mode Control and Reinforcement Learning","T. Long; E. Li; Y. Hu; L. Yang; J. Fan; Z. Liang; R. Guo","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; CRRC Zhuzhou Institute Company, Ltd., Zhuzhou, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Grid Shandong Electric Power Company, Jinan, China","IEEE Transactions on Neural Networks and Learning Systems","4 Feb 2021","2021","32","2","841","852","The hybrid-structured flexible manipulator has a complex structure and strong coupling between state variables. Meanwhile, the natural frequency of the hybrid-structured flexible manipulator varies with the motion of the telescopic joint, so it is difficult to suppress the vibration quickly. In this article, the tip state signal of the hybrid-structured flexible manipulator is decomposed into elastic vibration signal and tip vibration equilibrium position signal, and a combined control method is proposed to improve tip positioning accuracy and trajectory tracking accuracy. In the proposed combined control method, an improved nominal model-based sliding mode controller (NMBSMC) is used as the main controller to output the driving torque, and an actor-critic-based reinforcement learning controller (ACBRLC) is used as an auxiliary controller to output small compensation torque. The improved NMBSMC can be divided into a nominal model-based sliding mode robust controller and a practical model-based integral sliding mode controller. Two sliding mode controllers with different structures make full use of the mathematical model and the measured data of the actual system to improve the vibration equilibrium position tracking accuracy. The ACBRLC uses the tip elastic vibration signal and the prioritized experience replay method to obtain the small reverse compensation torque, which is superimposed with the output of the NMBSMC to suppress tip vibration and improve the positioning accuracy of the hybrid-structured flexible manipulator. Finally, several groups of experiments are designed to verify the effectiveness and robustness of the proposed combined control method.","2162-2388","","10.1109/TNNLS.2020.2979600","National Key Research and Development Program of China(grant numbers:2018YFB1307400); National Natural Science Foundation of China(grant numbers:61873267); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9062328","Hybrid-structured flexible manipulator;reinforcement learning;sliding mode control;vibration control method","Vibrations;Mathematical model;Manipulator dynamics;Torque;Robustness;Neural networks","adaptive control;control system synthesis;flexible manipulators;learning (artificial intelligence);learning systems;manipulator dynamics;neurocontrollers;nonlinear control systems;position control;robust control;tracking;variable structure systems;vibration control","vibration control method;sliding mode control;hybrid-structured flexible manipulator;tip state signal;combined control method;improved nominal model-based;auxiliary controller;mode robust controller;practical model-based integral sliding mode controller;sliding mode controllers;vibration equilibrium position;tip elastic vibration signal;tip vibration equilibrium position signal;tip positioning accuracy;trajectory tracking accuracy;nominal model-based sliding mode controller;NMBSMC;actor-critic-based reinforcement learning controller;ACBRLC","","28","","31","IEEE","9 Apr 2020","","","IEEE","IEEE Journals"
"Voltage Regulation of DC-DC Buck Converters Feeding CPLs via Deep Reinforcement Learning","C. Cui; N. Yan; B. Huangfu; T. Yang; C. Zhang","Intelligent Autonomous Systems Laboratory, College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; Intelligent Autonomous Systems Laboratory, College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; Intelligent Autonomous Systems Laboratory, College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; Intelligent Autonomous Systems Laboratory, College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; Intelligent Autonomous Systems Laboratory, College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China","IEEE Transactions on Circuits and Systems II: Express Briefs","16 Mar 2022","2022","69","3","1777","1781","Modeling accuracy of DC-DC converters may deviate largely in the presence of different variation levels of constant power loads (CPLs), hence is well acknowledged as a main hurdle for the design of advanced model-driven control strategies in the literature. Aiming to enhance the bus voltage regulation performance of DC-DC buck converters, a model-free deep reinforcement learning (DRL) control strategy is proposed in this brief. Firstly, a Markov Decision Process (MDP) model and a deep Q network (DQN) algorithm are utilized for the stabilization issue of the converter. Secondly, through a subgoal reward/penalty mechanism, the control objective and prescribed performance of the system are therefore guaranteed. Moreover, a specified action space is designed to match the switch speed of the switching element. As a distinguishable feature, the settling time under the proposed control scheme is significantly reduced in the occurrence of disturbance, resulting from the fast adaption ability of DRL. The simulation comparison results in reference to PI and fuzzy PI controllers demonstrate the efficacy and superiorities under large signal perturbation conditions.","1558-3791","","10.1109/TCSII.2021.3107535","Program for Professor of Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher Learning; National Natural Science Foundation of China(grant numbers:51607111,62173221); Shanghai Rising-Star Program(grant numbers:20QA1404000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521987","DC-DC converter;constant power load;deep reinforcement learning;large signal stability","Aerospace electronics;Voltage control;Switches;Buck converters;Mathematical model;Reinforcement learning;Approximation algorithms","control engineering computing;control system synthesis;DC-DC power convertors;deep learning (artificial intelligence);Markov processes;power engineering computing;reinforcement learning;voltage control;voltage regulators","DC-DC buck converters;variation levels;constant power loads;advanced model-driven control strategies;bus voltage regulation performance;model-free deep reinforcement learning control strategy;Markov Decision Process model;deep Q network algorithm;control objective;control scheme;converter stabilization;switching element","","23","","20","IEEE","25 Aug 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Fixed-Time Trajectory Tracking Control for Uncertain Robotic Manipulators With Input Saturation","S. Cao; L. Sun; J. Jiang; Z. Zuo","Key Laboratory of Knowledge Automation for Industrial Processes of Ministry of Education, School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Key Laboratory of Knowledge Automation for Industrial Processes of Ministry of Education, School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Department of Aeronautical and Automotive Engineering, Loughborough University, Leicestershire, U.K.; Seventh Research Division, School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","4 Aug 2023","2023","34","8","4584","4595","A fixed-time trajectory tracking control method for uncertain robotic manipulators with input saturation based on reinforcement learning (RL) is studied. The designed RL control algorithm is implemented by a radial basis function (RBF) neural network (NN), in which the actor NN is used to generate the control strategy and the critic NN is used to evaluate the execution cost. A new nonsingular fast terminal sliding mode technique is used to ensure the convergence of tracking error in fixed time, and the upper bound of convergence time is estimated. To solve the saturation problem of an actuator, a nonlinear antiwindup compensator is designed to compensate for the saturation effect of the joint torque actuator in real time. Finally, the stability of the closed-loop system based on the Lyapunov candidate is analyzed, and the timing convergence of the closed-loop system is proven. Simulation and experimental results show the effectiveness and superiority of the proposed control law.","2162-2388","","10.1109/TNNLS.2021.3116713","National Natural Science Foundation of China(grant numbers:61903025,62073019); China Scholarship Council(grant numbers:201906465028); Fundamental Research Funds for the Central Universities(grant numbers:FRF-BD-19-002A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9576098","Fixed-time control;input saturation;nonsingular terminal sliding mode (NTSM) control;reinforcement learning (RL);trajectory tracking control;uncertain robotic manipulators","Convergence;Uncertainty;Trajectory tracking;Trajectory;Control systems;Artificial neural networks;Actuators","adaptive control;closed loop systems;control nonlinearities;control system synthesis;Lyapunov methods;manipulators;nonlinear control systems;radial basis function networks;stability;uncertain systems;variable structure systems","actor NN;closed-loop system;control law;control strategy;convergence time;critic NN;designed RL control algorithm;fixed-time trajectory tracking control method;input saturation;nonsingular fast terminal sliding mode technique;radial basis function neural network;reinforcement learning-based fixed-time trajectory tracking control;saturation effect;saturation problem;timing convergence;uncertain robotic manipulators","","21","","41","IEEE","15 Oct 2021","","","IEEE","IEEE Journals"
"PFRL: Pose-Free Reinforcement Learning for 6D Pose Estimation","J. Shao; Y. Jiang; G. Wang; Z. Li; X. Ji","Department of Automation and BNRist, Tsinghua University, Beijing, China; Department of Automation and BNRist, Tsinghua University, Beijing, China; Department of Automation and BNRist, Tsinghua University, Beijing, China; Department of Automation and BNRist, Tsinghua University, Beijing, China; Department of Automation and BNRist, Tsinghua University, Beijing, China","2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","5 Aug 2020","2020","","","11451","11460","6D pose estimation from a single RGB image is a challenging and vital task in computer vision. The current mainstream deep model methods resort to 2D images annotated with real-world ground-truth 6D object poses, whose collection is fairly cumbersome and expensive, even unavailable in many cases. In this work, to get rid of the burden of 6D annotations, we formulate the 6D pose refinement as a Markov Decision Process and impose on the reinforcement learning approach with only 2D image annotations as weakly-supervised 6D pose information, via a delicate reward definition and a composite reinforced optimization method for efficient and effective policy training. Experiments on LINEMOD and T-LESS datasets demonstrate that our Pose-Free approach is able to achieve state-of-the-art performance compared with the methods without using real-world ground-truth 6D pose labels.","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.01147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9156902","","Two dimensional displays;Pose estimation;Three-dimensional displays;Solid modeling;Training;Learning (artificial intelligence);Task analysis","computer vision;image colour analysis;learning (artificial intelligence);Markov processes;object detection;pose estimation","current mainstream deep model methods;real-world ground-truth 6D object poses;6D pose refinement;reinforcement learning approach;2D image annotations;composite reinforced optimization method;Pose-Free approach;real-world ground-truth 6D pose labels;Pose-Free reinforcement;6D pose estimation;single RGB image;Markov Decision Process;weakly-supervised 6D pose information;computer vision;delicate reward definition","","19","","43","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"Distributed Reinforcement Learning Containment Control for Multiple Nonholonomic Mobile Robots","W. Xiao; Q. Zhou; Y. Liu; H. Li; R. Lu","School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation and Electronics Engineering, Qingdao University of Science and Technology, Qingdao, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Circuits and Systems I: Regular Papers","27 Jan 2022","2022","69","2","896","907","In this paper, the distributed optimal containment control problem for multiple nonholonomic mobile robots (NHMRs) differential game is studied via reinforcement learning. An approximation-based optimal control strategy is developed to ensure the optimal performance index and avoid the potential collision among agents. Firstly, the collision avoidance problem considered in this paper is addressed by exploiting a consensus-like interconnection on a directed graph and an error transformation function. Then, on the basis of the optimal backstepping technique, a single critic neural network is adopted to obtain the solution of the coupled Hamilton-Jacobi (HJ) equation, in which an improved learning mechanism is constructed to relax the requirement on initial control conditions. In addition, based on the Lyapunov stability theory, it is proved that all signals in the closed-loop optimal control are uniformly ultimately bounded. Finally, the proposed control protocol is applied to NHMRs system, which verifies that the solution of the coupled HJ equation solves the containment problem of differential game.","1558-0806","","10.1109/TCSI.2021.3121809","National Natural Science Foundation of China(grant numbers:62121004,62033003,62003097,61973091); Local Innovative and Research Teams Project of Guangdong Special Support Program(grant numbers:2019BT02X353); Special Project for Research and Development in Key areas of Guangdong Province(grant numbers:2021B0101410005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9594719","Containment control;collision avoidance;nonholonomic mobile robots;optimal backstepping technique;reinforcement learning","Collision avoidance;Mobile robots;Wheels;Mathematical models;Backstepping;Differential games;Protocols","adaptive control;approximation theory;closed loop systems;collision avoidance;control nonlinearities;control system synthesis;differential games;directed graphs;dynamic programming;learning (artificial intelligence);Lyapunov methods;mobile robots;multi-agent systems;multi-robot systems;neurocontrollers;nonlinear control systems;optimal control;optimisation;performance index;stability","coupled Hamilton-Jacobi equation;improved learning mechanism;initial control conditions;closed-loop optimal control;control protocol;coupled HJ equation;containment problem;distributed reinforcement learning containment control;distributed optimal containment control problem;multiple nonholonomic mobile robots differential game;approximation-based optimal control strategy;optimal performance index;potential collision;collision avoidance problem;error transformation function;optimal backstepping technique;single critic neural network","","16","","52","IEEE","29 Oct 2021","","","IEEE","IEEE Journals"
"Target Tracking Control of a Biomimetic Underwater Vehicle Through Deep Reinforcement Learning","Y. Wang; C. Tang; S. Wang; L. Cheng; R. Wang; M. Tan; Z. Hou","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; NUCTECH Company Ltd., Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","3 Aug 2022","2022","33","8","3741","3752","In this article, the underwater target tracking control problem of a biomimetic underwater vehicle (BUV) is addressed. Since it is difficult to build an effective mathematic model of a BUV due to the uncertainty of hydrodynamics, target tracking control is converted into the Markov decision process and is further achieved via deep reinforcement learning. The system state and reward function of underwater target tracking control are described. Based on the actor–critic reinforcement learning framework, the deep deterministic policy gradient actor–critic algorithm with supervision controller is proposed. The training tricks, including prioritized experience replay, actor network indirect supervision training, target network updating with different periods, and expansion of exploration space by applying random noise, are presented. Indirect supervision training is designed to address the issues of low stability and slow convergence of reinforcement learning in the continuous state and action space. Comparative simulations are performed to show the effectiveness of the training tricks. Finally, the proposed actor–critic reinforcement learning algorithm with supervision controller is applied to the physical BUV. Swimming pool experiments of underwater object tracking of the BUV are conducted in multiple scenarios to verify the effectiveness and robustness of the proposed method.","2162-2388","","10.1109/TNNLS.2021.3054402","Youth Innovation Promotion Association CAS(grant numbers:2018162); National Natural Science Foundation of China(grant numbers:U1713222,62025307,62073316,U1806204,62033013); Beijing Municipal Natural Science Foundation(grant numbers:JQ19020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9351698","Biomimetic underwater vehicle (BUV);reinforcement learning;target tracking control","Reinforcement learning;Target tracking;Robots;Sports;Aerospace electronics;Mobile robots;Underwater vehicles","biomimetics;gradient methods;learning (artificial intelligence);Markov processes;object tracking;random noise;target tracking;underwater vehicles","actor network indirect supervision training;actor-critic reinforcement learning framework;biomimetic underwater vehicle;BUV;deep deterministic policy gradient actor-critic algorithm;deep reinforcement learning;effective mathematic model;reward function;supervision controller;system state;training tricks;underwater object tracking;underwater target tracking control problem","Algorithms;Biomimetics;Markov Chains;Neural Networks, Computer;Reinforcement, Psychology","14","","41","IEEE","9 Feb 2021","","","IEEE","IEEE Journals"
"A Multi-Agent Reinforcement Learning Method With Route Recorders for Vehicle Routing in Supply Chain Management","L. Ren; X. Fan; J. Cui; Z. Shen; Y. Lv; G. Xiong","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Research Institute for Frontier Science, Beihang University, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Beijing Engineering Research Center of Intelligent Systems and Technology, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Intelligent Transportation Systems","14 Sep 2022","2022","23","9","16410","16420","In the modern supply chain system, large-scale transportation tasks require the collaborative work of multiple vehicles to be completed on time. Over the past few decades, multi-vehicle route planning was mainly implemented by heuristic algorithms. However, these algorithms face the dilemma of long computation time. In recent years, some machine learning-based methods are also proposed for vehicle route planning, but the existing algorithms can hardly solve multi-vehicle time-sensitive problems. To overcome this problem, we propose a novel multi-agent reinforcement learning model, which optimizes the route length and the vehicle’s arrival time simultaneously. The model is based on the encoder-decoder framework. The encoder mines the relationship between the customer nodes in the problem, and the decoder generates the route of each vehicle iteratively. Specially, we design multiple route recorders to extract the route history information of vehicles and realize the communication between them. In the inferring phase, the model could immediately generate routes for all vehicles in a new instance. To further improve the performance of the model, we devise a multi-sampling strategy and obtain the balance boundary between computation time and performance improvement. In addition, we propose a simulation-based vehicle configuration method to select the optimal number of vehicles in real applications. For validation, we conduct a series of experiments on problems with different customer amounts and various vehicle numbers. The results show that the proposed model outperforms other typical algorithms in both performance and calculation time.","1558-0016","","10.1109/TITS.2022.3150151","National Key Research and Development Program of China(grant numbers:2019YFB1705502); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714823","Vehicle routing;supply chain management;multi-agent reinforcement learning (MARL);route recorder","Reinforcement learning;Costs;Task analysis;Transportation;Optimization;Computational modeling;Vehicle routing","learning (artificial intelligence);multi-agent systems;supply chain management;transportation;vehicles","heuristic algorithms;long computation time;machine learning-based methods;vehicle route planning;multivehicle time-sensitive problems;novel multiagent reinforcement learning model;route length;arrival time;encoder-decoder framework;design multiple route recorders;route history information;multisampling strategy;simulation-based vehicle configuration method;vehicle numbers;typical algorithms;calculation time;multiagent reinforcement learning method;vehicle routing;supply chain management;modern supply chain system;large-scale transportation tasks;collaborative work;multiple vehicles;multivehicle route planning","","13","","34","IEEE","16 Feb 2022","","","IEEE","IEEE Journals"
"Multi-Reward Architecture based Reinforcement Learning for Highway Driving Policies","W. Yuan; M. Yang; Y. He; C. Wang; B. Wang","Department of Automation, Ministry of Education of China, Shanghai, CN; Department of Automation, Ministry of Education of China, Shanghai, CN; Department of Automation, Ministry of Education of China, Shanghai, CN; Department of Automation, Ministry of Education of China, Shanghai, CN; Department of Automation, Ministry of Education of China, Shanghai, CN","2019 IEEE Intelligent Transportation Systems Conference (ITSC)","28 Nov 2019","2019","","","3810","3815","A safe and efficient driving policy is essential for the future autonomous highway driving. However, driving policies are hard for modeling because of the diversity of scenes and uncertainties of the interaction with surrounding vehicles. The state-of-the-art deep reinforcement learning method is unable to learn good domain knowledge for highway driving policies using single reward architecture. This paper proposes a Multi-Reward Architecture (MRA) based reinforcement learning for highway driving policies. A single reward function is decomposed to multi-reward functions for better representation of multi-dimensional driving policies. Besides the big penalty for collision, the overall reward is decomposed to three dimensional rewards: the reward for speed, the reward for overtake, and the reward for lane-change. Then, each reward trains a branch of Q-network for corresponding domain knowledge. The Q-network is divided into two parts: low-level network is shared by three branches of high-level networks, which approximate the corresponding Q-value for the different reward functions respectively. The agent car chooses the action based on the sum of Q vectors from three branches. Experiments are conducted in a simulation platform, which performs the highway driving process and the agent car is able to provide the commonly used sensor data: the image and the point cloud. Experiment results show that the proposed method performs better than the DQN method on single reward architecture with three evaluations: higher speed, lower frequency of lane-change, more quantity of overtaking, which is more efficient and safer for the future autonomous highway driving.","","978-1-5386-7024-8","10.1109/ITSC.2019.8917304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917304","","Automobiles;Road transportation;Learning (artificial intelligence);Task analysis;Machine learning;Autonomous vehicles;Three-dimensional displays","driver information systems;learning (artificial intelligence);road vehicles","highway driving policies;safe driving policy;autonomous highway driving;deep reinforcement learning method;single reward architecture;MultiReward Architecture based reinforcement;single reward function;multireward functions;multidimensional driving policies;reward functions;highway driving process;three dimensional rewards","","10","","18","IEEE","28 Nov 2019","","","IEEE","IEEE Conferences"
"QoS and Jamming-Aware Wireless Networking Using Deep Reinforcement Learning","N. Abuzainab; T. Erpek; K. Davaslioglu; Y. E. Sagduyu; Y. Shi; S. J. Mackey; M. Patel; F. Panettieri; M. A. Qureshi; V. Isler; A. Yener","Intelligent Automation Inc., Rockville, MD, USA; Intelligent Automation Inc., Rockville, MD, USA; Intelligent Automation Inc., Rockville, MD, USA; Intelligent Automation Inc., Rockville, MD, USA; Intelligent Automation Inc., Rockville, MD, USA; U.S. Army C5ISR, APG, MD, USA; U.S. Army C5ISR, APG, MD, USA; U.S. Army C5ISR, APG, MD, USA; U.S. Army C5ISR, APG, MD, USA; University of Minnesota, Minneapolis, MN, USA; The Pennsylvania State University, University Park, PA, USA","MILCOM 2019 - 2019 IEEE Military Communications Conference (MILCOM)","5 Mar 2020","2019","","","610","615","The problem of quality of service (QoS) and jamming-aware communications is considered in an adversarial wireless network subject to external eavesdropping and jamming attacks. To ensure robust communication against jamming, an interference-aware routing protocol is developed that allows nodes to avoid communication holes created by jamming attacks. Then, a distributed cooperation framework, based on deep reinforcement learning, is proposed that allows nodes to assess network conditions and make deep learning-driven, distributed, and real-time decisions on whether to participate in data communications, defend the network against jamming and eavesdropping attacks, or jam other transmissions. The objective is to maximize the network performance that incorporates throughput, energy efficiency, delay, and security metrics. Simulation results show that the proposed jamming-aware routing approach is robust against jamming and when throughput is prioritized, the proposed deep reinforcement learning approach can achieve significant (measured as three-fold) increase in throughput, compared to a benchmark policy with fixed roles assigned to nodes.","2155-7586","978-1-7281-4280-7","10.1109/MILCOM47813.2019.9020985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020985","","Jamming;Routing;Machine learning;Routing protocols;Quality of service;Eavesdropping;Throughput","data communication;jamming;learning (artificial intelligence);quality of service;radio networks;radiofrequency interference;routing protocols;telecommunication computing","distributed cooperation framework;network conditions;data communications;eavesdropping attacks;network performance;jamming-aware routing approach;deep reinforcement learning approach;QoS;jamming-aware communications;adversarial wireless network subject;external eavesdropping;jamming attacks;robust communication;interference-aware routing protocol;communication holes;quality of ervice;jamming-aware wireless networking;real-time decisions;energy efficiency;security metrics;benchmark policy","","8","","16","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Integrating Reinforcement Learning and Optimal Power Dispatch to Enhance Power Grid Resilience","Q. Li; X. Zhang; J. Guo; X. Shan; Z. Wang; Z. Li; C. K. Tse","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; Power System Department, China Electric Power Research Institute, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; Department of Electrical Engineering, City University of Hong Kong, Hong Kong","IEEE Transactions on Circuits and Systems II: Express Briefs","16 Mar 2022","2022","69","3","1402","1406","Power grids are vulnerable to extreme events that may cause the failure of multiple components and lead to severe power outages. It is of practical importance to design effective restoration strategies to enhance the power grid resilience. In this brief, we consider different time scales of various restoration methods and propose an integrated strategy to maximize the total amount of electricity supplied to the loads in the recovery process. The strategy properly combines the slow restoration method of component repair and the fast restoration method of optimal power dispatch. The Q-learning algorithm is used to generate the sequential order of repairing damaged components and update the network topology. Linear optimization is used to obtain the largest amount of power supply on given network topology. Simulation results show that our proposed method can coordinate the available resources and manpower to effectively restore the power grid after extreme events.","1558-3791","","10.1109/TCSII.2021.3131316","National Science Foundation of China(grant numbers:61803351); Beijing Institute of Technology Research Fund Program for Young Scholars; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9628160","Power grid resilience;Q-Learning;sequential component repair;optimal power dispatch","Resilience;Power grids;Maintenance engineering;Power supplies;Reinforcement learning;Power system reliability;Power system protection","linear programming;load dispatching;network topology;power engineering computing;power grids;power supply quality;power system reliability;power system restoration;reinforcement learning","optimal power dispatch;power grid resilience;power outages;restoration methods;Q-learning algorithm;linear optimization;power supply;reinforcement learning;network topology","","8","","20","IEEE","30 Nov 2021","","","IEEE","IEEE Journals"
"An Optimization Control of Thermal Power Combustion Based on Reinforcement Learning","L. Zou; Y. Cheng; Z. Zhuang; Z. Sun; W. Zhang","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2018 37th Chinese Control Conference (CCC)","7 Oct 2018","2018","","","3553","3558","A simulator is constructed by a neural network with single input and double outputs, which to predict the expected technical indicators by the historical data. With such proposed prediction mode, a key step for its application is the transformation of the optimization problem into a Markov decision process with generalized information. The optimization framework underlying the deep deterministic policy gradient shows a great ability operating over continuous action spaces of high dimensions. Compared with the existing results, the proposed approach has the powerful generalization capacity in unexplored states. Finally, Numerical simulations are given to demonstrate the effectiveness of the proposed method.","1934-1768","978-988-15639-5-8","10.23919/ChiCC.2018.8482853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8482853","thermal power;combustion optimization;neural network;Markov process;deep deterministic policy gradient","Optimization;Combustion;Boilers;Power generation;Markov processes;Training","combustion;gradient methods;learning (artificial intelligence);Markov processes;neural nets;optimisation;power engineering computing;thermal power stations","deep deterministic policy gradient;optimization control;thermal power combustion;reinforcement learning;neural network;optimization problem;Markov decision process;generalized information","","5","","14","","7 Oct 2018","","","IEEE","IEEE Conferences"
"Double Sparse Deep Reinforcement Learning via Multilayer Sparse Coding and Nonconvex Regularized Pruning","H. Zhao; J. Wu; Z. Li; W. Chen; Z. Zheng","School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation and the Guangdong Key Laboratory of IoT Information Technology, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Cybernetics","13 Jan 2023","2023","53","2","765","778","Deep reinforcement learning (DRL), which highly depends on the data representation, has shown its potential in many practical decision-making problems. However, the process of acquiring representations in DRL is easily affected by interference from models, and moreover leaves unnecessary parameters, leading to control performance reduction. In this article, we propose a double sparse DRL via multilayer sparse coding and nonconvex regularized pruning. To alleviate interference in DRL, we propose a multilayer sparse-coding-structural network to obtain deep sparse representation for control in reinforcement learning. Furthermore, we employ a nonconvex log regularizer to promote strong sparsity, efficiently removing the unnecessary weights with a regularizer-based pruning scheme. Hence, a double sparse DRL algorithm is developed, which can not only learn deep sparse representation to reduce the interference but also remove redundant weights while keeping the robust performance. The experimental results in five benchmark environments of the deep  $q$  network (DQN) architecture demonstrate that the proposed method with deep sparse representations from the multilayer sparse-coding structure can outperform existing sparse-coding-based DRL in control, for example, completing Mountain Car with 140.81 steps, achieving near 10% reward increase from the single-layer sparse-coding DRL algorithm, and obtaining 286.08 scores in Catcher, which are over two times the rewards of the other algorithms. Moreover, the proposed algorithm can reduce over 80% parameters while keeping performance improvements from deep sparse representations.","2168-2275","","10.1109/TCYB.2022.3157892","National Key Research and Development Plan(grant numbers:2021YFB2700302); National Natural Science Foundation of China(grant numbers:62172453,61727810,61803096,62073086,62076077,U191140003); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X355); Guangdong Provincial Pearl River Talents Program(grant numbers:2019QN01X130); Guangzhou Science and Technology Program Project(grant numbers:202002030289,6142006200403); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9740040","Deep reinforcement learning (DRL);log regularizer;multilayer sparse coding;pruning","Nonhomogeneous media;Interference;Encoding;Reinforcement learning;Training;Image segmentation;Feature extraction","decision making;deep learning (artificial intelligence);reinforcement learning;sparse matrices","deep sparse representation;deepqnetwork architecture;double sparse deep reinforcement learning;double sparse DRL algorithm;multilayer sparse coding;multilayer sparse-coding structure;multilayer sparse-coding-structural network;nonconvex log regularizer;nonconvex regularized pruning;sparse-coding DRL algorithm;sparse-coding-based DRL","","5","","36","IEEE","22 Mar 2022","","","IEEE","IEEE Journals"
"End-to-end sensorimotor control problems of AUVs with deep reinforcement learning","H. Wu; S. Song; Y. Hsu; K. You; C. Wu","Department of Automation, Tsinghua University, Beijing, P.R.China; Department of Automation, Tsinghua University, Beijing, P.R.China; Department of Automation, Tsinghua University, Beijing, P.R.China; Department of Automation, Tsinghua University, Beijing, P.R.China; Department of Automation, Tsinghua University, Beijing, P.R.China","2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","28 Jan 2020","2019","","","5869","5874","This paper studies on sensorimotor control problems of Autonomous Underwater Vehicles (AUVs) using deep reinforcement learning. We design an end-to-end learning architecture mapping original sensor input to continuous control output without referring to the dynamics of vehicles. To avoid difficult and noisy underwater localization, we implement the learning without knowing the positions of AUVs by proposing novel state encoder and reward shaping strategies. Two distinct underwater tasks, obstacle avoidance with sonar sensor and pipeline following with visual sensor, are simulated to validate the effectiveness of proposed architecture and strategies. For the latter, we test the learned policy on realistic images of underwater pipelines to check its generalization ability.","2153-0866","978-1-7281-4004-9","10.1109/IROS40897.2019.8967612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967612","","","autonomous underwater vehicles;collision avoidance;control engineering computing;mobile robots;neural net architecture;robot dynamics;sonar","deep reinforcement learning;autonomous underwater vehicles;AUVs;end-to-end learning architecture;continuous control output;noisy underwater localization;state encoder;reward shaping strategies;distinct underwater tasks;sonar sensor;pipeline;visual sensor;underwater pipelines;end-to-end sensorimotor control problems;obstacle avoidance;generalization ability","","5","","18","IEEE","28 Jan 2020","","","IEEE","IEEE Conferences"
"HMDRL: Hierarchical Mixed Deep Reinforcement Learning to Balance Vehicle Supply and Demand","J. Xi; F. Zhu; P. Ye; Y. Lv; H. Tang; F. -Y. Wang","State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Intelligent Transportation Systems","8 Nov 2022","2022","23","11","21861","21872","The imbalance of vehicle supply and demand is a common phenomenon that influences the efficiency of online ride-hailing systems greatly. To address this problem, a three-level hierarchical mixed deep reinforcement learning method (HMDRL) is proposed to reposition idle vehicles. A manager operates at the top level, where action-abstraction is conducted from the time dimension and is adaptive for spatially scalable and time-varying systems. Coordinators locate at the middle level and a parallel coordination mechanism that is independent of the decision order is designed to improve the efficiency of the repositioning. The bottom level is composed of executive workers to reposition vehicles with mixed states and the states contain spatiotemporal information of agents’ neighbor areas. Two reward functions are designed for the manager and the coordinators, respectively, aiming to improve the training effect by avoiding sparse rewards. A simulator based on real orders is designed and HMDRL is compared with six methods. Experimental results demonstrate that HMDRL outperforms all the other methods. In three comparison experiments, the order response rate is increased by 0.62% to 8.29%, 1.5% to 7.78%, 1.18% to 4.75%, respectively.","1558-0016","","10.1109/TITS.2022.3191752","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0909050001); National Natural Science Foundation of China (NSFC)(grant numbers:U1811463,U1909204,61876011,52071312); Chinese Academy of Sciences (CAS) Science and Technology Service Network Plan (STS) Dongguan Project(grant numbers:20201600200132); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9839295","Deep reinforcement learning;online ride-hailing system;hierarchical repositioning framework;parallel coordination mechanism;mixed state","Reinforcement learning;Pricing;Heuristic algorithms;Supply and demand;Surges;Vehicle dynamics;Vehicles","deep learning (artificial intelligence);time-varying systems;traffic engineering computing","action-abstraction;coordinators;decision order;HMDRL;idle vehicles repositioning;middle level;mixed states;online ride-hailing systems;parallel coordination mechanism;spatially scalable time-varying systems;three-level hierarchical mixed deep reinforcement learning method;time dimension;vehicle supply and demand imbalance","","4","","53","IEEE","25 Jul 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Combustion Optimization System Using Synchronous Neural Episodic Control","Y. Cheng; L. Zou; Z. Zhuang; Z. Sun; W. Zhang","Department of Automation, Shanghai Jiao Tong University, Shanghai, PRC; Department of Automation, Shanghai Jiao Tong University, Shanghai, PRC; Department of Automation, Shanghai Jiao Tong University, Shanghai, PRC; Department of Automation, Shanghai Jiao Tong University, Shanghai, PRC; Department of Automation, Shanghai Jiao Tong University, Shanghai, PRC","2018 37th Chinese Control Conference (CCC)","7 Oct 2018","2018","","","8770","8775","Neural episodic control (NEC) is an emerging deep reinforcement learning algorithm, the dynamic-scale memory buffer of which is updated by appending experience in a steady stream. NEC has greater ability to utilize the information of the memory and outperforms other deep reinforcement learning agents. However, the scale of memory consumption increases dramatically along with the training episodes. It is even worse that, to query the large-scale memory buffer, kd-trees based approximate nearest neighbors algorithm has to be repeatedly performed. These defects of NEC consequently hinder the further application on the industrial control units with low-performance hardware; e.g. the thermal power plant. To overcome this problem, in this paper, a deep reinforcement learning system based on the synchronous neural episodic control (SNEC) algorithm is proposed for combustion optimization tasks on limited memory resources. The proposed system consists of two major components: 1) a predictor on the basis of convolutional neural networks and LSTM module; 2) an SNEC optimizer agent. The main research and innovations are as follows: 1) a deep neural network model is developed to predict system states accurately and serves as a proxy for the optimizer; 2) a fixed-scale memory buffer is designed specifically for the control agents with low-performance hardware; 3) a straightforward mechanism for lookup and update is established to eliminate the computation of kd-trees search algorithm; 4) different from the original one, only one public differentiable neural dictionary (PDND) is required for SNEC agent, and thus the Q-value for all the actions are updated synchronously. The experimental results show that the proposed SNEC algorithm outperforms the standard deep Q-networks on limited memory resources; with the application of this system, the power efficiency turns out to be improved, and original gas emissions are reduced. In addition, the control actions are designed to be simple enough for the practical implementation.","1934-1768","978-988-15639-5-8","10.23919/ChiCC.2018.8483919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8483919","Deep reinforcement learning (DRL);deep Q-networks (DQN);neural episodic control (NEC)","Combustion;Optimization;Machine learning;Power generation;Predictive models;Training;Fuels","learning (artificial intelligence);neural nets;tree searching","approximate nearest neighbors algorithm;memory consumption;deep reinforcement learning algorithm;convolutional neural networks;memory resources;combustion optimization tasks;synchronous neural episodic control algorithm;deep reinforcement learning system;thermal power plant;industrial control units;NEC;large-scale memory buffer;deep reinforcement learning agents;steady stream;dynamic-scale memory buffer;combustion optimization system;standard deep Q-networks;SNEC algorithm;SNEC agent;public differentiable neural dictionary;kd-trees search algorithm;fixed-scale memory buffer;deep neural network model;SNEC optimizer agent","","2","","20","","7 Oct 2018","","","IEEE","IEEE Conferences"
"DDRL: A Decentralized Deep Reinforcement Learning Method for Vehicle Repositioning","J. Xi; F. Zhu; Y. Chen; Y. Lv; C. Tan; F. Wang","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Chinese Academy of Sciences, State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Beijing, China; Chinese Academy of Sciences, State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Beijing, China; Chinese Academy of Sciences, State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Beijing, China; iFLYTEK CO. LTD, Hefei, China; Chinese Academy of Sciences, State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Beijing, China","2021 IEEE International Intelligent Transportation Systems Conference (ITSC)","25 Oct 2021","2021","","","3984","3989","Online Ride-hailing System improves the efficiency of vehicle utilization and the urban transportation. However, the imbalance between supply and demand is still a problem. To solve this problem and improve resource utilization efficiency, a Decentralized Deep Reinforcement Learning Method (DDRL) for vehicle repositioning is proposed. In DDRL, each vehicle is modeled as an independent agent and dispatched according to its own state to rebalance its local supply and demand. Thus, the global rebalance problem is divided into many small local rebalance problems. First, a new reward evaluation method is proposed and the long-term global reward in traditional reinforcement learning is transformed into many short-term local rewards. Second, a unified algorithm is designed by learning all the decentralized agents' sample data. Finally, the weight matrix of the state is introduced to magnify the differences between the states of adjacent vehicles. Experiments are carried out and the effectiveness of DDRL is verified.","","978-1-7281-9142-3","10.1109/ITSC48978.2021.9564925","NSFC(grant numbers:U1811463,U1909204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564925","decentralized deep reinforcement learning;vehicle repositioning;deep Q network;Online Ride-hailing System","Supply and demand;Costs;Conferences;Reinforcement learning;Collaborative work;Resource management;Optimization","deep learning (artificial intelligence);intelligent transportation systems;multi-agent systems","decentralized agents;adjacent vehicles;DDRL;decentralized deep reinforcement learning method;vehicle repositioning;vehicle utilization;resource utilization efficiency;local supply;global rebalance problem;local rebalance problems;reward evaluation method;long-term global reward;traditional reinforcement learning;short-term local rewards;online ride-hailing system;urban transportation;unified algorithm;weight matrix","","2","","17","IEEE","25 Oct 2021","","","IEEE","IEEE Conferences"
"MADES: A Unified Framework for Integrating Agent-Based Simulation with Multi-Agent Reinforcement Learning","X. Wang; L. Zhang; Y. Laili; K. Xie; H. Lu; C. Zhao","School of Automation Science and Electrical Engineering, Beihang University, Beijing, CHINA; School of Automation Science and Electrical Engineering, Beihang University, Beijing, CHINA; School of Automation Science and Electrical Engineering, Beihang University, Beijing, CHINA; School of Automation Science and Electrical Engineering, Beihang University, Beijing, CHINA; School of Automation Science and Electrical Engineering, Beihang University, Beijing, CHINA; Computer School, Beijing Information Science and Technology University, Beijing, CHINA","2021 Annual Modeling and Simulation Conference (ANNSIM)","5 Oct 2021","2021","","","1","12","Agent-Based Simulation (ABS) provides distributed entities for simulating agent emergence or interactive behaviors, but the agent behaviors usually rely on the hard rules, thus lacking the intelligent decision-making capability. With the development of artificial intelligence, Multi-Agent Reinforcement Learning (MARL) has shown positive potential in robot control, autonomous driving, and human-machine battles as its powerful learning capability for making intelligent decisions. There are many challenges in applying MARL directly to ABS, and there is no unified framework that integrates them. The paper proposed the Multi-Agent Discrete Event Simulation (MADES) framework based on several DEVS atomic models to construct the multi-agent system, which has advantages for representing various MARL architectures. A predator-prey system simulation with a mainstream MARL algorithm is built under our framework, the training curves and event transition time figure have verified the learning and the simulation performance of the framework.","","978-1-56555-375-0","10.23919/ANNSIM52504.2021.9552052","National Key R&D Program of China(grant numbers:2018YFB1701600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9552052","agent-based simulation;reinforcement learning;multi-agent system;discrete event simulation","Training;Analytical models;Decision making;Robot control;Reinforcement learning;Learning (artificial intelligence);Predator prey systems","discrete event simulation;learning (artificial intelligence);multi-agent systems","interactive behaviors;intelligent decision-making capability;artificial intelligence;multiagent reinforcement learning;MARL;learning capability;intelligent decisions;ABS;MultiAgent Discrete Event Simulation framework;MADES;multiagent system;predator-prey system simulation;simulation performance;agent-based simulation","","1","","26","","5 Oct 2021","","","IEEE","IEEE Conferences"
"Leader-Follower Formation Control for Fixed-Wing UAVs using Deep Reinforcement Learning","Y. Shi; J. Song; Y. Hua; J. Yu; X. Dong; Z. Ren","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Beijing Institute of Astronatical Systems Engineering, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","3456","3461","This paper studies a fundamental formation flight scenario for fixed-wing unmanned aerial vehicles (UAVs) based on the leader-follower guidance and control frame using deep reinforcement learning (DRL) method. Firstly, on the basis of typical path following guidance problem, this paper proposes a complete dynamics for fixed-wing vehicle formation tracking flight with both acceleration and angular rate control. The tracking error dynamics with respect to the Serret- Frenet frame is derived where the singularity problem is avoided. Secondly, DRL methods are further introduced to cope with the highly coupled nonlinear problem. Based on both original application and indirect modifications of error dynamics, the online learning environments are respectively constructed. Thirdly, the implementation and comparative analysis of both deep deterministic policy gradient (DDPG) and deep Q-network (DQN) methods for solving the formation control problem are provided using deep neural network (DNN) approximation. Finally, the learning and control results of both different models and diverse DRL methods are given to verify the efficiency and applicability.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9901799","National Natural Science Foundation of China(grant numbers:61922008,62103023,61973013,61873011,62103016); China Postdoctoral Science Foundation(grant numbers:2020M680297); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9901799","Fixed-wing UAVs;lead-follower formation;deep neural network;deep reinforcement learning","Deep learning;Technological innovation;Analytical models;Neural networks;Reinforcement learning;Autonomous aerial vehicles;Formation control","aircraft control;autonomous aerial vehicles;control system synthesis;deep learning (artificial intelligence);mobile robots;multi-robot systems;nonlinear control systems;position control;reinforcement learning;remotely operated vehicles;tracking","angular rate control;complete dynamics;control frame;control results;DDPG;deep deterministic policy gradient;deep neural network approximation;deep Q-network methods;deep reinforcement learning method;diverse DRL methods;DNN;DQN;fixed-wing UAV;fixed-wing unmanned aerial vehicles;fixed-wing vehicle formation;formation control problem;fundamental formation flight scenario;guidance problem;highly coupled nonlinear problem;leader-follower formation control;leader-follower guidance;online learning environments;Serret-Frenet frame;singularity problem;tracking error dynamics","","1","","28","","11 Oct 2022","","","IEEE","IEEE Conferences"
"A Combinatorial Recommendation System Framework Based on Deep Reinforcement Learning","F. Zhou; B. Luo; T. Hu; Z. Chen; Y. Wen","School of Automation Central South University, Changsha, China; School of Automation Central South University, Changsha, China; School of Automation Central South University, Changsha, China; School of Automation Central South University, Changsha, China; School of Automation Central South University, Changsha, China","2021 IEEE International Conference on Big Data (Big Data)","13 Jan 2022","2021","","","5733","5740","In this paper, we propose a combinatorial recommendation system based on deep reinforcement learning (DRL). Specifically, we focus on the combinatorial product recommendation problem, design a consumer behavior simulator, and utilize deep reinforcement learning to find appropriate product combinations that can improve the sales of the platform. In order to replace real users and conduct massive real-time interactive training with recommendation system, user short- term characteristics are extracted from user-click-history by hierarchical recurrent neural network to train a user simulator. Through ingenious modeling, we transform the NP-hard combinatorial optimization problem into a multi-step sequential decision problem, and construct a framework of combinatorial recommendation system based on DRL. Relevant experiments show that the binary accuracy of the user simulator in predicting users’ consumption behavior reaches more than 80%, and the combinatorial DRL based recommendation system improves the platform sales and provides attractive combinations of products for customers.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671593","National Natural Science Foundation of China; Central South University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671593","Recommendation System;Combinatorial Recommendation;Simulator;Deep Reinforcement Learning","Training;Recurrent neural networks;Consumer behavior;Conferences;Reinforcement learning;Transforms;Big Data","consumer behaviour;deep learning (artificial intelligence);recommender systems;recurrent neural nets;reinforcement learning","deep reinforcement learning;massive real-time interactive training;user short- term characteristics;user-click-history;user simulator;NP-hard combinatorial optimization problem;multistep sequential decision problem;combinatorial DRL based recommendation system;combinatorial recommendation system framework;combinatorial product recommendation problem;consumer behavior simulator","","1","","33","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Multi-Agent Cognition Difference Reinforcement Learning for Multi-Agent Cooperation","H. Wang; T. Qiu; Z. Liu; Z. Pu; J. Yi; W. Yuan","School of Artificial Intelligence University of Chinese Academy of Sciences Institute of Automation Chinese Academy of Sciences, Beijing, China; Institute of Automation Chinese Academy of Sciences, Beijing, China; Institute of Automation Chinese Academy of Sciences, Beijing, China; Institute of Automation Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; Institute of Automation Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; China Academy of Electronics and Information Technology, Beijing, China","2021 International Joint Conference on Neural Networks (IJCNN)","20 Sep 2021","2021","","","1","7","Multi-agent cooperation is one of the most attractive research fields in multi-agent systems. There are many attempts made by researchers in this field to promote the cooperation behavior. However, in partially-observable environments, a large number of agents and complex interactions among the agents cause huge difficulty for policy learning. Moreover, redundant communication contents caused by many agents make effective features hard to be extracted, which prevents the policy from converging. To address the limitations above, a novel method called multi-agent cognition difference reinforcement learning (MACD-RL) is proposed in this paper. The key feature of MACD-RL lies in cognition difference network (CDN) and a soft communication network (SCN). CDN is designed to allow each agent to choose its neighbors (communication targets) adaptively with its environment cognition difference. SCN is designed to handle the complex interactions among the agents with soft attention mechanism. The results of simulations including mixed cooperative and competitive tasks demonstrate that the effectiveness and robustness of the proposed model.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9533484","National Key Research and Development Program of China(grant numbers:2018AAA0102404); Chinese Academy of Sciences(grant numbers:CXYJJ19-ZD-02,CXYJJ20-QN-05); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533484","","Adaptation models;Neural networks;Reinforcement learning;Feature extraction;Cognition;Robustness;Communication networks","learning (artificial intelligence);multi-agent systems","multiagent cognition difference reinforcement learning;multiagent cooperation;multiagent systems;cognition difference network;environment cognition difference;MACD-RL;CDN;soft communication network;SCN;soft attention mechanism","","1","","28","IEEE","20 Sep 2021","","","IEEE","IEEE Conferences"
"Active Power Correction Strategies Based on Deep Reinforcement Learning—Part I: A Simulation-driven Solution for Robustness","P. Xu; J. Duan; J. Zhang; Y. Pei; D. Shi; Z. Wang; X. Dong; Y. Sun","School of Electrical Engineering and Automation Wuhan University, Wuhan, China; GEIRI North America, San Jose, CA, USA; School of Electrical Engineering and Automation Wuhan University, Wuhan, China; School of Electrical Engineering and Automation Wuhan University, Wuhan, China; GEIRI North America, San Jose, CA, USA; GEIRI North America, San Jose, CA, USA; School of Electrical Engineering and Automation Wuhan University, Wuhan, China; School of Electrical Engineering and Automation Wuhan University, Wuhan, China","CSEE Journal of Power and Energy Systems","25 Jul 2022","2022","8","4","1122","1133","Employing the novel Deep Reinforcement Learning approach, this paper addresses the active power corrective control in modern power systems. Seeking to minimize the joint effect engendered by operation cost and blackout penalty, this correction strategy focuses on evaluating the robustness and adaptability aspects of the control agent. In Part I of this paper, where robustness is the primary focus, the agent is developed to handle unexpected incidents and guide the stable operation of power grids A Simulation-driven Graph Attention Reinforcement Learning method is proposed to perform robust active power corrective control. The aim of the graph attention networks is to determine the representation of power system states considering the topological features. Monte Carlo tree search is adopted to select the best suitable action set out of the large action space, including generator redispatch and topology control actions. Finally, driven by simulation, a guided training mechanism along with a long-short-term action deployment strategy are designed to help the agent better evaluate the action set while training and to operate more stably when deployed. The efficacy of the proposed method has been demonstrated in the “2020 Learning to Run a Power Network - Neurips Track 1” global competition and the associated cases. Part II of this paper deals with the adaptability case, where the agent is equipped to better adapt to a grid that has an increasing share of renewable energies through the years.","2096-0042","","10.17775/CSEEJPES.2020.07090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9535409","Active power corrective control;deep reinforcement learning;graph attention networks;simulation-driven","Generators;Switches;Power systems;Costs;Reinforcement learning;Control systems;Power system stability","costing;deep learning (artificial intelligence);Monte Carlo methods;power engineering computing;power generation control;power generation dispatch;power generation economics;power generation reliability;power grids;reinforcement learning;tree searching","deep reinforcement learning approach;joint effect;operation cost;blackout penalty;adaptability aspects;power grids;power system states;Monte Carlo tree search;guided training mechanism;long-short-term action deployment strategy;active power corrective control;simulation-driven graph attention reinforcement learning method;power network-neurips track;renewable energies","","1","","32","","10 Sep 2021","","","CSEE","CSEE Journals"
"Intrinsic Reward with Peer Incentives for Cooperative Multi-Agent Reinforcement Learning","T. Zhang; Z. Liu; S. Wu; Z. Pu; J. Yi","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","1","7","In this paper, we propose a novel Intrinsic Reward method with Peer Incentives (IRPI) to promote the inter-agent direct interactions and implicitly address the credit assignment problem in cooperative multi-agent reinforcement learning (MARL). The IRPI method can build mutual incentives between agents by using their causal effect, to realize their advanced cooperation. Specifically, a new intrinsic reward mechanism is conducted, which equips each agent with the ability to reward other agent by using the causal effect between them. Moreover, the mechanism is built through a neural network and learned by using causal effect between the agents. Furthermore, the counterfactual reasoning is used to infer the causal effect between the agents using the joint action-state value function, and then assess the quality of the effect using individual state value function in MARL. Simulational results in Starcraft II Micromanagement demonstrate that the proposed IRPI can enhance cooperation among the RL agents to achieve better performance than some state-of-the-art MARL methods in various cooperative multi-aaent tasks.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892092","National Key Research and Development Program of China(grant numbers:2018AAA0101005,2018AAA0102402); Chinese Academy of Sciences(grant numbers:XDA27030204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892092","Cooperative multi-agent systems;multi-agent reinforcement learning;intrinsic reward;social learning","Visualization;Neural networks;Reinforcement learning;Cognition;Task analysis","computer games;learning (artificial intelligence);multi-agent systems;software agents","causal effect;cooperative multiagent reinforcement learning;credit assignment problem;inter-agent direct interactions;intrinsic reward mechanism;IRPI method;mutual incentives;novel Intrinsic Reward method;Peer Incentives;RL agents;state-of-the-art MARL methods","","1","","24","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Multi-agent Reinforcement Learning-based Twin-vehicle Fair Cooperative Driving in Dynamic Highway Scenarios","S. Chen; M. Wang; W. Song; Y. Yang; M. Fu","School of Automation, Beijing Institute of Technology, Beijing, P.R. China; School of Automation, Beijing Institute of Technology, Beijing, P.R. China; School of Automation, Beijing Institute of Technology, Beijing, P.R. China; School of Automation, Beijing Institute of Technology, Beijing, P.R. China; School of Automation, Beijing Institute of Technology, Beijing, P.R. China","2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)","1 Nov 2022","2022","","","730","736","Highway is an important scenario for autonomous driving application because of its clear rules and little social intervention. In this scenario, cooperative driving of the unmanned vehicles is also a key technology. To achieve a simpler system architecture and lighter computation than rules-based cooperative driving methods, a multi-agent reinforcement learning-based twin-vehicle cooperative driving method is proposed in this paper. This work implements the generalization adaptation of reinforcement learning method in high dynamic highway scenarios. Besides, it pays equal attention to the autonomy of each one and their cooperation through a fair cooperation algorithm, realizing the independent lane changing and overtaking in heavy traffic, while keeping a fixed formation in loose traffic. Thus, the twin-vehicle can speed up while avoiding the interference of rigid structure on traffic. Experiments in a variety of highway scenarios verify the cooperative performance, also further increase the possibility of creating a harmonious driving environment.","","978-1-6654-6880-0","10.1109/ITSC55140.2022.9922266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9922266","","Road transportation;Training;Collaboration;Systems architecture;Reinforcement learning;Interference;Vehicle dynamics","mobile robots;multi-agent systems;reinforcement learning;road traffic;road vehicles;traffic engineering computing","autonomous driving application;unmanned vehicles;high dynamic highway scenarios;fair cooperation algorithm;harmonious driving environment;multiagent reinforcement learning-based twin-vehicle fair cooperative driving;rules-based cooperative driving method;independent lane change","","1","","20","IEEE","1 Nov 2022","","","IEEE","IEEE Conferences"
"Traded Control of Human–Machine Systems for Sequential Decision-Making Based on Reinforcement Learning","Q. Zhang; Y. Kang; Y. -B. Zhao; P. Li; S. You","Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, Institute of Advanced Technology, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China","IEEE Transactions on Artificial Intelligence","21 Jul 2022","2022","3","4","553","566","Sequential decision-making (SDM) is a common type of decision-making problem with sequential and multistage characteristics. Among them, the learning and updating of policy are the main challenges in solving SDM problems. Unlike previous machine autonomy driven by artificial intelligence alone, we improve the control performance of SDM tasks by combining human intelligence and machine intelligence. Specifically, this article presents a paradigm of a human–machine traded control systems based on reinforcement learning methods to optimize the solution process of sequential decision problems. By designing the idea of autonomous boundary and credibility assessment, we enable humans and machines at the decision-making level of the systems to collaborate more effectively. And the arbitration in the human–machine traded control systems introduces the Bayesian neural network and the dropout mechanism to consider the uncertainty and security constraints. Finally, experiments involving machine traded control, human traded control were implemented. The preliminary experimental results of this article show that our traded control method improves decision-making performance and verifies the effectiveness for SDM problems.","2691-4581","","10.1109/TAI.2021.3127857","National Key Research and Development Program of China(grant numbers:2018AAA0100801); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613742","Human–machine systems;reinforcement learning;sequential decision-making (SDM);traded control","Decision making;Man-machine systems;Control systems;Robots;Task analysis;Machine intelligence;Human intelligence","belief networks;decision making;learning (artificial intelligence);man-machine systems;neural nets;optimisation","security constraints;uncertainty constraints;dropout mechanism;Bayesian neural network;credibility assessment;autonomous boundary;human-machine traded control system;machine intelligence;reinforcement learning;human-machine systems;sequential decision problems;human intelligence;artificial intelligence;SDM problems;multistage characteristics;sequential decision-making","","1","","37","IEEE","12 Nov 2021","","","IEEE","IEEE Journals"
"Second-Order Linear Multi-Agent Formation Control Based on Fuzzy Logic System Approximator and Actor-Critic Reinforcement Learning","Z. Zhang; J. Huang; F. Cai; J. Chen; Y. Chen","College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China","2020 39th Chinese Control Conference (CCC)","9 Sep 2020","2020","","","4918","4923","In this paper, an optimized leader-follower formation control for second-order linear multi-agent systems using the actor-critic reinforcement learning is proposed. An optimal control problem is formulated and solved using a combined actor- critic reinforcement learning and fuzzy logic systems(FLSs) approximator. Actor FLSs are constructed for performing control behavior and critic FLSs are responsible for evaluating the control effectiveness, respectively. Through theoretical analysis, we show that the desired optimal performance can be achieved. Finally, simulation examples are given to demonstrate the effectiveness of the proposed method.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9189545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9189545","Actor-Critic reinforcement learning;fuzzy logic system;optimized formation control;multi-agent systems","Multi-agent systems;Optimal control;Fuzzy logic;Learning (artificial intelligence);Approximation algorithms;Mathematical model","approximation theory;fuzzy control;fuzzy logic;fuzzy systems;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;optimal control","control behavior;critic FLSs;control effectiveness;optimal performance;order linear multiagent formation control;fuzzy logic system approximator;actor-critic reinforcement learning;optimized leader-follower formation control;second-order linear multiagent systems;optimal control problem;actor FLSs;combined actor-critic reinforcement learning;critic FLS","","1","","13","","9 Sep 2020","","","IEEE","IEEE Conferences"
"Manipulator Motion Planning based on Actor-Critic Reinforcement Learning","Q. Li; J. Nie; H. Wang; X. Lu; S. Song","College of Electrical Engineering and Automation, Shandong University of Science and Technology, Qingdao, China; College of Electrical Engineering and Automation, Shandong University of Science and Technology, Qingdao, China; College of Electrical Engineering and Automation, Shandong University of Science and Technology, Qingdao, China; College of Electrical Engineering and Automation, Shandong University of Science and Technology, Qingdao, China; College of Electrical Engineering and Automation, Shandong University of Science and Technology, Qingdao, China","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","4248","4254","The manipulator control model has the characteristics of high-order, nonlinear, multivariable and strong coupling, which makes it difficult for the manipulator to have good adaptability and autonomy. Aiming at the problem of poor reusability and poor autonomy of manipulator applications, a motion planning algorithm based on reinforcement learning is proposed. In this paper, the reinforcement learning continuous control algorithm Actor-Critic is applied to the motion planning of the manipulator to increase the environmental applicability and autonomy of the manipulator, and realize the intelligent control of the manipulator under simple kinematics modeling. At first, the simulation environment of the hand-eye system of the manipulator is constructed, then the reinforcement learning algorithm model is established according to the simulation environment, and finally, the motion planning training of the manipulator is completed in the simulation environment. Experimental results demonstrate that the proposed manipulator motion planning algorithm based on Actor-Critic reinforcement learning has good environmental adaptability and stability.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9550010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9550010","Reinforcement Learning;Actor-Critic Algorithm;Manipulator;Motion Planning;Value evaluation;Policy evaluation","Training;Couplings;Adaptation models;Reinforcement learning;Kinematics;Manipulators;Stability analysis","intelligent control;learning (artificial intelligence);manipulators;mobile robots;path planning","simulation environment;Actor-Critic reinforcement learning;manipulator motion planning;manipulator control model;nonlinear coupling;multivariable coupling;good adaptability;poor autonomy;manipulator applications;continuous control algorithm Actor-Critic;environmental applicability;motion planning training","","1","","14","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Residual Reinforcement Learning for Motion Control of a Bionic Exploration Robot—RoboDact","T. Zhang; R. Wang; S. Wang; Y. Wang; G. Zheng; M. Tan","State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Centrale Lille, CRIStAL-Centre de Recherche en Informatique Signal et Automatique de Lille, University of Lille, Lille, France; Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Instrumentation and Measurement","15 Jun 2023","2023","72","","1","13","This article aims to investigate the motion control method of a bionic underwater exploration robot (RoboDact). The robot is equipped with a double-joint tail fin and two undulating pectoral fins to obtain good mobility and stability. The hybrid propulsion mode helps perform stable and effective underwater exploration and measurement. To coordinate these two kinds of bionic propulsion fins and address the challenge of measurement noises and external disturbances during underwater exploration, a novel residual reinforcement learning method with parameter randomization (PR-RRL) is proposed. The control strategy is a weighted superposition of a feedback controller and a residual controller. The observation feedback controller based on active disturbance rejection control (ADRC) is adapted to improve stability and convergence. And the residual controller based on the soft actor–critic (SAC) algorithm is adapted to improve adaptability to uncertainties and disturbances. Moreover, the parameter randomization training strategy is proposed for adapting natural complicated scenarios by randomizing the partial dynamics of the underwater exploration robot during the training phase. Finally, the feasibility and efficacy of the presented motion control method are validated by comprehensive simulation tests and RoboDact prototype physical experiments.","1557-9662","","10.1109/TIM.2023.3282297","STI 2030-Major Projects(grant numbers:2021ZD0114504); National Natural Science Foundation of China(grant numbers:62276253,62203435,62122087); Beijing Natural Science Foundation(grant numbers:4222055,4222056); Youth Innovation Promotion Association CAS(grant numbers:2020137); Scientific Research Program of Beijing Municipal Commission of Education—Natural Science Foundation of Beijing(grant numbers:KZ202210017024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10143310","Active disturbance rejection control (ADRC);bionic exploration robot;motion control;residual reinforcement learning (RRL);soft actor-critic (SAC)","Autonomous underwater vehicles;Propulsion;Motion control;Robots;Robot kinematics;Biological system modeling;Training","active disturbance rejection control;biomimetics;control engineering computing;feedback;mobile robots;motion control;propulsion;reinforcement learning;robot dynamics;stability;uncertain systems;underwater vehicles","active disturbance rejection control;bionic exploration robot-RoboDact;bionic propulsion fins;bionic underwater exploration robot;control strategy;convergence;double joint tail fin;effective underwater exploration;external disturbances;hybrid propulsion mode;measurement noises;novel residual reinforcement learning method;observation feedback controller;parameter randomization training strategy;presented motion control method;residual controller;RoboDact prototype physical experiments;stability;stable exploration;uncertainties;undulating pectoral fins","","","","41","IEEE","2 Jun 2023","","","IEEE","IEEE Journals"
"Multi-Agent Reinforcement Learning-Based Decision Making for Twin-Vehicles Cooperative Driving in Stochastic Dynamic Highway Environments","S. Chen; M. Wang; W. Song; Y. Yang; M. Fu","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Vehicular Technology","17 Oct 2023","2023","72","10","12615","12627","In the past decade, reinforcement learning (RL) has achieved encouraging results in autonomous driving, especially in well-structured and regulated highway environments. However, few researches pay attention to RL-based multiple-vehicles cooperative driving, which is much more challenging because of dynamic real-time interactions and transient scenarios. This article proposes a Multi-Agent Reinforcement Learning (MARL) based twin-vehicles cooperative driving decision making method which achieves the generalization adaptation of the RL method in highly dynamic highway environments and enhances the flexibility and effectiveness of collaborative decision making system. The proposed fair cooperative MARL method pays equal attention to the individual intelligence and the cooperative performance, and employs a stable estimation method to reduce the propagation of overestimated joint $Q$-values between agents. Thus, the twin-vehicles system strikes a balance between maintaining formation and free overtaking in dynamic highway environments, to intelligently adapt to different scenarios, such as heavy traffic, loose traffic, even some emergency. Targeted experiments show that our method has strong cooperative performance, also further increases the possibility of creating a harmonious driving environment.","1939-9359","","10.1109/TVT.2023.3275582","National Natural Science Foundation of China(grant numbers:61903034,U1913203,61973034); Program for Changjiang Scholars and Innovative Research Team in University(grant numbers:IRT-16R06,T2014224); Beijing Institute of Technology Research Fund Program for Young Scholars; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10123696","Cooperative driving;fair cooperation;multi-agent reinforcement learning (MARL);overestimation","Collaboration;Vehicle dynamics;Task analysis;Decision making;Games;Autonomous vehicles;Reinforcement learning","","","","","","40","IEEE","12 May 2023","","","IEEE","IEEE Journals"
"Map-less End-to-end Navigation of Mobile Robots via Deep Reinforcement Learning","H. Ma; S. Wang; S. Zhang; S. Ren; H. Wang","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China","2023 IEEE 13th International Conference on Electronics Information and Emergency Communication (ICEIEC)","7 Aug 2023","2023","","","48","52","A deep reinforcement learning model with a unique long-term memory capability is proposed in this paper, which addresses the map-less navigation of mobile robots in dynamic environments. Based on the recurrent neural network, the proposed model takes continuous historical states as input and better handles dynamic obstacles. Furthermore, a novel reward function is designed to ensure smooth navigation trajectories and satisfactory navigation results in dynamic environments. The proposed approach is evaluated on the Gazebo simulation platform, and higher navigation success rates in dynamic environments are achieved.","2377-844X","979-8-3503-3172-1","10.1109/ICEIEC58029.2023.10200799","National Natural Science Foundation of China(grant numbers:62173029,62273033); State Key Laboratory of Automotive Safety and Energy(grant numbers:KFY2214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10200799","deep reinforcement learning;map-less;navigation;ROS;LSTM","Deep learning;Recurrent neural networks;Navigation;Redundancy;Reinforcement learning;Trajectory;Mobile robots","collision avoidance;deep learning (artificial intelligence);mobile robots;navigation;neurocontrollers;path planning;recurrent neural nets;reinforcement learning;robot programming","continuous historical states;deep reinforcement learning model;Gazebo simulation platform;long-term memory capability;map-less end-to-end navigation;map-less navigation;mobile robots;navigation success rates;recurrent neural network","","","","16","IEEE","7 Aug 2023","","","IEEE","IEEE Conferences"
"Multi-Agent Uncertainty Sharing for Cooperative Multi-Agent Reinforcement Learning","G. Yang; H. Chen; J. Zhang; Q. Yin; K. Huang","CRISE, Institute of Automation, Chinese Academy of Sciences, Beijing, P.R.China; CRISE, Institute of Automation, Chinese Academy of Sciences, Beijing, P.R.China; CRISE, Institute of Automation, Chinese Academy of Sciences, Beijing, P.R.China; CRISE, Institute of Automation, Chinese Academy of Sciences, Beijing, P.R.China; CRISE, Institute of Automation, Chinese Academy of Sciences, Beijing, P.R.China","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","1","8","Cooperative multi-agent reinforcement learning has been considered promising to complete many complex cooperative tasks in the real world such as coordination of robot swarms and self-driving. To promote multi-agent cooperation, Centralized Training with Decentralized Execution emerges as a popular learning paradigm due to partial observability and communication constraints during execution and computational complexity in training. Value decomposition has been known to produce competitive performance to other methods in complex environment within this paradigm such as VDN and QMIX, which approximates the global joint Q-value function with multiple local individual Q-value functions. However, existing works often neglect the uncertainty of multiple agents resulting from the partial observability and very large action space in the multi-agent setting and can only obtain the sub-optimal policy. To alleviate the limitations above, building upon the value decomposition, we propose a novel method called multi-agent uncertainty sharing (MAUS). This method utilizes the Bayesian neural network to explicitly capture the uncertainty of all agents and combines with Thompson sampling to select actions for policy learning. Besides, we impose the uncertainty-sharing mechanism among agents to stabilize training as well as coordinate the behaviors of all the agents for multi-agent cooperation. Extensive experiments on the StarCraft Multi-Agent Challenge (SMAC) environment demonstrate that our approach achieves significant performance to exceed the prior baselines and verify the effectiveness of our method.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9891948","National Natural Science Foundation of China(grant numbers:61876181); Youth Innovation Promotion Association, and CAS and the Projects of Chinese Academy of Science(grant numbers:QYZDB-SSWJSC006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9891948","Multi-Agent System;Bayesian Neural Network;Uncertainty;Exploration","Training;Uncertainty;Robot kinematics;Neural networks;Buildings;Reinforcement learning;Behavioral sciences","learning (artificial intelligence);multi-agent systems;multi-robot systems","multiagent uncertainty;multiagent reinforcement learning;complex cooperative tasks;multiagent cooperation;popular learning paradigm;partial observability;value decomposition;global joint Q-value function;multiple local individual Q-value functions;multiple agents;multiagent setting;policy learning;uncertainty-sharing mechanism;StarCraft MultiAgent Challenge environment","","","","40","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Dynamic Route Planning Method Based on Deep Reinforcement Learning and Velocity Obstacle","L. Mengmeng; Y. Xiaofei; X. Zhengrong; W. Qi; H. Jiabao","School of Automation, Jiangsu University of Science and Technology, Zhenjiang, China; School of Automation, Jiangsu University of Science and Technology, Zhenjiang, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Jiangsu University of Science and Technology, Zhenjiang, China; School of Automation, Jiangsu University of Science and Technology, Zhenjiang, China","2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)","7 Jul 2023","2023","","","627","632","Route planning is a key technology for unmanned surface vessel (USV) autonomous navigation. Traditional route planning algorithm usually has the shortcomings of complex calculation, long time and single algorithm function. In this paper, aiming at the shortcomings of traditional algorithm, the route planning strategy of USV based on deep reinforcement learning (DRL) and velocity obstacle (VO) is designed. Using electronic nautical chart to build visual environment model; Based on the kinematics of USV, Markov decision process is established, and combined with the advantages of VO method and DRL, a general reward mechanism is designed, so that USV can achieve fast and safe route planning strategy in the complex marine environment where dynamic obstacles and static obstacles exist at the same time. In order to prove our method, a simulation experiment is introduced, and the results confirm the correctness and effectiveness of the proposed method.","2767-9861","979-8-3503-2105-0","10.1109/DDCLS58216.2023.10166409","Jiangsu University of Science and Technology(grant numbers:1032932006); National Natural Science Foundation of China(grant numbers:62173341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10166409","Unmanned surface vessel;Reinforcement learning;Velocity obstacle;Route planning","Deep learning;Training;Learning systems;Visualization;Heuristic algorithms;Reinforcement learning;Kinematics","collision avoidance;deep learning (artificial intelligence);Markov processes;mobile robots;path planning;reinforcement learning;unmanned surface vehicles","complex calculation;complex marine environment;deep reinforcement learning;DRL;dynamic obstacles;dynamic route planning method;electronic nautical chart;Markov decision process;safe route planning strategy;single algorithm function;static obstacles;traditional route planning algorithm;unmanned surface vessel autonomous navigation;USV;velocity obstacle;visual environment model;VO method","","","","8","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"Reentry Guidance for Hypersonic Vehicle based on Reinforcement Learning","R. Liu; L. Cui; B. Gao; Y. Yao; Q. Zhang","School of Automation Science and Electrical Engineering, Beijing University of Aeronautics and Astronautics, Beijing, China; School of Automation Science and Electrical Engineering, Beijing University of Aeronautics and Astronautics, Beijing, China; School of Automation Science and Electrical Engineering, Beijing University of Aeronautics and Astronautics, Beijing, China; School of Automation Science and Electrical Engineering, Beijing University of Aeronautics and Astronautics, Beijing, China; School of Automation Science and Electrical Engineering, Beijing University of Aeronautics and Astronautics, Beijing, China","2023 3rd International Conference on Frontiers of Electronics, Information and Computation Technologies (ICFEICT)","1 Sep 2023","2023","","","419","430","For the problem of reentry guidance with online no-fly zone constraints, this paper combines the improved A* algorithm with DDPG and proposes a trajectory planning algorithm with high real-time property. Firstly, this paper constructs 2D environment grid map with online no-fly zone information. Secondly, this paper proposes a waypoint decision algorithm by conducting improved A* algorithm on the above map. The reentry guidance of the aircraft is thus simplified to the trajectory planning subtasks between waypoints. Finally, this paper proposes an aircraft trajectory generation algorithm. The algorithm first constructs the MDP model based on the requirements of the subtasks; the builds the DDPG agent training process. Through offline training simulation, the optimal parameters of the agent net is finally obtained. Finally, the aircraft uses the above agent to output the online bank angle guidance law and the expected trajectory of the aircraft.","","979-8-3503-0235-6","10.1109/ICFEICT59519.2023.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10229656","Trajectory planning;Reentry guidance;No-fly zone constraint;Reinforcement Learning","Training;Trajectory planning;Transforms;Reinforcement learning;Aerospace electronics;Real-time systems;Trajectory","aircraft;aircraft control;deep learning (artificial intelligence);Markov processes;mobile robots;optimisation;path planning;reinforcement learning;trajectory control","2D environment grid map;aircraft trajectory generation algorithm;DDPG agent training process;expected trajectory;hypersonic vehicle;improved A* algorithm;online bank angle guidance law;online no-fly zone constraints;real-time property;reentry guidance;reinforcement learning;trajectory planning algorithm;waypoint decision algorithm;zone information","","","","17","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Commander-Soldiers Reinforcement Learning for Cooperative Multi-Agent Systems","Y. Chen; W. Yang; T. Zhang; S. Wu; H. Chang","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","1","7","In ball sports, such as basketball, the coach can guide players to better offend and defend from a holistic perspective to win the game. Inspired by such scenarios, we introduce a coach-like concept into the decision-making process of cooperative multi-agent systems. We propose a new framework Commander-Soldiers Reinforcement Learning (CSRL), for Multi-Agent systems. Specifically, we introduce a virtual role, Commander, which can obtain and encode global information every T steps and send the encoded global guidance to Soldiers (real agents). Furthermore, we propose Policy Guidance Network (PGN), which can customize the encoded global guidance from Commander based on observations for each Soldier, providing each Soldier with specified guidance to the decision-making process. The Soldier takes into account not only the local action-observation histories but also the specified guidance from PGN when making decisions. We validate CSRL on the challenging StarCraft II micromanagement benchmark, proving that our approach can take advantage of intermittent global information to improve collaborative performance.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892794","National Key Research and Development Program of China(grant numbers:2019YFF0301801); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892794","Reinforcement Learning;Multi-Agent System;Multi-Agent Cooperation","Decision making;Neural networks;Collaboration;Reinforcement learning;Games;Benchmark testing;History","computer games;decision making;learning (artificial intelligence);multi-agent systems;sport","encoded global guidance;decision-making process;cooperative multiagent systems;coach-like concept;commander-soldiers reinforcement learning;policy guidance network;ball sports;CSRL;PGN;StarCraft II micromanagement benchmark","","","","23","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Multimodal Traffic Light Control with Connected Vehicles: A Deep Reinforcement Learning Approach","R. Zhou; T. Nousch; D. Adam; A. Hirrle; M. Wang","Chair of Traffic Process Automation, Institute of Traffic Telematics Technische Universität Dresden, Dresden, Germany; Chair of Traffic Process Automation, Institute of Traffic Telematics Technische Universität Dresden, Dresden, Germany; Chair of Traffic Process Automation, Institute of Traffic Telematics Technische Universität Dresden, Dresden, Germany; Chair of Traffic Process Automation, Institute of Traffic Telematics Technische Universität Dresden, Dresden, Germany; Chair of Traffic Process Automation, Institute of Traffic Telematics Technische Universität Dresden, Dresden, Germany","2023 8th International Conference on Models and Technologies for Intelligent Transportation Systems (MT-ITS)","11 Sep 2023","2023","","","1","6","Multimodal traffic light control is a cost-effective way to deal with urban congestion. The development of V2X (Vehicle to Everything) technologies offers unprecedented data and hence new opportunities for situation awareness, but the conventional control algorithms fall short of fully exploiting the real-time vehicle information at the intersections. In this work, a Double Deep Q-learning (DDQL) approach is proposed for multimodal traffic light control with different priority requests in a connected vehicle environment. The proposed DDQL approach is integrated with the existing actuated controller and is readily implementable. The integrated system can terminate the DDQL controller and switch to the actuated controller for safety when an urgent issue occurs such as an electric power outage. The simulation results demonstrate the advantage of the proposed approach compared with actuated control and indicate the reduction of delays for both public transportation and personal vehicle by the proposed approach.","","978-1-6654-5530-5","10.1109/MT-ITS56129.2023.10241625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10241625","Double Deep Q-learning;Multimodal traffic light control;Public transportation priority;V2X technology;Delay","Space vehicles;Connected vehicles;Q-learning;Simulation;Urban areas;Switches;Control systems","deep learning (artificial intelligence);intelligent transportation systems;learning (artificial intelligence);lighting control;reinforcement learning;road traffic;road traffic control;road vehicles;telecommunication computing;traffic engineering computing;vehicular ad hoc networks","actuated control;connected vehicle environment;connected vehicles;conventional control algorithms;Double Deep Q-learning approach;existing actuated controller;multimodal traffic light control;real-time vehicle information","","","","16","IEEE","11 Sep 2023","","","IEEE","IEEE Conferences"
"Integrated Optimal Control for Electrolyte Temperature With Temporal Causal Network and Reinforcement Learning","T. Liu; C. Yang; C. Zhou; Y. Li; B. Sun","School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","13","The electrowinning process is a critical operation in nonferrous hydrometallurgy and consumes large quantities of power consumption. Current efficiency is an important process index related to power consumption, and it is vital to operate the electrolyte temperature close to the optimum point to ensure high current efficiency. However, the optimal control of electrolyte temperature faces the following challenges. First, the temporal causal relationship between process variables and current efficiency makes it difficult to estimate the current efficiency accurately and set the optimal electrolyte temperature. Second, the substantial fluctuation of influencing variables of electrolyte temperature leads to difficulty in maintaining the electrolyte temperature close to the optimum point. Third, due to the complex mechanism, building a dynamic electrowinning process model is intractable. Hence, it is a problem of index optimal control in the multivariable fluctuation scenario without process modeling. To get around this issue, an integrated optimal control method based on temporal causal network and reinforcement learning (RL) is proposed. First, the working conditions are divided and the temporal causal network is used to estimate current efficiency accurately to solve the optimal electrolyte temperature under multiple working conditions. Then, an RL controller is established under each working condition, and the optimal electrolyte temperature is placed into the controller’s reward function to assist in control strategy learning. An experiment case study of the zinc electrowinning process is provided to verify the effectiveness of the proposed method and to show that it can stabilize the electrolyte temperature within the optimal range without modeling.","2162-2388","","10.1109/TNNLS.2023.3278729","National Key Research and Development Program of China(grant numbers:2022YFE0125000); Natural Science Foundation of China(grant numbers:92167105); Natural Science Foundation of Hunan Province(grant numbers:2023JJ10081); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10146479","Electrolyte temperature;optimal control;reinforcement learning (RL);temporal causality;zinc electrowinning process (ZEP)","Electrolytes;Zinc;Process control;Optimal control;Indexes;Ions;Electrochemical processes","","","","","","","IEEE","8 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Comparison of Different Domain Randomization Methods for Policy Transfer in Reinforcement Learning","M. Ma; H. Li; G. Hu; S. Liu; D. Zhao","State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)","7 Jul 2023","2023","","","1818","1823","Deep reinforcement learning algorithms have made great progress in the field of control with the help of many high-efficiency simulation environments. However, due to the difference in state distribution and dynamics, these algorithms trained in the simulation cannot be effectively applied to the real world. The ability to reduce the impact of the Sim2Real gap is critical for transferring policy from the simulation to the real world. Although there are many methods for studying the Sim2Real problem, it is difficult to evaluate the performance of different algorithms due to the different evaluation platforms and evaluation metrics. In this paper, we construct a uniform robot navigation scenario, and revisit the ability of the popular domain randomization methods to transfer the policies from the simulation to the real world under the dynamics gaps. With the analysis of the performence in the simulation environment and the real world, we provide some recommendations of the domain randomization methods and hope to make these methods more efficient to use.","2767-9861","979-8-3503-2105-0","10.1109/DDCLS58216.2023.10166417","National Natural Science Foundation of China (NSFC)(grant numbers:62103409); Strategic Priority Research Program of Chinese Academy of Sciences (CAS)(grant numbers:XDA27030400); International Partnership Program of the Chinese Academy of Sciences(grant numbers:104GJHZ2022013GC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10166417","Sim2Real;Domain randomization;Mobile robot navigation;Deep reinforcement learning","Measurement;Training;Navigation;Heuristic algorithms;Reinforcement learning;Data models;Trajectory","control engineering computing;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;path planning;reinforcement learning","deep reinforcement learning algorithms;different domain randomization methods;different evaluation platforms;dynamics gaps;evaluation metrics;high-efficiency simulation environments;policy transfer;popular domain randomization methods;Sim2Real gap;Sim2Real problem;simulation environment;state distribution;uniform robot navigation scenario","","","","34","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"RACA: Relation-Aware Credit Assignment for Ad-Hoc Cooperation in Multi-Agent Deep Reinforcement Learning","H. Chen; G. Yang; J. Zhang; Q. Yin; K. Huang","School of Artificial Intelligence, University of Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","1","8","In recent years, reinforcement learning has faced several challenges in the multi-agent domain, such as the credit assignment issue. Value function factorization emerges as a promising way to handle the credit assignment issue under the centralized training with decentralized execution (CTDE) paradigm. However, existing value function factorization methods cannot deal with ad-hoc cooperation, that is, adapting to new configurations of teammates at test time. Specifically, these methods do not explicitly utilize the relationship between agents and cannot adapt to different sizes of inputs. To address these limitations, we propose a novel method, called Relation-Aware Credit Assignment (RACA), which achieves zero-shot generalization in ad-hoc cooperation scenarios. RACA takes advantage of a graph-based relation encoder to encode the topological structure between agents. Furthermore, RACA utilizes an attention-based observation abstraction mechanism that can generalize to an arbitrary number of teammates with a fixed number of parameters. Experiments demonstrate that our method outperforms baseline methods on the StarCraftII micromanagement benchmark and ad-hoc cooperation scenarios.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892225","National Natural Science Foundation of China(grant numbers:61876181); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892225","Multi-Agent System;Deep Reinforcement Learning;Ad-Hoc Cooperation","Training;Neural networks;Reinforcement learning;Benchmark testing;Ad hoc networks","deep learning (artificial intelligence);graph theory;multi-agent systems","RACA;multiagent deep reinforcement learning;decentralized execution paradigm;value function factorization methods;ad-hoc cooperation scenarios;graph-based relation encoder;relation-aware credit assignment;centralized training with decentralized execution paradigm;CTDE paradigm;zero-shot generalization;topological structure;attention-based observation abstraction mechanism;StarCraftII micromanagement benchmark","","","","43","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Probabilistic Reward-Based Reinforcement Learning for Multi-Agent Pursuit and Evasion","B. -K. Zhang; B. Hu; L. Chen; D. -X. Zhang; X. -M. Cheng; Z. -H. Guan","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Petroleum Engineering, Yangtze University, Jingzhou, China; School of Automation, Central South University, Changsha, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","2021 33rd Chinese Control and Decision Conference (CCDC)","30 Nov 2021","2021","","","3352","3357","The reinforcement learning is studied to solve the problem of multi-agent pursuit and evasion games in this article. The main problem of current reinforcement learning for multi-agents is the low learning efficiency of agents. An important factor leading to this problem is that the delay of the Q function is related to the environment changing. To solve this problem, a probabilistic distribution reward value is used to replace the Q function in the multi-agent depth deterministic policy gradient framework (hereinafter referred to as MADDPG). The distribution Bellman equation is proved to be convergent, and can be brought into the framework of reinforcement learning algorithm. The probabilistic distribution reward value is updated in the algorithm, so that the reward value can be more adaptive to the complex environment. In the same time, eliminating the delay of rewards improves the efficiency of the strategy and obtains a better pursuit-evasion results. The final simulation and experiment show that the multi-agent algorithm with distribution rewards achieves better results under the setting environment.","1948-9447","978-1-6654-4089-9","10.1109/CCDC52312.2021.9601771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9601771","Reinforcement learning;Multi-agent;Pursuit-evasion;Probabilistic reward","Adaptation models;Reinforcement learning;Games;Probabilistic logic;Mathematical models;Delays","gradient methods;learning (artificial intelligence);multi-agent systems;statistical distributions","probabilistic reward-based reinforcement learning;multiagent pursuit;evasion games;low learning efficiency;Q function;probabilistic distribution reward value;multiagent depth deterministic policy gradient framework;distribution Bellman equation;reinforcement learning algorithm;multiagent algorithm;distribution rewards;MADDPG;complex environment","","","","21","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"False Data-Injection Attack Detection in Cyber–Physical Systems With Unknown Parameters: A Deep Reinforcement Learning Approach","K. Liu; H. Zhang; Y. Zhang; C. Sun","School of Automation and the Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation and the Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, China; School of Automation and the Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, China","IEEE Transactions on Cybernetics","17 Oct 2023","2023","53","11","7115","7125","This article studies the detection of discontinuous false data-injection (FDI) attacks on cyber–physical systems (CPSs). Considering the unknown stochastic properties of the process noise and measurement noise, deep reinforcement learning is applied to designing an FDI attack detector. First, the discontinuous attack detection problem is modeled as a partially observable Markov decision process (POMDP) and a neural network is used to explore the POMDP. In the network, sliding observation windows which are composed of the offline fragment historical data are used as the input. An approach to designing the reward in POMDP is provided to ensure the precision of the detection when there are even some state recognition errors. Second, sufficient conditions on attack frequency and duration to guarantee the applicability of the detector and the expected estimation performance are further given. Finally, simulation examples illustrate the effectiveness of the attack detector.","2168-2275","","10.1109/TCYB.2022.3225236","National Key Research and Development Program of China(grant numbers:2021ZD0112700); National Natural Science Foundation (NNSF) of China(grant numbers:61973082,62233003); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20202006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9983529","Attack detection;cyber–physical systems (CPSs);deep reinforcement learning;false data-injection (FDI) attacks;partially observable Markov decision process (POMDP)","Detectors;Deep learning;Reinforcement learning;Noise measurement;Markov processes;Cyberattack;Covariance matrices","","","","","","40","IEEE","13 Dec 2022","","","IEEE","IEEE Journals"
"Time-sequence Action-Decision and Navigation Through Stage Deep Reinforcement Learning in Complex Dynamic Environments","H. Wang; Z. Pu; J. Yi; T. Qiu; Z. Liu; Z. Liu","University of Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing, China; Institute of Automation, University of Chinese Academy of Sciences, Beijing, China; Institute of Automation, University of Chinese Academy of Sciences, Beijing, China; Institute of Automation, University of Chinese Academy of Sciences, Beijing, China; Institute of Automation, University of Chinese Academy of Sciences, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","2019 IEEE Symposium Series on Computational Intelligence (SSCI)","20 Feb 2020","2019","","","22","30","Navigation in a complex dynamic environment is one of the most attractive tasks. Although most of such algorithms can achieve navigation tasks effectively, they ignore the necessity of the mission planning in the process of navigation. Given the situation, a novel end-to-end two-stage deep reinforcement learning architecture for a time-sequence navigation and action-decision in a dynamic environment with randomly rapidly moving obstacles is proposed in this paper. During the first-stage training, a network with spatial and temporal information is designed to process the navigation task while a conventional recurrent full-connected network is adopted to resolve the action-decision task. During the second-stage training, the two networks are integrated and trained online with dynamic entropy to obtain a stable policy for dynamic missions. Simulations demonstrate that the navigation and the action-decision in different environments can be completed effectively under our architecture.","","978-1-7281-2485-8","10.1109/SSCI44817.2019.9003086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003086","navigation;action-decision;stage deep reinforcement learning","Robots;Navigation;Task analysis;Training;Tools;Fires;Machine learning","control engineering computing;entropy;learning (artificial intelligence);mobile robots;navigation;path planning;recurrent neural nets","action-decision task;second-stage training;dynamic entropy;dynamic missions;time-sequence action-decision;complex dynamic environment;time-sequence navigation;end-to-end two-stage deep reinforcement learning architecture;randomly rapidly moving obstacles;recurrent full-connected network","","","","19","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"Improving the Ability of Robots to Navigate Through Crowded Environments Safely using Deep Reinforcement Learning","Q. Shan; W. Wang; D. Guo; X. Sun; L. Jia","Research Center for Brain-inspired Intelligence, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Research Center for Brain-inspired Intelligence, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Research Center for Brain-inspired Intelligence, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Research Center for Brain-inspired Intelligence, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Research Center for Brain-inspired Intelligence, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2022 International Conference on Advanced Robotics and Mechatronics (ICARM)","29 Nov 2022","2022","","","575","580","Autonomous robot navigation in unpredictable and crowded environments requires a guarantee of safety and a stronger ability to pass through a narrow passage. However, it’s challenging to plan safe, dynamically-feasible trajectories in real-time. Previous approaches, such as Reachability-based Trajectory Design (RTD), focus on safety guarantee, but the lack of online strategy always makes the robot fail to pass through a narrow passage. This paper proposes to learn a policy that guides the robot to make successful plans using deep Reinforcement Learning (RL). We train a deep network based on the RTD method to create cost functions in real-time. The created cost function is expected to help the online planner optimize the robot’s feasible trajectory, satisfying its kino-dynamics model and collision avoidance constraints. In crowded simulated environments, our approach substantially improves the planning success rate compared to RTD and some other methods.","","978-1-6654-8306-3","10.1109/ICARM54641.2022.9959459","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9959459","","Deep learning;Mechatronics;Navigation;Reinforcement learning;Cost function;Real-time systems;Trajectory","collision avoidance;learning (artificial intelligence);mobile robots;path planning;robot dynamics","autonomous robot navigation;collision avoidance constraints;cost function;crowded environments safely;crowded simulated environments;deep network;deep Reinforcement Learning;dynamically-feasible trajectories;feasible trajectory;kino-dynamics model;narrow passage;online planner;online strategy;planning success rate;Reachability-based Trajectory Design;RTD method;safety guarantee;stronger ability;successful plans;unpredictable environments","","","","27","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"Carrier Aircraft Landing Control Technology Based on Deep Reinforcement Learning","R. Liu; J. Jiang; X. Liu; H. Sun; T. Ma","College Of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College Of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College Of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College Of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College Of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","2023 6th International Symposium on Autonomous Systems (ISAS)","3 Jul 2023","2023","","","1","6","In this paper, a pitching control method based on Deep Deterministic Policy Gradient (DDPG) algorithm for carrier aircraft landing and descending stage is studied. DDPG controller takes pitch angle rate error, pitch angle error and altitude error as input, and output as elevator deflection, realizing the rapid pitch angle response of carrier-aircraft under different landing states. Compared with traditional PID controller, network training of Actor-Critic for DDPG attitude controller greatly improves the calculation efficiency of control quantity, and reduces the difficulty of parameter optimization. The simulation experiment in this paper was based on the F/A-18 aircraft aerodynamics model constructed in Matlab/Simulink, and the intensive learning and training environment built on PyCharm platform was used to realize the data interaction between the two platforms through UDP communication. The simulation results show that the attitude controller based on reinforcement learning designed in this paper has the characteristics of fast response speed and small dynamic error, and meets the control accuracy requirements in the experiment.","","979-8-3503-1615-5","10.1109/ISAS59543.2023.10164523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10164523","Carrier aircraft landing;DDPG algorithm;Pitch attitude controller;UDP network communication;PyCharm","Training;Attitude control;Heuristic algorithms;Atmospheric modeling;Simulation;Reinforcement learning;Aerodynamics","aerodynamics;aircraft;aircraft control;attitude control;control engineering computing;control system synthesis;deep learning (artificial intelligence);gradient methods;learning (artificial intelligence);optimisation;pitch control (position);reinforcement learning;three-term control","altitude error;carrier aircraft landing control technology;carrier-aircraft;control accuracy requirements;control quantity;DDPG attitude controller;DDPG controller;Deep Deterministic Policy Gradient algorithm;Deep reinforcement learning;descending stage;different landing states;dynamic error;elevator deflection;intensive learning training environment;network training;pitch angle error;pitch angle rate error;pitching control method;rapid pitch angle response","","","","9","IEEE","3 Jul 2023","","","IEEE","IEEE Conferences"
"Privacy-preserving incentive mechanism for platoon assisted vehicular edge computing with deep reinforcement learning","X. Huang; Y. Zhong; Y. Wu; P. Li; R. Yu","School of Automation, University of Macau, Macau, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, University of Macau, Macau, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China","China Communications","22 Jul 2022","2022","19","7","294","309","Platoon assisted vehicular edge computing has been envisioned as a promising paradigm of implementing offloading services through platoon cooperation. In a platoon, a vehicle could play as a requester that employs another vehicles as performers for workload processing. An incentive mechanism is necessitated to stimulate the performers and enable decentralized decision making, which avoids the information collection from the performers and preserves their privacy. We model the interactions among the requester (leader) and multiple performers (followers) as a Stackelberg game. The requester incentivizes the performers to accept the workloads. We derive the Stackelberg equilibrium under complete information. Furthermore, deep reinforcement learning is proposed to tackle the incentive problem while keeping the performers' information private. Each game player becomes an agent that learns the optimal strategy by referring to the historical strategies of the others. Finally, numerical results are provided to demonstrate the effectiveness and efficiency of our scheme.","1673-5447","","10.23919/JCC.2022.07.022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837865","vehicular edge computing;Stackelberg game;deep reinforcement learning","Games;Task analysis;Costs;Privacy;Vehicle dynamics;Sensors;Resource management","budgeting data processing;data privacy;decision making;game theory;incentive schemes;learning (artificial intelligence);mobile computing","privacy-preserving incentive mechanism;platoon assisted vehicular edge;deep reinforcement learning;platoon cooperation;requester;decentralized decision making;preserves their privacy;multiple performers;incentive problem","","","","","","22 Jul 2022","","","IEEE","IEEE Magazines"
"Active Power Correction Strategies Based on Deep Reinforcement Learning—Part II: A Distributed Solution for Adaptability","S. Chen; J. Duan; Y. Bai; J. Zhang; D. Shi; Z. Wang; X. Dong; Y. Sun","School of Electrical Engineering and Automation Wuhan University, Wuhan, China; GEIRI North America, San Jose, CA, USA; School of Electrical Engineering and Automation Wuhan University, Wuhan, China; School of Electrical Engineering and Automation Wuhan University, Wuhan, China; GEIRI North America, San Jose, CA, USA; GEIRI North America, San Jose, CA, USA; School of Electrical Engineering and Automation Wuhan University, Wuhan, China; School of Electrical Engineering and Automation Wuhan University, Wuhan, China","CSEE Journal of Power and Energy Systems","25 Jul 2022","2022","8","4","1134","1144","This article is the second part of Active Power Correction Strategies Based on Deep Reinforcement Learning. In Part II, we consider the renewable energy scenarios plugged into the large-scale power grid and provide an adaptive algorithmic implementation to maintain power grid stability. Based on the robustness method in Part I, a distributed deep reinforcement learning method is proposed to overcome the influence of the increasing renewable energy penetration. A multi-agent system is implemented in multiple control areas of the power system, which conducts a fully cooperative stochastic game. Based on the Monte Carlo tree search mentioned in Part I, we select practical actions in each sub-control area to search the Nash equilibrium of the game. Based on the QMIX method, a structure of offline centralized training and online distributed execution is proposed to employ better practical actions in the active power correction control. Our proposed method is evaluated in the modified global competition scenario cases of “2020 Learning to Run a Power Network - Neurips Track 2”.","2096-0042","","10.17775/CSEEJPES.2020.07070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9535395","Active power correction strategies;distributed deep reinforcement learning;Nash equilibrium;renewable energies;stochastic game","Training;Topology;Switches;Reinforcement learning;Games;Voltage control;Renewable energy sources","game theory;learning (artificial intelligence);Monte Carlo methods;multi-agent systems;power engineering computing;power grids;power system control;power system stability;reinforcement learning;stochastic games;stochastic processes;tree searching","renewable energy scenarios;large-scale power grid;adaptive algorithmic implementation;active power correction strategies;deep reinforcement learning;power system control;cooperative stochastic game;Nash equilibrium;QMIX method;Monte Carlo tree search","","","","22","","10 Sep 2021","","","CSEE","CSEE Journals"
"Optimal Lateral Path-Tracking Control of Vehicles With Partial Unknown Dynamics Via DPG-Based Reinforcement Learning Methods","X. Shi; Y. Li; W. Hu; C. Du; C. Chen; W. Gui","School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Information and Electrical Engineering, Hunan University of Science and Technology, Xiangtan, China; School of Automation, Central South University, Changsha, China","IEEE Transactions on Intelligent Vehicles","","2023","PP","99","1","10","This article focuses on the optimal lateral path-tracking control problem of vehicles with unknown drift dynamics in a model-free manner through two novel deterministic policy gradient (DPG) based reinforcement learning (RL) methods. First, due to the difficulty of modeling the precise dynamics of vehicles, a policy gradient (PG) is derived to learn the optimal control gain by minimizing a predefined infinite-horizon performance index, where the knowledge of the system drift dynamics of vehicles is no longer needed. Then, to further remove the limitation of the initial admissibility of the control policy, a two-stage DPG-based RL optimal control algorithm is proposed, in which a novel finite-horizon performance index is employed in the pre-learning stage such that the control gain does not require to be initially admissible. It should be pointed out that the derived PGs in the two algorithms are based on an explicit form only using a single sampling data for each calculation rather than an estimated form via randomly perturbing feedback gains, which reduces the sampling and computational complexity of the algorithms. Finally, the simulations of the lateral path-tracking control of vehicles have verified the effectiveness and superiority of the proposed DPG-based RL algorithms compared with existing methods.","2379-8904","","10.1109/TIV.2023.3319642","National Natural Science Foundation of China(grant numbers:61977019,U1813206,62303492); Shenzhen Basic Research Program(grant numbers:JCYJ20220818102415033,JSGG20201103093802006); International Cooperation and Exchange of the National Natural Science Foundation of China(grant numbers:61860206014); 111 Project of China(grant numbers:B17048); China Postdoctoral Science Foundation(grant numbers:2023M733940); Science and Technology Innovation Program of Hunan Province(grant numbers:2022WZ1001); Natural Science Foundation of Hunan Province(grant numbers:2023JJ40765); Natural Science Foundation of Changsha(grant numbers:kq2208287); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10265145","Optimal lateral path-tracking control;unknown drift dynamics;deterministic policy gradient;reinforcement learning;two-stage learning","Heuristic algorithms;Optimal control;Vehicle dynamics;Computational modeling;Symmetric matrices;Safety;Performance analysis","","","","","","","IEEE","27 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Shared Autonomy Based on Human-in-the-loop Reinforcement Learning with Policy Constraints","M. Li; Y. Kang; Y. -B. Zhao; J. Zhu; S. You","Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","7349","7354","In shared autonomous systems, humans and agents cooperate to complete tasks. Since reinforcement learning enables agents to obtain good policies through trial and error without knowing the dynamic model of the environment, it has been well applied in shared autonomous systems. After inferring the target from human inputs, agents trained by RL can accurately act accordingly. However, existing methods of this kind have big problems: the training of reinforcement learning algorithms require lots of exploration, which is time-consuming, lack of security guarantee and likely to cause great losses in the training process. Moreover, most of shared control methods are human-oriented, and do not consider the situation that humans may make wrong actions. In view of the above problems, this paper proposes human-in-the-loop reinforcement learning with policy constraints. In the training process, human prior knowledge is used to constrain the exploration of agents to achieve fast and efficient learning. In the process of testing we incorporate policy constraints in the arbitration to avoid serious consequences caused by human mistakes.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902295","National Key Research and Development Program of China(grant numbers:2018AAA0100800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902295","Shared Autonomy;Reinforcement Learning;Human-in-the-loop;Policy Constraints","Training;Autonomous systems;System performance;Reinforcement learning;Human in the loop;Stability analysis;Safety","multi-agent systems;reinforcement learning","dynamic model;human mistakes;human prior knowledge;human-in-the-loop reinforcement learning;policy constraints;reinforcement learning algorithms;shared autonomous systems;shared control methods;training process","","","","30","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Model-Free Solution to the Discrete-Time Coupled Riccati Equation Using Off-Policy Reinforcement Learning","L. Li; L. Wang; Y. Yang; J. Dong; Y. Yin; S. Cheng","Key Laboratory of Knowledge Automation for Industrial Processes of Ministry of Education, University of Science and Technology Beijing, Beijing, China; Key Laboratory of Knowledge Automation for Industrial Processes of Ministry of Education, University of Science and Technology Beijing, Beijing, China; Key Laboratory of Knowledge Automation for Industrial Processes of Ministry of Education, University of Science and Technology Beijing, Beijing, China; Key Laboratory of Knowledge Automation for Industrial Processes of Ministry of Education, University of Science and Technology Beijing, Beijing, China; Key Laboratory of Knowledge Automation for Industrial Processes of Ministry of Education, University of Science and Technology Beijing, Beijing, China; School of Metallurgical and Ecological Engineering, University of Science and Technology Beijing, Beijing, China","2019 Chinese Control Conference (CCC)","17 Oct 2019","2019","","","6813","6818","In this paper, in order to solve the two-player nonzero-sum (NZS) differential games with completely unknown linear discrete-time dynamics, we develop a data-driven algorithm to learn the Nash equilibrium based on off-policy reinforcement learning (RL). This algorithm is a fully model-free method, which solves the couple algebraic Riccati equations (CAREs) forward in time using measured data along the system trajectories. It is shown that the two-player NZS differential games results in solving the CAREs. Then, model-based on-policy and model-free off-policy RL algorithms are presented to solve the CAREs. Compared to the on-policy RL, the off-policy RL algorithm can eliminate the influence of probing noise to guarantee unbiased solutions. Finally, a simulation example is carried out to show the efficacy of the proposed approach.","1934-1768","978-9-8815-6397-2","10.23919/ChiCC.2019.8865951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8865951","couple algebraic Riccati equations;off-policy reinforcement learning;nonzero-sum games;model-free method","Games;Heuristic algorithms;Mathematical model;Nash equilibrium;Biological system modeling;Riccati equations;Reinforcement learning","differential games;discrete time systems;learning (artificial intelligence);linear systems;Riccati equations","model-free solution;off-policy reinforcement learning;two-player nonzero-sum differential games;data-driven algorithm;discrete-time coupled Riccati equation;unknown linear discrete-time dynamics;game theory","","","","20","","17 Oct 2019","","","IEEE","IEEE Conferences"
"Energy Scheduling Strategy of Ice Storage Air Conditioning System Based on Deep Reinforcement Learning","D. Wan; M. Chi; Q. Peng; Y. Yu; Z. -W. Liu","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, PR China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, PR China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, PR China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, PR China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, PR China","2021 4th IEEE International Conference on Industrial Cyber-Physical Systems (ICPS)","5 Jul 2021","2021","","","846","851","The energy consumption of buildings accounts for about one third of total energy consumption of our society, and the energy consumption of ice storage air conditioning system accounts for about half of energy consumption of buildings. Therefore, effective energy scheduling strategy of ice storage air conditioning system is of great significance to energy saving and energy cost reduction. In this paper, we propose a method to intelligently learn energy scheduling strategy of ice storage air conditioning system by using deep reinforcement learning technology. This method does not need to establish a building dynamics model, and directly uses temperature and other data for learning. Compared with the traditional rule-based and model-based methods, the efficiency is greatly improved.","","978-1-7281-6207-2","10.1109/ICPS49255.2021.9468165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468165","Deep reinforcement learning;ice storage air conditioning system;energy scheduling strategy","Air conditioning;Energy consumption;Temperature distribution;Job shop scheduling;Atmospheric modeling;Heuristic algorithms;Buildings","air conditioning;cost reduction;deep learning (artificial intelligence);energy conservation;energy consumption;power engineering computing","total energy consumption;ice storage air conditioning system accounts;effective energy scheduling strategy;energy saving;energy cost reduction;deep reinforcement learning technology;building dynamics model;model-based methods;traditional rule-based methods","","","","16","IEEE","5 Jul 2021","","","IEEE","IEEE Conferences"
"Cooperative Multi-Agent Reinforcement Learning with Hypergraph Convolution","Y. Bai; C. Gong; B. Zhang; G. Fan; X. Hou; Y. Lu","Fusion Innovation Center, Institute of Automation, Chinese Academy of Sciences; Comprehensive information system research Center, Institute of Automation, Chinese Academy of Sciences; Fusion Innovation Center, Institute of Automation, Chinese Academy of Sciences; Fusion Innovation Center, Institute of Automation, Chinese Academy of Sciences; Comprehensive information system research Center, Institute of Automation, Chinese Academy of Sciences; Comprehensive information system research Center, Institute of Automation, Chinese Academy of Sciences","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","1","8","Recent years have witnessed the great success of multi-agent systems (MAS). Value decomposition, which decom-poses joint action values into individual action values, has been an important work in MAS. However, many value decomposition methods ignore the coordination among different agents, leading to the notorious “lazy agents” problem. To enhance the coordination in MAS, this paper proposes HyperGraph CoNvo-lution MIX (HGCN-MIX), a method that incorporates hyper-graph convolution with value decomposition. HGCN-MIX models agents as well as their relationships as a hypergraph, where agents are nodes and hyperedges among nodes indicate that the corresponding agents can coordinate to achieve larger rewards. Then, it trains a hypergraph that can capture the collaborative relationships among agents. Leveraging the learned hypergraph to consider how other agents' observations and actions affect their decisions, the agents in a MAS can better coordinate. We evaluate HGCN-MIX in the StarCraft II multi-agent challenge benchmark. The experimental results demonstrate that HGCN-MIX can train joint policies that outperform or achieve a similar level of performance as the current state-of-the-art techniques. We also observe that HGCN-MIX has an even more significant improvement of performance in the scenarios with a large amount of agents. Besides, we conduct additional analysis to emphasize that when the hypergraph learns more relationships, HGCN-MIX can train stronger joint policies.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9891942","NATIONAL DEFENSE BASIC SCIENTIFIC RESEARCH PROGRAM(grant numbers:JCKY2019203C029,JCKY2019-207B022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9891942","Hypergraph Convolution;Multi-Agent Rein-forcement Learning;Coordination and Control","Convolution;Neural networks;Collaboration;Reinforcement learning;Benchmark testing;Multi-agent systems","computer games;graph theory;learning (artificial intelligence);mobile radio;multi-agent systems","multiagent reinforcement learning;hypergraph convolution;multiagent systems;MAS;joint action;individual action values;value decomposition methods;notorious lazy agents problem;HyperGraph CoNvo-lution MIX;hyper-graph convolution;HGCN-MIX models agents;corresponding agents;learned hypergraph;StarCraft II multiagent challenge benchmark","","","","41","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"NeuronsMAE: A Novel Multi-Agent Reinforcement Learning Environment for Cooperative and Competitive Multi-Robot Tasks","G. Hu; H. Li; S. Liu; Y. Zhu; D. Zhao","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems Institute of Automation, Chinese Academy of Sciences, Beijing, China","2023 International Joint Conference on Neural Networks (IJCNN)","2 Aug 2023","2023","","","1","8","Multi-agent reinforcement learning (MARL) has achieved remarkable success in various challenging problems. Meanwhile, more and more benchmarks have emerged and provided some standards to evaluate the algorithms in different fields. On the one hand, the virtual MARL environments lack knowledge of real-world tasks and actuator abilities. On the other hand, the current task-specified multi-robot platform has poor support for the universality of multi-agent reinforcement learning algorithms and lacks support for transferring from simulation to the real environment. Bridging the gap between the virtual MARL environments and the real multi-robot platform becomes the key to promoting the practicability of MARL algorithms. This paper proposes a novel MARL environment for real multi-robot tasks named NeuronsMAE (Neurons Multi-Agent Environment). This environment supports cooperative and competitive multi-robot tasks and is configured with rich parameter interfaces to study the multi-agent policy transfer from simulation to reality. With this platform, we evaluate various popular MARL algorithms and build a new MARL benchmark for multi-robot tasks. We hope that this platform will facilitate the research and application of MARL algorithms for real robot tasks. Information about the benchmark and the open-source code are released at https://github.com/DRL-CASIA/NeuronsMAE.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10191291","National Natural Science Foundation of China (NSFC)(grant numbers:62103409,62136008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191291","multi-agent reinforcement learning;benchamark;multi-robot","Actuators;Codes;Neurons;Reinforcement learning;Benchmark testing;Task analysis;Observability","deep learning (artificial intelligence);learning (artificial intelligence);multi-agent systems;multi-robot systems;reinforcement learning","competitive multirobot tasks;current task-specified multirobot platform;MARL benchmark;multiagent policy transfer;Neurons MultiAgent Environment;NeuronsMAE;novel MARL environment;novel MultiAgent reinforcement learning Environment;popular MARL algorithms;real-world tasks;robot tasks;virtual MARL environments","","","","35","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Offline Reinforcement Learning for Autonomous Driving with Real World Driving Data","X. Fang; Q. Zhang; Y. Gao; D. Zhao","The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)","1 Nov 2022","2022","","","3417","3422","Since traditional reinforcement learning (RL) approaches need active online interaction with the environment, previous works are mainly investigated in the simulation environment rather than the real world environment, especially for safety-critical applications. Offline RL has recently emerged as a promising data-driven learning paradigm to learn a policy from offline dataset directly. It seems that offline RL is well suited for autonomous driving, as it is feasible to collect offline naturalized driving dataset. However, it remains unclear how to deploy offline RL with real world driving dataset only including observation data, and whether current offline RL algorithms work well to learn a driving policy than imitation learning? In this paper, we provide an offline RL benchmark for autonomous driving including the dataset, baselines, and a data driven simulator11Code: https://github.com/weiaiF/offiineRL-INTERACTION. First, we summarize and introduce the popular offline RL baseline methods. Then, we construct an offline RL dataset for the car following task based on the real world driving dataset INTERACTION. A data driven simulator is applied to obtain augmented data and test the driving policy. Further, we deploy four popular offline algorithms and analyze their performances under different datasets including real world driving data and augmented data. Finally, related conclusions and discussions are given to analyze the critical challenge for offline RL in autonomous driving.","","978-1-6654-6880-0","10.1109/ITSC55140.2022.9922100","National Natural Science Foundation of China (NSFC)(grant numbers:62173325); Beijing Municipal Natural Science Foundation(grant numbers:L191002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9922100","","Training;Uncertainty;Estimation;Reinforcement learning;Benchmark testing;Trajectory;Automobiles","automotive engineering;interactive systems;reinforcement learning;safety-critical software","imitation learning;offline RL benchmark;autonomous driving;offline RL dataset;dataset INTERACTION;driving policy;real world driving data;offline reinforcement learning;data-driven learning paradigm;offline RL algorithms;data driven simulator;safety-critical applications;active online interaction","","","","32","IEEE","1 Nov 2022","","","IEEE","IEEE Conferences"
"Look-Ahead Unit Commitment with Adaptive Horizon Based on Deep Reinforcement Learning","J. Yan; Y. Li; J. Yao; S. Yang; F. Li; K. Zhu","Department of Power Automation, China Electric Power Research Institute, Nanjing, Jiangsu, China; Department of Power Automation, China Electric Power Research Institute, Nanjing, Jiangsu, China; Department of Power Automation, China Electric Power Research Institute, Nanjing, Jiangsu, China; Department of Power Automation, China Electric Power Research Institute, Nanjing, Jiangsu, China; Department of Power Automation, China Electric Power Research Institute, Nanjing, Jiangsu, China; Department of Power Automation, China Electric Power Research Institute, Nanjing, Jiangsu, China","IEEE Transactions on Power Systems","","2023","PP","99","1","12","The highly variable nature of renewable energy has led to the concept of intra-day look-ahead unit commitment (LAUC), which aims to determine the on/off status and power outputs of generating units in a rolling-horizon fashion. LAUC is traditionally performed based on look-ahead horizon (LAH) with fixed length and resolution. Such practice can neither capture the high-risk time periods, nor achieve maximum computational efficiency. To address these issues, this paper proposes a LAUC method with adaptive horizon (LAUC-AH). The method uses three parameters to describe the shape of LAH, namely length, resolution, and myopia. Taking these parameters, the forecast profile of renewable energy and load demand is aggregated using a hierarchical clustering procedure to form the LAH, which is then used to construct the LAUC optimization model. Furthermore, a deep reinforcement learning-based agent is used to dynamically adjust the parameters of LAH, such that the LAUC model can adapt to different operation statuses of the power grid. The case studies are carried out on a modified IEEE-118 test case to validate the feasibility and effectiveness of the proposed method.","1558-0679","","10.1109/TPWRS.2023.3286094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10151949","renewable energy integration;look-ahead unit commitment;adaptive horizon;reinforcement learning","Renewable energy sources;Shape;Energy resolution;Adaptation models;Uncertainty;Computational modeling;Load modeling","","","","","","","IEEE","14 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Scheduling of Dual-Gripper Robotic Cells With Reinforcement Learning","H. -J. Kim; J. -H. Lee","Faculty of Department of Industrial and Systems Engineering, KAIST (Korea Advanced Institute of Science and Technology), Daejeon, Republic of Korea; Faculty of School of Business, Chungnam National University, Daejeon, Republic of Korea","IEEE Transactions on Automation Science and Engineering","6 Apr 2022","2022","19","2","1120","1136","A dual-gripper robotic cell consists of multiple processing machines and one material handling robot, which can perform an unloading or a loading task one at a time but can hold two parts at the same time. We address a scheduling problem of the robotic cell that determines a robot task sequence when two part types are processed in a different set of machines and all machines have variable processing times within a given interval. The objective is to minimize the makespan. This study proposes a learning-based method, i.e., a reinforcement learning (RL) approach, for the first time, to address a dual-gripper robotic cell scheduling problem. The problem is modeled with a Petri net, a graphical and mathematical modeling tool, which is used as an environment in RL. The states, actions, and rewards are defined by using flow shop scheduling properties, features from a Petri net, and knowledge from previous studies of scheduling robotized tools. Then, the RL approach is compared to the first-in-first-out (FIFO) rule, which is generally used in practice, a swap sequence, which is widely used for cyclic scheduling of dual-gripper robotic cells, and a lower bound. The extensive experiments show that the proposed method performs better than FIFO and the swap sequence; moreover, the gap between the makespan of the proposed method and the lower bound is not large. Note to Practitioners—We address a scheduling problem of dual-gripper robotic cells with two-part types when all machines have processing time variations. We propose an RL approach to obtain an efficient robot task sequence in order to minimize makespan. The proposed method is performed offline, and a robot task sequence is then obtained instantaneously. The proposed method performs better than the FIFO rule used in practice and the swap sequence used for cyclic scheduling of robotic cells. It can be easily extended for scheduling other configurations of robotic cells.","1558-3783","","10.1109/TASE.2020.3047924","National Research Foundation of Korea (NRF) Grant funded through the Korean Government (MSIT)(grant numbers:2019R1C1C1004667); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9325546","Dual-gripper robotic cell;reinforcement learning (RL);scheduling;time variations","Robots;Job shop scheduling;Tools;Task analysis;Manufacturing;Service robots;Mathematical model","flow shop scheduling;grippers;industrial robots;minimisation;Petri nets;reinforcement learning","swap sequence;cyclic scheduling;material handling robot;reinforcement learning approach;dual-gripper robotic cell scheduling problem;RL;Petri net;mathematical graphic modeling tool;flow shop scheduling properties;first-in-first-out rule;FIFO;lower bound;robot task sequence","","7","","54","IEEE","15 Jan 2021","","","IEEE","IEEE Journals"
"Federated Deep Reinforcement Learning for Task Scheduling in Heterogeneous Autonomous Robotic System","T. M. Ho; K. -K. Nguyen; M. Cheriet","Synchromedia Laboratory, École de Technologie Supérieure, Université du Québec, Montreal, Canada; Synchromedia Laboratory, École de Technologie Supérieure, Université du Québec, Montreal, Canada; Synchromedia Laboratory, École de Technologie Supérieure, Université du Québec, Montreal, Canada","IEEE Transactions on Automation Science and Engineering","","2022","PP","99","1","13","Autonomous robotics play a central role in smart logistics where robots can replace or aid humans in all kinds of tasks, such as items picking, moving, and storing. In this paper, we investigate the problem of task scheduling in automated warehouses with heterogeneous autonomous robotic (HAR) systems. We formulate a long-term non-convex queueing control optimization problem to minimize the queue length of tasks to be processed in the warehouse. Traditional task scheduling solutions based on optimization approaches are inefficient in handling the stochastic nature of the goods/tasks flow and a large number of robots in the system due to their computational cost. We propose a deep reinforcement learning (DRL) based task scheduling algorithm that employs the proximal policy optimization (PPO) method to find an optimal task scheduling policy. Due to the heterogeneity of the system, we propose a proximal weighted federated learning-based algorithm for implementing a decentralized PPO algorithm that improves the performance of the distributed PPO agents that are deployed in the workstations at the geographically distributed warehouses. The simulation results demonstrate the performance improvement of our proposed algorithm compared to the existing methods. Note to Practitioners—Task scheduling for robotic swarms in smart warehouses is substantial for e-commerce. State-of-the-art solutions have focused on efficient task scheduling for homogeneous robotic systems using machine learning techniques implemented in the warehouse management systems (WMS). However, task scheduling for a heterogeneous autonomous robotic (HAR) system has not fully been investigated so far. This article provides a comprehensive task scheduling algorithm for HAR systems that leverages innovative deep reinforcement learning and federated learning techniques. The proposed algorithm can be deployed in the geographically distributed warehouses of an e-commerce company and easily integrated into the WMS to optimally control the operation of the HAR system with stochastic goods/tasks flows in the smart warehousing.","1558-3783","","10.1109/TASE.2022.3221352","Mitacs; Ciena; Evolution of Networked Services through a Corridor in Québec and Ontario for Research and Innovation (ENCQOR)(grant numbers:IT13947); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950249","Smart warehousing;industrial automation;federated learning;deep reinforcement learning;heterogeneous autonomous robotic","Robots;Task analysis;Robot kinematics;Training;Optimization;Workstations;Reinforcement learning","","","","4","","","IEEE","14 Nov 2022","","","IEEE","IEEE Early Access Articles"
"Cat-Like Jumping and Landing of Legged Robots in Low Gravity Using Deep Reinforcement Learning","N. Rudin; H. Kolvenbach; V. Tsounis; M. Hutter","Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland","IEEE Transactions on Robotics","7 Feb 2022","2022","38","1","317","328","In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.","1941-0468","","10.1109/TRO.2021.3084374","European Space Agency; Network Partnering Initiative(grant numbers:481-2016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9453856","Deep learning in robotics and automation;legged robots;space robotics and automation","Robots;Legged locomotion;Task analysis;Actuators;Gravity;Training;Kinematics","attitude control;learning (artificial intelligence);legged locomotion;motion control;robot dynamics;zero gravity experiments","legged locomotion control tasks;extensive flight phases;space exploration;off-the-shelf deep reinforcement learning algorithm;neural network;jumping quadruped robot;attitude control;landing locomotion;low-gravity celestial bodies;deploy trained policies;SpaceBok robot;repetitive controlled jumping;legged robots;low gravity;learned policies","","25","","36","IEEE","14 Jun 2021","","","IEEE","IEEE Journals"
"Deep-Reinforcement-Learning-Based Predictive Maintenance Model for Effective Resource Management in Industrial IoT","K. S. H. Ong; W. Wang; D. Niyato; T. Friedrichs","School of Computer Science and Engineering, Nanyang Technological University, Singapore; Faculty of Engineering, Bar Ilan University, Ramat Gan, Israel; School of Computer Science and Engineering, Nanyang Technological University, Singapore; IT Strategy and Innovation Asia Pacific, Robert Bosch (SEA) Pte Ltd., Singapore","IEEE Internet of Things Journal","24 Mar 2022","2022","9","7","5173","5188","Unplanned breakdown of critical equipment interrupts production throughput in Industrial IoT (IIoT), and data-driven predictive maintenance (PdM) becomes increasingly important for companies seeking a competitive business advantage. Manufacturers, however, are constantly faced with the onerous challenge of manually allocating suitably competent manpower resources in the event of an unexpected machine breakdown. Furthermore, human error has a negative rippling impact on both overall equipment downtime and production schedules. In this article, we formulate the complex resource management problem as a resource optimization problem to determine if a model-free deep reinforcement learning (DRL)-based PdM framework can be used to automatically learn an optimal decision policy from a stochastic environment. Unlike the existing PdM frameworks, our approach considers PdM sensor information and the resources of both physical equipment and human as part of the optimization problem. The proposed DRL-based framework and proximal policy optimization long short term memory (PPO-LSTM) model are evaluated alongside baselines results from human participants using a maintenance repair simulator. Empirical results indicate that our PPO-LSTM efficiently learns the optimal decision-policy for the resource management problem, outperforming comparable DRL methods and human participants by 53% and 65%, respectively. Overall, the simulation results corroborate the proposed DRL-based PdM framework’s superiority in terms of convergence efficiency, simulation performance, and flexibility.","2327-4662","","10.1109/JIOT.2021.3109955","Collaboration Program between Computer Networks and Communications Lab, Nanyang Technological University of Singapore and Robert Bosch (SEA) Pte Ltd.; Alibaba Group through Alibaba Innovative Research (AIR) Program and Alibaba-NTU Singapore Joint Research Institute (JRI), National Research Foundation, Singapore, through AI Singapore Programme (AISG)(grant numbers:AISG-GC-2019-003); WASP/NTU(grant numbers:M4082187 (4080)); Singapore Ministry of Education (MOE) Tier 1(grant numbers:RG16/20); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9528837","Decision-support systems;deep reinforcement learning (DRL);Industrial Internet of Things (IIoT);predictive maintenance (PdM);resource management","Maintenance engineering;Resource management;Predictive maintenance;Optimization;Industrial Internet of Things;Data models;Task analysis","decision making;deep learning (artificial intelligence);Internet of Things;maintenance engineering;optimisation;production engineering computing;production equipment;resource allocation","deep-reinforcement-learning-based predictive maintenance model;effective resource management;data-driven predictive maintenance;unexpected machine breakdown;complex resource management problem;resource optimization problem;optimal decision policy;PdM sensor information;proximal policy optimization;maintenance repair simulator;DRL-based PdM framework;convergence efficiency","","11","","42","IEEE","3 Sep 2021","","","IEEE","IEEE Journals"
"Invariant Transform Experience Replay: Data Augmentation for Deep Reinforcement Learning","Y. Lin; J. Huang; M. Zimmer; Y. Guan; J. Rojas; P. Weng","School of Electromechanical Engineering, Guangdong University of Technology, Guangzhou, China; School of Electromechanical Engineering, Guangdong University of Technology, Guangzhou, China; UM-SJTU Joint Institute, Shanghai Jiao Tong University, Shanghai, China; School of Electromechanical Engineering, Guangdong University of Technology, Guangzhou, China; School of Mechanical and Automation Engineering, Chinese University of Hong Kong, Hong Kong, China; UM-SJTU Joint Institute, Shanghai Jiao Tong University, Shanghai, China","IEEE Robotics and Automation Letters","24 Aug 2020","2020","5","4","6615","6622","Deep Reinforcement Learning (RL) is a promising approach for adaptive robot control, but its current application to robotics is currently hindered by high sample requirements. To alleviate this issue, we propose to exploit the symmetries present in robotic tasks. Intuitively, symmetries from observed trajectories define transformations that leave the space of feasible RL trajectories invariant and can be used to generate new feasible trajectories, which could be used for training. Based on this data augmentation idea, we formulate a general framework, called Invariant Transform Experience Replay that we present with two techniques: (i) Kaleidoscope Experience Replay exploits reflectional symmetries and (ii) Goal-augmented Experience Replay which takes advantage of lax goal definitions. In the Fetch tasks from OpenAI Gym, our experimental results show significant increases in learning rates and success rates. Particularly, we attain a 13, 3, and 5 times speedup in the pushing, sliding, and pick-and-place tasks respectively in the multi-goal setting. Performance gains are also observed in similar tasks with obstacles and we successfully deployed a trained policy on a real Baxter robot. Our work demonstrates that invariant transformations on RL trajectories are a promising methodology to speed up learning in deep RL. Code, video, and supplementary materials are available at [1].","2377-3766","","10.1109/LRA.2020.3013937","GD Dept. of Science & Tech.(grant numbers:2019A050510040); National Natural Science Foundation of China(grant numbers:61950410758,61750110521,61872238); Shanghai NSF(grant numbers:19ZR1426700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9158366","AI-based methods, reinforcement learning;deep learning;dexterous manipulation","Trajectory;Robots;Task analysis;Training;Machine learning;Transforms;Grippers","adaptive control;control engineering computing;data handling;dexterous manipulators;learning (artificial intelligence);mobile robots;trajectory control","data augmentation;kaleidoscope experience replay;reflectional symmetries;goal-augmented experience replay;lax goal definitions;learning rates;pick-and-place tasks;multigoal setting;Baxter robot;deep RL;deep reinforcement learning;adaptive robot control;robotic tasks;RL trajectories;fetch tasks;invariant transform experience replay;OpenAI Gym;dexterous manipulation","","8","","30","IEEE","4 Aug 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning in M2M Communication for Resource Scheduling","Y. Che; F. Lin; J. Liu","School of Electrical Engineering and Automation, Qilu University of Technology, (Shandong Academy of Sciences), Jinan, China; Department of Physics, School of Electronic and Information Engineering, Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; School of Electronic and Information Engineering Qilu University of Technology (Shandong Academy of Sciences), Jinan, China","2021 World Conference on Computing and Communication Technologies (WCCCT)","3 Jun 2021","2021","","","97","100","The main traffic of cellular networks is Machine-type Communication (MTC), when a large number of machine type communication devices (MTCDs) request to access the network, it will be overloaded. Some basic scheduling algorithms, such as first-come, first-served algorithm, higher priority algorithm, cannot fully solve the multi-resource scheduling problem. So, in this paper, the resource management problems in Machine-to-Machine (M2M) communication were usually regarded as difficult online decision-making tasks by transforming the packing tasks with multiple resource requirements into a deep reinforcement learning problem. The design of an optimized deep neural network will make to the learning curve converges better. Compared with the RL algorithm, the Deep Reinforcement learning (DRL) algorithm proposed in this paper has a better convergence speed, so it is more suitable for Machine-to-machine(M2M) communication scenarios, it can ensure data fluidity and scheduling efficiency.","","978-1-6654-4110-0","10.1109/WCCCT52091.2021.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9443849","deep learning;reinforcement learning;convolutional neural network;resource scheduling;M2M","Machine-to-machine communications;Scheduling algorithms;Neural networks;Decision making;Reinforcement learning;Scheduling;Communications technology","cellular radio;decision making;deep learning (artificial intelligence);optimisation;resource allocation;telecommunication computing;telecommunication scheduling;telecommunication traffic","M2M communication;resource scheduling;cellular network traffic;machine type communication devices;multiresource scheduling problem;resource management problems;machine-to-machine communication;online decision-making tasks;packing tasks;multiple resource requirements;deep reinforcement learning problem;optimized deep neural network;learning curve;DRL algorithm;deep reinforcement learning algorithm;data fluidity;scheduling efficiency","","8","","12","IEEE","3 Jun 2021","","","IEEE","IEEE Conferences"
"A single-task and multi-decision evolutionary game model based on multi-agent reinforcement learning","M. Ye; C. Tianqing; F. Wenhui","Academy of Army Armored Force, Beijing, China; Academy of Army Armored Force, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","Journal of Systems Engineering and Electronics","29 Oct 2021","2021","32","3","642","657","In the evolutionary game of the same task for groups, the changes in game rules, personal interests, the crowd size, and external supervision cause uncertain effects on individual decision-making and game results. In the Markov decision framework, a single-task multi-decision evolutionary game model based on multi-agent reinforcement learning is proposed to explore the evolutionary rules in the process of a game. The model can improve the result of a evolutionary game and facilitate the completion of the task. First, based on the multi-agent theory, to solve the existing problems in the original model, a negative feedback tax penalty mechanism is proposed to guide the strategy selection of individuals in the group. In addition, in order to evaluate the evolutionary game results of the group in the model, a calculation method of the group intelligence level is defined. Secondly, the Q-learning algorithm is used to improve the guiding effect of the negative feedback tax penalty mechanism. In the model, the selection strategy of the Q-learning algorithm is improved and a bounded rationality evolutionary game strategy is proposed based on the rule of evolutionary games and the consideration of the bounded rationality of individuals. Finally, simulation results show that the proposed model can effectively guide individuals to choose cooperation strategies which are beneficial to task completion and stability under different negative feedback factor values and different group sizes, so as to improve the group intelligence level.","1004-4132","","10.23919/JSEE.2021.000055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9594755","multi-agent;reinforcement learning;evolutionary game;Q-learning","Games;Task analysis;Reinforcement learning;Negative feedback;Libraries;Finance;Stability analysis","evolutionary computation;game theory;Markov processes;multi-agent systems;reinforcement learning","bounded rationality evolutionary game strategy;Q-learning algorithm;group intelligence level;negative feedback tax penalty mechanism;evolutionary rules;single-task multidecision evolutionary game model;Markov decision framework;individual decision-making;game rules;multiagent reinforcement learning","","7","","","","29 Oct 2021","","","BIAI","BIAI Journals"
"Optimal automatic train operation via deep reinforcement learning","R. Zhou; S. Song","School of Economy, Lille 1 university, Lille, France; School of Automation, Tsinghua University, Beijing, China","2018 Tenth International Conference on Advanced Computational Intelligence (ICACI)","11 Jun 2018","2018","","","103","108","The energy consumption occupies a considerable part in the total cost of the high-speed train operation. This paper focuses on minimizing the energy consumption of high-speed train by providing an optimal trajectory planning method. In this case, several other conditions including punctuality standard, comfort standard and varying speed limitation are taken into consideration in order to systematically evaluate the performance of a designed trajectory. The air resistance related to the current speed of the train and the regenerative braking system related to the braking force enhance the difficulty of the trajectory planning problem. In previous studies of trajectory planning, either the effort produced by the high-speed train or the distance is regarded as a discrete variable, while the modern train usually provides continuous effort. This paper will propose an algorithm to deal with the trajectory planning problem mentioned based on the Deep Deterministic Policy Gradient.","","978-1-5386-4362-4","10.1109/ICACI.2018.8377589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377589","deep deterministic policy gradient;energy efficiency;automatic train operation","Force;Standards;Resistance;Trajectory;Mathematical model;Planning;Rail transportation","energy consumption;learning (artificial intelligence);optimisation;path planning;rail traffic;railways;regenerative braking","high-speed train operation;energy consumption;optimal trajectory planning method;comfort standard;varying speed limitation;regenerative braking system;trajectory planning problem;optimal automatic train operation;deep reinforcement learning;punctuality standard;deep deterministic policy gradient","","4","2","12","IEEE","11 Jun 2018","","","IEEE","IEEE Conferences"
"UAV Swarm Confrontation Based on Multi-agent Deep Reinforcement Learning","Z. Wang; F. Liu; J. Guo; C. Hong; M. Chen; E. Wang; Y. Zhao","Department of General Aviation, Civil Aviation Management Institute of China, Beijing, China; School of Electronic and Information Engineering, Shenyang Aerospace University, Shenyang, China; School of Electronic and Information Engineering, Shenyang Aerospace University, Shenyang, China; College of Robotics, Beijing Union University, Beijing, China; College of Robotics, Beijing Union University, Beijing, China; School of Electronic and Information Engineering, Shenyang Aerospace University, Shenyang, China; Department of Automation, University of Science and Technology of China, Hefei, China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","4996","5001","Multi-agent deep reinforcement learning (MADRL) has attracted a tremendous amount of interest in recent years. In this paper, we introduce MADRL into the confrontation scene of Unmanned Aerial Vehicle (UAV) swarm. To analysis the dynamic game process of UAV swarm confrontation, we build two non-cooperative game models based on MADRL paradigm. By using the multi-agent deep deterministic policy gradient (MADDPG) and the centralized training with decentralized execution method, we achieve the Nash equilibrium under 5 vs. 5 UAV confrontation scenes. We also introduce multi-agent soft actor critic (MASAC) method into the UAV swarm confrontation, simulation results indicate that the MASAC-based model outperforms the MADDPG-based model on exploring the UAV swarm combat environment, and more effectively converges to the Nash equilibrium. Our work will provide new insights into the confrontation of UAV swarm.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902663","National Key R&D Program of China(grant numbers:2018AAA0100804); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902663","UAV Swarm;Non-cooperative Game;Multi-agent;Deep Reinforcement Learning","Training;Analytical models;Simulation;Games;Reinforcement learning;Autonomous aerial vehicles;Nash equilibrium","autonomous aerial vehicles;game theory;learning (artificial intelligence);multi-agent systems;remotely operated vehicles","5 UAV confrontation scenes;confrontation scene;MADDPG-based model;MADRL paradigm;MASAC-based model;multiagent deep deterministic policy gradient;multiagent deep reinforcement learning;multiagent soft actor critic method;noncooperative game models;UAV swarm combat environment;UAV swarm confrontation;Unmanned Aerial Vehicle swarm","","4","","13","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Intersection Navigation Under Dynamic Constraints Using Deep Reinforcement Learning","A. Demir; V. Sezer","Department of Mechatronics Engineering, Istanbul Technical University, Istanbul, Turkey; Department of Control and Automation Engineering, Istanbul Technical University, Istanbul, Turkey","2018 6th International Conference on Control Engineering & Information Technology (CEIT)","1 Jul 2019","2018","","","1","5","In this study, we present a unified motion planner with low- level controller for continuous control of a differential drive mobile robot. Deep reinforcement agent takes 10 dimensional state vector as input and calculates each wheel's torque value as a 2 dimensional output vector. These torque values are fed into the dynamic model of the robot, and lastly steering commands are gathered. In previous studies, navigation problem solutions that uses deep - RL methods, have not been considered with agent's own dynamic constraints, but it has been done by only considering kinematic models. This is not reliable enough for real-world scenarios. In this paper, deep-RL based motion planning is performed by considering both kinematic and dynamic constraints. According to the simulations in a dynamic environment, the agent succesfully navigates through the intersection with 99.6% success rate.","","978-1-5386-7641-7","10.1109/CEIT.2018.8751788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8751788","","Mathematical model;Robot kinematics;Mobile robots;Dynamics;Torque;Vehicle dynamics","learning (artificial intelligence);mobile robots;path planning","intersection navigation;deep reinforcement learning;unified motion planner;level controller;continuous control;differential drive mobile robot;deep reinforcement agent;torque values;RL methods;deep-RL based motion planning;steering commands","","3","","21","IEEE","1 Jul 2019","","","IEEE","IEEE Conferences"
