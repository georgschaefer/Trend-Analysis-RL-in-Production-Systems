@inproceedings{10.1145/3295500.3356164,
author = {Qin, Heyang and Zawad, Syed and Zhou, Yanqi and Yang, Lei and Zhao, Dongfang and Yan, Feng},
title = {Swift Machine Learning Model Serving Scheduling: A Region Based Reinforcement Learning Approach},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356164},
doi = {10.1145/3295500.3356164},
abstract = {The success of machine learning has prospered Machine-Learning-as-a-Service (MLaaS) - deploying trained machine learning (ML) models in cloud to provide low latency inference services at scale. To meet latency Service-Level-Objective (SLO), judicious parallelization at both request and operation levels is utterly important. However, existing ML systems (e.g., Tensorflow) and cloud ML serving platforms (e.g., SageMaker) are SLO-agnostic and rely on users to manually configure the parallelism. To provide low latency ML serving, this paper proposes a swift machine learning serving scheduling framework with a novel Region-based Reinforcement Learning (RRL) approach. RRL can efficiently identify the optimal parallelism configuration under different workloads by estimating performance of similar configurations with that of the known ones. We both theoretically and experimentally show that the RRL approach can outperform state-of-the-art approaches by finding near optimal solutions over 8 times faster while reducing inference latency up to 79.0\% and reducing SLO violation up to 49.9\%.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {13},
numpages = {23},
keywords = {service-level-objective (SLO), workload scheduling, machine-learning-as-a-service (MLaaS), model inference, parallelism parameter tuning, reinforcement learning},
location = {Denver, Colorado},
series = {SC '19}
}

@inproceedings{10.1145/3408308.3427976,
author = {Pedasingu, Bala Suraj and Subramanian, Easwar and Bichpuriya, Yogesh and Sarangan, Venkatesh and Mahilong, Nidhisha},
title = {Bidding Strategy for Two-Sided Electricity Markets: A Reinforcement Learning Based Framework},
year = {2020},
isbn = {9781450380614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408308.3427976},
doi = {10.1145/3408308.3427976},
abstract = {We aim to increase the revenue or reduce the purchase cost of a given market participant in a double-sided, day-ahead, wholesale electricity market serving a smart city. Using an operations research based market clearing mechanism and attention based time series forecaster as sub-modules, we build a holistic interactive system. Through this system, we discover better bidding strategies for a market participant using reinforcement learning (RL). We relax several assumptions made in existing literature in order to make the problem setting more relevant to real life. Our Markov Decision Process (MDP) formulation enables us to tackle action space explosion and also compute optimal actions across time-steps in parallel. Our RL framework is generic enough to be used by either a generator or a consumer participating in the electricity market.We study the efficacy of the proposed RL based bidding framework from the perspective of a generator as well as a buyer on real world day-ahead electricity market data obtained from the European Power Exchange (EPEX). We compare the performance of our RL based bidding framework against three baselines: (a) an ideal but un-realizable bidding strategy; (b) a realizable approximate version of the ideal strategy; and (c) historical performance as found from the logs. Under both perspectives, we find that our RL based framework is more closer to the ideal strategy than other baselines. Further, the RL based framework improves the average daily revenue of the generator by nearly €7,200 (€2.64 M per year) and €9,000 (€3.28 M per year) over the realizable ideal and historical strategies respectively. When used on behalf of a buyer, it reduces average daily procurement cost by nearly €2,700 (€0.97 M per year) and €57,200 (€52.63 M per year) over the realizable ideal and historical strategies respectively. We also observe that our RL based framework automatically adapts its actions to changes in the market power of the participant.},
booktitle = {Proceedings of the 7th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {110–119},
numpages = {10},
keywords = {Reinforcement Learning, Electricity Markets, Optimization, Bidding, Forecasting},
location = {Virtual Event, Japan},
series = {BuildSys '20}
}

@inproceedings{10.1145/3301293.3302374,
author = {Joseph, Samuel and Misra, Rakesh and Katti, Sachin},
title = {Towards Self-Driving Radios: Physical-Layer Control Using Deep Reinforcement Learning},
year = {2019},
isbn = {9781450362733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301293.3302374},
doi = {10.1145/3301293.3302374},
abstract = {Modern radios, such as 5G New Radio, feature a large set of physical-layer control knobs in order to support an increasing number of communication scenarios spanning multiple use cases, device categories and wireless environments. The challenge however is that each scenario requires a different control algorithm to optimally determine how these knobs are adapted to the varying operating conditions. The traditional approach of manually designing different algorithms for different scenarios is increasingly becoming not just difficult to repeat but also suboptimal for new scenarios that previous-generation radios were not designed for.In this paper, we ask: can we make a radio automatically learn the optimal physical-layer control algorithm for any scenario given only high-level design specifications for the scenario, i.e., can we design a self-driving radio? We describe how recent advances in deep reinforcement learning can be applied to train a self-driving radio for several illustrative scenarios, and show that such a learning-based approach not only is easily repeatable but also performs closer to optimal than the current state of the art.},
booktitle = {Proceedings of the 20th International Workshop on Mobile Computing Systems and Applications},
pages = {69–74},
numpages = {6},
keywords = {reinforcement learning, physical layer, 5g, cellular networks, radios, deep learning, lte},
location = {Santa Cruz, CA, USA},
series = {HotMobile '19}
}

@inproceedings{10.1145/3524273.3528188,
author = {Turkkan, Bekir Oguzhan and Dai, Ting and Raman, Adithya and Kosar, Tevfik and Chen, Changyou and Bulut, Muhammed Fatih and Zola, Jaroslaw and Sow, Daby},
title = {GreenABR: Energy-Aware Adaptive Bitrate Streaming with Deep Reinforcement Learning},
year = {2022},
isbn = {9781450392839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524273.3528188},
doi = {10.1145/3524273.3528188},
abstract = {Adaptive bitrate (ABR) algorithms aim to make optimal bitrate decisions in dynamically changing network conditions to ensure a high quality of experience (QoE) for the users during video streaming. However, most of the existing ABRs share the limitations of predefined rules and incorrect assumptions about streaming parameters. They also come short to consider the perceived quality in their QoE model, target higher bitrates regardless, and ignore the corresponding energy consumption. This joint approach results in additional energy consumption and becomes a burden, especially for mobile device users. This paper proposes GreenABR, a new deep reinforcement learning-based ABR scheme that optimizes the energy consumption during video streaming without sacrificing the user QoE. GreenABR employs a standard perceived quality metric, VMAF, and real power measurements collected through a streaming application. GreenABR's deep reinforcement learning model makes no assumptions about the streaming environment and learns how to adapt to the dynamically changing conditions in a wide range of real network scenarios. GreenABR outperforms the existing state-of-the-art ABR algorithms by saving up to 57\% in streaming energy consumption and 60\% in data consumption while achieving up to 22\% more perceptual QoE due to up to 84\% less rebuffering time and near-zero capacity violations.},
booktitle = {Proceedings of the 13th ACM Multimedia Systems Conference},
pages = {150–163},
numpages = {14},
keywords = {energy efficiency, deep reinforcement learning, video streaming},
location = {Athlone, Ireland},
series = {MMSys '22}
}

@article{10.1145/3404191,
author = {Fraternali, Francesco and Balaji, Bharathan and Agarwal, Yuvraj and Gupta, Rajesh K.},
title = {ACES: Automatic Configuration of Energy Harvesting Sensors with Reinforcement Learning},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3404191},
doi = {10.1145/3404191},
abstract = {Many modern smart building applications are supported by wireless sensors to sense physical parameters, given the flexibility they offer and the reduced cost of deployment. However, most wireless sensors are powered by batteries today, and large deployments are inhibited by the requirement of periodic battery replacement. Energy harvesting sensors provide an attractive alternative, but they need to provide adequate quality of service to applications given the uncertainty of energy availability. We propose ACES, which uses reinforcement learning to maximize sensing quality of energy harvesting sensors for periodic and event-driven indoor sensing with available energy. Our custom-built sensor platform uses a supercapacitor to store energy and Bluetooth Low Energy to relay sensors data. Using simulations and real deployments, we use the data collected to continually adapt the sensing of each node to changing environmental patterns and transfer learning to reduce the training time in real deployments. In our 60-node deployment lasting 2 weeks, nodes stop operations only 0.1\% of the time, and collection of data is comparable with current battery-powered nodes. We show that ACES reduces the node duty-cycle period by an average of 33\% compared to three prior reinforcement learning techniques while continuously learning environmental changes over time.},
journal = {ACM Trans. Sen. Netw.},
month = {jul},
articleno = {36},
numpages = {31},
keywords = {energy harvesting, real deployment, reinforcement learning, smart buildings, Internet of Things, automatic configuration, battery-less}
}

@inproceedings{10.1145/3533702.3534908,
author = {Eldeeb, Tamer and Chen, Zhengneng and Cidon, Asaf and Yang, Junfeng},
title = {Neuroshard: Towards Automatic Multi-Objective Sharding with Deep Reinforcement Learning},
year = {2022},
isbn = {9781450393775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533702.3534908},
doi = {10.1145/3533702.3534908},
abstract = {Large databases whose data does not fit on a single server need to shard their rows across multiple different database instances. Distributed transactions are significantly more expensive than local transactions, so a popular approach is to collect a trace of past accesses to the database and model it as a graph (or a hypergraph), and solve an NP-Hard partitioning problem with an objective of minimizing the fanout, or the number of database instances that need to participate in each query. Due to the large amount of data that needs to be sharded, this problem cannot be solved optimally, and therefore, databases use heuristic partitioning algorithms, which can be fairly effective in practice. However, fanout is only one objective that affects performance. Other important objectives include load balancing, which ensures that no single database instance becomes too overloaded, or equalizing the write traffic for each database to avoid lock contention and I/O amplification. Designing heuristics for more than one objective is difficult and error-prone.We present Neuroshard, the first system that learns shard assignments directly from the workload, and optimizes for multiple sharding objectives simultaneously. Neuroshard represents past queries as a neural hypergraph, and uses Deep Reinforcement Learning with Multi-Task learning to generate a learned partitioner that is able to optimize for multiple objectives in parallel. We implement Neuroshard on a distributed database that uses MariaDB, and got very promising initial results showing that this approach can achieve our versatility and scalability goals, in contrast to baseline approaches that optimize for only one objective which can work well in one context but perform poorly in another.},
booktitle = {Proceedings of the Fifth International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {1},
numpages = {12},
location = {Philadelphia, Pennsylvania},
series = {aiDM '22}
}

@inproceedings{10.1145/3459637.3482494,
author = {Shi, Si and Li, Jianjun and Li, Guohui and Pan, Peng and Liu, Ke},
title = {XPM: An Explainable Deep Reinforcement Learning Framework for Portfolio Management},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482494},
doi = {10.1145/3459637.3482494},
abstract = {Reinforcement learning-based portfolio management has recently attracted extensive attention. However, deep reinforcement learning methods are unexplainable and considered to be potentially risky, difficult to be trusted and regulated by users. To address these problems, we propose an eXplainable reinforcement learning framework for Portfolio Management, named XPM, which is efficient, concise, and can provide faithful explanations for network outputs. Specifically, we first design a policy network for portfolio management, which uses temporal convolutional network (TCN) to extract temporal features of multiple time series in portfolio. Then, we employ global average pooling (GAP) and a fully connected layer to integrate the global feature maps to handle asset correlations. Finally, we utilize softmax to determine the output portfolio weights. To assemble explainability into our model, we employ an explainable artificial intelligence method, class activation mapping (CAM), to explain the network outputs, which computes an activation map for an asset of interest. The map highlights the important assets and time intervals in the input state. In this way, end users can understand which part of the portfolio's recent price movements makes the network decision to invest in the target asset. Experimental results show that XPM outperforms the current state-of-the-art portfolio management methods in NASDAQ and NYSE markets, and can provide faithful and informative explanations to end users.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {1661–1670},
numpages = {10},
keywords = {temporal convolutional network, reinforcement learning, portfolio management, explainable artificial intelligence},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3211954.3211956,
author = {Durand, Gabriel Campero and Pinnecke, Marcus and Piriyev, Rufat and Mohsen, Mahmoud and Broneske, David and Saake, Gunter and Sekeran, Maya S. and Rodriguez, Fabi\'{a}n and Balami, Laxmi},
title = {GridFormation: Towards Self-Driven Online Data Partitioning Using Reinforcement Learning},
year = {2018},
isbn = {9781450358514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211954.3211956},
doi = {10.1145/3211954.3211956},
abstract = {In this paper we define a research agenda to develop a general framework supporting online autonomous tuning of data partitioning and layouts with a reinforcement learning formulation. We establish the core elements of our approach: agent, environment, action space and supporting components. Externally predicted workloads and the current physical design serve as input to our agent. The environment guides the search process by generating immediate rewards based on fresh cost estimates, for either the entirety or a sample of queries from the workload, and by deciding the possible actions given a state. This set of actions is configurable, enabling the representation of different partitioning problems. For use in an online setting the agent learns a fixed-length sequence of n actions that maximize the temporal reward for the predicted workload. Through an initial implementation we assert the feasibility of our approach. To conclude, we list open challenges for this work.},
booktitle = {Proceedings of the First International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {1},
numpages = {7},
keywords = {Deep Q-Learning, Adaptive layouts, Physical design},
location = {Houston, TX, USA},
series = {aiDM'18}
}

@article{10.1145/3272127.3275048,
author = {Clegg, Alexander and Yu, Wenhao and Tan, Jie and Liu, C. Karen and Turk, Greg},
title = {Learning to Dress: Synthesizing Human Dressing Motion via Deep Reinforcement Learning},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3272127.3275048},
doi = {10.1145/3272127.3275048},
abstract = {Creating animation of a character putting on clothing is challenging due to the complex interactions between the character and the simulated garment. We take a model-free deep reinforcement learning (deepRL) approach to automatically discovering robust dressing control policies represented by neural networks. While deepRL has demonstrated several successes in learning complex motor skills, the data-demanding nature of the learning algorithms is at odds with the computationally costly cloth simulation required by the dressing task. This paper is the first to demonstrate that, with an appropriately designed input state space and a reward function, it is possible to incorporate cloth simulation in the deepRL framework to learn a robust dressing control policy. We introduce a salient representation of haptic information to guide the dressing process and utilize it in the reward function to provide learning signals during training. In order to learn a prolonged sequence of motion involving a diverse set of manipulation skills, such as grasping the edge of the shirt or pulling on a sleeve, we find it necessary to separate the dressing task into several subtasks and learn a control policy for each subtask. We introduce a policy sequencing algorithm that matches the distribution of output states from one task to the input distribution for the next task in the sequence. We have used this approach to produce character controllers for several dressing tasks: putting on a t-shirt, putting on a jacket, and robot-assisted dressing of a sleeve.},
journal = {ACM Trans. Graph.},
month = {dec},
articleno = {179},
numpages = {10},
keywords = {reinforcement learning, dressing, policy sequencing}
}

@article{10.1145/3306346.3322940,
author = {Xu, Jie and Du, Tao and Foshey, Michael and Li, Beichen and Zhu, Bo and Schulz, Adriana and Matusik, Wojciech},
title = {Learning to Fly: Computational Controller Design for Hybrid UAVs with Reinforcement Learning},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3322940},
doi = {10.1145/3306346.3322940},
abstract = {Hybrid unmanned aerial vehicles (UAV) combine advantages of multicopters and fixed-wing planes: vertical take-off, landing, and low energy use. However, hybrid UAVs are rarely used because controller design is challenging due to its complex, mixed dynamics. In this paper, we propose a method to automate this design process by training a mode-free, model-agnostic neural network controller for hybrid UAVs. We present a neural network controller design with a novel error convolution input trained by reinforcement learning. Our controller exhibits two key features: First, it does not distinguish among flying modes, and the same controller structure can be used for copters with various dynamics. Second, our controller works for real models without any additional parameter tuning process, closing the gap between virtual simulation and real fabrication. We demonstrate the efficacy of the proposed controller both in simulation and in our custom-built hybrid UAVs (Figure 1, 8). The experiments show that the controller is robust to exploit the complex dynamics when both rotors and wings are active in flight tests.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {42},
numpages = {12},
keywords = {neural network controllers, hybrid UAVs}
}

@inproceedings{10.1145/3178876.3186165,
author = {Feng, Jun and Li, Heng and Huang, Minlie and Liu, Shichen and Ou, Wenwu and Wang, Zhirong and Zhu, Xiaoyan},
title = {Learning to Collaborate: Multi-Scenario Ranking via Multi-Agent Reinforcement Learning},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186165},
doi = {10.1145/3178876.3186165},
abstract = {Ranking is a fundamental and widely studied problem in scenarios such as search, advertising, and recommendation. However, joint optimization for multi-scenario ranking, which aims to improve the overall performance of several ranking strategies in different scenarios, is rather untouched. Separately optimizing each individual strategy has two limitations. The first one is lack of collaboration between scenarios meaning that each strategy maximizes its own objective but ignores the goals of other strategies, leading to a sub-optimal overall performance. The second limitation is the inability of modeling the correlation between scenarios meaning that independent optimization in one scenario only uses its own user data but ignores the context in other scenarios. In this paper, we formulate multi-scenario ranking as a fully cooperative, partially observable, multi-agent sequential decision problem. We propose a novel model named Multi-Agent Recurrent Deterministic Policy Gradient (MA-RDPG) which has a communication component for passing messages, several private actors (agents) for making actions for ranking, and a centralized critic for evaluating the overall performance of the co-working actors. Each scenario is treated as an agent (actor). Agents collaborate with each other by sharing a global action-value function (the critic) and passing messages that encodes historical information across scenarios. The model is evaluated with online settings on a large E-commerce platform. Results show that the proposed model exhibits significant improvements against baselines in terms of the overall performance.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {1939–1948},
numpages = {10},
keywords = {learning to rank, joint optimization, reinforcement learning, multi-agent learning},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3565477.3569155,
author = {Choi, Wangyu and Yoon, Jongwon},
title = {MetaABR: Environment-Adaptive Video Streaming System with Meta-Reinforcement Learning},
year = {2022},
isbn = {9781450399371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565477.3569155},
doi = {10.1145/3565477.3569155},
abstract = {This work focuses on a video bitrate algorithm that quickly adapts to new and various environments with just a few update steps. This aspect is especially important for large-scale video streaming services used by a wide variety of users in different environments. Our proposed model is based on a neural network and employs meta-reinforcement learning to train it. After training, it can be easily customized for a variety of new environments with a few update steps, providing a user-specific streaming service.},
booktitle = {Proceedings of the 3rd International CoNEXT Student Workshop},
pages = {37–39},
numpages = {3},
keywords = {adaptive bitrate algorithm, meta-reinforcement learning},
location = {Rome, Italy},
series = {CoNEXT-SW '22}
}

@inproceedings{10.1145/3410566.3410603,
author = {Sadri, Zahra and Gruenwald, Le and Lead, Eleazar},
title = {DRLindex: Deep Reinforcement Learning Index Advisor for a Cluster Database},
year = {2020},
isbn = {9781450375030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410566.3410603},
doi = {10.1145/3410566.3410603},
abstract = {Cloud database providers provision different architectures to guarantee high availability. One of these architectures is a cluster database that consists of several database engine nodes, where data is replicated among the nodes. Although the cloud database providers provide various auto-indexing tools, these tools mostly address characteristics of a database deployed on a single node, not a cluster. It is possible to install an index advisor on each node, which recommends an index set for that node. The problem with this approach is that the current index advisors for a single node aim to minimize the processing cost of the workload; however, on a cluster database, other goals such as load balancing can be considered. Hence, the better solution could be an index advisor which has a comprehensive view of the cluster node.In this paper, we propose an index advisor for a replicated database on a database cluster for a read-only workload. The advisor considers both query processing cost and load balancing. It utilizes a Deep Reinforcement Learning (DRL) approach in which a DRL agent learns to select a set of index configurations for nodes in a cluster. We describe the components of the DRL-advisor such as the agent, the environment, a set of actions, the reward function, and other modules. Experimental results validate the effectiveness of the algorithm.},
booktitle = {Proceedings of the 24th Symposium on International Database Engineering \&amp; Applications},
articleno = {11},
numpages = {8},
keywords = {cluster database, deep reinforcement learning, index tuning},
location = {Seoul, Republic of Korea},
series = {IDEAS '20}
}

@inproceedings{10.1145/3266037.3266083,
author = {Xu, Tongda and Wang, Dinglu and You, Xiaohui},
title = {Mindgame: Mediating People's EEG Alpha Band Power through Reinforcement Learning},
year = {2018},
isbn = {9781450359498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266037.3266083},
doi = {10.1145/3266037.3266083},
abstract = {This paper presents Mindgame, a reinforcement learning optimized neurofeedback mindfulness system. To avoid the potential bias and difficulties of designing mapping between neural signal and output, we adopt a trial-and-error learning method to explore the preferred mapping. In a pilot study we assess the effectiveness of Mindgame in mediating people's EEG alpha band. All participants' alpha band change towards the desired direction.},
booktitle = {Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {5–6},
numpages = {2},
keywords = {brain-computer interface, neurofeedback, reinforcement learning},
location = {Berlin, Germany},
series = {UIST '18 Adjunct}
}

@inproceedings{10.1145/2732158.2732189,
author = {Gao, Yuan and Ilves, Kalle and G\l{}owacka, Dorota},
title = {OfficeHours: A System for Student Supervisor Matching through Reinforcement Learning},
year = {2015},
isbn = {9781450333085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2732158.2732189},
doi = {10.1145/2732158.2732189},
abstract = {We describe OfficeHours, a recommender system that assists students in finding potential supervisors for their dissertation projects. OfficeHours is an interactive recommender system that combines reinforcement learning techniques with a novel interface that assists the student in formulating their query and allows active engagement in directing their search. Students can directly manipulate document features (keywords) extracted from scientific articles written by faculty members to indicate their interests and reinforcement learning is used to model the student's interests by allowing the system to trade off between exploration and exploitation. The goal of system is to give the student the opportunity to more effectively search for possible project supervisors in a situation where the student may have difficulties formulating their query or when very little information may be available on faculty members' websites about their research interests.},
booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion},
pages = {29–32},
numpages = {4},
keywords = {student support systems, exploratory search},
location = {Atlanta, Georgia, USA},
series = {IUI Companion '15}
}

@inproceedings{10.1145/3486611.3486670,
author = {Fraternali, Francesco and Balaji, Bharathan and Hong, Dezhi and Agarwal, Yuvraj and Gupta, Rajesh K.},
title = {Marble: Collaborative Scheduling of Batteryless Sensors with Meta Reinforcement Learning},
year = {2021},
isbn = {9781450391146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486611.3486670},
doi = {10.1145/3486611.3486670},
abstract = {Batteryless energy-harvesting sensing systems are attractive for low maintenance but face challenges in real-world applications due to low quality of service from sporadic and unpredictable energy availability. To overcome this challenge, recent data-driven energy management techniques optimize energy usage to maximize application performance even in low harvested energy scenarios by learning energy availability patterns in the environment. These techniques require prior knowledge of the environment in which the sensor nodes are deployed to work correctly. In the absence of historical data, the application performance deteriorates.To overcome this challenge, we describe here the use of meta reinforcement learning to increase the application performance of newly deployed batteryless sensor nodes without historical data. Our system, called Marble, exploits information from other sensor node locations to expedite the learning of newly deployed sensor nodes, and improves application performance in the initial period of deployment. Our evaluation using real-world data traces shows that Marble detects up to 66\% more events in low lighting conditions, and up to 25.6\% more events on average on the first 3 days of deployment compared to the state-of-the-art.1},
booktitle = {Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {140–149},
numpages = {10},
keywords = {perpetual operations, energy harvesting, smart buildings, deep reinforcement learning, batteryless, wireless sensor network, collaborative learning},
location = {Coimbra, Portugal},
series = {BuildSys '21}
}

@inproceedings{10.1145/3397271.3401134,
author = {Wang, Pengfei and Fan, Yu and Xia, Long and Zhao, Wayne Xin and Niu, Shaozhang and Huang, Jimmy},
title = {KERL: A Knowledge-Guided Reinforcement Learning Model for Sequential Recommendation},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401134},
doi = {10.1145/3397271.3401134},
abstract = {For sequential recommendation, it is essential to capture and predict future or long-term user preference for generating accurate recommendation over time. To improve the predictive capacity, we adopt reinforcement learning (RL) for developing effective sequential recommenders. However, user-item interaction data is likely to be sparse, complicated and time-varying. It is not easy to directly apply RL techniques to improve the performance of sequential recommendation.Inspired by the availability of knowledge graph (KG), we propose a novel Knowledge-guidEd Reinforcement Learning model (KERL for short) for fusing KG information into a RL framework for sequential recommendation. Specifically, we formalize the sequential recommendation task as a Markov Decision Process (MDP), and make three major technical extensions in this framework, including state representation, reward function and learning algorithm. First, we propose to enhance the state representations with KG information considering both exploitation and exploration. Second, we carefully design a composite reward function that is able to compute both sequence- and knowledge-level rewards. Third, we propose a new algorithm for more effectively learning the proposed model. To our knowledge, it is the first time that knowledge information has been explicitly discussed and utilized in RL-based sequential recommenders, especially for the exploration process. Extensive experiment results on both next-item and next-session recommendation tasks show that our model can significantly outperform the baselines on four real-world datasets.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {209–218},
numpages = {10},
keywords = {sequential recommendation, reinforcement learning, knowledge graph},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3365871.3365911,
author = {Murad, Abdulmajid and Bach, Kerstin and Kraemer, Frank Alexander and Taylor, Gavin},
title = {IoT Sensor Gym: Training Autonomous IoT Devices with Deep Reinforcement Learning},
year = {2019},
isbn = {9781450372077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365871.3365911},
doi = {10.1145/3365871.3365911},
abstract = {We describe IoT Sensor Gym, a framework to train the behavior of constrained IoT devices using deep reinforcement learning. We focus on the main architectural choices to align problems from the IoT domain with cutting-edge reinforcement learning algorithms and exemplify our results with the autonomous control of a solar-powered IoT device.},
booktitle = {Proceedings of the 9th International Conference on the Internet of Things},
articleno = {37},
numpages = {4},
keywords = {Internet of Things, Embedded Systems, Deep Reinforcement Learning, IoT, Energy Management},
location = {Bilbao, Spain},
series = {IoT '19}
}

@inproceedings{10.5555/1030453.1030742,
author = {Creighton, Douglas C. and Nahavandi, Saeid},
title = {General Methodology 1: Optimising Discrete Event Simulation Models Using a Reinforcement Learning Agent},
year = {2002},
isbn = {0780376153},
publisher = {Winter Simulation Conference},
abstract = {A reinforcement learning agent has been developed to determine optimal operating policies in a multi-part serial line. The agent interacts with a discrete event simulation model of a stochastic production facility. This study identifies issues important to the simulation developer who wishes to optimise a complex simulation or develop a robust operating policy. Critical parameters pertinent to 'tuning' an agent quickly and enabling it to rapidly learn the system were investigated.},
booktitle = {Proceedings of the 34th Conference on Winter Simulation: Exploring New Frontiers},
pages = {1945–1950},
numpages = {6},
location = {San Diego, California},
series = {WSC '02}
}

@inproceedings{10.1145/1015330.1015411,
author = {Rudary, Matthew and Singh, Satinder and Pollack, Martha E.},
title = {Adaptive Cognitive Orthotics: Combining Reinforcement Learning and Constraint-Based Temporal Reasoning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015411},
doi = {10.1145/1015330.1015411},
abstract = {Reminder systems support people with impaired prospective memory and/or executive function, by providing them with reminders of their functional daily activities. We integrate temporal constraint reasoning with reinforcement learning (RL) to build an adaptive reminder system and in a simulated environment demonstrate that it can personalize to a user and adapt to both short- and long-term changes. In addition to advancing the application domain, our integrated algorithm contributes to research on temporal constraint reasoning by showing how RL can select an optimal policy from amongst a set of temporally consistent ones, and it contributes to the work on RL by showing how temporal constraint reasoning can be used to dramatically reduce the space of actions from which an RL agent needs to learn.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {91},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/3316781.3317849,
author = {Rathore, Vijeta and Chaturvedi, Vivek and Singh, Amit K. and Srikanthan, Thambipillai and Shafique, Muhammad},
title = {LifeGuard: A Reinforcement Learning-Based Task Mapping Strategy for Performance-Centric Aging Management},
year = {2019},
isbn = {9781450367257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316781.3317849},
doi = {10.1145/3316781.3317849},
abstract = {Device scaling to subdeca nanometer has pushed device aging as a primary design concern. In manycore systems, inevitable process variation further adds to delay degradation and, coupled with the scalability issues in manycores, makes aging management, while meeting performance demands, a complex problem. LifeGuard is a performance-centric reinforcement learning-based task mapping strategy that leverages the different impact of applications on aging for improving system health. Experimental results, comparing LifeGuard with two state-of-the-art aging optimizing techniques, on a 256-core system, showed that LifeGuard led to improved health for, respectively, 57\% and 74\% of the cores, and also an enhanced aggregate core frequency.},
booktitle = {Proceedings of the 56th Annual Design Automation Conference 2019},
articleno = {179},
numpages = {6},
keywords = {mapping, negative-bias temperature instability (NBTI), many-core systems, reinforcement learning, aging},
location = {Las Vegas, NV, USA},
series = {DAC '19}
}

@inproceedings{10.5555/3201607.3201635,
author = {Cheng, Mingxi and Li, Ji and Nazarian, Shahin},
title = {DRL-Cloud: Deep Reinforcement Learning-Based Resource Provisioning and Task Scheduling for Cloud Service Providers},
year = {2018},
publisher = {IEEE Press},
abstract = {Cloud computing has become an attractive computing paradigm in both academia and industry. Through virtualization technology, Cloud Service Providers (CSPs) that own data centers can structure physical servers into Virtual Machines (VMs) to provide services, resources, and infrastructures to users. Profit-driven CSPs charge users for service access and VM rental, and reduce power consumption and electric bills so as to increase profit margin. The key challenge faced by CSPs is data center energy cost minimization. Prior works proposed various algorithms to reduce energy cost through Resource Provisioning (RP) and/or Task Scheduling (TS). However, they have scalability issues or do not consider TS with task dependencies, which is a crucial factor that ensures correct parallel execution of tasks. This paper presents DRL-Cloud, a novel Deep Reinforcement Learning (DRL)-based RP and TS system, to minimize energy cost for large-scale CSPs with very large number of servers that receive enormous numbers of user requests per day. A deep Q-learning-based two-stage RP-TS processor is designed to automatically generate the best long-term decisions by learning from the changing environment such as user request patterns and realistic electric price. With training techniques such as target network, experience replay, and exploration and exploitation, the proposed DRL-Cloud achieves remarkably high energy cost efficiency, low reject rate as well as low runtime with fast convergence. Compared with one of the state-of-the-art energy efficient algorithms, the proposed DRL-Cloud achieves up to 320\% energy cost efficiency improvement while maintaining lower reject rate on average. For an example CSP setup with 5,000 servers and 200,000 tasks, compared to a fast round-robin baseline, the proposed DRL-Cloud achieves up to 144\% runtime reduction.},
booktitle = {Proceedings of the 23rd Asia and South Pacific Design Automation Conference},
pages = {129–134},
numpages = {6},
keywords = {cloud resource management, cloud computing, resource provisioning, deep Q-learning, deep reinforcement learning, task scheduling},
location = {Jeju, Republic of Korea},
series = {ASPDAC '18}
}

@inproceedings{10.5555/3586210.3586435,
author = {Brunetti, Matteo and Campuzano, Giovanni and Mes, Martijn},
title = {Simulation of the Internal Electric Fleet Dispatching Problem at a Seaport: A Reinforcement Learning Approach},
year = {2023},
publisher = {IEEE Press},
abstract = {Through discrete-event simulation, we evaluate the impact of using a fleet of electric and autonomous vehicles (EAVs) to decouple inbound trucks from the internal freight flows in a seaport located in the Netherlands. To support the operational control of EAVs, we use agent-based modeling and support the decision-making capabilities using a reinforcement learning (RL) approach. More specifically, to model the assignment of EAVs to container transport or battery charge, we introduce the Internal Electric Fleet Dispatching Problem (IEFDP). To solve the IEFDP, we propose an RL approach and benchmark its performance against four different assignment heuristics. Our results are compelling: the RL approach outperforms the benchmark heuristics, and the decoupling process significantly reduces congestion and waiting times for truck drivers as well as potentially improve the traffic's sustainability, against a slight increase in length of stay of containers at the port.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2675–2686},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.5555/3398761.3398915,
author = {Sun, Yanchao and Huang, Furong},
title = {Can Agents Learn by Analogy? An Inferable Model for PAC Reinforcement Learning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Model-based reinforcement learning algorithms make decisions by building and utilizing a model of the environment. However, none of the existing algorithms attempts to infer the dynamics of any state-action pair from known state-action pairs before meeting it for sufficient times. We propose a new model-based method called Greedy Inference Model (GIM) that infers the unknown dynamics from known dynamics based on the internal spectral properties of the environment. In other words, GIM can "learn by analogy". We further introduce a new exploration strategy which ensures that the agent rapidly and evenly visits unknown state-action pairs. GIM is much more computationally efficient than state-of-the-art model-based algorithms, as the number of dynamic programming operations is independent of the environment size. Lower sample complexity could also be achieved under mild conditions compared against methods without inferring. Experimental results demonstrate the effectiveness and efficiency of GIM in a variety of real-world tasks.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1332–1340},
numpages = {9},
keywords = {sample complexity, spectral method, model-based reinforcement learning, computational complexity},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3447548.3467417,
author = {Pang, Guansong and van den Hengel, Anton and Shen, Chunhua and Cao, Longbing},
title = {Toward Deep Supervised Anomaly Detection: Reinforcement Learning from Partially Labeled Anomaly Data},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467417},
doi = {10.1145/3447548.3467417},
abstract = {We consider the problem of anomaly detection with a small set of partially labeled anomaly examples and a large-scale unlabeled dataset. This is a common scenario in many important applications. Existing related methods either exclusively fit the limited anomaly examples that typically do not span the entire set of anomalies, or proceed with unsupervised learning from the unlabeled data. We propose here instead a deep reinforcement learning-based approach that enables an end-to-end optimization of the detection of both labeled and unlabeled anomalies. This approach learns the known abnormality by automatically interacting with an anomaly-biased simulation environment, while continuously extending the learned abnormality to novel classes of anomaly (i.e., unknown anomalies) by actively exploring possible anomalies in the unlabeled data. This is achieved by jointly optimizing the exploitation of the small labeled anomaly data and the exploration of the rare unlabeled anomalies. Extensive experiments on 48 real-world datasets show that our model significantly outperforms five state-of-the-art competing methods.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {1298–1308},
numpages = {11},
keywords = {anomaly detection, deep learning, intrusion detection, neural networks, outlier detection, reinforcement learning},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3126908.3126951,
author = {Li, Yan and Chang, Kenneth and Bel, Oceane and Miller, Ethan L. and Long, Darrell D. E.},
title = {CAPES: Unsupervised Storage Performance Tuning Using Neural Network-Based Deep Reinforcement Learning},
year = {2017},
isbn = {9781450351140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126908.3126951},
doi = {10.1145/3126908.3126951},
abstract = {Parameter tuning is an important task of storage performance optimization. Current practice usually involves numerous tweak-benchmark cycles that are slow and costly. To address this issue, we developed CAPES, a model-less deep reinforcement learning-based unsupervised parameter tuning system driven by a deep neural network (DNN). It is designed to find the optimal values of tunable parameters in computer systems, from a simple client-server system to a large data center, where human tuning can be costly and often cannot achieve optimal performance. CAPES takes periodic measurements of a target computer system's state, and trains a DNN which uses Q-learning to suggest changes to the system's current parameter values. CAPES is minimally intrusive, and can be deployed into a production system to collect training data and suggest tuning actions during the system's daily operation. Evaluation of a prototype on a Lustre file system demonstrates an increase in I/O throughput up to 45\% at saturation point.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {42},
numpages = {14},
keywords = {q-learning, performance tuning, deep learning},
location = {Denver, Colorado},
series = {SC '17}
}

@inproceedings{10.1145/3274783.3274843,
author = {Elmalaki, Salma and Tsai, Huey-Ru and Srivastava, Mani},
title = {Sentio: Driver-in-the-Loop Forward Collision Warning Using Multisample Reinforcement Learning},
year = {2018},
isbn = {9781450359528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274783.3274843},
doi = {10.1145/3274783.3274843},
abstract = {Thanks to the adoption of more sensors in the automotive industry, context-aware Advanced Driver Assistance Systems (ADAS) become possible. On one side, a common thread in ADAS applications is to focus entirely on the context of the vehicle and its surrounding vehicles leaving the human (driver) context out of consideration. On the other side, and due to the increasing sensing capabilities in mobile phones and wearable technologies, monitoring complex human context becomes feasible which paves the way to develop driver-in-the-loop context-aware ADAS that provide personalized driving experience. In this paper, we propose Sentio1; a Reinforcement Learning based algorithm to enhance the Forward Collision Warning (FCW) system leading to Driver-in-the-Loop FCW system. Since the human driving preference is unknown a priori, varies between different drivers, and moreover, varies across time for the same driver, the proposed Sentio algorithm needs to take into account all these variabilities which are not handled by the standard reinforcement learning algorithms. We verified the proposed algorithm against several human drivers. Our evaluation, across distracted human drivers, shows a significant enhancement in driver experience---compared to standard FCW systems---reflected by an increase in the driver safety by 94.28\%, an improvement in the driving experience by 20.97\%, a decrease in the false negatives from 55.90\% down to 3.26\%, while adding less than 130 ms runtime execution overhead.},
booktitle = {Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems},
pages = {28–40},
numpages = {13},
keywords = {Personalized ADAS, Human-in-the-Loop, Human Vehicular Interaction, Autonomous Systems, Reinforcement Learning},
location = {Shenzhen, China},
series = {SenSys '18}
}

@article{10.1145/3197517.3201311,
author = {Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and van de Panne, Michiel},
title = {DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3197517.3201311},
doi = {10.1145/3197517.3201311},
abstract = {A longstanding goal in character animation is to combine data-driven specification of behavior with a system that can execute a similar behavior in a physical simulation, thus enabling realistic responses to perturbations and environmental variation. We show that well-known reinforcement learning (RL) methods can be adapted to learn robust control policies capable of imitating a broad range of example motion clips, while also learning complex recoveries, adapting to changes in morphology, and accomplishing user-specified goals. Our method handles keyframed motions, highly-dynamic actions such as motion-captured flips and spins, and retargeted motions. By combining a motion-imitation objective with a task objective, we can train characters that react intelligently in interactive settings, e.g., by walking in a desired direction or throwing a ball at a user-specified target. This approach thus combines the convenience and motion quality of using motion clips to define the desired style and appearance, with the flexibility and generality afforded by RL methods and physics-based animation. We further explore a number of methods for integrating multiple clips into the learning process to develop multi-skilled agents capable of performing a rich repertoire of diverse skills. We demonstrate results using multiple characters (human, Atlas robot, bipedal dinosaur, dragon) and a large variety of skills, including locomotion, acrobatics, and martial arts.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {143},
numpages = {14},
keywords = {physics-based character animation, reinforcement learning, motion control}
}

@article{10.1145/3219752,
author = {Bentaleb, Abdelhak and Begen, Ali C. and Zimmermann, Roger},
title = {ORL-SDN: Online Reinforcement Learning for SDN-Enabled HTTP Adaptive Streaming},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1551-6857},
url = {https://doi.org/10.1145/3219752},
doi = {10.1145/3219752},
abstract = {In designing an HTTP adaptive streaming (HAS) system, the bitrate adaptation scheme in the player is a key component to ensure a good quality of experience (QoE) for viewers. We propose a new online reinforcement learning optimization framework, called ORL-SDN, targeting HAS players running in a software-defined networking (SDN) environment. We leverage SDN to facilitate the orchestration of the adaptation schemes for a set of HAS players. To reach a good level of QoE fairness in a large population of players, we cluster them based on a perceptual quality index. We formulate the adaptation process as a Partially Observable Markov Decision Process and solve the per-cluster optimization problem using an online Q-learning technique that leverages model predictive control and parallelism via aggregation to avoid a per-cluster suboptimal selection and to accelerate the convergence to an optimum. This framework achieves maximum long-term revenue by selecting the optimal representation for each cluster under time-varying network conditions. The results show that ORL-SDN delivers substantial improvements in viewer QoE, presentation quality stability, fairness, and bandwidth utilization over well-known adaptation schemes.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {aug},
articleno = {71},
numpages = {28},
keywords = {fastMPC, QoE optimization, HAS, HAS scalability issues, reinforcement learning, SDN, POMDP}
}

@inproceedings{10.1145/3230543.3230551,
author = {Chen, Li and Lingys, Justinas and Chen, Kai and Liu, Feng},
title = {AuTO: Scaling Deep Reinforcement Learning for Datacenter-Scale Automatic Traffic Optimization},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230551},
doi = {10.1145/3230543.3230551},
abstract = {Traffic optimizations (TO, e.g. flow scheduling, load balancing) in datacenters are difficult online decision-making problems. Previously, they are done with heuristics relying on operators' understanding of the workload and environment. Designing and implementing proper TO algorithms thus take at least weeks. Encouraged by recent successes in applying deep reinforcement learning (DRL) techniques to solve complex online control problems, we study if DRL can be used for automatic TO without human-intervention. However, our experiments show that the latency of current DRL systems cannot handle flow-level TO at the scale of current datacenters, because short flows (which constitute the majority of traffic) are usually gone before decisions can be made.Leveraging the long-tail distribution of datacenter traffic, we develop a two-level DRL system, AuTO, mimicking the Peripheral \&amp; Central Nervous Systems in animals, to solve the scalability problem. Peripheral Systems (PS) reside on end-hosts, collect flow information, and make TO decisions locally with minimal delay for short flows. PS's decisions are informed by a Central System (CS), where global traffic information is aggregated and processed. CS further makes individual TO decisions for long flows. With CS\&amp;PS, AuTO is an end-to-end automatic TO system that can collect network information, learn from past decisions, and perform actions to achieve operator-defined goals. We implement AuTO with popular machine learning frameworks and commodity servers, and deploy it on a 32-server testbed. Compared to existing approaches, AuTO reduces the TO turn-around time from weeks to ~100 milliseconds while achieving superior performance. For example, it demonstrates up to 48.14\% reduction in average flow completion time (FCT) over existing solutions.},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {191–205},
numpages = {15},
keywords = {traffic optimization, datacenter networks, reinforcement learning},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

@inproceedings{10.1145/3408308.3427986,
author = {Ding, Xianzhong and Du, Wan and Cerpa, Alberto E.},
title = {MB2C: Model-Based Deep Reinforcement Learning for Multi-Zone Building Control},
year = {2020},
isbn = {9781450380614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408308.3427986},
doi = {10.1145/3408308.3427986},
abstract = {Reinforcement learning has been widely studied for controlling Heating, Ventilation, and Air conditioning (HVAC) systems. Most of the existing works are focused on Model-Free Reinforcement Learning (MFRL), which learns an agent by extensively trial-and-error interaction with a real building. However, one of the fundamental problems with MFRL is the very large amount of training data required to converge to acceptable performance. Although simulation models have been used to generate sufficient training data to accelerate the training process, MFRL needs a high-fidelity building model for simulation, which is also hard to calibrate. As a result, Model-Based Reinforcement Learning (MBRL) has been used for HVAC control. While MBRL schemes can achieve excellent sample efficiency (i.e. less training data), they often lag behind model-free approaches in terms of asymptotic control performance (i.e. high energy savings while meeting occupants' thermal comfort).In this paper, we conduct a set of experiments to analyze the limitations of current MBRL-based HVAC control methods, in terms of model uncertainty and controller effectiveness. Using the lessons learned, we develop MB2C, a novel MBRL-based HVAC control system that can achieve high control performance with excellent sample efficiency. MB2C learns the building dynamics by employing an ensemble of environment-conditioned neural networks. It then applies a new control method, Model Predictive Path Integral (MPPI), for HVAC control. It produces candidate action sequences by using an importance sampling weighted algorithm that scales better to high state and action dimensions of multi-zone buildings. We evaluate MB2C using EnergyPlus simulations in a five-zone office building. The results show that MB2C can achieve 8.23\% more energy savings compared to the state-of-the-art MBRL solution while maintaining similar thermal comfort. MB2C can reduce the training data set by an order of magnitude (10.52\texttimes{}) while achieving comparable performance to MFRL approaches.},
booktitle = {Proceedings of the 7th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {50–59},
numpages = {10},
keywords = {HVAC Control, Model-based Deep Reinforcement Learning, Model Predictive Control},
location = {Virtual Event, Japan},
series = {BuildSys '20}
}

@article{10.5555/3586589.3586863,
author = {Huang, Shengyi and Dossa, Rousslan Fernand JulienDossa and Ye, Chang and Braga, Jeff and Chakraborty, Dipam and Mehta, Kinal and Ara\'{u}jo, Jo\~{a}o G.M.},
title = {CleanRL: High-Quality Single-File Implementations of Deep Reinforcement Learning Algorithms},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {CleanRL is an open-source library that provides high-quality single-file implementations of Deep Reinforcement Learning (DRL) algorithms. These single-file implementations are self-contained algorithm variant files such as dqn.py, ppo.py, and ppo_atari.py that individually include all algorithm variant's implementation details. Such a paradigm significantly reduces the complexity and the lines of code (LOC) in each implemented variant, which makes them quicker and easier to understand. This paradigm gives the researchers the most fine-grained control over all aspects of the algorithm in a single file, allowing them to prototype novel features quickly. Despite having succinct implementations, CleanRL's codebase is thoroughly documented and benchmarked to ensure performance is on par with reputable sources. As a result, CleanRL produces a repository tailor-fit for two purposes: 1) understanding all implementation details of DRL algorithms and 2) quickly prototyping novel features.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {274},
numpages = {18},
keywords = {deep reinforcement learning, single-file implementation, open-source}
}

@article{10.1613/jair.1.11251,
author = {Liu, Bo and Gemp, Ian and Ghavamzadeh, Mohammad and Liu, Ji and Mahadevan, Sridhar and Petrik, Marek},
title = {Proximal Gradient Temporal Difference Learning: Stable Reinforcement Learning with Polynomial Sample Complexity},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11251},
doi = {10.1613/jair.1.11251},
abstract = {In this paper, we introduce proximal gradient temporal difference learning, which provides a principled way of designing and analyzing true stochastic gradient temporal difference learning algorithms. We show how gradient TD (GTD) reinforcement learning methods can be formally derived, not by starting from their original objective functions, as previously attempted, but rather from a primal-dual saddle-point objective function. We also conduct a saddle-point error analysis to obtain finite-sample bounds on their performance. Previous analyses of this class of algorithms use stochastic approximation techniques to prove asymptotic convergence, and do not provide any finite-sample analysis. We also propose an accelerated algorithm, called GTD2-MP, that uses proximal "mirror maps" to yield an improved convergence rate. The results of our theoretical analysis imply that the GTD family of algorithms are comparable and may indeed be preferred over existing least squares TD methods for off-policy learning, due to their linear complexity. We provide experimental results showing the improved performance of our accelerated gradient TD methods.},
journal = {J. Artif. Int. Res.},
month = {sep},
pages = {461–494},
numpages = {34}
}

@inproceedings{10.1145/3491087.3493673,
author = {Tahir, Jawad},
title = {Config 2.0: Towards Reinforcement Learning Based Configuration of Stream Processing Systems},
year = {2021},
isbn = {9781450391559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491087.3493673},
doi = {10.1145/3491087.3493673},
abstract = {Distributed stream processing systems (DSPSs) have enabled us to build scalable, fast, and reactive applications. As faults are common in any distributed system, DSPSs allow for recovering from faults using various techniques, for example, snapshotting their state on a key-value (KV) store and then replaying the events in case of a failure. The performance of the recovery process is dependent on the workload, configuration of the DSPS and performance of the KV store. Performance tuning of a KV store is a notoriously difficult job. In this thesis proposal, we propose a reinforcement learning (RL) based agent which configures the DSPS and the KV store under varying workloads to minimize the impact of a failure on the performance.},
booktitle = {Proceedings of the 22nd International Middleware Conference: Doctoral Symposium},
pages = {1–3},
numpages = {3},
keywords = {fault-tolerance, reinforcement learning, stream processing},
location = {Qu\'{e}bec city, Canada},
series = {Middleware '21}
}

@inproceedings{10.1145/3405671.3405809,
author = {Zhang, Junjie and Guo, Zehua and Ye, Minghao and Chao, H. Jonathan},
title = {SmartEntry: Mitigating Routing Update Overhead with Reinforcement Learning for Traffic Engineering},
year = {2020},
isbn = {9781450380430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405671.3405809},
doi = {10.1145/3405671.3405809},
abstract = {Traffic Engineering (TE) has been used by Internet service providers to improve their network performance and provide better service quality to users. While flow-based TE is an alternative, destination-based TE is a more readily deployed solution. This is because destination-based forwarding is ubiquitously supported by today's routers. A challenge faced by state-of-the-art destination-based TE solutions is considerable time taken by a centralized controller to update traffic split ratios for each entry of the forwarding table of each router. This could impose a fundamental limitation on how responsively the network can react to dynamic changes of traffic demands. In this paper, we propose SmartEntry, a destination-based routing solution coupled with Reinforcement Learning (RL) to reduce the number of the forwarding entries that need to be updated to respond to dynamic change of traffic demands. SmartEntry forwards majority traffic on Equal-Cost Multi-Path (ECMP) and redistributes a small portion of traffic using our proposed RL algorithm. SmartEntry adopts Linear Programming (LP) to produce reward signals. This RL + LP combined approach turns out to be surprisingly effective. We evaluate SmartEntry by conducting extensive experiments on different network topologies with both real and synthesized traffic. The simulation results show that SmartEntry achieves near-optimal performance with a saving of 90\% forwarding entry updates, and generalizes well to unseen traffic matrices.},
booktitle = {Proceedings of the Workshop on Network Meets AI \&amp; ML},
pages = {1–7},
numpages = {7},
keywords = {Traffic Engineering, Routing Update Overhead, Reinforcement Learning, Linear Programming},
location = {Virtual Event, USA},
series = {NetAI '20}
}

@inproceedings{10.1145/3459637.3482360,
author = {Mills, Keith G. and Han, Fred X. and Salameh, Mohammad and Changiz Rezaei, Seyed Saeed and Kong, Linglong and Lu, Wei and Lian, Shuo and Jui, Shangling and Niu, Di},
title = {L2NAS: Learning to Optimize Neural Architectures via Continuous-Action Reinforcement Learning},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482360},
doi = {10.1145/3459637.3482360},
abstract = {Neural architecture search (NAS) has achieved remarkable results in deep neural network design. Differentiable architecture search converts the search over discrete architectures into a hyperparameter optimization problem which can be solved by gradient descent. However, questions have been raised regarding the effectiveness and generalizability of gradient methods for solving non-convex architecture hyperparameter optimization problems. In this paper, we propose L2NAS, which learns to intelligently optimize and update architecture hyperparameters via an actor neural network based on the distribution of high-performing architectures in the search history. We introduce a quantile-driven training procedure which efficiently trains L2NAS in an actor-critic framework via continuous-action reinforcement learning. Experiments show that L2NAS achieves state-of-the-art results on NAS-Bench-201 benchmark as well as DARTS search space and Once-for-All MobileNetV3 search space. We also show that search policies generated by L2NAS are generalizable and transferable across different training datasets with minimal fine-tuning.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {1284–1293},
numpages = {10},
keywords = {neural architecture search, deep deterministic policy gradient},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3511808.3557400,
author = {Fang, Zekuan and Zhang, Fan and Wang, Ting and Lian, Xiang and Chen, Mingsong},
title = {MonitorLight: Reinforcement Learning-Based Traffic Signal Control Using Mixed Pressure Monitoring},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557400},
doi = {10.1145/3511808.3557400},
abstract = {Although Reinforcement Learning (RL) has achieved significant success in the Traffic Signal Control (TSC), most of them focus on the design of RL elements while the impact of the phase duration is neglected. Due to the lack of exploring dynamic phase duration, the overall performance and convergence rate of RL-based TSC approaches cannot be guaranteed, which may result in poor adaptability of RL methods to different traffic conditions. To address these issues, in this paper, we formulate a novel phase-duration-aware TSC (PDA-TSC) problem and propose an effective RL-based TSC approach, named MonitorLight. Our approach adopts a new traffic indicator, mixed pressure, which enables RL agents to simultaneously analyze the impacts of stationary and moving vehicles on intersections. Based on the observed mixed pressure of intersections, RL agents can autonomously determine whether or not to change the current signals in real-time. In addition, MonitorLight can adjust the control method for scenarios with different real-time requirements and achieve excellent results in different situations. Extensive experiments on both real-world and synthetic datasets demonstrate that MonitorLight outperforms the current state-of-the-art IPDALight by up to 2.84\% and 5.71\% in average vehicle travel time, respectively. Moreover, our method significantly speeds up the convergence, leading IPDALight by 36.87\% and 34.58\% in the start to converge episode and jumpstart performance, respectively.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {478–487},
numpages = {10},
keywords = {average travel time, phase duration, reinforcement learning, fairness, traffic signal control},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.14778/3352063.3352129,
author = {Li, Guoliang and Zhou, Xuanhe and Li, Shifu and Gao, Bo},
title = {QTune: A Query-Aware Database Tuning System with Deep Reinforcement Learning},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352129},
doi = {10.14778/3352063.3352129},
abstract = {Database knob tuning is important to achieve high performance (e.g., high throughput and low latency). However, knob tuning is an NP-hard problem and existing methods have several limitations. First, DBAs cannot tune a lot of database instances on different environments (e.g., different database vendors). Second, traditional machine-learning methods either cannot find good configurations or rely on a lot of high-quality training examples which are rather hard to obtain. Third, they only support coarse-grained tuning (e.g., workload-level tuning) but cannot provide fine-grained tuning (e.g., query-level tuning).To address these problems, we propose a query-aware database tuning system QTune with a deep reinforcement learning (DRL) model, which can efficiently and effectively tune the database configurations. QTune first featurizes the SQL queries by considering rich features of the SQL queries. Then QTune feeds the query features into the DRL model to choose suitable configurations. We propose a Double-State Deep Deterministic Policy Gradient (DS-DDPG) model to enable query-aware database configuration tuning, which utilizes the actor-critic networks to tune the database configurations based on both the query vector and database states. QTune provides three database tuning granularities: query-level, workload-level, and cluster-level tuning. We deployed our techniques onto three real database systems, and experimental results show that QTune achieves high performance and outperforms the state-of-the-art tuning methods.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2118–2130},
numpages = {13}
}

@inproceedings{10.5555/1782254.1782306,
author = {Celiberto, Luiz A. and Matsuura, Jackson and Bianchi, Reinaldo A. C.},
title = {Heuristic Q-Learning Soccer Players: A New Reinforcement Learning Approach to RoboCup Simulation},
year = {2007},
isbn = {3540770003},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes the design and implementation of a 4 player RoboCup Simulation 2D team, which was build by adding Heuristic Accelerated Reinforcement Learning capabilities to basic players of the well-known UvA Trilearn team. The implemented agents learn by using a recently proposed Heuristic Reinforcement Learning algorithm, the Heuristically Accelerated Q-Learning (HAQL), which allows the use of heuristics to speed up the well-known Reinforcement Learning algorithm Q-Learning. A set of empirical evaluations was conducted in the RoboCup 2D Simulator, and experimental results obtained while playing with other teams shows that the approach adopted here is very promising.},
booktitle = {Proceedings of the Aritficial Intelligence 13th Portuguese Conference on Progress in Artificial Intelligence},
pages = {520–529},
numpages = {10},
keywords = {reinforcement learning, robocup simulation 2D, cognitive robotics},
location = {Guimar\~{a}es, Portugal},
series = {EPIA'07}
}

@inproceedings{10.1145/3477314.3507691,
author = {Sharma, Vishal and Dyreson, Curtis},
title = {Indexer++: Workload-Aware Online Index Tuning with Transformers and Reinforcement Learning},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507691},
doi = {10.1145/3477314.3507691},
abstract = {With the increasing workload complexity in modern databases, the manual process of index selection is a challenging task. There is a growing need for a database with an ability to learn and adapt to evolving workloads. This paper proposes Indexer++, an autonomous, workload-aware, online index tuner. Unlike existing approaches, Indexer++ imposes low overhead on the DBMS, is responsive to changes in query workloads and swiftly selects indexes. Our approach uses a combination of text analytic techniques and reinforcement learning. Indexer++ consist of two phases: Phase (i) learns workload trends using a novel trend detection technique based on a pre-trained transformer model. Phase (ii) performs online, i.e., continuous or while the DBMS is processing workloads, index selection using a novel online deep reinforcement learning technique using our proposed priority experience sweeping. This paper provides an experimental evaluation of Indexer++ in multiple scenarios using benchmark (TPC-H) and real-world datasets (IMDB). In our experiments, Indexer++ effectively identifies changes in workload trends and selects the set of optimal indexes.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {372–380},
numpages = {9},
keywords = {pre-trained transformers, reinforcement learning, workload trend detection, online index selection},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3538637.3539616,
author = {Biagioni, David and Zhang, Xiangyu and Wald, Dylan and Vaidhynathan, Deepthi and Chintala, Rohit and King, Jennifer and Zamzam, Ahmed S.},
title = {PowerGridworld: A Framework for Multi-Agent Reinforcement Learning in Power Systems},
year = {2022},
isbn = {9781450393973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538637.3539616},
doi = {10.1145/3538637.3539616},
abstract = {We present the PowerGridworld open source software package to provide users with a lightweight, modular, and customizable framework for creating power-systems-focused, multi-agent Gym environments that readily integrate with existing training frameworks for reinforcement learning (RL). Although many frameworks exist for training multi-agent RL (MARL) policies, none can rapidly prototype and develop the environments themselves, especially in the context of heterogeneous (composite, multi-device) power systems where power flow solutions are required to define grid-level variables and costs. PowerGridworld helps to fill this gap. To highlight PowerGridworld's key features, we present two case studies and demonstrate learning MARL policies using both OpenAI's multi-agent deep deterministic policy gradient (MADDPG) and RL-Lib's proximal policy optimization (PPO) algorithms. In both cases, at least some subset of agents incorporates elements of the power flow solution at each time step as part of their reward (negative cost) structures.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Future Energy Systems},
pages = {565–570},
numpages = {6},
keywords = {deep learning, OpenAI gym, multi-agent systems, power systems, reinforcement learning},
location = {Virtual Event},
series = {e-Energy '22}
}

@article{10.1145/3414685.3417788,
author = {Wang, Hanqing and Liang, Wei and Yu, Lap-Fai},
title = {Scene Mover: Automatic Move Planning for Scene Arrangement by Deep Reinforcement Learning},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3414685.3417788},
doi = {10.1145/3414685.3417788},
abstract = {We propose a novel approach for automatically generating a move plan for scene arrangement.1 Given a scene like an apartment with many furniture objects, to transform its layout into another layout, one would need to determine a collision-free move plan. It could be challenging to design this plan manually because the furniture objects may block the way of each other if not moved properly; and there is a large complex search space of move action sequences that grow exponentially with the number of objects. To tackle this challenge, we propose a learning-based approach to generate a move plan automatically. At the core of our approach is a Monte Carlo tree that encodes possible states of the layout, based on which a search is performed to move a furniture object appropriately in the current layout. We trained a policy neural network embedded with a LSTM module for estimating the best actions to take in the expansion step and simulation step of the Monte Carlo tree search process. Leveraging the power of deep reinforcement learning, the network learned how to make such estimations through millions of trials of moving objects. We demonstrated our approach for moving objects under different scenarios and constraints. We also evaluated our approach on synthetic and real-world layouts, comparing its performance with that of humans and other baseline approaches.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {233},
numpages = {15},
keywords = {layout design, reinforcement learning}
}

@inproceedings{10.1145/3383652.3423868,
author = {Pagalyte, Elinga and Mancini, Maurizio and Climent, Laura},
title = {Go with the Flow: Reinforcement Learning in Turn-Based Battle Video Games},
year = {2020},
isbn = {9781450375863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383652.3423868},
doi = {10.1145/3383652.3423868},
abstract = {Game flow represents a state where the player is neither frustrated nor bored. In turn-based battle video games it can be achieved by Dynamic Difficulty Adjustment (DDA), whose research has begun rising over the last decade. This paper introduces an idea for incorporating DDA through the use of Reinforcement Learning (RL) to agents of turn-based battle video games. We design and implement an RL agent that shows, in a simple environment, the idea of how a game could achieve balance through adequate choices in actions depending on the player's level of skill.For achieving this purpose, we incorporated the design and implementation of state-action-reward-state-action (SARSA) algorithm to the agent of our implemented game. In addition, we added tracking of the on-going games and depending on the frequency of the player's repeated wins or losses, the rewards of the RL agent are modified. This modification of the rewards has an impact on the RL agent's actions, which involves an increase/decrease of the difficulty of the battle game. The evaluation performed shows that the idea of the paper is demonstrated, since players face personalized challenges that we believe are in range of game flow.},
booktitle = {Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents},
articleno = {44},
numpages = {8},
keywords = {Reinforcement Learning, RL, Game Flow, Turn-based battle video game, SARSA, Dynamic Difficulty Adjustment, DDA},
location = {Virtual Event, Scotland, UK},
series = {IVA '20}
}

@inproceedings{10.1145/3356250.3360038,
author = {Shen, Zhihao and Yang, Kang and Du, Wan and Zhao, Xi and Zou, Jianhua},
title = {DeepAPP: A Deep Reinforcement Learning Framework for Mobile Application Usage Prediction},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360038},
doi = {10.1145/3356250.3360038},
abstract = {This paper aims to predict the apps a user will open on her mobile device next. Such an information is essential for many smartphone operations, e.g., app pre-loading and content pre-caching, to save mobile energy. However, it is hard to build an explicit model that accurately depicts the affecting factors and their affecting mechanism of time-varying app usage behavior. This paper presents a deep reinforcement learning framework, named as DeepAPP, which learns a model-free predictive neural network from historical app usage data. Meanwhile, an online updating strategy is designed to adapt the predictive network to the time-varying app usage behavior. To transform DeepAPP into a practical deep reinforcement learning system, several challenges are addressed by developing a context representation method for complex contextual environment, a general agent for overcoming data sparsity and a lightweight personalized agent for minimizing the prediction time. Extensive experiments on a large-scale anonymized app usage dataset reveal that DeepAPP provides high accuracy (precision 70.6\% and recall of 62.4\%) and reduces the prediction time of the state-of-the-art by 6.58\texttimes{}. A field experiment of 29 participants also demonstrates DeepAPP can effectively reduce time of loading apps.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {153–165},
numpages = {13},
keywords = {deep reinforcement learning, neural networks, app usage prediction, mobile devices},
location = {New York, New York},
series = {SenSys '19}
}

@inproceedings{10.5555/2615731.2615837,
author = {Das, Sanmay and Lavoie, Allen},
title = {The Effects of Feedback on Human Behavior in Social Media: An Inverse Reinforcement Learning Model},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We introduce and validate a learning model of human behavior change in response to feedback on social media. People who participate in these types of websites, like Wikipedia, Reddit, and others, are learning agents whose choices about how to allocate their effort are dynamic and responsive to how they feel their efforts were received in the past. By explicitly taking into account the reinforcement effects of different types of feedback received on prior contributions, our model is able to significantly outperform all known baselines in predicting future contributions both on synthetic data and on real data collected from the social news site reddit.com. Our model has an intuitive interpretation as users playing mixed strategies in a game-like setting with thousands of other users and thousands of available pure strategies. In this interpretation, our task is then inverse reinforcement learning: recovering users' reward functions based on observed behavior.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {653–660},
numpages = {8},
keywords = {social simulation, multi-agent learning, social media},
location = {Paris, France},
series = {AAMAS '14}
}

@article{10.14778/3476249.3476303,
author = {Yang, Junwen and He, Yeye and Chaudhuri, Surajit},
title = {Auto-Pipeline: Synthesizing Complex Data Pipelines by-Target Using Reinforcement Learning and Search},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476303},
doi = {10.14778/3476249.3476303},
abstract = {Recent work has made significant progress in helping users to automate single data preparation steps, such as string-transformations and table-manipulation operators (e.g., Join, GroupBy, Pivot, etc.). We in this work propose to automate multiple such steps end-to-end, by synthesizing complex data-pipelines with both string-transformations and table-manipulation operators.We propose a novel by-target paradigm that allows users to easily specify the desired pipeline, which is a significant departure from the traditional by-example paradigm. Using by-target, users would provide input tables (e.g., csv or json files), and point us to a "target table" (e.g., an existing database table or BI dashboard) to demonstrate how the output from the desired pipeline would schematically "look like". While the problem is seemingly under-specified, our unique insight is that implicit table constraints such as FDs and keys can be exploited to significantly constrain the space and make the problem tractable. We develop an AUTO-PIPELINE system that learns to synthesize pipelines using deep reinforcement-learning (DRL) and search. Experiments using a benchmark of 700 real pipelines crawled from GitHub and commercial vendors suggest that AUTO-PIPELINE can successfully synthesize around 70\% of complex pipelines with up to 10 steps.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2563–2575},
numpages = {13}
}

@inproceedings{10.1145/3459637.3481967,
author = {Personnaz, Aur\'{e}lien and Amer-Yahia, Sihem and Berti-Equille, Laure and Fabricius, Maximilian and Subramanian, Srividya},
title = {DORA THE EXPLORER: Exploring Very Large Data With Interactive Deep Reinforcement Learning},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481967},
doi = {10.1145/3459637.3481967},
abstract = {We demonstrate DORA THE EXPLORER, a system that guides users in finding items of interest in a very large data set. DORA THE EXPLORER provides users with the full spectrum of exploration modes and is driven by Data Familiarity or Curiosity, as well as User Interventions. DORA THE EXPLORER is able to handle data and search scenario complexity, i.e., the difficulty to find scattered/clustered individual records in the data set, and user ability to express what s/he needs. DORA THE EXPLORER relies on Deep Reinforcement Learning that combines intrinsic (curiosity) and extrinsic (familiarity) rewards. DORA's main goal is to support scientific discovery from data. We describe the system architecture and illustrate it with three demonstration scenarios on a 2.6 mil-lion galaxies SDSS, a large sky survey data set1. A video of DORA THE EXPLORER is available at https://bit.ly/dora-demo, the codehttps://github.com/apersonnaz/rl-guided-galaxy-exploration, and the application at https://bit.ly/dora-application},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {4769–4773},
numpages = {5},
keywords = {user feedback, data exploration, sdss datasets, reinforcement learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.5555/3306127.3331715,
author = {Pan, Xinlei and Wang, Weiyao and Zhang, Xiaoshuai and Li, Bo and Yi, Jinfeng and Song, Dawn},
title = {How You Act Tells a Lot: Privacy-Leaking Attack on Deep Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Machine learning has been widely applied to various applications, some of which involve training with privacy-sensitive data. A modest number of data breaches have been studied, including credit card information in natural language data and identities from face dataset. However, most of these studies focus on supervised learning models. As deep reinforcement learning (DRL) has been deployed in a number of real-world systems, such as indoor robot navigation, whether trained DRL policies can leak private information requires in-depth study.To explore such privacy breaches in general, we mainly propose two methods: environment dynamics search via genetic algorithm and candidate inference based on shadow policies. We conduct extensive experiments to demonstrate such privacy vulnerabilities in DRL under various settings. We leverage the proposed algorithms to infer floor plans from some trained Grid World navigation DRL agents with LiDAR perception. The proposed algorithm can correctly infer most of the floor plans and reaches an average recovery rate of 95.83\% using policy gradient trained agents. In addition, we are able to recover the robot configuration in continuous control environments and an autonomous driving simulator with high accuracy. To the best of our knowledge, this is the first work to investigate privacy leakage in DRL settings and we show that DRL-based agents do potentially leak privacy-sensitive information from the trained policies.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {368–376},
numpages = {9},
keywords = {dynamics recovery, deep reinforcement learning, privacy},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3326285.3329056,
author = {Xiao, Yikai and Zhang, Qixia and Liu, Fangming and Wang, Jia and Zhao, Miao and Zhang, Zhongxing and Zhang, Jiaxing},
title = {NFVdeep: Adaptive Online Service Function Chain Deployment with Deep Reinforcement Learning},
year = {2019},
isbn = {9781450367783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3326285.3329056},
doi = {10.1145/3326285.3329056},
abstract = {With the evolution of network function virtualization (NFV), diverse network services can be flexibly offered as service function chains (SFCs) consisted of different virtual network functions (VNFs). However, network state and traffic typically exhibit unpredictable variations due to stochastically arriving requests with different quality of service (QoS) requirements. Thus, an adaptive online SFC deployment approach is needed to handle the real-time network variations and various service requests. In this paper, we firstly introduce a Markov decision process (MDP) model to capture the dynamic network state transitions. In order to jointly minimize the operation cost of NFV providers and maximize the total throughput of requests, we propose NFVdeep, an adaptive, online, deep reinforcement learning approach to automatically deploy SFCs for requests with different QoS requirements. Specifically, we use a serialization-and-backtracking method to effectively deal with large discrete action space. We also adopt a policy gradient based method to improve the training efficiency and convergence to optimality. Extensive experimental results demonstrate that NFVdeep converges fast in the training process and responds rapidly to arriving requests especially in large, frequently transferred network state space. Consequently, NFVdeep surpasses the state-of-the-art methods by 32.59\% higher accepted throughput and 33.29\% lower operation cost on average.},
booktitle = {Proceedings of the International Symposium on Quality of Service},
articleno = {21},
numpages = {10},
keywords = {deep reinforcement learning, QoS-aware resource management, network function virtualization (NFV), service function chain},
location = {Phoenix, Arizona},
series = {IWQoS '19}
}

@inproceedings{10.1145/3397271.3401170,
author = {Zhao, Dongyang and Zhang, Liang and Zhang, Bo and Zheng, Lizhou and Bao, Yongjun and Yan, Weipeng},
title = {MaHRL: Multi-Goals Abstraction Based Deep Hierarchical Reinforcement Learning for Recommendations},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401170},
doi = {10.1145/3397271.3401170},
abstract = {As huge commercial value of the recommender system, there has been growing interest to improve its performance in recent years. The majority of existing methods have achieved great improvement on the metric of click, but perform poorly on the metric of conversion possibly due to its extremely sparse feedback signal. To track this challenge, we design a novel deep hierarchical reinforcement learning based recommendation framework to model consumers' hierarchical purchase interest. Specifically, the high-level agent catches long-term sparse conversion interest, and automatically sets abstract goals for low-level agent, while the low-level agent follows the abstract goals and catches short-term click interest via interacting with real-time environment. To solve the inherent problem in hierarchical reinforcement learning, we propose a novel multi-goals abstraction based deep hierarchical reinforcement learning algorithm (MaHRL). Our proposed algorithm contains three contributions: 1) the high-level agent generates multiple goals to guide the low-level agent in different sub-periods, which reduces the difficulty of approaching high-level goals; 2) different goals share the same state encoder structure and its parameters, which increases the update frequency of the high-level agent and thus accelerates the convergence of our proposed algorithm; 3) an appreciated reward assignment mechanism is designed to allocate rewards in each goal so as to coordinate different goals in a consistent direction. We evaluate our proposed algorithm based on a real-world e-commerce dataset and validate its effectiveness.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {871–880},
numpages = {10},
keywords = {recommender systems, conversion, multi-goals, deep hierarchical reinforcement learning},
location = {Virtual Event, China},
series = {SIGIR '20}
}

