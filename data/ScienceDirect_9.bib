@article{BERTOLINI2022116995,
title = {Power output optimization of electric vehicles smart charging hubs using deep reinforcement learning},
journal = {Expert Systems with Applications},
volume = {201},
pages = {116995},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116995},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422004158},
author = {Andrea Bertolini and Miguel S.E. Martins and Susana M. Vieira and João M.C. Sousa},
keywords = {Reinforcement learning, Electric vehicles, Real-time charging scheduling, Neural network, Clustering algorithm},
abstract = {Since most branches of the distribution grid may already be close to their maximum capacity, smart management when charging electric vehicles (EVs) is becoming more and more crucial. In fact, office buildings might not be able to handle several transactions at the same time, especially considering the next generation of fast chargers which are very power expensive. Thus, an efficient charging policy needs to be found. This paper proposes the scheduling of real-time EVs charging through deep reinforcement learning (DRL) techniques. DRL has been chosen because it can adaptively learn from interacting with the surrounding environment. The focus of the optimization is to ensure the completion of the charging transactions in a timely manner, while shifting the load from the times of peak demand. The novelty of the proposed approach lies in its innovative framework: pools of electric vehicles with different characteristics are categorized using a clustering algorithm, a tree-based classifier has been developed to sort new instances of EVs, and a multilayer perceptron artificial deep neural network has been trained to predict the expected duration of each charging session. These features are used as inputs to the DRL agent, and are mapped into actions that adjust the maximum power associated to each charging station. The model has been compared to a traditional charging algorithm and increasingly challenging scenarios have been considered. Results have shown that the developed algorithm fails less than the baseline, with a reduction of the load due to EVs charging of 80% during peak times.}
}
@article{SOLEIMANZADE2022119184,
title = {Novel data-driven energy management of a hybrid photovoltaic-reverse osmosis desalination system using deep reinforcement learning},
journal = {Applied Energy},
volume = {317},
pages = {119184},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119184},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922005566},
author = {Mohammad Amin Soleimanzade and Amit Kumar and Mohtada Sadrzadeh},
keywords = {Energy management, Deep reinforcement learning, Actor-critic methods, Partially observable Markov decision process, Reverse osmosis, Pressure retarded osmosis},
abstract = {This paper proposes a novel deep reinforcement learning-accelerated energy management system for a hybrid grid-connected photovoltaic-reverse osmosis-pressure retarded osmosis desalination plant. The energy management problem is formulated as a partially observable Markov decision process by using historical photovoltaic (PV) power data in order to cope with uncertainties related to the generation of solar power and provide more information regarding the true state of the system. The soft actor-critic (SAC) algorithm is employed as the core of the energy management system to maximize water production rate and contaminant removal efficiency while minimizing the supplied power from the external grid. We introduce 1-dimensional convolutional neural networks (1-D CNNs) to the actor, critic, and value function networks of the SAC algorithm to address the partial observability dilemma involved in PV-powered energy systems, extract essential features from the PV power time series, and achieve immensely improved performance ultimately. Furthermore, it is assumed that the proposed CNN-SAC algorithm does not have access to the current output power data of the PV system. The development of more practical energy management systems necessitates this assumption, and we demonstrate that the proposed method is capable of forecasting the current PV power data. The superiority of the CNN-SAC model is verified by comparing its learning performance and simulation results with those of four state-of-the-art deep reinforcement learning algorithms: Deep deterministic policy gradient (DDPG), proximal policy optimization (PPO), twin delayed DDPG (TD3), and vanilla SAC. The results show that the CNN-SAC model outperforms the benchmark methods in terms of effective solar energy exploitation and power scheduling, manifesting the necessity of exploiting historical PV power data and 1-D CNNs. Moreover, the CNN-SAC algorithm is benchmarked against a powerful energy management system we developed in our previous investigation by studying three scenarios, and it is demonstrated that considerable improvement in energy efficiency can be obtained without using any solar power generation forecasting algorithm. By conducting ablation studies, the critical contribution of the introduced 1-D CNN is demonstrated, and we highlight the significance of providing historical PV power data for substantial performance enhancement. The average and standard deviation of evaluation scores obtained during the last stages of training reveal that the 1-D CNN significantly improves the final performance and stability of the SAC algorithm. These results demonstrate that the modifications we detail in our investigation render deep reinforcement learning algorithms extremely powerful for the energy management of PV-powered microgrids, including PV-driven reverse osmosis desalination plants.}
}
@article{GLASS20223367,
title = {Synthetic Pedestrian Routes Generation: Exploring Mobility Behavior of Citizens through Multi-Agent Reinforcement Learning},
journal = {Procedia Computer Science},
volume = {207},
pages = {3367-3375},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.395},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922012856},
author = {Ayşe Glass and Jörg Rainer Noennig},
keywords = {Synthetic generation, mobility behavior, reinforcement learning, smart city},
abstract = {The data-driven city is improving the quality of everyday life for citizens and the efficient use of resources. While data-driven cities are benefiting from artificial intelligence technologies, one of the major challenges are the security, privacy, managing issues and costs. Furthermore, the training data for machine learning methods are in many cases not sufficiently available. Simulation tools are available for the designers or administration, and synthetic data generation based on real data is a promising approach on the development of new methods. This paper reports an exploratory research, targeting data generation with agent-based simulations combined with reinforcement learning. The study is still in its starting phase, but the conceptual outline and methodology has been created. The goal of this study is to establish a model to retrieve insights into the integration from a restricted data set from Google Maps and agent-based simulations. One of the primary observations is that intelligent agents, trained/reinforced paths instead of random behavior, show vastly different behavior than randomly acting agents. Evaluate the behavior according to empirical data, mostly because of the advantage of this method for complex behavior, where it would impractical to iterate through all possible paths, resulting in methodological issues. The paper exposes the following methodical challenges. (1) Realistic data sets will be large and full of decisions that appear to be random. To verify the collected, empirical data might lead to unreliable results, as actors will, as in real life, make different decisions according to circumstance. (2) The more complex a model has to be to be equivalent to the synthetic model, the more powerful the computational resources have to be. This leads to a dilemma between the model's complexity to make results comparable, and the simplicity to make computation realistic. Outline of the papers framework: (1) The generated data set challenges explored with the goal of providing realistic synthetic data. (2) A method for integration of synthetic and real-world data based on the reinforcement learning is developed. The premise is that implementing a reinforcement learning framework on top of multi-agent systems makes it possible to understand the mobility behavior of citizens. The paper explains the feasibility of both points, showing that this approach forms a solid basis for further investigation of the synthetic data generation for recognizing the mobility behavior of the citizens and enables the researchers to investigate the usage of reinforcement learning approach on the human mobility routes.}
}
@article{ZHAO202267,
title = {A deep reinforcement learning based searching method for source localization},
journal = {Information Sciences},
volume = {588},
pages = {67-81},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.12.041},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521012615},
author = {Yong Zhao and Bin Chen and XiangHan Wang and Zhengqiu Zhu and Yiduo Wang and Guangquan Cheng and Rui Wang and Rongxiao Wang and Ming He and Yu Liu},
keywords = {Autonomous searching method, Deep reinforcement learning, DBSCAN algorithm, Entrotaxis algorithm},
abstract = {The localization of hazardous sources (e.g. poisonous gas sources) is an important task regarding the security of human society. To find the unknown source in time, various autonomous source searching methods have mushroomed and been employed over the past decade. This paper designs a fresh source searching approach, namely particle clustering-deep Q-network, PC-DQN, which applies the deep reinforcement learning (DRL) techniques as a source searching approach for the first time. Specifically, the search process is formulated as the partially observable Markov decision process, then converted into the Markov decision process based on the belief state (represented by the particle filter). PC-DQN leverages the density-based spatial clustering of applications with noise (DBSCAN) algorithm to extract the feature of belief state, and employ the deep Q-network (DQN) algorithm to find the optimal policy for the source searching task. Through the comparison with two baseline methods (i.e. RANDOM and Entrotaxis algorithm) under various experimental conditions, the viability of our proposed PC-DQN is testified. Results explicitly reveal that the success rate of the PC-DQN maintains at a high level (beyond 99.6%) in all scenarios in this paper, and the mean search step shows evident superiority over baseline methods in most scenarios. Significantly, we also introduce the transfer learning concept to reuse the well-trained Q-network into new scenarios. These findings show important implications of the DRL-based approach as an alternative and more effective source searching approach.}
}
@article{HOU2023102176,
title = {Secondary crash mitigation controller after rear-end collisions using reinforcement learning},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102176},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102176},
url = {https://www.sciencedirect.com/science/article/pii/S147403462300304X},
author = {Xiaohui Hou and Minggang Gan and Junzhi Zhang and Shiyue Zhao and Yuan Ji},
keywords = {Rear-end collision, Post-collision control, Reinforcement learning, Drift operation mechanism, Vehicle stability control, Vehicle dynamics},
abstract = {Rear-end collisions result in a large number of casualties and property losses, and the serious injury risk in multiple impact accidents is much higher than that in single impact accidents. In this paper, we propose a novel controller to facilitate the prevention of secondary crashes after an initial rear-end collision, which expands the operational horizon of conventional vehicle active safety systems from preventive measures to post-event mitigation measures. Considering the complexity of the problem with multi-object synthesis optimization and vehicle nonlinear dynamics, this study combines the pre-collision control and post-collision control to reduce the initial crash loss and the subsequent control difficulty. The rule-based switching control and drift manipulation are embedded into the reinforcement learning algorithm to improve the training efficiency and control performance. The bench test results validate the superiority of the proposed controller over other strategies and algorithms in different rear-end collision scenarios.}
}
@article{LIU2022105044,
title = {Constructing growth evolution laws of arteries via reinforcement learning},
journal = {Journal of the Mechanics and Physics of Solids},
volume = {168},
pages = {105044},
year = {2022},
issn = {0022-5096},
doi = {https://doi.org/10.1016/j.jmps.2022.105044},
url = {https://www.sciencedirect.com/science/article/pii/S002250962200223X},
author = {Minliang Liu and Liang Liang and Hai Dong and Wei Sun and Rudolph L. Gleason},
keywords = {Vascular growth, Reinforcement learning, Hypertensive remodeling, Residual stress, Kinematic growth},
abstract = {Growth evolution laws, which mathematically describe how living tissues change their shape and properties in response to external stimuli, are required for modeling arterial growth. Traditionally, specific forms of growth laws are devised by domain experts. Since in vivo animal studies usually provide limited experimental data, generalization and inference are often employed to prescribe the functional form of growth laws. In this work, we employed the finite growth theory and developed a reinforcement learning (RL) approach to construct growth evolution laws by formulating the arterial growth problem under the framework of the Markov decision process (MDP). To maintain homeostatic stress levels in an optimal manner, RL agents were employed to determine stress-modulated anisotropic growth evolution at each time step. We illustrate the capabilities of the RL-based growth laws in two representative applications: 1) predicting homogenous growth of a thin-walled artery in response to hypertensive blood pressure, and 2) generating residual stress with heterogeneous growth in a thick-walled bi-layer aorta via distributed growth policies. Experimental data, where available, were used to compare expert-prescribed and RL-based growth laws. Our results demonstrated the capabilities of RL to effectively control the growth processes in response to hypertension, and the predictions are in good agreement with experimental observations. In particular, the RL growth laws captured the reduction of in vivo axial stretch without using experimental data for training. Moreover, the distributed RL growth policies achieved residual stress generation in a collaborative manner, which may pave the way for implementation in a finite element setting. This study sheds light on a new avenue to uncover growth evolution laws via RL, perhaps reducing the need for large experimental datasets and expert intelligence during growth law construction.}
}
@article{KULATHUNGA2022152,
title = {A Reinforcement Learning based Path Planning Approach in 3D Environment},
journal = {Procedia Computer Science},
volume = {212},
pages = {152-160},
year = {2022},
note = {11th International Young Scientist Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.217},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922016891},
author = {Geesara Kulathunga},
keywords = {Motion Planning, Q-learning, Approximate Policy Gradient, Path Planning},
abstract = {Optimal motion planning involves obstacles avoidance whereas path planning is the key to success in optimal motion planning. Due to the computational demands, most of the path planning algorithms can not be employed for real-time-based applications. Model-based reinforcement learning approaches for path planning have received particular success in the recent past. Yet, most such approaches do not have deterministic output due to randomness. In this paper, we investigate existing reinforcement learning-based approaches for path planning and propose such an approach for path planning in the 3D environment. One such reinforcement learning-based approach is a deterministic tree-based approach, and the other two approaches are based on Q-learning and approximate policy gradient, respectively. We tested the preceding approaches on two different simulators, each of which consists of a set of random obstacles that can be changed or moved dynamically. After analysing the result and computation time, we concluded that the deterministic tree search approach provides highly stable results. However, the computational time is considerably higher than the other two approaches. Finally, the comparative results are provided in terms of accuracy and computational time.}
}
@article{LIU2019102687,
title = {Large-scale and adaptive service composition based on deep reinforcement learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {65},
pages = {102687},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.102687},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319303086},
author = {Jiang-Wen Liu and Li-Qiang Hu and Zhao-Quan Cai and Li-Ning Xing and Xu Tan},
keywords = {Service composition, Deep reinforcement learning, QoS, Behavior strategy},
abstract = {Service composition is a research hotspot with practical value. With the development of Web service, many Web services with the same functional attributes emerge. However, service composition optimization is still a big challenge since the complex and unstable composition environment. To solve this problem, we propose an adaptive service composition based on deep reinforcement learning, where recurrent neural network (RNN) is utilized for predicting the objective function, improving its expression and generalization ability, and effectively solving the shortcomings of traditional reinforcement learning in the face of large-scale or continuous state space problems. We leverage heuristic behavior selection strategy to divide the state set into hidden state and fully visible state. Effective simulation of hidden state space and fully visible state of the evaluation function can further improve the accuracy and efficiency of the combined results. We conduct comprehensive experiment and experimental results have shown the effectiveness of our method.}
}
@article{ZHOU2022116564,
title = {Deep reinforcement learning approach for solving joint pricing and inventory problem with reference price effects},
journal = {Expert Systems with Applications},
volume = {195},
pages = {116564},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116564},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422000628},
author = {Qiang Zhou and Yefei Yang and Shaochuan Fu},
keywords = {Dynamic pricing, Inventory control, Reference price effects, Machine learning, Deep reinforcement learning},
abstract = {Reference prices, developed by consumers who frequently buy their desired products or services and form psychological price expectations as a benchmark, have significant impacts on customers’ purchase behaviors and firms’ operational strategies. Therefore, to determine an appropriate pricing and ordering strategy to maximize the total discounted revenues of a retailer, we consider joint pricing and inventory management system under reference price effects in an infinite horizon. Such a system involves uncertain market turbulence and customers’ sensitivities to gains and losses (loss-averse, gain-seeking and loss-neutral). We aggregate those factors into a general value function model with only a few realistic constraints on the variables and structure parameters. A deep reinforcement learning approach based on Double Deep Q-Networks with a target network (TN-DDQN) algorithm is proposed and forms the core of the expert decision system. Two ground truth algorithms (value iteration and real-demand-response policy) and two classical RL algorithms (Double Q-learning and Q-learning) are compared with the TN-DDQN algorithm in discrete and continuous state spaces respectively. Through a sequence of experiments, we find that the retailer should not ignore the impact of current prices on future demand, and a myopic policy will cause the retailer’s profits to decrease through reference price effects. Moreover, we also find that if customers have high abilities to remember previous prices, the retailer must bring sales prices down and raise the order-up-to level accordingly. Our system with the TN-DDQN algorithm provides a new way to handle complicated behavioral science and operation problems, which can be applied in a broader field.}
}
@article{ZHAO2023110368,
title = {An inverse reinforcement learning framework with the Q-learning mechanism for the metaheuristic algorithm},
journal = {Knowledge-Based Systems},
volume = {265},
pages = {110368},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110368},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123001181},
author = {Fuqing Zhao and Qiaoyun Wang and Ling Wang},
keywords = {Inverse reinforcement learning (IRL), Q-learning, Metaheuristic algorithm, Moth–flame​ optimization algorithm (MFO), Competition mechanism},
abstract = {A reward function is learned from the expert examples by inverse reinforcement learning (IRL), which is more reliable than an artificial method. The moth–flame optimization algorithm (MFO), which is based on the navigation mechanism of a moth flying at night, has been extensively employed to address the complex optimization problem. An inverse reinforcement learning framework with the Q-learning mechanism (IRLMFO) is designed to strengthen the performance of the MFO algorithm in a large-scale real-parameter optimization problem. The right strategy is chosen by the Q-learning mechanism, using historical data provided by the relevant approach in the strategy pool, which stores strategies that include diverse functions. The competition mechanism is designed to strengthen the exploitation capability of the IRLMFO algorithm. The performance of the IRLMFO is verified on the benchmark test suite in CEC 2017. Experimental results illustrate that the IRLMFO outperforms state-of-the-art algorithms.}
}
@article{GUO2021102980,
title = {Hybrid deep reinforcement learning based eco-driving for low-level connected and automated vehicles along signalized corridors},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {124},
pages = {102980},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.102980},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21000164},
author = {Qiangqiang Guo and Ohay Angah and Zhijun Liu and Xuegang (Jeff) Ban},
keywords = {Connected and automated vehicles, Hybrid reinforcement learning, Policy gradient, Deep Q-learning, Eco-driving, Lane-changing},
abstract = {Eco-Driving has great potential in reducing the fuel consumption of road vehicles, especially under the connected and automated vehicles (CAVs) environment. Traditional model-based Eco-Driving methods usually require sophisticated models and cannot deal with complex driving scenarios. This paper proposes a hybrid reinforcement learning (RL) based Eco-Driving algorithm considering both the longitudinal acceleration/deceleration and the lateral lane-changing operations. A deep deterministic policy gradient (DDPG) algorithm is designed to learn the continuous longitudinal acceleration/deceleration to reduce the fuel consumption as well as to maintain acceptable travel time. Collecting the critic’s value of each single lane from DDPG and integrating the information of adjacent lanes, a deep Q-learning algorithm is developed to make the discrete lane-changing decision. Together, a hybrid deep Q-learning and policy gradient (HDQPG) method is developed for vehicles driving along multi-lane urban signalized corridors. The method can enable the controlled vehicle to learn well-established longitudinal fuel-saving strategies, and to perform appropriate lane-changing operations at proper times to avoid congested lanes. Numerical experiments show that HDQPG can reduce fuel consumption by up to 46% with marginal or no increase of travel times.}
}
@article{FOTUHI20133,
title = {Modeling yard crane operators as reinforcement learning agents},
journal = {Research in Transportation Economics},
volume = {42},
number = {1},
pages = {3-12},
year = {2013},
note = {Freight Transport and Sustainability},
issn = {0739-8859},
doi = {https://doi.org/10.1016/j.retrec.2012.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S073988591200162X},
author = {Fateme Fotuhi and Nathan Huynh and Jose M. Vidal and Yuanchang Xie},
keywords = {Reinforcement learning, Q-learning, Multi-agent systems, Yard crane scheduling, Drayage operations},
abstract = {Due to the importance of drayage operations, operators at marine container terminals are increasingly looking to reduce the time a truck spends at the terminal to complete a transaction. This study introduces an agent-based approach to model yard cranes for the analysis of truck turn time. The objective of the model is to solve the yard crane scheduling problem (i.e. determining the sequence of drayage trucks to serve to minimize their waiting time). It is accomplished by modeling the yard crane operators as agents that employ reinforcement learning; specifically, q-learning. The proposed agent-based, q-learning model is developed using Netlogo. Experimental results show that the q-learning model is very effective in assisting the yard crane operator to select the next best move. Thus, the proposed q-learning model could potentially be integrated into existing yard management systems to automate the truck selection process and thereby improve yard operations.}
}
@article{LISSA2021100043,
title = {Deep reinforcement learning for home energy management system control},
journal = {Energy and AI},
volume = {3},
pages = {100043},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2020.100043},
url = {https://www.sciencedirect.com/science/article/pii/S2666546820300434},
author = {Paulo Lissa and Conor Deane and Michael Schukat and Federico Seri and Marcus Keane and Enda Barrett},
keywords = {Deep reinforcement learning, Residential home energy management, Demand response, Autonomous control},
abstract = {The use of machine learning techniques has been proven to be a viable solution for smart home energy management. These techniques autonomously control heating and domestic hot water systems, which are the most relevant loads in a dwelling, helping consumers to reduce energy consumption and also improving their comfort. Moreover, the number of houses equipped with renewable energy resources is increasing, and this is a key element for energy usage optimization, where coordinating loads and production can bring additional savings and reduce peak loads. In this regard, we propose the development of a deep reinforcement learning (DRL) algorithm for indoor and domestic hot water temperature control, aiming to reduce energy consumption by optimizing the usage of PV energy production. Furthermore, a methodology for a new dynamic indoor temperature setpoint definition is presented, thus allowing greater flexibility and savings. The results show that the proposed DRL algorithm combined with the dynamic setpoint achieved on average 8% of energy savings compared to a rule-based algorithm, reaching up to 16% of savings over the summer period. Moreover, the users’ comfort has not been compromised, as the algorithm is calibrated to not exceed more than 1% of the time out the specified temperature setpoints. Additional analysis shows that further savings could be achieved if the time out of comfort is increased, which could be agreed according to users’ needs. Regarding demand side management, the DRL control shows efficiency by anticipating and delaying actions for a PV self-consumption optimization, performing over 10% of load shifting. Finally, the renewable energy consumption is 9.5% higher for the DRL-based model compared to the rule-based, which means less energy consumed from the grid.}
}
@article{YU2023121704,
title = {Finding Nash equilibrium based on reinforcement learning for bidding strategy and distributed algorithm for ISO in imperfect electricity market},
journal = {Applied Energy},
volume = {350},
pages = {121704},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121704},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923010681},
author = {Liying Yu and Peng Wang and Zhe Chen and Dewen Li and Ning Li and Rachid Cherkaoui},
keywords = {Bidding strategy, Imperfect electricity market, Nash equilibrium, Distributed optimization, Reinforcement learning, Multi-agent},
abstract = {Finding Nash equilibrium in an imperfect market with active and strategic resources is key to the successful operation of the electricity market. A hierarchical Nash distributed reinforcement learning (HNDRL) framework is proposed to realize the interaction between the market participants and independent system operator (ISO). In the bidding stage, multi-agent Nash policy reinforcement learning is utilized to update the bidding strategies for market participants, i.e., generation company (GenCo) and distributed company (DisCo), which maximizes their own profit with imperfect information. The bidding action is updated by the probability iteration to deal with the continuous bidding strategy. And in the market clearing stage, the distributed dynamic-average consensus optimization algorithm is proposed at the ISO level to obtain the locational marginal price (LMP) and transaction quantities. With the participation of the demand-side in the electricity market, the proposed HNDRL framework can reach a Nash equilibrium with the optimal bidding strategy and handle global resource constraints in a distributed way. In addition, theoretical analysis is provided to ensure that the participants’ strategy exponentially converges to the Nash equilibrium for both convex and non-convex problems. Finally, the IEEE 30-bus system is employed to illustrate the efficiency. Compared with the existing method, the simulation results show that the proposed HNDRL framework has a faster convergence rate with the range of 2.667–3.333 times and can obtain higher profit with the range of 1.4286%–7.1429% at the Nash equilibrium point.}
}
@article{WANG2022539,
title = {A surrogate-assisted controller for expensive evolutionary reinforcement learning},
journal = {Information Sciences},
volume = {616},
pages = {539-557},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.10.134},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522012658},
author = {Yuxing Wang and Tiantian Zhang and Yongzhe Chang and Xueqian Wang and Bin Liang and Bo Yuan},
keywords = {Deep reinforcement learning, Evolutionary algorithm, Evolutionary reinforcement learning, Surrogate model},
abstract = {The integration of Reinforcement Learning (RL) and Evolutionary Algorithms (EAs) aims at simultaneously exploiting the sample efficiency as well as the diversity and robustness of the two paradigms. Recently, hybrid learning frameworks based on this principle have achieved great success in robot control tasks. However, in these methods, policies from the genetic population are evaluated via interactions with the real environments, severely restricting their applicability when such interactions are prohibitively costly. In this work, we propose Surrogate-assisted Controller (SC), a generic module that can be applied on top of existing hybrid frameworks to alleviate the computational burden of expensive fitness evaluation. The key to our approach is to leverage the critic network that is implemented in existing hybrid frameworks as a novel surrogate model, making it possible to estimate the fitness of individuals without environmental interactions. In addition, two model management strategies with the elite protection mechanism are introduced in SC to control the workflow, leading to a fast and stable optimization process. In the empirical studies, we combine SC with two state-of-the-art evolutionary reinforcement learning approaches to highlight its functionality and effectiveness. Experiments on six challenging continuous control benchmarks from the OpenAI Gym platform show that SC can not only significantly reduce the cost of interaction with the environment, but also bring better sample efficiency and dramatically boost the learning progress of the original hybrid framework.}
}
@article{DOGRU2022107760,
title = {Reinforcement learning approach to autonomous PID tuning},
journal = {Computers & Chemical Engineering},
volume = {161},
pages = {107760},
year = {2022},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.107760},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422001016},
author = {Oguzhan Dogru and Kirubakaran Velswamy and Fadi Ibrahim and Yuqi Wu and Arun Senthil Sundaramoorthy and Biao Huang and Shu Xu and Mark Nixon and Noel Bell},
keywords = {Contextual bandits, PID tuning, Process control, Reinforcement learning, Step-response model},
abstract = {Many industrial processes utilize proportional-integral-derivative (PID) controllers due to their practicality and often satisfactory performance. The proper controller parameters depend highly on the operational conditions and process uncertainties. This study combines the recent developments in computer sciences and control theory to address the tuning problem. It formulates the PID tuning problem as a reinforcement learning task with constraints. The proposed scheme identifies an initial approximate step-response model and lets the agent learn dynamics off-line from the model with minimal effort. After achieving a satisfactory training performance on the model, the agent is fine-tuned on-line on the actual process to adapt to the real dynamics, thereby minimizing the training time on the real process and avoiding unnecessary wear, which can be beneficial for industrial applications. This sample efficient method is tested and demonstrated through a pilot-scale multi-modal tank system. The performance of the method is verified through setpoint tracking and disturbance regulatory experiments.}
}
@article{WANG2023930,
title = {Solving combinatorial optimization problems over graphs with BERT-Based Deep Reinforcement Learning},
journal = {Information Sciences},
volume = {619},
pages = {930-946},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.073},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522013627},
author = {Qi Wang and Kenneth H. Lai and Chunlei Tang},
keywords = {Combinatorial optimization, BERT, Deep reinforcement learning},
abstract = {Combinatorial optimization, such as vehicle routing and traveling salesman problems for graphs, is NP-hard and has been studied for decades. Many methods have been proposed for its possible solution, including, but not limited to, exact algorithms, approximate algorithms, heuristic algorithms, and solution solvers. However, these methods cannot learn the problem’s internal structure nor generalize to similar or larger-scale problems. Recently, deep reinforcement learning has been applied to combinatorial optimization and has achieved convincing results. Nevertheless, the challenge of effective integration and training improvement still exists. In this study, we propose a novel framework (BDRL) that combines BERT (Bidirectional Encoder Representations from Transformers) and deep reinforcement learning to tackle combinatorial optimization over graphs by treating general optimization problems as data points under an identified data distribution. We first improved the transformer encoder of BERT to embed the combinatorial optimization graph effectively. By employing contrastive objectives, we extend BERT-like training to reinforcement learning and acquire self-attention-consistent representations. Next, we used hierarchical reinforcement learning to pre-train our model; that is, to train and fine-tune the model through an iterative process to make it more suitable for a specific combinatorial optimization problem. The results demonstrate our proposed framework’s generalization ability, efficiency, and effectiveness in multiple tasks.}
}
@article{VIMAL2020355,
title = {Enhanced resource allocation in mobile edge computing using reinforcement learning based MOACO algorithm for IIOT},
journal = {Computer Communications},
volume = {151},
pages = {355-364},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419319255},
author = {S. Vimal and Manju Khari and Nilanjan Dey and Rubén González Crespo and Y. {Harold Robinson}},
keywords = {Mobile edge computing, Industrial IOT, Reinforcement learning, Multi objective ant colony optimization, Resource allocation, Cognitive agent},
abstract = {The Mobile networks deploy and offers a multiaspective approach for various resource allocation paradigms and the service based options in the computing segments with its implication in the Industrial Internet of Things (IIOT) and the virtual reality. The Mobile edge computing (MEC) paradigm runs the virtual source with the edge communication between data terminals and the execution in the core network with a high pressure load. The demand to meet all the customer requirements is a better way for planning the execution with the support of cognitive agent. The user data with its behavioral approach is clubbed together to fulfill the service type for IIOT. The swarm intelligence based and reinforcement learning techniques provide a neural caching for the memory within the task execution, the prediction provides the caching strategy and cache business that delay the execution. The factors affecting this delay are predicted with mobile edge computing resources and to assess the performance in the neighboring user equipment. The effectiveness builds a cognitive agent model to assess the resource allocation and the communication network is established to enhance the quality of service. The Reinforcement Learning techniques Multi Objective Ant Colony Optimization (MOACO) algorithms has been applied to deal with the accurate resource allocation between the end users in the way of creating the cost mapping tables creations and optimal allocation in MEC.}
}
@article{HE2023104006,
title = {A bi-objective deep reinforcement learning approach for low-carbon-emission high-speed railway alignment design},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {147},
pages = {104006},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.104006},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22004193},
author = {Qing He and Tianci Gao and Yan Gao and Huailong Li and Paul Schonfeld and Ying Zhu and Qilong Li and Ping Wang},
keywords = {Low-carbon-emission railway design, Railway alignment optimization, Bi-objective deep reinforcement learning, Railway environmental impact},
abstract = {Reasonable design and planning of alignments are crucial for both economic investment and the environmental impact of high-speed railway projects. Approaches that can integrate economic investment and environmental factors, thus selecting an economical and eco-friendly railway alignment, are very demanding. To address the above issue, this study focuses on optimizing a railway’s comprehensive investment, including the construction and environmental costs, as well as the railway’s life-cycle carbon emission caused by the production of building materials and the trains’ energy consumption. A novel railway alignment optimization model is formulated based on the multi-objective reinforcement learning (MORL) framework to reduce the railway total cost, accounting for both the construction cost and environmental factors. In the proposed model, a deep deterministic policy gradient (DDPG) algorithm is enhanced with an envelope algorithm that can optimize the convex envelope of multi-objective Q-values to ensure an efficient consistency between the entire space of preferences in a domain and the corresponding optimal policies. Finally, the proposed model is applied to a real-world high-speed railway project. Results show that the MORL model can automatically explore and optimize railway alignment, and produce less expensive and more eco-friendly solutions than manual work while satisfying various alignment constraints.}
}
@article{SHI2023318,
title = {Distributed output formation tracking control of heterogeneous multi-agent systems using reinforcement learning},
journal = {ISA Transactions},
volume = {138},
pages = {318-328},
year = {2023},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2023.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0019057823001015},
author = {Yu Shi and Xiwang Dong and Yongzhao Hua and Jianglong Yu and Zhang Ren},
keywords = {Heterogeneous system, Distributed trajectory generator, Output formation tracking, Reinforcement learning},
abstract = {This paper studies the distributed time-varying output formation tracking problem for heterogeneous multi-agent systems with both diverse dimensions and parameters. The output of each follower is supposed to track that of the virtual leader while accomplishing a time-varying formation configuration. First, a distributed trajectory generator is proposed based on neighboring interactions to reconstitute the state of virtual leader and provide expected trajectories with the formation incorporated. Second, an optimal tracking controller is designed by the model-free reinforcement learning technique using online off-policy data instead of requiring any knowledge of the followers’ dynamics. Stabilities of the learning process and resulting controller are analyzed while solutions to the output regulator equations are equivalently obtained. Third, a compensational input is designed for each follower based on previous learning results and a derived feasibility condition. It is proved that the output formation tracking error converges to zero asymptotically with the biases to cost functions being restricted arbitrarily small. Finally, numerical simulations verify the proposed learning and control scheme.}
}
@article{HOU2023112594,
title = {Model-free dynamic management strategy for low-carbon home energy based on deep reinforcement learning accommodating stochastic environments},
journal = {Energy and Buildings},
volume = {278},
pages = {112594},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112594},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822007654},
author = {Hui Hou and Xiangdi Ge and Yue Chen and Jinrui Tang and Tingting Hou and Rengcun Fang},
keywords = {Home energy management system, Dynamic optimal management, Deep reinforcement learning, Low-carbon, Model-free},
abstract = {This paper presents a model-free dynamic optimal management strategy for a low-carbon home energy management system (HEMS) based on deep reinforcement learning (DRL). The method can ideally handle the uncertainties and dynamics of renewable energy and demand-side load. Firstly, the load model is established by a deep Q network (DQN) algorithm with the advantage of ignoring traditional forecasting steps on stochastic environments such as renewable energy generation, load demand, price, etc. Then multi-agents are established for dynamic management based on the DRL. Through “dynamic acquisition, dynamic decision” mechanism, the proposed model-free strategy achieves real-time energy management that can adaptively respond to stochastic environments. Secondly, considering the constraints of system carbon emissions and carbon trading, the proposed strategy can minimize the energy consumption cost, carbon trading cost, and user satisfaction penalties. Ultimately, the effectiveness of the proposed strategy is verified through case studies. Experimental results demonstrate that the strategy can significantly reduce the overall cost, including a 36.7% reduction in carbon trading. At the same time, user satisfaction penalties are reduced by 50.2%. Further, the agent hyperparameter could also be adjusted to capture the trade-off between cost savings and satisfaction penalties. And compared with the traditional forecast-based management strategy, it overcomes the problem of uncertainties and avoids forecasting errors to better accommodate the stochastic environment.}
}
@article{LIU202267,
title = {Neural network-based reinforcement learning control for combined spacecraft attitude tracking maneuvers},
journal = {Neurocomputing},
volume = {484},
pages = {67-78},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.07.099},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221015782},
author = {Yuhan Liu and Guangfu Ma and Yueyong Lyu and Pengyu Wang},
keywords = {Combined spacecraft, Attitude tracking, Reinforcement learning, Q-learning},
abstract = {This paper proposes a novel reinforcement learning-based attitude tracking control strategy for combined spacecraft takeover maneuvers with completely unknown dynamics. One major issue in the context of combined spacecraft attitude takeover control is that the accurate dynamic model is highly nonlinear, complex and costly to identify online, which makes it impractical for control design. To address this issue, we take the advantage of the Q-learning algorithm to acquire the control strategy directly from system input/output measurement data in a model-free manner, and thus the online inertia parameter identification procedure is avoided. More specifically, first, the attitude tracking is formulated as a regulation problem by introducing an argumented system, where the system dynamic model is still required in control design. Then, in order to achieve a model-free control strategy, an online policy-iteration (PI) Q-learning procedure is derived to solve the Bellman optimality equation by utilizing the generated measurement data. In theoretical analysis, it is proved that the iteration sequences of Q value function and control strategy can converge to the optimal ones. In addition, rigorous proof of the stability and monotonicity guarantees of the proposed control strategy are also provided. Furthermore, for the purpose of online implementation, off-policy learning scheme is employed to find the optimal Q value function approximator with neural network structure after data-collection phase. Numerical simulations are exhibited to validate the effectiveness of the proposed strategy.}
}
@article{NAGARAJAN20232734,
title = {Detection of Reading Impairment from Eye-Gaze Behaviour using Reinforcement Learning},
journal = {Procedia Computer Science},
volume = {218},
pages = {2734-2743},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.245},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923002454},
author = {Harshitha Nagarajan and Vishnu Sai Inakollu and Punitha Vancha and J Amudha},
keywords = {reinforcement learning, eye-gaze behaviour, eye-tracking, modelling gaze behaviour, dyslexia},
abstract = {Experimental psychology and neuroscience reveal that decision-behavior plays a dominant role in human-selective-attention when it comes to reading, object and scene detection. Difficulties in reading are easily reflected by eye-movement patterns. Hence, modelling eye-gaze behaviour for normal readers and people with reading impairments can greatly help in contrasting reading strategies used, which can in turn help in early identification and diagnosis of impairments such as dyslexia. This paper introduces a novel method of formulating a reinforcement learning model that is explainable, and can obtain the sequence of gaze targets based on recorded observations of dyslexic and non-dyslexic children. Results reveal that despite being a less sophisticated model, it is able to obtain the optimal reading policy of the ideal reader, from a set of good and poor readers with the help of a strong reward system and Q-Learning agent.}
}
@article{HUANG2023407,
title = {Adaptive reinforcement learning optimal tracking control for strict-feedback nonlinear systems with prescribed performance},
journal = {Information Sciences},
volume = {621},
pages = {407-423},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.109},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522014153},
author = {Zongsheng Huang and Weiwei Bai and Tieshan Li and Yue Long and C.L. Philip Chen and Hongjing Liang and Hanqing Yang},
keywords = {Reinforcement learning, Prescribed performance control, Adaptive dynamic programming, Tracking control, Strict-feedback nonlinear systems},
abstract = {The reinforcement learning-based prescribed performance optimal tracking control problem is considered for a class of strict-feedback nonlinear systems in this paper. The unknown nonlinearities and cost function are approximated by radial-basis-function (RBF) neural network (NN). The overall controller consists of an adaptive controller and an optimal compensation term. Firstly, the adaptive controller is designed by backstepping control method. Subsequently, the optimal compensation term is derived via policy iteration by minimizing cost function. In addition, depending on the prescribed performance control, the tracking error can be limited in the prescribed area. Therefore, the whole control scheme can effectively guarantee that the tracking error converges to a bound with prescribed performance while the cost function is minimized. The stability analysis shows that all signals in the closed-loop system are bounded. Finally, the effectiveness and advantages of the designed control strategy are illustrated by the simulation examples.}
}
@article{BEAUDOIN2022104654,
title = {Improving gearshift controllers for electric vehicles with reinforcement learning},
journal = {Mechanism and Machine Theory},
volume = {169},
pages = {104654},
year = {2022},
issn = {0094-114X},
doi = {https://doi.org/10.1016/j.mechmachtheory.2021.104654},
url = {https://www.sciencedirect.com/science/article/pii/S0094114X21003839},
author = {Marc-Antoine Beaudoin and Benoit Boulet},
keywords = {Electric vehicle, Multi-speed transmission, Reinforcement learning, Automatic tuning, Gearshift controller},
abstract = {During a multi-speed transmission development process, the final calibration of the gearshift controller parameters is usually performed on a physical test bench. Engineers typically treat the mapping from the controller parameters to the gearshift quality as a black-box, and use methods rooted in experimental design – a purely statistical approach – to infer the parameter combination that will maximize a chosen gearshift performance indicator. This approach unfortunately requires thousands of gearshift trials, ultimately discouraging the exploration of different control strategies. In this work, we calibrate the feedforward and feedback parameters of a gearshift controller using a model-based reinforcement learning algorithm adapted from pilco. Experimental results show that the method optimizes the controller parameters with few gearshift trials. This approach can accelerate the exploration of gearshift control strategies, which is especially important for the emerging technology of multi-speed transmissions for electric vehicles.}
}
@article{HAO2023104345,
title = {A V2G-oriented reinforcement learning framework and empirical study for heterogeneous electric vehicle charging management},
journal = {Sustainable Cities and Society},
volume = {89},
pages = {104345},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.104345},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722006497},
author = {Xu Hao and Yue Chen and Hewu Wang and Han Wang and Yu Meng and Qing Gu},
keywords = {V2G (vehicle-to-grid), EV (electric vehicle), Charging, Deep Q-network, Travel pattern heterogeneity},
abstract = {Vehicle-to-grid (V2G) technology is a promising solution to energy supply security issues associated with future electric grids. A decisive factor to successful V2G is effective electric vehicle (EV) charging management aimed at meeting travel demands with minimal charging costs, especially how to account for uncertainties and EV heterogeneity. In this study, a deep Q-network (DQN)-based reinforcement learning (RL) method is proposed to learn the optimal EV charging strategy considering empirical travel pattern heterogeneities and unpredictable electricity prices. The effectiveness and generalizability of the proposed DQN-based RL method was validated using actual five-million-km driving data in typical Chinese cities. In particular, EVs can save over 98% of the electricity cost without future electricity price information via the proposed method compared to the charging as-soon-as-possible method. The empirical experimental results also reveal that V2G-oriented charging management is sensitive to the charging/discharging power rate, electricity-price fluctuation frequency and range, and departure-time. We quantified the sensitivity with value of information (VOI) and found that: (1) Knowing the departure-time information can significantly reduce charging costs in most cases (average VOI: 5.4 CNY per charging/discharging session); (2) More historical data does not always lead to a higher electricity price VOI, and prices with sudden surges may even have a negative VOI.}
}
@article{FANG2022100016,
title = {Online power management strategy for plug-in hybrid electric vehicles based on deep reinforcement learning and driving cycle reconstruction},
journal = {Green Energy and Intelligent Transportation},
volume = {1},
number = {2},
pages = {100016},
year = {2022},
issn = {2773-1537},
doi = {https://doi.org/10.1016/j.geits.2022.100016},
url = {https://www.sciencedirect.com/science/article/pii/S2773153722000160},
author = {Zhiyuan Fang and Zeyu Chen and Quanqing Yu and Bo Zhang and Ruixin Yang},
keywords = {Electric vehicle, Deep reinforcement learning, Power management strategy, Driving cycle reconstruction, Optimal control strategy},
abstract = {This paper proposes a novel power management strategy for plug-in hybrid electric vehicles based on deep reinforcement learning algorithm. Three parallel soft actor-critic (SAC) networks are trained for high speed, medium speed, and low-speed conditions respectively; the reward function is designed as minimizing the cost of energy cost and battery aging. During operation, the driving condition is recognized at each moment for the algorithm invoking based on the learning vector quantization (LVQ) neural network. On top of that, a driving cycle reconstruction algorithm is proposed. The historical speed segments that were recorded during the operation are reconstructed into the three categories of high speed, medium speed, and low speed, based on which the algorithms are online updated. The SAC-based control strategy is evaluated based on the standard driving cycles and Shenyang practical data. The results indicate the presented method can obtain the effect close to dynamic programming and can be further improved by up to 6.38% after the online update for uncertain driving conditions.}
}
@article{ALBEAIK2022105026,
title = {Deep truck cruise control: Field experiments and validation of heavy duty truck cruise control using deep reinforcement learning},
journal = {Control Engineering Practice},
volume = {121},
pages = {105026},
year = {2022},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2021.105026},
url = {https://www.sciencedirect.com/science/article/pii/S0967066121002835},
author = {Saleh Albeaik and Trevor Wu and Ganeshnikhil Vurimi and Fang-Chieh Chou and Xiao-Yun Lu and Alexandre M. Bayen},
keywords = {Model-free deep reinforcement learning, Cruise control, Control validation, Field experiments, Heavy duty trucks},
abstract = {Building control systems for heavy duty trucks have historically been dependent on availability of the details of the mechanical configuration of each target truck. This article investigates transfer and robustness of continuous control systems learned using model free deep-RL as an alternative; a configuration agnostic strategy for control system development. For this purpose, deep-RL cruise control policies are developed and validated in simulation and field experiments using two differently configured trucks; full-size Volvo and Freightliner trucks. Their performance are validated for step, ramp, and sinusoidal reference speed trajectories to stimulate steady-state and transient behavior, and to test speed-tracking for low, high, and variable accelerations. The robustness of these controllers were validated for unmodeled gravity effects and for operating the controllers outside of the engine command training distribution bounds. In addition, the controllers were validated against a classical model-based controller.}
}
@article{BAKHSHI2023109611,
title = {Multi-provider NFV network service delegation via average reward reinforcement learning},
journal = {Computer Networks},
volume = {224},
pages = {109611},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109611},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623000567},
author = {Bahador Bakhshi and Josep Mangues-Bafalluy and Jorge Baranda},
keywords = {Multi-provider service delegation, Admission control, MDP, Average reward RL, Dynamic programming},
abstract = {In multi-provider 5G/6G networks, service delegation enables administrative domains to federate in provisioning NFV network services. Admission control in selecting the appropriate domain for service deployment, without prior knowledge of service requests’ statistical distributions, is fundamental to maximize average profit. This paper analyzes a general federation contract model for service delegation in various ways. First, under the assumption of known system dynamics, we obtain the theoretically optimal performance bound by formulating the admission control problem as an infinite-horizon Markov decision process (MDP) and solving it through dynamic programming, which is used as a benchmark to evaluate practical solutions. Second, we apply Reinforcement Learning (RL) to practically tackle the problem when the arrival and departure rates are not known. For the first time in this context, we analyze the performance of the widely used Q-Learning algorithm, and prove as it maximizes the discounted rewards, it is not an efficient solution due to its sensitivity to the discount factor. Then, we propose the average reward reinforcement learning approach (named “R-Learning”) to find the policy that directly maximizes the average profit. Finally, we evaluate different solutions through extensive simulations and experimentally using the 5Growth management and orchestration platform. Results confirm that the proposed R-Learning solution always outperforms Q-Learning and the greedy policies. Furthermore, while there is at most a 9% optimality gap in the ideal simulation environment, it competes with the MDP solution in the experimental assessment.}
}
@article{SPECHT2023100215,
title = {Deep reinforcement learning for the optimized operation of large amounts of distributed renewable energy assets},
journal = {Energy and AI},
volume = {11},
pages = {100215},
year = {2023},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2022.100215},
url = {https://www.sciencedirect.com/science/article/pii/S2666546822000611},
author = {Jan Martin Specht and Reinhard Madlener},
keywords = {Reinforcement learning, Virtual power plant, Aggregation of energy, Value stacking, Flexibility of decentral energy assets},
abstract = {This study utilizes machine learning and, more specifically, reinforcement learning (RL) to allow for an optimized, real-time operation of large numbers of decentral flexible assets on private household scale in the electricity domain. The potential and current obstacles of RL are demonstrated and a guide for interested practitioners is provided on how to tackle similar tasks without advanced skills in neural network programming. For the application in the energy domain it is demonstrated that state-of-the-art RL algorithms can be trained to control potentially millions of small-scale assets in private households. In detail, the applied RL algorithm outperforms common heuristic algorithms and only falls slightly short of the results provided by linear optimization, but at less than a thousandth of the simulation time. Thus, RL paves the way for aggregators of flexible energy assets to optimize profit over multiple use cases in a smart energy grid and thus also provide valuable grid services and a more sustainable operation of private energy assets.}
}
@article{PENG2020124821,
title = {A novel optimal bipartite consensus control scheme for unknown multi-agent systems via model-free reinforcement learning},
journal = {Applied Mathematics and Computation},
volume = {369},
pages = {124821},
year = {2020},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2019.124821},
url = {https://www.sciencedirect.com/science/article/pii/S0096300319308136},
author = {Zhinan Peng and Jiangping Hu and Kaibo Shi and Rui Luo and Rui Huang and Bijoy Kumar Ghosh and Jiuke Huang},
keywords = {Optimal bipartite consensus control, Multi-agent systems, Coopetition network, Model-free, Reinforcement learning},
abstract = {In this paper, the optimal bipartite consensus control (OBCC) problem is investigated for unknown multi-agent systems (MASs) with coopetition networks. A novel distributed OBCC scheme is proposed based on model-free reinforcement learning method to achieve OBCC, where the agent’s dynamics are no longer required. First, The coopetition networks are applied to establish the cooperative and competitive interactions among agents, and then the OBCC problem is formulated by introducing local neighbor bipartite consensus errors and performance index functions (PIFs) for each agent. Second, in order to obtain the OBCC laws, a policy iteration algorithm (PIA) is employed to learn the solutions to discrete-time (DT) Hamilton-Jacobi-Bellman (HJB) equations. Third, to implement the proposed methods, we adopt a data-driven actor-critic-based neural networks (NNs) framework to approximate the control laws and the PIFs, respectively, in an online learning manner. Finally, some simulation results are given to demonstrate the effectiveness of the developed approaches.}
}
@article{LUO2015150,
title = {Reinforcement learning solution for HJB equation arising in constrained optimal control problem},
journal = {Neural Networks},
volume = {71},
pages = {150-158},
year = {2015},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2015.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608015001616},
author = {Biao Luo and Huai-Ning Wu and Tingwen Huang and Derong Liu},
keywords = {Constrained optimal control, Data-based, Off-policy reinforcement learning, Hamilton–Jacobi–Bellman equation, The method of weighted residuals},
abstract = {The constrained optimal control problem depends on the solution of the complicated Hamilton–Jacobi–Bellman equation (HJBE). In this paper, a data-based off-policy reinforcement learning (RL) method is proposed, which learns the solution of the HJBE and the optimal control policy from real system data. One important feature of the off-policy RL is that its policy evaluation can be realized with data generated by other behavior policies, not necessarily the target policy, which solves the insufficient exploration problem. The convergence of the off-policy RL is proved by demonstrating its equivalence to the successive approximation approach. Its implementation procedure is based on the actor–critic neural networks structure, where the function approximation is conducted with linearly independent basis functions. Subsequently, the convergence of the implementation procedure with function approximation is also proved. Finally, its effectiveness is verified through computer simulations.}
}
@article{YANG2023128395,
title = {Applying deep reinforcement learning to the HP model for protein structure prediction},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {609},
pages = {128395},
year = {2023},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2022.128395},
url = {https://www.sciencedirect.com/science/article/pii/S0378437122009530},
author = {Kaiyuan Yang and Houjing Huang and Olafs Vandans and Adithya Murali and Fujia Tian and Roland H.C. Yap and Liang Dai},
keywords = {HP model, Reinforcement learning, Deep Q-network, LSTM, Protein structure, Self-avoiding walks},
abstract = {A central problem in computational biophysics is protein structure prediction, i.e., finding the optimal folding of a given amino acid sequence. This problem has been studied in a classical abstract model, the HP model, where the protein is modeled as a sequence of H (hydrophobic) and P (polar) amino acids on a lattice. The objective is to find conformations maximizing H–H contacts. It is known that even in this reduced setting, the problem is intractable (NP-hard). In this work, we apply deep reinforcement learning (DRL) to the two-dimensional HP model. We can obtain the conformations of best known energies for benchmark HP sequences with lengths from 20 to 50. Our DRL is based on a deep Q-network (DQN). We find that a DQN based on long short-term memory (LSTM) architecture greatly enhances the RL learning ability and significantly improves the search process. DRL can sample the state space efficiently, without the need of manual heuristics. Experimentally we show that it can find multiple distinct best-known solutions per trial. This study demonstrates the effectiveness of deep reinforcement learning in the HP model for protein folding.}
}
@article{WANG2023107593,
title = {Coverage path planning for kiwifruit picking robots based on deep reinforcement learning},
journal = {Computers and Electronics in Agriculture},
volume = {205},
pages = {107593},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107593},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922009012},
author = {Yinchu Wang and Zhi He and Dandan Cao and Li Ma and Kai Li and Liangsheng Jia and Yongjie Cui},
keywords = {Coverage path planning, Deep Reinforcement Learning, Harvesting robot, Kiwifruit projection, Evaluation function},
abstract = {In this paper, a deep reinforcement learning-based path planning method for kiwifruit picking robot coverage is proposed. Compared with existing approaches, the novelty of this paper is twofold. 1. Using a LiDAR to collect the environmental point cloud information of the kiwifruit orchard and construct a two-dimensional grid map. In the process of constructing the map, the fruit coordinate information is collected in real time, and the fruit coordinates are projected onto the grid map to obtain the distribution of kiwifruit in the orchard environment. Combined with the effective picking area of a kiwifruit picking robot, a kiwifruit area division algorithm is proposed, which converts the traditional grid-based coverage path planning into a travelling salesman (TSP) problem of solving the traversal order of each area. 2. An improved deep reinforcement learning algorithm, the re-DQN algorithm, is proposed to solve the traversal order of each region. The model training results show that the algorithm is more effective than the traditional DQN algorithm, completing model convergence to a better solution. The experimental results of kiwifruit orchard navigation show that the coverage path length of the method proposed in this paper is 220.67 m, which is 31.56 % shorter than that of the boustrophedon algorithm. The overall navigation time is 1200 s, which is 35.72 % shorter than that of the boustrophedon algorithm. This shows that the coverage path planning method proposed in this paper can effectively shorten the coverage path of kiwifruit orchards and improve the navigation efficiency of kiwifruit picking robots.}
}
@article{HAJAR2023110073,
title = {3R: A reliable multi agent reinforcement learning based routing protocol for wireless medical sensor networks},
journal = {Computer Networks},
pages = {110073},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110073},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623005182},
author = {Muhammad Shadi Hajar and Harsha Kumara Kalutarage and M. Omar Al-Kadri},
keywords = {Routing, Reinforcement learning, Trust management, Energy, Blackhole, Selective forwarding, Sinkhole, On-off},
abstract = {Interest in the Wireless Medical Sensor Network (WMSN) is rapidly gaining attention thanks to recent advances in semiconductors and wireless communication. However, by virtue of the sensitive medical applications and the stringent resource constraints, there is a need to develop a routing protocol to fulfill WMSN requirements in terms of delivery reliability, attack resiliency, computational overhead, and energy efficiency. This paper proposes 3R, a reliable multi agent reinforcement learning routing protocol for WMSN. 3R uses a novel resource-conservative Reinforcement Learning (RL) model to reduce the computational overhead, along with two updating methods to speed up the algorithm convergence. The reward function is re-defined as a punishment, combining the proposed trust management system to defend against well-known dropping attacks. Furthermore, an energy model is integrated with the reward function to enhance the network lifetime and balance energy consumption across the network. The proposed energy model only uses local information to avoid the resource burdens and the security concerns of exchanging energy information. Experimental results prove the lightweightness, attacks resiliency and energy efficiency of 3R, making it a potential routing candidate for WMSN.}
}
@article{HEIDARI2022118833,
title = {An occupant-centric control framework for balancing comfort, energy use and hygiene in hot water systems: A model-free reinforcement learning approach},
journal = {Applied Energy},
volume = {312},
pages = {118833},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.118833},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922002744},
author = {Amirreza Heidari and François Maréchal and Dolaana Khovalyg},
keywords = {Energy, Building, Reinforcement learning, Machine learning, Occupant behavior, Deep Q-learning},
abstract = {Occupants’ behavior is a major source of uncertainty for the optimal operation of building energy systems. The highly stochastic hot water use behavior of occupants has led to conservative operational strategies for hot water systems, that try to ensure occupants’ comfort by following energy-intensive operational approaches. Intending to integrate the occupants’ behavior into hot water systems control, this study proposes a control framework based on Reinforcement Learning, which can learn the stochastic occupants' behavior and make a balance between opposing objectives of water hygiene, comfort, and energy use. A model-free approach is implemented to ensure transferability. To achieve fast convergence on the target house while being model-free, this study proposes an offline training procedure integrating a stochastic hot water use model to mimic the hot water use behavior of occupants. The proposed framework is then evaluated using an actual hot water use and weather dataset collected over 29 weeks from a residential house in Switzerland. The performance of the proposed control framework is compared to the conventional rule-based controller as the common practice in hot water systems. While the hot water use dataset has been collected during the COVID-19 pandemic, with an abnormal schedule of occupants, results indicate that the proposed control framework could successfully learn and adapt to the occupants' behavior and achieve 23.8% of energy saving, while maintaining the occupants' comfort and water hygiene. The adaptive nature of the proposed control framework shows significant potential in reducing the discrepancy between supply and demand in hot water systems.}
}
@article{CHANG2023102570,
title = {Hierarchical multi-robot navigation and formation in unknown environments via deep reinforcement learning and distributed optimization},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {83},
pages = {102570},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102570},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000467},
author = {Lu Chang and Liang Shan and Weilong Zhang and Yuewei Dai},
keywords = {Multi-robot systems (MRSs), Deep reinforcement learning, Mobile robot navigation, Collision avoidance, Formation control, Distributed optimization},
abstract = {Compared with a single robot, Multi-robot Systems (MRSs) can undertake more challenging tasks in complex scenarios benefiting from the increased transportation capacity and fault tolerance. This paper presents a hierarchical framework for multi-robot navigation and formation in unknown environments with static and dynamic obstacles, where the robots compute and maintain the optimized formation while making progress to the target together. In the proposed framework, each single robot is capable of navigating to the global target in unknown environments based on its local perception, and only limited communication among robots is required to obtain the optimal formation. Accordingly, three modules are included in this framework. Firstly, we design a learning network based on Deep Deterministic Policy Gradient (DDPG) to address the global navigation task for single robot, which derives end-to-end policies that map the robot’s local perception into its velocity commands. To handle complex obstacle distributions (e.g. narrow/zigzag passage and local minimum) and stabilize the training process, strategies of Curriculum Learning (CL) and Reward Shaping (RS) are combined. Secondly, for an expected formation, its real-time configuration is optimized by a distributed optimization. This configuration considers surrounding obstacles and current formation status, and provides each robot with its formation target. Finally, a velocity adjustment method considering the robot kinematics is designed which adjusts the navigation velocity of each robot according to its formation target, making all the robots navigate to their targets while maintaining the expected formation. This framework allows for formation online reconfiguration and is scalable with the number of robots. Extensive simulations and 3-D evaluations verify that our method can navigate the MRS in unknown environments while maintaining the optimal formation.}
}
@article{ZHANG2023105995,
title = {Fusing domain knowledge and reinforcement learning for home integrated demand response online optimization},
journal = {Engineering Applications of Artificial Intelligence},
volume = {121},
pages = {105995},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.105995},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623001793},
author = {Zhiyao Zhang and Yongxin Su and Mao Tan and Rui Cao},
keywords = {Reinforcement learning, Integrated demand response, Rule-based control, Home energy management, Knowledge integration},
abstract = {Electricity–gas integrated household energy systems (HESs) expose obvious system uncertainties, requiring their integrated demand response (IDR) programs to be able to adapt automatically and quickly to system changes. Deep Reinforcement Learning (DRL) methods, though having been proven promising to tackle such problems, are typically not efficient in learning from random explorations. This paper proposes a method based on DRL with HESIDR knowledge penetration. By interpreting the domain IDR knowledge as a set of control rules, the DRL agent gains learning samples from knowledge-based exploration in addition to the traditional exploration–exploitation tradeoff. Correspondingly, we develop a cooperation scheme for action selection and replay sampling, which is based on exponential probability functions to balance the penetration of knowledge-based exploration, random exploration and policy exploitation. We conduct case studies in a typical home multi-energy system environment. After determining parameters in the exponential probability functions, the learning and cost reduction performance of the proposed algorithm was tested. The results show that the method proposed in the present study spends 48.78% less training time than the standard DQN, which enables few-minute optimization on a lightweight PC. Also, the proposed method can further reduce energy bills by 26.17% compared to the uncontrolled scenario and by 9.88% to the integrated rule-based controller.}
}
@article{ZHENG2021149,
title = {Active disturbance rejection controller for multi-area interconnected power system based on reinforcement learning},
journal = {Neurocomputing},
volume = {425},
pages = {149-159},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.03.070},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220304574},
author = {Yuemin Zheng and Zengqiang Chen and Zhaoyang Huang and Mingwei Sun and Qinglin Sun},
keywords = {Reinforcement learning (RL), Q-learning, Active disturbance rejection control (ADRC), Linear active disturbance rejection control (LADRC), Deregulated power system, Load frequency control},
abstract = {In this paper, a method of Active Disturbance Rejection Controller (ADRC) is proposed based on Q-learning of Reinforcement Learning (RL) for multi-area interconnected power system. Excessive changes in load can cause instability to the system. Therefore, the ADRC controller is used to keep the load within rated range for its strong anti-interference performance and Q-learning algorithm to select the adaptive parameters of the controller. Finally, through simulation experiments on traditional and deregulated three-area interconnected power system respectively, the effectiveness of the proposed method is proved and the results show that reinforcement learning can indeed be used to solve the problem of controller parameter adjustment.}
}
@article{MILLARD2007576,
title = {Tuning pianos using reinforcement learning},
journal = {Applied Acoustics},
volume = {68},
number = {5},
pages = {576-593},
year = {2007},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2006.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X06000673},
author = {Matthew Millard and Hamid R. Tizhoosh},
keywords = {Piano tuning, Reinforcement learning, Impact tuning hammer, Automated piano tuning system},
abstract = {The tuning system of a piano has remained relatively unchanged since the instrument’s inception. A piano’s tuning system has been designed to be both inexpensive to manufacture and to preserve the tension and thus pitch of each string over long periods of time. This tuning system requires such a high degree of skill to manipulate that only trained professionals are able to tune pianos. This paper presents a novel adjustable impact tuning hammer and a reinforcement learning control system that may allow piano owners to tune their own pianos in the future.}
}
@article{AN2022108437,
title = {A reinforcement learning guided adaptive cost-sensitive feature acquisition method},
journal = {Applied Soft Computing},
volume = {117},
pages = {108437},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108437},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622000163},
author = {Chaojie An and Qifeng Zhou and Shen Yang},
keywords = {Cost-sensitive, Feature acquisition, Recurrent neural network, Reinforcement learning},
abstract = {Most of the existing feature selection methods tend to pursue the learning performance of the selected feature subset while ignoring the costs of acquiring each feature. However, in real-world problems, we often face the tradeoff between model performance and feature costs because of limited resources. Moreover, in some applications (e.g. medical tests), features are acquired sequentially in the learning process instead of having known the information of the whole feature set in advance. To solve these problems we design a reinforcement learning agent to guide the cost-sensitive feature acquisition process and propose a deep learning-based model to select the informative and lower-cost features for each instance adaptively. The whole process of feature acquisition will be determined by an agent according to what it has observed from inputs. In particular, a Recurrent Neural Network (RNN) model will learn the knowledge from the current sample and the agent will give the instructions on whether the RNN model will continue to select the next feature or stop the sequential feature acquisition process. Moreover, the proposed method can also select the features per block thus it can deal with high dimensional data. We evaluate the effectiveness of the proposed method on a variety of datasets including benchmark datasets, gene datasets, and medical datasets. Compared with the state-of-the-art feature selection methods, the proposed method can achieve comparable learning accuracy while maintaining lower feature costs.}
}
@article{MWASINGA2023104745,
title = {RASM: Resource-Aware Service Migration in Edge Computing based on Deep Reinforcement Learning},
journal = {Journal of Parallel and Distributed Computing},
volume = {182},
pages = {104745},
year = {2023},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2023.104745},
url = {https://www.sciencedirect.com/science/article/pii/S0743731523001156},
author = {Lusungu Josh Mwasinga and Duc-Tai Le and Syed M. Raza and Rajesh Challa and Moonseong Kim and Hyunseung Choo},
keywords = {Multi-access Edge computing, Service migration, Resource management, Deep Reinforcement Learning (DRL), Deep Q-Network (DQN)},
abstract = {Multi-access Edge Computing (MEC) paradigm allows devices to offload their intensive service tasks that require high Quality of Experience (QoE). Devices mobility forces services to migrate between MECs to maintain QoE in terms of delay. The decision on when to migrate a service requires a cost and QoE tradeoff, and destination MEC selection needs to be done upon latency and resource availability constraints to minimize migrations. To this end, we propose a novel Resource-Aware Service Migration (RASM) mechanism using Deep Q-Network (DQN) to make migration decisions by achieving tradeoff between the QoE in terms of delay and migration cost. Moreover, DQN learns the best policy for maximizing QoE by selecting the migration destination based on the MECs proximity to the device and estimated resource availability at the servers using queuing model. Results show faster convergence to optimal policy, reduced average end-to-end service delay by 27%, and smaller service rejection rate by 24% comparing to the state-of-the-art.}
}
@article{JAISWAL2023110577,
title = {Comment on “Deep reinforcement learning approach for MPPT control of partially shaded PV systems in Smart Grids”},
journal = {Applied Soft Computing},
volume = {146},
pages = {110577},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110577},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623005951},
author = {Vicky Jaiswal and Archit Wadehra and Siddhant Bhalla and K.P.S. Rana and Vineet Kumar},
keywords = {MPPT, Deep RL, PV systems, OpenAI gym},
abstract = {In a recent work, (Avila et al., 2020), an environment was developed and reported for partial shading conditions (PSC) in the open-source OpenAI Gym platform. This work presented deep reinforcement learning (DRL) techniques to address the maximum power point tracking (MPPT) problem of a photovoltaic (PV) array under PSC. Two DRL algorithms, namely, Deep Deterministic Policy Gradient (DDPG) and Twin Delayed DDPG (TD3) were investigated. A deviation of less than 1%, compared to the theoretical maximum power, was claimed for the DDPG algorithm. Based on the presented fact-based investigations, this comment highlights the issues in the reported approach. The main issues are found to be the approximate PV array modeling, erroneous performance metric and erroneous choice of DRL algorithms. Of the presented PSCs, the proposed technique in Avila et al. (2020) always attained the very first peak of the PV characteristic of the PV system. It may be noted that the first peak may not always be the global maximum power point. Overall, based on the presented investigations this comment demonstrates that the DDPG technique used in Avila et al. (2020) is not able to effectively address the MPPT problem in PV array under PSC.}
}
@article{ZHOU2022118078,
title = {A data-driven strategy using long short term memory models and reinforcement learning to predict building electricity consumption},
journal = {Applied Energy},
volume = {306},
pages = {118078},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.118078},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921013647},
author = {Xinlei Zhou and Wenye Lin and Ritunesh Kumar and Ping Cui and Zhenjun Ma},
keywords = {Long short term memory, Data-driven method, Reinforcement learning, Electricity consumption prediction},
abstract = {Data-driven modeling emerges as a promising approach to predicting building electricity consumption and facilitating building energy management. However, the majority of the existing models suffer from performance degradation during the prediction process. This paper presents a new strategy that integrates Long Short Term Memory (LSTM) models and Reinforcement Learning (RL) agents to forecast building next-day electricity consumption and peak electricity demand. In this strategy, LSTM models were first developed and trained using the historical data as the base models for prediction. RL agents were further constructed and introduced to learn a policy that can dynamically tune the parameters of the LSTM models according to the prediction error. This strategy was tested using the electricity consumption data collected from a group of university buildings and student accommodations. The results showed that for the student accommodations which showed relatively large monthly variations in daily electricity consumption, the proposed strategy can increase the prediction accuracy by up to 23.5% as compared with the strategy using the LSTM models only. However, when it was applied to the buildings with insignificant monthly variations in the daily electricity consumption, the prediction accuracy did not show an obvious improvement when compared with the use of the LSTM models alone. This study demonstrated how to use LSTM models and reinforcement learning with self-optimization capability to likely provide more reliable prediction in daily electricity consumption and thus to facilitate building optimal operation and demand side management.}
}
@article{WANG2023120759,
title = {Secure energy management of multi-energy microgrid: A physical-informed safe reinforcement learning approach},
journal = {Applied Energy},
volume = {335},
pages = {120759},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.120759},
url = {https://www.sciencedirect.com/science/article/pii/S030626192300123X},
author = {Yi Wang and Dawei Qiu and Mingyang Sun and Goran Strbac and Zhiwei Gao},
keywords = {Multi-energy microgrid, Energy management, Dynamic security assessment, Physical-informed safety layer, Reinforcement learning},
abstract = {The large-scale integration of distributed energy resources into the energy industry enables the fast transition to a decarbonized future but raises some potential challenges of insecure and unreliable operations. Multi-energy Microgrids (MEMGs), as localized small multi-energy systems, can effectively integrate a variety of energy components with multiple energy sectors, which have been recently recognized as a valid solution to improve the operational security and reliability. As a result, a massive amount of research has been conducted to investigate MEMG energy management problems, including both model-based optimization and model-free learning approaches. Compared to optimization approaches, reinforcement learning is being widely deployed in MEMG energy management problems owing to its ability to handle highly dynamic and stochastic processes without knowing any system knowledge. However, it is still difficult for conventional model-free reinforcement learning methods to capture the physical constraints of the MEMG model, which may therefore destroy its secure operation. To address this research challenge, this paper proposes a novel safe reinforcement learning method by learning a dynamic security assessment rule to abstract a physical-informed safety layer on top of the conventional model-free reinforcement learning energy management policy, which can respect all the physical constraints through mathematically solving an action correction formulation. In this setting, the secure energy management of the MEMG can be guaranteed for both training and test procedures. Extensive case studies based on two integrated systems (i.e., a small 6-bus power and 7-node gas network, and a large 33-bus power and 20-node gas network) are carried out to verify the superior performance of the proposed physical-informed reinforcement learning method in achieving a cost-effective MEMG energy management performance while respecting all the physical constraints, compared to conventional reinforcement learning and optimization approaches.}
}
@article{WANG2022110868,
title = {Deep reinforcement learning and adaptive policy transfer for generalizable well control optimization},
journal = {Journal of Petroleum Science and Engineering},
volume = {217},
pages = {110868},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2022.110868},
url = {https://www.sciencedirect.com/science/article/pii/S0920410522007240},
author = {Zhongzheng Wang and Kai Zhang and Jinding Zhang and Guodong Chen and Xiaopeng Ma and Guojing Xin and Jinzheng Kang and Hanjun Zhao and Yongfei Yang},
keywords = {Well control optimization, Deep reinforcement learning, Policy transfer, Generalizable optimization},
abstract = {Well control optimization is a challenging task but plays a critical role in reservoir management. Traditional methods independently solve each task from scratch and the obtained scheme is only applicable to the environment where the optimization process is run. In stark contrast, human experts are adept at learning and building generalizable skills and using them to efficiently draw inferences and make decisions for similar scenarios. Inspired by the recently proposed generalizable field development optimization approach, this work presents an adaptive and robust deep learning-based Representation-Decision-Transfer (DLRDT) framework to deal with the generalization problem in well control optimization. Specifically, DLRDT uses a three-stage workflow to train an artificial agent. First, the agent develops its vision and understands its surroundings by learning a latent state representation with domain adaptation techniques. Second, the agent is tasked with using high-performance deep reinforcement learning algorithms to train the optimal control policy in the latent state space. Finally, the agent is transferred and evaluated in several environments that were not seen during the training. Compared with previous methods that optimize a solution for a specific scenario, our approach trains a policy that is not only robust to variations in their environments but can adapt to unseen (but similar) environments without additional training. For a demonstration, we validate the proposed framework on waterflooding well control optimization problems. Experimental evaluations on two three-dimensional reservoir models demonstrate the trained agent has excellent optimization efficiency and generalization performance. Our approach is particularly favorable when considering the deployment of schemes in the real world as it can handle unforeseen situations.}
}
@article{LI2022241,
title = {Entropy-based Reinforcement Learning for computation offloading service in software-defined multi-access edge computing},
journal = {Future Generation Computer Systems},
volume = {136},
pages = {241-251},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22002011},
author = {Kexin Li and Xingwei Wang and Qiang Ni and Min Huang},
keywords = {Multi-access edge computing, Computation offloading, Reinforcement learning, Entropy, Software-defined networking, Markov decision process},
abstract = {The rapid growth of Internet of Things (IoT) devices and the emergence of multiple edge applications have resulted in an explosive growth of data traffic at the edge of the networks. Computation offloading services in Multi-access edge computing (MEC) enabled networks to offer potentials of a better Quality of Service (QoS) than traditional networks. They are expected to reduce the propagation delay and enhance the computational capability for delay-sensitive tasks especially. Nevertheless, the distributed computing resources of edge devices urgently need reasonable resource controllers to ensure such distributed computing resources to be effectively scheduled. The benefits of Software-Defined Networking (SDN) may be explored to demonstrate their full potential through MEC services to reduce the response time of programs. In this paper, a new SDN-based MEC computation offloading service architecture is proposed to increase the coordination and offloading capabilities at the control plane. Besides, to deal with dynamic network changes and increase the exploration degree, we propose a novel Entropy-based Reinforcement Learning algorithm for delay-sensitive tasks computation offloading at the edge of the networks. Finally, the evaluation findings indicate that our proposed model has the potential to improve the network resource allocation and balanced performance significantly.}
}
@article{AMIN2023102148,
title = {A deep reinforcement learning for energy efficient resource allocation Intelligent Reflecting Surface (IRS) driven Non-Orthogonal Multiple Access Beamforming (NOMA-BF)},
journal = {Physical Communication},
volume = {60},
pages = {102148},
year = {2023},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2023.102148},
url = {https://www.sciencedirect.com/science/article/pii/S1874490723001519},
author = {Subba Amin and Javaid A. Sheikh and  Mehboob-ul-Amin and Bilal A. Malik},
keywords = {Non Orthogonal Multiple Access (NOMA), Intelligent Reflective Surface (IRS), Resource Allocation (RA), Outage Probability (OP), Deep Neural Networks (DNN), Deep Learning (DL), Reinforcement Learning (RL)},
abstract = {This paper proposes a deep reinforcement learning centralized intelligent reflective surface (IRS) assisted non-orthogonal multiple access beamforming (NOMA-BF) system to improve the capacity gains and energy efficiency of next generation wireless communication networks. A single IRS is deployed at base station (BS) and consists of with R reflecting elements which is used to communicate with N single receiver antennas, referred to as centralized deployment of IRS- assisted NOMA. In the proposed central control scheme, we consider the practical situation where the instantaneous Channel State Information (CSI) of the channel between the IRS and the users is unknown. However, its long-term average is known and therefore makes the system more practical. Consequently, we exploit deep reinforcement technique learning technique to investigate the phase shift design and to tackle optimization problem. The energy efficiency problem is formulated as a non-convex optimization problem and is solved using deep and reinforcement learning algorithms. The first order Taylor approximation and difference of convex (DC) programming method using MATLAB tool box has been adopted to obtain the optimal values for power allocation coefficients. For interference mitigation, efficient user clustering along with beamforming algorithm is exploited. The NOMA user interference is tackled efficiently through the proposed algorithms that compute power allocation coefficients along with desirable phase shifts. The closed form expressions for achievable rates and SINR are derived under the required constraints. Simulation results demonstrate that proposed deep learning and reinforcement learning algorithms achieve improved performances in terms of sum rate, energy efficiency and coverage with lower computational complexity and symbol error rate. The computational complexity reduction has been proved both analytically and graphically and is in agreement.}
}
@article{YI2022120113,
title = {Deep reinforcement learning based optimization for a tightly coupled nuclear renewable integrated energy system},
journal = {Applied Energy},
volume = {328},
pages = {120113},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120113},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922013708},
author = {Zonggen Yi and Yusheng Luo and Tyler Westover and Sravya Katikaneni and Binaka Ponkiya and Suba Sah and Sadab Mahmud and David Raker and Ahmad Javaid and Michael J. Heben and Raghav Khanna},
keywords = {Nuclear renewable integrated energy system, Deep reinforcement learning, System control, Operation optimization},
abstract = {New ways to integrate energy systems to maximize efficiency are being sought to meet carbon emissions goals. Nuclear-renewable integrated energy system (NR-IES) concepts are a leading solution that couples a nuclear power plant with renewable energy, hydrogen generation plants, and energy storage systems, such that thermal and electrical power are dispatchable to fulfill grid-flexibility requirements while also producing hydrogen and maximizing revenue. This paper introduces a deep reinforcement learning (DRL)-based framework to address the complex decision-making tasks for NR-IES. The objective is to maximize revenue by generating and selling hydrogen and electricity simultaneously according to their time-varying prices while keeping the energy flow in the subsystems in balance. A Python-based simulator for a NR-IES concept has been developed to integrate with OpenAI Gym and Ray/RLlib to enable an efficient and flexible computational framework for DRL research and development. Three state-of-the-art DRL algorithms have been investigated, including two-delayed deep deterministic policy gradient (TD3), soft-actor critic (SAC), proximal policy optimization (PPO), to illustrate DRL’s superiority for controlling NR-IES by comparing it with a conventional control approach, particle swarm optimization (PSO). In this effort, PPO has shown more-stable performance and also better generalization capability than SAC and TD3. Comparisons with PSO have demonstrated that, on average, PPO can achieve 13.9% more mean episode returns from the training process and 29.4% more mean episode returns from the testing process when different hydrogen-production targets are applied.}
}
@article{WEINBERG2023104351,
title = {A Review of Reinforcement Learning for Controlling Building Energy Systems From a Computer Science Perspective},
journal = {Sustainable Cities and Society},
volume = {89},
pages = {104351},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.104351},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722006552},
author = {David Weinberg and Qian Wang and Thomas Ohlson Timoudas and Carlo Fischione},
keywords = {Building Energy System, HVAC, Heating, Cooling, Reinforcement learning, Machine learning, RL, ML},
abstract = {Energy efficient control of energy systems in buildings is a widely recognized challenge due to the use of low temperature heating, renewable electricity sources, and the incorporation of thermal storage. Reinforcement Learning (RL) has been shown to be effective at minimizing the energy usage in buildings with maintained thermal comfort despite the high system complexity. However, RL has certain disadvantages that make it challenging to apply in engineering practices. In this review, we take a computer science approach to identifying three main categories of challenges of using RL for control of Building Energy Systems (BES). The three categories are the following: RL in single buildings, RL in building clusters, and multi-agent aspects. For each topic, we analyse the main challenges, and the state-of-the-art approaches to alleviate them. We also identify several future research directions on subjects such as sample efficiency, transfer learning, and the theoretical properties of RL in building energy systems. In conclusion, our review shows that the work on RL for BES control is still in its initial stages. Although significant progress has been made, more research is needed to realize the goal of RL-based control of BES at scale.}
}
@article{LI2021102355,
title = {Deep reinforcement learning-based energy management of hybrid battery systems in electric vehicles},
journal = {Journal of Energy Storage},
volume = {36},
pages = {102355},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.102355},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21001158},
author = {Weihan Li and Han Cui and Thomas Nemeth and Jonathan Jansen and Cem Ünlübayir and Zhongbao Wei and Lei Zhang and Zhenpo Wang and Jiageng Ruan and Haifeng Dai and Xuezhe Wei and Dirk Uwe Sauer},
keywords = {Lithium-ion battery, Hybrid battery system, Reinforcement learning, Deep Q-learning, Energy management, Electric vehicle},
abstract = {In this paper, we propose an energy management strategy based on deep reinforcement learning for a hybrid battery system in electric vehicles consisting of a high-energy and a high-power battery pack. The energy management strategy of the hybrid battery system was developed based on the electrical and thermal characterization of the battery cells, aiming at minimizing the energy loss and increasing both the electrical and thermal safety level of the whole system. Primarily, we designed a novel reward term to explore the optimal operating range of the high-power pack without imposing a rigid constraint of state of charge. Furthermore, various load profiles were randomly combined to train the deep Q-learning model, which avoided the overfitting problem. The training and validation results showed both the effectiveness and reliability of the proposed strategy in loss reduction and safety enhancement. The proposed energy management strategy has demonstrated its superiority over the reinforcement learning-based methods in both computation time and energy loss reduction of the hybrid battery system, highlighting the use of such an approach in future energy management systems.}
}
@article{LIU2023110207,
title = {Exploring the first-move balance point of Go-Moku based on reinforcement learning and Monte Carlo tree search},
journal = {Knowledge-Based Systems},
volume = {261},
pages = {110207},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110207},
url = {https://www.sciencedirect.com/science/article/pii/S095070512201303X},
author = {Pengsen Liu and Jizhe Zhou and Jiancheng Lv},
keywords = {Go-Moku, First-move balance problem, Reinforcement learning, Monte Carlo tree search},
abstract = {In most chess games without additional rule restrictions, the side that makes the first move (i.e., the first-move side) has an absolute advantage, which affects the game’s balance to a certain extent. Artificial intelligence (AI) training in some chess games can be bottlenecked by the imbalance of opening moves, making it challenging to improve chess strength. The first-move balance problem can be explored to achieve a balanced win rate in chess games. This study uses Go-Moku as an example to explore the first-move balance point problem for different sizes of Go-Moku boards. We design a self-playing Go-Moku intelligence algorithm using deep reinforcement learning and Monte Carlo tree search (MCTS), which can considerably save arithmetic power without affecting the strength of the AI. To address the characteristics of Go-Moku and its complexity, we propose an algorithm using dynamic MCTS simulation counts, which only employs a reasonable amount of hyperparameters to achieve better performance with the cost of a relatively small number of simulations. By symmetrically expanding the data and optimizing the exploration and selection allocation, the training efficiency of the Go-Moku AI is improved through Multiple Process Interface (MPI) multi-processes. Building the test model of first-move balance points for a universal Go-Moku board, we obtain a set of first-move balance points for different board sizes. The first-move balance point of Go-Moku that makes the game even is found by simulating the game win rate for all first-move drop points. The experimental results demonstrate that the proposed algorithm can achieve world-leading chess strength in Gomocup by engine play tests and can find the first-move balance point of Go-Moku on boards of various sizes. The results of this study will help optimize the rule setting of Go-Moku and improve the training efficiency of AI in the field of Go-Moku, which can be extended to the exploration of balance in other chess games.}
}
@article{NASIR2023111945,
title = {Deep reinforcement learning for optimal well control in subsurface systems with uncertain geology},
journal = {Journal of Computational Physics},
volume = {477},
pages = {111945},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.111945},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123000402},
author = {Yusuf Nasir and Louis J. Durlofsky},
keywords = {Deep reinforcement learning, Closed-loop modeling, Stochastic optimal control, Reservoir simulation, Transformers, Proximal policy optimization},
abstract = {A general control policy framework based on deep reinforcement learning (DRL) is introduced for closed-loop decision making in subsurface flow settings. Traditional closed-loop modeling workflows in this context involve the repeated application of data assimilation/history matching and robust optimization steps. Data assimilation can be particularly challenging in cases where both the geological style (scenario) and individual model realizations are uncertain. The closed-loop reservoir management (CLRM) problem is formulated here as a partially observable Markov decision process (POMDP), with the associated optimization problem solved using a proximal policy optimization algorithm. This provides a control policy that instantaneously maps flow data observed at wells (as are available in practice) to optimal well pressure settings. The policy is represented by a temporal convolution and gated transformer blocks. Training is performed in a preprocessing step with an ensemble of prior geological models, which can be drawn from multiple geological scenarios. Example cases involving the production of oil via water injection, with both 2D and 3D geological models, are presented. The DRL-based methodology is shown to result in an increase in NPV of 15% (for the 2D cases) and 33% (3D cases) relative to robust optimization over prior models, and to an improvement of 2 - 7% in NPV relative to traditional CLRM. The solutions from the control policy are found to be comparable to those from deterministic optimization, in which the geological model is assumed to be known, even when multiple geological scenarios are considered. The control policy approach results in a 76% decrease in computational cost relative to traditional CLRM with the algorithms and parameter settings considered in this work.}
}
@article{WANG2020100815,
title = {A reinforcement learning-based predator-prey model},
journal = {Ecological Complexity},
volume = {42},
pages = {100815},
year = {2020},
issn = {1476-945X},
doi = {https://doi.org/10.1016/j.ecocom.2020.100815},
url = {https://www.sciencedirect.com/science/article/pii/S1476945X1930039X},
author = {Xueting Wang and Jun Cheng and Lei Wang},
keywords = {Q-learning, Monte Carlo simulation, Population dynamics},
abstract = {Classic population models can often predict the dynamics of biological populations in nature. However, the adaptation process and learning mechanism of species are rarely considered in the study of population dynamics, due to the complex interaction of species, seasonal variation, spatial distribution or other factors. We use reinforcement learning algorithms to improve the existing individual-based ecosystem simulation algorithms, which allows species to spontaneously adjust their strategies according to a short period of experience and then feed back to improve their abilities to make action decisions. Our results show that the reinforcement learning of predators is beneficial to the stability of the ecosystem, and predators can learn to spontaneously form hunting patterns that surround their prey. The learning of prey makes the ecosystem oscillate and meanwhile leads to a higher risk of extinction for predators. When individuals are more likely to die, these herbivores rely on reproductive behavior to maintain their populations; when individuals live longer, herbivores spend more time eating to maintain their own survival. The co-reinforcement learning of predators and prey helps predators to find a more suitable way to survive with their prey, that is, the number of predators is more stable and larger than when only predator or only prey learns.}
}
@article{ELFAKDI2008155,
title = {Direct Policy Search Reinforcement Learning for Autonomous Underwater Cable Tracking},
journal = {IFAC Proceedings Volumes},
volume = {41},
number = {1},
pages = {155-160},
year = {2008},
note = {2nd IFAC Workshop on Navigation, Guidance and Control of Underwater Vehicles},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20080408-3-IE-4914.00028},
url = {https://www.sciencedirect.com/science/article/pii/S147466701535518X},
author = {A. El-Fakdi and M. Carreras and J. Batlle},
keywords = {Machine Learning, Robot Control, Underwater Vehicles},
abstract = {This paper proposes a field application of a high-level Reinforcement Learning (RL) control system for solving the action selection problem of an autonomous robot in a cable tracking task. The learning system is characterized by using a Direct Policy Search method for learning the internal state/action mapping. Policy only algorithms may suffer from long convergence times when dealing with real robotics. In order to speed up the process, the learning phase has been carried out in a simulated environment and, in a second step, the policy has been transferred and tested successfully on a real robot. Future steps plan to continue the learning process on-line while on the real robot while performing the mentioned task. We demonstrate its feasibility with real experiments on the underwater robot ICTINEUAUV.}
}
@article{JEON2023121198,
title = {Deep reinforcement learning for cooperative robots based on adaptive sentiment feedback},
journal = {Expert Systems with Applications},
pages = {121198},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121198},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423017001},
author = {Haein Jeon and Dae-Won Kim and Bo-Yeong Kang},
keywords = {Human–robot interaction, Deep reinforcement learning, Interactive reinforcement learning, Human-in-the-loop, Reward shaping},
abstract = {Human–robot cooperative tasks have gained importance with the emergence of robotics and artificial intelligence technology. In interactive reinforcement learning techniques, robots learn target tasks by receiving feedback from an experienced human trainer. However, most interactive reinforcement learning studies require a separate process to integrate the trainer’s feedback into the training dataset, making it challenging for robots to learn new tasks from humans in real- time. Furthermore, the types of feedback sentences that trainers can use are limited in previous research. To address these limitations, this paper proposes a robot teaching strategy that uses deep RL via human–robot interaction to learn table balancing tasks interactively. The proposed system employs Deep Q-Network with real-time sentiment feedback delivered through the trainer’s speech to learn cooperative tasks. We designed a novel reward function that incorporates sentiment feedback from human speech in real-time during the learning process. The paper presents an improved reward shaping technique based on subdivided feedback levels and shrinking feedback. This function serves as a guide for the robot to engage in natural interactions with humans and enables it to learn the tasks effectively. Experimental results demonstrate that the proposed interactive deep reinforcement learning model achieved a high success rate of up to 99.06%, outperforming the model without sentiment feedback.}
}
@article{ZHU2021126107,
title = {Ecological scheduling of the middle route of south-to-north water diversion project based on a reinforcement learning model},
journal = {Journal of Hydrology},
volume = {596},
pages = {126107},
year = {2021},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2021.126107},
url = {https://www.sciencedirect.com/science/article/pii/S0022169421001542},
author = {Jie Zhu and Zhao Zhang and Xiaohui Lei and Xiang Jing and Hao Wang and Peiru Yan},
keywords = {Middle route, South-to-North Water Diversion project, Abnormal algae proliferation, Reinforcement learning model, Ecological scheduling model},
abstract = {The Middle Route of the South-to-North Water Diversion project (MRP) effectively alleviates the problem of serious shortage of water resources in North China. However, owing to the long-term operation of this project, abnormal algal proliferation has occurred in the upper reaches of the main canal, and the water diversion outlet is easily blocked by the large colonies of algae falling into the canal, which has seriously threatened the water quality and water supply safety of the canal. In light of this issue, this study coupled the reinforcement learning model with the one-dimensional hydrodynamic model of the series sluice group to build an ecological scheduling model for prevention and control of algal proliferation in the MRP. The model was applied to the canal section (approximately 300 km long, from No. G2 to No. G17) where in algae grows easily. The results show that: (1) The model can achieve the desired reciprocating adjustment of the canal water level in the process of decline stability rise within 0.8 m and stably maintain the low water level for more than 72 h; (2) during the process of ecological scheduling, the maximum drawdown of water level satisfying the safety regulation and control constraints of the MRP is 1.0 m; (3) for an ecological regulation water level drop greater than 0.3 m, it is more difficult to restore the initial water level, and the water level rise range should therefore be greater than the decrease range; (4) the constructed model exhibits high robustness. Even for an observation error level less than 0.08 m, the success rate of the model can still exceed 80%. The ecological scheduling scheme generated by the model can be used to destroy the algal habitat conditions through hydrodynamic dynamic regulation, whereby decisional support can be provided for the prevention and control of abnormal proliferation of algae in the main canal.}
}
@article{VINCENT2012599,
title = {A combined reactive and reinforcement learning controller for an autonomous tracked vehicle},
journal = {Robotics and Autonomous Systems},
volume = {60},
number = {4},
pages = {599-608},
year = {2012},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2011.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0921889011002223},
author = {Isabelle Vincent and Qiao Sun},
keywords = {Control, Learning, Unmanned ground vehicle, Terrain mapping, Tracked robot},
abstract = {Unmanned ground vehicles currently exhibit simple autonomous behaviours. This paper presents a control algorithm developed for a tracked vehicle to autonomously climb obstacles by varying its front and back track orientations. A reactive controller computes a desired geometric configuration based on terrain information. A reinforcement learning algorithm enhances vehicle mobility by finding effective exit strategies in deadlock situations. It is capable of incorporating complex information including terrain and vehicle dynamics through learned experiences. Experiments illustrate the effectiveness of the proposed approach for climbing various obstacles.}
}
@article{ZHANG2022111453,
title = {AUV path tracking with real-time obstacle avoidance via reinforcement learning under adaptive constraints},
journal = {Ocean Engineering},
volume = {256},
pages = {111453},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.111453},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822008320},
author = {Chenming Zhang and Peng Cheng and Bin Du and Botao Dong and Weidong Zhang},
keywords = {Autonomous underwater vehicle (AUV), Path tracking, Real-time obstacle avoidance, Deep deterministic policy gradient (DDPG), Adaptive constraints},
abstract = {In this paper, the methods are proposed for underactuated autonomous underwater vehicle (AUV) to address three-dimensional (3D) path tracking and real-time obstacle avoidance. The errors of path tracking are generated based on the Serret–Frenet frame and line-of-sight (LOS) guidance law, while the errors in obstacle avoidance are obtained based on the carrier coordinate system to filter irrelevant environment information. On this basis, to deal with the complicated target path and unknown obstacles, the controller is designed by deep deterministic policy gradient (DDPG) algorithm and adaptive multi-constraints. The safety constraints are adopted in reward functions to avoid useless explorations and facilitate convergence. The training proceeds for path tracking and obstacle avoidance respectively. Compared to the original DDPG algorithm in the training, the proposed algorithm shows faster convergence. Various simulations are conducted under different initial conditions, and the results demonstrate the effectiveness of the proposed algorithm.}
}
@article{WANG202226,
title = {Reinforcement learning-based finite-time tracking control of an unknown unmanned surface vehicle with input constraints},
journal = {Neurocomputing},
volume = {484},
pages = {26-37},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.133},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221015733},
author = {Ning Wang and Ying Gao and Chen Yang and Xuefeng Zhang},
keywords = {Reinforcement learning-based finite-time control, Optimal tracking control, Unknown system dynamics, Input constraints, Unmanned surface vehicle},
abstract = {In this paper, subject to completely unknown system dynamics and input constraints, a reinforcement learning-based finite-time trajectory tracking control (RLFTC) scheme is innovatively created for an unmanned surface vehicle (USV) by combining actor-critic reinforcement learning (RL) mechanism with finite-time control technique. Unlike previous RL-based tracking which requires infinite-time convergence thereby rather sensitive to complex unknowns, an actor-critic finite-time control structure is created by employing adaptive neural network identifiers to recursively update actor and critic, such that learning-based robustness can be sufficiently enhanced. Moreover, deduced from the Bellman error formulation, the proposed RLFTC is directly optimized in a finite-time manner. Theoretical analysis eventually shows that the proposed RLFTC scheme can ensure semi-global practical finite-time stability (SGPFS) for a closed-loop USV system and tracking errors converge to an arbitrarily small neighborhood of the origin in a finite time, subject to optimal cost. Both mathematical simulation and virtual-reality experiments demonstrate remarkable effectiveness and superiority of the proposed RLFTC scheme.}
}
@article{XU2023115018,
title = {Real-time planning and collision avoidance control method based on deep reinforcement learning},
journal = {Ocean Engineering},
volume = {281},
pages = {115018},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.115018},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823014026},
author = {Xinli Xu and Peng Cai and Yunlong Cao and Zhenzhong Chu and Wenbo Zhu and Weidong Zhang},
keywords = {Unmanned surface vehicle, Real-time planning, Collision avoidance control, Deep reinforcement learning, Neural network, Experience playback},
abstract = {Real time planning and collision avoidance is an important part of the motion planning for USVs, its core is to design effective planning and collision avoidance methods. This paper proposes a real-time planning and collision avoidance method based on deep reinforcement learning for USVs in complex environments. Firstly, a novel reward function is designed, which transforms the collision avoidance problem into a deep reinforcement learning strategy solution problem that includes distance, direction, speed, and GOLREGs constraints, it ensures the smooth and safe operation of the USV. Secondly, inefficient exploration and exploitation methods make it difficult for intelligent agents to discover key information in the environment, high value experience is obtained through the designed layered sampling exploration mechanism in this article, the intelligent agent learns effective control strategies through this mechanism, which accelerates the convergence of the strategies and reduces the waste of computing resources. Finally, a real-time collision avoidance simulation platform for the USV is established, and experimental results of different exploration mechanisms are compared, the efficiency of the layered sampling exploration mechanism has been verified. The simulation scenarios from easy to difficult are designed, and the results indicate that the USV can complete real-time planning and collision avoidance tasks in complex environments.}
}
@article{ZHENG2020479,
title = {Reinforcement learning control for underactuated surface vessel with output error constraints and uncertainties},
journal = {Neurocomputing},
volume = {399},
pages = {479-490},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220303581},
author = {Zewei Zheng and Linping Ruan and Ming Zhu and Xiao Guo},
keywords = {Reinforcement learning, Actor-Critic (AC), Output constraints, Underactuated marine vessel, Trajectory tracking, Neural networks},
abstract = {This study investigates the trajectory tracking control problem of an underactuated marine vessel in the presence of output constraints, model uncertainties and environmental disturbances. The error transformation technique can ensure that the tracking errors remain within the predefined constraint boundaries. The controller is designed in combination with the critic function and the reinforcement learning (RL) algorithm based on actor-critic neural networks. The RL method is applied to solve model uncertainties and disturbances, and the critic function modifies the control action to supervise the system performance. Based on Lyapunov’s direct method, a stability analysis is proposed to prove that the boundedness of system signals and the desired tracking performance can be guaranteed. Finally, the simulation illustrates the effectiveness and feasibility of the proposed controller.}
}
@article{CHEN2023338,
title = {Locally generalised multi-agent reinforcement learning for demand and capacity balancing with customised neural networks},
journal = {Chinese Journal of Aeronautics},
volume = {36},
number = {4},
pages = {338-353},
year = {2023},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2023.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S1000936123000109},
author = {Yutong CHEN and Minghua HU and Yan XU and Lei YANG},
keywords = {Air traffic flow management, Demand and capacity balancing, Deep Q-learning network, Flight delays, Generalisation, Ground delay program, Multi-agent reinforcement learning},
abstract = {Reinforcement Learning (RL) techniques are being studied to solve the Demand and Capacity Balancing (DCB) problems to fully exploit their computational performance. A locally generalised Multi-Agent Reinforcement Learning (MARL) for real-world DCB problems is proposed. The proposed method can deploy trained agents directly to unseen scenarios in a specific Air Traffic Flow Management (ATFM) region to quickly obtain a satisfactory solution. In this method, agents of all flights in a scenario form a multi-agent decision-making system based on partial observation. The trained agent with the customised neural network can be deployed directly on the corresponding flight, allowing it to solve the DCB problem jointly. A cooperation coefficient is introduced in the reward function, which is used to adjust the agent’s cooperation preference in a multi-agent system, thereby controlling the distribution of flight delay time allocation. A multi-iteration mechanism is designed for the DCB decision-making framework to deal with problems arising from non-stationarity in MARL and to ensure that all hotspots are eliminated. Experiments based on large-scale high-complexity real-world scenarios are conducted to verify the effectiveness and efficiency of the method. From a statistical point of view, it is proven that the proposed method is generalised within the scope of the flights and sectors of interest, and its optimisation performance outperforms the standard computer-assisted slot allocation and state-of-the-art RL-based DCB methods. The sensitivity analysis preliminarily reveals the effect of the cooperation coefficient on delay time allocation.}
}
@article{YU2023109195,
title = {Offline economic dispatch for multi-area power system via hierarchical reinforcement learning},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {152},
pages = {109195},
year = {2023},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2023.109195},
url = {https://www.sciencedirect.com/science/article/pii/S0142061523002521},
author = {Liying Yu and Dewen Li and Ning Li},
keywords = {Hierarchical reinforcement learning, Multi-area economic dispatch problem, Tie line capacity constraint, Primal-dual consensus-based distributed Q-learning, Off-line optimization manner},
abstract = {In this paper, a novel hierarchical reinforcement learning (HRL) architecture is put forward to resolve the multi-area economic dispatch (MAED) problem in a fully off-line optimization manner, while the focus of MAED is to minimize the total generation cost of the entire multi-area power system by determining output power within area and the power transfer among areas. The original MAED problem is transformed into an augmented sub-MAED problem by exploring the relationship between power balance constraint and tie line capacity constraint in each area. This architecture first calculates the scheduled area load in the top layer through the primal–dual consensus-based distributed Q-learning, and then calculates load-power table assembled by the optimal output power within single area according to the multi-stage decision algorithm in the bottom layer. Acting as an underlying framework, the assigned load-power table is implemented with the combination of off-line calculation and on-line inquiry. It is capable of parallel and asynchronous optimization in each area while satisfying various constraints. Furthermore, a power-triggered algorithm is proposed to satisfy the power balance among areas, and its convergence is theoretically analyzed, which illustrates that the area power error will converge to zero in the limited iteration. Finally, the proposed approach is validated in three different cases: 20-unit, 40-unit, and 120-unit. The comparative results confirmed the superiority of the proposed HRL architecture with high real-time performance and less computation burden. Specifically, the HRL architecture reduces the fuel cost in the range of 0.01656%–0.19173% and accelerates the speed more than 10 times than others.}
}
@article{CATURANO2021102204,
title = {Discovering reflected cross-site scripting vulnerabilities using a multiobjective reinforcement learning environment},
journal = {Computers & Security},
volume = {103},
pages = {102204},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102204},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821000286},
author = {Francesco Caturano and Gaetano Perrone and Simon Pietro Romano},
keywords = {Penetration testing, Network security, Web application vulnerabilities, Multiobjective reinforcement learning, Intelligent agent, Cross-site scripting},
abstract = {Tools that automate testing of web applications for Cross-Site Scripting (XSS) vulnerabilities perform well when they have a strong knowledge base. Though, they heavily rely on brute force, which is not always an effective choice. On the other hand, expert penetration testers adopt exploit methods that are more accurate, but often not structured. We propose to solve the above mentioned problems, by designing and implementing an intelligent agent, called Suggester, that recommends actions to penetration testers. First, a black-box testing methodology inspired by a penetration tester’s behavior, is developed. Such methodology consists of sending a sequence of strings to a web application and observing the responses. Then, an agent is trained to produce attack strings using the framework of a Multiobjective Reinforcement Learning environment (MORL), with a parameterized action space. Each complete attack string is identified as a separate objective to reach. Q-Learning is used to train the agent upon separate, unrelated objectives. Then, the learned actions are suggested to a human-in-the-loop, who performs the actions and collects observations. This allows to orchestrate the agent into pursuing the right objective and selecting the next best action to recommend. The final evaluation proves the scalability of the proposed solution, as well as show an increase in accuracy when compared to other automated scanners.}
}
@article{HOU2023102198,
title = {Vehicle ride comfort optimization in the post-braking phase using residual reinforcement learning},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102198},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102198},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003269},
author = {Xiaohui Hou and Minggang Gan and Junzhi Zhang and Shiyue Zhao and Yuan Ji},
keywords = {Vehicle ride comfort, LuGre tire model, Grey wolf optimizer, Particle swarm optimization, Reinforcement learning, Nonlinear dynamics},
abstract = {Owing to increasing urban congestion, ensuring vehicle ride comfort during the post-braking phase has become an essential requirement. However, achieving vehicle ride comfort using current conventional methods is challenging due to the vehicles’ complex dynamics. This paper proposes a novel controller with residual reinforcement learning, combining the advantages of the model-free reinforcement learning algorithm, heuristic optimization algorithm, and prior expert knowledge to significantly improve training efficiency. The nonlinear and transient characteristics of the tire and vehicle are modeled to improve the control accuracy. On-vehicle experiments are performed using a skateboard chassis. The experimental results show that the proposed strategy achieves significant improvement in vehicle ride comfort under various braking scenarios. We believe that this technology has the potentialto alleviate vehicle discomfort issues in daily life.}
}
@article{YANG2023110905,
title = {Deep reinforcement learning for portfolio management},
journal = {Knowledge-Based Systems},
volume = {278},
pages = {110905},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110905},
url = {https://www.sciencedirect.com/science/article/pii/S095070512300655X},
author = {Shantian Yang},
keywords = {Reinforcement learning, Graph representation learning, Portfolio management, Mutual information, Attention mechanism},
abstract = {Portfolio management facilitates trading off risks against returns for multiple financial assets. Reinforcement Learning (RL) is one of the most promising algorithms for portfolio management. However, these state-of-the-art RL algorithms only complete the task of portfolio management, i.e., acquire the different asset features of portfolio, without considering the global context information from portfolio, which leads to non-optimal portfolio representations; Moreover, the corresponding optimizations are implemented using only the loss function in the viewpoint of RL, without considering the relationships between the local asset information and global context embeddings, which leads to non-optimal portfolio policies. To deal with these issues, this paper proposes a Task-Context Mutual Actor–Critic (TC-MAC) algorithm for portfolio management. Specifically, TC-MAC algorithm is developed based on: (1) representation learning introduces a proposed Task-Context (TC) learning algorithm, which not only encodes the task (i.e., acquire different asset features) of portfolio, but also encodes the global dynamic context of portfolio, thus which helps to learn optimal portfolio embeddings; (2) policy learning introduces a proposed Mutual Actor–Critic (MAC) framework, which can measure the relationships between local embedding of each asset and global context embeddings by maximizing mutual information, the corresponding Mutual-Information loss function combines with RL loss function (i.e., Actor–Critic loss) to collectively optimize the whole algorithm, thus which helps to learn optimal portfolio policies. Experimental results on real-world datasets demonstrate the superior performance of TC-MAC algorithm over the well-known traditional portfolio methods and these state-of-the-art RL algorithms, at the same time, show its advantageous transferability.}
}
@article{DONG2023119534,
title = {Accelerating wargaming reinforcement learning by dynamic multi-demonstrator ensemble},
journal = {Information Sciences},
volume = {648},
pages = {119534},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119534},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523011192},
author = {Liwei Dong and Ni Li and Haitao Yuan and Guanghong Gong},
keywords = {Reinforcement learning, Wargaming, Decision-making, Expert demonstrations},
abstract = {Deep Reinforcement Learning (DRL) has become a promising technique to deal with tough wargaming decision-making problems. However, DRL suffers an inherent problem of low learning efficiency and it often requires massive cost of training steps, which may be alleviated with expert demonstrations in wargaming domains. Most learning methods with demonstrations generally treat the demonstration data from different expert demonstrators without distinction. Besides, a more appropriate and effective mechanism is highly needed to control sampling balance of expert-generated demonstration samples and agent-generated interaction ones. To tackle the two issues, this work proposes an improved approach to leverage expert demonstrations to further accelerate DRL. It innovatively extracts inherent diversity in multiple demonstrators by pre-training agents individually from multiple demonstration sources, thereby producing a strong and initial ensemble model. In addition, a novel technique to evaluate the learning importance of each demonstrator is designed to dynamically tune sampling ratios of learning data in a more adaptive and effective manner. Through the evaluation on several classic game tasks and a typical wargaming scenario, our method shows superior performance over several state-of-the-art methods and significantly raises DRL’s efficiency for typical wargaming decision-making applications.}
}
@article{JANG2023119556,
title = {Deep reinforcement learning for stock portfolio optimization by connecting with modern portfolio theory},
journal = {Expert Systems with Applications},
volume = {218},
pages = {119556},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119556},
url = {https://www.sciencedirect.com/science/article/pii/S095741742300057X},
author = {Junkyu Jang and NohYoon Seong},
keywords = {Portfolio management, Modern portfolio theory, Deep reinforcement learning, Technical analysis, Tensor decomposition},
abstract = {With artificial intelligence and data quality development, portfolio optimization has improved rapidly. Traditionally, researchers in the financial market have utilized the modern portfolio theory for portfolio optimization; however, with the recent development of artificial intelligence, attempts to optimize portfolios with reinforcement learning are increasing. Many studies have developed reinforcement learning and deep learning algorithms and conducted portfolio optimization research. However, in reality, thus far, the securities industry thus has used the modern portfolio theory, which is sufficiently valuable. Nevertheless, to the best of our knowledge, there has yet to be an attempt to combine modern portfolio theory and reinforcement learning. To bridge this gap in the literature, we propose a novel deep reinforcement learning approach that combines the modern portfolio theory and a deep learning approach. As far as we know, we are the first to combine recent deep learning technology and traditional financial theory. Specifically, we solved the multimodal problem through the Tucker decomposition of a model with the input of technical analysis and stock return covariates. The results show that the proposed method outperforms state-of-the-art algorithms regarding the Sharpe ratio, annualized return, and maximum drawdown. In addition, the proposed method dynamically changes the weight according to the market trend, unlike other state-of-the-art algorithms.}
}
@article{MIN2023109983,
title = {Reinforcement learning based routing for time-aware shaper scheduling in time-sensitive networks},
journal = {Computer Networks},
volume = {235},
pages = {109983},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109983},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623004280},
author = {Junhong Min and Yongjun Kim and Moonbeom Kim and Jeongyeup Paek and Ramesh Govindan},
keywords = {Time-Aware Shaper (TAS), Time-Sensitive Network (TSN), Reinforcement learning, Routing, Scheduling, Network simulation, Network performance evaluation},
abstract = {To guarantee real-time performance and quality-of-service (QoS) of time-critical industrial systems, time-aware shaper (TAS) in time-sensitive networking (TSN) controls frame transmission times in a bridged network using a scheduled gate control mechanism. However, most TAS scheduling methods generate schedules based on pre-configured routes without exploring alternatives for better schedulability, and methods that jointly consider routing and scheduling require enormous runtime and computing resources. To address this problem, we propose a TSN Scheduler with Reinforcement Learning-based Routing (TSLR) that identifies improved load balanced routes for higher schedulability with acceptable complexity using distributional reinforcement learning. We evaluate TSLR through TSN simulations and compare it against state-of-the-art algorithms to demonstrate that TSLR effectively improves TAS schedulability and link utilization in TSN with lower complexity. Specifically, TSLR shows a more than 66% increase in schedulability compared to the other algorithms, and TSLR’s scheduling time is reduced by more than 1 h. It also shows flows’ transmission latency is less than 25% of their latency deadline requirement and reduces maximum link utilization by approximately 50%.}
}
@article{GUO2021103116,
title = {Traffic Engineering in Hybrid Software Defined Network via Reinforcement Learning},
journal = {Journal of Network and Computer Applications},
volume = {189},
pages = {103116},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103116},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521001363},
author = {Yingya Guo and Weipeng Wang and Han Zhang and Wenzhong Guo and Zhiliang Wang and Ying Tian and Xia Yin and Jianping Wu},
keywords = {Traffic engineering, Hybrid software defined network, Routing optimization, Reinforcement learning},
abstract = {The emergence of Software Defined Network (SDN) provides a centralized and flexible approach to route network flows. Due to the technical and economic challenges in upgrading to a fully SDN-enabled network, hybrid SDN, with a partial deployment of SDN switches in a traditional network, has been a prevailing network architecture. Meanwhile, Traffic Engineering (TE) in the hydbrid SDN has attracted wide attentions from academia and industry. Previous studies on TE in the hybrid SDN are either traffic-oblivious or time-consuming, which causes routing schemes failed in responding to the dynamically-changing traffic rapidly and intelligently. Therefore, in this paper, we propose a Reinforcement Learning (RL) based method, which learns a traffic-splitting agent to address the dynamically-changing traffic and achieve the link load balancing in the hybrid SDN. Specifically, to rapidly and intelligently determine a routing scheme to the new traffic demands, a traffic-splitting agent is designed and learnt offline by exploiting the RL algorithm to establish the direct relationship between traffic demands and traffic-splitting policies. Once the traffic-splitting agent is learnt, the effective traffic-splitting policies, which are used to determine the traffic-splitting ratios on SDN switches, can be generated rapidly. Additionally, to meet the interactive requirements for learning a traffic-splitting agent, a reasonable simulation environment is proposed to be constructed to avoid routing loops when traffic-splitting policies are taken. Extensive evaluations on different topologies and real traffic demands demonstrate that the proposed method achieves the comparable network performance and performs superiorities in rapidly generating the satisfying routing schemes.}
}
@article{BRANDI2020110225,
title = {Deep reinforcement learning to optimise indoor temperature control and heating energy consumption in buildings},
journal = {Energy and Buildings},
volume = {224},
pages = {110225},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.110225},
url = {https://www.sciencedirect.com/science/article/pii/S0378778820308963},
author = {Silvio Brandi and Marco Savino Piscitelli and Marco Martellacci and Alfonso Capozzoli},
keywords = {Deep reinforcement learning, Building adaptive control, Energy efficiency, Temperature control, HVAC},
abstract = {In this work, Deep Reinforcement Learning (DRL) is implemented to control the supply water temperature setpoint to terminal units of a heating system. The experiment was carried out for an office building in an integrated simulation environment. A sensitivity analysis is carried out on relevant hyperparameters to identify their optimal configuration. Moreover, two sets of input variables were considered for assessing their impact on the adaptability capabilities of the DRL controller. In this context a static and dynamic deployment of the DRL controller is performed. The trained control agent is tested for four different scenarios to determine its adaptability to the variation of forcing variables such as weather conditions, occupant presence patterns and different indoor temperature setpoint requirements. The performance of the agent is evaluated against a reference controller that implements a combination of rule-based and climatic-based logics. As a result, when the set of variables are adequately selected a heating energy saving ranging between 5 and 12% is obtained with an enhanced indoor temperature control with both static and dynamic deployment. Eventually the study proves that if the set of input variables are not carefully selected a dynamic deployment is strictly required for obtaining good performance.}
}
@article{HEUILLET2021106685,
title = {Explainability in deep reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {214},
pages = {106685},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106685},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120308145},
author = {Alexandre Heuillet and Fabien Couthouis and Natalia Díaz-Rodríguez},
keywords = {Reinforcement Learning, Explainable artificial intelligence, Machine Learning, Deep Learning, Responsible artificial intelligence, Representation learning},
abstract = {A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent’s behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainability. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems.}
}
@article{XU2021104791,
title = {Model-free reinforcement learning approach to optimal speed control of combustion engines in start-up mode},
journal = {Control Engineering Practice},
volume = {111},
pages = {104791},
year = {2021},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2021.104791},
url = {https://www.sciencedirect.com/science/article/pii/S096706612100068X},
author = {Zhenhui Xu and Linjun Pan and Tielong Shen},
keywords = {Reinforcement learning, Optimal control, Internal combustion engine, Continuous-time nonlinear systems},
abstract = {This paper presents a model-free reinforcement learning approach for optimal speed control of gasoline engines. First, the physics of the controlled internal combustion engines are discussed to show the uncertainty and the complexity in the model of the dynamics during start-up operation mode, which is the main motivation for challenging learning-based design. Then, a learning algorithm, particularly focused on the continuous time nonlinear dynamics, is constructed to avoid the use of the probing noise usually required in the existing learning algorithms. With the constructed learning algorithm, a learning-based control scheme is designed to solve the optimal speed control problem of a production gasoline engine. Finally, experiments are conducted on a full-scale test bench with a 4-cylinder gasoline engine used for the production of hybrid electric vehicles, and simulation and experimental validation are demonstrated.}
}
@article{LOU2022746,
title = {Offline reinforcement learning with representations for actions},
journal = {Information Sciences},
volume = {610},
pages = {746-758},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522009033},
author = {Xingzhou Lou and Qiyue Yin and Junge Zhang and Chao Yu and Zhaofeng He and Nengjie Cheng and Kaiqi Huang},
keywords = {Offline reinforcement learning, Action embedding},
abstract = {Prevailing offline reinforcement learning (RL) methods limit the policy within the area supported by the offline dataset to avoid the distributional shift problem. But potential high-reward actions, which are out of the distribution of the dataset, are neglected in these methods. To address such issue, we propose a new method, which generalizes from the offline dataset to out-of-distribution (OOD) actions. Specifically, we design a novel action embedding model to help infer the effect of actions. As a result, our value function reaches a better generalization over the action space, and further alleviate the distributional shift caused by overestimation of OOD actions. Theoretically, we give an information-theoretic explanation on the improvement of the value function’s generalization over the action space. Experiments on D4RL demonstrate that our model improves the performance compared to previous offline RL methods, especially when the experience in the offline dataset is good. We conduct further study and validate that the value function’s generalization on OOD actions is improved, which reinforces the effectiveness of our proposed action embedding model.}
}
@article{BELGACEM20222391,
title = {Intelligent multi-agent reinforcement learning model for resources allocation in cloud computing},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {6, Part A},
pages = {2391-2404},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822001008},
author = {Ali Belgacem and Saïd Mahmoudi and Maria Kihl},
keywords = {Cloud computing, Resource allocation, Multi-agent system, Q-learning, Energy consumption, Fault tolerance, Load balancing},
abstract = {Now more than ever, optimizing resource allocation in cloud computing is becoming more critical due to the growth of cloud computing consumers and meeting the computing demands of modern technology. Cloud infrastructures typically consist of heterogeneous servers, hosting multiple virtual machines with potentially different specifications, and volatile resource usage. This makes the resource allocation face many issues such as energy conservation, fault tolerance, workload balancing, etc. Finding a comprehensive solution that considers all these issues is one of the essential concerns of cloud service providers. This paper presents a new resource allocation model based on an intelligent multi-agent system and reinforcement learning method (IMARM). It combines the multi-agent characteristics and the Q-learning process to improve the performance of cloud resource allocation. IMARM uses the properties of multi-agent systems to dynamically allocate and release resources, thus responding well to changing consumer demands. Meanwhile, the reinforcement learning policy makes virtual machines move to the best state according to the current state environment. Also, we study the impact of IMARM on execution time. The experimental results showed that our proposed solution performs better than other comparable algorithms regarding energy consumption and fault tolerance, with reasonable load balancing and respectful execution time.}
}
@article{NYONGBASSEY2020116622,
title = {Reinforcement learning based adaptive power pinch analysis for energy management of stand-alone hybrid energy storage systems considering uncertainty},
journal = {Energy},
volume = {193},
pages = {116622},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2019.116622},
url = {https://www.sciencedirect.com/science/article/pii/S0360544219323175},
author = {Bassey Etim Nyong-Bassey and Damian Giaouris and Charalampos Patsios and Simira Papadopoulou and Athanasios I. Papadopoulos and Sara Walker and Spyros Voutetakis and Panos Seferlis and Shady Gadoue},
keywords = {Hybrid energy storage systems, Energy management strategies, Model predictive control, Kalman filter, Reinforcement learning},
abstract = {Hybrid energy storage systems (HESS) involve synergies between multiple energy storage technologies with complementary operating features aimed at enhancing the reliability of intermittent renewable energy sources (RES). Nevertheless, coordinating HESS through optimized energy management strategies (EMS) introduces complexity. The latter has been previously addressed by the authors through a systems-level graphical EMS via Power Pinch Analysis (PoPA). Although of proven efficiency, accounting for uncertainty with PoPA has been an issue, due to the assumption of a perfect day ahead (DA) generation and load profiles forecast. This paper proposes three adaptive PoPA-based EMS, aimed at negating load demand and RES stochastic variability. Each method has its own merits such as; reduced computational complexity and improved accuracy depending on the probability density function of uncertainty. The first and simplest adaptive scheme is based on a receding horizon model predictive control framework. The second employs a Kalman filter, whereas the third is based on a machine learning algorithm. The three methods are assessed on a real isolated HESS microgrid built in Greece. In validating the proposed methods against the DA PoPA, the proposed methods all performed better with regards to violation of the energy storage operating constraints and plummeting carbon emission footprint.}
}
@article{SHI201947,
title = {Concept learning through deep reinforcement learning with memory-augmented neural networks},
journal = {Neural Networks},
volume = {110},
pages = {47-54},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303137},
author = {Jing Shi and Jiaming Xu and Yiqun Yao and Bo Xu},
keywords = {One-shot learning, Memory, Attention, Deep reinforcement learning, Neural networks},
abstract = {Deep neural networks have shown superior performance in many regimes to remember familiar patterns with large amounts of data. However, the standard supervised deep learning paradigm is still limited when facing the need to learn new concepts efficiently from scarce data. In this paper, we present a memory-augmented neural network which is motivated by the process of human concept learning. The training procedure, imitating the concept formation course of human, learns how to distinguish samples from different classes and aggregate samples of the same kind. In order to better utilize the advantages originated from the human behavior, we propose a sequential process, during which the network should decide how to remember each sample at every step. In this sequential process, a stable and interactive memory serves as an important module. We validate our model in some typical one-shot learning tasks and also an exploratory outlier detection problem. In all the experiments, our model gets highly competitive to reach or outperform those strong baselines.}
}
@article{ZHAO2023103020,
title = {Energy efficient resource allocation method for 5G access network based on reinforcement learning algorithm},
journal = {Sustainable Energy Technologies and Assessments},
volume = {56},
pages = {103020},
year = {2023},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2023.103020},
url = {https://www.sciencedirect.com/science/article/pii/S2213138823000127},
author = {Shasha Zhao},
keywords = {5G Network, Edge Computing, Industrial Internet of Things, Intensive Learning, Learning Algorithm, Network Access Energy Saving, Resource Allocation},
abstract = {Edge computing and IIoT (Industrial Internet of Things) are two representative application scenarios in 5G (5th Generation) mobile communication technology network. Therefore, this paper studies the resource allocation methods for these two typical scenarios, and proposes an energy-efficient resource allocation method by using DRL algorithm, which enhances the network performance and reduces the network operation cost. Firstly, according to the characteristics of edge computing, taking the connection relationship between base stations, users and the transmission power allocated by base stations to users as decision variables, minimizing the overall energy efficiency as the goal, and taking the needs of mobile users as constraints, a resource allocation model for user quality of service (QoS) guarantee is designed. Secondly, simulation experiments are used to verify that the proposed method has faster training speed and convergence speed, ensures the quality-of-service requirements of mobile users, and realizes intelligent and energy-saving resource allocation. The approximate amounts of steps to convergence under four cases are 500, 350, 250, and 220, respectively, and the values of awards are 21.76, 21.09, 20.38, and 20.25, respectively. Thirdly, aiming at the IIoT environment scenario, this paper proposes a resource allocation method for 5G wide connection low delay services based on asynchronous dominant action evaluation algorithm (A3C). Firstly, according to the characteristics of IIoT environment, taking the connection relationship between base stations and users and the transmission power allocated by base stations to users as decision variables and maximizing energy efficiency while satisfying needs of each user as optimization goal, a resource allocation model for 5G wide connection low delay services is designed. On this basis, an energy-efficient resource allocation method according to A3C is proposed. The hierarchical aggregation clustering algorithm (HAC) is adopted to determine the connection relationship between the base station and users, and the A3C algorithm is adopted to allocate transmission power to users, so as to maximize the overall energy efficiency and ensure the needs of each user.}
}
@article{WANG2022101588,
title = {A context-aware sensing strategy with deep reinforcement learning for smart healthcare},
journal = {Pervasive and Mobile Computing},
volume = {83},
pages = {101588},
year = {2022},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2022.101588},
url = {https://www.sciencedirect.com/science/article/pii/S1574119222000347},
author = {Lili Wang and Siyao Xi and Yuwen Qian and Cheng Huang},
keywords = {Deep reinforcement learning, Health sensing, Context recognition, Sensing schedule},
abstract = {Health sensing system (HSS), offering a variety of health services, has attracted considerable research attention in the area of smart healthcare. However, continuous sensing inevitably brings dramatic energy consumption of mobile sensing devices. On the other hand, the reduction of sensing time duration causes excessive delay in sensing a user state change and the missing of critical physiologic signal. Thus, the trade-off between energy consumption and delay constitutes a primary challenge in the design of HSS. In this paper, we propose an adaptive sensing strategy to intelligently determine the trigger time for sensing physiological parameters at a HSS. Furthermore, human context recognition (HCR) is adopted to design context-aware sensing strategy, where the health condition, sensing requirements, and dependence on physiological data are considered simultaneously. To devise the sensing strategy, we first generate a dynamic observation model. Next, we propose a sort retention double-DQN based sensing strategy. In comparison to traditional double-DQN, the proposed approach can effectively enhance learning stability and sample efficiency. With SRD-DQN, we can obtain the optimized solution for the schedule of the successive window according to the current state. We implement blood pressure and heart rate monitoring simulations to evaluate the performance of the proposed sensing strategy. Simulation results reveal that the sensing strategy can effectively restrain energy consumption and delay, and SRD-DQN converges faster than traditional DQN.}
}
@article{HAN2023126913,
title = {Data-driven heat pump operation strategy using rainbow deep reinforcement learning for significant reduction of electricity cost},
journal = {Energy},
volume = {270},
pages = {126913},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.126913},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223003079},
author = {Gwangwoo Han and Hong-Jin Joo and Hee-Won Lim and Young-Sub An and Wang-Je Lee and Kyoung-Ho Lee},
keywords = {Deep reinforcement learning, Heat pump, Electricity cost, Rainbow deep Q network, Load demand, Renewable energy},
abstract = {The need for reducing carbon emissions and achieving “Net Zero” energy has made improving heat pumps’ (HPs) operational efficiency a crucial goal. However, current rule or model-based control strategies have limitations of inability to consider the entire heat production-storage-utilization cycle and inherent difficulties in achieving both high performance and generality. Here, we propose a model-free deep reinforcement learning (DRL)-based HP operation strategy that utilizes the Rainbow deep Q network algorithm to minimize electricity costs by considering thermal load demand, renewable generation, coefficient of performance (COP) of HPs, and state of charge (SOC) of thermal storage. We employ artificial neural networks to train for the regression of future load demands and COP, creating a data-driven and connectable environment with DRL. The Rainbow agent learns a creative strategy of limiting the maximum number of HP operations by increasing the SOC in advance to match future load demands. The performance of the Rainbow agent is evaluated against rule-based control in cases of future states, future uncertainty, and five-year long-term deployment. The proposed method reduces the year-round demand charge by 23.1% and the energy charge by 21.7%, resulting in a 22.2% reduction in the electricity cost.}
}
@article{LAN2023102122,
title = {Innovation design oriented functional knowledge integration framework based on reinforcement learning},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102122},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102122},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002501},
author = {Xiang Lan and Yahong Hu and Youbai Xie and Xianghui Meng and Yilun Zhang and Qiangang Pan and Yishen Ding},
keywords = {Functional knowledge integration, Computational design synthesis, Reinforcement learning, Knowledge graph},
abstract = {According to the basic law of Design Science, new product design is based on existing design knowledge. Knowledge integration can be applied to product function design to shorten design time and improve the design quality through effective use of the existing knowledge. With the increase of the product design complexity and the number of design knowledge, it is harder and harder for traditional traversal-based algorithms to complete knowledge integration under acceptable time cost. A Reinforcement Learning (RL) based functional knowledge integration framework is proposed. The functional knowledge is represented by its input and output, and organized using a knowledge graph. The Q-network is constructed and trained for the deep Monte Carlo method-based functional unit chain generation algorithm. The performance experiments show that comparing with the traditional searching algorithms, the RL based algorithm can provide same quality design scheme with much shorter time. The proposed algorithm is promising to realize real-time functional knowledge integration in large-scale knowledge bases.}
}
@article{AVILA2020106711,
title = {Deep reinforcement learning approach for MPPT control of partially shaded PV systems in Smart Grids},
journal = {Applied Soft Computing},
volume = {97},
pages = {106711},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106711},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620306499},
author = {Luis Avila and Mariano {De Paula} and Maximiliano Trimboli and Ignacio Carlucho},
keywords = {MPPT, Deep RL, PV systems, OpenAI Gym},
abstract = {Photovoltaic systems (PV) are having an increased importance in modern smart grids systems. Usually, in order to maximize the energy output of the PV arrays a maximum power point tracking (MPPT) algorithm is used. However, once deployed, weather conditions such as clouds can cause shades in the PV arrays affecting the dynamics of each panel differently. These conditions directly affect the available energy output of the arrays and in turn make the MPPT task extremely difficult. For these reasons, under partial shading conditions, it is necessary to have algorithms that are able to learn and adapt online to the changing state of the system. In this work we propose the use of deep reinforcement learning (DRL) techniques to address the MPPT problem of a PV array under partial shading conditions. We develop a model free RL algorithm to maximize the efficiency in MPPT control. The agent’s policy is parameterized by neural networks, which take the sensory information as input and directly output the control signal. Furthermore, a PV environment under shading conditions was developed in the open source OpenAI Gym platform and is made available in an open repository. Several tests are performed, using the developed simulated environment, to test the robustness of the proposed control strategies to different climate conditions. The obtained results show the feasibility of our proposal with a successful performance with fast responses and stable behaviors. The best results for the presented methodology show that the maximum operating power point achieved has a deviation less than 1% compared to the theoretical maximum power point.}
}
@article{FAN2021422,
title = {Reinforced knowledge distillation: Multi-class imbalanced classifier based on policy gradient reinforcement learning},
journal = {Neurocomputing},
volume = {463},
pages = {422-436},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.08.040},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221012248},
author = {Saite Fan and Xinmin Zhang and Zhihuan Song},
keywords = {Class imbalance learning, Multi-class imbalanced classification, Fine-grained architecture, Knowledge distillation, Policy gradient, Reinforcement learning},
abstract = {The real-world datasets often exhibit imbalanced class distribution, which is a common challenge for multi-class classification algorithms. To settle the multi-class imbalanced classification problem of class imbalance learning, a novel reinforced knowledge distillation method is proposed in this paper. In the reinforced knowledge distillation, an improved fine-grained classification architecture based on knowledge distillation strategy and policy gradient reinforcement learning is proposed. In addition, reinforced knowledge distillation uses a newly designed reward signal and a novel sample weights update strategy to train the policies to find the optimal student-network, which makes reinforced knowledge distillation more powerful in handling the multi-class imbalanced classification problem. The effectiveness and practicability of the proposed reinforced knowledge distillation method are verified through its application to a simulated industrial process benchmark and extensive real-world datasets.}
}
@article{KROHLING2021101229,
title = {A context-aware approach to automated negotiation using reinforcement learning},
journal = {Advanced Engineering Informatics},
volume = {47},
pages = {101229},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101229},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620301981},
author = {Dan E. Kröhling and Omar J.A. Chiotti and Ernesto C. Martínez},
keywords = {Agent intelligence, Automated negotiation, Context-aware agents, Peer-to-peer markets, Reinforcement learning},
abstract = {Agents negotiate depending on individual perceptions of facts, events, trends and special circumstances that define the negotiation context. The negotiation context affects in different ways each agent’s preferences, bargaining strategies and resulting benefits, given the possible negotiation outcomes. Despite the relevance of the context, the existing literature on automated negotiation is scarce about how to account for it in learning and adapting negotiation strategies. In this paper, a novel contextual representation of the negotiation setting is proposed, where an agent resorts to private and public data to negotiate using an individual perception of its necessity and risk. A context-aware negotiation agent that learns through Self-Play and Reinforcement Learning (RL) how to use key contextual information to gain a competitive edge over its opponents is discussed in two levels of temporal abstraction. Learning to negotiate in an Eco-Industrial Park (EIP) is presented as a case study. In the Peer-to-Peer (P2P) market of an EIP, two instances of context-aware agents, in the roles of a buyer and a seller, are set to bilaterally negotiate exchanges of electrical energy surpluses over a discrete timeline to demonstrate that they can profit from learning to choose a negotiation strategy while selfishly accounting for contextual information under different circumstances in a data-driven way. Furthermore, several negotiation episodes are conducted in the proposed EIP between a context-aware agent and other types of agents proposed in the existing literature. Results obtained highlight that context-aware agents do not only reap selfishly higher benefits, but also promote social welfare as they resort to contextual information while learning to negotiate.}
}
@incollection{OH202247,
title = {Q-MPC: Integration of Reinforcement Learning and Model Predictive Control for Safe Learning},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {47-55},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50007-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596500075},
author = {Tae Hoon Oh and Jong Min Lee},
keywords = {Reinforcement Learning, Model Predictive Control, Optimal Control, Safe Learning},
abstract = {Model-free reinforcement learning (RL) learns an optimal control policy by using the process data only. However, simple application of model-free RL to a practical process has a high risk of failure because the available amount of data and the number of trial runs are limited. Moreover, it is likely that state constraints are violated during the learning period. In this work, we propose Q-MPC framework, an integrated algorithm of RL and model predictive control (MPC) for safe learning. The Q-MPC learns the action-value function in an off7-policy fashion and solves a model-based optimal control problem where the trained action-value function is assigned as the terminal cost. Because the Q-MPC utilizes a model, the state constraints can be respected during the learning period. For simulation study, Q-MPC, MPC, and double deep Q-network (DDQN) were applied with varying prediction horizons. The results show the advantages of Q-MPC that outperforms MPC by reducing the model-plant mismatch and shows much fewer constraint violations than DDQN.}
}
@article{LEGUIZAMO2022193,
title = {Deep Reinforcement Learning for Robotic Control with Multi-Fidelity Models},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {37},
pages = {193-198},
year = {2022},
note = {2nd Modeling, Estimation and Control Conference MECC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.11.183},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322028269},
author = {David Felipe Leguizamo and Hsin-Jung Yang and Xian Yeow Lee and Soumik Sarkar},
keywords = {Robotic Systems, Self-learning Models, Real-time Artificial Intelligence, Reinforcement learning control, Engineering Applications of Artificial Intelligence, Optimization and Control, Multi-Fidelity Modeling},
abstract = {Deep reinforcement learning (DRL) can be used for the development of robotic controllers. Complicated kinematic relationships can be learned by a DRL agent, which will result in a control policy that takes actions based on an observed state. However, a DRL agent typically goes through much trial and error before beginning to take appropriate actions. Therefore, it is often useful to leverage simulated robotic manipulators before performing any training or testing on actual hardware. There are several options for such simulation, ranging from simple kinematic models to more complex models seeking to accurately simulate the effects of gravity, inertia, and friction. The latter models can provide excellent representations of a robotic plant, but typically with a noticeably increased computational expense. Reducing the expense of simulating the robotic plant (while still maintaining a reasonable degree of accuracy) can accelerate an already expensive DRL training loop. In this work, we present a methodology for using a lower-fidelity model (based on Denavit-Hartenberg parameters) to initialize the training of a DRL agent for control of a Sawyer robotic arm. We show that the trained DRL policy can then be fine-tuned in a higher-fidelity simulation provided by the robot's manufacturer. We demonstrate the accuracy of the fully trained policy by transferring it to the actual hardware, demonstrating the power of DRL to learn complicated robotic tasks entirely in simulation. Finally, we benchmark the time required to train a policy using each level of fidelity.}
}
@article{LUO2023126620,
title = {Relay Hindsight Experience Replay: Self-guided continual reinforcement learning for sequential object manipulation tasks with sparse rewards},
journal = {Neurocomputing},
volume = {557},
pages = {126620},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126620},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223007439},
author = {Yongle Luo and Yuxin Wang and Kun Dong and Qiang Zhang and Erkang Cheng and Zhiyong Sun and Bo Song},
keywords = {Deep reinforcement learning, Robotic manipulation, Continual learning, Hindsight experience replay, Sparse reward},
abstract = {Learning with sparse rewards remains a challenging problem in reinforcement learning (RL). In particular, for sequential object manipulation tasks, the RL agent generally only receives a reward upon successful completion of the entire task, leading to low exploration efficiency. To address this sample inefficiency, we propose a novel self-guided continual RL framework, named Relay Hindsight Experience Replay (RHER). RHER decomposes a sequential task into several subtasks with increasing complexity, allowing the agent to learn from the simplest subtask and gradually complete the task. It is crucial that a Self-Guided Exploration Strategy (SGES) is proposed to use the already-learned simpler subtask policy to guide the exploration of a more complex subtask. This strategy allows the agent to break the barriers of sparse reward sequential tasks and achieve efficient learning stage by stage. As a result, the proposed RHER method achieves state-of-the-art performance on the benchmark tasks (FetchPush and FetchPickAndPlace). Furthermore, the experimental results demonstrate the superiority and high efficiency of RHER on a variety of single-object and multi-object manipulation tasks (e.g., ObstaclePush, DrawerBox, TStack, etc.). Finally, the proposed RHER method can also learn a contact-rich task on a real robot from scratch within 250 episodes.}
}
@article{PUSHPANGATHAN202255,
title = {Deep Reinforcement Learning and Simultaneous Stabilization-Based Flight Controller for Nano Aerial Vehicle},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {22},
pages = {55-60},
year = {2022},
note = {22nd IFAC Symposium on Automatic Control in Aerospace ACA 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323002689},
author = {Jinraj V Pushpangathan and Harikumar Kandath and Bibin Francis},
keywords = {Flight controller, deep reinforcement learning, proximal policy optimization agent},
abstract = {The plants of nano aerial vehicles (NAVs) are inherently unstable. Hence, a NAV needs a flight controller to accomplish a mission. Furthermore, the sensing and computational capabilities of NAV's autopilot hardware are limited. Hence, the implementation of the full state feedback controllers with gain scheduling is difficult. This paper proposes a flight controller scheme that consists of two parts: a Simultaneously Stabilizing Output Feedback Linear (SSOFL) controller and a Proximal Policy Optimization (PPO) deep reinforcement learning agent, which is connected in parallel to the SSOFL controller. In this scheme, the single SSOFL controller provides stabilization and nominal tracking performance to the NAV throughout its flight envelope by accomplishing simultaneous stabilization (SS). Additionally, the PPO agent is trained using the closed-loop (CL) nonlinear plant with this SSOFL controller to enhance the tracking performance. The effectiveness of the proposed flight controller scheme is verified using the six-degree-of-freedom nonlinear simulations of the fixed-wing nano aerial vehicle.}
}
@article{JIANG2017226,
title = {H∞ control with constrained input for completely unknown nonlinear systems using data-driven reinforcement learning method},
journal = {Neurocomputing},
volume = {237},
pages = {226-234},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.11.041},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216314424},
author = {He Jiang and Huaguang Zhang and Yanhong Luo and Xiaohong Cui},
keywords = {Reinforcement learning, Adaptive dynamic programming, Data-driven, Neural networks},
abstract = {This paper investigates the H∞ control problem for nonlinear systems with completely unknown dynamics and constrained control input by utilizing a novel data-driven reinforcement learning method. It is known that nonlinear H∞ control problem relies on the solution of Hamilton-Jacobi-Isaacs (HJI) equation, which is essentially a nonlinear partial differential equation and generally impossible to be solved analytically. In order to overcome this difficulty, firstly, we propose a model-based simultaneous policy update algorithm to learn the solution of HJI equation iteratively and provide its convergence proof. Then, based on this model-based method, we develop a data-driven model-free algorithm, which only requires the real system sampling data generated by arbitrary different control inputs and external disturbances instead of accurate system models, and prove that these two algorithms are equivalent. To implement this model-free algorithm, three neural networks (NNs) are employed to approximate the iterative performance index function, control policy and disturbance policy, respectively, and the least-square approach is used to minimize the NN approximation residual errors. Finally, the proposed scheme is tested on the rotational/translational actuator nonlinear system.}
}
@article{DONG2021103192,
title = {Space-weighted information fusion using deep reinforcement learning: The context of tactical control of lane-changing autonomous vehicles and connectivity range assessment},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {128},
pages = {103192},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103192},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21002084},
author = {Jiqian Dong and Sikai Chen and Yujie Li and Runjia Du and Aaron Steinfeld and Samuel Labi},
keywords = {Connected autonomous vehicle, Deep reinforcement learning, Information fusion, Connectivity range, Lane changing decision, Safety and mobility},
abstract = {The connectivity aspect of connected autonomous vehicles (CAV) is beneficial because it facilitates dissemination of traffic-related information to vehicles through Vehicle-to-External (V2X) communication. Onboard sensing equipment including LiDAR and camera can reasonably characterize the traffic environment in the immediate locality of the CAV. However, their performance is limited by their sensor range (SR). On the other hand, longer-range information is helpful for characterizing imminent conditions downstream. By contemporaneously coalescing the short- and long-range information, the CAV can construct comprehensively its surrounding environment and thereby facilitate informed, safe, and effective movement planning in the short-term (local decisions including lane change) and long-term (route choice). Current literature provides useful information on CAV control approaches that use only local information sensed from the proximate traffic environment but relatively little guidance on how to fuse this information with that obtained from downstream sources and from different time stamps, and how to use the fused information to enhance CAV movements. In this paper, we describe a Deep Reinforcement Learning based approach that integrates the data collected through sensing and connectivity capabilities from other vehicles located in the proximity of the CAV and from those located further downstream, and we use the fused data to guide lane changing, a specific context of CAV operations. In addition, recognizing the importance of the connectivity range (CR) to the performance of not only the algorithm but also of the vehicle in the actual driving environment, the study carried out a case study. The case study demonstrates the application of the proposed algorithm and duly identifies the appropriate CR for each level of prevailing traffic density. It is expected that implementation of the algorithm in CAVs can enhance the safety and mobility associated with CAV driving operations. From a general perspective, its implementation can provide guidance to connectivity equipment manufacturers and CAV operators, regarding the default CR settings for CAVs or the recommended CR setting in a given traffic environment.}
}
@article{BU2019500,
title = {A smart agriculture IoT system based on deep reinforcement learning},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {500-507},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19307277},
author = {Fanyu Bu and Xin Wang},
keywords = {Deep reinforcement learning, Smart agriculture IoT, Edge computing, Cloud computing},
abstract = {Smart agriculture systems based on Internet of Things are the most promising to increase food production and reduce the consumption of resources like fresh water. In this study, we present a smart agriculture IoT system based on deep reinforcement learning which includes four layers, namely agricultural data collection layer, edge computing layer, agricultural data transmission layer, and cloud computing layer. The presented system integrates some advanced information techniques, especially artificial intelligence and cloud computing, with agricultural production to increase food production. Specially, the most advanced artificial intelligence model, deep reinforcement learning is combined in the cloud layer to make immediate smart decisions such as determining the amount of water needed to be irrigated for improving crop growth environment. We present several representative deep reinforcement learning models with their broad applications. Finally, we talk about the open challenges and the potential applications of deep reinforcement learning in smart agriculture IoT systems.}
}
@article{LI2023110613,
title = {MER: Modular Element Randomization for robust generalizable policy in deep reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {273},
pages = {110613},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110613},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123003635},
author = {Yihan Li and Jinsheng Ren and Tianren Zhang and Ying Fang and Feng Chen},
keywords = {Deep reinforcement learning, Modular Element Randomization, Generalizable policy, One-shot generalization},
abstract = {Improving the generalization ability of reinforcement learning (RL) agents is an open and challenging problem and has gradually received attention in recent years. Previous work attempts to partially solve this problem by recombining several learned policies to achieve compositional generalization. However, these methods are usually limited to tasks that are composed of fixed subtasks with no task-agnostic environment change. In this paper, we propose a more flexible method termed MER to learn a compositionally generalizable policy that depends on task-dependent invariant elements among tasks, instead of depending on fixed subtasks. As a result, our learned policy can overcome the task-agnostic change in the environment and generalize to more different compositional tasks. Theoretical analysis and experimental results show that our method outperforms traditional methods and exhibits superior generalization ability in unseen new tasks.}
}
@article{LIU2020166,
title = {Dynamic selective maintenance optimization for multi-state systems over a finite horizon: A deep reinforcement learning approach},
journal = {European Journal of Operational Research},
volume = {283},
number = {1},
pages = {166-181},
year = {2020},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2019.10.049},
url = {https://www.sciencedirect.com/science/article/pii/S0377221719309014},
author = {Yu Liu and Yiming Chen and Tao Jiang},
keywords = {Maintenance, Dynamic selective maintenance, Deep reinforcement learning, Imperfect maintenance, Multi-state system},
abstract = {Selective maintenance, which aims to choose a subset of feasible maintenance actions to be performed for a repairable system with limited maintenance resources, has been extensively studied over the past decade. Most of the reported works on selective maintenance have been dedicated to maximizing the success of a single future mission. Cases of multiple consecutive missions, which are oftentimes encountered in engineering practices, have been rarely investigated to date. In this paper, a new selective maintenance optimization for multi-state systems that can execute multiple consecutive missions over a finite horizon is developed. The selective maintenance strategy can be dynamically optimized to maximize the expected number of future mission successes whenever the states and effective ages of the components become known at the end of the last mission. The dynamic optimization problem, which accounts for imperfect maintenance, is formulated as a discrete-time finite-horizon Markov decision process with a mixed integer-discrete-continuous state space. Based on the framework of actor-critic algorithms, a customized deep reinforcement learning method is put forth to overcome the “curse of dimensionality” and mitigate the uncountable state space. In our proposed method, a postprocess is developed for the actor to search the optimal maintenance actions in a large-scale discrete action space, whereas the techniques of the experience replay and the target network are utilized to facilitate the agent training. The performance of the proposed method is examined by an illustrative example and an engineering example of a coal transportation system.}
}
@article{MARKOU20132149,
title = {Measuring reinforcement learning and motivation constructs in experimental animals: Relevance to the negative symptoms of schizophrenia},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {37},
number = {9, Part B},
pages = {2149-2165},
year = {2013},
note = {CNTRICS: Modeling psychosis related cognition in animal systems to enhance translational research + Life-Span Plasticity of Brain and Behavior: A Cognitive Neuroscience Perspective},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2013.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0149763413001978},
author = {Athina Markou and John D. Salamone and Timothy J. Bussey and Adam C. Mar and Daniela Brunner and Gary Gilmour and Peter Balsam},
keywords = {Reinforcement, Reward, Motivation, Learning, Cognition},
abstract = {The present review article summarizes and expands upon the discussions that were initiated during a meeting of the Cognitive Neuroscience Treatment Research to Improve Cognition in Schizophrenia (CNTRICS; http://cntrics.ucdavis.edu) meeting. A major goal of the CNTRICS meeting was to identify experimental procedures and measures that can be used in laboratory animals to assess psychological constructs that are related to the psychopathology of schizophrenia. The issues discussed in this review reflect the deliberations of the Motivation Working Group of the CNTRICS meeting, which included most of the authors of this article as well as additional participants. After receiving task nominations from the general research community, this working group was asked to identify experimental procedures in laboratory animals that can assess aspects of reinforcement learning and motivation that may be relevant for research on the negative symptoms of schizophrenia, as well as other disorders characterized by deficits in reinforcement learning and motivation. The tasks described here that assess reinforcement learning are the Autoshaping Task, Probabilistic Reward Learning Tasks, and the Response Bias Probabilistic Reward Task. The tasks described here that assess motivation are Outcome Devaluation and Contingency Degradation Tasks and Effort-Based Tasks. In addition to describing such methods and procedures, the present article provides a working vocabulary for research and theory in this field, as well as an industry perspective about how such tasks may be used in drug discovery. It is hoped that this review can aid investigators who are conducting research in this complex area, promote translational studies by highlighting shared research goals and fostering a common vocabulary across basic and clinical fields, and facilitate the development of medications for the treatment of symptoms mediated by reinforcement learning and motivational deficits.}
}
@article{BROWN2023106998,
title = {Deep reinforcement learning for the rapid on-demand design of mechanical metamaterials with targeted nonlinear deformation responses},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106998},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106998},
url = {https://www.sciencedirect.com/science/article/pii/S095219762301182X},
author = {Nathan K. Brown and Anthony P. Garland and Georges M. Fadel and Gang Li},
keywords = {Reinforcement learning, Metamaterials, Deep learning, Engineering design, Data-driven},
abstract = {Mechanical metamaterials are artificial materials with unique global properties due to the structural geometry and material composition of their unit cell. Typically, mechanical metamaterial unit cells are designed such that, when tessellated, they exhibit unique mechanical properties such as zero or negative Poisson's ratio and negative stiffness. Beyond these applications, mechanical metamaterials can be used to achieve tailorable nonlinear deformation responses. Computational methods such as gradient-based topology optimization (TO) and size/shape optimization (SSO) can be implemented to design these metamaterials. However, both methods can lead to suboptimal solutions or a lack of generalizability. Therefore, this research used deep reinforcement learning (DRL), a subset of deep machine learning that teaches an agent to complete tasks through interactive experiences, to design mechanical metamaterials with specific nonlinear deformation responses in compression or tension. The agent learned to design the unit cells by sequentially adding material to a discrete design domain and being rewarded for achieving the desired deformation response. After training, the agent successfully designed unit cells to exhibit desired deformation responses not experienced during training. This work shows the potential of DRL as a high-level design tool for a wide array of engineering applications.}
}
@article{LIU202318,
title = {Robustness challenges in Reinforcement Learning based time-critical cloud resource scheduling: A Meta-Learning based solution},
journal = {Future Generation Computer Systems},
volume = {146},
pages = {18-33},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.03.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001061},
author = {Hongyun Liu and Peng Chen and Xue Ouyang and Hui Gao and Bing Yan and Paola Grosso and Zhiming Zhao},
keywords = {Robustness, Reinforcement Learning, Meta Learning, Resource management, Task scheduling, Cloud computing},
abstract = {Cloud computing attracts increasing attention in processing dynamic computing tasks and automating the software development and operation pipeline. In many cases, the computing tasks have strict deadlines. The cloud resource manager (e.g., orchestrator) effectively manages the resources and provides tasks Quality of Service (QoS). Cloud task scheduling is tricky due to the dynamic nature of task workload and resource availability. Reinforcement Learning (RL) has attracted lots of research attention in scheduling. However, those RL-based approaches suffer from low scheduling performance robustness when the task workload and resource availability change, particularly when handling time-critical tasks. This paper focuses on both challenges of robustness and deadline guarantee among such RL, specifically Deep RL (DRL)-based scheduling approaches. We quantify the robustness measurements as the retraining time and investigate how to improve both robustness and deadline guarantee of DRL-based scheduling. We propose MLR-TC-DRLS, a practical, robust Meta Deep Reinforcement Learning-based scheduling solution to provide time-critical tasks deadline guarantee and fast adaptation under highly dynamic situations. We comprehensively evaluate MLR-TC-DRLS performance against RL-based and RL advanced variants-based scheduling approaches using real-world and synthetic data. The evaluations validate that our proposed approach improves the scheduling performance robustness of typical DRL variants scheduling approaches with 97%–98.5% deadline guarantees and 200%–500% faster adaptation.}
}
@article{WEI20234534,
title = {On adaptive attitude tracking control of spacecraft: A reinforcement learning based gain tuning way with guaranteed performance},
journal = {Advances in Space Research},
volume = {71},
number = {11},
pages = {4534-4548},
year = {2023},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2023.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S0273117723000406},
author = {Caisheng Wei and Yunwen Xiong and Qifeng Chen and Dan Xu},
keywords = {Attitude control, Reinforcement learning, Performance guaranteed control, Finite-time control},
abstract = {This paper investigates the attitude tracking control problem of a rigid spacecraft subject to inertial uncertainties, uncertain space perturbations and actuator saturation. Firstly, a static inertial-matrix free attitude controller is designed to guarantee the appointed-time convergence and tracking accuracy indicators of the spacecraft. Then, two-layer critic-action NNs are used to tune and optimize the control gains adaptively to improve the robustness and tracking performance of the devised static attitude controller via exploring the reinforcement learning (RL) technique. Compared with the existing attitude control methods, the prominent advantage of our work is that the devised controller is inertial-matrix free with a simple RL-based adaptive parameter tuning scheme, which reduces the conservativeness of the traditional attitude controllers with fixed control gains and is also easy to be achieved. Finally, two groups of illustrative examples are organized to validate the effectiveness of the proposed attitude control method.}
}
@article{LI2020106172,
title = {Deep reinforcement learning for robust emotional classification in facial expression recognition},
journal = {Knowledge-Based Systems},
volume = {204},
pages = {106172},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106172},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120304081},
author = {Huadong Li and Hua Xu},
keywords = {Emotion classification, Reinforcement learning, Image selector, Deep neural network},
abstract = {For emotion classification in facial expression recognition (FER), the performance of both traditional statistical methods and state-of-the-art deep learning methods are highly dependent on the quality of data. Traditional methods use image preprocessing (such as smoothing and segmentation) to improve image quality. However, the results still fail to meet the quality requirements of the emotion classifiers in FER. To address the above issues, this paper proposed a novel framework based on reinforcement learning for pre-selecting useful images(RLPS) for emotion classification in FER, which is made up of two modules: image selector and rough emotion classifier. Image selector is used to select useful images for emotion classification through reinforcement strategy and rough emotion classifier acts as a teacher to train image selector. Our framework improves classification performance by improving the quality of the dataset and can be applied to any classifier. Experiment results on RAF-DB, ExpW, and FER2013 datasets show that the proposed strategy achieves consistent improvements compared with the state-of-the-art emotion classification methods in FER.1 1The code will be available at https://github.com/lhd777/RLPS.}
}
@article{LERMEN2023,
title = {Reinforcement Learning system to capture value from Brazilian post-harvest offers},
journal = {Information Processing in Agriculture},
year = {2023},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2023.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214317323000641},
author = {Fernando Henrique Lermen and Vera Lúcia Milani Martins and Marcia Elisa Echeveste and Filipe Ribeiro and Carla Beatriz {da Luz Peralta} and José Luis Duarte Ribeiro},
keywords = {Product-service system, Agriculture, Value capture, Choice experiments, Reinforcement learning},
abstract = {This study assesses the value capture of a result-oriented Product-Service System offer that constitutes a post-harvest solution. Applying the reinforcement learning reward system and general linear models, we identified the Brazilian farmer's propensities to choose different products and services from the proposed system. Reinforcement learning enables one to understand the choice process by rewarding the attributes selected and applying penalties to those not chosen. Regarding product options, farmers' most valued attributes were extended capacity, fixed installation, automatic dryer, and CO2 emission control, considering the investigated system. Regarding service options, the farmers opted for maintenance plans, performance reports, no photovoltaic energy, and purchase over the rental modality. These results assist managers through a reward learning system that constantly updates the value assigned by farmers to product and service attributes. They allow real-time visualization of changes in farmers' preferences regarding the product-service system configurations.}
}