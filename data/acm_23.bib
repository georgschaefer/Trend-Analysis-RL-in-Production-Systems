@inproceedings{10.5555/3306127.3332074,
author = {Verma, Richa and Saikia, Sarmimala and Khadilkar, Harshad and Agarwal, Puneet and Shroff, Gautam and Srinivasan, Ashwin},
title = {A Reinforcement Learning Framework for Container Selection and Ship Load Sequencing in Ports},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We describe a reinforcement learning (RL) framework for selecting and sequencing containers to load onto ships in ports. The goal is to minimize an approximation of the number of crane movements require to load a given ship, known as the shuffle count. It can be viewed as a version of the assignment problem in which the sequence of assignment is of importance and the task rewards are order dependent. The proposed methodology is developed specifically to be usable on ship and yard layouts of arbitrary scale, by dividing the full problem into fixed future horizon segments and through a redefinition of the action space into a binary choice framework. Using data from real-world yard and ship layouts, we show that our approach solves the single crane version of the loading problem for entire ships with better objective values than those computed using standard metaheuristics.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2250–2252},
numpages = {3},
keywords = {reinforcement learning, single agent planning \&amp; scheduling},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3511616.3513104,
author = {Rajapakshe, Thejan and Rana, Rajib and Khalifa, Sara and Liu, Jiajun and Schuller, Bjorn},
title = {A Novel Policy for Pre-Trained Deep Reinforcement Learning for Speech Emotion Recognition},
year = {2022},
isbn = {9781450396066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511616.3513104},
doi = {10.1145/3511616.3513104},
abstract = {Deep Reinforcement Learning (deep RL) has gained tremendous success in gaming but it has rarely been explored for Speech Emotion Recognition (SER). In the RL literature, policy used by the RL agent plays a major role in action selection, however, there is no RL policy tailored for SER. Also, an extended learning period is a general challenge for deep RL, which can impact the speed of learning for SER. In this paper, we introduce a novel policy, the “Zeta policy” tailored for SER and introduce pre-training in deep RL to achieve a faster learning rate. Pre-training with a cross dataset was also studied to discover the feasibility of pre-training the RL agent with a similar dataset in a scenario where real environmental data is not available. We use “IEMOCAP” and “SAVEE” datasets for the evaluation with the problem of recognising four emotions, namely happy, sad, angry, and neutral. The experimental results show that the proposed policy performs better than existing policies. Results also support that pre-training can reduce training time and is robust to a cross-corpus scenario.},
booktitle = {Proceedings of the 2022 Australasian Computer Science Week},
pages = {96–105},
numpages = {10},
location = {Brisbane, Australia},
series = {ACSW '22}
}

@inproceedings{10.1145/3534678.3539480,
author = {Wang, Minrui and Feng, Mingxiao and Zhou, Wengang and Li, Houqiang},
title = {Stabilizing Voltage in Power Distribution Networks via Multi-Agent Reinforcement Learning with Transformer},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539480},
doi = {10.1145/3534678.3539480},
abstract = {The increased integration of renewable energy poses a slew of technical challenges for the operation of power distribution networks. Among them, voltage fluctuations caused by the instability of renewable energy are receiving increasing attention. Utilizing MARL algorithms to coordinate multiple control units in the grid, which is able to handle rapid changes of power systems, has been widely studied in active voltage control task recently. However, existing approaches based on MARL ignore the unique nature of the grid and achieve limited performance. In this paper, we introduce the transformer architecture to extract representations adapting to power network problems and propose a Transformer-based Multi-Agent Actor-Critic framework (T-MAAC) to stabilize voltage in power distribution networks. In addition, we adopt a novel auxiliary-task training process tailored to the voltage control task, which improves the sample efficiency and facilitating the representation learning of the transformer-based model. We couple T-MAAC with different multi-agent actor-critic algorithms, and the consistent improvements on the active voltage control task demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1899–1909},
numpages = {11},
keywords = {transformer, active voltage control, multi-agent reinforcement learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3349341.3349412,
author = {Shang, Tongfei and Ma, Jianfeng and Han, Kun and Yu, Yuan},
title = {Research on Game Problem Under Incomplete Information Condition Based on Deep Reinforcement Learning},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349412},
doi = {10.1145/3349341.3349412},
abstract = {Deep reinforcement learning has been widely used in the military field and applied to many fields, especially in the areas of identification, recommendation, decision-making and so on. This paper analyzes the prediction error of game confrontation by deep reinforcement learning under the condition of incomplete information, and proves the effectiveness of deep reinforcement learning for machine game.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {255–257},
numpages = {3},
keywords = {Game, Incomplete information, Deep reinforcement learning},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.5555/3306127.3331924,
author = {Barat, Souvik and Khadilkar, Harshad and Meisheri, Hardik and Kulkarni, Vinay and Baniwal, Vinita and Kumar, Prashant and Gajrani, Monika},
title = {Actor Based Simulation for Closed Loop Control of Supply Chain Using Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reinforcement Learning (RL) has achieved a degree of success in control applications such as online gameplay and robotics, but has rarely been used to manage operations of business-critical systems such as supply chains. A key aspect of using RL in the real world is to train the agent before deployment, so as to minimise experimentation in live operation. While this is feasible for online gameplay (where the rules of the game are known) and robotics (where the dynamics are predictable), it is much more difficult for complex systems due to associated complexities, such as uncertainty, adaptability and emergent behaviour. In this paper, we describe a framework for effective integration of a reinforcement learning controller with an actor-based simulation of the complex networked system, in order to enable deployment of the RL agent in the real system with minimal further tuning.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1802–1804},
numpages = {3},
keywords = {reinforcement learning, model based simulation, simulation of complex systems},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.1145/3560265,
author = {Jiang, Jielin and Guo, Jiajie and Khan, Maqbool and Cui, Yan and Lin, Wenmin},
title = {Energy-Saving Service Offloading for the Internet of Medical Things Using Deep Reinforcement Learning},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3560265},
doi = {10.1145/3560265},
abstract = {As a critical branch of the Internet of Things (IoT) in the medicine industry, the Internet of Medical Things (IoMT) significantly improves the quality of healthcare due to its real-time monitoring and low medical cost. Benefiting from edge and cloud computing, IoMT is provided with more computing and storage resources near the terminal to meet the low-delay requirements of computation-intensive services. However, the service offloading from health monitoring units (HMUs) to edge servers generates additional energy consumption. Fortunately, artificial intelligence (AI), which has developed rapidly in recent years, has proved effective in some resource allocation applications. Taking both energy consumption and delay into account, we propose an energy-aware service offloading algorithm under an end-edge-cloud collaborative IoMT system with Asynchronous Advantage Actor-critic (A3C), named ECAC. Technically, ECAC uses the structural similarity between the natural distributed IoMT system and A3C, whose parameters are asynchronously updated. Besides, due to the typical delay-sensitivity mechanism and time-energy correction, ECAC can adjust dynamically to the diverse service types and system requirements. Finally, the effectiveness of ECAC for IoMT is proved on real data.},
journal = {ACM Trans. Sen. Netw.},
month = {mar},
articleno = {55},
numpages = {20},
keywords = {internet of medical things, deep reinforcement learning, asynchronous advantage actor-critic, Service offloading}
}

@inproceedings{10.1145/3299869.3300085,
author = {Zhang, Ji and Liu, Yu and Zhou, Ke and Li, Guoliang and Xiao, Zhili and Cheng, Bin and Xing, Jiashu and Wang, Yangtao and Cheng, Tianheng and Liu, Li and Ran, Minwei and Li, Zekang},
title = {An End-to-End Automatic Cloud Database Tuning System Using Deep Reinforcement Learning},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3300085},
doi = {10.1145/3299869.3300085},
abstract = {Configuration tuning is vital to optimize the performance of database management system (DBMS). It becomes more tedious and urgent for cloud databases (CDB) due to the diverse database instances and query workloads, which make the database administrator (DBA) incompetent. Although there are some studies on automatic DBMS configuration tuning, they have several limitations. Firstly, they adopt a pipelined learning model but cannot optimize the overall performance in an end-to-end manner. Secondly, they rely on large-scale high-quality training samples which are hard to obtain. Thirdly, there are a large number of knobs that are in continuous space and have unseen dependencies, and they cannot recommend reasonable configurations in such high-dimensional continuous space. Lastly, in cloud environment, they can hardly cope with the changes of hardware configurations and workloads, and have poor adaptability. To address these challenges, we design an end-to-end automatic CDB tuning system, CDBTune, using deep reinforcement learning (RL). CDBTune utilizes the deep deterministic policy gradient method to find the optimal configurations in high-dimensional continuous space. CDBTune adopts a try-and-error strategy to learn knob settings with a limited number of samples to accomplish the initial training, which alleviates the difficulty of collecting massive high-quality samples. CDBTune adopts the reward-feedback mechanism in RL instead of traditional regression, which enables end-to-end learning and accelerates the convergence speed of our model and improves efficiency of online tuning. We conducted extensive experiments under 6 different workloads on real cloud databases to demonstrate the superiority of CDBTune. Experimental results showed that CDBTune had a good adaptability and significantly outperformed the state-of-the-art tuning tools and DBA experts.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {415–432},
numpages = {18},
keywords = {storage, cloud, database tuning},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.1145/3388176.3388187,
author = {Trinh, Thanh-Trung and Vu, Dinh-Minh and Kimura, Masaomi},
title = {A Pedestrian Path-Planning Model in Accordance with Obstacle's Danger with Reinforcement Learning},
year = {2020},
isbn = {9781450377256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388176.3388187},
doi = {10.1145/3388176.3388187},
abstract = {Most microscopic pedestrian navigation models use the concept of "forces" applied to the pedestrian agents to replicate the navigation environment. While the approach could provide believable results in regular situations, it does not always resemble natural pedestrian navigation behaviour in many typical settings. In our research, we proposed a novel approach using reinforcement learning for simulation of pedestrian agent path planning and collision avoidance problem. The primary focus of this approach is using human perception of the environment and danger awareness of interferences. The implementation of our model has shown that the path planned by the agent shares many similarities with a human pedestrian in several aspects such as following common walking conventions and human behaviours.},
booktitle = {Proceedings of the 3rd International Conference on Information Science and Systems},
pages = {115–120},
numpages = {6},
keywords = {navigation, Pedestrian, PPO, reinforcement learning, path planning},
location = {Cambridge, United Kingdom},
series = {ICISS '20}
}

@inproceedings{10.1145/3534678.3539195,
author = {Feng, Tao and Xia, Tong and Fan, Xiaochen and Wang, Huandong and Zong, Zefang and Li, Yong},
title = {Precise Mobility Intervention for Epidemic Control Using Unobservable Information via Deep Reinforcement Learning},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539195},
doi = {10.1145/3534678.3539195},
abstract = {To control the outbreak of COVID-19, efficient individual mobility intervention for EPidemic Control (EPC) strategies are of great importance, which cut off the contact among people at epidemic risks and reduce infections by intervening the mobility of individuals. Reinforcement Learning (RL) is powerful for decision making, however, there are two major challenges in developing an RL-based EPC strategy: (1) the unobservable information about asymptomatic infections in the incubation period makes it difficult for RL's decision-making, and (2) the delayed rewards for RL causes the deficiency of RL learning. Since the results of EPC are reflected in both daily infections (including unobservable asymptomatic infections) and long-term cumulative cases of COVID-19, it is quite daunting to design an RL model for precise mobility intervention. In this paper, we propose a Variational hiErarcHICal reinforcement Learning method for Epidemic control via individual-level mobility intervention, namely Vehicle. To tackle the above challenges, Vehicle first exploits an information rebuilding module that consists of a contact-risk bipartite graph neural network and a variational LSTM to restore the unobservable information. The contact-risk bipartite graph neural network estimates the possibility of an individual being an asymptomatic infection and the risk of this individual spreading the epidemic, as the current state of RL. Then, the Variational LSTM further encodes the state sequence to model the latency of epidemic spreading caused by unobservable asymptomatic infections. Finally, a Hierarchical Reinforcement Learning framework is employed to train Vehicle, which contains dual-level agents to solve the delayed reward problem. Extensive experimental results demonstrate that Vehicle can effectively control the spread of the epidemic. Vehicle outperforms the state-of-the-art baseline methods with remarkably high-precision mobility interventions on both symptomatic and asymptomatic infections.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2882–2892},
numpages = {11},
keywords = {hierarchical reinforcement learning, contact-risk bipartite graph neural network, variational lstm, epidemic control strategy},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3539618.3592018,
author = {Shi, Xiaowen and Wang, Ze and Cai, Yuanying and Wu, Xiaoxu and Yang, Fan and Liao, Guogang and Wang, Yongkang and Wang, Xingxing and Wang, Dong},
title = {MDDL: A Framework for Reinforcement Learning-Based Position Allocation in Multi-Channel Feed},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3592018},
doi = {10.1145/3539618.3592018},
abstract = {Nowadays, the mainstream approach in position allocation system is to utilize a reinforcement learning model to allocate appropriate locations for items in various channels and then mix them into the feed. There are two types of data employed to train reinforcement learning (RL) model for position allocation, named strategy data and random data. Strategy data is collected from the current online model, it suffers from an imbalanced distribution of stateaction pairs, resulting in severe overestimation problems during training. On the other hand, random data offers a more uniform distribution of state-action pairs, but is challenging to obtain in industrial scenarios as it could negatively impact platform revenue and user experience due to random exploration. As the two types of data have different distributions, designing an effective strategy to leverage both types of data to enhance the efficacy of the RL model training has become a highly challenging problem. In this study, we propose a framework namedMulti-Distribution Data Learning (MDDL) to address the challenge of effectively utilizing both strategy and random data for training RL models on mixed multi-distribution data. Specifically, MDDL incorporates a novel imitation learning signal to mitigate overestimation problems in strategy data and maximizes the RL signal for random data to facilitate effective learning. In our experiments, we evaluated the proposed MDDL framework in a real-world position allocation system and demonstrated its superior performance compared to the previous baseline. MDDL has been fully deployed on the Meituan food delivery platform and currently serves over 300 million users.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2159–2163},
numpages = {5},
keywords = {reinforcement learning, position allocation, multi-distribution data learning},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.5555/3535850.3536123,
author = {Delcourt, Kevin},
title = {Towards Multi-Agent Interactive Reinforcement Learning for Opportunistic Software Composition in Ambient Environments},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In order to manage the ever-growing number of devices present in modern and future ambient environments, as well as their dynamics and openness, we aim to propose a distributed multi-agent system that learns, in interaction with a human user, what would be their preferred applications given the services available.The goal of this Ph.D. thesis is to focus on the interaction between a reinforcement learning system and the human user, to improve the system's learning capabilities as well as the user's ease with the system, and ultimately build a working prototype, usable by end-users.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1839–1840},
numpages = {2},
keywords = {human-in-the-loop, emergence, ambient intelligence, multi-agent system, human-AI interaction, machine learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3447548.3467070,
author = {Wang, Hao and Liu, Chi Harold and Dai, Zipeng and Tang, Jian and Wang, Guoren},
title = {Energy-Efficient 3D Vehicular Crowdsourcing for Disaster Response by Distributed Deep Reinforcement Learning},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467070},
doi = {10.1145/3447548.3467070},
abstract = {Fast and efficient access to environmental and life data is key to the successful disaster response. Vehicular crowdsourcing (VC) by a group of unmanned vehicles (UVs) like drones and unmanned ground vehicles to collect these data from Point-of-Interests (PoIs) e.g., possible survivor spots and fire site, provides an efficient way to assist disaster rescue. In this paper, we explicitly consider to navigate a group of UVs in a 3-dimensional (3D) disaster workzone to maximize the amount of collected data, geographical fairness, energy efficiency, while minimizing data dropout due to limited transmission rate. We propose DRL-DisasterVC(3D), a distributed deep reinforcement learning framework, with a repetitive experience replay (RER) to improve learning efficiency, and a clipped target network to increase learning stability. We also use a 3D convolutional neural network (3D CNN) with multi-head-relational attention (MHRA) for spatial modeling, and add auxiliary pixel control (PC) for spatial exploration. We designed a novel disaster response simulator, called "DisasterSim", and conduct extensive experiments to show that DRL-DisasterVC(3D) outperforms all five baselines in terms of energy efficiency when varying the numbers of UVs, PoIs and SNR threshold.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {3679–3687},
numpages = {9},
keywords = {energy-efficiency, distributed deep reinforcement learning, disaster response, vehicular crowdsourcing},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/2593069.2593199,
author = {Das, Anup and Shafik, Rishad A. and Merrett, Geoff V. and Al-Hashimi, Bashir M. and Kumar, Akash and Veeravalli, Bharadwaj},
title = {Reinforcement Learning-Based Inter- and Intra-Application Thermal Optimization for Lifetime Improvement of Multicore Systems},
year = {2014},
isbn = {9781450327305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593069.2593199},
doi = {10.1145/2593069.2593199},
abstract = {The thermal profile of multicore systems vary both within an application's execution (intra) and also when the system switches from one application to another (inter). In this paper, we propose an adaptive thermal management approach to improve the lifetime reliability of multicore systems by considering both inter- and intra-application thermal variations. Fundamental to this approach is a reinforcement learning algorithm, which learns the relationship between the mapping of threads to cores, the frequency of a core and its temperature (sampled from on-board thermal sensors). Action is provided by overriding the operating system's mapping decisions using affinity masks and dynamically changing CPU frequency using in-kernel governors. Lifetime improvement is achieved by controlling not only the peak and average temperatures but also thermal cycling, which is an emerging wear-out concern in modern systems. The proposed approach is validated experimentally using an Intel quad-core platform executing a diverse set of multimedia benchmarks. Results demonstrate that the proposed approach minimizes average temperature, peak temperature and thermal cycling, improving the mean-time-to-failure (MTTF) by an average of 2x for intra-application and 3x for inter-application scenarios when compared to existing thermal management techniques. Furthermore, the dynamic and static energy consumption are also reduced by an average 10\% and 11\% respectively.},
booktitle = {Proceedings of the 51st Annual Design Automation Conference},
pages = {1–6},
numpages = {6},
location = {San Francisco, CA, USA},
series = {DAC '14}
}

@inproceedings{10.1145/3603781.3603901,
author = {Chen, Tan and Ai, Jiahao and Xiong, Xin and Tan, Fuxing},
title = {Concurrent Multipath Transmission for Ultra-Reliable and Low Latency with Deep Reinforcement Learning},
year = {2023},
isbn = {9798400700705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603781.3603901},
doi = {10.1145/3603781.3603901},
abstract = {Concurrent multipath transmission combining FEC is an effective approach to achieve ultra-reliable low latency communication in modern network. Rate allocation algorithm which distributing packets over multiple parallel paths plays a critical role in this architecture. However, traditional heuristic algorithm becomes less applicable due to great computational burdens and increasing demands for real-time. In this paper, we propose a deep reinforcement learning based algorithm NeuroCMT to solve this problem. Moreover, in order to deal with large discrete action space problem in NeuroCMT, we introduce a method with low complexity and high generalization ability. Furthermore, we implement prioritized experience replay technique in DDPG to attain better sample efficiency and enhanced convergence speed utilizing prioritized sampling. Extensive experiments are performed and the evaluation results demonstrate clearly that our approach achieves high reliability as well as low latency effectively.},
booktitle = {Proceedings of the 2023 4th International Conference on Computing, Networks and Internet of Things},
pages = {678–683},
numpages = {6},
keywords = {ultra-reliable low latency communication, deep reinforcement learning, rate allocation, concurrent multipath transmission},
location = {Xiamen, China},
series = {CNIOT '23}
}

@inproceedings{10.1145/3520304.3528897,
author = {Custode, Leonardo Lucio and Iacca, Giovanni},
title = {Interpretable Pipelines with Evolutionary Optimized Modules for Reinforcement Learning Tasks with Visual Inputs},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3528897},
doi = {10.1145/3520304.3528897},
abstract = {The importance of explainability in AI has become a pressing concern, for which several explainable AI (XAI) approaches have been recently proposed. However, most of the available XAI techniques are post-hoc methods, which however may be only partially reliable, as they do not reflect exactly the state of the original models. Thus, a more direct way for achieving XAI is through interpretable (also called glass-box) models. These models have been shown to obtain comparable (and, in some cases, better) performance with respect to black-boxes models in various tasks such as classification and reinforcement learning. However, they struggle when working with raw data, especially when the input dimensionality increases and the raw inputs alone do not give valuable insights on the decision-making process. Here, we propose to use end-to-end pipelines composed of multiple interpretable models co-optimized by means of evolutionary algorithms, that allows us to decompose the decision-making process into two parts: computing high-level features from raw data, and reasoning on the extracted high-level features. We test our approach in reinforcement learning environments from the Atari benchmark, where we obtain comparable results (with respect to black-box approaches) in settings without stochastic frame-skipping, while performance degrades in frame-skipping settings.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {224–227},
numpages = {4},
keywords = {interpretability, co-evolution, atari, reinforcement learning},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1145/3539618.3591671,
author = {Zheng, Shangfei and Yin, Hongzhi and Chen, Tong and Nguyen, Quoc Viet Hung and Chen, Wei and Zhao, Lei},
title = {DREAM: Adaptive Reinforcement Learning Based on Attention Mechanism for Temporal Knowledge Graph Reasoning},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591671},
doi = {10.1145/3539618.3591671},
abstract = {Temporal knowledge graphs (TKGs) model the temporal evolution of events and have recently attracted increasing attention. Since TKGs are intrinsically incomplete, it is necessary to reason out missing elements. Although existing TKG reasoning methods have the ability to predict missing future events, they fail to generate explicit reasoning paths and lack explainability. As reinforcement learning (RL) for multi-hop reasoning on traditional knowledge graphs starts showing superior explainability and performance in recent advances, it has opened up opportunities for exploring RL techniques on TKG reasoning. However, the performance of RL-based TKG reasoning methods is limited due to: (1) lack of ability to capture temporal evolution and semantic dependence jointly; (2) excessive reliance on manually designed rewards. To overcome these challenges, we propose an adaptive reinforcement learning model based on attention mechanism (DREAM) to predict missing elements in the future. Specifically, the model contains two components: (1) a multi-faceted attention representation learning method that captures semantic dependence and temporal evolution jointly; (2) an adaptive RL framework that conducts multi-hop reasoning by adaptively learning the reward functions. Experimental results demonstrate DREAM outperforms state-of-the-art models on public datasets.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1578–1588},
numpages = {11},
keywords = {link prediction, multi-hop knowledge reasoning, temporal knowledge graph},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.5555/3545946.3598725,
author = {Park, Junyoung and Kwon, Changhyun and Park, Jinkyoo},
title = {Learn to Solve the Min-Max Multiple Traveling Salesmen Problem with Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We propose ScheduleNet, a scalable scheduler that minimizes task completion time by coordinating multiple agents. We formulate the min-max Multiple Traveling Salesmen Problem (mTSP) as a Markov decision process with an episodic reward and derive a scalable decision-making policy using Reinforcement Learning (RL). The decision-making procedure of ScheduleNet includes (1) representing the state of a problem with the agent-task graph, (2) extracting node embedding for agents and tasks by employing the type-aware graph attention, (3) and computing the task assignment probability with the computed node embedding. We show that ScheduleNet can outperform other heuristic approaches and existing deep RL approaches, particularly validating its exceptional effectiveness in solving large and practical problems. We also confirm that ScheduleNet can effectively solve practical mTSP variants, which include limited observation and online mTSP.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {878–886},
numpages = {9},
keywords = {routing, graph neural network, scheduling, minmax multiple traveling salesmen problem (mTSP), reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3545946.3598825,
author = {Gallici, Matteo and Martin, Mario and Masmitja, Ivan},
title = {TransfQMix: Transformers for Leveraging the Graph Structure of Multi-Agent Reinforcement Learning Problems},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Coordination is one of the most difficult aspects of multi-agent reinforcement learning (MARL). One reason is that agents normally choose their actions independently of one another. In order to see coordination strategies emerging from the combination of independent policies, the recent research has focused on the use of a centralized function (CF) that learns each agent's contribution to the team reward. However, the structure in which the environment is presented to the agents and to the CF is typically overlooked. We have observed that the features used to describe the coordination problem can be represented as vertex features of a latent graph structure. Here, we present TransfQMix, a new approach that uses transformers to leverage this latent structure and learn better coordination policies. Our transformer agents perform a graph reasoning over the state of the observable entities. Our transformer Q-mixer learns a monotonic mixing-function from a larger graph that includes the internal and external states of the agents. TransfQMix is designed to be entirely transferable, meaning that same parameters can be used to control and train larger or smaller teams of agents. This enables to deploy promising approaches to save training time and derive general policies in MARL, such as transfer learning, zero-shot transfer, and curriculum learning. We report TransfQMix's performances in the Spread and StarCraft II environments. In both settings, it outperforms state-of-the-art Q-Learning models, and it demonstrates effectiveness in solving problems that other methods can not solve.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1679–1687},
numpages = {9},
keywords = {transfer learning, transformers, coordination graphs, multi-agent reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3486611.3492391,
author = {Jang, Doseok and Spangher, Lucas and Khattar, Manan and Agwan, Utkarsha and Nadarajah, Selvaprabu and Spanos, Costas},
title = {Offline-Online Reinforcement Learning for Generalizing Demand Response Price-Setting to Energy Systems},
year = {2021},
isbn = {9781450391146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486611.3492391},
doi = {10.1145/3486611.3492391},
abstract = {Our team is proposing to run a full-scale energy demand response experiment in an office building. Although this is an exciting endeavor, collecting training data for the reinforcement learning agent is costly and will be limited. In this work, we examine how offline training can be leveraged to minimize data costs (accelerate convergence) and program implementation costs by pretraining our model to warm start the experiment with simulated tasks. We present results that demonstrate the utility of offline reinforcement learning to efficient price-setting in the energy demand response problem.},
booktitle = {Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {220–221},
numpages = {2},
location = {Coimbra, Portugal},
series = {BuildSys '21}
}

@article{10.5555/3586589.3586734,
author = {Patterson, Andrew and White, Adam and White, Martha},
title = {A Generalized Projected Bellman Error for Off-Policy Value Estimation in Reinforcement Learning},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {Many reinforcement learning algorithms rely on value estimation, however, the most widely used algorithms--namely temporal difference algorithms--can diverge under both off-policy sampling and nonlinear function approximation. Many algorithms have been developed for off-policy value estimation based on the linear mean squared projected Bellman error (PBE) and are sound under linear function approximation. Extending these methods to the nonlinear case has been largely unsuccessful. Recently, several methods have been introduced that approximate a different objective--the mean-squared Bellman error (BE)-- which naturally facilitate nonlinear approximation. In this work, we build on these insights and introduce a new generalized PBE that extends the linear PBE to the nonlinear setting. We show how this generalized objective unifies previous work and obtain new bounds for the value error of the solutions of the generalized objective. We derive an easy-to-use, but sound, algorithm to minimize the generalized objective, and show that it is more stable across runs, is less sensitive to hyperparameters, and performs favorably across four control domains with neural network function approximation.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {145},
numpages = {61},
keywords = {reinforcement learning, off-policy learning, temporal difference learning}
}

@inproceedings{10.1145/2024724.2024735,
author = {Wang, Yanzhi and Xie, Qing and Ammari, Ahmed and Pedram, Massoud},
title = {Deriving a Near-Optimal Power Management Policy Using Model-Free Reinforcement Learning and Bayesian Classification},
year = {2011},
isbn = {9781450306362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024724.2024735},
doi = {10.1145/2024724.2024735},
abstract = {To cope with the variations and uncertainties that emanate from hardware and application characteristics, dynamic power management (DPM) frameworks must be able to learn about the system inputs and environment and adjust the power management policy on the fly. In this paper we present an online adaptive DPM technique based on model-free reinforcement learning (RL), which is commonly used to control stochastic dynamical systems. In particular, we employ temporal difference learning for semi-Markov decision process (SMDP) for the model-free RL. In addition a novel workload predictor based on an online Bayes classifier is presented to provide effective estimates of the workload states for the RL algorithm. In this DPM framework, power and latency tradeoffs can be precisely controlled based on a user-defined parameter. Experiments show that amount of average power saving (without any increase in the latency) is up to 16.7\% compared to a reference expert-based approach. Alternatively, the per-request latency reduction without any power consumption increase is up to 28.6\% compared to the expert-based approach.},
booktitle = {Proceedings of the 48th Design Automation Conference},
pages = {41–46},
numpages = {6},
keywords = {reinforcement learning, dynamic power management, Bayes classification},
location = {San Diego, California},
series = {DAC '11}
}

@inproceedings{10.1145/3284869.3284920,
author = {Boudreault, Maxime and Bouchard, Bruno and Bouchard, K\'{e}vin and Gaboury, S\'{e}bastien},
title = {Maximizing Player Engagement in a Global Warming Sensitization Video Game Through Reinforcement Learning},
year = {2018},
isbn = {9781450365819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284869.3284920},
doi = {10.1145/3284869.3284920},
abstract = {Global warming's consequences must be known and humanity has to take action. In spite of the efforts already taken toward that goal, the challenge remains. Of all media used to achieve that goal, video games are not as used as their increase in popularity may suggest; even when they are, their entertainment value is too low to raise interest. In this paper, we present a serious game called "Penguin Panic!", specifically developed to increase sensitization about climate change. In this game, a Dynamic Difficulty Adjustment (DDA) system is used to increase the player's interest by providing him or her with a flow-friendly game world. Using Reinforcement Learning, this DDA system adjusts difficulty in real-time based on the player's skills.},
booktitle = {Proceedings of the 4th EAI International Conference on Smart Objects and Technologies for Social Good},
pages = {196–201},
numpages = {6},
location = {Bologna, Italy},
series = {Goodtechs '18}
}

@inproceedings{10.5555/3545946.3598617,
author = {Kim, Woojun and Jung, Whiyoung and Cho, Myungsik and Sung, Youngchul},
title = {A Variational Approach to Mutual Information-Based Coordination for Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper, we propose a new mutual information (MMI) framework for multi-agent reinforcement learning (MARL) to enable multiple agents to learn coordinated behaviors by regularizing the accumulated return with the simultaneous mutual information between multi-agent actions. By introducing a latent variable to induce nonzero mutual information between multi-agent actions and applying a variational bound, we derive a tractable lower bound on the considered MMI-regularized objective function. The derived tractable objective can be interpreted as maximum entropy reinforcement learning combined with uncertainty reduction of other agents' actions. Applying policy iteration to maximize the derived lower bound, we propose a practical algorithm named variational maximum mutual information multi-agent actor-critic (VM3-AC), which follows centralized learning with decentralized execution (CTDE). We evaluated VM3-AC for several games requiring coordination, and numerical results show that VM3-AC outperforms other MARL algorithms in multi-agent tasks requiring high-quality coordination.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {40–48},
numpages = {9},
keywords = {multi-agent reinforcement learning, mutual information, coordination},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3604915.3608854,
author = {Xu, Ruiyang and Bhandari, Jalaj and Korenkevych, Dmytro and Liu, Fan and He, Yuchen and Nikulkov, Alex and Zhu, Zheqing},
title = {Optimizing Long-Term Value for Auction-Based Recommender Systems via On-Policy Reinforcement Learning},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608854},
doi = {10.1145/3604915.3608854},
abstract = {Auction-based recommender systems are prevalent in online advertising platforms, but they are typically optimized to allocate recommendation slots based on immediate expected return metrics, neglecting the downstream effects of recommendations on user behavior. In this study, we employ reinforcement learning to optimize for long-term return metrics in an auction-based recommender system. Utilizing temporal difference learning, a fundamental reinforcement learning algorithm, we implement a one-step policy improvement approach that biases the system towards recommendations with higher long-term user engagement metrics. This optimizes value over long horizons while maintaining compatibility with the auction framework. Our approach is grounded in dynamic programming ideas which show that our method provably improves upon the existing auction-based base policy. Through an online A/B test conducted on an auction-based recommender system which handles billions of impressions and users daily, we empirically establish that our proposed method outperforms the current production system in terms of long-term user engagement metrics.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {955–962},
numpages = {8},
keywords = {Policy improvement, Recommender systems, Reinforcement learning, Long-term user engagement},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@inproceedings{10.1145/3570361.3615758,
author = {Le, Ta Dang Khoa and Nikaein, Navid},
title = {RAN Simulator Is NOT What You Need: O-RAN Reinforcement Learning for the Wireless Factory},
year = {2023},
isbn = {9781450399906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570361.3615758},
doi = {10.1145/3570361.3615758},
abstract = {As modern manufacturing lines embrace greater modularity and flexibility, the need to transition factory networks from wired to wireless grows. Yet the mission-critical nature of factory networks poses a key challenge - connecting numerous diverse machines with high QoS predictability. After formulating this challenge as predictable RAN optimization via Reinforcement Learning (RL), we highlight a major-yet-overlooked modeling issue: matching the packet handling mechanics of a production/real RAN software. In this paper, we show that these mismatches inside RAN simulators can cause non-trivial QoS gaps in production. Then, we present Twin5G, a novel training solution that brings scalable and near-discrete-time emulations to real RAN software, removing the need for RAN simulators. In a RAN Slicing example, Twin5G-trained policy outperforms simulator-trained and standard RL-trained policies in both QoS achieved (+16\%) and predictability (+19\%) during tests.},
booktitle = {Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
articleno = {151},
numpages = {3},
location = {Madrid, Spain},
series = {ACM MobiCom '23}
}

@article{10.1145/3625236,
author = {Qu, Ao and Tang, Yihong and Ma, Wei},
title = {Adversarial Attacks on Deep Reinforcement Learning-Based Traffic Signal Control Systems with Colluding Vehicles},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3625236},
doi = {10.1145/3625236},
abstract = {The rapid advancements of Internet of Things (IoT) and Artificial Intelligence (AI) have catalyzed the development of adaptive traffic control systems (ATCS) for smart cities. In particular, deep reinforcement learning (DRL) models produce state-of-the-art performance and have great potential for practical applications. In the existing DRL-based ATCS, the controlled signals collect traffic state information from nearby vehicles, and then optimal actions (e.g., switching phases) can be determined based on the collected information. The DRL models fully “trust” that vehicles are sending the true information to the traffic signals, making the ATCS vulnerable to adversarial attacks with falsified information. In view of this, this paper first time formulates a novel task in which a group of vehicles can cooperatively send falsified information to “cheat” DRL-based ATCS in order to save their total travel time. To solve the proposed task, we develop CollusionVeh, a generic and effective vehicle-colluding framework composed of a road situation encoder, a vehicle interpreter, and a communication mechanism. We employ our framework to attack established DRL-based ATCS and demonstrate that the total travel time for the colluding vehicles can be significantly reduced with a reasonable number of learning episodes, and the colluding effect will decrease if the number of colluding vehicles increases. Additionally, insights and suggestions for the real-world deployment of DRL-based ATCS are provided. The research outcomes could help improve the reliability and robustness of the ATCS and better protect the smart mobility systems.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {sep},
keywords = {adaptive traffic control system, adversarial attacks, connected vehicles, reinforcement learning}
}

@inproceedings{10.1109/WI-IAT.2014.167,
author = {Teng, Teck Hou and Tan, Ah Hwee and Starzyk, Janusz A. and Tan, Yuan Sin and Teow, Loo Nin},
title = {Integrating Motivated Learning and K-Winner-Take-All to Coordinate Multi-Agent Reinforcement Learning},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.167},
doi = {10.1109/WI-IAT.2014.167},
abstract = {This work addresses the coordination issue in distributed optimization problem (DOP) where multiple distinct and time-critical tasks are performed to satisfy a global objective function. The performance of these tasks has to be coordinated due to the sharing of consumable resources and the dependency on non-consumable resources. Knowing that it can be sub-optimal to predefine the performance of the tasks for large DOPs, the multi-agent reinforcement learning (MARL) framework is adopted wherein an agent is used to learn the performance of each distinct task using reinforcement learning. To coordinate MARL, we propose a novel coordination strategy integrating Motivated Learning (ML) and the k-Winner-Take-All (k-WTA) approach. The priority of the agents to the shared resources is determined using Motivated Learning in real time. Due to the finite amount of the shared resources, the k-WTA approach is used to allow for the maximum number of the most urgent tasks to execute. Agents performing tasks dependent on resources produced by other agents are coordinated using domain knowledge. Comparing our proposed contribution to the existing approaches, results from our experiments based on a 16-task DOP and a 68-task DOP show our proposed approach to be most effective in coordinating multi-agent reinforcement learning.},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 03},
pages = {190–197},
numpages = {8},
keywords = {Real-Time Strategy Game, k-Winner-Take-All, Multi-Agent System, Motivated Learning, Reinforcement Learning},
series = {WI-IAT '14}
}

@inproceedings{10.5555/3586210.3586409,
author = {Kim, Namkyoun and Jung, Minhyuk and Yoon, Inseok and Park, Moonseo and Ahn, Changbum R.},
title = {Reinforcement Learning-Based Transportation and Sway Suppression Methods for Gantry Cranes in Simulated Environment},
year = {2023},
publisher = {IEEE Press},
abstract = {To improve the productivity and safety of cranes, deep reinforcement learning (DRL) has received widespread attention as a framework for developing automated control methods. However, the major challenge of DRL is sample efficiency, which is further exacerbated by the operational and kinematic characteristics of the crane. Our study proposes an approach to improve the sample efficiency in training control policies for two subtasks: horizontal transportation and sway suppression. To do this, we built a simulation environment and defined the state of the environment and the reward. Then, we performed experiments to find out whether three DRL techniques (reward shaping, curriculum learning, and generative adversarial imitation learning) can mitigate the sample efficiency degradation caused by operational and kinematic characteristics. The results show that the techniques used in our experiment are effective in the improvement of the sample efficiency and learning performance of the DRL model for crane operation.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2377–2385},
numpages = {9},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3205455.3205536,
author = {Peng, Yiming and Chen, Gang and Singh, Harman and Zhang, Mengjie},
title = {NEAT for Large-Scale Reinforcement Learning through Evolutionary Feature Learning and Policy Gradient Search},
year = {2018},
isbn = {9781450356183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205455.3205536},
doi = {10.1145/3205455.3205536},
abstract = {NeuroEvolution of Augmenting Topology (NEAT) is one of the most successful algorithms for solving traditional reinforcement learning (RL) tasks such as pole-balancing. However, the algorithm faces serious challenges while tackling problems with large state spaces, particularly the Atari game playing tasks. This is due to the major flaw that NEAT aims at evolving a single neural network (NN) that must be able to simultaneously extract high-level state features and select action outputs. However such complicated NNs cannot be easily evolved directly through NEAT. To address this issue, we propose a new reinforcement learning scheme based on NEAT with two key technical advancements: (1) a new three-stage learning scheme is introduced to clearly separate feature learning and policy learning to allow effective knowledge sharing and learning across multiple agents; (2) various policy gradient search algorithms can be seamlessly integrated with NEAT for training policy networks with deep structures to achieve effective and sample efficient RL. Experiments on several Atari games confirm that our new learning scheme can be more effective and has higher sample efficiency than NEAT and three state-of-the-art algorithms from the most recent RL literature.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {490–497},
numpages = {8},
keywords = {policy learning, policy gradient search, neuroevolution, NEAT, reinforcement learning, feature learning},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@article{10.1109/TNET.2021.3126933,
author = {Sun, Penghao and Guo, Zehua and Li, Junfei and Xu, Yang and Lan, Julong and Hu, Yuxiang},
title = {Enabling Scalable Routing in Software-Defined Networks With Deep Reinforcement Learning on Critical Nodes},
year = {2021},
issue_date = {April 2022},
publisher = {IEEE Press},
volume = {30},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3126933},
doi = {10.1109/TNET.2021.3126933},
abstract = {Traditional routing schemes usually use fixed models for routing policies and thus are not good at handling complicated and dynamic traffic, leading to performance degradation (e.g., poor quality of service). Emerging Deep Reinforcement Learning (DRL) coupled with Software-Defined Networking (SDN) provides new opportunities to improve network performance with automatic traffic analysis and policy generation. However, existing DRL-based routing solutions usually rely on all node information to make routing decisions for the network and hence are both hard to converge in large networks and vulnerable to topology changes. In this paper, we propose ScaleDeep, a scalable DRL-based routing scheme for SDN, which improves the routing performance and is resilient to topology changes. Essentially, ScaleDeep takes advantage of partial control on network nodes and DRL. We select a set of critical nodes from a network as driver nodes, which can simulate the entire network operation, based on the control theory. By observing the traffic variation on the driver nodes, DRL dynamically adjusts some link weights for a weighted shortest path algorithm to change the routing paths and improve the routing performance. Limiting the control on driver nodes improves the convergence ability of DRL and reduces the dependency of the DRL agent on the fixed network topology. To validate the performance of ScaleDeep, we conduct packet-level simulations on different topologies. The results show that ScaleDeep outperforms existing DRL-based schemes by reducing the average flow completion time by up to 36\% and exhibiting better robustness against minor topology changes.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {629–640},
numpages = {12}
}

@inproceedings{10.5555/3466184.3466364,
author = {Heger, Jens and Voss, Thomas},
title = {Dynamically Changing Sequencing Rules with Reinforcement Learning in a Job Shop System with Stochastic Influences},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Sequencing operations can be difficult, especially under uncertain conditions. Applying decentral sequencing rules has been a viable option; however, no rule exists that can outperform all other rules under varying system performance. For this reason, reinforcement learning (RL) is used as a hyper heuristic to select a sequencing rule based on the system status. Based on multiple training scenarios considering stochastic influences, such as varying inter arrival time or customers changing the product mix, the advantages of RL are presented. For evaluation, the trained agents are exploited in a generic manufacturing system. The best agent trained is able to dynamically adjust sequencing rules based on system performance, thereby matching and outperforming the presumed best static sequencing rules by ≈ 3\%. Using the trained policy in an unknown scenario, the RL heuristic is still able to change the sequencing rule according to the system status, thereby providing robust performance.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1608–1618},
numpages = {11},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/1839294.1839377,
author = {Rajruangrabin, Jartuwat and Popa, Dan O.},
title = {Reinforcement Learning of Interface Mapping for Interactivity Enhancement of Robot Control in Assistive Environments},
year = {2010},
isbn = {9781450300711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1839294.1839377},
doi = {10.1145/1839294.1839377},
abstract = {The supervisory control of robots is a very demanding application. In the context of robots control in assistive environments, it is important that the robot user is able to give commands to robots in a way that is easy and intuitive. There are several tasks that can be achieved using robots under assistive environments. It is challenging to efficiently control multiple robots / robots with degrees of freedom with a simple/intuitive interface by a single operator. In this proposal, we propose the use of Reinforcement Learning for intuitive interface mapping. Based on interaction with the environments, we can determine the optimal interface mapping through the process of Reinforcement Learning. The novelty of this paper is the use of changing reward functions based on qualitative performance evaluation for the Reinforcement Learning algorithm. In this paper, we show that the use of proposed reward functions can result in optimal/intuitive interface mapping for multiple robots / robots with degrees of freedom control applications.},
booktitle = {Proceedings of the 3rd International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {70},
numpages = {3},
keywords = {human-robot interface, reinforcement learning},
location = {Samos, Greece},
series = {PETRA '10}
}

@inproceedings{10.5555/3545946.3598966,
author = {Kharyal, Chaitanya and Sinha, Tanmay and Gottipati, Sai Krishna and Abdollahi, Fatemeh and Das, Srijita and Taylor, Matthew E.},
title = {Do As You Teach: A Multi-Teacher Approach to Self-Play in Deep Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The future of industrial automation is hinged on the ability of the industrial robots to precisely finish the tasks designated for them [5]. These tasks are usually specified in terms of a state the robot is required to reach (i.e., a goal state). Goal-conditioned reinforcement learning [7, 8] is an emerging sub-field that trains policies with goal inputs. This enables the agent to generalize to new unseen goals, learn multiple complex tasks and acquire new skills along the way.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2457–2459},
numpages = {3},
keywords = {reinforcement learning, curriculum learning, self-play},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3538401.3546598,
author = {Gholami, Anousheh and Rao, Kunal and Hsiung, Wang-Pin and Po, Oliver and Sankaradas, Murugan and Baras, John S. and Chakradhar, Srimat},
title = {Application-Specific, Dynamic Reservation of 5G Compute and Network Resources by Using Reinforcement Learning},
year = {2022},
isbn = {9781450393959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538401.3546598},
doi = {10.1145/3538401.3546598},
abstract = {5G services and applications explicitly reserve compute and network resources in today's complex and dynamic infrastructure of multi-tiered computing and cellular networking to ensure application-specific service quality metrics, and the infrastructure providers charge the 5G services for the resources reserved. A static, one-time reservation of resources at service deployment typically results in extended periods of under-utilization of reserved resources during the lifetime of the service operation. This is due to a plethora of reasons like changes in content from the IoT sensors (for example, change in number of people in the field of view of a camera) or a change in the environmental conditions around the IoT sensors (for example, time of the day, rain or fog can affect data acquisition by sensors). Under-utilization of a specific resource like compute can also be due to temporary inadequate availability of another resource like the network bandwidth in a dynamic 5G infrastructure. We propose a novel Reinforcement Learning-based online method to dynamically adjust an application's compute and network resource reservations to minimize under-utilization of requested resources, while ensuring acceptable service quality metrics. We observe that a complex application-specific coupling exists between the compute and network usage of an application. Our proposed method learns this coupling during the operation of the service, and dynamically modulates the compute and network resource requests to mimimize under-utilization of reserved resources. Through experimental evaluation using real-world video analytics application, we show that our technique is able to capture complex compute-network coupling relationship in an online manner i.e. while the application is running, and dynamically adapts and saves upto 65\% compute and 93\% network resources on average (over multiple runs), without significantly impacting application accuracy.},
booktitle = {Proceedings of the ACM SIGCOMM Workshop on Network-Application Integration},
pages = {19–25},
numpages = {7},
location = {Amsterdam, Netherlands},
series = {NAI '22}
}

@article{10.1109/TNET.2020.2987866,
author = {Tian, Ying and Wang, Zhiliang and Yin, Xia and Shi, Xingang and Guo, Yingya and Geng, Haijun and Yang, Jiahai},
title = {Traffic Engineering in Partially Deployed Segment Routing Over IPv6 Network With Deep Reinforcement Learning},
year = {2020},
issue_date = {Aug. 2020},
publisher = {IEEE Press},
volume = {28},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2987866},
doi = {10.1109/TNET.2020.2987866},
abstract = {Segment Routing (SR) is a source routing paradigm which is widely used in Traffic Engineering (TE). By using SR, a node steers a packet through an ordered list of instructions called segments. By some extensions of interior gateway protocol, SR can be applied to IP/MPLS or IPv6 network without signal protocol. SR over IPv6 (SRv6) is attracting wide attention because of its interoperation ability with IPv6. However, upgrading the existing IPv6 network directly to a full SRv6 one can be difficult, because large-scale equipment replacement or software upgrade may cause economic and technical problems. TE in partially deployed SR network is becoming a hot research topic. In this paper, we propose the TE algorithm Weight Adjustment-SRTE (WA-SRTE) in partially deployed SRv6 network, in which SRv6 capable nodes are dispersedly deployed. Our objective is to minimize the network's maximum link utilization. WA-SRTE converts the TE problem into a Deep Reinforcement Learning problem and optimizes the OSPF weight, SRv6 node deployment and traffic paths simultaneously. Besides, traffic variation is also considered and we use a representative Traffic Matrix (TM) to epitomize the traffic characteristics over a period of time. Experiments demonstrate that with 20\% to 40\% of the SRv6 nodes deployed, we can achieve TE performance as good as in a full SR network for the experiment topologies. The results with WA remarkably outperform the results without it. Our algorithm also gets near-optimal results with changing traffic.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {1573–1586},
numpages = {14}
}

@inproceedings{10.1145/3394486.3403261,
author = {Heffetz, Yuval and Vainshtein, Roman and Katz, Gilad and Rokach, Lior},
title = {DeepLine: AutoML Tool for Pipelines Generation Using Deep Reinforcement Learning and Hierarchical Actions Filtering},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403261},
doi = {10.1145/3394486.3403261},
abstract = {Automatic Machine Learning (AutoML) is an area of research aimed at automating Machine Learning (ML) activities that currently require the involvement of human experts. One of the most challenging tasks in this field is the automatic generation of end-to-end ML pipelines: combining multiple types of ML algorithms into a single architecture used for analysis of previously-unseen data. This task has two challenging aspects: the first is the need to explore a large search space of algorithms and pipeline architectures. The second challenge is the computational cost of training and evaluating multiple pipelines. In this study we present DeepLine, a reinforcement learning-based approach for automatic pipeline generation. Our proposed approach utilizes an efficient representation of the search space together with a novel method for operating in environments with large and dynamic action spaces. By leveraging past knowledge gained from previously-analyzed datasets, our approach only needs to generate and evaluate few dozens of pipelines to reach comparable or better performance than current state-of-the-art AutoML systems that evaluate hundreds and even thousands of pipelines in their optimization process. Evaluation on 56 classification datasets demonstrates the merits of our approach.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2103–2113},
numpages = {11},
keywords = {AutoML, deep reinforcement learning, classification},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.5555/3523760.3523825,
author = {van Waveren, Sanne and Pek, Christian and Tumova, Jana and Leite, Iolanda},
title = {Correct Me If I'm Wrong: Using Non-Experts to Repair Reinforcement Learning Policies},
year = {2022},
publisher = {IEEE Press},
abstract = {Reinforcement learning has shown great potential for learning sequential decision-making tasks. Yet, it is difficult to anticipate all possible real-world scenarios during training, causing robots to inevitably fail in the long run. Many of these failures are due to variations in the robot's environment. Usually experts are called to correct the robot's behavior; however, some of these failures do not necessarily require an expert to solve them. In this work, we query non-experts online for help and explore 1) if/how non-experts can provide feedback to the robot after a failure and 2) how the robot can use this feedback to avoid such failures in the future by generating shields that restrict or correct its high-level actions. We demonstrate our approach on common daily scenarios of a simulated kitchen robot. The results indicate that non-experts can indeed understand and repair robot failures. Our generated shields accelerate learning and improve data-efficiency during retraining.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {493–501},
numpages = {9},
keywords = {policy repair, robot failure, non-experts, shielded reinforcement learning},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@article{10.1145/3400024,
author = {Xie, Hong and Li, Yongkun and Lui, John C. S.},
title = {A Reinforcement Learning Approach to Optimize Discount and Reputation Tradeoffs in E-Commerce Systems},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3400024},
doi = {10.1145/3400024},
abstract = {Feedback-based reputation systems are widely deployed in E-commerce systems. Evidence shows that earning a reputable label (for sellers of such systems) may take a substantial amount of time, and this implies a reduction of profit. We propose to enhance sellers’ reputation via price discounts. However, the challenges are as follows: (1) The demands from buyers depend on both the discount and reputation, and (2) the demands are unknown to the seller. To address these challenges, we first formulate a profit maximization problem via a semi-Markov decision process to explore the optimal tradeoffs in selecting price discounts. We prove the monotonicity of the optimal profit and optimal discount. Based on the monotonicity, we design a Q-learning with forward projection (QLFP) algorithm, which infers the optimal discount from historical transaction data. We prove that the QLFP algorithm convergences to the optimal policy. We conduct trace-driven simulations using a dataset from eBay to evaluate the QLFP algorithm. Evaluation results show that QLFP improves the profit by as high as 50\% over both Q-learning and Speedy Q-learning. The QLFP algorithm also improves both the reputation and profit by as high as two times over the scheme of not providing any price discount.},
journal = {ACM Trans. Internet Technol.},
month = {oct},
articleno = {37},
numpages = {26},
keywords = {Reputation systems, discount, reinforcement learning}
}

@inproceedings{10.1145/3291801.3291831,
author = {Kang, Qinma and Zhou, Huizhuo and Kang, Yunfan},
title = {An Asynchronous Advantage Actor-Critic Reinforcement Learning Method for Stock Selection and Portfolio Management},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291831},
doi = {10.1145/3291801.3291831},
abstract = {Computation finance has been a classical field that uses computer techniques to handle financial challenges. The most popular domains include financial forecast and portfolio management. They often involve large datasets with complex relations. Due to the special properties of computation finance problems, machine learning techniques, especially deep learning techniques, are widely used as the quantitative analysis tool. In this paper, we try to apply the state-of-art Asynchronous Advantage Actor-Critic algorithm to solve the portfolio management problem and design a standalone deep reinforcement learning model. In the simulated market environment with practical portfolio constrain settings, asset value managed by the proposed machine learning model largely outperforms S\&amp;P500 stock index in the test period.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {141–145},
numpages = {5},
keywords = {portfolio management, asynchronous advantage actor-critic, deep reinforcement learning, machine learning, Computational finance},
location = {Weihai, China},
series = {ICBDR '18}
}

@inproceedings{10.1145/3361242.3361258,
author = {Wu, Zhaolin and Yang, Yang and Li, Zheng and Zhao, Ruilian},
title = {A Time Window Based Reinforcement Learning Reward for Test Case Prioritization in Continuous Integration},
year = {2019},
isbn = {9781450377010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361242.3361258},
doi = {10.1145/3361242.3361258},
abstract = {Continuous integration refers to the practice of merging the working copies of all developers into the mainline frequently. Regression testing for each mergence is characterized by continually changing test suite, limited execution time, and fast feedback, which demands new test optimization techniques. Reinforcement learning is introduced for test case prioritization to save computing resources in continuous integration environment, where a reasonable reward function is highly important for learning strategy, since the process of reinforcement learning is a reward-guided behavior. In this paper, APHFW, a novel reward function is proposed by using partial historical information of test cases effectively for fast feedback and cost reduction. The experiments are based on three open-source data sets, and the results show that the proposed reward function is more cost-effect than other reinforcement learning rewards in continuous integration environment.},
booktitle = {Proceedings of the 11th Asia-Pacific Symposium on Internetware},
articleno = {4},
numpages = {6},
keywords = {continuous integration, test case prioritization, reward function, reinforcement learning},
location = {Fukuoka, Japan},
series = {Internetware '19}
}

@inproceedings{10.1145/3383313.3412233,
author = {HE, Xu and An, Bo and Li, Yanghua and Chen, Haikai and Wang, Rundong and Wang, Xinrun and Yu, Runsheng and Li, Xin and Wang, Zhirong},
title = {Learning to Collaborate in Multi-Module Recommendation via Multi-Agent Reinforcement Learning without Communication},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3412233},
doi = {10.1145/3383313.3412233},
abstract = {With the rise of online e-commerce platforms, more and more customers prefer to shop online. To sell more products, online platforms introduce various modules to recommend items with different properties such as huge discounts. A web page often consists of different independent modules. The ranking policies of these modules are decided by different teams and optimized individually without cooperation, which might result in competition between modules. Thus, the global policy of the whole page could be sub-optimal. In this paper, we propose a novel multi-agent cooperative reinforcement learning approach with the restriction that different modules cannot communicate. Our contributions are three-fold. Firstly, inspired by a solution concept in game theory named correlated equilibrium, we design a signal network to promote cooperation of all modules by generating signals (vectors) for different modules. Secondly, an entropy-regularized version of the signal network is proposed to coordinate agents’ exploration of the optimal global policy. Furthermore, experiments based on real-world e-commerce data demonstrate that our algorithm obtains superior performance over baselines.},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {210–219},
numpages = {10},
keywords = {Reinforcement learning},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@inproceedings{10.5555/3306127.3331794,
author = {Li, Xihan and Zhang, Jia and Bian, Jiang and Tong, Yunhai and Liu, Tie-Yan},
title = {A Cooperative Multi-Agent Reinforcement Learning Framework for Resource Balancing in Complex Logistics Network},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Resource balancing within complex transportation networks is one of the most important problems in real logistics domain. Traditional solutions on these problems leverage combinatorial optimization with demand and supply forecasting. However, the high complexity of transportation routes, severe uncertainty of future demand and supply, together with non-convex business constraints make it extremely challenging in the traditional resource management field. In this paper, we propose a novel sophisticated multi-agent reinforcement learning approach to address these challenges. In particular, inspired by the externalities especially the interactions among resource agents, we introduce an innovative cooperative mechanism for state and reward design resulting in more effective and efficient transportation. Extensive experiments on a simulated ocean transportation service demonstrate that our new approach can stimulate cooperation among agents and lead to much better performance. Compared with traditional solutions based on combinatorial optimization, our approach can give rise to a significant improvement in terms of both performance and stability.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {980–988},
numpages = {9},
keywords = {resource balancing, reinforcement learning, logistics network, multi-agent},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.5555/3586210.3586353,
author = {Xie, Shufang and Zhang, Tao and Rose, Oliver},
title = {Real-Time Scheduling Based on Simulation and Deep Reinforcement Learning with Featured Action Space},
year = {2023},
publisher = {IEEE Press},
abstract = {In this study, real-time scheduling is narrowed to the selection of one job to be processed from the queue of a machine when the machine becomes idle. It is considered to be one kind of sequential decision-making. Deep reinforcement learning with simulation has been widely used to make such decisions for most environments where the action space is either continuous or discrete but limited in size. However, in the real-time scheduling environment, the number of actions is the number of jobs in the queue which are varying over time. Moreover, if jobs arrive randomly, it is impossible to fix the actions. The action space is dynamic and stochastic. To overcome the difficulties raised by this, the action space is transformed into a featured action space. Actions are distinguished by their features. To apply the featured action space, three innovative structures of neural networks are proposed and compared with each other.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1731–1739},
numpages = {9},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3594739.3612907,
author = {Zhao, Chenyu and Wang, Haoyang and Li, Jiaqi and Man, Fanhang and Mu, Shilong and Ding, Wenbo and Zhang, Xiao-Ping and Chen, Xinlei},
title = {SmoothLander: A Quadrotor Landing Control System with Smooth Trajectory Guarantee Based on Reinforcement Learning},
year = {2023},
isbn = {9798400702006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594739.3612907},
doi = {10.1145/3594739.3612907},
abstract = {The landing process of the quadrotors can be affected by the disturbance from the ground effect when approaching the landing surface. Such a disturbance significantly increases the chances of collision and jittering of the quadrotors, thereby posing threats to the safety of both the quadrotors and the mounted equipment. In light of this, we propose SmoothLander, an aerodynamics and reinforcement learning-based control system to stabilize the quadrotors under the influence of the ground effect and control noise. First, we design a landing trajectory for the quadrotor in accordance with aerodynamics. Then we design a reinforcement learning-based command generator to effectively optimize the quadrotor’s landing behavior. We evaluate our control system through physical feature-based simulation and in-field experiments. The results show that our method can enable the quadrotor to land more smoothly and stably against control noise than the baseline.},
booktitle = {Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing \&amp; the 2023 ACM International Symposium on Wearable Computing},
pages = {682–687},
numpages = {6},
keywords = {Reinforcement Learning, Ground Effect, Quadrotor},
location = {Cancun, Quintana Roo, Mexico},
series = {UbiComp/ISWC '23 Adjunct}
}

@inproceedings{10.1145/3604915.3609493,
author = {Paparella, Vincenzo and Anelli, Vito Walter and Boratto, Ludovico and Di Noia, Tommaso},
title = {Reproducibility of Multi-Objective Reinforcement Learning Recommendation: Interplay between Effectiveness and Beyond-Accuracy Perspectives},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3609493},
doi = {10.1145/3604915.3609493},
abstract = {Providing effective suggestions is of predominant importance for successful Recommender Systems (RSs). Nonetheless, the need of accounting for additional multiple objectives has become prominent, from both the final users’ and the item providers’ points of view. This need has led to a new class of RSs, called Multi-Objective Recommender Systems (MORSs). These systems are designed to provide suggestions by considering multiple (conflicting) objectives simultaneously, such as diverse, novel, and fairness-aware recommendations. In this work, we reproduce a state-of-the-art study on MORSs that exploits a reinforcement learning agent to satisfy three objectives, i.e., accuracy, diversity, and novelty of recommendations. The selected study is one of the few MORSs where the source code and datasets are released to ensure the reproducibility of the proposed approach. Interestingly, we find that some challenges arise when replicating the results of the original work, due to the nature of multiple-objective problems. We also extend the evaluation of the approach to analyze the impact of improving user-centered objectives of recommendations (i.e., diversity and novelty) in terms of algorithmic bias. To this end, we take into consideration both popularity and category of the items. We discover some interesting trends in the recommendation performance according to different evaluation metrics. In addition, we see that the multi-objective reinforcement learning approach is responsible for increasing the bias disparity in the output of the recommendation algorithm for those items belonging to positively/negatively biased categories. We publicly release datasets and codes in the following GitHub repository: https://github.com/sisinflab/MORS_reproducibility.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {467–478},
numpages = {12},
keywords = {Reinforcement Learning, Multi-Objective Recommender Systems, Fairness, Bias, Novelty, Diversity},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@article{10.1145/3582576,
author = {Vinitsky, Eugene and Lichtl\'{e}, Nathan and Parvate, Kanaad and Bayen, Alexandre},
title = {Optimizing Mixed Autonomy Traffic Flow with Decentralized Autonomous Vehicles and Multi-Agent Reinforcement Learning},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2378-962X},
url = {https://doi.org/10.1145/3582576},
doi = {10.1145/3582576},
abstract = {We study the ability of autonomous vehicles to improve the throughput of a bottleneck using a fully decentralized control scheme in a mixed autonomy setting. We consider the problem of improving the throughput of a scaled model of the San Francisco–Oakland Bay Bridge: a two-stage bottleneck where four lanes reduce to two and then reduce to one. Although there is extensive work examining variants of bottleneck control in a centralized setting, there is less study of the challenging multi-agent setting where the large number of interacting AVs leads to significant optimization difficulties for reinforcement learning methods. We apply multi-agent reinforcement algorithms to this problem and demonstrate that significant improvements in bottleneck throughput, from 20\% at a 5\% penetration rate to 33\% at a 40\% penetration rate, can be achieved. We compare our results to a hand-designed feedback controller and demonstrate that our results sharply outperform the feedback controller despite extensive tuning. Additionally, we demonstrate that the RL-based controllers adopt a robust strategy that works across penetration rates whereas the feedback controllers degrade immediately upon penetration rate variation. We investigate the feasibility of both action and observation decentralization and demonstrate that effective strategies are possible using purely local sensing. Finally, we open-source our code at .},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {apr},
articleno = {13},
numpages = {22},
keywords = {mixed autonomy, autonomous vehicles, traffic optimization, Reinforcement learning}
}

@inproceedings{10.1145/3447548.3467135,
author = {Ni, Fei and Hao, Jianye and Lu, Jiawen and Tong, Xialiang and Yuan, Mingxuan and Duan, Jiahui and Ma, Yi and He, Kun},
title = {A Multi-Graph Attributed Reinforcement Learning Based Optimization Algorithm for Large-Scale Hybrid Flow Shop Scheduling Problem},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467135},
doi = {10.1145/3447548.3467135},
abstract = {Hybrid Flow Shop Scheduling Problem (HFSP) is an essential problem in the automated warehouse scheduling, aiming at optimizing the sequence of jobs and the assignment of machines to utilize the makespan or other objectives. Existing algorithms adopt fixed search paradigm based on expert knowledge to seek satisfactory solutions. However, considering the varying data distribution and large scale of the practical HFSP, these methods fail to guarantee the quality of the obtained solution under the real-time requirement, especially facing extremely different data distribution. To address this challenge, we propose a novel Multi-Graph Attributed Reinforcement Learning based Optimization (MGRO) algorithm to better tackle the practical large-scale HFSP and improve the existing algorithm. Owing to incorporating the reinforcement learning-based policy search approach with classic search operators and the powerful multi-graph based representation, MGRO is capable of adjusting the search paradigm according to specific instances and enhancing the search efficiency. Specifically, we formulate the Gantt chart of the instance into the multi-graph-structured data. Then Graph Neural Network (GNN) and attention-based adaptive weighted pooling are employed to represent the state and make MGRO size-agnostic across arbitrary sizes of instances. In addition, a useful reward shaping approach is designed to facilitate model convergence. Extensive numerical experiments on both the publicly available dataset and real industrial dataset from Huawei Supply Chain Business Unit demonstrate the superiority of MGRO over existing baselines.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {3441–3451},
numpages = {11},
keywords = {graph neural network, reinforcement learning, hybrid flow shop scheduling},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3502223.3502224,
author = {Zhou, Xingchen and Wang, Peng and Luo, Qiqing and Pan, Zhe},
title = {Multi-Hop Knowledge Graph Reasoning Based on Hyperbolic Knowledge Graph Embedding and Reinforcement Learning},
year = {2022},
isbn = {9781450395656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502223.3502224},
doi = {10.1145/3502223.3502224},
abstract = {Some unseen facts in knowledge graphs (KGs) can be complemented by multi-hop knowledge graph reasoning. The process of multi-hop reasoning can be presented as a serialized decision problem, and then be solved with reinforcement learning (RL). However, existing RL-based reasoning methods cannot deal with the hierarchical information in KGs effectively. To solve this issue, we propose a novel RL-based multi-hop KG reasoning model Path Additional Action-space Ranking (PAAR). Our model first proposes a hyperbolic knowledge graph embedding (KGE) model which can effectively capture hierarchical information in KGs. To introduce hierarchical information into RL, the hyperbolic KGE vector is subsequently added to the state space and helps to expand the action space. In order to alleviate the reward sparsity problem in RL, our model utilizes the score function of hyperbolic KGE model as a soft reward. Finally, the metric of hyperbolic space is added to the training of RL as a penalty strategy to constrain the sufficiency of multi-hop reasoning paths. Experimental results on two real-world datasets show that our proposed model not only provides effective answers but also offers sufficient paths.},
booktitle = {Proceedings of the 10th International Joint Conference on Knowledge Graphs},
pages = {1–9},
numpages = {9},
keywords = {knowledge graph embedding, reinforcement learning, knowledge graph reasoning},
location = {Virtual Event, Thailand},
series = {IJCKG '21}
}

@article{10.1145/3447623,
author = {Chen, Jianguo and Li, Kenli and Li, Keqin and Yu, Philip S. and Zeng, Zeng},
title = {Dynamic Bicycle Dispatching of Dockless Public Bicycle-Sharing Systems Using Multi-Objective Reinforcement Learning},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3447623},
doi = {10.1145/3447623},
abstract = {As a new generation of Public Bicycle-sharing Systems (PBS), the Dockless PBS (DL-PBS) is an important application of cyber-physical systems and intelligent transportation. How to use artificial intelligence to provide efficient bicycle dispatching solutions based on dynamic bicycle rental demand is an essential issue for DL-PBS. In this article, we propose MORL-BD, a dynamic bicycle dispatching algorithm based on multi-objective reinforcement learning to provide the optimal bicycle dispatching solution for DL-PBS. We model the DL-PBS system from the perspective of cyber-physical systems and use deep learning to predict the layout of bicycle parking spots and the dynamic demand of bicycle dispatching. We define the multi-route bicycle dispatching problem as a multi-objective optimization problem by considering the optimization objectives of dispatching costs, dispatch truck's initial load, workload balance among the trucks, and the dynamic balance of bicycle supply and demand. On this basis, the collaborative multi-route bicycle dispatching problem among multiple dispatch trucks is modeled as a multi-agent and multi-objective reinforcement learning model. All dispatch paths between parking spots are defined as state spaces, and the reciprocal of dispatching costs is defined as a reward. Each dispatch truck is equipped with an agent to learn the optimal dispatch path in the dynamic DL-PBS network. We create an elite list to store the Pareto optimal solutions of bicycle dispatch paths found in each action, and finally get the Pareto frontier. Experimental results on the actual DL-PBS show that compared with existing methods, MORL-BD can find a higher quality Pareto frontier with less execution time.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {sep},
articleno = {34},
numpages = {24},
keywords = {multi-objective reinforcement learning, Pareto optimality, bicycle dispatching, intelligent transportation, Bicycle-sharing systems}
}

@article{10.1145/3418498,
author = {Wu, Nan and Deng, Lei and Li, Guoqi and Xie, Yuan},
title = {Core Placement Optimization for Multi-Chip Many-Core Neural Network Systems with Reinforcement Learning},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/3418498},
doi = {10.1145/3418498},
abstract = {Multi-chip many-core neural network systems are capable of providing high parallelism benefited from decentralized execution, and they can be scaled to very large systems with reasonable fabrication costs. As multi-chip many-core systems scale up, communication latency related effects will take a more important portion in the system performance. While previous work mainly focuses on the core placement within a single chip, there are two principal issues still unresolved: the communication-related problems caused by the non-uniform, hierarchical on/off-chip communication capability in multi-chip systems, and the scalability of these heuristic-based approaches in a factorially growing search space. To this end, we propose a reinforcement-learning-based method to automatically optimize core placement through deep deterministic policy gradient, taking into account information of the environment by performing a series of trials (i.e., placements) and using convolutional neural networks to extract spatial features of different placements. Experimental results indicate that compared with a naive sequential placement, the proposed method achieves 1.99\texttimes{} increase in throughput and 50.5\% reduction in latency; compared with the simulated annealing, an effective technique to approximate the global optima in an extremely large search space, our method improves the throughput by 1.22\texttimes{} and reduces the latency by 18.6\%. We further demonstrate that our proposed method is capable to find optimal placements taking advantages of different communication properties caused by different system configurations, and work in a topology-agnostic manner.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {oct},
articleno = {11},
numpages = {27},
keywords = {core placement optimization, Multi-chip many-core architecture, machine learning for system, neural network accelerator}
}

