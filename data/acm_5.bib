@inproceedings{10.1145/3292500.3330868,
author = {Liu, Kunpeng and Fu, Yanjie and Wang, Pengfei and Wu, Le and Bo, Rui and Li, Xiaolin},
title = {Automating Feature Subspace Exploration via Multi-Agent Reinforcement Learning},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330868},
doi = {10.1145/3292500.3330868},
abstract = {Feature selection is the preprocessing step in machine learning which tries to select the most relevant features for the subsequent prediction task. Effective feature selection could help reduce dimensionality, improve prediction accuracy and increase result comprehensibility. It is very challenging to find the optimal feature subset from the subset space as the space could be very large. While much effort has been made by existing studies, reinforcement learning can provide a new perspective for the searching strategy in a more global way. In this paper, we propose a multi-agent reinforcement learning framework for the feature selection problem. Specifically, we first reformulate feature selection with a reinforcement learning framework by regarding each feature as an agent. Then, we obtain the state of environment in three ways, i.e., statistic description, autoencoder and graph convolutional network (GCN), in order to make the algorithm better understand the learning progress. We show how to learn the state representation in a graph-based way, which could tackle the case when not only the edges, but also the nodes are changing step by step. In addition, we study how the coordination between different features would be improved by more reasonable reward scheme. The proposed method could search the feature subset space globally and could be easily adapted to the real-time case (real-time feature selection) due to the nature of reinforcement learning. Also, we provide an efficient strategy to accelerate the convergence of multi-agent reinforcement learning. Finally, extensive experimental results show the significant improvement of the proposed method over conventional approaches.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {207–215},
numpages = {9},
keywords = {multi-agent reinforcement learning, automated exploration, feature selection},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.5555/1005332.1016794,
author = {Sallans, Brian and Hinton, Geoffrey E.},
title = {Reinforcement Learning with Factored States and Actions},
year = {2004},
issue_date = {12/1/2004},
publisher = {JMLR.org},
volume = {5},
issn = {1532-4435},
abstract = {A novel approximation method is presented for approximating the value function and selecting good actions for Markov decision processes with large state and action spaces. The method approximates state-action values as negative free energies in an undirected graphical model called a product of experts. The model parameters can be learned efficiently because values and derivatives can be efficiently computed for a product of experts. Actions can be found even in large factored action spaces by the use of Markov chain Monte Carlo sampling. Simulation results show that the product of experts approximation can be used to solve large problems. In one simulation it is used to find actions in action spaces of size 240.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {1063–1088},
numpages = {26}
}

@article{10.1145/3310090,
author = {Garc\'{\i}a, Javier and Fern\'{a}ndez, Fernando},
title = {Probabilistic Policy Reuse for Safe Reinforcement Learning},
year = {2019},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3310090},
doi = {10.1145/3310090},
abstract = {This work introduces Policy Reuse for Safe Reinforcement Learning, an algorithm that combines Probabilistic Policy Reuse and teacher advice for safe exploration in dangerous and continuous state and action reinforcement learning problems in which the dynamic behavior is reasonably smooth and the space is Euclidean. The algorithm uses a continuously increasing monotonic risk function that allows for the identification of the probability to end up in failure from a given state. Such a risk function is defined in terms of how far such a state is from the state space known by the learning agent. Probabilistic Policy Reuse is used to safely balance the exploitation of actual learned knowledge, the exploration of new actions, and the request of teacher advice in parts of the state space considered dangerous. Specifically, the π-reuse exploration strategy is used. Using experiments in the helicopter hover task and a business management problem, we show that the π-reuse exploration strategy can be used to completely avoid the visit to undesirable situations while maintaining the performance (in terms of the classical long-term accumulated reward) of the final policy achieved.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {mar},
articleno = {14},
numpages = {24},
keywords = {Reinforcement learning, software agents, case-based reasoning}
}

@inproceedings{10.1145/1553374.1553504,
author = {Taylor, Gavin and Parr, Ronald},
title = {Kernelized Value Function Approximation for Reinforcement Learning},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553504},
doi = {10.1145/1553374.1553504},
abstract = {A recent surge in research in kernelized approaches to reinforcement learning has sought to bring the benefits of kernelized machine learning techniques to reinforcement learning. Kernelized reinforcement learning techniques are fairly new and different authors have approached the topic with different assumptions and goals. Neither a unifying view nor an understanding of the pros and cons of different approaches has yet emerged. In this paper, we offer a unifying view of the different approaches to kernelized value function approximation for reinforcement learning. We show that, except for different approaches to regularization, Kernelized LSTD (KLSTD) is equivalent to a modelbased approach that uses kernelized regression to find an approximate reward and transition model, and that Gaussian Process Temporal Difference learning (GPTD) returns a mean value function that is equivalent to these other approaches. We also discuss the relationship between our modelbased approach and the earlier Gaussian Processes in Reinforcement Learning (GPRL). Finally, we decompose the Bellman error into the sum of transition error and reward error terms, and demonstrate through experiments that this decomposition can be helpful in choosing regularization parameters.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1017–1024},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{10.5555/3586210.3586438,
author = {Jiang, Jinyang and Peng, Yijie and Hu, Jiaqiao},
title = {Quantile-Based Policy Optimization for Reinforcement Learning},
year = {2023},
publisher = {IEEE Press},
abstract = {Classical reinforcement learning (RL) aims to optimize the expected cumulative rewards. In this work, we consider the RL setting where the goal is to optimize the quantile of the cumulative rewards. We parameterize the policy controlling actions by neural networks and propose a novel policy gradient algorithm called Quantile-Based Policy Optimization (QPO) and its variant Quantile-Based Proximal Policy Optimization (QPPO) to solve deep RL problems with quantile objectives. QPO uses two coupled iterations running at different time scales for simultaneously estimating quantiles and policy parameters. Our numerical results demonstrate that the proposed algorithms outperform the existing baseline algorithms under the quantile criterion.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2712–2723},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3308558.3313517,
author = {Fang, Zheng and Cao, Yanan and Li, Qian and Zhang, Dongjie and Zhang, Zhenyu and Liu, Yanbing},
title = {Joint Entity Linking with Deep Reinforcement Learning},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313517},
doi = {10.1145/3308558.3313517},
abstract = {Entity linking is the task of aligning mentions to corresponding entities in a given knowledge base. Previous studies have highlighted the necessity for entity linking systems to capture the global coherence. However, there are two common weaknesses in previous global models. First, most of them calculate the pairwise scores between all candidate entities and select the most relevant group of entities as the final result. In this process, the consistency among wrong entities as well as that among right ones are involved, which may introduce noise data and increase the model complexity. Second, the cues of previously disambiguated entities, which could contribute to the disambiguation of the subsequent mentions, are usually ignored by previous models. To address these problems, we convert the global linking into a sequence decision problem and propose a reinforcement learning model which makes decisions from a global perspective. Our model makes full use of the previous referred entities and explores the long-term influence of current selection on subsequent decisions. We conduct experiments on different types of datasets, the results show that our model outperforms state-of-the-art systems and has better generalization performance.},
booktitle = {The World Wide Web Conference},
pages = {438–447},
numpages = {10},
keywords = {Entity linking, reinforcement learning, joint disambiguation, knowledge base},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@article{10.5555/1577069.1755867,
author = {Strehl, Alexander L. and Li, Lihong and Littman, Michael L.},
title = {Reinforcement Learning in Finite MDPs: PAC Analysis},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {We study the problem of learning near-optimal behavior in finite Markov Decision Processes (MDPs) with a polynomial number of samples. These "PAC-MDP" algorithms include the well-known E3 and R-MAX algorithms as well as the more recent Delayed Q-learning algorithm. We summarize the current state-of-the-art by presenting bounds for the problem in a unified theoretical framework. A more refined analysis for upper and lower bounds is presented to yield insight into the differences between the model-free Delayed Q-learning and the model-based R-MAX.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {2413–2444},
numpages = {32}
}

@article{10.1145/3510381,
author = {Jagadheesh, Samala and Bhanu, P. Veda and J., Soumya},
title = {NoC Application Mapping Optimization Using Reinforcement Learning},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {6},
issn = {1084-4309},
url = {https://doi.org/10.1145/3510381},
doi = {10.1145/3510381},
abstract = {Application mapping is one of the early stage design processes aimed to improve the performance of Network-on-Chip. Mapping is an NP-hard problem. A massive amount of high-quality supervised data is required to solve the application mapping problem using traditional neural networks. In this article, a reinforcement learning–based neural framework is proposed to learn the heuristics of the application mapping problem. The proposed reinforcement learning–based mapping algorithm (RL-MAP) has actor and critic networks. The actor is a policy network, which provides mapping sequences. The critic network estimates the communication cost of these mapping sequences. The actor network updates the policy distribution in the direction suggested by the critic. The proposed RL-MAP is trained with unsupervised data to predict the permutations of the cores to minimize the overall communication cost. Further, the solutions are improved using the 2-opt local search algorithm. The performance of RL-MAP is compared with a few well-known heuristic algorithms, the Neural Mapping Algorithm&nbsp;(NMA) and message-passing neural network-pointer network-based genetic algorithm (MPN-GA). Results show that the communication cost and runtime of the RL-MAP improved considerably in comparison with the heuristic algorithms. The communication cost of the solutions generated by RL-MAP is nearly equal to MPN-GA and improved by 4.2\% over NMA, while consuming less runtime.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {jun},
articleno = {55},
numpages = {16},
keywords = {neural networks, reinforcement learning, actor-critic network, network-on-chip, Application mapping}
}

@article{10.1145/3291045,
author = {Mendon\c{C}a, Matheus R. F. and Ziviani, Artur and Barreto, Andr\'{E} M. S.},
title = {Graph-Based Skill Acquisition For Reinforcement Learning},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3291045},
doi = {10.1145/3291045},
abstract = {In machine learning, Reinforcement Learning (RL) is an important tool for creating intelligent agents that learn solely through experience. One particular subarea within the RL domain that has received great attention is how to define macro-actions, which are temporal abstractions composed of a sequence of primitive actions. This subarea, loosely called skill acquisition, has been under development for several years and has led to better results in a diversity of RL problems. Among the many skill acquisition approaches, graph-based methods have received considerable attention. This survey presents an overview of graph-based skill acquisition methods for RL. We cover a diversity of these approaches and discuss how they evolved throughout the years. Finally, we also discuss the current challenges and open issues in the area of graph-based skill acquisition for RL.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {6},
numpages = {26},
keywords = {Skill acquisition, clustering, graph analytics, centrality, reinforcement learning}
}

@inproceedings{10.5555/3535850.3535956,
author = {Neustroev, Grigory and Andringa, Sytze P. E. and Verzijlbergh, Remco A. and De Weerdt, Mathijs M.},
title = {Deep Reinforcement Learning for Active Wake Control},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Wind farms suffer from so-called wake effects: when turbines are located in the wind shadows of other turbines, their power output is substantially reduced. These losses can be partially mitigated via actively changing the yaw from the individually optimal direction. Most existing wake control techniques have two major limitations: they use simplified wake models to optimize the control strategy, and they assume that the atmospheric conditions remain stable. In this paper, we address these limitations by applying reinforcement learning (RL). RL forgoes the wake model entirely and learns an optimal control strategy based on the observed atmospheric conditions and a reward signal, in this case the power output of the farm. It also accounts for random transitions in the observations, such as turbulent fluctuations in the wind. To evaluate RL for active wake control, we provide a simulator based on the state-of-the-art FLORIS model in the OpenAI gym format. Next, we propose three different state-action representations of the active wake control problem and investigate their effect on the performance of RL-based wake control. Finally, we compare RL to a state-of-the-art wake control strategy based on FLORIS and show that RL is less sensitive to changes in unobservable data.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {944–953},
numpages = {10},
keywords = {wind energy, active wake control, deep reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.5555/3545946.3598964,
author = {Koprulu, Cevahir and Topcu, Ufuk},
title = {Reward-Machine-Guided, Self-Paced Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Self-paced reinforcement learning (RL) aims to improve the sample efficiency of RL by automatically creating sequences, i.e., curricula, of probability distributions over contexts. However, existing self-paced RL methods fail in tasks that involve temporally extended behaviors. As a remedy, we exploit prior knowledge about the underlying task structure and develop a self-paced RL algorithm guided by reward machines, i.e., a finite-state machine that encodes such structure. The proposed algorithm integrates reward machines in the updates of 1) the policy and value functions obtained by an RL algorithm, and 2) the automated curriculum that generates context distributions. Our empirical results evidence that the proposed algorithm achieves optimal behavior in cases where existing methods fail, and also reduces curriculum length and variance.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2451–2453},
numpages = {3},
keywords = {reward machines, curriculum learning, reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/1553374.1553512,
author = {Vlassis, Nikos and Toussaint, Marc},
title = {Model-Free Reinforcement Learning as Mixture Learning},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553512},
doi = {10.1145/1553374.1553512},
abstract = {We cast model-free reinforcement learning as the problem of maximizing the likelihood of a probabilistic mixture model via sampling, addressing both the infinite and finite horizon cases. We describe a Stochastic Approximation EM algorithm for likelihood maximization that, in the tabular case, is equivalent to a non-bootstrapping optimistic policy iteration algorithm like Sarsa(1) that can be applied both in MDPs and POMDPs. On the theoretical side, by relating the proposed stochastic EM algorithm to the family of optimistic policy iteration algorithms, we provide new tools that permit the design and analysis of algorithms in that family. On the practical side, preliminary experiments on a POMDP problem demonstrated encouraging results.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1081–1088},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{10.5555/3306127.3331700,
author = {Subramanian, Jayakumar and Mahajan, Aditya},
title = {Reinforcement Learning in Stationary Mean-Field Games},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent reinforcement learning has made significant progress in recent years, but it remains a hard problem. Hence, one often resorts to developing learning algorithms for specific classes of multi-agent systems. In this paper we study reinforcement learning in a specific class of multi-agent systems systems called mean-field games. In particular, we consider learning in stationary mean-field games. We identify two different solution concepts---stationary mean-field equilibrium and stationary mean-field social-welfare optimal policy---for such games based on whether the agents are non-cooperative or cooperative, respectively. We then generalize these solution concepts to their local variants using bounded rationality based arguments. For these two local solution concepts, we present two reinforcement learning algorithms. We show that the algorithms converge to the right solution under mild technical conditions and demonstrate this using two numerical examples.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {251–259},
numpages = {9},
keywords = {stationary mean-field games, bounded rationality, mean-field games, multi-agent reinforcement learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3523150.3523152,
author = {Li, Ruyang and Zhang, Yaqiang and Zhao, Yaqian and Wui, Hui and Xu, Zhe and Zhao, Kun},
title = {Deep Reinforcement Learning with Noisy Exploration for Autonomous Driving},
year = {2022},
isbn = {9781450387477},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523150.3523152},
doi = {10.1145/3523150.3523152},
abstract = {Autonomous driving decision-making is a great challenge in complex traffic environment, and the deep reinforcement learning (DRL) can contribute to the more intelligent strategy. In the autonomous driving scenarios with DRL algorithms, sufficient exploration to the traffic environment is vital for constructing the state spaces, training the driving decision model and transferring to a new environment. In this paper, three different noise modes are presented to investigate the performance of noisy exploration and generalization in self-driving tasks. Extensive experiments indicate that the noisy exploration is not necessary for the easy traffic environments, and the correlated noisy exploration is an effective technique in generalizing to complex traffic environments, while the uncorrelated noisy exploration may result in a counter-productive effect in inertial autonomous driving system.},
booktitle = {Proceedings of the 2022 6th International Conference on Machine Learning and Soft Computing},
pages = {8–14},
numpages = {7},
keywords = {Autonomous Driving, Deep Reinforcement Learning, Generalization, Noisy Exploration},
location = {Haikou, China},
series = {ICMLSC '22}
}

@inproceedings{10.1145/3394486.3403315,
author = {Wei, Yu and Mao, Minjia and Zhao, Xi and Zou, Jianhua and An, Ping},
title = {City Metro Network Expansion with Reinforcement Learning},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403315},
doi = {10.1145/3394486.3403315},
abstract = {City metro network expansion, included in the transportation network design, aims to design new lines based on the existing metro network. Existing methods in the field of transportation network design either (i) can hardly formulate this problem efficiently, (ii) depend on expert guidance to produce solutions, or (iii) appeal to problem-specific heuristics which are difficult to design. To address these limitations, we propose a reinforcement learning based method for the city metro network expansion problem. In this method, we formulate the metro line expansion as a Markov decision process (MDP), which characterizes the problem as a process of sequential station selection. Then, we train an actor-critic model to design the next metro line on the basis of the existing metro network. The actor is an encoder-decoder network with an attention mechanism to generate the parameterized policy which is used to select the stations. The critic estimates the expected cumulative reward to assist the training of the actor by reducing training variance. The proposed method does not require expert guidance during design, since the learning procedure only relies on the reward calculation to tune the policy for better station selection. Also, it avoids the difficulty of heuristics designing by the policy formalizing the station selection. Considering origin-destination (OD) trips and social equity, we expand the current metro network in Xi'an, China, based on the real mobility information of 24,770,715 mobile phone users in the whole city. The results demonstrate the advantages of our method compared with existing approaches.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2646–2656},
numpages = {11},
keywords = {metro network expansion, social equity, reinforcement learning, actor-critic model},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/2903220.2903257,
author = {Kofinas, Panagiotis and Vouros, George and Dounis, Anastasios I.},
title = {Energy Management in Solar Microgrid via Reinforcement Learning},
year = {2016},
isbn = {9781450337342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2903220.2903257},
doi = {10.1145/2903220.2903257},
abstract = {This paper proposes a single agent system towards solving energy management issues in solar microgrids. The system considered consists of a Photovoltaic (PV) source, a battery bank, a desalination unit (responsible for providing the demanded water) and a local consumer. The trade-offs and complexities involved in the operation of the different units, and the quality of services' demanded from energy consumer units (e.g. the desalination unit), makes the energy management a challenging task. The goal of the agent is to satisfy the energy demand in the solar microgrid, optimizing the battery usage, in conjunction to satisfying the quality of services provided. It is assumed that the solar microgrid operates in island-mode. Thus, no connection to the electrical grid is considered. The agent collects data from the elements of the system and learns the suitable policy towards optimizing system performance. Simulation results provided, show the performance of the agent.},
booktitle = {Proceedings of the 9th Hellenic Conference on Artificial Intelligence},
articleno = {12},
numpages = {7},
keywords = {Q-learning, energy management, Reinforcement learning, microgrid},
location = {Thessaloniki, Greece},
series = {SETN '16}
}

@inproceedings{10.1145/1082473.1082794,
author = {Whiteson, Shimon},
title = {Improving Reinforcement Learning Function Approximators via Neuroevolution},
year = {2005},
isbn = {1595930930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1082473.1082794},
doi = {10.1145/1082473.1082794},
abstract = {Reinforcement learning problems are commonly tackled with temporal difference methods, which estimate the long-term value of taking each action in each state. In most problems of real-world interest, learning this value function requires a function approximator. However, the feasibility of using function approximators depends on the ability of the human designer to select an appropriate representation for the value function. My thesis presents a new approach to function approximation that automates some of these difficult design choices by coupling temporal difference methods with policy search methods such as evolutionary computation. It also presents a particular implementation which combines NEAT, a neuroevolutionary policy search method, and Q-learning, a popular temporal difference method, to yield a new method called NEAT+Q that automatically learns effective representations for neural network function approximators. Empirical results in a server job scheduling task demonstrate that NEAT+Q can outperform both NEAT and Q-learning with manually designed neural networks.},
booktitle = {Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems},
pages = {1386},
numpages = {1},
keywords = {neural networks, genetic algorithms, reinforcement learning},
location = {The Netherlands},
series = {AAMAS '05}
}

@article{10.5555/1248547.1248612,
author = {Kok, Jelle R. and Vlassis, Nikos},
title = {Collaborative Multiagent Reinforcement Learning by Payoff Propagation},
year = {2006},
issue_date = {12/1/2006},
publisher = {JMLR.org},
volume = {7},
issn = {1532-4435},
abstract = {In this article we describe a set of scalable techniques for learning the behavior of a group of agents in a collaborative multiagent setting. As a basis we use the framework of coordination graphs of Guestrin, Koller, and Parr (2002a) which exploits the dependencies between agents to decompose the global payoff function into a sum of local terms. First, we deal with the single-state case and describe a payoff propagation algorithm that computes the individual actions that approximately maximize the global payoff function. The method can be viewed as the decision-making analogue of belief propagation in Bayesian networks. Second, we focus on learning the behavior of the agents in sequential decision-making tasks. We introduce different model-free reinforcement-learning techniques, unitedly called Sparse Cooperative Q-learning, which approximate the global action-value function based on the topology of a coordination graph, and perform updates using the contribution of the individual agents to the maximal global action value. The combined use of an edge-based decomposition of the action-value function and the payoff propagation algorithm for efficient action selection, result in an approach that scales only linearly in the problem size. We provide experimental evidence that our method outperforms related multiagent reinforcement-learning methods based on temporal differences.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {1789–1828},
numpages = {40}
}

@inproceedings{10.5555/3237383.3237850,
author = {Silva, Felipe Leno Da and Costa, Anna Helena Reali},
title = {Object-Oriented Curriculum Generation for Reinforcement Learning},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Autonomously learning a complex task takes a very long time for Reinforcement Learning (RL) agents. One way to learn faster is by dividing a complex task into several simple subtasks and organizing them into a Curriculum that guides Transfer Learning (TL) methods to reuse knowledge in a convenient sequence. However, previous works do not take into account the TL method to build specialized Curricula , leaving the burden of a careful subtask selection to a human. We here contribute novel procedures for: (i) dividing the target task into simpler ones under minimal human supervision; (ii) automatically generating Curricula based on object-oriented task descriptions; and (iii) using generated Curricula for reusing knowledge across tasks. Our experiments show that our proposal achieves a better performance using both manually given and generated subtasks when compared to the state-of-the-art technique in two different domains.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1026–1034},
numpages = {9},
keywords = {reinforcement learning, curriculum learning, transfer learning},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/3317640.3317658,
author = {Wang, Lei and Zhu, Yaping and Pan, Hong},
title = {Unsupervised Reinforcement Learning For Video Summarization Reward Function},
year = {2019},
isbn = {9781450361750},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3317640.3317658},
doi = {10.1145/3317640.3317658},
abstract = {We propose a new reward function based on Deep Summarization Network (DSN), which is used to synthesize short video summaries to facilitate large-scale browsing of videos. The DSN uses the video summarization as a process of sequential decision making, predicting the probability of each video frame to indicate the likelihood that the video frame is selected, and then selecting the frame based on the probability distribution to form video summaries. By designing a new DSN reward function, the rewards for representative and diversity rewards are higher, and a large number of experiments are performed on the two benchmark datasets, demonstrating that our summary network is significantly better than existing unsupervised video summaries.},
booktitle = {Proceedings of the 2019 International Conference on Image, Video and Signal Processing},
pages = {40–44},
numpages = {5},
keywords = {Bi-directional Long Short-Term Memory, Deep Summarization Network, Convolutional Neural Network, Video Summarization, Deep Reinforcement Learning},
location = {Shanghai, China},
series = {IVSP '19}
}

@inproceedings{10.5555/3463952.3463994,
author = {Chen, Kangjie and Guo, Shangwei and Zhang, Tianwei and Li, Shuxin and Liu, Yang},
title = {Temporal Watermarks for Deep Reinforcement Learning Models},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Watermarking has become a popular and attractive technique to protect the Intellectual Property (IP) of Deep Learning (DL) models. However, very few studies explore the possibility of watermarking Deep Reinforcement Learning (DRL) models. Common approaches in the DL context embed backdoors into the protected model and use special samples to verify the model ownership. These solutions are easy to be detected, and can potentially affect the performance and behaviors of the target model. Such limitations make existing solutions less applicable to safety- and security-critical tasks and scenarios, where DRL has been widely used.In this work, we propose a novel watermarking scheme for DRL protection. Instead of using spatial watermarks as in DL models, we introduce temporal watermarks, which can reduce the potential impact and damage to the target model, while achieving ownership verification with high fidelity. Specifically, (1) we design a new damage metric to select sequential states for watermark generation; (2) we introduce a new reward function to efficiently alter the model's behaviors for watermark embedding; (3) we propose to utilize a predefined probability density function of actions over the watermark states as the verification evidence. The integration of these techniques enables a DRL model owner to embed the watermarks for ownership verification and IP protection. Our method is general and can be applied to various DRL tasks with either deterministic or stochastic reinforcement learning algorithms. Extensive experimental results show that it can effectively preserve the functionality of DRL models and exhibit significant robustness against common model modifications, e.g., fine-tuning and model compression.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {314–322},
numpages = {9},
keywords = {intellectual property, watermarking, deep reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1109/WI-IAT.2011.44,
author = {Kemmerich, Thomas and Buning, Hans Kleine},
title = {Coordination in Large Multiagent Reinforcement Learning Problems},
year = {2011},
isbn = {9780769545134},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2011.44},
doi = {10.1109/WI-IAT.2011.44},
abstract = {Large distributed systems often require intelligent behavior. Although multiagent reinforcement learning can be applied to such systems, several yet unsolved challenges arise due to the large number of simultaneous learners. Among others, these include exponential growth of state-action spaces and coordination. In this work, we deal with these two issues. Therefore, we consider a subclass of stochastic games called cooperative sequential stage games. With the help of a stateless distributed learning algorithm we solve the problem of growing state-action spaces. Then, we present six different techniques to coordinate action selection during the learning process. We prove a property of the learning algorithm that helps to reduce computational costs of one technique. An experimental analysis in a distributed agent partitioning problem with hundreds of agents reveals that the proposed techniques can lead to higher quality solutions and increase convergence speed compared to the basic approach. Some techniques even outperform a state-of-the-art special purpose approach.},
booktitle = {Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {236–239},
numpages = {4},
keywords = {Coordination, Cooperative Stochastic Games, Multiagent Reinforcement Learning},
series = {WI-IAT '11}
}

@inproceedings{10.5555/1697236.1697284,
author = {Zhang, Lidan and Chan, Kwok Ping},
title = {Dependency Parsing with Energy-Based Reinforcement Learning},
year = {2009},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We present a model which integrates dependency parsing with reinforcement learning based on Markov decision process. At each time step, a transition is picked up to construct the dependency tree in terms of the long-run reward. The optimal policy for choosing transitions can be found with the SARSA algorithm. In SARSA, an approximation of the state-action function can be obtained by calculating the negative free energies for the Restricted Boltzmann Machine. The experimental results on CoNLL-X multilingual data show that the proposed model achieves comparable results with the current state-of-the-art methods.},
booktitle = {Proceedings of the 11th International Conference on Parsing Technologies},
pages = {234–237},
numpages = {4},
location = {Paris, France},
series = {IWPT '09}
}

@inproceedings{10.1145/3394486.3403135,
author = {Dou, Yingtong and Ma, Guixiang and Yu, Philip S. and Xie, Sihong},
title = {Robust Spammer Detection by Nash Reinforcement Learning},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403135},
doi = {10.1145/3394486.3403135},
abstract = {Online reviews provide product evaluations for customers to make decisions. Unfortunately, the evaluations can be manipulated using fake reviews ("spams") by professional spammers, who have learned increasingly insidious and powerful spamming strategies by adapting to the deployed detectors. Spamming strategies are hard to capture, as they can be varying quickly along time, different across spammers and target products, and more critically, remained unknown in most cases. Furthermore, most existing detectors focus on detection accuracy, which is not well-aligned with the goal of maintaining the trustworthiness of product evaluations. To address the challenges, we formulate a minimax game where the spammers and spam detectors compete with each other on their practical goals that are not solely based on detection accuracy. Nash equilibria of the game lead to stable detectors that are agnostic to any mixed detection strategies. However, the game has no closed-form solution and is not differentiable to admit the typical gradient-based algorithms. We turn the game into two dependent Markov Decision Processes (MDPs) to allow efficient stochastic optimization based on multi-armed bandit and policy gradient. We experiment on three large review datasets using various state-of-the-art spamming and detection strategies and show that the optimization algorithm can reliably find an equilibrial detector that can robustly and effectively prevent spammers with any mixed spamming strategies from attaining their practical goal. Our code is available at https://github.com/YingtongDou/Nash-Detect.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {924–933},
numpages = {10},
keywords = {spam detection, adversarial learning, reinforcement learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3577530.3577561,
author = {Yu, Mengfei and Zheng, Zheng and Zeng, Delu},
title = {Continuous Self-Adaptive Calibration by Reinforcement Learning},
year = {2023},
isbn = {9781450397773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577530.3577561},
doi = {10.1145/3577530.3577561},
abstract = {It is well-known that hand-eye calibration plays an important role in the application of vision-based robot systems. Despite traditional calibration methods achieved huge success, the reduction in calibration accuracy whenever the relative hand-eye position changes reflects the fact that such methods are only suitable for scenarios where the components of the robot system are relatively fixed. To tackle this problem, a continuous self-adaptive calibration approach is proposed by applying the deep reinforcement learning algorithm to the calibration task. The experimental results demonstrate that our method can calibrate accurately in more flexible situations where the relative position of the hand and eye changes frequently.},
booktitle = {Proceedings of the 2022 6th International Conference on Computer Science and Artificial Intelligence},
pages = {189–194},
numpages = {6},
keywords = {Calibration, Continuous self-adaptive, Deep reinforcement learning},
location = {Beijing, China},
series = {CSAI '22}
}

@inproceedings{10.1145/2983323.2983379,
author = {Han, Miyoung and Senellart, Pierre and Bressan, St\'{e}phane and Wu, Huayu},
title = {Routing an Autonomous Taxi with Reinforcement Learning},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983379},
doi = {10.1145/2983323.2983379},
abstract = {Singapore's vision of a Smart Nation encompasses the development of effective and efficient means of transportation. The government's target is to leverage new technologies to create services for a demand-driven intelligent transportation model including personal vehicles, public transport, and taxis. Singapore's government is strongly encouraging and supporting research and development of technologies for autonomous vehicles in general and autonomous taxis in particular. The design and implementation of intelligent routing algorithms is one of the keys to the deployment of autonomous taxis. In this paper we demonstrate that a reinforcement learning algorithm of the Q-learning family, based on a customized exploration and exploitation strategy, is able to learn optimal actions for the routing autonomous taxis in a real scenario at the scale of the city of Singapore with pick-up and drop-off events for a fleet of one thousand taxis.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {2421–2424},
numpages = {4},
keywords = {exploration, reinforcement learning},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.1145/1835804.1835817,
author = {Abe, Naoki and Melville, Prem and Pendus, Cezar and Reddy, Chandan K. and Jensen, David L. and Thomas, Vince P. and Bennett, James J. and Anderson, Gary F. and Cooley, Brent R. and Kowalczyk, Melissa and Domick, Mark and Gardinier, Timothy},
title = {Optimizing Debt Collections Using Constrained Reinforcement Learning},
year = {2010},
isbn = {9781450300551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835804.1835817},
doi = {10.1145/1835804.1835817},
abstract = {The problem of optimally managing the collections process by taxation authorities is one of prime importance, not only for the revenue it brings but also as a means to administer a fair taxing system. The analogous problem of debt collections management in the private sector, such as banks and credit card companies, is also increasingly gaining attention. With the recent successes in the applications of data analytics and optimization to various business areas, the question arises to what extent such collections processes can be improved by use of leading edge data modeling and optimization techniques. In this paper, we propose and develop a novel approach to this problem based on the framework of constrained Markov Decision Process (MDP), and report on our experience in an actual deployment of a tax collections optimization system at New York State Department of Taxation and Finance (NYS DTF).},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {75–84},
numpages = {10},
keywords = {business analytics and optimization, constrained markov decision process, debt collection optimization, reinforcement learning},
location = {Washington, DC, USA},
series = {KDD '10}
}

@inproceedings{10.1145/3447555.3464853,
author = {Agwan, Utkarsha and Spangher, Lucas and Arnold, William and Srivastava, Tarang and Poolla, Kameshwar and Spanos, Costas J.},
title = {Pricing in Prosumer Aggregations Using Reinforcement Learning},
year = {2021},
isbn = {9781450383332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447555.3464853},
doi = {10.1145/3447555.3464853},
abstract = {Prosumers with generation and storage capabilities can supply energy back to the grid, or trade their surplus with other prosumers for their mutual benefit. A prosumer aggregation that facilitates such trades will price the energy being traded to achieve an objective such as profit maximization, social welfare, or market equilibrium. We propose the use of reinforcement learning to design a transactive controller to price energy in a prosumer aggregation. This has an advantage over other decentralized pricing mechanisms as it does not rely on iterative price settlement or load estimation by prosumers, and estimates the price in a day ahead manner. We present numerical case studies to evaluate our controller, and discuss extensions to implement this in real prosumer aggregations.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Future Energy Systems},
pages = {220–224},
numpages = {5},
location = {Virtual Event, Italy},
series = {e-Energy '21}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00058,
author = {Yerushalmi, Raz},
title = {Enhancing Deep Reinforcement Learning with Executable Specifications},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00058},
doi = {10.1109/ICSE-Companion58688.2023.00058},
abstract = {Deep reinforcement learning (DRL) has become a dominant paradigm for using deep learning to carry out tasks where complex policies are learned for reactive systems. However, these policies are "black-boxes", e.g., opaque to humans and known to be susceptible to bugs. For example, it is hard --- if not impossible --- to guarantee that the trained DRL agent adheres to specific safety and fairness properties that may be required. This doctoral dissertation's first and primary contribution is a novel approach to developing DRL agents, which will improve the DRL training process by pushing the learned policy toward high performance on its main task and compliance with such safety and fairness properties, guaranteeing a high probability of compliance while not compromising the performance of the resulting agent. The approach is realized by incorporating domain-specific knowledge captured as key properties defined by domain experts directly into the DRL optimization process while leveraging behavioral languages that are natural to the domain experts. We have validated the proposed approach by extending the AI-Gym Python framework [1] for training DRL agents and integrating it with the BP-Py framework [2] for specifying scenario-based models [3] in a way that allows scenario objects to affect the training process through reward and cost functions, demonstrating dramatic improvement in the safety and performance of the agent. In addition, we have validated the resulting DRL agents using the Marabou verifier [4], confirming that the resulting agents indeed comply (in full) with the required safety and fairness properties. We have applied the approach, training DRL agents for use cases from network communication and robotic navigation domains, exhibiting strong results. A second contribution of this doctoral dissertation is to develop and leverage probabilistic verification methods for deep neural networks to overcome the current scalability limitations of neural network verification technology, limiting the applicability of verification to practical DRL agents. We carried out an initial validation of the concept in the domain of image classification, showing promising results.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {213–217},
numpages = {5},
keywords = {domain expertise, scenario-based modeling, rule-based specifications, machine learning, deep reinforcement learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3281032,
author = {Mu, Ting-Yu and Al-Fuqaha, Ala and Shuaib, Khaled and Sallabi, Farag M. and Qadir, Junaid},
title = {SDN Flow Entry Management Using Reinforcement Learning},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3281032},
doi = {10.1145/3281032},
abstract = {Modern information technology services largely depend on cloud infrastructures to provide their services. These cloud infrastructures are built on top of Datacenter Networks (DCNs) constructed with high-speed links, fast switching gear, and redundancy to offer better flexibility and resiliency. In this environment, network traffic includes long-lived (elephant) and short-lived (mice) flows with partitioned/aggregated traffic patterns. Although SDN-based approaches can efficiently allocate networking resources for such flows, the overhead due to network reconfiguration can be significant. With limited capacity of Ternary Content-Addressable Memory (TCAM) deployed in an OpenFlow enabled switch, it is crucial to determine which forwarding rules should remain in the flow table and which rules should be processed by the SDN controller in case of a table-miss on the SDN switch. This is needed in order to obtain the flow entries that satisfy the goal of reducing the long-term control plane overhead introduced between the controller and the switches. To achieve this goal, we propose a machine learning technique that utilizes two variations of Reinforcement Learning (RL) algorithms—the first of which is a traditional RL-based algorithm, while the other is deep reinforcement learning-based. Emulation results using the RL algorithm show around 60\% improvement in reducing the long-term control plane overhead and around 14\% improvement in the table-hit ratio compared to the Multiple Bloom Filters (MBF) method, given a fixed size flow table of 4KB.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {nov},
articleno = {11},
numpages = {23},
keywords = {elephant and mice flows, big data, machine learning, reinforcement learning, Flow entry, ternary content addressable memory, software defined networking (SDN), Openflow, MiniNet}
}

@inproceedings{10.5555/3306127.3332095,
author = {Zhang, Jianyu and Hao, Jianye and Fogelman-Souli\'{e}, Fran\c{c}oise and Wang, Zan},
title = {Automatic Feature Engineering by Deep Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We present a framework calledLearning Automatic Feature Engineering Machine (LAFEM), which formalizes theFeature Engineering (FE) problem as an optimization problem over aHeterogeneous Transformation Graph (HTG). We propose a Deep Q-learning on HTG to support efficient learning of fine-grained and generalized FE policies that can transfer knowledge of engineering "good" features from a collection of datasets to other unseen datasets.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2312–2314},
numpages = {3},
keywords = {feature generation, innovative agents and multiagent applications, deep learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.5555/2567709.2567728,
author = {B\"{o}hmer, Wendelin and Gr\"{u}new\"{a}lder, Steffen and Shen, Yun and Musial, Marek and Obermayer, Klaus},
title = {Construction of Approximation Spaces for Reinforcement Learning},
year = {2013},
issue_date = {January 2013},
publisher = {JMLR.org},
volume = {14},
number = {1},
issn = {1532-4435},
abstract = {Linear reinforcement learning (RL) algorithms like least-squares temporal difference learning (LSTD) require basis functions that span approximation spaces of potential value functions. This article investigates methods to construct these bases from samples. We hypothesize that an ideal approximation spaces should encode diffusion distances and that slow feature analysis (SFA) constructs such spaces. To validate our hypothesis we provide theoretical statements about the LSTD value approximation error and induced metric of approximation spaces constructed by SFA and the state-of-the-art methods Krylov bases and proto-value functions (PVF). In particular, we prove that SFA minimizes the average (over all tasks in the same environment) bound on the above approximation error. Compared to other methods, SFA is very sensitive to sampling and can sometimes fail to encode the whole state space. We derive a novel importance sampling modification to compensate for this effect. Finally, the LSTD and least squares policy iteration (LSPI) performance of approximation spaces constructed by Krylov bases, PVF, SFA and PCA is compared in benchmark tasks and a visual robot navigation experiment (both in a realistic simulation and with a robot). The results support our hypothesis and suggest that (i) SFA provides subspace-invariant features for MDPs with self-adjoint transition operators, which allows strong guarantees on the approximation error, (ii) the modified SFA algorithm is best suited for LSPI in both discrete and continuous state spaces and (iii) approximation spaces encoding diffusion distances facilitate LSPI performance.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2067–2118},
numpages = {52},
keywords = {visual robot navigation, proto value functions, least-squares policy iteration, slow feature analysis, diffusion distance, reinforcement learning}
}

@inproceedings{10.1145/1390156.1390259,
author = {Reisinger, Joseph and Stone, Peter and Miikkulainen, Risto},
title = {Online Kernel Selection for Bayesian Reinforcement Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390259},
doi = {10.1145/1390156.1390259},
abstract = {Kernel-based Bayesian methods for Reinforcement Learning (RL) such as Gaussian Process Temporal Difference (GPTD) are particularly promising because they rigorously treat uncertainty in the value function and make it easy to specify prior knowledge. However, the choice of prior distribution significantly affects the empirical performance of the learning agent, and little work has been done extending existing methods for prior model selection to the online setting. This paper develops Replacing-Kernel RL, an online model selection method for GPTD using sequential Monte-Carlo methods. Replacing-Kernel RL is compared to standard GPTD and tile-coding on several RL domains, and is shown to yield significantly better asymptotic performance for many different kernel families. Furthermore, the resulting kernels capture an intuitively useful notion of prior state covariance that may nevertheless be difficult to capture manually.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {816–823},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/3503823.3503905,
author = {Sidiropoulos, George and Kiourt, Chairi and Sevetlidis, Vasileios and Pavlidis, George},
title = {Shaping the Behavior of Reinforcement Learning Agents},
year = {2022},
isbn = {9781450395557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503823.3503905},
doi = {10.1145/3503823.3503905},
abstract = {With the advent of machine learning and agent-based approaches, behavior-shaping in environments composed of several autonomous entities has become a popular and active research field for the development of unique realistic behaviors. Realistic simulations have been particularly studied in the fields of crowd management, swarm behavior analysis and civilization simulation. In this study, we present a new dynamic rewarding approach for shaping the behavior of reinforcement learning agents in mixed (cooperative and competitive) multi-agent environments. The evaluation of the proposed rewarding approach is tested in a developed 3D environment of two groups of ancient Greek warriors fighting inside an octagonal arena, testing different agent behaviors in various scenarios. Interestingly, the results reveal that the trained agents’ behaviors vary based on the situations and the constraints of the environment, resembling realistic behavior variations.},
booktitle = {Proceedings of the 25th Pan-Hellenic Conference on Informatics},
pages = {448–453},
numpages = {6},
keywords = {multi-agent environments, reinforcement learning, self-playing agents, behavior-shaping},
location = {Volos, Greece},
series = {PCI '21}
}

@article{10.5555/2188385.2343689,
author = {Konidaris, George and Scheidwasser, Ilya and Barto, Andrew G.},
title = {Transfer in Reinforcement Learning via Shared Features},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
number = {null},
issn = {1532-4435},
abstract = {We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills.},
journal = {J. Mach. Learn. Res.},
month = {may},
pages = {1333–1371},
numpages = {39},
keywords = {shaping, reinforcement learning, transfer, skills}
}

@inproceedings{10.5555/3545946.3599102,
author = {Afzal, Mohammad and Gambhir, Sankalp and Gupta, Ashutosh and S, Krishna and Trivedi, Ashutosh and Velasquez, Alvaro},
title = {LTL-Based Non-Markovian Inverse Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The successes of reinforcement learning in recent years are underpinned by the characterization of suitable reward functions. However, in settings where such rewards are non-intuitive, difficult to define, or otherwise error-prone in their definition, it is useful to instead learn the reward signal from expert demonstrations. This is the crux of inverse reinforcement learning (IRL). While eliciting learning requirements in the form of scalar reward signals has been shown to be effective, such representations lack explainability and lead to opaque learning. We aim to mitigate this situation by presenting a novel IRL method for eliciting declarative learning requirements in the form of a popular formal logic---Linear Temporal Logic (LTL)---from a set of traces given by the expert policy.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2857–2859},
numpages = {3},
keywords = {inverse reinforcement learning, linear temporal logic, constraints optimization},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.1145/2766910,
author = {Peng, Xue Bin and Berseth, Glen and van de Panne, Michiel},
title = {Dynamic Terrain Traversal Skills Using Reinforcement Learning},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2766910},
doi = {10.1145/2766910},
abstract = {The locomotion skills developed for physics-based characters most often target flat terrain. However, much of their potential lies with the creation of dynamic, momentum-based motions across more complex terrains. In this paper, we learn controllers that allow simulated characters to traverse terrains with gaps, steps, and walls using highly dynamic gaits. This is achieved using reinforcement learning, with careful attention given to the action representation, non-parametric approximation of both the value function and the policy; epsilon-greedy exploration; and the learning of a good state distance metric. The methods enable a 21-link planar dog and a 7-link planar biped to navigate challenging sequences of terrain using bounding and running gaits. We evaluate the impact of the key features of our skill learning pipeline on the resulting performance.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {80},
numpages = {11},
keywords = {physics simulation, computer animation}
}

@inproceedings{10.5555/2484920.2485265,
author = {Kraemer, Landon},
title = {Reinforcement Learning for Decentralized Planning under Uncertainty},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Decentralized partially-observable Markov decision processes (Dec-POMDPs) are a powerful tool for modeling multi-agent planning and decision-making under uncertainty. Prevalent Dec-POMDP solution techniques require centralized computation given full knowledge of the underlying model. But in real world scenarios, model parameters may not be known a priori, or may be difficult to specify. We propose to address these limitations with distributed reinforcement learning (RL).},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1439–1440},
numpages = {2},
keywords = {multi-agent reinforcement learning, decentralized partially-observable markov decision processes},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.5555/3398761.3398951,
author = {Zheng, Han and Jiang, Jing and Wei, Pengfei and Long, Guodong and Zhang, Chengqi},
title = {Competitive and Cooperative Heterogeneous Deep Reinforcement Learning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Numerous deep reinforcement learning methods have been proposed, including deterministic, stochastic, and evolutionary-based hybrid methods. However, among these various methodologies, there is no clear winner that consistently outperforms the others in every task in terms of effective exploration, sample efficiency, and stability. In this work, we present a competitive and cooperative heterogeneous deep reinforcement learning framework called C2HRL. C2HRL aims to learn a superior agent that exceeds the capabilities of the individual agent in an agent pool through two agent management mechanisms: one competitive, the other cooperative. The competitive mechanism forces agents to compete for computing resources and to explore and exploit diverse regions of the solution space. To support this strategy, resources are distributed to the most suitable agent for that specific task and random seed setting, which results in better sample efficiency and stability. The other mechanic, cooperation, asks heterogeneous agents to share their exploration experiences so that all agents can learn from a diverse set of policies. The experiences are stored in a two-level replay buffer and the result is an overall more effective exploration strategy. We evaluated C2HRL on a range of continuous control tasks from the benchmark Mujoco. The experimental results demonstrate that C2HRL has better sample efficiency and greater stability than three state-of-the-art DRL baselines.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1656–1664},
numpages = {9},
keywords = {competition and cooperation, heterogeneous agents, deep reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3394486.3403089,
author = {Huai, Mengdi and Sun, Jianhui and Cai, Renqin and Yao, Liuyi and Zhang, Aidong},
title = {Malicious Attacks against Deep Reinforcement Learning Interpretations},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403089},
doi = {10.1145/3394486.3403089},
abstract = {The past years have witnessed the rapid development of deep reinforcement learning (DRL), which is a combination of deep learning and reinforcement learning (RL). However, the adoption of deep neural networks makes the decision-making process of DRL opaque and lacking transparency. Motivated by this, various interpretation methods for DRL have been proposed. However, those interpretation methods make an implicit assumption that they are performed in a reliable and secure environment. In practice, sequential agent-environment interactions expose the DRL algorithms and their corresponding downstream interpretations to extra adversarial risk. In spite of the prevalence of malicious attacks, there is no existing work studying the possibility and feasibility of malicious attacks against DRL interpretations. To bridge this gap, in this paper, we investigate the vulnerability of DRL interpretation methods. Specifically, we introduce the first study of the adversarial attacks against DRL interpretations, and propose an optimization framework based on which the optimal adversarial attack strategy can be derived. In addition, we study the vulnerability of DRL interpretation methods to the model poisoning attacks, and present an algorithmic framework to rigorously formulate the proposed model poisoning attack. Finally, we conduct both theoretical analysis and extensive experiments to validate the effectiveness of the proposed malicious attacks against DRL interpretations.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {472–482},
numpages = {11},
keywords = {adversarial attacks, deep reinforcement learning, poisoning attacks, model interpretation},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1109/MODELS-C.2019.00030,
author = {Barriga, Angela and Rutle, Adrian and Heldal, Rogardt},
title = {Personalized and Automatic Model Repairing Using Reinforcement Learning},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00030},
doi = {10.1109/MODELS-C.2019.00030},
abstract = {When performing modeling activities, the chances of breaking a model increase together with the size of development teams and number of changes in software specifications. Model repair research mostly proposes two different solutions to this issue: fully automatic, non-interactive model repairing tools or support systems where the repairing choice is left to the developer's criteria. In this paper, we propose the use of reinforcement learning algorithms to achieve the repair of broken models allowing both automation and personalization. We validate our proposal by repairing a large set of broken models randomly generated with a mutation tool.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems},
pages = {175–181},
numpages = {7},
keywords = {model repair, reinforcement learning, personalization},
location = {Munich, Germany},
series = {MODELS '19}
}

@inproceedings{10.1145/3397271.3401147,
author = {Xin, Xin and Karatzoglou, Alexandros and Arapakis, Ioannis and Jose, Joemon M.},
title = {Self-Supervised Reinforcement Learning for Recommender Systems},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401147},
doi = {10.1145/3397271.3401147},
abstract = {In session-based or sequential recommendation, it is important to consider a number of factors like long-term user engagement, multiple types of user-item interactions such as clicks, purchases etc. The current state-of-the-art supervised approaches fail to model them appropriately. Casting sequential recommendation task as a reinforcement learning (RL) problem is a promising direction. A major component of RL approaches is to train the agent through interactions with the environment. However, it is often problematic to train a recommender in an on-line fashion due to the requirement to expose users to irrelevant recommendations. As a result, learning the policy from logged implicit feedback is of vital importance, which is challenging due to the pure off-policy setting and lack of negative rewards (feedback).In this paper, we propose self-supervised reinforcement learning for sequential recommendation tasks. Our approach augments standard recommendation models with two output layers: one for self-supervised learning and the other for RL. The RL part acts as a regularizer to drive the supervised layer focusing on specific rewards (e.g., recommending items which may lead to purchases rather than clicks) while the self-supervised layer with cross-entropy loss provides strong gradient signals for parameter updates. Based on such an approach, we propose two frameworks namely Self-Supervised Q-learning (SQN) and Self-Supervised Actor-Critic (SAC). We integrate the proposed frameworks with four state-of-the-art recommendation models. Experimental results on two real-world datasets demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {931–940},
numpages = {10},
keywords = {self-supervised learning, sequential recommendation, Q-learning, session-based recommendation, reinforcement learning},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1145/3449356,
author = {Balakrishnan, Aravind and Lee, Jaeyoung and Gaurav, Ashish and Czarnecki, Krzysztof and Sedwards, Sean},
title = {Transfer Reinforcement Learning for Autonomous Driving: From WiseMove to WiseSim},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/3449356},
doi = {10.1145/3449356},
abstract = {Reinforcement learning (RL) is an attractive way to implement high-level decision-making policies for autonomous driving, but learning directly from a real vehicle or a high-fidelity simulator is variously infeasible. We therefore consider the problem of transfer reinforcement learning and study how a policy learned in a simple environment using WiseMove can be transferred to our high-fidelity simulator, WiseMove. WiseMove is a framework to study safety and other aspects of RL for autonomous driving. WiseMove accurately reproduces the dynamics and software stack of our real vehicle. We find that the accurately modelled perception errors in WiseMove contribute the most to the transfer problem. These errors, when even naively modelled in WiseMove, provide an RL policy that performs better in WiseMove than a hand-crafted rule-based policy. Applying domain randomization to the environment in WiseMove yields an even better policy. The final RL policy reduces the failures due to perception errors from 10\% to 2.75\%. We also observe that the RL policy has significantly less reliance on velocity compared to the rule-based policy, having learned that its measurement is unreliable.},
journal = {ACM Trans. Model. Comput. Simul.},
month = {jul},
articleno = {15},
numpages = {26},
keywords = {autonomous driving, deep reinforcement learning, policy distillation, Transfer reinforcement learning}
}

@inproceedings{10.5555/2936924.2936998,
author = {Odom, Phillip and Natarajan, Sriraam},
title = {Active Advice Seeking for Inverse Reinforcement Learning},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Intelligent systems that interact with humans typically require input in the form of demonstrations and/or advice for optimal decision making. In more traditional systems, such interactions require detailed and tedious effort on the part of the human expert. Alternatively, active learning systems allow for incremental acquisition of the demonstrations from the human expert where the learning system generates the queries. However, active learning allows for only labeled examples as input, significantly restricting the interaction between expert and learning algorithm. Advice-based learning systems increase the expressiveness of the interaction, but typically require all the advice about the domain in advance. By combining active learning and advice-based learning, we consider the problem of actively soliciting human advice. We present the algorithm in an inverse reinforcement learning setting where the utilities are learned from demonstrations. We show empirically the contribution of a more expressive advice over traditional active learning approaches.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {512–520},
numpages = {9},
keywords = {inverse reinforcement learning, active advice seeking, advice-based learning},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.5555/2431518.2431822,
author = {Mattila, Ville and Virtanen, Kai},
title = {Scheduling Fighter Aircraft Maintenance with Reinforcement Learning},
year = {2011},
publisher = {Winter Simulation Conference},
abstract = {This paper presents two problem formulations for scheduling the maintenance of a fighter aircraft fleet under conflict operating conditions. In the first formulation, the average availability of aircraft is maximized by choosing when to start the maintenance of each aircraft. In the second formulation, the availability of aircraft is preserved above a specific target level by choosing to either perform or not perform each maintenance activity. Both formulations are cast as semi-Markov decision problems (SMDPs) that are solved using reinforcement learning (RL) techniques. As the solution, maintenance policies dependent on the states of the aircraft are obtained. Numerical experiments imply that RL is a viable approach for considering conflict time maintenance policies. The obtained solutions provide knowledge of efficient maintenance decisions and the level of readiness that can be maintained by the fleet.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2540–2551},
numpages = {12},
location = {Phoenix, Arizona},
series = {WSC '11}
}

@inproceedings{10.1145/3195970.3199855,
author = {Sadasivam, Shankar and Chen, Zhuo and Lee, Jinwon and Jain, Rajeev},
title = {Efficient Reinforcement Learning for Automating Human Decision-Making in SoC Design},
year = {2018},
isbn = {9781450357005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195970.3199855},
doi = {10.1145/3195970.3199855},
abstract = {The exponential growth in PVT corners due to Moore's law scaling, and the increasing demand for consumer applications and longer battery life in mobile devices, has ushered in significant cost and power-related challenges for designing and productizing mobile chips within a predictable schedule. Two main reasons for this are the reliance on human decision-making to achieve the desired performance within the target area and power budget, and significant increases in complexity of the human decision-making space. The problem is that to-date human design experience has not been replaced by design automation tools, and tasks requiring experience of past designs are still being performed manually.In this paper we investigate how machine learning may be applied to develop tools that learn from experience just like human designers, thus automating tasks that still require human intervention. The potential advantage of the machine learning approach is the ability to scale with increasing complexity and therefore hold the design-time constant with same manpower.Reinforcement Learning (RL) is a machine learning technique that allows us to mimic a human designers' ability to learn from experience and automate human decision-making, without loss in quality of the design, while making the design time independent of the complexity. In this paper we show how manual design tasks can be abstracted as RL problems. Based on the experience with applying RL to one of these problems, we show that RL can automatically achieve results similar to human designs, but in a predictable schedule. However, a major drawback is that the RL solution can require a prohibitively large number of iterations for training. If efficient training techniques can be developed for RL, it holds great promise to automate tasks requiring human experience. In this paper we present a Bayesian Optimization technique for reducing the RL training time.},
booktitle = {Proceedings of the 55th Annual Design Automation Conference},
articleno = {37},
numpages = {6},
location = {San Francisco, California},
series = {DAC '18}
}

@inproceedings{10.1145/3449726.3463171,
author = {Stork, J\"{o}rg and Zaefferer, Martin and Eisler, Nils and Tichelmann, Patrick and Bartz-Beielstein, Thomas and Eiben, A. E.},
title = {Behavior-Based Neuroevolutionary Training in Reinforcement Learning},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3463171},
doi = {10.1145/3449726.3463171},
abstract = {In addition to their undisputed success in solving classical optimization problems, neuroevolutionary and population-based algorithms have become an alternative to standard reinforcement learning methods. However, evolutionary methods often lack the sample efficiency of standard value-based methods that leverage gathered state and value experience. If reinforcement learning for real-world problems with significant resource cost is considered, sample efficiency is essential. The enhancement of evolutionary algorithms with experience exploiting methods is thus desired and promises valuable insights. This work presents a hybrid algorithm that combines topology-changing neuroevolutionary optimization with value-based reinforcement learning. We illustrate how the behavior of policies can be used to create distance and loss functions, which benefit from stored experiences and calculated state values. They allow us to model behavior and perform a directed search in the behavior space by gradient-free evolutionary algorithms and surrogate-based optimization. For this purpose, we consolidate different methods to generate and optimize agent policies, creating a diverse population. We exemplify the performance of our algorithm on standard benchmarks and a purpose-built real-world problem. Our results indicate that combining methods can enhance the sample efficiency and learning speed for evolutionary approaches.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1753–1761},
numpages = {9},
keywords = {neural networks, neuroevolution, surrogate optimization, reinforcement learning, evolutionary algorithms},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.1145/3167132.3167165,
author = {Bougie, Nicolas and Ichise, Ryutaro},
title = {Deep Reinforcement Learning Boosted by External Knowledge},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167165},
doi = {10.1145/3167132.3167165},
abstract = {Recent improvements in deep reinforcement learning have allowed to solve problems in many 2D domains such as Atari games. However, in complex 3D environments, numerous learning episodes are required which may be too time consuming or even impossible especially in real-world scenarios. We present a new architecture to combine external knowledge and deep reinforcement learning using only visual input. A key concept of our system is augmenting image input by adding environment feature information and combining two sources of decision. We evaluate the performances of our method in a 3D partially-observable environment from the Microsoft Malmo platform. Experimental evaluation exhibits higher performance and faster learning compared to a single reinforcement learning model.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {331–338},
numpages = {8},
keywords = {external knowledge, deep learning, object recognition, reinforcement learning, knowledge reasoning},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3588983.3596692,
author = {Chen, Samuel Yen-Chi},
title = {Quantum Reinforcement Learning for Quantum Architecture Search},
year = {2023},
isbn = {9798400701627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588983.3596692},
doi = {10.1145/3588983.3596692},
abstract = {This paper presents a quantum architecture search (QAS) framework using quantum reinforcement learning (QRL) to generate quantum gate sequences for multi-qubit GHZ states. The proposed framework employs the asynchronous advantage actor-critic (A3C) algorithm to optimize the QRL agent, which has access to Pauli-X, Y, Z expectation values and a predefined set of quantum operations. Our approach does not require any prior knowledge of quantum physics. The framework can be used with other QRL architectures or optimization methods to explore gate synthesis and compilation for various quantum states.},
booktitle = {Proceedings of the 2023 International Workshop on Quantum Classical Cooperative},
pages = {17–20},
numpages = {4},
keywords = {variational quantum circuits, quantum neural networks, reinforcement learning, quantum architecture search},
location = {Orlando, FL, USA},
series = {QCCC '23}
}

@inproceedings{10.1145/3240323.3240374,
author = {Zhao, Xiangyu and Xia, Long and Zhang, Liang and Ding, Zhuoye and Yin, Dawei and Tang, Jiliang},
title = {Deep Reinforcement Learning for Page-Wise Recommendations},
year = {2018},
isbn = {9781450359016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240323.3240374},
doi = {10.1145/3240323.3240374},
abstract = {Recommender systems can mitigate the information overload problem by suggesting users' personalized items. In real-world recommendations such as e-commerce, a typical interaction between the system and its users is - users are recommended a page of items and provide feedback; and then the system recommends a new page of items. To effectively capture such interaction for recommendations, we need to solve two key problems - (1) how to update recommending strategy according to user's real-time feedback, and 2) how to generate a page of items with proper display, which pose tremendous challenges to traditional recommender systems. In this paper, we study the problem of page-wise recommendations aiming to address aforementioned two challenges simultaneously. In particular, we propose a principled approach to jointly generate a set of complementary items and the corresponding strategy to display them in a 2-D page; and propose a novel page-wise recommendation framework based on deep reinforcement learning, DeepPage, which can optimize a page of items with proper display based on real-time feedback from users. The experimental results based on a real-world e-commerce dataset demonstrate the effectiveness of the proposed framework.},
booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
pages = {95–103},
numpages = {9},
keywords = {sequential preference, actor-critic, deep reinforcement learning, item display strategy, recommender systems},
location = {Vancouver, British Columbia, Canada},
series = {RecSys '18}
}

