@article{GAISELMANN2022100235,
title = {Deep reinforcement learning for gearshift controllers in automatic transmissions},
journal = {Array},
volume = {15},
pages = {100235},
year = {2022},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2022.100235},
url = {https://www.sciencedirect.com/science/article/pii/S2590005622000728},
author = {Gerd Gaiselmann and Stefan Altenburg and Stefan Studer and Steven Peters},
keywords = {Deep reinforcement learning, Sim-to-real transfer, Domain adaption, Automotive, Automatic transmission, Gear shifting},
abstract = {Control design for gearshifts in modern automotive automatic transmissions constitutes a challenging, time consuming task performed by highly trained experts. This is due to the fact that a variety of non-linear and partially observable systems need to be actuated, such that a comfortable shifting behavior is achieved within an sufficiently low shifting time. The presented approach leverages deep reinforcement learning (DRL) to control gear shifts, outperforming current state of the art controller performance. This requires formulating the shifting task as a Markov decision process by designing suitable action and observation spaces as well as a meaningful reward function. Due to the sample complexity of DRL methods, the control agents are trained in simulation and are subsequently transferred to a real transmission on a test bench. To successfully transfer DRL agents from simulation to reality, methods such as domain randomization and domain adaption leveraging evolutionary optimization are applied. To the best of the authors’ knowledge, this work is the first to successfully apply DRL for the closed loop control of a real world automotive automatic transmission of realistic complexity.}
}
@article{DUDARENKO2019445,
title = {Reinforcement Learning Approach for Navigation of Ground Robotic Platform in Statically and Dynamically Generated Environments},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {25},
pages = {445-450},
year = {2019},
note = {19th IFAC Conference on Technology, Culture and International Stability TECIS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.579},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319325005},
author = {Dmitry Dudarenko and Julia Rubtsova and Artem Kovalev and Oleg Sivchenko},
keywords = {Mobile Platform, Robotics, Machine Learning, Reinforcement Learning, Neural Networks, Procedural Generation},
abstract = {This paper considers robotic platform navigation in terms of logistics, movement and track routing within indoor environments. Smart navigation and platform routing using a neural network are investigated. The paper discusses environment modeling with Unity ML software suite in static (prefabricated) and dynamically generated environments. Along with reinforcement learning, a procedural generation approach and its possible industrial applications are considered. The proposed algorithm for environment generation is characterized by higher performance comparing to analogues and allows to avoid model overfitting.}
}
@article{GUEVARA2021108735,
title = {Optimization of steam injection in SAGD using reinforcement learning},
journal = {Journal of Petroleum Science and Engineering},
volume = {206},
pages = {108735},
year = {2021},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.108735},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521003958},
author = {J.L. Guevara and Rajan Patel and Japan Trivedi},
keywords = {Reinforcement learning, SAGD optimization, SARSA, Function approximation},
abstract = {Conventional steam injection strategies or policies in SAGD are typically not a result of a formal optimization process but rather empirically found. However, finding the steam injection policy that maximizes cumulative performance over the entire production horizon represents a major challenge due to the complex dynamics of the physical phenomena. To address this challenge one alternative is to use a reinforcement learning (RL) approach; here an agent is trained to find the optimal policy only by continuous interactions with the reservoir simulation model (environment). In this work, the RL-SARSA on-line on-policy learning algorithm is used to find the optimal steam injection policy and offer insight in the physics of the SAGD process. Net Present Value (NPV) is used as a performance measure of objective function, the action-value function is approximated using a stochastic gradient regression strategy, and the state vector is featurized using radial basis kernels. The action space is discrete, three (3) possible actions: increase/decrease/no change on the steam injection rate w.r.t the previous value, and the state space is continuous and is made up of cumulative values for water and oil production and steam injection. Additionally, the environment is represented by a one well pair reservoir simulation model built using data from a reservoir located in northern Alberta and a production horizon of 250 days is considered. For the considered case study, the agent showed a significant improvement on the cumulative NPV value after 200 episodes or simulations and the optimal policies are characterized by three distinct regions, (1) constant rate to increase the average reservoir pressure and allow the steam chamber reach the overburden, (2) sharp increase to increase average reservoir temperature and abrupt decrease and (3) constant minimum value to maintain the average reservoir temperature.}
}
@article{SYAFIIE20083568,
title = {Intelligent Control Based on Reinforcement Learning for Batch Thermal Sterilization of Canned Foods},
journal = {IFAC Proceedings Volumes},
volume = {41},
number = {2},
pages = {3568-3573},
year = {2008},
note = {17th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20080706-5-KR-1001.00603},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016395027},
author = {S. Syafiie and Carlos Vilas and Miriam R. Garcia and Fernando Tadeo and Antonio A. Alonso and Ernesto Martinez},
keywords = {Reinforcement Learning, Intelligent Process Control, Sterilization Process, Batch Process},
abstract = {A control technique based on Reinforcement Learning is proposed for controlling thermal sterilization of canned food. Without using an a-priori mathematical model of the process, the proposed Model-Free Learning Controller (MFLC) aims to follow a temperature profile during two stages of the process: first heating by manipulating the saturated steam valve and then cooling by opening the water valve) by learning. From the defined state-action space, the MFLC agent learns the environment interacting with the process batch to batch and then using a tabular state-action mapping. The results show the advantages of the proposed technique for this kind of processes.}
}
@article{ELALLID20227366,
title = {A Comprehensive Survey on the Application of Deep and Reinforcement Learning Approaches in Autonomous Driving},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {9},
pages = {7366-7390},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822000970},
author = {Badr Ben Elallid and Nabil Benamar and Abdelhakim Senhaji Hafid and Tajjeeddine Rachidi and Nabil Mrani},
keywords = {Autonomous driving, Deep learning, Reinforcement learning, Motion planning, Decision making, Vehicle control},
abstract = {Recent advances in Intelligent Transport Systems (ITS) and Artificial Intelligence (AI) have stimulated and paved the way toward the widespread introduction of Autonomous Vehicles (AVs). This has opened new opportunities for smart roads, intelligent traffic safety, and traveler comfort. Autonomous Vehicles have become a highly popular research topic in recent years because of their significant capability to reduce road accidents and human injuries. This paper is an attempt to survey all recent AI based techniques used to deal with major functions in AVs, namely scene understanding, motion planning, decision making, vehicle control, social behavior, and communication. Our survey focuses solely on deep learning and reinforcement learning based approaches; it does not include conventional (shallow) shallow based techniques, a subject that has been extensively investigated in the past. Our survey builds a taxonomy of DL and RL algorithms that have been used so far to bring solutions to the four main issues in autonomous driving. Finally, this survey highlights the open challenges and points out possible future research directions.}
}
@article{CHENG201997,
title = {An extensible approach for real-time bidding with model-free reinforcement learning},
journal = {Neurocomputing},
volume = {360},
pages = {97-106},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219308367},
author = {Yin Cheng and Luobao Zou and Zhiwei Zhuang and Jingwei Liu and Bin Xu and Weidong Zhang},
keywords = {Deep reinforcement learning, Model-free, Extensible approach, Real-time bidding},
abstract = {In this paper, we propose an extensible framework for model-free reinforcement learning (RL) for real-time bidding (RTB) in display advertising. This framework can be applied into both simple environments and extend to the comprehensive environment that the DSP bids for multiple advertisers at the same time. To process new information that is collected via real-time interaction with the environment, an extensible model is first introduced, which is based on the distribution of the recharging probability. Substantial effort is expended to alleviate the problem of the sparsity of the click signal with the reward function. The proposed scheme has high feasibility and can address dynamic environments in contrast to prior works, which assumed that the distribution of the feature vectors and the dealing price were already known. Furthermore, a fund-recharging mechanism is introduced for transforming the RTB model into an endless task, which allows the policy to be optimized in a farsighted rather than a myopic manner. Illustrative experiments on both the small- and large-scale real datasets demonstrate the state-of-the-art performance of the proposed framework for the issue of interest.}
}
@article{LEHNA2023100276,
title = {Managing power grids through topology actions: A comparative study between advanced rule-based and reinforcement learning agents},
journal = {Energy and AI},
volume = {14},
pages = {100276},
year = {2023},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2023.100276},
url = {https://www.sciencedirect.com/science/article/pii/S2666546823000484},
author = {Malte Lehna and Jan Viebahn and Antoine Marot and Sven Tomforde and Christoph Scholz},
keywords = {Deep reinforcement learning, Electricity grids, Learning to run a power network, Topology control, Proximal policy optimisation},
abstract = {The operation of electricity grids has become increasingly complex due to the current upheaval and the increase in renewable energy production. As a consequence, active grid management is reaching its limits with conventional approaches. In the context of the Learning to Run a Power Network (L2RPN) challenge, it has been shown that Reinforcement Learning (RL) is an efficient and reliable approach with considerable potential for automatic grid operation. In this article, we analyse the submitted agent from Binbinchen and provide novel strategies to improve the agent, both for the RL and the rule-based approach. The main improvement is a N-1 strategy, where we consider topology actions that keep the grid stable, even if one line is disconnected. More, we also propose a topology reversion to the original grid, which proved to be beneficial. The improvements are tested against reference approaches on the challenge test sets and are able to increase the performance of the rule-based agent by 27%. In direct comparison between rule-based and RL agent we find similar performance. However, the RL agent has a clear computational advantage. We also analyse the behaviour in an exemplary case in more detail to provide additional insights. Here, we observe that through the N-1 strategy, the actions of both the rule-based and the RL agent become more diversified.}
}
@article{ZHENG2024109628,
title = {Joint maintenance and spare part ordering from multiple suppliers for multicomponent systems using a deep reinforcement learning algorithm},
journal = {Reliability Engineering & System Safety},
volume = {241},
pages = {109628},
year = {2024},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109628},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023005422},
author = {Meimei Zheng and Zhiyun Su and Dong Wang and Ershun Pan},
keywords = {Joint optimization, Maintenance, Multiple suppliers, Reinforcement learning, Value iteration},
abstract = {This paper investigates the joint optimization of maintenance and spare part ordering from multiple suppliers for systems consisting of multiple components. When components degrade to poor conditions, they are replaced with spare parts if available. Spare parts can be purchased from multiple suppliers, each with distinct lead times, unit prices, and setup costs. To determine the optimal replacement and ordering decisions, this paper establishes a model through a Markov decision process and designs a value iteration algorithm to solve the model. However, the value iteration algorithm takes too much time to solve problems involving large numbers of components and suppliers. Thus, we design a hybrid deep reinforcement learning algorithm (HDRL) based on the reinforcement learning algorithm to solve large-scale problems. Numerical experiments are conducted to validate the effectiveness of the HDRL algorithm and analyze the joint decisions. The results show that, compared with the value iteration algorithm, the average cost gap is 3.86%, and the solving time can be reduced by at least 95.99% for systems with more than 2 suppliers and more than 3 components under the HDRL algorithm.}
}
@article{WU2011430,
title = {A novel multi-agent reinforcement learning approach for job scheduling in Grid computing},
journal = {Future Generation Computer Systems},
volume = {27},
number = {5},
pages = {430-439},
year = {2011},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2010.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X10002025},
author = {Jun Wu and Xin Xu and Pengcheng Zhang and Chunming Liu},
keywords = {Multi-agent reinforcement learning, Load balancing, Job scheduling, Grid computing, Resource allocation, Coordination},
abstract = {Grid computing utilizes distributed heterogeneous resources to support large-scale or complicated computing tasks, and an appropriate resource scheduling algorithm is fundamentally important for the success of Grid applications. Due to the complex and dynamic properties of Grid environments, traditional model-based methods may result in poor scheduling performance in practice. Scalability and adaptability are among the key objectives of Grid job scheduling. In this paper, a novel multi-agent reinforcement learning method, called ordinal sharing learning (OSL) method, is proposed for job scheduling problems, especially, for realizing load balancing in Grids. The approach circumvents the scalability problem by using an ordinal distributed learning strategy, and realizes multi-agent coordination based on an information-sharing mechanism with limited communication. Simulation results show that the OSL method can achieve the goal of load balancing effectively, and its performance is even comparable to some centralized scheduling algorithm in most cases. The convergence property and adaptability of the proposed method are also illustrated.}
}
@article{LEHNA2022100139,
title = {A Reinforcement Learning approach for the continuous electricity market of Germany: Trading from the perspective of a wind park operator},
journal = {Energy and AI},
volume = {8},
pages = {100139},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2022.100139},
url = {https://www.sciencedirect.com/science/article/pii/S2666546822000039},
author = {Malte Lehna and Björn Hoppmann and Christoph Scholz and René Heinrich},
keywords = {Deep Reinforcement Learning, German intraday electricity trading, Deep neural networks, Markov Decision Process, Proximal Policy Optimization, Electricity price forecast},
abstract = {With the rising extension of renewable energies, the intraday electricity markets have recorded a growing popularity amongst traders as well as electric utilities to cope with the induced volatility of the energy supply. Through their short trading horizon and continuous nature, the intraday markets offer the ability to adjust trading decisions from the day-ahead market or reduce trading risk in a short-term notice. Producers of renewable energies utilize the intraday market to lower their forecast risk, by modifying their provided capacities based on current forecasts. However, the market dynamics are complex due to the fact that the power grids have to remain stable and electricity is only partly storable. Consequently, robust and intelligent trading strategies are required that are capable to operate in the intraday market. In this work, we propose a novel autonomous trading approach based on Deep Reinforcement Learning (DRL) algorithms as a possible solution. For this purpose, we model the intraday trade as a Markov Decision Process (MDP) and employ the Proximal Policy Optimization (PPO) algorithm as our DRL approach. A simulation framework is introduced that enables the trading of the continuous intraday price in a resolution of one minute steps. We test our framework in a case study from the perspective of a wind park operator. We include next to general trade information both price and wind forecasts. On a test scenario of German intraday trading results from 2018, we are able to outperform multiple baselines with at least 45.24% improvement, showing the advantage of the DRL algorithm. However, we also discuss limitations and enhancements of the DRL agent, in order to increase the performance in future works.}
}
@article{ALHAZMI202387,
title = {Nonintrusive parameter adaptation of chemical process models with reinforcement learning},
journal = {Journal of Process Control},
volume = {123},
pages = {87-95},
year = {2023},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2023.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0959152423000264},
author = {Khalid Alhazmi and S. Mani Sarathy},
keywords = {Nonlinear system identification, Parameter estimation, Reinforcement learning},
abstract = {Model-based control is one of the most prevalent techniques for designing and controlling engineering systems. However, many of these systems are complex and characterized by changing dynamics. Hence, online system identification is required to achieve optimum adaptive control performance for such complex systems. This work proposes an algorithm for nonintrusive, online, nonlinear parameter estimation of physical models using deep reinforcement learning (RL). The problem of training a neural network for parameter estimation is formulated as a reinforcement learning problem. The RL-based parameter estimation policy is tested on a simulation of the selective hydrogenation of acetylene, which is a highly nonlinear system. The learned model estimation policy is able to correctly predict the states of the system with a prediction error of less than 1% in various conditions, such as in the presence of measurement noise and structural differences in models.}
}
@article{WU2023100073,
title = {Distributional reinforcement learning for inventory management in multi-echelon supply chains},
journal = {Digital Chemical Engineering},
volume = {6},
pages = {100073},
year = {2023},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2022.100073},
url = {https://www.sciencedirect.com/science/article/pii/S2772508122000643},
author = {Guoquan Wu and Miguel Ángel {de Carvalho Servia} and Max Mowbray},
keywords = {Distributional reinforcement learning, Optimal control, Inventory management, Multi-echelon supply chains, Machine learning},
abstract = {Reinforcement Learning (RL) is an effective method to solve stochastic sequential decision-making problems. This is a problem description common to supply chain operations, however, most RL algorithms are tailored for game-based benchmarks. Here, we propose a deep RL method tailored for supply chain problems. The proposed algorithm deploys a derivative free approach to balance exploration and exploitation of the neural policy’s parameter space, providing means to avoid low quality local optima. Furthermore, the method allows consideration of risk-sensitive formulations to learn a policy that optimizes, for example, the conditional value-at-risk. The capabilities of our algorithm are tested on a multi-echelon supply chain problem, and several combinatorial optimization problems. The results empirically demonstrate the method’s improved sample efficiency compared to the benchmark algorithm proximal policy optimization, and superior performance to shrinking horizon mixed integer formulations. Additionally, its risk-sensitive policy can offer protection from low probability, high severity scenarios. Finally, we provide a sensitivity analysis for technical intuition.}
}
@article{TAVAKOLAGHAEI2023121108,
title = {Energy optimization of wind turbines via a neural control policy based on reinforcement learning Markov chain Monte Carlo algorithm},
journal = {Applied Energy},
volume = {341},
pages = {121108},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121108},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923004725},
author = {Vahid {Tavakol Aghaei} and Arda Ağababaoğlu and Biram Bawo and Peiman Naseradinmousavi and Sinan Yıldırım and Serhat Yeşilyurt and Ahmet Onat},
keywords = {Reinforcement learning (RL), Wind turbine energy maximization, Neural control, Markov chain Monte Carlo (MCMC), Bayesian learning, Deep deterministic policy gradient (DDPG)},
abstract = {This study focuses on the numerical analysis and optimal control of vertical-axis wind turbines (VAWT) using Bayesian reinforcement learning (RL). We specifically address small-scale wind turbines, which are well-suited to local and compact production of electrical energy on a small scale, such as urban and rural infrastructure installations. Existing literature concentrates on large scale wind turbines which run in unobstructed, mostly constant wind profiles. However urban installations generally must cope with rapidly changing wind patterns. To bridge this gap, we formulate and implement an RL strategy using the Markov chain Monte Carlo (MCMC) algorithm to optimize the long-term energy output of a wind turbine. Our MCMC-based RL algorithm is a model-free and gradient-free algorithm, in which the designer does not have to know the precise dynamics of the plant and its uncertainties. Our method addresses the uncertainties by using a multiplicative reward structure, in contrast with additive reward used in conventional RL approaches. We have shown numerically that the method specifically overcomes the shortcomings typically associated with conventional solutions, including, but not limited to, component aging, modeling errors, and inaccuracies in the estimation of wind speed patterns. Our results show that the proposed method is especially successful in capturing power from wind transients; by modulating the generator load and hence the rotor torque load, so that the rotor tip speed quickly reaches the optimum value for the anticipated wind speed. This ratio of rotor tip speed to wind speed is known to be critical in wind power applications. The wind to load energy efficiency of the proposed method was shown to be superior to two other methods; the classical maximum power point tracking method and a generator controlled by deep deterministic policy gradient (DDPG) method.}
}
@article{WU2023109199,
title = {Driving style-aware energy management for battery/supercapacitor electric vehicles using deep reinforcement learning},
journal = {Journal of Energy Storage},
volume = {73},
pages = {109199},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2023.109199},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X23025975},
author = {Yue Wu and Zhiwu Huang and Rui Zhang and Pei Huang and Yang Gao and Heng Li and Yongjie Liu and Jun Peng},
keywords = {Deep reinforcement learning, Proximal policy optimization, Driving style, S3VM, Energy management, Hybrid energy storage system},
abstract = {Driving style can significantly affect the energy consumption, battery lifespan, and driving economy of electric vehicles. In this context, this paper proposes a novel driving style-aware energy management strategy for electric vehicles with battery/supercapacitor hybrid energy storage systems based on deep reinforcement learning. Firstly, a semi-supervised support vector machine-based driving style recognition method is presented to recognize the driving style, where twenty features are extracted from limited labeled velocity/acceleration data and then reduced to six dimensions by locally linear embedding. The six dimension features are used to obtain accurate recognition results. Then a proximal policy optimization-based energy management strategy is proposed with the driving style as an additional input state, to optimize the power allocation and minimize the battery capacity loss cost. Extensive results illustrate the effectiveness of the proposed methods, e.g., the proposed driving style recognition method can recognize the real-time driving style with an accuracy of over 95%. Taking the recognized style as input, the proposed driving style-aware energy management strategy can reduce the battery capacity loss cost by 3.30–4.19% and 1.77–8.15%, compared with no driving style and incorrect driving style input energy management methods, respectively.}
}
@article{DEVARAJAN2023100890,
title = {DDNSAS: Deep reinforcement learning based deep Q-learning network for smart agriculture system},
journal = {Sustainable Computing: Informatics and Systems},
volume = {39},
pages = {100890},
year = {2023},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2023.100890},
url = {https://www.sciencedirect.com/science/article/pii/S2210537923000458},
author = {Ganesh Gopal Devarajan and Senthil Murugan Nagarajan and Ramana T.V. and Vignesh T. and Uttam Ghosh and Waleed Alnumay},
keywords = {Smart agriculture, Deep reinforcement learning, Ant colony optimization, Convergence speed, Unmanned ariel vehicle},
abstract = {As the global population continues to grow and environmental conditions become increasingly unpredictable, meeting the demands for food becomes increasingly difficult. To overcome these challenges, smart agriculture has emerged as the key technology. Deep Learning model with Internet of Things (IoT), unmanned aerial vehicle (UAV), and edge–fog–cloud architecture enabled smart agriculture as a key component for next agriculture revolution. In this work, we present two stage end-to-end DRL based smart agricultural system. In stage one, we proposed ACO enabled DQN (MACO-DQN) model to offload task including fire detection, pest detection, crop growth monitoring, irrigation scheduling, soil monitoring, climate monitoring, field monitoring etc. MACO-DQN model offload the task to either edge, fog or cloud networking devices based on latency, energy consumption and computing power. Once the task offloaded to computing devices (edge, fog or cloud), task of prediction and monitoring various agriculture activities is performed at stage two. In stage two, we proposed DRL based DQN (RL-DQN) model for prediction and monitoring agricultural task activities. Finally, we demonstrate experimental findings of our proposed model that represent a marked enhancement in terms of convergence speed, planning success rate, and path accuracy. To evaluate its performance, the method presented in this paper was compared to traditional deep Q-networks-based intensive learning method under consistent experimental conditions. Overall, 98.5% precision, 99.1% recall, 98.1% F-measure, and 98.5% accuracy is obtained when using our proposed methodology and based on the performance results the model outperforms other existing methodologies.}
}
@article{LEVINSON2023110180,
title = {Simultaneous stochastic optimization of an open-pit mining complex with preconcentration using reinforcement learning},
journal = {Applied Soft Computing},
volume = {138},
pages = {110180},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110180},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623001989},
author = {Zachary Levinson and Roussos Dimitrakopoulos and Julien Keutchayan},
keywords = {Mining complex, Preconcentration, Reinforcement learning, Stochastic programming},
abstract = {A preconcentration facility is a major operational component that is critical for managing capacities and improving process plant efficiency in a mining complex. These facilities have not been considered in previous short-term production scheduling frameworks for mining complexes. Short-term production scheduling is a vital part of planning that helps ensure long-term production targets are meet without compromising value. In this work, a new stochastic mathematical programming formulation for simultaneously optimizing the short-term production schedule with preconcentration considerations is proposed. The optimization formulation considers optimizing the extraction sequence, destination policy, stockpiling and preconcentration decisions jointly to capture potential synergies. In addition, this work investigates a new approach for short-term production scheduling that combines reinforcement learning with stochastic mathematical programming. An actor–critic reinforcement learning agent learns to optimize the short-term production schedule and provides a more flexible framework for adapting heuristics to the scheduling problem. The optimization approach and stochastic formulation are tested in a copper mining complex with multiple mining areas, several material properties, stockpiles, preconcentration facilities, leach pads, process plants and waste dumps. The case study shows the practical aspects of the proposed optimization and the direct benefit of integrating preconcentration decisions in the short-term production schedule; this led to a $140M improvement in annual cashflow. Additionally, the actor–critic​ reinforcement learning algorithm learns a stable policy that provides operational extraction sequences.}
}
@article{LIAO202328,
title = {Online computation offloading with double reinforcement learning algorithm in mobile edge computing},
journal = {Journal of Parallel and Distributed Computing},
volume = {171},
pages = {28-39},
year = {2023},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731522001976},
author = {Linbo Liao and Yongxuan Lai and Fan Yang and Wenhua Zeng},
keywords = {Mobile edge computing, Power control, Computation offloading, Deep deterministic policy gradient, Double Deep Q-Networks},
abstract = {Smart mobile devices have recently emerged as a promising computing platform for computation tasks. However, the task performance is restricted by the computing power and battery capacity of mobile devices. Mobile edge computing, an extension of cloud computing, solves this problem well by providing computational support to mobile devices. In this paper, we discuss a mobile edge computing system with a server and multiple mobile devices that need to perform computation tasks with priorities. The limited resources of the mobile edge computing server and mobile device make it challenging to develop an offloading strategy to minimize both delay and energy consumption in the long term. To this end, an online algorithm is proposed, namely, the double reinforcement learning computation offloading (DRLCO) algorithm, which jointly decides the offloading decision, the CPU frequency, and transmit power for computation offloading. Concretely, we first formulate the power scheduling problem for mobile users to minimize energy consumption. Inspired by reinforcement learning, we solve the problem by presenting a power scheduling algorithm based on the deep deterministic policy gradient (DDPG). Then, we model the task offloading problem to minimize the delay of tasks and propose a double Deep Q-networks (DQN) based algorithm. In the decision-making process, we fully consider the influence of task queue information, channel state information, and task information. Moreover, we propose an adaptive prioritized experience replay algorithm to improve the model training efficiency. We conduct extensive simulations to verify the effectiveness of the scheme, and the simulation results show that compared with the conventional schemes, our method reduces the delay by 48% and the energy consumption by 53%.}
}
@article{HU2022104655,
title = {Optimal maintenance scheduling under uncertainties using Linear Programming-enhanced Reinforcement Learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {109},
pages = {104655},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104655},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621004565},
author = {Jueming Hu and Yuhao Wang and Yutian Pang and Yongming Liu},
keywords = {Maintenance scheduling, Rollout, Linear programming, Infinite horizon, Stochastic maintenance},
abstract = {Maintenance is of great importance for the safety and integrity of infrastructures. The expected optimal maintenance policy in this study should be able to minimize system maintenance cost while satisfying the system reliability requirements. Stochastic maintenance scheduling with an infinite horizon has not been investigated thoroughly in the literature. In this work, we formulate the maintenance optimization under uncertainties as a Markov Decision Process (MDP) problem and solve it using a modified Reinforcement Learning method. A Linear Programming-enhanced RollouT (LPRT) is proposed, which considers both constrained deterministic and stochastic maintenance scheduling with an infinite horizon. The novelty of the proposed approach is that it is suitable for online maintenance scheduling, which can include random unexpected maintenance performance and system degradation. The proposed method is demonstrated with numerical examples and compared with several existing methods. Results show that LPRT is able to determine the suitable optimal maintenance policy efficiently compared with existing methods with similar accuracy. Parametric studies are used to investigate the effect of uncertainty, subproblem size, and the number of stochastic stages on the final maintenance cost. Limitations and future work are given based on the proposed study.}
}
@article{KOFINAS2017461,
title = {A reinforcement learning approach for MPPT control method of photovoltaic sources},
journal = {Renewable Energy},
volume = {108},
pages = {461-473},
year = {2017},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2017.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0960148117301891},
author = {P. Kofinas and S. Doltsinis and A.I. Dounis and G.A. Vouros},
keywords = {Photovoltaic systems, Maximum power point tracking, On line learning, Reinforcement learning MPPT control method},
abstract = {Photovoltaic arrays are the means to convert solar power into electricity, and a significant way to generate renewable and clean energy. To be efficient, a photovoltaic must generate constantly the maximum possible power and under different environmental conditions. Finding the maximum generated power has been a known issue in the industry using methods of classic control theory with very good results. However, those solutions are case-specific resulting to increased set-up effort. This work proposes a universal RLMPPT control method based on a reinforcement learning (RL) method that tracks and adjusts the maximum power point of a photovoltaic source without any prior knowledge. A Markov Decision Process (MDP) model for the Maximum Power Point Tracking (MPPT) photovoltaic process is defined and an RL algorithm is proposed and evaluated on a number of photovoltaic sources. The proposed RLMPPT control method has the advantage of being applicable to different PV sources with minimum set-up time. To evaluate the RLMPPT control method performance, a number of simulations run under different environmental and operating conditions and a comparison with the conventional method of Perturb and Observe (P&O) is performed. Results show quick response and close to optimal behavior without requiring any prior knowledge.}
}
@article{MOHAMMADI2022108615,
title = {A deep reinforcement learning approach for rail renewal and maintenance planning},
journal = {Reliability Engineering & System Safety},
volume = {225},
pages = {108615},
year = {2022},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2022.108615},
url = {https://www.sciencedirect.com/science/article/pii/S0951832022002575},
author = {Reza Mohammadi and Qing He},
keywords = {Rail renewal and maintenance, Deep Reinforcement Learning, Double Deep Q-Network},
abstract = {Developing optimal rail renewal and maintenance planning that minimizes long-term costs and risks of failure is of paramount importance for railroad industry. However, intrinsic uncertainty, presence of constraints, and curse of dimensionality induce a challenging engineering problem. Despite the potential capabilities of Deep Reinforcement Learning (DRL), there is very limited research in the area of employing DRL methods to solve renewal and maintenance planning. Inspired by the recent advances in the area of DRL, a DRL-based approach is developed to optimize maintenance and renewal planning. This approach optimizes renewal and maintenance planning over a planning horizon by considering cost-effectiveness and risk reduction. We consider both predictive and condition-based maintenance tasks and incorporate time, resource, and related engineering constraints into the model to capture realistic features of the problem. Available historic inspection and maintenance data is used to simulate the rail environment and feed into DRL method. A Double Deep Q-Network (DDQN) is applied to overcome the uncertainty of the environment. In addition, prioritized replay memory is applied which improves the feedback from the improvement by giving high weight to important experiences of the agent. The proposed DDQN approach is applied to a Class I railroad network to demonstrate the applicability and efficiency the approach. Our analyses demonstrate that the proposed approach develops an optimal policy that not only reduces budget consumption but also improves the reliability and safety of the network.}
}
@article{XIAO202320,
title = {Sampled-data control through model-free reinforcement learning with effective experience replay},
journal = {Journal of Automation and Intelligence},
volume = {2},
number = {1},
pages = {20-30},
year = {2023},
issn = {2949-8554},
doi = {https://doi.org/10.1016/j.jai.2023.100018},
url = {https://www.sciencedirect.com/science/article/pii/S2949855423000011},
author = {Bo Xiao and H.K. Lam and Xiaojie Su and Ziwei Wang and Frank P.-W. Lo and Shihong Chen and Eric Yeatman},
keywords = {Reinforcement learning, Neural networks, Sampled-data control, Model-free, Effective experience replay},
abstract = {Reinforcement Learning (RL) based control algorithms can learn the control strategies for nonlinear and uncertain environment during interacting with it. Guided by the rewards generated by environment, a RL agent can learn the control strategy directly in a model-free way instead of investigating the dynamic model of the environment. In the paper, we propose the sampled-data RL control strategy to reduce the computational demand. In the sampled-data control strategy, the whole control system is of a hybrid structure, in which the plant is of continuous structure while the controller (RL agent) adopts a discrete structure. Given that the continuous states of the plant will be the input of the agent, the state–action value function is approximated by the fully connected feed-forward neural networks (FCFFNN). Instead of learning the controller at every step during the interaction with the environment, the learning and acting stages are decoupled to learn the control strategy more effectively through experience replay. In the acting stage, the most effective experience obtained during the interaction with the environment will be stored and during the learning stage, the stored experience will be replayed to customized times, which helps enhance the experience replay process. The effectiveness of proposed approach will be verified by simulation examples.}
}
@article{MACHALEK2021107496,
title = {A novel implicit hybrid machine learning model and its application for reinforcement learning},
journal = {Computers & Chemical Engineering},
volume = {155},
pages = {107496},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107496},
url = {https://www.sciencedirect.com/science/article/pii/S009813542100274X},
author = {Derek Machalek and Titus Quah and Kody M. Powell},
keywords = {Hybrid model, Gray-box model, Implicit methods, Reinforcement learning, Automatic differentiation},
abstract = {A novel methodology to develop implicit hybrid models is presented. PyTorch is used to integrate physics-based equations with machine learning models. Automatic differentiation of the hybrid model is leveraged to solve the implicit equations. Iterative solving enables gradient based updates to the machine learning model. The novel methodology is compared to an explicit hybrid approach on a continuously stirred tank reactor (CSTR). The novel method results in a lower modelling error. Both hybrid models effectively train with noisy data. To test the implicit hybrid model, it is employed as a reinforcement learning (RL) training model. The RL algorithm trained on the hybrid model outperforms real time optimization of the CSTR and performs nearly as well as RL trained directly on the CSTR and a traditional gradient based approach. Training RL directly on the CSTR requires over 60,000 system interactions compared to 6000 historical data points for hybrid model development.}
}
@article{LIU2022,
title = {Multi-task safe reinforcement learning for navigating intersections in dense traffic},
journal = {Journal of the Franklin Institute},
year = {2022},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2022.06.052},
url = {https://www.sciencedirect.com/science/article/pii/S0016003222004549},
author = {Yuqi Liu and Yinfeng Gao and Qichao Zhang and Dawei Ding and Dongbin Zhao},
abstract = {Multi-task intersection navigation, which includes unprotected turning left, turning right, and going straight in heavy traffic, remains a difficult task for autonomous vehicles. For the human driver, negotiation skills with other interactive vehicles are the key to guaranteeing safety and efficiency. However, it is hard to balance the safety and efficiency of the autonomous vehicle for multi-task intersection navigation. In this paper, we formulate a multi-task safe reinforcement learning framework with social attention to improve the safety and efficiency when interacting with other traffic participants. Specifically, the social attention module is used to focus on the states of negotiation vehicles. In addition, a safety layer is added to the multi-task reinforcement learning framework to guarantee safe negotiation. We deploy experiments in the simulators SUMO, which has abundant traffic flows, and CARLA, which has high-fidelity vehicle models. Both show that the proposed algorithm improves safety while maintaining stable traffic efficiency for the multi-task intersection navigation problem. More details and demonstrations are available at https://github.com/liuyuqi123/SAT.}
}
@article{BAI2023120294,
title = {An adaptive active power rolling dispatch strategy for high proportion of renewable energy based on distributed deep reinforcement learning},
journal = {Applied Energy},
volume = {330},
pages = {120294},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120294},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922015513},
author = {Yuyang Bai and Siyuan Chen and Jun Zhang and Jian Xu and Tianlu Gao and Xiaohui Wang and David {Wenzhong Gao}},
keywords = {Active power rolling dispatch, High proportion of renewable energy, Distributed deep reinforcement learning, Regional graph attention network},
abstract = {In this article, an adaptive active power rolling dispatch strategy based on distributed deep reinforcement learning is proposed to deal with the uncertainty of high-proportioned renewable energy. For each agent, by using recurrent neural network layers and graph attention layers in its network structure, we aim to improve the generalization ability of the multiple agents in active power flow control. Furthermore, a regional graph attention network algorithm, which can effectively help agents aggregate the regional information of their neighborhood, is proposed to improve the information capture ability of agents. We adopt the structure of ‘centralized training, distributed execution’ to help agents improve the effectiveness of proposed methods in dynamic environments. The case studies demonstrate that the proposed algorithm can help multi-agents learn effective active power control strategies. Each agent has a strong generalization ability in terms of time granularity and network topology. We expect that such an approach can improve the practicability and adaptability of distributed AI method on power system control issues.}
}
@article{WEN2023117323,
title = {Data-driven energy management system for flexible operation of hydrogen/ammonia-based energy hub: A deep reinforcement learning approach},
journal = {Energy Conversion and Management},
volume = {291},
pages = {117323},
year = {2023},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2023.117323},
url = {https://www.sciencedirect.com/science/article/pii/S0196890423006696},
author = {Du Wen and Muhammad Aziz},
keywords = {Optimal planning, Deep reinforcement learning, Mixed integer linear programming, Scenario analysis, Hydrogen and ammonia},
abstract = {In the context of carbon neutrality, multi-energy systems are being designed to enhance the integration of renewable energy, and the deployment of large-scale energy storage solutions is vital to ensure a flexible and cost-effective system operation. Hydrogen and ammonia are two promising energy carriers that can serve as storage media, feedstock, and fuels. This study designed an energy hub that integrates multiple energy resources and energy storage methods. Two pathways, power-to-gas-to-power (P2X2P) and biomass-to-gas-to-power (B2X2P), which use hydrogen and ammonia as energy carriers, are proposed for the energy hub. Considering the flexibility and profitability of the energy hub, scheduling of the energy hub was controlled using model-based mixed-integer linear programming and model-free deep reinforcement learning methods. A modified double-deep Q-network framework was trained on a monthly dataset and executed on a yearly dataset. The results indicate that the B2X2P pathway outperforms the P2X2P pathway in terms of profitability, while the P2X2P pathway has greater operational flexibility. Because the optimal biomass-to-ammonia-to-power has a 550.76 MUSD net present value and a five-year discounted payback period, ammonia is best suited for mass production and storage. Moreover, the modified double deep Q network framework demonstrates superior generality for solving similar optimal scheduling problems.}
}
@article{DANDACHI2022109366,
title = {A robust control-theory-based exploration strategy in deep reinforcement learning for virtual network embedding},
journal = {Computer Networks},
volume = {218},
pages = {109366},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109366},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622004005},
author = {Ghina Dandachi and Sophie Cerf and Yassine Hadjadj-Aoul and Abdelkader Outtagarts and Eric Rutten},
keywords = {5G slicing, Virtual network embedding, Deep reinforcement learning, Monte Carlo, Control theory},
abstract = {Network slice management and, more generally, resource orchestration should be fully automated in 6G networks, as envisioned by the ETSI ENI. In this context, artificial intelligence (AI) and context-aware policies are certainly major options to move in this direction and to adapt service delivery to changing user needs, environmental conditions and business objectives. In this paper, we step towards this objective by addressing the problem of optimal placement of dynamic virtual networks through a self-adaptive learning-based strategy. These constantly evolving networks present, however, several challenges, mainly due to their stochastic nature, and the high dimensionality of the state and the action spaces. This curse of dimensionality requires, indeed, a broader exploration, which is not always compatible with a real-time execution in an operational network. Thus, we propose DQMC, a new strategy for virtual network embedding in mobile networks combining a Deep Reinforcement Learning (DRL) strategy, namely a Deep Q-Network (DQN), and Monte Carlo (MC). As learning is costly in time and computing resources, and sensitive to changes in the network, we suggest a control-theory-based techniques to dynamically leverage exploration in DQMC. This leads to fast, efficient, and sober learning compared to a Monte Carlo-based strategy. This also ensures a reliable solution even in the case of a change in the requests’ sizes or a node’s failure, showing promising perspectives for solutions combining control-theory and machine learning.}
}
@article{KORATAMADDI2021848,
title = {Market sentiment-aware deep reinforcement learning approach for stock portfolio allocation},
journal = {Engineering Science and Technology, an International Journal},
volume = {24},
number = {4},
pages = {848-859},
year = {2021},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2021.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S2215098621000070},
author = {Prahlad Koratamaddi and Karan Wadhwani and Mridul Gupta and Sriram G. Sanjeevi},
keywords = {Reinforcement learning, Portfolio allocation, Sentiment analysis, Deep learning, Stock trading},
abstract = {The stock market currently remains one of the most difficult systems to model in finance. Hence, it is a challenge to solve stock portfolio allocation wherein an optimal investment strategy must be found for a curated collection of stocks that effectively maximizes return while minimizing the risk involved. Deep reinforcement learning approaches have shown promising results when used to automate portfolio allocation, by training an intelligent agent on historical stock prices. However, modern investors are actively engaging with digital platforms such as social media and online news websites to understand and better analyze portfolios. The overall attitude thus formed by investors toward a particular stock or financial market is known as market sentiment. Existing approaches do not incorporate market sentiment which has been empirically shown to influence investor decisions. In our paper, we propose a novel deep reinforcement learning approach to effectively train an intelligent automated trader, that not only uses the historical stock price data but also perceives market sentiment for a stock portfolio consisting of the Dow Jones companies. We demonstrate that our approach is more robust in comparison to existing baselines across standardized metrics such as the Sharpe ratio and annualized investment return.}
}
@article{NG200683,
title = {A DECENTRALIZED REINFORCEMENT LEARNING CONTROLLER FOR COLLABORATIVE DRIVING},
journal = {IFAC Proceedings Volumes},
volume = {39},
number = {20},
pages = {83-88},
year = {2006},
note = {1st IFAC Workshop on Multivehicle Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20061002-2-BR-4906.00015},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015311915},
author = {Luke Ng and Chris Clark and Jan Huissoon and Gabriele D'Eleuterio},
keywords = {autonomous vehicles, co-ordination, decentralized control, machine learning, robot control},
abstract = {Research in the collaborative driving domain strives to create control systems that coordinate the motion of multiple vehicles in order to navigate traffic both efficiently and safely. In this paper a novel individual vehicle controller based on reinforcement learning is introduced. This controller is capable of both lateral and longitudinal control while driving in a multi-vehicle platoon. The design and development of this controller is discussed in detail and simulation results showing learning progress and performance are presented.}
}
@article{HU2021107056,
title = {Reinforcement learning-driven maintenance strategy: A novel solution for long-term aircraft maintenance decision optimization},
journal = {Computers & Industrial Engineering},
volume = {153},
pages = {107056},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.107056},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220307269},
author = {Yang Hu and Xuewen Miao and Jun Zhang and Jie Liu and Ershun Pan},
keywords = {Aircraft maintenance optimization, Aircraft maintenance scenario modeling, Reinforcement learning, Extreme learning machine based Q-learning},
abstract = {A novel Reinforcement Learning (RL) driven maintenance strategy is proposed in this paper for solving the problem of aircraft long-term maintenance decision optimization. Specifically, it is targeted to process the information of aircraft future mission requirement, repair cost, spare components storage and aircraft Prognostics and Health Management (PHM) output, and provide real-time End-to-End sequential maintenance action decisions based on the coordination between short and long-term operation performance. The proposed RL-driven strategy is designed in the RL framework with Extreme Learning Machine based Q-learning algorithm, and an integrated aircraft maintenance simulation model is developed for training/testing RL-driven strategy. We test the proposed RL-driven strategy in several simulated dynamic aircraft maintenance scenarios together with 3 other commonly used maintenance strategies. The obtained results demonstrate that RL-driven strategy has prior performance in adjusting its decision principle for handling the variations of mission reward, repair/spare component storage cost and PHM ability in different maintenance scenarios. Some practical application suggestions and future perspectives of RL-driven strategy are discussed based on the obtained experiment results.}
}
@article{YAN202329,
title = {Reinforcement learning based adaptive optimal control for constrained nonlinear system via a novel state-dependent transformation},
journal = {ISA Transactions},
volume = {133},
pages = {29-41},
year = {2023},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2022.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0019057822003639},
author = {Lei Yan and Zhi Liu and C.L. Philip Chen and Yun Zhang and Zongze Wu},
keywords = {Asymmetric time-varying state constraints, Adaptive optimal control, Nonlinear state-dependent function, Reinforcement learning},
abstract = {Existing schemes for state-constrained systems either impose feasibility conditions or ignore the optimality. In this article, an adaptive optimal control scheme for the strict-feedback nonlinear system is proposed, which benefits from two design steps. Firstly, a novel nonlinear state-dependent function (NSDF) is formulated to equivalently transform the system into a non-constrained one to deal with state constraints without the requirements on feasibility conditions. Secondly, an adaptive optimal control scheme is designed for the non-constrained system, in which reinforcement learning (RL) is utilized to yield the optimal controller in each designing procedure. Updating rules of the actor and critic neural network are driven by the modified adaptive laws, used to approximate the optimal virtual and actual controllers. It is proved that all the signals in the closed-loop system are bounded and the output tracking error converges to an adjustable neighborhood of the origin not affected by the proposed NSDF. Two simulation examples are presented illustrating the effectiveness of the proposed scheme.}
}
@article{YANG2021699,
title = {Multi-step Greedy Reinforcement Learning Based on Model Predictive Control},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {3},
pages = {699-705},
year = {2021},
note = {16th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.323},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321010971},
author = {Yucheng Yang and Sergio Lucia},
keywords = {Reinforcement learning, Model predictive control, Nonlinear system, Batch reactor},
abstract = {Reinforcement learning aims to compute optimal control policies with the help of data from closed-loop trajectories. Traditional model-free approaches need huge number of data points to achieve an acceptable performance, rendering them not applicable in most real situations, even if the data can be obtained from a detailed simulator. Model-based reinforcement learning approaches try to leverage model knowledge to drastically reduce the amount of data needed or to enforce important constraints to the closed-loop operation, which is another important drawback of model-free approaches. This paper proposes a novel model-based reinforcement learning approach. The main novelty is the fact that we exploit all the information of a model predictive control (MPC) computing step, and not only the first input that is actually applied to the plant, to efficiently learn a good approximation of the state value function. This approximation can be included into a model predictive control formulation as a terminal cost with a short prediction horizon, achieving a similar performance to an MPC with a very long prediction horizon. Simulation results of a discretized batch bioreactor illustrate the potential of the proposed methodology.}
}
@article{KIM2022235,
title = {State-space segmentation for faster training reinforcement learning},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {25},
pages = {235-240},
year = {2022},
note = {10th IFAC Symposium on Robust Control Design ROCOND 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.352},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322016093},
author = {Jongrae Kim},
keywords = {reinforcement learning, learning convergence, reward, linear control},
abstract = {Nonlinear control problems have been the main subjects in control engineering from theoretical and applicational aspects. Reinforcement learning shows promising results for solving highly nonlinear control problems. Among many variants of reinforcement learning, Deep Deterministic Policy Gradient (DDPG) considers continuous control signals, which makes it an ideal candidate for solving nonlinear control problems. The training requires frequently, however, a large number of computations. To improve the convergence of DDPG, we present a state-space segmentation method dividing the state-space to expand the target space defined by the best reward. An inverted pendulum control example demonstrates the performance of the proposed segmentation method.}
}
@article{WESTBRINK2021602,
title = {Optimization of DEM parameters using multi-objective reinforcement learning},
journal = {Powder Technology},
volume = {379},
pages = {602-616},
year = {2021},
issn = {0032-5910},
doi = {https://doi.org/10.1016/j.powtec.2020.10.067},
url = {https://www.sciencedirect.com/science/article/pii/S0032591020310172},
author = {Fabian Westbrink and Alexander Elbel and Andreas Schwung and Steven X. Ding},
keywords = {Discrete element method (DEM), Reinforcement learning, Multi-objective, Bulk handling, Parameter calibration},
abstract = {Simulations with the Discrete Element Method (DEM) have become prominent for analyzing bulk behavior in various industries. For each application the material has to be analyzed while the material parameters have to be determined to ensure a valid and reliable result. However, material properties available in the literature are hardly usable and unsuitable for a macroscopic analysis of the bulk behavior. Thus, the material has to be tested and evaluated to calibrate it with suitable DEM material parameters. In this work, a novel approach for DEM calibration with a parameter optimization based on multi-objective reinforcement learning is proposed. This approach uses the results of two different environments and trains an agent to find a suitable material parameter-set with a low number of required iterations and a small number of hyper-parameters. To ensure the applicability of the developed approach, three materials with different characteristics are calibrated and validated.}
}
@article{ZHU2023120212,
title = {Reinforcement learning in deregulated energy market: A comprehensive review},
journal = {Applied Energy},
volume = {329},
pages = {120212},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120212},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922014696},
author = {Ziqing Zhu and Ze Hu and Ka Wing Chan and Siqi Bu and Bin Zhou and Shiwei Xia},
keywords = {Energy market, Reinforcement learning, Bidding strategy, Optimal dispatching},
abstract = {The increasing penetration of renewable generations, along with the deregulation and marketization of power industry, promotes the transformation of energy market operation paradigms. The optimal bidding strategy and dispatching methodologies under these new paradigms are prioritized concerns for both market participants and power system operators. In contrast with conventional solution methodologies, the Reinforcement Learning (RL), as an emerging machine learning technique that exhibits a more favorable computational performance, is playing an increasingly significant role in both academia and industry. This paper presents a comprehensive review of RL applications in deregulated energy market operation including bidding and dispatching strategy optimization, based on more than 150 carefully selected papers. For each application, apart from a paradigmatic summary of generalized methodology, in-depth discussions of applicability and obstacles while deploying RL techniques are also provided. Finally, some RL techniques that have great potentiality to be deployed in bidding and dispatching problems are recommended and discussed.}
}
@article{SHIPMAN2019111,
title = {Reinforcement Learning and Deep Neural Networks for PI Controller Tuning},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {14},
pages = {111-116},
year = {2019},
note = {18th IFAC Symposium on Control, Optimization and Automation in Mining, Mineral and Metal Processing, MMM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.09.173},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319308055},
author = {William J. Shipman and Loutjie C. Coetzee},
keywords = {artificial intelligence, autotuners, neural networks, SISO, proportional plus integral controllers, reinforcement learning},
abstract = {Reinforcement Learning, using deep neural networks, has recently gained prominence owing to its ability to train autonomous agents that have defeated human players in various complex games. Here, Reinforcement Learning is applied to the challenge of automatically tuning a proportional-integral controller, given only the process variable, set-point, manipulated variable and prior controller gains. The training considers random changes in plant dynamics, disturbances and measurement noise. Two training procedures were tested in this work, one that built up the difficulty of the simulation over time, and another that used the full complexity of the simulation throughout the training. The results show that building up the difficulty of the simulation by introducing greater degrees of randomness as the training progresses, produces an agent that is better able to tune the controller in question. Additional experience gathered in completing this work is also discussed to enable the reader to avoid some of the challenges encountered.}
}
@article{ABEDI2022107368,
title = {Battery energy storage control using a reinforcement learning approach with cyclic time-dependent Markov process},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {134},
pages = {107368},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107368},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521006074},
author = {Sara Abedi and Sang Won Yoon and Soongeol Kwon},
keywords = {Energy management system, Battery energy storage, Reinforcement learning, Q-learning algorithm, Cyclic time-dependent Markov process},
abstract = {Scheduling efficient energy management system operations to respond to the unstable customer demand, electricity prices, and weather increases the complexity of the control systems and requires a flexible and cost-effective control policy. This study develops an intelligent and real-time battery energy storage control based on a reinforcement learning model focused on residential houses connected to the grid and equipped with solar photovoltaic panels and a battery energy storage system. Because the reinforcement learning’s performance is very dependent on the design of the underlying Markov decision process, a cyclic time-dependent Markov Process is uniquely designed to capture existing daily cyclic patterns in demand, electricity price, and solar energy. The Markov Process is successfully used in the Q-learning algorithm, resulting in more efficient battery energy control and saving electricity costs. The proposed Q-learning algorithm is compared with benchmark models of a deterministic equivalent solution and a One-step Roll-out algorithm. Numerical experiments show the gap between the deterministic equivalent solution and Q-learning approaches for one-month electricity cost decreased from 7.99% to 3.63% for house 27 and 6.91% to 3.26% for house 387 when the discrete size of demand, solar energy, price, and battery energy level adjusted to 20. Accordingly, the better performance of the proposed Q-learning is demonstrated compared to the One-step Roll-out algorithm. Moreover, the effect of discrete size of state-space parameters on the adaptive Q-learning performance and computational time are investigated. Variations in the electricity price significantly affect the Q-learning algorithm’s performance more than other parameters.}
}
@article{MALUS2020397,
title = {Real-time order dispatching for a fleet of autonomous mobile robots using multi-agent reinforcement learning},
journal = {CIRP Annals},
volume = {69},
number = {1},
pages = {397-400},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620300226},
author = {Andreja Malus and Dominik Kozjek and Rok Vrabič},
keywords = {Logistics, Machine learning, Distributed control},
abstract = {Autonomous mobile robots (AMRs) are increasingly being used to enable efficient material flow in dynamic production environments. Dispatching transport orders in such environments is difficult due to the complexity arising from the rapid changes in the environment as well as due to a tight coupling between dispatching, path planning, and route execution. For order dispatching, an approach is proposed that uses multi-agent reinforcement learning, where AMR agents learn to bid on orders based on their individual observations. The approach is investigated in a robot simulation environment. The results show a more efficient order allocation compared to commonly used dispatching rules.}
}
@article{HUSSIN201593,
title = {Improving reliability in resource management through adaptive reinforcement learning for distributed systems},
journal = {Journal of Parallel and Distributed Computing},
volume = {75},
pages = {93-100},
year = {2015},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0743731514001919},
author = {Masnida Hussin and Nor {Asilah Wati Abdul Hamid} and Khairul Azhar Kasmiran},
keywords = {Distributed systems, Resource management, Adaptive reinforcement learning, System reliability, Computational complexity},
abstract = {Demands on capacity of distributed systems (e.g., Grid and Cloud) play a crucial role in today’s information era due to the growing scale of the systems. While the distributed systems provide a vast amount of computing power their reliability is often hard to be guaranteed. This paper presents effective resource management using adaptive reinforcement learning (RL) that focuses on improving successful execution with low computational complexity. The approach uses an emerging methodology of RL in conjunction with neural network to help a scheduler that effectively observes and adapts to dynamic changes in execution environments. The observation of environment at various learning stages that normalize by resource-aware availability and feedback-based scheduling significantly brings the environments closer to the optimal solutions. Our approach also solves a high computational complexity in RL system through on-demand information sharing. Results from our extensive simulations demonstrate the effectiveness of adaptive RL for improving system reliability.}
}
@article{QIAN2022104401,
title = {Development of deep reinforcement learning-based fault diagnosis method for rotating machinery in nuclear power plants},
journal = {Progress in Nuclear Energy},
volume = {152},
pages = {104401},
year = {2022},
issn = {0149-1970},
doi = {https://doi.org/10.1016/j.pnucene.2022.104401},
url = {https://www.sciencedirect.com/science/article/pii/S014919702200275X},
author = {Gensheng Qian and Jingquan Liu},
keywords = {Deep reinforcement learning, Small sample, Rotating machinery, Fault diagnosis, Nuclear power plant},
abstract = {Rotating machinery fault can cause accidents like loss of flow or turbine trip that seriously threaten the operation safety of nuclear power plants (NPPs). Artificial intelligence algorithms, like machine learning or deep learning methods, can implement fault diagnosis by sample learning with no reliance on fault mechanism or physics model of the equipment. However, the accumulated fault samples are small due to high operation safety requirements of the plant. Small sample learning is challenging and usually leads to degradation of model performance. The emerging deep reinforcement learning (DRL) algorithm can incorporate the advantages of automatic feature extraction from deep learning algorithm and interactive learning from reinforcement learning algorithm, is expected to have better learning ability and robustness. In this paper, two DRL fault diagnosis models are proposed and compared. Experiment results show that the proposed models can achieve very high diagnosis accuracy of over 99% and outperform all the baseline models (support vector machine, convolutional neural network and gated recurrent unit neural network) in all test cases in this paper.}
}
@article{WANG2023222,
title = {Cooperative USV–UAV marine search and rescue with visual navigation and reinforcement learning-based control},
journal = {ISA Transactions},
volume = {137},
pages = {222-235},
year = {2023},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2023.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0019057823000071},
author = {Yuanda Wang and Wenzhang Liu and Jian Liu and Changyin Sun},
keywords = {Unmanned surface vehicle, Unmanned aerial vehicle, Marine search and rescue, Convolutional neural network, Reinforcement learning},
abstract = {This paper investigates visual navigation and control of a cooperative unmanned surface vehicle (USV)-unmanned aerial vehicle (UAV) system for marine search and rescue. First, a deep learning-based visual detection architecture is developed to extract positional information from the images taken by the UAV. With specially designed convolutional layers and spatial softmax layers, the visual positioning accuracy and computational efficiency are improved. Next, a reinforcement learning-based USV control strategy is proposed, which could learn a motion control policy with an enhanced ability to reject wave disturbances. The simulation experiment results show that the proposed visual navigation architecture can provide stable and accurate position and heading angle estimation in different weather and lighting conditions. The trained control policy also demonstrates satisfactory USV control ability under wave disturbances.}
}
@article{MA2018108,
title = {A saliency-based reinforcement learning approach for a UAV to avoid flying obstacles},
journal = {Robotics and Autonomous Systems},
volume = {100},
pages = {108-118},
year = {2018},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2017.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0921889017301136},
author = {Zhaowei Ma and Chang Wang and Yifeng Niu and Xiangke Wang and Lincheng Shen},
keywords = {UAV, Flying obstacle avoidance, Convolution neural networks based saliency detection, Reinforcement learning},
abstract = {Obstacle avoidance is a necessary behavior to guarantee the safety of an unmanned aerial vehicle (UAV). However, it is a challenge for the UAV to detect and avoid high-speed flying obstacles such as other UAVs or birds. In this paper, we propose a generic framework that integrates an autonomous obstacle detection module and a reinforcement learning (RL) module to develop reactive obstacle avoidance behavior for a UAV. In the obstacle detection module, we design a saliency detection algorithm using deep convolution neural networks (CNNs) to extract monocular visual cues. The algorithm imitates human’s visual detection system, and it can accurately estimate the location of obstacles in the field of view (FOV). The RL module uses an actor–critic structure that chooses the RBF neural network to approximate the value function and control policy in continuous state and action spaces. We have tested the effectiveness of the proposed learning framework in a semi-physical experiment. The results show that the proposed saliency detection algorithm performs better than state-of-the-art, and the RL algorithm can learn the avoidance behavior from the manual experiences.}
}
@article{SHAKYA2022734,
title = {Deep Reinforcement Learning based Super Twisting Controller for Liquid Slosh Control Problem},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {1},
pages = {734-739},
year = {2022},
note = {7th International Conference on Advances in Control and Optimization of Dynamical Systems ACODS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.120},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322001203},
author = {Ashish Kumar Shakya and Kshitij Bithel and G.N. Pillai and Sohom Chakrabarty},
keywords = {Deep reinforcement learning, super twisting control, lateral slosh, under-actuated system},
abstract = {Deep Reinforcement Learning (DRL) based parameter optimization of super twisting control (STC) for the liquid slosh control problem in a moving vehicle is proposed in this paper. The slosh control problem, including the vehicle dynamics, represents an under-actuated nonlinear dynamical system. The slosh phenomenon is modeled by a simple pendulum on a cart and STC had been designed in the literature for the system when the vehicle motion is in a straight line. In this paper, a DRL framework is designed for the first time to tune the STC parameters in order to deliver near optimal performance. The effectiveness of this proposed learning-based STC for the slosh control problem is validated in a Python simulation environment and its performance compared to that of the existing STC design without the learning.}
}
@article{WANG2023211678,
title = {Hierarchical optimization of reservoir development strategy based on reinforcement learning},
journal = {Geoenergy Science and Engineering},
volume = {226},
pages = {211678},
year = {2023},
issn = {2949-8910},
doi = {https://doi.org/10.1016/j.geoen.2023.211678},
url = {https://www.sciencedirect.com/science/article/pii/S2949891023002658},
author = {Haochen Wang and Kai Zhang and Nancy Chen and Zhongzheng Wang and Guojing Xin and ChengCheng Liu and Liming Zhang and Wensheng Zhou and Chen Liu},
keywords = {Production optimization, Reinforcement learning, Hierarchical reinforcement learning},
abstract = {Production optimization is to increase the profitability of reservoir development by adjusting the type of wells and the rate of injection and production. Previous methods put both variables at the same level when optimizing and thus they change at the same frequency. However, these two variables belong to different scales, because well types require time-consuming and labor consuming operations such as borehole operations, while rate adjustment don't. Therefore, in this paper, a hierarchical optimization method is proposed to optimally control well types and corresponding rate in production sequence. Specifically, a hierarchical framework is first employed based on reinforcement learning method, with well types as high-level variables and quantities as low-level variables. Then, two hindsight mechanisms, named action hindsight and goal hindsight, are used to achieve stably joint optimization of two-level variables at different frequencies. Our findings from the case study reveal that our approach is highly practical, as it has successfully reduced operational costs by 90% and increased cumulative oil production by 18% within three years when compared to the single-level reinforcement method.}
}
@article{RENAULT2023124147,
title = {Investigating gas furnace control practices with reinforcement learning},
journal = {International Journal of Heat and Mass Transfer},
volume = {209},
pages = {124147},
year = {2023},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2023.124147},
url = {https://www.sciencedirect.com/science/article/pii/S0017931023003009},
author = {M. Renault and J. Viquerat and P. Meliga and G.-A. Grandin and N. Meynet and E. Hachem},
keywords = {Deep reinforcement learning, Artificial neural networks, Conjugate heat transfer, Computational fluid dynamics, Thermal control, Serpentine},
abstract = {Gas furnaces are the most widely used means of heating in industry, and with the growing concern for environmental issues, and a global energy crisis at our doorstep, the optimization of the processes related to them becomes a key challenge. This paper aims at introducing a new way of practicing gas furnace control involving simulations, virtual sensors and deep reinforcement learning (DRL) techniques. In order to do so we designed a set of simulations of conjugate heat transfer systems governed by the coupled Navier–Stokes and heat equations for single-step control. The DRL algorithm used in this paper is the policy-based optimization (PBO) algorithm specialized in single-step (or open-loop) control. We explore its ability to find global maxima in different situations and under various constraints. Therefore, various 2D and 3D cases are tackled, in which the position of the work piece, the flow rate, and other parameters are controlled. The obtained results highlight the potential of the DRL framework combined with computational fluid dynamics (CFD) conjugate heat transfer systems for optimizing searches in large parameter spaces. For the 2D case, PPO achieved an increase of 89% in temperature homogeneity, and for the 3D case an increase of 7% in final temperature with the same total input.}
}
@article{XUE20221,
title = {Jamming attack against remote state estimation over multiple wireless channels: A reinforcement learning based game theoretical approach},
journal = {ISA Transactions},
volume = {130},
pages = {1-9},
year = {2022},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2022.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0019057822001173},
author = {Lei Xue and Bei Ma and Jian Liu and Yao Yu},
keywords = {Nash equilibrium, Potential game, Reinforcement learning, Remote state estimation, Multiple wireless channels, Channel jamming attack},
abstract = {In this paper, we design a potential game to investigate the interplay between sensors and attackers in wireless remote estimation systems (RESs), where we consider both closed-loop and open-loop scenarios that depend on whether the sensor’s actions are known to the attacker or not. As a typical application of Cyber–Physical Systems (CPSs), wireless RESs have been showing vulnerable to channel jamming attacks. As a countermeasure, the wireless sensor in the estimation system can switch to another independent wireless channel to transmit sensor data. However, a smart attacker can also switch the target channel to launch a jamming attack, resulting in a game between the sensor and attacker. Moreover, the reinforcement learning based methods are proposed to obtain solutions of the designed game models in two scenarios. We also provide extensive simulation results to validate the effectiveness of the proposed methods.}
}
@article{ZHANG2022108543,
title = {Auto uning of price prediction models for high-frequency trading via reinforcement learning},
journal = {Pattern Recognition},
volume = {125},
pages = {108543},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108543},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000243},
author = {Weipeng Zhang and Ning Zhang and Junchi Yan and Guofu Li and Xiaokang Yang},
keywords = {High-frequency trading, Inverse reinforcement learning, Parameter optimization, Multi-armed bandit},
abstract = {In this paper, we propose an online model optimization algorithm based on reinforcement learning for quantitative trading. The combination of prediction model and trading policy is the most commonly used framework in practical quantitative trading. Integrated with machine learning methods, this framework brings huge profits to quantified companies. In the framework, the prediction model is used to predict future trading price trend, and the trading policy is used to determine the price and number of orders. Even though, the shortcomings of machine learning models are obvious, mainly are, (1) Slow prediction speed. Huge human-craft features and model computing cost much time, which is ten times of pure trading policy without model. (2) Poor generalization. This kind of models can hardly adapt to market data in each period, because market traders will change time to time at micro level, thus the distribution of market data will change. But current model is trained on a long period dataset, it achieves best effect at average, but can not adapt to different market at each period. To address this problem, we propose a novel online model optimization algorithm. A light model library will be constructed. Each light model in this library corresponds to a different market distribution. By devising the appropriate reward function via inverse reinforcement learning algorithm, the algorithm can accurately estimate the profits of each model. Then the model can be selected automatically in real-time trading, so that the trading policies can automatically adapt to changes in trading market, overcoming previous shortcoming of manually updating model and slow prediction speed. Experimental results show that the proposed algorithm achieves state-of-the-art performance on China Commodity Futures Market Data.}
}
@article{DING2022108120,
title = {A safe reinforcement learning approach for multi-energy management of smart home},
journal = {Electric Power Systems Research},
volume = {210},
pages = {108120},
year = {2022},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2022.108120},
url = {https://www.sciencedirect.com/science/article/pii/S0378779622003443},
author = {Hongyuan Ding and Yan Xu and Benjamin {Chew Si Hao} and Qiaoqiao Li and Antonis Lentzakis},
keywords = {Safe deep reinforcement learning, Constrained Markov decision process, Artificial neural network, Demand response, Home energy management system},
abstract = {This paper proposes a data-driven approach for multi-energy management of a smart home with different types of appliances, including battery energy storage system (BESS), thermal energy storage system (TES), micro combined heat and power system (mCHP), electrical heat pump (EHP), rooftop photovoltaics (PV) and electrical vehicle (EV). Firstly, home energy management (HEM) is formulated as a cost minimization problem with hard constraints to optimize energy generation, storage, and consumption. Secondly, this paper proposes a safe reinforcement learning (SRL) approach with Primal-Dual Optimization (PDO) policy search-based algorithm to solve the HEM problem. Unlike existing DRL methods, the proposed approach learns to minimize costs from accumulated cost functions and automatically tunes the cost function coefficients to achieve zero constraints violation. Besides, a dynamic electricity price forecasting model based on CNN-LSTM neural network is designed to deal with the uncertainties in future prices. To verify the performance of the proposed HEM method, simulations are conducted using Singapore wholesale electricity price data. Numerical results demonstrate that the proposed method has a better ability to minimize energy costs and satisfy constraints compared to existing methods.}
}
@article{REN2020103515,
title = {Advising reinforcement learning toward scaling agents in continuous control environments with sparse rewards},
journal = {Engineering Applications of Artificial Intelligence},
volume = {90},
pages = {103515},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.103515},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620300269},
author = {Hailin Ren and Pinhas Ben-Tzvi},
keywords = {Reinforcement learning, Advising framework, Continuous control, Sparse reward, Multi-agent},
abstract = {This paper adapts the success of the teacher–student framework for reinforcement learning to a continuous control environment with sparse rewards. Furthermore, the proposed advising framework is designed for the scaling agents problem, wherein the student policy is trained to control multiple agents while the teacher policy is well trained for a single agent. Existing research on teacher–student frameworks have been focused on discrete control domain. Moreover, they rely on similar target and source environments and as such they do not allow for scaling the agents. On the other hand, in this work the agents face a scaling agents problem where the value functions of the source and target task converge at different rates. Existing concepts from the teacher–student framework are adapted to meet new challenges including early advising, importance of advising, and mistake correction, but a modified heuristic was used to decide on when to teach. The performance of the proposed algorithm was evaluated using the case study of pushing, and picking and placing objects with a dual arm manipulation system. The teacher policy was trained using a simulated scenario consisting of a single arm. The student policy was trained to handle the dual arm manipulation system in simulation under the advice of the teacher agent. The trained student policy was then validated using two Quanser Mico arms for experimental demonstration. The effects of varying parameters on the student performance in the advising framework was also analyzed and discussed. The results showed that the proposed advising framework expedited the training process and achieved the desired scaling within a limited advising budget.}
}
@incollection{MENG20231667,
title = {Multi-Agent Reinforcement Learning and RL-Based Adaptive PID Control of Crystallization Processes},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {1667-1672},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50265-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152740502651},
author = {Qingbo Meng and Paul Danny Anandan and Chris D. Rielly and Brahim Benyahia},
keywords = {Multi-agent Reinforcement Learning, Transfer Learning, Adaptive PID, Semi-Batch Antisolvent and Cooling Crystallization, 2-Dimensional Crystallization Model},
abstract = {In this work, two model-based reinforcement learning (RL) control strategies are investigated namely a multi-agent RL and RL-based adaptive PID control. An off-policy deep deterministic policy gradient (DDPG) was adopted in both cases to achieve optimal trajectory tracking control of crystallization processes. Two case studies were considered validate the new control strategies. The first is the cooling and antisolvent crystallization of aspirin in a mixture of ethanol and water, and the second is a 2-dimensional (2D) cooling crystallization of potassium dihydrogen phosphate in water. The optimal reference trajectories were identified using model-based dynamic optimization approaches which aim at maximizing the mean crystal size/minimizing the aspect ratio. Transfer Learning (TL) techniques and various reward-shaping strategies were also investigated to enhance the learning capabilities of the RL control. The results indicate that multi-agent RL saves massive training costs, compared to single agent, and RL-based adaptive PID exhibits excellent performance against state-of-the-art MPC.}
}
@article{LI2021109862,
title = {Model-free H∞ tracking control for de-oiling hydrocyclone systems via off-policy reinforcement learning},
journal = {Automatica},
volume = {133},
pages = {109862},
year = {2021},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2021.109862},
url = {https://www.sciencedirect.com/science/article/pii/S0005109821003824},
author = {Shaobao Li and Petar Durdevic and Zhenyu Yang},
keywords = {De-oiling hydrocyclone system, Three-phase separator, Reinforcement learning,  control, Output feedback control, Zero-sum game},
abstract = {In offshore Oil and Gas production, it is important to ensure the quality of discharge water meets government regulations; the de-oiling hydrocyclone system is important for this kind of water treatment. Hydrocyclone’s de-oiling performance is very sensitive to the inflow variation, which is often introduced by the upstream three-phase separator’s operation. Thereby, the coordination control of both the separator and hydrocyclone turns to be crucial. Many model-based control methods can be employed for handling this coordination if mathematical models of the considered systems are developed. However, to develop models of these interacted systems is far more than trivial, with respect to the situation that each offshore de-oiling installation at different fields is a kind of tailor-made solution. Thereby, instead of using conventional model-based control design methods, this paper investigates the reinforcement-learning-based H∞ control method for synthesizing an advanced model-free control solution for the coordination control of the separator’s (water) level and hydrocyclone’s Pressure-Drop-Ratio (PDR). The H∞ tracking control is formulated as a 2-player zero-sum differential game and derived by employing an off-policy reinforcement learning algorithm based on state feedback and output feedback, respectively. Stability and optimality of the proposed solution are analyzed. Finally, simulation studies demonstrate the effectiveness of the proposed solution.}
}
@article{SHADEMAN2023106865,
title = {Safe resource management of non-cooperative microgrids based on deep reinforcement learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106865},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106865},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623010497},
author = {Mahdi Shademan and Hamid Karimi and Shahram Jadid},
keywords = {Bi-level optimization, Deep neural network, Energy management, Multi-microgrid, Reinforcement learning, Transactive energy},
abstract = {This paper solves a bi-level optimization problem in which at the first level, the distribution system operator (DSO) as a retailer tends to extract selling energy price signals so that increases its profit and reduces the peak-to-average ratio (PAR) of the MMG transactive energy; and at the second level energy management problems of non-cooperative networked microgrids (MGs) are solved individually to minimize their cost. This game theoretically problem is known as the Stackelberg game and has been solved with classical optimization methods like Karush–Kuhn–Tucker (KKT) method previously. To solve this problem with the classical KKT method, the MGs’ information should be provided for the DSO as a game starter, which is not the MGs’ desire for their privacy concerns. With the aim of preserving the privacy of MGs, this paper has developed a decision-making mechanism based on reinforcement learning (RL) to help the DSO extract energy prices individually for each MG. Also, the deep neural network (DNN) is used as a practical tool to predict MGs’ behavior in response to signal price. Furthermore, in the model used in this study, energy storage systems (ESSs) are dedicated to MGs which makes the model non-linear, thus the proposed method unlike the classic method is able to solve such a problem. The results obtained from the comparison between the classic and proposed method show that the implementation of this machine learning-based method is not only accurate enough but also is faster than the classical method, in addition to its privilege of privacy-preserving. Finally, a sensitivity analysis has been conducted to evaluate the DSO profit and PAR under different weighting factors of the objective function to obtain an appropriate performance.}
}
@article{YIN2018152,
title = {Automatic selection of fittest energy demand predictors based on cyber swarm optimization and reinforcement learning},
journal = {Applied Soft Computing},
volume = {71},
pages = {152-164},
year = {2018},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.06.042},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618303764},
author = {Peng-Yeng Yin and Ching-Hui Chao},
keywords = {Demand forecasting, Cyber swarm algorithm, Reinforcement learning, Landscape analysis, Optimization},
abstract = {Effective demand forecasting is essential for regulating power distribution, scheduling production, and initiating new energy projects. Existing forecasting models have contrasting features and manifest various types of errors. This paper proposes a multi-predictor approach which applies reinforcement learning for selecting the fittest predictors to enhance the collaborative performance. A new univariate predictor is developed based on the Gaussian mixture and phase shifting and rescaling techniques, and two multivariate predictors are developed from a landscape analysis with potential econometrics. Each individual predictor is trained by the cyber swarm algorithm (CSA) to find the optimal parameter values. The 10-fold cross validation for regression parameter optimization by using CSA and the constriction factor particle swarm optimization (CFPSO) shows the effectiveness of the former against the latter. Our reinforcement learning forecasting method is able to automatically select the best predictor to perform at various time instances and allow the embedding predictors to complement one another. Our experimental results experimented with Taiwan’s electricity demand time series during 2001–2014 show that the prediction improvement contributed by the proposed approach over the original individual predictors is significant in terms of the mean absolute percentage error (MAPE) and the mean square error (MSE).}
}
@article{SIRASKAR2021100030,
title = {Reinforcement learning for control of valves},
journal = {Machine Learning with Applications},
volume = {4},
pages = {100030},
year = {2021},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2021.100030},
url = {https://www.sciencedirect.com/science/article/pii/S2666827021000116},
author = {Rajesh Siraskar},
keywords = {Curriculum learning, Graded learning, MATLAB, Optimal-control, Reinforcement learning, Valve control},
abstract = {This paper is a study of reinforcement learning (RL) as an optimal-control strategy for control of nonlinear valves. It is evaluated against the PID (proportional–integral–derivative) strategy, using a unified framework. RL is an autonomous learning mechanism that learns by interacting with its environment. It is gaining increasing attention in the world of control systems as a means of building optimal-controllers for challenging dynamic and nonlinear processes. Published RL research often uses open-source tools (Python and OpenAI Gym environments). We use MATLAB’s recently launched (R2019a) Reinforcement Learning Toolbox\texttrademark to develop the valve controller; trained using the DDPG (Deep Deterministic Policy-Gradient) algorithm and Simulink® to simulate the nonlinear valve and create the experimental test-bench for evaluation. Simulink allows industrial engineers to quickly adapt and experiment with other systems of their choice. Results indicate that the RL controller is extremely good at tracking the signal with speed and produces a lower error with respect to the reference signal. The PID, however, is better at disturbance rejection and hence provides a longer life for the valves. Successful machine learning involves tuning many hyperparameters requiring significant investment of time and efforts. We introduce “Graded Learning” as a simplified, application oriented adaptation of the more formal and algorithmic “Curriculum for Reinforcement Learning”. It is shown via experiments that it helps converge the learning task of complex non-linear real world systems. Finally, experiential learnings gained from this research are corroborated against published research.}
}
@article{WANG20208150,
title = {Multi-agent Formation Control with Obstacles Avoidance under Restricted Communication through Graph Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {8150-8156},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2300},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320329669},
author = {Huimu Wang and Tenghai Qiu and Zhen Liu and Zhiqiang Pu and Jianqiang Yi},
keywords = {Reinforcement learning control, Multi-agent system, Deep reinforcement learning, Graph attention network, Formation control, Obstacles avoidance},
abstract = {Multi-agent formation control with obstacles avoidance (MAFC-OA) is one of the attractive tasks of multi-agent cooperation. Although a number of algorithms can achieve formation control effectively, they ignore the nature structure feature of the graph formed by agents. Given this problem, a model, MAFC-OA, which is composed of observation attention network, action attention network and Multi-long short-term memory (Multi-LSTM) is proposed. With MAFC-OA, the agents can be trained to form the desired formation and avoid dynamic obstacles in the environments with restricted communication. Specifically, the above two attention networks not only incorporate the influence of the nearby agents’ observation and actions, but also enlarge the agents’ receptive field (communication range) through the chain propagation characteristics to promote cooperation among agents. Moreover, the Multi-LSTM allows the agents to take obstacles into consideration in the order of distance and to avoid the obstacles effectively. Simulations demonstrate that the agents can form the desired formation and avoid dynamic obstacles effectively.}
}
@article{JIANG2016176,
title = {Optimal tracking control for completely unknown nonlinear discrete-time Markov jump systems using data-based reinforcement learning method},
journal = {Neurocomputing},
volume = {194},
pages = {176-182},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216002447},
author = {He Jiang and Huaguang Zhang and Yanhong Luo and Junyi Wang},
keywords = {Optimal tracking control, Markov jump systems, Data-based, Reinforcement learning, Adaptive dynamic programming, Neural networks},
abstract = {In this paper, we develop a novel optimal tracking control scheme for a class of nonlinear discrete-time Markov jump systems (MJSs) by utilizing a data-based reinforcement learning method. It is not practical to obtain accurate system models of the real-world MJSs due to the existence of abrupt variations in their system structures. Consequently, most traditional model-based methods for MJSs are invalid for the practical engineering applications. In order to overcome the difficulties without any identification scheme which would cause estimation errors, a model-free adaptive dynamic programming (ADP) algorithm will be designed by using system data rather than accurate system functions. Firstly, we combine the tracking error dynamics and reference system dynamics to form an augmented system. Then, based on the augmented system, a new performance index function with discount factor is formulated for the optimal tracking control problem via Markov chain and weighted sum technique. Neural networks are employed to implement the on-line ADP learning algorithm. Finally, a simulation example is given to demonstrate the effectiveness of our proposed approach.}
}
@article{PARK2022136364,
title = {Deep reinforcement learning in an ultrafiltration system: Optimizing operating pressure and chemical cleaning conditions},
journal = {Chemosphere},
volume = {308},
pages = {136364},
year = {2022},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2022.136364},
url = {https://www.sciencedirect.com/science/article/pii/S0045653522028570},
author = {Sanghun Park and Jaegyu Shim and Nakyung Yoon and Sungman Lee and Donggeun Kwak and Seungyong Lee and Young Mo Kim and Moon Son and Kyung Hwa Cho},
keywords = {Deep reinforcement learning, Machine learning, Ultrafiltration, Chemical cleaning, Optimization},
abstract = {Enhancing engineering efficiency and reducing operating costs are permanent subjects that face all engineers over the world. To effectively improve the performance of filtration systems, it is necessary to determine an optimal operating condition beyond conventional methods of periodic and empirical operation. Herein, this paper proposes an effective approach to finding an optimal operating strategy using deep reinforcement learning (DRL), particularly for an ultrafiltration (UF) system. Deep learning was developed to represent the UF system utilizing a long-short term memory and provided an environment for DRL. DRL was designed to control three actions; operating pressure, cleaning time, and cleaning concentration. Ultimately, DRL proposed the UF system to actively change the operating pressure and cleaning conditions over time toward better water productivity and operating efficiency. DRL denoted ∼20.9% of specific energy consumption can be reduced by increasing average water flux (39.5–43.7 L m−2 h−1) and reducing operating pressure (0.617–0.540 bar). Moreover, the optimal action of DRL was reasonable to achieve better performance beyond the conventional operation. Crucially, this study demonstrated that due to the nature of DRL, the approach is tractable for engineering systems that have structurally complex relationships among operating conditions and resultants.}
}
@article{XU202012103,
title = {Model-Free Optimization Scheme for Efficiency Improvement of Wind Farm Using Decentralized Reinforcement Learning⁎⁎This work was supported by the National Natural Science Foundation of China under Grants 61722307 and 5191101838.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {12103-12108},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.767},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320310910},
author = {Zhiwei Xu and Hua Geng and Bing Chu and Menghao Qian and Ni Tan},
keywords = {Wind farm, power optimization, model-free approach, decentralized control, Q learning method},
abstract = {Wake interactions caused by the complex wakes between the turbines within a wind farm have significant adverse effect on the total power generation of the wind farm. To mitigate the effect of wake interactions and optimize the total power output of wind farm, this paper proposes a model-free control scheme using reinforcement learning by developing a decentralized Q learning method. The proposed approach guarantees that the output power of wind farm converges to the optimal total power under different wind conditions, and further ensures the gradual changes of control variables of wind turbines and thus avoids the unexpected sharp drop of the power generation performance of wind farm. Simulation results are provided to demonstrate the effectiveness of the proposed method.}
}
@article{HWANG2022109386,
title = {Deep reinforcement learning with a critic-value-based branch tree for the inverse design of two-dimensional optical devices},
journal = {Applied Soft Computing},
volume = {127},
pages = {109386},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109386},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622005312},
author = {Hyo-Seok Hwang and Minhyeok Lee and Junhee Seok},
keywords = {Deep learning, Deep reinforcement learning, Multitask reinforcement learning, Device design automation},
abstract = {In optical engineering, designing a device or a system with the desired property is an important but challenging task. It is relatively straightforward to compute the physical properties of a given design; however, there is no general method for the reverse, i.e., designing with desired properties. To address this problem with a computational method, this paper proposes a deep reinforcement learning-based inverse design framework consisting of two methods: Inverse DEsign Agent (IDEA) and Critic-Value-based Branch Tree (CVBT) algorithm. IDEA is a deep reinforcement learning model based on the Advantage Actor–Critic (A2C) method using a deep learning simulator as a replacement for a real environment, significantly reducing training time compared to conventional methods. The CVBT algorithm suggests several design candidates using the critic values of IDEA, while conventional methods propose only one candidate. In this study, the proposed framework was applied to a two-dimensional optical device design problem. Experimental results with untrained target properties demonstrated that the proposed model, IDEA-CVBT, achieved state-of-the-art performance in terms of accuracy and stability. For instance, in a scenario with a binary type design, IDEA-CVBT exhibited an accuracy of 91.5%, while conventional deep reinforcement models showed 36.1% and 7.4% accuracies. Extensive analyses verified that IDEA-CVBT can be employed as an assistance system for engineers since IDEA-CVBT suggests several appropriate candidates that satisfy target properties.}
}
@article{ZHANG2023133,
title = {Enhanced reinforcement learning in two-layer economic model predictive control for operation optimization in dynamic environment},
journal = {Chemical Engineering Research and Design},
volume = {196},
pages = {133-143},
year = {2023},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2023.06.023},
url = {https://www.sciencedirect.com/science/article/pii/S0263876223003842},
author = {Zengjun Zhang and Shaoyuan Li},
keywords = {Economic model predictive control, Reinforcement learning, Dynamic optimization, Process control},
abstract = {Economic model predictive control (EMPC) has attracted an abundance of interest in both academic and industrial communities in recent years because it is able to increase the economic profits of dynamic systems. For greater computational efficiency, two-layer EMPC schemes are applied in some complex process control. However, the performance of two-layer EMPC is significantly influenced by the accuracy of the chosen process model. Reinforcement learning (RL) has been studied as a model-free strategy of model-based control approaches, but its safety and stability remain a concern. In order to estimate the model parameters of nonlinear dynamic systems in real time, this work introduces a unique scheme for merging two-layer EMPC and RL. In this scheme, the two-layer EMPC technique maintains closed-loop stability and recursive feasibility while operating the closed-loop dynamic system optimally. And the RL agent continually compares the observed states to the predictions made by the EMPC and modifies the time-varying parameters as necessary. The usability of the proposed scheme is shown on a chemical process of ethylene oxide production in dynamic environment. This work enables online and continuous control, optimization, and model correction, and makes process production optimization more feasible and profitable.}
}
@article{LI2020157,
title = {Accelerating deep reinforcement learning model for game strategy},
journal = {Neurocomputing},
volume = {408},
pages = {157-168},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.06.110},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220303337},
author = {Yifan Li and Yuchun Fang and Zahid Akhtar},
keywords = {Deep reinforcement learning, Convolutional neural network, Depthwise separable convolution, Binary weight network},
abstract = {In recent years, deep reinforcement learning has achieved impressing accuracies in games compared with traditional methods. Prior schemes utilized Convolutional Neural Networks (CNNs) or Long Short-Term Memory networks (LSTMs) to improve the performances of the agents. In this paper, we consider the issue from a different perspective when the training and inference of deep reinforcement learning are required to be performed with limited computing resources. Mainly, we propose two efficient neural network architectures of deep reinforcement learning: Light-Q-Network (LQN) and Binary-Q-Network (BQN). In LQN, The depth-wise separable CNNs are utilized in memory and computation saving. While, in BQN, the weights of convolutional layers are binary that help in shortening the training time and reduce memory consumption. We evaluate our approach on Atari 2600 domain and StarCraft II mini-games. The results demonstratethe efficiency of the proposed architectures. Though performances of agents in most games are still super-human, the proposed methods advance the agent from sub to super-human performance in particular games. Also, we empirically find that non-standard convolution and non-full-precision networks do not affect agent learning game strategy.}
}
@article{LIU2023562,
title = {Safe reinforcement learning for affine nonlinear systems with state constraints and input saturation using control barrier functions},
journal = {Neurocomputing},
volume = {518},
pages = {562-576},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222013790},
author = {Shihan Liu and Lijun Liu and Zhen Yu},
keywords = {Reinforcement learning, Policy iteration, Control barrier function, Safety, State constraints, Input saturation},
abstract = {This paper provides a novel safe reinforcement learning (RL) control algorithm to solve safe optimal problems for discrete-time affine nonlinear systems, while the safety and convergence of the control algorithm are proven. The algorithm is proposed based on an adjusted policy iteration (PI) framework using only the measured data along the system trajectories in the environment. The adjusted PI algorithm combines with the system predictive information. Unlike most PI algorithms, an effective method of obtaining an initial safe and stable control policy is given here. In addition, control barrier functions (CBFs) and an input constraint function are introduced to augment reward functions. And the monotonically nonincreasing property of the iterative value function maintains the safe set forward invariant in the PI framework. Moreover, the safety and convergence of the proposed algorithm are proven in theory. Then, the design and implementation of the proposed algorithm are presented based on the identifier-actor-critic structure, where neural networks are employed to approximate the system dynamics, the iterative control policy, and the iterative value function, respectively. Finally, the simulation results illustrate the effectiveness and safety of the proposed algorithm.}
}
@article{GHORBEL20151061,
title = {Forward management of spare parts stock shortages via causal reasoning using reinforcement learning},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {3},
pages = {1061-1066},
year = {2015},
note = {15th IFAC Symposium onInformation Control Problems inManufacturing},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.06.224},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315004632},
author = {N. Ghorbel and S-A. Addouche and A. El Mhamedi},
keywords = {Supply chain, inventory management, replenishment cost, risk management of stock rupture, spare parts, performance evaluation},
abstract = {Our role in this paper was to search the appropriate means serving as a decision support tool for the choice of a policy and procurement planning of spare parts, contributing to the maintenance in operational conditions of industrial equipment and enabling to avoid stock outs and all at a lower cost. For this, we present a generic Bayesian model of consumption of spare parts in a replenishment policy type (T, s, S) adapted (mT, s *, S). The originality of this research is the fact that we characterize the process of consumption of spare parts by a set of typical scenarios, called "consumption configurations" and identified by a system of performance indicators using variables state in a Bayesian model. After defining all these indicators, the research enchain to deploy a Bayesian network which allow, through Bayesian simulation, obtaining a replenishment planning indicating the optimal combination by period: the durations of these replenishment periods, quantities to purchased, types of SP (new or revalorized), costs and associated risk of rupture, purchasing costs and induced storage.}
}
@article{KWON2008389,
title = {Case-based myopic reinforcement learning for satisfying target service level in supply chain},
journal = {Expert Systems with Applications},
volume = {35},
number = {1},
pages = {389-397},
year = {2008},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2007.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0957417407002576},
author = {Ick-Hyun Kwon and Chang Ouk Kim and Jin Jun and Jung Hoon Lee},
keywords = {Reinforcement learning, Case-based reasoning, Supply chain, Inventory control, Service level},
abstract = {In the last decade, driven by global competition in the marketplace, many companies have taken initiatives to revamp their supply chains in order to increase responsiveness to changes in the marketplace. The renovation of inventory control system is central to such an effort. However, experiences in industry have shown that the control of inventory in supply chain is not an easy task because of uncertainties inherent in customer demand. In this paper, we propose a reinforcement learning algorithm appropriate for the nonstationary inventory control problem of supply chain that has a large state space. Traditional reinforcement learning algorithms such as learning automata and Q-learning have the difficulty of slow convergence when applied to the situations with large state spaces. To resolve the problems of nonstationary customer demand and large state space, we develop a case-based myopic reinforcement learning (CMRL) algorithm. A simulation-based experiment was performed to show good performance of CMRL.}
}
@article{LIU2023912,
title = {Distributed game coordination control method based on reinforcement learning for multi-microgrid},
journal = {Energy Reports},
volume = {9},
pages = {912-921},
year = {2023},
note = {2022 The 3rd International Conference on Power Engineering},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2023.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S2352484723007576},
author = {Wei Liu and Sicong Zhang and Xiao-Ping Zhang and Zhihao Qian and Tianhuan Hu},
keywords = {Multi-microgrid (MMG), Potential game (PG), -learning, Distributed coordinated control},
abstract = {The traditional centralized control method hardly meets the coordination control demand for multi-microgrid (MMG), due to the conflict between the individual interests of a single microgrid (MG) and the global interests of MMG. In this study, a distributed coordination control method that integrates potential game (PG) and reinforcement learning (RL) is proposed to achieve balance of interests of an MMG. The proposed method fully exploits the distributed characteristic of the PG by considering each MG as an agent. It also establishes a PG-based distributed coordination control structure to maximize and balance the economy of single MG and overall MMG. Then, it combines the PG with the RL algorithm by the parameter transfer to obtain the optimal Nash equilibrium (NE) solution and improve the optimization performance based on Q-learning algorithm. Eventually, a simulation model is performed in MATLAB to demonstrate the effectiveness and superiority of the proposed control method.}
}
@article{WU2023110999,
title = {Deep reinforcement learning control approach to mitigating actuator attacks},
journal = {Automatica},
volume = {152},
pages = {110999},
year = {2023},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2023.110999},
url = {https://www.sciencedirect.com/science/article/pii/S0005109823001528},
author = {Chengwei Wu and Wei Pan and Rick Staa and Jianxing Liu and Guanghui Sun and Ligang Wu},
keywords = {Cyber–physical systems, False data injection attacks, Deep reinforcement learning, Lyapunov stability},
abstract = {This paper investigates the deep reinforcement learning based secure control problem for cyber–physical systems (CPS) under false data injection attacks. We describe the CPS under attacks as a Markov decision process (MDP), based on which the secure controller design for CPS under attacks is formulated as an action policy learning using data. Rendering the soft actor–critic learning algorithm, a Lyapunov-based soft actor–critic learning algorithm is proposed to offline train a secure policy for CPS under attacks. Different from the existing results, not only the convergence of the learning algorithm but the stability of the system using the learned policy is proved, which is quite important for security and stability-critical applications. Finally, both a satellite attitude control system and a robot arm system are used to show the effectiveness of the proposed scheme, and comparisons between the proposed learning algorithm and the classical PD controller are also provided to demonstrate the advantages of the control algorithm designed in this paper.}
}
@article{LI2023120540,
title = {Optimal scheduling of island integrated energy systems considering multi-uncertainties and hydrothermal simultaneous transmission: A deep reinforcement learning approach},
journal = {Applied Energy},
volume = {333},
pages = {120540},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120540},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922017974},
author = {Yang Li and Fanjin Bu and Yuanzheng Li and Chao Long},
keywords = {Island integrated energy system, Deep reinforcement learning, Multi-uncertainties, Desalination, Hydrothermal simultaneous transmission, Optimal scheduling},
abstract = {Multi-uncertainties from power sources and loads have brought significant challenges to the stable demand supply of various resources at islands. To address these challenges, a comprehensive scheduling framework is proposed by introducing a model-free deep reinforcement learning (DRL) approach based on modeling an island integrated energy system (IES). In response to the shortage of freshwater on islands, in addition to the introduction of seawater desalination systems, a transmission structure of “hydrothermal simultaneous transmission” (HST) is proposed. The essence of the IES scheduling problem is the optimal combination of each unit’s output, which is a typical timing control problem and conforms to the Markov decision-making solution framework of deep reinforcement learning. Deep reinforcement learning adapts to various changes and timely adjusts strategies through the interaction of agents and the environment, avoiding complicated modeling and prediction of multi-uncertainties. The simulation results show that the proposed scheduling framework properly handles multi-uncertainties from power sources and loads, achieves a stable demand supply for various resources, and has better performance than other real-time scheduling methods, especially in terms of computational efficiency. In addition, the HST model constitutes an active exploration to improve the utilization efficiency of island freshwater.}
}
@article{JIANG20171,
title = {Neural-network-based control scheme for a class of nonlinear systems with actuator faults via data-driven reinforcement learning method},
journal = {Neurocomputing},
volume = {239},
pages = {1-8},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.01.047},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217301212},
author = {He Jiang and Huaguang Zhang and Yang Liu and Ji Han},
keywords = {Reinforcement learning, Adaptive dynamic programming, Data-driven, Model-free, Neural networks},
abstract = {This paper investigates the fault tolerant control problem for a class of continuous-time nonlinear systems with completely unknown dynamics via the data-based adaptive dynamic programming method. The proposed controller can be divided into two parts: (1) optimal control policy of the fault-free systems and (2) fault compensator. Firstly, a model-based policy iteration algorithm is introduced to obtain the optimal control law. Subsequently, a fault compensator is derived to get rid of the impact of the actuator fault. The stability analysis of the model-based control scheme is presented by using Lyapunov theory. However, for the complex practical systems, system models are generally unavailable, and thus the model-based approaches may be invalid. To overcome this difficulty, we provide a data-driven reinforcement learning method and an identification approach to design the two parts of the proposed controller, respectively, without any knowledge of the system models. Neural networks are employed to implement these two data-based methods. Finally, two simulation examples are shown in details to demonstrate the effectiveness of our proposed scheme.}
}
@article{ZHU2021104490,
title = {Deep reinforcement learning-based radio function deployment for secure and resource-efficient NG-RAN slicing},
journal = {Engineering Applications of Artificial Intelligence},
volume = {106},
pages = {104490},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104490},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621003389},
author = {Pengfei Zhu and Jiawei Zhang and Yuming Xiao and Jiabin Cui and Lin Bai and Yuefeng Ji},
keywords = {Deep reinforcement learning, Wolpertinger policy, NG-RAN slicing, Security, Resource efficiency},
abstract = {Network functions virtualization is a prominent technology for next-generation radio access network (NG-RAN) slicing to achieve customization for various vertical services, such as auto-manufacturing and auto-driving. However, when virtualized radio function blocks with different security levels of services share a common server to achieve network resource-saving, a co-resident threat is exposed due to the lack of physical isolation. Therefore, designing a secure and efficient NG-RAN slicing strategy is necessary but difficult, especially for such distributed networks in which different tenants have distinct network bandwidth and security level requirements. To this end, we first formulate this slicing problem as an ILP model called Secure isolation and resource Efficiency-oriented Multi-Objective NG-RAN Slicing (SEMONS). However, the SEMONS is not practical for online execution due to its high computational complexity. Then, we propose a secure and efficient RAN slicing method for fast online execution based on a deep reinforcement learning (DRL) framework. The Wolpertinger policy algorithm is leveraged to train the agent in the DRL framework, which combines the deep deterministic policy gradient with the K-Nearest-Neighbor algorithm to reduce the size of the exploration space. Then the training complexity is reduced, and the learning results are optimized. Extensive simulation results show that the DRL framework can obtain the near-optimal secure and efficient NG-RAN slicing strategies compared to the SEMONS model, with only 6% target deviation on average, and it also outperforms the greedy baseline algorithm by 14.5%.}
}
@article{JENDOUBI2023120500,
title = {Multi-agent hierarchical reinforcement learning for energy management},
journal = {Applied Energy},
volume = {332},
pages = {120500},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120500},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922017573},
author = {Imen Jendoubi and François Bouffard},
keywords = {Eco-neighborhood, Energy management, Hierarchical reinforcement learning, Microgrid, Multi-agent reinforcement learning, Options’ framework},
abstract = {The increasingly complex energy systems are turning the attention towards model-free control approaches such as reinforcement learning (RL). This work proposes novel RL-based energy management approaches for scheduling the operation of controllable devices within an electric network. The proposed approaches provide a tool for efficiently solving multi-dimensional, multi-objective and partially observable power system problems. The novelty in this work is threefold: We implement a hierarchical RL-based control strategy to solve a typical energy scheduling problem. Second, multi-agent reinforcement learning (MARL) is put forward to efficiently coordinate different units with no communication burden. Third, a control strategy that merges hierarchical RL and MARL theory is proposed for a robust control framework that can handle complex power system problems. A comparative performance evaluation of various RL-based and model-based control approaches is also presented. Experimental results of three typical energy dispatch scenarios show the effectiveness of the proposed control framework.}
}
@article{SAAD20223662,
title = {RAMARL: Robustness Analysis with Multi-Agent Reinforcement Learning - Robust Reasoning in Autonomous Cyber-Physical Systems},
journal = {Procedia Computer Science},
volume = {207},
pages = {3662-3671},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.426},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922013199},
author = {Aya Saad and Anne Håkansson},
keywords = {Robust reasoning, robustness analysis, autonomous cyber-physical systems (CPS), dynamic environment, Reinforcement Learning},
abstract = {A key driver to ofering smart services is an infrastructure of Cyber-Physical systems (CPS)s. By definition, CPSs are intertwined physical and computational components that integrate physical behaviour with computation. The reason is to autonomously execute a task or a set of tasks providing a service or a list of end-users services. In real-life applications, CPSs operate in dynamically changing surroundings characterized by unexpected or unpredictable situations. Such operations involve complex interactions between multiple intelligent agents in a highly non-stationary environment. For safety reasons, a CPS should withstand a certain amount of disruption and exert the operations in a stable and robust manner when performing complex tasks. Recent advances in reinforcement learning have proven suitable for enabling multi-agents to robustly adapt to their environment, yet they often depend on a massive amount of training data and experiences. In these cases, robustness analysis outlines necessary components and specifications in a framework, ensuring reliable and stable behaviour while considering the dynamicity of the environment. This paper presents a combination of multi-agent reinforcement learning with robustness analysis shaping a cyber-physical system infrastructure that reasons robustly in a dynamically changing environment. The combination strengthens the reinforcement learning, increasing the reliability and flexibility of the system by applying robustness analysis. Robustness analysis identifies vulnerability issues when the system interacts within a dynamically changing environment. Based on this identification, when incorporated into the system, robustness analysis suggests robust solutions and actions rather than optimal ones provided by reinforcement learning alone. Results from the combination show that this infrastructure can enable reliable operations with the flexibility to adapt to the changing environment dynamics.}
}
@article{XU2022181,
title = {Path planning and dynamic collision avoidance algorithm under COLREGs via deep reinforcement learning},
journal = {Neurocomputing},
volume = {468},
pages = {181-197},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.09.071},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221014491},
author = {Xinli Xu and Peng Cai and Zahoor Ahmed and Vidya Sagar Yellapu and Weidong Zhang},
keywords = {Unmanned surface vehicle, COLREGs, Dynamic collision avoidance, DDPG, Cumulative priority sampling mechanism},
abstract = {As one of the core technologies of the automatic control system for unmanned surface vehicles (USVs), autonomous collision avoidance algorithm is the key to ensure the safe navigation of USVs. In this paper, path planning and dynamic collision avoidance (PPDC) algorithm which obeys COLREGs is proposed for USVs. In order to avoid unnecessary collision avoidance actions, the risk assessment model is developed, which is used to determine the switching time of path planning and dynamic collision avoidance. In order to train the algorithm which complies with the COLREGs, the encounter situation is divided quantitatively, which is regarded as the input state of the system, so that the high-dimensional input is successfully avoided. The state space of the USV is defined by relative parameters to improve the generalization ability of the algorithm, meanwhile, a network structure based on DDPG is designed to achieve the continuous output of thrust and rudder angle. Combined with path planning, collision avoidance, compliance with COLREGs and smooth arrival task, four kinds of reward functions are designed. In order to solve the problem of low training efficiency of experience replay mechanism in DDPG, cumulative priority sampling mechanism is proposed. Through the simulation and verification in a variety of scenarios, it is proved that PPDC algorithm has the function of path planning and dynamic collision avoidance in compliance with COLREGs, which has good real-time performance and security.}
}
@article{LAZARIDIS202286,
title = {REIN-2: Giving birth to prepared reinforcement learning agents using reinforcement learning agents},
journal = {Neurocomputing},
volume = {497},
pages = {86-93},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222005331},
author = {Aristotelis Lazaridis and Ioannis Vlahavas},
keywords = {Deep reinforcement learning, Meta-learning, Neural networks, Games},
abstract = {Deep Reinforcement Learning (Deep RL) has been in the spotlight for the past few years, due to its remarkable abilities to solve problems which were considered to be practically unsolvable using traditional Machine Learning methods. However, even state-of-the-art Deep RL algorithms have various weaknesses that prevent them from being used extensively within industry applications, with one such major weakness being their sample-inefficiency. In an effort to patch these issues, we integrated a meta-learning technique in order to shift the objective of learning to solve a task into the objective of learning how to learn to solve a task (or a set of tasks), which we empirically show that improves overall stability and performance of Deep RL algorithms. Our model, named REIN-2, is a meta-learning scheme formulated within the RL framework, the goal of which is to develop a meta-RL agent (meta-learner) that learns how to produce other RL agents (inner-learners) that are capable of solving given environments. For this task, we convert the typical interaction of an RL agent with the environment into a new, single environment for the meta-learner to interact with. Compared to traditional state-of-the-art Deep RL algorithms, experimental results show remarkable performance of our model in popular OpenAI Gym environments in terms of scoring and sample efficiency, including the Mountain Car hard-exploration environment.}
}
@article{ZHANG2021166,
title = {Ground maneuver for front-wheel drive aircraft via deep reinforcement learning},
journal = {Chinese Journal of Aeronautics},
volume = {34},
number = {10},
pages = {166-176},
year = {2021},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2021.03.029},
url = {https://www.sciencedirect.com/science/article/pii/S1000936121001205},
author = {Hao ZHANG and Zongxia JIAO and Yaoxing SHANG and Xiaochao LIU and Pengyuan QI and Shuai WU},
keywords = {Front-wheel drive aircraft, Ground maneuver, Nose wheel steering, Tire model, Magic equation, Deep reinforcement learning},
abstract = {The maneuvering time on the ground accounts for 10%–30% of their flight time, and it always exceeds 50% for short-haul aircraft when the ground traffic is congested. Aircraft also contribute significantly to emissions, fuel burn, and noise when taxiing on the ground at airports. There is an urgent need to reduce aircraft taxiing time on the ground. However, it is too expensive for airports and aircraft carriers to build and maintain more runways, and it is space-limited to tow the aircraft fast using tractors. Autonomous drive capability is currently the best solution for aircraft, which can save the maneuver time for aircraft. An idea is proposed that the wheels are driven by APU-powered (auxiliary power unit) motors, APU is working on its efficient point; consequently, the emissions, fuel burn, and noise will be reduced significantly. For Front-wheel drive aircraft, the front wheel must provide longitudinal force to tow the plane forward and lateral force to help the aircraft make a turn. Forward traction effects the aircraft’s maximum turning ability, which is difficult to be modeled to guide the controller design. Deep reinforcement learning provides a powerful tool to help us design controllers for black-box models; however, the models of related works are always simplified, fixed, or not easily modified, but that is what we care about most. Only with complex models can the trained controller be intelligent. High-fidelity models that can easily modified are necessary for aircraft ground maneuver controller design. This paper focuses on the maneuvering problem of front-wheel drive aircraft, a high-fidelity aircraft taxiing dynamic model is established, including the 6-DOF airframe, landing gears, and nonlinear tire force model. A deep reinforcement learning based controller was designed to improve the maneuver performance of front-wheel drive aircraft. It is proved that in some conditions, the DRL based controller outperformed conventional look-ahead controllers.}
}
@article{QIAN202165,
title = {Zero-shot policy generation in lifelong reinforcement learning},
journal = {Neurocomputing},
volume = {446},
pages = {65-73},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.02.058},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221003143},
author = {Yi-Ming Qian and Fang-Zhou Xiong and Zhi-Yong Liu},
keywords = {Lifelong reinforcement learning, Generalization policy, Task domain},
abstract = {Lifelong reinforcement learning (LRL) is an important approach to achieve continual lifelong learning of multiple reinforcement learning tasks. The two major methods used in LRL are task decomposition and policy knowledge extraction. Policy knowledge extraction method in LRL can share knowledge for tasks in different task domains and for tasks in the same task domain with different system environmental coefficients. However, the generalization ability of policy knowledge extraction method is limited on learned tasks rather than learned task domains. In this paper, we propose a cross-domain lifelong reinforcement learning algorithm with zero-shot policy generation ability (CDLRL-ZPG) to improve generalization ability of policy knowledge extraction method from learned tasks to learned task domains. In experiments, we evaluated CDLRL-ZPG performance on four task domains. And our results show that the proposed algorithm can directly generate satisfactory results without needing a trial and error learning process to achieve zero-shot learning in general.}
}
@article{WU202375,
title = {Toward Human-in-the-Loop AI: Enhancing Deep Reinforcement Learning via Real-Time Human Guidance for Autonomous Driving},
journal = {Engineering},
volume = {21},
pages = {75-91},
year = {2023},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S2095809922004878},
author = {Jingda Wu and Zhiyu Huang and Zhongxu Hu and Chen Lv},
keywords = {Human-in-the-loop AI, Deep reinforcement learning, Human guidance, Autonomous driving},
abstract = {Due to its limited intelligence and abilities, machine learning is currently unable to handle various situations thus cannot completely replace humans in real-world applications. Because humans exhibit robustness and adaptability in complex scenarios, it is crucial to introduce humans into the training loop of artificial intelligence (AI), leveraging human intelligence to further advance machine learning algorithms. In this study, a real-time human-guidance-based (Hug)-deep reinforcement learning (DRL) method is developed for policy training in an end-to-end autonomous driving case. With our newly designed mechanism for control transfer between humans and automation, humans are able to intervene and correct the agent’s unreasonable actions in real time when necessary during the model training process. Based on this human-in-the-loop guidance mechanism, an improved actor-critic architecture with modified policy and value networks is developed. The fast convergence of the proposed Hug-DRL allows real-time human guidance actions to be fused into the agent’s training loop, further improving the efficiency and performance of DRL. The developed method is validated by human-in-the-loop experiments with 40 subjects and compared with other state-of-the-art learning approaches. The results suggest that the proposed method can effectively enhance the training efficiency and performance of the DRL algorithm under human guidance without imposing specific requirements on participants’ expertise or experience.}
}
@article{YU2022109458,
title = {Energy-efficient personalized thermal comfort control in office buildings based on multi-agent deep reinforcement learning},
journal = {Building and Environment},
volume = {223},
pages = {109458},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.109458},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322006898},
author = {Liang Yu and Zhanbo Xu and Tengfei Zhang and Xiaohong Guan and Dong Yue},
keywords = {Office buildings, HVAC systems, Personal comfort systems, Personalized thermal comfort control, Energy-efficient, Multi-agent deep reinforcement learning},
abstract = {In a shared office space, the percentage of occupants with satisfied thermal comfort is typically low. The main reason is that heating, ventilation, and air conditioning (HVAC) systems cannot provide individual thermal environment for each occupant within the shared office space. Although personal comfort systems (PCSs) can be adopted to implement heterogeneous thermal environments, they have limited adjustment abilities. At this time, coordinating the operations of PCSs and an HVAC system is a good choice. In this paper, the coordination control problem of PCSs and an HVAC system in a shared office space is investigated to minimize the total energy consumption while maintaining comfortable individual thermal environment for each occupant. Specifically, we first formulate an expected energy consumption minimization problem related to PCSs and an HVAC system. Due to the existence of an inexplicit building thermal dynamics model and uncertain parameters, it is challenging to solve the problem. To overcome the challenge, we reformulate the problem as a Markov game with heterogeneous agents. To promote an efficient cooperation of such agents, we propose a real-time control algorithm based on attention-based multi-agent deep reinforcement learning, which does not require an explicit building thermal dynamics model and any prior knowledge of uncertain parameters. Simulation results based on real-world traces show that the proposed algorithm can reduce energy consumption by 0.7%–4.18% and reduce average thermal comfort deviation by 64.13%–72.08% simultaneously compared with baselines.}
}
@article{WU2023108388,
title = {Space manipulator optimal impedance control using integral reinforcement learning},
journal = {Aerospace Science and Technology},
volume = {139},
pages = {108388},
year = {2023},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2023.108388},
url = {https://www.sciencedirect.com/science/article/pii/S1270963823002857},
author = {Han Wu and Qinglei Hu and Yongxia Shi and Jianying Zheng and Kaipeng Sun and Jiawen Wang},
keywords = {Optimal impedance control, Integral reinforcement learning (IRL), State reconstruction, Unknown contact dynamics, Space manipulator},
abstract = {This paper examines the optimal impedance control problem for large-scale space manipulator operational tasks with unknown contact dynamics and partial measurements. More specifically, by quantifying the interaction performance using a discounted value function, the optimal impedance control problem is tactfully transformed into a linear quadratic tracking problem. By resorting to the historical inputs and outputs, an improved state reconstruction method is presented, which obviates the velocity measurement. Unlike the existing state reconstruction method for continuous-time systems, the estimation bias caused by probing noise is completely eliminated under the improved state reconstruction method. Based on this, a novel model-free value iteration integral reinforcement learning algorithm is developed to approximate optimal impedance parameters. Compared with the earlier integral reinforcement learning algorithms, the proposed algorithm not only averts any prior contact dynamics knowledge and full-state measurements, but also eliminates the heavy dependence on the specific initial stabilization control. In addition, the implementation and convergence of the proposed algorithm are discussed successively. Finally, numerical simulations verify the effectiveness of the theoretical results.}
}
@article{LAN2023104385,
title = {Efficient reinforcement learning with least-squares soft Bellman residual for robotic grasping},
journal = {Robotics and Autonomous Systems},
volume = {164},
pages = {104385},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104385},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000246},
author = {Yixing Lan and Junkai Ren and Tao Tang and Xin Xu and Yifei Shi and Zixin Tang},
keywords = {Reinforcement learning, Policy evaluation, Robotic grasping, Sparse kernel},
abstract = {Grasping control of intelligent robots has to deal with the difficulties of model uncertainties and nonlinearities. In this paper, we propose the Kernel-based Least-Squares Soft Bellman residual Actor–Critic (KLSAC) algorithm for robotic grasping. In the proposed approach, a novel linear temporal-difference learning algorithm using the least-squares soft Bellman residual (LS2BR) method is designed for policy evaluation. In addition, KLSAC adopts a sparse-kernel feature representation method based on approximate linear dependency (ALD) analysis to construct features for continuous state–action space. Compared with typical deep reinforcement learning algorithms, KLSAC has two main advantages: firstly, the critic module has the capacity for rapid convergence by computing the fixed point of the linear soft Bellman equation via the least-squares optimization method. Secondly, the kernel-based features construction approach only requires predefining the basic kernel function and can improve the generalization ability of KLSAC. The simulation studies on robotic grasping control were conducted in the V-REP simulator. The results demonstrate that compared with other typical RL algorithms (e.g., SAC and BMPO), the proposed KLSAC algorithm can achieve better performance in terms of sample efficiency and asymptotic convergence property. Furthermore, experimental results on a real UR5 robot validated that KLSAC performed well in the real world.}
}
@article{IBRAHIM2023105110,
title = {Reinforcement learning for high-quality reality mapping of indoor construction using unmanned ground vehicles},
journal = {Automation in Construction},
volume = {156},
pages = {105110},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105110},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523003709},
author = {Amir Ibrahim and Wilfredo Torres-Calderon and Mani Golparvar-Fard},
keywords = {Reality capture, Photogrammetry, Automatic data collection, Indoor construction visual data, Reinforcement learning, Divide-and-conquer, Data collection simulation, Path planning, Robotic navigation policy},
abstract = {Recent advances in reality capture technology focused on automating reality capture and devising robust computational models to convert the collected data into usable formats. However, these modern approaches are still challenged by the insufficient capacity of the resulting information to relay accurate and complete representations of the construction state due to the data's poor visual quality. In addition, the complexity of robotic path planning –especially for indoor construction– hinders automatic visual data collection due to cluttered, narrow, and dynamic construction spaces. This work targets both challenges by presenting a reinforcement learning model that optimizes indoor data collection policy for acquiring high-quality visual data using camera-equipped unmanned ground rovers. Results from three learned navigation policies show the capability of the method to provide high visual quality for the collected data. The learned policies reduced the data collection duration by 38.23% on average compared to the currently used automatic data collection strategies. The policies also provided a 31.04% average reduction in data collection distance compared to lawn-mower patterns.}
}
@incollection{HUANG2022145,
title = {Chapter 10 - Reinforcement learning in EEG-based human-robot interaction},
editor = {Chang S. Nam and Jae-Yoon Jung and Sangwon Lee},
booktitle = {Human-Centered Artificial Intelligence},
publisher = {Academic Press},
pages = {145-154},
year = {2022},
isbn = {978-0-323-85648-5},
doi = {https://doi.org/10.1016/B978-0-323-85648-5.00020-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323856485000207},
author = {Jiali Huang and Chang S. Nam},
keywords = {Deep learning, Electroencephalography, Q-learning, Reinforcement learning, Robot},
abstract = {Reinforcement learning (RL) offers human–robot interactions a novel and efficient way to improve performances. Using human intrinsic feedback like electroencephalography (EEG) in the RL framework showed great potential for achieving effective robot training. This chapter introduces the basics of a RL problem, the usage of RL in EEG classification, and the use of RL in EEG-based robot learning. Challenges and future directions were also discussed.}
}
@article{ZHANG2019162,
title = {Synchronous optimal control method for nonlinear systems with saturating actuators and unknown dynamics using off-policy integral reinforcement learning},
journal = {Neurocomputing},
volume = {356},
pages = {162-169},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.04.036},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219306174},
author = {Zenglian Zhang and Ruizhuo Song and Min Cao},
keywords = {Approximate dynamic programming, Adaptive dynamic programming, Off-policy, Integral reinforcement learning},
abstract = {The present study establishes an approximate optimal critic learning algorithm, based on the single-network integral reinforcement learning (IRL) algorithm and intends to solve the optimal control problem for an unknown nonlinear system with saturating actuators. The value function is formulated through building generalized nonquadratic functions. In order to solve the Hamilton–Jacobi–Bellman (HJB) equation, a novel optimal scheme for the control approximation, based on the off-policy iteration is presented. Moreover, the single-neural network implementation procedure is introduced to complete the iteration algorithm. The synchronous IRL policy iteration is proposed to update the weight of the critic neural network. Finally, reasonable simulation results are provided for confirming the effectiveness of the proposed optimal approximation control technique in solving equations for a linear and oscillating systems.}
}
@article{LIU2023108608,
title = {Deep reinforcement learning based energy storage management strategy considering prediction intervals of wind power},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {145},
pages = {108608},
year = {2023},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2022.108608},
url = {https://www.sciencedirect.com/science/article/pii/S0142061522006044},
author = {Fang Liu and Qianyi Liu and Qing Tao and Yucong Huang and Danyun Li and Denis Sidorov},
keywords = {Wind power generation, Prediction intervals, Energy storage, LSTM-LUBE, Deep reinforcement learning},
abstract = {Wind power generation combined with energy storage is able to maintain energy balance and realize stable operation. This article proposes a data-driven energy storage management strategy considering the prediction intervals of wind power. Firstly, a power interval prediction model is established based on long-short term memory and lower and upper bound estimation (LUBE) to quantify the uncertainty of wind power, which solves the issue that traditional LUBE cannot adopt gradient descent method. Secondly, the energy storage management is transformed into Markov decision process and solved by deep reinforcement learning. The state space, action space and reward function of the interaction between agent and environment are established, and the value function is approximated through the deep Q network. Then, according to the real-time state, such as wind power, power prediction intervals, local load, dynamic electricity price and state of charge, the proposed strategy can make the charge/discharge schedule automatically. Finally, the effectiveness and superiority of the proposed energy storage management strategy are verified based on real wind farm dataset. The proportion of wrong decisions is zero, and daily transaction cost and wear cost of energy storage management system decrease significantly.}
}
@article{RUAN2021114399,
title = {A reinforcement learning-based algorithm for the aircraft maintenance routing problem},
journal = {Expert Systems with Applications},
volume = {169},
pages = {114399},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114399},
url = {https://www.sciencedirect.com/science/article/pii/S095741742031071X},
author = {J.H. Ruan and Z.X. Wang and Felix T.S. Chan and S. Patnaik and M.K. Tiwari},
keywords = {Aircraft routing problem, Sequential decision-making problem, Markov Decision Process (MDP), Reinforcement Learning},
abstract = {With recent developments in the airline industry worldwide, the competition among the industry has increased largely with many key players in the market. In order to generate profits, the industry has paid much attention to generate optimal routes that are maintenance feasible. The main aim of operational aircraft maintenance routing problem (OAMRP) is to generate these optimal routes for each aircraft that are maintenance feasible and follow the constraints defined by the Federal Aviation Administration (FAA). In this paper, the OAMRP is studied with two main objectives. First, to propose a formulation of a network flow-based Integer Linear Programming (ILP) framework for the OAMRP that considers three main maintenance constraints simultaneously: maximum flying-hour, limit on the number of take-offs between two consecutive maintenance checks and the work-force capacity. Second, to develop a new reinforcement learning-based algorithm which can be used to solve the problem, quickly and efficiently, as compared to commonly available optimization software. Finally, the evaluation of the proposed algorithm on real case datasets obtained from a major airline located in the Middle East verifies that the algorithm generates high-quality solutions quickly for both medium and large-scale flight schedule dataset.}
}
@article{YANG2021107498,
title = {HackRL: Reinforcement learning with hierarchical attention for cross-graph knowledge fusion and collaborative reasoning},
journal = {Knowledge-Based Systems},
volume = {233},
pages = {107498},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107498},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007607},
author = {Linyao Yang and Xiao Wang and Yuxin Dai and Kejun Xin and Xiaolong Zheng and Weiping Ding and Jun Zhang and Fei-Yue Wang},
keywords = {Knowledge fusion, Knowledge reasoning, Decision-making, Hierarchical graph attention, Reinforcement learning},
abstract = {Reasoning aiming at inferring implicit facts over knowledge graphs (KGs) is a critical and fundamental task for various intelligent knowledge-based services. With multiple distributed and complementary KGs, the effective and efficient capture and fusion of knowledge from different KGs is becoming an increasingly important topic, which has not been well studied. To fill this gap, we propose to explore cross-KG relation paths with the anchor links identified by entity alignment for the knowledge fusion and collaborative reasoning of multiple KGs. To address the heterogeneity of different KGs, this paper proposes a novel reasoning model named HackRL based on the reinforcement learning framework, which incorporates the long short-term memory and hierarchical graph attention in the policy network to infer indicative cross-KG relation paths from the history trajectory and the heterogeneous environment for predicting corresponding relations. Meanwhile, an entity alignment-oriented representation learning method is utilized to embed different KGs into a unified vector space based on the anchor links to reduce the impact of distinct vector spaces, and two training mechanisms, action mask and retrain with sampled paths, are proposed to optimize the training process to learn more successful indicative paths. The proposed HackRL is validated on three cross-lingual datasets built from DBpedia on the link prediction and fact prediction tasks. Experimental results demonstrate that HackRL achieves better performance on most tasks than existing methods. This work provides an industrially-applicable framework for fusing distributed KGs to make better decisions.}
}
@article{MUSSI2023119883,
title = {ARLO: A framework for Automated Reinforcement Learning},
journal = {Expert Systems with Applications},
volume = {224},
pages = {119883},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119883},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423003846},
author = {Marco Mussi and Davide Lombarda and Alberto Maria Metelli and Francesco Trovó and Marcello Restelli},
keywords = {AutoRL, Automated Reinforcement Learning},
abstract = {Automated Reinforcement Learning (AutoRL) is a relatively new area of research that is gaining increasing attention. The objective of AutoRL consists in easing the employment of Reinforcement Learning (RL) techniques for the broader public by alleviating some of its main challenges, including data collection, algorithm selection, and hyper-parameter tuning. In this work, we propose a general and flexible framework, namely ARLO: Automated Reinforcement Learning Optimizer, to construct automated pipelines for AutoRL. Based on this, we propose a pipeline for offline and one for online RL, discussing the components, interaction, and highlighting the difference between the two settings. Furthermore, we provide a Python implementation of such pipelines, released as an open-source library. Our implementation is tested on an illustrative LQG domain and on classic MuJoCo environments, showing the ability to reach competitive performances requiring limited human intervention. We also showcase the full pipeline on a realistic dam environment, automatically performing the feature selection and the model generation tasks.}
}
@article{YIN2018353,
title = {Control Design of a Marine Vessel System Using Reinforcement Learning},
journal = {Neurocomputing},
volume = {311},
pages = {353-362},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.05.061},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218306465},
author = {Zhao Yin and Wei He and Chenguang Yang and Changyin Sun},
keywords = {Reinforcement Learning, Critic Neural Networks, Actor neural networks, Lyapunov method, Marine Vessel},
abstract = {In this paper, our main goal is to solve optimal control problem by using reinforcement learning (RL) algorithm for marine surface vessel system with known dynamic. And this algorithm is an optimal control algorithm based on policy iteration (PI), and it can obtain the suitable approximations of cost function and the optimized control policy. There are two neural networks (NNs), where critic NN aims to estimate the cost-to-go and actor NN is utilized to design suitable input controller and minimize the tracking error. A novel tuning method is given for critic NN and actor NN. The stability and convergence are proven by Lyapunov’s direct method. Finally, the numerical simulations are conducted to demonstrate the feasibility and superiority of presented algorithm.}
}
@article{KHAN2020107027,
title = {Searching for optimal process routes: A reinforcement learning approach},
journal = {Computers & Chemical Engineering},
volume = {141},
pages = {107027},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.107027},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420303999},
author = {Ahmad Khan and Alexei Lapkin},
keywords = {Process design, Process intensification, Process systems engineering, Reinforcement learning, Machine learning},
abstract = {Developing optimisation tools is a key target in supporting computer-aided process design as the complexity of the designed space grows beyond conventional unit operations. A process design problem can be formulated as a search of an optimal processing route in the thermodynamic state space, going from feedstock to products. This paper describes a design architecture that enables reinforcement learning agent to use trial-and-error to narrow its search to the most promising routes, rather than exhaustively enumerating solutions. In each iteration, the agent employs previously collected data to guide the search for new trajectories. This is successfully demonstrated in a hydrogen production process using both conventional and intensified process design principles. The agent outperformed standard nonlinear optimisation methods in competitive computational time. Limitations and future work are discussed.}
}
@article{YANG2022,
title = {A digital twins enabled underwater intelligent internet vehicle path planning system via reinforcement learning and edge computing},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000967},
author = {Jiachen Yang and Meng Xi and Jiabao Wen and Yang Li and Houbing Herbert Song},
keywords = {Digital twins, Reinforcement learning, Edge computing, Underwater intelligent internet vehicle, Path planning},
abstract = {The Autonomous Underwater Glider (AUG) is a kind of prevailing underwater intelligent internet vehicle and occupies a dominant position in industrial applications, in which path planning is an essential problem. Due to the complexity and variability of the ocean, accurate environment modeling and flexible path planning algorithms are pivotal challenges. The traditional models mainly utilize mathematical functions, which are not complete and reliable. Most existing path planning algorithms depend on the environment and lack flexibility. To overcome these challenges, we propose a path planning system for underwater intelligent internet vehicles. It applies digital twins and sensor data to map the real ocean environment to a virtual digital space, which provides a comprehensive and reliable environment for path simulation. We design a value-based reinforcement learning path planning algorithm and explore the optimal network structure parameters. The path simulation is controlled by a closed-loop model integrated into the terminal vehicle through edge computing. The integration of state input enriches the learning of neural networks and helps to improve generalization and flexibility. The task-related reward function promotes the rapid convergence of the training. The experimental results prove that our reinforcement learning based path planning algorithm has great flexibility and can effectively adapt to a variety of different ocean conditions.}
}
@article{LI2023103259,
title = {ATS-O2A: A state-based adversarial attack strategy on deep reinforcement learning},
journal = {Computers & Security},
volume = {129},
pages = {103259},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103259},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823001694},
author = {Xiangjuan Li and Yang Li and Zhaowen Feng and Zhaoxuan Wang and Quan Pan},
keywords = {Deep reinforcement learning, Adversarial attack, Targeted attack, Deep learning security, Machine learning},
abstract = {In recent years, deep reinforcement learning has been widely applied in many decision-making tasks requiring high safety and security due to its excellent performance. However, if an adversary attacks when the agent making critical decisions, it is bound to bring disastrous consequences because humans cannot detect it. Therefore, it is necessary to study adversarial attacks against deep reinforcement learning to help researchers design highly robust and secure algorithms and systems. In this paper, we proposed an attack method based on Attack Time Selection (ATS) function and Optimal Attack Action (O2A) strategy, named ATS-O2A. We select the critical attack moment through the ATS function, and then combine the state-based strategy with the O2A strategy to select the optimal attack action which has profound influence as targeted action, finally we launch an attack by making targeted adversarial examples. In order to measure the stealthiness and effectiveness of the attack, we designed a new measurement index. Experiments show that our method can effectively reduce unnecessary attacks and improve the efficiency of attacks.}
}
@article{OLIVEIRA20181039,
title = {Capacity expansion under uncertainty in an oligopoly using indirect reinforcement-learning},
journal = {European Journal of Operational Research},
volume = {267},
number = {3},
pages = {1039-1050},
year = {2018},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2017.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S037722171731024X},
author = {Fernando S. Oliveira and Manuel L.G. Costa},
keywords = {OR in energy, Capacity expansion, Computational learning, Electricity markets, Oligopoly},
abstract = {We model capacity expansion in oligopolistic markets, with endogenous prices, under uncertainty, considering multiple production technologies. As this environment is complex, capacity expansion is the outcome of a learning process by individual firms. We propose indirect reinforcement-learning to model the interaction between price determination and capacity decisions, in the context of an oligopoly game. We apply our model to the analysis of the Iberian electricity market, considering multiple technologies, focusing on how subsidies, the prices of CO2 emissions and gas affect the capacity expansion policies.}
}
@article{GULLAPALLI1995237,
title = {Skillful control under uncertainty via direct reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {15},
number = {4},
pages = {237-246},
year = {1995},
note = {Reinforcement Learning and Robotics},
issn = {0921-8890},
doi = {https://doi.org/10.1016/0921-8890(95)00006-2},
url = {https://www.sciencedirect.com/science/article/pii/0921889095000062},
author = {Vijaykumar Gullapalli},
keywords = {Reinforcement learning, Learning control, Direct methods, Peg-in-the hole insertion},
abstract = {Complexity and uncertainty in modern robots and other autonomous systems make it difficult to design controllers for such systems that can achieve desired levels of precision and robustness. Therefore learning methods are being incorporated into controllers for such systems, thereby providing the adaptibility necessary to meet the performance demands of the task. We argue that for learning tasks arising frequently in control applications, the most useful methods in practice probably are those we call direct associative reinforcement learning methods. We describe direct reinforcement learning methods and also illustrate with an example the utility of these methods for learning skilled robot control under uncertainty.}
}
@article{LIU2023118,
title = {Safe reinforcement learning for discrete-time fully cooperative games with partial state and control constraints using control barrier functions},
journal = {Neurocomputing},
volume = {517},
pages = {118-132},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.10.058},
url = {https://www.sciencedirect.com/science/article/pii/S092523122201339X},
author = {Shihan Liu and Lijun Liu and Zhen Yu},
keywords = {Reinforcement learning, Value iteration, Control barrier function, Partial state and control constraints, Fully cooperative games},
abstract = {In this paper, a novel safe reinforcement learning is proposed for fully cooperative games of discrete-time multi-player systems with partial state and control constraints. The fully cooperative game is a special case of the nonzero-sum games where all players cooperate to accomplish the common task. However, there are few works for fully cooperative game issues of discrete-time systems with partial state and control constraints. The issue is addressed by our algorithm based on the constrained value iteration framework using the measured data along the system trajectories, and the Nash equilibrium of the constrained fully cooperative game is achieved. Compared to previous methods for fully cooperative game issues, neither the accurate system dynamics nor the initial admissible control policies are required via the algorithm. Meanwhile, the discrete-time exponential control barrier functions are adopted to address the issue of state constraints. Moreover, the convergence of the proposed algorithm is proven in theory. Then, the system dynamics, the control policies and the value function are approximated by the three-layer neural networks, respectively. Finally, two experiments are presented to demonstrate the safety and effectiveness of the proposed algorithm.}
}
@article{LU2023104341,
title = {Disturbance-aware reinforcement learning for rejecting excessive disturbances},
journal = {Robotics and Autonomous Systems},
volume = {161},
pages = {104341},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104341},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022002305},
author = {Wenjie Lu and Manman Hu},
keywords = {Reinforcement learning, Disturbance rejection, Disturbance observer},
abstract = {This paper presents a disturbance-aware Reinforcement Learning (RL) approach for stabilizing a free-floating platform under excessive external disturbances. In particular, we consider the scenarios where disturbances frequently exceed actuator limits and largely affect the dynamics characterizing the disturbed platform. This stabilization problem is better described by a set of Unknown Partially Observable Markovian Decision Processes (POMDPs), as opposed to a single-POMDP formulation, making online disturbance awareness necessary. This paper proposes a new Disturbance-Observer network (DO-net) that mimics prediction procedures through an auxiliary Gated Recurrent Unit (GRU), for the purpose of estimating and encoding the disturbance states and the disturbance transition functions, respectively. Then the controller subnetwork is trained with joint optimization of the observer subnetwork in an RL manner for mutual robustness and runtime efficiency. Numerical simulations on position regulation tasks have demonstrated that the DO-net outperforms the DOB-net and reduces the gap with an ideal performance estimate, the latter of which is obtained by a commercial solver given precise disturbance knowledge.}
}
@article{ZHENG2023127636,
title = {Real-time dispatch of an integrated energy system based on multi-stage reinforcement learning with an improved action-choosing strategy},
journal = {Energy},
volume = {277},
pages = {127636},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.127636},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223010307},
author = {Lingwei Zheng and Hao Wu and Siqi Guo and Xinyu Sun},
keywords = {Integrated energy system, Real-time dispatch, Uncertainty, Multi-stage reinforcement learning, Improved action-choosing strategy},
abstract = {The uncertainty of renewable energy has brought challenges to the real-time dispatch of integrated energy systems (IES). Nowadays, reinforcement learning (RL) is widely used in IES real-time dispatch to deal with the uncertainty of renewable energy. However, traditional RL algorithms often face the problem of dimensionality when there are numerous controllable units in the system, which will increase operating costs and training time significantly. Based on the above issues, we developed a novel real-time dispatch method for IES with RL model training in stages based on the dueling double deep quality network (D3QN). Dispatches of different controllable units in IES are decomposed into a multi-stage training process according to the degree of thermoelectric coupling and the complexity of equipment operation. This makes the action space of each training stage independent, alleviating the problem of extra-large action space in the traditional RL method. In addition, an improved action-choosing strategy is proposed to enhance local optimization in the process of algorithm training by introducing “offset” according to probability in the training progress. The simulations were carried out on four different types of days in an IES. The results show that the proposed method can effectively reduce operating costs and accelerate convergence.}
}
@article{XIONG202073,
title = {Comparison of end-to-end and hybrid deep reinforcement learning strategies for controlling cable-driven parallel robots},
journal = {Neurocomputing},
volume = {377},
pages = {73-84},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219313943},
author = {Hao Xiong and Tianqi Ma and Lin Zhang and Xiumin Diao},
keywords = {Deep reinforcement learning, End-to-end DRL strategy, Hybrid DRL strategy, Deep deterministic policy gradient, Cable-driven parallel robot},
abstract = {Deep reinforcement learning (DRL) has been proven effective in learning policies of high-dimensional states and actions. Recently, a variety of robot manipulation tasks have been accomplished using end-to-end DRL strategies. An end-to-end DRL strategy accomplishes a robot manipulation task as a black box. On the other hand, a robot manipulation task can be divided into multiple subtasks and accomplished by non-learning-based approaches. A hybrid DRL strategy integrates DRL with non-learning-based approaches. The hybrid DRL strategy accomplishes some subtasks of a robot manipulation task by DRL and the rest subtasks by non-learning-based approaches. However, the effects of integrating DRL with non-learning-based approaches on the learning speed and the robustness of DRL to model uncertainties have not been discussed. In this study, an end-to-end DRL strategy and a hybrid DRL strategy are developed and compared in controlling a cable-driven parallel robot. This study shows that, by integrating DRL with non-learning-based approaches, the hybrid DRL strategy learns faster and is more robust to model uncertainties than the end-to-end DRL strategy. This study demonstrates that, by taking advantages of both learning and non-learning-based approaches, the hybrid DRL strategy provides an alternative to accomplish a robot manipulation task.}
}
@article{DONG2022264,
title = {A deep reinforcement learning (DRL) based approach for well-testing interpretation to evaluate reservoir parameters},
journal = {Petroleum Science},
volume = {19},
number = {1},
pages = {264-278},
year = {2022},
issn = {1995-8226},
doi = {https://doi.org/10.1016/j.petsci.2021.09.046},
url = {https://www.sciencedirect.com/science/article/pii/S1995822621000947},
author = {Peng Dong and Zhi-Ming Chen and Xin-Wei Liao and Wei Yu},
keywords = {Well testing, Deep reinforcement learning, Automatic interpretation, Parameter evaluation},
abstract = {Parameter inversions in oil/gas reservoirs based on well test interpretations are of great significance in oil/gas industry. Automatic well test interpretations based on artificial intelligence are the most promising to solve the problem of non-unique solution. In this work, a new deep reinforcement learning (DRL) based approach is proposed for automatic curve matching for well test interpretation, by using the double deep Q-network (DDQN). The DDQN algorithms are applied to train agents for automatic parameter tuning in three conventional well-testing models. In addition, to alleviate the dimensional disaster problem of parameter space, an asynchronous parameter adjustment strategy is used to train the agent. Finally, field applications are carried out by using the new DRL approaches. Results show that step number required for the DDQN to complete the curve matching is the least among, when comparing the naive deep Q-network (naive DQN) and deep Q-network (DQN). We also show that DDQN can improve the robustness of curve matching in comparison with supervised machine learning algorithms. Using DDQN algorithm to perform 100 curve matching tests on three traditional well test models, the results show that the mean relative error of the parameters is 7.58% for the homogeneous model, 10.66% for the radial composite model, and 12.79% for the dual porosity model. In the actual field application, it is found that a good curve fitting can be obtained with only 30 steps of parameter adjustment.}
}
@article{DOGRU20211248,
title = {Actor–Critic Reinforcement Learning and Application in Developing Computer-Vision-Based Interface Tracking},
journal = {Engineering},
volume = {7},
number = {9},
pages = {1248-1261},
year = {2021},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2021.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S209580992100326X},
author = {Oguzhan Dogru and Kirubakaran Velswamy and Biao Huang},
keywords = {Interface tracking, Object tracking, Occlusion, Reinforcement learning, Uniform manifold approximation and projection},
abstract = {This paper synchronizes control theory with computer vision by formalizing object tracking as a sequential decision-making process. A reinforcement learning (RL) agent successfully tracks an interface between two liquids, which is often a critical variable to track in many chemical, petrochemical, metallurgical, and oil industries. This method utilizes less than 100 images for creating an environment, from which the agent generates its own data without the need for expert knowledge. Unlike supervised learning (SL) methods that rely on a huge number of parameters, this approach requires far fewer parameters, which naturally reduces its maintenance cost. Besides its frugal nature, the agent is robust to environmental uncertainties such as occlusion, intensity changes, and excessive noise. From a closed-loop control context, an interface location-based deviation is chosen as the optimization goal during training. The methodology showcases RL for real-time object-tracking applications in the oil sands industry. Along with a presentation of the interface tracking problem, this paper provides a detailed review of one of the most effective RL methodologies: actor–critic policy.}
}
@article{WANG2023101818,
title = {Transfer reinforcement learning method with multi-label learning for compound fault recognition},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101818},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101818},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002762},
author = {Zisheng Wang and Qing Zhang and Lv Tang and Tielin Shi and Jianping Xuan},
keywords = {Compound fault recognition, Deep reinforcement learning, Multi-label learning, Transfer learning, Trust region policy optimization},
abstract = {In complex working site, bearings used as the important part of machine, could simultaneously have faults on several positions. Consequently, multi-label learning approach considering fully the correlation between different faulted positions of bearings becomes the popular learning pattern. Deep reinforcement learning (DRL) combining the perception ability of deep learning and the decision-making ability of reinforcement learning, could be adapted to the compound fault diagnosis while having a strong ability extracting the fault feature from the raw data. However, DRL is difficult to converge and easily falls into the unstable training problem. Therefore, this paper integrates the feature extraction ability of DRL and the knowledge transfer ability of transfer learning (TL), and proposes the multi-label transfer reinforcement learning (ML-TRL). In detail, the proposed method utilizes the improved trust region policy optimization (TRPO) as the basic DRL framework and pre-trains the fixed convolutional networks of ML-TRL using the multi-label convolutional neural network method. In compound fault experiment, the final results demonstrate powerfully that the proposed method could have the higher accuracy than other multi-label learning methods. Hence, the proposed method is a remarkable alternative when recognizing the compound fault of bearings.}
}
@article{QIN2021108251,
title = {Multi-agent reinforcement learning-based dynamic task assignment for vehicles in urban transportation system},
journal = {International Journal of Production Economics},
volume = {240},
pages = {108251},
year = {2021},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2021.108251},
url = {https://www.sciencedirect.com/science/article/pii/S0925527321002279},
author = {Wei Qin and Yan-Ning Sun and Zi-Long Zhuang and Zhi-Yao Lu and Yao-Ming Zhou},
keywords = {Urban transportation system, Transportation task assignment, Multi-agent reinforcement learning, Actor-critic algorithm},
abstract = {The task assignment for vehicles plays an important role in urban transportation system, which is the key to cost reduction and efficiency improvement. The development of information technology and the emergence of “sharing economy” create a more convenient transportation mode, but also bring a greater challenge to efficient operation of urban transportation system. On the one hand, considering the complex and dynamic environment of urban transportation, an efficient method for assigning transportation tasks to idle vehicles is desired. On the other hand, to meet the users' expectations on immediate response of vehicle, the task assignment problem with dynamic arrival remains to be resolved. In this study, we propose a dynamic task assignment method for vehicles in urban transportation system based on the multi-agent reinforcement learning (RL). The transportation task assignment problem is transformed into a stochastic game process from vehicles’ perspective, and then an extended actor-critic (AC) algorithm is employed to obtain the optimal strategy. Based on the proposed method, vehicles can independently make decisions in real time, thus eliminating a lot of communication cost. Compared with the methods based on first-come-first-service (FCFS) rule and classic contract net algorithm (CNA), the results show that the proposed method can obtain higher acceptance rate and profit rate in the service cycle.}
}
@article{ZHONG2019117,
title = {Multi Workflow Fair Scheduling Scheme Research Based on Reinforcement Learning},
journal = {Procedia Computer Science},
volume = {154},
pages = {117-123},
year = {2019},
note = {Proceedings of the 9th International Conference of Information and Communication Technology [ICICT-2019] Nanning, Guangxi, China January 11-13, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919307872},
author = {Ji Hai Zhong and De Long Cui and Zhi Ping Peng and Qi Rui Li and Jie Guang He},
keywords = {cloud workflow, virtual resources, reinforce Learning, fair scheduling},
abstract = {In this study, aiming to optimize the multi-workflow scheduling order, in which tasks submitted at different time require different service quality, we present a fair multi-workflow scheduling scheme based on reinforcement learning. Firstly we design a dynamic priority-driven algorithm, in order to set the initial state of the task priority according to the type of cloud workflow and service quality on the one hand, and on the other hand, to adjust the tasks priority dynamically while scheduling so as to avoid violating the Service Level Agreement by delaying the workflow provisioning. Secondly, we design a fine-grained cloud computing model and apply the reinforcement-learning based scheduling algorithm to balance the cluster loads. Finally the experimental results prove the effectiveness of this scheme.}
}