@inproceedings{10.5555/3398761.3398856,
author = {Lyu, Xueguang and Amato, Christopher},
title = {Likelihood Quantile Networks for Coordinating Multi-Agent Reinforcement Learning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {When multiple agents learn in a decentralized manner, the environment appears non-stationary from the perspective of an individual agent due to the exploration and learning of the other agents. Recently proposed deep multi-agent reinforcement learning methods have tried to mitigate this non-stationarity by attempting to determine which samples are from other agent exploration or suboptimality and take them less into account during learning. Based on the same philosophy, this paper introduces a decentralized quantile estimator, which aims to improve performance by distinguishing non-stationary samples based on the likelihood of returns. In particular, each agent considers the likelihood that other agent explorations and policy changes are occurring, essentially utilizing the agent's own estimations to weigh the learning rate that should be applied towards the given samples. We introduce a formal method of calculating differences of our return distribution representations and methods for utilizing it to guide updates. We also explore the effect of risk-seeking strategies for adjusting learning over time and propose adaptive risk distortion functions that guide risk sensitivity. Our experiments, on traditional benchmarks and new domains, show our methods are more stable, sample efficient and more likely to converge to a joint optimal policy than previous methods.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {798–806},
numpages = {9},
keywords = {multi-agent learning, deep learning, reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3308558.3313632,
author = {Yao, Ziyu and Peddamail, Jayavardhan Reddy and Sun, Huan},
title = {CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313632},
doi = {10.1145/3308558.3313632},
abstract = {To accelerate software development, much research has been performed to help people understand and reuse the huge amount of available code resources. Two important tasks have been widely studied: code retrieval, which aims to retrieve code snippets relevant to a given natural language query from a code base, and code annotation, where the goal is to annotate a code snippet with a natural language description. Despite their advancement in recent years, the two tasks are mostly explored separately. In this work, we investigate a novel perspective of Code annotation for Code retrieval (hence called “CoaCor”), where a code annotation model is trained to generate a natural language annotation that can represent the semantic meaning of a given code snippet and can be leveraged by a code retrieval model to better distinguish relevant code snippets from others. To this end, we propose an effective framework based on reinforcement learning, which explicitly encourages the code annotation model to generate annotations that can be used for the retrieval task. Through extensive experiments, we show that code annotations generated by our framework are much more detailed and more useful for code retrieval, and they can further improve the performance of existing code retrieval models significantly.1},
booktitle = {The World Wide Web Conference},
pages = {2203–2214},
numpages = {12},
keywords = {Reinforcement Learning, Code Retrieval, Code Annotation},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@article{10.5555/1005332.1044710,
author = {Greensmith, Evan and Bartlett, Peter L. and Baxter, Jonathan},
title = {Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning},
year = {2004},
issue_date = {12/1/2004},
publisher = {JMLR.org},
volume = {5},
issn = {1532-4435},
abstract = {Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {1471–1530},
numpages = {60}
}

@inproceedings{10.1145/3410992.3411001,
author = {Murad, Abdulmajid and Kraemer, Frank Alexander and Bach, Kerstin and Taylor, Gavin},
title = {Information-Driven Adaptive Sensing Based on Deep Reinforcement Learning},
year = {2020},
isbn = {9781450387583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410992.3411001},
doi = {10.1145/3410992.3411001},
abstract = {In order to make better use of deep reinforcement learning in the creation of sensing policies for resource-constrained IoT devices, we present and study a novel reward function based on the Fisher information value. This reward function enables IoT sensor devices to learn to spend available energy on measurements at otherwise unpredictable moments, while conserving energy at times when measurements would provide little new information. This is a highly general approach, which allows for a wide range of use cases without significant human design effort or hyperparameter tuning. We illustrate the approach in a scenario of workplace noise monitoring, where results show that the learned behavior outperforms a uniform sampling strategy and comes close to a near-optimal oracle solution.},
booktitle = {Proceedings of the 10th International Conference on the Internet of Things},
articleno = {2},
numpages = {8},
keywords = {Gaussian processes, adaptive sensing, deep reinforcement learning, internet of things},
location = {Malm\"{o}, Sweden},
series = {IoT '20}
}

@inproceedings{10.1145/3285017.3285024,
author = {Marantos, Charalampos and Lamprakos, Christos P. and Tsoutsouras, Vasileios and Siozios, Kostas and Soudris, Dimitrios},
title = {Towards Plug\&amp;play Smart Thermostats Inspired by Reinforcement Learning},
year = {2018},
isbn = {9781450365987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3285017.3285024},
doi = {10.1145/3285017.3285024},
abstract = {Buildings are immensely energy-demanding and this fact is enhanced by the expectation of even more increment of energy consumption in the future. In order to mitigate this problem, a low-cost, flexible and high-quality Decision-Making Mechanism for supporting the tasks of a Smart Thermostat is proposed. Energy efficiency and thermal comfort are the two primary quantities regarding control performance of a building's HVAC system. Apart from demonstrating a conflicting relationship, they depend not only on the building's dynamics, but also on the surrounding climate and weather, thus rendering the problem of finding a long-term control scheme hard, and of stochastic nature. The introduced mechanism is inspired by Reinforcement Learning techniques and aims at satisfying both occupants' thermal comfort and limiting energy consumption. In contrast to to existing methods, this approach focuses on a plug\&amp;play solution, that does not require detailed building models and is applicable to a wide variety of buildings as it learns the dynamics using gathered information from the environment. The proposed control mechanisms were evaluated via a well-known building simulation framework and implemented on ARM-based, low-cost embedded devices.},
booktitle = {Proceedings of the Workshop on INTelligent Embedded Systems Architectures and Applications},
pages = {39–44},
numpages = {6},
keywords = {embedded software, learning systems, HVAC control, energy efficiency, intelligent agents, decision making},
location = {Turin, Italy},
series = {INTESA '18}
}

@inproceedings{10.1145/3583133.3590610,
author = {Zuo, Weiyi and Pedersen, Joachim and Risi, Sebastian},
title = {Evolution of an Internal Reward Function for Reinforcement Learning},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583133.3590610},
doi = {10.1145/3583133.3590610},
abstract = {Artificial neural networks (ANNs) can be trained with reinforcement learning (RL) in simulation to control robots. However, changes in the environment resulting in out-of-distribution situations put learned policies at risk of failure. Since the world can change in unpredictable ways, it might be desirable to be able to continue to update the parameters of the ANNs even after deployment, to prevent failures stemming from a distributional shift. However, in order to optimize with RL, a reward signal is needed. This is usually provided in the simulated environment, but might not necessarily always be available after training. We propose a solution to this problem that involves evolving a function that provides a reward signal to an RL algorithm based only on the inputs and outputs of the policy. We call this approach Evolved Internal Reward Reinforcement Learning (EIR-RL) and test it on various control tasks that have different reward structures and difficulty levels. Our method shows improved training stability and speed of the RL agent under standard circumstances, as well as the ability to train the RL agent under circumstances unseen during the initial optimization phase. We discuss how our results could inform future studies on autonomous, adapting agents.},
booktitle = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
pages = {351–354},
numpages = {4},
location = {Lisbon, Portugal},
series = {GECCO '23 Companion}
}

@inproceedings{10.1109/WIIAT.2008.259,
author = {Xiao, Dan and Tan, Ah-Hwee},
title = {Scaling Up Multi-Agent Reinforcement Learning in Complex Domains},
year = {2008},
isbn = {9780769534961},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WIIAT.2008.259},
doi = {10.1109/WIIAT.2008.259},
abstract = {TD-FALCON (Temporal Difference - Fusion Architecture for Learning, COgnition, and Navigation) is a class of self-organizing neural networks that incorporates Temporal Difference (TD) methods for real-time reinforcement learning. In this paper, we present two strategies, i.e. policy sharing and neighboring-agent mechanism, to further improve the learning efficiency of TD-FALCON in complex multi-agent domains. Through experiments on a traffic control problem domain and the herding task, we demonstrate that those strategies enable TD-FALCON to remain functional and adaptable in complex multi-agent domains.},
booktitle = {Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {326–329},
numpages = {4},
keywords = {Multi-Agent Reinforcement Learning, policy sharing, TD-FALCON, neighboring-agent mechanism},
series = {WI-IAT '08}
}

@inproceedings{10.1145/3573428.3573729,
author = {Xu, Yingyu},
title = {Deep Reinforcement Learning and Imitation Learning Based on VizDoom},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573729},
doi = {10.1145/3573428.3573729},
abstract = {Reinforcement learning is a field of machine learning that focuses on intelligent agents, primarily the concept of what actions an intelligent agent takes in the environment to maximize cumulative reward. In environments where rewards are scarce, a manual approach is necessary. However, manually designing the reward function to meet the desired behavior can be very complicated. A very useful solution is Imitation Learning (IL). This paper proposes two reinforcement learning algorithms for the basic scene of the VizDoom video game, and uses IL to improve the performance of one of the models.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1700–1706},
numpages = {7},
keywords = {Reinforcement learning, Imitation learning},
location = {Xiamen, China},
series = {EITCE '22}
}

@inproceedings{10.1145/3278721.3278776,
author = {Iyer, Rahul and Li, Yuezhang and Li, Huao and Lewis, Michael and Sundar, Ramitha and Sycara, Katia},
title = {Transparency and Explanation in Deep Reinforcement Learning Neural Networks},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278776},
doi = {10.1145/3278721.3278776},
abstract = {Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of "object saliency maps", to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {144–150},
numpages = {7},
keywords = {deep reinforcement learning, human factors, system transparency, human-ai interaction, explainable ai},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3488933.3489029,
author = {Huang, Jiateng and Huang, Wanrong and Wu, Dan and Lan, Long},
title = {Meta Actor-Critic Framework for Multi-Agent Reinforcement Learning},
year = {2022},
isbn = {9781450384087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488933.3489029},
doi = {10.1145/3488933.3489029},
abstract = {In recent years, multi-agent reinforcement learning has received sustained attention in the last few years. The typical Actor-Critic methods learn mappings directly from observation to action without understanding the tasks themselves. In this paper, we present a meta actor-critic framework for meta-actor critique based on observational learning processes and the additional loss of meta-learning actors to accelerate and improve multi-agent learning across agents' experiences. Within our framework, all agents are deliberately designed to share the same meta-critic loss to achieve the optimum actor learning progress. Meanwhile, by minimizing the loss of meta-actors, the meta actor learns the features of the meta-observation, leading to better actions. We implemented the MADDPG and MATD3 algorithms in our proposed framework and empirically demonstrated the superiority of our framework on two kinds of multi-agent tasks. In addition, the framework can be flexibly incorporated into various contemporary multi-agent Actor-Critic methods.},
booktitle = {Proceedings of the 2021 4th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {636–643},
numpages = {8},
keywords = {multi-agent system, meta-learning, Reinforcement learning},
location = {Xiamen, China},
series = {AIPR '21}
}

@inproceedings{10.1145/3512290.3528870,
author = {Howard, David and Munn, Humphrey and Dolcetti, Davide and Kannemeyer, Josh and Robinson, Nicole},
title = {Assessing Evolutionary Terrain Generation Methods for Curriculum Reinforcement Learning},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528870},
doi = {10.1145/3512290.3528870},
abstract = {Curriculum learning allows complex tasks to be mastered via incremental progression over 'stepping stone' goals towards a final desired behaviour. Typical implementations learn locomotion policies for challenging environments through gradual complexification of a terrain mesh generated through a parameterised noise function. To date, researchers have predominantly generated terrains from a limited range of noise functions, and the effect of the generator on the learning process is underrepresented in the literature. We compare popular noise-based terrain generators to two indirect encodings, CPPN and GAN. To allow direct comparison between both direct and indirect representations, we assess the impact of a range of representation-agnostic MAP-Elites feature descriptors that compute metrics directly from the generated terrain meshes. Next, performance and coverage are assessed when training a humanoid robot in a physics simulator using the PPO algorithm. Results describe key differences between the generators that inform their use in curriculum learning, and present a range of useful feature descriptors for uptake by the community.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {377–384},
numpages = {8},
keywords = {representations, CPPN, GAN, curriculum learning, procedural content generation, quality-diversity, reinforcement learning},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1145/3539618.3591656,
author = {Ren, Zhaochun and Huang, Na and Wang, Yidan and Ren, Pengjie and Ma, Jun and Lei, Jiahuan and Shi, Xinlei and Luo, Hengliang and Jose, Joemon and Xin, Xin},
title = {Contrastive State Augmentations for Reinforcement Learning-Based Recommender Systems},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591656},
doi = {10.1145/3539618.3591656},
abstract = {Learning reinforcement learning (RL)-based recommenders from historical user-item interaction sequences is vital to generate high-reward recommendations and improve long-term cumulative benefits. However, existing RL recommendation methods encounter difficulties (i) to estimate the value functions for states which are not contained in the offline training data, and (ii) to learn effective state representations from user implicit feedback due to the lack of contrastive signals.In this work, we propose contrastive state augmentations (CSA) for the training of RL-based recommender systems. To tackle the first issue, we propose four state augmentation strategies to enlarge the state space of the offline data. The proposed method improves the generalization capability of the recommender by making the RL agent visit the local state regions and ensuring the learned value functions are similar between the original and augmented states. For the second issue, we propose introducing contrastive signals between augmented states and the state randomly sampled from other sessions to improve the state representation learning further.To verify the effectiveness of the proposed CSA, we conduct extensive experiments on two publicly accessible datasets and one dataset collected from a real-life e-commerce platform. We also conduct experiments on a simulated environment as the online evaluation setting. Experimental results demonstrate that CSA can effectively improve recommendation performance.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {922–931},
numpages = {10},
keywords = {sequential recommendation, data augmentation, recommender system, reinforcement learning, contrastive learning},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.1145/1966407.1966412,
author = {Pietquin, Olivier and Geist, Matthieu and Chandramohan, Senthilkumar and Frezza-Buet, Herv\'{e}},
title = {Sample-Efficient Batch Reinforcement Learning for Dialogue Management Optimization},
year = {2011},
issue_date = {May 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1550-4875},
url = {https://doi.org/10.1145/1966407.1966412},
doi = {10.1145/1966407.1966412},
abstract = {Spoken Dialogue Systems (SDS) are systems which have the ability to interact with human beings using natural language as the medium of interaction. A dialogue policy plays a crucial role in determining the functioning of the dialogue management module. Handcrafting the dialogue policy is not always an option, considering the complexity of the dialogue task and the stochastic behavior of users. In recent years approaches based on Reinforcement Learning (RL) for policy optimization in dialogue management have been proved to be an efficient approach for dialogue policy optimization. Yet most of the conventional RL algorithms are data intensive and demand techniques such as user simulation. Doing so, additional modeling errors are likely to occur. This paper explores the possibility of using a set of approximate dynamic programming algorithms for policy optimization in SDS. Moreover, these algorithms are combined to a method for learning a sparse representation of the value function. Experimental results show that these algorithms when applied to dialogue management optimization are particularly sample efficient, since they learn from few hundreds of dialogue examples. These algorithms learn in an off-policy manner, meaning that they can learn optimal policies with dialogue examples generated with a quite simple strategy. Thus they can learn good dialogue policies directly from data, avoiding user modeling errors.},
journal = {ACM Trans. Speech Lang. Process.},
month = {jun},
articleno = {7},
numpages = {21},
keywords = {Spoken dialogue systems, reinforcement learning}
}

@inproceedings{10.1145/3397271.3401237,
author = {Lei, Yu and Pei, Hongbin and Yan, Hanqi and Li, Wenjie},
title = {Reinforcement Learning Based Recommendation with Graph Convolutional Q-Network},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401237},
doi = {10.1145/3397271.3401237},
abstract = {Reinforcement learning (RL) has been successfully applied to recommender systems. However, the existing RL-based recommendation methods are limited by their unstructured state/action representations. To address this limitation, we propose a novel way that builds high-quality graph-structured states/actions according to the user-item bipartite graph. More specifically, we develop an end-to-end RL agent, termed Graph Convolutional Q-network (GCQN), which is able to learn effective recommendation policies based on the inputs of the proposed graph-structured representations. We show that GCQN achieves significant performance margins over the existing methods, across different datasets and task settings.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1757–1760},
numpages = {4},
keywords = {reinforcement learning, reinforcement learning based recommendation, recommender systems, graph convolutional Q-network},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.5555/3545946.3599076,
author = {Wang, Xiangsen and Zhan, Xianyuan},
title = {Offline Multi-Agent Reinforcement Learning with Coupled Value Factorization},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In offline multi-agent reinforcement learning (RL), most existing methods directly apply offline RL ingredients in the multi-agent setting without fully leveraging the decomposable problem structure, leading to less satisfactory performance in complex tasks. We present OMAC, a new offline multi-agent RL algorithm with coupled value factorization. OMAC adopts a coupled value factorization scheme that decomposes the global value function into local and shared components, and also maintains the credit assignment consistency between the state-value and action-value functions. Moreover, OMAC performs in-sample learning on the decomposed local state-value functions, which implicitly conducts max-Q operation at the local level while avoiding distributional shift caused by evaluating out-of-distribution actions. Based on the comprehensive evaluations of the offline multi-agent StarCraft II micro-management tasks, we demonstrate the superior performance of OMAC over existing offline multi-agent RL methods.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2781–2783},
numpages = {3},
keywords = {multi-agent cooperation, offline reinforcement learning, multi-agent reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.1145/3446428,
author = {Zeng, Weixin and Zhao, Xiang and Tang, Jiuyang and Lin, Xuemin and Groth, Paul},
title = {Reinforcement Learning–based Collective Entity Alignment with Adaptive Features},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3446428},
doi = {10.1145/3446428},
abstract = {Entity alignment (EA) is the task of identifying the entities that refer to the same real-world object but are located in different knowledge graphs (KGs). For entities to be aligned, existing EA solutions treat them separately and generate alignment results as ranked lists of entities on the other side. Nevertheless, this decision-making paradigm fails to take into account the interdependence among entities. Although some recent efforts mitigate this issue by imposing the 1-to-1 constraint on the alignment process, they still cannot adequately model the underlying interdependence and the results tend to be sub-optimal.To fill in this gap, in this work, we delve into the dynamics of the decision-making process, and offer a reinforcement learning (RL)–based model to align entities collectively. Under the RL framework, we devise the coherence and exclusiveness constraints to characterize the interdependence and restrict collective alignment. Additionally, to generate more precise inputs to the RL framework, we employ representative features to capture different aspects of the similarity between entities in heterogeneous KGs, which are integrated by an adaptive feature fusion strategy. Our proposal is evaluated on both cross-lingual and mono-lingual EA benchmarks and compared against state-of-the-art solutions. The empirical results verify its effectiveness and superiority.},
journal = {ACM Trans. Inf. Syst.},
month = {may},
articleno = {26},
numpages = {31},
keywords = {reinforcement learning, adaptive feature fusion, Entity alignment}
}

@inproceedings{10.1145/2749469.2749473,
author = {Peled, Leeor and Mannor, Shie and Weiser, Uri and Etsion, Yoav},
title = {Semantic Locality and Context-Based Prefetching Using Reinforcement Learning},
year = {2015},
isbn = {9781450334020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749469.2749473},
doi = {10.1145/2749469.2749473},
abstract = {Most modern memory prefetchers rely on spatio-temporal locality to predict the memory addresses likely to be accessed by a program in the near future. Emerging workloads, however, make increasing use of irregular data structures, and thus exhibit a lower degree of spatial locality. This makes them less amenable to spatio-temporal prefetchers.In this paper, we introduce the concept of Semantic Locality, which uses inherent program semantics to characterize access relations. We show how, in principle, semantic locality can capture the relationship between data elements in a manner agnostic to the actual data layout, and we argue that semantic locality transcends spatio-temporal concerns.We further introduce the context-based memory prefetcher, which approximates semantic locality using reinforcement learning. The prefetcher identifies access patterns by applying reinforcement learning methods over machine and code attributes, that provide hints on memory access semantics.We test our prefetcher on a variety of benchmarks that employ both regular and irregular patterns. For the SPEC 2006 suite, it delivers speedups as high as 2.8X (20\% on average) over a baseline with no prefetching, and outperforms leading spatio-temporal prefetchers. Finally, we show that the context-based prefetcher makes it possible for naive, pointer-based implementations of irregular algorithms to achieve performance comparable to that of spatially optimized code.},
booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
pages = {285–297},
numpages = {13},
location = {Portland, Oregon},
series = {ISCA '15}
}

@article{10.1145/2872887.2749473,
author = {Peled, Leeor and Mannor, Shie and Weiser, Uri and Etsion, Yoav},
title = {Semantic Locality and Context-Based Prefetching Using Reinforcement Learning},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3S},
issn = {0163-5964},
url = {https://doi.org/10.1145/2872887.2749473},
doi = {10.1145/2872887.2749473},
abstract = {Most modern memory prefetchers rely on spatio-temporal locality to predict the memory addresses likely to be accessed by a program in the near future. Emerging workloads, however, make increasing use of irregular data structures, and thus exhibit a lower degree of spatial locality. This makes them less amenable to spatio-temporal prefetchers.In this paper, we introduce the concept of Semantic Locality, which uses inherent program semantics to characterize access relations. We show how, in principle, semantic locality can capture the relationship between data elements in a manner agnostic to the actual data layout, and we argue that semantic locality transcends spatio-temporal concerns.We further introduce the context-based memory prefetcher, which approximates semantic locality using reinforcement learning. The prefetcher identifies access patterns by applying reinforcement learning methods over machine and code attributes, that provide hints on memory access semantics.We test our prefetcher on a variety of benchmarks that employ both regular and irregular patterns. For the SPEC 2006 suite, it delivers speedups as high as 2.8X (20\% on average) over a baseline with no prefetching, and outperforms leading spatio-temporal prefetchers. Finally, we show that the context-based prefetcher makes it possible for naive, pointer-based implementations of irregular algorithms to achieve performance comparable to that of spatially optimized code.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {285–297},
numpages = {13}
}

@inproceedings{10.1145/1569901.1570034,
author = {Jung, Jae-Yoon and Reggia, James A.},
title = {Evolving an Autonomous Agent for Non-Markovian Reinforcement Learning},
year = {2009},
isbn = {9781605583259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1569901.1570034},
doi = {10.1145/1569901.1570034},
abstract = {In this paper, we investigate the use of nested evolution in which each step of one evolutionary process involves running a second evolutionary process. We apply this approach to build an evolutionary system for reinforcement learning (RL) problems. Genetic programming based on a descriptive encoding is used to evolve the neural architecture, while an evolution strategy is used to evolve the connection weights. We test this method on a non-Markovian RL problem involving an autonomous foraging agent, finding that the evolved networks significantly outperform a rule-based agent serving as a control. We also demonstrate that nested evolution, partitioning into subpopulations, and crossover operations all act synergistically in improving performance in this context.},
booktitle = {Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation},
pages = {971–978},
numpages = {8},
keywords = {descriptive encoding, evolution strategy, reinforcement learning, genetic programming},
location = {Montreal, Qu\'{e}bec, Canada},
series = {GECCO '09}
}

@article{10.5555/2188385.2343705,
author = {Tamar, Aviv and Di Castro, Dotan and Meir, Ron},
title = {Integrating a Partial Model into Model Free Reinforcement Learning},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
number = {null},
issn = {1532-4435},
abstract = {In reinforcement learning an agent uses online feedback from the environment in order to adaptively select an effective policy. Model free approaches address this task by directly mapping environmental states to actions, while model based methods attempt to construct a model of the environment, followed by a selection of optimal actions based on that model. Given the complementary advantages of both approaches, we suggest a novel procedure which augments a model free algorithm with a partial model. The resulting hybrid algorithm switches between a model based and a model free mode, depending on the current state and the agent's knowledge. Our method relies on a novel definition for a partially known model, and an estimator that incorporates such knowledge in order to reduce uncertainty in stochastic approximation iterations. We prove that such an approach leads to improved policy evaluation whenever environmental knowledge is available, without compromising performance when such knowledge is absent. Numerical simulations demonstrate the effectiveness of the approach on policy gradient and Q-learning algorithms, and its usefulness in solving a call admission control problem.},
journal = {J. Mach. Learn. Res.},
month = {jun},
pages = {1927–1966},
numpages = {40},
keywords = {markov decision processes, hybrid model based model free algorithms, temporal difference, stochastic approximation, reinforcement learning}
}

@inproceedings{10.1145/3427773.3427862,
author = {Christensen, Morten Herget and Ernewein, C\'{e}dric and Pinson, Pierre},
title = {Demand Response through Price-Setting Multi-Agent Reinforcement Learning},
year = {2020},
isbn = {9781450381932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427773.3427862},
doi = {10.1145/3427773.3427862},
abstract = {Price based demand response is a cost-effective way of obtaining flexibility needed in power systems with high penetration of intermittent renewable energy sources. Model-free deep reinforcement learning is proposed as a way to train autonomous agents for enabling buildings to participate in demand response programs as well as coordinating such programs though price setting in a multiagent setup. First, we show price responsive control of buildings with electric heat pumps using deep deterministic policy gradient. Then a coordinating agent is trained to manage a population of buildings by adjusting the price in order to keep the total load from exceeding the available capacity considering also the non-flexible base load.},
booktitle = {Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings \&amp; Cities},
pages = {1–5},
numpages = {5},
keywords = {demand response, neural networks, deep reinforcement learning, multi-agent systems},
location = {Virtual Event, Japan},
series = {RLEM'20}
}

@inproceedings{10.1145/3352593.3352625,
author = {Kaushik, Meha and Singhania, Nirvan and S., Phaniteja and Krishna, K. Madhava},
title = {Parameter Sharing Reinforcement Learning Architecture for Multi Agent Driving},
year = {2020},
isbn = {9781450366502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352593.3352625},
doi = {10.1145/3352593.3352625},
abstract = {Multi-agent learning provides a potential solution for frameworks to learn and simulate traffic behaviors. This paper proposes a novel architecture to learn multiple driving behaviors in a traffic scenario. The proposed architecture can learn multiple behaviors independently as well as simultaneously. We take advantage of the homogeneity of agents and learn in a parameter sharing paradigm. To further speed up the training process asynchronous updates are employed into the architecture. While learning different behaviors simultaneously, the given framework was also able to learn cooperation between the agents, without any explicit communication. We applied this framework to learn two important behaviors in driving: 1) Lane-Keeping and 2) Over-Taking. Results indicate faster convergence and learning of a more generic behavior, that is scalable to any number of agents. When compared the results with existing approaches, our results indicate equal and even better performance in some cases.},
booktitle = {Proceedings of the Advances in Robotics 2019},
articleno = {31},
numpages = {7},
keywords = {Parameter sharing, Reinforcement learning, Autonomous vehicles, Multi-agents},
location = {Chennai, India},
series = {AIR 2019}
}

@inproceedings{10.1145/3514221.3526181,
author = {Chunduri, Pramod and Bang, Jaeho and Lu, Yao and Arulraj, Joy},
title = {Zeus: Efficiently Localizing Actions in Videos Using Reinforcement Learning},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3526181},
doi = {10.1145/3514221.3526181},
abstract = {Detection and localization of actions in videos is an important problem in practice. State-of-the-art video analytics systems are unable to efficiently and effectively answer such action queries because actions often involve a complex interaction between objects and are spread across a sequence of frames; detecting and localizing them requires computationally expensive deep neural networks. It is also important to consider the entire sequence of frames to answer the query effectively.In this paper, we present ZEUS, a video analytics system tailored for answering action queries. We present a novel technique for efficiently answering these queries using deep reinforcement learning. ZEUS trains a reinforcement learning agent that learns to adaptively modify the input video segments that are subsequently sent to an action classification network. The agent alters the input segments along three dimensions - sampling rate, segment length, and resolution. To meet the user-specified accuracy target, ZEUS's query optimizer trains the agent based on an accuracy-aware, aggregate reward function. Evaluation on three diverse video datasets shows that ZEUS outperforms state-of-the-art frame- and window-based filtering techniques by up to 22.1x and 4.7x, respectively. It also consistently meets the user-specified accuracy target across all queries.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {545–558},
numpages = {14},
keywords = {video database management systems, reinforcement learning, action localization, video analytics},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.5555/3463952.3464094,
author = {Sim\~{a}o, Thiago D. and Jansen, Nils and Spaan, Matthijs T. J.},
title = {AlwaysSafe: Reinforcement Learning without Safety Constraint Violations during Training},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Deploying reinforcement learning (RL) involves major concerns around safety. Engineering a reward signal that allows the agent to maximize its performance while remaining safe is not trivial. Safe RL studies how to mitigate such problems. For instance, we can decouple safety from reward using constrained Markov decision processes (CMDPs), where an independent signal models the safety aspects. In this setting, an RL agent can autonomously find tradeoffs between performance and safety. Unfortunately, most RL agents designed for CMDPs only guarantee safety after the learning phase, which might prevent their direct deployment. In this work, we investigate settings where a concise abstract model of the safety aspects is given, a reasonable assumption since a thorough understanding of safety-related matters is a prerequisite for deploying RL in typical applications. Factored CMDPs provide such compact models when a small subset of features describe the dynamics relevant for the safety constraints. We propose an RL algorithm that uses this abstract model to learn policies for CMDPs safely, that is without violating the constraints. During the training process, this algorithm can seamlessly switch from a conservative policy to a greedy policy without violating the safety constraints. We prove that this algorithm is safe under the given assumptions. Empirically, we show that even if safety and reward signals are contradictory, this algorithm always operates safely and, when they are aligned, this approach also improves the agent's performance.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1226–1235},
numpages = {10},
keywords = {reinforcement learning, safe reinforcement learning, CMDP},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@article{10.1145/3404197,
author = {Raza, Syed Ali and Williams, Mary-Anne},
title = {Human Feedback as Action Assignment in Interactive Reinforcement Learning},
year = {2020},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3404197},
doi = {10.1145/3404197},
abstract = {Teaching by demonstrations and teaching by assigning rewards are two popular methods of knowledge transfer in humans. However, showing the right behaviour (by demonstration) may appear more natural to a human teacher than assessing the learner’s performance and assigning a reward or punishment to it. In the context of robot learning, the preference between these two approaches has not been studied extensively. In this article, we propose a method that replaces the traditional method of reward assignment with action assignment (which is similar to providing a demonstration) in interactive reinforcement learning. The main purpose of the suggested action is to compute a reward by seeing if the suggested action was followed by the self-acting agent or not. We compared action assignment with reward assignment via a user study conducted over the web using a two-dimensional maze game. The logs of interactions showed that action assignment significantly improved users’ ability to teach the right behaviour. The survey results showed that both action and reward assignment seemed highly natural and usable, reward assignment required more mental effort, repeatedly assigning rewards and seeing the agent disobey commands caused frustration in users, and many users desired to control the agent’s behaviour directly.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {aug},
articleno = {14},
numpages = {24},
keywords = {reinforcement learning, Interactive machine learning, learning from human teachers, reward shaping}
}

@inproceedings{10.5555/2615731.2615762,
author = {Bogert, Kenneth and Doshi, Prashant},
title = {Multi-Robot Inverse Reinforcement Learning under Occlusion with Interactions},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We consider the problem of learning the behavior of multiple mobile robots executing fixed trajectories in a common space and possibly interacting with each other in their execution. The mobile robots are observed by a subject robot from a vantage point from which it can observe a portion of their trajectories only. This problem exhibits wide-ranging applications and the specific application we consider here is that of the subject robot who desires to penetrate a simple perimeter patrol by two interacting robots and reach a goal location. Our approach extends single-agent inverse reinforcement learning (IRL) to a multi-robot setting and partial observability, and models the interaction between the mobile robots as equilibrium behavior. IRL provides weights over the features of the robots' reward functions, thereby allowing us to learn their preferences. Subsequently, we derive a Markov decision process based policy for each other robot. We extend a predominant IRL technique and empirically evaluate its performance in our application setting. We show that our approach in the application setting results in significant improvement in the subject's ability to predict the patroller positions at different points in time with a corresponding increase in its successful penetration rate.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {173–180},
numpages = {8},
keywords = {inverse reinforcement, multi-robot systems, machine learning, patrolling},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{10.1145/3512290.3528844,
author = {Turner, Matthew J. and Hemberg, Erik and O'Reilly, Una-May},
title = {Analyzing Multi-Agent Reinforcement Learning and Coevolution in Cybersecurity},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528844},
doi = {10.1145/3512290.3528844},
abstract = {Cybersecurity simulations can offer deep insights into the behavior of agents in the battle to secure computer systems. We build on existing work modeling the competition between an attacker and defender on a network architecture in a zero-sum game using a graph database linking cybersecurity attack patterns, vulnerabilities, and software. We apply coevolution to this challenging environment, and in a novel modeling approach for this problem, interpret each population as a distribution over fixed strategies to form a mixed strategy Nash equilibrium. We compare the results to solutions generated by multi-agent reinforcement learning and show that evolutionary methods demonstrate a considerable degree of robustness to parameter misspecification in this environment.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1290–1298},
numpages = {9},
keywords = {cybersecurity, coevolution, nash equilibrium, evolutionary algorithms, machine learning},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.5555/3463952.3464029,
author = {Ilhan, Ercument and Gow, Jeremy and Perez Liebana, Diego},
title = {Action Advising with Advice Imitation in Deep Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Action advising is a peer-to-peer knowledge exchange technique built on the teacher-student paradigm to alleviate the sample inefficiency problem in deep reinforcement learning. Recently proposed student-initiated approaches have obtained promising results. However, due to being in the early stages of development, these also have some substantial shortcomings. One of the abilities that are absent in the current methods is further utilising advice by reusing, which is especially crucial in the practical settings considering the budget constraints in peer-to-peer interactions. In this study, we present an approach to enable the student agent to imitate previously acquired advice to reuse them directly in its exploration policy, without any interventions in the learning mechanism itself. In particular, we employ a behavioural cloning module to imitate the teacher policy and use dropout regularisation to have a notion of epistemic uncertainty to keep track of which state-advice pairs are actually collected. As the results of experiments we conducted in three Atari games show, advice reusing via imitation is indeed a feasible option in deep RL and our approach can successfully achieve this while significantly improving the learning performance, even when it is paired with a simple early advising heuristic.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {629–637},
numpages = {9},
keywords = {deep reinforcement learning, deep Q-networks, action advising},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3447548.3467255,
author = {Liu, Zhuo and Li, Yanxuan and Sun, Xingzhi and Wang, Fei and Hu, Gang and Xie, Guotong},
title = {Dialogue Based Disease Screening Through Domain Customized Reinforcement Learning},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467255},
doi = {10.1145/3447548.3467255},
abstract = {In this paper, we study the problem of leveraging dialogue agents learned from reinforcement learning (RL) that can interact with patients for automatic disease screening. This application requires efficient and effective inquiry of appropriate symptoms to make accurate diagnosis recommendations. Existing studies have tried to use RL to perform both symptom inquiry and diagnosis simultaneously, which needs to deal with a large, heterogeneous action space that affects the learning efficiency and effectiveness. To address the challenge, we propose to leverage the models learned from the dialogue data to customize the settings of the reinforcement learning for more efficient action space exploration. In particular, a supervised diagnosis model is built and involved in the definition of state and reward. We also develop the clustering method to form a hierarchy in the action space. These customizations can make the learning task focus on checking the most relevant symptoms, which effectively boost the confidence of diagnosis. Besides, a novel hierarchical reinforcement learning framework with the pretraining strategy is used to reduce the dimension of action space and help the model to converge. For empirical evaluations, we conduct extensive experiments on both synthetic and real-world datasets. The results have demonstrated the superiority of our approach in diagnostic accuracy and interaction efficiency compared with other baseline methods.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {1120–1128},
numpages = {9},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.5555/3306127.3331971,
author = {Everett, Richard and Cobb, Adam and Markham, Andrew and Roberts, Stephen},
title = {Optimising Worlds to Evaluate and Influence Reinforcement Learning Agents},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Training reinforcement learning agents on a distribution of procedurally generated environments has become an increasingly common method for obtaining more generalisable agents. However, this makes evaluation challenging, as the space of possible environment settings is large; simply looking at the average performance is insufficient for understanding how well - or how poorly - the agents perform. To address this, we introduce a method for strategically evaluating and influencing the behaviour of reinforcement learning agents. Using deep generative modelling to encode the environment, we propose a World Agent which efficiently generates and optimises worlds (i.e. environment settings) relative to the performance of the agents. Through the use of our method on two distinct environments, we demonstrate the existence of worlds which minimise and maximise agent reward beyond the typically reported average reward. Additionally, we show how our method can also be used to modify the distribution of worlds that agents train on, influencing their emergent behaviour to be more desirable.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1943–1945},
numpages = {3},
keywords = {evaluation, agent simulation, training, reinforcement learning, procedurally generated environments},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/2905055.2905093,
author = {Gupta, Yogendra and Bhargava, Lava},
title = {Reinforcement Learning Based Routing for Cognitive Network on Chip},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905093},
doi = {10.1145/2905055.2905093},
abstract = {In a Multicore processor two or more independent central processing unit or cores acts as a single computing element. In the last decade the number of cores in a multicore chip has been increasing and it will continue for the discern future. As the number of processing cores in the chip increases the communication complexity between the cores will also increase. At that level managing, such a complex system requires some cognition in a network on chip. In this paper, we have used the methodology of implementing cognitive network on chip that can recognize the changes in the environment and it learn the new ways to adapt the changes. We used Q-routing which is based on reinforcement learning based aspect of machine learning (Q-learning) for a cognitive network on chip. The routing algorithm learns from the entire network and distribute traffic accordance to that. Experimental results show that learning based method achieves significant performance improvement over the other routing algorithms.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {35},
numpages = {3},
keywords = {Q-learning, machine learning, Network on chip},
location = {Udaipur, India},
series = {ICTCS '16}
}

@article{10.1145/3277904,
author = {Krening, Samantha and Feigh, Karen M.},
title = {Interaction Algorithm Effect on Human Experience with Reinforcement Learning},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/3277904},
doi = {10.1145/3277904},
abstract = {A goal of interactive machine learning (IML) is to enable people with no specialized training to intuitively teach intelligent agents how to perform tasks. Toward achieving that goal, we are studying how the design of the interaction method for a Bayesian Q-Learning algorithm impacts aspects of the human’s experience of teaching the agent using human-centric metrics such as frustration in addition to traditional ML performance metrics. This study investigated two methods of natural language instruction: critique and action advice. We conducted a human-in-the-loop experiment in which people trained two agents with different teaching methods but, unknown to each participant, the same underlying reinforcement learning algorithm. The results show an agent that learns from action advice creates a better user experience compared to an agent that learns from binary critique in terms of frustration, perceived performance, transparency, immediacy, and perceived intelligence. We identified nine main characteristics of an IML algorithm’s design that impact the human’s experience with the agent, including using human instructions about the future, compliance with input, empowerment, transparency, immediacy, a deterministic interaction, the complexity of the instructions, accuracy of the speech recognition software, and the robust and flexible nature of the interaction algorithm.},
journal = {J. Hum.-Robot Interact.},
month = {oct},
articleno = {16},
numpages = {22},
keywords = {natural language interface, Human-agent interaction, sentiment, human factors, reinforcement learning}
}

@inproceedings{10.1145/3487075.3487094,
author = {Huang, Conghui and Wang, Chaozhe and Tong, Qi},
title = {Infrared Air Combat Simulation Model for Deep Reinforcement Learning},
year = {2021},
isbn = {9781450389853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487075.3487094},
doi = {10.1145/3487075.3487094},
abstract = {Aiming at the problem of lacking credible and realistic infrared air combat simulation platform for applying the deep reinforcement learning method, this paper explored the design requirements for the construction of simulation system, built the overall architecture of infrared air combat simulation system, described the structure, principle and working process of the fighter jet, infrared air-to-air missile, point source decoy and environment model. The implementation method of the simulation system was given, and the credibility of the system was verified through attack and defense simulation examples and error analysis of missile anti-jamming probability, which indicated that the simulation system can be used for the training and testing of agents based on deep reinforcement learning in infrared air combat scenarios.},
booktitle = {Proceedings of the 5th International Conference on Computer Science and Application Engineering},
articleno = {19},
numpages = {7},
keywords = {Deep reinforcement learning, Agent, Air combat simulation},
location = {Sanya, China},
series = {CSAE '21}
}

@inproceedings{10.5555/3463952.3464113,
author = {Xu, Hang and Wang, Rundong and Raizman, Lev and Rabinovich, Zinovi},
title = {Transferable Environment Poisoning: Training-Time Attack on Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Studying adversarial attacks on Reinforcement Learning (RL) agents has become a key aspect of developing robust, RL-based solutions. Test-time attacks, which target the post-learning performance of an RL agent's policy, have been well studied in both white- and black-box settings. More recently, however, state-of-the-art works have shifted to investigate training-time attacks on RL agents, i.e., forcing the learning process towards a target policy designed by the attacker. Alas, these SOTA works continue to rely on white-box settings and/or use a reward-poisoning approach. In contrast, this paper studies environment-dynamics poisoning attacks at training time. Furthermore, while environment-dynamics poisoning presumes a transfer-learning capable agent, it also allows us to expand our approach to black-box attacks. Our overall framework, inspired by hierarchical RL, seeks the minimal environment-dynamics manipulation that will prompt the momentary policy of the agent to change in a desired manner. We show the attack efficiency by comparing it with the reward-poisoning approach, and empirically demonstrate the transferability of the environment-poisoning attack strategy. Finally, we seek to exploit the transferability of the attack strategy to handle black-box settings.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1398–1406},
numpages = {9},
keywords = {reinforcement learning, security, environment poisoning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3544549.3585611,
author = {Shakerimov, Aidar and Sarmonov, Shamil and Amirova, Aida and Oralbayeva, Nurziya and Zhanatkyzy, Aida and Telisheva, Zhansaule and Aimysheva, Arna and Sandygulova, Anara},
title = {QWriter: Technology-Enhanced Alphabet Acquisition Based on Reinforcement Learning},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585611},
doi = {10.1145/3544549.3585611},
abstract = {In Kazakhstan, the ongoing Cyrillic-to-Latin alphabet shift raises challenges for early literacy development and acquisition in the Kazakh language. This paper proposes the QWriter system to help young children learn the Latin-based Kazakh alphabet and its handwriting. The system consists of a humanoid robot NAO, a tablet with a stylus, and a Reinforcement Learning (RL) agent that learns a child’s mistakes and progress to maximize alphabet learning in the shortest period of time by adapting the order of practice words according to the child’s mistakes. To evaluate the effectiveness of the QWriter system, we conducted a between-subject design experiment with 59 Kazakh children aged 6-8 years old and compared their learning performance with a human tutor and the CoWriting Kazakh robot system. The results did not support our assumption, we found that the proposed system received significantly higher likability scores than the baseline human tutor.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {241},
numpages = {7},
keywords = {chilld-robot interaction, Kazakh, robot-assisted language learning, reinforcement learning},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3289602.3293934,
author = {Chen, Hongzheng and Shen, Minghua},
title = {A Deep-Reinforcement-Learning-Based Scheduler for High-Level Synthesis},
year = {2019},
isbn = {9781450361378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289602.3293934},
doi = {10.1145/3289602.3293934},
abstract = {As the most important stage in high-level synthesis (HLS), scheduling mostly relies on heuristic algorithms due to their speed, flexibility, and scalability. However, designing heuristics easily involves human bias, which makes the scheduling unpredictable in some specific cases. In this paper, we propose a deep-reinforcement-learning (Deep-RL) based scheduler for HLS. It maximumly reduces the human involvement and learns to schedule by itself. Firstly, we introduce a novel state and action representation for constrained scheduling problems, which is the foundation of the learning task. Secondly, we use a training pipeline to train the policy network. Supervised learning is used to initialize the weight of the network, and reinforcement learning is used to improve the performance, which makes the Deep-RL based scheduler practical for HLS. Finally, we compare our scheduler with the ASAP schedule and the optimal ILP schedule. Experimental results show our scheduler can reduce up to 74\% resource usage compared with the original ASAP schedule, and the gap between the optimal solution is small. Notably, this is the first work leveraging reinforcement learning in HLS and has great potential to be integrated into different HLS systems.},
booktitle = {Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {117},
numpages = {1},
keywords = {scheduling, high-level synthesis, fpga, deep reinforcement learning},
location = {Seaside, CA, USA},
series = {FPGA '19}
}

@inproceedings{10.1145/3490354.3494445,
author = {Lima Paiva, Francisco Caio and Felizardo, Leonardo Kanashiro and Bianchi, Reinaldo Augusto da Costa and Costa, Anna Helena Reali},
title = {Intelligent Trading Systems: A Sentiment-Aware Reinforcement Learning Approach},
year = {2022},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494445},
doi = {10.1145/3490354.3494445},
abstract = {The feasibility of making profitable trades on a single asset on stock exchanges based on patterns identification has long attracted researchers. Reinforcement Learning (RL) and Natural Language Processing have gained notoriety in these single-asset trading tasks, but only a few works have explored their combination. Moreover, some issues are still not addressed, such as extracting market sentiment momentum through the explicit capture of sentiment features that reflect the market condition over time and assessing the consistency and stability of RL results in different situations. Filling this gap, we propose the Sentiment-Aware RL (SentARL) intelligent trading system that improves profit stability by leveraging market mood through an adaptive amount of past sentiment features drawn from textual news. We evaluated SentARL across twenty assets, two transaction costs, and five different periods and initializations to show its consistent effectiveness against baselines. Subsequently, this thorough assessment allowed us to identify the boundary between news coverage and market sentiment regarding the correlation of price-time series above which SentARL's effectiveness is outstanding.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {40},
numpages = {9},
keywords = {stock markets, sentiment analysis, deep reinforcement learning},
location = {Virtual Event},
series = {ICAIF '21}
}

@article{10.1613/jair.1.12372,
author = {Furelos-Blanco, Daniel and Law, Mark and Jonsson, Anders and Broda, Krysia and Russo, Alessandra},
title = {Induction and Exploitation of Subgoal Automata for Reinforcement Learning},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12372},
doi = {10.1613/jair.1.12372},
abstract = {In this paper we present ISA, an approach for learning and exploiting subgoals in episodic reinforcement learning (RL) tasks. ISA interleaves reinforcement learning with the induction of a subgoal automaton, an automaton whose edges are labeled by the task’s subgoals expressed as propositional logic formulas over a set of high-level events. A subgoal automaton also consists of two special states: a state indicating the successful completion of the task, and a state indicating that the task has finished without succeeding. A state-of-the-art inductive logic programming system is used to learn a subgoal automaton that covers the traces of high-level events observed by the RL agent. When the currently exploited automaton does not correctly recognize a trace, the automaton learner induces a new automaton that covers that trace. The interleaving process guarantees the induction of automata with the minimum number of states, and applies a symmetry breaking mechanism to shrink the search space whilst remaining complete. We evaluate ISA in several gridworld and continuous state space problems using different RL algorithms that leverage the automaton structures. We provide an in-depth empirical analysis of the automaton learning performance in terms of the traces, the symmetry breaking and specific restrictions imposed on the final learnable automaton. For each class of RL problem, we show that the learned automata can be successfully exploited to learn policies that reach the goal, achieving an average reward comparable to the case where automata are not learned but handcrafted and given beforehand.},
journal = {J. Artif. Int. Res.},
month = {may},
pages = {1031–1116},
numpages = {86}
}

@inproceedings{10.5555/3398761.3398941,
author = {Yang, Jiachen and Borovikov, Igor and Zha, Hongyuan},
title = {Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill Discovery},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Human players in professional team sports achieve high level coordination by dynamically choosing complementary skills and executing primitive actions to perform these skills. As a step toward creating intelligent agents with this capability for fully cooperative multi-agent settings, we propose a two-level hierarchical multi-agent reinforcement learning (MARL) algorithm with unsupervised skill discovery. Agents learn useful and distinct skills at the low level via independent Q-learning, while they learn to select complementary latent skill variables at the high level via centralized multi-agent training with an extrinsic team reward. The set of low-level skills emerges from an intrinsic reward that solely promotes the decodability of latent skill variables from the trajectory of a low-level skill, without the need for hand-crafted rewards for each skill. For scalable decentralized execution, each agent independently chooses latent skill variables and primitive actions based on local observations. Our overall method enables the use of general cooperative MARL algorithms for training high level policies and single-agent RL for training low level skills. Experiments on a stochastic high dimensional team game show the emergence of useful skills and cooperative team play. The interpretability of the learned skills show the promise of the proposed method for achieving human-AI cooperation in team sports games.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1566–1574},
numpages = {9},
keywords = {hierarchical learning, multi-agent learning, option discovery},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@article{10.1613/jair.1.12594,
author = {Fu, Justin and Tacchetti, Andrea and Perolat, Julien and Bachrach, Yoram},
title = {Evaluating Strategic Structures in Multi-Agent Inverse Reinforcement Learning},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12594},
doi = {10.1613/jair.1.12594},
abstract = {A core question in multi-agent systems is understanding the motivations for an agent's actions based on their behavior. Inverse reinforcement learning provides a framework for extracting utility functions from observed agent behavior, casting the problem as finding domain parameters which induce such a behavior from rational decision makers.&nbsp; We show how to efficiently and scalably extend inverse reinforcement learning to multi-agent settings, by reducing the multi-agent problem to N single-agent problems while still satisfying rationality conditions such as strong rationality. However, we observe that rewards learned naively tend to lack insightful structure, which causes them to produce undesirable behavior when optimized in games with different players from those encountered during training. We further investigate conditions under which rewards or utility functions can be precisely identified, on problem domains such as normal-form and Markov games, as well as auctions, where we show we can learn reward functions that properly generalize to new settings.},
journal = {J. Artif. Int. Res.},
month = {sep},
pages = {925–951},
numpages = {27},
keywords = {reinforcement learning, multiagent systems, game theory}
}

@article{10.5555/3291125.3309631,
author = {\v{S}o\v{s}i\'{c}, Adrian and Zoubir, Abdelhak M. and Rueckert, Elmar and Peters, Jan and Koeppl, Heinz},
title = {Inverse Reinforcement Learning via Nonparametric Spatio-Temporal Subgoal Modeling},
year = {2018},
issue_date = {January 2018},
publisher = {JMLR.org},
volume = {19},
number = {1},
issn = {1532-4435},
abstract = {Advances in the field of inverse reinforcement learning (IRL) have led to sophisticated inference frameworks that relax the original modeling assumption of observing an agent behavior that reflects only a single intention. Instead of learning a global behavioral model, recent IRL methods divide the demonstration data into parts, to account for the fact that different trajectories may correspond to different intentions, e.g., because they were generated by different domain experts. In this work, we go one step further: using the intuitive concept of subgoals, we build upon the premise that even a single trajectory can be explained more efficiently locally within a certain context than globally, enabling a more compact representation of the observed behavior. Based on this assumption, we build an implicit intentional model of the agent's goals to forecast its behavior in unobserved situations. The result is an integrated Bayesian prediction framework that significantly outperforms existing IRL solutions and provides smooth policy estimates consistent with the expert's plan. Most notably, our framework naturally handles situations where the intentions of the agent change over time and classical IRL algorithms fail. In addition, due to its probabilistic nature, the model can be straightforwardly applied in active learning scenarios to guide the demonstration process of the expert.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2777–2821},
numpages = {45},
keywords = {Bayesian nonparametric modeling, graphical models, gibbs sampling, inverse reinforcement learning, learning from demonstration, subgoal inference}
}

@inproceedings{10.1145/3468218.3469037,
author = {Carmack, Joseph and Schmidt, Steve and Kuzdeba, Scott},
title = {Multi-Agent Reinforcement Learning Approaches to RF Fingerprint Enhancement},
year = {2021},
isbn = {9781450385619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468218.3469037},
doi = {10.1145/3468218.3469037},
abstract = {Deep learning based RF Fingerprinting has shown great promise for IoT device security. This work explores various multi-agent reinforcement learning approaches to enable RF Fingerprint enhancement for an ensemble of transmitters. A RiftNetTM Reconstruction Model (RRM) is used to learn a latent Wi-Fi signal representation and how to reconstruct from that latent representation at the transmitter such that the reconstruction uniquely excites parts of the front-end to enhance the fingerprint. Deep reinforcement learning is then employed to learn the RRM control policy. Details on the design of the control interface, state representation, and rewards structure are presented for four different policy approaches. The resulting computational and security characteristics are discussed.},
booktitle = {Proceedings of the 3rd ACM Workshop on Wireless Security and Machine Learning},
pages = {67–72},
numpages = {6},
location = {Abu Dhabi, United Arab Emirates},
series = {WiseML '21}
}

@inproceedings{10.5555/3463952.3464044,
author = {Li, Sheng and Gupta, Jayesh K. and Morales, Peter and Allen, Ross and Kochenderfer, Mykel J.},
title = {Deep Implicit Coordination Graphs for Multi-Agent Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent reinforcement learning (MARL) requires coordination to efficiently solve certain tasks. Fully centralized control is often infeasible in such domains due to the size of joint action spaces. Coordination graph based formalization allows reasoning about the joint action based on the structure of interactions. However,they often require domain expertise in their design. This paper introduces the deep implicit coordination graph (DICG) architecture for such scenarios. DICG consists of a module for inferring the dynamic coordination graph structure which is then used by a graph neural network based module to learn to implicitly reason about the joint actions or values. DICG allows learning the tradeoff between full centralization and decentralization via standard actor-critic methods to significantly improve coordination for domains with large number of agents. We apply DICG to both centralized-training-centralized-execution and centralized-training-decentralized-execution regimes. We demonstrate that DICG solves the relative over generalization pathology in predatory-prey tasks as well as outperforms various MARL baselines on the challenging StarCraft II Multi-agent Challenge (SMAC) and traffic junction environments.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {764–772},
numpages = {9},
keywords = {deep reinforcement learning, multi-agent system, coordination, multi-agent reinforcement learning, graph neural network},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3302509.3311053,
author = {Gao, Qitong and Hajinezhad, Davood and Zhang, Yan and Kantaros, Yiannis and Zavlanos, Michael M.},
title = {Reduced Variance Deep Reinforcement Learning with Temporal Logic Specifications},
year = {2019},
isbn = {9781450362856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302509.3311053},
doi = {10.1145/3302509.3311053},
abstract = {In this paper, we propose a model-free reinforcement learning method to synthesize control policies for mobile robots modeled as Markov Decision Process (MDP) with unknown transition probabilities that satisfy Linear Temporal Logic (LTL) specifications. Specifically, we develop a reduced variance deep Q-Learning technique that relies on Neural Networks (NN) to approximate the state-action values of the MDP and employs a reward function that depends on the accepting condition of the Deterministic Rabin Automaton (DRA) that captures the LTL specification. The key idea is to convert the deep Q-Learning problem into a nonconvex max-min optimization problem with a finite-sum structure, and develop an Arrow-Hurwicz-Uzawa type stochastic reduced variance algorithm with constant stepsize to solve it. Unlike Stochastic Gradient Descent (SGD) methods that are often used in deep reinforcement learning, our method can estimate the gradients of an unknown loss function more accurately and can improve the stability of the training process. Moreover, our method does not require learning the transition probabilities in the MDP, constructing a product MDP, or computing Accepting Maximal End Components (AMECs). This allows the robot to learn an optimal policy even if the environment cannot be modeled accurately or if AMECs do not exist. In the latter case, the resulting control policies minimize the frequency with which the system enters bad states in the DRA that violate the task specifications. To the best of our knowledge, this is the first model-free deep reinforcement learning algorithm that can synthesize policies that maximize the probability of satisfying an LTL specification even if AMECs do not exist. Rigorous convergence analysis and rate of convergence are provided for the proposed algorithm as well as numerical experiments that validate our method.},
booktitle = {Proceedings of the 10th ACM/IEEE International Conference on Cyber-Physical Systems},
pages = {237–248},
numpages = {12},
keywords = {reinforcement learning, linear temporal logic, reduced variance stochastic optimization},
location = {Montreal, Quebec, Canada},
series = {ICCPS '19}
}

@inproceedings{10.1145/3343031.3350935,
author = {Yu, Tong and Shen, Yilin and Zhang, Ruiyi and Zeng, Xiangyu and Jin, Hongxia},
title = {Vision-Language Recommendation via Attribute Augmented Multimodal Reinforcement Learning},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350935},
doi = {10.1145/3343031.3350935},
abstract = {Interactive recommenders have demonstrated the advantage over traditional recommenders with dynamic change of items. However, the traditional user feedback in the format of clicks or ratings, provides limited user preference information and limited history tracking capabilities. As a result, it takes a user many interactions to find a desired item. Data of other modalities, such as item visual appearance and user comments in natural language, may enable richer user feedback. However, there are several critical challenges to be addressed when utilizing these multimodal data: multimodal matching, user preference tracking, and adaptation to dynamic unseen items. Without properly handling these challenges, the recommendations can easily violate the users' preference from their past natural language feedback. In this paper, we introduce a novel approach, called vision-language recommendation, that enables users to provide natural language feedback on visual products to have more natural and effective interactions. To model more explicit and accurate multimodal matching, we propose a novel visual attribute augmented reinforcement learning approach that enhances the grounding of natural language to visual items. Furthermore, to effectively track the users' preference and overcome the performance deficiency on dynamic unseen items after deployment, we propose a novel history multimodal matching reward to continuously adapt the model on-the-fly. Empirical results show that, our system augmented by visual attribute and history multimodal matching can significantly increase the success rate, reduce the number of recommendations that violate the user's previous feedback, and need less number of user interactions to find the desired items.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {39–47},
numpages = {9},
keywords = {reinforcement learning, vision and language, interactive recommender system, multimodal},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/2663474.2663481,
author = {Zhu, Minghui and Hu, Zhisheng and Liu, Peng},
title = {Reinforcement Learning Algorithms for Adaptive Cyber Defense against Heartbleed},
year = {2014},
isbn = {9781450331500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663474.2663481},
doi = {10.1145/2663474.2663481},
abstract = {In this paper, we investigate a model where a defender and an attacker simultaneously and repeatedly adjust the defenses and attacks. Under this model, we propose two iterative reinforcement learning algorithms which allow the defender to identify optimal defenses when the information about the attacker is limited. With probability one, the adaptive reinforcement learning algorithm converges to the best response with respect to the attacks when the attacker diminishingly explores the system. With a probability arbitrarily close to one, the robust reinforcement learning algorithm converges to the min-max strategy despite that the attacker persistently explores the system. The algorithm convergence is formally proven and the algorithm performance is verified via numerical simulations.},
booktitle = {Proceedings of the First ACM Workshop on Moving Target Defense},
pages = {51–58},
numpages = {8},
keywords = {security, algorithms},
location = {Scottsdale, Arizona, USA},
series = {MTD '14}
}

@inproceedings{10.1145/3416504.3424333,
author = {Paduraru, Ciprian and Stefanescu, Alin and Ghimis, Bogdan},
title = {Testing Multi-Tenant Applications Using Fuzzing and Reinforcement Learning},
year = {2020},
isbn = {9781450381239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416504.3424333},
doi = {10.1145/3416504.3424333},
abstract = {Testing cloud applications has recently gained in importance since many companies migrated their operations in the cloud. To optimise resources, cloud applications may serve several users at once in a so-called multi-tenant setting. We propose a new technique for testing multi-tenant applications using reinforcement learning combined with gray-box fuzzing techniques. A preliminary evaluation using a combination of fuzzing techniques and genetic algorithms is also provided.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Languages and Tools for Next-Generation Testing},
pages = {1–6},
numpages = {6},
keywords = {fuzz testing, security, Multi-tenancy, reinforcement learning, cloud},
location = {Virtual, USA},
series = {LANGETI 2020}
}

@article{10.1145/3610300,
author = {Zhadan, Anastasia and Allahverdyan, Alexander and Kondratov, Ivan and Mikheev, Vikenty and Petrosian, Ovanes and Romanovskii, Aleksei and Kharin, Vitaliy},
title = {Multi-Agent Reinforcement Learning-Based Adaptive Heterogeneous DAG Scheduling},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3610300},
doi = {10.1145/3610300},
abstract = {Static scheduling of computational workflow represented by a directed acyclic graph (DAG) is an important problem in many areas of computer science. The main idea and novelty of the proposed algorithm is an adaptive heuristic or graph metric that uses a different heuristic rule at each scheduling step depending on local workflow. It is also important to note that multi-agent reinforcement learning is used to determine scheduling policy based on adaptive metrics. To prove the efficiency of the approach, a comparison with the state-of-the-art DAG scheduling algorithms is provided: DONF, CPOP, HCPT, HPS, and PETS. Based on the simulation results, the proposed algorithm shows an improvement of up to 30\% on specific graph topologies and an average performance gain of 5.32\%, compared to the best scheduling algorithm, DONF (suitable for large-scale scheduling), on a large number of random DAGs. Another important result is that using the proposed algorithm it was possible to cover 30.01\% of the proximity interval from the best scheduling algorithm to the global optimal solution. This indicates that the idea of an adaptive metric for DAG scheduling is important and requires further research and development.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {oct},
articleno = {87},
numpages = {26},
keywords = {Multi-agent deep reinforcement learning, deep learning, directed acyclic graph, scheduling, proximal policy optimization}
}

@article{10.1145/3039902.3039915,
author = {Su, Jiang and Liu, Jianxiong and Thomas, David B. and Cheung, Peter Y.K.},
title = {Neural Network Based Reinforcement Learning Acceleration on FPGA Platforms},
year = {2017},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0163-5964},
url = {https://doi.org/10.1145/3039902.3039915},
doi = {10.1145/3039902.3039915},
abstract = {Deep Q-learning (DQN) is a recently proposed reinforcement learning algorithm where a neural network is applied as a non-linear approximator to its value function. The exploitation-exploration mechanism allows the training and prediction of the NN to execute simultaneously in an agent during its interaction with the environment. Agents often act independently on battery power, so the training and prediction must occur within the agent and on a limited power budget. In this work, We propose an FPGA acceleration system design for Neural Network Q-learning (NNQL). Our proposed system has high flexibility due to the support to run-time network parameterization, which allows neuroevolution algorithms to dynamically restructure the network to achieve better learning results. Additionally, the power consumption of our proposed system is adaptive to the network size because of a new processing element design. Based on our test cases on networks with hidden layer size ranging from 32 to 16384, our proposed system achieves 7x to 346x speedup compared to GPU implementation and 22x to 77x speedup to hand-coded CPU counterpart.},
journal = {SIGARCH Comput. Archit. News},
month = {jan},
pages = {68–73},
numpages = {6}
}

@inproceedings{10.5555/3545946.3598816,
author = {Gangopadhyay, Briti and Dasgupta, Pallab and Dey, Soumyajit},
title = {Counterexample-Guided Policy Refinement in Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-Agent Reinforcement Learning (MARL) policies are being incorporated into a wide range of safety-critical applications. It is important for these policies to be free of counterexamples and adhere to safety requirements. We present a methodology for the counterexample-guided refinement of an optimized MARL policy with respect to given safety specifications. The proposed algorithm refines a calibrated MARL policy to become safer by eliminating counterexamples found during testing, using targeted gradient updates. We empirically validate our method on different cooperative multi-agent tasks and demonstrate that targeted gradient updates induce safety in MARL policies.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1606–1614},
numpages = {9},
keywords = {counterexample-guided refinement, multi-agent reinforcement learning, multi-agent proximal policy optimization},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3209978.3210127,
author = {Rosset, Corby and Jose, Damien and Ghosh, Gargi and Mitra, Bhaskar and Tiwary, Saurabh},
title = {Optimizing Query Evaluations Using Reinforcement Learning for Web Search},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210127},
doi = {10.1145/3209978.3210127},
abstract = {In web search, typically a candidate generation step selects a small set of documents---from collections containing as many as billions of web pages---that are subsequently ranked and pruned before being presented to the user. In Bing, the candidate generation involves scanning the index using statically designed match plans that prescribe sequences of different match criteria and stopping conditions. In this work, we pose match planning as a reinforcement learning task and observe up to 20\% reduction in index blocks accessed, with small or no degradation in the quality of the candidate sets.},
booktitle = {The 41st International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval},
pages = {1193–1196},
numpages = {4},
keywords = {query evaluation, reinforcement learning, web search},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

