@article{ZHOU2022108078,
title = {Maintenance optimisation of multicomponent systems using hierarchical coordinated reinforcement learning},
journal = {Reliability Engineering & System Safety},
volume = {217},
pages = {108078},
year = {2022},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2021.108078},
url = {https://www.sciencedirect.com/science/article/pii/S0951832021005767},
author = {Yifan Zhou and Bangcheng Li and Tian Ran Lin},
keywords = {Condition based maintenance, Coordinated reinforcement learning, Hierarchical multiagent reinforcement learning, Deep reinforcement learning},
abstract = {The Markov decision process (MDP) is a widely used method to optimise the maintenance of multicomponent systems, which can provide a system-level maintenance action at each decision point to address various dependences among components. However, MDP suffers from the “curse of dimensionality” and can only process small-scale systems. This paper develops a hierarchical coordinated reinforcement learning (HCRL) algorithm to optimise the maintenance of large-scale multicomponent systems. Both parameters of agents and the coordination relationship among agents are designed based on system characteristics. Furthermore, the hierarchical structure of agents is established according to the structural importance measures of components. The effectiveness of the proposed HCRL algorithm is validated using two maintenance optimisation problems, one on a natural gas plant system and the other using a 12-component series system under dependant competing risks. Results show that the proposed HCRL outperforms methods in two recently published papers and other benchmark approaches including the new emerging deep reinforcement learning.}
}
@article{ZEYNIVAND2022103497,
title = {Traffic flow control using multi-agent reinforcement learning},
journal = {Journal of Network and Computer Applications},
volume = {207},
pages = {103497},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103497},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522001394},
author = {A. Zeynivand and A. Javadpour and S. Bolouki and A.K. Sangaiah and F. Ja’fari and P. Pinto and W. Zhang},
keywords = {Smart control of traffic lights, Service quality, Artificial intelligence, Multi-way systems, Reinforcement learning and Q-learning},
abstract = {One of the technologies based on information technology used today is the VANET network used for inter-road communication. Today, many developed countries use this technology to optimize travel times, queue lengths, number of vehicle stops, and overall traffic network efficiency. In this research, we investigate the critical and necessary factors to increase the quality of VANET networks. This paper focuses on increasing the quality of service using multi-agent learning methods. The innovation of this study is using artificial intelligence to improve the network’s quality of service, which uses a mechanism and algorithm to find the optimal behavior of agents in the VANET. The result indicates that the proposed method is more optimal in the evaluation criteria of packet delivery ratio (PDR), transaction success rate, phase duration, and queue length than the previous ones. According to the evaluation criteria, TSR 6.342%, PDR 9.105%, QL 7.143%, and PD 6.783% are more efficient than previous works.}
}
@article{BANERJEE2021102508,
title = {Deep neural network based missing data prediction of electrocardiogram signal using multiagent reinforcement learning},
journal = {Biomedical Signal Processing and Control},
volume = {67},
pages = {102508},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102508},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421001051},
author = {Soumyendu Banerjee and Girish Kumar Singh},
keywords = {Beat delineation, Bidirectional LSTM recurrent neural network, Electrocardiogram, Missing data prediction, Multiagent reinforcement learning},
abstract = {Objective
Clinical morphology of electrocardiogram (ECG) signal is compulsory to analyze the cardiac activity. During long term measurement, missing of data is a common factor, caused by sensor loosening, resulting in devastating feature extraction procedure; hence, prediction of those missing data is indispensable.
Methods
In this work, bidirectional long short-term memory recurrent neural network (LSTM-RNN) based prediction of missing segment of ECG signal is accomplished, governed by reinforcement learning (RL) using multiagent. The LSTM-RNN has internal gate architecture, which can predict the upcoming sequence using the past features of ECG. Each agent of the multiagent system (MAS), which represents various fictitious domain of ECG, has independent learning module to predict respective domain using a ‘critic’ network, which was conducted by a multilayer perceptron neural network (MLPNN). The MAS finally predicted the missing segment by ‘cooperative learning’ algorithm using a ‘coordinator agent’.
Results
The proposed algorithm was tested on MIMIC-II ECG database, available in Physionet. Experimental result shows that, using the RL, the trained LSTM-RNN was able to predict the missing segments more precisely with correlation coefficient higher than 0.9 and low root mean squared error.
Conclusion
The proposed method is applicable to any single channel ECG signal, and the quality of predicted signal ensures a wide application in medical applications as well as telecardiology system.
Significance
A comparative study with previously published works showed an improved performance, related to ECG missing data prediction, implemented on MIMIC-II records.}
}
@article{MOURA2020331,
title = {Wireless control using reinforcement learning for practical web QoE},
journal = {Computer Communications},
volume = {154},
pages = {331-346},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.032},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419314860},
author = {Henrique D. Moura and Daniel F. Macedo and Marcos A.M. Vieira},
keywords = {Wireless network, Software defined network, Reinforcement learning, Q-Learning, Multi-armed bandit, Quality of Experience},
abstract = {Wireless networks show several challenges not found in wired networks, due to the dynamics of data transmission. Besides, home wireless networks are managed by non-technical people, and providers do not implement full management services because of the difficulties of manually managing thousands of devices. Thus, automatic management mechanisms are desirable. However, such control mechanisms are hard to achieve in practice because we do not always have a model of the process to be controlled, or the behavior of the environment is dynamic. Thus, the control must adapt to changing conditions, and it is necessary to identify the quality of the control executed from the perspective of the user of the network service. This article proposes a control loop for transmission power and channel selection, based on Software Defined Networking and Reinforcement Learning (RL), and capable of improving Web Quality of Experience metrics, thus benefiting the user. We evaluate a prototype in which some Access Points are controlled by a single controller or by independent controllers. The control loop uses the predicted Mean Opinion Score (MOS) as a reward, thus the system needs to classify the web traffic. We proposed a semi-supervised learning method to classify the web sites into three classes (light, average and heavy) that groups pages by their complexity, i.e. number and size of page elements. These classes define the MOS predictor used by the control loop. The proposed web site classifier achieves an average score of 87%±1%, classifying 500 unlabeled examples with only fifteen known examples, with a sub-second runtime. Further, the RL control loop achieves higher Mean Opinion Score (up to 167% in our best result) than the baselines. The page load time of clients browsing heavy web sites is improved by up to 6.6x.}
}
@article{LIANG2023103363,
title = {Crude oil price prediction using deep reinforcement learning},
journal = {Resources Policy},
volume = {81},
pages = {103363},
year = {2023},
issn = {0301-4207},
doi = {https://doi.org/10.1016/j.resourpol.2023.103363},
url = {https://www.sciencedirect.com/science/article/pii/S0301420723000715},
author = {Xuedong Liang and Peng Luo and Xiaoyan Li and Xia Wang and Lingli Shu},
keywords = {Crude oil price forecasting, Natural resource prices, Time-series forecasting, Benchmark oil price, Deep reinforcement learning},
abstract = {Crude oil price forecasting has received considerable attention owing to its significance in the commodity market and non-linear complexity in forecasting tasks. This study aims to develop a novel deep reinforcement learning algorithm for multi-step ahead crude oil price forecasting in three major commodity exchanges. The proposed algorithm includes two main improvements: (a) A dynamic action exploration mechanism based on the stochastic processes conforming to commodity price fluctuations is designed for accuracy and generalization. (b) A dynamic update policy of network parameters based on approximate optimization theory is developed to improve the network's learning efficiency. The algorithm's effectiveness is experimentally verified and compared with five state-of-the-art algorithms. The main findings are as follows. (a) DRL's forecasting ability is developed in crude oil price forecasting, which may be extended to the forecasting of other natural resource prices. (b) The proposed algorithm can be applied to the data of the world's three major crude oil price benchmarks with considerable universality. (c) The accuracy of the proposed algorithm declines indistinctively with the expansion of the forecasting step; however, it reflects the actual price and fluctuation. These findings have implications in accelerating the global economic recovery and exploring AI in the energy market.}
}
@article{JONES20201,
title = {Reframing appetitive reinforcement learning and reward valuation as effects mediated by hippocampal-dependent behavioral inhibition},
journal = {Nutrition Research},
volume = {79},
pages = {1-12},
year = {2020},
issn = {0271-5317},
doi = {https://doi.org/10.1016/j.nutres.2020.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0271531720304401},
author = {Sabrina Jones and Alexia Hyde and Terry L. Davidson},
keywords = {Hippocampus, Inhibition, Reinforcement learning, Reward valuation, Western diet, GLP-1},
abstract = {Traditional theories of neuroeconomics focus on reinforcement learning and reward value. We propose here a novel reframing of reinforcement learning and motivation that includes a hippocampal-dependent regulatory mechanism which balances cue-induced behavioral excitation with behavioral inhibition. This mechanism enables interoceptive cues produced by respective food or drug satiety to antagonize the ability of excitatory food- and drug-related environmental cues to retrieve the memories of food and drug reinforcers, thereby suppressing the power of those cues to evoke appetitive behavior. When the operation of this mechanism is impaired, ability of satiety signals to inhibit appetitive behavior is weakened because the relative balance between inhibition and simple excitation is shifted toward increased retrieval of food and drug memories by environmental cues. In the present paper, we (1) describe the associative processes that constitute this mechanism of hippocampal-dependent behavior inhibition; (2) describe how a prevailing obesity-promoting diet and drugs of abuse produce hippocampal pathophysiologies that can selectively impair this inhibitory function; and (3) propose how glucagon-like peptide 1 (GLP-1), an incretin hormone that is recognized as an important satiety signal, may work to protect the hippocampal-dependent inhibition. Our perspective may add to neuroscientific and neuroeconomic analyses of both overeating and drug abuse by outlining the role of hippocampal-dependent memory processes in the control of both food and drug seeking behaviors. In addition, this view suggests that consideration should be given to diet- and drug induced hippocampal pathophysiologies, as potential novel targets for the treatment of dysregulated energy and drug intake.}
}
@article{MARTINEZ1999S527,
title = {Solving batch process scheduling/planning tasks using reinforcement learning},
journal = {Computers & Chemical Engineering},
volume = {23},
pages = {S527-S530},
year = {1999},
note = {European Symposium on Computer Aided Process Engineering},
issn = {0098-1354},
doi = {https://doi.org/10.1016/S0098-1354(99)80130-6},
url = {https://www.sciencedirect.com/science/article/pii/S0098135499801306},
author = {E.C. Martínez},
keywords = {Batch Process Management, Scheduling, Learning, Combinatorial Optimization},
abstract = {The complex and dynamic nature of shop-floor environments, coupled with unpredictable market demands, makes batch plant's reactivity a crucial management issue. In this work, reinforcement learning and a repair-based search strategy are integrated together in a learning problem-solver for scheduling/planning tasks. The overall design of the learning algorithm is based on a state-space search perspective in which the associated optimization problem is solved by starting in some initial infeasible solution and then proceeding to progressively repair intermediate solutions until a feasible one is found. A key component to accelerate search in the state space is learning an evaluation function that accumulates context-dependent knowledge about the goodness of applying a small set of repair operators so that future (re)scheduling and (re)planning problems can be solved with less effort. A demonstrative example is used to illustrate the importance of improving plant reactivity by learning to repair infeasible plans and schedules.}
}
@article{SUN2023109749,
title = {Deep reinforcement learning-based resilience enhancement strategy of unmanned weapon system-of-systems under inevitable interferences},
journal = {Reliability Engineering & System Safety},
pages = {109749},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109749},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023006634},
author = {Qin Sun and Hongxu Li and Yuanfu Zhong and Kezhou Ren and Yingchao Zhang},
keywords = {Resilience enhancement strategy, Unmanned weapon system-of-systems (UWSoS), Deep reinforcement learning (DRL), Graph convolution network (GCN), Proximal policy optimization (PPO)},
abstract = {Unmanned weapon system-of-systems (UWSoS) is a collection of unmanned weapon systems providing multiple interdependent capabilities to support the mission completion. The soaring number of interconnected systems makes UWSoS vulnerable in the face of inevitable uncertain interferences. Thus, devising an effective resilience enhancement strategy is critical to handling inevitable disruption events. However, preventive and protective methods are not all-inclusive because of the high uncertainty operation environment and the lack of autonomy. Hence, we studied the resilience enhancement problem of UWSoS from the recovery perspective based on deep reinforcement learning (DRL). First, a DRL-based resilience enhancement strategy framework is proposed, combining the graph convolution network and proximal policy optimization algorithm to extract the entities’ representation features and autonomously learn the resilience enhancement strategy to handle various interferences scenarios better. Subsequently, a collaboration action resilience contribution index-guided proximal policy optimization algorithm is proposed to improve training efficiency. Finally, extensive simulation experiments and comparisons with five similar algorithms demonstrate the effectiveness, adaptability, and superiority of the proposed strategy. This work could provide valuable scheduling schemes for decision-makers to guide the reliable operation of UWSoSs.}
}
@article{MASON2019300,
title = {A review of reinforcement learning for autonomous building energy management},
journal = {Computers & Electrical Engineering},
volume = {78},
pages = {300-312},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2019.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0045790618333421},
author = {Karl Mason and Santiago Grijalva},
keywords = {Reinforcement learning, Building energy management, Smart homes, Smart grid, Deep learning, Machine learning},
abstract = {The area of building energy management has received a significant amount of interest in recent years. This area is concerned with combining advancements in sensor technologies, communications and advanced control algorithms to optimize energy utilization. Reinforcement learning is one of the most prominent machine learning algorithms used for control problems and has had many successful applications in the area of building energy management. This research gives a comprehensive review of the literature relating to the application of reinforcement learning to developing autonomous building energy management systems. Energy savings of greater than 20% are reported in the literature for more complex building energy management problems when implementing reinforcement learning. The main direction for future research and challenges in reinforcement learning are also outlined.}
}
@article{JARAETTINGER2019105,
title = {Theory of mind as inverse reinforcement learning},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {105-110},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618302055},
author = {Julian Jara-Ettinger},
abstract = {We review the idea that Theory of Mind—our ability to reason about other people's mental states—can be formalized as inverse reinforcement learning. Under this framework, expectations about how mental states produce behavior are captured in a reinforcement learning (RL) model. Predicting other people’s actions is achieved by simulating a RL model with the hypothesized beliefs and desires, while mental-state inference is achieved by inverting this model. Although many advances in inverse reinforcement learning (IRL) did not have human Theory of Mind in mind, here we focus on what they reveal when conceptualized as cognitive theories. We discuss landmark successes of IRL, and key challenges in building human-like Theory of Mind.}
}
@article{MIAO20202154,
title = {Humming-Query and Reinforcement-Learning based Modeling Approach for Personalized Music Recommendation},
journal = {Procedia Computer Science},
volume = {176},
pages = {2154-2163},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.252},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920321566},
author = {Dezhuang Miao and Xuesong Lu and Qiwen Dong and Daocheng Hong},
keywords = {Playlist Prediction, Recommendation, User Model, Interactive Learning},
abstract = {Music recommendation is a prominent application of recommender systems, which has been attracting more and more attentions. There are two research streams of music recommender systems: one is static recommendation based on learning user’s preference according to historical data, and the other is dynamic recommendation considering user’s feedback. But the individual music preference for a certain moment is closely related to personal experience of the music and music literacy, as well as temporal scenario with diversity. Thus, it’s necessary to design a new music recommendation framework by integrating static recommendation and dynamic recommendation. Therefore, we propose a novel approach for music recommendation HRRS (Humming-Query and Reinforcement-Learning based Recommender Systems) by integrating prior two research streams. This novel recommendation framework HRRS based on humming query and reinforcement learning is learning and adapting to user’s current preference continually by collecting interactive data in real time. This preliminary recommendation framework captures song characters, personal dynamic preferences, and yields a better listening experience with proper interaction.}
}
@article{HEIBERG202217,
title = {Risk-based implementation of COLREGs for autonomous surface vehicles using deep reinforcement learning},
journal = {Neural Networks},
volume = {152},
pages = {17-33},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022001435},
author = {Amalie Heiberg and Thomas Nakken Larsen and Eivind Meyer and Adil Rasheed and Omer San and Damiano Varagnolo},
keywords = {Deep reinforcement learning, Collision avoidance, Path following, Collision risk indices, Machine learning controller, Autonomous surface vehicle},
abstract = {Autonomous systems are becoming ubiquitous and gaining momentum within the marine sector. Since the electrification of transport is happening simultaneously, autonomous marine vessels can reduce environmental impact, lower costs, and increase efficiency. Although close monitoring is still required to ensure safety, the ultimate goal is full autonomy. One major milestone is to develop a control system that is versatile enough to handle any weather and encounter that is also robust and reliable. Additionally, the control system must adhere to the International Regulations for Preventing Collisions at Sea (COLREGs) for successful interaction with human sailors. Since the COLREGs were written for the human mind to interpret, they are written in ambiguous prose and therefore not machine-readable or verifiable. Due to these challenges and the wide variety of situations to be tackled, classical model-based approaches prove complicated to implement and computationally heavy. Within machine learning (ML), deep reinforcement learning (DRL) has shown great potential for a wide range of applications. The model-free and self-learning properties of DRL make it a promising candidate for autonomous vessels. In this work, a subset of the COLREGs is incorporated into a DRL-based path following and obstacle avoidance system using collision risk theory. The resulting autonomous agent dynamically interpolates between path following and COLREG-compliant collision avoidance in the training scenario, isolated encounter situations, and AIS-based simulations of real-world scenarios.}
}
@article{LIANG2021273,
title = {Event-triggered reinforcement learning H∞ control design for constrained-input nonlinear systems subject to actuator failures},
journal = {Information Sciences},
volume = {543},
pages = {273-295},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.07.055},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520307180},
author = {Yuling Liang and Huaguang Zhang and Jie Duan and Shaoxin Sun},
keywords = {Sliding mode control, Event-triggered control, Reinforcement learning, Neural network,  fault-tolerant control},
abstract = {In the paper, a novel input-constrained H∞ fault-tolerant control approach is developed by using sliding mode control technology and event-triggered reinforcement learning (RL) algorithm. To reduce or even eliminate the impacts of the time-varying actuator failures, a properly sliding mode control strategy is proposed for the controlled system, while the event-triggered H∞ control scheme is established via RL algorithm for the equivalent sliding mode dynamics. By utilizing a single neural network (NN), the Hamilton–Jacobi–Bellman (HJB) equation can be solved approximately, thereby gaining time-triggered worst-case disturbance law, as well as event-triggered optimal control policy. Besides, it is unnecessary to given a initial stabilizing control input in the learning process of neural networks (NNs) in this paper. Moreover, the Lyapunov stability principle is applied to guarantee that the controlled system is uniformly ultimately bounded (UUB). Finally, to verify the feasibility and efficient performance of the developed approach, three simulations are carried out.}
}
@article{WEN2020118019,
title = {Modified deep learning and reinforcement learning for an incentive-based demand response model},
journal = {Energy},
volume = {205},
pages = {118019},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.118019},
url = {https://www.sciencedirect.com/science/article/pii/S0360544220311269},
author = {Lulu Wen and Kaile Zhou and Jun Li and Shanyong Wang},
keywords = {Demand response, Modified deep learning, Reinforcement learning, Smart grid},
abstract = {Incentive-based demand response (DR) program can induce end users (EUs) to reduce electricity demand during peak period through rewards. In this study, an incentive-based DR program with modified deep learning and reinforcement learning is proposed. A modified deep learning model based on recurrent neural network (MDL-RNN) was first proposed to identify the future uncertainties of environment by forecasting day-ahead wholesale electricity price, photovoltaic (PV) power output, and power load. Then, reinforcement learning (RL) was utilized to explore the optimal incentive rates at each hour which can maximize the profits of both energy service providers (ESPs) and EUs. The results showed that the proposed modified deep learning model can achieve more accurate forecasting results compared with some other methods. It can support the development of incentive-based DR programs under uncertain environment. Meanwhile, the optimized incentive rate can increase the total profits of ESPs and EUs while reducing the peak electricity demand. A short-term DR program was developed for peak electricity demand period, and the experimental results show that peak electricity demand can be reduced by 17%. This contributes to mitigating the supply-demand imbalance and enhancing power system security.}
}
@article{HOWELL2001549,
title = {Continuous action reinforcement learning automata and their application to adaptive digital filter design},
journal = {Engineering Applications of Artificial Intelligence},
volume = {14},
number = {5},
pages = {549-561},
year = {2001},
issn = {0952-1976},
doi = {https://doi.org/10.1016/S0952-1976(01)00034-3},
url = {https://www.sciencedirect.com/science/article/pii/S0952197601000343},
author = {M.N. Howell and T.J. Gordon},
keywords = {Reinforcement learning, Adaptive signal processing, System identification},
abstract = {In the design of adaptive IIR filters, the multi-modal nature of the error surfaces can limit the use of gradient-based and other iterative search methods. Stochastic learning automata have previously been shown to have global optimisation properties making them suitable for the optimisation of filter coefficients. Continuous action reinforcement learning automata are presented as an extension to the standard automata which operate over discrete parameter sets. Global convergence is claimed, and demonstrations are carried out via a number of computer simulations.}
}
@article{MATSUO2022267,
title = {Deep learning, reinforcement learning, and world models},
journal = {Neural Networks},
volume = {152},
pages = {267-275},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.03.037},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022001150},
author = {Yutaka Matsuo and Yann LeCun and Maneesh Sahani and Doina Precup and David Silver and Masashi Sugiyama and Eiji Uchibe and Jun Morimoto},
keywords = {Deep learning, Reinforcement learning, World models, Machine learning, Artificial intelligence},
abstract = {Deep learning (DL) and reinforcement learning (RL) methods seem to be a part of indispensable factors to achieve human-level or super-human AI systems. On the other hand, both DL and RL have strong connections with our brain functions and with neuroscientific findings. In this review, we summarize talks and discussions in the “Deep Learning and Reinforcement Learning” session of the symposium, International Symposium on Artificial Intelligence and Brain Science. In this session, we discussed whether we can achieve comprehensive understanding of human intelligence based on the recent advances of deep learning and reinforcement learning algorithms. Speakers contributed to provide talks about their recent studies that can be key technologies to achieve human-level intelligence.}
}
@article{QIU2023120826,
title = {Hierarchical multi-agent reinforcement learning for repair crews dispatch control towards multi-energy microgrid resilience},
journal = {Applied Energy},
volume = {336},
pages = {120826},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.120826},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923001903},
author = {Dawei Qiu and Yi Wang and Tingqi Zhang and Mingyang Sun and Goran Strbac},
keywords = {Repair crews, Multi-energy microgrid, Resilience, Power-gas-transportation network, Hierarchical multi-agent reinforcement learning},
abstract = {Extreme events are greatly impacting the normal operations of microgrids, which can lead to severe outages and affect the continuous supply of energy to customers, incurring substantial restoration costs. Repair crews (RCs) are regarded as crucial resources to provide system resilience owing to their mobility and flexibility characteristics in handling both transportation and energy systems. Nevertheless, effectively coordinating the dispatch of RCs towards system resilience is a complex decision-making problem, especially in the context of a multi-energy microgrid (MEMG) with enormous dynamics and uncertainties. To this end, this paper formulates the dispatch problem of RCs in a coupled transportation and power-gas network as a decentralized partially observable Markov decision process (Dec-POMDP). To solve this Dec-POMDP, a hierarchical multi-agent reinforcement learning (MARL) algorithm is proposed by featuring a two-level framework, where the high-level action is used for switching decision-making between transportation and power-gas networks, and the lower-level action constructed via the multi-agent proximal policy optimization (MAPPO) algorithm is used to compute the routing and repairing decisions of RCs in the transportation and power-gas networks, respectively. The proposed algorithm also introduces an abstracted critic network by integrating the load restoration status, which captures the system dynamics and stabilizes the training performance with privacy protection. Extensive case studies are evaluated on a coupled 6-bus power and 6-bus gas network integrated with a 9-node 12-edge transportation network. The proposed algorithm outperforms the conventional MARL algorithms in terms of policy quality, learning stability, and computational performance. Furthermore, the dispatch strategies of RCs are analyzed and their corresponding benefits for load restoration are also evaluated. Finally, the scalability of the proposed method is also investigated for a larger 33-bus power and 15-bus gas network integrated with an 18-node 27-edge transportation network.}
}
@article{LOFTUS2020253,
title = {Decision analysis and reinforcement learning in surgical decision-making},
journal = {Surgery},
volume = {168},
number = {2},
pages = {253-266},
year = {2020},
issn = {0039-6060},
doi = {https://doi.org/10.1016/j.surg.2020.04.049},
url = {https://www.sciencedirect.com/science/article/pii/S0039606020302610},
author = {Tyler J. Loftus and Amanda C. Filiberto and Yanjun Li and Jeremy Balch and Allyson C. Cook and Patrick J. Tighe and Philip A. Efron and Gilbert R. Upchurch and Parisa Rashidi and Xiaolin Li and Azra Bihorac},
abstract = {Background
Surgical patients incur preventable harm from cognitive and judgment errors made under time constraints and uncertainty regarding patients’ diagnoses and predicted response to treatment. Decision analysis and techniques of reinforcement learning theoretically can mitigate these challenges but are poorly understood and rarely used clinically. This review seeks to promote an understanding of decision analysis and reinforcement learning by describing their use in the context of surgical decision-making.
Methods
Cochrane, EMBASE, and PubMed databases were searched from their inception to June 2019. Included were 41 articles about cognitive and diagnostic errors, decision-making, decision analysis, and machine-learning. The articles were assimilated into relevant categories according to Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews guidelines.
Results
Requirements for time-consuming manual data entry and crude representations of individual patients and clinical context compromise many traditional decision-support tools. Decision analysis methods for calculating probability thresholds can inform population-based recommendations that jointly consider risks, benefits, costs, and patient values but lack precision for individual patient-centered decisions. Reinforcement learning, a machine-learning method that mimics human learning, can use a large set of patient-specific input data to identify actions yielding the greatest probability of achieving a goal. This methodology follows a sequence of events with uncertain conditions, offering potential advantages for personalized, patient-centered decision-making. Clinical application would require secure integration of multiple data sources and attention to ethical considerations regarding liability for errors and individual patient preferences.
Conclusion
Traditional decision-support tools are ill-equipped to accommodate time constraints and uncertainty regarding diagnoses and the predicted response to treatment, both of which often impair surgical decision-making. Decision analysis and reinforcement learning have the potential to play complementary roles in delivering high-value surgical care through sound judgment and optimal decision-making.}
}
@article{MOGHADAM2018402,
title = {Makespan reduction for dynamic workloads in cluster-based data grids using reinforcement-learning based scheduling},
journal = {Journal of Computational Science},
volume = {24},
pages = {402-412},
year = {2018},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2017.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S1877750317302314},
author = {Mahshid Helali Moghadam and Seyed Morteza Babamir},
keywords = {Data grid, Data-intensive task scheduling algorithm, Data communication cost, Reinforcement learning},
abstract = {Scheduling is one of the important problems within the scope of control and management in grid and cloud-based systems. Data grid still as a primary solution to process data-intensive tasks, deals with managing large amounts of distributed data in multiple nodes. In this paper, a two-phase learning-based scheduling algorithm is proposed for data-intensive tasks scheduling in cluster-based data grids. In the proposed scheduling algorithm, a hierarchical multi agent system, consisting of one global broker agent and several local agents, is applied to scheduling procedure in the cluster-based data grids. At the first step of the proposed scheduling algorithm, the global broker agent selects the cluster with the minimum data cost based on the data communication cost measure, then an adaptive policy based on Q-learning is used by the local agent of the selected cluster to schedule the task to the proper node of the cluster. The impacts of three action selection strategies have been investigated in the proposed scheduling algorithm, and the performance of different versions of the scheduling algorithm regarding different action selection strategies, has been evaluated under three types of workloads with heterogeneous tasks. Experimental results show that for dynamic workloads with varying task submission patterns, the proposed learning-based scheduling algorithm gives better performance compared to four common scheduling algorithm, Queue Length (Shortest Queue), Access Cost, Queue Access Cost (QAC) and HCS, which use regular combinations of primary parameters such as, data communication cost and queue length. Applying a learning-based strategy provides the scheduling algorithm with more adaptability to the changing conditions in the environment.}
}
@article{ZIELINSKI2021107602,
title = {3D robotic navigation using a vision-based deep reinforcement learning model},
journal = {Applied Soft Computing},
volume = {110},
pages = {107602},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107602},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621005238},
author = {P. Zieliński and U. Markowska-Kaczmar},
keywords = {Deep reinforcement learning, A2C, PPO, Vision-based navigation, YOLO, CNN},
abstract = {In this paper, we address a problem of vision-based 3D robotic navigation using deep reinforcement learning for an Autonomous Underwater Vehicle (AUV). Our research offers conclusions from the experimental study based on one of the RoboSub 2018 competition tasks. However, it can be generalized to any navigation task consisting of movement from a starting point to the front of the next station. The presented reinforcement learning-based model predicts the robot’s steering settings using the data acquired from the robot’s sensors. Its Vision Module may be based on a built-in convolutional network or a pre-trained TinyYOLO network so that a comparison of various levels of features’ complexity is possible. To enable evaluation of the proposed solution, we prepared a test environment imitating the real conditions. It provides the ability to steer the agent simulating the AUV and calculate values of rewards, used for training the model by evaluating its decisions. We study the solution in terms of the reward function form, the model’s hyperparameters and the exploited camera images processing method, and provide an analysis of the correctness and speed of the model’s functioning. As a result, we obtain a valid model able to steer the robot from the starting point to the destination based on visual cues and inputs from other sensors.}
}
@article{NGUYEN2020103915,
title = {A multi-objective deep reinforcement learning framework},
journal = {Engineering Applications of Artificial Intelligence},
volume = {96},
pages = {103915},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.103915},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620302475},
author = {Thanh Thi Nguyen and Ngoc Duy Nguyen and Peter Vamplew and Saeid Nahavandi and Richard Dazeley and Chee Peng Lim},
keywords = {Reinforcement learning, Multi-objective, Deep learning, Single-policy, Multi-policy},
abstract = {This paper introduces a new scalable multi-objective deep reinforcement learning (MODRL) framework based on deep Q-networks. We develop a high-performance MODRL framework that supports both single-policy and multi-policy strategies, as well as both linear and non-linear approaches to action selection. The experimental results on two benchmark problems (two-objective deep sea treasure environment and three-objective Mountain Car problem) indicate that the proposed framework is able to find the Pareto-optimal solutions effectively. The proposed framework is generic and highly modularized, which allows the integration of different deep reinforcement learning algorithms in different complex problem domains. This therefore overcomes many disadvantages involved with standard multi-objective reinforcement learning methods in the current literature. The proposed framework acts as a testbed platform that accelerates the development of MODRL for solving increasingly complicated multi-objective problems.}
}
@article{LIU2022529,
title = {Bid optimization using maximum entropy reinforcement learning},
journal = {Neurocomputing},
volume = {501},
pages = {529-543},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.05.108},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222006907},
author = {Mengjuan Liu and Jinyu Liu and Zhengning Hu and Yuchen Ge and Xuyun Nie},
keywords = {Real-time bidding, Bidding strategy, Maximum entropy reinforcement learning},
abstract = {Real-time bidding (RTB) has become a critical way for online advertising. It allows advertisers to display their ads by bidding on ad impressions. Therefore, advertisers in RTB always seek an optimal bidding strategy to improve their cost-efficiency. Unfortunately, it is challenging to optimize the bidding strategy at the granularity of impression due to the highly dynamic nature of the RTB environment. In this paper, we focus on optimizing the single advertiser’s bidding strategy using a stochastic reinforcement learning (RL) algorithm. Firstly, we utilize a widely adopted linear bidding function to compute every impression’s base price and optimize it with a mutable adjustment factor, thus making the bidding price conform to not only the impression’s value to the advertiser but also the RTB environment. Secondly, we use the maximum entropy RL algorithm (Soft Actor-Critic) to optimize every impression’s adjustment factor to overcome the deterministic RL algorithm’s convergence problem. Finally, we evaluate the proposed strategy on a benchmark dataset (iPinYou), and the results demonstrate it obtained the most click numbers in 9 of 12 experiments compared to baselines.}
}
@article{XIE2023121162,
title = {Multi-Agent attention-based deep reinforcement learning for demand response in grid-responsive buildings},
journal = {Applied Energy},
volume = {342},
pages = {121162},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121162},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923005263},
author = {Jiahan Xie and Akshay Ajagekar and Fengqi You},
keywords = {Demand response, Deep reinforcement learning, Multi-agent, Buildings},
abstract = {Integrating renewable energy resources and deploying energy management devices offer great opportunities to develop autonomous energy management systems in grid-responsive buildings. Demand response can promote enhancing demand flexibility and energy efficiency while reducing consumer costs. In this work, we propose a novel multi-agent deep reinforcement learning (MADRL) based approach with an agent assigned to individual buildings to facilitate demand response programs with diverse loads, including space heating/cooling and electrical equipment. Achieving real-time autonomous demand response in networks of buildings is challenging due to uncertain system parameters, the dynamic market price, and complex coupled operational constraints. To develop a scalable approach for automated demand response in networks of interconnected buildings, coordination between buildings is necessary to ensure demand flexibility and the grid's stability. We propose a MADRL technique that utilizes an actor-critic algorithm incorporating shared attention mechanism to enable effective and scalable real-time coordinated demand response in grid-responsive buildings. The presented case studies demonstrate the ability of the proposed approach to obtain decentralized cooperative policies for electricity costs minimization and efficient load shaping without knowledge of building energy systems. The viability of the proposed control approach is also demonstrated by a reduction of over 6% net load demand compared to standard reinforcement learning approaches, deep deterministic policy gradient, and soft actor-critic algorithm, as well as a tailored MADRL approach for demand response.}
}
@article{LI2024473,
title = {A flow based formulation and a reinforcement learning based strategic oscillation for cross-dock door assignment},
journal = {European Journal of Operational Research},
volume = {312},
number = {2},
pages = {473-492},
year = {2024},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2023.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0377221723005520},
author = {Mingjie Li and Jin-Kao Hao and Qinghua Wu},
keywords = {Heuristics, Formulation, Cross-docking assignment, Reinforcement learning, Strategic oscillation},
abstract = {Cross-dock door assignment is a critical warehouse optimization problem in supply chain management. It involves assigning incoming trucks to inbound doors and outgoing trucks to outbound doors to minimize the total pallet-handling cost inside the warehouse. This study investigates a flow based formulation and a reinforcement learning based heuristic approach to solve this problem. The flow based formulation relies on the flow of goods. It is significantly smaller than the existing mixed integer programming formulations in the literature. The proposed heuristic algorithm relies on a Q-learning reinforced procedure to guide the search toward promising areas, and a strategic oscillation method to adaptively explore feasible and infeasible search spaces. It also relies on an improved tabu strategy using attributive and explicit memories. The formulation and proposed heuristic algorithm were tested on two sets of benchmark instances widely used in the literature and compared with several state-of-the-art algorithms. The computational results demonstrated the high competitiveness of the proposed methods in solution quality and computation time. In particular, the flow based formulation can optimally solve more and larger instances and produce better lower and upper bounds than the existing mixed integer programming formulations in the literature. The heuristic approach improved the best solutions (new upper bounds) for 43 of the 99 tested instances while matching the other best-known solutions, except in two cases. The key components of the algorithm were analyzed to justify the algorithm design. The code of the proposed algorithm will be publicly available.}
}
@article{RUI2021103058,
title = {Service migration in multi-access edge computing: A joint state adaptation and reinforcement learning mechanism},
journal = {Journal of Network and Computer Applications},
volume = {183-184},
pages = {103058},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103058},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521000825},
author = {LanLan Rui and Menglei Zhang and Zhipeng Gao and Xuesong Qiu and Zhili Wang and Ao Xiong},
keywords = {Service migration, Multi-access edge computing, User movement, State adaptation},
abstract = {With the development of the internet of things (IoT), the concept of an edge network has been gradually expanding to other fields including internet of vehicles, mobile communication networks and smart grids. Because the resources of terminals are limited, the long-distance movements of users will increase the running costs of the services that are offloaded to edge servers, and even the services on terminals will stop running. Another problem is that resource shortages or hardware failures of these edge networks can affect the service migration policy. In this paper, a novel service migration method based on state adaptation and deep reinforcement learning is proposed to efficiently overcome network failures. Before migration, we define four edge network states to discuss the migration policy and adopt the two-dimensional movement around the edge servers to adapt to the applications scenarios of our work. Then, we use the satisfiability modulo theory (SMT) method to solve the candidate space of migration policies based on cost constraints, delay constraints and available resource capacity constraints to shorten the interruption time. Finally, the service migration problem can be transformed into the optimal destination server and low-cost migration path problem based on the Markov decision process by the deep Q-learning (DQN) algorithm. Moreover, we theoretically prove the rate of convergence in the learning rate function of our algorithm to improve the convergence rate. Our experimental results demonstrate that our proposed service migration mechanism can effectively shorten the delays from service interruptions, and better avoid the impact of edge network failure on the migration results and, thus, improve the users’ satisfaction.}
}
@article{ZHANG2023120561,
title = {Deep reinforcement learning towards real-world dynamic thermal management of data centers},
journal = {Applied Energy},
volume = {333},
pages = {120561},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120561},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922018189},
author = {Qingang Zhang and Wei Zeng and Qinjie Lin and Chin-Boon Chng and Chee-Kong Chui and Poh-Seng Lee},
keywords = {Data Center, Dynamic Thermal Management, Deep Reinforcement Learning, Machine Learning},
abstract = {Deep Reinforcement Learning has been increasingly researched for Dynamic Thermal Management in Data Centers. However, existing works typically evaluate the performance of algorithms on a specific task, utilizing models or data trajectories without discussing in detail their implementation feasibility and their ability to deal with diverse work scenarios. The lack of these works limits the real-world deployment of Deep Reinforcement Learning. To this end, this paper comprehensively evaluates the strengths and limitations of state-of-the-art algorithms by conducting analytical and numerical studies. The analysis is conducted in four dimensions: algorithms, tasks, system dynamics, and knowledge transfer. As an inherent property, the sensitivity to algorithms settings is first evaluated in a simulated data center model. The ability to deal with various tasks and the sensitivity to reward functions are subsequently studied. The trade-off between constraints and power savings is identified by conducting ablation experiments. Next, the performance under different work scenarios is investigated, including various equipment, workload schedules, locations, and power densities. Finally, the transferability of algorithms across tasks and scenarios is also evaluated. The results show that actor-critic, off-policy, and model-based algorithms outperform others in optimality, robustness, and transferability. They can reduce violations and achieve around 8.84% power savings in some scenarios compared to the default controller. However, deploying these algorithms in real-world systems is challenging since they are sensitive to specific hyperparameters, reward functions, and work scenarios. Constraint violations and sample efficiency are some aspects that are still unsatisfactory. This paper presents our well-structured investigations, new findings, and challenges when deploying deep reinforcement learning in Data Centers.}
}
@article{HUO202224026,
title = {Lifespan-consciousness and minimum-consumption coupled energy management strategy for fuel cell hybrid vehicles via deep reinforcement learning},
journal = {International Journal of Hydrogen Energy},
volume = {47},
number = {57},
pages = {24026-24041},
year = {2022},
note = {Hydrogen Society},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2022.05.194},
url = {https://www.sciencedirect.com/science/article/pii/S036031992202328X},
author = {Weiwei Huo and Dong Chen and Sheng Tian and Jianwei Li and Tianyu Zhao and Bo Liu},
keywords = {Energy management strategy, Fuel cell hybrid vehicle, Minimum fuel consumption, Fuel cell stack lifetime, DQL-PER algorithm, DDPG-PER algorithm},
abstract = {Energy management strategy (EMS) based on optimized deep reinforcement learning plays a critical role in minimizing fuel consumption and prolonging the fuel cell stack lifespan for fuel cell hybrid vehicles. The deep Q-learning (DQL) and deep deterministic policy gradient (DDPG) algorithms with priority experience replay are proposed in this research. The factors of fuel economy and power fluctuation are incorporated into the multi-objective reward functions to decline the fuel consumption and extend the lifetime of fuel cell stack. In addition, the degradation rate is introduced to reflect the lifetime of fuel cell stack. Furthermore, compared to the referenced optimally energy management strategy (dynamic planning), the DQL-based and DDPG-based EMS with prioritized experience replay (DQL-PER, DDPG-PER) are evaluated in hydrogen consumption and cumulative degradation of fuel cell stack under four driving cycles, FTP75, US06-2, NEDC and LA92-2, respectively. The training results reveal that the DQL-PER-based EMS performances better under FTP75 and US06-2 driving cycles, whereas DDPG-PER-based EMS has better performance under NEDC driving cycle, which provide a potential for applying the proposed algorithm into multi-cycles.}
}
@article{ZHU2022792,
title = {Optimal scheduling of a wind energy dominated distribution network via a deep reinforcement learning approach},
journal = {Renewable Energy},
volume = {201},
pages = {792-801},
year = {2022},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2022.10.094},
url = {https://www.sciencedirect.com/science/article/pii/S0960148122015968},
author = {Jiaoyiling Zhu and Weihao Hu and Xiao Xu and Haoming Liu and Li Pan and Haoyang Fan and Zhenyuan Zhang and Zhe Chen},
keywords = {Deep reinforcement learning, Distribution network, Wind energy, Optimal scheduling},
abstract = {With the development of clean energy systems, large-scale renewable energy is being connected to the traditional distribution network, which also brings new challenges to the reliable and economic scheduling of the power grid. To address these challenges, this paper proposes an intelligent scheduling strategy for a wind energy dominated distribution network, which aims to reduce the fluctuation caused by the wind energy. First, the energy scheduling model and objective function of the distribution network system are established and the constraints of various types of components are considered. Then, deep reinforcement learning is introduced to realize real-time decision in distribution network to solve the problem of fluctuation caused by the uncertain wind power output. The energy scheduling method is developed into a Markov decision process based on deep deterministic policy gradient (DDPG) algorithm. Finally, the simulation is verified on the IEEE14 node system. The results verify that the proposed approach can effectively reduce power fluctuations in the distribution network. The superiority of the adopted DDPG algorithm is demonstrated by comparing with the deep Q network algorithm.}
}
@article{JAYANETTI202214,
title = {Deep reinforcement learning for energy and time optimized scheduling of precedence-constrained tasks in edge–cloud computing environments},
journal = {Future Generation Computer Systems},
volume = {137},
pages = {14-30},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22002230},
author = {Amanda Jayanetti and Saman Halgamuge and Rajkumar Buyya},
keywords = {Workflow scheduling, Edge-computing, Deep reinforcement learning, Proximal policy optimization, Energy efficiency, Internet of things},
abstract = {The wide-spread embracement and integration of Internet of Things (IoT) has inevitably lead to an explosion in the number of IoT devices. This in turn has led to the generation of massive volumes of data that needs to be transmitted, processed and stored for efficient interpretation and utilization. Edge computing has emerged as a viable solution which complements cloud thereby enabling the integrated edge–cloud paradigm to successfully satisfy the design requirements of IoT applications. A vast majority of existing studies have proposed scheduling frameworks for individual tasks and only very few works have considered the more challenging problem of scheduling complex workloads such as workflows across edge–cloud environments. Workflow scheduling is an NP hard problem in distributed infrastructures. It is further complicated when scheduling framework needs to coordinate workflow executions across resource constrained and highly distributed edge–cloud environments. In this work, we leverage Deep Reinforcement Learning for designing a workflow scheduling framework capable of overcoming the aforementioned challenges. Different from all existing works we have designed a novel hierarchical action space for promoting a clear distinction between edge and cloud nodes. Coupled with this a hybrid actor–critic based scheduling framework enhanced with proximal policy optimization technique is proposed to efficiently deal with the complex workflow scheduling problem in edge–cloud environments. Performance of the proposed framework was compared against several baseline algorithms using energy consumption, execution time, percentage of deadline hits and percentage of jobs completed as evaluation metrics. Proposed Deep Reinforcement Learning technique performed 56% better with respect to energy consumption and 46% with respect to execution time compared to time and energy optimized baselines, respectively. This was achieved while also maintaining the energy efficiency in par with the energy optimized baseline and execution time in par with the time optimized baseline. The results thus demonstrate the superiority of the proposed technique in establishing the best-trade off between the conflicting goals of minimizing energy consumption and execution time.}
}
@article{GAWEDA2005826,
title = {Individualization of pharmacological anemia management using reinforcement learning},
journal = {Neural Networks},
volume = {18},
number = {5},
pages = {826-834},
year = {2005},
note = {IJCNN 2005},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2005.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005001450},
author = {Adam E. Gaweda and Mehmet K. Muezzinoglu and George R. Aronoff and Alfred A. Jacobs and Jacek M. Zurada and Michael E. Brier},
keywords = {Reinforcement learning, Drug dosing, Anemia management},
abstract = {Effective management of anemia due to renal failure poses many challenges to physicians. Individual response to treatment varies across patient populations and, due to the prolonged character of the therapy, changes over time. In this work, a Reinforcement Learning-based approach is proposed as an alternative method for individualization of drug administration in the treatment of renal anemia. Q-learning, an off-policy approximate dynamic programming method, is applied to determine the proper dosing strategy in real time. Simulations compare the proposed methodology with the currently used dosing protocol. Presented results illustrate the ability of the proposed method to achieve the therapeutic goal for individuals with different response characteristics and its potential to become an alternative to currently used techniques.}
}
@article{YU2014102,
title = {Filtering trust opinions through reinforcement learning},
journal = {Decision Support Systems},
volume = {66},
pages = {102-113},
year = {2014},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2014.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167923614001808},
author = {Han Yu and Zhiqi Shen and Chunyan Miao and Bo An and Cyril Leung},
keywords = {Trust, Reputation, Credibility, Collusion},
abstract = {In open online communities such as e-commerce, participants need to rely on services provided by others in order to thrive. Accurately estimating the trustworthiness of a potential interaction partner is vital to a participant's well-being. It is generally recognized in the research community that third-party testimony sharing is an effective way for participants to gain knowledge about the trustworthiness of potential interaction partners without having to incur the risk of actually interacting with them. However, the presence of biased testimonies adversely affects a participant's long term well-being. Existing trust computational models often require complicated manual tuning of key parameters to combat biased testimonies. Such an approach heavily involves subjective judgments and adapts poorly to changes in an environment. In this study, we propose the Actor–Critic Trust (ACT) model, which is an adaptive trust evidence aggregation model based on the principles of reinforcement learning. The proposed method dynamically adjusts the selection of credible witnesses as well as the key parameters associated with the direct and indirect trust evidence sources based on the observed benefits received by the trusting entity. Extensive simulations have shown that the ACT approach significantly outperforms existing approaches in terms of mitigating the adverse effect of biased testimonies. Such a performance is due to the proposed accountability mechanism that enables ACT to attribute the outcome of an interaction to individual witnesses and sources of trust evidence, and adjust future evidence aggregation decisions without the need for human intervention. The advantage of the proposed model is particularly significant when service providers and witnesses strategically collude to improve their chances of being selected for interaction by service consumers.}
}
@article{ILG1995321,
title = {A learning architecture based on reinforcement learning for adaptive control of the walking machine LAURON},
journal = {Robotics and Autonomous Systems},
volume = {15},
number = {4},
pages = {321-334},
year = {1995},
note = {Reinforcement Learning and Robotics},
issn = {0921-8890},
doi = {https://doi.org/10.1016/0921-8890(95)00009-5},
url = {https://www.sciencedirect.com/science/article/pii/0921889095000095},
author = {Winfried Ilg and Karsten Berns},
keywords = {Reinforcement learning, Learning architecture, Walking machines, Neural networks},
abstract = {The learning of complex control behaviour of autonomous mobile robots is one of the actual research topics. In this article an intelligent control architecture is presented which integrates learning methods and available domain knowledge. This control architecture is based on Reinforcement Learning and allows continuous input and output parameters, hierarchical learning, multiple goals, self-organized topology of the used networks and online learning. As a testbed this architecture is applied to the six-legged walking machine LAURON to learn leg control and leg coordination.}
}
@article{REYNOLDS2009281,
title = {Developing PFC representations using reinforcement learning},
journal = {Cognition},
volume = {113},
number = {3},
pages = {281-292},
year = {2009},
note = {Reinforcement learning and higher cognition},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2009.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0010027709001164},
author = {Jeremy R. Reynolds and Randall C. O’Reilly},
keywords = {PFC, Representation, Reinforcement learning, Functional organization},
abstract = {From both functional and biological considerations, it is widely believed that action production, planning, and goal-oriented behaviors supported by the frontal cortex are organized hierarchically [Fuster (1991); Koechlin, E., Ody, C., & Kouneiher, F. (2003). Neuroscience: The architecture of cognitive control in the human prefrontal cortex. Science, 424, 1181–1184; Miller, G. A., Galanter, E., & Pribram, K. H. (1960). Plans and the structure of behavior. New York: Holt]. However, the nature of the different levels of the hierarchy remains unclear, and little attention has been paid to the origins of such a hierarchy. We address these issues through biologically-inspired computational models that develop representations through reinforcement learning. We explore several different factors in these models that might plausibly give rise to a hierarchical organization of representations within the PFC, including an initial connectivity hierarchy within PFC, a hierarchical set of connections between PFC and subcortical structures controlling it, and differential synaptic plasticity schedules. Simulation results indicate that architectural constraints contribute to the segregation of different types of representations, and that this segregation facilitates learning. These findings are consistent with the idea that there is a functional hierarchy in PFC, as captured in our earlier computational models of PFC function and a growing body of empirical data.}
}
@article{TANG202243,
title = {Periodic event-triggered adaptive tracking control design for nonlinear discrete-time systems via reinforcement learning},
journal = {Neural Networks},
volume = {154},
pages = {43-55},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.06.039},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022002544},
author = {Fanghua Tang and Ben Niu and Guangdeng Zong and Xudong Zhao and Ning Xu},
keywords = {Periodic event-triggered mechanism, Reinforcement learning (RL), Neural networks (NNs), Discrete-time systems},
abstract = {In this paper, an event-triggered control scheme with periodic characteristic is developed for nonlinear discrete-time systems under an actor–critic architecture of reinforcement learning (RL). The periodic event-triggered mechanism (ETM) is constructed to decide whether the sampling data are delivered to controllers or not. Meanwhile, the controller is updated only when the event-triggered condition deviates from a prescribed threshold. Compared with traditional continuous ETMs, the proposed periodic ETM can guarantee a minimal lower bound of the inter-event intervals and avoid sampling calculation point-to-point, which means that the partial communication resources can be efficiently economized. The critic and actor neural networks (NNs), consisting of radial basis function neural networks (RBFNNs), aim to approximate the unknown long-term performance index function and the ideal event-triggered controller, respectively. A rigorous stability analysis based on the Lyapunov difference method is provided to substantiate that the closed-loop system can be stabilized. All error signals of the closed-loop system are uniformly ultimately bounded (UUB) under the guidance of the proposed control scheme. Finally, two simulation examples are given to validate the effectiveness of the control design.}
}
@article{RAHEB2022105860,
title = {Subcutaneous insulin administration by deep reinforcement learning for blood glucose level control of type-2 diabetic patients},
journal = {Computers in Biology and Medicine},
volume = {148},
pages = {105860},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105860},
url = {https://www.sciencedirect.com/science/article/pii/S001048252200614X},
author = {Mohammad Ali Raheb and Vahid Reza Niazmand and Navid Eqra and Ramin Vatankhah},
keywords = {Type-2 diabetes, Insulin administration, Subcutaneous injection, Normalized advantage function},
abstract = {Background
Type-2 diabetes mellitus is characterized by insulin resistance and impaired insulin secretion in the human body. Many endeavors have been made in terms of controlling and reducing blood glucose via the medium of automated controlling tools to increase precision and efficiency and reduce human error. Recently, reinforcement learning algorithms are proved to be powerful in the field of intelligent control, which was the motivation for the current study.
Methods
For the first time, a reinforcement algorithm called normalized advantage function (NAF) algorithm has been applied as a model-free reinforcement learning method to regulate the blood glucose level of type-2 diabetic patients through subcutaneous injection. The algorithm has been designed and developed in a model-free approach to avoid additional inaccuracies and parameter uncertainty introduced by the mathematical models of the glucoregulatory system. Insulin doses constitute the control action that is designed to be stated directly in clinical language with the unit IU. In this regard, a new environment state is considered in addition to the glucose level to take into account the delayed effect of insulin elimination under the skin. Finally, a simple but practical reward function is developed to be used with the NAF algorithm to correct the glucose level and maintain it in the desired range.
Results
The simulation environment was set up to imitate the basal-bolus process accurately. Results for 30 days of simulation of the designed controller on three different average virtual patients verify the feasibility and effectiveness of the method and reveal our proposed controller's learning ability. Moreover, as the insulin elimination dynamic was taken into account, a more complete and more realistic model than the previously studied models has emerged.
Conclusion
NAF has proved a promising control approach, able to successfully regulate and significantly reduce the fluctuation of the blood glucose without meal announcements, compared to standard optimized open-loop basal-bolus therapies. The method and its results, which are directly in the clinical language, are applicable in real-time clinical situations.}
}
@article{VANECK20081999,
title = {Application of reinforcement learning to the game of Othello},
journal = {Computers & Operations Research},
volume = {35},
number = {6},
pages = {1999-2017},
year = {2008},
note = {Part Special Issue: OR Applications in the Military and in Counter-Terrorism},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2006.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0305054806002553},
author = {Nees Jan {van Eck} and Michiel {van Wezel}},
keywords = {Dynamic programming, Markov decision processes, Reinforcement learning, -learning, Multiagent learning, Neural networks, Game playing, Othello},
abstract = {Operations research and management science are often confronted with sequential decision making problems with large state spaces. Standard methods that are used for solving such complex problems are associated with some difficulties. As we discuss in this article, these methods are plagued by the so-called curse of dimensionality and the curse of modelling. In this article, we discuss reinforcement learning, a machine learning technique for solving sequential decision making problems with large state spaces. We describe how reinforcement learning can be combined with a function approximation method to avoid both the curse of dimensionality and the curse of modelling. To illustrate the usefulness of this approach, we apply it to a problem with a huge state space—learning to play the game of Othello. We describe experiments in which reinforcement learning agents learn to play the game of Othello without the use of any knowledge provided by human experts. It turns out that the reinforcement learning agents learn to play the game of Othello better than players that use basic strategies.}
}
@article{WU2023100951,
title = {Towards efficient long-horizon decision-making using automated structure search method of hierarchical reinforcement learning for edge artificial intelligence},
journal = {Internet of Things},
volume = {24},
pages = {100951},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100951},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523002743},
author = {Guanlin Wu and Weidong Bao and Jiang Cao and Xiaomin Zhu and Ji Wang and Wenhua Xiao and Wenqian Liang},
keywords = {IoT decision-making tasks, Hierarchical reinforcement learning, Embedded exploration and exploitation process, Synchronous training architecture, Adaptive evolutionary method},
abstract = {Hierarchical reinforcement learning (HRL) is a promising approach for efficiently solving various long-horizon decision-making tasks in the Internet of Things (IoT) domain. However, HRL algorithms are known to rely on expert knowledge to preset an appropriate hierarchical structure for different IoT tasks, which leads to higher trial costs and limits its wider application. In this paper, we propose a new method called DHRL (Dynamic-Level Hierarchical Reinforcement Learning) and it is able to adaptively search for the optimal hierarchical structure while maintaining the generality of framework design. DHRL incorporates an embedded exploration and exploitation mechanism that effectively solves the challenges caused by dependence between different levels and achieves a balance between maximizing benefits and current evaluation accuracy. Nonetheless, the more exploration processes inevitably has a negative impact on the performance. To mitigate this influences, we propose a synchronous training architecture to support DHRL operating in a distributed and parallel manner, in which the adaptive evolutionary method is also introduced to accelerate the convergence. Extensive experimental evaluations are conducted to demonstrate the effectiveness of our theory and method.}
}
@article{LI2023127627,
title = {Modeling and energy dynamic control for a ZEH via hybrid model-based deep reinforcement learning},
journal = {Energy},
volume = {277},
pages = {127627},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.127627},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223010216},
author = {Yanxue Li and Zixuan Wang and Wenya Xu and Weijun Gao and Yang Xu and Fu Xiao},
keywords = {ZEH, Thermal comfort, Deep reinforcement learning, Energy management strategy},
abstract = {Efficient and flexible energy management strategy can play an important role in energy conservation in building sector. The model-free reinforcement learning control of building energy systems generally requires an enormous amount of training data and low learning efficiency creates an obstacle to practice. This work proposes a hybrid model-based reinforcement learning framework to optimize the indoor thermal comfort and energy cost-saving performances of a ZEH (zero energy house) space heating system using relatively short-period monitored data. The reward function is designed regarding energy cost, PV self-consumption and thermal discomfort, proposed agents can interact with the reduced-order thermodynamic model and an uncertain environment, and makes optimal control policies through the learning process. Simulation results demonstrate that proposed agents achieve efficient convergence, D3QN presents a superiority of convergence performance. To evaluate the performances of proposed algorithms, the trained agents are tested using monitored data. With learned policies, the self-learning agents could balance the needs of thermal comfort, energy cost saving and increasing on-site PV consumption compared with the baselines. The comparative analysis shows that D3QN achieved over 30% cost savings compared with measurement results. D3QN outperforms DQN and Double DQN agents in test scenarios maintaining more stable temperatures under various outside conditions.}
}
@article{ZHANG2023311,
title = {Reinforcement learning based energy-neutral operation for hybrid EH powered TBAN},
journal = {Future Generation Computer Systems},
volume = {140},
pages = {311-320},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003600},
author = {Lei Zhang and Panyue Lin},
keywords = {TBAN, ENO, Energy harvesting, Optimization, Reinforcement learning},
abstract = {The aging population, outbreak of new infectious diseases and shortage of medical resources promote rapid development of telemedicine. Wireless textile body area network (TBAN), which combines functional textile and wireless body area network (WBAN), is gaining great attention as an efficient medium of remote medical care. This is because of its unique materials and application scenario, as well as its convenience and friendliness to the elderly. Moreover, it is an effective application for integrating edge computing with next generation of wearable technology. Nonetheless, it is unavoidable that TBAN has to deal with reliability and energy issues. Given these deficiencies and challenges, this paper focuses on the feasibility of achieving wearable energy neutral operation (ENO) in TBAN while maintaining robustness. In addition to adding user posture factors regarding network specifics, we combine hybrid energy harvesting (EH) techniques and duty cycle schemes. A hybrid radio frequency (RF) energy and Triboelectric nanogenerator (TENG) EH-assisted TBAN system is built in this work. We analyze and discuss the delay, data rate and packet error rate (PER) under five typical daily activities (standing, sitting, lying, walking, and running). To optimize the ENO problem, two reinforcement learning (Q-learning and Deep Q-Network (DQN)) based algorithms are proposed. According to numerical results, both algorithms ultimately lead to stable power levels compared to the continuous decline of battery power without optimization. DQN-based optimization performs better than Q-Learning. For instance, 14% and 56% improvements in PER and battery power, respectively.}
}
@article{WANG2022103354,
title = {Context-aware distribution of fog applications using deep reinforcement learning},
journal = {Journal of Network and Computer Applications},
volume = {203},
pages = {103354},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103354},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522000236},
author = {Nan Wang and Blesson Varghese},
keywords = {Fog computing, Decentralised cloud, Edge computing, Context-aware distribution},
abstract = {Fog computing is an emerging paradigm that aims to meet the increasing computation demands arising from the billions of devices connected to the Internet. Offloading services of an application from the Cloud to the edge of the network can improve the overall latency of the application since it can process data closer to user devices. Diverse Fog nodes ranging from Wi-Fi routers to mini-clouds with varying resource capabilities makes it challenging to determine which services of an application need to be offloaded. In this paper, a context-aware mechanism for distributing applications across the Cloud and the Fog is proposed. The mechanism dynamically generates (re)deployment plans for the application to maximise the performance efficiency of the application by taking operational conditions, such as hardware utilisation and network state, and running costs into account. The mechanism relies on deep Q-networks to generate a distribution plan without prior knowledge of the available resources on the Fog node, the network condition, and the application. The feasibility of the proposed context-aware distribution mechanism is demonstrated on two use-cases, namely a face detection application and a location-based mobile game. The benefits are increased utility of dynamic distribution by 50% and 20% for the two use-cases respectively when compared to a static distribution approach used in existing research.}
}
@article{ZHANG2020113063,
title = {Dynamic energy conversion and management strategy for an integrated electricity and natural gas system with renewable energy: Deep reinforcement learning approach},
journal = {Energy Conversion and Management},
volume = {220},
pages = {113063},
year = {2020},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2020.113063},
url = {https://www.sciencedirect.com/science/article/pii/S0196890420306075},
author = {Bin Zhang and Weihao Hu and Jinghua Li and Di Cao and Rui Huang and Qi Huang and Zhe Chen and Frede Blaabjerg},
keywords = {Renewable energy accommodation, Dynamic energy conversion and management, Deep reinforcement learning},
abstract = {With the application of advanced information technology for the integration of electricity and natural gas systems, formulating an excellent energy conversion and management strategy has become an effective method to achieve established goals. Differing from previous works, this paper proposes a peak load shifting model to smooth the net load curve of an integrated electricity and natural gas system by coordinating the operations of the power-to-gas unit and generators. Moreover, the study aims to achieve multi-objective optimization while considering the economy of the system. A dynamic energy conversion and management strategy is proposed, which coordinates both the economic cost target and the peak load shifting target by adjusting an economic coefficient. To illustrate the complex energy conversion process, deep reinforcement learning is used to formulate the dynamic energy conversion and management problem as a discrete Markov decision process, and a deep deterministic policy gradient is adopted to solve the decision-making problem. By using the deep reinforcement learning method, the system operator can adaptively determine the conversion ratio of wind power, power-to-gas and gas turbine operations, and generator output through an online process, where the flexibility of wind power generation, wholesale gas price, and the uncertainties of energy demand are considered. Simulation results show that the proposed algorithm can increase the profit of the system operator, reduce wind power curtailment, and smooth the net load curves effectively in real time.}
}
@article{VENTURI2021106631,
title = {DReLAB - Deep REinforcement Learning Adversarial Botnet: A benchmark dataset for adversarial attacks against botnet Intrusion Detection Systems},
journal = {Data in Brief},
volume = {34},
pages = {106631},
year = {2021},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2020.106631},
url = {https://www.sciencedirect.com/science/article/pii/S2352340920315110},
author = {Andrea Venturi and Giovanni Apruzzese and Mauro Andreolini and Michele Colajanni and Mirco Marchetti},
keywords = {Cyber security, Adversarial attacks, Deep reinforcement learning, Intrusion detection system, Botnet},
abstract = {We present the first dataset that aims to serve as a benchmark to validate the resilience of botnet detectors against adversarial attacks. This dataset includes realistic adversarial samples that are generated by leveraging two widely used Deep Reinforcement Learning (DRL) techniques. These adversarial samples are proved to evade state of the art detectors based on Machine- and Deep-Learning algorithms. The initial corpus of malicious samples consists of network flows belonging to different botnet families presented in three public datasets containing real enterprise network traffic. We use these datasets to devise detectors capable of achieving state-of-the-art performance. We then train two DRL agents, based on Double Deep Q-Network and Deep Sarsa, to generate realistic adversarial samples: the goal is achieving misclassifications by performing small modifications to the initial malicious samples. These alterations involve the features that can be more realistically altered by an expert attacker, and do not compromise the underlying malicious logic of the original samples. Our dataset represents an important contribution to the cybersecurity research community as it is the first including thousands of automatically generated adversarial samples that are able to thwart state of the art classifiers with a high evasion rate. The adversarial samples are grouped by malware variant and provided in a CSV file format. Researchers can validate their defensive proposals by testing their detectors against the adversarial samples of the proposed dataset. Moreover, the analysis of these samples can pave the way to a deeper comprehension of adversarial attacks and to some sort of explainability of machine learning defensive algorithms. They can also support the definition of novel effective defensive techniques.}
}
@article{YANG2023104237,
title = {Single-track railway scheduling with a novel gridworld model and scalable deep reinforcement learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {154},
pages = {104237},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2023.104237},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X23002267},
author = {Feiyu Yang and Yuhua Yang and Shaoquan Ni and Su Liu and Changan Xu and Dingjun Chen and Qingpeng Zhang},
keywords = {Convolutional neural networks, Deep reinforcement learning, Gridworld, Railway scheduling},
abstract = {This study proposes a deep reinforcement learning approach for bi-direction single-track railway scheduling called DRLA-eTGM. The goal is to define the departure times and track allocation without conflict for all trains on the line given their speed, priority, origin, and destination while minimizing the total priority-weighted dwelling time. A novel time–space-capacity gridworld model (TGM), which contains a two-dimensional state representation to dynamically describe the distribution of constraints and trains in single-track railway scheduling, is proposed. Moreover, DRLA-eTGM innovatively extracts and exploits features from the TGM state automatically with deep convolutional neural networks to enhance scheduling decision strategies. Further, a shaping technique inspired by dynamic programming is adopted in DRLA-eTGM. Computational experiments are presented to verify the effectiveness of the proposed approach using hypothesized instances and a busy passenger-cargo single-track railway corridor in China. The experimental results show that DRLA-eTGM outperforms some heuristic and reinforcement learning algorithms, especially in real-world instances. Compared to some deep reinforcement learning algorithms, DRLA-eTGM shows robustness and efficiency owing to its partial scalable observation and parameter sharing mechanisms. This study provides a new approach to exploiting constraint information in railway scheduling and a prototype system for automatic single-track railway scheduling.}
}
@article{WANG2023106056,
title = {Balanced incremental deep reinforcement learning based on variational autoencoder data augmentation for customer credit scoring},
journal = {Engineering Applications of Artificial Intelligence},
volume = {122},
pages = {106056},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106056},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623002403},
author = {Yadong Wang and Yanlin Jia and Yu Zhong and Jing Huang and Jin Xiao},
keywords = {Incremental deep reinforcement learning, Data augmentation, Customer credit scoring, Variational autoencoder, Deep Q-network},
abstract = {Incremental deep reinforcement learning has been successfully applied to different fields in the real-world. However, training deep reinforcement learning models with the data set generated in a certain stage may lead to catastrophic forgetting of trained models, that is, when training models on the new data set, the performance of models will be seriously degraded on the old data set. Therefore, we construct the balanced incremental deep Q-network based on variational autoencoder data augmentation (BIDQN-VADA) model for customer credit scoring. First, the original training set is processed by the random undersampling method to obtain the balanced training set, in which the initial deep Q-network model is trained. Then, the balanced training subset with a fixed number of samples is selected from the original training set, and these samples are augmented by the variational autoencoder data augmentation technology to obtain the balanced augmented training subset. Finally, the balanced training subset and balanced augmented training subset are stored in data stream cache according to the first-in first-out principle for incrementally updating parameters of the deep Q-network model. In order to verify the performance of the model, we conduct a series of experiments on eight real-world customer credit scoring data sets. The experimental results show that the BIDQN-VADA model can achieve the best customer credit scoring performance by combining the balance, incremental and data augmentation process at the same time. More importantly, the performance of the BIDQN-VADA model is significantly better than the other seven classification models.}
}
@article{PEREZ2021891,
title = {Energy-conscious optimization of Edge Computing through Deep Reinforcement Learning and two-phase immersion cooling},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {891-907},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.07.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002934},
author = {Sergio Pérez and Patricia Arroba and José M. Moya},
keywords = {Energy-aware optimization, Deep Reinforcement Learning, Edge Computing, Two-phase immersion cooling, Advanced driver assistance systems},
abstract = {Until now, the reigning computing paradigm has been Cloud Computing, whose facilities concentrate in large and remote areas. Novel data-intensive services with critical latency and bandwidth constraints, such as autonomous driving and remote health, will suffer under an increasingly saturated network. On the contrary, Edge Computing brings computing facilities closer to end-users to offload workloads in Edge Data Centers (EDCs). Nevertheless, Edge Computing raises other concerns like EDC size, energy consumption, price, and user-centered design. This research addresses these challenges by optimizing Edge Computing scenarios in two ways, two-phase immersion cooling systems and smart resource allocation via Deep Reinforcement Learning. To this end, several Edge Computing scenarios have been modeled, simulated, and optimized with energy-aware strategies using real traces of user demand and hardware behavior. These scenarios include air-cooled and two-phase immersion-cooled EDCs devised using hardware prototypes and a resource allocation manager based on an Advantage Actor–Critic (A2C) agent. Our immersion-cooled EDC’s IT energy model achieved an NRMSD of 3.15% and an R2 of 97.97%. These EDCs yielded an average energy saving of 22.8% compared to air-cooled. Our DRL-based allocation manager further reduced energy consumption by up to 23.8% in comparison to the baseline.}
}
@article{VOGIATZIS2000169,
title = {Reinforcement learning for symbolic expression induction},
journal = {Mathematics and Computers in Simulation},
volume = {51},
number = {3},
pages = {169-179},
year = {2000},
issn = {0378-4754},
doi = {https://doi.org/10.1016/S0378-4754(99)00115-9},
url = {https://www.sciencedirect.com/science/article/pii/S0378475499001159},
author = {Dimitrios Vogiatzis and Andreas Stafylopatis},
keywords = {Neural networks, Reinforcement learning, Symbolic/subsymbolic processing, Rule extraction},
abstract = {We propose a neural network method for the generation of symbolic expressions using reinforcement learning. Usually, the symbolic form expressed in terms of a calculus (propositional, first-order, lambda, etc.) is deemed comprehensible by humans and it is necessary as far as the acceptance of neural networks is concerned. According to the proposed method, a human decides on the kind and number of primitive functions which, with the appropriate composition (in the mathematical sense), can represent a mapping between two domains. The appropriate composition is achieved by an agent which tries many compositions and receives a reward depending on the quality of the composed function. Naturally, the learning agent (which in our case is a recurrent neural net) must perform the credit assignment task. Results are encouraging concerning the derivation of simple arithmetic expressions.}
}
@article{WEI2021108439,
title = {Computation offloading over multi-UAV MEC network: A distributed deep reinforcement learning approach},
journal = {Computer Networks},
volume = {199},
pages = {108439},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108439},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621003984},
author = {Dawei Wei and Jianfeng Ma and Linbo Luo and Yunbo Wang and Lei He and Xinghua Li},
keywords = {Multi-UAV assisted MEC-enabled network, Computation offloading, Distributed reinforcement learning},
abstract = {Unmanned aerial vehicle (UAV)-assisted computation offloading allows mobile devices (MDs) to process computation-intensive and latency-sensitive tasks with limited or no-available infrastructures. To achieve long-term performance under changing environment, deep reinforcement-based methods have been applied to solve the UAV-assisted computation offloading problem. However, the deployment of multiple UAVs for computation offloading in mobile edge computing (MEC) network still faces the challenge of lacking flexible learning scheme to efficiently adjust computation offloading policy according to dynamic UAV mobility pattern and UAV failure. To this end, a distributed deep reinforcement learning (DRL)-based method with the cooperative exploring and prioritized experience replay (PER) is proposed in this paper. Our distributed exploring process achieves flexible learning scheme under UAV failure by allowing MDs to learning cost-efficient offloading policy cooperatively. Furthermore, PER allows MDs can explore the transitions with high TD-error, which can improve the performance under dynamic UAV mobility patterns. The efficiency of the proposed method is demonstrated by comparing with the existing computation offloading methods, and results show that the proposed method outperforms the compared methods in terms of convergence rate, energy-task efficiency and average processing time.}
}
@article{SHI2023104019,
title = {A deep reinforcement learning based distributed control strategy for connected automated vehicles in mixed traffic platoon},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {148},
pages = {104019},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2023.104019},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X23000086},
author = {Haotian Shi and Danjue Chen and Nan Zheng and Xin Wang and Yang Zhou and Bin Ran},
keywords = {Mixed traffic environment, Distributed control, Deep reinforcement learning, Traffic oscillation dampening, Connected automated vehicle},
abstract = {This paper proposes an innovative distributed longitudinal control strategy for connected automated vehicles (CAVs) in the mixed traffic environment of CAV and human-driven vehicles (HDVs), incorporating high-dimensional platoon information. For mixed traffic, the traditional CAV control method focuses on microscopic trajectory information, which may not be efficient in handling the HDV stochasticity (e.g., long reaction time; various driving styles) and mixed traffic heterogeneities. Different from traditional methods, our method, for the first time, characterizes consecutive HDVs as a whole (i.e., AHDV) to reduce the HDV stochasticity and utilize its macroscopic features to control the following CAVs. The new control strategy takes advantage of platoon information to anticipate the disturbances and traffic features induced downstream under mixed traffic scenarios and greatly outperforms the traditional methods. In particular, the control algorithm is based on deep reinforcement learning (DRL) to fulfill car-following control efficiency and further address the stochasticity for the aggregated car following behavior by embedding it in the training environment. To better utilize the macroscopic traffic features, a general platoon of mixed traffic is categorized as a CAV-HDVs-CAV pattern and described by corresponding DRL states. The macroscopic traffic flow properties are built upon the Newell car-following model to capture the characteristics of aggregated HDVs' joint behaviors. Simulated experiments are conducted to validate our proposed strategy. The results demonstrate that the proposed control method has outstanding performances in terms of oscillation dampening, eco-driving, and generalization capability.}
}
@article{QIU2023120526,
title = {Federated reinforcement learning for smart building joint peer-to-peer energy and carbon allowance trading},
journal = {Applied Energy},
volume = {333},
pages = {120526},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120526},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922017834},
author = {Dawei Qiu and Juxing Xue and Tingqi Zhang and Jianhong Wang and Mingyang Sun},
keywords = {Multi-energy system, Smart buildings, Peer-to-peer energy trading, Emission trading scheme, Multi-agent reinforcement learning, Federated learning},
abstract = {The multi-energy system (MES), which is regarded as an optimum solution to a high-efficiency, green energy system and a crucial shift towards the future low-carbon energy system, has attracted great attention at the district building level. However, the current exploration of flexible MES operation has been hampered by (1) the increasing penetration of renewable energies and the complicated operation of coupling multi-energy sectors; (2) the privacy concern in the decentralization of the energy system; and (3) the lack of integration of the energy market and carbon emission trading scheme. To address the aforementioned challenges, this paper proposes a joint peer-to-peer energy and carbon allowance trading mechanism for a building community, and then models it as a multi-agent reinforcement learning (MARL) paradigm. In this setting, the flexibility of building local trading and the decarbonization of building energy management can both be fully utilized. To stabilize the training performance, an abstract critic network capturing system dynamics is introduced based on a deep deterministic policy gradient method. The technique of federated learning (FL) is also applied to speed up the training and safeguard the private information of each building in the community. Empirical results on a real-world test case evaluate its superior performance in terms of achieving both economic and environmental benefits, resulting in 5.87% and 8.02% lower total energy and environment costs than the two baseline mechanisms of peer-to-grid energy trading and peer-to-peer energy trading, respectively.}
}
@article{VENGEROV2007383,
title = {A reinforcement learning approach to dynamic resource allocation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {20},
number = {3},
pages = {383-390},
year = {2007},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2006.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0952197606001205},
author = {David Vengerov},
keywords = {Reinforcement learning, Utility computing, Resource allocation},
abstract = {This paper presents a general framework for performing adaptive reconfiguration of a distributed system based on maximizing the long-term business value, defined as the discounted sum of all future rewards and penalties. The problem of dynamic resource allocation among multiple entities sharing a common set of resources is used as an example. A specific architecture (DRA-FRL) is presented, which uses the emerging methodology of reinforcement learning in conjunction with fuzzy rulebases to achieve the desired objective. This architecture can work in the context of existing resource allocation policies and learn the values of the states that the system encounters under these policies. Once the learning process begins to converge, the user can allow the DRA-FRL architecture to make some additional resource allocation decisions or override the ones suggested by the existing policies so as to improve the long-term business value of the system. The DRA-FRL architecture can also be deployed in an environment without any existing resource allocation policies. An implementation of the DRA-FRL architecture in Solaris 10 demonstrated a robust performance improvement in the problem of dynamically migrating CPUs and memory blocks between three resource partitions so as to match the stochastically changing workload in each partition, both in the presence and in the absence of resource migration costs.}
}
@article{ABOUSSALAH2020112891,
title = {Continuous control with Stacked Deep Dynamic Recurrent Reinforcement Learning for portfolio optimization},
journal = {Expert Systems with Applications},
volume = {140},
pages = {112891},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.112891},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419306074},
author = {Amine Mohamed Aboussalah and Chi-Guhn Lee},
keywords = {Reinforcement learning, Policy gradient, Deep learning, Sequential model-based optimization, Financial time series, Portfolio management, Trading systems},
abstract = {Recurrent reinforcement learning (RRL) techniques have been used to optimize asset trading systems and have achieved outstanding results. However, the majority of the previous work has been dedicated to systems with discrete action spaces. To address the challenge of continuous action and multi-dimensional state spaces, we propose the so called Stacked Deep Dynamic Recurrent Reinforcement Learning (SDDRRL) architecture to construct a real-time optimal portfolio. The algorithm captures the up-to-date market conditions and rebalances the portfolio accordingly. Under this general vision, Sharpe ratio, which is one of the most widely accepted measures of risk-adjusted returns, has been used as a performance metric. Additionally, the performance of most machine learning algorithms highly depends on their hyperparameter settings. Therefore, we equipped SDDRRL with the ability to find the best possible architecture topology using an automated Gaussian Process (GP) with Expected Improvement (EI) as an acquisition function. This allows us to select the best architectures that maximizes the total return while respecting the cardinality constraints. Finally, our system was trained and tested in an online manner for 20 successive rounds with data for ten selected stocks from different sectors of the S&P 500 from January 1st, 2013 to July 31st, 2017. The experiments reveal that the proposed SDDRRL achieves superior performance compared to three benchmarks: the rolling horizon Mean-Variance Optimization (MVO) model, the rolling horizon risk parity model, and the uniform buy-and-hold (UBAH) index.}
}
@article{ELKAEL2022109204,
title = {Monkey Business: Reinforcement learning meets neighborhood search for Virtual Network Embedding},
journal = {Computer Networks},
volume = {216},
pages = {109204},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109204},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622002924},
author = {Maxime Elkael and Massinissa {Ait Aba} and Andrea Araldo and Hind Castel-Taleb and Badii Jouaber},
keywords = {Virtual network embedding, Reinforcement learning, Nested Rollout Policy Adaptation},
abstract = {In this article, we consider the Virtual Network Embedding (VNE) problem for 5G networks slicing. This problem requires to allocate multiple Virtual Networks (VN) on a substrate virtualized physical network while maximizing among others, resource utilization, maximum number of placed VNs and network operator’s benefit. We solve the online version of the problem where slices arrive over time. Inspired by the Nested Rollout Policy Adaptation (NRPA) algorithm, a variant of the well known Monte Carlo Tree Search (MCTS) that learns how to perform good simulations over time, we propose a new algorithm that we call Neighborhood Enhanced Policy Adaptation (NEPA). The key feature of our algorithm is to observe NRPA cannot exploit knowledge acquired in one branch of the state tree for another one which starts differently. NEPA learns by combining NRPA with Neighborhood Search in a frugal manner which improves only promising solutions while keeping the running time low. We call this technique a monkey business because it comes down to jumping from one interesting branch to the other, similar to how monkeys jump from tree to tree instead of going down everytime. NEPA achieves better results in terms of acceptance ratio and revenue-to-cost ratio compared to other state-of-the-art algorithms, both on real and synthetic topologies.}
}
@article{ZHENG2022101475,
title = {A new PM2.5 forecasting model based on data preprocessing, reinforcement learning and gated recurrent unit network},
journal = {Atmospheric Pollution Research},
volume = {13},
number = {7},
pages = {101475},
year = {2022},
issn = {1309-1042},
doi = {https://doi.org/10.1016/j.apr.2022.101475},
url = {https://www.sciencedirect.com/science/article/pii/S1309104222001581},
author = {Guangji Zheng and Hui Liu and Chengqing Yu and Ye Li and Zijie Cao},
keywords = {PM2.5 forecasting, Secondary decomposition, Feature selection, Reinforcement learning, Gated recurrent unit, Hybrid model},
abstract = {Accurate PM2.5 forecasting is of great significance to atmosphere pollution monitoring and control. To accurately predict PM2.5 concentration, a novel hybrid model is proposed. Our novel model includes the following three modeling processes: In stage I, a novel secondary decomposition method is adopted to decompose the raw PM2.5 data into several subseries. In stage II, a feature selection method based on reinforcement learning selects optimal features of each subseries for the predictor. In stage III, the selected features are input into a gated recurrent unit network and output the final forecasting result of all subseries. The experimental results of the paper on different data sets have verified that: (1) The proposed feature selection method based on reinforcement learning can select the optimal features, and our method outperforms the traditional feature selection method in the forecasting accuracy; (2) The novel model has excellent prediction performance in all cases and can obtain the optimal forecasting accuracy compared with twenty benchmark models and three state-of-the-art models.}
}
@article{MOLLEL2020101133,
title = {Intelligent handover decision scheme using double deep reinforcement learning},
journal = {Physical Communication},
volume = {42},
pages = {101133},
year = {2020},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2020.101133},
url = {https://www.sciencedirect.com/science/article/pii/S187449072030210X},
author = {Michael S. Mollel and Attai Ibrahim Abubakar and Metin Ozturk and Shubi Kaijage and Michael Kisangiri and Ahmed Zoha and Muhammad Ali Imran and Qammer H. Abbasi},
keywords = {Double deep reinforcement learning, Handover management, Millimetre-wave communication},
abstract = {Handovers (HOs) have been envisioned to be more challenging in 5G networks due to the inclusion of millimetre wave (mm-wave) frequencies, resulting in more intense base station (BS) deployments. This, by its turn, increases the number of HOs taken due to smaller footprints of mm-wave BSs thereby making HO management a more crucial task as reduced quality of service (QoS) and quality of experience (QoE) along with higher signalling overhead are more likely with the growing number of HOs. In this paper, we propose an offline scheme based on double deep reinforcement learning (DDRL) to minimize the frequency of HOs in mm-wave networks, which subsequently mitigates the adverse QoS. Due to continuous and substantial state spaces arising from the inherent characteristics of the considered 5G environment, DDRL is preferred over conventional Q-learning algorithm. Furthermore, in order to alleviate the negative impacts of online learning policies in terms of computational costs, an offline learning framework is adopted in this study, a known trajectory is considered in a simulation environment while ray-tracing is used to estimate channel characteristics. The number of HO occurrence during the trajectory and the system throughput are taken as performance metrics. The results obtained reveal that the proposed method largely outperform conventional and other artificial intelligence (AI)-based models.}
}
@article{COLIN2016196,
title = {Hierarchical reinforcement learning as creative problem solving},
journal = {Robotics and Autonomous Systems},
volume = {86},
pages = {196-206},
year = {2016},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2016.08.021},
url = {https://www.sciencedirect.com/science/article/pii/S0921889016305371},
author = {Thomas R. Colin and Tony Belpaeme and Angelo Cangelosi and Nikolas Hemion},
keywords = {Creativity, Insight, Hierarchical reinforcement learning, Robotics},
abstract = {Although creativity is studied from philosophy to cognitive robotics, a definition has proven elusive. We argue for emphasizing the creative process (the cognition of the creative agent), rather than the creative product (the artifact or behavior). Owing to developments in experimental psychology, the process approach has become an increasingly attractive way of characterizing creative problem solving. In particular, the phenomenon of insight, in which an individual arrives at a solution through a sudden change in perspective, is a crucial component of the process of creativity. These developments resonate with advances in machine learning, in particular hierarchical and modular approaches, as the field of artificial intelligence aims for general solutions to problems that typically rely on creativity in humans or other animals. We draw a parallel between the properties of insight according to psychology and the properties of Hierarchical Reinforcement Learning (HRL) systems for embodied agents. Using the Creative Systems Framework developed by Wiggins and Ritchie, we analyze both insight and HRL, establishing that they are creative in similar ways. We highlight the key challenges to be met in order to call an artificial system “insightful”.}
}
@article{LIU2023119898,
title = {A reinforcement learning-based hybrid Aquila Optimizer and improved Arithmetic Optimization Algorithm for global optimization},
journal = {Expert Systems with Applications},
volume = {224},
pages = {119898},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119898},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423003998},
author = {Haiyang Liu and Xingong Zhang and Hanxiao Zhang and Chunyan Li and Zhaohui Chen},
keywords = {Reinforcement learning, Aquila Optimizer, Arithmetic Optimization Algorithm, Q-learning},
abstract = {This study constructs a reinforcement learning-based hybrid algorithm for Aquila Optimizer (AO) and improved Arithmetic Optimization Algorithm (IAOA). The point of the hybrid algorithm is that Q-learning can dynamically select the AO and the IAOA at different stages for different problems. In Arithmetic Optimization Algorithm (AOA), the mathematical optimization acceleration (MOA) function is restructured to balance global search and local exploitation, which can effectively stay away from the local optimum. Moreover, an improved reward function is modeled for Q-learning, which makes our hybrid algorithm more efficient and accurate. A set of benchmark functions and two engineering optimization problems are employed to test the performance of the proposed hybrid algorithm in this paper. Compared with other algorithms, the results show that the proposed hybrid algorithm has higher convergence speed and accuracy.}
}
@article{WANG2023128182,
title = {Collaborative optimization of multi-microgrids system with shared energy storage based on multi-agent stochastic game and reinforcement learning},
journal = {Energy},
volume = {280},
pages = {128182},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.128182},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223015761},
author = {Yijian Wang and Yang Cui and Yang Li and Yang Xu},
keywords = {Partially observable dynamic stochastic game, Multi-agent reinforcement learning, Nonlinear conditions, Multi-microgrids, Shared energy storage},
abstract = {Achieving the economical and stable operation of Multi-microgrids (MMG) systems is vital. However, there are still some challenging problems to be solved. Firstly, from the perspective of stable operation, it is necessary to minimize the energy fluctuation of the main grid. Secondly, the characteristics of energy conversion equipment need to be considered. Finally, privacy protection while reducing the operating cost of an MMG system is crucial. To address these challenges, a Data-driven strategy for MMG systems with Shared Energy Storage (SES) is proposed. In this paper, the Mixed-Attention is applied to fit the conditions of the equipment, and Multi-Agent Soft Actor-Critic(MA-SAC) , Multi-Agent Win or Learn Fast Policy Hill-Climbing (MA-WoLF-PHC) are proposed to solve the partially observable dynamic stochastic game problem. By testing the operation data of the MMG system in Northwest China, following conclusions are drawn: the R-Square (R2) values of results reach 0.999, indicating the neural network effectively models the nonlinear conditions. The proposed MMG system framework can reduce energy fluctuations in the main grid by 1746.5 kW in 24 h and achieve a cost reduction of 16.21% in the test. Finally, the superiority of the proposed algorithms is verified through their fast convergence speed and excellent optimization performance.}
}
@article{HU2021187,
title = {Relevant experience learning: A deep reinforcement learning method for UAV autonomous motion planning in complex unknown environments},
journal = {Chinese Journal of Aeronautics},
volume = {34},
number = {12},
pages = {187-204},
year = {2021},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2020.12.027},
url = {https://www.sciencedirect.com/science/article/pii/S100093612030594X},
author = {Zijian HU and Xiaoguang GAO and Kaifang WAN and Yiwei ZHAI and Qianglong WANG},
keywords = {Autonomous Motion Planning (AMP), Deep Deterministic Policy Gradient (DDPG), Deep Reinforcement Learning (DRL), Sampling method, UAV},
abstract = {Unmanned Aerial Vehicles (UAVs) play a vital role in military warfare. In a variety of battlefield mission scenarios, UAVs are required to safely fly to designated locations without human intervention. Therefore, finding a suitable method to solve the UAV Autonomous Motion Planning (AMP) problem can improve the success rate of UAV missions to a certain extent. In recent years, many studies have used Deep Reinforcement Learning (DRL) methods to address the AMP problem and have achieved good results. From the perspective of sampling, this paper designs a sampling method with double-screening, combines it with the Deep Deterministic Policy Gradient (DDPG) algorithm, and proposes the Relevant Experience Learning-DDPG (REL-DDPG) algorithm. The REL-DDPG algorithm uses a Prioritized Experience Replay (PER) mechanism to break the correlation of continuous experiences in the experience pool, finds the experiences most similar to the current state to learn according to the theory in human education, and expands the influence of the learning process on action selection at the current state. All experiments are applied in a complex unknown simulation environment constructed based on the parameters of a real UAV. The training experiments show that REL-DDPG improves the convergence speed and the convergence result compared to the state-of-the-art DDPG algorithm, while the testing experiments show the applicability of the algorithm and investigate the performance under different parameter conditions.}
}
@article{AWASTHI2022100060,
title = {VacSIM: Learning effective strategies for COVID-19 vaccine distribution using reinforcement learning},
journal = {Intelligence-Based Medicine},
volume = {6},
pages = {100060},
year = {2022},
issn = {2666-5212},
doi = {https://doi.org/10.1016/j.ibmed.2022.100060},
url = {https://www.sciencedirect.com/science/article/pii/S2666521222000138},
author = {Raghav Awasthi and Keerat Kaur Guliani and Saif Ahmad Khan and Aniket Vashishtha and Mehrab Singh Gill and Arshita Bhatt and Aditya Nagori and Aniket Gupta and Ponnurangam Kumaraguru and Tavpritesh Sethi},
keywords = {COVID-19, Vaccine distribution, Policy modeling, Reinforcement learning, Contextual bandits problem},
abstract = {A COVID-19 vaccine is our best bet for mitigating the ongoing onslaught of the pandemic. However, vaccine is also expected to be a limited resource. An optimal allocation strategy, especially in countries with access inequities and temporal separation of hot-spots, might be an effective way of halting the disease spread. We approach this problem by proposing a novel pipeline VacSIM that dovetails Deep Reinforcement Learning models into a Contextual Bandits approach for optimizing the distribution of COVID-19 vaccine. Whereas the Reinforcement Learning models suggest better actions and rewards, Contextual Bandits allow online modifications that may need to be implemented on a day-to-day basis in the real world scenario. We evaluate this framework against a naive allocation approach of distributing vaccine proportional to the incidence of COVID-19 cases in five different States across India (Assam, Delhi, Jharkhand, Maharashtra and Nagaland) and demonstrate up to 9039 potential infections prevented and a significant increase in the efficacy of limiting the spread over a period of 45 days through the VacSIM approach. Our models and the platform are extensible to all states of India and potentially across the globe. We also propose novel evaluation strategies including standard compartmental model-based projections and a causality-preserving evaluation of our model. Since all models carry assumptions that may need to be tested in various contexts, we open source our model VacSIM and contribute a new reinforcement learning environment compatible with OpenAI gym to make it extensible for real-world applications across the globe. 22http://vacsim.tavlab.iiitd.edu.in:8000/.}
}
@article{HEO2019109440,
title = {A deep reinforcement learning-based autonomous ventilation control system for smart indoor air quality management in a subway station},
journal = {Energy and Buildings},
volume = {202},
pages = {109440},
year = {2019},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2019.109440},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819313684},
author = {SungKu Heo and KiJeon Nam and Jorge Loy-Benitez and Qian Li and SeungChul Lee and ChangKyoo Yoo},
keywords = {Deep Reinforcement Learning (DeepRL), Ventilation system, Autonomous control, Smart energy management, Indoor air quality, Human health risk},
abstract = {Mechanical ventilation has been widely implemented to alleviate poor indoor air quality (IAQ) in confined underground public facilities. However, due to time-varying IAQ properties that are influenced by unpredictable factors, including outdoor air quality, subway schedules, and passenger volumes, real-time control that incorporates a trade-off between energy saving and IAQ is limited in conventional rule-based and model-based approaches. We propose a data-driven and intelligent approach for a smart ventilation control system based on a deep reinforcement learning (DeepRL) algorithm. This study utilized a deep Q-network (DQN) algorithm of DeepRL to design the ventilation system. The DQN agent was trained in a virtual environment defined by a gray-box model to simulate an IAQ system in a subway station. Performance of the proposed method over three weeks was evaluated by a comprehensive indoor air-quality index (CIAI) and energy consumption under different outdoor air quality scenarios. The results show that the proposed DeepRL-based ventilation control system reduced energy consumption by up to 14.4% for the validation dataset time interval and improved IAQ from unhealthy to acceptable.}
}
@article{XU2020706,
title = {Deep Reinforcement Learning-based resource allocation strategy for Energy Harvesting-Powered Cognitive Machine-to-Machine Networks},
journal = {Computer Communications},
volume = {160},
pages = {706-717},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419314264},
author = {Yi-Han Xu and Yong-Bo Tian and Prosper Komla Searyoh and Gang Yu and Yueh-Tiam Yong},
keywords = {Energy Harvesting, M2M communication, Resource allocation, Deep Reinforcement Learning},
abstract = {Machine-to-Machine (M2M) communication is a promising technology that may realize the Internet of Things (IoTs) in future networks. However, due to the features of massive devices and concurrent access requirement, it will cause performance degradation and enormous energy consumption. Energy Harvesting-Powered Cognitive M2M Networks (EH-CMNs) as an attractive solution is capable of alleviating the escalating spectrum deficient to guarantee the Quality of Service (QoS) meanwhile decreasing the energy consumption to achieve Green Communication (GC) became an important research topic. In this paper, we investigate the resource allocation problem for EH-CMNs underlaying cellular uplinks. We aim to maximize the energy efficiency of EH-CMNs with consideration of the QoS of Human-to-Human (H2H) networks and the available energy in EH-devices. In view of the characteristic of EH-CMNs, we formulate the problem to be a decentralized Discrete-time and Finite-state Markov Decision Process (DFMDP), in which each device acts as agent and effectively learns from the environment to make allocation decision without the complete and global network information. Owing to the complexity of the problem, we propose a Deep Reinforcement Learning (DRL)-based algorithm to solve the problem. Numerical results validate that the proposed scheme outperforms other schemes in terms of average energy efficiency with an acceptable convergence speed.}
}
@article{GROENEWALD20071034,
title = {SIMULATED NEUROCONTROL OF AN AUTOGENOUS MILL WITH EVOLUTIONARY REINFORCEMENT LEARNING},
journal = {IFAC Proceedings Volumes},
volume = {40},
number = {12},
pages = {1034-1039},
year = {2007},
note = {7th IFAC Symposium on Nonlinear Control Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20070822-3-ZA-2920.00172},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016356658},
author = {J.Wde.V. Groenewald and C. Aldrich and J.J. Eksteen and A.V.E. Conradie and L.P. Coetzer},
keywords = {Neural Networks, Nonlinearity, Neural Control, Time Series Analysis, System Identification},
abstract = {In this investigation the development of nonlinear control system for an autogenous mill was considered. A symbiotic adaptive neuroevolution algorithm was used in conjunction with a dynamic multilayer perceptron model fitted to actual plant data to evolve neurocontrol systems. Simulation studies established the potential of the approach, which yielded satisfactory results, despite having had to learn from a model that covered part of the state space only.}
}
@article{JIANG2023135704,
title = {Research on short-term optimal scheduling of hydro-wind-solar multi-energy power system based on deep reinforcement learning},
journal = {Journal of Cleaner Production},
volume = {385},
pages = {135704},
year = {2023},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.135704},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622052787},
author = {Wenyuan Jiang and Yongqiang Liu and Guohua Fang and Ziyu Ding},
keywords = {Multi-energy complementarity of hydro, Wind and solar power, Deep reinforcement learning, Forecast uncertainty, Short-term optimal scheduling},
abstract = {Recently, grid-integrated wind power and photovoltaics have experienced rapid growth. However, their uncertainty increases the difficulty of grid scheduling and operation. Short-term power generation decisions made by conventional scheduling methods, which are based on the output forecast information of wind and solar power often impact the power benefit owing to forecast errors during the actual operation. This study explores the application of deep reinforcement learning methods for short-term optimal scheduling of hydro-wind-solar multi-energy power system, considering forecast uncertainty. First, with the objective of maximizing power generation benefit from the multi-energy complementary system, the Deep Q Network (DQN) method in deep reinforcement learning is employed to construct the model framework of the short-term optimal scheduling of hydro-wind-solar multi-energy power system (collectively referred to as DQN model later). Thereafter, the impact of each learning parameter in the DQN model on performances is analyzed, and the model parameters are reasonably configured. Furthermore, the DQN model is driven by wind and solar power output forecast data to develop short-term power generation decisions. Finally, the actual wind and solar power output is used as input to implement short-term decisions, whereas the dynamic programming (DP) method is used to build the model as a comparison to evaluate the power benefit and computational efficiency of the DQN model. The multi-energy complementary system of hydro, wind, and solar power of the Jinping-1 Hydropower Station in the Yalong river basin is used as an example for the study. In the flood season, compared with the DP model and the historical actual operation process, the total power generation benefits obtained from the DQN model's implementation of short-term decision-making increases by 6.18% and 1.38%, respectively. In the dry season, the total power generation benefits obtained from the DQN model's implementation of short-term decision-making increases by 2.26% and 19.00% respectively compared with the DP model and the historical actual operation process. Additionally, the DQN model can significantly improve the decision-making efficiency. The DP model requires minutes or hours of computation time; the DQN model consumes less than 1s to compute, which is conducive to efficient decision making of the complementary system.}
}
@article{YOO2022157,
title = {A dynamic penalty approach to state constraint handling in deep reinforcement learning},
journal = {Journal of Process Control},
volume = {115},
pages = {157-166},
year = {2022},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2022.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959152422000816},
author = {Haeun Yoo and Victor M. Zavala and Jay H. Lee},
keywords = {Reinforcement learning, Penalty approach, Dynamic penalty, Constraint handling},
abstract = {Deep reinforcement learning (RL) has emerged as a promising approach to solving sequential optimization problems that involve high dimensional state/action space and stochastic uncertainties. Unfortunately, many such problems, especially those related to process control, involve state constraints (typically expressed as inequality constraints) that are difficult to handle. Most RL application studies have incorporated inequality constraints into the training by adding penalty terms for violating the constraints to the reward function. However, while training neural networks to learn the value (or Q) function, one can run into numerical difficulties caused by sharp changes in the value function (VF) at the constraint boundary. This problem can lead to an array of issues including getting stuck in local minima and slow convergence during training which ultimately manifest as poor closed-loop performance when training samples are limited. In this paper, we first examine the effect of the penalty function form on the neural network training performance in deep RL algorithms. To address the slow convergence, we propose a dynamic penalty (DP) approach where the penalty factor is gradually and systematically increased as the iteration episodes proceed during training. The agents trained by a Deep Q-Learning algorithm with the proposed approach were compared with agents trained with other constant penalty functions in a vehicle control problem and in a battery management control problem. Results show that the DP approach can help improve the accuracy of the VF approximation, leading to superior results in the constraint satisfaction and in the degree of violation.}
}
@article{MODARES20141780,
title = {Optimal tracking control of nonlinear partially-unknown constrained-input systems using integral reinforcement learning},
journal = {Automatica},
volume = {50},
number = {7},
pages = {1780-1792},
year = {2014},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2014.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0005109814001861},
author = {Hamidreza Modares and Frank L. Lewis},
keywords = {Optimal tracking control, Integral reinforcement learning, Input constrainers, Neural networks},
abstract = {In this paper, a new formulation for the optimal tracking control problem (OTCP) of continuous-time nonlinear systems is presented. This formulation extends the integral reinforcement learning (IRL) technique, a method for solving optimal regulation problems, to learn the solution to the OTCP. Unlike existing solutions to the OTCP, the proposed method does not need to have or to identify knowledge of the system drift dynamics, and it also takes into account the input constraints a priori. An augmented system composed of the error system dynamics and the command generator dynamics is used to introduce a new nonquadratic discounted performance function for the OTCP. This encodes the input constrains into the optimization problem. A tracking Hamilton–Jacobi–Bellman (HJB) equation associated with this nonquadratic performance function is derived which gives the optimal control solution. An online IRL algorithm is presented to learn the solution to the tracking HJB equation without knowing the system drift dynamics. Convergence to a near-optimal control solution and stability of the whole system are shown under a persistence of excitation condition. Simulation examples are provided to show the effectiveness of the proposed method.}
}
@article{WERBOS1990179,
title = {Consistency of HDP applied to a simple reinforcement learning problem},
journal = {Neural Networks},
volume = {3},
number = {2},
pages = {179-189},
year = {1990},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(90)90088-3},
url = {https://www.sciencedirect.com/science/article/pii/0893608090900883},
author = {Paul J. Werbos},
keywords = {Reinforcement learning, HDP, Dynamic programming, Neurocontrol, Adaptive critics, Consistency, Optimization, Convergence},
abstract = {In “reinforcement learning over time,” a system of neural networks learns to control a system of motors or muscles so as to maximize some measure of performance or reinforcement in the future. Two architectures or designs are now widely used to address this problem in an engineering context: backpropagation through time and the adaptive critic family. This article begins with a brief review of these and other neurocontrol methods and their applications. Then it addresses the issue of consistency in using Heuristic Dynamic Programming (HDP), a procedure for adapting a “critic” neural network, closely related to Sutton's method of temporal differences. In a multivariate linear environment, HDP converges to the correct system of weights. However, a variant of HDP—which appeals to common sense and which uses backpropagation with a complete gradient—leads to the wrong weights almost always. Similar consistency tests may be useful in evaluating architectures for neural nets to identify or emulate dynamic systems.}
}
@article{ZHANG2021113608,
title = {Data-driven optimal energy management for a wind-solar-diesel-battery-reverse osmosis hybrid energy system using a deep reinforcement learning approach},
journal = {Energy Conversion and Management},
volume = {227},
pages = {113608},
year = {2021},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2020.113608},
url = {https://www.sciencedirect.com/science/article/pii/S0196890420311365},
author = {Guozhou Zhang and Weihao Hu and Di Cao and Wen Liu and Rui Huang and Qi Huang and Zhe Chen and Frede Blaabjerg},
keywords = {Hybrid energy system, Energy management, Information entropy theory, Cost reduction, Deep reinforcement learning},
abstract = {Significant dependence on fossil fuels and freshwater shortage are common problems in remote and arid regions. In this context, the operation of a wind-solar-diesel-battery-reverse osmosis hybrid energy system has become a suitable option to solve this problem. However, owing to the uncertainties of renewable energy availability and load demand, it is a challenge for operators to develop an energy management scheme for such a system. This study aims to determine a real-time dynamic energy management strategy considering the uncertainties of the system. To this end, the energy management of a hybrid energy system is presented as an optimal control objective, and multi-targets are considered along with constraints. The information entropy theory is introduced to calculate the weight factor for the trade-off between different targets. Then, a deep reinforcement learning algorithm is adopted to solve this problem and obtain the optimal control policy. Finally, the proposed method is applied to a typical hybrid energy system, and numerous data are applied to train an agent to obtain the optimal energy management policy. Simulation results demonstrate that a well-trained agent can provide a better control policy and reduce costs by up to 14.17% in comparison with other methods.}
}
@article{KE2019785,
title = {Self-learning control for wavefront sensorless adaptive optics system through deep reinforcement learning},
journal = {Optik},
volume = {178},
pages = {785-793},
year = {2019},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2018.09.160},
url = {https://www.sciencedirect.com/science/article/pii/S0030402618314785},
author = {Hu Ke and Bing Xu and Zhenxing Xu and Lianghua Wen and Ping Yang and Shuai Wang and Lizhi Dong},
keywords = {Adaptive optics, Machine learning, Deep reinforcement learning},
abstract = {An aberration correction algorithm for wavefront sensorless adaptive optics (WFSless AO) systems based on deep reinforcement learning is presented. An actor–critic structure is designed to evaluate a control policy through the deep deterministic policy gradient (DDPG) algorithm. The algorithm performance is verified with a set-up simulation environment. According to the correction results, the aberration correction process can be expressed as a Markov decision process (MDP). The method exhibits excellent performances in correction capacity and speed. Similar correction effects are obtained with the stochastic parallel-gradient descent (SPGD) algorithm and WFSless AO based on the general-modes (AOG) algorithm. Moreover, the correction speed is improved by approximately 9 and 2.5 times, respectively.}
}
@article{HIRCHOUA2021114553,
title = {Deep reinforcement learning based trading agents: Risk curiosity driven learning for financial rules-based policy},
journal = {Expert Systems with Applications},
volume = {170},
pages = {114553},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114553},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420311970},
author = {Badr Hirchoua and Brahim Ouhbi and Bouchra Frikh},
keywords = {Deep reinforcement learning, Partially observable Markov decision process, Knowledge uncertainty, Financial data, Trading system, Financial engineering},
abstract = {Financial markets are complex dynamic systems influenced by a high number of active agents, which produce a behavior with high randomness and noise. Trading strategies are well depicted as an online decision-making problem involving imperfect information and aiming to maximize the return while restraining the risk. However, it is challenging to obtain an optimal strategy in the complex and dynamic stock market. Therefore, recent developments in similar environments have pushed researchers towards exciting new horizons. In this paper, a novel rule-based policy approach is proposed to train a deep reinforcement learning agent for automated financial trading. Precisely, a continuous virtual environment has been created, with different versions of agents trading against one another. During this multiplex process, the agents which are trained on 504 risky datasets, use the fundamental concepts of proximal policy optimization to improve their own decision making by adjusting their action choice against the uncertainty of states. Risk curiosity-driven learning acts as an intrinsic reward function and is heavily laden with signals to find salient relationships between actions and market behaviors. The trained agent based on curiosity-driven risk has steadily and progressively improved actions quality. The self-learned rules driven by the agent curiosity push the policy towards actions that yield a high performance over the environment. Experiments on 8 real-world stocks are given to verify the appropriateness and efficiency of the self-learned rules. The proposed system has achieved promising performances, made better trades using fewer transactions, and outperformed the state-of-the-art baselines.}
}
@article{TOUZET1997251,
title = {Neural reinforcement learning for behaviour synthesis},
journal = {Robotics and Autonomous Systems},
volume = {22},
number = {3},
pages = {251-281},
year = {1997},
note = {Robot Learning: The New Wave},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(97)00042-0},
url = {https://www.sciencedirect.com/science/article/pii/S0921889097000420},
author = {Claude F. Touzet},
keywords = {Neural Q-learning, Reinforcement learning, Obstacle avoidance behaviour, Self-organising map, Autonomous robotics},
abstract = {We present the results of a research aimed at improving the Q-learning method through the use of artificial neural networks. Neural implementations are interesting due to their generalisation ability. Two implementations are proposed: one with a competitive multilayer perceptron and the other with a self-organising map. Results obtained on a task of learning an obstacle avoidance behaviour for the mobile miniature robot Khepera show that this last implementation is very effective, learning more than 40 times faster than the basic Q-learning implementation. These neural implementations are also compared with several Q-learning enhancements, like the Q-learning with Hamming distance, Q-learning with statistical clustering and Dyna-Q.}
}
@article{DOGRU202186,
title = {Online reinforcement learning for a continuous space system with experimental validation},
journal = {Journal of Process Control},
volume = {104},
pages = {86-100},
year = {2021},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959152421000950},
author = {Oguzhan Dogru and Nathan Wieczorek and Kirubakaran Velswamy and Fadi Ibrahim and Biao Huang},
keywords = {Reinforcement learning, A3C, Process control, Exploration},
abstract = {Reinforcement learning (RL) for continuous state/action space systems has remained a challenge for nonlinear multivariate dynamical systems even at a simulation level. Implementing such schemes for real-time control is still of a difficulty and remains largely unanswered. In this study, several critical strategies for practical implementation of RL are developed, and a multivariable, multi-modal, hybrid three-tank (HTT) physical process is utilized to illustrate the proposed strategies. A successful real-time implementation of RL is reported. The first step is a meta-heuristic first principles model parameter optimization, where a custom pseudo random binary signal (PRBS) is used to obtain open-loop experimental data. This is followed by in silico asynchronous advantage actor–critic (A3C/A-A2C) based policy learning. In the second step, three different approaches (namely proximal learning, single trajectory learning, and multiple trajectory learning) are utilized to explore the state/action space. In the final step, online learning (A2C) using the best in silico policy on the real process using a socket connection is established. The extent of exploration (EoE, a measure of exploration) is proposed as a parameter for quantifying exploration of the state/action space. While the online sample efficiency of RL application is enhanced, a soft constraint based constrained learning is proposed and validated. With considerations of the proposed strategies, this work demonstrates the possibility of applying RL to solve practical control problems.}
}
@article{HAN2023110801,
title = {Multi-strategy multi-objective differential evolutionary algorithm with reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {277},
pages = {110801},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110801},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123005518},
author = {Yupeng Han and Hu Peng and Changrong Mei and Lianglin Cao and Changshou Deng and Hui Wang and Zhijian Wu},
keywords = {Multiobjective optimization, Differential evolution, Multistrategy, Reinforcement learning, Reference point adaptation},
abstract = {Multiobjective evolutionary algorithms (MOEAs) have gained much attention due to their high effectiveness and efficiency in solving multiobjective optimization problems (MOPs). However, when solving MOPs, it is important but difficult to maintain a good balance of exploration and exploitation. In addition, some reference point based MOEAs with fixed reference points perform poorly on MOPs with irregular frontiers. Therefore, this paper proposes a new multistrategy multiobjective differential evolutionary (DE) algorithm, named RLMMDE. In RLMMDE, a multistrategy and multicrossover DE optimizer is utilized to alleviate the exploration and exploitation dilemma. An adaptive reference point activation mechanism based on RL is proposed to activate the adaptive adjustment of reference points. Moreover, a reference point adaptation method is proposed to improve the performance of RLMMDE on irregular frontier problems. Experimental results of RLMMDE tested on some benchmark test suites (i.e., ZDT, DTLZ, UF, WFG, and LSMOP) and two practical mixed-variable optimization problems show that the algorithm outperforms some advanced MOEAs.}
}
@article{SHEIKHI201642,
title = {Dynamic load management for a residential customer; Reinforcement Learning approach},
journal = {Sustainable Cities and Society},
volume = {24},
pages = {42-51},
year = {2016},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2016.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2210670716300543},
author = {A. Sheikhi and M. Rayati and A.M. Ranjbar},
keywords = {Smart Energy Hub (SEH), Smart grids (SG), Reinforcement Learning (RL), Energy efficiency, Optimization},
abstract = {United Nation aims to double the global rate of improvement in energy efficiency as one of the sustainable development goals. It means researchers should focus on energy systems to enhance their overall efficiency. One of the effective solution to move from suboptimal energy systems to optimal ones is analyzing energy system in Energy Hub (EH) framework. In EH framework, interactions between different energy carriers are considered in supplying the required loads. The couplings and selecting proper combinations of inputs energy carriers lead to more optimized and intelligent consumption. The appropriate combination is found by solving an optimization problem at each time step. Utilizing intelligent technologies such as Advanced Metering Infrastructures (AMIs) inevitably facilitate the decision making processes. This paper modifies the classic Energy Hub model to present an upgraded model in the smart environment entitling “Smart Energy Hub” and optimizes the operation of a residential customer equipped with combined heat and power (CHP), auxiliary boiler, electricity storage and heating storage in this framework. Supporting real time, two-way communication between utility companies and smart energy hubs, and allowing AMIs at both ends to manage power consumption necessitates large-scale real-time computing capabilities to handle the communication and the storage of huge transferable data. To address this concern and reduce the amount of calculations, Reinforcement Learning (RL) method is employed to find a near optimal solution, which does not need massive computations. Finally, communications to large numbers of endpoints in a secure, scalable, and highly-available environment, in this paper, we propose a cloud computing (CC) architecture. Simulation results show that by applying RL technique in smart energy hub framework for a residential customer, efficiency of the energy system is increased substantially and leads to decrease energy bills and electricity peak load.}
}
@article{ROGGEVEEN2021102003,
title = {Transatlantic transferability of a new reinforcement learning model for optimizing haemodynamic treatment for critically ill patients with sepsis},
journal = {Artificial Intelligence in Medicine},
volume = {112},
pages = {102003},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2020.102003},
url = {https://www.sciencedirect.com/science/article/pii/S0933365720312689},
author = {Luca Roggeveen and Ali {el Hassouni} and Jonas Ahrendt and Tingjie Guo and Lucas Fleuren and Patrick Thoral and Armand RJ Girbes and Mark Hoogendoorn and Paul WG Elbers},
keywords = {Sepsis, Reinforcement learning, Deep Q learning, ICU},
abstract = {Introduction
In recent years, reinforcement learning (RL) has gained traction in the healthcare domain. In particular, RL methods have been explored for haemodynamic optimization of septic patients in the Intensive Care Unit. Most hospitals however, lack the data and expertise for model development, necessitating transfer of models developed using external datasets. This approach assumes model generalizability across different patient populations, the validity of which has not previously been tested. In addition, there is limited knowledge on safety and reliability. These challenges need to be addressed to further facilitate implementation of RL models in clinical practice.
Method
We developed and validated a new reinforcement learning model for hemodynamic optimization in sepsis on the MIMIC intensive care database from the USA using a dueling double deep Q network. We then transferred this model to the European AmsterdamUMCdb intensive care database. T-Distributed Stochastic Neighbor Embedding and Sequential Organ Failure Assessment scores were used to explore the differences between the patient populations. We apply off-policy policy evaluation methods to quantify model performance. In addition, we introduce and apply a novel deep policy inspection to analyse how the optimal policy relates to the different phases of sepsis and sepsis treatment to provide interpretable insight in order to assess model safety and reliability.
Results
The off-policy evaluation revealed that the optimal policy outperformed the physician policy on both datasets despite marked differences between the two patient populations and physician's policies. Our novel deep policy inspection method showed insightful results and unveiled that the model could initiate therapy adequately and adjust therapy intensity to illness severity and disease progression which indicated safe and reliable model behaviour. Compared to current physician behavior, the developed policy prefers a more liberal use of vasopressors with a more restrained use of fluid therapy in line with previous work.
Conclusion
We created a reinforcement learning model for optimal bedside hemodynamic management and demonstrated model transferability between populations from the USA and Europe for the first time. We proposed new methods for deep policy inspection integrating expert domain knowledge. This is expected to facilitate progression to bedside clinical decision support for the treatment of critically ill patients.}
}
@article{SINGH2021101665,
title = {Reinforcement learning with fuzzified reward approach for MPPT control of PV systems},
journal = {Sustainable Energy Technologies and Assessments},
volume = {48},
pages = {101665},
year = {2021},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101665},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821006792},
author = {Yaduvir Singh and Nitai Pal},
keywords = {Deep Q-Network (DQN), Fuzzification, Reward, Reinforcement learning framework, Photovoltaic array, Maximum Power Point Tracking},
abstract = {The array of photovoltaic cells converts solar power into electricity and generate a clean and renewable source of energy. For them to be efficient, they must constantly generate the maximum possible power under various environmental conditions. This is the well-known problem of tracking Maximum Power Point (MPP). The classical methods fall short as they are not able to cope with the changing environmental conditions. This work aims to track MPPT by employing Deep Reinforcement Learning (RL) with a fuzzy reward mechanism introduced for better translation of continuous space into various levels of abstraction. The author simulates a model of a PV array connected to a variable load. The introduction of RL has provided an advantage as employing a Deep-Q learning algorithm makes the structure a model-free and thereby eliminates the various modeling parameters with their effect on the design. The designed setup renders 2.8 W of power under the maximum load conditions and has been able to track the maximum power under the different varying conditions of temperature and irradiations.}
}
@article{ZHAO2020105879,
title = {A decomposition-based many-objective artificial bee colony algorithm with reinforcement learning},
journal = {Applied Soft Computing},
volume = {86},
pages = {105879},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2019.105879},
url = {https://www.sciencedirect.com/science/article/pii/S156849461930660X},
author = {Haitong Zhao and Changsheng Zhang},
keywords = {Swarm intelligence, Artificial bee colony, Many-objective optimization, Reinforcement learning, Decomposition strategy},
abstract = {When optimizing many-objective optimization problems (MaOPs), the optimization effect is normally related to the problem types. Therefore, enhancing the generalization ability is essential to the application of the algorithms. In this paper, a novel decomposition-based Artificial bee colony algorithm (ABC) for MaOP optimization, MaOABC/D-LA, is presented to enhance the generalization ability. A reinforcement learning-based searching strategy is designed in the MaOABC/D-LA, with which the algorithm adjusts its searching actions according to their performance. And a variant of the onlooker bee mechanism is proposed to balance the optimization quality. To investigate performance of the proposed algorithm, a comparison experiment is conducted. The experimental results show that the MaOABC/D-LA outperforms the peer algorithms in efficiency and solution quality for MaOPs with different types of features. This indicates the proposed method has a definite effect on improving generalization ability.}
}
@article{LI2023103933,
title = {COOR-PLT: A hierarchical control model for coordinating adaptive platoons of connected and autonomous vehicles at signal-free intersections based on deep reinforcement learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {146},
pages = {103933},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103933},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22003461},
author = {Duowei Li and Feng Zhu and Tianyi Chen and Yiik Diew Wong and Chunli Zhu and Jianping Wu},
keywords = {Connected and autonomous vehicle (CAV), Signal-free intersection, Adaptive platoon, Multi-agent coordination, Hierarchical control, Deep reinforcement learning},
abstract = {Platooning and coordination are two implementation strategies that are frequently proposed for traffic control of connected and autonomous vehicles (CAVs) at signal-free intersections instead of using conventional traffic signals. However, few studies have attempted to integrate both strategies to better facilitate the CAV control at signal-free intersections. To this end, this study proposes a hierarchical control model, named COOR-PLT, to coordinate adaptive CAV platoons at a signal-free intersection based on deep reinforcement learning (DRL). COOR-PLT has a two-layer framework. The first layer uses a centralized control strategy to form adaptive platoons. The optimal size of each platoon is determined by considering multiple objectives (i.e., efficiency, fairness and energy saving). The second layer employs a decentralized control strategy to coordinate multiple platoons passing through the intersection. Each platoon is labeled with coordinated status or independent status, upon which its passing priority is determined. As an efficient DRL algorithm, Deep Q-network (DQN) is adopted to determine platoon sizes and passing priorities respectively in the two layers. The model is validated and examined on the simulator Simulation of Urban Mobility (SUMO). The simulation results demonstrate that the model is able to: (1) achieve satisfactory convergence performances; (2) adaptively determine platoon size in response to varying traffic conditions; and (3) completely avoid deadlocks at the intersection. By comparison with other control methods, the model manifests its superiority of adopting adaptive platooning and DRL-based coordination strategies. Also, the model outperforms several state-of-the-art methods on reducing travel time and fuel consumption in different traffic conditions.}
}
@article{TAKAYAMA2018209,
title = {Autonomous Decentralized Control of Distribution Network Voltage using Reinforcement Learning⁎⁎This study was supported by research grant from Japan Power Academy.},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {28},
pages = {209-214},
year = {2018},
note = {10th IFAC Symposium on Control of Power and Energy Systems CPES 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.703},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318334220},
author = {Satoshi Takayama and Atsushi Ishigame},
keywords = {Voltage control, Distribution network, Reactive power, Inverters, Machine learning},
abstract = {Recently, power flow patterns in distribution network has considerably modified due to a large amount of PV installation and increase of consumers behavior patterns. Hence, a system operator has become difficult to control voltage by the conventional method. As a countermeasure of this problem, the reactive power control method with the PV inverters has been reported. In this paper, we proposed a new voltage control method with reactive control of PV inverters using reinforcement learning. In particular, the purpose of proposed method is to bring out the cooperative reactive power control without the information between PV inverters. We verify the effectiveness of proposed method through computational simulation using distribution network model.}
}
@article{WANG2020107313,
title = {An energy-efficient distributed adaptive cooperative routing based on reinforcement learning in wireless multimedia sensor networks},
journal = {Computer Networks},
volume = {178},
pages = {107313},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107313},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619313568},
author = {Denghui Wang and Jian Liu and Dezhong Yao},
keywords = {WMSN, Adaptive, Cooperative routing, Reinforcement learning, QoS, Energy-efficient distributed},
abstract = {Complex task processing and frequent data communication in Wireless Multimedia Sensor Networks (WMSN) demand for energy-efficient and Quality of Service (QoS) guarantee to support new applications especially in the sensing layer of Internet-of-Vehicles. However, the WMSN is heterogeneous and the energy distribution is not uniform, the current routing protocols do not take energy consumption into account while ensuring QoS. Therefore, to make energy distribution more efficiently while ensuring QoS has become a challenging problem. In this paper, we propose an energy-efficient distributed adaptive cooperative routing (EDACR) for WMSN, taking into account the constraints of QoS and energy consumption. Particularly, we design a reinforcement learning based mechanism to perform QoS and energy balanced routing according to the knowledge of reliability and delay. The simulation results show that the energy consumption is reduced while ensuring QoS compared with the traditional cooperative protocol and the distributed adaptive cooperative routing protocol.}
}
@article{HADDAD2022105019,
title = {A deep reinforcement learning-based cooperative approach for multi-intersection traffic signal control},
journal = {Engineering Applications of Artificial Intelligence},
volume = {114},
pages = {105019},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105019},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622001993},
author = {Tarek Amine Haddad and Djalal Hedjazi and Sofiane Aouag},
keywords = {Traffic congestion, Intelligent transportation systems, Adaptive traffic signal control, Deep reinforcement learning, Multi-intersection},
abstract = {Recently, Adaptive Traffic Signal Control (ATSC) in the multi-intersection system is considered as one of the most critical issues in Intelligent Transportation Systems (ITS). Among the proposed AI-based approaches, Deep Reinforcement Learning (DRL) has been largely applied while showing better performances. This paper proposes a new DRL-based cooperative approach for controlling multiple intersections. The problem is modelled as a Multi-Agent Reinforcement Learning (MARL) system, while each agent is trained to select the best action to control an intersection by obtaining information about its local lanes state. The cooperation aspect is manifested in this approach by considering the effect of the state, action and reward of neighbour agents in the process of policy learning. An intersection controller applies a Deep Q-Network (DQN) method, while transferring state, action and reward received from their neighbour agents to its own loss function during the learning process. The experimental results under different scenarios shows that the proposed approach outperforms many state-of-the-art approaches in terms of three metrics: Average Waiting Time (AWT), Average Queue Length (AQL) and Average Emission CO2 (AEC). In addition, the cooperation between the different trained DRL-based controllers allows the system to continuously learn and improve its performance by interacting with the environment, particularly when the traffic is congested.}
}
@article{ROKHFOROZ2023109081,
title = {Safe multi-agent deep reinforcement learning for joint bidding and maintenance scheduling of generation units},
journal = {Reliability Engineering & System Safety},
volume = {232},
pages = {109081},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2022.109081},
url = {https://www.sciencedirect.com/science/article/pii/S0951832022006962},
author = {Pegah Rokhforoz and Mina Montazeri and Olga Fink},
keywords = {Maintenance scheduling, Generation units, Reinforcement learning, Multi-agent system},
abstract = {This paper proposes a safe reinforcement learning algorithm for generation bidding decisions and unit maintenance scheduling in a competitive electricity market environment. In this problem, each unit aims to find a bidding strategy that maximizes its revenue while concurrently retaining its reliability by scheduling preventive maintenance. The maintenance scheduling provides some safety constraints which should be satisfied at all times. Meeting the critical safety and reliability requirements when the generation units have incomplete information regarding each other’s bidding strategy is a challenging problem. Bi-level optimization and reinforcement learning are state-of-the-art approaches for solving this type of problem. However, neither bi-level optimization nor reinforcement learning can handle the challenges of incomplete information and critical safety constraints. To tackle these challenges, we propose the safe deep deterministic policy gradient reinforcement learning algorithm, which is based on a combination of reinforcement learning and a predicted safety filter. The case study demonstrates that the proposed approach can yield a higher profit compared to other state-of-the-art methods while concurrently satisfying the system safety constraints. Moreover, the case study shows that the reward of the learning algorithm with incomplete information can converge to a reward of the complete information game.}
}
@article{WU20228152,
title = {Explore the weakness: Instructive exploration adversarial robust reinforcement learning},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {10, Part A},
pages = {8152-8161},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822002737},
author = {Chunyang Wu and Fei Zhu and Quan Liu},
keywords = {Exploration, Robustness, Quantifying uncertainty, Intrinsic curiosity mechanism},
abstract = {Although reinforcement learning has been proved to be effective in many simulated platforms, it may still fail in environments due to the difference between simulation environment and real world environment, as well as being subjected to unexcepted attacks that objectively exist. Therefore, it calls for improving the robustness of the agent to increase its stability. To address the problem, an algorithm that uses the curiosity mechanism to improve the model exploration, referred to as instructive exploration adversarial robust reinforcement learning(Iearrl), is proposed, which enhances the adaption ability of agents through adversary learning, ensuring that the agent chooses a better action in practical environments with different settings from the training environment. At the same time, in order to increase the efficiency of exploration and reduce the cost, a model used to evaluate the competency of the agent is built for mentoring internal rewards determining whether further exploration is needed by analyzing the agent’s action in the current state space. The experiments in MuJoCo platforms verified the effectiveness of the proposed method.}
}
@article{ZHANG2023127183,
title = {Hybrid data-driven method for low-carbon economic energy management strategy in electricity-gas coupled energy systems based on transformer network and deep reinforcement learning},
journal = {Energy},
volume = {273},
pages = {127183},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.127183},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223005777},
author = {Bin Zhang and Weihao Hu and Xiao Xu and Zhenyuan Zhang and Zhe Chen},
keywords = {Deep reinforcement learning, Neural network, Energy management system, Integrated energy system, Low-carbon},
abstract = {Because of their attractive economic and environmental benefits, integrated energy systems (IESs), especially electricity-gas coupled energy systems (EGCESs), have received great interest. In this study, to minimize carbon trading and generation costs, a model-free deep-reinforcement-learning (DRL) method is integrated into the low-carbon economic autonomous energy management system of an EGCES. Unlike previous works, this work proposes an innovative transformer-deep deterministic policy gradient (TDDPG) that combines the superior feature extraction ability of the transformer network with the strong decision-making ability of a state-of-the-art TDDPG. The proposed method is tailored to the specific energy management problem to meet the requirements of multi-dimensional and continuous control. To validate the advantages of the TDDPG, the proposed method is compared with benchmark optimization methods. The simulation results illustrate that TDDPG performs more effectively than the examined DRL approaches in terms of optimizing low-carbon and economy targets, computation efficiency, and optimization of the results. Besides, the TDDPG method achieves lower average comprehensive costs than DDPG and requires less training time for real-time energy scheduling.}
}
@article{HUANG2023,
title = {Cognitive interference decision method for air defense missile fuze based on reinforcement learning},
journal = {Defence Technology},
year = {2023},
issn = {2214-9147},
doi = {https://doi.org/10.1016/j.dt.2023.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2214914723000910},
author = {Dingkun Huang and Xiaopeng Yan and Jian Dai and Xinwei Wang and Yangtian Liu},
keywords = {Cognitive radio, Interference decision, Radio fuze, Reinforcement learning, Interference strategy optimization},
abstract = {To solve the problem of the low interference success rate of air defense missile radio fuzes due to the unified interference form of the traditional fuze interference system, an interference decision method based Q-learning algorithm is proposed. First, dividing the distance between the missile and the target into multiple states to increase the quantity of state spaces. Second, a multidimensional motion space is utilized, and the search range of which changes with the distance of the projectile, to select parameters and minimize the amount of ineffective interference parameters. The interference effect is determined by detecting whether the fuze signal disappears. Finally, a weighted reward function is used to determine the reward value based on the range state, output power, and parameter quantity information of the interference form. The effectiveness of the proposed method in selecting the range of motion space parameters and designing the discrimination degree of the reward function has been verified through offline experiments involving full-range missile rendezvous. The optimal interference form for each distance state has been obtained. Compared with the single-interference decision method, the proposed decision method can effectively improve the success rate of interference.}
}
@article{SUBRAMANIAN2022271,
title = {Reinforcement learning and its connections with neuroscience and psychology},
journal = {Neural Networks},
volume = {145},
pages = {271-287},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003944},
author = {Ajay Subramanian and Sharad Chitlangia and Veeky Baths},
keywords = {Reinforcement learning, Neuroscience, Psychology},
abstract = {Reinforcement learning methods have recently been very successful at performing complex sequential tasks like playing Atari games, Go and Poker. These algorithms have outperformed humans in several tasks by learning from scratch, using only scalar rewards obtained through interaction with their environment. While there certainly has been considerable independent innovation to produce such results, many core ideas in reinforcement learning are inspired by phenomena in animal learning, psychology and neuroscience. In this paper, we comprehensively review a large number of findings in both neuroscience and psychology that evidence reinforcement learning as a promising candidate for modeling learning and decision making in the brain. In doing so, we construct a mapping between various classes of modern RL algorithms and specific findings in both neurophysiological and behavioral literature. We then discuss the implications of this observed relationship between RL, neuroscience and psychology and its role in advancing research in both AI and brain science.}
}
@article{MENDONCA201837,
title = {Reinforcement learning with optimized reward function for stealth applications},
journal = {Entertainment Computing},
volume = {25},
pages = {37-47},
year = {2018},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1875952117300587},
author = {Matheus R.F. Mendonça and Heder S. Bernardino and Raul Fonseca Neto},
keywords = {Machine learning, Reinforcement learning, Evolved reward function, Genetic algorithms, Stealth applications},
abstract = {Stealth applications are focused in accomplishing a certain objective without being spotted by enemy patrols. Although very diffused in modern applications (security, robotic, military, games), stealthy behaviors has not been extensively studied. Here, we focus on how to obtain good stealthy behaviors by tackling two different problems: (i) how to use a machine learning approach in order to allow the stealthy agent to learn good behaviors for any environment, and (ii) how to use evolutionary computing in order to define specific parameters for our machine learning approach without any prior knowledge of the problem. We use Reinforcement Learning in order to learn good covert behaviors capable of achieving a high success rate in random trials of a purpose built stealth simulator. We also propose an evolutionary approach that is capable of automatically defining a good reward function for our reinforcement learning model. The experiments performed shows that using reinforcement learning with evolved reward function through evolutionary computing achieves a higher performance than using reinforcement learning with the hand-crafted reward function.}
}
@article{WANG2019216,
title = {Meta-modeling game for deriving theory-consistent, microstructure-based traction–separation laws via deep reinforcement learning},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {346},
pages = {216-241},
year = {2019},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2018.11.026},
url = {https://www.sciencedirect.com/science/article/pii/S0045782518305851},
author = {Kun Wang and WaiChing Sun},
keywords = {Meta-modeling, Traction–separation law, Data-driven computational mechanics, Path-dependent responses, Fracture opening and closure},
abstract = {This paper presents a new meta-modeling framework that employs deep reinforcement learning (DRL) to generate mechanical constitutive models for interfaces. The constitutive models are conceptualized as information flow in directed graphs. The process of writing constitutive models is simplified as a sequence of forming graph edges with the goal of maximizing the model score (a function of accuracy, robustness and forward prediction quality). Thus meta-modeling can be formulated as a Markov decision process with well-defined states, actions, rules, objective functions and rewards. By using neural networks to estimate policies and state values, the computer agent is able to efficiently self-improve the constitutive model it generated through self-playing, in the same way AlphaGo Zero (the algorithm that outplayed the world champion in the game of Go) improves its gameplay. Our numerical examples show that this automated meta-modeling framework does not only produces models which outperform existing cohesive models on benchmark traction–separation data, but is also capable of detecting hidden mechanisms among micro-structural features and incorporating them in constitutive models to improve the forward prediction accuracy, both of which are difficult tasks to do manually.}
}
@article{QIU201943,
title = {A novel QoS-enabled load scheduling algorithm based on reinforcement learning in software-defined energy internet},
journal = {Future Generation Computer Systems},
volume = {92},
pages = {43-51},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.09.023},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1830308X},
author = {Chao Qiu and Shaohua Cui and Haipeng Yao and Fangmin Xu and F. Richard Yu and Chenglin Zhao},
keywords = {Reinforcement learning, Software-defined networking, Load scheduling, Quality of Service (QoS), Energy internet, Smart grid},
abstract = {Recently, smart grid and Energy Internet (EI) are proposed to solve energy crisis and global warming, where improved communication mechanisms are important. Software-defined networking (SDN) has been used in smart grid for real-time monitoring and communicating, which requires steady web-environment with no packet loss and less time delay. With the explosion of network scales, the idea of multiple controllers has been proposed, where the problem of load scheduling needs to be solved. However, some traditional load scheduling algorithms have inferior robustness under the complicated environments in smart grid, and inferior time efficiency without pre-strategy, which are hard to meet the requirement of smart grid. Therefore, we present a novel controller mind (CM) framework to implement automatic management among multiple controllers. Specially, in order to solve the problem of complexity and pre-strategy in the system, we propose a novel Quality of Service (QoS) enabled load scheduling algorithm based on reinforcement learning in this paper. Simulation results show the effectiveness of our proposed scheme in the aspects of load variation and time efficiency.}
}
@article{LEE2012154,
title = {Reinforcement learning in young adults with developmental language impairment},
journal = {Brain and Language},
volume = {123},
number = {3},
pages = {154-163},
year = {2012},
issn = {0093-934X},
doi = {https://doi.org/10.1016/j.bandl.2012.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0093934X12001411},
author = {Joanna C. Lee and J. Bruce Tomblin},
keywords = {Reinforcement learning, Language, Developmental language impairment, Corticostriatal loops, Procedural Deficit Hypothesis},
abstract = {The aim of the study was to examine reinforcement learning (RL) in young adults with developmental language impairment (DLI) within the context of a neurocomputational model of the basal ganglia-dopamine system (Frank, Seeberger, & O’Reilly, 2004). Two groups of young adults, one with DLI and the other without, were recruited. A probabilistic selection task was used to assess how participants implicitly extracted reinforcement history from the environment based on probabilistic positive/negative feedback. The findings showed impaired RL in individuals with DLI, indicating an altered gating function of the striatum in testing. However, they exploited similar learning strategies as comparison participants at the beginning of training, reflecting relatively intact functions of the prefrontal cortex to rapidly update reinforcement information. Within the context of Frank’s model, these results can be interpreted as evidence for alterations in the basal ganglia of individuals with DLI.}
}
@article{QIN2021103239,
title = {Optimizing matching time intervals for ride-hailing services using reinforcement learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {129},
pages = {103239},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103239},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21002527},
author = {Guoyang Qin and Qi Luo and Yafeng Yin and Jian Sun and Jieping Ye},
keywords = {Ride-hailing service, Online matching, Reinforcement learning, Policy gradient method},
abstract = {Matching trip requests and available drivers efficiently is considered a central operational problem for ride-hailing services. A widely adopted matching strategy is to accumulate a batch of potential passenger-driver matches and solve bipartite matching problems repeatedly. The efficiency of matching can be improved substantially if the matching is delayed by adaptively adjusting the matching time interval. The optimal delayed matching is subject to the trade-off between the delay penalty and the reduced wait cost and is dependent on the system’s supply and demand states. Searching for the optimal delayed matching policy is challenging, as the current policy is compounded with past actions. To this end, we tailor a family of reinforcement learning-based methods to overcome the curse of dimensionality and sparse reward issues. In addition, this work provides a solution to spatial partitioning balance between the state representation error and the optimality gap of asynchronous matching. Lastly, we examine the proposed methods with real-world taxi trajectory data and garner managerial insights into the general delayed matching policies. The focus of this work is single-ride service due to limited access to shared ride data, while the general framework can be extended to the setting with a ride-pooling component.}
}
@article{ZOU2021468,
title = {Dynamic multiobjective optimization driven by inverse reinforcement learning},
journal = {Information Sciences},
volume = {575},
pages = {468-484},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.06.054},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521006459},
author = {Fei Zou and Gary G. Yen and Chen Zhao},
keywords = {Dynamic multiobjective optimization, Machine learning, Inverse reinforcement learning, Evolutionary algorithm, Surrogate},
abstract = {Due to the widespread interest in dynamic multiobjective optimization in real-world applications, more and more approaches exploiting machine learning are deployed to tackle this type of problems. Unfortunately, recent works do not make full use of the data obtained during the optimization process, which could be benefit for model training thereby mining the dynamic characteristics of the underlying problem. To address this issue, this paper proposes a dynamic multiobjective evolutionary algorithm driven by inverse reinforcement learning to solve the dynamic multiobjective optimization problems. IRL is widely used to recover the unknown reward function, making it possible to perform at an expert level. The notable features of the proposed algorithm mainly consist of data-driven evolutionary technique, which uses inverse reinforcement learning as a surrogate-assisted model for model training. This design makes full use of the surrogate management strategy based on inverse reinforcement learning to optimize the reward function within a reinforcement learning framework. At the same time, the algorithm can generate a promising policy based on limited training data during the optimization process to achieve better algorithm evolution and guide the search. The experimental results on the benchmark problems validate that the proposed algorithm is effective in dealing with dynamic multiobjective optimization.}
}
@article{XU2024121258,
title = {SCA-MADRL: Multiagent deep reinforcement learning framework based on state classification and assignment for intelligent shield attitude control},
journal = {Expert Systems with Applications},
volume = {235},
pages = {121258},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121258},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423017608},
author = {Jin Xu and Jinfeng Bu and Na Qin and Deqing Huang},
keywords = {Consistent state space, Deep reinforcement learning, Multiagent learning, Shield attitude control, State classification and assignment},
abstract = {With the wide application of the shield tunneling method in tunnel engineering, the untimely and incorrect attitude control of shield systems has become an essential factor affecting the quality of shield tunneling. The use of multi-agent deep reinforcement learning (MADRL) algorithm to learn correct and timely shield attitude correction policies is beneficial for handling shield attitude control tasks which are complex and separable. However, the sensing data of the shield systems are complicated and widely distributed, which affects the learning efficiency of the agents. To resolve this problem, this paper proposes a MADRL framework based on state classification and assignment (SCA) for adaptive shield attitude control. SCA-MADRL uses a clustering algorithm to classify states and uses a planning model to make the agent's learning space a consistent state space so that each agent can learn efficiently in its own learning space. The state classification algorithm in the framework can improve the learning efficiency of the agent, and the proposed assignment algorithm can further enhance the learning effect when reusing the agent. This paper verifies the necessity and effectiveness of using task decomposition to implement MADRL for equipment control and provides a practical framework for automated and intelligent shield systems.}
}
@article{ZHANG201772,
title = {Social behavior study under pervasive social networking based on decentralized deep reinforcement learning},
journal = {Journal of Network and Computer Applications},
volume = {86},
pages = {72-81},
year = {2017},
note = {Special Issue on Pervasive Social Networking},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516302867},
author = {Yue Zhang and Bin Song and Peng Zhang},
keywords = {Decentralized deep reinforcement learning, Market models, Monopolistically competitive market, Pervasive social networking, Social behavior, Users' patterns},
abstract = {Pervasive social networking (PSN) provides instant social activities such as communications and gaming, which attracts a growing attention. Under this circumstance, the study and analysis of users’ behaviors has a profound meaning, which may be extended to other relevant fields. In this article, we define and quantify users’ patterns to study social behaviors, after discussing and reconsidering social characteristics in PSN. Meanwhile, we treat PSN as a market, based on the standpoint that data can be priced and tradable. After analyzing its market structure, we describe PSN as a monopolistically competitive market, which contains multiple providers selling specialized goods. Afterwards, we study the social behaviors in this market from an economic perspective, namely applying market models. Finally, decentralized deep reinforcement learning is proposed to estimate users’ patterns and to solve market models, the prisoner's dilemma and the Cournot model to be specific. Simulation results demonstrate the flexibility and efficiency of our algorithms.}
}
@article{DUGULEANA2016104,
title = {Neural networks based reinforcement learning for mobile robots obstacle avoidance},
journal = {Expert Systems with Applications},
volume = {62},
pages = {104-115},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.06.021},
url = {https://www.sciencedirect.com/science/article/pii/S0957417416303001},
author = {Mihai Duguleana and Gheorghe Mogan},
keywords = {Obstacle avoidance, Neural networks, Q-learning, Virtual reality},
abstract = {This study proposes a new approach for solving the problem of autonomous movement of robots in environments that contain both static and dynamic obstacles. The purpose of this research is to provide mobile robots a collision-free trajectory within an uncertain workspace which contains both stationary and moving entities. The developed solution uses Q-learning and a neural network planner to solve path planning problems. The algorithm presented proves to be effective in navigation scenarios where global information is available. The speed of the robot can be set prior to the computation of the trajectory, which provides a great advantage in time-constrained applications. The solution is deployed in both Virtual Reality (VR) for easier visualization and safer testing activities, and on a real mobile robot for experimental validation. The algorithm is compared with Powerbot's ARNL proprietary navigation algorithm. Results show that the proposed solution has a good conversion rate computed at a satisfying speed.}
}
@article{ESFAHANI20221,
title = {Policy Gradient Reinforcement Learning for Uncertain Polytopic LPV Systems based on MHE-MPC},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {15},
pages = {1-6},
year = {2022},
note = {6th IFAC Conference on Intelligent Control and Automation Sciences ICONS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.07.599},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322010102},
author = {Hossein Nejatbakhsh Esfahani and Sébastien Gros},
keywords = {Model Predictive Control, Moving Horizon Estimation, Reinforcement Learning, Polytopic LPV, Multi-Model Linear System},
abstract = {In this paper, we propose a learning-based Model Predictive Control (MPC) approach for the polytopic Linear Parameter-Varying (LPV) systems with inexact scheduling parameters (as exogenous signals with inexact bounds), where the Linear Time Invariant (LTI) models (vertices) captured by combinations of the scheduling parameters becomes wrong. We first propose to adopt a Moving Horizon Estimation (MHE) scheme to simultaneously estimate the convex combination vector and unmeasured states based on the observations and model matching error. To tackle the wrong LTI models used in both the MPC and MHE schemes, we then adopt a Policy Gradient (PG) Reinforcement Learning (RL) to learn both the estimator (MHE) and controller (MPC) so that the best closed-loop performance is achieved. The effectiveness of the proposed RL-based MHE/MPC design is demonstrated using an illustrative example.}
}
@article{BOTVINICK2009262,
title = {Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective},
journal = {Cognition},
volume = {113},
number = {3},
pages = {262-280},
year = {2009},
note = {Reinforcement learning and higher cognition},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2008.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0010027708002059},
author = {Matthew M. Botvinick and Yael Niv and Andew G. Barto},
keywords = {Reinforcement learning, Prefrontal cortex},
abstract = {Research on human and animal behavior has long emphasized its hierarchical structure—the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.}
}
@article{SONG2023108653,
title = {A reinforcement learning based job scheduling algorithm for heterogeneous computing environment},
journal = {Computers and Electrical Engineering},
volume = {107},
pages = {108653},
year = {2023},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2023.108653},
url = {https://www.sciencedirect.com/science/article/pii/S0045790623000782},
author = {Yutao Song and Chen Li and Lihua Tian and Hui Song},
keywords = {DAG scheduling, Heterogeneous computing environment, Graph convolution network, Reinforcement learning, Heuristics},
abstract = {Efficient job scheduling for heterogeneous computing environments has attracted widespread attention, jobs are usually modeled as directed acyclic graphs(DAG). Optimizing scheduling can improve system throughput. We propose a two-stage scheduling algorithm, which calculates task selection and processor allocation respectively. In task selection stage, we utilize a bidirectional graph convolution network to learn DAG structural features, and a fully-connected network to generate proper scheduling scheme. In the processor allocation stage, we propose a heuristic based on optimistic cost table(OCT) and task duplication, which trade-off scheduling allocation better. Experiments of various scheduling scenarios have been conducted, and the results show that the proposed algorithm has better scheduling performance than the compared heterogeneous DAG scheduling algorithms.}
}
@article{VARGASPEREZ2023644,
title = {Deep reinforcement learning in agent-based simulations for optimal media planning},
journal = {Information Fusion},
volume = {91},
pages = {644-664},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.10.029},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522002056},
author = {Víctor A. Vargas-Pérez and Pablo Mesejo and Manuel Chica and Oscar Cordón},
keywords = {Deep reinforcement learning, Deep Q-Network, Agent-based modeling, Media planning, Marketing},
abstract = {Agent-based models establish a suitable simulation technique to recreate real complex systems, such as those approached in marketing. Reinforcement learning is about learning a behavior policy in order to maximize a long-term reward signal. In this work, we develop a deep reinforcement learning agent that represents a brand in an agent-based model of a market. The goal of the learning agent is to obtain a marketing investment strategy that improves the awareness of its corresponding brand in the marketing scenario. In opposition to conventional marketing investment strategies, the learned strategy is dynamic, so the agent makes its investment decision on-line based on the current state of the market. We choose the Double Deep Q-Network algorithm to train this agent on diverse instances of the model, each of them with different knowledge levels of the brand. First we adjust a subset of the hyperparameters of Double Deep Q-Network on two of the model instances, and then we use the best configuration found to train the agent on all the available instances. The brand agent learns a dynamic policy that optimizes brand’s awareness levels. We perform an expert analysis of the policy obtained, where we observe that the learning brand agent tends to increase investment in media channels with greater awareness impact, but it also invests in other channels according to the situation and the characteristics of the model instance. These results show the benefits of having an on-line dynamic learning environment in a decision support system for media planning in marketing.}
}
@article{DARMON2005119,
title = {Convergence of reinforcement learning to Nash equilibrium: A search-market experiment},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {355},
number = {1},
pages = {119-130},
year = {2005},
note = {Market Dynamics and Quantitative Economics},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2005.02.074},
url = {https://www.sciencedirect.com/science/article/pii/S0378437105002839},
author = {Eric Darmon and Roger Waldeck},
keywords = {Reinforcement learning, Nash equilibrium, Search market, Agent-based modeling},
abstract = {Since the introduction of Reinforcement Learning (RL) in Game Theory, a growing literature is concerned with the theoretical convergence of RL-driven outcomes towards Nash equilibrium. In this paper, we apply this issue to a search-theoretic framework (posted-price market) where sellers are confronted with a population of imperfectly informed buyers and take one decision per period (posted prices) with no direct interactions between sellers. We focus on three different scenarios with varying buyers’ characteristics. For each of these scenarios, we quantitatively and qualitatively test whether the learned variable (price strategy) converges to the Nash equilibrium. We also study the impact of the temperature parameter (defining the exploitation/exploration trade off) on these results.}
}
@incollection{BARTO199135,
title = {On the Computational Economics of Reinforcement Learning},
editor = {David S. Touretzky and Jeffrey L. Elman and Terrence J. Sejnowski and Geoffrey E. Hinton},
booktitle = {Connectionist Models},
publisher = {Morgan Kaufmann},
pages = {35-44},
year = {1991},
isbn = {978-1-4832-1448-1},
doi = {https://doi.org/10.1016/B978-1-4832-1448-1.50010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978148321448150010X},
author = {Andrew G. Barto and Satinder Pal Singh},
abstract = {Following terminology used in adaptive control, we distinguish between indirect learning methods, which learn explicit models of the dynamic structure of the system to be controlled, and direct learning methods, which do not. We compare an existing indirect method, which uses a conventional dynamic programming algorithm, with a closely related direct reinforcement learning method by applying both methods to an infinite horizon Markov decision problem with unknown state-transition probabilities. The simulations show that although the direct method requires much less space and dramatically less computation per control action, its learning ability in this task is superior to, or compares favorably with, that of the more complex indirect method. Although these results do not address how the methods’ performances compare as problems become more difficult, they suggest that given a fixed amount of computational power available per control action, it may be better to use a direct reinforcement learning method augmented with indirect techniques than to devote all available resources to a computationally costly indirect method. Comprehensive answers to the questions raised by this study depend on many factors making up the economic context of the computation.}
}