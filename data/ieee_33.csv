"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Deep Reinforcement Learning for Optimization of RAN Slicing Relying on Control-and User-Plane Separation","H. Tu; L. Zhao; Y. Zhang; G. Zheng; C. Feng; S. Song; K. Liang","State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xi’an; College of Electronic Information Engineering, Hebei University, Baoding, China; Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, U.K.; School of Engineering, The University of British Columbia, Kelowna, Canada; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, Hong Kong; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China","IEEE Internet of Things Journal","","2023","PP","99","1","1","The rapid development of radio access network (RAN) slicing and control-and user-plane separation (CUPS) has created a new paradigm for future networks, namely CUPS-based RAN slicing. In this paper, we formulate the utility optimization problems of the CUPS-based RAN slicing system and propose a Lyapunov-based deep reinforcement learning (L-DRL) framework to solve them. Specifically, we propose that the CP and UP slices should control their respective power and subcarrier resources. Firstly, we provide coverage-driven slices in the CP for coverage control and data-driven slices in the UP for diverse user requests, where we consider the influence of coverage-driven slices on data-driven slices. Secondly, we define the system’s utilities as income minus cost, and we formulate the utility maximization problem of the UP as a mixed-integer nonlinear programming problem (MINLP), which is NP-hard because it considers both continuous actions (densities deployment and power allocation) and discrete action (subcarrier allocation). Furthermore, we design an alternating optimization method for the CP and UP based on the densities of deployment. Finally, we develop a novel L-DRL framework for mixed-action optimization problems and propose a specific Lyapunov-based asynchronous advantage actor-critic (L-A3C) algorithm. Simulation results demonstrate that our proposed L-A3C algorithm outperforms the standard A3C algorithm in terms of the convergence while achieving higher performance than Lyapunov optimization. Moreover, our proposed CUPS-based RAN slicing scheme surpasses the benchmark RAN slicing schemes in terms of the achievable rate and delay.","2327-4662","","10.1109/JIOT.2023.3320434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10266672","Radio access network slicing;control-and User-plane separation;Lyapunov optimization;A3C","Optimization;Resource management;Radio access networks;Base stations;Reinforcement learning;Network slicing;Deep learning","","","","","","","IEEE","28 Sep 2023","","","IEEE","IEEE Early Access Articles"
"A Balance Control Method for Wheeled Bipedal Robot Based on Reinforcement Learning","J. Zhang; B. Lu; Z. Liu; Y. Xie; W. Chen; W. Jiang","Institute of Robotics and Automatic Information System, Nankai University, Tianjin; Institute of Robotics and Automatic Information System, Nankai University, Tianjin; Institute of Robotics and Automatic Information System, Nankai University, Tianjin; Intelligent Manufacturing Department, CISDI Information Technology Co., Ltd, Chongqing; Revo Robotics Co., Ltd., Hangzhou; Revo Robotics Co., Ltd., Hangzhou","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","2288","2293","Wheeled bipedal robot (WBR) fully combines the advantages of wheeled robot and legged robot, which enables it to adapt to more complex terrains and complete more challenging tasks. Therefore, WBR has been widely researched in recent years. However, at the same time, due to its under-actuation, nonlinearity, and instability characteristics, the balance control of WBR is a difficult problem. In this paper, a reinforcement learning (RL) algorithm based on path integral is proposed. First, the relationship between path integral and stochastic optimal control is analyzed, and the numerical solution of stochastic dynamical systems is obtained. Second, the solution framework is extended to a parameter update strategy of RL. Since the matrix inversion and gradient calculation are avoided during the strategy update process, the training is accelerated. Third, a model-based Lyapunov method is introduced to parameterize the control variables, which not only ensures the stability of the system in training, but also guarantees good performance of the system when applying simulation results to actual system. Simulation results show that the proposed method has satisfactory balance control performance and can effectively resist external disturbances.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240160","National Natural Science Foundation of China(grant numbers:62203235,62203450,61873132); Natural Science Foundation of Tianjin(grant numbers:21JCQNJC00090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240160","Wheeled Bipedal Robot;Balance Control;Path Integral;Reinforcement Learning","Training;Legged locomotion;Simulation;Stochastic processes;Optimal control;Reinforcement learning;Stability analysis","","","","","","22","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Digital Twin Enhanced Federated Reinforcement Learning with Lightweight Knowledge Distillation in Mobile Networks","X. Zhou; X. Zheng; X. Cui; J. Shi; W. Liang; Z. Yan; L. T. Yang; S. Shimizu; K. I. -K. Wang","Faculty of Data Science, Shiga University, Hikone, Japan; School of Frontier Crossover Studies, Hunan University of Technology and Business, Changsha, China; School of Intelligent Engineering and Intelligent Manufacturing, Hunan University of Technology and Business, Changsha, China; School of Frontier Crossover Studies, Hunan University of Technology and Business, Changsha, China; Xiangjiang Laboratory, China; State Key Laboratory on Integrated Services Networks and the School of Cyber Engineering, Xidian University, Xi’an, China; Department of Computer Science, St. Francis Xavier University, Antigonish, NS, Canada; Faculty of Data Science, Shiga University, Hikone, Japan; Department of Electrical, Computer and Software Engineering, The University of Auckland, Auckland, New Zealand","IEEE Journal on Selected Areas in Communications","","2023","PP","99","1","1","The high-speed mobile networks offer great potentials to many future intelligent applications, such as autonomous vehicles in smart transportation systems. Such networks provide the possibility to interconnect mobile devices to achieve fast knowledge sharing for efficient collaborative learning and operations, especially with the help of distributed machine learning, e.g., Federated Learning (FL), and modern digital technologies, e.g., Digital Twin (DT) systems. Typically, FL requires a fixed group of participants that have Independent and Identically Distributed (IID) data for accurate and stable model training, which is highly unlikely in real-world mobile network scenarios. In this paper, in order to facilitate the lightweight model training and real-time processing in high-speed mobile networks, we design and introduce an end-edge-cloud structured three-layer Federated Reinforcement Learning (FRL) framework, incorporated with an edge-cloud structured DT system. A dual-Reinforcement Learning (dual-RL) scheme is devised to support optimizations of client node selection and global aggregation frequency during FL via a cooperative decision-making strategy, which is assisted by a two-layer DT system deployed in the edge-cloud for real-time monitoring of mobile devices and environment changes. A model pruning and federated bidirectional distillation (Bi-distillation) mechanism is then developed locally for the lightweight model training, while a model splitting scheme with a lightweight data augmentation mechanism is developed globally to separately optimize the aggregation weights based on a splitted neural network structure (i.e., the encoder and classifier) in a more targeted manner, which can work together to effectively reduce the overall communication cost and improve the non-IID problem. Experiment and evaluation results compared with three baseline methods using two different real-world datasets demonstrate the usefulness and outstanding performance of our proposed FRL model in communication-efficient model training and non-IID issue alleviation for high-speed mobile network scenarios.","1558-0008","","10.1109/JSAC.2023.3310046","Grants-in-Aid for Scientific Research (C) from Japan Society for the Promotion of Science(grant numbers:23K11064); National Natural Science Foundation of China(grant numbers:62072171,72091515); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10269106","Federated Learning;Reinforcement Learning;Digital Twin;Knowledge Distillation;Lightweight Training;Mobile Networks","Training;Data models;Real-time systems;Reinforcement learning;Knowledge engineering;Semantics;Mobile handsets","","","","","","","IEEE","3 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Towards Optimal Energy Management Strategy for Hybrid Electric Vehicle with Reinforcement Learning","X. Wu; E. Wedernikow; C. Nitsche; M. F. Huber","Department Cyber Cognitive Intelligence (CCI), Fraunhofer IPA; Department Cyber Cognitive Intelligence (CCI), Fraunhofer IPA; Department Cyber Cognitive Intelligence (CCI), Fraunhofer IPA; Department Cyber Cognitive Intelligence (CCI), Fraunhofer IPA","2023 IEEE Intelligent Vehicles Symposium (IV)","27 Jul 2023","2023","","","1","7","In recent years, the development of Artificial Intelligence (AI) has shown tremendous potential in diverse areas. Among them, reinforcement learning (RL) has proven to be an effective solution for learning intelligent control strategies. As an inevitable trend for mitigating climate change, hybrid electric vehicles (HEVs) rely on efficient energy management strategies (EMS) to minimize energy consumption. Many researchers have employed RL to learn optimal EMS for specific vehicle models. However, most of these models tend to be complex and proprietary, making them unsuitable for broad applicability. This paper presents a novel framework, in which we implement and integrate RL-based EMS with the open-source vehicle simulation tool called FASTSim. The learned RL-based EMSs are evaluated on various vehicle models using different test drive cycles and prove to be effective in improving energy efficiency.","2642-7214","979-8-3503-4691-6","10.1109/IV55152.2023.10186787","Ministry of Economic Affairs; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10186787","","Training;Energy consumption;Systematics;Intelligent vehicles;Reinforcement learning;Market research;Energy efficiency","artificial intelligence;energy conservation;energy management systems;hybrid electric vehicles;intelligent control;power engineering computing;reinforcement learning","artificial intelligence;EMS;energy consumption;FASTSim;HEV;hybrid electric vehicle;intelligent control strategies;open-source vehicle simulation tool;optimal energy management strategy;reinforcement learning","","","","40","IEEE","27 Jul 2023","","","IEEE","IEEE Conferences"
"Configuration of the Actor and Critic Network of the Deep Reinforcement Learning controller for Multi-Energy Storage System","P. Páramo-Balsa; F. Gonzalez-Longatt; M. N. Acosta; J. L. R. Torres; P. Palensky; F. Sanchez; J. M. Roldán-Fernández; M. Burgos-Payán","Department of Electrical Engineering, Universidad de Sevilla, Seville, Spain; Department of Electrical Engineering, Information Technology and Cybernetics, University of South-Eastern Norway, Porsgrunn, Norway; Department of Electrical Engineering, Information Technology and Cybernetics, University of South-Eastern Norway, Porsgrunn, Norway; Department of Electrical Sustainable Energy, Delft University of Technology, Delft, The Netherlands; Department of Electrical Sustainable Energy, Delft University of Technology, Delft, The Netherlands; Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, UK; Department of Electrical Sustainable Energy, Delft University of Technology, Delft, The Netherlands; Department of Electrical Sustainable Energy, Delft University of Technology, Delft, The Netherlands","2022 4th Global Power, Energy and Communication Conference (GPECOM)","11 Jul 2022","2022","","","564","568","The computational burden and the time required to train a deep reinforcement learning (DRL) can be appreciable, especially for the particular case of a DRL control used for frequency control of multi-electrical energy storage (MEESS). This paper presents an assessment of four training configurations of the actor and critic network to determine the configuration training that produces the lower computational time, considering the specific case of frequency control of MEESS. The training configuration cases are defined considering two processing units: CPU and GPU and are evaluated considering serial and parallel computing using MATLAB® 2020b Parallel Computing Toolbox. The agent used for this assessment is the Deep Deterministic Policy Gradient (DDPG) agent. The environment represents the dynamic model to provide enhanced frequency response to the power system by controlling the state of charge of energy storage systems. Simulation results demonstrated that the best configuration to reduce the computational time is training both actor and critic network on CPU using parallel computing.","","978-1-6654-6925-8","10.1109/GPECOM55404.2022.9815793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815793","actor-network;critic network;deep reinforcement learning;energy storage systems;enhanced frequency response;parallel computing","Training;Economics;Computational modeling;Power system dynamics;Graphics processing units;Reinforcement learning;Parallel processing","control engineering computing;deep learning (artificial intelligence);energy storage;frequency control;frequency response;gradient methods;learning (artificial intelligence);parallel processing;power engineering computing;reinforcement learning","deep reinforcement learning controller;multienergy storage system;DRL control;frequency control;multielectrical energy storage;MEESS;critic network;configuration training;parallel computing;deep deterministic policy gradient agent;enhanced frequency response;energy storage systems;actor network","","","","15","IEEE","11 Jul 2022","","","IEEE","IEEE Conferences"
"Deep reinforcement learning based dynamic power allocation for uplink device-to-device enabled cell-free network","G. Xia; Y. Zhang; L. Ge; H. Zhou","School of engineering University of Leicester, Leicester, UK; School of engineering University of Leicester, Leicester, UK; Wolfson school of Mechanical, Electrical and Manufacturing Engineering Loughborough University, Leicester, UK; School of computing and mathematical science University of Leicester, Leicester, UK","2022 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","25 Jul 2022","2022","","","01","06","In this paper, we consider device-to-device enabled uplink cell-free communication between external users with the base station. By exploiting the channel gain differences, external and cellular users are multiplexed into the transmission power domain and then non-orthogonally scheduled for transmission with the same spectrum resources. Successive interference can-cellation is then applied at the base station to decode the message signals. We introduce an effective deep reinforcement learning (DRL) scheme to optimise the worst-case user rate through the dynamic power allocation of both external and cellular users. We also compare the performance of the DRL scheme under zero-forcing beamforming and conjugate beamforming methods. Simulation results verify the effectiveness of the DRL method for guaranteeing the user fairness through the worst-case rate maximisation.","2155-5052","978-1-6654-6901-2","10.1109/BMSB55706.2022.9828568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9828568","Cell-free network;worst-case rate maximisation;uplink beamforming;power allocation;deep reinforcement learning","Performance evaluation;Multiplexing;Base stations;Array signal processing;Simulation;Reinforcement learning;Dynamic scheduling","array signal processing;cellular radio;decoding;interference suppression;learning (artificial intelligence);mobile computing;multi-access systems;radiofrequency interference;scheduling;telecommunication scheduling;wireless channels","interference cancellation;base station;deep reinforcement learning scheme;worst-case user rate;dynamic power allocation;cellular users;worst-case rate maximisation;uplink device-to-device;cell-free network;uplink cell-free communication;channel gain differences;transmission power domain;spectrum resources;uplink device-to-device enabled cell-free network;zero-forcing beamforming;conjugate beamforming methods","","","","21","IEEE","25 Jul 2022","","","IEEE","IEEE Conferences"
"A Computational Framework for Robot Hand Design via Reinforcement Learning","Z. Zhang; Y. Zheng; Z. Hu; L. Liu; X. Zhao; X. Li; J. Pan","Department of Biomedical Engineering, City University of Hong Kong; Tencent Robotics X, Shenzhen, China; Department of Biomedical Engineering, City University of Hong Kong; Faculty of Intelligent Manufacturing, Wuyi University; Department of Biomedical Engineering, City University of Hong Kong; Tencent Robotics X, Shenzhen, China; Department of Computer Science, The University of Hong Kong","2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","16 Dec 2021","2021","","","7216","7222","Robot hand is essential for a fully functional robot and designing a good robot hand is a sophisticated job that challenges the designer’s knowledge and experience. This paper presents a computational framework for automatic optimal robot hand design based on reinforcement learning (RL), which considers desired grasping tasks, grasp control strategies, and performance quality measures altogether. The RL-based framework intends to grow finger joints with different types and link lengths at different positions from null. Then, the reward function for such a growing action is defined in terms of quality indexes of the generated robot hand to perform desired grasping tasks under expected control strategies. To demonstrate the effectiveness of this framework, in this paper we set the desired task to simply grasping objects of three primitive shapes (i.e., box, cylinder, and sphere) with predefined hand positions and strategies to close fingers to achieve grasps for each object. The force closure condition, quantitative stability indexes, and energy consumption of grasps as well as some penalty terms are used to assemble the reward function. Through simulation and practical prototype experiments, we show that capable robot hands can be automatically generated by the proposed framework. Potential factors that affect the output of the framework and deserve further exploration are also discussed.","2153-0866","978-1-6654-1714-3","10.1109/IROS51168.2021.9636305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636305","","Shape;Force;Prototypes;Grasping;Reinforcement learning;Stability analysis;Indexes","dexterous manipulators;energy consumption;grippers;learning (artificial intelligence);mobile robots","computational framework;automatic optimal robot hand design;reinforcement learning;performance quality measures;RL-based framework;finger joints;link lengths;reward function;quality indexes;predefined hand positions;fully functional robot;grasping tasks;grasp control strategies;force closure condition;quantitative stability indexes;grasp energy consumption;penalty terms","","","","30","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning With Data Envelopment Analysis and Conditional Value-At-Risk for the Capacity Expansion Problem","C. -Y. Lee; Y. -W. Chen","Department of Information Management, National Taiwan University, Taipei, Taiwan; Institute of Manufacturing Information and Systems, National Cheng Kung University, Tainan, Taiwan","IEEE Transactions on Engineering Management","","2023","PP","99","1","12","The capacity expansion problem is solved by accurately measuring the existing demand-supply mismatch and controlling the emissions output, considering multiple objectives, specific constraints, resource diversity, and resource allocation. This article proposes a reinforcement learning (RL) framework embedded with data envelopment analysis (DEA) to generate the optimal policy and guide the productivity improvement. The proposed framework uses DEA to evaluate efficiency and effectiveness for reward estimation in RL, and also assesses conditional value-at-risk to characterize the risk-averse capacity decision. Instead of focusing on short-term fluctuations in demand, RL optimizes the expected future reward with sequential capacity decisions over time. An empirical study of U.S. power generation validates the proposed framework and provides the managerial implications to policy makers. The results show that the RL agent can successfully learn the optimal policy through observing the interactions between the agent and the environment, and suggest the capacity adjustment that can improve efficiency by 8.3% and effectiveness by 0.9%. We conclude that RL complements productivity analysis, and emphasizes ex-ante planning over ex-post evaluation.","1558-0040","","10.1109/TEM.2023.3264566","Ministry of Science and Technology of Taiwan(grant numbers:MOST111-2628-E-002-019-MY3); 2021 Seed Research Grant from College of Management; National Taiwan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10104118","Capacity expansion;conditional value-at-risk (CVAR);data envelopment analysis (DEA);efficiency and effectiveness measure;reinforcement learning (RL);risk-averse decision","Productivity;Costs;Indexes;Capacity planning;Power generation;Uncertainty;Optimization","","","","","","","IEEE","18 Apr 2023","","","IEEE","IEEE Early Access Articles"
"Multitask Neuroevolution for Reinforcement Learning With Long and Short Episodes","N. Zhang; A. Gupta; Z. Chen; Y. -S. Ong","School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; Agency for Science, Technology and Research, Singapore Institute of Manufacturing Technology, 2 Fusionopolis Way, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; Data Science and Artificial Intelligence Research Centre, School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore","IEEE Transactions on Cognitive and Developmental Systems","7 Sep 2023","2023","15","3","1474","1486","Studies have shown evolution strategies (ES) to be a promising approach for reinforcement learning (RL) with deep neural networks. However, the issue of high sample complexity persists in applications of ES to deep RL over long horizons. This article is the first to address the shortcoming of today’s methods via a novel neuroevolutionary multitasking (NuEMT) algorithm, designed to transfer information from a set of auxiliary tasks (of short episode length) to the target (full length) RL task at hand. The auxiliary tasks, extracted from the target, allow an agent to update and quickly evaluate policies on shorter time horizons. The evolved skills are then transferred to guide the longer and harder task toward an optimal policy. We demonstrate that the NuEMT algorithm achieves data-efficient evolutionary RL, reducing expensive agent-environment interaction data requirements. Our key algorithmic contribution in this setting is to introduce a first multitask skills transfer mechanism based on the statistical importance sampling technique. In addition, an adaptive resource allocation strategy is utilized to assign computational resources to auxiliary tasks based on their gleaned usefulness. Experiments on a range of continuous control tasks from the OpenAI Gym confirm that our proposed algorithm is efficient compared to recent ES baselines.","2379-8939","","10.1109/TCDS.2022.3221805","Data Science and Artificial Intelligence Research Center (DSAIR), School of Computer Science and Engineering, Nanyang Technological University; A*STAR AI3 Seed Grant(grant numbers:C211118016); A*STAR RIE2020 IAF-PP(grant numbers:A19C1a0018); A*Star Center for Frontier AI Research; National Natural Science Foundation of China(grant numbers:62206313); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950429","Evolution strategies (ESs);evolutionary multi-tasking (EMT);reinforcement learning (RL)","Task analysis;Genetic algorithms;Multitasking;Monte Carlo methods;Computer science;Complexity theory;Neural networks","","","","","","47","IEEE","14 Nov 2022","","","IEEE","IEEE Journals"
"Secure Task Offloading in Blockchain-Enabled Mobile Edge Computing With Deep Reinforcement Learning","A. Samy; I. A. Elgendy; H. Yu; W. Zhang; H. Zhang","School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; Department of Computer Science, Faculty of Computers and Information, Menoufia University, Shibin El Kom, Egypt; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Network and Service Management","1 Feb 2023","2022","19","4","4872","4887","Mobile Edge Computing (MEC) is a promising and fast-developing paradigm that provides cloud services at the edge of the network. MEC enables IoT devices to offload and execute their real-time applications at the proximity of these devices with low latency. Such applications include efficient manufacture inspection, virtual/augmented reality, image recognition, Internet of Vehicles (IoV), and e-Health. However, task offloading experiences security and privacy attacks such as data tampering, private data leakage, data replication, etc. To this end, in this paper, we propose a new blockchain-based framework for secure task offloading in MEC systems with guaranteed performance in terms of execution delay and energy consumption. First, blockchain technology is introduced as a platform to achieve data confidentiality, integrity, authentication, and privacy of task offloading in MEC. Second, we formulate an integration model of resource allocation and task offloading for a multi-user with multi-task MEC systems to optimize the energy and time cost. This is an NP-hard problem because of the curse-of-dimensionality and dynamic characteristics challenges of the considered scenario. Therefore, a deep reinforcement learning-based algorithm is developed to derive the close-optimal task offloading decision efficiently. Theoretical analysis and experimental results demonstrate that the proposed framework is resilient to several task offloading security attacks and it can save about 22.2% and 19.4% of system consumption with respect to the local and edge execution scenarios. Moreover, the benchmark analysis proves that the framework consumes few resources in terms of memory and disk usage, CPU utilization, and transaction throughput.","1932-4537","","10.1109/TNSM.2022.3190493","National Natural Science Foundation of China (NSFC)(grant numbers:62172123,61732022,61872110); National Key Research and Development Program of China(grant numbers:2020YFB1406902); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101360001); Shenzhen Science and Technology Research and Development Foundation(grant numbers:JCYJ20190806143418198); Peng Cheng Laboratory Project(grant numbers:PCL2021A02); Fundamental Research Funds for the Central Universities(grant numbers:HIT.OCEF.2021007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831127","Blockchain;mobile edge computing;task offloading;security;privacy;deep reinforcement learning","Task analysis;Blockchains;Internet of Things;Peer-to-peer computing;Security;Hafnium;Privacy","augmented reality;blockchains;cloud computing;computer crime;data privacy;deep learning (artificial intelligence);edge computing;image recognition;Internet of Things;mobile computing;reinforcement learning;resource allocation;telecommunication computing;vehicular ad hoc networks","blockchain technology;blockchain-based framework;blockchain-enabled mobile edge computing;close-optimal task offloading decision;cloud services;data replication;data tampering;deep reinforcement learning-based algorithm;e-health;image recognition;integration model;Internet of Vehicles;IoV;manufacture inspection;multitask MEC systems;NP-hard problem;private data leakage;resource allocation;secure task offloading;task offloading security attacks;virtual/augmented reality","","6","","49","IEEE","15 Jul 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning for Resource Allocation in Steerable Laser-Based Optical Wireless Systems","A. S. Elgamal; O. Z. Aletri; A. A. Qidan; T. E. H. El-Gorashi; J. M. H. Elmirghani","School of Electronics and Electrical Engineering, University of Leeds, Leeds, United Kingdom; School of Electronics and Electrical Engineering, University of Leeds, Leeds, United Kingdom; School of Electronics and Electrical Engineering, University of Leeds, Leeds, United Kingdom; School of Electronics and Electrical Engineering, University of Leeds, Leeds, United Kingdom; School of Electronics and Electrical Engineering, University of Leeds, Leeds, United Kingdom","2021 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)","26 Oct 2021","2021","","","1","6","Vertical Cavity Surface Emitting Lasers (VCSELs) have demonstrated suitability for data transmission in indoor optical wireless communication (OWC) systems due to the high modulation bandwidth and low manufacturing cost of these sources. Specifically, resource allocation is one of the major challenges that can affect the performance of multi-user optical wireless systems. In this paper, an optimisation problem is formulated to optimally assign each user to an optical access point (AP) composed of multiple VCSELs within a VCSEL array at a certain time to maximise the signal to interference plus noise ratio (SINR). In this context, a mixed-integer linear programming (MILP) model is introduced to solve this optimisation problem. Despite the optimality of the MILP model, it is considered impractical due to its high complexity, high memory and full system information requirements. Therefore, reinforcement Learning (RL) is considered, which recently has been widely investigated as a practical solution for various optimisation problems in cellular networks due to its ability to interact with environments with no previous experience. In particular, a Q-learning (QL) algorithm is investigated to perform resource management in a steerable VCSEL-based OWC systems. The results demonstrate the ability of the QL algorithm to achieve optimal solutions close to the MILP model. Moreover, the adoption of beam steering, using holograms implemented by exploiting liquid crystal devices, results in further enhancement in the performance of the network considered.","2576-7046","978-1-6654-4864-2","10.1109/CCECE53047.2021.9569123","Engineering and Physical Sciences Research Council(grant numbers:EP/H040536/1,EP/K016873/1,EP/S016570/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9569123","VCSEL;OWC;resource allocation;MILP;reinforcement learning","Wireless communication;Performance evaluation;Stimulated emission;Interference;Reinforcement learning;Resource management;Optical modulation","indoor communication;integer programming;linear programming;multi-access systems;reinforcement learning;resource allocation;surface emitting lasers;telecommunication computing","reinforcement learning;resource allocation;steerable laser-based optical wireless systems;vertical cavity surface emitting lasers;data transmission;indoor optical wireless communication systems;high modulation bandwidth;low manufacturing cost;multiuser optical wireless systems;optimisation problem;optical access point;VCSEL array;interference plus noise ratio;mixed-integer linear programming model;MILP model;system information requirements;Q-learning algorithm;resource management;steerable VCSEL-based OWC systems","","6","","48","IEEE","26 Oct 2021","","","IEEE","IEEE Conferences"
"A Curriculum Framework for Autonomous Network Defense using Multi-agent Reinforcement Learning","R. G. Campbell; M. Eirinaki; Y. Park","Computer Engineering Department, San José State University (SJSU), San José, CA, USA; Computer Engineering Department, San José State University (SJSU), San José, CA, USA; Computer Engineering Department, San José State University (SJSU), San José, CA, USA","2023 Silicon Valley Cybersecurity Conference (SVCC)","30 Jun 2023","2023","","","1","8","Early threat detection is an increasing part of the cybersecurity landscape given the growing scale and scope of cyberattacks in the recent years. Increasing exploitation of software vulnerabilities, especially in the manufacturing sector, demonstrates the ongoing need for autonomous network defense. In this work, we model the problem as a zero-sum Markov game between an attacker and defender reinforcement learning agents. Previous methods test their approach on a single topology or limit the agents to a subset of the network. However, real world networks are rarely fixed and often add or remove hosts based on demand, link failures, outages, or other factors. We consider two types of topologies: static topologies that remain fixed throughout training and a dynamic topology curriculum. The proposed robust training curriculum incorporates network topologies to build more general, capable agents. We also use Proximal Policy optimization (PPO) which offers a good balance of computational complexity and convergence speed. We evaluate various threat scenarios in terms of the exploitability and impact and conclude that the curriculum improves the defender’s win rate over training on a static topology by exposing the agent to more challenging environments over time.","","979-8-3503-2157-9","10.1109/SVCC56964.2023.10165310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10165310","Multi-agent reinforcement learning;MARL;curriculum learning;self-play;autonomous network defense;cybersecurity","Training;Autonomous networks;Network topology;Games;Reinforcement learning;Markov processes;Software","computational complexity;computer crime;computer network security;game theory;learning (artificial intelligence);Markov processes;multi-agent systems;optimisation;reinforcement learning;security of data;telecommunication network reliability","autonomous network defense;capable agents;curriculum framework;cybersecurity landscape;defender reinforcement learning agents;dynamic topology curriculum;early threat detection;general agents;growing scale;increasing exploitation;increasing part;manufacturing sector;multiagent reinforcement learning;network topologies;robust training curriculum;single topology;software vulnerabilities;static topology;zero-sum Markov game","","","","28","IEEE","30 Jun 2023","","","IEEE","IEEE Conferences"
"Simulation-to-Reality Transfer of a Two-Stage Deep Reinforcement Learning Controller for Autonomous Load Carrier Approaching","S. Hadwiger; V. Lavrik; L. X. Liao; T. Meisen","Siemens AG, 90475 Nuremberg, Germany; Siemens AG, 90475 Nuremberg, Germany; Siemens AG, 100102 Beijing, China; Institute for Technologies and Management of Digital Transformation, University of Wuppertal, Germany","2023 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)","25 May 2023","2023","","","232","238","Nowadays, transportation tasks in logistic and manufacturing are commonly performed by Automated Guided Vehicles (AGV). To enhance the driving and navigation capabilities of these AGVs agents based on Deep Reinforcement Learning (DRL) are extensively researched. Such agents are usually trained in an end-to-end fashion in simulations. However, when being placed in a real environment these agents often perform poorly. This effect is caused by the gap between simulation and reality, the so-called simulation-to-reality (sim-to-real) gap. In this work we mitigate this gap for a state-of-the-art DRL method designed to approach freely positioned load carriers based on RGB image data [1]. To study the transfer from simulation to reality, we utilize a monofork AGV, which is typically used in an industrial environment to transport load carriers. We show that introducing Domain Randomization (DR) techniques into the training enables a successful mitigation of the gap mentioned above. Additionally, we compensate the influence of an inaccurately placed camera, extending the used method, and show that it is applicable to other types of load carriers.","2573-9387","979-8-3503-0121-2","10.1109/ICARSC58346.2023.10129618","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129618","Deep Reinforcement Learning;Sim-to-Real;Domain Randomization;Vision-Based Navigation","Deep learning;Training;Remotely guided vehicles;Navigation;Robot vision systems;Transportation;Reinforcement learning","automatic guided vehicles;control engineering computing;deep learning (artificial intelligence);image colour analysis;learning (artificial intelligence);multi-agent systems;reinforcement learning","AGVs agents;Automated Guided Vehicles;autonomous load carrier;Domain Randomization techniques;freely positioned load carriers;inaccurately placed camera;industrial environment;logistic manufacturing;monofork AGV;navigation capabilities;RGB image data;simulation-to-reality transfer;state-of-the-art DRL method;successful mitigation;transportation tasks;two-stage Deep Reinforcement","","","","23","IEEE","25 May 2023","","","IEEE","IEEE Conferences"
"6G Deterministic Network Technology Based on Hierarchical Reinforcement Learning Framework","Y. Xing; L. Yang; X. Hu; C. Mei; H. Wang; J. Li","China Telecom Corporation Limited Research Institute, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; China Telecom Corporation Limited Research Institute, Beijing, China; China Telecom Corporation Limited Research Institute, Beijing, China; China Telecom Corporation Limited Research Institute, Beijing, China","2023 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","16 Aug 2023","2023","","","1","6","With the rapid development of mobile networks, various IoT services are booming, and these services have different characteristics and requirements. Among them, services such as remote surgery and intelligent manufacturing are sensitive to delay and have high requirements for delay certainty. In order to meet the needs of delay-sensitive services, 3GPP defined the technical solution of 5G network as a TSN bridge in version R16 to support deterministic services of mobile networks. However, this solution has technical shortcomings, for example, it does not support aperiodic service scheduling, and does not define a coordination mechanism between applications and networks. Therefore, this paper sorts out the promotion process of 3GPP supporting mobile deterministic networks, clarifies the reasons for the existence of problems, and provides a GCN-based hierarchical reinforcement learning model to flexibly handle dynamic and diverse traffic for 6G networks. This model realizes joint scheduling of periodic services and aperiodic services and makes full use of the characteristics of the slow change of the whole traffic and the rapid change of individuals in the network to update sub-models with different frequencies and ensures the deterministic delay.","2155-5052","979-8-3503-2152-4","10.1109/BMSB58369.2023.10211210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10211210","deterministic network;6G;5G TSC;reinforcement learning","6G mobile communication;Bridges;Wide area networks;Job shop scheduling;5G mobile communication;Network slicing;Collaboration","3G mobile communication;5G mobile communication;6G mobile communication;Internet of Things;reinforcement learning;scheduling;telecommunication computing;telecommunication network management","3GPP;6G deterministic network technology;aperiodic service scheduling;delay certainty;delay-sensitive services;deterministic delay;GCN-based hierarchical reinforcement learning model;hierarchical reinforcement learning framework;intelligent manufacturing;IoT services;mobile deterministic networks;mobile networks;remote surgery;TSN bridge","","","","12","IEEE","16 Aug 2023","","","IEEE","IEEE Conferences"
"Real-Time Decision Support with Reinforcement Learning for Dynamic Flowshop Scheduling","J. Wang; S. Qu; J. Wang; J. O. Leckie; R. Xu",NA; NA; NA; NA; NA,"Smart SysTech 2017; European Conference on Smart Objects, Systems and Technologies","26 Oct 2017","2017","","","1","9","The dynamic flowshop scheduling problem has attracted a lot of attention from both academia and industry because of its nature of NP-hardness in computation on the one hand, and its great value for optimization of manufacturing systems on the other. Traditionally, effective heuristic methods have been widely adopted in industry to solve the problem. In recent years, machine learning has also exhibited great potential in the field. In this paper, we propose a reinforcement learning approach to this problem. We discuss settings for orders, performance measurements, and learning methods in detail to construct a controlled environment for this research. We then establish a manufacturing simulation system to compare the performance of the reinforcement learning approach and three heuristic approaches. While the experimental results revealed the superiority of the reinforcement learning method, investigations into dispatching decisions exposed its limitations in actual industry applications. Hence, two strategies were designed and employed to improve the reinforcement learning approach. After validating the reinforcement learning method with the improved strategies, we summarized and presented the dispatching rules of the reinforcement learning method as one type of complementary decision for supporting real-time flowshop scheduling problems.","","978-3-8007-4428-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8084561","","","","","","","","","","26 Oct 2017","","","VDE","VDE Conferences"
"Reinforcement learning for high-level fuzzy Petri nets","V. R. L. Shen","Department of Computer Science and Information Engineering, National Huwei Institute of Technology Huwei, Yulan, Taiwan","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","20 Mar 2003","2003","33","2","351","362","The author has developed a reinforcement learning algorithm for the high-level fuzzy Petri net (HLFPN) models in order to perform structure and parameter learning simultaneously. In addition to the HLFPN itself, the difference and similarity among a variety of subclasses concerning Petri nets are also discussed. As compared with the fuzzy adaptive learning control network (FALCON), the HLFPN model preserves the advantages that: 1) it offers more flexible learning capability because it is able to model both IF-THEN and IF-THEN-ELSE rules; 2) it allows multiple heterogeneous outputs to be drawn if they exist; 3) it offers a more compact data structure for fuzzy production rules so as to save information storage; and 4) it is able to learn faster due to its structural reduction. Finally, main results are presented in the form of seven propositions and supported by some experiments.","1941-0492","","10.1109/TSMCB.2003.810448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1187445","","Learning;Petri nets;Fuzzy control;Fuzzy systems;Fuzzy sets;Feedback;Automatic control;Fuzzy logic;Neurons;Fires","learning (artificial intelligence);fuzzy logic;Petri nets;knowledge representation","high-level fuzzy Petri nets;reinforcement learning;HLFPN model;fuzzy production rules;structural reduction;knowledge representation;serial structures;parallel structures","","41","","26","IEEE","20 Mar 2003","","","IEEE","IEEE Journals"
"Coordination of PV Smart Inverters Using Deep Reinforcement Learning for Grid Voltage Regulation","C. Li; C. Jin; R. Sharma","Center for Energy Research, University of California, San Diego La Jolla, CA, USA; Energy Management, NEC Laboratories America, San Jose, CA, USA; Energy Management, NEC Laboratories America, San Jose, CA, USA","2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)","17 Feb 2020","2019","","","1930","1937","Increasing adoption of solar photovoltaic (PV) presents new challenges to modern power grid due to its variable and intermittent nature. Fluctuating outputs from PV generation can cause the grid violating voltage operation limits. PV smart inverters (SIs) provide a fast-response method to regulate voltage by modulating real and/or reactive power at the connection point. Yet existing local autonomous control scheme of SIs is based on local information without coordination, which can lead to suboptimal performance. In this paper, a deep reinforcement learning (DRL) based algorithm is developed and implemented for coordinating multiple SIs. The reward scheme of the DRL is carefully designed to ensure voltage operation limits of the grid are met with more effective utilization of SI reactive power. The proposed DRL agent for voltage control can learn its policy through interaction with massive offline simulations, and adapts to load and solar variations. The performance of the DRL agent is compared against the local autonomous control on the IEEE 37 node system with thousands of scenarios. The results show a properly trained DRL agent can intelligently coordinate different SIs for maintaining grid voltage within allowable ranges, achieving reduction of PV production curtailment, and decreasing system losses.","","978-1-7281-4550-1","10.1109/ICMLA.2019.00310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999296","Artificial-Intelligence;Distribution-Power-Grid;Deep-Reinforcement-Learning;Photovoltaics;Voltage-Regulation","Silicon;Voltage control;Reactive power;Inverters;Capacitors;Power grids;Machine learning","invertors;learning (artificial intelligence);photovoltaic power systems;power engineering computing;power generation control;power grids;reactive power control;solar power stations;voltage control","deep reinforcement learning based algorithm;SI reactive power;voltage control;DRL agent;PV production curtailment;PV smart inverters;grid voltage regulation;solar photovoltaic;power grid;PV generation;fast-response method;local autonomous control scheme;IEEE 37 node system","","23","","28","IEEE","17 Feb 2020","","","IEEE","IEEE Conferences"
"Generative Adversarial Network-Based Transfer Reinforcement Learning for Routing With Prior Knowledge","T. Dong; Q. Qi; J. Wang; A. X. Liu; H. Sun; Z. Zhuang; J. Liao","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Department of Computer Science and Technology, Qilu University of Technology, Jinan, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Network and Service Management","10 Jun 2021","2021","18","2","1673","1689","With the incremental deployment of software defined networking, the routing algorithms have gained more power on observability and controllability. Deep reinforcement learning, as an experience-driven approach, shows considerable potential in routing problem with the help of the centralized controller. It is an adaptive, lightweight, and model-free approach to coping with dynamic runtime status, large-scale traffic, and heterogeneous objective of SDN routing. However, it is still not suitable for the variable and complex emerging networks, because the huge training cost prevents fast convergence in a varying or discrepant environment. In this paper, we propose a transfer reinforcement learning algorithm to improve the training efficiency, and handle the variation in network status and topology. Specifically, we leverage the generative adversarial network to learn domain-invariant features that is suitable for deep reinforcement learning-based routing in different network environments. This mechanism utilizes the previous model and accelerates the training process. We implement our routing algorithm in the production level software switches and controller, while evaluating it comprehensively with many topologies and network status distributions. The experimental results show that our work not only outperforms the state-of-the-art deep reinforcement learning-based routing frameworks, but also has more training efficiency than the naive transfer learning algorithm both on different topologies and network status distributions.","1932-4537","","10.1109/TNSM.2021.3077249","National Key R&D Program of China(grant numbers:2020YFB1807803,2020YFB1807800); National Natural Science Foundation of China(grant numbers:62071067,62001054,61872082,61771068,61472184); National Postdoctoral Program for Innovative Talents(grant numbers:BX20200067); Ministry of Education and China Mobile Joint Fund(grant numbers:MCM20180101); Beijing University of Posts and Telecommunications-China Mobile Research Institute Joint Innovation Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9422771","Routing;transfer reinforcement learning;generative adversarial network;software defined networking","Routing;Training;Network topology;Topology;Neural networks;Convergence;Reinforcement learning","centralised control;deep learning (artificial intelligence);software defined networking;telecommunication control;telecommunication network routing","transfer reinforcement learning;deep reinforcement learning-based routing frameworks;production level software switches;training process;training cost;SDN routing;dynamic runtime status;lightweight model-free approach;adaptive model-free approach;centralized controller;routing problem;experience-driven approach;routing algorithm;software defined networking;incremental deployment;generative adversarial network;network status distributions;naive transfer learning algorithm","","7","","67","IEEE","4 May 2021","","","IEEE","IEEE Journals"
"An Energy Management System at the Edge based on Reinforcement Learning","F. Cicirelli; A. F. Gentile; E. Greco; A. Guerrieri; G. Spezzano; A. Vinci","National Research Council of Italy - Institute for High Performance Computing and Networking (ICAR), Rende, CS, Italy; National Research Council of Italy - Institute for High Performance Computing and Networking (ICAR), Rende, CS, Italy; National Research Council of Italy - Institute for High Performance Computing and Networking (ICAR), Rende, CS, Italy; National Research Council of Italy - Institute for High Performance Computing and Networking (ICAR), Rende, CS, Italy; National Research Council of Italy - Institute for High Performance Computing and Networking (ICAR), Rende, CS, Italy; National Research Council of Italy - Institute for High Performance Computing and Networking (ICAR), Rende, CS, Italy","2020 IEEE/ACM 24th International Symposium on Distributed Simulation and Real Time Applications (DS-RT)","6 Oct 2020","2020","","","1","8","In this work, we propose an IoT edge-based energy management system devoted to minimizing the energy cost for the daily-use of in-home appliances. The proposed approach employs a load scheduling based on a load shifting technique, and it is designed to operate in an edge-computing environment naturally. The scheduling considers all together time-variable profiles for energy cost, energy production, and energy consumption for each shiftable appliance. Deadlines for load termination can also be expressed. In order to address these goals, the scheduling problem is formulated as a Markov decision process and then processed through a reinforcement learning technique. The approach is validated by the development of an agent-based real-world test case deployed in an edge context.","1550-6525","978-1-7281-7343-6","10.1109/DS-RT50469.2020.9213697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9213697","Edge Computing;Reinforcement Learning;Energy Management Systems;Internet of Things;Multi-Agent Systems","Buildings;Learning (artificial intelligence);Home appliances;Energy consumption;Energy management;Markov processes;Edge computing","building management systems;cost reduction;demand side management;domestic appliances;energy management systems;Internet of Things;learning (artificial intelligence);Markov processes;scheduling;smart power grids","shiftable appliance;load termination;scheduling problem;reinforcement learning technique;agent-based real-world test case;edge context;IoT edge-based energy management system;energy cost;in-home appliances;load scheduling;load shifting technique;edge-computing environment;time-variable profiles;energy production;energy consumption","","7","","24","IEEE","6 Oct 2020","","","IEEE","IEEE Conferences"
"Multi-agent Battery Storage Management using MPC-based Reinforcement Learning","A. B. Kordabad; W. Cai; S. Gros","Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), Trondheim, Norway","2021 IEEE Conference on Control Technology and Applications (CCTA)","3 Jan 2022","2021","","","57","62","In this paper, we present the use of Model Predictive Control (MPC) based on Reinforcement Learning (RL) to find the optimal policy for a multi-agent battery storage system. A time-varying prediction of the power price and production-demand uncertainty are considered. We focus on optimizing an economic objective cost while avoiding very low or very high state of charge, which can damage the battery. We consider the bounded power provided by the main grid and the constraints on the power input and state of each agent. A parametrized MPC-scheme is used as a function approximator for the deterministic policy gradient method and RL optimizes the closed-loop performance by updating the parameters. Simulation results demonstrate that the proposed method is able to tackle the constraints and deliver the optimal policy.","2768-0770","978-1-6654-3643-4","10.1109/CCTA48906.2021.9659202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9659202","","Gradient methods;Uncertainty;Costs;Simulation;Power system dynamics;Storage management;Reinforcement learning","closed loop systems;function approximation;gradient methods;learning (artificial intelligence);predictive control;pricing","multiagent battery storage management;MPC-based Reinforcement Learning;Model Predictive Control;RL;optimal policy;multiagent battery storage system;time-varying prediction;power price;production-demand uncertainty;economic objective cost;bounded power;power input;parametrized MPC-scheme;deterministic policy gradient method","","7","","21","IEEE","3 Jan 2022","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning for Network Selection and Resource Allocation in Heterogeneous Multi-RAT Networks","M. S. Allahham; A. A. Abdellatif; N. Mhaisen; A. Mohamed; A. Erbad; M. Guizani","Department of Computer Science and Engineering, Qatar University, Doha, Qatar; College of Engineering, Qatar University, Doha, Qatar; Qatar University, Doha, Qatar; College of Engineering, Qatar University, Doha, Qatar; Division of Information and Computing Technology, College of Science and Engineering, Hamad Bin Khalifa University, Ar-Rayyan, Qatar; Machine Learning Department, MBZUAI, Abu Dhabi, UAE","IEEE Transactions on Cognitive Communications and Networking","8 Jun 2022","2022","8","2","1287","1300","The rapid production of mobile devices along with the wireless applications boom is continuing to evolve daily. This motivates the exploitation of wireless spectrum using multiple Radio Access Technologies (multi-RAT) and developing innovative network selection techniques to cope with such intensive demand while improving Quality of Service (QoS). Thus, we propose a distributed framework for dynamic network selection at the edge level, and resource allocation at the Radio Access Network (RAN) level, while taking into consideration diverse applications’ characteristics. In particular, our framework employs a deep Multi-Agent Reinforcement Learning (DMARL) algorithm, that aims to maximize the edge nodes’ quality of experience while extending the battery lifetime of the nodes and leveraging adaptive compression schemes. Indeed, our framework enables data transfer from the network’s edge nodes, with multi-RAT capabilities, to the cloud in a cost and energy-efficient manner, while maintaining QoS requirements of different supported applications. Our results depict that our solution outperforms state-of-the-art techniques of network selection in terms of energy consumption, latency, and cost.","2332-7731","","10.1109/TCCN.2022.3155727","NPRP grant # NPRP12S-0305-190231 from the Qatar National Research Fund (a member of Qatar Foundation); NPRP grant # NPRP13S-0205-200265; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9726129","Heterogeneous networks;edge computing;wireless healthcare systems;multi-RAT architecture;deep reinforcement learning","Quality of service;Resource management;Optimization;5G mobile communication;Task analysis;Reinforcement learning;Radio access technologies","distributed processing;energy consumption;multi-agent systems;quality of experience;quality of service;radio access networks;reinforcement learning;resource allocation;telecommunication power management","energy consumption;adaptive compression schemes;edge nodes;deep multiagent reinforcement learning;radio access network level;edge level;dynamic network selection;distributed framework;innovative network selection techniques;multiple radio access technologies;wireless spectrum;wireless applications boom;mobile devices;rapid production;heterogeneous MultiRAT networks;resource allocation","","6","","52","IEEE","2 Mar 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Self-Scheduling Strategy for a CAES-PV System Using Accurate Sky Images-Based Forecasting","A. Dolatabadi; H. Abdeltawab; Y. A. -R. I. Mohamed","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; School of Engineering - Penn State Behrend, Erie, PA, USA; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada","IEEE Transactions on Power Systems","27 Feb 2023","2023","38","2","1608","1618","Compressed air energy storage (CAES) is a scalable and clean energy storage technology with great potential in renewables accommodation. From the point of view of the facility owner participating in the energy market, the profit of a CAES-PV system’s coordinated operation is still at a notable risk. This paper addresses this problem by using a novel model-free deep reinforcement learning (DRL) method to optimize the CAES energy arbitrage in the presence of a sky images-based short-term solar irradiance forecasting model. To overcome the risk associated with the highly intermittent solar power productions, and thus efficient participation in an electricity market, a hybrid forecasting model based on 2-D convolutional neural networks (CNNs) and bidirectional long short-term memory (BLSTM) units is developed to capture high levels of abstractions in solar irradiance data, especially during cloudy days. Moreover, the thermodynamic characteristics of the CAES facility are considered to achieve more realistic real-time scheduling results. The comparative results based on a realistic-based case study demonstrate the effectiveness and applicability of the proposed framework compared to the state-of-the-art methods in the recent literature.","1558-0679","","10.1109/TPWRS.2022.3177704","Canada First Research Excellence Fund; University of Alberta's Future Energy Systems; Alberta Innovates Graduate Student Scholarships for Data-enabled Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782559","Deep reinforcement learning;compressed air energy storage (CAES);convolutional neural network;sky images;bidirectional long short-term memory","Thermodynamics;Optimal scheduling;Mathematical models;Forecasting;Predictive models;Convolutional neural networks;Uncertainty","compressed air energy storage;convolutional neural nets;deep learning (artificial intelligence);energy storage;learning (artificial intelligence);photovoltaic power systems;power engineering computing;power generation economics;power markets;recurrent neural nets;reinforcement learning;solar power;solar power stations","2-D convolutional neural networks;accurate sky images-based;CAES energy arbitrage;CAES facility;CAES-PV system;clean energy storage technology;compressed air energy storage;efficient participation;electricity market;energy market;facility owner;highly intermittent solar power productions;hybrid forecasting model;notable risk;novel model-free deep reinforcement learning method;real-time scheduling results;realistic-based case study;renewables accommodation;scalable energy storage technology;self-scheduling strategy;short-term memory units;sky images-based short-term;solar irradiance data;solar irradiance forecasting model","","5","","34","IEEE","26 May 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning for Logic Recipe Generation: Bridging Gaps From Images to Plans","M. Zhang; G. Tian; Y. Zhang; P. Duan","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Computer Science, Liaocheng University, Liaocheng, China","IEEE Transactions on Multimedia","21 Jan 2022","2022","24","","352","365","It is a challenging task to produce recipes from images, due to the difficulty in bridging the gap from intuitive, static images to sequential, dynamic recipes. In this paper, we propose a novel recipe generation system for producing effective recipes from images. As medium steps, ingredient generation is introduced to guide recipe generation in our system. With potential information in ingredient lists, ingredient selection and ingredient sequence, the system is taught to generate effective recipes. For information representation, a hierarchical attention mechanism is designed to extract effective features for ingredient production and recipe generation. In order to guarantee the comprehensiveness and logic in recipes, a specific and explicit criterion around ingredients is designed under the framework of reinforcement learning. In ingredient generation, the system is required to generate ingredients with correct sequence in cooking procedures. And in recipe generation, ingredients in recipes are required to be consistent with produced ingredients. In experiments, the proposed method is compared with state-of-the-art methods to evaluate the feasibility. The results indicate that the proposed system achieves a better performance than other methods on both aspects of producing proper ingredients and effective recipes.","1941-0077","","10.1109/TMM.2021.3050090","National Natural Science Foundation of China(grant numbers:U1813215,61773239); Special fund for Taishan Scholars Program of Shandong Province; China Postdoctoral Science Foundation(grant numbers:2020M672060); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9318510","Question answering;attention mechanism;reinforcement learning;recipe generation","Reinforcement learning;Feature extraction;Decoding;Artificial neural networks;Generators;Visualization;Task analysis","feature extraction;image representation;neural nets;reinforcement learning","intuitive images;static images;sequential recipes;dynamic recipes;ingredient generation;ingredient lists;ingredient selection;ingredient sequence;ingredient production;reinforcement learning;logic recipe generation system;information representation;hierarchical attention mechanism;feature extraction;cooking procedures","","2","","45","IEEE","8 Jan 2021","","","IEEE","IEEE Journals"
"Towards Real-Time Routing Optimization with Deep Reinforcement Learning: Open Challenges","P. Almasan; J. Suárez-Varela; B. Wu; S. Xiao; P. Barlet-Ros; A. Cabellos-Aparicio","Barcelona Neural Networking Center, Universitat Politècnica de Catalunya, Spain; Barcelona Neural Networking Center, Universitat Politècnica de Catalunya, Spain; Network Technology Lab., Huawei Technologies Co.,Ltd.; Network Technology Lab., Huawei Technologies Co.,Ltd.; Barcelona Neural Networking Center, Universitat Politècnica de Catalunya, Spain; Barcelona Neural Networking Center, Universitat Politècnica de Catalunya, Spain","2021 IEEE 22nd International Conference on High Performance Switching and Routing (HPSR)","15 Jul 2021","2021","","","1","6","The digital transformation is pushing the existing network technologies towards new horizons, enabling new applications (e.g., vehicular networks). As a result, the networking community has seen a noticeable increase in the requirements of emerging network applications. One main open challenge is the need to accommodate control systems to highly dynamic network scenarios. Nowadays, existing network optimization technologies do not meet the needed requirements to effectively operate in real time. Some of them are based on hand-crafted heuristics with limited performance and adaptability, while some technologies use optimizers which are often too time-consuming. Recent advances in Deep Reinforcement Learning (DRL) have shown a dramatic improvement in decision-making and automated control problems. Consequently, DRL represents a promising technique to efficiently solve a variety of relevant network optimization problems, such as online routing. In this paper, we explore the use of state-of-the-art DRL technologies for real-time routing optimization and outline some relevant open challenges to achieve production-ready DRL-based solutions.","2325-5609","978-1-6654-4005-9","10.1109/HPSR52026.2021.9481864","European Social Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9481864","","Network topology;Digital transformation;Decision making;Switches;Reinforcement learning;Routing;Real-time systems","computer networks;decision making;learning (artificial intelligence);optimisation;telecommunication network routing","deep reinforcement learning;noticeable increase;networking community;vehicular networks;existing network technologies;digital transformation;towards real-time routing optimization;production-ready DRL-based solutions;outline some relevant open challenges;state-of-the-art DRL technologies;online routing;relevant network optimization problems;automated control problems;optimizers;adaptability;hand-crafted heuristics;network optimization technologies;highly dynamic network scenarios;control systems;main open challenge;network applications","","1","","33","IEEE","15 Jul 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning with semi-expert distillation for autonomous UAV cinematography","A. Sochopoulos; I. Mademlis; E. Charalampakis; S. Papadopoulos; I. Pitas","Department of Electical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece","2023 IEEE International Conference on Multimedia and Expo (ICME)","25 Aug 2023","2023","","","1325","1330","Unmanned Aerial Vehicles (UAVs, or drones) have revolutionized modern media production. Being rapidly deployable ""flying cameras"", they can easily capture aesthetically pleasing aerial footage of static or moving filming targets/subjects. Current approaches rely either on manual UAV/gimbal control by human experts, or on a combination of complex computer vision algorithms and hardware configurations for automating the flight+filming process. This paper explores an efficient Deep Reinforcement Learning (DRL) alternative, which implicitly merges the target detection and path planning steps into a single algorithm. To achieve this, a baseline DRL approach is augmented with a novel policy distillation component, which transfers knowledge from a suitable, semi-expert Model Predictive Control (MPC) controller into the DRL agent. Thus, the latter is able to autonomously execute a specific UAV cinematography task with purely visual input. Unlike the MPC controller, the proposed DRL agent does not need to know the 3D world position of the filming target during inference. Experiments conducted in a photorealistic simulator showcase superior performance and training speed compared to the baseline agent, while surpassing the MPC controller in terms of visual occlusion avoidance.","1945-788X","978-1-6654-6891-6","10.1109/ICME55011.2023.00230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10219847","autonomous drones;UAV cinematography;Deep Reinforcement Learning;policy distillation;Model Predictive Control;Deep Neural Networks","Training;Deep learning;Visualization;Three-dimensional displays;Reinforcement learning;Cinematography;Autonomous aerial vehicles","autonomous aerial vehicles;cameras;cinematography;computer vision;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;object detection;path planning;predictive control;reinforcement learning;remotely operated vehicles","aerial footage;autonomous UAV cinematography;baseline agent;baseline DRL approach;cameras;complex computer vision algorithms;current approaches rely;DRL agent;drones;efficient Deep Reinforcement;filming target;flight+filming process;hardware configurations;human experts;modern media production;MPC controller;novel policy distillation component;semiexpert distillation;semiexpert Model Predictive Control controller;specific UAV cinematography task;static moving;target detection;UAVs;Unmanned Aerial Vehicles","","","","24","IEEE","25 Aug 2023","","","IEEE","IEEE Conferences"
"A Study on Battery Storage Exploitation using Reinforcement Learning","P. B. Narayana; G. Mahalakshmi; Y. R. M; B. S. Kumar; R. B. R. Prakash; D. Suganthi","Dept Of Mechanical Engg, Mahatma Gandhi Institute Of Technology (Mgit), Hyderabad; EEE Easwari Engineering College, Chennai; Dept Of Mechanical Engg, Mahatma Gandhi Institute Of Technology (Mgit), Hyderabad; Department of EEE, Chaitanya Bharathi Institute of Technology, Hyderabad, Telangana; Electrical and Electronics Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram; Department of Computer Science, Saveetha College of Liberal Arts and Sciences, Saveetha Institute of Medical And Technical Sciences, Chennai","2023 2nd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)","8 Jun 2023","2023","","","467","473","An integral part of the new smart grid, battery storage is becoming an increasingly common requirement. Batteries are considered as an attractive alternative for other distributed intelligent energy resources because of their speed in responding to events like changes in renewable power or grid interruptions. Various research studied exist to explain about making money out of this talent. Any solution must take into account both the fast-paced nature of electrical phenomena and the more gradual changes in the relevant power markets. An area of AI called reinforcement learning has demonstrated promising results in optimizing difficult tasks with high degrees of uncertainty. As a result, the need for first-of-its-kind smart buildings is growing in tandem with the complexity of decentralized power systems and the widespread use of renewable energy sources. The initial portion of the algorithm is based on deep learning using recurrent neural networks to deal with the unknowns of power pricing and load needs in the future. The other method uses reinforcement learning to determine the best time to charge or discharge a battery bank based on profit, load, and grid peak conditions. This research study presents a reinforcement learning approach for battery scheduling, which is considered as a crucial component to meet the consumer objectives. The framework relies on a customer making a multi-criteria choice in order to maximize local energy production from batteries during peak demand. This will save costs by reducing the amount of power used by the grid. The reinforcement learning system used to choose the best battery scheduling actions uses predictions of available wind power. Consumers' understanding of what to do to maximize battery life in a variety of time-dependent settings is expanded thanks to the built-in learning mechanism. The established framework enables smart consumers to acquire knowledge of the stochastic environment and apply it to the process of making decisions about energy management strategies.","","978-1-6654-5630-2","10.1109/ICAAIC56838.2023.10141011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10141011","Smart Grid;Reinforcement Learning (RL);Energy Storage;Battery Management;Uncertainty;Deep Learning","Renewable energy sources;Costs;Water storage;Water heating;Reinforcement learning;Learning (artificial intelligence);Wind power generation","decision making;deep learning (artificial intelligence);energy management systems;learning (artificial intelligence);power engineering computing;power grids;power markets;pricing;recurrent neural nets;reinforcement learning;renewable energy sources;smart power grids;wind power","available wind power;batteries;battery bank;battery life;battery scheduling actions;battery storage exploitation;decentralized power systems;deep learning;distributed intelligent energy resources;first-of-its-kind smart buildings;gradual changes;grid interruptions;grid peak conditions;increasingly common requirement;learning mechanism;local energy production;power pricing;reinforcement learning approach;reinforcement learning system;relevant power markets;renewable energy sources;renewable power;research studied;smart consumers;smart grid","","","","18","IEEE","8 Jun 2023","","","IEEE","IEEE Conferences"
"Hierarchical Multiagent Reinforcement Learning for Allocating Guaranteed Display Ads","L. Wang; L. Han; X. Chen; C. Li; J. Huang; W. Zhang; W. Zhang; X. He; D. Luo","School of Computer Science and Technology, East China Normal University, Shanghai, China; Tencent AI Lab, Tencent, Inc., Shenzhen, China; Tencent AI Lab, Tencent, Inc., Shenzhen, China; Tencent AI Lab, Tencent, Inc., Shenzhen, China; Tencent AI Lab, Tencent, Inc., Shenzhen, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Computer Science and Technology, East China Normal University, Shanghai, China; School of Computer Science and Technology, East China Normal University, Shanghai, China; Tencent AI Lab, Tencent, Inc., Shenzhen, China","IEEE Transactions on Neural Networks and Learning Systems","5 Oct 2022","2022","33","10","5361","5373","In this article, we study the problem of guaranteed display ads (GDAs) allocation, which requires proactively allocate display ads to different impressions to fulfill their impression demands indicated in the contracts. Existing methods for this problem either assume the impressions that are static or solely consider a specific ad’s benefits. Thus, it is hard to generalize to the industrial production scenario where the impressions are dynamical and large-scale, and the overall allocation optimality of all the considered GDAs is required. To bridge this gap, we formulate this problem as a sequential decision-making problem in the scope of multiagent reinforcement learning (MARL), by assigning an allocation agent to each ad and coordinating all the agents for allocating GDAs. The inputs are the states (e.g., the demands of the ad and the remaining time steps for displaying the ads) of each ad and the impressions at different time steps, and the outputs are the display ratios of each ad for each impression. Specifically, we propose a novel hierarchical MARL (HMARL) method that creates hierarchies over the agent policies to handle a large number of ads and the dynamics of impressions. HMARL contains: 1) a manager policy to navigate the agent to choose an appropriate subpolicy and 2) a set of subpolicies that let the agents perform diverse conditioning on their states. Extensive experiments on three real-world data sets from the Tencent advertising platform with tens of millions of records demonstrate significant improvements of HMARL over state-of-the-art approaches.","2162-2388","","10.1109/TNNLS.2021.3070484","National Key Research and Development Program of China(grant numbers:2016YFB1000904); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9432789","Artificial intelligence;computational and artificial intelligence;decision support systems","Resource management;Contracts;Advertising;Statistics;Sociology;Reinforcement learning;Real-time systems","advertising data processing;decision making;multi-agent systems;reinforcement learning","hierarchical multiagent reinforcement learning;guaranteed display ads allocation;impression demands;industrial production scenario;allocation optimality;sequential decision-making problem;allocation agent;display ratios;novel hierarchical MARL method;agent policies;GDA allocation","","","","39","IEEE","17 May 2021","","","IEEE","IEEE Journals"
"Research and Application of Safe Reinforcement Learning in Power System","J. Li; X. Wang; S. Chen; D. Yan","Artificial Intelligence Application Research Department, China Electric Power Research Institute, Beijing, China; Artificial Intelligence Application Research Department, China Electric Power Research Institute, Beijing, China; Artificial Intelligence Application Research Department, China Electric Power Research Institute, Beijing, China; Artificial Intelligence Application Research Department, China Electric Power Research Institute, Beijing, China","2023 8th Asia Conference on Power and Electrical Engineering (ACPEE)","31 May 2023","2023","","","1977","1982","Agent exploration of reinforcement learning is a necessary way for reinforcement learning algorithms to obtain information. In order to obtain more exploratory information, some deep reinforcement learning algorithms even increase the exploration of agents. Reinforcement learning has been successfully applied in many intelligent control fields, however unlimited exploration may bring disastrous consequences to agents, there are still many concerns that need attention in the application of real world, one of which is the safety issue. The safe reinforcement learning approximately enforces the constraint conditions in each policy update, thus further improving the security and robustness of intelligent algorithm. Furthermore, according to the particularity of electric energy production, transmission and consumption, power system operation needs to meet the requirements of safety, stability and efficiency. This paper summarizes the theory and characteristics of safe reinforcement learning, and then discusses the application of safe reinforcement learning in power system. Finally, we propose a prospect for the challenging problems of safe reinforcement learning in power field.","","979-8-3503-4552-0","10.1109/ACPEE56931.2023.10135995","Electric Power Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10135995","safe reinforcement learning;intelligent algorithm;power system;application","Training;Renewable energy sources;Reinforcement learning;Power system stability;Wind power generation;Approximation algorithms;Power grids","deep learning (artificial intelligence);power system security;reinforcement learning","constraint condition;deep reinforcement learning algorithms;electric energy production;power system;power system operation;safe reinforcement learning","","","","33","IEEE","31 May 2023","","","IEEE","IEEE Conferences"
"Enhancing Energy Trading Between Different Islanded Microgrids A Reinforcement Learning Algorithm Case Study in Northern Kordofan State","M. ELamin; F. Elhassan; M. A. Manzoul","School of Electrical and Electronic Engineering, University of Khartoum, Khartoum, Sudan; School of Electrical and Electronic Engineering, University of Khartoum, Khartoum, Sudan; Department of Electrical & Computer Engineering, Jackson State University, Jackson, MS","2020 International Conference on Computer, Control, Electrical, and Electronics Engineering (ICCCEEE)","17 May 2021","2021","","","1","6","This paper tackles the problem of rural electrification and the lack of grid connection to large areas of Sudan. It introduces microgrids as an alternative to conventional centralized generation as they provide stability in electricity supply in addition to the environmental benefits accompanied with using renewable energy sources. A new method is introduced to facilitate the fluctuation in energy production when using renewable sources by creating a Reinforcement Learning algorithm to conduct the process of energy trading between different islanded microgrids. The goal of the trading process is to achieve stability and generation-load balance in the microgrids. The paper also presents a case study of three villages in North Kordufan State; Hamza Elsheikh, Tannah and Um-Bader. The study uses real solar irradiance and wind speed data to create a MATLAB simulation for a fully functional microgrid. An RL environment of the grids is created which can be used for future research and modelling in the field of smart grids. The paper explores Vanilla Policy Gradients VPG as a solution algorithm for the problem. The algorithm achieved generationload stability when applied to data extracted from the MATLAB simulation; satisfying the loads while also achieving profit from the trading process; reducing the return of investment period for the microgrid.","","978-1-7281-9111-9","10.1109/ICCCEEE49695.2021.9429584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9429584","","Machine learning algorithms;Wind speed;Microgrids;Reinforcement learning;Power system stability;Stability analysis;Data models","distributed power generation;investment;learning (artificial intelligence);power engineering computing;power generation economics;smart power grids;solar power stations;sunlight;wind power plants","energy trading;islanded microgrids;Reinforcement Learning algorithm case study;rural electrification;grid connection;electricity supply;environmental benefits;renewable energy sources;energy production;renewable sources;generation-load balance;MATLAB simulation;fully functional microgrid;smart grids;generation load stability;Tannah;Hamza Elsheikh;Um-Bader;RL environment;vanilla policy gradients;VPG;Sudan;Northern Kordofan State;solar irradiance;wind speed data","","","","15","IEEE","17 May 2021","","","IEEE","IEEE Conferences"
"Data Center Microgrid Energy Management Based on Model-Based Reinforcement Learning","H. Zhao; J. Wan; L. Li","Inner Mongolia University of Technology, Hohhot, China; Inner Mongolia University of Technology Inner Mongolia Autonomous Region Engineering &TechnologyResearch Center of Big Data Based Software Service, Hohhot, China; Inner Mongolia University of Technology Inner Mongolia Autonomous Region Engineering &TechnologyResearch Center of Big Data Based Software Service, Hohhot, China","2022 4th International Conference on Frontiers Technology of Information and Computer (ICFTIC)","27 Mar 2023","2022","","","365","371","With the increasing of energy consumption and price, energy management is becoming increasingly important for data center with larger and larger scale. Microgrid can provide reliable, stable and continuous power services for data centers, and improve energy efficiency and reduce the operating cost. In this paper, we study the energy management problem for data center microgrid (DCMG) integrated with renewable energy. We introduce the energy storage device to mediate the intermittent nature of renewable energy. The energy management problem is formulated as a Markov Decision Process (MDP). Traditional methods greedy choice actions does not consider the impact of the actions on future cumulative operating cost. Model-free reinforcement learning is a candidate solution to this problem. However, model-free reinforcement learning methods converge slowly. Therefore, in this paper, we propose a model- based deep reinforcement learning algorithms to solve the energy management problem (Model-Based Reinforcement Learning Energy Management, MBRL-EM) of DCMG. We use long short-term memory (LSTM) model to design a system transition model, which predicts the future renewable energy production and energy demand. Then we obtain the optimal policy based on model predictive control to minimize the long-term operation cost. The simulation results show that MBRL-EM outperforms the baseline algorithms of model-based PPO and model-free PPO and reduces the average cost by up to 23% and 38% respectively.","","979-8-3503-2195-1","10.1109/ICFTIC57696.2022.10075219","National Natural Science Foundation of China(grant numbers:61862048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10075219","Model-Based Reinforcement Learning;Data center Microgrid;Energy management","Data centers;Renewable energy sources;Costs;Computational modeling;Reinforcement learning;Microgrids;Predictive models","deep learning (artificial intelligence);distributed power generation;energy conservation;energy consumption;energy management systems;energy storage;learning (artificial intelligence);Markov processes;optimisation;power engineering computing;power generation control;predictive control;recurrent neural nets;reinforcement learning","data center microgrid Energy Management;deep reinforcement learning algorithms;energy consumption;energy demand;energy efficiency;energy management problem;energy storage device;future renewable energy production;larger scale;model-based PPO;Model-Based Reinforcement Learning Energy Management;model-free PPO;model-free reinforcement learning methods;short-term memory model","","","","20","IEEE","27 Mar 2023","","","IEEE","IEEE Conferences"
"Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning","Y. Zheng; X. Xie; T. Su; L. Ma; J. Hao; Z. Meng; Y. Liu; R. Shen; Y. Chen; C. Fan","College of Intelligence and Computing, Tianjin University, China; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Kyushu University, Japan; College of Intelligence and Computing, Tianjin University, China; College of Intelligence and Computing, Tianjin University, China; Nanyang Technological University, Singapore; Fuxi AI Lab, Netease, Inc., Hangzhou, China; Fuxi AI Lab, Netease, Inc., Hangzhou, China; Fuxi AI Lab, Netease, Inc., Hangzhou, China","2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)","9 Jan 2020","2019","","","772","784","Game testing has been long recognized as a notoriously challenging task, which mainly relies on manual playing and scripting based testing in game industry. Even until recently, automated game testing still remains to be largely untouched niche. A key challenge is that game testing often requires to play the game as a sequential decision process. A bug may only be triggered until completing certain difficult intermediate tasks, which requires a certain level of intelligence. The recent success of deep reinforcement learning (DRL) sheds light on advancing automated game testing, without human competitive intelligent support. However, the existing DRLs mostly focus on winning the game rather than game testing. To bridge the gap, in this paper, we first perform an in-depth analysis of 1349 real bugs from four real-world commercial game products. Based on this, we propose four oracles to support automated game testing, and further propose Wuji, an on-the-fly game testing framework, which leverages evolutionary algorithms, DRL and multi-objective optimization to perform automatic game testing. Wuji balances between winning the game and exploring the space of the game. Winning the game allows the agent to make progress in the game, while space exploration increases the possibility of discovering bugs. We conduct a large-scale evaluation on a simple game and two popular commercial games. The results demonstrate the effectiveness of Wuji in exploring space and detecting bugs. Moreover, Wuji found 3 previously unknown bugs, which have been confirmed by the developers, in the commercial games.","2643-1572","978-1-7281-2508-4","10.1109/ASE.2019.00077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952543","Game Testing;Artificial Intelligence;Deep Reinforcement Learning;Evolutionary Multi-Objective Optimization","Games;Computer bugs;Testing;Space exploration;Reinforcement learning;Manuals;Sociology","competitive intelligence;computer games;evolutionary computation;learning (artificial intelligence)","Wuji;automatic online combat game testing;evolutionary deep reinforcement learning;game industry;automated game testing;real-world commercial game products;on-the-fly game;automatic game testing;simple game;commercial games","","65","","47","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"QoE-Based Task Offloading With Deep Reinforcement Learning in Edge-Enabled Internet of Vehicles","X. He; H. Lu; M. Du; Y. Mao; K. Wang","College of Computer and Information, Hohai University, Nanjing, China; College of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; Department of Electrical and Computer Engineering, University of California at Los Angeles, Los Angeles, CA, USA","IEEE Transactions on Intelligent Transportation Systems","30 Mar 2021","2021","22","4","2252","2261","In the transportation industry, task offloading services of edge-enabled Internet of Vehicles (IoV) are expected to provide vehicles with the better Quality of Experience (QoE). However, the various status of diverse edge servers and vehicles, as well as varying vehicular offloading modes, make a challenge of task offloading service. Therefore, to enhance the satisfaction of QoE, we first introduce a novel QoE model. Specifically, the emerging QoE model restricted by the energy consumption: 1) intelligent vehicles equipped with caching spaces and computing units may work as carriers; 2) various computational and caching capacities of edge servers can empower the offloading; and 3) unpredictable routings of the vehicles and edge servers can lead to diverse information transmission. We then propose an improved deep reinforcement learning (DRL) algorithm named PS-DDPG with the prioritized experience replay (PER) and the stochastic weight averaging (SWA) mechanisms based on deep deterministic policy gradients (DDPG) to seek an optimal offloading mode, saving energy consumption. Specifically, the PER scheme is proposed to enhance the availability of the experience replay buffer, thus accelerating the training. Moreover, reducing the noise in the training process and thus stabilizing the rewards, the SWA scheme is introduced to average weights. Extensive experiments certify the better performance, i.e., stability and convergence, of our PS-DDPG algorithm compared to existing work. Moreover, the experiments indicate that the QoE value can be improved by the proposed algorithm.","1558-0016","","10.1109/TITS.2020.3016002","National Natural Science Foundation of China(grant numbers:61872195,61832005); National Key Research and Development Program of China(grant numbers:2018YFC0407105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173570","Internet of vehicles (IoV);edge;task offloading;deep deterministic policy gradients (DDPG);QoE","Task analysis;Quality of experience;Servers;Training;Computational modeling;Energy consumption;Convergence","cache storage;Internet;learning (artificial intelligence);mobile computing;quality of experience;quality of service","edge-enabled internet;transportation industry;task offloading service;diverse edge servers;vehicular offloading modes;novel QoE model;emerging QoE model;energy consumption;caching spaces;computing units;computational caching capacities;diverse information transmission;improved deep reinforcement learning algorithm;prioritized experience replay;deep deterministic policy gradients;optimal offloading mode;experience replay buffer;PS-DDPG algorithm;QoE value;QoE-based task offloading","","39","","28","IEEE","21 Aug 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning in First Person Shooter Games","M. McPartland; M. Gallagher","School of Information Technology and Electrical Engineering, University of Queensland, Australia; School of Information Technology and Electrical Engineering, University of Queensland, Australia","IEEE Transactions on Computational Intelligence and AI in Games","14 Mar 2011","2011","3","1","43","56","Reinforcement learning (RL) is a popular machine learning technique that has many successes in learning how to play classic style games. Applying RL to first person shooter (FPS) games is an interesting area of research as it has the potential to create diverse behaviors without the need to implicitly code them. This paper investigates the tabular Sarsa (λ) RL algorithm applied to a purpose built FPS game. The first part of the research investigates using RL to learn bot controllers for the tasks of navigation, item collection, and combat individually. Results showed that the RL algorithm was able to learn a satisfactory strategy for navigation control, but not to the quality of the industry standard pathfinding algorithm. The combat controller performed well against a rule-based bot, indicating promising preliminary results for using RL in FPS games. The second part of the research used pretrained RL controllers and then combined them by a number of different methods to create a more generalized bot artificial intelligence (AI). The experimental results indicated that RL can be used in a generalized way to control a combination of tasks in FPS bots such as navigation, item collection, and combat.","1943-0698","","10.1109/TCIAIG.2010.2100395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5672586","Artificial intelligence (AI);computer games;reinforcement learning (RL)","Games;Learning;Navigation;Computers;Complexity theory;Robots","computer games;knowledge based systems;learning (artificial intelligence);path planning","reinforcement learning;first person shooter game;machine learning;tabular Sarsa RL algorithm;learn bot controller;navigation control;industry standard pathfinding algorithm;rule based bot;artificial intelligence","","35","","24","IEEE","20 Dec 2010","","","IEEE","IEEE Journals"
"Cooperative Multiagent Deep Reinforcement Learning for Reliable Surveillance via Autonomous Multi-UAV Control","W. J. Yun; S. Park; J. Kim; M. Shin; S. Jung; D. A. Mohaisen; J. -H. Kim","School of Electrical Engineering, Korea University, Seoul, South Korea; School of Electrical Engineering, Korea University, Seoul, South Korea; School of Electrical Engineering, Korea University, Seoul, South Korea; Mofl Inc., Daejeon, South Korea; School of Software, Hallym University, Chuncheon, South Korea; Department of Computer Science, University of Central Florida, Orlando, FL, USA; Department of Electrical and Computer Engineering, Ajou University, Suwon, South Korea","IEEE Transactions on Industrial Informatics","13 Jul 2022","2022","18","10","7086","7096","CCTV-based surveillance using unmanned aerial vehicles (UAVs) is considered a key technology for security in smart city environments.This article creates a case where the UAVs with CCTV-cameras fly over the city area for flexible and reliable surveillance services. UAVs should be deployed to cover a large area while minimizing overlapping and shadow areas for a reliable surveillance system. However, the operation of UAVs is subject to high uncertainty, necessitating autonomous recovery systems. This article develops a multiagent deep reinforcement learning-based management scheme for reliable industry surveillance in smart city applications. The core idea this article employs is autonomously replenishing the UAV's deficient network requirements with communications. Via intensive simulations, our proposed algorithm outperforms the state-of-the-art algorithms in terms of surveillance coverage, user support capability, and computational costs.","1941-0050","","10.1109/TII.2022.3143175","National Research Foundation of Korea(grant numbers:2021R1A4A1030775); Basic Research Laboratory(grant numbers:2019M3E3A1084054); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9682599","Multiagent systems;neural networks;surveillance;unmanned aerial vehicle (UAV)","Surveillance;Reliability;Optimization;Uncertainty;Electronic mail;Multi-agent systems;Image resolution","aircraft control;autonomous aerial vehicles;closed circuit television;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;remotely operated vehicles;surveillance;video surveillance","autonomous multiUAV control;CCTV-based surveillance;unmanned aerial vehicles;UAVs;smart city environments;CCTV-cameras;city area;overlapping shadow areas;reliable surveillance system;necessitating autonomous recovery systems;multiagent deep reinforcement learning-based management scheme;reliable industry surveillance;smart city applications;UAV's deficient network requirements;surveillance coverage","","28","","34","IEEE","14 Jan 2022","","","IEEE","IEEE Journals"
"Trajectory tracking algorithm for autonomous vehicles using adaptive reinforcement learning","M. De Paula; G. G. Acosta","Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires, Olavarría, Argentina; Facultad de Ingeniería, Universidad Nacional del Centro de la Provincia de Buenos Aires, Olavarría, Argentina","OCEANS 2015 - MTS/IEEE Washington","11 Feb 2016","2015","","","1","8","The off-shore industry requires periodic monitoring of underwater infrastructure for preventive maintenance and security reasons. The tasks in hostile environments can be achieved automatically through autonomous robots like UUV, AUV and ASV. When the robot controllers are based on prespecified conditions they could not function properly in these hostile changing environments. It is beneficial to count with adaptive control strategies that are capable to readapt the control policies when deviations, from the desired behavior, are detected. In this paper, we present an online selective reinforcement learning approach for learning reference tracking control policies given no prior knowledge of the dynamical system. The proposed approach enables real-time adaptation of the control policies, executed by the adaptive controller, based on ongoing interactions with the non-stationary environments. Applications on surface vehicles under non-stationary and perturbed environments are simulated. The presented simulation results demonstrate the performance of this novel algorithm to find optimal control policies that solve the trajectory tracking control task in unmanned vehicles.","","978-0-9339-5743-5","10.23919/OCEANS.2015.7401861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401861","Autonomous vehicles;trajectory tracking;cognitive control;reinforcement learning;adaptive control","Monitoring;Security;Robots;Vehicles;Gaussian distribution","adaptive control;intelligent robots;learning (artificial intelligence);mobile robots;optimal control;preventive maintenance;remotely operated vehicles;trajectory control","autonomous vehicles;adaptive reinforcement learning;off-shore industry;periodic monitoring;underwater infrastructure;preventive maintenance;security reasons;hostile environments;autonomous robots;robot controllers;adaptive control;online selective reinforcement learning approach;learning reference tracking control policies;nonstationary environments;surface vehicles;perturbed environments;optimal control policies;trajectory tracking control task;unmanned vehicles","","15","","30","","11 Feb 2016","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning-Based Transcoder Selection Framework for Blockchain-Enabled Wireless D2D Transcoding","M. Liu; Y. Teng; F. R. Yu; V. C. M. Leung; M. Song","Beijing Key Laboratory of Space-Ground Interconnection and Convergence, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Space-Ground Interconnection and Convergence, Beijing University of Posts and Telecommunications, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada; College of Computer Science and Software Engineering, Shenzhen, China; Beijing Key Laboratory of Space-Ground Interconnection and Convergence, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Communications","16 Jun 2020","2020","68","6","3426","3439","The boom of video streaming industry has resulted in the increasing demands for transcoding services from heterogeneous users. Recent advances of blockchain technology allow some startups to realize decentralized collaborative transcoding through device-to-device (D2D) networks, where a group of transcoders are selected to perform transcoding cooperatively. For the blockchain-enabled D2D transcoding systems, it's imperative to jointly design transcoder selection, task scheduling and resource allocation schemes in order to provide efficient and trustworthy transcoding services. In this paper, viewing the involved multi-dimensional complex factors and channel fluctuation, we propose a novel deep reinforcement learning (DRL) based transcoder selection framework for blockchain enabled D2D transcoding systems where both the platform dynamics and channel statistics are captured. To reduce the action space size, we adopt a two-stage decision approach to first select the transcoders through a normal DRL based framework and then obtain the optimal task scheduling, power control, and resource allocation scheme by solving a stochastic optimization problem with the constrained stochastic successive convex approximation (CSSCA) approach. Simulation results show that our proposed framework can achieve high transcoding revenue while meeting the quality of service (QoS) requirements, and it can well handle dynamic cases.","1558-0857","","10.1109/TCOMM.2020.2974738","National Natural Science Foundation of China(grant numbers:61871046); Beijing Natural Science Foundation(grant numbers:L171011); Beijing Major Science and Technology Special Projects(grant numbers:Z181100003118012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9001021","Blockchain;deep reinforcement learning (DRL);transcoder selection","Transcoding;Device-to-device communication;Resource management;Task analysis;Quality of service;Streaming media","approximation theory;convex programming;cooperative communication;cryptography;learning (artificial intelligence);mobile radio;quality of service;resource allocation;telecommunication computing;telecommunication scheduling;transcoding;wireless channels","deep reinforcement learning-based transcoder selection framework;Blockchain-Enabled Wireless D2D;video streaming industry;blockchain technology;decentralized collaborative transcoding;device-to-device networks;transcoders;D2D transcoding systems;task scheduling;resource allocation schemes;efficient transcoding services;trustworthy transcoding services;resource allocation scheme;high transcoding revenue","","10","","40","IEEE","18 Feb 2020","","","IEEE","IEEE Journals"
"Simultaneous Planning for Item Picking and Placing by Deep Reinforcement Learning","T. Tanaka; T. Kaneko; M. Sekine; V. Tangkaratt; M. Sugiyama","Corporate Research & Development Center, Toshiba Corporation, Japan; Corporate Research & Development Center, Toshiba Corporation, Japan; Corporate Research & Development Center, Toshiba Corporation, Japan; Center for Advanced Intelligence Project, RIKEN, Japan; Center for Advanced Intelligence Project, RIKEN, Japan","2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","10 Feb 2021","2020","","","9705","9711","Container loading by a picking robot is an important challenge in the logistics industry. When designing such a robotic system, item picking and placing have been planned individually thus far. However, since the condition of picking an item affects the possible candidates for placing, it is preferable to plan picking and placing simultaneously. In this paper, we propose a deep reinforcement learning (DRL) method for simultaneously planning item picking and placing. A technical challenge in the simultaneous planning is its scalability: even for a practical container size, DRL can be computationally intractable due to large action spaces. To overcome the intractability, we adopt a fully convolutional network for policy approximation and determine the action based only on local information. This enables us to produce a shared policy which can be applied to larger action spaces than the one used for training. We experimentally demonstrate that our method can successfully solve the simultaneous planning problem and achieve a higher occupancy rate than conventional methods.","2153-0866","978-1-7281-6212-6","10.1109/IROS45743.2020.9340929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340929","","Training;Service robots;Shape;Reinforcement learning;Containers;Planning;Task analysis","approximation theory;containers;convolutional neural nets;deep learning (artificial intelligence);industrial robots;loading;logistics;planning","shared policy;policy approximation;fully convolutional network;action spaces;container size;DRL method;logistics industry;container loading;item placing;simultaneous planning problem;deep reinforcement learning method;robotic system;picking robot;item picking","","7","","19","IEEE","10 Feb 2021","","","IEEE","IEEE Conferences"
"Proactive Caching in Auto Driving Scene via Deep Reinforcement Learning","Z. Zhu; Z. Zhang; W. Yan; Y. Huang; L. Yang","School of Information Science and Engineering, Southeast University, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China","2019 11th International Conference on Wireless Communications and Signal Processing (WCSP)","8 Dec 2019","2019","","","1","6","The Internet of Vehicles is a product of the mobile Internet background and an innovative driving force for the industrial upgrading of the automotive industry. The rapid and reliable completion of the vehicle to roadside unit (V2R) communication is an important research direction. In order to improve the communication efficiency between the vehicles and the roadside units (RSUs), we propose a deep reinforcement learning approach for joint proactive caching and automatic vehicle control in the vehicle network. First, we model the autonomous vehicle system and the proactive caching system as separate Markov decision processes, respectively. Then, we propose a model-free algorithm based on deep Q-learning to find effective automatic vehicle control policy and proactive caching policy. Finally, the simulation results are given to demonstrate that the proposed solution can effectively maintain high-quality user experience in the V2R communication.","2472-7628","978-1-7281-3555-7","10.1109/WCSP.2019.8928131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8928131","Internet of vehicles;V2R;proactive caching;automatic vehicle control;deep reinforcement learning","Machine learning;Internet;Road transportation;Base stations;Prediction algorithms;Data models;Aerospace electronics","cache storage;decision theory;Internet;Internet of Things;learning (artificial intelligence);Markov processes;road vehicles;vehicular ad hoc networks","high-quality user experience;separate Markov decision processes;Internet of Vehicles;proactive caching policy;effective automatic vehicle control policy;deep Q-learning;model-free algorithm;proactive caching system;autonomous vehicle system;vehicle network;joint proactive caching;deep reinforcement learning approach;roadside units;communication efficiency;V2R communication;roadside unit communication;automotive industry;innovative driving force;mobile Internet background;auto driving scene","","6","","19","IEEE","8 Dec 2019","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning (DRL)-Based Transcoder Selection for Blockchain-Enabled Video Streaming","M. Liu; F. R. Yu; Y. Teng; V. C. M. Leung; M. Song","Beijing Key Laboratory of Space-ground Interconnection and Convergence, Beijing University of Posts and Telecommunications, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Beijing Key Laboratory of Space-ground Interconnection and Convergence, Beijing University of Posts and Telecommunications, Beijing, China; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Beijing Key Laboratory of Space-ground Interconnection and Convergence, Beijing University of Posts and Telecommunications, Beijing, China","2018 IEEE Globecom Workshops (GC Wkshps)","21 Feb 2019","2018","","","1","6","Video transcoding has been widely applied in video streaming industry to convert videos into multiple formats for heterogeneous users. To provide fast and reliable transcoding services, some emerging platforms are leveraging blockchain and smart contract technology to build decentralized transcoding hubs with flexible monetization mechanisms, where any users can rent their idle computing resources to become transcoders in order to receive reward. On these blockchain-enabled platforms, transcoder selection together with resource allocation is a crucial but challenging issue, which should maximize the transcoding revenue to motivate the transcoder candidates as well as handling the dynamic quality of service (QoS) requirements and candidates' characteristics. In this paper, we propose a novel deep reinforcement learning (DRL)-based transcoder selection framework for blockchain-enabled video streaming. Specifically, we propose an evaluation mechanism to facilitate transcoder selection, and design the transcoder selection and resource allocation scheme using the DRL approach. Simulation results demonstrate the effectiveness of our proposed framework.","","978-1-5386-4920-6","10.1109/GLOCOMW.2018.8644290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8644290","","Transcoding;Streaming media;Quality of service;Resource management;Computational modeling;Energy consumption;Blockchain","cryptography;distributed databases;learning (artificial intelligence);quality of service;resource allocation;transcoding;video coding;video streaming","blockchain-enabled video streaming;video transcoding;video streaming industry;blockchain-enabled platforms;deep reinforcement learning-based transcoder selection framework;transcoding services;DRL-based transcoder selection;resource allocation;quality of service","","4","","15","IEEE","21 Feb 2019","","","IEEE","IEEE Conferences"
"Robo-Advising: Enhancing Investment with Inverse Optimization and Deep Reinforcement Learning","H. Wang; S. Yu","The Vanguard Group, Inc., Malvern, PA, USA; The Vanguard Group, Inc., Malvern, PA, USA","2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)","25 Jan 2022","2021","","","365","372","Machine Learning (ML) has been embraced as a powerful tool by the financial industry, with notable applications spreading in various domains including investment management. In this work, we propose the first full-cycle data-driven investment robo-advising framework, consisting of two ML agents. The first agent, an inverse portfolio optimization agent, infers an investor’s risk preference and expected return directly from historical allocation data using online inverse optimization. The second agent, a deep reinforcement learning (RL) agent, aggregates the inferred sequence of expected returns to formulate a new multi-period mean-variance portfolio optimization problem that can be solved using deep RL approaches. The proposed investment pipeline is applied on real market data from April 1, 2016 to February 1, 2021 and has shown to consistently outperform the S&P 500 benchmark portfolio that represents the aggregate market optimal allocation. The outperformance may be attributed to the the multi-period planning (versus single-period planning) and the data-driven RL approach (versus classical estimation approach).","","978-1-6654-4337-1","10.1109/ICMLA52953.2021.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680218","Robo-advising;mean-variance portfolio allocation;reinforcement learning;inverse optimization","Aggregates;Pipelines;Estimation;Reinforcement learning;Planning;Resource management;Optimization","deep learning (artificial intelligence);investment;optimisation;reinforcement learning;risk analysis","financial industry;notable applications;domains including investment management;full-cycle data-driven investment robo-advising framework;ML agents;inverse portfolio optimization agent;historical allocation data;online inverse optimization;deep reinforcement learning agent;inferred sequence;expected returns;multiperiod mean-variance portfolio optimization problem;deep RL approaches;investment pipeline;market data;S&P 500 benchmark portfolio;aggregate market optimal allocation;multiperiod planning;data-driven RL approach","","3","","23","IEEE","25 Jan 2022","","","IEEE","IEEE Conferences"
"QoE-driven Task Offloading with Deep Reinforcement Learning in Edge intelligent IoV","X. He; H. Lu; Y. Mao; K. Wang","College of Computer and Information, Hohai University, Nanjing, China; College of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; Department of Electrical and Computer Engineering, University of California, Los Angeles, CA, USA","GLOBECOM 2020 - 2020 IEEE Global Communications Conference","15 Feb 2021","2020","","","1","6","In the transportation industry, task offloading services of edge intelligent Internet of Vehicles (IoV) are expected to provide vehicles with the better Quality of Experience (QoE). However, the various status of diverse edge servers and vehicles, as well as varying vehicular offloading modes, make a challenge of task offloading service. Therefore, to enhance the satisfaction of QoE, we first introduce a novel QoE model. Specifically, the emerging QoE model restricted by the energy consumption, (1) intelligent vehicles equipped with caching spaces and computing units may work as carriers; (2) various computational and caching capacities of edge servers can empower the offloading; (3) unpredictable routings of the vehicles and edge servers can lead to diverse information transmission. We then propose an improved deep reinforcement learning (DRL) algorithm named RA-DDPG with the prioritized experience replay (PER) and the stochastic weight averaging (SWA) mechanisms based on deep deterministic policy gradients (DDPG) to seek an optimal offloading mode, saving energy consumption. Extensive experiments certify the better performance, i.e., stability and convergence, of our RA-DDPG algorithm compared to existing work. Moreover, the experiments indicate that the QoE value can be improved by the proposed algorithm.","2576-6813","978-1-7281-8298-8","10.1109/GLOBECOM42002.2020.9348050","National Natural Science Foundation of China(grant numbers:61872195,61832005); National Key Research and Development Program of China(grant numbers:2018YFC0407105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9348050","Internet of Vehicles (IoV);Edge;Task Offloading;Deep Deterministic Policy Gradients (DDPG);QoE","Energy consumption;Stochastic processes;Reinforcement learning;Internet;Quality of experience;Servers;Task analysis","cache storage;deep learning (artificial intelligence);Internet of Things;mobile computing;network servers;optimisation;quality of experience;stochastic processes;telecommunication network routing;telecommunication power management;traffic engineering computing;vehicle routing","QoE-driven task offloading;edge intelligent IoV;transportation industry;task offloading service;diverse edge servers;vehicular offloading modes;QoE model;energy consumption saving;computational caching capacities;diverse information transmission;improved deep reinforcement learning algorithm;prioritized experience replay;deep deterministic policy gradients;optimal offloading mode;QoE value;edge intelligent Internet of Vehicles;quality of experience;unpredictable routing;stochastic weight averaging mechanism;SWA mechanisms;RA-DDPG algorithm","","2","","20","IEEE","15 Feb 2021","","","IEEE","IEEE Conferences"
"Effective Automated Feature Derivation via Reinforcement Learning for Microcredit Default Prediction","M. Song; J. Wang; T. Zhang; G. Zhang; R. Zhang; S. Su","360 Financial, Beijing, China; 360 Financial, Beijing, China; 360 Financial, Beijing, China; 360 Financial, Beijing, China; 360 Financial, Beijing, China; 360 Financial, Beijing, China","2020 International Joint Conference on Neural Networks (IJCNN)","28 Sep 2020","2020","","","1","8","Microcredit is a new financial instrument serving the segment of population that typically lack collateral and are highly likely to be rejected by traditional financial institutions, by lending very small loans. For platforms that participate in such consumer finance activities, the key challenge lies in risk management and the popular credit scoring method predicting whether a borrower would default or not takes an important role in this field. However, the fact is that we are often facing mass of raw data and the traditional credit scoring is heavily depending on feature engineering involving domain expert knowledge, intuition and trial and error, which is often time consuming. It is very challenging to derive effective features from raw data as the searching space can be very large with noninformative features. In this paper, we propose a new performance-driven framework automated generating discriminating features from raw data via reinforcement learning to help improve the default prediction of the downstream classifier which may be a logistic regression or boosting tree. Specially, we first define a formal paradigm for the automated feature derivation framework which unifies the feature structure, its interpretation and the calculation logic together. For the particularity of the financial industry, the interpretation of the feature is often of high interest. Then we reformulate the feature generation problem as reinforcement learning by constructing a transformation link and regarding it as a sequential decision process. In addition, we carry out an effective practice on default prediction in consumer finance. Finally, experimental results on the data of user behavior log from 360 Financial show the significant improvement of the proposed method over our years of domain expert knowledge and the Genetic Programming. Moreover, this FDRL framework can be easily adapted to other applications due to its versatility.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9207410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207410","Credit default;Feature derivation;Feature interpretation;Reinforcement learning","Reinforcement learning;Mathematical model;Logistics;Support vector machines;Search problems;Computer architecture;Risk management","credit transactions;financial data processing;financial management;genetic algorithms;learning (artificial intelligence);pattern classification;regression analysis;risk management;search problems;trees (mathematics)","logistic regression;financial industry;feature generation problem;reinforcement learning;domain expert knowledge;microcredit default prediction;financial institutions;consumer finance activities;risk management;feature engineering;automated feature derivation;credit scoring;boosting tree;downstream classifier;genetic programming;FDRL","","1","","21","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
"Offshore Petroleum Leaking Source Detection Method From Remote Sensing Data via Deep Reinforcement Learning With Knowledge Transfer","Y. Wang; L. Wang; X. Chen; D. Liang","School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","29 Jul 2022","2022","15","","5826","5840","A marine oil spill is an environmental pollution incident that generally has the attributes of a high speed, widespread, and long duration. It seriously threatens the marine ecological environment and related industries. It is vital to determine the source of the oil leakage so that it may be stopped and related hazards can be reduced. Oil spill accidents in the sea are generally located in offshore and navigation channels. With the rapid development of remote-sensing techniques, oil leak extraction using remote-sensing data has played an essential role in oil spill research. This article proposes a Monte Carlo-based deep Q-transfer-learning network (DQTN) offshore oil leak detection method that uses remote-sensing data. Remote-sensing data are utilized to continuously monitor a marine oil spill on the surface. The estuarine and coastal ocean model is utilized to simulate a marine oil spill event. The deep-Q-network method with offline transferred knowledge is then utilized to determine the marine oil spill source location. In an experiment, based on the Bohai oil spill incident on June 2, 2011, the effectiveness of the remote-sensing-based DQTN marine oil spill search algorithm is verified. The accuracy of the targeted oil spill point is up to 98.97%.","2151-1535","","10.1109/JSTARS.2022.3191122","National Natural Science Foundation of China(grant numbers:U1711266,41925007); Chinese Academy of Sciences(grant numbers:XDA19090128); Fundamental Research Funds for National Universities; China University of Geosciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832464","Deep Q-transfer-learning network (DQTN);estuarine and coastal ocean model (ECOM);Monte Carlo;oil spill detection;remote sensing","Oils;Remote sensing;Numerical models;Monitoring;Accidents;Petroleum;Numerical simulation","ecology;feature extraction;geophysical image processing;image segmentation;marine pollution;Monte Carlo methods;oceanographic techniques;oil pollution;remote sensing","remote-sensing data;oil spill research;Monte Carlo-based deep Q-transfer-learning network offshore oil leak detection method;marine oil spill event;deep-Q-network method;marine oil spill source location;Bohai oil spill incident;remote-sensing-based DQTN marine oil spill search algorithm;targeted oil spill point;offshore petroleum leaking source detection method;remote sensing data;deep reinforcement learning;marine ecological environment;related industries;oil leakage;oil spill accidents;offshore navigation channels;remote-sensing techniques;oil leak extraction","","1","","53","CCBYNCND","18 Jul 2022","","","IEEE","IEEE Journals"
"A Reinforcement Learning Based Service Scheduling Algorithm for Internet of Drones","C. Pu","Dept. of CSEE, Marshall University, Huntington, WV, USA","2022 IEEE International Conference on Communications Workshops (ICC Workshops)","11 Jul 2022","2022","","","999","1004","Originally invented by the military for warfighting, drones have broken through adamant barriers established by traditional commercial and civilian industry and are quickly becoming an accepted part of mainstream. In order to enable drone technology to reach its full potential and integrate heterogeneous drones into existing workflows, Internet of Drones (IoD) has been proposed as a future aerial-ground communication architecture, where drones frequently contact Zone Service Providers (ZSPs) for up-to-date information. When many drones intend to access data through a ZSP concurrently, service scheduling plays a significant role in improving data accessibility. In practice, however, the limited bandwidth and coverage range of ZSP and the high speed of drones make the problem of service scheduling challenging. In this paper, we propose a reinforcement learning based service scheduling algorithm, also called RELESS, to optimally satisfy the service requests of drones in the IoD. In RELESS, the interaction between the ZSP and drones is formulated as a Markov decision process (MDP) which will be solved by the Q-learning algorithm to produce an optimal service scheduling policy. During this process, the ZSP adopts an ∊-greedy exploration method to continuously fine-tune its service scheduling policy with various system states, which is guaranteed to converge to an optimal policy. We develop a discrete-event driven simulation framework using OMNeT++, implement RELESS and its counterparts, and conduct simulation experiments for performance evaluation and comparison. Numerical results demonstrate that RELESS can improve service request satisfaction ratio, service request satisfaction latency, as well as data size satisfaction ratio, indicating a superior service scheduling approach in the IoD.","2694-2941","978-1-6654-2671-8","10.1109/ICCWorkshops53468.2022.9814662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9814662","Service Scheduling;Reinforcement Learning;Drones;Internet of Drones","Performance evaluation;Job shop scheduling;Q-learning;Scheduling algorithms;Conferences;Markov processes;Numerical models","decision theory;greedy algorithms;Internet;Markov processes;performance evaluation;reinforcement learning;remotely operated vehicles;telecommunication scheduling","ZSP;zone service providers;reinforcement learning based service scheduling algorithm;heterogeneous drone technology;warfighting;civilian industry;IoD;RELESS;MDP;Markov decision process;Q-learning algorithm;ϵ-greedy exploration method;discrete-event driven simulation framework;OMNeT++","","1","","21","IEEE","11 Jul 2022","","","IEEE","IEEE Conferences"
"Resource Allocation for Cellular Zero-Touch Deterministic Industrial M2M Networks: A Reinforcement Learning-Based Scheme","Y. -H. Xu; M. Hua; W. Zhou; G. Yu","College of Information Science and Technology, Nanjing Forestry University, Nanjing, China; College of Information Science and Technology, Nanjing Forestry University, Nanjing, China; College of Information Science and Technology, Nanjing Forestry University, Nanjing, China; Department of Electronic and Electrical Engineering, The University of Sheffield, Sheffield, U.K.","IEEE Sensors Letters","11 Aug 2022","2022","6","8","1","4","In this letter, we investigate the resource allocation problem for cellular zero-touch deterministic industrial machine-to-machine networks. We consider a scenario in which multiple self-adaptive industrial machine-to-machine (M2M) networks are deployed underlaying cellular networks to emulate a variety of vertical use cases in future Industry 4.0 with the objective of guaranteeing the determinacy of latency by learning-inspired resource allocation policy. Specifically, in order to figure out the relationship between latency determinacy and the utilization of wireless resources meanwhile modeling the uncertainty of the stochastic environment, we first formulate the resource allocation problem as a Markov game by jointly considering transmission power control, frequency spectrum allocation, and the selection of base stations. Afterward, a random graph-based sparse long short-term memory network is proposed to solve the optimization problem while reducing the computational complexity. Finally, numerical result demonstrates the effectiveness of the proposed scheme.","2475-1472","","10.1109/LSENS.2022.3194141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9842305","Sensor networks;deterministic networks;industrial IOT;long short-term memory (LSTM);resource allocation;zero-touch networks","Resource management;Machine-to-machine communications;Bandwidth;Sensors;Radio spectrum management;Transmitters;Time-frequency analysis","cellular radio;computational complexity;game theory;graph theory;learning (artificial intelligence);Markov processes;power control;radiofrequency interference;resource allocation","Cellular Zero-Touch Deterministic Industrial M2M Networks;reinforcement learning-based scheme;resource allocation problem;machine-to-machine networks;cellular networks;future Industry;learning-inspired resource allocation policy;latency determinacy;wireless resources;frequency spectrum allocation;random graph-based;short-term memory network","","","","7","IEEE","27 Jul 2022","","","IEEE","IEEE Journals"
"Autonomous RACH Resource Slicing for Heterogeneous IoT Devices Communication Using Deep Reinforcement Learning","H. Y. Ali; S. Goulin; A. M. Seid","Ethiopian Technology and Innovation Institute, Addis Ababa, Ethiopia; Department of Computer science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Department of Computer science and Engineering, Zhejiang Normal university, Zhejiang province, China","2021 International Conference on Information and Communication Technology for Development for Africa (ICT4DA)","17 Jan 2022","2021","","","125","130","In a wireless network infrastructure, the initial synchronization process primarily decides whether to send or receive data between a device and base station. This process is usually powered by a random access (RA) mechanism to share and allocate radio resources dynamically. Over the past years, telecommunication industry has witnessed a massive growth in the Internet of Things (IoT) technologies which continue to be rolled out around the world with different services and having a variety of requirements. However, when massive IoT (mIoT) devices attempt to access the network over a limited number of Random Access Channel (RACH) resources within a time frame, the network becomes overloaded, leading to a low performance of human to human (H2H) communication and Quality of Services (QoS) may not be assured. To solve the above problems, we propose a dynamic resource slicing and access class barring (ACB) mechanism using deep reinforcement learning (DRL) for a new RACH scenario to control and manage the resource dynamically. Simulation results prove that our proposed technique provides a fair RACH resource allocation for each class according to the available radio resource.","","978-1-6654-3666-3","10.1109/ICT4DA53266.2021.9672226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672226","deep reinforcement learning;random access channel;massive IoT communication;resource slicing;access class control mechanism;quality of service","Performance evaluation;Simulation;Wireless networks;Process control;Quality of service;Reinforcement learning;Dynamic scheduling","cellular radio;deep learning (artificial intelligence);Internet of Things;Long Term Evolution;quality of service;radio networks;reinforcement learning;resource allocation;telecommunication traffic","dynamic resource slicing;access class;deep reinforcement learning;fair RACH resource allocation;autonomous RACH resource;heterogeneous IoT devices communication;wireless network infrastructure;initial synchronization process;base station;random access mechanism;RA;radio resources;telecommunication industry;Internet of Things technologies;massive IoT devices;Random Access Channel;time frame;human communication","","","","13","IEEE","17 Jan 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Autonomous E-Vehicle Speed Control","S. E; M. S. Krishnan; K. S; R. c. Tanguturi; S. Arunagirinathan; K. N. Gunasekaran; M. S. Ramkumar","Department of Information Technology, Mahendra Institute of Technology (Autonomo us), Namakkal, India; Department of EEE, Karpagam College of Engineering, Coimbatore, India; Department of Electrical and Electronics Engineering, Coimbatore, India; Department of Computer Science and Engineering, Pace Institute of Technology and Sciences, Ongole, Andhra Pradesh, India; Department of Electrical and Electronics Engineering, Government College of Technology, Coimbatore, India; Department of Mechanical Engineering, Sri Krishna College of Engineering and Technology, Coimbatore, India; Dept of EEE, Karpagam Academy of Higher Education, India","2023 International Conference on Inventive Computation Technologies (ICICT)","1 Jun 2023","2023","","","192","198","Greater efficiency in both energy use and traffic flow are two benefits of autonomous self-driving cars. Due to their superior performance, effectiveness, and lack of carbon emission, electric vehicles (EVs) have recently become popular and used in an autonomous vehicle. As EVs are making a splash in the automotive industry, researchers are taking a greater interest in studying, modelling, and simulating them. Controlling EV speed is not an easy task. Modelling and Conventional Proportional Integral Derivative (PID) controller tuning for EV speed regulation are presented in this paper. To design and simulation of EV speed control in MATLAB, the transfer function of EV is derived and considered. PID controller is found to be easy to implement, practical, and provide superior closed-loop performance. Two PID tuning techniques, Ziegler-Nichols (ZN) and a Reinforcement Learning (RL) method are designed to regulate the EV speed. Characteristics of the time domain have been used to perform a comparative analysis. Additionally, the Integral Square Error (ISE) is analysed to determine the optimal PID tuning strategy for EV speed regulation.","2767-7788","979-8-3503-9849-6","10.1109/ICICT57646.2023.10134037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10134037","Electric Vehicle;Reinforcement Learning;Speed;Integral Square Error;PID Controller;Tuning Techniques","Velocity control;Transportation;Transfer functions;Reinforcement learning;Electric vehicles;Regulation;Time measurement","closed loop systems;mobile robots;reinforcement learning;road traffic control;road vehicles;three-term control;transfer functions;velocity control","automotive industry;autonomous e-vehicle speed control;autonomous self-driving cars;autonomous vehicle;carbon emission;closed-loop performance;conventional proportional integral derivative controller tuning;electric vehicles;energy use;EV speed control;EV speed regulation;integral square error;MATLAB;optimal PID tuning strategy;PID controller;reinforcement learning;self-driving cars;traffic flow;transfer function;Ziegler-Nichols method","","","","20","IEEE","1 Jun 2023","","","IEEE","IEEE Conferences"
"An Improved Data Management Approach for IoT-Enabled Smart Healthcare: Integrating Semantic Web and Reinforcement Learning","A. Turkmen; O. Can","Graduate School of Natural and Applied Sciences, Ege University, Izmir, Turkiye; Department of Computer Engineering, Ege University, Izmir, Turkiye","2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC)","2 Aug 2023","2023","","","1471","1475","Smart healthcare systems that are able to monitor and manage patients' health in real-time have emerged as a result of the Internet of Things (IoT)-enabled healthcare systems. However, the volume of data created by IoT devices makes data management more challenging in the healthcare industry. Thus, Reinforcement Learning (RL) and Ontology-Based Data Access (OBDA) have been suggested as approaches to address these issues. RL enables computers to learn from their surroundings by making mistakes. Hence, it is used to enhance data management procedures by recognizing patterns in the data and automatically adapting to changes in the data environment. OBDA provides interoperability and data integration through the usage of ontologies that offer a common language and shared understanding of data. Moreover, OBDA allows effective query processing and the smooth integration of several heterogeneous data sources. Ontologies provide formal representations for concepts and their relationships and facilitate the integration of data from multiple sources by overcoming the variety and variability of data. In this study, an OBDA-based approach for IoT-enabled smart healthcare systems is proposed. The proposed approach integrates OBDA with RL to enhance data management and leverages OBDA to offer a common understanding of data across various healthcare applications by supporting effective data integration and interoperability. Therefore, the paper investigates the usage of OBDA and RL to integrate diverse data sources, adjust index architectures, learn from query patterns, and enhance query performance. Thus, the proposed approach offers a promising solution for handling the vast amounts of data produced by IoT-enabled smart healthcare systems. Besides, the query performance will be optimized and query response time will be decreased.","0730-3157","979-8-3503-2697-0","10.1109/COMPSAC57700.2023.00226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10197029","Ontology;Data Access;Semantic Web;Query;Reinforcement Learning;Internet of Things (IoT);Healthcare","Semantic Web;Soft sensors;Data integration;Medical services;Reinforcement learning;Ontologies;Real-time systems","computer network security;data integration;data integrity;health care;Internet of Things;learning (artificial intelligence);ontologies (artificial intelligence);open systems;query processing;reinforcement learning;semantic Web","data environment;data management procedures;diverse data sources;effective data integration;healthcare applications;healthcare industry;heterogeneous data sources;improved Data management approach;integrating semantic web;interoperability;IoT devices;IoT-enabled smart healthcare systems;leverages OBDA;OBDA-based approach;ontologies;Reinforcement Learning;RL;smooth integration;Things-enabled healthcare systems","","","","29","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Discrete phase shifts control and beam selection in RIS-aided MISO system via deep reinforcement learning","D. Lin; Y. Liu","School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China","China Communications","1 Sep 2023","2023","20","8","198","208","Reconfigurable intelligent surface (RIS) for wireless networks have drawn lots of attention in both academic and industry communities. RIS can dynamically control the phases of the reflection elements to send the signal in the desired direction, thus it provides supplementary links for wireless networks. Most of prior works on RIS-aided wireless communication systems consider continuous phase shifts, but phase shifts of RIS are discrete in practical hardware. Thus we focus on the actual discrete phase shifts on RIS in this paper. Using the advanced deep reinforcement learning (DRL), we jointly optimize the transmit beamforming matrix from the discrete Fourier transform (DFT) codebook at the base station (BS) and the discrete phase shifts at the RIS to maximize the received signal-to-interference plus noise ratio (SINR). Unlike the traditional schemes usually using alternate optimization methods to solve the transmit beamforming and phase shifts, the DRL algorithm proposed in the paper can jointly design the transmit beamforming and phase shifts as the output of the DRL neural network. Numerical results indicate that the DRL proposed can dispose the complicated optimization problem with low computational complexity.","1673-5447","","10.23919/JCC.fa.2022-0128.202308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10238413","reconfigurable intelligent surface;discrete phase shifts;transmit beamforming;deep reinforcement learning","Reflection;Array signal processing;Optimization;Discrete Fourier transforms;MISO communication;Interference;Training","array signal processing;computational complexity;deep learning (artificial intelligence);discrete Fourier transforms;MISO communication;optimisation;reinforcement learning;telecommunication computing;wireless channels","academic industry communities;actual discrete phase shifts;advanced deep reinforcement learning;continuous phase shifts;discrete phase shifts control;DRL neural network;reconfigurable intelligent surface;RIS-aided MISO system;RIS-aided wireless communication systems;wireless networks","","","","","","1 Sep 2023","","","IEEE","IEEE Magazines"
"Integral Reinforcement Learning Control for a Class of High-Order Multivariable Nonlinear Dynamics With Unknown Control Coefficients","Q. Wang","School of Automation, Southeast University, Nanjing, China","IEEE Access","18 May 2020","2020","8","","86223","86229","This paper develops an integral reinforcement learning (IRL) controller for a class of high-order multivariable nonlinear systems with unknown control coefficients (UCCs). A new long-term performance index is first presented, and then the critic neural network (NN) and the action NN are presented to estimate the unobtainable long-term performance index and the unknown drift of systems, respectively. By combining the critic and action NNs with Nussbaum-type functions, the IRL controllers for high-order, nonsquare multivariable systems are proposed to cope with the problem of UCCs. The analysis are given to illustrate that the stability of the closed-loop system can be obtained, and the signals of the closed-loop systems are semiglobally uniformly ultimately bounded (UUB). Finally, one simulation example is provided to show the effectiveness of the proposed IRL controllers.","2169-3536","","10.1109/ACCESS.2020.2993265","Science and Technology on Information System Engineering Laboratory(grant numbers:05201902); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9090153","Nussbaum-type functions;integral reinforcement learning;unknown control coefficients;nonsquare multivariable systems","MIMO communication;Performance analysis;Nonlinear systems;Closed loop systems;Manganese;Learning (artificial intelligence);Artificial neural networks","closed loop systems;Lyapunov methods;multivariable control systems;neurocontrollers;nonlinear control systems;stability;uncertain systems","unknown control coefficients;high-order multivariable nonlinear systems;UCC;long-term performance index;critic neural network;action NN;IRL controllers;nonsquare multivariable systems;closed-loop system;integral reinforcement learning control;high-order multivariable nonlinear dynamics;uniformly ultimately bounded;UUB","","4","","36","CCBY","8 May 2020","","","IEEE","IEEE Journals"
"Optimizing Flying Base Station Connectivity by RAN Slicing and Reinforcement Learning","D. Carrillo Melgarejo; J. Pokorny; P. Seda; A. Narayanan; P. H. J. Nardelli; M. Rasti; J. Hosek; M. Seda; D. Z. Rodríguez; Y. Koucheryavy; G. Fraidenraich","Department of Electrical Engineering, School of Energy Systems, Lappeenranta-Lahti University of Technology (LUT), Lappeenranta, Finland; Department of Telecommunications, Faculty of Electrical Engineering and Communication, Brno University of Technology, Brno, Czech Republic; Department of Telecommunications, Faculty of Electrical Engineering and Communication, Brno University of Technology, Brno, Czech Republic; Department of Electrical Engineering, School of Energy Systems, Lappeenranta-Lahti University of Technology (LUT), Lappeenranta, Finland; Department of Electrical Engineering, School of Energy Systems, Lappeenranta-Lahti University of Technology (LUT), Lappeenranta, Finland; Department of Electrical Engineering, School of Energy Systems, Lappeenranta-Lahti University of Technology (LUT), Lappeenranta, Finland; Department of Telecommunications, Faculty of Electrical Engineering and Communication, Brno University of Technology, Brno, Czech Republic; Institute of Automation and Computer Science, Faculty of Mechanical Engineering, Brno University of Technology, Brno, Czech Republic; Department of Computer Science, Federal University of Lavras, Lavras, Minas Gerais, Brazil; Unit of Electrical Engineering, Tampere University, Tampere, Finland; School of Electrical and Computer Engineering, State University of Campinas (UNICAMP), Campinas, Brazil","IEEE Access","24 May 2022","2022","10","","53746","53760","The application of flying base stations (FBS) in wireless communication is becoming a key enabler to improve cellular wireless connectivity. Following this tendency, this research work aims to enhance the spectral efficiency of FBSs using the radio access network (RAN) slicing framework; this optimization considers that FBSs’ location was already defined previously. This framework splits the physical radio resources into three RAN slices. These RAN slices schedule resources by optimizing individual slice spectral efficiency by using a deep reinforcement learning approach. The simulation indicates that the proposed framework generally outperforms the spectral efficiency of the network that only considers the heuristic predefined FBS location, although the gains are not always significant in some specific cases. Finally, spectral efficiency is analyzed for each RAN slice resource and evaluated in terms of service-level agreement (SLA) to indicate the performance of the framework.","2169-3536","","10.1109/ACCESS.2022.3175487","Academy of Finland through the Framework for the Identification of Rare Events via MAchine learning and IoT Networks (FIREMAN) Consortium(grant numbers:CHIST-ERA-17-BDSI-003/n.326270); EnergyNet Research Fellowship(grant numbers:321265/n.328869); Jane and Aatos Erkko Foundation through the Swarming Technology for Reliable and Energy-aware Aerial Missions (STREAM) Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9775679","Flying base stations;UAVs;location optimization;wireless communication;deep-reinforcement learning","Interference;Wireless communication;Resource management;Radio access networks;Deep learning;Electrical engineering;Reinforcement learning","femtocellular radio;learning (artificial intelligence);optimisation;radio access networks;resource allocation;telecommunication scheduling","flying base station connectivity;RAN slicing;base stations;wireless communication;cellular wireless connectivity;radio access network slicing framework;FBS location;physical radio resources;RAN slices schedule resources;individual slice spectral efficiency;deep reinforcement learning approach;heuristic predefined FBS location;RAN slice resource","","4","","34","CCBY","16 May 2022","","","IEEE","IEEE Journals"
"Weapon-Target Assignment Strategy in Joint Combat Decision-Making Based on Multi-Head Deep Reinforcement Learning","S. Li; X. He; X. Xu; T. Zhao; C. Song; J. Li","College of Joint Operations, National Defense University, Beijing, China; College of Joint Operations, National Defense University, Beijing, China; College of Joint Operations, National Defense University, Beijing, China; College of Joint Operations, National Defense University, Beijing, China; College of Joint Operations, National Defense University, Beijing, China; Department of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China","IEEE Access","19 Oct 2023","2023","11","","113740","113751","In response to the modeling difficulties and low search efficiency of traditional weapon-target assignment algorithms, this paper proposes a deep reinforcement learning-based intelligent weapon-target assignment method. A weapon-target intelligent assignment model with strong decision-making capabilities (RL4WTA) is obtained by training. Firstly, a multi-constraint weapon-target assignment optimization model is established to discretize the dynamic weapon-target assignment problem into a static weapon-target assignment problem. Furthermore, a planning and solving environment for the weapon-target assignment (WTA) problem is designed, and a Markov Decision Process (MDP) for WTA tasks is constructed based on the planning and solving model. This provides a foundation for solving the WTA problem using reinforcement learning algorithms. Additionally, a reinforcement learning-based WTA-solving model is proposed in this paper. By utilizing a multi-head Q-value network, the complex joint decision space is decoupled, thereby improving the efficiency of the WTA model. The use of a masking mechanism allows for inferring valid actions that satisfy the constraint conditions under the current situation, reducing uncertainty during the reinforcement learning training process. Experimental results show that the proposed model, RL4WTA, can generate satisfactory solutions adaptively in both small-scale and large-scale scenarios. Compared with traditional optimization algorithms, the model is superior in adaptability and computational efficiency, meeting the requirements of making optimal decisions for weapon-target assignment problems.","2169-3536","","10.1109/ACCESS.2023.3324193","national(grant numbers:62006235); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10283838","Weapon target allocation;deep reinforcement learning;operations research;mission planning","Weapons;Optimization;Reinforcement learning;Heuristic algorithms;Discrete wavelet transforms;Computational modeling;Resource management;Strategic planning;Object detection;Target tracking","","","","","","34","CCBY","12 Oct 2023","","","IEEE","IEEE Journals"
"Off-Policy Reinforcement Learning for  $ H_\infty $  Control Design","B. Luo; H. -N. Wu; T. Huang","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Science and Technology on Aircraft Control Laboratory, Beihang University (Beijing University of Aeronautics and Astronautics), Beijing, China; Texas A& M University at Qatar, Doha, Qatar","IEEE Transactions on Cybernetics","20 May 2017","2015","45","1","65","76","The H∞ control design problem is considered for nonlinear systems with unknown internal system model. It is known that the nonlinear H∞ control problem can be transformed into solving the so-called Hamilton-Jacobi-Isaacs (HJI) equation, which is a nonlinear partial differential equation that is generally impossible to be solved analytically. Even worse, model-based approaches cannot be used for approximately solving HJI equation, when the accurate system model is unavailable or costly to obtain in practice. To overcome these difficulties, an off-policy reinforcement leaning (RL) method is introduced to learn the solution of HJI equation from real system data instead of mathematical system model, and its convergence is proved. In the off-policy RL method, the system data can be generated with arbitrary policies rather than the evaluating policy, which is extremely important and promising for practical systems. For implementation purpose, a neural network (NN)-based actor-critic structure is employed and a least-square NN weight update algorithm is derived based on the method of weighted residuals. Finally, the developed NN-based off-policy RL method is tested on a linear F16 aircraft plant, and further applied to a rotational/translational actuator system.","2168-2275","","10.1109/TCYB.2014.2319577","National Basic Research Program of China 973 Program(grant numbers:2012CB720003); National Natural Science Foundation of China(grant numbers:61121003); General Research Fund project from Science and Technology on Aircraft Control Laboratory of Beihang University(grant numbers:9140C480301130C48001); NPRP from the Qatar National Research Fund (a member of Qatar Foundation)(grant numbers:# NPRP 4-1162-1-181); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6813673","H∞ control design;Hamilton--Jacobi--Isaacs equation;neural network;off-policy learning;reinforcement learning;$ H_\infty $ control design;Hamilton–Jacobi–Isaacs equation;neural network;off-policy learning;reinforcement learning","Mathematical model;Equations;Artificial neural networks;Control design;Approximation methods;Cost function;Algorithm design and analysis","control system synthesis;H∞ control;learning (artificial intelligence);least squares approximations;neurocontrollers;nonlinear control systems;nonlinear differential equations;partial differential equations","off-policy reinforcement learning;H∞ control design problem;nonlinear systems;Hamilton-Jacobi-Isaacs equation;HJI equation;nonlinear partial differential equation;off-policy RL method;neural network;actor-critic structure;least-square NN weight update algorithm;weighted residuals method;linear F16 aircraft plant;rotational-translational actuator system","","266","","64","IEEE","9 May 2014","","","IEEE","IEEE Journals"
"Model-Free Real-Time EV Charging Scheduling Based on Deep Reinforcement Learning","Z. Wan; H. Li; H. He; D. Prokhorov","Department of Electrical, University of Rhode Island, South Kingstown, RI, USA; Laboratory of Networked Control Systems, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Department of Electrical, University of Rhode Island, South Kingstown, RI, USA; Mobility Research Department, Toyota Research Institute, North America, Ann Arbor, MI, USA","IEEE Transactions on Smart Grid","21 Aug 2019","2019","10","5","5246","5257","Driven by the recent advances in electric vehicle (EV) technologies, EVs have become important for smart grid economy. When EVs participate in demand response program which has real-time pricing signals, the charging cost can be greatly reduced by taking full advantage of these pricing signals. However, it is challenging to determine an optimal charging strategy due to the existence of randomness in traffic conditions, user's commuting behavior, and the pricing process of the utility. Conventional model-based approaches require a model of forecast on the uncertainty and optimization for the scheduling process. In this paper, we formulate this scheduling problem as a Markov Decision Process (MDP) with unknown transition probability. A model-free approach based on deep reinforcement learning is proposed to determine the optimal strategy for this problem. The proposed approach can adaptively learn the transition probability and does not require any system model information. The architecture of the proposed approach contains two networks: a representation network to extract discriminative features from the electricity prices and a Q network to approximate the optimal action-value function. Numerous experimental results demonstrate the effectiveness of the proposed approach.","1949-3061","","10.1109/TSG.2018.2879572","Office of Naval Research(grant numbers:N00014-18-1-2396); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8521585","Deep reinforcement learning;model-free;EV charging scheduling","Batteries;Real-time systems;Optimal scheduling;Electric vehicle charging;Uncertainty;Pricing","electric vehicle charging;learning (artificial intelligence);Markov processes;optimisation;power engineering computing;smart power grids","real-time EV charging scheduling;deep reinforcement learning;electric vehicle technologies;smart grid economy;demand response program;real-time pricing signals;optimal charging strategy;traffic conditions;pricing process;unknown transition probability;model-free approach;electricity prices;Markov decision process;optimal action-value function","","254","","50","IEEE","4 Nov 2018","","","IEEE","IEEE Journals"
"A Deep Reinforcement Learning Based Offloading Game in Edge Computing","Y. Zhan; S. Guo; P. Li; J. Zhang","Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan; School of Automation, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Computers","8 May 2020","2020","69","6","883","893","Edge computing is a new paradigm to provide strong computing capability at the edge of pervasive radio access networks close to users. A critical research challenge of edge computing is to design an efficient offloading strategy to decide which tasks can be offloaded to edge servers with limited resources. Although many research efforts attempt to address this challenge, they need centralized control, which is not practical because users are rational individuals with interests to maximize their benefits. In this article, we study to design a decentralized algorithm for computation offloading, so that users can independently choose their offloading decisions. Game theory has been applied in the algorithm design. Different from existing work, we address the challenge that users may refuse to expose their information about network bandwidth and preference. Therefore, it requires that our solution should make the offloading decision without such knowledge. We formulate the problem as a partially observable Markov decision process (POMDP), which is solved by a policy gradient deep reinforcement learning (DRL) based approach. Extensive simulation results show that our proposal significantly outperforms existing solutions.","1557-9956","","10.1109/TC.2020.2969148","General Research Fund of the Research Grants Council of Hong Kong(grant numbers:PolyU 152221/19E); National Natural Science Foundation of China(grant numbers:61872310); JSPS Grants-in-Aid for Scientific Research(grant numbers:JP19K20258); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967118","Edge computing;computation offloading;Nash equilibrium;partially observable Markov decision process (POMDP);deep reinforcement learning (DRL)","Servers;Task analysis;Edge computing;Games;Computational modeling;Reinforcement learning;Nash equilibrium","decision theory;distributed processing;game theory;gradient methods;learning (artificial intelligence);Markov processes;mobile computing;radio access networks","mobile applications;POMDP;partially observable Markov decision process;deep reinforcement learning based offloading game;policy gradient deep reinforcement learning based approach;computation offloading;edge servers;pervasive radio access networks;strong computing capability;edge computing","","106","","42","IEEE","23 Jan 2020","","","IEEE","IEEE Journals"
"Edge QoE: Computation Offloading With Deep Reinforcement Learning for Internet of Things","H. Lu; X. He; M. Du; X. Ruan; Y. Sun; K. Wang","College of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; College of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China; National-Local Joint Engineering Laboratory for Digitalized Electrical Design Technology, Wenzhou University, Wenzhou, China; School of Automation and School of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Electrical and Computer Engineering, University of California at Los Angeles, Los Angeles, CA, USA","IEEE Internet of Things Journal","9 Oct 2020","2020","7","10","9255","9265","In edge-enabled Internet of Things (IoT), computation offloading service is expected to offer users with better Quality of Experience (QoE) than traditional IoT. Unfortunately, the growing multiple tasks from users are occuring with the emergence of the IoT environment. Meanwhile, the current computation offloading with QoE is solved by deep reinforcement learning (DRL) with the issue of instability and slow convergence. Therefore, improving the QoE in edge-enabled IoT is still the ultimate challenge. In this article, to enhance the QoE, we propose a new QoE model to study the computation offloading. Specifically, the emerged QoE model can capture three influential elements: 1) service latency determined by local computing latency and transmission latency; 2) energy consumption according to local calculation and transmission consumption; and 3) task success rate based on the coding error probability. Moreover, we improve the deep deterministic policy gradients (DDPG) algorithm and propose a algorithm named the double-dueling-deterministic policy gradients (D3PG) based on the proposed model. Specifically, the actor network highly relies on the critic network, which makes the performance of the DDPG sensitive to the critic and thus leads to poor stability and slow convergence in the computation offloading process. To solve this, we redesign the critic network by using Double Q -learning and Dueling networks. Extensive experiments verify the better stability and faster convergence of our proposed algorithm than existing methods. In addition, experiments also indicate that our proposed algorithm can improve the QoE performance.","2327-4662","","10.1109/JIOT.2020.2981557","National Natural Science Foundation of China(grant numbers:61872195,61772286); “333” Project of Jiangsu Province, China(grant numbers:BRA2017401); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9039641","Computation offloading;deep reinforcement learning (DRL);edge;Internet of Things (IoT);Quality of Experience (QoE)","Quality of experience;Task analysis;Internet of Things;Computational modeling;Servers;Convergence;Energy consumption","energy consumption;Internet of Things;learning (artificial intelligence);neural nets;quality of experience;telecommunication power management","Double Q -learning;double-dueling-deterministic policy gradients;deep deterministic policy gradients algorithm;QoE model;IoT environment;computation offloading service;edge-enabled Internet of Things;deep reinforcement learning;edge QoE","","103","","40","IEEE","17 Mar 2020","","","IEEE","IEEE Journals"
"Language-Driven Temporal Activity Localization: A Semantic Matching Reinforcement Learning Model","W. Wang; Y. Huang; L. Wang","National Laboratory of Pattern Recognition (NLPR), Center for Research on Intelligent Perception and Computing (CRIPAC); National Laboratory of Pattern Recognition (NLPR), Center for Research on Intelligent Perception and Computing (CRIPAC); National Laboratory of Pattern Recognition (NLPR), Center for Research on Intelligent Perception and Computing (CRIPAC)","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","334","343","Current studies on action detection in untrimmed videos are mostly designed for action classes, where an action is described at word level such as jumping, tumbling, swing, etc. This paper focuses on a rarely investigated problem of localizing an activity via a sentence query which would be more challenging and practical. Considering that current methods are generally time-consuming due to the dense frame-processing manner, we propose a recurrent neural network based reinforcement learning model which selectively observes a sequence of frames and associates the given sentence with video content in a matching-based manner. However, directly matching sentences with video content performs poorly due to the large visual-semantic discrepancy. Thus, we extend the method to a semantic matching reinforcement learning (SM-RL) model by extracting semantic concepts of videos and then fusing them with global context features. Extensive experiments on three benchmark datasets, TACoS, Charades-STA and DiDeMo, show that our method achieves the state-of-the-art performance with a high detection speed, demonstrating both effectiveness and efficiency of our method.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954090","Video Analytics;Vision + Language","","image representation;learning (artificial intelligence);natural language processing;recurrent neural nets;video signal processing","visual-semantic discrepancy;matching sentences;video content;recurrent neural network based reinforcement learning model;dense frame-processing manner;sentence query;word level;action classes;untrimmed videos;action detection;semantic matching reinforcement learning model;language-driven temporal activity localization","","90","","28","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Approach for VNF Forwarding Graph Embedding","P. T. A. Quang; Y. Hadjadj-Aoul; A. Outtagarts","Inria, Univ. Rennes, CNRS, IRISA, Rennes, France; Inria, Univ. Rennes, CNRS, IRISA, Rennes, France; E2E Network & Service Automation Lab, Nokia Bell Labs, Nozay, France","IEEE Transactions on Network and Service Management","10 Dec 2019","2019","16","4","1318","1331","Network Function Virtualization (NFV) and service orchestration simplify the deployment and management of network and telecommunication services. The deployment of these services requires, typically, the allocation of Virtual Network Function - Forwarding Graph (VNF-FG), which implies not only the fulfillment of the service's requirements in terms of Quality of Service (QoS), but also considering the constraints of the underlying infrastructure. This topic has been well-studied in existing literature, however, its complexity and uncertainty of available information unveil challenges for researchers and engineers. In this paper, we explore the potential of reinforcement learning techniques for the placement of VNF-FGs. However, it turns out that even the most well-known learning technique is ineffective in the context of a large-scale action space. In this respect, we propose approaches to find out feasible solutions while improving significantly the exploration of the action space. The simulation results clearly show the effectiveness of the proposed learning approach for this category of problems. Moreover, thanks to the deep learning process, the performance of the proposed approach is improved over time.","1932-4537","","10.1109/TNSM.2019.2947905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8873660","Network function virtualization;VNF-FG embedding;Deep reinforcement learning;Quality of Services","Resource management;Optimization;Space exploration;Reinforcement learning;Quality of service;Complexity theory;Convergence","graph theory;learning (artificial intelligence);quality of service;telecommunication services;virtualisation","VNF-FG;VNF forwarding graph;network function virtualization;NFV;telecommunication services;virtual network function;deep reinforcement learning;quality of service","","84","","60","IEEE","17 Oct 2019","","","IEEE","IEEE Journals"
"An approach to tune fuzzy controllers based on reinforcement learning for autonomous vehicle control","Xiaohui Dai; Chi-Kwong Li; A. B. Rad","Rockwell Automation Shanghai Research Center, Shanghai, China; Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong, China; Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong, China","IEEE Transactions on Intelligent Transportation Systems","6 Sep 2005","2005","6","3","285","293","In this paper, we suggest a new approach for tuning parameters of fuzzy controllers based on reinforcement learning. The architecture of the proposed approach is comprised of a Q estimator network (QEN) and a Takagi-Sugeno-type fuzzy inference system (TSK-FIS). Unlike other fuzzy Q-learning approaches that select an optimal action based on finite discrete actions, the proposed controller obtains the control output directly from TSK-FIS. With the proposed architecture, the learning algorithms for all the parameters of the QEN and the FIS are developed based on the temporal-difference (TD) methods as well as the gradient-descent algorithm. The performance of the proposed design technique is illustrated by simulation studies of a vehicle longitudinal-control system.","1558-0016","","10.1109/TITS.2005.853698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1504788","Autonomous vehicles;fuzzy controllers;longitudinal control;reinforcement learning","Fuzzy control;Remotely operated vehicles;Mobile robots;Fuzzy systems;Control systems;Optimal control;Supervised learning;Programmable control;Adaptive control;Intelligent transportation systems","fuzzy control;learning (artificial intelligence);fuzzy systems;inference mechanisms;gradient methods;vehicles","fuzzy controller tuning;reinforcement learning;autonomous vehicle control;Q estimator network;Takagi-Sugeno-type fuzzy inference system;fuzzy Q-learning;temporal-difference methods;gradient-descent algorithm","","82","","36","IEEE","6 Sep 2005","","","IEEE","IEEE Journals"
"Maneuver Decision of UAV in Short-Range Air Combat Based on Deep Reinforcement Learning","Q. Yang; J. Zhang; G. Shi; J. Hu; Y. Wu","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China","IEEE Access","2 Jan 2020","2020","8","","363","378","With the development of artificial intelligence and integrated sensor technologies, unmanned aerial vehicles (UAVs) are more and more applied in the air combats. A bottleneck that constrains the capability of UAVs against manned vehicles is the autonomous maneuver decision, which is a very challenging problem in the short-range air combat undergoing highly dynamic and uncertain maneuvers of enemies. In this paper, an autonomous maneuver decision model is proposed for the UAV short-range air combat based on reinforcement learning, which mainly includes the aircraft motion model, one-to-one short-range air combat evaluation model and the maneuver decision model based on deep Q network (DQN). However, such model includes a high dimensional state and action space which requires huge computation load for DQN training using traditional methods. Then, a phased training method, called “basic-confrontation”, which is based on the idea that human beings gradually learn from simple to complex is proposed to help reduce the training time while getting suboptimal but efficient results. Finally, one-to-one short-range air combats are simulated under different target maneuver policies. Simulation results show that the proposed maneuver decision model and training method can help the UAV achieve autonomous decision in the air combats and obtain an effective decision policy to defeat the opponent.","2169-3536","","10.1109/ACCESS.2019.2961426","Aeronautical Science Foundation of China(grant numbers:2017ZC53033); National Natural Science Foundation of China(grant numbers:61603303,61803309); Natural Science Foundation of Shaanxi Province(grant numbers:2018JQ6070); China Postdoctoral Science Foundation(grant numbers:2018M633574); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938773","Deep reinforcement learning;maneuver decision;independent decision;deep Q network;network training","Atmospheric modeling;Aircraft;Reinforcement learning;Training;Unmanned aerial vehicles;Aerospace control;Optimization","aircraft control;autonomous aerial vehicles;decision making;learning (artificial intelligence);military aircraft;motion control;neurocontrollers","basic-confrontation phased training;DQN training;deep Q network;aircraft motion model;unmanned aerial vehicles;integrated sensor technologies;artificial intelligence;target maneuver;UAV short-range air combat;autonomous maneuver decision model;air combats;deep reinforcement learning","","72","","25","CCBY","23 Dec 2019","","","IEEE","IEEE Journals"
"Event-Triggered Optimal Neuro-Controller Design With Reinforcement Learning for Unknown Nonlinear Systems","X. Yang; H. He; D. Liu","Department of Electrical, Computer, and Biomedical Engineering, University of Rhode Island, Kingston, RI, USA; Department of Electrical, Computer, and Biomedical Engineering, University of Rhode Island, Kingston, RI, USA; School of Automation, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","16 Aug 2019","2019","49","9","1866","1878","This paper develops an optimal control scheme for continuous-time unknown nonlinear systems using the event-triggering mechanism. Different from designing controllers using the time-triggering mechanism, the event-triggered controller is updated only when the system state deviates more than a certain threshold from a prescribed value. To obtain the event-triggered optimal controller, we develop an identifier-critic architecture under the framework of reinforcement learning. The identifier network, composed of a feedforward neural network (FNN), aims to derive the knowledge of unknown system dynamics, and the critic network, constituted of an FNN, intends to derive the event-triggered optimal controller. The identifier network is tuned via the combination of a standard back-propagation algorithm and an e-modification method, and the critic network is updated using a modification of the gradient descent method. By introducing an additional stability term to update the critic network, the initial admissible control is no longer required. Meanwhile, by using historical and instantaneous state data together, the persistence of excitation condition is relaxed. A stability analysis of the closed-loop system is provided based on the Lyapunov method. The effectiveness of the proposed designs is illustrated through simulations of a nonlinear example and a single link robot arm system.","2168-2232","","10.1109/TSMC.2017.2774602","National Natural Science Foundation of China(grant numbers:51529701,61503379,61533017); China Scholarship Council; National Science Foundation(grant numbers:CMMI 1526835); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8183439","Adaptive dynamic programming (ADP);event-triggered control;neural networks (NNs);nonlinear systems;optimal control;reinforcement learning (RL)","Optimal control;Nonlinear systems;Computer architecture;Heuristic algorithms;Stability analysis;Dynamic programming;Artificial neural networks","backpropagation;closed loop systems;continuous time systems;control system synthesis;feedforward neural nets;gradient methods;learning systems;Lyapunov methods;neurocontrollers;nonlinear control systems;optimal control;stability","event-triggered optimal controller;identifier-critic architecture;reinforcement learning;feedforward neural network;unknown system dynamics;critic network;closed-loop system;single link robot arm system;optimal control scheme;continuous-time unknown nonlinear systems;event-triggering mechanism;time-triggering mechanism;admissible control;event-triggered optimal neurocontroller design;standard back-propagation algorithm;e-modification method;gradient descent method;stability term;Lyapunov method","","64","","52","IEEE","11 Dec 2017","","","IEEE","IEEE Journals"
"Parallel reinforcement learning-based energy efficiency improvement for a cyber-physical system","T. Liu; B. Tian; Y. Ai; F. -Y. Wang","Department of Automotive Engineering, Chongqing University, Chongqing, China; Department of Automotive Engineering, Chongqing University, Chongqing, China; Vehicle Intelligence Pioneers Inc., Qingdao, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Beijing, China","IEEE/CAA Journal of Automatica Sinica","27 Feb 2020","2020","7","2","617","626","As a complex and critical cyber-physical system ( CPS ) , the hybrid electric powertrain is significant to mitigate air pollution and improve fuel economy. Energy management strategy ( EMS ) is playing a key role to improve the energy efficiency of this CPS. This paper presents a novel bidirectional long short-term memory ( LSTM ) network based parallel reinforcement learning ( PRL ) approach to construct EMS for a hybrid tracked vehicle ( HTV ) . This method contains two levels. The high-level establishes a parallel system first, which includes a real powertrain system and an artificial system. Then, the synthesized data from this parallel system is trained by a bidirectional LSTM network. The lower-level determines the optimal EMS using the trained action state function in the model-free reinforcement learning ( RL ) framework. PRL is a fully data-driven and learning-enabled approach that does not depend on any prediction and predefined rules. Finally, real vehicle testing is implemented and relevant experiment data is collected and calibrated. Experimental results validate that the proposed EMS can achieve considerable energy efficiency improvement by comparing with the conventional RL approach and deep RL.","2329-9274","","10.1109/JAS.2020.1003072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9016408","","Logic gates;Neurons;Mechanical power transmission;Energy efficiency;Reinforcement learning;Biological neural networks","air pollution;cyber-physical systems;energy management systems;fuel economy;hybrid electric vehicles;learning (artificial intelligence);mechanical engineering computing;power transmission (mechanical)","CPS;hybrid electric powertrain;air pollution;fuel economy;energy management strategy;bidirectional long short-term memory network;hybrid tracked vehicle;powertrain system;bidirectional LSTM network;optimal EMS;trained action state function;model-free reinforcement learning framework;parallel reinforcement learning;complex cyber-physical system;critical cyber-physical system;energy efficiency improvement","","63","","","","27 Feb 2020","","","IEEE","IEEE Journals"
"Privacy-Preserving Localization for Underwater Sensor Networks via Deep Reinforcement Learning","J. Yan; Y. Meng; X. Yang; X. Luo; X. Guan","Institute of Electrical Engineering, Yanshan University, Qinhuangdao, China; Institute of Electrical Engineering, Yanshan University, Qinhuangdao, China; Institute of Information Science and Engineering, Yanshan University, Qinhuangdao, China; Institute of Electrical Engineering, Yanshan University, Qinhuangdao, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Information Forensics and Security","5 Jan 2021","2021","16","","1880","1895","Underwater sensor networks (USNs) are envisioned to enable a large variety of marine applications. Such applications require accurate position information of sensor nodes. However, the openness and inhomogeneity characteristics of underwater medium make it much more challenging to solve the localization issue. This paper is concerned with a privacy-preserving localization issue for USNs in inhomogeneous underwater medium. An honest-but-curious model is considered to develop a privacy-preserving localization protocol. Based on this, a localization problem is constructed for sensor nodes to minimize the sum of all measurement errors, where a ray compensation strategy is incorporated to remove the localization bias from assuming the straight-line transmission. To make the above problem tractable, we consider the unsupervised, supervised and semisupervised scenarios, through which deep reinforcement learning (DRL) based localization estimators are utilized to estimate the positions of sensor nodes. It is noted that, the proposed localization solution in this paper can hide the private position information of USNs, and more importantly, it is robust to local optimum for nonconvex and nonsmooth localization problem in inhomogeneous underwater medium. Finally, simulation studies are given to show the position privacy can be preserved, while the localization accuracy can be enhanced as compared with the other existing works.","1556-6021","","10.1109/TIFS.2020.3045320","NSFC(grant numbers:62033011,61873345,61973263); Youth Talent Support Program of Hebei(grant numbers:BJ2018050,BJ2020031); NSF of Hebei(grant numbers:F2020203002); Postgraduate Innovation Fund Project of Hebei(grant numbers:CXZZSS2021066); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9296303","Privacy preserving;localization;deep reinforcement learning;underwater sensor networks;inhomogeneous","Protocols;Nonhomogeneous media;Encryption;Privacy;Reinforcement learning;Receivers;Water","data privacy;learning (artificial intelligence);neural nets;telecommunication computing;underwater acoustic communication;wireless sensor networks","underwater sensor networks;USNs;marine applications;accurate position information;sensor nodes;inhomogeneity characteristics;privacy-preserving localization issue;inhomogeneous underwater medium;honest-but-curious model;privacy-preserving localization protocol;localization bias;deep reinforcement learning based localization estimators;localization solution;private position information;nonconvex localization problem;nonsmooth localization problem;position privacy;localization accuracy","","54","","49","IEEE","16 Dec 2020","","","IEEE","IEEE Journals"
"Optimal Robust Output Containment of Unknown Heterogeneous Multiagent System Using Off-Policy Reinforcement Learning","S. Zuo; Y. Song; F. L. Lewis; A. Davoudi","UTA Research Institute, University of Texas at Arlington, Fort Worth, TX, USA; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; UTA Research Institute, University of Texas at Arlington, Fort Worth, TX, USA; UTA Research Institute, University of Texas at Arlington, Fort Worth, TX, USA","IEEE Transactions on Cybernetics","14 Oct 2018","2018","48","11","3197","3207","This paper investigates optimal robust output containment problem of general linear heterogeneous multiagent systems (MAS) with completely unknown dynamics. A modelbased algorithm using offline policy iteration (PI) is first developed, where the p-copy internal model principle is utilized to address the system parameter variations. This offline PI algorithm requires the nominal model of each agent, which may not be available in most real-world applications. To address this issue, a discounted performance function is introduced to express the optimal robust output containment problem as an optimal output-feedback design problem with bounded L2-gain. To solve this problem online in real time, a Bellman equation is first developed to evaluate a certain control policy and find the updated control policies, simultaneously, using only the state/output information measured online. Then, using this Bellman equation, a model-free off-policy integral reinforcement learning algorithm is proposed to solve the optimal robust output containment problem of heterogeneous MAS, in realtime, without requiring any knowledge of the system dynamics. Simulation results are provided to verify the effectiveness of the proposed method.","2168-2275","","10.1109/TCYB.2017.2761878","State Key Development Program for Basic Research of China(grant numbers:2012CB215202); National Science Foundation(grant numbers:ECCS-1405173); Office of Naval Research(grant numbers:N00014-17-1-2239); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8089381","Heterogeneous systems;integral reinforcement learning (RL);internal model principle;optimal robust output containment;output-feedback","Robustness;Heuristic algorithms;Mathematical model;System dynamics;Learning (artificial intelligence);Multi-agent systems;Real-time systems","control system synthesis;feedback;iterative methods;learning (artificial intelligence);multi-agent systems;nonlinear control systems;optimal control;robust control","optimal robust output containment problem;unknown heterogeneous multiagent system;offline policy iteration;offline PI algorithm;optimal output-feedback design problem;linear heterogeneous multiagent systems;model-free off-policy integral reinforcement learning;MAS;p-copy internal model principle;system parameter variation;Bellman equation;state-output information measured online;system dynamics;bounded l2-gain","","52","","49","IEEE","30 Oct 2017","","","IEEE","IEEE Journals"
"Data-Driven Multi-Agent Deep Reinforcement Learning for Distribution System Decentralized Voltage Control With High Penetration of PVs","D. Cao; J. Zhao; W. Hu; F. Ding; Q. Huang; Z. Chen; F. Blaabjerg","School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Power Systems Engineering Center, National Renewable Energy Laboratory, Golden, CO, USA; School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Energy Technology, Aalborg University, Aalborg, Denmark; Department of Energy Technology, Aalborg University, Aalborg, Denmark","IEEE Transactions on Smart Grid","20 Aug 2021","2021","12","5","4137","4150","This paper proposes a novel model-free/data-driven centralized training and decentralized execution multi-agent deep reinforcement learning (MADRL) framework for distribution system voltage control with high penetration of PVs. The proposed MADRL can coordinate both the real and reactive power control of PVs with existing static var compensators and battery storage systems. Unlike the existing DRL-based voltage control methods, our proposed method does not rely on a system model during both the training and execution stages. This is achieved by developing a new interaction scheme between the surrogate modeling of the original system and the multi-agent soft actor critic (MASAC) MADRL algorithm. In particular, the sparse pseudo-Gaussian process with a few-shots of measurements is utilized to construct the surrogate model of the original environment, i.e., power flow model. This is a data-driven process and no model parameters are needed. Furthermore, the MASAC enabled MADRL allows to achieve better scalability by dividing the original system into different voltage control regions with the aid of real and reactive power sensitivities to voltage, where each region is treated as an agent. This also serves as the foundation for the centralized training and decentralized execution, thus significantly reducing the communication requirements as only local measurements are required for control. Comparative results with other alternatives on the IEEE 123-nodes and 342-nodes systems demonstrate the superiority of the proposed method.","1949-3061","","10.1109/TSG.2021.3072251","National Key Research and Development Program of China(grant numbers:2018YFE0127600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9399637","Voltage regulation;Gaussian process regression;network partition;multi-agent deep reinforcement learning;distribution network;PVs","Voltage control;Training;Reactive power;Static VAr compensators;Optimization;Load modeling;Uncertainty","decentralised control;distributed power generation;distribution networks;Gaussian processes;learning (artificial intelligence);load flow;multi-agent systems;power distribution control;reactive power;reactive power control;static VAr compensators;voltage control","system model;surrogate modeling;multiagent soft actor critic MADRL algorithm;MASAC;sparse pseudoGaussian process;surrogate model;original environment;power flow model;data-driven process;model parameters;different voltage control regions;reactive power sensitivities;342-nodes systems;data-driven multiagent deep reinforcement learning;distribution system decentralized voltage control;execution multiagent deep reinforcement learning framework;distribution system voltage control;real power control;reactive power control;static var compensators;battery storage systems;existing DRL-based voltage control methods","","51","","28","IEEE","9 Apr 2021","","","IEEE","IEEE Journals"
"AUV-Aided Localization for Internet of Underwater Things: A Reinforcement-Learning-Based Method","J. Yan; Y. Gong; C. Chen; X. Luo; X. Guan","Department of Electrical Engineering, Yanshan University, Qinhuangdao, China; Department of Electrical Engineering, Yanshan University, Qinhuangdao, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical Engineering, Yanshan University, Qinhuangdao, China; Department of Electrical Engineering, Yanshan University, Qinhuangdao, China","IEEE Internet of Things Journal","9 Oct 2020","2020","7","10","9728","9746","Localization is a critical issue for many location-based applications in the Internet of Underwater Things (IoUT). Nevertheless, the asynchronous time clock, stratification effect, and mobility properties of the underwater environment make it much more challenging to solve the localization issue. This article is concerned with an autonomous underwater vehicle (AUV)-aided localization issue for IoUT. We first provide a hybrid network architecture that includes surface buoys, AUVs, and active and passive sensor nodes. On the basis of this architecture, an asynchronous localization protocol is designed, through which the localization problem is provided to minimize the sum of all measurement errors. In order to make this problem tractable, a reinforcement-learning (RL)-based localization algorithm is developed to estimate the locations of AUVs, and active and passive sensor nodes, where an online value iteration procedure is performed to seek the optimization locations. It is worth mentioning that the proposed localization algorithm adopts two neural networks to approximate the increment policy and value function, and more importantly, it is much preferable for the nonsmooth and nonconvex underwater localization problem due to its insensitivity to the local optimal. Performance analyses for the RL-based localization algorithm are also provided. Finally, simulation and experimental results reveal that the localization performance in this article can be significantly improved as compared with the other works.","2327-4662","","10.1109/JIOT.2020.2993012","NSFC(grant numbers:61873345,61973263,61633017,61933009); Youth Talent Support Program of Hebei(grant numbers:BJ2018050); NSF of Hebei(grant numbers:F2020203002); Science and Technology Commission Foundation of Shanghai Municipal(grant numbers:19XD1421800); Postgraduate Innovation Fund Project of Hebei(grant numbers:CXZZSS2020049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9089240","Acoustic communication;Internet of Underwater Things (IoUT);localization;reinforcement learning (RL)","Clocks;Navigation;Internet of Things;Protocols;Unmanned aerial vehicles;Optimization","autonomous underwater vehicles;Internet of Things;learning (artificial intelligence);neural nets;optimisation;protocols;underwater acoustic communication","neural networks;online value iteration procedure;autonomous underwater vehicle-aided localization;reinforcement-learning-based localization algorithm;asynchronous localization protocol;passive sensor nodes;active sensor nodes;IoUT;location-based applications;reinforcement-learning-based method;Internet of Underwater Things;AUV-aided localization;RL-based localization algorithm;nonconvex underwater localization problem;nonsmooth localization problem","","49","","58","IEEE","7 May 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning Based Efficiency Optimization Scheme for the DAB DC–DC Converter With Triple-Phase-Shift Modulation","Y. Tang; W. Hu; J. Xiao; Z. Chen; Q. Huang; Z. Chen; F. Blaabjerg","School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Energy Technology, Aalborg University, Aalborg, Denmark; Department of Energy Technology, Aalborg University, Aalborg, Denmark","IEEE Transactions on Industrial Electronics","3 May 2021","2021","68","8","7350","7361","Aim to improve the power efficiency of the dual-active-bridge (DAB) dc–dc converter, an efficiency optimization scheme with triple-phase-shift (TPS) modulation using reinforcement learning (RL) is proposed in this article. More specifically, the Q-learning algorithm, as a typical algorithm of the RL, is applied to train an agent offline to obtain an optimized modulation strategy, and then the trained agent provides control decisions online in a real-time manner for the DAB dc–dc converter according to the current operating environment. The main objective is to obtain the optimal phase-shift angles for the DAB dc–dc converter, which can achieve the maximum power efficiency by reducing the power losses. Moreover, all possible operation modes of the TPS modulation are considered during the offline training process of the Q-learning algorithm. Thus, the cumbersome process for selecting the optimal operation mode in the conventional schemes can be circumvented successfully. Based on these merits, the proposed efficiency optimization scheme using the RL can realize the excellent performances for the whole load conditions and voltage conversion ratios. Finally, a 1.2-KW prototyped is built, and the simulation and the experimental results demonstrate that the power efficiency can be improved by using the optimization scheme based on the RL.","1557-9948","","10.1109/TIE.2020.3007113","Sichuan Science and Technology Program; Sichuan Distinguished Young Scholars(grant numbers:2020JDJQ0037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138774","Dual-active-bridge (DAB) dc–dc converter;optimization;power efficiency;Q-learning;reinforcement learning (RL)","DC-DC power converters;Optimization;Phase modulation;Heuristic algorithms;Inductors;Magnetic losses","bridge circuits;DC-DC power convertors;learning (artificial intelligence);load (electric);losses;optimisation;power engineering computing","DAB DC-DC converter;triple-phase-shift modulation;dual-active-bridge DC-DC converter;RL;Q-learning algorithm;optimal phase-shift angles;maximum power efficiency;optimal operation mode;reinforcement learning based efficiency optimization scheme;power losses reduction;TPS modulation;load conditions;voltage conversion ratio","","49","","31","IEEE","10 Jul 2020","","","IEEE","IEEE Journals"
"Semi-Distributed Resource Management in UAV-Aided MEC Systems: A Multi-Agent Federated Reinforcement Learning Approach","Y. Nie; J. Zhao; F. Gao; F. R. Yu","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; Institute for Artificial Intelligence, Tsinghua University (THUAI), State Key Lab of Intelligent Technologies and Systems, Tsinghua University, Beijing National Research Center for Information Science and Technology (BNRist), Department of Automation, Tsinghua University, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada","IEEE Transactions on Vehicular Technology","17 Dec 2021","2021","70","12","13162","13173","Recently, unmanned aerial vehicle (UAV)-enabled multi-access edge computing (MEC) has been introduced as a promising edge paradigm for the future space-aerial-terrestrial integrated communications. Due to the high maneuverability of UAVs, such a flexible paradigm can improve the communication and computation performance for multiple user equipments (UEs). In this paper, we consider the sum power minimization problem by jointly optimizing resource allocation, user association, and power control in an MEC system with multiple UAVs. Since the problem is nonconvex, we propose a centralized multi-agent reinforcement learning (MARL) algorithm to solve it. However, the centralized method ignores essential issues like distributed framework and privacy concern. We then propose a multi-agent federated reinforcement learning (MAFRL) algorithm in a semi-distributed framework. Meanwhile, we introduce the Gaussian differentials to protect the privacy of all UEs. Simulation results show that the semi-distributed MAFRL algorithm achieves close performances to the centralized MARL algorithm and significantly outperform the benchmark schemes. Moreover, the semi-distributed MAFRL algorithm costs 23$\%$ lower opeartion time than the centralized algorithm.","1939-9359","","10.1109/TVT.2021.3118446","National Natural Science Foundation of China(grant numbers:U2001213,61971191); Beijing Natural Science Foundation(grant numbers:L182018,L201011); National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2020YFB1807204); Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences(grant numbers:20190910); Key Project of Natural Science Foundation of Jiangxi Province(grant numbers:20202ACBL202006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9563249","Unmanned aerial vehicle (UAV);multi-access edge computing (MEC);deep reinforcement learning (DRL);federated learning (FL);resource allocation","Unmanned aerial vehicles;Edge computing;Deep learning;Artificial intelligence;Resource management;Reinforcement learning","autonomous aerial vehicles;cellular radio;learning (artificial intelligence);multi-agent systems;power control;remotely operated vehicles;resource allocation","multiple UAVs;multiagent reinforcement learning algorithm;centralized method;distributed framework;multiagent federated reinforcement learning algorithm;MAFRL algorithm;centralized MARL algorithm;centralized algorithm;semidistributed resource management;UAV-aided MEC systems;multiagent federated reinforcement learning approach;unmanned aerial vehicle-enabled multiaccess edge computing;promising edge paradigm;future space-aerial-terrestrial;flexible paradigm;computation performance;multiple user equipments;sum power minimization problem;resource allocation;user association;power control;MEC system","","47","","39","IEEE","7 Oct 2021","","","IEEE","IEEE Journals"
"3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-Scale 3D Point Clouds","F. Liu; S. Li; L. Zhang; C. Zhou; R. Ye; Y. Wang; J. Lu","Faculty of Geographical Science, Beijing Normal University, Beijing, China; Faculty of Geographical Science, Beijing Normal University, Beijing, China; Faculty of Geographical Science, Beijing Normal University, Beijing, China; Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences; Faculty of Geographical Science, Beijing Normal University, Beijing, China; Faculty of Geographical Science, Beijing Normal University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","2017 IEEE International Conference on Computer Vision (ICCV)","25 Dec 2017","2017","","","5679","5688","Semantic parsing of large-scale 3D point clouds is an important research topic in computer vision and remote sensing fields. Most existing approaches utilize hand-crafted features for each modality independently and combine them in a heuristic manner. They often fail to consider the consistency and complementary information among features adequately, which makes them difficult to capture high-level semantic structures. The features learned by most of the current deep learning methods can obtain high-quality image classification results. However, these methods are hard to be applied to recognize 3D point clouds due to unorganized distribution and various point density of data. In this paper, we propose a 3DCNN-DQN-RNN method which fuses the 3D convolutional neural network (CNN), Deep Q-Network (DQN) and Residual recurrent neural network (RNN)for an efficient semantic parsing of large-scale 3D point clouds. In our method, an eye window under control of the 3D CNN and DQN can localize and segment the points of the object's class efficiently. The 3D CNN and Residual RNN further extract robust and discriminative features of the points in the eye window, and thus greatly enhance the parsing accuracy of large-scale point clouds. Our method provides an automatic process that maps the raw data to the classification results. It also integrates object localization, segmentation and classification into one framework. Experimental results demonstrate that the proposed method outperforms the state-of-the-art point cloud classification methods.","2380-7504","978-1-5386-1032-9","10.1109/ICCV.2017.605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237867","","Three-dimensional displays;Machine learning;Semantics;Feature extraction;Shape;Image color analysis","computer vision;convolution;feature extraction;grammars;image classification;image segmentation;learning (artificial intelligence);recurrent neural nets","large-scale 3D point clouds;high-level semantic structures;current deep learning methods;3DCNN-DQN-RNN method;3D convolutional neural network;efficient semantic parsing","","44","","34","IEEE","25 Dec 2017","","","IEEE","IEEE Conferences"
"Event-triggered reinforcement learning approach for unknown nonlinear continuous-time system","X. Zhong; Z. Ni; H. He; X. Xu; D. Zhao","Department of Electrical Computer and Biomedical Engineering, University of Rhode Island, Kingston, USA; Department of Electrical Computer and Biomedical Engineering, University of Rhode Island, Kingston, USA; Department of Electrical Computer and Biomedical Engineering, University of Rhode Island, Kingston, USA; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; State Key Lab of Management and Control for Complex Systems, Chinese Academy of Sciences, Beijing, P. R. China","2014 International Joint Conference on Neural Networks (IJCNN)","4 Sep 2014","2014","","","3677","3684","This paper provides an adaptive event-triggered method using adaptive dynamic programming (ADP) for the nonlinear continuous-time system. Comparing to the traditional method with fixed sampling period, the event-triggered method samples the state only when an event is triggered and therefore the computational cost is reduced. We demonstrate the theoretical analysis on the stability of the event-triggered method, and integrate it with the ADP approach. The system dynamics are assumed unknown. The corresponding ADP algorithm is given and the neural network techniques are applied to implement this method. The simulation results verify the theoretical analysis and justify the efficiency of the proposed event-triggered technique using the ADP approach.","2161-4407","978-1-4799-1484-5","10.1109/IJCNN.2014.6889787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6889787","","Equations;Neural networks;Stability analysis;Performance analysis;Approximation algorithms;Approximation methods;Heuristic algorithms","adaptive systems;continuous time systems;dynamic programming;learning (artificial intelligence);neurocontrollers;nonlinear dynamical systems;stability","event triggered reinforcement learning approach;unknown nonlinear continuous time system;adaptive event triggered method;adaptive dynamic programming;computational cost reduction;stability;ADP approach;system dynamics;neural network technique","","41","","37","IEEE","4 Sep 2014","","","IEEE","IEEE Conferences"
"Feature Aggregation With Reinforcement Learning for Video-Based Person Re-Identification","W. Zhang; X. He; W. Lu; H. Qiao; Y. Li","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China; School of Control Science and Engineering, Shandong University, Jinan, China","IEEE Transactions on Neural Networks and Learning Systems","10 Dec 2019","2019","30","12","3847","3852","Video-based person re-identification (re-id) matches two tracks of persons from different cameras. Features are extracted from the images of a sequence and then aggregated as a track feature. Compared to existing works that aggregate frame features by simply averaging them or using temporal models such as recurrent neural networks, we propose an intelligent feature aggregate method based on reinforcement learning. Specifically, we train an agent to determine which frames in the sequence should be abandoned in the aggregation, which can be treated as a decision making process. By this way, the proposed method avoids introducing noisy information of the sequence and retains these valuable frames when generating a track feature. On benchmark data sets, experimental results show that our method can boost the re-id accuracy obviously based on the state-of-the-art models.","2162-2388","","10.1109/TNNLS.2019.2899588","National Key Research and Development Plan of China(grant numbers:2017YFB1300205); National Natural Science Foundation of China(grant numbers:61573222,61801264); Major Research Program of Shandong Province(grant numbers:2018CXGC1503); Shandong University(grant numbers:2016JC014); Basic Research Program of Shenzhen(grant numbers:JCYJ20170307153635551); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666162","Feature aggregation;reinforcement learning (RL);sequential decision making;video-based person re-identification (re-id)","Feature extraction;Task analysis;Decision making;Noise measurement;Learning systems;Reinforcement learning","feature extraction;image matching;learning (artificial intelligence);object recognition;recurrent neural nets;video signal processing","recurrent neural networks;temporal models;feature extraction;frame features;video-based person re-identification;track feature;reinforcement learning;intelligent feature aggregate method","Humans;Image Processing, Computer-Assisted;Neural Networks, Computer;Pattern Recognition, Automated;Reinforcement, Psychology;Video Recording","35","","33","IEEE","12 Mar 2019","","","IEEE","IEEE Journals"
"Joint Interference Alignment and Power Control for Dense Networks via Deep Reinforcement Learning","C. Wang; D. Deng; L. Xu; W. Wang; F. Gao","School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Research Institute, China United Network Communications Corporation, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Wireless Communications Letters","7 May 2021","2021","10","5","966","970","This letter proposes a joint interference suppression scheme in heterogeneous networks (HetNets) with dense small cells (SCs) and users. Different from the majority of existing studies, we adopt the co-tier intra-cell interference alignment (IA), while the co-tier inter-cell and cross-tier interference is suppressed by centralized power control in the macro base station (MBS). Specifically, the power control problem is modeled as a Markov Decision Process (MDP) with the aim of maximizing the sum spectrum efficiency. Considering the exponential growth of the output layer neurons faced by general deep reinforcement learning (DRL) algorithms, we propose a deep deterministic policy gradient (DDPG)-based algorithm to solve the problem. Simulation results demonstrate that the proposed algorithm is able to achieve better performance and wider application scope comparing with existing algorithms.","2162-2345","","10.1109/LWC.2021.3052079","National Natural Science Foundation of China(grant numbers:61971054); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326357","Interference alignment;power control;Markov decision process;deep deterministic policy gradient","Interference suppression;Simulation;Power control;Neurons;Process control;Reinforcement learning;Quality of service","cellular radio;femtocellular radio;interference suppression;learning (artificial intelligence);Markov processes;picocellular radio;power control;radiofrequency interference","output layer neurons;general deep reinforcement learning algorithms;deep deterministic policy gradient-based algorithm;joint interference alignment;dense networks;joint interference suppression scheme;heterogeneous networks;co-tier intra-cell interference alignment;co-tier inter-cell;cross-tier interference;centralized power control;macro base station;power control problem;Markov Decision Process;sum spectrum efficiency","","34","","17","IEEE","18 Jan 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Approximate Optimal Control for Attitude Reorientation Under State Constraints","H. Dong; X. Zhao; H. Yang","School of Engineering, University of Warwick, Coventry, U.K.; School of Engineering, University of Warwick, Coventry, U.K.; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","IEEE Transactions on Control Systems Technology","10 Jun 2021","2021","29","4","1664","1673","This article addresses the attitude reorientation problems of rigid bodies under multiple state constraints. A novel reinforcement learning (RL)-based approximate optimal control method is proposed to make the tradeoff between control cost and performance. The novelty lies in that it guarantees constraint handling abilities on attitude forbidden zones and angular velocity limits. To achieve this, barrier functions are employed to encode the constraint information into the cost function. Then, an RL-based learning strategy is developed to approximate the optimal cost function and control policy. A simplified critic-only neural network (NN) is employed to replace the conventional actor-critic structure once adequate data are collected online. This design guarantees the uniform boundedness of reorientation errors and NN weight estimation errors subject to the satisfaction of a finite excitation condition, which is a relaxation compared with the persistent excitation condition that is typically required for this class of problems. More importantly, all underlying state constraints are strictly obeyed during the online learning process. The effectiveness and advantages of the proposed controller are verified by both numerical simulations and experimental tests based on a comprehensive hardware-in-loop testbed.","1558-0865","","10.1109/TCST.2020.3007401","U.K. Engineering and Physical Sciences Research Council(grant numbers:EP/S001905/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9141418","Adaptive dynamic programming (ADP);approximate optimal control;attitude control;reinforcement learning (RL);state constraints","Attitude control;Reinforcement learning;Angular velocity;Optimal control;Dynamic programming","adaptive control;aircraft control;angular velocity control;approximation theory;attitude control;constraint handling;control engineering computing;learning (artificial intelligence);learning systems;Lyapunov methods;neurocontrollers;nonlinear control systems;optimal control","conventional actor-critic structure;reorientation errors;NN weight estimation errors;underlying state constraints;online learning process;attitude reorientation problems;rigid bodies;multiple state constraints;reinforcement learning-based approximate optimal control method;control cost;constraint handling abilities;attitude forbidden zones;angular velocity limits;barrier functions;constraint information;RL-based learning strategy;approximate the optimal cost function;control policy","","29","","33","IEEE","15 Jul 2020","","","IEEE","IEEE Journals"
"Systems Control With Generalized Probabilistic Fuzzy-Reinforcement Learning","W. M. Hinojosa; S. Nefti; U. Kaymak","Robotics and Automation Laboratory, University of Sanford, Greater Manchester, UK; School of Computing Science and Engineering, University of Sanford, Greater Manchester, UK; Econometric Institute, Erasmus School of Economics, Erasmus University, Rotterdam, Netherlands","IEEE Transactions on Fuzzy Systems","31 Jan 2011","2011","19","1","51","64","Reinforcement learning (RL) is a valuable learning method when the systems require a selection of control actions whose consequences emerge over long periods for which input-output data are not available. In most combinations of fuzzy systems and RL, the environment is considered to be deterministic. In many problems, however, the consequence of an action may be uncertain or stochastic in nature. In this paper, we propose a novel RL approach to combine the universal-function-approximation capability of fuzzy systems with consideration of probability distributions over possible consequences of an action. The proposed generalized probabilistic fuzzy RL (GPFRL) method is a modified version of the actor-critic (AC) learning architecture. The learning is enhanced by the introduction of a probability measure into the learning structure, where an incremental gradient-descent weight-updating algorithm provides convergence. Our results show that the proposed approach is robust under probabilistic uncertainty while also having an enhanced learning speed and good overall performance.","1941-0034","","10.1109/TFUZZ.2010.2081994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590288","Actor–critic (AC);learning agent;probabilistic fuzzy systems;reinforcement learning (RL);systems control","Probabilistic logic;Learning;Uncertainty;Fuzzy systems;Stochastic processes;Function approximation;Control systems","approximation theory;fuzzy set theory;gradient methods;learning (artificial intelligence);probability","generalized probabilistic fuzzy-reinforcement learning;fuzzy systems;universal-function-approximation capability;actor-critic learning architecture;incremental gradient-descent weight-updating algorithm","","26","","36","IEEE","30 Sep 2010","","","IEEE","IEEE Journals"
"Highway Exiting Planner for Automated Vehicles Using Reinforcement Learning","Z. Cao; D. Yang; S. Xu; H. Peng; B. Li; S. Feng; D. Zhao","Department of Automotive Engineering, Tsinghua University, Beijing, China; Department of Automotive Engineering, Tsinghua University, Beijing, China; Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Automation, Tsinghua University, Beijing, China; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Transactions on Intelligent Transportation Systems","2 Feb 2021","2021","22","2","990","1000","Exiting from highways in crowded dynamic traffic is an important path planning task for autonomous vehicles (AVs). This task can be challenging because of the uncertain motion of surrounding vehicles and limited sensing/observing window. Conventional path planning methods usually compute a mandatory lane change (MLC) command, but the lane change behavior (e.g., vehicle speed and gap acceptance) should also adapt to traffic conditions and the urgency for exiting. In this paper, we propose a reinforcement learning-enhanced highway-exit planner. The learning-based strategy learns from past failures and adjusts the vehicle motion when the AV fails to exit. The reinforcement learning is based on the Monte Carlo tree search (MCTS) approach. The proposed learning-enhanced highway-exit planner is tested 6000 times in stochastic simulations. The results indicate that the proposed planner achieves a higher probability of successful highway exiting than a benchmark MLC planner.","1558-0016","","10.1109/TITS.2019.2961739","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950131","Autonomous vehicle;motion planning;decision making;reinforcement learning","Road transportation;Safety;Reinforcement learning;Vehicle dynamics;Vehicles;Dynamics;Trajectory","control engineering computing;intelligent transportation systems;learning (artificial intelligence);mobile robots;Monte Carlo methods;path planning;probability;road traffic control;road vehicles;stochastic processes;traffic engineering computing;tree searching","highway exiting planner;automated vehicles;crowded dynamic traffic;path planning;autonomous vehicles;uncertain motion;mandatory lane change command;lane change behavior;vehicle speed;traffic conditions;reinforcement learning;vehicle motion;MLC planner;Monte Carlo tree search;gap acceptance;MCTS;stochastic simulations;probability","","26","","33","IEEE","6 Jan 2020","","","IEEE","IEEE Journals"
"Neural Networks Enhanced Optimal Admittance Control of Robot–Environment Interaction Using Reinforcement Learning","G. Peng; C. L. P. Chen; C. Yang","Department of Computer and Information Science, Faculty of Science and Technology, University of Macau, Macau, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Neural Networks and Learning Systems","31 Aug 2022","2022","33","9","4551","4561","In this paper, an adaptive admittance control scheme is developed for robots to interact with time-varying environments. Admittance control is adopted to achieve a compliant physical robot–environment interaction, and the uncertain environment with time-varying dynamics is defined as a linear system. A critic learning method is used to obtain the desired admittance parameters based on the cost function composed of interaction force and trajectory tracking without the knowledge of the environmental dynamics. To deal with dynamic uncertainties in the control system, a neural-network (NN)-based adaptive controller with a dynamic learning framework is developed to guarantee the trajectory tracking performance. Experiments are conducted and the results have verified the effectiveness of the proposed method.","2162-2388","","10.1109/TNNLS.2021.3057958","National Key Research and Development Program of China(grant numbers:2019YFB1703600); National Natural Science Foundation of China(grant numbers:61702195,61751202,U1813203,U1801262,61751205,U20A20200,61861136009); Science and Technology Major Project of Guangzhou(grant numbers:202007030006); Science and Technology Development Fund, Macau(grant numbers:0119/2018/A3); Multiyear Research Grants of University of Macau; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9367005","Adaptive control;admittance control;neural networks (NNs);reinforcement learning (RL);robot–environment interaction","Admittance;Impedance;Artificial neural networks;Uncertainty;Trajectory;Manipulators;Learning systems","adaptive control;control system synthesis;force control;mobile robots;neurocontrollers;reinforcement learning;trajectory control","neural networks;optimal admittance control;reinforcement learning;adaptive admittance control scheme;time-varying environments;compliant physical robot-environment interaction;time-varying dynamics;linear system;critic learning method;admittance parameters;neural-network-based adaptive controller;dynamic learning framework;trajectory tracking performance","Computer Simulation;Gene-Environment Interaction;Neural Networks, Computer;Nonlinear Dynamics;Robotics","25","","57","IEEE","2 Mar 2021","","","IEEE","IEEE Journals"
"Residential Energy Management with Deep Reinforcement Learning","Z. Wan; H. Li; H. He","Department of Electrical, Computer and Biomedical Engineering, University of Rhode Island, RI, USA; Lab. of Networked Control Systems, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Department of Electrical, Computer and Biomedical Engineering, University of Rhode Island, RI, USA","2018 International Joint Conference on Neural Networks (IJCNN)","14 Oct 2018","2018","","","1","7","A smart home with battery energy storage can take part in the demand response program. With proper energy management, consumers can purchase more energy at off-peak hours than at on-peak hours, which can reduce the electricity costs and help to balance the electricity demand and supply. However, it is hard to determine an optimal energy management strategy because of the uncertainty of the electricity consumption and the real-time electricity price. In this paper, a deep reinforcement learning based approach has been proposed to solve this residential energy management problem. The proposed approach does not require any knowledge about the uncertainty and can directly learn the optimal energy management strategy based on reinforcement learning. Simulation results demonstrate the effectiveness of the proposed approach.","2161-4407","978-1-5090-6014-6","10.1109/IJCNN.2018.8489210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489210","","Batteries;Computer architecture;Energy management;Load modeling;Microprocessors;Feature extraction","demand side management;energy management systems;energy storage;learning (artificial intelligence);power consumption;pricing","smart home;battery energy storage;demand response program;off-peak hours;on-peak hours;electricity costs;optimal energy management strategy;electricity consumption;real-time electricity price;deep reinforcement learning based approach;residential energy management problem;electricity demand and supply","","24","","33","IEEE","14 Oct 2018","","","IEEE","IEEE Conferences"
"Multiagent Federated Reinforcement Learning for Secure Incentive Mechanism in Intelligent Cyber–Physical Systems","M. Xu; J. Peng; B. B. Gupta; J. Kang; Z. Xiong; Z. Li; A. A. A. El-Latif","School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Data Science and Technology, Heilongjiang University, Harbin, China; Department of Computer Engineering, National Institute of Technology Kurukshetra, Kurukshetra, India; Joint NTU-WeBank Research Centre on Fintech, Nanyang Technological University, Jurong West, Singapore; Pillar of Information Systems Technology and Design, Singapore University of Technology and Design, Tampines, Singapore; School of Automation and the Guangdong Key Laboratory of IoT Information Technology, Guangdong University of Technology, Guangzhou, China; Department of Mathematics and Computer Science, Faculty of Science, Menoufia University, Shebin El-Koom, Egypt","IEEE Internet of Things Journal","4 Nov 2022","2022","9","22","22095","22108","Federated learning (FL) is an emerging technology for empowering various applications that generate large amounts of data in intelligent cyber–physical systems (ICPS). Though FL can address users’ concerns about data privacy, its maintenance still depends on efficient incentive mechanisms. For long-term incentivization to participants in data federation under dynamic environments, deep reinforcement learning as a promising technology has been extensively studied. However, the nonstationary problem caused by the heterogeneity of ICPS devices results in a serious effect on the convergence rate of existing single-agent reinforcement learning. In this article, we propose a multiagent learning-based incentive mechanism to capture the stationarity approximation in FL with heterogeneous ICPS. First, we formulate the secure communication and data resource allocation problem as a Stackelberg game in FL with multiple participants. Then, to tackle the heterogeneous problem, we model this multiagent game as a partially observable Markov decision process. In particular, a multiagent federated reinforcement learning algorithm is proposed to learn the allocation policies efficiently by dwindling variances in policy evaluation caused by interaction among multiple devices without the requirement of sharing privacy information. Moreover, the proposed algorithm is proved to attain convergence at an expected rate. Finally, extensive experimental results demonstrate that our proposed algorithm significantly outperforms baseline approaches.","2327-4662","","10.1109/JIOT.2021.3081626","National Key Research and Development Plan(grant numbers:2018YFB1003803); National Natural Science Foundation of China(grant numbers:61803096,62073086,61802450); Natural Science Foundation of Guangdong(grant numbers:2018A030313005); Guangdong Provincial Pearl River Talents Program(grant numbers:2019QN01X130); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X355); Guangzhou Science and Technology Program Project(grant numbers:202002030289); Fundamental Research Funds for Heilongjiang Universities, China(grant numbers:2020-KYYWF-1014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9434397","Deep reinforcement learning (DRL);federated learning (FL);intelligent cyber–physical systems (ICPS)","Privacy;Collaborative work;Adaptation models;Reinforcement learning;Resource management;Convergence;Cyber-physical systems","data privacy;decision theory;game theory;Markov processes;multi-agent systems;reinforcement learning;resource allocation;telecommunication security","data federation;data privacy;data resource allocation problem;deep reinforcement learning;federated learning;heterogeneous ICPS;ICPS devices;intelligent cyber-physical systems;long-term incentivization;multiagent federated reinforcement learning;multiagent game;multiagent learning-based incentive mechanism;nonstationary problem;partially observable Markov decision process;secure communication;secure incentive mechanism;single-agent reinforcement learning","","22","","55","IEEE","18 May 2021","","","IEEE","IEEE Journals"
"Distributed Reinforcement Learning for Cyber-Physical System With Multiple Remote State Estimation Under DoS Attacker","P. Dai; W. Yu; H. Wang; G. Wen; Y. Lv","School of Mathematics, Southeast University, Nanjing, China; School of Mathematics, Southeast University, Nanjing, China; College of Engineering and Computer Science, Australian National University, Australia; School of Mathematics, Southeast University, Nanjing, China; School of Mathematics, Southeast University, Nanjing, China","IEEE Transactions on Network Science and Engineering","31 Dec 2020","2020","7","4","3212","3222","In this paper, we consider cyber-physical system (CPS) with multiple remote state estimation under denial-of-service (DoS) attack in infinite time-horizon. The sensors monitor the system and send their local state estimate to remote estimators by choosing the local channels in “State 0” or “State 1”. The aim of sensors is to find policies for choosing local channel in a specific state to transmit message to minimize the total estimation error covariance on account of energy-saving in an infinite time-horizon. The DoS attacker aims to achieve the opposite goal by choosing channels to attack or not. The games between sensors and DoS attacker under two different structures of public information are investigated, that is the open-loop case (where sensors and attacker cannot observe others' behaviors) and the closed-loop case (where sensors and attacker can observe the others' behaviors causally). For the open-loop case with assumption that the DoS attacker can get the information from the remote estimators to the sensors, the distributed reinforcement learning algorithms for sensors and attacker based on local information are proposed to find their Nash equilibrium policies, respectively. Further, we consider in closed loop case that the DoS attacker cannot get the information from the remote estimators to the sensors which leads to asymmetric information between the sensors and attacker. To derive Nash equilibrium policies for sensors and attacker, we convert the original game into a belief-based continuous-state stochastic game. The convergence of distributed reinforcement learning method is proved. Some simulations are presented to demonstrate its effectiveness.","2327-4697","","10.1109/TNSE.2020.3018871","National Natural Science Foundation of China(grant numbers:61673107); Jiangsu Provincial Key Laboratory of Networked Collective Intelligence(grant numbers:BM2017002); National Natural Science Foundation of China(grant numbers:61722303); Six Talent Peaks of Jiangsu Province(grant numbers:2019-DZXX-006); National Natural Science Foundation of China(grant numbers:61903083); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20190333); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9174773","Cyber-physical system;DoS attack;infinite time-horizon;distributed reinforcement learning","Learning (artificial intelligence);Games;State estimation;Sensor systems;Nash equilibrium;Channel estimation","computer network security;covariance analysis;cyber-physical systems;learning (artificial intelligence);state estimation;stochastic games","cyber-physical system;multiple remote state estimation;DoS attacker;denial-of-service attack;infinite time-horizon;total estimation error covariance;open-loop case;closed-loop case;belief-based continuous-state stochastic game;distributed reinforcement learning;Nash equilibrium policies","","22","","37","IEEE","24 Aug 2020","","","IEEE","IEEE Journals"
"Interpretable Decision-Making for Autonomous Vehicles at Highway On-Ramps With Latent Space Reinforcement Learning","H. Wang; H. Gao; S. Yuan; H. Zhao; K. Wang; X. Wang; K. Li; D. Li","School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; Department of Automation, University of Science and Technology of China, Hefei, China; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; Postdoctoral Workstation of Jinling Hospital, Nanjing University, Nanjing, China; GAIC GROUP, Beijing, China; School of Management Science and Engineering, Nanjing University of Information Science and Technology, Nanjing, China; State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Vehicular Technology","20 Sep 2021","2021","70","9","8707","8719","This paper presents a latent space reinforcement learning method for interpretable decision-making of autonomous vehicles at highway on-ramps. This method is based on the latent model and the combination model of the hidden Markov model and Gaussian mixture regression (HMM-GMR). It is difficult for the traditional decision-making method to understand the environment because its input is high-dimensional and lacks an understanding of the task. By utilizing the HMM-GMR model, we can obtain the interpretable state providing semantic information and environment understanding. A framework is proposed to unify representation learning with the deep reinforcement learning (DRL) approach, in which the latent model is used to reduce the dimension of interpretable state by extracting underlying task-relevant information. Experimental results are presented and the results show the right balance between driving safety and efficiency in the challenging scenarios of highway on-ramps merging.","1939-9359","","10.1109/TVT.2021.3098321","National Natural Science Foundation of China(grant numbers:U20A20225,U2013601); Natural Science Foundation of Hefei, China(grant numbers:2021032); Key Research and Development Plan of Anhui Province(grant numbers:202004a05020058); Fundamental Research Funds for the Central Universities; Science and Technology Innovation Planning Project of Ministry of Education of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9492817","Autonomous driving;interpretability;highway on-ramps;reinforcement learning;latent states","Hidden Markov models;Task analysis;Road transportation;Autonomous vehicles;Merging;Entropy;Decision making","control engineering computing;decision making;deep learning (artificial intelligence);Gaussian processes;hidden Markov models;mixture models;mobile robots;regression analysis;road safety;road traffic control;road vehicles;traffic engineering computing","interpretable decision-making;autonomous vehicles;latent model;combination model;interpretable state;task-relevant information;highway on-ramps;deep reinforcement learning;HMM-GMR;latent space reinforcement learning;hidden Markov model;Gaussian mixture regression;representation learning;DRL;dimension reduction;driving safety","","21","","55","IEEE","21 Jul 2021","","","IEEE","IEEE Journals"
"Application of Reinforcement Learning to Deep Brain Stimulation in a Computational Model of Parkinson’s Disease","M. Lu; X. Wei; Y. Che; J. Wang; K. A. Loparo","School of Information Technology Engineering, Tianjin University of Technology and Education, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, USA","IEEE Transactions on Neural Systems and Rehabilitation Engineering","8 Jan 2020","2020","28","1","339","349","Deep brain stimulation (DBS) has been proven to be an effective treatment to deal with the symptoms of Parkinson's disease (PD). Currently, the DBS is in an open-loop pattern with which the stimulation parameters remain constant regardless of fluctuations in the disease state, and adjustments of parameters rely mostly on trial and error of experienced clinicians. This could bring adverse effects to patients due to possible overstimulation. Thus closed-loop DBS of which stimulation parameters are automatically adjusted based on variations in the ongoing neurophysiological signals is desired. In this paper, we present a closed-loop DBS method based on reinforcement learning (RL) to regulate stimulation parameters based on a computational model. The network model consists of interconnected biophysically-based spiking neurons, and the PD state is described as distorted relay reliability of thalamus (TH). Results show that the RL-based closed-loop control strategy can effectively restore the distorted relay reliability of the TH but with less DBS energy expenditure.","1558-0210","","10.1109/TNSRE.2019.2952637","National Natural Science Foundation of China(grant numbers:61501330,61372010); Tianjin Municipal Special Program of Talents Development for Excellent Youth Scholars(grant numbers:TJTZJH-QNBJRC-2-2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8895773","Closed-loop DBS;reinforcement learning;relay reliability;basal ganglia network;Parkinson’s disease","Neurons;Satellite broadcasting;Computational modeling;Mathematical model;Biological system modeling;Relays;Membrane potentials","bioelectric phenomena;brain;closed loop systems;diseases;learning (artificial intelligence);medical computing;medical control systems;neurophysiology;patient treatment","thalamus;distorted relay reliability;DBS energy expenditure;RL-based closed-loop control strategy;PD state;interconnected biophysically-based spiking neurons;network model;computational model;reinforcement learning;closed-loop DBS method;disease state;stimulation parameters;open-loop pattern;Parkinson's disease;deep brain stimulation","Algorithms;Basal Ganglia;Computer Simulation;Deep Brain Stimulation;Humans;Learning;Neurons;Parkinson Disease;Reinforcement, Psychology;Reproducibility of Results;Thalamus","20","","36","IEEE","11 Nov 2019","","","IEEE","IEEE Journals"
"Reinforcement learning control based on multi-goal representation using hierarchical heuristic dynamic programming","Z. Ni; H. He; Dongbin Zhao; D. V. Prokhorov","Department of Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, RI, USA; Department of Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, RI, USA; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; TTC, Toyota Research Institute NA, Ann Arbor, MI, USA","The 2012 International Joint Conference on Neural Networks (IJCNN)","30 Jul 2012","2012","","","1","8","We are interested in developing a multi-goal generator to provide detailed goal representations that help to improve the performance of the adaptive critic design (ACD). In this paper we propose a hierarchical structure of goal generator networks to cascade external reinforcement into more informative internal goal representations in the ACD. This is in contrast with previous designs in which the external reward signal is assigned to the critic network directly. The ACD control system performance is evaluated on the ball-and-beam balancing benchmark under noise-free and various noisy conditions. Simulation results in the form of a comparative study demonstrate effectiveness of our approach.","2161-4407","978-1-4673-1490-9","10.1109/IJCNN.2012.6252524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6252524","","Generators;Vectors;Tuning;Neural networks;Trajectory;Dynamic programming","adaptive control;dynamic programming;learning (artificial intelligence);nonlinear control systems","reinforcement learning control;multigoal representation;hierarchical heuristic dynamic programming;multigoal generator;adaptive critic design;goal generator network hierarchical structure;informative internal goal representations;external reward signal;ACD control system performance;ball-and-beam balancing benchmark","","20","","25","IEEE","30 Jul 2012","","","IEEE","IEEE Conferences"
"A reinforcement learning approach for sequential decision-making process of attacks in smart grid","Z. Ni; S. Paul; X. Zhong; Q. Wei","Department of Electrical Engineering and Computer Science, South Dakota State University, Brookings, SD, United States; Department of Electrical Engineering and Computer Science, South Dakota State University, Brookings, SD, United States; Department of Electrical Engineering University of North Texas, Denton, TX, United States; Institute of Automation, Chinese Academy of Science, Beijing, China","2017 IEEE Symposium Series on Computational Intelligence (SSCI)","8 Feb 2018","2017","","","1","8","An attacker can very possibly make significant damage for the power grid with a proper sequence of timing and attacks. Existing approaches neglect the power system generation loss and also identification of critical attack sequences. In this paper, we investigate a reinforcement learning approach to identify the minimum number of attacks/actions to reach blackout threshold. The attacker will only have limited topological information of the power systems. Proper state vectors, action vectors and also reward are designed in this smart grid security environment. The proposed method is evaluated on a W & W 6 bus system and an IEEE 30 bus system. The attack performance is tested for different percentages of line outage. The amount of load shedding is also considered as an attack objective and demonstrated on W & W 6 bus system. The optimal attack sequence is identified through a trial-and-error learning process and is then validated on a power system simulator.","","978-1-5386-2726-6","10.1109/SSCI.2017.8285291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8285291","Reinforcement learning;Markov decision process;smart grid security;multi-bus power system;line outage;cascaded failures","Generators;Learning (artificial intelligence);Smart grids;Security;Power system faults","decision making;learning (artificial intelligence);load shedding;power system reliability;power system security;smart power grids","reinforcement learning approach;power grid;power system generation loss;critical attack sequences;blackout threshold;topological information;action vectors;smart grid security environment;IEEE 30 bus system;attack performance;optimal attack sequence;power system simulator;state vectors;sequential decision-making process;trial-and-error learning process;load shedding;W & W 6 bus system","","19","","23","IEEE","8 Feb 2018","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Combinatorial Optimization: Covering Salesman Problems","K. Li; T. Zhang; R. Wang; Y. Wang; Y. Han; L. Wang","College of System Engineering, National University of Defense Technology, Changsha, China; College of System Engineering, National University of Defense Technology, Changsha, China; College of System Engineering, National University of Defense Technology, Changsha, China; Graduate College, National University of Defense Technology, Changsha, China; Science and Technology on Parallel and Distributed Processing Laboratory, College of Computer, National University of Defense Technology, Changsha, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Cybernetics","18 Nov 2022","2022","52","12","13142","13155","This article introduces a new deep learning approach to approximately solve the covering salesman problem (CSP). In this approach, given the city locations of a CSP as input, a deep neural network model is designed to directly output the solution. It is trained using the deep reinforcement learning without supervision. Specifically, in the model, we apply the multihead attention (MHA) to capture the structural patterns, and design a dynamic embedding to handle the dynamic patterns of the problem. Once the model is trained, it can generalize to various types of CSP tasks (different sizes and topologies) without the need of retraining. Through controlled experiments, the proposed approach shows desirable time complexity: it runs more than 20 times faster than the traditional heuristic solvers with a tiny gap of optimality. Moreover, it significantly outperforms the current state-of-the-art deep learning approaches for combinatorial optimization in the aspect of both training and inference. In comparison with traditional solvers, this approach is highly desirable for most of the challenging tasks in practice that are usually large scale and require quick decisions.","2168-2275","","10.1109/TCYB.2021.3103811","National Natural Science Foundation of China(grant numbers:72071205,61873328,61773390); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9523517","Attention;covering salesman problem (CSP);deep learning;deep reinforcement learning (DRL)","Deep learning;Optimization;NP-hard problem;Reinforcement learning","computational complexity;deep learning (artificial intelligence);neural nets;reinforcement learning;travelling salesman problems","city locations;combinatorial optimization;covering salesman problem;CSP tasks;deep neural network model;deep reinforcement learning;dynamic embedding;dynamic patterns;MHA;multihead attention;structural patterns","Neural Networks, Computer;Reinforcement, Psychology","18","","51","IEEE","26 Aug 2021","","","IEEE","IEEE Journals"
"Structural Scheduling of Transient Control Under Energy Storage Systems by Sparse-Promoting Reinforcement Learning","J. Sun; G. Qi; N. Mazur; Z. Zhu","School of Electronic, and Information Engineering, Southwest University, Chongqing, China; Department of Computer Information Systems, State University of New York at Buffalo, Buffalo, NY, USA; Department of Computer Information Systems, State University of New York at Buffalo, Buffalo, NY, USA; College of Automation, Chongqing University of Post, and Telecommunications, Chongqing, China","IEEE Transactions on Industrial Informatics","29 Oct 2021","2022","18","2","744","756","Machine learning related research in transient control has drawn considerable attention with the rapid increase in data measurement from power grids. Two key components, the control algorithm and system structure, work together to determine the control performance. The design of control laws, the selection of phase measurement units, the allocation of power resources, and the scheduling of communication topology in limited cyber-physical resources need to be considered. Many existing scheduling or planning schemes specialized for control structure are designed based on various linearized analytical models or the optimization of steady states. However, the transient dynamics of power grids are nonlinear and parts of these dynamics are usually unknown. Linearized analytical models cannot represent the transient dynamics of power grids with large disturbances. This article proposes a sparse neural network based reinforcement learning scheme to optimize the control system structure for the transient stability enhancement of power grids with energy storage systems. One adjustable group sparse weight matrix is introduced to formulate both control structure and actor–critic networks. This strategy enables the proposed scheme to simultaneously schedule the control system structure and design the control laws by online learning without solving any combinational optimization problems or requiring any linearized analytical models. The sufficient conditions of learning stability, control stability, and group sparsity are thoroughly studied by mathematical analysis. The proposed scheme is simulated on an IEEE 118-bus test system for verification. The simulation results confirm the feasibility, advantages, and adaptability of the proposed method.","1941-0050","","10.1109/TII.2021.3084139","Fundamental Research Funds for the Central Universities(grant numbers:XDJK2020B010); National Natural Science Foundation of China(grant numbers:61703347,61803061,61906026); Innovation research group of universities in Chongqing; Natural Science Foundation of Chongqing(grant numbers:cstc2020jcyj-msxmX0577); Chengdu-Chongqing Economic Circle; Chongqing Municipal Education Commission(grant numbers:KJCXZD2020028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442353","Control structure;neural network;reinforcement learning (RL);transient control","Transient analysis;Power system dynamics;Power grids;Power system stability;Job shop scheduling;Analytical models;Reinforcement learning","IEEE standards;mathematical analysis;optimisation;phase measurement;power engineering computing;power generation control;power generation planning;power generation scheduling;power grids;power system measurement;power system transient stability;reinforcement learning","structural scheduling;transient control;energy storage system;sparse-promoting reinforcement learning;machine learning;power grid;phase measurement unit;linearized analytical model;adjustable group sparse weight matrix;IEEE 118-bus test system;communication topology;cyber-physical resource;planning scheme;optimization problem;actor-critic network","","18","","35","IEEE","26 May 2021","","","IEEE","IEEE Journals"
"Asynchronous Multithreading Reinforcement-Learning-Based Path Planning and Tracking for Unmanned Underwater Vehicle","Z. He; L. Dong; C. Sun; J. Wang","College of Electronics and Information Engineering, Tongji University, Shanghai, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","14 Apr 2022","2022","52","5","2757","2769","The underwater unmanned vehicle (UUV) is widely used in various marine operations, in which path planning and trajectory tracking are the critical technologies to achieve autonomous motion planning. Unlike previous research methods, this article proposes the asynchronous multithreading proximal policy optimization-based path planning (AMPPO-PP) and trajectory tracking (AMPPO-TT) algorithms and applies these two methods to different task scenarios of UUVs. Taking advantage of the AMPPO, the expensive online computational procedure is converted to an offline training process. The proposed algorithms enable the UUV to learn autonomous planning, tracking, and emergency obstacle avoiding. Besides, the algorithm architecture of the AMPPO-PP and the AMPPO-TT is described in detail. By refining the reward in each timestep and utilizing the reward-shaping trick, the reward sparsity is avoided. The goal-distance heuristic reward function is used to make the UUV explore more directionally. Various simulation environments are developed from simple to complex, along with multiple comparative experiments to verify the effectiveness of the proposed algorithms.","2168-2232","","10.1109/TSMC.2021.3050960","National Key Research and Development Program of China(grant numbers:2018AAA0101400); National Natural Science Foundation of China(grant numbers:61803085,61921004,U1713209); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20202006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354210","Path planning;proximal policy optimization (PPO);trajectory tracking;underwater unmanned vehicle (UUV)","Planning;Trajectory tracking;Robots;Trajectory;Multithreading;Heuristic algorithms;Task analysis","autonomous underwater vehicles;collision avoidance;learning (artificial intelligence);mobile robots;motion control;optimisation","UUV;autonomous planning;algorithm architecture;AMPPO-PP;AMPPO-TT;reward-shaping trick;goal-distance heuristic reward function;asynchronous multithreading reinforcement-learning-based path planning;unmanned underwater vehicle;underwater unmanned vehicle;marine operations;autonomous motion planning;trajectory tracking algorithms;offline training process;asynchronous multithreading proximal policy optimization-based path planning","","17","","55","IEEE","12 Feb 2021","","","IEEE","IEEE Journals"
"A combined hierarchical reinforcement learning based approach for multi-robot cooperative target searching in complex unknown environments","Y. Cai; S. X. Yang; X. Xu","The School of Engineering, University of Guelph, Guelph, Ontario, Canada; The School of Engineering, University of Guelph, Guelph, Ontario, Canada; The College of Mechatronics and Automation, National University of Defense Technology, Changsha, Hunan Province, China","2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)","30 Sep 2013","2013","","","52","59","Effective cooperation of multi-robots in unknown environments is essential in many robotic applications, such as environment exploration and target searching. In this paper, a combined hierarchical reinforcement learning approach, together with a designed cooperation strategy, is proposed for the real-time cooperation of multi-robots in completely unknown environments. Unlike other algorithms that need an explicit environment model or select parameters by trial and error, the proposed cooperation method obtains all the required parameters automatically through learning. By integrating segmental options with the traditional MAXQ algorithm, the cooperation hierarchy is built. In new tasks, the designed cooperation method can control the multi-robot system to complete the task effectively. The simulation results demonstrate that the proposed scheme is able to effectively and efficiently lead a team of robots to cooperatively accomplish target searching tasks in completely unknown environments.","2325-1867","978-1-4673-5925-2","10.1109/ADPRL.2013.6614989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614989","Hierarchical reinforcement learning;multi-robot cooperation;complex unknown environment;target searching","Robot kinematics;Learning (artificial intelligence);Real-time systems;Algorithm design and analysis;Dynamic programming;Robot sensing systems","cooperative systems;learning (artificial intelligence);multi-robot systems;target tracking","combined hierarchical reinforcement learning based approach;multi-robot cooperative target searching;complex unknown environments;cooperation method;segmental options;MAXQ algorithm;cooperation hierarchy;multi-robot system;target searching tasks","","16","","15","IEEE","30 Sep 2013","","","IEEE","IEEE Conferences"
"Design and Experimental Validation of Deep Reinforcement Learning-Based Fast Trajectory Planning and Control for Mobile Robot in Unknown Environment","R. Chai; H. Niu; J. Carrasco; F. Arvin; H. Yin; B. Lennox","Department of Electrical and Electronic Engineering, The University of Manchester, Manchester, U.K.; Department of Electrical and Electronic Engineering, The University of Manchester, Manchester, U.K.; Department of Electrical and Electronic Engineering, The University of Manchester, Manchester, U.K.; Department of Computer Science, Durham University, Durham, U.K.; Department of Electrical and Electronic Engineering, The University of Manchester, Manchester, U.K.; Department of Electrical and Electronic Engineering, The University of Manchester, Manchester, U.K.","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","15","This article is concerned with the problem of planning optimal maneuver trajectories and guiding the mobile robot toward target positions in uncertain environments for exploration purposes. A hierarchical deep learning-based control framework is proposed which consists of an upper level motion planning layer and a lower level waypoint tracking layer. In the motion planning phase, a recurrent deep neural network (RDNN)-based algorithm is adopted to predict the optimal maneuver profiles for the mobile robot. This approach is built upon a recently proposed idea of using deep neural networks (DNNs) to approximate the optimal motion trajectories, which has been validated that a fast approximation performance can be achieved. To further enhance the network prediction performance, a recurrent network model capable of fully exploiting the inherent relationship between preoptimized system state and control pairs is advocated. In the lower level, a deep reinforcement learning (DRL)-based collision-free control algorithm is established to achieve the waypoint tracking task in an uncertain environment (e.g., the existence of unexpected obstacles). Since this approach allows the control policy to directly learn from human demonstration data, the time required by the training process can be significantly reduced. Moreover, a noisy prioritized experience replay (PER) algorithm is proposed to improve the exploring rate of control policy. The effectiveness of applying the proposed deep learning-based control is validated by executing a number of simulation and experimental case studies. The simulation result shows that the proposed DRL method outperforms the vanilla PER algorithm in terms of training speed. Experimental videos are also uploaded, and the corresponding results confirm that the proposed strategy is able to fulfill the autonomous exploration mission with improved motion planning performance, enhanced collision avoidance ability, and less training time.","2162-2388","","10.1109/TNNLS.2022.3209154","Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/S03286X/1); EPSRC Robotics and Artificial Intelligence for Nuclear (RAIN)(grant numbers:EP/R026084/1); EPSRC Robotics for Nuclear Environments (RNE)(grant numbers:EP/P01366X/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9913936","Deep reinforcement learning (DRL);mobile robot;motion control;noisy prioritized experience replay (PER);optimal motion planning;recurrent neural network;unexpected obstacles","Mobile robots;Trajectory;Planning;Collision avoidance;Training;Robot sensing systems;Noise measurement","","","","16","","","IEEE","10 Oct 2022","","","IEEE","IEEE Early Access Articles"
"Reinforcement Learning for Building Energy Optimization Through Controlling of Central HVAC System","J. Hao; D. W. Gao; J. J. Zhang","Department of Electrical and Computer Engineering, University of Denver, Denver, CO, USA; Department of Electrical and Computer Engineering, University of Denver, Denver, CO, USA; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China","IEEE Open Access Journal of Power and Energy","8 Oct 2020","2020","7","","320","328","This paper presents a novel methodology to control HVAC system and minimize energy cost on the premise of satisfying power system constraints. A multi-agent architecture based on game theory and reinforcement learning is developed so as to reduce the cost and computational complexity of the microgrid. The multi-agent architecture comprising agents, state variables, action variables, reward function and cost game is formulated. The paper fills the gap between multi-agent HVAC systems control and power system optimization and planning. The results and analysis indicate that the proposed algorithm is beneficial to deal with the problem of “curse of dimensionality” for multi-agent microgrid HVAC system control and speed up learning of unknown power system conditions.","2687-7910","","10.1109/OAJPE.2020.3023916","U.S. National Science Foundation(grant numbers:1711951); Key Technologies Research and Development Program of Tianjin City(grant numbers:17ZXRGGX00170); Corporate Research and Development Program of State Grid of China(grant numbers:SGZJDK00DYJS1900232); Science and Technology Program of Central China Branch of State Grid Corporation of China(grant numbers:521400180005); National Key Research and Development Program of China(grant numbers:SQ2018AAA010127); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9195858","Game theory;reinforcement learning;multi-agent system;HVAC control;cost minimization","Control systems;Buildings;Open Access;Tin;Microgrids","building management systems;game theory;HVAC;learning (artificial intelligence);multi-agent systems;power engineering computing","unknown power system conditions;multiagent microgrid HVAC system control;planning;power system optimization;reward function;action variables;state variables;multiagent architecture comprising agents;computational complexity;game theory;power system constraints;energy cost;central HVAC system;building energy optimization;reinforcement learning","","15","","30","CCBY","14 Sep 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning Based Power Control for In-Body Sensors in WBANs Against Jamming","G. Chen; Y. Zhan; Y. Chen; L. Xiao; Y. Wang; N. An","School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; School of Engineering, Sun Yat-sen University, Guangzhou, China; Department of Communication Engineering, Xiamen University, Xiamen, China; Department of Communication Engineering, Xiamen University, Xiamen, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Computer and Information, Hefei University of Technology, Hefei, China","IEEE Access","23 Jul 2018","2018","6","","37403","37412","Wireless body area networks (WBANs) have to address jamming attacks to support healthcare applications. In this paper, we present a reinforcement learning-based power control scheme for the communication between the in-body sensors and the WBAN coordinator to resist jamming attacks. This scheme applies Q-learning to guide the coordinator to achieve an optimal power control strategy without being aware of the in-body sensor's transmission parameters and the WBAN model of the other sensors in the dynamic anti-jamming transmission. In addition, a transfer learning method is adopted to accelerate the learning speed. Stackelberg equilibria and their existence conditions are deduced in a single time slot to upper bound the performance of the learning-based sensor power control scheme. Simulation results show that the proposed scheme can efficiently increase the utilities and decrease the transmission energy consumptions for the in-body sensors and the WBAN coordinator, and simultaneously reduce the attack possibility of the jammer compared with a standard Q-learning-based sensor power control scheme.","2169-3536","","10.1109/ACCESS.2018.2850659","National Natural Science Foundation of China(grant numbers:61671396); Guangdong Public Creation and Environment Programming(grant numbers:508300984106); Science and Technology Innovation Project of Foshan(grant numbers:2016AG100382); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8395379","Wireless body area networks;power control;in-body sensors;jamming attacks;game theory","Sensors;Body area networks;Wireless communication;Jamming;Power control;Games;Energy consumption","body area networks;body sensor networks;health care;jamming;learning (artificial intelligence);power control;telecommunication control","wireless body area networks;jamming attacks;in-body sensors;WBAN coordinator;optimal power control strategy;WBAN model;dynamic anti-jamming transmission;transfer learning method;learning speed;standard Q-learning-based sensor power control scheme;reinforcement learning;transmission energy consumptions","","14","","25","OAPA","25 Jun 2018","","","IEEE","IEEE Journals"
"IRS-Aided Energy-Efficient Secure WBAN Transmission Based on Deep Reinforcement Learning","L. Xiao; S. Hong; S. Xu; H. Yang; X. Ji","Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Communications","15 Jun 2022","2022","70","6","4162","4174","Wireless body area networks (WBANs) are vulnerable to active eavesdropping that simultaneously perform sniffing and jamming to raise the sensor transmit power, and thus steal more healthcare data. In this paper, we propose an intelligent reflecting surface (IRS)-aided reinforcement learning (RL) based secure WBAN transmission scheme that enables the coordinator to jointly optimize the sensor encryption key and transmit power, as well as the IRS phase shifts against active eavesdropping. A Dyna architecture is designed to improve the learning efficiency with the simulated transmission experiences and safe exploration is applied to avoid the risky policies that result in severe data leakage. A deep RL based WBAN transmission scheme is proposed to further improve the secure transmission with lower eavesdropping rate, intercept probability, sensor energy consumption and transmission latency for the coordinators that support deep learning. We analyze the computational complexity and investigate the equilibrium of the secure transmission game between the coordinator and the eavesdropper to provide the performance bounds, which is verified via the simulation results, showing the efficacy of our proposed schemes.","1558-0857","","10.1109/TCOMM.2022.3169813","National Natural Science Foundation of China(grant numbers:U21A20444,61971366); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762319","Wireless body area networks;active eavesdropping;intelligent reflecting surface;reinforcement learning;game theory","Encryption;Wireless communication;Eavesdropping;Body area networks;Jamming;Medical services;Resource management","body area networks;computational complexity;deep learning (artificial intelligence);energy conservation;game theory;probability;reinforcement learning;telecommunication power management;telecommunication security;wireless sensor networks","secure transmission game;IRS-aided energy-efficient secure WBAN transmission;deep reinforcement learning;wireless body area networks;WBANs;active eavesdropping;sensor transmit power;healthcare data;intelligent reflecting surface-aided reinforcement learning;secure WBAN transmission scheme;sensor encryption key;IRS phase shifts;learning efficiency;severe data leakage;deep RL;lower eavesdropping rate;sensor energy consumption;deep learning","","14","","50","IEEE","22 Apr 2022","","","IEEE","IEEE Journals"
"Distributed Deep Reinforcement Learning based Indoor Visual Navigation","S. Hsu; S. Chan; P. Wu; K. Xiao; L. Fu","Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Director of NTU Center for Artificial Intelligence & Advanced Robotics, Taipei, Taiwan","2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","6 Jan 2019","2018","","","2532","2537","Recently, as the rise of deep reinforcement learning, it not only can help the robot to convert the complicated environment scene to motor control command directly but also can accomplish the navigation task properly. In this paper, we propose a novel structure, where the objective is to achieve navigation in large-scale indoor complex environment without pre-constructed map. Generally, it requires good understanding of such indoor environment to make complex spatial perception possible, especially when the indoor space consists of many walls and doors which might block the view of robot leading to complex navigation path. By the proposed distributed deep reinforcement learning in different local regions, our method can achieve indoor visual navigation in the aforementioned large-scale environment without extra map information and human instruction. In the experiments, we validate our proposed method by conducting highly promising navigation tasks both in simulation and real environments.","2153-0866","978-1-5386-8094-0","10.1109/IROS.2018.8594352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8594352","deep reinforcement learning;visual navigation","Navigation;Visualization;Task analysis;Training;Reinforcement learning;Robots;Indoor environments","indoor environment;indoor navigation;learning (artificial intelligence);mobile robots;object detection;path planning;robot vision","complicated environment scene;motor control command;navigation task;large-scale indoor complex environment;pre-constructed map;indoor environment;complex spatial perception possible;indoor space;complex navigation path;aforementioned large-scale environment;real environments;distributed deep reinforcement learning based indoor visual navigation","","14","","18","IEEE","6 Jan 2019","","","IEEE","IEEE Conferences"
"Training Drift Counteraction Optimal Control Policies Using Reinforcement Learning: An Adaptive Cruise Control Example","Z. Li; T. Chu; I. V. Kolmanovsky; X. Yin","Department of Mechanical Engineering, Michigan State University, East Lansing, MI, USA; Department of Civil and Environmental Engineering, Stanford University, CA, USA; Department of Aerospace Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Intelligent Transportation Systems","11 Sep 2018","2018","19","9","2903","2912","The objective of drift counteraction optimal control (DCOC) problem is to compute an optimal control law that maximizes the expected time of violating specified system constraints. In this paper, we reformulate the DCOC problem as a reinforcement learning (RL) one, removing the requirements of disturbance measurements and prior knowledge of the disturbance evolution. The optimal control policy for the DCOC is then trained with RL algorithms. As an example, we treat the problem of adaptive cruise control, where the objective is to maintain desired distance headway and time headway from the lead vehicle, while the acceleration and speed of the host vehicle are constrained based on safety, comfort, and fuel economy considerations. An informed approximate Q-learning algorithm is developed with efficient training, fast convergence, and good performance. The control performance is compared with a heuristic driver model in simulation and superior performance is demonstrated.","1558-0016","","10.1109/TITS.2017.2767083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8119937","Adaptive cruise control;approximate Q-learning;drift counteraction control;reinforcement learning","Markov processes;Learning (artificial intelligence);Optimal control;Cruise control;Convergence;Adaptation models;Aerospace electronics","adaptive control;fuel economy;learning (artificial intelligence);optimal control;road vehicles;velocity control","training drift counteraction optimal control policies;reinforcement learning;DCOC problem;disturbance measurements;disturbance evolution;RL algorithms;desired distance headway;informed approximate Q-learning algorithm;adaptive cruise control;heuristic driver model;lead vehicle;host vehicle;safety;comfort;fuel economy considerations","","12","","29","IEEE","27 Nov 2017","","","IEEE","IEEE Journals"
"Multi-H∞ Controls for Unknown Input-Interference Nonlinear System With Reinforcement Learning","Y. Lv; J. Na; X. Zhao; Y. Huang; X. Ren","College of Electrical and Power Engineering, Taiyuan University of Technology, Taiyuan, China; Faculty of Mechanical and Electrical Engineering, Kunming University of Science and Technology, Kunming, China; School of Engineering, University of Warwick, Coventry, U.K.; Faculty of Mechanical and Electrical Engineering, Kunming University of Science and Technology, Kunming, China; Intelligent Control and Smart Energy (ICSE) Research Group, School of Automation, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","1 Sep 2023","2023","34","9","5601","5613","This article studies the multi- $\text{H}\infty $  controls for the input-interference nonlinear systems via adaptive dynamic programming (ADP) method, which allows for multiple inputs to have the individual selfish component of the strategy to resist weighted interference. In this line, the ADP scheme is used to learn the Nash-optimization solutions of the input-interference nonlinear system such that multiple  $\text{H}\infty $  performance indices can reach the defined Nash equilibrium. First, the input-interference nonlinear system is given and the Nash equilibrium is defined. An adaptive neural network (NN) observer is introduced to identify the input-interference nonlinear dynamics. Then, the critic NNs are used to learn the multiple  $\text{H}\infty $  performance indices. A novel adaptive law is designed to update the critic NN weights by minimizing the Hamiltonian–Jacobi–Isaacs (HJI) equation, which can be used to directly calculate the multi- $\text{H}\infty $  controls effectively by using input–output data such that the actor structure is avoided. Moreover, the control system stability and updated parameter convergence are proved. Finally, two numerical examples are simulated to verify the proposed ADP scheme for the input-interference nonlinear system.","2162-2388","","10.1109/TNNLS.2021.3130092","National Natural Science Foundation of China (NSFC)(grant numbers:62103296,61922037,61873115,62003153,61973036); U.K. Engineering and Physical Sciences Research Council(grant numbers:EP/S001905/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9640331","Adaptive dynamic programming (ADP);H∞ control;multi-input system;neural networks (NNs);nonlinear system","Interference;Games;Nonlinear systems;Nash equilibrium;Resists;Mathematical models;Cost function","adaptive control;dynamic programming;game theory;H∞ control;neurocontrollers;nonlinear control systems;nonlinear systems","adaptive dynamic programming method;input-interference nonlinear dynamics;multiH∞ controls;unknown input-interference nonlinear system","","12","","46","IEEE","7 Dec 2021","","","IEEE","IEEE Journals"
"Hybrid Policy-Based Reinforcement Learning of Adaptive Energy Management for the Energy Transmission-Constrained Island Group","L. Yang; X. Li; M. Sun; C. Sun","School of Artificial Intelligence, Anhui University, Hefei, China; School of Artificial Intelligence, Anhui University, Hefei, China; Institutes of Physical Science and Information Technology, Anhui University, Hefei, China; School of Automation, Southeast University, Nanjing, China","IEEE Transactions on Industrial Informatics","19 Sep 2023","2023","19","11","10751","10762","This article proposes a hybrid policy-based reinforcement learning (HPRL) adaptive energy management to realize the optimal operation for the island group energy system with energy transmission-constrained environment. An island energy hub (IEH) model that can realize the energy cascade utilization is proposed. Compared with the traditional model, the IEH can satisfy the special energy demand of island, meanwhile, ensure the energy supply of island. Moreover, an energy management model of islands group (EMIG) based on the IEH is formulated which comprehensively considers the inverse distribution of energy demand and resources, as well as the limited energy transmission. Since the environment model of the island is difficult to construct due to the increase of proportion of renewable energy generation and civilian load, the EMIG is transformed into a reinforcement learning (RL) task which features model-free. Considering the limitations of traditional RL in discrete-continuous hybrid action space, HPRL is proposed to achieve optimal operation without simplifying the model. Numerical simulations demonstrate the effectiveness of the proposed adaptive energy management.","1941-0050","","10.1109/TII.2023.3241682","National Key R&D Program of China(grant numbers:2018AAA0101400); National Natural Science Foundation of China(grant numbers:62236002,61921004,62273093); China Postdoctoral Science Foundation(grant numbers:2022M710174); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20202006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035427","Adaptive energy management;energy cascade utilization;hybrid policy-based reinforcement learning;island energy hub (IEH);island group","Adaptation models;Energy management;Turbines;Water heating;Optimization;Load modeling;Sun","","","","11","","40","IEEE","2 Feb 2023","","","IEEE","IEEE Journals"
"Training Reinforcement Learning Agent for Traffic Signal Control under Different Traffic Conditions","J. Zeng; J. Hu; Y. Zhang","Department of Automation, Tsinghua University, Beijing, China; Beijing National Research Center for Information Science and Technology, Beijing, China; Beijing National Research Center for Information Science and Technology, Beijing, China","2019 IEEE Intelligent Transportation Systems Conference (ITSC)","28 Nov 2019","2019","","","4248","4254","The model-free reinforcement learning algorithm relieves traffic signal control problem from complex traffic modeling, and is able to learn a reasonable traffic light control policy from virtual simulation. However, the intrinsic characteristic of traffic flow might be helpful for learning a more suitable traffic signal control policy. Hence, in this paper, we investigate the performance of training reinforcement learning agent under different traffic conditions and propose a framework to combine prior traffic knowledge with deep reinforcement learning idea. The proposed network structure contains a simple Softmax classification branch and Q-value network branch, namely Mixed Q-network (MQN), is trained by using Q-learning with memory palace that maintains different replay buffers for different classifications. The comparative deep Q-network (DQN) is trained by using Q-learning with experience replay. Based on the experiments, both DQN and MQN method are able to learn a reasonable traffic signal timing and achieve lower average delay, shorter queue length and less waiting time than fixed time control. Moreover, the MQN could classify the traffic environment and select corresponding reinforcement learning controller at the same time.","","978-1-5386-7024-8","10.1109/ITSC.2019.8917342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917342","","Green products;Learning (artificial intelligence);Neural networks;Training;Machine learning;Measurement;IEEE members","control engineering computing;learning (artificial intelligence);multi-agent systems;neural nets;road traffic control;traffic engineering computing","Softmax classification branch;reinforcement learning agent;Q-learning;mixed Q-network;traffic light control policy;deep reinforcement learning;reinforcement learning controller;traffic environment;fixed time control;traffic signal timing;deep Q-network;replay buffers;traffic signal control policy;traffic flow;complex traffic modeling;traffic signal control problem;model-free reinforcement learning algorithm","","11","","18","IEEE","28 Nov 2019","","","IEEE","IEEE Conferences"
"A Human-Machine Reinforcement Learning Method for Cooperative Energy Management","Y. Tao; J. Qiu; S. Lai; X. Zhang; Y. Wang; G. Wang","School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; College of Mechatronics and Control Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Industrial Informatics","3 Feb 2022","2022","18","5","2974","2985","The increasing penetration of distributed energy resources and a large volume of unprecedented data from smart metering infrastructure can help consumers transit to an active role in the smart grid. In this article, we propose a human-machine reinforcement learning (RL) framework in the smart grid context to formulate an energy management strategy for electric vehicles and thermostatically controlled loads aggregators. The proposed model-free method accelerates the decision-making speed by substituting the conventional optimization process, and it is more capable of coping with the diverse system environment via online learning. The human intervention is coordinated with machine learning to: 1) prevent the huge loss during the learning process; 2) realize emergency control; and 3) find preferable control policy. The performance of the proposed human-machine RL framework is verified in case studies. It can be concluded that our proposed method performs better than the conventional deep Q-learning and deep deterministic policy gradient in terms of convergence capability and preferable result exploration. Besides, the proposed method can better deal with emergent events, such as a sudden drop of photovoltaic (PV) output. Compared with the conventional model-based method, there are slight deviations between our method and the optimal solution, but the decision-making time is significantly reduced.","1941-0050","","10.1109/TII.2021.3105115","ARC Research Hub(grant numbers:IH180100020); ARC Training Centre(grant numbers:IC200100023); ARC Linkage(grant numbers:LP200100056); Sir William Tyree Foundation-Distributed Power Generation Research Fund; Shenzhen Institute of Artificial Intelligence and Robotics for Society; Foundations of Shenzhen Science and Technology Committee(grant numbers:JCYJ20170817100412438,JCYJ20190808141019317); National Natural Science Foundation of China(grant numbers:72001058); General Program of Foundations of Shenzhen Science and Technology Committee(grant numbers:GXWD20201230155427003-20200822103658001); Research Start-Up Foundation; Harbin Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516907","Electric vehicles (EVs);energy management;human-machine;reinforcement learning (RL);thermostatically controlled loads (TCLs)","Optimization;Man-machine systems;Load modeling;Vehicle-to-grid;Reinforcement learning;Batteries;Voltage control","decision making;distributed power generation;electric vehicles;energy management systems;optimisation;reinforcement learning;smart meters;smart power grids;thermostats","distributed energy resources;smart metering infrastructure;human-machine reinforcement learning framework;smart grid;cooperative energy management strategy;electric vehicles;optimization process;diverse system;online learning;machine learning;deep Q-learning;deep deterministic policy gradient;decision-making time","","10","","28","IEEE","18 Aug 2021","","","IEEE","IEEE Journals"
"Safe Exploration in Wireless Security: A Safe Reinforcement Learning Algorithm With Hierarchical Structure","X. Lu; L. Xiao; G. Niu; X. Ji; Q. Wang","Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Automation, Tsinghua University, Beijing, China; School of Cyber Science and Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Information Forensics and Security","21 Feb 2022","2022","17","","732","743","Most safe reinforcement learning (RL) algorithms depend on the accurate reward that is rarely available in wireless security applications and suffer from severe performance degradation for the learning agents that have to choose the policy from a large action set. In this paper, we propose a safe RL algorithm, which uses a policy priority-based hierarchical structure to divide each policy into sub-policies with different selection priorities and thus compresses the action set. By applying inter-agent transfer learning to initialize the learning parameters, this algorithm accelerates the initial exploration of the optimal policy. Based on a security criterion that evaluates the risk value, the sub-policy distribution formulation avoids the dangerous sub-policies that cause learning failure such as severe network security problems in wireless security applications, e.g., Internet services interruption. We also propose a deep safe RL and design four deep neural networks in each sub-policy selection to further improve the learning efficiency for the learning agents that support four convolutional neural networks (CNNs): The Q-network evaluates the long-term expected reward of each sub-policy under the current state, and the E-network evaluates the long-term risk value. The target Q and E-networks update the learning parameters of the corresponding CNN to improve the policy exploration stability. As a case study, our proposed safe RL algorithms are implemented in the anti-jamming communication of unmanned aerial vehicles (UAVs) to select the frequency channel and transmit power to the ground node. Experimental results show that our proposed schemes significantly improve the UAV communication performance, save the UAV energy and increase the reward compared with the benchmark against jamming.","1556-6021","","10.1109/TIFS.2022.3149396","Natural Science Foundation of China(grant numbers:U21A20444,61971366,U20B2049,61822207); Fundamental Research Funds for the Central Universities(grant numbers:2042021gf0006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9705557","Reinforcement learning;deep learning;safe exploration;wireless security;jamming attacks","Security;Jamming;Communication system security;Optimization;Q-learning;Complexity theory;Bit error rate","autonomous aerial vehicles;convolutional neural nets;Internet;jamming;neural nets;reinforcement learning;telecommunication network reliability","learning parameters;policy exploration stability;safe RL algorithm;safe exploration;safe reinforcement learning algorithm;accurate reward;wireless security applications;severe performance degradation;learning agents;policy priority-based hierarchical structure;different selection priorities;inter-agent transfer;initial exploration;optimal policy;security criterion;sub-policy distribution formulation;dangerous sub-policies;learning failure;severe network security problems;deep safe RL;design four deep neural networks;sub-policy selection;learning efficiency;convolutional neural networks;Q-network;long-term expected reward;E-network;long-term risk value","","10","","33","IEEE","7 Feb 2022","","","IEEE","IEEE Journals"
"Fault-Tolerant Predictive Control With Deep-Reinforcement-Learning-Based Torque Distribution for Four In-Wheel Motor Drive Electric Vehicles","H. Deng; Y. Zhao; A. -T. Nguyen; C. Huang","Department of Vehicle Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Vehicle Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Laboratory of Industrial and Human Automation Control, Mechanical Engineering and Computer Science, UMR CNRS 8201, Université Polytechnique Hauts-de-France, Valenciennes, France; Department of Industrial and System Engineering, The Hong Kong Polytechnical University, Hong Kong","IEEE/ASME Transactions on Mechatronics","17 Apr 2023","2023","28","2","668","680","This article proposes a fault-tolerant control (FTC) method for four in-wheel motor drive electric vehicles considering both vehicle stability and motor power consumption. First, a seven-degree-of-freedom vehicle nonlinear model integrating motor faults is built to design a hierarchical FTC control scheme. The control structure is composed of two levels: an upper level nonlinear model-predictive controller and a lower level fault-tolerant coordinated controller. The upper level controller provides an appropriate reference in terms of additional yaw moment and vehicle longitudinal force, required for vehicle stability control, to the lower level controller. This latter aims at distributing the four-wheel torques taking into account both vehicle stability and power consumption. Specifically, the weighting factor involved in the optimization-based design of the lower level controller is determined online by the randomized ensembled double $Q$-learning reinforcement learning algorithm to achieve an optimal control strategy for the whole vehicle operating range. Moreover, the tradeoff between vehicle stability and power consumption is analyzed, and the necessity of using reinforcement learning is discussed. Numerical experiments are performed under various driving scenarios with a high-fidelity CarSim vehicle model to demonstrate the effectiveness of the proposed control method. Via a comparative study, we highlight the advantages of the new FTC control method over many related existing control results in terms of improving the vehicle stability and driver comfort as well as reducing the power consumption.","1941-014X","","10.1109/TMECH.2022.3233705","Postgraduate Research & Practice Innovation Program of Jiangsu Province(grant numbers:KYCX21_0188); National Engineering Laboratory of High Mobility Anti-riot Vehicle Technology(grant numbers:B20210017); National Natural Science Foundation of China(grant numbers:11672127); Fundamental Research Funds for the Central Universities(grant numbers:NP2022408); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024405","Electric ground vehicles;fault-tolerant control (FTC);in-wheel/hub motor;reinforcement learning (RL);torque vectoring;vehicle motion dynamics","Numerical stability;Power demand;Fault tolerant systems;Fault tolerance;Torque;Stability criteria;Actuators","automobiles;control engineering computing;control system synthesis;deep learning (artificial intelligence);electric vehicles;fault tolerance;fault tolerant control;nonlinear control systems;predictive control;reinforcement learning;stability","control results;control structure;deep-reinforcement-learning-based torque distribution;fault-tolerant control method;fault-tolerant predictive control;four-wheel torques;FTC control method;hierarchical FTC control scheme;high-fidelity CarSim vehicle model;in-wheel motor drive electric vehicles;lower level controller;motor faults;motor power consumption;optimal control strategy;optimization-based design;randomized ensembled doubleQ-learning reinforcement;reinforcement learning;seven-degree-of-freedom vehicle nonlinear model;upper level controller;upper level nonlinear model-predictive controller;vehicle longitudinal force;vehicle operating range;vehicle stability control;yaw moment","","10","","37","IEEE","23 Jan 2023","","","IEEE","IEEE Journals"
"Beyond-Visual-Range Air Combat Tactics Auto-Generation by Reinforcement Learning","H. Piao; Z. Sun; G. Meng; H. Chen; B. Qu; K. Lang; Y. Sun; S. Yang; X. Peng","School of Electronics and Information, Northwestern Polytechnical University, Xian, China; Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China; School of Automation, Shenyang Aerospace University, Shenyang, China; School of Artificial Intelligence, Jilin University, Changchun, China; School of Artificial Intelligence, Jilin University, Changchun, China; Department of Electronics System, SADRI Institute, Shenyang, China; Department of Electronics System, SADRI Institute, Shenyang, China; Department of Electronics System, SADRI Institute, Shenyang, China; Department of Electronics System, SADRI Institute, Shenyang, China","2020 International Joint Conference on Neural Networks (IJCNN)","28 Sep 2020","2020","","","1","8","For quite a long time, effective Beyond-Visual-Range (BVR) air combat tactics can only be discovered by human pilots in the actual combat process. However, due to the lack of actual combat opportunities, making new air combat tactics innovation was generally considered quite difficult. To address this challenge, we first introduced a solely end-to-end Reinforcement Learning (RL) approach for training competitive air combat agents with adversarial self-play from scratch in a high fidelity air combat simulation environment during training. Furthermore, a Key Air Combat Event Reward Shaping (KAERS) mechanism was proposed to provide sparse but objective shaped rewards beyond episodic win/lose signal to accelerate the initial machine learning process. Experimental results showed that multiple valuable air combat tactical behaviors emerged progressively. We hope this study could be extended to the future of air combat machine intelligence research.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9207088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207088","air combat;reinforcement learning","Aircraft;Training;Games;Learning (artificial intelligence);Atmospheric modeling;Markov processes","learning (artificial intelligence);military aircraft;military computing","visual-range air combat tactics auto-generation;reinforcement learning;air combat tactics innovation;training competitive air combat agents;high fidelity air combat simulation environment;machine learning process;air combat machine intelligence research;air combat tactical behaviors;key air combat event reward shaping mechanism;beyond-visual-range air combat tactics;machine intelligence research","","10","","32","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
