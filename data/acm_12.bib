@inproceedings{10.1145/3067695.3076035,
author = {Peng, Yiming and Chen, Gang and Holdaway, Scott and Mei, Yi and Zhang, Mengjie},
title = {Automated State Feature Learning for Actor-Critic Reinforcement Learning through NEAT},
year = {2017},
isbn = {9781450349390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3067695.3076035},
doi = {10.1145/3067695.3076035},
abstract = {Actor-Critic (AC) algorithms are important approaches to solving sophisticated reinforcement learning problems. However, the learning performance of these algorithms rely heavily on good state features that are often designed manually. To address this issue, we propose to adopt an evolutionary approach based on NeuroEvolution of Augmenting Topology (NEAT) to automatically evolve neural networks that directly transform the raw environmental inputs into state features. Following this idea, we have successfully developed a new algorithm called NEAT+AC which combines Regular-gradient Actor-Critic (RAC) with NEAT. It can simultaneously learn suitable state features as well as good policies that are expected to significantly improve the reinforcement learning performance. Preliminary experiments on two benchmark problems confirm that our new algorithm can clearly outperform the baseline algorithm, i.e., NEAT.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {135–136},
numpages = {2},
keywords = {feature extraction, NEAT, reinforcement learning, feature learning, NeuroEvolution, actor-critic},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.1145/3488560.3498526,
author = {Montazeralghaem, Ali and Allan, James},
title = {Learning Relevant Questions for Conversational Product Search Using Deep Reinforcement Learning},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498526},
doi = {10.1145/3488560.3498526},
abstract = {We propose RelQuest, a conversational product search model based on reinforcement learning to generate questions from product descriptions in each round of the conversation, directly maximizing any desired metrics (i.e., the ultimate goal of the conversation), objectives, or even an arbitrary user satisfaction signal. By enabling systems to ask questions about user needs, conversational product search has gained increasing attention in recent years. Asking the right questions through conversations helps the system collect valuable feedback to create better user experiences and ultimately increase sales. In contrast, existing conversational product search methods are based on an assumption that there is a set of effectively pre-defined candidate questions for each product to be asked. Moreover, they make strong assumptions to estimate the value of questions in each round of the conversation. Estimating the true value of questions in each round of the conversation is not trivial since it is unknown. Experiments on real-world user purchasing data show the effectiveness of RelQuest at generating questions that maximize standard evaluation measures such as NDCG.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {746–754},
numpages = {9},
keywords = {intelligent assistants, reinforcement learning, conversational product search, relevant question},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.5555/3545946.3599088,
author = {Lee, Namyeong and Moon, Jun},
title = {Transformer Actor-Critic with Regularization: Automated Stock Trading Using Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recently, with the increasing interest in investments in financial stock markets, several methods have been proposed to automatically trade stocks and/or predict future stock prices using machine learning techniques, such as reinforcement learning (RL), LSTM, and transformers. Among them, RL has been applied to manage portfolio assets with a sequence of optimal actions. The most important factor in investing in stocks is the utilization of past stock price data. However, existing RL algorithms applied to stock markets do not consider past stock data when taking optimal actions, as RL is formulated based on the Markov decision process (MDP). To resolve this limitation, we propose Transformer Actor-Critic with Regularization (TACR) using decision transformer to train the model with the correlation of past MDP elements using an attention network. In addition, a critic network is added to improve the performance by updating the parameters based on the evaluation of an action. For an efficient learning method, we train our model using an offline RL algorithm through suboptimal trajectories. To prevent overestimating the value of actions and reduce learning time, we train TACR through a regularization technique with an added behavior cloning term. The experimental results using various stock market data show that TACR performs better than other state-of-the-art methods in terms of the Sharpe ratio and profit.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2815–2817},
numpages = {3},
keywords = {reinforcement learning, sequence modeling, portfolio allocation},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3597926.3598047,
author = {Yu, Shiwen and Wang, Ting and Wang, Ji},
title = {Loop Invariant Inference through SMT Solving Enhanced Reinforcement Learning},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598047},
doi = {10.1145/3597926.3598047},
abstract = {Inferring loop invariants is one of the most challenging problems in program verification. It is highly desired to incorporate machine learning when inferring. This paper presents a Reinforcement Learning (RL) pruning framework to infer loop invariants over a general nonlinear hypothesis space. The key idea is to synergize the RL-based pruning and SMT solving to generate candidate invariants efficiently. To address the sparse reward problem in learning, we design a novel two-dimensional reward mechanism that enables the RL pruner to recognize the capability boundary of SMT solvers and learn the pruning heuristics in a few rounds. We have implemented our approach with Z3 SMT solver in the tool called LIPuS and conducted extensive experiments over the linear and nonlinear benchmarks. Experiment results show that LIPuS can solve the most cases compared to the state-of-the-art loop invariant inference tools such as Code2Inv, ICE-DT, GSpacer, SymInfer, ImplCheck, and Eldarica. Especially, LIPuS outperforms them significantly on nonlinear benchmarks.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {175–187},
numpages = {13},
keywords = {loop invariant, program verification, reinforcement learning},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1109/WI-IAT.2009.147,
author = {Wu, Cheng and Meleis, Waleed},
title = {Adaptive Fuzzy Function Approximation for Multi-Agent Reinforcement Learning},
year = {2009},
isbn = {9780769538013},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2009.147},
doi = {10.1109/WI-IAT.2009.147},
abstract = {Reinforcement learning has difficulties in solving multi-agent problems because of the inefficiency of function approximation. Sparse distributed memories, which is implemented using Radial Basis Functions or Kanerva Coding, can be used to improve the efficiency. But this approach still often give poor performance when applied to large-scale multi-agent systems. In this paper, we attempt to solve a collection of instances in the predator-prey pursuit domain and argue that the poor performance that we observe is caused by frequent prototype collisions. We show that dynamic prototype allocation and adaptation can give better results by reducing these collisions. We then describe our novel approach, fuzzy Kanerva-based function approximation, that uses a fine-grained fuzzy membership grade to describe a state-action pair's adjacency with respect to each prototype. This approach completely eliminates prototype collisions. We further show that prototype density varies widely across the state-action space and that this variation causes prototypes' receptive fields to be unevenly distributed. This distribution limits the ability of fuzzy Kanerva Coding to achieve better results. We demonstrate that another advantage of fuzzy Kanerva Coding is that it allows prototypes to tune their receptive fields for a target application. We conclude that fuzzy Kanerva Coding with prototype tuning and adaptation can significantly improve a reinforcement learner's ability to solve large-scale multi-agent problems.},
booktitle = {Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {169–176},
numpages = {8},
keywords = {Reinforcement Learning, Sparse Distributed Memory, Fuzzy Logic, Function Approximation},
series = {WI-IAT '09}
}

@article{10.1145/3579829,
author = {Xu, Meng and Wang, Jianping},
title = {Deep Reinforcement Learning for Parameter Tuning of Robot Visual Servoing},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3579829},
doi = {10.1145/3579829},
abstract = {Robot visual servoing controls the motion of a robot through real-time visual observations. Kinematics is a key approach to achieving visual servoing. One key challenge of kinematics-based visual servoing is that it requires time-varying parameter configuration throughout the entire process of one task. Parameter tuning is also necessary when applying to different tasks. The existing work on parameter tuning either lacks adaptation or cannot automate the tuning of all parameters. Meanwhile, the transferability of existing methods from one task to another is low. This work develops a Deep Reinforcement Learning (DRL) framework for robot visual servoing, which can automate all parameters tuning for one task and across tasks. In visual servoing, forward kinematics focuses on motion speed, while inverse kinematics focuses on the smoothness of motion. Therefore, we develop two separate modules in the proposed DRL framework. One tunes time-varying Forward Kinematics parameters to accelerate the motion, and the other tunes the Inverse Kinematics parameters to ensure smoothness. Moreover, we customize a knowledge transfer method to generalize the proposed DRL models to various robot tasks without reconstructing the neural network. We verify the proposed method on simulated robot tasks. The experimental results show that the proposed method outperforms the state-of-the-art methods and manual parameter configuration in terms of movement speed and smoothness in one task and across tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {feb},
articleno = {33},
numpages = {27},
keywords = {knowledge transfer, Deep Reinforcement Learning, parameter tuning, kinematics, Robot visual servoing}
}

@article{10.1145/3514260,
author = {Chang, Yi-Hsiang and Chang, Kuan-Yu and Kuo, Henry and Lee, Chun-Yi},
title = {Reusability and Transferability of Macro Actions for Reinforcement Learning},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3514260},
doi = {10.1145/3514260},
abstract = {Conventional reinforcement learning (RL) typically determines an appropriate primitive action at each timestep. However, by using a proper macro action, defined as a sequence of primitive actions, an RL agent is able to bypass intermediate states to a farther state and facilitate its learning procedure. The problem we would like to investigate is what associated beneficial properties that macro actions may possess. In this article, we unveil the properties of reusability and transferability of macro actions. The first property, reusability, means that a macro action derived along with one RL method can be reused by another RL method for training, while the second one, transferability, indicates that a macro action can be utilized for training agents in similar environments with different reward settings. In our experiments, we first derive macro actions along with RL methods. We then provide a set of analyses to reveal the properties of reusability and transferability of the derived macro actions.},
journal = {ACM Trans. Evol. Learn. Optim.},
month = {apr},
articleno = {4},
numpages = {16},
keywords = {reusability, transferability, genetic algorithm, macro actions, Reinforcement learning}
}

@inproceedings{10.1145/3442381.3449935,
author = {Oda, Takuma},
title = {Equilibrium Inverse Reinforcement Learning for Ride-Hailing Vehicle Network},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449935},
doi = {10.1145/3442381.3449935},
abstract = {Ubiquitous mobile computing have enabled ride-hailing services to collect vast amounts of behavioral data of riders and drivers and optimize supply and demand matching in real time. While these mobility service providers have some degree of control over the market by assigning vehicles to requests, they need to deal with the uncertainty arising from self-interested driver behavior since workers are usually free to drive when they are not assigned tasks. If a driver’s behavior can be accurately replicated on the digital twin, more detailed and realistic counterfactual simulations will enable decision making to improve mobility services as well as to validate urban planning. In this work, we formulate the problem of passenger-vehicle matching in a sparsely connected graph and proposed an algorithm to derive an equilibrium policy in a multi-agent environment. Our framework combines value iteration methods to estimate the optimal policy given expected state visitation and policy propagation to compute multi-agent state visitation frequencies. Furthermore, we developed a method to learn the driver’s reward function transferable to an environment with significantly different dynamics from training data. We evaluated the robustness to changes in spatio-temporal supply-demand distributions and deterioration in data quality using a real-world taxi trajectory dataset; our approach significantly outperforms several baselines in terms of imitation accuracy. The computational time required to obtain an equilibrium policy shared by all vehicles does not depend on the number of agents, and even on the scale of real-world services, it takes only a few seconds on a single CPU.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2281–2290},
numpages = {10},
keywords = {inverse reinforcement learning, Counterfactual simulation, equilibrium, Markov games},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3590003.3590101,
author = {Shen, Jiamin and Xu, Li and Wan, Xu and Chai, Jixuan and Fan, Chunlong},
title = {Research on Constant Perturbation Strategy for Deep Reinforcement Learning},
year = {2023},
isbn = {9781450399449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590003.3590101},
doi = {10.1145/3590003.3590101},
abstract = {The development of attack algorithms for deep reinforcement learning is an important part of its security research. In this paper, we propose a deep reinforcement constant perturbation strategy approach for deep reinforcement learning with long-range time-series dependence from the perspective of the sequence of interaction between an agent and its environment.The algorithm is based on a small amount of historical interaction information, and a constant perturbation is designed to disrupt the long-range temporal association of the deep reinforcement learning algorithm based on sensitive region selection to achieve the attack effect.The experimental results show that the constant perturbation based on time series has a good effect, i.e. inducing agents to make frequent wrong decisions and get minimal reward. At the same time, this algorithm still has an attacking effect on the defensively trained agents, and it effectively reduces the number of computations adversarial perturbations.},
booktitle = {Proceedings of the 2023 2nd Asia Conference on Algorithms, Computing and Machine Learning},
pages = {526–533},
numpages = {8},
keywords = {time series dependence., robustness of models, constant perturbation strategies, Deep reinforcement learning},
location = {Shanghai, China},
series = {CACML '23}
}

@inproceedings{10.1145/3533271.3561760,
author = {Huang, Yilie and Jia, Yanwei and Zhou, Xunyu},
title = {Achieving Mean–Variance Efficiency by Continuous-Time Reinforcement Learning},
year = {2022},
isbn = {9781450393768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533271.3561760},
doi = {10.1145/3533271.3561760},
abstract = {We conduct an extensive empirical analysis to evaluate the performance of the recently developed reinforcement learning algorithms by Jia and Zhou [11] in asset allocation tasks. We propose an efficient implementation of the algorithms in a dynamic mean-variance portfolio selection setting. We compare it with the conventional plug-in estimator and two state-of-the-art deep reinforcement learning algorithms, deep deterministic policy gradient (DDPG) and proximal policy optimization (PPO), with both simulated and real market data. On both data sets, our algorithm significantly outperforms the others. In particular, using the US stocks data from Jan 2000 to Dec 2019, we demonstrate the effectiveness of our algorithm in reaching the target return and maximizing the Sharpe ratio for various periods under consideration, including the period of the financial crisis in 2007-2008. By contrast, the plug-in estimator performs poorly on real data sets, and PPO performs better than DDPG but still has lower Sharpe ratio than the market. Our algorithm also outperforms two well-diversified portfolios: the market and equally weighted portfolios.},
booktitle = {Proceedings of the Third ACM International Conference on AI in Finance},
pages = {377–385},
numpages = {9},
keywords = {actor-critic, dynamic mean-variance analysis, reinforcement learning, policy gradient, portfolio choice},
location = {New York, NY, USA},
series = {ICAIF '22}
}

@article{10.5555/2503308.2343705,
author = {Tamar, Aviv and Di Castro, Dotan and Meir, Ron},
title = {Integrating a Partial Model into Model Free Reinforcement Learning},
year = {2012},
issue_date = {January 2012},
publisher = {JMLR.org},
volume = {13},
number = {1},
issn = {1532-4435},
abstract = {In reinforcement learning an agent uses online feedback from the environment in order to adaptively select an effective policy. Model free approaches address this task by directly mapping environmental states to actions, while model based methods attempt to construct a model of the environment, followed by a selection of optimal actions based on that model. Given the complementary advantages of both approaches, we suggest a novel procedure which augments a model free algorithm with a partial model. The resulting hybrid algorithm switches between a model based and a model free mode, depending on the current state and the agent's knowledge. Our method relies on a novel definition for a partially known model, and an estimator that incorporates such knowledge in order to reduce uncertainty in stochastic approximation iterations. We prove that such an approach leads to improved policy evaluation whenever environmental knowledge is available, without compromising performance when such knowledge is absent. Numerical simulations demonstrate the effectiveness of the approach on policy gradient and Q-learning algorithms, and its usefulness in solving a call admission control problem.},
journal = {J. Mach. Learn. Res.},
month = {jun},
pages = {1927–1966},
numpages = {40},
keywords = {markov decision processes, hybrid model based model free algorithms, stochastic approximation, reinforcement learning, temporal difference}
}

@inproceedings{10.1145/3461702.3462615,
author = {Ramachandran, Govardana Sachithanandam and Brugere, Ivan and Varshney, Lav R. and Xiong, Caiming},
title = {GAEA: Graph Augmentation for Equitable Access via Reinforcement Learning},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462615},
doi = {10.1145/3461702.3462615},
abstract = {Disparate access to resources by different subpopulations is a prevalent issue in societal and sociotechnical networks. For example, urban infrastructure networks may enable certain racial groups to more easily access resources such as high-quality schools, grocery stores, and polling places. Similarly, social networks within universities and organizations may enable certain groups to more easily access people with valuable information or influence. Here we introduce a new class of problems, Graph Augmentation for Equitable Access (GAEA), to enhance equity in networked systems by editing graph edges under budget constraints. We prove such problems are NP-hard, and cannot be approximated within a factor of (1-1/3e). We develop a principled, sample- and time- efficient Markov Reward Process (MRP)-based mechanism design framework for GAEA. Our algorithm outperforms baselines on a diverse set of synthetic graphs. We further demonstrate the method on real-world networks, by merging public census, school, and transportation datasets for the city of Chicago and applying our algorithm to find human-interpretable edits to the bus network that enhance equitable access to high-quality schools across racial groups. Further experiments on Facebook networks of universities yield sets of new social connections that would increase equitable access to certain attributed nodes across gender groups.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {884–894},
numpages = {11},
keywords = {social networks, equity, fairness, reinforcement learning, dataset},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{10.1145/3397271.3401174,
author = {Zhou, Sijin and Dai, Xinyi and Chen, Haokun and Zhang, Weinan and Ren, Kan and Tang, Ruiming and He, Xiuqiang and Yu, Yong},
title = {Interactive Recommender System via Knowledge Graph-Enhanced Reinforcement Learning},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401174},
doi = {10.1145/3397271.3401174},
abstract = {Interactive recommender system (IRS) has drawn huge attention because of its flexible recommendation strategy and the consideration of optimal long-term user experiences. To deal with the dynamic user preference and optimize accumulative utilities, researchers have introduced reinforcement learning (RL) into IRS. However, RL methods share a common issue of sample efficiency, i.e., huge amount of interaction data is required to train an effective recommendation policy, which is caused by the sparse user responses and the large action space consisting of a large number of candidate items. Moreover, it is infeasible to collect much data with explorative policies in online environments, which will probably harm user experience. In this work, we investigate the potential of leveraging knowledge graph (KG) in dealing with these issues of RL methods for IRS, which provides rich side information for recommendation decision making. Instead of learning RL policies from scratch, we make use of the prior knowledge of the item correlation learned from KG to (i) guide the candidate selection for better candidate item retrieval, (ii) enrich the representation of items and user states, and (iii) propagate user preferences among the correlated items over KG to deal with the sparsity of user feedback. Comprehensive experiments have been conducted on two real-world datasets, which demonstrate the superiority of our approach with significant improvements against state-of-the-arts.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {179–188},
numpages = {10},
keywords = {interactive recommender systems, graph neural networks, reinforcement learning, knowledge graphs},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.5555/1577069.1577109,
author = {Li, Hui and Liao, Xuejun and Carin, Lawrence},
title = {Multi-Task Reinforcement Learning in Partially Observable Stochastic Environments},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {We consider the problem of multi-task reinforcement learning (MTRL) in multiple partially observable stochastic environments. We introduce the regionalized policy representation (RPR) to characterize the agent's behavior in each environment. The RPR is a parametric model of the conditional distribution over current actions given the history of past actions and observations; the agent's choice of actions is directly based on this conditional distribution, without an intervening model to characterize the environment itself. We propose off-policy batch algorithms to learn the parameters of the RPRs, using episodic data collected when following a behavior policy, and show their linkage to policy iteration. We employ the Dirichlet process as a nonparametric prior over the RPRs across multiple environments. The intrinsic clustering property of the Dirichlet process imposes sharing of episodes among similar environments, which effectively reduces the number of episodes required for learning a good policy in each environment, when data sharing is appropriate. The number of distinct RPRs and the associated clusters (the sharing patterns) are automatically discovered by exploiting the episodic data as well as the nonparametric nature of the Dirichlet process. We demonstrate the effectiveness of the proposed RPR as well as the RPR-based MTRL framework on various problems, including grid-world navigation and multi-aspect target classification. The experimental results show that the RPR is a competitive reinforcement learning algorithm in partially observable domains, and the MTRL consistently achieves better performance than single task reinforcement learning.},
journal = {J. Mach. Learn. Res.},
month = {jun},
pages = {1131–1186},
numpages = {56}
}

@inproceedings{10.5555/2936924.2937000,
author = {Malialis, Kleanthis and Devlin, Sam and Kudenko, Daniel},
title = {Resource Abstraction for Reinforcement Learning in Multiagent Congestion Problems},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Real-world congestion problems (e.g. traffic congestion) are typically very complex and large-scale. Multiagent reinforcement learning (MARL) is a promising candidate for dealing with this emerging complexity by providing an autonomous and distributed solution to these problems. However, there are three limiting factors that affect the deployability of MARL approaches to congestion problems. These are learning time, scalability and decentralised coordination i.e. no communication between the learning agents. In this paper we introduce Resource Abstraction, an approach that addresses these challenges by allocating the available resources into abstract groups. This abstraction creates new reward functions that provide a more informative signal to the learning agents and aid the coordination amongst them. Experimental work is conducted on two benchmark domains from the literature, an abstract congestion problem and a realistic traffic congestion problem. The current state-of-the-art for solving multiagent congestion problems is a form of reward shaping called difference rewards. We show that the system using Resource Abstraction significantly improves the learning speed and scalability, and achieves the highest possible or near-highest joint performance/social welfare for both congestion problems in large-scale scenarios involving up to 1000 reinforcement learning agents.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {503–511},
numpages = {9},
keywords = {congestion problems, multiagent learning, resource abstraction},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.1145/3321707.3321813,
author = {Sharma, Mudita and Komninos, Alexandros and L\'{o}pez-Ib\'{a}\~{n}ez, Manuel and Kazakov, Dimitar},
title = {Deep Reinforcement Learning Based Parameter Control in Differential Evolution},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321813},
doi = {10.1145/3321707.3321813},
abstract = {Adaptive Operator Selection (AOS) is an approach that controls discrete parameters of an Evolutionary Algorithm (EA) during the run. In this paper, we propose an AOS method based on Double Deep Q-Learning (DDQN), a Deep Reinforcement Learning method, to control the mutation strategies of Differential Evolution (DE). The application of DDQN to DE requires two phases. First, a neural network is trained offline by collecting data about the DE state and the benefit (reward) of applying each mutation strategy during multiple runs of DE tackling benchmark functions. We define the DE state as the combination of 99 different features and we analyze three alternative reward functions. Second, when DDQN is applied as a parameter controller within DE to a different test set of benchmark functions, DDQN uses the trained neural network to predict which mutation strategy should be applied to each parent at each generation according to the DE state. Benchmark functions for training and testing are taken from the CEC2005 benchmark with dimensions 10 and 30. We compare the results of the proposed DE-DDQN algorithm to several baseline DE algorithms using no online selection, random selection and other AOS methods, and also to the two winners of the CEC2005 competition. The results show that DE-DDQN outperforms the non-adaptive methods for all functions in the test set; while its results are comparable with the last two algorithms.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {709–717},
numpages = {9},
keywords = {differential evolution, parameter control, reinforcement learning},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.5555/3306127.3331925,
author = {Barati, Elaheh and Chen, Xuewen and Zhong, Zichun},
title = {Attention-Based Deep Reinforcement Learning for Multi-View Environments},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In reinforcement learning algorithms, it is a common practice to account for only a single view of the environment to make the desired decisions; however, utilizing multiple views of the environment can help to promote the learning of complicated policies. Since the views may frequently suffer from partial observability, their provided observation can have different levels of importance. In this paper, we present a novel attention-based deep reinforcement learning method in a multi-view environment in which each view can provide various representative information about the environment. Specifically, our method learns a policy to dynamically attend to views of the environment based on their importance in the decision-making process. We evaluate the performance of our method on TORCS racing car simulator and three other complex 3D environments with obstacles.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1805–1807},
numpages = {3},
keywords = {reinforcement learning, attention networks, deep learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3301326.3301374,
author = {Liu, Hao and Zhu, Dandan and Liu, Yi and Du, Aimin and Chen, Dong and Ye, Zhihui},
title = {A Reinforcement Learning Based 3D Guided Drilling Method: Beyond Ground Control},
year = {2018},
isbn = {9781450365536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301326.3301374},
doi = {10.1145/3301326.3301374},
abstract = {The current drilling guide operation relies on the two-way transmission of signals between the downhole drilling tools and the ground control center. However, the downhole environment is sometimes not conducive to such real-time signal transmission, and the analysis and decision-making in the ground involves complex human expert analysis and fine management. To deal with these problems, this paper proposes a downhole self-steering guided drilling method based on a reinforcement learning framework to achieve the 3D well trajectory design and control in real-time. In every time interval of the drilling process, the proposed system evaluates the drilling status and gives the adjustment action of drill bit in 3D space according to the received data, guiding the drill bit to the target reservoir without the involvement of human. The main module is a modified deep Q network using Sarsa algorithm for online self-learning. The experimental results show that after training, the drill bit is increasingly able to select control actions closer to the target reservoir. The frequency of effective actions is approximately 258\% higher after the algorithm converges. The proposed system has the ability of online self-learning, which can automatically adjust the evaluation and decision models without manual monitoring.},
booktitle = {Proceedings of the 2018 VII International Conference on Network, Communication and Computing},
pages = {44–48},
numpages = {5},
keywords = {self-steering drilling, 3D guided drilling, Deep Q Network, reinforcement learning, logging while drilling data},
location = {Taipei City, Taiwan},
series = {ICNCC '18}
}

@inproceedings{10.1145/3448734.3450784,
author = {Han, Wen and Hou, Wenjing and Wen, Hong and Lei, Wenxin and Xu, Xinchen and Lin, Haojie},
title = {Reinforcement Learning-Based Multidimensional Resource Management in Edge Cloud Regions},
year = {2021},
isbn = {9781450389570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448734.3450784},
doi = {10.1145/3448734.3450784},
abstract = {Aim to achieve low latency, high efficiency, reliability, and low network cost, a management model of multidimensional resources is proposed under an architecture of an edge cloud server and multiple terminal devices in this paper. By taking advantages of reinforcement learning, the optimal task processing in a region is approached. For tasks with different time delays, use Deep Deterministic Policy Gradient to analyze task execution delays and costs in different scenarios, and use resource unloading and allocation to minimize overall task execution delay and overall resource consumption to optimize system operation.},
booktitle = {The 2nd International Conference on Computing and Data Science},
articleno = {54},
numpages = {5},
keywords = {DDPG, RL, Resource Management, Edge Computing, Cybertwin},
location = {Stanford, CA, USA},
series = {CONF-CDS 2021}
}

@inproceedings{10.1145/3409501.3409517,
author = {Gao, Huifan and Pan, Yinghui and Tang, Jing and Zeng, Yifeng and Chai, Peihua and Cao, Langcai},
title = {Value Function Dynamic Estimation in Reinforcement Learning Based on Data Adequacy},
year = {2020},
isbn = {9781450375603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409501.3409517},
doi = {10.1145/3409501.3409517},
abstract = {In recent years, reinforcement learning has played an important role in the study of decision problem in computer games. To solve the problem of how to better estimate the value function with limited computational resources, this paper proposes a dynamic estimation method of value function based on data adequacy. In consideration of the varying complexity of each state in the MDP model, we propose a dynamic value function estimation method which is different from the fixed value function estimation method in traditional methods. Based on the PigChase challenge of the Malmo project launched by Microsoft in 2017, we compare the new method with the existing techniques. Experimental results show that the performance of the proposed algorithm is better than traditional algorithms.},
booktitle = {Proceedings of the 2020 4th High Performance Computing and Cluster Technologies Conference \&amp; 2020 3rd International Conference on Big Data and Artificial Intelligence},
pages = {204–208},
numpages = {5},
keywords = {q-learning, reinforcement learning, confidence interval, dynamic programming, probability distribution},
location = {Qingdao, China},
series = {HPCCT \&amp; BDAI '20}
}

@article{10.1145/3366703,
author = {Sinclair, Sean R. and Banerjee, Siddhartha and Yu, Christina Lee},
title = {Adaptive Discretization for Episodic Reinforcement Learning in Metric Spaces},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3366703},
doi = {10.1145/3366703},
abstract = {We present an efficient algorithm for model-free episodic reinforcement learning on large (potentially continuous) state-action spaces. Our algorithm is based on a novel Q-learning policy with adaptive data-driven discretization. The central idea is to maintain a finer partition of the state-action space in regions which are frequently visited in historical trajectories, and have higher payoff estimates. We demonstrate how our adaptive partitions take advantage of the shape of the optimal Q-function and the joint space, without sacrificing the worst-case performance. In particular, we recover the regret guarantees of prior algorithms for continuous state-action spaces, which additionally require either an optimal discretization as input, and/or access to a simulation oracle. Moreover, experiments demonstrate how our algorithm automatically adapts to the underlying structure of the problem, resulting in much better performance compared both to heuristics and Q-learning with uniform discretization.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {dec},
articleno = {55},
numpages = {44},
keywords = {q-learning, model-free, reinforcement learning, adaptive discretization, metric spaces}
}

@inproceedings{10.5555/3535850.3536022,
author = {Avalos, Rapha\"{e}l and Reymond, Mathieu and Now\'{e}, Ann and Roijers, Diederik M.},
title = {Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent reinforcement learning (MARL) enables us to create adaptive agents in challenging environments, even when the agents have limited observation. Modern MARL methods have focused on finding factorized value functions. While successful, the resulting methods have convoluted network structures. We take a radically different approach and build on the structure of independent Q-learners. Our algorithm LAN leverages a dueling architecture to represent decentralized policies as separate individual advantage functions w.r.t. a centralized critic that is cast aside after training. The critic works as a stabilizer that coordinates the learning and to formulate DQN targets. This enables LAN to keep the number of parameters of its centralized network independent in the number of agents, without imposing additional constraints like monotonic value functions. When evaluated on the SMAC, LAN shows SOTA performance overall and scores more than 80\% wins in two super-hard maps where even QPLEX does not obtain almost any wins. Moreover, when the number of agents becomes large, LAN uses significantly fewer parameters than QPLEX or even QMIX. We thus show that LAN's structure forms a key improvement that helps MARL methods remain scalable.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1524–1526},
numpages = {3},
keywords = {coordination, reinforcement learning, multi-agent, cooperation},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.5555/3535850.3535906,
author = {Geist, Matthieu and P\'{e}rolat, Julien and Lauri\`{e}re, Mathieu and Elie, Romuald and Perrin, Sarah and Bachem, Oliver and Munos, R\'{e}mi and Pietquin, Olivier},
title = {Concave Utility Reinforcement Learning: The Mean-Field Game Viewpoint},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Concave Utility Reinforcement Learning (CURL) extends RL from linear to concave utilities in the occupancy measure induced by the agent's policy. This encompasses not only RL but also imitation learning and exploration, among others. Yet, this more general paradigm invalidates the classical Bellman equations, and calls for new algorithms. Mean-field Games (MFGs) are a continuous approximation of many-agent RL. They consider the limit case of a continuous distribution of identical agents, anonymous with symmetric interests, and reduce the problem to the study of a single representative agent in interaction with the full population. Our core contribution consists in showing that CURL is a subclass of MFGs. We think this important to bridge together both communities. It also allows to shed light on aspects of both fields: we show the equivalence between concavity in CURL and monotonicity in the associated MFG, between optimality conditions in CURL and Nash equilibrium in MFG, or that Fictitious Play (FP) for this class of MFGs is simply Frank-Wolfe, bringing the first convergence rate for discrete-time FP for MFGs. We also experimentally demonstrate that, using algorithms recently introduced for solving MFGs, we can address the CURL problem more efficiently.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {489–497},
numpages = {9},
keywords = {mean-field games, concave utility reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.5555/2936924.2937076,
author = {Bogert, Kenneth and Lin, Jonathan Feng-Shun and Doshi, Prashant and Kulic, Dana},
title = {Expectation-Maximization for Inverse Reinforcement Learning with Hidden Data},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We consider the problem of performing inverse reinforcement learning when the trajectory of the agent being observed is partially occluded from view. Motivated by robotic scenarios in which limited sensor data is available to a learner, we treat the missing information as hidden variables and present an algorithm based on expectation-maximization to solve the non-linear, non-convex problem. Previous work in this area simply removed the occluded portions from consideration when computing feature expectations; in contrast our technique takes expectations over the missing values, enabling learning even in the presence of dynamic occlusion. We evaluate our new algorithm in a simulated reconnaissance scenario in which the visible portion of the state space varies. Finally, we show our approach enables apprenticeship learning by observing a human performing a sorting task in spite of key information missing from observations.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {1034–1042},
numpages = {9},
keywords = {inverse reinforcement learning, fruit sorting, machine learning, latent variables},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.5555/3463952.3464079,
author = {Ryu, Heechang and Shin, Hayong and Park, Jinkyoo},
title = {Cooperative and Competitive Biases for Multi-Agent Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Training a multi-agent reinforcement learning (MARL) algorithm is more challenging than training a single-agent reinforcement learning algorithm, because the result of a multi-agent task strongly depends on the complex interactions among agents and their interactions with a stochastic and dynamic environment. We propose an algorithm that boosts MARL training using the biased action information of other agents based on a friend-or-foe concept. For a cooperative and competitive environment, there are generally two groups of agents: cooperative-agents and competitive-agents. In the proposed algorithm, each agent updates its value function using its own action and the biased action information of other agents in the two groups. The biased joint action of cooperative agents is computed as the sum of their actual joint action and the imaginary cooperative joint action, by assuming all the cooperative agents jointly maximize the target agent's value function. The biased joint action of competitive agents can be computed similarly. Each agent then updates its own value function using the biased action information, resulting in a biased value function and corresponding biased policy. Subsequently, the biased policy of each agent is inevitably subjected to recommend an action to cooperate and compete with other agents, thereby introducing more active interactions among agents and enhancing the MARL policy learning. We empirically demonstrate that our algorithm outperforms existing algorithms in various mixed cooperative-competitive environments. Furthermore, the introduced biases gradually decrease as the training proceeds and the correction based on the imaginary assumption vanishes.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1091–1099},
numpages = {9},
keywords = {cooperation, competition, multi-agent reinforcement learning, bias},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3449258.3449260,
author = {Oliveira Pereira, Tiago and Abbasi, Maryam and Ribeiro, Bernardete and P. Arrais, Joel},
title = {End-to-End Deep Reinforcement Learning for Targeted Drug Generation},
year = {2021},
isbn = {9781450388443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449258.3449260},
doi = {10.1145/3449258.3449260},
abstract = {The long period of time and the enormous financial costs required to bring a new drug to the market are a clear impediment to the development of new drugs. Deep Learning techniques at early stages of drug discovery can help to select candidate drugs with biological properties of interest, reduce the enormous research space of drug-like compounds and minimize these issues. This study aims to perform generation of targeted molecules by training the recurrent neural network to learn the building rules of production of valid molecules in the form of SMILES strings and optimize it to produce molecules with bespoke properties through Reinforcement Learning. The fitness of the newly generated molecules is obtained by a second neural network model. To demonstrate the effectiveness of the method, we trained the proposed model to design molecules with high inhibitory power for the k-opioid receptor (KOR). The optimized model was able to generate molecules with a stronger affinity for KOR, maintaining the percentage of valid molecules and, with satisfactory internal and external diversities based on Tanimoto similarity over 95\%.},
booktitle = {Proceedings of the 2020 4th International Conference on Computational Biology and Bioinformatics},
pages = {7–13},
numpages = {7},
keywords = {LSTM, QSARACM, Reinforcement Learning, RNN, Drug Design, SMILES},
location = {Bali Island, Indonesia},
series = {ICCBB '20}
}

@inproceedings{10.5555/3306127.3332057,
author = {Sun, Fan-Yun and Chang, Yen-Yu and Wu, Yueh-Hua and Lin, Shou-De},
title = {A Regulation Enforcement Solution for Multi-Agent Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Human behaviors are regularized by a variety of norms or regulations, either to maintain orders or to enhance social welfare. However, if artificially intelligent (AI) agents make decisions on behalf of human beings, it is possible that an AI agent can opt to disobey the regulations (being defective) for self-interests. In this paper, we aim to answer the following question: In a decentralized environment (no centralized authority can control agents), given that not all agents are compliant to regulations at first, can we develop a mechanism such that it is in the self-interest of non-compliant agents to comply after all. We first introduce the problem as Regulation Enforcement and formulate it using reinforcement learning and game theory. Then we propose our solution based on the key idea that although we could not alter how defective agents choose to behave, we can, however, leverage the aggregated power of compliant agents to boycott the defective ones. We conducted simulated experiments on two scenarios: Replenishing Resource Management Dilemma and Diminishing Reward Shaping Enforcement, using deep multi-agent reinforcement learning algorithms. We further use empirical game-theoretic analysis to show that the method alters the resulting empirical payoff matrices in a way that promotes compliance (making mutual compliant a Nash Equilibrium).},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2201–2203},
numpages = {3},
keywords = {multi-agent reinforcement learning, reward shaping, empirical game-theoretic analysis},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3341069.3341082,
author = {Ke, Fengkai and Zhao, Daxing and Sun, Guodong and Feng, Wei},
title = {Precise Evaluation for Continuous Action Control in Reinforcement Learning},
year = {2019},
isbn = {9781450371858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341069.3341082},
doi = {10.1145/3341069.3341082},
abstract = {With the development of deep learning, reinforcement learning also gradually into the eye, reinforcement learning has made remarkable achievements in games, go games and other fields, but most of the control problems involved in these fields or tasks are discrete action control with sufficient rewards. Continuous action control in reinforcement learning is closer to the actual control problem, and is considered as one of the main channels leading to artificial intelligence, so it is also one of the research hotspots of researchers. The traditional continuous control algorithm for reinforcement learning evaluates the network with multiple outputs of a single scalar value. In this paper, an accurate evaluation mechanism and corresponding objective function are proposed to accelerate the reinforcement learning training process. The experimental results show that the accurate evaluation of log-cosh objective function can make the robot arm grasp the task more quickly, converge and complete the training task.},
booktitle = {Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference},
pages = {67–70},
numpages = {4},
keywords = {Reinforcement Learning, Continuous Control, Robot, Precise Evaluation},
location = {Guangzhou, China},
series = {HPCCT '19}
}

@inproceedings{10.5555/3545946.3598772,
author = {Wang, Caroline and Warnell, Garrett and Stone, Peter},
title = {D-Shape: Demonstration-Shaped Reinforcement Learning via Goal-Conditioning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {While combining imitation learning (IL) and reinforcement learning (RL) is a promising way to address poor sample efficiency in autonomous behavior acquisition, methods that do so typically assume that the requisite behavior demonstrations are provided by an expert that behaves optimally with respect to a task reward. If, however, suboptimal demonstrations are provided, a fundamental challenge appears in that the demonstration-matching objective of IL conflicts with the return-maximization objective of RL. This paper introduces D-Shape, a new method for combining IL and RL that uses ideas from reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape allows learning from suboptimal demonstrations while retaining the ability to find the optimal policy with respect to the task reward. We experimentally validate D-Shape in sparse-reward gridworld domains, showing that it both improves over RL in terms of sample efficiency and converges consistently to the optimal policy in the presence of suboptimal demonstrations.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1267–1275},
numpages = {9},
keywords = {suboptimal demonstrations, goal-conditioned reinforcement learning, imitation from observation, reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.1613/jair.1.14398,
author = {Xu, Jiawei and Li, Shuxing and Yang, Rui and Yuan, Chun and Han, Lei},
title = {Efficient Multi-Goal Reinforcement Learning via Value Consistency Prioritization},
year = {2023},
issue_date = {Jun 2023},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {77},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.14398},
doi = {10.1613/jair.1.14398},
abstract = {Goal-conditioned reinforcement learning (RL) with sparse rewards remains a challenging problem in deep RL. Hindsight Experience Replay (HER) has been demonstrated to be an effective solution, where HER replaces desired goals in failed experiences with practically achieved states. Existing approaches mainly focus on either exploration or exploitation to improve the performance of HER. From a joint perspective, exploiting specific past experiences can also implicitly drive exploration. Therefore, we concentrate on prioritizing both original and relabeled samples for efficient goal-conditioned RL. To achieve this, we propose a novel value consistency prioritization (VCP) method, where the priority of samples is determined by the consistency of ensemble Q-values. This distinguishes the VCP method with most existing prioritization approaches which prioritizes samples based on the uncertainty of ensemble Q-values. Through extensive experiments, we demonstrate that VCP achieves significantly higher sample efficiency than existing algorithms on a range of challenging goal-conditioned manipulation tasks. We also visualize how VCP prioritizes good experiences to enhance policy learning.},
journal = {J. Artif. Int. Res.},
month = {jun},
numpages = {22}
}

@article{10.5555/3455716.3455886,
author = {Hoiles, William and Krishnamurthy, Vikram and Pattanayak, Kunal},
title = {Rationally Inattentive Inverse Reinforcement Learning Explains Youtube Commenting Behavior},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {We consider a novel application of inverse reinforcement learning with behavioral economics constraints to model, learn and predict the commenting behavior of YouTube viewers. Each group of users is modeled as a rationally inattentive Bayesian agent which solves a contextual bandit problem. Our methodology integrates three key components. First, to identify distinct commenting patterns, we use deep embedded clustering to estimate framing information (essential extrinsic features) that clusters users into distinct groups. Second, we present an inverse reinforcement learning algorithm that uses Bayesian revealed preferences to test for rationality: does there exist a utility function that rationalizes the given data, and if yes, can it be used to predict commenting behavior? Finally, we impose behavioral economics constraints stemming from rational inattention to characterize the attention span of groups of users. The test imposes a R\'{e}nyi mutual information cost constraint which impacts how the agent can select attention strategies to maximize their expected utility. After a careful analysis of a massive YouTube dataset, our surprising result is that in most YouTube user groups, the commenting behavior is consistent with optimizing a Bayesian utility with rationally inattentive constraints. The paper also highlights how the rational inattention model can accurately predict commenting behavior. The massive YouTube dataset and analysis used in this paper are available on GitHub and completely reproducible.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {170},
numpages = {39},
keywords = {R\'{e}nyi mutual information, deep embedded clustering, youtube, framing, behavioral economics, Bayesian revealed preference, inverse reinforcement learning, contextual bandits, rational inattention}
}

@inproceedings{10.5555/3306127.3331672,
author = {Palmer, Gregory and Savani, Rahul and Tuyls, Karl},
title = {Negative Update Intervals in Deep Multi-Agent Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In Multi-Agent Reinforcement Learning (MA-RL), independent cooperative learners must overcome a number of pathologies to learn optimal joint policies. Addressing one pathology often leaves approaches vulnerable towards others. For instance, hysteretic Q-learning citematignon2007hysteretic addresses miscoordination while leaving agents vulnerable towards misleading stochastic rewards. Other methods, such as leniency, have proven more robust when dealing with multiple pathologies simultaneously citeJMLR:v17:15-417. However, leniency has predominately been studied within the context of strategic form games (bimatrix games) and fully observable Markov games consisting of a small number of probabilistic state transitions. This raises the question of whether these findings scale to more complex domains. For this purpose we implement a temporally extend version of the Climb Game citeclaus1998dynamics, within which agents must overcome multiple pathologies simultaneously, including relative overgeneralisation, stochasticity, the alter-exploration and moving target problems, while learning from a large observation space. We find that existing lenient and hysteretic approaches fail to consistently learn near optimal joint-policies in this environment. To address these pathologies we introduce Negative Update Intervals-DDQN (NUI-DDQN), a Deep MA-RL algorithm which discards episodes yielding cumulative rewards outside the range of expanding intervals. NUI-DDQN consistently gravitates towards optimal joint-policies in our environment, overcoming the outlined pathologies.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {43–51},
numpages = {9},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.5555/3546258.3546468,
author = {Rakhsha, Amin and Radanovic, Goran and Devidze, Rati and Zhu, Xiaojin and Singla, Adish},
title = {Policy Teaching in Reinforcement Learning via Environment Poisoning Attacks},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes reward in infinite-horizon problem settings. The attacker can manipulate the rewards and the transition dynamics in the learning environment at training-time, and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an optimal stealthy attack for different measures of attack cost. We provide lower/upper bounds on the attack cost, and instantiate our attacks in two settings: (i) an offline setting where the agent is doing planning in the poisoned environment, and (ii) an online setting where the agent is learning a policy with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {210},
numpages = {45},
keywords = {policy teaching, security threat, reinforcement learning, training-time adversarial attacks, environment poisoning}
}

@article{10.1145/3527448,
author = {Vouros, George A.},
title = {Explainable Deep Reinforcement Learning: State of the Art and Challenges},
year = {2022},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3527448},
doi = {10.1145/3527448},
abstract = {Interpretability, explainability, and transparency are key issues to introducing artificial intelligence methods in many critical domains. This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability, and fairness, and has important consequences toward keeping the human in the loop in high levels of automation, especially in critical cases for decision making, where both (human and the machine) play important roles. Although the research community has given much attention to explainability of closed (or black) prediction boxes, there are tremendous needs for explainability of closed-box methods that support agents to act autonomously in the real world. Reinforcement learning methods, and especially their deep versions, are such closed-box methods. In this article, we aim to provide a review of state-of-the-art methods for explainable deep reinforcement learning methods, taking also into account the needs of human operators—that is, of those who make the actual and critical decisions in solving real-world problems. We provide a formal specification of the deep reinforcement learning explainability problems, and we identify the necessary components of a general explainable reinforcement learning framework. Based on these, we provide a comprehensive review of state-of-the-art methods, categorizing them into classes according to the paradigm they follow, the interpretable models they use, and the surface representation of explanations provided. The article concludes by identifying open questions and important challenges.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {92},
numpages = {39},
keywords = {transparency, Deep learning, deep reinforcement learning, interpretability, explainability}
}

@inproceedings{10.1145/3490354.3494376,
author = {Li, Lin},
title = {An Automated Portfolio Trading System with Feature Preprocessing and Recurrent Reinforcement Learning},
year = {2022},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494376},
doi = {10.1145/3490354.3494376},
abstract = {We propose a novel portfolio trading system, which contains a feature preprocessing module and a trading module. The feature preprocessing module consists of various data processing operations, while in the trading part, we integrate the portfolio weight rebalance function with the trading algorithm and make the trading system fully automated and suitable for individual investors, holding a handful of stocks. The data preprocessing procedures are applied to remove the white noise in the raw data set and uncover the general pattern underlying the data set before the processed feature set is inputted into the trading algorithm. Our empirical results reveal that the proposed portfolio trading system can efficiently earn high profit and maintain a relatively low drawdown, which clearly outperforms other portfolio trading strategies.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {11},
numpages = {8},
keywords = {automated trading systems, portfolio trading, feature preprocessing, recurrent reinforcement learning},
location = {Virtual Event},
series = {ICAIF '21}
}

@inproceedings{10.1145/3404835.3462818,
author = {Zhang, Weinan and Zhao, Xiangyu and Zhao, Li and Yin, Dawei and Yang, Grace Hui},
title = {DRL4IR: 2nd Workshop on Deep Reinforcement Learning for Information Retrieval},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462818},
doi = {10.1145/3404835.3462818},
abstract = {Modern information retrieval (IR) consists of a series of processes, including query expansion, candidate item recall, item ranking, item re-ranking, etc. The final ranked item list will be exposed to the user, which will accordingly provide feedback through some expected actions such as browsing and click. Such a whole process can be formulated as a decision-making process where the agent is the IR system while the environment is the specific user. This decision-making process can be one-step or sequential, depending on the scenarios or the ways of problem formulation. Since 2013, Deep reinforcement learning (DRL) has been a fast-developing technique for decision-making tasks. The high capacity of deep learning models is incorporated in the reinforcement learning framework so that the agent may successfully handle complex decision-making. In recent years, there have been a bunch of publications attempting to leverage DRL techniques for different IR tasks such as ad hoc retrieval, learning to rank and interactive recommendation. Nonetheless, the fundamental theory, the principle of RL methods or the recognized experimental protocols of decision-making in IR, has not been well developed, making it challenging to evaluate the correctness of a proposed method or judge whether the reported experimental performance is valid. We propose the second DRL4IR workshop at SIGIR 2021, which provides a venue to gather the academia researchers and industry practitioners to present the recent progress of DRL techniques for IR. More importantly, people in this workshop are expected to discuss more about the fundamental principles of formulating a decision-making IR task, the underlying theory as well as the practical effectiveness of the experiment protocol design, which would foster further research on novel methodologies, innovative experimental findings and new applications of DRL for information retrieval. DRL4IR organized at SIGIR'20 was one of the most popular workshops and attracted over 200 conference attendees. In this year, we will pay more attention to fundamental research topics and recent applications, and expect about 300 participants.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2681–2684},
numpages = {4},
keywords = {deep reinforcement learning, information retrieval},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.5555/3545946.3598817,
author = {Yu, Yang and Yin, Qiyue and Zhang, Junge and Huang, Kaiqi},
title = {Prioritized Tasks Mining for Multi-Task Cooperative Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-task learning improves data efficiency in cooperative multi-agent reinforcement learning, since agents can learn multiple related tasks simultaneously and the cooperation knowledge in a task can be utilized by others. However, existing methods mainly learn multiple cooperation tasks uniformly, regardless of their complexity and significance. In this paper, we propose a new framework called Prioritized Tasks Mining (PTM) for multi-task cooperation problems, which helps agents to identify and mine higher priority cooperation tasks, so as to learn more effective coordinated strategies for multiple cooperation tasks. Specially, agents will use the hindsight during training to identify the priority of different tasks, and make an exploration and exploitation in higher priority cooperative tasks to mine more sophisticated coordinated strategies. We evaluate PTM in challenging multi-task StarCraft micromanagement games with different scales, and results demonstrate that our method consistently outperforms all strong baselines.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1615–1623},
numpages = {9},
keywords = {multi-agent cooperation, reinforcement learning, multi-task learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3472538.3472541,
author = {Babin, Mathias and Katchabaw, Michael},
title = {Leveraging Reinforcement Learning and WaveFunctionCollapse for Improved Procedural Level Generation},
year = {2021},
isbn = {9781450384223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472538.3472541},
doi = {10.1145/3472538.3472541},
abstract = {This work presents a novel approach to training Reinforcement Learning (RL) agents to serve as heuristics for the WaveFunctionCollapse (WFC) algorithm in the production of procedurally generated video game levels. While the algorithm’s original minimal entropy heuristic is sufficient for constructing levels that look visually appealing, it is often the case that these levels suffer when playability is concerned. The approach presented in this work involves replacing this heuristic with a set of deep neural networks trained using RL to direct the algorithm in the construction of playable levels for the original Super Mario Bros. (SMB). We evaluate the performance of our models using a game-playing A*-based agent provided by the Mario AI Competition Framework and designate a simple reward function which reflects the quality of generated levels based on the A* agent’s ability to successfully navigate them. Results using this approach show an increase in the percentage of playable levels generated using our learned heuristics over those using minimal entropy.},
booktitle = {Proceedings of the 16th International Conference on the Foundations of Digital Games},
articleno = {4},
numpages = {8},
keywords = {reinforcement learning, wavefunctioncollapse, procedural content generation},
location = {Montreal, QC, Canada},
series = {FDG '21}
}

@inproceedings{10.1145/3555041.3589727,
author = {Lipman, Tavor and Milo, Tova and Somech, Amit},
title = {ATENA-PRO: Generating Personalized Exploration Notebooks with Constrained Reinforcement Learning},
year = {2023},
isbn = {9781450395076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555041.3589727},
doi = {10.1145/3555041.3589727},
abstract = {One of the most common, helpful practices of data scientists, when starting the exploration of a given dataset, is to examine existing data exploration notebooks prepared by other data analysts or scientists. These notebooks contain curated sessions of contextually-related query operations that together demonstrate interesting hypotheses and conjectures on the data. Unfortunately,relevant such notebooks, that had been prepared on the same dataset, and in light of thesame analysis task, are often nonexistent or unavailable.In this work, we describe ATENA-PRO, a framework for auto-generating such relevant, personalized exploratory sessions. Using a novel specification language, users first describe their desired output notebook. Our language contains dedicated constructs for contextually connecting future output queries. These specifications are then used as input for a Deep Reinforcement Learning (DRL) engine, which auto-generates the personalized notebook. Our DRL engine relies on an existing, general-purpose, DRL framework for data exploration. However, augmenting the generic framework with user specifications requires overcoming a difficult sparsity challenge, as only a small portion of the possible sessions may be compliant with the specifications. Inspired by solutions for constrained reinforcement learning, we devise a compound, flexible reward scheme as well as specification-aware neural network architecture. Our experimental evaluation shows that the combination of these components allows ATENA-PRO to consistently generate interesting, personalized exploration sessions for various analysis tasks and datasets.},
booktitle = {Companion of the 2023 International Conference on Management of Data},
pages = {167–170},
numpages = {4},
keywords = {AI for data analytics, automated data exploration},
location = {Seattle, WA, USA},
series = {SIGMOD '23}
}

@inproceedings{10.5555/3545946.3598810,
author = {Chen, Shuang and Xu, Qisen and Zhang, Liang and Jin, Yongbo and Li, Wenhao and Mo, Linjian},
title = {Model-Based Reinforcement Learning for Auto-Bidding in Display Advertising},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Real-time bidding (RTB) achieves outstanding success in online display advertising, which has become one of the most influential businesses. Given historical ad impressions under the second price auction mechanism, the advertiser's optimal bidding strategy is determined by the core parameter corresponding to the optimal solution of a constrained optimization problem. However, the sequentially arrived impressions in online display advertising make it highly non-trivial to obtain the optimal core parameter in advance without knowing the complete impression set. For this reason, recent methods have generally transformed the core parameter determination problem into a sequential parameter adjustment problem and solved it using reinforcement learning (RL). This paper proposes a simple and effective Model-Based Automatic Bidding algorithm, MBAB, which explicitly models the uncertainty of the dynamic auction environment and then uses the dynamic programming algorithm to obtain the current optimal adjustment of the core parameter. MBAB can avoid burdensome simulated environment construction and is more suitable for production deployment without the thorny sim-to-real issue than model-free methods. Furthermore, MBAB uses the optimal bidding formula to carry out coarse-grained modeling of the online market environment to alleviate the scalability problem caused by fine-grained environment modeling of previous model-based methods. In order to accurately describe the impression distribution and non-stationarity of the online market environment, we introduce the probabilistic modeling method and propose a novel monotonicity constraint to regulate the model output. Numerical experiments show that the proposed MBAB substantially outperforms existing baselines on various constrained RTB tasks in the production environment.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1560–1568},
numpages = {9},
keywords = {model-based reinforcement learning, constrained bidding, real-time bidding, display advertising},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3408352.3408448,
author = {Iranfar, Arman and Terraneo, Federico and Csordas, Gabor and Zapater, Marina and Fornaciari, William and Atienza, David},
title = {Dynamic Thermal Management with Proactive Fan Speed Control through Reinforcement Learning},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Dynamic Thermal Management (DTM) has become a major challenge since it directly affects Multiprocessors Systems-on-chip (MPSoCs) performance, power consumption, and reliability. In this work, we propose a transient fan model, enabling adaptive fan speed control simulation for efficient DTM. Our model is validated through a thermal test chip achieving less than 2°C error in the worst case. With multiple fan speeds, however, the DTM design space grows significantly, which can ultimately make conventional solutions impractical. We address this challenge through a reinforcement learning-based solution to proactively determine the number of active cores, operating frequency, and fan speed. The proposed solution is able to reduce fan power by up to 40\% compared to a DTM with constant fan speed with less than 1\% performance degradation. Also, compared to a state-of-the-art DTM technique our solution improves the performance by up to 19\% for the same fan power.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {418–423},
numpages = {6},
location = {Grenoble, France},
series = {DATE '20}
}

@inproceedings{10.1145/3611450.3611457,
author = {Xu, Nuo and Shao, Yanpeng and Fan, Xiaoyan},
title = {Research on Control Strategy of Manipulator Based on Deep Reinforcement Learning},
year = {2023},
isbn = {9798400707605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611450.3611457},
doi = {10.1145/3611450.3611457},
abstract = {Abstract: Today, the traditional control algorithm of the manipulator is difficult to meet in the complex and changing working environment, and the development of artificial intelligence has achieved remarkable results in the field of manipulator control. As a branch of artificial intelligence, deep reinforcement learning combines the perception ability of deep learning with the decision-making ability of reinforcement learning, which can greatly improve the work efficiency and adaptability of the manipulator. This paper expounds on the principle of deep reinforcement learning, compares and analyzes the application examples of the single manipulator and multi-manipulator control algorithms based on deep reinforcement learning, and demonstrates the achievements of deep reinforcement learning in the motion control of manipulators. The intelligent control research provides a reference.},
booktitle = {Proceedings of the 2023 3rd International Conference on Artificial Intelligence, Automation and Algorithms},
pages = {44–49},
numpages = {6},
keywords = {intelligent control, a single arm, deep reinforcement learning, Multi-arm},
location = {Beijing, China},
series = {AI2A '23}
}

@inproceedings{10.1145/3583780.3615303,
author = {Xin, Xin and Zhao, Xiangyu and Huang, Jin and Zhang, Weinan and Zhao, Li and Yin, Dawei and Yang, Grace Hui},
title = {DRL4IR: 4th Workshop on Deep Reinforcement Learning for Information Retrieval},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615303},
doi = {10.1145/3583780.3615303},
abstract = {AcIR is one of the most important fields to help users find relevant information. The interaction between IR systems and users can be naturally formulated as a decision-making problem. In the last decade, deep reinforcement learning (DRL) has become a promising direction to utilize the high model capacity of deep learning to improve long-term gains. On the one hand, there have been emerging research works focusing on leveraging DRL for IR tasks while the fundamental information theory under DRL settings, the principle of RL methods for IR tasks, or the experimental evaluation protocols of DRL-based IR systems, has not been deeply investigated. On the other hand, the emerging ChatGPT also provides new insights and challenges for DRL-based IR.Therefore, we propose the fourth DRL4IR workshop1 at CIKM 2023, which provides a venue for both academia researchers and industry practitioners to present the recent advances of DRL-based IR system, to foster novel research, interesting findings, and new applications of DRL for IR. We will pay more attention to fundamental research topics and recent application advances such as ChatGPT, with an expectation of over 300 workshop participants.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5304–5307},
numpages = {4},
keywords = {information retrieval, reinforcement learning},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.5555/3535850.3536006,
author = {Xu, Zhiwei and Bai, Yunpeng and Li, Dapeng and Zhang, Bin and Fan, Guoliang},
title = {SIDE: State Inference for Partially Observable Cooperative Multi-Agent Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {As one of the solutions to the decentralized partially observable Markov decision process (Dec-POMDP) problems, the value decomposition method has achieved significant results recently. However, most value decomposition methods require the fully observable state of the environment during training, but this is not feasible in some scenarios where only incomplete and noisy observations can be obtained. Therefore, we propose a novel value decomposition framework, named State Inference for value DEcomposition (SIDE), which eliminates the need to know the global state by simultaneously seeking solutions to the two problems of optimal control and state inference. SIDE can be extended to any value decomposition method to tackle partially observable problems. By comparing with the performance of different algorithms in StarCraft II micromanagement tasks, we verified that though without accessible states, SIDE can infer the current state that contributes to the reinforcement learning process based on past local observations and even achieve superior results to many baselines in some complex scenarios.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1400–1408},
numpages = {9},
keywords = {graph neural networks, reinforcement learning, multi-agent learning, variational inference},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3397536.3422246,
author = {Pan, Menghai and Huang, Weixiao and Li, Yanhua and Zhou, Xun and Liu, Zhenming and Bao, Jie and Zheng, Yu and Luo, Jun},
title = {Is Reinforcement Learning the Choice of Human Learners? A Case Study of Taxi Drivers},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3422246},
doi = {10.1145/3397536.3422246},
abstract = {Learning to make optimal decisions is a common yet complicated task. While computer agents can learn to make decisions by running reinforcement learning (RL), it remains unclear how human beings learn. In this paper, we perform the first data-driven case study on taxi drivers to validate whether humans mimic RL to learn. We categorize drivers into three groups based on their performance trends and analyze the correlations between human drivers and agents trained using RL. We discover that drivers that become more efficient at earning over time exhibit similar learning patterns to those of agents, whereas drivers that become less efficient tend to do the opposite. Our study (1) provides evidence that some human drivers do adapt RL when learning, (2) enhances the deep understanding of taxi drivers' learning strategies, (3) offers a guideline for taxi drivers to improve their earnings, and (4) develops a generic analytical framework to study and validate human learning strategies.},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {357–366},
numpages = {10},
keywords = {urban computing, human learning strategy, reinforcement learning},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@inproceedings{10.1145/3430984.3430994,
author = {Shelke, Omkar and Baniwal, Vinita and Khadilkar, Harshad},
title = {Anticipatory Decisions in Retail E-Commerce Warehouses Using Reinforcement Learning},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3430994},
doi = {10.1145/3430984.3430994},
abstract = {This paper describes the use of Reinforcement Learning (RL) in retail warehouses serving e-commerce demand, for (i) reducing fulfilment time (from order receipt to shipping hand-off), and (ii) improving labour utilisation. A complex sequence of operations is performed in order to pick the products from shelves and deliver them to the customer. This process requires a large amount of labour (manual or robotic), and its poor utilisation leads to high operating costs. In this paper, we focus on effectual labour utilisation for performing these tasks and for serving more orders with existing labour. We define our problem in RL framework and use the Deep-Deterministic Policy Gradient (DDPG) algorithm for computing anticipatory picking decisions simultaneously for a large number of products. These picking (retrieval from storage) decisions are generated before orders are actually received, and can be executed overnight in bulk batches, significantly improving labour efficiency. Experiments show that our approach allows the system to process more orders in comparison to baseline heuristic and imitation learning algorithms.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science \&amp; Management of Data (8th ACM IKDD CODS \&amp; 26th COMAD)},
pages = {272–280},
numpages = {9},
keywords = {E-Commerce, Supply&nbsp;Chain, Scalability, Reinforcement&nbsp;Learning, Multi&nbsp;Product},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@inproceedings{10.5555/2755753.2757163,
author = {Chen, Zhuo and Marculescu, Diana},
title = {Distributed Reinforcement Learning for Power Limited Many-Core System Performance Optimization},
year = {2015},
isbn = {9783981537048},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {As power density emerges as the main constraint for many-core systems, controlling power consumption under the Thermal Design Power (TDP) while maximizing the performance becomes increasingly critical. To dynamically save power, Dynamic Voltage Frequency Scaling (DVFS) techniques have proved to be effective and are widely available commercially. In this paper, we present an On-line Distributed Reinforcement Learning (OD-RL) based DVFS control algorithm for many-core system performance improvement under power constraints. At the finer grain, a per-core Reinforcement Learning (RL) method is used to learn the optimal control policy of the Voltage/Frequency (VF) levels in a system model-free manner. At the coarser grain, an efficient global power budget reallocation algorithm is used to maximize the overall performance. The experiments show that compared to the state-of-the-art algorithms: 1) OD-RL produces up to 98\% less budget overshoot, 2) up to 44.3x better throughput per over-the-budget energy and up to 23\% higher energy efficiency, and 3) two orders of magnitude speedup over state-of-the-art techniques for systems with hundreds of cores.},
booktitle = {Proceedings of the 2015 Design, Automation \&amp; Test in Europe Conference \&amp; Exhibition},
pages = {1521–1526},
numpages = {6},
location = {Grenoble, France},
series = {DATE '15}
}

@inproceedings{10.1109/ASE51524.2021.9678566,
author = {T\"{u}rker, Uraz Cengiz and Hierons, Robert M. and Mousavi, Mohammad Reza and Tyukin, Ivan Y.},
title = {Efficient State Synchronisation in Model-Based Testing through Reinforcement Learning},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678566},
doi = {10.1109/ASE51524.2021.9678566},
abstract = {Model-based testing is a structured method to test complex systems. Scaling up model-based testing to large systems requires improving the efficiency of various steps involved in test-case generation and more importantly, in test-execution. One of the most costly steps of model-based testing is to bring the system to a known state, best achieved through synchronising sequences. A synchronising sequence is an input sequence that brings a given system to a predetermined state regardless of system's initial state. Depending on the structure, the system might be complete, i.e., all inputs are applicable at every state of the system. However, some systems are partial and in this case not all inputs are usable at every state. Derivation of synchronising sequences from complete or partial systems is a challenging task. In this paper, we introduce a novel Q-learning algorithm that can derive synchronising sequences from systems with complete or partial structures. The proposed algorithm is faster and can process larger systems than the fastest sequential algorithm that derives synchronising sequences from complete systems. Moreover, the proposed method is also faster and can process larger systems than the most recent massively parallel algorithm that derives synchronising sequences from partial systems. Furthermore, the proposed algorithm generates shorter synchronising sequences.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {368–380},
numpages = {13},
keywords = {reinforcement learning, synchronising sequence, model based testing, Q-learning},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.5555/2936924.2937002,
author = {Ceren, Roi and Doshi, Prashant and Banerjee, Bikramjit},
title = {Reinforcement Learning in Partially Observable Multiagent Settings: Monte Carlo Exploring Policies with PAC Bounds},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Perkins' Monte Carlo exploring starts for partially observable Markov decision processes (MCES-P) integrates Monte Carlo exploring starts into a local search of policy space to offer a template for reinforcement learning that operates under partial observability of the state. In this paper, we generalize the reinforcement learning under partial observability to the self-interested multiagent setting. We present a new template, MCES-IP, which extends MCES-P by maintaining predictions of the other agent's actions based on dynamic beliefs over models. MCES-IP is instantiated to be approximately locally optimal with some probability by deriving a theoretical bound on the sample size that in part depends on the allowed error from the sampling; we refer to this algorithm as MCESIP+PAC. Our experiments demonstrate that MCESIP+PAC learns policies whose values are comparable or better than those from MCESP+PAC in multiagent domains while utilizing much less samples for each transformation.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {530–538},
numpages = {9},
keywords = {partial observability, reinforcement learning, multiple agents, probably approximately correct},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.1145/3495018.3495448,
author = {Lu, Shouqing},
title = {Automatic Tracking Method of Unmanned Vehicle Trajectory Based on Reinforcement Learning},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495448},
doi = {10.1145/3495018.3495448},
abstract = {Up to now, unmanned driving is still a challenging research field in academia at home and abroad. The vehicle trajectory tracking technology is a very critical and urgent link, because it provides important information for intelligent traffic monitoring. The reinforcement learning method is an important method for learning in an unknown environment. In the field of artificial intelligence machine learning, reinforcement learning research has made great progress in theory, algorithm and application, and has become a current hotspot in research. Unmanned vehicle trajectory tracking is one of the key technologies in the field of unmanned driving research. It uses built-in sensors to perceive the environment, uses trajectory planning algorithms to generate the required path in real time, and the decision system selects the best path. Finally, the built-in path tracking controller implement it. This article mainly adopts the experimental analysis method to discuss how to break through the problem of automatic trajectory tracking technology in the support of enhanced learning by unmanned vehicles, and compare and analyze the expected yaw rate and actual yaw rate and frequency of the target vehicle. According to the experimental research results, the expected yaw rate and actual yaw rate of the unmanned vehicle trajectory automatic tracking test are relatively close, and the test system in this test has a certain tracking effect.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1596–1599},
numpages = {4},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

