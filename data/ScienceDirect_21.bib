@article{TREJO201835,
title = {Adapting attackers and defenders patrolling strategies: A reinforcement learning approach for Stackelberg security games},
journal = {Journal of Computer and System Sciences},
volume = {95},
pages = {35-54},
year = {2018},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2017.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0022000018300035},
author = {Kristal K. Trejo and Julio B. Clempner and Alexander S. Poznyak},
keywords = {Security games, Reinforcement learning, Stackelberg games, Behavioral games, Multiple players, Strong Stackelberg/Nash equilibrium},
abstract = {This paper presents a novel approach for adapting attackers and defenders preferred patrolling strategies using reinforcement learning (RL) based-on average rewards in Stackelberg security games. We propose a framework that combines three different paradigms: prior knowledge, imitation and temporal-difference method. The overall RL architecture involves two highest components: the Adaptive Primary Learning architecture and the Actor–critic architecture. In this work we consider that defenders and attackers conforms coalitions in the Stackelberg security game, these are reached by computing the Strong Lp-Stackelberg/Nash equilibrium. We present a numerical example that validates the proposed RL approach measuring the benefits for security resource allocation.}
}
@article{JIANG2023104817,
title = {Adaptive control of resource flow to optimize construction work and cash flow via online deep reinforcement learning},
journal = {Automation in Construction},
volume = {150},
pages = {104817},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104817},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523000778},
author = {Can Jiang and Xin Li and Jia-Rui Lin and Ming Liu and Zhiliang Ma},
keywords = {Construction project, Flow management, Resource planning, Deep reinforcement learning, Markov decision process, Adaptive control, Proximal policy optimization},
abstract = {Due to complexity and dynamics of construction work, resource, and cash flows, poor management of them usually leads to time and cost overruns, bankruptcy, even project failure. Existing approaches in construction failed to achieve optimal control of resource flow in a dynamic environment with uncertainty. Therefore, this paper proposes a model and method to adaptive control the resource flows to optimize the work and cash flows of construction projects. First, a mathematical model based on a partially observable Markov decision process is established to formulate the complex interactions of construction work, resource, and cash flows as well as uncertainty and variability of diverse influence factors. Meanwhile, to efficiently find the optimal solutions, a Deep Reinforcement Learning (DRL) based method is introduced to realize the continuous adaptive optimal control of labor and material flows, thereby optimizing the work and cash flows. To assist the training process of DRL, a simulator based on discrete event simulation is also developed to mimic the dynamic features and external environments of a project. Experiments in simulated scenarios illustrate that our method outperforms the vanilla empirical method and genetic algorithm, possesses remarkable capability in diverse projects and external environments, and a hybrid agent of DRL and empirical method leads to the best result. This paper contributes to adaptive control and optimization of coupled work, resource, and cash flows, and may serve as a step stone for adopting DRL technology in construction project management.}
}
@article{KRISHNALAKSHMANAN2020103078,
title = {Complete coverage path planning using reinforcement learning for Tetromino based cleaning and maintenance robot},
journal = {Automation in Construction},
volume = {112},
pages = {103078},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103078},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519305813},
author = {Anirudh {Krishna Lakshmanan} and Rajesh {Elara Mohan} and Balakrishnan Ramalingam and Anh {Vu Le} and Prabahar Veerajagadeshwar and Kamlesh Tiwari and Muhammad Ilyas},
keywords = {Tiling robotics, Cleaning and maintenance, Inspection, Path planing, Reinforcement learning},
abstract = {Tiling robotics have been deployed in autonomous complete area coverage tasks such as floor cleaning, building inspection, and maintenance, surface painting. One class of tiling robotics, polyomino-based reconfigurable robots, overcome the limitation of fixed-form robots in achieving high-efficiency area coverage by adopting different morphologies to suit the needs of the current environment. Since the reconfigurable actions of these robots are produced by real-time intelligent decisions during operations, an optimal path planning algorithm is paramount to maximize the area coverage while minimizing the energy consumed by these robots. This paper proposes a complete coverage path planning (CCPP) model trained using deep blackreinforcement learning (RL) for the tetromino based reconfigurable robot platform called hTetro to simultaneously generate the optimal set of shapes for any pretrained arbitrary environment shape with a trajectory that has the least overall cost. To this end, a Convolutional Neural Network (CNN) with Long Short Term Memory (LSTM) layers is trained using Actor Critic Experience Replay (ACER) reinforcement learning algorithm. The results are compared with existing approaches which are based on the traditional tiling theory model, including zigzag, spiral, and greedy search schemes. The model is also compared with the Travelling salesman problem (TSP) based Genetic Algorithm (GA) and Ant Colony Optimization (ACO) schemes. The proposed scheme generates a path with lower cost while also requiring lesser time to generate it. The model is also highly robust and can generate a path in any pretrained arbitrary environments.}
}
@article{DEPAULA2015310,
title = {Controlling blood glucose variability under uncertainty using reinforcement learning and Gaussian processes},
journal = {Applied Soft Computing},
volume = {35},
pages = {310-332},
year = {2015},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2015.06.041},
url = {https://www.sciencedirect.com/science/article/pii/S1568494615003932},
author = {Mariano {De Paula} and Luis Omar Ávila and Ernesto C. Martínez},
keywords = {Artificial pancreas, Diabetes, Gaussian processes, Policy iteration, Reinforcement learning, Stochastic optimal control},
abstract = {Automated control of blood glucose (BG) concentration with a fully automated artificial pancreas will certainly improve the quality of life for insulin-dependent patients. Closed-loop insulin delivery is challenging due to inter- and intra-patient variability, errors in glucose sensors and delays in insulin absorption. Responding to the varying activity levels seen in outpatients, with unpredictable and unreported food intake, and providing the necessary personalized control for individuals is a challenging task for existing control algorithms. A novel approach for controlling glycemic variability using simulation-based learning is presented. A policy iteration algorithm that combines reinforcement learning with Gaussian process approximation is proposed. To account for multiple sources of uncertainty, a control policy is learned off-line using an Ito's stochastic model of the glucose-insulin dynamics. For safety and performance, only relevant data are sampled through Bayesian active learning. Results obtained demonstrate that a generic policy is both safe and efficient for controlling subject-specific variability due to a patient's lifestyle and its distinctive metabolic response.}
}
@article{ZOLFPOURAROKHLO2014163,
title = {Modeling of route planning system based on Q value-based dynamic programming with multi-agent reinforcement learning algorithms},
journal = {Engineering Applications of Artificial Intelligence},
volume = {29},
pages = {163-177},
year = {2014},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2014.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0952197614000086},
author = {Mortaza Zolfpour-Arokhlo and Ali Selamat and Siti Zaiton {Mohd Hashim} and Hossein Afkhami},
keywords = {Route planning system (RPS), Multi-agent system (MAS), Multi-agent reinforcement learning (MARL), Q-learning, Traffic congestion},
abstract = {In this paper, a new model for a route planning system based on multi-agent reinforcement learning (MARL) algorithms is proposed. The combined Q-value based dynamic programming (QVDP) with Boltzmann distribution was used to solve vehicle delay's problems by studying the weights of various components in road network environments such as weather, traffic, road safety, and fuel capacity to create a priority route plan for vehicles. The important part of the study was to use a multi-agent system (MAS) with learning abilities which in order to make decisions about routing vehicles between Malaysia's cities. The evaluation was done using a number of case studies that focused on road networks in Malaysia. The results of these experiments indicated that the travel durations for the case studies predicted by existing approaches were between 0.00 and 12.33% off from the actual travel times by the proposed method. From the experiments, the results illustrate that the proposed approach is a unique contribution to the field of computational intelligence in the route planning system.}
}
@article{LI2022131142,
title = {Exploring the formation conditions and dynamic trends of rural residents’ clean heating behaviour in northern China based on reinforcement learning},
journal = {Journal of Cleaner Production},
volume = {344},
pages = {131142},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.131142},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622007740},
author = {Na Li and Xi Luo and Menglin Xing and Jianghua Liu and Yaru Gao and Tingting Zhou and Min Zhang and Jin Li and Yanfeng Liu},
keywords = {Clean heating behaviour, Reinforcement learning, Intervention information, Clean heating rate, Perceived value},
abstract = {Although Chinese government has enacted a series of policies to substitute bulk coal combustion with clean heating for space heating in northern rural areas, the clean heating rate in China is still lower than expected. In order to effectively boost the clean heating behaviour of rural residents, it is of great importance to fully understand their willingness to clean heating, which is largely dependent on the perceived value. Because the perceived value can be enhanced through continuous intervention by the environment information, this study used the reinforcement learning model to portray the interactive relationship between rural residents and the environment, aiming to quantitatively analyse the dynamic trends of rural residents' clean heating behaviour. A social cognition model was combined with the reinforcement learning model to further clarify the formation conditions of rural residents’ clean heating behaviour. The value of intervention intensity and efficiency was divided into three levels, i.e., strong, weak, and adverse. The result indicates that the clean heating behaviour has a higher correlation with the intervention intensity than with the intervention efficiency. The intervention intensity and efficiency were indicated by the parameter of personal intensity λi, contextual intensity λe, and effective rate γ, delay degree τ. Additionally, under the condition that the parameter values of λi, λe, γ, τ are 1, 1, 1, 0, respectively (the hypothetical strongest intervention intensity and efficiency), it will take approximately 8 years to achieve a 90% clean heating rate in northern rural China. Under the condition that the parameter values of λi, λe, γ, τ are 0.67, 0.34, 0.75, 0.28, respectively (the actual intervention intensity and efficiency), it will take approximately 43 years to achieve a 90% clean heating rate in the pilot villages.}
}
@incollection{FAGG1994281,
title = {Chapter 14 Reinforcement Learning for Robotic Reaching and Grasping},
editor = {Keree M.B. Bennett and Umberto Castiello},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {105},
pages = {281-308},
year = {1994},
booktitle = {Advances in Psychology},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(08)61283-2},
url = {https://www.sciencedirect.com/science/article/pii/S0166411508612832},
author = {Ah. Fagg},
abstract = {Summary
A reinforcement learning approach is used to train a neural controller to perform a robotic reaching task. Unlike supervised learning techniques, where the teacher must provide the correct sequence of motor actions, only an evaluation of the robot's performance is provided. From this limited information, the robot must discover the appropriate motor programs that best satisfy the teacher's evaluation criterion. This type of learning approach is important because in a real-world environment, the teacher is generally not able to describe the motor program that performs the desired motor skill. This chapter utilizes the language of schema theory [1] as a mechanism for describing functional decompositions of motor programs. A connection is made from schema descriptions to a neural-level implementation of the schemas. It is at this low level of processing that we define a reinforcement learning algorithm that acquires motor programs that satisfy the reinforcement policy defined by the teacher.}
}
@article{LEE1990231,
title = {Corrective and reinforcement learning for speaker-independent continuous speech recognition},
journal = {Computer Speech & Language},
volume = {4},
number = {3},
pages = {231-245},
year = {1990},
issn = {0885-2308},
doi = {https://doi.org/10.1016/0885-2308(90)90006-R},
url = {https://www.sciencedirect.com/science/article/pii/088523089090006R},
author = {Kai-Fu Lee and Sanjoy Mahajan},
abstract = {This paper addresses the issue of learning hidden Markov model (HMM) parameters for speaker-independent continuous speech recognition. Bahl et al. (IEEE conference on acoustics, speech and signal processing, April 1988a) introduced the corrective training algorithm for speaker-dependent isolated-word recognition. Their algorithm attempted to improve the recognition accuracy on the training data. In this work, we extend this algorithm to speaker-independent continjous speech recognition. We use cross-validation to increase the effective training size. We also introduce a near-miss sentence hypothesization algorithm for continuous speech training. The combination of these two approaches resulted in over 20% error reductions both with and without grammar.}
}
@article{DRIDI201587,
title = {A model for the evolution of reinforcement learning in fluctuating games},
journal = {Animal Behaviour},
volume = {104},
pages = {87-114},
year = {2015},
issn = {0003-3472},
doi = {https://doi.org/10.1016/j.anbehav.2015.01.037},
url = {https://www.sciencedirect.com/science/article/pii/S0003347215000573},
author = {Slimane Dridi and Laurent Lehmann},
keywords = {evolution of cognition, evolutionarily stable learning rules, exploration–exploitation trade-off, repeated games, social interactions, trial-and-error learning},
abstract = {Many species are able to learn to associate behaviours with rewards as this gives fitness advantages in changing environments. Social interactions between population members may, however, require more cognitive abilities than simple trial-and-error learning, in particular the capacity to make accurate hypotheses about the material payoff consequences of alternative action combinations. It is unclear in this context whether natural selection necessarily favours individuals to use information about payoffs associated with nontried actions (hypothetical payoffs), as opposed to simple reinforcement of realized payoff. Here, we develop an evolutionary model in which individuals are genetically determined to use either trial-and-error learning or learning based on hypothetical reinforcements, and ask what is the evolutionarily stable learning rule under pairwise symmetric two-action stochastic repeated games played over the individual's lifetime. We analyse through stochastic approximation theory and simulations the learning dynamics on the behavioural timescale, and derive conditions where trial-and-error learning outcompetes hypothetical reinforcement learning on the evolutionary timescale. This occurs in particular under repeated cooperative interactions with the same partner. By contrast, we find that hypothetical reinforcement learners tend to be favoured under random interactions, but stable polymorphisms can also obtain where trial-and-error learners are maintained at a low frequency. We conclude that specific game structures can select for trial-and-error learning even in the absence of costs of cognition, which illustrates that cost-free increased cognition can be counterselected under social interactions.}
}
@article{WANG201696,
title = {A multi-agent reinforcement learning approach to dynamic service composition},
journal = {Information Sciences},
volume = {363},
pages = {96-119},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516303085},
author = {Hongbing Wang and Xiaojun Wang and Xingguo Hu and Xingzhi Zhang and Mingzhu Gu},
keywords = {Web service composition, Reinforcement learning, Multi-agent},
abstract = {As a promising implementation of software systems, service composition has attracted significant attention and research, which constructs complex and value-added applications by composing existing single services to reduce the deployment time and cost. However, as the services on the Internet and the external environment are frequently changeable, these demand the service composition must be adaptive and dynamic to address these changes automatically. Therefore, this paper describes a multi-agent reinforcement learning model for the dynamic optimization of web service composition. In this model, agent can utilize reinforcement learning algorithms to interact with environment in real time to compute optimal composition strategy dynamically, and multi-agents mechanism can keep higher effectiveness in contrast to single-agent reinforcement learning. We propose a distributed Q-learning algorithm, which decompose the task into many sub-tasks and make every agent focus on own sub-task, to accelerate the convergence rate. In addition, we also introduce experience sharing strategy to improve the efficiency. As a result, these methods allow composite service to dynamically adjust itself to fit a varying environment, where the properties of the component services continue changing. Finally, a series of comparable experiments with traditional Q-learning algorithm demonstrate that our algorithms have certain validity, higher efficiency and obvious advantages.}
}
@incollection{ZHANG2021275,
title = {Chapter nine - Automated optimal control in energy systems: the reinforcement learning approach},
editor = {Huaiguang Jiang and Yingchen Zhang and Eduard Muljadi},
booktitle = {New Technologies for Power System Operation and Analysis},
publisher = {Academic Press},
pages = {275-318},
year = {2021},
isbn = {978-0-12-820168-8},
doi = {https://doi.org/10.1016/B978-0-12-820168-8.00015-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128201688000158},
author = {Xiangyu Zhang and Huaiguang Jiang},
keywords = {Reinforcement learning, artificial intelligence, optimal control, smart grid},
abstract = {With the development of smart grid technologies an increasing number of new devices and participants have joined modern energy systems and are inevitably making them more complicated and interdependent than ever. Optimally controlling such a complex energy system and maintaining its operation in a high-efficient, secure, and resilient manner are challenging tasks to the system operators. Fortunately, the revolution in deep learning and artificial intelligence (AI), both from hardware and algorithms perspectives, has provided new ideas and solutions to many previously intractable problems. As a result, this advance in computer science also sparked great research interests in utilizing AI in solving engineering problems related to the modern energy systems. Among many AI techniques, deep reinforcement learning (DRL) has demonstrated great potential for solving sequential optimization problems, which are very common in the engineering domains. Its ability to handle nonlinearity and stochasticity in controlled systems has out-competed many traditional optimal control algorithms. Therefore in this chapter, we focus on the state-of-the-art of DRL concepts and related algorithms, compare their pros and cons with traditional optimal control approaches and discuss the typical workflow for leveraging RL in solving complex problems in modern energy systems.}
}
@incollection{DONAHOE1997336,
title = {Chapter 18 - Selection Networks: Simulation of Plasticity through Reinforcement Learning},
editor = {John W. Donahoe and Vivian {Packard Dorsel}},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {121},
pages = {336-357},
year = {1997},
booktitle = {Neural-Network Models of Cognition},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(97)80104-5},
url = {https://www.sciencedirect.com/science/article/pii/S0166411597801045},
author = {John W. Donahoe},
abstract = {ABSTRACT
Evolution through natural selection has addressed the problem of modifying synapses throughout large networks of neurons by exploiting diffusely projecting neuromodulatory systems. When pre- and postsynaptic neurons are coactive, synaptic efficacies increase or decrease dependent upon whether the neuromodulator dopamine is simultaneously present or absent. Salient characteristics of this process can be simulated with selection networks, artificial neural networks whose architecture instantiates a processing system whose connection weights are modified by a scalar reinforcing signal. This arrangement resolves both the temporal paradox and the binding paradox, twin challenges to any attempt to interpret complex behavior by means of neural networks. Further, by exploiting an emergent property of selection networks—acquired reinforcement—critical aspects of imagining, thinking, and language acquisition can also be interpreted.}
}
@article{LI2021128929,
title = {A novel data-driven controller for solid oxide fuel cell via deep reinforcement learning},
journal = {Journal of Cleaner Production},
volume = {321},
pages = {128929},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128929},
url = {https://www.sciencedirect.com/science/article/pii/S095965262103122X},
author = {Jiawen Li and Tao Yu},
keywords = {Large-scale deep reinforcement learning, Data-driven PID controller, Solid oxide fuel cell, Output voltage control, Fuel utilization},
abstract = {Solid oxide fuel cells (SOFC) are complex nonlinear and time-varying systems with operational constraints. How to effectively control and stabilize the output voltage while preventing constraint violations becomes a main challenge to its wide application. To improve the efficiency of its operation and power tracking control and prevent constraint violations, this paper designs a data-driven adaptive proportional integral derivative (PID) controller, which maintains the output voltage at the reference value via the optimal control of the hydrogen flow. Moreover, a novel large-scale deep reinforcement learning (DRL) algorithm, called the two-stage training strategy large-scale twin delayed deep determination policy gradient (TGSL-TD3PG), is adopted to adaptively adjust the baseline coefficients of the designed controller scaffolded by the high adaptability and model-free features of reinforcement learning. In the training of TGSL-TD3PG, multiple agents are simultaneously employed to acquire the best policy, whereby the training of optimal agents in practice is structured upon the principles of imitation learning and curriculum learning. This method solves the common problem of low robustness in conventional deep reinforcement learning and can be applied to the field of control. Moreover, the TGSL-TD3PG algorithm incorporates the baseline PID coefficients into the design objective and offers the controller with the online coefficient-adjusting ability through learning. Therefore, such limitations as no adaptability, low robustness and neglect of constraints associated with conventional PID control are significantly addressed. Simulation results show that the proposed controller can promote the SOFC to track the load power demand effectively while maintaining the fuel utilization ratio constant. The proposed algorithm reduces 45.2% of the output voltage setting time and 30% of the voltage overshoot to the maximum extent, while generating the constraint violation time of fuel utilization at 0.}
}
@article{HE2020119302,
title = {Energy optimization of electric vehicle’s acceleration process based on reinforcement learning},
journal = {Journal of Cleaner Production},
volume = {248},
pages = {119302},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.119302},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619341721},
author = {Hongwen He and Jianfei Cao and Xing Cui},
keywords = {Unmanned driving, Electric vehicles, Pedal control stratgey, Energy optimization, Q-learning, Deep Q-learning},
abstract = {Under the situation of unmanned driving, the energy consumption in an electric vehicle’s acceleration process can be reduced by controlling the driving behavior. So in this paper, a pedal control strategy which could optimize the energy consumption of electric vehicle’s acceleration process is proposed. The strategy is generated by the training results of reinforcement learning framework and the specific method of building such framework is discussed in details. Based on the training results of Q-learning-based algorithm, the relationship between the proportion of energy consumption reduction and vehicle’s acceleration time is analyzed, which illustrates the energy-saving potential of the algorithm. In order to improve the control effect of the strategy, an updated algorithm framework based on Deep Q-learning (DQN) is proposed and an improved pedal’s control strategy is obtained. Compared with the strategy obtained by Q-learning-based algorithm, the improved strategy not only achieves the same energy-saving effect, but also guarantees the stability of control effect, which is more suitable for actual use.}
}
@article{HU201990,
title = {On convergence rates of game theoretic reinforcement learning algorithms},
journal = {Automatica},
volume = {104},
pages = {90-101},
year = {2019},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2019.02.032},
url = {https://www.sciencedirect.com/science/article/pii/S000510981930086X},
author = {Zhisheng Hu and Minghui Zhu and Ping Chen and Peng Liu},
keywords = {Distributed control, Game theory, Learning algorithms},
abstract = {This paper investigates a class of multi-player discrete games where each player aims to maximize its own utility function. Each player does not know the other players’ action sets, their deployed actions or the structures of its own or the others’ utility functions. Instead, each player only knows its own deployed actions and its received utility values in recent history. We propose a reinforcement learning algorithm which converges to the set of action profiles which have maximal stochastic potential with probability one. Furthermore, an upper bound on the convergence rate is derived and is minimized when the exploration rates are restricted to p-series. The algorithm performance is verified using a case study in the smart grid.}
}