@article{KWAK2023592,
title = {Self-attention based deep direct recurrent reinforcement learning with hybrid loss for trading signal generation},
journal = {Information Sciences},
volume = {623},
pages = {592-606},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.12.042},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522015377},
author = {Dongkyu Kwak and Sungyoon Choi and Woojin Chang},
keywords = {Recurrent reinforcement learning, Self-attention mechanism, Algorithmic trading, Decision support},
abstract = {Algorithmic trading based on machine learning has the advantage of using intrinsic features and embedded causality in complex stock price time series. We propose a novel algorithmic trading model based on recurrent reinforcement learning, optimized for making consecutive trading signals. This paper elaborates on how temporal features from complex observation are optimally extracted to maximize the expected rewards of the reinforcement learning model. Our model incorporates the hybrid learning loss to allow sequences of hidden features for reinforcement learning to contain the original state’s characteristics fully. The self-attention mechanism is also introduced to our model for learning the temporal importance of the hidden representation series, which helps the reinforcement learning model to be aware of temporal dependence for its decision-making. In this paper, we verify the effectiveness of proposed model using some major market indices and the representative stocks in each sector of S&P500. The augmented structure that we propose has a significant dominance on trading performance. Our proposed model, self-attention based deep direct recurrent reinforcement learning with hybrid loss (SA-DDR-HL), shows superior performance over well-known baseline benchmark models, including machine learning and time series models.}
}
@article{CALINON2013369,
title = {Compliant skills acquisition and multi-optima policy search with EM-based reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {61},
number = {4},
pages = {369-379},
year = {2013},
note = {Models and Technologies for Multi-modal Skill Training},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2012.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0921889012001662},
author = {Sylvain Calinon and Petar Kormushev and Darwin G. Caldwell},
keywords = {Reinforcement learning, Learning by imitation, Skills transfer, Expectation-maximization, Dynamical systems, Gaussian mixture model},
abstract = {The democratization of robotics technology and the development of new actuators progressively bring robots closer to humans. The applications that can now be envisaged drastically contrast with the requirements of industrial robots. In standard manufacturing settings, the criterions used to assess performance are usually related to the robot’s accuracy, repeatability, speed or stiffness. Learning a control policy to actuate such robots is characterized by the search of a single solution for the task, with a representation of the policy consisting of moving the robot through a set of points to follow a trajectory. With new environments such as homes and offices populated with humans, the reproduction performance is portrayed differently. These robots are expected to acquire rich motor skills that can be generalized to new situations, while behaving safely in the vicinity of users. Skills acquisition can no longer be guided by a single form of learning, and must instead combine different approaches to continuously create, adapt and refine policies. The family of search strategies based on expectation-maximization (EM) looks particularly promising to cope with these new requirements. The exploration can be performed directly in the policy parameters space, by refining the policy together with exploration parameters represented in the form of covariances. With this formulation, RL can be extended to a multi-optima search problem in which several policy alternatives can be considered. We present here two applications exploiting EM-based exploration strategies, by considering parameterized policies based on dynamical systems, and by using Gaussian mixture models for the search of multiple policy alternatives.}
}
@article{ZHAO2021203,
title = {Imitation of Real Lane-Change Decisions Using Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {2},
pages = {203-209},
year = {2021},
note = {16th IFAC Symposium on Control in Transportation Systems CTS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.06.023},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321004596},
author = {Lu Zhao and Nadir Farhi and Zoi Christoforou and Nadia Haddadou},
keywords = {Traffic Models, Artificial intelligence in transportation, lane-change model, reinforcement learning, human driving behavior},
abstract = {Microscopic modeling of human driving consists generally in combining both car-following and lane-change models. While the human car-following process has been extensively developed and well modeled, the lane-change behavior is more complex to understand and still remains to be explored. Classical lane-change models are usually rule-based and handcrafted, that tend to exhibit limited performance. Machine Learning algorithms, particularly Reinforcement Learning (RL) ones, provide an alternative approach and have recently achieved high success in modeling difficult decision-making processes in many fields. We propose in this article a reinforcement learning based model for the human lane-change behavior, with an online calibration of real lane-change decisions, extracted from the NGSIM data-set. In addition, we use the traffic vehicular simulator SUMO ("Simulation of Urban Mobility") to create a numerical simulation environment. The utilization of numerical traffic simulation allows us enriching the data-set, for training the agent to find an optimal policy for lane change. Thus, about 13% additional traffic situations, not present in the real data, are created by the traffic simulation environment. The trained agent is collision-free and human-like who is satisfactory to real data and also to the additional simulated data. Moreover, our RL model can perform up to 95.37% of the real decisions observed in the data-set.}
}
@article{JIANG201868,
title = {Robust control scheme for a class of uncertain nonlinear systems with completely unknown dynamics using data-driven reinforcement learning method},
journal = {Neurocomputing},
volume = {273},
pages = {68-77},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.07.058},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217313814},
author = {He Jiang and Huaguang Zhang and Yang Cui and Geyang Xiao},
keywords = {Reinforcement learning, Adaptive dynamic programming, Data-driven, Model-free, Neural networks},
abstract = {This paper deals with the robust control issues for a class of uncertain nonlinear systems with completely unknown dynamics via a data-driven reinforcement learning method. Firstly, we formulate the optimal regulation control problem for the nominal system, and then, the robust controller for the original uncertain system is designed by adding a constant feedback gain to the optimal controller of the nominal system. Then, this scheme is extended to the optimal tracking control by means of augmented system and discount factor. It is also demonstrated that the proposed robust controller can achieve optimality with a new defined performance index function when there is no control perturbation. It is well known that the nonlinear optimal control problem relies on the solution of Hamilton–Jacobi–Bellman (HJB) equation, which is a nonlinear partial differential equation and impossible to be solved analytically. In order to overcome this difficulty, we introduce a model-based iterative learning algorithm to successively approximate the solution of HJB equation and provide its convergence proof. Subsequently, based on the structure of the model-based approach, a data-driven reinforcement learning method is derived, which only requires the sampling data from real system with different control inputs rather than the accurate mathematical system models. Neural networks (NNs) are utilized to implement this model-free method to approximate the optimal solutions and the least-square approach is employed to minimize the NN approximation residual errors. Finally, two numerical simulation examples are given to illustrate the effectiveness of our proposed method.}
}
@article{CAO2023108796,
title = {A Reliable Energy Trading Strategy in Intelligent Microgrids Using Deep Reinforcement Learning},
journal = {Computers and Electrical Engineering},
volume = {110},
pages = {108796},
year = {2023},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2023.108796},
url = {https://www.sciencedirect.com/science/article/pii/S0045790623002203},
author = {Man Cao and Zhiyong Yin and Yajun Wang and Le Yu and Peiran Shi and Zhi Cai},
keywords = {Microgrids, Energy trading, blockchain, Reinforcement learning, Markov decision process},
abstract = {A microgrid is a system that incorporates various decentralized power sources and manages the distribution of electricity. In microgrid, prosumers play a significant role in trading electricity, and it is essential to establish an effective power trading mechanism to incentivize their active participation. This study proposes a power trading mechanism for prosumers in microgrids, which incorporates blockchain technology to protect their rights and interests. The mechanism employs reinforcement learning to optimize trading decisions and develops a reputation mechanism to evaluate prosumers' trustworthiness based on their past transactions. The proposed strategy encourages prosumers to conduct more transactions with honest peers by introducing reputation value rewards into the benefit function. The simulation results indicate that the proposed strategy outperforms traditional approaches and reputation value significantly impacts prosumers' utility. Overall, the proposed strategy aims to promote honest transactions and enhance prosumers' participation in microgrid power transactions.}
}
@article{DUAN2022289,
title = {Reinforcement learning based model-free optimized trajectory tracking strategy design for an AUV},
journal = {Neurocomputing},
volume = {469},
pages = {289-297},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.10.056},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221015423},
author = {Kairong Duan and Simon Fong and C.L. Philip Chen},
keywords = {AUV, HJI equation, RL algorithm, Robust control, Model-free},
abstract = {Considering the fact that it is very difficult to fully model an autonomous underwater vehicle (AUV) in the complex water environment, this paper presents a model-free tracking control strategy for an AUV in the presence of unknown disturbances. We first formulate an optimized control problem by defining a tracking Hamilton–Jacobi–Isaac (HJI) equation. Then, we present a reinforcement learning (RL) algorithm to compute an optimized solution by learning from the HJI equation online. It is noted that during the learning period, no information about the AUV’s dynamics is needed. In order to demonstrate the efficiency of the proposed strategy, numerical simulation is considered, results are validated and discussed.}
}
@article{LI2023122,
title = {Adaptive optimal trajectory tracking control of AUVs based on reinforcement learning},
journal = {ISA Transactions},
volume = {137},
pages = {122-132},
year = {2023},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2022.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0019057822006334},
author = {Zhifu Li and Ming Wang and Ge Ma},
keywords = {Reinforcement learning (RL), Optimal control, Neural networks (NNs), Autonomous underwater vehicle (AUV), Input saturation},
abstract = {In this paper, an adaptive model-free optimal reinforcement learning (RL) neural network (NN) control scheme based on filter error is proposed for the trajectory tracking control problem of an autonomous underwater vehicle (AUV) with input saturation. Generally, the optimal control is realized by solving the Hamilton–Jacobi–Bellman (HJB) equation. However, due to its inherent nonlinearity and complexity, the HJB equation of AUV dynamics is challenging to solve. To deal with this problem, an RL strategy based on an actor–critic framework is proposed to approximate the solution of the HJB equation, where actor and critic NNs are used to perform control behavior and evaluate control performance, respectively. In addition, for the AUV system with the second-order strict-feedback dynamic model, the optimal controller design method based on filtering errors is proposed for the first time to simplify the controller design and accelerate the response speed of the system. Then, to solve the model-dependent problem, an extended state observer (ESO) is designed to estimate the unknown nonlinear dynamics, and an adaptive law is designed to estimate the unknown model parameters. To deal with the input saturation, an auxiliary variable system is utilized in the control law. The strict Lyapunov analysis guarantees that all signals of the system are semi-global uniformly ultimately bounded (SGUUB). Finally, the superiority of the proposed method is verified by comparative experiments.}
}
@article{DEMARS2021117519,
title = {Applying reinforcement learning and tree search to the unit commitment problem},
journal = {Applied Energy},
volume = {302},
pages = {117519},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117519},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921008990},
author = {Patrick {de Mars} and Aidan O’Sullivan},
keywords = {Unit commitment, Reinforcement learning, Tree search, Deep learning, Power systems},
abstract = {Recent advances in artificial intelligence have demonstrated the capability of reinforcement learning (RL) methods to outperform the state of the art in decision-making problems under uncertainty. Day-ahead unit commitment (UC), scheduling power generation based on forecasts, is a complex power systems task that is becoming more challenging in light of increasing uncertainty. While RL is a promising framework for solving the UC problem, the space of possible actions from a given state is exponential in the number of generators and it is infeasible to apply existing RL methods in power systems larger than a few generators. Here we present a novel RL algorithm, guided tree search, which does not suffer from an exponential explosion in the action space with increasing number of generators. The method augments a tree search algorithm with a policy that intelligently reduces the branching factor. Using data from the GB power system, we demonstrate that guided tree search outperforms an unguided method in terms of computational complexity, while producing solutions that show no performance loss in terms of operating costs. We compare solutions against mixed-integer linear programming (MILP) and find that guided tree search outperforms a solution using reserve constraints, the current industry approach. The RL solutions exhibit complex behaviours that differ qualitatively from MILP, demonstrating its potential as a decision support tool for human operators.}
}
@article{LI2023127759,
title = {Performance-constrained fault-tolerant DSC based on reinforcement learning for nonlinear systems with uncertain parameters},
journal = {Applied Mathematics and Computation},
volume = {443},
pages = {127759},
year = {2023},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2022.127759},
url = {https://www.sciencedirect.com/science/article/pii/S009630032200827X},
author = {Dongdong Li and Jiuxiang Dong},
keywords = {Fault-tolerant control, Reinforcement learning, Prescribed performance control (PPC), Neural network, Adaptive dynamic programming (ADP)},
abstract = {In this paper, a performance-constrained fault-tolerant dynamic surface control (DSC) algorithm based on reinforcement learning (RL) is proposed for nonlinear systems with unknown parameters and actuator failures. Considering the problem of multiple actuator failures, the bound for sum of the failure parameters are estimated rather than the parameters themselves, an infinite number of actuator failures can be handled. To improve the performance of the system, based on actor-critic neural networks (NNs) and optimized backstepping control (OBC), RL is introduced to optimize the tracking errors and inputs. By introducing an intermediate controller, the controllers derived from RL algorithm and the fault-tolerant controller are isolated, the difficulties of using RL in fault-tolerant control (FTC) are reduced. In addition, an initial unbounded boundary function is used so that the initial value of the error does not need to be within a prescribed range, not only the tracking error can be reduced to the prescribed accuracy, but also all closed-loop signals are bounded. Finally, the effectiveness and advantages of the proposed algorithm are verified by two examples.}
}
@article{ZHANG2023258,
title = {Multi-agent deep reinforcement learning for online request scheduling in edge cooperation networks},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {258-268},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003788},
author = {Yaqiang Zhang and Ruyang Li and Yaqian Zhao and Rengang Li and Yanwei Wang and Zhangbing Zhou},
keywords = {Edge cooperation networks, Online request scheduling, Multi-agent system, Deep reinforcement learning, Value decomposition, Long-term performance},
abstract = {Edge computing as a complementary paradigm of cloud computing has gained more attention by providing mobile users with diversified services at the network edge. However, the increasingly complex mobile applications put a heavier load on edge networks. It is challenging to provide concurrency requests with high-quality service processing, especially when the edge networks are dynamically changing. To address the above issues, this paper investigates the online concurrent user requests scheduling optimization problem in edge cooperation networks. We model it as an online multi-stage decision-making problem, where requests are divided into a group of independent and logically related sub-tasks. We proposed a centralized training distributed execution based multi-agent deep reinforcement learning technique to realize the implicit cooperation scheduling decision-making policy learning among edge nodes. At the centralized training stage of the proposed mechanism, a value-decomposition-based policy learning technique is adopted to improve the long-term system performance, while at the distributed execution stage, only local environment status information is needed for each edge node to make the request scheduling decision. Extensive experiments are conducted, and simulation results demonstrate that the proposed mechanism outperforms other request scheduling mechanisms in reducing the long-term average system delay and energy consumption while improving the throughput rate of the system.}
}
@article{CHEN2023102096,
title = {Knowledge distillation for portfolio management using multi-agent reinforcement learning},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102096},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102096},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002240},
author = {Min-You Chen and Chiao-Ting Chen and Szu-Hao Huang},
keywords = {Multi-agent reinforcement learning, Knowledge distillation, Artificial stock market, Portfolio management},
abstract = {Many studies have employed reinforcement learning (RL) techniques to successfully create portfolio strategies in recent years. However, since financial markets are extremely noisy, past research has found it challenging to train a stable RL agent using historical data. In this work, we first apply a role-aware multi-agent system to model volatile security markets. Three major roles that are used in our system are presented, and while maximizing their own targets in the Taiwan stock exchange (TWSE) historical data, they also observe trading behavior and compete with other agents. To build a trading strategy, we construct a student–teacher framework in which multi-agent targeting distills the market information and a student RL model is taught using the distilled target. The results show that our method is capable of developing profitable strategies in a quickly changing financial market. In addition, our market distilling technique has the potential to develop a flexible asset allocation strategy by using different student networks.}
}
@article{CHEN2023450,
title = {LJIR: Learning Joint-Action Intrinsic Reward in cooperative multi-agent reinforcement learning},
journal = {Neural Networks},
volume = {167},
pages = {450-459},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023004355},
author = {Zihan Chen and Biao Luo and Tianmeng Hu and Xiaodong Xu},
keywords = {Reinforcement learning, Multi-agent system, Intrinsic reward, Curiosity-driven exploration, Transformer},
abstract = {Effective exploration is the key to achieving high returns for reinforcement learning. Agents must explore jointly in multi-agent systems to find the optimal joint policy. Due to the exploration problem and the shared reward, the policy-based multi-agent reinforcement learning algorithms face policy overfitting, which may lead to the joint policy falling into a local optimum. This paper introduces a novel general framework called Learning Joint-Action Intrinsic Reward (LJIR) for improving multi-agent reinforcement learners’ joint exploration ability and performance. LJIR observes agents’ state and joint actions to learn to construct an intrinsic reward online that can guide effective joint exploration. With the novel combination of Transformer and random network distillation, LJIR selects the novel states to give more intrinsic rewards, which help agents find the best joint actions. LJIR can dynamically adjust the weight of exploration and exploitation during training and keep the policy invariance finally. To ensure LJIR seamlessly adopts existing MARL algorithms, we also provide a flexible combination method for intrinsic and external rewards. Empirical results on the SMAC benchmark show that the proposed method achieves state-of-the-art performance in challenging tasks.}
}
@article{MA2021290,
title = {A parallel multi-module deep reinforcement learning algorithm for stock trading},
journal = {Neurocomputing},
volume = {449},
pages = {290-302},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221005233},
author = {Cong Ma and Jiangshe Zhang and Junmin Liu and Lizhen Ji and Fei Gao},
keywords = {Parallel multi-module, Reinforcement learning, Capital asset pricing model, Long short-term memory},
abstract = {In recent years, deep reinforcement learning (DRL) algorithm has been widely used in algorithmic trading. Many fully automated trading systems or strategies have been built using DRL agents, which integrate price prediction and trading signal generation in one system. However, the previous agents extract the current state from the market data without considering the long-term market historical trend when making decisions. Besides, plenty of related and useful information has not been considered. To address these two problems, we propose a novel model named Parallel Multi-Module Deep Reinforcement Learning (PMMRL) algorithm. Here, two parallel modules are used to extract and encode the feature: one module employing Fully Connected (FC) layers is used to learn the current state from the market data of the traded stock and the fundamental data of the issuing company; another module using Long Short-Term Memory (LSTM) layers aims to detect the long-term historical trend of the market. The proposed model can extract features from the whole environment by the above two modules simultaneously, taking the advantages of both LSTM and FC layers. Extensive experiments on China stock market illustrate that the proposed PMMRL algorithm achieves a higher profit and a lower drawdown than several state-of-the-art algorithms.}
}
@article{YANG2023108603,
title = {Impact time control guidance law with time-varying velocity based on deep reinforcement learning},
journal = {Aerospace Science and Technology},
volume = {142},
pages = {108603},
year = {2023},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2023.108603},
url = {https://www.sciencedirect.com/science/article/pii/S127096382300500X},
author = {Zhuoqiao Yang and Xiangdong Liu and Haikuo Liu},
keywords = {Time-varying velocity, Deep reinforcement learning, Impact time control guidance, Missile guidance},
abstract = {This paper investigates the problem of impact-time-control guidance law with the time-varying velocity caused by gravity and aerodynamic drag. Using the deep reinforcement learning (DRL) algorithm, we propose a novel impact time control guidance (ITCG) law in which a DRL agent is trained from scratch without using any prior knowledge. Different from the traditional ITCG law, the proposed method doesn't rely on the time-to-go estimation, which is difficult to derive and inaccurate with the time-varying velocity. Further, a prioritized experience replay method and a novel action exploration method are introduced in the DRL algorithm to improve learning efficiency. Additionally, the agent action is shaped to provide smooth guidance command, which avoids the problem that the guidance command generated by the intelligent algorithm may not be continuous. Numerical simulations are conducted to support the validity of the proposed algorithm.}
}
@article{RANA2014116,
title = {Real-time dynamic pricing in a non-stationary environment using model-free reinforcement learning},
journal = {Omega},
volume = {47},
pages = {116-126},
year = {2014},
issn = {0305-0483},
doi = {https://doi.org/10.1016/j.omega.2013.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S030504831300100X},
author = {Rupal Rana and Fernando S. Oliveira},
keywords = {Revenue management, Dynamic pricing, Reinforcement learning, Simulation},
abstract = {This paper examines the problem of establishing a pricing policy that maximizes the revenue for selling a given inventory by a fixed deadline. This problem is faced by a variety of industries, including airlines, hotels and fashion. Reinforcement learning algorithms are used to analyze how firms can both learn and optimize their pricing strategies while interacting with their customers. We show that by using reinforcement learning we can model the problem with inter-dependent demands. This type of model can be useful in producing a more accurate pricing scheme of services or products when important events affect consumer preferences. This paper proposes a methodology to optimize revenue in a model-free environment in which demand is learned and pricing decisions are updated in real-time. We compare the performance of the learning algorithms using Monte-Carlo simulation.}
}
@article{ZHANG2022124849,
title = {A generalized energy management framework for hybrid construction vehicles via model-based reinforcement learning},
journal = {Energy},
volume = {260},
pages = {124849},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124849},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222017522},
author = {Wei Zhang and Jixin Wang and Zhenyu Xu and Yuying Shen and Guangzong Gao},
keywords = {Hybrid construction vehicle, Energy management, Model-based learning, Value function approximation, Gaussian mixture model},
abstract = {Hybrid construction vehicles (HCVs) have more specific tasks and highly repetitive patterns than on-road vehicles. Consequently, they are more suitable for model-based energy management. However, distinctions between work cycles result in adverse scenarios for generalizing model-based energy management. In this study, we solve this problem by proposing a generalized strategy using a model-based reinforcement learning framework. The generalized design highlights three aspects: 1) long-term stability, 2) self-learning ability, and 3) state transition model reuse. A reward function with a trend term is proposed to avoid the cumulative errors between operation cycles and improve the long-term stability of learning. In addition, Gaussian process regression is leveraged to approximate the value function, thereby reducing the computational load and improving the learning efficiency. To further enhance the reusability of the environmental model, a modelling method based on the Gaussian mixture model is put forward. Finally, a generalized HCV energy management framework that includes offline and online learning is designed, where a pre-learning model and an approximation function are adopted for reuse and dynamic learning. Simulation results demonstrate the superiority of the proposed framework to conventional model-based methods in terms of stability, generality, and adaptability, accompanied by a reduction of 5.9% in fuel consumption.}
}
@article{DING2019100977,
title = {Intelligent fault diagnosis for rotating machinery using deep Q-network based health state classification: A deep reinforcement learning approach},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100977},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100977},
url = {https://www.sciencedirect.com/science/article/pii/S1474034619305506},
author = {Yu Ding and Liang Ma and Jian Ma and Mingliang Suo and Laifa Tao and Yujie Cheng and Chen Lu},
keywords = {Fault diagnosis, Rotating machinery, Deep reinforcement learning, Deep Q-network},
abstract = {Fault diagnosis methods for rotating machinery have always been a hot research topic, and artificial intelligence-based approaches have attracted increasing attention from both researchers and engineers. Among those related studies and methods, artificial neural networks, especially deep learning-based methods, are widely used to extract fault features or classify fault features obtained by other signal processing techniques. Although such methods could solve the fault diagnosis problems of rotating machinery, there are still two deficiencies. (1) Unable to establish direct linear or non-linear mapping between raw data and the corresponding fault modes, the performance of such fault diagnosis methods highly depends on the quality of the extracted features. (2) The optimization of neural network architecture and parameters, especially for deep neural networks, requires considerable manual modification and expert experience, which limits the applicability and generalization of such methods. As a remarkable breakthrough in artificial intelligence, AlphaGo, a representative achievement of deep reinforcement learning, provides inspiration and direction for the aforementioned shortcomings. Combining the advantages of deep learning and reinforcement learning, deep reinforcement learning is able to build an end-to-end fault diagnosis architecture that can directly map raw fault data to the corresponding fault modes. Thus, based on deep reinforcement learning, a novel intelligent diagnosis method is proposed that is able to overcome the shortcomings of the aforementioned diagnosis methods. Validation tests of the proposed method are carried out using datasets of two types of rotating machinery, rolling bearings and hydraulic pumps, which contain a large number of measured raw vibration signals under different health states and working conditions. The diagnosis results show that the proposed method is able to obtain intelligent fault diagnosis agents that can mine the relationships between the raw vibration signals and fault modes autonomously and effectively. Considering that the learning process of the proposed method depends only on the replayed memories of the agent and the overall rewards, which represent much weaker feedback than that obtained by the supervised learning-based method, the proposed method is promising in establishing a general fault diagnosis architecture for rotating machinery.}
}
@article{VALVERDE2023106657,
title = {Causal reinforcement learning based on Bayesian networks applied to industrial settings},
journal = {Engineering Applications of Artificial Intelligence},
volume = {125},
pages = {106657},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106657},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623008412},
author = {Gabriel Valverde and David Quesada and Pedro Larrañaga and Concha Bielza},
keywords = {Reinforcement learning, Bayesian networks, Causality, Parameter learning, Dynamic simulators, Ordinary differential equations},
abstract = {The increasing amount of real-time data collected from sensors in industrial environments has accelerated the application of machine learning in decision-making. Reinforcement learning (RL) is a powerful tool to find optimal policies for achieving a given goal. However, RL’s typical application is risky and insufficient in environments where actions can have irreversible consequences and require interpretability and fairness. While new trends in RL may provide guidance based on expert knowledge, they do not often consider uncertainty or include prior knowledge in the learning process. We propose a causal reinforcement learning alternative based on Bayesian networks (RLBNs) to address this challenge. The RLBN simultaneously models a policy and takes advantage of the joint distribution of the state and action space, reducing uncertainty in unknown situations. We propose a training algorithm for the network’s parameters and structure based on the reward function and likelihood of the effects and measurements taken. Our experiment with the CartPole benchmark and industrial fouling using ordinary differential equations (ODEs) demonstrates that RLBNs are interpretable, secure, flexible, and more robust than their competitors. Our contributions include a novel method that incorporates expert knowledge into the decision-making engine. It uses Bayesian networks with a predefined structure as a causal graph and a hybrid learning strategy that considers both likelihood and reward. This would avoid losing the virtues of the Bayesian network.}
}
@article{PARK2019397,
title = {LightLearn: An adaptive and occupant centered controller for lighting based on reinforcement learning},
journal = {Building and Environment},
volume = {147},
pages = {397-414},
year = {2019},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2018.10.028},
url = {https://www.sciencedirect.com/science/article/pii/S0360132318306462},
author = {June Young Park and Thomas Dougherty and Hagen Fritz and Zoltan Nagy},
keywords = {Smart building, Occupant centered control, Lighting control, Adaptive control, Reinforcement learning, Occupant behavior},
abstract = {In commercial buildings, lighting contributes to about 20% of the total energy consumption. Lighting controllers that integrate occupancy and luminosity sensors to improve energy efficiency have been proposed. However, they are often ineffective because they focus solely on energy consumption rather than providing comfort to the occupants. An ideal controller should adapt itself to the preferences of the occupant and the environmental conditions. In this article, we introduce LightLearn, an occupant centered controller (OCC) for lighting based on Reinforcement Learning (RL). We describe the theory and hardware implementation of LightLearn. Our experiment during eight weeks in five offices shows that LightLearn learns the individual occupant behaviors and indoor environmental conditions, and adapts its control parameters accordingly by determining personalized set-points. Participants reported that the overall lighting was slightly improved compared to prior lighting conditions. We compare LightLearn to schedule-based and occupancy-based control scenarios, and evaluate their performance with respect to total energy use, light-utilization-ratio, unmet comfort hours, as well as light-comfort-ratio, which we introduce in this paper. We show that only LightLearn balances successfully occupant comfort and energy consumption. The adaptive nature of LightLearn suggests that reinforcement learning based occupant centered control is a viable approach to mitigate the discrepancy between occupant comfort and the goals of building control.}
}
@article{SUN2024122027,
title = {GraphSAGE with deep reinforcement learning for financial portfolio optimization},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122027},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122027},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423025290},
author = {Qiguo Sun and Xueying Wei and Xibei Yang},
keywords = {Portfolio management, Deep reinforcement learning, GraphSAGE, Explainable AI, SHAP},
abstract = {Portfolio optimization is an active management strategy that aims to maximize returns and control risk within reasonable limits. The Proximal Policy Optimization (PPO), a robust on-policy actor-critic deep reinforcement learning (DRL) model, is gaining popularity in portfolio optimization because it can help reduce emotional biases and take systematic investment actions. However, some research has found that the PPO model cannot achieve such remarkable performance in portfolio optimization as in games or robot control. In this paper, a novel GraphSAGE and DRL coupled model (GRL) is proposed to improve the architecture of the PPO agent by introducing a GraphSAGE-based feature extractor to capture the complex non-Euclidean relationships among market indexes, industry indexes and stocks. In addition, the explainable model SHAP is used to select a few but important features for GRL learning, and a method for generating a static financial graph is defined. This improves the robustness and training efficiency of the GRL model. We provide a holistic performance evaluation for GRL on three datasets using five metrics, i.e., Return On Investment (ROI), Sharpe Ratio, Sortino Ratio, Maximum Drawdown, and Calmar Ratio. The results show that the GRL model outperforms the Equal Weight strategy and the S&P 500 index. In addition, the results of the comparative analysis show that the Share-Extractor GRL and the Separate-Extractor GRL significantly outperform the PPO baseline without a feature extractor. This implies that integrating a GraphSAGE-based feature extractor into the PPO agent can improve its performance and robustness in portfolio optimization tasks.}
}
@article{PARK2023121364,
title = {Performance, robustness, and portability of imitation-assisted reinforcement learning policies for shading and natural ventilation control},
journal = {Applied Energy},
volume = {347},
pages = {121364},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121364},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923007286},
author = {Bumsoo Park and Alexandra R. Rempel and Sandipan Mishra},
keywords = {Imitation learning, Policy gradient reinforcement learning, Natural ventilation control, Shading control, Movable insulation control, Climate-responsive design},
abstract = {Space heating and cooling account for approximately half of all building-related energy consumption, emitting 3Gt of CO2 annually, or nearly 10% of the global total. Operable shading, natural ventilation, and solar heating are promising strategies for reducing these emissions, leveraging minimal mechanical energy to condition space with cool night air, cold night skies, and solar radiation. However, these strategies are under-utilized because their performance depends on rigorous coordination among their operable elements. Additionally, the individuality of such systems, and the lack of physics-based models suitable for control design, have thwarted the development of widely-applicable control strategies. To address this problem, here we develop a new data-driven strategy for the design of shading and natural ventilation controls in residential buildings using policy-based reinforcement learning (RL). To limit undesirable actions and reduce training time, we first used imitation learning to initialize RL training with expert knowledge, yielding an initial policy that reduced simulated late-spring space conditioning loads by ≥40% in 24 climatically diverse cities. This policy was then trained with RL in four cities representing Mediterranean, semi-arid, humid subtropical, and continental climates. When deployed in cities with unfamiliar yet related climates, these new policies reduced space conditioning loads by ≥50% in the humid subtropics and by ≥90% in the other three climates, showing exceptional portability. Further, their performance was unexpectedly robust to variations in dwelling orientation, glazing, internal heat gain, and air leakage. These results show the extraordinary potential of imitation-assisted RL in developing high-performance policies for dynamic passive heating and cooling control that remain effective in unfamiliar situations, removing a substantial barrier to passive systems advancement in carbon-free building operation.}
}
@article{SRESAKOOLCHAI2023100193,
title = {Interactive reinforcement learning innovation to reduce carbon emissions in railway infrastructure maintenance},
journal = {Developments in the Built Environment},
volume = {15},
pages = {100193},
year = {2023},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2023.100193},
url = {https://www.sciencedirect.com/science/article/pii/S2666165923000753},
author = {Jessada Sresakoolchai and Sakdirat Kaewunruen},
keywords = {Reinforcement learning, Carbon emission, Railway system, Maintenance, Railway defects, Environmental impact},
abstract = {Carbon emission is one of the primary contributors to global warming. The global community is paying great attention to this negative impact. The goal of this study is to reduce the negative impact of railway maintenance by applying reinforcement learning (RL) by optimizing maintenance activities. Railway maintenance is a complex process that may not be efficient in terms of environmental aspect. This study is the world's first to use the potential of RL to reduce carbon emission from railway maintenance. The data used to create the RL model are gathered from the field data between 2016–019. The study section is 30 km long. Proximal Policy Optimization (PPO) is applied in the study to develop the RL model. The results demonstrate that using RL reduces carbon emission from railway maintenance by 48%, which generates a considerable amount of carbon emission reduction and reduces railway defects by 68%, which also improves maintenance efficiency significantly.}
}
@article{ROSAFALCO2023104947,
title = {Optimised graded metamaterials for mechanical energy confinement and amplification via reinforcement learning},
journal = {European Journal of Mechanics - A/Solids},
volume = {99},
pages = {104947},
year = {2023},
issn = {0997-7538},
doi = {https://doi.org/10.1016/j.euromechsol.2023.104947},
url = {https://www.sciencedirect.com/science/article/pii/S0997753823000396},
author = {Luca Rosafalco and Jacopo Maria {De Ponti} and Luca Iorio and Raffaele Ardito and Alberto Corigliano},
keywords = {Mechanical energy confinement and amplification, Metamaterials, Reinforcement learning, Markov decision process},
abstract = {A reinforcement learning approach to design optimised graded metamaterials for mechanical energy confinement and amplification is described. Through the proximal policy optimisation algorithm, the reinforcement agent is trained to optimally set the lengths and the spacing of an array of resonators. The design optimisation problem is formalised in a Markov decision problem by splitting the optimisation procedure into a discrete number of decisions. Being the physics of graded metamaterials governed by the spatial distribution of local resonances, the space of possible configurations is constrained by using a continuous function for the resonators arrangement. A preliminary analytical investigation has been performed to characterise the dispersive properties of the analysed system by treating it as a locally resonant system. The outcomes of the optimisation procedure confirms the results of previous investigations, highlighting both the validity of the proposed approach and the robustness of the systems of graded resonators when employed for mechanical energy confinement and amplification. The role of the resonator spacing is shown to be secondary with respect to the resonator lengths or, in other words, with respect to the oscillation frequencies of the resonators. However, it is also demonstrated that reducing the number of resonators can be advantageous. The outcomes related to the joint optimisation of the resonator lengths and spacing, thanks also to the adaptive control of the analysis duration, overcome significantly the performance of previously known systems by working almost uniquely on enlarging the time in which the harvester oscillations take place without amplifying these oscillations. The proposed procedure is suitable to be applied to a wide range of design optimisation problems in which the effect of the design choices can be assessed through numerical simulations.}
}
@article{ZHU2020104331,
title = {Scalable reinforcement learning for plant-wide control of vinyl acetate monomer process},
journal = {Control Engineering Practice},
volume = {97},
pages = {104331},
year = {2020},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2020.104331},
url = {https://www.sciencedirect.com/science/article/pii/S0967066120300186},
author = {Lingwei Zhu and Yunduan Cui and Go Takami and Hiroaki Kanokogi and Takamitsu Matsubara},
keywords = {Chemical process control, Reinforcement learning, Vinyl acetate monomer},
abstract = {This paper explores a reinforcement learning (RL) approach that designs automatic control strategies in a large-scale chemical process control scenario as the first step for leveraging an RL method to intelligently control real-world chemical plants. The huge number of units for chemical reactions as well as feeding and recycling the materials of a typical chemical process induces a vast amount of samples and subsequent prohibitive computation complexity in RL for deriving a suitable control policy due to high-dimensional state and action spaces. To tackle this problem, a novel RL algorithm: Factorial Fast-food Dynamic Policy Programming (FFDPP) is proposed. By introducing a factorial framework that efficiently factorizes the action space, Fast-food kernel approximation that alleviates the curse of dimensionality caused by the high dimensionality of state space, into Dynamic Policy Programming (DPP) that achieves stable learning even with insufficient samples. FFDPP is evaluated in a commercial chemical plant simulator for a Vinyl Acetate Monomer (VAM) process. Experimental results demonstrate that without any knowledge of the model, the proposed method successfully learned a stable policy with reasonable computation resources to produce a larger amount of VAM product with comparative performance to a state-of-the-art model-based control.}
}
@article{DIN2022108089,
title = {A deep reinforcement learning-based multi-agent area coverage control for smart agriculture},
journal = {Computers and Electrical Engineering},
volume = {101},
pages = {108089},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108089},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622003445},
author = {Ahmad Din and Muhammed Yousoof Ismail and Babar Shah and Mohammad Babar and Farman Ali and Siddique Ullah Baig},
keywords = {Area coverage, Smart sensors, Deep reinforcement learning, Smart agriculture, Precision agriculture, Multi-robotics systems, Internet of agricultural things (IoAT)},
abstract = {Precision agriculture (PA) is a collage of strategies and technologies to optimize operations and decisions in farms by using spatial and temporal variabilities in yield, crops, and soil within an agricultural plot. It is a data-driven technique, therefore, selective treatment of crops and soil, and managing variabilities using robots and smart sensors is the next improvement in PA. In this paper, it is modeled as a multi-agent patrolling problem, where robots visit subregions that required immediate attention in the agricultural field. Furthermore, for area coverage / patrolling task in the agricultural plot, a centralized Convolutional Neural Network (CNN) based Dual Deep Q-learning (DDQN) is proposed. A customized reward function is designed, which rewards worth-visiting idle regions, and punishes undesirable actions. A proposed algorithm has been compared with various algorithms including individual Q-learning (IRL), uniform coverage (UC), and Behavior-Based Robotics coverage (BBR) for different scenarios in the agricultural plots.}
}
@article{MUKHTAR2023396,
title = {CCGN: Centralized collaborative graphical transformer multi-agent reinforcement learning for multi-intersection signal free-corridor},
journal = {Neural Networks},
volume = {166},
pages = {396-409},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023003854},
author = {Hamza Mukhtar and Adil Afzal and Sultan Alahmari and Saud Yonbawi},
keywords = {Collaborative intersection signal control, Graph convolutional network, Multi-agent centralized reinforcement learning, Cooperative traffic signal control, Markov decision processes, Intelligent transportation},
abstract = {Tackling traffic signal control through multi-agent reinforcement learning is a widely-employed approach. However, current state-of-the-art models have drawbacks: intersections optimize their own local rewards and cause traffic to waste time and fuel with a start-stop mode at each intersection. They also lack information sharing among intersections and their specialized policy hinders the ability to adapt to new traffic scenarios. To overcome these limitations, This work presents a centralized collaborative graph network (CCGN) with the core objective of a signal-free corridor once the traffic flows have waited at the entry intersection of the traffic intersection network on either side, the subsequent intersection gives the open signal as the traffic flows arrive. CCGN combines local policy networks (LPN) and global policy networks, where LPN employed at each intersection predicts actions based on Transformer and Graph Convolutional Network (GCN). In contrast, GPN is based on GCN and Q-network that receives the LPN states, traffic flow and road information to manage intersections to provide a signal-free corridor. We developed the Deep Graph Convolution Q-Network (DGCQ) by combining Deep Q-Network (DQN) and GCN to achieve a signal-free corridor. DGCQ leverages GCN’s intersection collaboration and DQN’s information aggregation for traffic control decisions Proposed CCGN model is trained on the robust synthetic traffic network and evaluated on the real-world traffic networks that outperform the other state-of-the-art models.}
}
@article{FREIREDEOLIVEIRA2021100089,
title = {Q-Managed: A new algorithm for a multiobjective reinforcement learning},
journal = {Software Impacts},
volume = {9},
pages = {100089},
year = {2021},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2021.100089},
url = {https://www.sciencedirect.com/science/article/pii/S2665963821000324},
author = {Thiago Henrique {Freire de Oliveira} and Luiz Paulo {de Souza Medeiros} and Adrião Duarte {Dória Neto} and Jorge Dantas Melo},
keywords = {Multiobjective reinforcement learning, Pareto dominance, Single-policy approach, constraint, , Hypervolume},
abstract = {Multi-objective reinforcement learning involves the use of reinforcement learning techniques to address problems with multiple objectives. To resolve this, we use a hybrid multi-objective optimization method that provides the mathematical guarantee that all policies belonging to the Pareto Front can be found. The hybridization gave rise to Q-Managed, which is given by the ε−constraint method and the Q-Learning algorithm, where the first limits the environment dynamically based on the agent’s learning. Thus, when a region no longer provides improvement, it becomes a constraint, preventing the agent from returning. The simplicity and its performance come from a single-policy algorithms.}
}
@article{HAO2023,
title = {Monte Carlo Tree Search-based Deep Reinforcement Learning for Flexible Operation & Maintenance Optimization of a Nuclear Power Plant},
journal = {Journal of Safety and Sustainability},
year = {2023},
issn = {2949-9267},
doi = {https://doi.org/10.1016/j.jsasus.2023.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S294992672300001X},
author = {Zhaojun Hao and Francesco {Di Maio} and Enrico Zio},
keywords = {Nuclear Power Plant (NPP), Cyber-Physical Energy System (CPES), Flexible Operation, Operation & Maintenance (O&M), Multi-Objective Optimization, Deep Reinforcement Learning (DRL), Monte Carlo Tree Search (MCTS)},
abstract = {Nuclear Power Plants (NPPs) are required to operate on a flexible profitable production plan while guaranteeing high safety standards. Deep Reinforcement Learning (DRL) is an effective method to find the most profitable Operation & Maintenance (O&M) strategy to adopt in a complex system. However, profit-driven only DRL neglects safety-related issues. In this paper, we propose a DRL approach to solve Single-Objective Sequential Decision Problems (SOSDPs) and Multi-Objective Sequential Decision Problems (MOSDPs) to find O&M strategies that trade off reliability and profit. The combinatorial problem related with the training of the RL agent to search for the optimal solution is addressed by Monte Carlo Tree Search (MCTS), whose performance is compared with the traditionally adopted Proximal Policy Optimization (PPO) & Imitation Learning (IL). A case study is considered for demonstration.}
}
@article{PRITZKOLEIT20201581,
title = {Reinforcement Learning and Trajectory Planning based on Model Approximation with Neural Networks applied to Transition Problems},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {1581-1587},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2193},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320328470},
author = {Max Pritzkoleit and Carsten Knoll and Klaus Röbenack},
keywords = {Trajectory planning, Reinforcement learning, Learning control, Neural-network models, Model approximation, Tracking control},
abstract = {In this paper we use a multilayer neural network to approximate the dynamics of nonlinear (mechanical) control systems. Furthermore, these neural network models are combined with offline trajectory planning, to form a model-based reinforcement learning (RL) algorithm, suitable for transition problems of nonlinear dynamical systems. We evaluate the algorithm on the swing-up of the cart-pole benchmark system and observe a significant performance gain in terms of data efficiency compared to a state-of-the-art model-free RL method (Deep Deterministic Policy Gradient (DDPG)). Additionally, we present first experimental results on a cart-triple-pole system test bench. For a simple transition problem, the proposed algorithm shows a good controller performance.}
}
@article{WANG2023105090,
title = {Multi-source information fusion deep self-attention reinforcement learning framework for multi-label compound fault recognition},
journal = {Mechanism and Machine Theory},
volume = {179},
pages = {105090},
year = {2023},
issn = {0094-114X},
doi = {https://doi.org/10.1016/j.mechmachtheory.2022.105090},
url = {https://www.sciencedirect.com/science/article/pii/S0094114X22003366},
author = {Zisheng Wang and Jianping Xuan and Tielin Shi},
keywords = {Multi-source information fusion feature, Compound fault recognition, Deep reinforcement learning, Self-attention mechanism, Multi-label learning, Wavelet packet decomposition},
abstract = {Aiming at compound fault recognition, multi-label learning easily has a strong comprehension on relevance between simultaneous mechanism faults, such as bearing defect fault and tool wear fault. Moreover, compared with single data source, multiple data sources can more fully monitor the working status of equipment. Consequently, this paper proposes a multi-source information fusion (MSIF) feature to train the multi-label deep reinforcement learning (ML-DRL) model, and develops a multi-source information fusion deep self-attention reinforcement learning (MSIF-DSARL) framework. Firstly, compound fault samples with multiple data sources are transformed into 3D wavelet coefficient tensors. Then the MSIF features are extracted from 3D tensors, using a position self-attention fusion (PSAF) module and a channel self-attention fusion (CSAF) module. Especially, the PSAF module can excavate the internal time–frequency information in every source, and the CSAF module can integrate the information differences between multiple sources. Finally, the ML-DRL model is trained with the MSIF features. In a laboratory experiment and an engineering application, diagnostic results demonstrate powerfully that the proposed framework has better superiority and practicability in recognizing compound fault, than present popular multi-label learning methods.}
}
@article{MELNIK201929,
title = {Workflow scheduling using Neural Networks and Reinforcement Learning},
journal = {Procedia Computer Science},
volume = {156},
pages = {29-36},
year = {2019},
note = {8th International Young Scientists Conference on Computational Science, YSC2019, 24-28 June 2019, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.126},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919310440},
author = {Mikhail Melnik and Denis Nasonov},
keywords = {workflow scheduling, artificial intelligence, reinforcement learning, cloud computing},
abstract = {The development of information technologies entails a nonlinear growth of both volumes of data and the complexity of data processing itself. Scheduling is one of the main components for optimizing the operation of the computing system. Currently, there are a large number of scheduling algorithms. However, even in spite of existing hybrid schemes, there remains a need for a scheduling scheme that can quickly and efficiently solve a scheduling problem on a wide range of possible states of the computing environment, including the high heterogeneity of computational models and resources, and should have an ability to self-adapt and self-learn. At present, artificial intelligence and neural networks are the most popular methods for working with data and solving a wide range of problems, but they are not developed enough to solve the scheduling problem. Therefore, in this paper we propose a scheduling scheme based on Artificial Neural Networks and the principles of Reinforcement Learning.}
}
@article{PATERNINAARBOLEDA2005389,
title = {A multi-agent reinforcement learning approach to obtaining dynamic control policies for stochastic lot scheduling problem},
journal = {Simulation Modelling Practice and Theory},
volume = {13},
number = {5},
pages = {389-406},
year = {2005},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2004.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X04001406},
author = {Carlos D. Paternina-Arboleda and Tapas K. Das},
keywords = {SELSP, Scheduling, Reinforcement learning, Simulation optimization},
abstract = {This paper presents a methodology that, for the problem of scheduling of a single server on multiple products, finds a dynamic control policy via intelligent agents. The dynamic (state dependent) policy optimizes a cost function based on the WIP inventory, the backorder penalty costs and the setup costs, while meeting the productivity constraints for the products. The methodology uses a simulation optimization technique called Reinforcement Learning (RL) and was tested on a stochastic lot-scheduling problem (SELSP) having a state–action space of size 1.8×107. The dynamic policies obtained through the RL-based approach outperformed various cyclic policies. The RL approach was implemented via a multi-agent control architecture where a decision agent was assigned to each of the products. A Neural Network based approach (least mean square (LMS) algorithm) was used to approximate the reinforcement value function during the implementation of the RL-based methodology. Finally, the dynamic control policy over the large state space was extracted from the reinforcement values using a commercially available tree classifier tool.}
}
@article{ZHU2023113466,
title = {Analysis of strategic interactions among distributed virtual alliances in electricity and carbon emission auction markets using risk-averse multi-agent reinforcement learning},
journal = {Renewable and Sustainable Energy Reviews},
volume = {183},
pages = {113466},
year = {2023},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2023.113466},
url = {https://www.sciencedirect.com/science/article/pii/S1364032123003234},
author = {Ziqing Zhu and Ka Wing Chan and Siqi Bu and Siu Wing Or and Shiwei Xia},
keywords = {Distributed network market, Distributed virtual alliances, Ancillary service market, Carbon emission auction market, Multi-agent reinforcement learning},
abstract = {The incorporation of carbon emission auction market (CEAM) and ancillary service market (ASM) is an emerging trading paradigm in active distribution network (ADN). Such regime not only promotes the elimination of carbon emission, but also facilitates the secure operation of power network, especially considering the participation of distributed virtual alliances (DVAs) consisting of renewable distributed generators (RDGs) with uncertain output. In this research, a bi-level bidding and market clearing dynamic programming model is developed for in-depth analysis of market participants’ bidding strategies and market equilibrium. This model allows DVAs to modify their bidding strategies in the energy market (EM), ASM and CEAM based on the market clearing results and uncertainty of RDG output. Also, a new Meta-Learning based Win-or-Learn-Fast (MLWoLF-PHC) algorithm, which not only enables the fully distributed bidding strategy modification, but also performs well considering uncertainty as a risk-averse method, is proposed to solve this model. Its computational performance, the market equilibrium analysis, and the impact of CEAM on the converged market clearing price of EM and ASM would be thoroughly investigated and examined in the case studies.}
}
@article{ZHANG2023121490,
title = {A safe reinforcement learning-based charging strategy for electric vehicles in residential microgrid},
journal = {Applied Energy},
volume = {348},
pages = {121490},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121490},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923008541},
author = {Shulei Zhang and Runda Jia and Hengxin Pan and Yankai Cao},
keywords = {Electric vehicles, Charging scheduling, Safe reinforcement learning, Constrained soft actor-critic, Residential microgrid, Nonlinear charging characteristics},
abstract = {With the growing popularity of electric vehicles (EVs), it is a new challenge for the residential microgrid system to conduct charging scheduling to meet the charging demands of EVs while maximizing its profit. In this work, a safe reinforcement learning (RL)-based charging scheduling strategy is proposed to meet this challenge. We construct a complete microgrid system equipped with a large charging station and consider different types of EVs, as well as the vehicle-to-grid (V2G) mode and nonlinear charging characteristics of EVs. Subsequently, the charging scheduling problem is formulated as a constrained Markov decision process (CMDP) due to the various limitations of power and demands. To effectively capture the uncertainties of the supply side and demand side of the microgrid, a model-free RL framework is employed. However, the curse of dimensionality of the action space is inevitable as EVs increase. To solve this problem, a charging and discharging strategy based on a general ladder electricity pricing scheme is designed. Different EVs are divided into different sets according to their states under this strategy, and the agent gives control signals of different sets instead of controlling each EV individually, which effectively reduces the dimension of the action space. Subsequently, a constrained soft actor-critic (CSAC) algorithm is designed to solve the established CMDP, and a safety filter is introduced to ensure safety. In the end, a numerical case is conducted to verify the effectiveness of the proposed method.}
}
@article{WANG2023110019,
title = {CRLM: A cooperative model based on reinforcement learning and metaheuristic algorithms of routing protocols in wireless sensor networks},
journal = {Computer Networks},
volume = {236},
pages = {110019},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110019},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623004644},
author = {Zhendong Wang and Liwei Shao and Shuxin Yang and Junling Wang and Dahai Li},
keywords = {Wireless sensor networks, Hierarchical routing, Cluster-based protocol, Nonuniform cluster, Cluster rules, Network lifetime},
abstract = {In wireless sensor networks, reasonable clustering and routing are keys to efficient energy utilization. However, the selection of cluster heads and routes is NP-hard. Most of the existing routing protocols use heuristic or metaheuristic optimization algorithms to solve this problem. Most protocols regard the selection of the cluster head and routing as two independent problems. However, the selection of cluster heads will affect the selection of routes, and there is a certain relationship between the two stages. Therefore, considering these two problems independently, the solution obtained is not necessarily the optimal solution in the network. In addition, most of the existing routing protocols are still subject to conventional clustering and conventional multi-hop communication in the network, which is extremely unfavorable for reducing the energy consumption of nodes. In this paper, we propose a cooperative model based on reinforcement learning and metaheuristic algorithms called CRLM, in which we use reinforcement learning to enhance the merit-seeking capability of the metaheuristic algorithm and use the algorithm to solve network communication schemes (clustering and routing are considered as one phase). The communication scheme also achieves load balancing of clusters within the network through pruning and employs a novel multi-hop model to reduce network energy waste. Compared to E-ALWO, ChOA-HGS, GATERP, GWO, IPSO-GWO, and LEACH, CRLM has 56%, 95%, 34.5%, 85.7%, 116.7%, and 140.7% improvements in network lifetime.}
}
@article{LIU2023101977,
title = {Parallel hyper heuristic algorithm based on reinforcement learning for the corridor allocation problem and parallel row ordering problem},
journal = {Advanced Engineering Informatics},
volume = {56},
pages = {101977},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101977},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623001052},
author = {Junqi Liu and Zeqiang Zhang and Silu Liu and Yu Zhang and Tengfei Wu},
keywords = {Corridor allocation problem, Parallel row ordering problem, Hyper heuristics, Reinforcement learning, Combinatorial optimisation},
abstract = {Hyper heuristics is a relatively new optimisation algorithm. Numerous studies have reported that hyper heuristics are well applied in combinatorial optimisation problems. As a classic combinatorial optimisation problem, the row layout problem has not been publicly reported on applying hyper heuristics to its various sub-problems. To fill this gap, this study proposes a parallel hyper-heuristic approach based on reinforcement learning for corridor allocation problems and parallel row ordering problems. For the proposed algorithm, an outer layer parallel computing framework was constructed based on the encoding of the problem. The simulated annealing, tabu search, and variable neighbourhood algorithms were used in the algorithm as low-level heuristic operations, and Q-learning in reinforcement learning was used as a high-level strategy. A state space containing sequences and fitness values was designed. The algorithm performance was then evaluated for benchmark instances of the corridor allocation problem (37 groups) and parallel row ordering problem (80 groups). The results showed that, in most cases, the proposed algorithm provided a better solution than the best-known solutions in the literature. Finally, the meta-heuristic algorithm applied to three low-level heuristic operations is taken as three independent algorithms and compared with the proposed hyper-heuristic algorithm on four groups of parallel row ordering problem instances. The effectiveness of Q-learning in selection is illustrated by analysing the comparison results of the four algorithms and the number of calls of the three low-level heuristic operations in the proposed method.}
}
@article{WANG2020101,
title = {Cooperative control for multi-player pursuit-evasion games with reinforcement learning},
journal = {Neurocomputing},
volume = {412},
pages = {101-114},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S092523122031002X},
author = {Yuanda Wang and Lu Dong and Changyin Sun},
keywords = {Pursuit-evasion game, Reinforcement learning, Distributed control, Communication network},
abstract = {In this paper, we consider a pursuit-evasion game in which multiple pursuers attempt to capture one superior evader. A distributed cooperative pursuit strategy with communication is developed based on reinforcement learning. The centralized critic and distributed actor structure and the learning-based communication mechanism are adopted to solve the cooperative pursuit control problem. Instead of using broadcast to share information among the pursuers, we construct the ring topology network and the leader-follower line topology network for communication, which could significantly reduce the complexity and save the communication and computation resources. The training algorithms for these two network topologies are developed based on the deep deterministic policy gradient algorithm. Furthermore, the proposed approach is implemented in a simulation environment. The training and evaluation results demonstrate that the pursuit team could learn highly efficient cooperative control and communication policies. The pursuers can capture a superior evader driven by an intelligent escape policy with a high success rate.}
}
@article{AJAO2023200216,
title = {Secure edge computing vulnerabilities in smart cities sustainability using petri net and genetic algorithm-based reinforcement learning},
journal = {Intelligent Systems with Applications},
volume = {18},
pages = {200216},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200216},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323000418},
author = {Lukman Adewale Ajao and Simon Tooswem Apeh},
keywords = {Edge computing, Fog computing, Industrial internet of things, Reinforcement learning, Smart cities},
abstract = {The Industrial Internet of Things (IIoT) revolution has emerged as a promising network that enhanced information dissemination about the city's resources. This city's resources are wirelessly connected to different constrained devices (such as sensors, robotics, and actuators). However, the communication of this wireless information is threatened by several malicious attacks, cyber-attacks, and hackers. This is due to unsecured IIoT networks that were exposed as a potential back door entry point for the attacks. Consequently, this study aims to develop a security framework for the smart cities’ sustainability edge computing vulnerabilities using Petri Net and Genetic Algorithm-Based Reinforcement Learning (GARL). First, a common trust model for addressing information outflows in the network using a distributed authorization algorithm is proposed. This algorithm is implemented on a secure framework modeling in Petri Net called secure trust-aware philosopher privacy and authentication (STAPPA) for mitigation of the privacy breach in the networks. Genetic Algorithm-based Reinforcement Learning (GARL) is used to optimize the search, detect anomalies, and shortest route during the agent learning in the environment. The detection and accuracy rate results obtained over a secure framework using reinforcement learning are 98.75, 99, 99.50, 99.75, and 100% during simulation in the network environment. The average sensitivity of the detection rate is 1.000, while the average specificity outcome is 0.868. The result of the GARL simulation model obtained shows the best distance of 238.84 * 10−3 fitness when the search space is optimized by reducing the number of chromosomes to 10 in the model. These approaches help to detect anomalies and prevent unauthorized users from accessing edge computing components in the city architecture.}
}
@article{WANG2021101315,
title = {Intelligent fault recognition framework by using deep reinforcement learning with one dimension convolution and improved actor-critic algorithm},
journal = {Advanced Engineering Informatics},
volume = {49},
pages = {101315},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101315},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000689},
author = {Zisheng Wang and Jianping Xuan},
keywords = {Fault recognition, Deep reinforcement learning, Actor-critic algorithm, 1D convolution},
abstract = {The quality of fault recognition part is one of the key factors affecting the efficiency of intelligent manufacturing. Many excellent achievements in deep learning (DL) have been realized recently as methods of fault recognition. However, DL models have inherent shortcomings. In particular, the phenomenon of over-fitting or degradation suggests that such an intelligent algorithm cannot fully use its feature perception ability. Researchers have mainly adapted the network architecture for fault diagnosis, but the above limitations are not taken into account. In this study, we propose a novel deep reinforcement learning method that combines the perception of DL with the decision-making ability of reinforcement learning. This method enhances the classification accuracy of the DL module to autonomously learn much more knowledge hidden in raw data. The proposed method based on the convolutional neural network (CNN) also adopts an improved actor-critic algorithm for fault recognition. The important parts in standard actor-critic algorithm, such as environment, neural network, reward, and loss functions, have been fully considered in improved actor-critic algorithm. Additionally, to fully distinguish compound faults under heavy background noise, multi-channel signals are first stacked synchronously and then input into the model in the end-to-end training mode. The diagnostic results on the compound fault of the bearing and tool in the machine tool experimental system show that compared with other methods, the proposed network structure has more accurate results. These findings demonstrate that under the guidance of the improved actor-critic algorithm and processing method for multi-channel data, the proposed method thus has stronger exploration performance.}
}
@article{YUNGAICELANAULA2023637,
title = {SDN/NFV-based framework for autonomous defense against slow-rate DDoS attacks by using reinforcement learning},
journal = {Future Generation Computer Systems},
volume = {149},
pages = {637-649},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003047},
author = {Noe M. Yungaicela-Naula and Cesar Vargas-Rosales and Jesús A. Pérez-Díaz},
keywords = {Network function virtualization (NFV), Moving target defense (MTD), Reinforcement learning (RL), Slow-rate DDoS, Software defined networking (SDN), Zero touch networks and service management (ZSM)},
abstract = {The unforeseen and skyrocketed shift in the number of connections to the Internet during the last years has created vast and critical vulnerabilities in networks that cybercriminals have quickly seized to launch high-volume DDoS attacks. Existing tools, such as advanced firewalls or intrusion prevention systems (IPS), cannot handle such an elevated volume of attacks because these solutions are dependent on humans. Therefore, adaptation of the current network security solutions to automated ones is more significant than ever to foster the development of the zero-touch networks and service management (ZSM) paradigm. Building on our preliminary work in this field, in this study, we provide a software-defined networking (SDN)-based framework that automates the detection and mitigation of slow-rate DDoS attacks. The framework uses deep learning (DL) to detect attacks and reinforcement learning (RL) to mitigate them. Furthermore, a network function virtualization (NFV)-assisted moving target defense (MTD) mechanism is included to amplify the effectiveness and flexibility of the solution. The framework is tested on a simulated network using open-source tools, namely Open Network Operating System (ONOS), Containernet, Apache Web Server, and Docker. The source code of a prototype of the framework is shared, which can be used and improved by interested researchers. Finally, the experimental results demonstrate that RL agents learn optimal DDoS mitigation policies in different scenarios and that they quickly adapt to new conditions that vary in short periods of time.}
}
@article{PETRUSEV2023100959,
title = {Reinforcement learning for robust voltage control in distribution grids under uncertainties},
journal = {Sustainable Energy, Grids and Networks},
volume = {33},
pages = {100959},
year = {2023},
issn = {2352-4677},
doi = {https://doi.org/10.1016/j.segan.2022.100959},
url = {https://www.sciencedirect.com/science/article/pii/S2352467722002041},
author = {Aleksandr Petrusev and Muhammad Andy Putratama and Rémy Rigo-Mariani and Vincent Debusschere and Patrick Reignier and Nouredine Hadjsaid},
keywords = {Voltage control, Reinforcement learning, TD3PG, PPO, Flexibility, PV production, Batteries, Distribution grid, Second-order conic relaxation, Optimal power flow},
abstract = {Traditional optimization-based voltage controllers for distribution grid applications require consumption/production values from the meters as well as accurate grid data (i.e., line impedances) for modeling purposes. Those algorithms are sensitive to uncertainties, notably in consumption and production forecasts or grid models. This paper focuses on the latter. Indeed, line parameters gradually deviate from their original values over time due to exploitation and weather conditions. Also, those data are oftentimes not fully available at the low voltage side thus creating sudden changes between the datasheet and the actual value. To mitigate the impact of uncertain line parameters, this paper proposes the use of a deep reinforcement learning algorithm for voltage regulation purposes in a distribution grid with PV production by controlling the setpoints of distributed storage units as flexibilities. Two algorithms are considered, namely TD3PG and PPO. A two-stage strategy is also proposed, with offline training on a grid model and further online training on an actual system (with distinct impedance values). The controllers’ performances are assessed concerning the algorithms’ hyperparameters, and the obtained results are compared with a second-order conic relaxation optimization-based control. The results show the relevance of the RL-based control, in terms of accuracy, robustness to gradual or sudden variations on the line impedances, and significant speed improvement (once trained). Validation runs are performed on a simple 11-bus system before the method’s scalability is tested on a 55-bus network.}
}
@article{VO2023105999,
title = {Toward complete coverage planning using deep reinforcement learning by trapezoid-based transformable robot},
journal = {Engineering Applications of Artificial Intelligence},
volume = {122},
pages = {105999},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.105999},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623001835},
author = {Dinh Tung Vo and Anh Vu Le and Tri Duc Ta and Minh Tran and Phan Van Duc and Minh Bui Vu and Nguyen Huu Khanh Nhan},
keywords = {Transformable robotics, Reconfigurable tiling robotics, Cleaning and maintenance, Complete coverage planning, Deep reinforcement learning},
abstract = {Shape-shifting robots are the feasible solutions to solve the Complete Coverage Planning (CCP) problem. These robots can extend the covered areas by reconfiguring their shape to different forms according to space conditions. Since energy usage while navigating is constrained by the number of shape-shifting, it is desirable to cover the confined areas by trajectory using minimal robot actions within finite states. This paper presents a CCP method using deep reinforcement learning (DRL) for a reconfigurable robot with a trapezoid shape called Transbot. The framework derives optimal action policy for robot trajectory within the grid-based workspace. DRL model relies on Convolutional Neural Networks (CNNs) with Long Short Temporary Memory (LSTM) layers using Actor-Critic with Experience Replay (ACER) as the decision layers. The trained DRL model simultaneously generates robot shapes and directions with optimal energy cost by maximizing the cumulative reward representing the Transbot actions. By creating the trajectory with less 28.95% energy and 19.42% time in tested simulation and real-world experiments, the proposed CCP framework outperforms the existing tiling-based heuristic optimization techniques.}
}
@article{YUAN2019119,
title = {End-to-end nonprehensile rearrangement with deep reinforcement learning and simulation-to-reality transfer},
journal = {Robotics and Autonomous Systems},
volume = {119},
pages = {119-134},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018304913},
author = {Weihao Yuan and Kaiyu Hang and Danica Kragic and Michael Y. Wang and Johannes A. Stork},
keywords = {Nonprehensile rearrangement, Deep reinforcement learning, Transfer learning},
abstract = {Nonprehensile rearrangement is the problem of controlling a robot to interact with objects through pushing actions in order to reconfigure the objects into a predefined goal pose. In this work, we rearrange one object at a time in an environment with obstacles using an end-to-end policy that maps raw pixels as visual input to control actions without any form of engineered feature extraction. To reduce the amount of training data that needs to be collected using a real robot, we propose a simulation-to-reality transfer approach. In the first step, we model the nonprehensile rearrangement task in simulation and use deep reinforcement learning to learn a suitable rearrangement policy, which requires in the order of hundreds of thousands of example actions for training. Thereafter, we collect a small dataset of only 70 episodes of real-world actions as supervised examples for adapting the learned rearrangement policy to real-world input data. In this process, we make use of newly proposed strategies for improving the reinforcement learning process, such as heuristic exploration and the curation of a balanced set of experiences. We evaluate our method in both simulation and real setting using a Baxter robot to show that the proposed approach can effectively improve the training process in simulation, as well as efficiently adapt the learned policy to the real world application, even when the camera pose is different from simulation. Additionally, we show that the learned system not only can provide adaptive behavior to handle unforeseen events during executions, such as distraction objects, sudden changes in positions of the objects, and obstacles, but also can deal with obstacle shapes that were not present in the training process.}
}
@article{ZHOU2021285,
title = {Multi-target tracking for unmanned aerial vehicle swarms using deep reinforcement learning},
journal = {Neurocomputing},
volume = {466},
pages = {285-297},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.09.044},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221014223},
author = {Wenhong Zhou and Zhihong Liu and Jie Li and Xin Xu and Lincheng Shen},
keywords = {UAV swarms, Multi-target tracking, Multi-agent reinforcement learning, Scalability, Feature representation},
abstract = {In recent years, deep reinforcement learning (DRL) has proved its great potential in multi-agent cooperation. However, how to apply DRL to multi-target tracking (MTT) problem for unmanned aerial vehicle (UAV) swarms is challenging: 1) the scale of UAVs may be large, but the existing multi-agent reinforcement learning (MARL) methods that rely on global or joint information of all agents suffer from the dimensionality curse; 2) the dimension of each UAV’s received information is variable, which is incompatible with the neural networks with fixed input dimensions; 3) the UAVs are homogeneous and interchangeable that each UAV’s policy should be irrelevant to the permutation of its received information. To this end, we propose a DRL method for UAV swarms to solve the MTT problem. Firstly, a decentralized swarm-oriented Markov Decision Process (MDP) model is presented for UAV swarms, which involves each UAV’s local communication and partial observation. Secondly, to achieve better scalability, a cartogram feature representation (FR) is proposed to integrate the variable-dimensional information set into a fixed-shape input variable, and the cartogram FR can also maintain the permutation irrelevance to the information. Then, the double deep Q-learning network with dueling architecture is adapted to the MTT problem, and the experience-sharing training mechanism is adopted to learn the shared cooperative policy for UAV swarms. Extensive experiments are provided and the results show that our method can successfully learn a cooperative tracking policy for UAV swarms and outperforms the baseline method in the tracking ratio and scalability.}
}
@article{JANG2022103225,
title = {Generative Design by Reinforcement Learning: Enhancing the Diversity of Topology Optimization Designs},
journal = {Computer-Aided Design},
volume = {146},
pages = {103225},
year = {2022},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2022.103225},
url = {https://www.sciencedirect.com/science/article/pii/S0010448522000239},
author = {Seowoo Jang and Soyoung Yoo and Namwoo Kang},
keywords = {Generative design, Topology optimization, Deep learning, Reinforcement learning, Design diversity},
abstract = {Generative design refers to computational design methods that can automatically conduct design exploration under constraints defined by designers. Among many approaches, topology optimization-based generative designs aim to explore diverse topology designs, which cannot be represented by conventional parametric design approaches. Recently, data-driven topology optimization research has started to exploit artificial intelligence, such as deep learning or machine learning, to improve the capability of design exploration. This study proposes a reinforcement learning (RL) based generative design process, with reward functions maximizing the diversity of topology designs. We formulate generative design as a sequential problem of finding optimal design parameter combinations in accordance with a given reference design. Proximal Policy Optimization is used as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. To reduce the heavy computational burden of the wheel topology optimization process required by our RL formulation, we approximate the optimization process with neural networks. With efficient data preprocessing/augmentation and neural architecture, the neural networks achieve a generalized performance and symmetricity-reserving characteristics. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention.}
}
@article{HEJASE2022129,
title = {Dynamic and Interpretable State Representation for Deep Reinforcement Learning in Automated Driving},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {24},
pages = {129-134},
year = {2022},
note = {10th IFAC Symposium on Advances in Automotive Control AAC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.273},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322023059},
author = {Bilal Hejase and Ekim Yurtsever and Teawon Han and Baljeet Singh and Dimitar P. Filev and H. Eric Tseng and Umit Ozguner},
keywords = {autonomous vehicles, reinforcement learning control, state representation, interpretability, generalization, deep learning},
abstract = {Understanding the causal relationship between an autonomous vehicle's input state and its output action is important for safety mitigation and explainable automated driving. However, reinforcement learning approaches have the drawback of being black box models. This work proposes an interpretable state representation that can capture state-action causalities for an automated driving agent, while also allowing for the underlying formulation to be general enough to be adapted to different driving scenarios. It also proposes encoding temporally-extended information in the state representation for better driving performance. We test this approach on a reinforcement learning agent in a highway simulation environment and demonstrate that the proposed state representation can capture state-action causalities in an interpretable manner. Experimental results show that the formulation and interpretation can be used to adapt the behavior of the driving agent to achieve desired, even unseen, driving behaviors after training.}
}
@article{DU2022110477,
title = {Safe deep reinforcement learning-based adaptive control for USV interception mission},
journal = {Ocean Engineering},
volume = {246},
pages = {110477},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.110477},
url = {https://www.sciencedirect.com/science/article/pii/S002980182101756X},
author = {Bin Du and Bin Lin and Chenming Zhang and Botao Dong and Weidong Zhang},
keywords = {Unmanned surface vessels, Safe reinforcement learning, Data-based learning control, Uniformly ultimate bounded stability, Interception mission},
abstract = {This paper aims to develop a safe learning scheme of the USV interception mission. A safe Lyapunov boundary deep deterministic policy gradient (SLDDPG) algorithm is presented for the USV interception mission. The uniformly ultimate bounded (UUB) stability of control systems is analyzed under finite safety constraints. A single neuron proportional adaptive control (SNPAC) is applied to pre-train the deep policy network for speeding up the training process. The proposed method is evaluated by a series of simulations of the USVs interception and tracking mission. Compared with the existing results, our method can fast converge to the feasible solution subject to safety constraints and demonstrate a high performance in stability and safety by virtual-reality experiments.}
}
@article{AKRAMIZADEH2010487,
title = {Model-based Reinforcement Learning in Multiagent Systems in Extensive Forms with Perfect Information},
journal = {IFAC Proceedings Volumes},
volume = {43},
number = {8},
pages = {487-494},
year = {2010},
note = {12th IFAC Symposium on Large Scale Systems: Theory and Applications},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20100712-3-FR-2020.00080},
url = {https://www.sciencedirect.com/science/article/pii/S147466701533439X},
author = {Ali Akramizadeh and Ahmad Afshar and Mohammad {-B Menhaj} and Samira Jafari},
keywords = {Multiagent learning, model-based reinforcement learning, subgame perfect equilibrium points, Markov games},
abstract = {Regarding the fact that model-based reinforcement learning has a superior performance over traditional RL, in this paper, we extend traditional model-based reinforcement learning for a group of self-interested agents with consecutive action selection trying to find the optimal policy. Every single decision making situation is modeled as extensive form games with perfect information. A modified version of prioritized sweeping is proposed in which subgame perfect equilibrium point is the optimal joint action. Finally, we discuss the algorithm analytically, and provide a formal convergence proof.}
}
@article{WANG2023126504,
title = {Simplified reinforcement learning control algorithm for p-norm multiagent systems with full-state constraints},
journal = {Neurocomputing},
volume = {551},
pages = {126504},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126504},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223006276},
author = {Min Wang and Liang Cao and Hongjing Liang and Wenbin Xiao},
keywords = {Birpartite tracking control, Full-state constraints, Optimized backstepping, p-norm multiagent systems, Simplified reinforcement learning},
abstract = {Abtract
This paper studies the bipartite consensus tracking control problem with full-state constraints for p-norm multiagent systems. For the full-state constraints problem of p-norm multiagent systems, a transformed function is utilized to achieve the objective of the constraints, which has the property of low complexity because it avoids the intervention of log-type functions or trigonometric functions in the controllers. Meanwhile, the bipartite control performance of p-norm multiagent systems is also guaranteed. Moreover, under the simplified reinforcement learning framework, a compensation strategy is utilized to compensate the unknown ideal weights caused by the simplified reinforcement learning algorithm of critic-actor method, and greatly improve the accuracy of the tracking performance for p-norm multiagent systems. Furthermore, the effectiveness of the proposed strategy is illustrated by an actual simulation.}
}
@article{DIEL2023103639,
title = {RSCAT: Towards zero touch congestion control based on actor–critic reinforcement learning and software-defined networking},
journal = {Journal of Network and Computer Applications},
volume = {215},
pages = {103639},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103639},
url = {https://www.sciencedirect.com/science/article/pii/S1084804523000589},
author = {Gustavo Diel and Charles Christian Miers and Maurício Aronne Pillon and Guilherme Piêgas Koslovski},
keywords = {Congestion control, Network, Actor–critic, Deep reinforcement learning, Classification, Zero touch},
abstract = {Network congestion is a phenomenon present in contemporaneous data centers (DCs) independently of scale and underlying technologies. The small-scale presence of congestion causes information delay for the DC hosted services, while in large-scale data losses or even service downtimes are not uncommon. Several congestion control algorithms out in the market have no network support, and as such, they must act and rely only on the data provided by the servers they are running on. Some algorithms have used network’s feedback to improve the performance, however these solutions depend on tuning parameters for marking queued packets, an arduous task on DCs shared by multiple applications with different workloads. In turn, Software-Defined Networking (SDN) created an opportunity to avoid congestion once the centralized controller can gather ongoing and historical information from all network switches and flows. The data gathered is enormous, and fast-computing algorithms are crucial for decision-making. In this sense, this work proposes Reinforcement Learning and SDN-aided Congestion Avoidance Tool (RSCAT), which uses data classification to determine if the network is congested and actor–critic reinforcement learning to find better Transmission Control Protocol (TCP) parameters. We evaluated RSCAT based on three workloads: a diversified DC traffic simulation; high-performance computing from NAS benchmark; and Hadoop TeraSort application. Our results show RSCAT fundamentals (reinforcement learning, data classification, and SDN) are potential candidates to achieve zero touch network congestion control. Specifically, our experimental analysis shows RSCAT, running alongside with traditional TCP congestion control algorithms like DCTCP and CUBIC, can decrease the flow completion time and applications’ runtimes in several cases, without requiring any software update on DC end-points.}
}
@incollection{LI2024325,
title = {17 - Emergence of tool construction and tool use through hierarchical reinforcement learning},
editor = {Robert Kozma and Cesare Alippi and Yoonsuck Choe and Francesco Carlo Morabito},
booktitle = {Artificial Intelligence in the Age of Neural Networks and Brain Computing (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {325-341},
year = {2024},
isbn = {978-0-323-96104-2},
doi = {https://doi.org/10.1016/B978-0-323-96104-2.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323961042000087},
author = {Qinbo Li and Yoonsuck Choe},
keywords = {Tool use, Tool construction, Reinforcement learning, Sensorimotor learning},
abstract = {Tool use and tool construction are important indicators of intelligence and high-level cognition. Various animals have been found to use rudimentary tools, and in limited cases tool construction has been observed. However, humans are by far the most advanced in terms of tool construction and use. Considering the potential impact tools have had on the enhancement of human intelligence and vice versa, understanding the nature of this kind of brain-tool interaction can provide us with key insights on how to build self-improving AI. In this chapter, as a first step, we investigated primitive tool construction and use using deep hierarchical reinforcement learning. The results show that with minimal reward shaping, an agent can learn to construct and use a simple tool. These are primitive results, but we hope they can serve as a steppingstone toward a full-blown tool-intelligence coevolution for open-ended AI.}
}
@article{MODARES2016334,
title = {Optimal model-free output synchronization of heterogeneous systems using off-policy reinforcement learning},
journal = {Automatica},
volume = {71},
pages = {334-341},
year = {2016},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2016.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0005109816302023},
author = {Hamidreza Modares and Subramanya P. Nageshrao and Gabriel A. Delgado Lopes and Robert Babuška and Frank L. Lewis},
keywords = {Output synchronization, Heterogeneous systems, Reinforcement learning, Leader–follower systems},
abstract = {This paper considers optimal output synchronization of heterogeneous linear multi-agent systems. Standard approaches to output synchronization of heterogeneous systems require either the solution of the output regulator equations or the incorporation of a p-copy of the leader’s dynamics in the controller of each agent. By contrast, in this paper neither one is needed. Moreover, here both the leader’s and the follower’s dynamics are assumed to be unknown. First, a distributed adaptive observer is designed to estimate the leader’s state for each agent. The output synchronization problem is then formulated as an optimal control problem and a novel model-free off-policy reinforcement learning algorithm is developed to solve the optimal output synchronization problem online in real time. It is shown that this optimal distributed approach implicitly solves the output regulation equations without actually doing so. Simulation results are provided to verify the effectiveness of the proposed approach.}
}
@article{JIANG20096520,
title = {Case-based reinforcement learning for dynamic inventory control in a multi-agent supply-chain system},
journal = {Expert Systems with Applications},
volume = {36},
number = {3, Part 2},
pages = {6520-6526},
year = {2009},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2008.07.036},
url = {https://www.sciencedirect.com/science/article/pii/S0957417408005034},
author = {Chengzhi Jiang and Zhaohan Sheng},
keywords = {Inventory control, Reinforcement learning, Supply-chain management, Multi-agent simulation},
abstract = {Reinforcement learning (RL) appeals to many researchers in recent years because of its generality. It is an approach to machine intelligence that learns to achieve the given goal by trial-and-error iterations with its environment. This paper proposes a case-based reinforcement learning algorithm (CRL) for dynamic inventory control in a multi-agent supply-chain system. Traditional time-triggered and event-triggered ordering policies remain popular because they are easy to implement. But in the dynamic environment, the results of them may become inaccurate causing excessive inventory (cost) or shortage. Under the condition of nonstationary customer demand, the S value of (T, S) and (Q, S) inventory review method is learnt using the proposed algorithm for satisfying target service level, respectively. Multi-agent simulation of a simplified two-echelon supply chain, where proposed algorithm is implemented, is run for a few times. The results show the effectiveness of CRL in both review methods. We also consider a framework for general learning method based on proposed one, which may be helpful in all aspects of supply-chain management (SCM). Hence, it is suggested that well-designed ‘‘connections” are necessary to be built between CRL, multi-agent system (MAS) and SCM.}
}
@article{PYLOROF2022105454,
title = {A reinforcement learning approach to long-horizon operations, health, and maintenance supervisory control of advanced energy systems},
journal = {Engineering Applications of Artificial Intelligence},
volume = {116},
pages = {105454},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105454},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622004444},
author = {Dimitrios Pylorof and Humberto E. Garcia},
keywords = {Supervisory control, Reinforcement learning, Maintenance management, Energy systems, Nuclear reactors},
abstract = {We develop a Reinforcement Learning (RL) approach to the supervisory control problem for advanced energy systems, such as novel nuclear reactors and other demand-driven, mission-critical, and component-health-sensitive energy plants. The inclusive problem landscape considered captures the stochastic confluence of plant performance, component health evolution, power demand from the grid, diverse maintenance actions, and operator-defined goals and constraints, all considered over meaningfully long-enough reasoning horizons. Key aspects of the proposed approach are a receding horizon control-inspired technique dictating time- or event-triggered supervisory policy (re-)constructions, as well as additional capability-enabling contributions such as timescale compression, to handle long reasoning horizons and uncertainty in parts of the problem, and practical yet demonstrably-effective handling of hybrid action spaces with continuous and discrete decision variables. The resulting algorithm consists of a simulation-based RL agent constructing stochastic supervisory control policies over nontrivial action spaces and for long horizons, applying the learned policy to the system for a much shorter interval, and perpetually repeating, to construct the next long-horizon policy. That next policy will only be applied, again, for a short interval, yet originally far-in-time events move progressively closer, their associated uncertainty decreases, and new events and aspects enter the reasoning horizon. The proposed methodology bridges fundamental receding horizon concepts with the unequivocally stronger and more scalable reasoning of contemporary RL. Numerical examples using Soft Actor–Critic Deep RL illustrate the operation and efficacy of the proposed technique for a power plant tasked with health-aware load following missions in a dynamic electricity market landscape.}
}
@article{TANG2023102547,
title = {Assisted driving system based on federated reinforcement learning},
journal = {Displays},
volume = {80},
pages = {102547},
year = {2023},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2023.102547},
url = {https://www.sciencedirect.com/science/article/pii/S0141938223001804},
author = {Xiaolan Tang and Yuting Liang and Guan Wang and Wenlong Chen},
keywords = {Federated learning, Reinforcement learning, Privacy protection, Driving},
abstract = {For the visually impaired persons, the partial loss of vision brings some challenges to vehicle driving. How to assist the visually impaired to safely control the vehicle is an important issue. Reinforcement learning is used to train the driving control models on the cloud server based on the data collected from several vehicles. However, the images gathered by the camera on a vehicle imply the privacy of the drivers and passengers. In order to balance the model accuracy and the privacy protection, we design an assisted driving system based on federated reinforcement learning, called DFRL, which supports the secure model aggregation among several vehicles. Each vehicle trains the local model through the deep deterministic policy gradient algorithm, and transmits the encrypted weights to the server. Then, the server takes step-based federated averaging to obtain the global model, and distributes it to each participant. Finally, each vehicle decrypts the model and updates its local model accordingly. In this way, only the encrypted parameters rather than original data are shared between the vehicles and the cloud server, which stops the attackers to recover the data with privacy. The experiments on TORCS show that, DFRL improves the accuracy of driving control, while providing privacy protection for the users.}
}
@article{XU2020107704,
title = {Intelligent collision avoidance algorithms for USVs via deep reinforcement learning under COLREGs},
journal = {Ocean Engineering},
volume = {217},
pages = {107704},
year = {2020},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2020.107704},
url = {https://www.sciencedirect.com/science/article/pii/S0029801820306934},
author = {Xinli Xu and Yu Lu and Xiaocheng Liu and Weidong Zhang},
keywords = {Unmanned surface vehicles, Dynamic collision avoidance, Deep learning, Reinforcement learning, COLREGs},
abstract = {In the field of unmanned surface vehicles, intelligent collision avoidance technology is essential to ensure the safety of navigating. In this paper, the problem of avoiding moving boats for USVs under the constraints of COLREGs is studied. A COLREGs intelligent collision avoidance (CICA) algorithm based on deep reinforcement learning is proposed, which can automatically extract state features by using powerful deep neural networks. The reward function is designed, which ensures that the USV navigates to the target while obeying COLREGs to avoid dynamic obstacles. A method is proposed to track the current network weight to update the target network weight, which improves the stability of the algorithm in learning the optimal strategy. It is shown that the CICA algorithm converges with fewer training times through ε-greedy with both decaying ε and reward threshold than other three strategies. By comparing the CICA algorithm with the artificial potential field method and the velocity obstacle method, it is concluded that the CICA algorithm is superior to the other two algorithms.}
}
@article{LI2021171,
title = {Output event-triggered tracking synchronization of heterogeneous systems on directed digraph via model-free reinforcement learning},
journal = {Information Sciences},
volume = {559},
pages = {171-190},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.01.056},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521000943},
author = {Qing Li and Lina Xia and Ruizhuo Song and Lu Liu},
keywords = {Event-triggered, Heterogeneous MASs, Directed digraph, Optimal controller, Reinforcement learning},
abstract = {In this study, an event-triggered controller for the output tracking synchronization problem in heterogeneous multi-agent systems on a directed digraph is developed. Initially, the design of the distributed observer is proposed to estimate the information of the leader. Owing to the communication among agents, there exists a coupling behavior in the design of controllers for followers. The separation theorem is exploited to realize decoupling between agents. Then, an optimal conventional controller and event-triggered controller are designed to achieve the synchronization service of the output tracking problem for agents; the latter also improves the efficiency precisely by deliberately aperiodic sampling. Additionally, we prove that the inter-event time interval is strictly positive in the event-triggered controller; that is, the Zeno behavior can be avoided. Furthermore, the optimal control gain is obtained in a model-free manner. More specifically, a model-free reinforcement learning algorithm is investigated to learn the control gain, which does not require the followers’ dynamics. Finally, two simulation examples show the availability of the algorithm.}
}
@article{ODEKUNLE2020108672,
title = {Reinforcement learning and non-zero-sum game output regulation for multi-player linear uncertain systems},
journal = {Automatica},
volume = {112},
pages = {108672},
year = {2020},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2019.108672},
url = {https://www.sciencedirect.com/science/article/pii/S0005109819305357},
author = {Adedapo Odekunle and Weinan Gao and Masoud Davari and Zhong-Ping Jiang},
keywords = {Reinforcement learning (RL), Adaptive optimal control, Game theory, Output regulation, Data-Driven control},
abstract = {This paper studies the non-zero-sum game output regulation problem (GORP) for a class of continuous-time multi-player linear systems. Without the knowledge of state and input matrices, the Nash equilibrium solution, N-tuple of feedback control policy, is learned through online data collected along the system trajectories. A key strategy is, for the first time, to combine techniques from reinforcement learning (RL), differential game theory, and output regulation for data-driven control design. Different from the existing literature of adaptive optimal output regulation, the feedforward matrices are considered nontrivial. Theoretical analysis shows the disturbance rejection and tracking ability of the closed-loop system. Simulation results demonstrate the efficacy of the developed data-driven control approach.}
}
@article{LI2023119577,
title = {Output-feedback optimized consensus for directed graph multi-agent systems based on reinforcement learning and subsystem error derivatives},
journal = {Information Sciences},
volume = {649},
pages = {119577},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119577},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523011623},
author = {Dongdong Li and Jiuxiang Dong},
keywords = {Output-feedback consensus, Reinforcement learning, Optimized backstepping control, Neural network, State observer},
abstract = {A distributed output-feedback optimal tracking control (OTC) method based on reinforcement learning (RL) is proposed for state-unmeasured multi-agent systems (MASs) under a directed graph. Firstly, the state observers are designed to estimate the states of MASs using the output signals, and the gains of the observers are not required to satisfy the Hurwitz matrix inequality. Then, a class of value functions based on error derivatives is proposed to solve the unbounded problem of traditional value functions for strict-feedback MASs. By using the value functions, the Hamilton-Jacobi Bellman (HJB) equations and the optimal control inputs are derived. The traditional RL-based backstepping control (OBC) methods are difficult to deal with the optimized consensus problem under a directed digraph, it is solved by using the observer-actor-critic structures and the new value functions in this paper, and online learning is implemented. Furthermore, there is no “dimensional pressure” to approximate the optimal control inputs and value functions using actor-critic neural networks (NNs), and the observed error and the consensus error are shown to be bounded. Finally, the effectiveness and advantages of the algorithm are verified by simulation.}
}
@article{YUAN2019107,
title = {A novel multi-step Q-learning method to improve data efficiency for deep reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {175},
pages = {107-117},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119301431},
author = {Yinlong Yuan and Zhu Liang Yu and Zhenghui Gu and Yao Yeboah and Wu Wei and Xiaoyan Deng and Jingcong Li and Yuanqing Li},
keywords = {Deep reinforcement learning, Robotics, Multi-step methods, Data efficiency},
abstract = {Deep reinforcement learning (DRL) algorithms with experience replays have been used to solve many sequential learning problems. However, in practice, DRL algorithms still suffer from the data inefficiency problem, which limits their applicability in many scenarios, and renders them inefficient in solving real-world problems. To improve the data efficiency of DRL, in this paper, a new multi-step method is proposed. Unlike traditional algorithms, the proposed method uses a new return function, which alters the discount of future rewards while decreasing the impact of the immediate reward when selecting the current state action. This approach has the potential to improve the efficiency of reward data. By combining the proposed method with classic DRL algorithms, deep Q-networks (DQN) and double deep Q-networks (DDQN), two novel algorithms are proposed for improving the efficiency of learning from experience replay. The performance of the proposed algorithms, expected n-step DQN (EnDQN) and expected n-step DDQN (EnDDQN), are validated using two simulation environments, CartPole and DeepTraffic. The experimental results demonstrate that the proposed multi-step methods greatly improve the data efficiency of DRL agents while further improving the performance of existing classic DRL algorithms when incorporated into their training.}
}
@article{BOHN2021314,
title = {Reinforcement Learning of the Prediction Horizon in Model Predictive Control},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {6},
pages = {314-320},
year = {2021},
note = {7th IFAC Conference on Nonlinear Model Predictive Control NMPC 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.563},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321013380},
author = {Eivind Bøhn and Sebastien Gros and Signe Moe and Tor Arne Johansen},
keywords = {Adaptive horizon model predictive control, Reinforcement learning control},
abstract = {Model predictive control (MPC) is a powerful trajectory optimization control technique capable of controlling complex nonlinear systems while respecting system constraints and ensuring safe operation. The MPC’s capabilities come at the cost of a high online computational complexity, the requirement of an accurate model of the system dynamics, and the necessity of tuning its parameters to the specific control application. The main tunable parameter affecting the computational complexity is the prediction horizon length, controlling how far into the future the MPC predicts the system response and thus evaluates the optimality of its computed trajectory. A longer horizon generally increases the control performance, but requires an increasingly powerful computing platform, excluding certain control applications. The performance sensitivity to the prediction horizon length varies over the state space, and this motivated adaptive horizon model predictive control (AHMPC), which adapts the prediction horizon according to some criteria. In this paper we propose to learn the optimal prediction horizon as a function of the state using reinforcement learning (RL). We show how the RL learning problem can be formulated and test our method on two control tasks — showing clear improvements over the fixed horizon MPC scheme — while requiring only minutes of learning.}
}
@article{GAN20202501,
title = {Reinforcement Learning Based Anti-Jamming Schedule in Cyber-Physical Systems},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {2501-2506},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.221},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320304924},
author = {Ruimeng Gan and Yue Xiao and Jinliang Shao and Heng Zhang and Wei {Xing Zheng}},
keywords = {Cyber-physical systems (CPSs), reinforcement learning, cognitive radio, softmax method},
abstract = {In this paper, the security issue of cyber-physical systems is investigated, where the observation data is transmitted from a sensor to an estimator through wireless channels disturbed by an attacker. The failure of this data transmission occurs, when the sensor accesses the channel that happens to be attacked by the jammer. Since the system performance measured by the estimation error depends on whether the data transmission is a success, the problem of selecting the channel to alleviate the attack effect is studied. Moreover, the state of each channel is time-variant due to various factors, such as path loss and shadowing. Motivated by energy conservation, the problem of selecting the channel with the best state is also considered. With the help of cognitive radio technique, the sensor has the ability of selecting a sequence of channels dynamically. Based on this, the problem of selecting the channel is resolved by means of reinforcement learning to jointly avoid the attack and enjoy the channel with the best state. A corresponding algorithm is presented to obtain the sequence of channels for the sensor, and its effectiveness is proved analytically. Numerical simulations further verify the derived results.}
}
@article{DESHPANDE202190,
title = {Robust Deep Reinforcement Learning for Quadcopter Control},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {20},
pages = {90-95},
year = {2021},
note = {Modeling, Estimation and Control Conference MECC 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.11.158},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321022023},
author = {Aditya M. Deshpande and Ali A. Minai and Manish Kumar},
keywords = {Reinforcement learning control, Robust adaptive control, Robotics, Flying robots},
abstract = {Deep reinforcement learning (RL) has made it possible to solve complex robotics problems using neural networks as function approximators. However, the policies trained on stationary environments suffer in terms of generalization when transferred from one environment to another. In this work, we use Robust Markov Decision Processes (RMDP) to train the drone control policy, which combines ideas from Robust Control and RL. It opts for pessimistic optimization to handle potential gaps between policy transfer from one environment to another. The trained control policy is tested on the task of quadcopter positional control. RL agents were trained in a MuJoCo simulator. During testing, different environment parameters (unseen during the training) were used to validate the robustness of the trained policy for transfer from one environment to another. The robust policy outperformed the standard agents in these environments, suggesting that the added robustness increases generality and can adapt to nonstationary environments.}
}
@article{CHU20213437,
title = {Energy saving of fans in air-cooled server via deep reinforcement learning algorithm},
journal = {Energy Reports},
volume = {7},
pages = {3437-3448},
year = {2021},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S2352484721003607},
author = {Wen-Xiao Chu and Yun-Hsuan Lien and Kuei-Ru Huang and Chi-Chuan Wang},
keywords = {Deep reinforcement learning, Simulated server, Fan control, Energy saving},
abstract = {The present paper aims at using an artificial intelligence algorithm to minimize the fan power consumption in air-cooled servers. The proposed algorithm can handle the complex thermal environments within the servers to tailor the influences and interactions amid numerous heat sources, airflow, bypass phenomenon, fan operation, and the transient operations. Modified correlations are first proposed to effectively predict the thermal-hydraulic performance of heat sinks and the corresponding predictive ability against Nusselt number and pressure drop is within 5.0% and 10%, respectively. Without the algorithm control, the maximum deviation between the prediction and the experimental data is within 2.0 °C. By introducing the deep reinforcement learning (DRL) algorithm subject to the interactions of complex thermal environments, the fan power consumption can be saved by 55.7%, 40.3% and 26.3%, respectively, in comparison with the strategy with 100% fan duty. Yet the DRL agent still offers 16.7% energy saving when compared to a fixed 40% fan duty.}
}
@article{MCKENZIE202311,
title = {Hyperparameter Selection in Reinforcement Learning Using the “Design of Experiments” Method},
journal = {Procedia Computer Science},
volume = {222},
pages = {11-24},
year = {2023},
note = {International Neural Network Society Workshop on Deep Learning Innovations and Applications (INNS DLIA 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.08.140},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923009055},
author = {Mark C. McKenzie and Mark D. McDonnell},
keywords = {Reinforcement Learning, Hyperparameter Selection, Design of Experiments},
abstract = {Artificial Intelligence and Machine Learning is a highly active area of research across numerous subgenres. One such example is Reinforcement Learning, which relies on trial and error based sampling of the environment to train an agent at completing a given task, commonly applied to Atari 2600 games within research applications. Many variants of Reinforcement Learning algorithms exist, all of which apply numerous hyperparameters to control the learning process in some way, from strength of backpropagation updates through to rates of exploration. The breadth of choice across these hyperparameters makes optimal training a challenging task, with no feedback given until significant time has lapsed in training. What is of interest here is the relative importance across these hyperparameters as well as any relationship amongst them at deriving high performance agents. Through this research we apply the common statistical approach of Design of Experiments to the task of understanding the state space of the numerous hyperparameters present in Reinforcement Learning algorithms such as the Double Deep Q-Network and Prioritized Experience Replay methods. We identify the learning rate as the only primary contributor of success or failure of value based Reinforcement Learning approaches to achieve optimal reward gain. This finding suggests the possibility of significantly reduced effort and time for considering the effects of non-dependent hyperparameters.}
}
@article{CHEN20221152,
title = {Fault-tolerant tracking control based on reinforcement learning with application to a steer-by-wire system},
journal = {Journal of the Franklin Institute},
volume = {359},
number = {3},
pages = {1152-1171},
year = {2022},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2021.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0016003221007377},
author = {Huan Chen and Yidong Tu and Hai Wang and Kaibo Shi and Shuping He},
abstract = {In this paper, a novel complete model-free integral reinforcement learning (CMFIRL) algorithm based fault tolerant control scheme is proposed to solve the tracking problem of steer-by-wire (SBW) system. We begin with the recognition that the reference errors can eventually converge to zero based on the command generator model. Then an augmented tracking system is constructed with a corresponding performance index which is considered as a type of actuator failure. By using the reinforcement learning (RL) technique, three novel online update strategies are respectively developed to cope with the following three cases, i.e., model-based, partially model-free, and completely model-free. Especially, the RL algorithm for the complete model-free case eliminates the constraints of requiring the known system dynamics in fault-tolerant tracking controlling. The system stability and the convergence of the CMFIRL iteration algorithm are also rigorously proved. Finally, a simulation example is given to illustrate the effectiveness of the proposed approach.}
}
@article{BRAMMER202275,
title = {Permutation flow shop scheduling with multiple lines and demand plans using reinforcement learning},
journal = {European Journal of Operational Research},
volume = {299},
number = {1},
pages = {75-86},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2021.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0377221721006743},
author = {Janis Brammer and Bernhard Lutz and Dirk Neumann},
keywords = {Scheduling, Permutation flow shop problem, Reinforcement learning, Mixed-integer programming, Constraint programming},
abstract = {Existing studies on the permutation flow shop problem (PFSP) commonly assume that jobs are produced on a single line. However, manufacturers may speed up their production by employing multiple lines, where each line produces sub-parts of the final product; which must be assembled by a synchronization machine. This study presents a novel reinforcement learning (RL) approach for the PFSP with multiple lines and demand plans. Our approach differs from existing RL-based scheduling methods as we train the policy to directly generate the sequence in an iterative way, where actions denote the job type to be sequenced next. During cutoff time, we follow a multistart approach that generates sequences with the trained policy, which are subsequently optimized by local search. Our numerical evaluation based on 1050 problem instances with up to three production lines shows that our approach outperforms existing methods on the multi-line problems for short cutoff times, while there is a tie with existing methods for medium and long cutoff times. A further analysis suggests that our approach can also be applied to problems with imbalanced demand plans.}
}
@article{SAVAGE2021504,
title = {Model-free safe reinforcement learning for chemical processes using Gaussian processes},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {3},
pages = {504-509},
year = {2021},
note = {16th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.292},
url = {https://www.sciencedirect.com/science/article/pii/S240589632101065X},
author = {Thomas Savage and Dongda Zhang and Max Mowbray and Ehecatl Antonio Del {Río Chanona}},
keywords = {Batch Processes, Modelling, Identification, Scheduling, Optimization},
abstract = {Model-free reinforcement learning has been recently investigated for use in chemical process control. Through the iterative creation of an approximate process model, control actions are able to be explored and optimal policies generated. Typically, this approximate process model has taken the form of a neural network that is continuously updated. However when small quantities of historical data are available, for example in novel processes, neural networks tend to over-fit to data providing poor performance. In this paper Gaussian processes are used as a method of function approximation to describe the action-value function of a non-isothermal semi-batch reactor. Through the use of analytical uncertainty obtained from Gaussian process predictions, trade off between exploration and exploitation is enabled, allowing for efficient generation of effective policies. Importantly Gaussian processes also enable probabilistic constraint violation to be modelled, ensuring safe constraint satisfaction throughout the learning procedure. On application to the in-silico case study, a safe, effective policy was generated utilising only 100 evaluations of process trajectory with no prior knowledge of the process dynamics. A result that would require significantly more trajectory evaluations when compared to a neural network based approach.}
}
@article{KOU2023106977,
title = {Dynamic robust analysis of IoV link delay in cellular Telematics and smart edge networking base on deep reinforcement learning},
journal = {Results in Physics},
volume = {53},
pages = {106977},
year = {2023},
issn = {2211-3797},
doi = {https://doi.org/10.1016/j.rinp.2023.106977},
url = {https://www.sciencedirect.com/science/article/pii/S2211379723007702},
author = {Aijun Kou and Xiaojun Li and Zhiwen Zou},
keywords = {TSP server attacks, IoV, Dynamic Bayesian network, Error rate},
abstract = {Applications for the Internet of Vehicles (IoV) are progressing from more fundamental ones like those for traffic efficiency, road safety, and information services to more sophisticated ones like autonomous driving and intelligent mobility that have a variety of connectivity requirements. Therefore, IoV communication systems must overcome various obstacles in order to enable a variety of IoV applications. Attacks on TSP servers are the major target of telematics attack threat events. This article analyses data security monitoring from the vehicle end to the TSP platform, as well as gathering and analysing the status and intelligence of linked vehicles, in order to develop a Telematics TSP server attack monitoring system. Experiments show that the error detection rate of the benchmark algorithms decreases significantly as the error rate of vehicle speed data increases, with the exponential smoothing algorithm showing the largest decrease in the error detection rate. However, the proposed combined model is very close to the error detection rate of the benchmark algorithm at the later stages when the error rate is high, with an error detection rate of more than 80%.It provides great assistance for anomaly detection and data recovery, especially in researching key technologies for remote information processing data security and reliability, greatly improving their performance.}
}
@article{ZHANG2019344,
title = {Tracking control optimization scheme for a class of partially unknown fuzzy systems by using integral reinforcement learning architecture},
journal = {Applied Mathematics and Computation},
volume = {359},
pages = {344-356},
year = {2019},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2019.04.084},
url = {https://www.sciencedirect.com/science/article/pii/S0096300319303868},
author = {Kun Zhang and Huaguang Zhang and Yunfei Mu and Shaoxin Sun},
keywords = {Fuzzy control, Tracking control, Integral reinforcement learning, T-S fuzzy models, Adaptive dynamic programming},
abstract = {In this paper, a novel fuzzy integral reinforcement learning (RL) based tracking control algorithm is first proposed for partially unknown fuzzy systems. Firstly, by using the precompensation and augmentation techniques, a new augmented fuzzy tracking system is constructed by combining the fuzzy logic model and desired reference trajectory, where the solution of actual working feedback control policy is converted into a virtual optimal control problem. Secondly, to overcome the requirements of exact original system information, the integral RL technique is utilized to learn the fuzzy control solution, which relaxes the repeatedly transmissions of system matrices during the solving process. Thirdly, compared with the existing standard solution, some crucial and strict aforementioned assumptions are removed and the system can be partially unknown by using the designed algorithm. Besides, under the novel fuzzy control policy, the tracking objective is achieved and the stability is guaranteed by Lyapunov theory. Finally, the developed integral RL tracking control algorithm for partially unknown systems is applied in a mechanical system and the simulation results demonstrate the effectiveness of the proposed new method.}
}
@article{SCHERRER2005229,
title = {Asynchronous neurocomputing for optimal control and reinforcement learning with large state spaces},
journal = {Neurocomputing},
volume = {63},
pages = {229-251},
year = {2005},
note = {New Aspects in Neurocomputing: 11th European Symposium on Artificial Neural Networks},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2004.01.192},
url = {https://www.sciencedirect.com/science/article/pii/S0925231204003248},
author = {Bruno Scherrer},
keywords = {Neurocomputing, Optimal control, Reinforcement learning},
abstract = {We consider two machine learning related problems, optimal control and reinforcement learning. We show that, even when their state space is very large (possibly infinite), natural algorithmic solutions can be implemented in an asynchronous neurocomputing way, that is by an assembly of interconnected simple neuron-like units which does not require any synchronization. From a neuroscience perspective, this work might help understanding how an asynchronous assembly of simple units can give rise to efficient control. From a computational point of view, such neurocomputing architectures can exploit their massively parallel structure and be significantly faster than standard sequential approaches. The contributions of this paper are the following: (1) We introduce a theoretically sound methodology for designing a whole class of asynchronous neurocomputing algorithms. (2) We build an original asynchronous neurocomputing architecture for optimal control in a small state space, then we show how to improve this architecture so that also solves the reinforcement learning problem. (3) Finally, we show how to extend this architecture to address the case where the state space is large (possibly infinite) by using an asynchronous neurocomputing adaptive approximation scheme. We illustrate this approximation scheme on two continuous space control problems.}
}
@article{DUBEY2023,
title = {Optimal path selection using reinforcement learning based ant colony optimization algorithm in IoT-Based wireless sensor networks with 5G technology},
journal = {Computer Communications},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2023.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0140366423003250},
author = {Ghanshyam Prasad Dubey and Shalini Stalin and Omar Alqahtani and Areej Alasiry and Madhu Sharma and Aliya Aleryani and Piyush Kumar Shukla and M. Turki-Hadj Alouane},
keywords = {Internet of things (IoTs), Wireless sensor networks (WSN), Proximal policy optimization (PPO), Ant colony optimization (ACO), Reinforcement learning (RL), Optimal path selection},
abstract = {The Internet of Things (IoTs) expanded quickly, giving rise to numerous services, apps, electronic devices with integrated sensors, and associated protocols, which are still being developed today. By enabling physical objects to communicate with each other and share important information while making decisions and carrying out their essential jobs, the IoTs enable them to see, hear, think, and execute crucial tasks. Wireless sensor networks (WSN), which act as the IoT's permanent layer, are essential for fifth-generation (5G) communications, which need the IoT to be considerably helped. A WSN comprises many sensor nodes that track and transmit data to the sink. Every round's data transmission ends at the sink (or base station). This work presents a Proximal Policy Optimization based Ant Colony Optimization (PPO-ACO) algorithm for optimal path selection in WSN. The proposed algorithm combines the strengths of both PPO with a reinforcement learning (RL) method, and ACO, a swarm intelligence method, to address the stochastic nature of the network and the complex trade-off between energy efficiency and security. The PPO component learns the policy for path selection based on the sampled rewards, while the ACO component updates the pheromone levels to guide the search toward the optimal path. Compared to the state-of-the-art, our simulation findings show that the suggested PPO-ACO algorithm performs better in terms of the number of active nodes. The average residual energy of the suggested algorithm decreases later than existing algorithms, indicating its higher efficiency.}
}
@article{WU2023106401,
title = {A spatial pyramid pooling-based deep reinforcement learning model for dynamic job-shop scheduling problem},
journal = {Computers & Operations Research},
volume = {160},
pages = {106401},
year = {2023},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2023.106401},
url = {https://www.sciencedirect.com/science/article/pii/S0305054823002654},
author = {Xinquan Wu and Xuefeng Yan},
keywords = {Dynamic job shop scheduling problem, Deep reinforcement learning, Spatial pyramid pooling networks, PPO, Random job arrivals},
abstract = {The dynamic job-shop scheduling problem (DJSP) is a typical of scheduling tasks where rescheduling is performed when encountering unexpected events such as random job arrivals and rush order. However, the current rescheduling approaches cannot reuse the trained scheduling policies or the experiences due to the variant size of scheduling problems. In this paper, we propose a deep reinforcement learning (DRL) scheduling model for DJSP based on spatial pyramid pooling networks (SPP-Net). A new state representation is proposed based on the machine matrix and remaining time matrix which is decomposed from the scheduling instance matrix. And a new reward function is derived from the area of total scheduling time where the accumulated reward is negatively linearly dependent with the make-span of a scheduling task. Moreover, a size-agnostic scheduling policy is designed based on the SPP-Net and SoftMax function, which is trained by the proximal policy optimization (PPO). Besides, various paired priority dispatching rules (PDR) are used as available actions. Static experiments on classic benchmark instances show that our scheduling model achieves better results on average than existing DRL methods. In addition, dynamic scheduling experiments are tested and our model obtains better results than the PDR scheduling methods in reasonable time when encountering unexpected events such as random job arrivals and rush order.}
}
@article{PENG2021107321,
title = {Reinforcement learning with Gaussian processes for condition-based maintenance},
journal = {Computers & Industrial Engineering},
volume = {158},
pages = {107321},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107321},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221002254},
author = {Shenglin Peng and Qianmei (May) Feng},
keywords = {Condition-based maintenance, Reinforcement learning, Gaussian process regression, Markov decision process, Gaussian processes for reinforcement learning, Function approximation},
abstract = {Condition-based maintenance strategies are effective in enhancing reliability and safety for complex engineering systems that exhibit degradation phenomena with uncertainty. Such sequential decision-making problems are often modeled as Markov decision processes (MDPs) when the underlying process has a Markov property. Recently, reinforcement learning (RL) becomes increasingly efficient to address MDP problems with large state spaces. In this paper, we model the condition-based maintenance problem as a discrete-time continuous-state MDP without discretizing the deterioration condition of the system. The Gaussian process regression is used as function approximation to model the state transition and the value functions of states in reinforcement learning. A RL algorithm is then developed to minimize the long-run average cost (instead of the commonly-used discounted reward) with iterations on the state-action value function and the state value function, respectively. We verify the capability of the proposed algorithm by simulation experiments and demonstrate its advantages in a case study on a battery maintenance decision-making problem. The proposed algorithm outperforms the discrete MDP approach by achieving lower long-run average costs.}
}
@article{CHEN2023120801,
title = {Balancing exploration and exploitation in episodic reinforcement learning},
journal = {Expert Systems with Applications},
volume = {231},
pages = {120801},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120801},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423013039},
author = {Qihang Chen and Qiwei Zhang and Yunlong Liu},
keywords = {Reinforcement learning, Episodic tasks, Sparse and delayed rewards, Exploration and exploitation, Entropy-based intrinsic incentives},
abstract = {One of the major challenges in reinforcement learning (RL) is its applications in episodic tasks, such as chess game, molecular structure design, healthcare, among others, where the rewards in such scenarios are usually sparse and can only be obtained at the end of an episode. The challenges posed by such episodic RL tasks place stringent demands on the exploration and credit assignment capabilities of the agent. In the current literature, many techniques have been presented to address these two issues, for example, various exploration methods have been proposed to increase the exploration ability of the agents to obtain diverse experience samples, and for the delayed reward problem, reward redistribution methods have provided dense task-oriented guidance to the agents by reshaping the sparse and delayed environmental rewards with the assistance of the episodic feedback. Although some successes have been achieved, with current existing techniques, the agents are usually unable to quickly assign credits to the explored key transitions or the related methods are prone to be misled by behavioral policies that fall into local optima and lead to sluggish learning efficiency. To alleviate inefficient learning due to sparse and delayed rewards, we propose a guided reward approach, namely Exploratory Intrinsic with Mission Guidance Reward (EMR), which organically combines intrinsic rewards of exploration mechanisms with reward redistribution in RL to balance exploration and exploitation of RL agents in such tasks. By using entropy-based intrinsic incentives and a simple uniform reward redistribution method, EMR will enable an agent with both the strong exploration and exploitation capability to efficiently overcome challenging tasks with such sparse and delayed rewards. We evaluated and analyzed EMR on several tasks in the Deep Mind Control Suite benchmark, experimental results show that the EMR-equipped agent has faster learning efficiency and even better performance than those using the exploration bonus or the reward redistribution method alone.}
}
@article{BOADA2001173,
title = {Visual Tracking Skill Reinforcement Learning for a Mobile Robot},
journal = {IFAC Proceedings Volumes},
volume = {34},
number = {19},
pages = {173-178},
year = {2001},
note = {4th IFAC Symposium on Intelligent Autonomous Vehicles (IAV 2001), Sapporo, Japan, 5-7 September 2001},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)33132-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017331324},
author = {M.J.L. Boada and M.A. Salichs},
keywords = {Reinforcement learning, visual tracking, skill learning, neural network, mobile robotics, intelligent autonomous vehicle},
abstract = {In this paper.we implement a reinforcement learning algorithm that allows an autonomous mobile robot.with a single CCD camera mounted on a pan-tilt platform, to learn simple skills such as watch and orientation, and to obtain the skill called approach combining the learned skills previously.The results obtained show the advantages of I) decomposing complex skills in simpler skills due to the learning rate improves.2) using the reinforcement as a learning mechanism because of the possibility to learn on-line without the robot having a previous knowledge of what it has to do for a given situation.We also present the neural network architecture used for the learning mechanism implementation.}
}
@incollection{WU2023457,
title = {Real-time optimization of a chemical plant with continuous flow reactors via reinforcement learning},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {457-462},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50073-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152740500731},
author = {Min Wu and Furkan Elmaz and Ulderico Di Caprio and Dries De Clercq and Siegfried Mercelis and Peter Hellinckx and Leen Braeken and Florence Vermeire and M. Enis Leblebici},
keywords = {reinforcement learning, real-time optimization, continuous flow reactors, continuous action space},
abstract = {Reinforcement learning (RL) has many new applications in recent years, and its results often exceed human performance, especially in environments where the action space is discrete. However, it is challenging to use RL in the chemical industry, where variables are often continuous and various constraints are complex. This study applies RL with continuous actions to maximize the productivity of a continuous process. The RL agent provides optimal setpoints of flow rates and temperatures while the concentrations of raw materials are changing. Two environments with one and six actions were established after the sensitivity analysis. In the one-action environment, the agents SAC, PPO and A2C showed similar performances, but A2C needed fewer timesteps for training. SAC outperforms PPO and A2C in the environment with six actions. This paper shows the successful RL applications in a continuous process and the high applicability of SAC in both low-dimension and high-dimension environments.}
}
@article{ALMASAN2022184,
title = {Deep reinforcement learning meets graph neural networks: Exploring a routing optimization use case},
journal = {Computer Communications},
volume = {196},
pages = {184-194},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.09.029},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422003784},
author = {Paul Almasan and José Suárez-Varela and Krzysztof Rusek and Pere Barlet-Ros and Albert Cabellos-Aparicio},
keywords = {Graph neural networks, Deep reinforcement learning, Routing, Optimization},
abstract = {Deep Reinforcement Learning (DRL) has shown a dramatic improvement in decision-making and automated control problems. Consequently, DRL represents a promising technique to efficiently solve many relevant optimization problems (e.g., routing) in self-driving networks. However, existing DRL-based solutions applied to networking fail to generalize, which means that they are not able to operate properly when applied to network topologies not observed during training. This lack of generalization capability significantly hinders the deployment of DRL technologies in production networks. This is because state-of-the-art DRL-based networking solutions use standard neural networks (e.g., fully connected, convolutional), which are not suited to learn from information structured as graphs. In this paper, we integrate Graph Neural Networks (GNN) into DRL agents and we design a problem specific action space to enable generalization. GNNs are Deep Learning models inherently designed to generalize over graphs of different sizes and structures. This allows the proposed GNN-based DRL agent to learn and generalize over arbitrary network topologies. We test our DRL+GNN agent in a routing optimization use case in optical networks and evaluate it on 180 and 232 unseen synthetic and real-world network topologies respectively. The results show that the DRL+GNN agent is able to outperform state-of-the-art solutions in topologies never seen during training.}
}
@article{AHMADI20227445,
title = {DQRE-SCnet: A novel hybrid approach for selecting users in Federated Learning with Deep-Q-Reinforcement Learning based on Spectral Clustering},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {9},
pages = {7445-7458},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821002226},
author = {Mohsen Ahmadi and Ali Taghavirashidizadeh and Danial Javaheri and Armin Masoumian and Saeid {Jafarzadeh Ghoushchi} and Yaghoub Pourasad},
keywords = {Federated Learning, Independent heterogeneous data, Deep learning, Reinforcement Learning, Ensemble model, Spectral Clustering},
abstract = {Machine learning models based on sensitive data in the real-world promise advances in areas ranging from medical screening to disease outbreaks, agriculture, industry, defense science, and more. In many applications, learning participant communication rounds benefit from collecting their own private data sets, teaching detailed machine learning models on the real data, and sharing the benefits of using these models. Due to existing privacy and security concerns, most people avoid sensitive data sharing for training. Without each user demonstrating their local data to a central server, Federated Learning allows various parties to train a machine learning algorithm on their shared data jointly. This method of collective privacy learning results in the expense of important communication during training. Most large-scale machine learning applications require decentralized learning based on data sets generated on various devices and places. Such datasets represent an essential obstacle to decentralized learning, as their diverse contexts contribute to significant differences in the delivery of data across devices and locations. Researchers have proposed several ways to achieve data privacy in Federated Learning systems. However, there are still challenges with homogeneous local data. This research’s approach is to select nodes (users) to share their data in Federated Learning for independent data-based equilibrium to improve accuracy, reduce training time, and increase convergence. Therefore, this research presents a combined Deep-Q-Reinforcement Learning Ensemble based on Spectral Clustering called DQRE-SCnet to choose a subset of devices in each communication round. Based on the results, it has been displayed that it is possible to decrease the number of communication rounds needed in Federated Learning. The realized reduction in the communication rounds are 51%, 25%, and 44% on the three datasets MNIST, Fashion MNIST, and CIFAR-10, respectively.}
}
@article{WANG2022128172,
title = {Generating merging strategies for connected autonomous vehicles based on spatiotemporal information extraction module and deep reinforcement learning},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {607},
pages = {128172},
year = {2022},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2022.128172},
url = {https://www.sciencedirect.com/science/article/pii/S0378437122007300},
author = {Shuo Wang and Hideki Fujii and Shinobu Yoshimura},
keywords = {Connected and autonomous vehicle, Mixed traffic flow system, Deep reinforcement learning, Long–short term memory neural network, Graph convolution network, Cooperative merging},
abstract = {A major challenge concerning a mixed traffic flow system, composed of connected autonomous vehicles (CAVs) and human-driven vehicles (HDVs), is how to improve overall efficiency and safety by assigning appropriate control strategies to CAVs. Deep reinforcement learning (DRL) is a promising approach to address this challenge. It enables the joint training of multiple CAVs by fusing CAV sensing information and does not need compliance of HDVs. However, the fusion of CAV sensing information is non-trivial. Traditional DRL models usually fail to take advantage of connectivity among CAVs and time series characteristics of vehicle sensing information, leading to insufficient awareness of the traffic environment. Aimed at tackling these issues, this study proposes a DRL framework named spatiotemporal deep Q network (STDQN), by integrating a double deep Q network (DDQN) and a spatiotemporal information extraction module. A long–short term memory neural network with an attention mechanism (AttenLSTMNN) is leveraged to extract temporal dependencies from vehicle perceptive information. In addition, a graph convolution network (GCN) is employed to model the spatial correlations among vehicles in a local range, as well as the connectivity of multiple CAVs in a global range. Simulation experiments are conducted in an onramp merging scenario, which is one of the most important and commonly seen scenarios in highway or city expressway systems. Experimental results prove that as compared to baseline DRL and rule-based methods, the proposed STDQN can improve the overall traffic efficiency, safety, and driving comfort. The proposed framework is promised to be deployed into real CAVs, to realize cooperative, safe, and efficient autonomous driving.}
}
@article{DAI2022107932,
title = {Enhanced Oblique Decision Tree Enabled Policy Extraction for Deep Reinforcement Learning in Power System Emergency Control},
journal = {Electric Power Systems Research},
volume = {209},
pages = {107932},
year = {2022},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2022.107932},
url = {https://www.sciencedirect.com/science/article/pii/S0378779622001626},
author = {Yuxin Dai and Qimei Chen and Jun Zhang and Xiaohui Wang and Yilin Chen and Tianlu Gao and Peidong Xu and Siyuan Chen and Siyang Liao and Huaiguang Jiang and David Wen-zhong Gao},
keywords = {Policy extraction, Knowledge distillation, Deep reinforcement learning, IGR-WODT, Explainability, Power system emergency control},
abstract = {Deep reinforcement learning (DRL) algorithms have successfully solved many challenging problems in various power system control scenarios. However, their decision-making process is usually regarded as black-boxes. Furthermore, how DRL models interact with human intelligence remains an open problem. Thus, this paper proposes a policy extraction framework to extract a complex DRL model into an explainable policy. This framework includes three parts: 1) DRL training and data generation. We train an agent for a specific control task and generate data, which contains the control policy of the agent. 2) Policy extraction. We propose an information gain rate based weighted oblique decision tree (IGR-WODT) for DRL policy extraction. 3) Policy evaluation. We define three metrics to evaluate the performance of the proposed approach. A case study for the under-voltage load shedding problem shows that the IGR-WODT presents a performance enhancement compared with DRL, weighted oblique decision tree, and univariate decision tree. The proposed policy extraction method could provide an intuitive explanation of the neural network decision-making process to the dispatchers when making final decisions on power grid operation. Also, the resulted rule-based controller could replace the deep neural network-based controller in many field edge devices with limited computing resources, providing comparable performance.}
}
@article{WANG202038,
title = {Discriminative sampling via deep reinforcement learning for kinship verification},
journal = {Pattern Recognition Letters},
volume = {138},
pages = {38-43},
year = {2020},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2020.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167865520302373},
author = {Shiwei Wang and Haibin Yan},
keywords = {Kinship verification, Deep learning, Reinforcement learning, Sampling, Biometrics},
abstract = {In this paper, we propose a discriminative sampling method to select most effective negative samples via deep reinforcement learning for kinship verification. Unlike most existing facial kinship verification methods which focus on extracting effective features with the random sampling strategy, we develop a deep reinforcement learning method to select samples which are more suitable for learning discriminative features, so that the overall performance can be improved. Specifically, our method uses two subnetworks to achieve the kinship verification task: one DQN-based sampling network to filter the negative samples, and one multi-layer convolutional network to verify the kin relationship. Experimental results on the KinFaceW-I and KinFaceW-II datasets show the superiority of our proposed approach over the state-of-the-arts.}
}
@article{HUA2023121526,
title = {Energy management of multi-mode plug-in hybrid electric vehicle using multi-agent deep reinforcement learning},
journal = {Applied Energy},
volume = {348},
pages = {121526},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121526},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923008905},
author = {Min Hua and Cetengfei Zhang and Fanggang Zhang and Zhi Li and Xiaoli Yu and Hongming Xu and Quan Zhou},
keywords = {Multiple-input and multiple-output control, Multi-mode plug-in hybrid electric vehicle, Multi-agent deep reinforcement learning, Deep deterministic policy gradient},
abstract = {The recently emerging multi-mode plug-in hybrid electric vehicle (PHEV) technology is one of the pathways making contributions to decarbonization, and its energy management requires multiple-input and multiple-output (MIMO) control. At the present, the existing methods usually decouple the MIMO control into single-output (MISO) control and can only achieve its local optimal performance. To optimize the multi-mode vehicle globally, this paper studies a MIMO control method for energy management of the multi-mode PHEV based on multi-agent deep reinforcement learning (MADRL). By introducing a relevance ratio, a hand-shaking strategy is proposed to enable two learning agents to work collaboratively under the MADRL framework using the deep deterministic policy gradient (DDPG) algorithm. Unified settings for the DDPG agents are obtained through a sensitivity analysis of the influencing factors to the learning performance. The optimal working mode for the hand-shaking strategy is attained through a parametric study on the relevance ratio. The advantage of the proposed energy management method is demonstrated on a software-in-the-loop testing platform. The result of the study indicates that the learning rate of the DDPG agents is the greatest influencing factor for learning performance. Using the unified DDPG settings and a relevance ratio of 0.2, the proposed MADRL system can save up to 4% energy compared to the single-agent learning system and up to 23.54% energy compared to the conventional rule-based system.}
}
@article{HU2023121227,
title = {An apprenticeship-reinforcement learning scheme based on expert demonstrations for energy management strategy of hybrid electric vehicles},
journal = {Applied Energy},
volume = {342},
pages = {121227},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121227},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923005913},
author = {Dong Hu and Hui Xie and Kang Song and Yuanyuan Zhang and Long Yan},
keywords = {Deep reinforcement learning, Energy management strategy, Hybrid electric vehicles, Apprenticeship learning, Meta-learning},
abstract = {Deep reinforcement learning (DRL) is a potential solution to develop efficient energy management strategies (EMS) for hybrid electric vehicles (HEV) that can adapt to the changing topology of electrified powertrains and the uncertainty of various driving scenarios. However, traditional DRL has many disadvantages, such as low efficiency and poor stability. This study proposes an apprenticeship-reinforcement learning (A-RL) framework based on expert demonstration (ED) model embedding to improve DRL. First, the demonstration data, calculated by dynamic programming (DP), were collected, and domain adaptive meta-learning (DAML) was used to train the ED model with the adaptive capability of working conditions. Then combined apprenticeship learning (AL) with DRL, and the ED model was used to guide the DRL to output action. The method was validated on three HEV models, and the results show that the training convergence rate increases significantly under the framework. The average increase that the apprenticeship-deep deterministic policy gradient (A-DDPG) based method applied to three HEVs achieved was 34.9 %. Apprenticeship-twin delayed twin delayed deep deterministic policy gradient (A-TD3) achieved 23 % acceleration in the power-split HEV. Because A-DDPG's EMS is more forward-looking and can mimic ED to some extent, the frequency of engine operation in the high-efficiency range has increased. Therefore, A-DDPG can improve the fuel economy of the series hybrid electric bus (HEB) by 0.2–2.7 %, and improvements averaged to about 9.6 % in the series–parallel HEV while maintaining the final SOC. This study aims to improve the sampling efficiency and optimal performance of EMS-based DRL and provide a basis for the design and development of vehicle energy saving and emission reduction.}
}
@article{KADOCHE2023119129,
title = {MARLYC: Multi-Agent Reinforcement Learning Yaw Control},
journal = {Renewable Energy},
volume = {217},
pages = {119129},
year = {2023},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2023.119129},
url = {https://www.sciencedirect.com/science/article/pii/S0960148123010431},
author = {Elie Kadoche and Sébastien Gourvénec and Maxime Pallud and Tanguy Levent},
keywords = {Large-scale wind farms, Wind farm control, Wake steering, Yaw control, Reinforcement learning, Multi-agent systems},
abstract = {Inside wind farms, turbines are subject to physical interactions such as the wake effects. Such phenomena damage the performance of wind turbines, especially offshore. Yaw control consists in rotating a turbine’s nacelle on a horizontal plane and can be used to reduce the detrimental consequences of wake effects by steering them. In this work, a new method called multi-agent reinforcement learning yaw control (MARLYC) is proposed to control the yaw of each turbine in order to improve the total energy production of the farm. MARLYC consists in the centralized training and decentralized execution of multiple reinforcement learning agents, each agent controlling the setting of one turbine’s yaw. Agents are trained together so that collective control strategies can emerge. During execution, agents are completely independent, making their usage simpler. Numerical simulations are conducted on 15 different wind farms whose size ranges from 4 to 151 turbines with real time-varying wind data. For each wind farm, MARLYC increases the total energy production by controlling the yaws of the turbines judiciously with negligible increase of the computation time.}
}
@article{LIU2023103106,
title = {Two-dimensional iterative learning control with deep reinforcement learning compensation for the non-repetitive uncertain batch processes},
journal = {Journal of Process Control},
volume = {131},
pages = {103106},
year = {2023},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2023.103106},
url = {https://www.sciencedirect.com/science/article/pii/S0959152423001932},
author = {Jianan Liu and Zike Zhou and Wenjing Hong and Jia Shi},
keywords = {Iterative learning control, Deep reinforcement learning, Non-repetitive batch processes, System model mismatch},
abstract = {Iterative learning control (ILC) is an advantage control strategy widely used in batch systems. Nevertheless, designing an effective iterative learning control scheme remains crucial for complex batch systems with model mismatch and non-repetitive nature. In this paper, we propose a two-dimensional iterative learning control-reinforcement learning (2D ILC-RL) control scheme composed of a two-dimensional ILC controller and a two-dimensional DRL compensator. Based on the 2D system theory, the 2D ILC controller is proposed to ensure the primary control performance and its stability and convergence are verified. Meanwhile, the DRL compensator counteracts the negative impact of the model mismatch and the non-repetitive nature. In addition, we proposed a real-time implementation scheme to guarantee the safety of the practical batch systems compared to the conventional online training method. Finally, the simulation results in the injection molding batch process and the nonlinear continuously stirred tank reactor demonstrate the proposed control scheme’s effectiveness, significant control performance, and strong robustness.}
}
@article{HUANG2020269,
title = {A self-organizing developmental cognitive architecture with interactive reinforcement learning},
journal = {Neurocomputing},
volume = {377},
pages = {269-285},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.07.109},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219314742},
author = {Ke Huang and Xin Ma and Rui Song and Xuewen Rong and Xincheng Tian and Yibin Li},
keywords = {Cognitive development, Online learning, Self-organizing neural network, Object recognition, Interactive reinforcement learning},
abstract = {Developmental cognitive systems can endow robots with the abilities to incrementally learn knowledge and autonomously adapt to complex environments. Conventional cognitive methods often acquire knowledge through passive perception, such as observing and listening. However, this learning way may generate incorrect representations inevitably and cannot correct them online without any feedback. To tackle this problem, we propose a biologically-inspired hierarchical cognitive system called Self-Organizing Developmental Cognitive Architecture with Interactive Reinforcement Learning (SODCA-IRL). The architecture introduces interactive reinforcement learning into hierarchical self-organizing incremental neural networks to simultaneously learn object concepts and fine-tune the learned knowledge by interacting with humans. In order to realize the integration, we equip individual neural networks with a memory model, which is designed as an exponential function controlled by two forgetting factors to simulate the consolidation and forgetting processes of humans. Besides, an interactive reinforcement strategy is designed to provide appropriate rewards and execute mistake correction. The feedback acts on the forgetting factors to reinforce or weaken the memory of neurons. Therefore, correct knowledge is preserved while incorrect representations are forgotten. Experimental results show that the proposed method can make effective use of the feedback from humans to improve the learning effectiveness significantly and reduce the model redundancy.}
}
@article{SOHEGE20208175,
title = {Deep Reinforcement Learning and Randomized Blending for Control under Novel Disturbances},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {8175-8180},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2313},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320329803},
author = {Yves Sohège and Gregory Provan and Marcos Quiñones-Grueiro and Gautam Biswas},
keywords = {Design of fault tolerant/reliable systems, Fault accommodation, Reconfiguration strategies, Methods based on neural networks and/or fuzzy logic for FDI},
abstract = {Enabling autonomous vehicles to maneuver in novel scenarios is a key unsolved problem. A well-known approach, Weighted Multiple Model Adaptive Control (WMMAC), uses a set of pre-tuned controllers and combines their control actions using a weight vector. Although WMMAC offers an improvement to traditional switched control in terms of smooth control oscillations, it depends on accurate fault isolation and cannot deal with unknown disturbances. A recent approach avoids state estimation by randomly assigning the controller weighting vector; however, this approach uses a uniform distribution for control-weight sampling, which is sub-optimal compared to state-estimation methods. In this article, we propose a framework that uses deep reinforcement learning (DRL) to learn weighted control distributions that optimize the performance of the randomized approach for both known and unknown disturbances. We show that RL-based randomized blending dominates pure randomized blending, a switched FDI-based architecture and pre-tuned controllers on a quadcopter trajectory optimisation task in which we penalise deviations in both position and attitude.}
}
@article{WEN2021100162,
title = {DTDE: A new cooperative multi-agent reinforcement learning framework},
journal = {The Innovation},
volume = {2},
number = {4},
pages = {100162},
year = {2021},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2021.100162},
url = {https://www.sciencedirect.com/science/article/pii/S2666675821000874},
author = {Guanghui Wen and Junjie Fu and Pengcheng Dai and Jialing Zhou}
}
@article{CHENG201863,
title = {Concise deep reinforcement learning obstacle avoidance for underactuated unmanned marine vessels},
journal = {Neurocomputing},
volume = {272},
pages = {63-73},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.06.066},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217311943},
author = {Yin Cheng and Weidong Zhang},
keywords = {Deep learning, Reinforcement learning, Obstacle avoidance, Nonlinear dynamics, Underactuated unmanned marine vessel},
abstract = {This research is concerned with the problem of obstacle avoidance for the underactuated unmanned marine vessel under unknown environmental disturbance. A concise deep reinforcement learning obstacle avoidance (CDRLOA) algorithm is proposed with the powerful deep Q-networks architecture to overcome the usability issue caused by the complicated control law in the traditional analytic approach. Furthermore, a comprehensive reward function is specifically designed for obstacle avoidance, target approaching, speed modification, and attitude correction. Compared to the analytic methods, the proposed algorithm based on reinforcement learning shows notable advantages in utility and extendibility. With the same CDRLOA system, the targets and the constraints are highly customizable for various of special requirements. Extensive experiments conducted have demonstrated the effectiveness and conciseness of the proposed algorithm.}
}
@article{LI2023126281,
title = {AID-RL: Active information-directed reinforcement learning for autonomous source seeking and estimation},
journal = {Neurocomputing},
volume = {544},
pages = {126281},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126281},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223004046},
author = {Zhongguo Li and Wen-Hua Chen and Jun Yang and Yunda Yan},
keywords = {Autonomous search, Reinforcement learning, Dual control, Active learning, Exploration, Exploitation},
abstract = {This paper proposes an active information-directed reinforcement learning (AID-RL) framework for autonomous source seeking and estimation problem. Source seeking requires the search agent to move towards the true source, and source estimation demands the agent to maintain and update its knowledge regarding the source properties such as release rate and source position. These two objectives give rise to the newly developed framework, namely, dual control for exploration and exploitation. In this paper, the greedy RL forms an exploitation search strategy that navigates the agent to the source position, while the information-directed search commands the agent to explore most informative positions to reduce belief uncertainty. Extensive results are presented using a high-fidelity dataset for autonomous search, which validates the effectiveness of the proposed AID-RL and highlights the importance of active exploration in improving sampling efficiency and search performance.}
}
@article{CABREJASEGEA2021638,
title = {Reinforcement Learning for Traffic Signal Control: Comparison with Commercial Systems},
journal = {Transportation Research Procedia},
volume = {58},
pages = {638-645},
year = {2021},
note = {XIV Conference on Transport Engineering, CIT2021},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2021.11.084},
url = {https://www.sciencedirect.com/science/article/pii/S2352146521008437},
author = {Alvaro Cabrejas-Egea and Raymond Zhang and Neil Walton},
keywords = {Modelling, Simulation, Urban Traffic Control, Reinforcement Learning},
abstract = {Intelligent Transportation Systems are leveraging the power of increased sensory coverage and computing power to deliver data-intensive solutions achieving higher levels of performance than traditional systems. Within Traffic Signal Control (TSC), this has allowed the emergence of Machine Learning (ML) based systems. Among this group, Reinforcement Learning (RL) approaches have performed particularly well. Given the lack of industry standards in ML for TSC, literature exploring RL often lacks comparison against commercially available systems and straightforward formulations of how the agents operate. Here we attempt to bridge that gap. We propose three different architectures for TSC RL agents and compare them against the currently used commercial systems MOVA, SurTrac and Cyclic controllers and provide pseudo-code for them. The agents use variations of Deep Q-Learning and Actor Critic, using states and rewards based on queue lengths. Their performance is compared in across different map scenarios with variable demand, assessing them in terms of the global delay and average queue length. We find that the RL-based systems can significantly and consistently achieve lower delays when compared with existing commercial systems.}
}
@article{WANG2023105735,
title = {Optimal formation tracking control based on reinforcement learning for multi-UAV systems},
journal = {Control Engineering Practice},
volume = {141},
pages = {105735},
year = {2023},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2023.105735},
url = {https://www.sciencedirect.com/science/article/pii/S0967066123003040},
author = {Weizhen Wang and Xin Chen and Jiangbo Jia and Kaili Wu and Mingyang Xie},
keywords = {Optimal formation tracking, Multi-UAV systems, Adaptive sliding mode controller, Reinforcement learning},
abstract = {This paper investigates the problem of optimal formation tracking control for multi-UAV systems with model uncertainty and external disturbances. Firstly, by combining the sliding mode method and a neural network, an adaptive sliding mode controller is derived that counteracts the effects of modeling uncertainty and external disturbance. Subsequently, the optimal formation tracking control problem of the original system is then converted to the optimal control problem of a nominal system, and an actor–critic reinforcement learning framework is built using adaptive neural network identifiers to recursively approximate the total optimal policy and cost function. The Lyapunov analysis method shows that the stability of the closed-loop system and the convergence of the estimation weights for the actor–critic network are guaranteed. Additionally, a formation tracking using virtual experiment platform for multi-UAV systems are constructed based on the Robot Operating System (ROS) and Gazebo simulator. Finally, virtual-reality experiments is performed to demonstrate the effectiveness of the proposed control scheme.}
}
@article{CHEN2023104489,
title = {A deep multi-agent reinforcement learning framework for autonomous aerial navigation to grasping points on loads},
journal = {Robotics and Autonomous Systems},
volume = {167},
pages = {104489},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104489},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023001288},
author = {Jingyu Chen and Ruidong Ma and John Oyekan},
keywords = {Cooperative navigation, Multi-agent reinforcement learning, Learning from demonstration, Curriculum learning},
abstract = {Deep reinforcement learning, by taking advantage of neural networks, has made great strides in the continuous control of robots. However, in scenarios where multiple robots are required to collaborate with each other to accomplish a task, it is still challenging to build an efficient and scalable multi-agent control system due to increasing complexity. In this paper, we regard each unmanned aerial vehicle (UAV) with its manipulator as one agent, and leverage the power of multi-agent deep deterministic policy gradient (MADDPG) for the cooperative navigation and manipulation of a load. We propose solutions for addressing navigation to grasping point problem in targeted and flexible scenarios, and mainly focus on how to develop model-free policies for the UAVs without relying on a trajectory planner. To overcome the challenges of learning in scenarios with an increasing number of grasping points, we incorporate the demonstrations from an Optimal Reciprocal Collision Avoidance (ORCA) algorithm into our framework to guide the policy training and adapt two novel techniques into the architecture of MADDPG. Furthermore, curriculum learning with the attention mechanism is utilized by reusing knowledge from fewer grasping points to facilitate the training of a load with more points. Our experiments were validated by a load with three, four and six grasping points respectively in Coppeliasim simulator and then transferred into the real world with Crazyflie quadrotors. Our results show that the average tracking deviations from the desirable grasping point to the final position of the UAV can be less than 10 cm in some real-world experiments. Compared with state-of-the-art model-free reinforcement learning and swarm optimization algorithms, results show that our proposed methods outperform other baselines with a reasonable success rate especially in the scenarios with more grasping points. Furthermore, the learned optimal policies enable UAVs to reach and hover over all the grasping points before manipulation without any collision. We conducted a comprehensive analysis of both targeted and flexible navigation, highlighting their respective advantages and disadvantages.}
}
@article{TIPALDI20221,
title = {Reinforcement learning in spacecraft control applications: Advances, prospects, and challenges},
journal = {Annual Reviews in Control},
volume = {54},
pages = {1-23},
year = {2022},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2022.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S136757882200089X},
author = {Massimo Tipaldi and Raffaele Iervolino and Paolo Roberto Massenio},
keywords = {Reinforcement learning, Spacecraft control applications, Agent environment interface, Adaptive guidance and control, Policy gradient methods, Deep reinforcement learning},
abstract = {This paper presents and analyzes Reinforcement Learning (RL) based approaches to solve spacecraft control problems. Different application fields are considered, e.g., guidance, navigation and control systems for spacecraft landing on celestial bodies, constellation orbital control, and maneuver planning in orbit transfers. It is discussed how RL solutions can address the emerging needs of designing spacecraft with highly autonomous on-board capabilities and implementing controllers (i.e., RL agents) robust to system uncertainties and adaptive to changing environments. For each application field, the RL framework core elements (e.g., the reward function, the RL algorithm and the environment model used for the RL agent training) are discussed with the aim of providing some guidelines in the formulation of spacecraft control problems via a RL framework. At the same time, the adoption of RL in real space projects is also analyzed. Different open points are identified and discussed, e.g., the availability of high-fidelity simulators for the RL agent training and the verification of RL-based solutions. This way, recommendations for future work are proposed with the aim of reducing the technological gap between the solutions proposed by the academic community and the needs/requirements of the space industry.}
}
@article{LI2020101847,
title = {Trajectory smoothing method using reinforcement learning for computer numerical control machine tools},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {61},
pages = {101847},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101847},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519300419},
author = {Bingran Li and Hui Zhang and Peiqing Ye and Jinsong Wang},
keywords = {CNC, Trajectory smoothing, Neuron network, Reinforcement learning},
abstract = {Tool-path codes output by computer-aided manufacturing software for high-speed machining are composed of discontinuous G01 line segments. The discontinuity of these tool movements causes computer numerical control (CNC) inefficiency. To achieve high-speed continuous motion, corner smoothing algorithms based on pre-planning methods are widely used. However, it is difficult to optimize smoothing trajectories in real-time systems. To obtain smooth trajectories efficiently, this paper proposes a neural network-based direct trajectory smoothing method. An intelligent neural network agent outputs servo commands directly based on the current tool path and running state in every cycle. To achieve direct control, motion feature and reward models were built, and reinforcement learning was used to train the neural network parameters without additional experimental data. The proposed method provides higher cutting efficiency than the local and global smoothing algorithms. Given its simple structure and low computational demands, it can easily be applied to real-time CNC systems.}
}
@article{YANG2022104523,
title = {Reinforcement learning based optimal dynamic policy determination for natural gas hydrate reservoir exploitation},
journal = {Journal of Natural Gas Science and Engineering},
volume = {101},
pages = {104523},
year = {2022},
issn = {1875-5100},
doi = {https://doi.org/10.1016/j.jngse.2022.104523},
url = {https://www.sciencedirect.com/science/article/pii/S1875510022001135},
author = {Zili Yang and Hu Si and Dongliang Zhong},
keywords = {Natural gas hydrate, Depressurization, Thermal stimulation, Energy efficiency, Optimization},
abstract = {The commercial exploitation of natural gas hydrate reservoirs requires scientific policies to reduce the exploitation cost and improve production efficiency. A multineural network composite model for NGH reservoir exploitation is designed and coupled with the secondary-developed T + H code in the PYTHON environment. Under the preconditions of realizing multiobjective optimization in terms of efficiency, safety, and economy, the model achieves the optimal dynamic production policy and quantitatively presents the advantages and disadvantages of different exploitation policies in each exploitation state. In the initial exploitation period, high-temperature and increasingly low-rate exploitation policies achieve the strongest production performance and energy efficiency. The huff and puff heat injection method can effectively reduce the permeability drop caused by secondary hydrate formation. In the middle exploitation period, the injection density of thermal energy controls the hydrate exploitation efficiency, the huff and puff method is not applicable, and more intensive heat injection results in higher exploitation efficiency and less water production. In the posterior exploitation period, the methane recovery rate does not depend on heat injection, and more energy supply can only achieve a lower energy efficiency return. The proposed methodology provides a feasible means of NGH reservoir exploitation policy optimization and analysis.}
}
@article{GARRAD2023100970,
title = {Reinforcement learning in VANET penetration testing},
journal = {Results in Engineering},
volume = {17},
pages = {100970},
year = {2023},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2023.100970},
url = {https://www.sciencedirect.com/science/article/pii/S259012302300097X},
author = {Phillip Garrad and Saritha Unnikrishnan},
keywords = {Cybersecurity, Connected vehicles, Software simulation, Artificial intelligence, Penetration testing},
abstract = {The recent popularity of Connected and Autonomous Vehicles (CAV) corresponds with an increase in the risk of cyber-attacks. These cyber-attacks are instigated by white-coat hackers, and cyber-criminals. As Connected Vehicles move towards full autonomy the impact of these cyber-attacks also grows. The current research highlights challenges faced in cybersecurity testing of CAV, including access, the cost of representative test setup and the lack of experts in the field. Possible solutions of how these challenges can be overcome are reviewed and discussed. From these findings a software simulated Vehicular Ad Hoc NETwork (VANET) is established as a cost-effective representative testbed. Penetration tests are then performed on this simulation, demonstrating a cyber-attack in CAV. Studies have shown Artificial Intelligence (AI) to improve runtime, increase efficiency and comprehensively cover all the typical test aspects, in penetration testing in other industries. In this research a Reinforcement Learning model, called Q-Learning, is applied to automate the software simulation. The expectation from this implementation is to see improvements in runtime and efficiency for the VANET model. The results show this approach to be promising and using AI in penetration testing for VANET to improve efficiency in most cases. Each case is reviewed in detail before discussing possible ways to improve the implementation and get a truer reflection of the real-world application.}
}
@article{JOSHI2023105462,
title = {TASAC: A twin-actor reinforcement learning framework with a stochastic policy with an application to batch process control},
journal = {Control Engineering Practice},
volume = {134},
pages = {105462},
year = {2023},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2023.105462},
url = {https://www.sciencedirect.com/science/article/pii/S096706612300031X},
author = {Tanuja Joshi and Hariprasad Kodamana and Harikumar Kandath and Niket Kaisare},
keywords = {Reinforcement learning, Actor–critic algorithm, Deep learning, Batch process},
abstract = {Due to their complex nonlinear dynamics and batch-to-batch variability, batch processes pose a challenge for process control. Due to the absence of accurate models and resulting plant-model mismatch, these problems become harder to address for advanced model-based control strategies. Reinforcement Learning (RL), wherein an agent learns the policy by directly interacting with the environment, offers a potential alternative in this context. RL frameworks with actor–critic architecture have recently become popular for controlling systems where state and action spaces are continuous. The current study proposes a stochastic actor–critic RL algorithm, termed Twin Actor Soft Actor–Critic (TASAC), by incorporating an ensemble of actors in a maximum entropy framework to improve learning due to enhanced exploration. The efficacy of the proposed approach is showcased by applying the same for the control of batch transesterification.}
}
@article{KIUMARSI2017144,
title = {H∞ control of linear discrete-time systems: Off-policy reinforcement learning},
journal = {Automatica},
volume = {78},
pages = {144-152},
year = {2017},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2016.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0005109816305179},
author = {Bahare Kiumarsi and Frank L. Lewis and Zhong-Ping Jiang},
keywords = { control, Off-policy reinforcement learning, Optimal control},
abstract = {In this paper, a model-free solution to the H∞ control of linear discrete-time systems is presented. The proposed approach employs off-policy reinforcement learning (RL) to solve the game algebraic Riccati equation online using measured data along the system trajectories. Like existing model-free RL algorithms, no knowledge of the system dynamics is required. However, the proposed method has two main advantages. First, the disturbance input does not need to be adjusted in a specific manner. This makes it more practical as the disturbance cannot be specified in most real-world applications. Second, there is no bias as a result of adding a probing noise to the control input to maintain persistence of excitation (PE) condition. Consequently, the convergence of the proposed algorithm is not affected by probing noise. An example of the H∞ control for an F-16 aircraft is given. It is seen that the convergence of the new off-policy RL algorithm is insensitive to probing noise.}
}