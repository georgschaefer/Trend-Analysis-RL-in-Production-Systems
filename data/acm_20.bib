@inproceedings{10.1145/3301326.3301375,
author = {Wu, Qianlin and Zhu, Dandan and Liu, Yi and Du, Aimin and Chen, Dong and Ye, Zhihui},
title = {Comprehensive Control System for Gathering Pipe Network Operation Based on Reinforcement Learning},
year = {2018},
isbn = {9781450365536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301326.3301375},
doi = {10.1145/3301326.3301375},
abstract = {In the transmission process of crude oil gathering system, water-assisted heat transfer is often used to avoid wax formation, and pipeline temperature and pressure control are often controlled manually. In order to improve control efficiency and save labor cost. In this paper, we propose a DQN-based algorithm. The intensive learning model completes the temperature and pressure control in the pipeline. At the same time, because these two parameters have strong coupling, which affects the global control, this paper focuses on the joint optimization of valve opening and heating furnace and pressure pump. Finally, in order to verify the effectiveness of the system, the simulation control experiment is adopted. The test results show that the system control effect is excellent and the robustness is good.},
booktitle = {Proceedings of the 2018 VII International Conference on Network, Communication and Computing},
pages = {34–39},
numpages = {6},
keywords = {Reinforcement learning, Deep Q Network, Gathering Pipe Network, Intelligent Control System},
location = {Taipei City, Taiwan},
series = {ICNCC '18}
}

@article{10.14778/3184470.3184474,
author = {Li, Teng and Xu, Zhiyuan and Tang, Jian and Wang, Yanzhi},
title = {Model-Free Control for Distributed Stream Data Processing Using Deep Reinforcement Learning},
year = {2018},
issue_date = {February 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3184470.3184474},
doi = {10.14778/3184470.3184474},
abstract = {In this paper, we focus on general-purpose Distributed Stream Data Processing Systems (DSDPSs), which deal with processing of unbounded streams of continuous data at scale distributedly in real or near-real time. A fundamental problem in a DSDPS is the scheduling problem (i.e., assigning workload to workers/machines) with the objective of minimizing average end-to-end tuple processing time. A widely-used solution is to distribute workload evenly over machines in the cluster in a round-robin manner, which is obviously not efficient due to lack of consideration for communication delay. Model-based approaches (such as queueing theory) do not work well either due to the high complexity of the system environment.We aim to develop a novel model-free approach that can learn to well control a DSDPS from its experience rather than accurate and mathematically solvable system models, just as a human learns a skill (such as cooking, driving, swimming, etc). Specifically, we, for the first time, propose to leverage emerging Deep Reinforcement Learning (DRL) for enabling model-free control in DSDPSs; and present design, implementation and evaluation of a novel and highly effective DRL-based control framework, which minimizes average end-to-end tuple processing time by jointly learning the system environment via collecting very limited runtime statistics data and making decisions under the guidance of powerful Deep Neural Networks (DNNs). To validate and evaluate the proposed framework, we implemented it based on a widely-used DSDPS, Apache Storm, and tested it with three representative applications: continuous queries, log stream processing and word count (stream version). Extensive experimental results show 1) Compared to Storm's default scheduler and the state-of-the-art model-based method, the proposed framework reduces average tuple processing by 33.5\% and 14.0\% respectively on average. 2) The proposed framework can quickly reach a good scheduling solution during online learning, which justifies its practicability for online control in DSDPSs.},
journal = {Proc. VLDB Endow.},
month = {feb},
pages = {705–718},
numpages = {14}
}

@inproceedings{10.1145/2808719.2812596,
author = {Camara, Michael and Bonham-Carter, Oliver and Jumadinova, Janyl},
title = {A Multi-Agent System with Reinforcement Learning Agents for Biomedical Text Mining},
year = {2015},
isbn = {9781450338530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808719.2812596},
doi = {10.1145/2808719.2812596},
abstract = {Due to the expanding growth of information in the biomedical literature and biomedical databases, researchers and practitioners in the biomedical field require efficient methods of handling and extracting useful information. We present a novel framework for biomedical text mining based on a learning multi-agent system. Our distributed system comprises of several software agents, where each agent uses a reinforcement learning method to update the sentiment of a relevant text from a particular set of research articles related to specific keywords. Our system was tested on the biomedical research articles from PubMed, where the goal of each agent is to accrue utility by correctly determining the relevant information that is communicated with other agents. Our results tested on the abstracts collected from PubMed related to muscular atrophy, Alzheimer's disease, and diabetes show that our system is able to appropriately learn the sentiment score related to specific keywords by parallel and distributed analysis of the documents by multiple software agents.},
booktitle = {Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {634–643},
numpages = {10},
keywords = {biomedical text mining, reinforcement learning, multi-agent},
location = {Atlanta, Georgia},
series = {BCB '15}
}

@inproceedings{10.1145/3485447.3512234,
author = {Lee, Soyoung and Wi, Seongil and Son, Sooel},
title = {Link: Black-Box Detection of Cross-Site Scripting Vulnerabilities Using Reinforcement Learning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512234},
doi = {10.1145/3485447.3512234},
abstract = {Black-box web scanners have been a prevalent means of performing penetration testing to find reflected cross-site scripting (XSS) vulnerabilities. Unfortunately, off-the-shelf black-box web scanners suffer from unscalable testing as well as false negatives that stem from a testing strategy that employs fixed attack payloads, thus disregarding the exploitation of contexts to trigger vulnerabilities. To this end, we propose a novel method of adapting attack payloads to a target reflected XSS vulnerability using reinforcement learning (RL). We present Link, a general RL framework whose states, actions, and a reward function are designed to find reflected XSS vulnerabilities in a black-box and fully automatic manner. Link finds 45, 213, and 60 vulnerabilities with no false positives in Firing-Range, OWASP, and WAVSEP benchmarks, respectively, outperforming state-of-the-art web scanners in terms of finding vulnerabilities and ending testing campaigns earlier. Link also finds 43 vulnerabilities in 12 real-world applications, demonstrating the promising efficacy of using RL in finding reflected XSS vulnerabilities.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {743–754},
numpages = {12},
keywords = {penetration testing;, reinforcement learning, cross-site scripting},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.5555/3539845.3540165,
author = {Chen, Lin and Li, Xiao and Xu, Jiang},
title = {Improve the Stability and Robustness of Power Management through Model-Free Deep Reinforcement Learning},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Achieving high performance with low energy consumption has become a primary design objective in multi-core systems. Recently, power management based on reinforcement learning has shown great potential in adapting to dynamic environments without much prior knowledge. However, conventional Q-learning (QL) algorithms adopted in most existing works encounter serious problems about scalability, instability, and overestimation. In this paper, we present a deep reinforcement learning-based approach to improve the stability and robustness of power management while reducing the energy-delay product (EDP) under user-specified performance requirements. The comprehensive status of the system is monitored periodically, making our controller sensitive to environmental change. To further improve the learning effectiveness, knowledge sharing among multiple devices is implemented in our approach. Experimental results on multiple realistic applications show that the proposed method can reduce the instability up to 68\% compared with QL. Through knowledge sharing among multiple devices, our federated approach achieves around 4.8\% EDP improvement over QL on average.},
booktitle = {Proceedings of the 2022 Conference \&amp; Exhibition on Design, Automation \&amp; Test in Europe},
pages = {1371–1376},
numpages = {6},
keywords = {deep reinforcement learning, power management, experience replay, federated learning, multicore system},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.5555/1867750.1867757,
author = {Lunsford, Rebecca and Heeman, Peter},
title = {Using Reinforcement Learning to Create Communication Channel Management Strategies for Diverse Users},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Spoken dialogue systems typically do not manage the communication channel, instead using fixed values for such features as the amplitude and speaking rate. Yet, the quality of a dialogue can be compromised if the user has difficulty understanding the system. In this proof-of-concept research, we explore using reinforcement learning (RL) to create policies that manage the communication channel to meet the needs of diverse users. Towards this end, we first formalize a preliminary communication channel model, in which users provide explicit feedback regarding issues with the communication channel, and the system implicitly alters its amplitude to accommodate the user's optimal volume. Second, we explore whether RL is an appropriate tool for creating communication channel management strategies, comparing two different hand-crafted policies to policies trained using both a dialogue-length and a novel annoyance cost. The learned policies performed better than hand-crafted policies, with those trained using the annoyance cost learning an equitable tradeoff between users with differing needs and also learning to balance finding a user's optimal amplitude against dialogue-length. These results suggest that RL can be used to create effective communication channel management policies for diverse users.},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies},
pages = {53–61},
numpages = {9},
location = {Los Angeles, California},
series = {SLPAT '10}
}

@inproceedings{10.1145/1178823.1178828,
author = {Merrick, Kathryn and Maher, Mary Lou},
title = {Motivated Reinforcement Learning for Non-Player Characters in Persistent Computer Game Worlds},
year = {2006},
isbn = {1595933808},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1178823.1178828},
doi = {10.1145/1178823.1178828},
abstract = {Massively multiplayer online computer games are played in complex, persistent virtual worlds. Over time, the landscape of these worlds evolves and changes as players create and personalise their own virtual property. In contrast, many non-player characters that populate virtual game worlds possess a fixed set of pre-programmed behaviours and lack the ability to adapt and evolve in time with their surroundings. This paper presents motivated reinforcement learning agents as a means of creating non-player characters that can both evolve and adapt. Motivated reinforcement learning agents explore their environment and learn new behaviours in response to interesting experiences, allowing them to display progressively evolving behavioural patterns. In dynamic worlds, environmental changes provide an additional source of interesting experiences triggering further learning and allowing the agents to adapt their existing behavioural patterns in time with their surroundings.},
booktitle = {Proceedings of the 2006 ACM SIGCHI International Conference on Advances in Computer Entertainment Technology},
pages = {3–es},
keywords = {persistent virtual worlds, computer games, motivation, reinforcement learning},
location = {Hollywood, California, USA},
series = {ACE '06}
}

@article{10.5555/3455716.3455912,
author = {Lehnert, Lucas and Littman, Michael L.},
title = {Successor Features Combine Elements of Model-Free and Model-Based Reinforcement Learning},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {A key question in reinforcement learning is how an intelligent agent can generalize knowledge across different inputs. By generalizing across different inputs, information learned for one input can be immediately reused for improving predictions for another input. Reusing information allows an agent to compute an optimal decision-making strategy using less data. State representation is a key element of the generalization process, compressing a high-dimensional input space into a low-dimensional latent state space. This article analyzes properties of different latent state spaces, leading to new connections between model-based and model-free reinforcement learning. Successor features, which predict frequencies of future observations, form a link between model-based and model-free learning: Learning to predict future expected reward outcomes, a key characteristic of model-based agents, is equivalent to learning successor features. Learning successor features is a form of temporal difference learning and is equivalent to learning to predict a single policy's utility, which is a characteristic of model-free agents. Drawing on the connection between model-based reinforcement learning and successor features, we demonstrate that representations that are predictive of future reward outcomes generalize across variations in both transitions and rewards. This result extends previous work on successor features, which is constrained to fixed transitions and assumes re-learning of the transferred state representation.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {196},
numpages = {53},
keywords = {model-based reinforcement learning, state representations, successor features, state abstractions}
}

@inproceedings{10.1145/3163080.3163117,
author = {Sharma, Rajneesh and Kukker, Amit},
title = {Neural Reinforcement Learning Based Identifier for Typing Keys Using Forearm EMG Signals},
year = {2017},
isbn = {9781450353847},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3163080.3163117},
doi = {10.1145/3163080.3163117},
abstract = {This work proposes a neural reinforcement learning (NRL) identifier for accurate classification finger movements for typing tasks using forearm Electromyogram (EMG) signals. We first extract four key statistical features from the EMG signals (channel 1 and channel 2) corresponding to seven typing keys. Next, these features are fed to a reinforcement learning based k nearest neighbor neural classifier for identifying the keys using a "trial and error" approach. We use EMG data from typing tasks of ten subjects using two acquisition electrodes: channel 1 and channel 2. In the first part of our work, we attempt to classify typing keys using EMG data corresponding to one subject only. After sufficient learning, NRL classifier achieved an accuracy of 99.01\% and 98.29\% for channel 1 and channel 2, respectively. In second part of our work, we fed the EMG data of all the ten subjects to the NRL. The NRL is able to achieve a classification accuracy of 92.7\%. We also employ a subspace ensemble nearest neighbor approximator yielding a classification accuracy of 94.3\% with 5-cross fold validation and 97.1\% with 3-cross fold validation. Results show the effectiveness and viability of using NRL for identifying typing movements using forearm EMG signals.},
booktitle = {Proceedings of the 9th International Conference on Signal Processing Systems},
pages = {225–229},
numpages = {5},
keywords = {Electromyogram, Neural networks, Reinforcement Learning},
location = {Auckland, New Zealand},
series = {ICSPS 2017}
}

@inproceedings{10.1145/3576050.3576104,
author = {Zhang, Fan and Xing, Wanli and Li, Chenglu},
title = {Predicting Students’ Algebra I Performance Using Reinforcement Learning with Multi-Group Fairness},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576104},
doi = {10.1145/3576050.3576104},
abstract = {Numerous studies have successfully adopted learning analytics techniques such as machine learning (ML) to address educational issues. However, limited research has addressed the problem of algorithmic bias in ML. In the few attempts to develop strategies to concretely mitigate algorithmic bias in education, the focus has been on debiasing ML models with single group membership. This study aimed to propose an algorithmic strategy to mitigate bias in a multi-group context. The results showed that our proposed model could effectively reduce algorithmic bias in a multi-group setting while retaining competitive accuracy. The findings implied that there could be a paradigm shift from focusing on debiasing a single group to multiple groups in educational attempts on ML.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {657–662},
numpages = {6},
keywords = {fair AI, reinforcement learning, multi-group fairness, math achievement prediction},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3546790.3546804,
author = {Akl, Mahmoud and Sandamirskaya, Yulia and Ergene, Deniz and Walter, Florian and Knoll, Alois},
title = {Fine-Tuning Deep Reinforcement Learning Policies with r-STDP for Domain Adaptation},
year = {2022},
isbn = {9781450397896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546790.3546804},
doi = {10.1145/3546790.3546804},
abstract = {Using deep reinforcement learning policies that are trained in simulation on real robotic platforms requires fine-tuning due to discrepancies between simulated and real environments. Multiple methods like domain randomization and system identification have been suggested to overcome this problem. However, sim-to-real transfer remains an open problem in robotics and deep reinforcement learning. In this paper, we present a spiking neural network (SNN) alternative for dealing with the sim-to-real problem. In particular, we train SNNs with backpropagation using surrogate gradients and the (Deep Q-Network) DQN algorithm to solve two classical control reinforcement learning tasks. The performance of the trained DQNs degrades when evaluated on randomized versions of the environments used during training. To compensate for the drop in performance, we apply the biologically plausible reward-modulated spike timing dependent plasticity (r-STDP) learning rule. Our results show that r-STDP can be successfully utilized to restore the network’s ability to solve the task. Furthermore, since r-STDP can be directly implemented on neuromorphic hardware, we believe it provides a promising neuromorphic solution to the sim-to-real problem.},
booktitle = {Proceedings of the International Conference on Neuromorphic Systems 2022},
articleno = {14},
numpages = {8},
keywords = {neural networks, spiking neural networks, reinforcement learning},
location = {Knoxville, TN, USA},
series = {ICONS '22}
}

@inproceedings{10.1145/3360322.3360861,
author = {Zhang, Chi and Kuppannagari, Sanmukh R. and Kannan, Rajgopal and Prasanna, Viktor K.},
title = {Building HVAC Scheduling Using Reinforcement Learning via Neural Network Based Model Approximation},
year = {2019},
isbn = {9781450370059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360322.3360861},
doi = {10.1145/3360322.3360861},
abstract = {Buildings sector is one of the major consumers of energy in the United States. The buildings HVAC (Heating, Ventilation, and Air Conditioning) systems, whose functionality is to maintain thermal comfort and indoor air quality (IAQ), account for almost half of the energy consumed by the buildings. Thus, intelligent scheduling of the building HVAC system has the potential for tremendous energy and cost savings while ensuring that the control objectives (thermal comfort, air quality) are satisfied.Traditionally, rule-based and model-based approaches such as linear-quadratic regulator (LQR) have been used for scheduling HVAC. However, the system complexity of HVAC and the dynamism in the building environment limit the accuracy, efficiency and robustness of such methods. Recently, several works have focused on model-free deep reinforcement learning based techniques such as Deep Q-Network (DQN). Such methods require extensive interactions with the environment. Thus, they are impractical to implement in real systems due to low sample efficiency. Safety-aware exploration is another challenge in real systems since certain actions at particular states may result in catastrophic outcomes.To address these issues and challenges, we propose a modelbased reinforcement learning approach that learns the system dynamics using a neural network. Then, we adopt Model Predictive Control (MPC) using the learned system dynamics to perform control with random-sampling shooting method. To ensure safe exploration, we limit the actions within safe range and the maximum absolute change of actions according to prior knowledge. We evaluate our ideas through simulation using widely adopted EnergyPlus tool on a case study consisting of a two zone data-center. Experiments show that the average deviation of the trajectories sampled from the learned dynamics and the ground truth is below 20\%. Compared with baseline approaches, we reduce the total energy consumption by 17.1\% ~ 21.8\%. Compared with model-free reinforcement learning approach, we reduce the required number of training steps to converge by 10x.},
booktitle = {Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {287–296},
numpages = {10},
keywords = {model-based reinforcement learning, smart buildings, model predictive control, hvac control, neural network dynamics, data center cooling},
location = {New York, NY, USA},
series = {BuildSys '19}
}

@inproceedings{10.1145/3511808.3557435,
author = {Xia, Yuyang and Liu, Shuncheng and Chen, Xu and Xu, Zhi and Zheng, Kai and Su, Han},
title = {RISE: A Velocity Control Framework with Minimal Impacts Based on Reinforcement Learning},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557435},
doi = {10.1145/3511808.3557435},
abstract = {Velocity control in autonomous driving is an emerging technology that has achieved rapid progress over the last decade. However, existing velocity control models are developed in single-lane scenarios and ignore the negative impacts caused by harsh velocity changes. In this work, we propose a velocity control framework based on reinforcement learning, called RISE (contRol velocIty for autonomouS vEhicle). In multi-lane circumstances, RISE improves velocity decisions regarding the autonomous vehicle itself, while minimizing impacts on rear vehicles. To achieve multiple objectives, we propose a hybrid reward function to rate each velocity decision from four aspects: safety, efficiency, comfort, and negative impact to guide the autonomous vehicle. Among these reward factors, the negative impact is used to penalize the harsh actions of the autonomous vehicle, thus prompting it to reduce the negative impacts on its rear vehicles. To detect the latent perturbations among surrounding vehicles in multiple lanes, we propose an attention-based encoder to learn the positions and interactions from an impact graph. Extensive experiments evidence that RISE enables safe driving, and outperforms state-of-the-art methods in efficiency, comfort, and alleviating negative impacts.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {2210–2219},
numpages = {10},
keywords = {autonomous vehicle, velocity control, reinforcement learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.5555/3535850.3535933,
author = {Kazemi, Milad and Perez, Mateo and Somenzi, Fabio and Soudjani, Sadegh and Trivedi, Ashutosh and Velasquez, Alvaro},
title = {Translating Omega-Regular Specifications to Average Objectives for Model-Free Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent success in reinforcement learning (RL) has brought renewed attention to the design of reward functions by which agent behavior is reinforced or deterred. Manually designing reward functions is tedious and error-prone. An alternative approach is to specify a formal, unambiguous logic requirement, which is automatically translated into a reward function to be learned from. Omega-regular languages, of which Linear Temporal Logic (LTL) is a subset, are a natural choice for specifying such requirements due to their use in verification and synthesis. However, current techniques based on omega-regular languages learn in an episodic manner whereby the environment is periodically reset to an initial state during learning. In some settings, this assumption is challenging or impossible to satisfy. Instead, in the continuing setting the agent explores the environment without resets over a single lifetime. This is a more natural setting for reasoning about omega-regular specifications defined over infinite traces of agent behavior. Optimizing the average reward instead of the usual discounted reward is more natural in this case due to the infinite-horizon objective that poses challenges to the convergence of discounted RL solutions.We restrict our attention to the omega-regular languages which correspond to absolute liveness specifications. These specifications cannot be invalidated by any finite prefix of agent behavior, in accordance with the spirit of a continuing problem. We propose a translation from absolute liveness omega-regular languages to an average reward objective for RL. Our reduction can be done on-the-fly, without full knowledge of the environment, thereby enabling the use of model-free RL algorithms. Additionally, we propose a reward structure that enables RL without episodic resetting in communicating MDPs, unlike previous approaches. We demonstrate empirically with various benchmarks that our proposed method of using average reward RL for continuing tasks defined by omega-regular specifications is more effective than competing approaches that leverage discounted RL.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {732–741},
numpages = {10},
keywords = {omega-regular, linear temporal logic, reinforcement learning, formal synthesis, average reward, continuing RL, reward machine},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.5555/3091125.3091202,
author = {Bogert, Kenneth and Doshi, Prashant},
title = {Scaling Expectation-Maximization for Inverse Reinforcement Learning to Multiple Robots under Occlusion},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We consider inverse reinforcement learning (IRL) when portions of the expert's trajectory are occluded from the learner. For example, two experts performing tasks in close proximity may block each other from the learner's view or the learner is a robot observing mobile robots from a fixed position with limited sensor range. Previous methods mitigate this challenge by either focusing on the observed data only or by forming an expectation over the missing portion of the expert's trajectories given observed data. However, not only is the resulting optimization nonlinear and nonconvex, the space of occluded trajectories may be very large especially when multiple agents are observed over an extended time, which makes it intractable to compute the expectation. We present methods for speeding up the computation of conditional expectations by employing blocked Gibbs sampling. Challenged by a time-limited, multi-robot domain we explore various blocking schemes and demonstrate that our methods offer significantly improved performance over existing IRL techniques under occlusion.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {522–529},
numpages = {8},
keywords = {robotics, gibbs sampling, inverse reinforcement learning, occlusion, expectation-maximization},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@article{10.1145/3591108,
author = {Ye, Jin and Dan, Meng and Jiang, Wenchao},
title = {A Visual Sensitivity Aware ABR Algorithm&nbsp;for DASH via Deep Reinforcement Learning},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3591108},
doi = {10.1145/3591108},
abstract = {In order to cope with the fluctuation of network bandwidth and provide smooth video services, adaptive video streaming technology is proposed. In particular, the adaptive bitrate (ABR) algorithm is widely used in dynamic adaptive streaming over HTTP (DASH) to improve quality of experience (QoE). However, existing ABR algorithms still ignore the inherent visual sensitivity of human visual system (HVS). As the final receiver of video, HVS has different sensitivity to the quality distortion of different video content, and video content with high visual sensitivity needs to allocate more bitrate resources. Therefore, existing ABR algorithms still have limitations in reasonably allocating bitrate and maximizing QoE. To solve this problem, this paper designs an adaptive bitrate strategy from the perspective of user vision, studies the modeling of visual sensitivity, and proposes a visual sensitivity aware ABR algorithm. We extract a set of content features and attribute features from the video, and consider the simulation of HVS to establish a total masking effect model that reflects the visual sensitivity more accurately. Further, the network status, buffer occupancy, and visual sensitivity are comprehensively considered under a deep reinforcement learning framework to select the appropriate bitrate for maximizing QoE. We implement the proposed algorithm over a realistic trace-driven evaluation and compare its performance with several latest algorithms. Experimental results show that our algorithm can align ABR strategy with visual sensitivity to achieve better QoE in high visual sensitivity content, and improves the average perceptual video quality and overall user QoE by 18.3\% and 22.8\% respectively. Additionally, we prove the feasibility of our algorithm through subjective evaluation in the real environment.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jun},
keywords = {QoE, DASH, ABR, deep reinforcement learning, visual sensitivity}
}

@inproceedings{10.1145/3357384.3358046,
author = {Liu, Ye and Zhang, Chenwei and Yan, Xiaohui and Chang, Yi and Yu, Philip S.},
title = {Generative Question Refinement with Deep Reinforcement Learning in Retrieval-Based QA System},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358046},
doi = {10.1145/3357384.3358046},
abstract = {In real-world question-answering (QA) systems, ill-formed questions, such as wrong words, ill word order and noisy expressions, are common and may prevent the QA systems from understanding and answering them accurately. In order to eliminate the effect of ill-formed questions, we approach the question refinement task and propose a unified model, QREFINE, to refine the ill-formed questions to well-formed question. The basic idea is to learn a Seq2Seq model to generate a new question from the original one. To improve the quality and retrieval performance of the generated questions, we make two major improvements: 1) To better encode the semantics of ill-formed questions, we enrich the representation of questions with character embedding and the recent proposed contextual word embedding such as BERT, besides the traditional context-free word embeddings; 2) To make it capable to generate desired questions, we train the model with deep reinforcement learning techniques that considers an appropriate wording of the generation as an immediate reward and the correlation between generated question and answer as time-delayed long-term rewards. Experimental results on real-world datasets show that the proposed QREFINE method can generate refined questions with more readability but fewer mistakes than the original questions provided by users. Moreover, the refined questions also significantly improve the accuracy of answer retrieval.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1643–1652},
numpages = {10},
keywords = {off-policy reinforcement learning, question refinement, multigrain word embedding},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1007/978-3-642-27216-5_23,
author = {Dasgupta, Prithviraj and Cheng, Ke and Banerjee, Bikramjit},
title = {Adaptive Multi-Robot Team Reconfiguration Using a Policy-Reuse Reinforcement Learning Approach},
year = {2011},
isbn = {9783642272158},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-27216-5_23},
doi = {10.1007/978-3-642-27216-5_23},
abstract = {We consider the problem of dynamically adjusting the formation and size of robot teams performing distributed area coverage, when they encounter obstacles or occlusions along their path. Based on our earlier formulation of the robotic team formation problem as a coalitional game called a weighted voting game (WVG), we show that the robot team size can be dynamically adapted by adjusting the WVG's quota parameter. We use a Q-learning algorithm to learn the value of the quota parameter and a policy reuse mechanism to adapt the learning process to changes in the underlying environment. Experimental results using simulated e-puck robots within the Webots simulator show that our Q-learning algorithm converges within a finite number of steps in different types of environments. Using the learning algorithm also improves the performance of an area coverage application where multiple robot teams move in formation to explore an initially unknown environment by 5−10\%.},
booktitle = {Proceedings of the 10th International Conference on Advanced Agent Technology},
pages = {330–345},
numpages = {16},
keywords = {Q-learning, multi-robot formation, coalition game},
location = {Taipei, Taiwan},
series = {AAMAS'11}
}

@inproceedings{10.1145/3056540.3076191,
author = {Tsiakas, Konstantinos and Papakostas, Michalis and Theofanidis, Michail and Bell, Morris and Mihalcea, Rada and Wang, Shouyi and Burzo, Mihai and Makedon, Fillia},
title = {An Interactive Multisensing Framework for Personalized Human Robot Collaboration and Assistive Training Using Reinforcement Learning},
year = {2017},
isbn = {9781450352277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3056540.3076191},
doi = {10.1145/3056540.3076191},
abstract = {There is a recent trend of research and applications of Cyber-Physical Systems (CPS) in manufacturing to enhance human-robot collaboration and production. In this paper, we propose a CPS framework for personalized Human-Robot Collaboration and Training to promote safe human-robot collaboration in manufacturing environments. We propose a human-centric CPS approach that focuses on multimodal human behavior monitoring and assessment, to promote human worker safety and enable human training in Human-Robot Collaboration tasks. We present the architecture of our proposed system, our experimental testbed and our proposed methods for multimodal physiological sensing, human state monitoring and interactive robot adaptation, to enable personalized interaction.},
booktitle = {Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {423–427},
numpages = {5},
keywords = {Human Robot Collaboration, Intelligent Manufacturing, Vocational Assessment and Training, Cyber Physical Systems},
location = {Island of Rhodes, Greece},
series = {PETRA '17}
}

@inproceedings{10.1145/3459104.3459123,
author = {Eichenlaub, Tobias and Rinderknecht, Stephan},
title = {Intelligent Set Speed Estimation for Vehicle Longitudinal Control with Deep Reinforcement Learning},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459123},
doi = {10.1145/3459104.3459123},
abstract = {Besides the goal of reducing driving tasks, modern longitudinal control systems also aim to improve fuel efficiency and driver comfort. Most of the vehicles use Adaptive Cruise Control (ACC) systems that track constant set speeds and set headways which makes the trajectory of the vehicle in headway mode highly dependent on the trajectory of a preceding vehicle. Hence, this might lead to increased consumptions in dense traffic situations or when the leader has a less careful driving style. In this work, a method based on Deep Reinforcement Learning (DRL) is presented that finds a control strategy by estimating an intelligent variable set speed based on the system state. Additional control objectives, such as minimizing consumption, are considered explicitly through the feedback in a reward function. A DRL framework is set up that enables the training of a neural set speed estimator for vehicle longitudinal control in a simulative environment. The Deep Deterministic Policy Gradient algorithm is used for the training of the agent. Training is carried out on a simple test track to teach the basic concepts of the control objective to the DRL agent. The learned behavior is then examined in a more complex, stochastic microscopic traffic simulation of the city center of Darmstadt and is compared to a conventional ACC algorithm. The analysis shows that the DRL controller is capable of finding fuel efficient trajectories which are less dependent on the preceding vehicle and is able to generalize to more complex traffic environments, but still shows some unexpected behavior in certain situations. The combination of DRL and conventional models to build up on the existing engineering knowledge is therefore expected to yield promising results in the future.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {101–107},
numpages = {7},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@inproceedings{10.1145/3308558.3313401,
author = {He, Suining and Shin, Kang G.},
title = {Spatio-Temporal Capsule-Based Reinforcement Learning for Mobility-on-Demand Network Coordination},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313401},
doi = {10.1145/3308558.3313401},
abstract = {As an alternative means of convenient and smart transportation, mobility-on-demand (MOD), typified by online ride-sharing and connected taxicabs, has been rapidly growing and spreading worldwide. The large volume of complex traffic and the uncertainty of market supplies/demands have made it essential for many MOD service providers to proactively dispatch vehicles towards ride-seekers. To meet this need effectively, we propose STRide, an MOD coordination-learning mechanism reinforced spatio-temporally with capsules. We formalize the adaptive coordination of vehicles into a reinforcement learning framework. STRide incorporates spatial and temporal distributions of supplies (vehicles) and demands (ride requests), customers' preferences and other external factors. A novel spatio-temporal capsule neural network is designed to predict the provider's rewards based on MOD network states, vehicles and their dispatch actions. This way, the MOD platform adapts itself to the supply-demand dynamics with the best potential rewards. We have conducted extensive data analytics and experimental evaluation with three large-scale datasets (~ 21 million rides from Uber, Yellow Taxis and Didi). STRide is shown to outperform state-of-the-arts, substantially reducing request-rejection rate and passenger waiting time, and also increasing the service provider's profits, often making 30\% improvement over state-of-the-arts.},
booktitle = {The World Wide Web Conference},
pages = {2806–2813},
numpages = {8},
keywords = {reinforcement learning, capsule network, Mobility-on-demand, smart transportation coordination, smart city., ride-sharing platform},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3580305.3599900,
author = {Wei, Penghui and Chen, Yongqiang and Liu, ShaoGuo and Wang, Liang and Zheng, Bo},
title = {RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599900},
doi = {10.1145/3580305.3599900},
abstract = {To increase brand awareness, many advertisers conclude contracts with advertising platforms to purchase traffic and deliver advertisements to target audiences. In a whole delivery period, advertisers desire a certain impression count for the ads, and they expect that the delivery performance is as good as possible. Advertising platforms employ real-time pacing algorithms to satisfy the demands. However, the delivery procedure is also affected by publishers. Preloading is a widely used strategy for many types of ads (e.g., video ads) to make sure that the response time for displaying is legitimate, which results in delayed impression phenomenon. In this paper, we focus on a new research problem of impression pacing for preloaded ads, and propose a Reinforcement Learning To Pace framework RLTP. It learns a pacing agent that sequentially produces selection probabilities in the whole delivery period. To jointly optimize the objectives of impression count and delivery performance, RLTP employs tailored reward estimator to satisfy guaranteed impression count, penalize over-delivery and maximize traffic value. Experiments on large-scale datasets verify that RLTP outperforms baselines by a large margin. We have deployed it online to our advertising platform, and it achieves significant uplift for delivery completion rate and click-through rate.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5204–5214},
numpages = {11},
keywords = {reinforcement learning, pacing algorithms, online advertising},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.5555/3535850.3536005,
author = {Xiao, Baicen and Ramasubramanian, Bhaskar and Poovendran, Radha},
title = {Agent-Temporal Attention for Reward Redistribution in Episodic Multi-Agent Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper considers multi-agent reinforcement learning (MARL) tasks where agents receive a shared global reward at the end of an episode. The delayed nature of this reward affects the ability of the agents to assess the quality of their actions at intermediate time-steps. This paper focuses on developing methods to learn a temporal redistribution of the episodic reward to obtain a dense reward signal. Solving such MARL problems requires addressing two challenges: identifying (1) relative importance of states along the length of an episode (along time), and (2) relative importance of individual agents' states at any single time-step (among agents). In this paper, we introduce Agent-Temporal Attention for Reward Redistribution in Episodic Multi-Agent Reinforcement Learning (AREL) to address these two challenges. AREL uses attention mechanisms to characterize the influence of actions on state transitions along trajectories (temporal attention), and how each agent is affected by other agents at each time-step (agent attention). The redistributed rewards predicted by AREL are dense, and can be integrated with any given MARL algorithm. We evaluate AREL on challenging tasks from the Particle World environment and the StarCraft Multi-Agent Challenge. AREL results in higher rewards in Particle World, and improved win rates in StarCraft compared to three state-of-the-art reward redistribution methods. Our code is available at https://github.com/baicenxiao/AREL.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1391–1399},
numpages = {9},
keywords = {maulti-agent reinforcement learning, episodic rewards, attention mechanism, credit assignment},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3426826.3426835,
author = {Rais Mart\'{\i}nez, Jasmina and Aznar Gregori, Fidel},
title = {Comparison of Evolutionary Strategies for Reinforcement Learning in a Swarm Aggregation Behaviour},
year = {2020},
isbn = {9781450388344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426826.3426835},
doi = {10.1145/3426826.3426835},
abstract = {This article studies the performance of different evolutionary strategies for deep reinforcement learning policy optimization. The policy will be centred in an important swarm robotic task: the aggregation of simple robots in the environment. The main inspiration for robotic swarm comes from the observation of social animals. Ants, bees, birds, and fish are some examples of how simple individuals can succeed when they gather in groups. In addition, is important to highlight that aggregation may be considered as a previous requirement to tackle another tasks.Due the design of a swarm behaviour is a comprehensive process of experimentation, one of the current solutions is learn a policy able to control a robot. Gradient descent techniques have demonstrated their learning power in the field of neural networks and reinforcement learning, among others. But some of these techniques are difficult to be applied in the field of robotics because the requirements needed to calculate the gradient to be informative are not met. For that reason, in this article we are going to use and compare the evolutionary strategies CMA-ES, PEPG, SES, GA y OpenAI-ES. A fast simulator, based in the differential robot Mbot Ranger, will be used. Once the aggregation task is learned we will compare each strategy for different swarm sizes to analyse the convergence time, the quality of the policies, its scalability and its capacity to generalize.},
booktitle = {Proceedings of the 2020 3rd International Conference on Machine Learning and Machine Intelligence},
pages = {40–45},
numpages = {6},
keywords = {Evolutionary Strategies, Evolutionary Robotics, Swarm Robotics, Deep Reinforcement Learning, Swarm Intelligence},
location = {Hangzhou, China},
series = {MLMI '20}
}

@article{10.1145/3579443,
author = {Zhang, Yizhou and Qu, Guannan and Xu, Pan and Lin, Yiheng and Chen, Zaiwei and Wierman, Adam},
title = {Global Convergence of Localized Policy Iteration in Networked Multi-Agent Reinforcement Learning},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/3579443},
doi = {10.1145/3579443},
abstract = {We study a multi-agent reinforcement learning (MARL) problem where the agents interact over a given network. The goal of the agents is to cooperatively maximize the average of their entropy-regularized long-term rewards. To overcome the curse of dimensionality and to reduce communication, we propose a Localized Policy Iteration (LPI) algorithm that provably learns a near-globally-optimal policy using only local information. In particular, we show that, despite restricting each agent's attention to only its κ-hop neighborhood, the agents are able to learn a policy with an optimality gap that decays polynomially in κ. In addition, we show the finite-sample convergence of LPI to the global optimal policy, which explicitly captures the trade-off between optimality and computational complexity in choosing κ. Numerical simulations demonstrate the effectiveness of LPI.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {mar},
articleno = {13},
numpages = {51},
keywords = {distributed algorithms, networked systems, multi-agent reinforcement learning, machine learning}
}

@inproceedings{10.1145/3325730.3325732,
author = {Meng, Hao and Chao, Daichong and Guo, Qianying},
title = {Deep Reinforcement Learning Based Task Offloading Algorithm for Mobile-Edge Computing Systems},
year = {2019},
isbn = {9781450362580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325730.3325732},
doi = {10.1145/3325730.3325732},
abstract = {Mobile-edge computing(MEC) is deemed to a promising paradigm. By deploying high-performance servers on the mobile access network side, MEC can provide auxiliary computing power for mobile devices, greatly reducing the computing pressure of mobile devices and improving the quality of the computing experience. In this paper, we consider the offloading problem of tasks in single-user MEC system. In order to minimize the mean energy consumption of mobile devices and the mean slowdown of tasks in the queue, we propose a deep reinforcement learning(DRL) based task offloading algorithm, and a new reward function is designed, which can guide the algorithm to optimize the trade-off between mean energy consumption and mean slowdown. The simulation results show that the deep reinforcement learning based algorithm outperforms the baseline algorithms.},
booktitle = {Proceedings of the 2019 4th International Conference on Mathematics and Artificial Intelligence},
pages = {90–94},
numpages = {5},
keywords = {Mobile-edge computing(MEC), deep reinforcement learning(DRL), task offloading, mean slowdown, mean energy consumption},
location = {Chegndu, China},
series = {ICMAI '19}
}

@inproceedings{10.1145/3427773.3427863,
author = {Spangher, Lucas and Gokul, Akash and Khattar, Manan and Palakapilly, Joseph and Agwan, Utkarsha and Tawade, Akaash and Spanos, Costas},
title = {Augmenting Reinforcement Learning with a Planning Model for Optimizing Energy Demand Response},
year = {2020},
isbn = {9781450381932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427773.3427863},
doi = {10.1145/3427773.3427863},
abstract = {While reinforcement learning (RL) on humans has shown incredible promise, it often suffers from a scarcity of data and few steps. In instances like these, a planning model of human behavior may greatly help. We present an experimental setup for the development and testing of an Soft Actor Critic (SAC) V2 RL architecture for several different neural architectures for planning models: an autoML optimized LSTM, an OLS, and a baseline model. We present the effects of including a planning model in agent learning within a simulation of the office, currently reporting a limited success with the LSTM.},
booktitle = {Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings \&amp; Cities},
pages = {39–42},
numpages = {4},
keywords = {Transactive control learning, Planning models for Reinforcement Learning, Energy demand response, Social Energy competitions, Reinforcement learning, AutoML neural architecture search},
location = {Virtual Event, Japan},
series = {RLEM'20}
}

@inproceedings{10.1145/2701126.2701191,
author = {Ho, Han Nguyen and Lee, Eunseok},
title = {Model-Based Reinforcement Learning Approach for Planning in Self-Adaptive Software System},
year = {2015},
isbn = {9781450333771},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701126.2701191},
doi = {10.1145/2701126.2701191},
abstract = {Policy-based adaptation is one of interesting topics in self-adaptive software research community. Current works in the field proposed the term of policy evolution, which concentrate to tackle the impact of environmental uncertainty on adaptation decision. These works adopted the advances of Reinforcement Learning (RL) to continuously optimize system behavior in run-time. However, there are several issues remain very primitive in current researches, especially the arbitrary exploitation--exploration trade-off and random exploration, which could lead to slow learning, hence, frail decision in exceptional situations. With model-free approach, these works could not leverage the knowledge about underlying system, which is essential and plentiful in software engineering, to enhance their learning. In this paper, we introduce the advantages of model-based RL. By utilizing engineering knowledge, system maintains a model of interaction with its environment and predicts the consequence of its action, to improve and guarantee system performance. We also discuss the engineering issues and propose a procedure to adopt model-based RL to build a self-adaptive software and bring policy evolution closer to real-world applications.},
booktitle = {Proceedings of the 9th International Conference on Ubiquitous Information Management and Communication},
articleno = {103},
numpages = {8},
keywords = {Bayesian inference, reinforcement learning, model-based RL, policy evolution},
location = {Bali, Indonesia},
series = {IMCOM '15}
}

@article{10.5555/3586589.3586763,
author = {Ghadirzadeh, Ali and Poklukar, Petra and Arndt, Karol and Finn, Chelsea and Kyrki, Ville and Kragic, Danica and Bj\"{o}rkman, M\r{a}rten},
title = {Training and Evaluation of Deep Policies Using Reinforcement Learning and Generative Models},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {We present a data-efficient framework for solving sequential decision-making problems which exploits the combination of reinforcement learning (RL) and latent variable generative models. The framework, called GenRL, trains deep policies by introducing an action latent variable such that the feed-forward policy search can be divided into two parts: (i) training a sub-policy that outputs a distribution over the action latent variable given a state of the system, and (ii) unsupervised training of a generative model that outputs a sequence of motor actions conditioned on the latent action variable. GenRL enables safe exploration and alleviates the data-inefficiency problem as it exploits prior knowledge about valid sequences of motor actions. Moreover, we provide a set of measures for evaluation of generative models such that we are able to predict the performance of the RL policy training prior to the actual training on a physical robot. We experimentally determine the characteristics of generative models that have most influence on the performance of the final policy training on two robotics tasks: shooting a hockey puck and throwing a basketball. Furthermore, we empirically demonstrate that GenRL is the only method which can safely and efficiently solve the robotics tasks compared to two state-of-the-art RL methods.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {174},
numpages = {37},
keywords = {representation learning, reinforcement learning, robot learning, policy search, deep generative models}
}

@inproceedings{10.1145/2396761.2398676,
author = {Chali, Yllias and Hasan, Sadid A. and Imam, Kaisar},
title = {Improving the Performance of the Reinforcement Learning Model for Answering Complex Questions},
year = {2012},
isbn = {9781450311564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396761.2398676},
doi = {10.1145/2396761.2398676},
abstract = {This paper addresses the task of answering complex questions using a multi-document summarization approach within a reinforcement learning setting. Given a set of complex questions, a list of relevant documents per question, and the corresponding human-generated summaries (i.e. answers to the questions) as training data, the reinforcement learning module iteratively learns a number of feature weights in order to facilitate the automatic generation of summaries i.e. answers to unseen complex questions. Previous works on this task have utilized a fully automatic reinforcement learning framework that selects the document sentences as the potential candidate (i.e. machine-generated) summary sentences by exploiting a relatedness measure with the available human-written summaries. In this paper, we propose an extension to this model that incorporates user interaction into the reinforcement learner to guide the candidate summary sentence selection process. Experimental results reveal the effectiveness of the user interaction component in the reinforcement learning framework.},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
pages = {2499–2502},
numpages = {4},
keywords = {complex question answering, user interaction, multi-document summarization, reinforcement learning},
location = {Maui, Hawaii, USA},
series = {CIKM '12}
}

@article{10.1145/3475991,
author = {Ray, Kaustabha and Banerjee, Ansuman},
title = {Horizontal Auto-Scaling for Multi-Access Edge Computing Using Safe Reinforcement Learning},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3475991},
doi = {10.1145/3475991},
abstract = {Multi-Access Edge Computing (MEC) has emerged as a promising new paradigm allowing low latency access to services deployed on edge servers to avert network latencies often encountered in accessing cloud services. A key component of the MEC environment is an auto-scaling policy which is used to decide the overall management and scaling of container instances corresponding to individual services deployed on MEC servers to cater to traffic fluctuations. In this work, we propose a Safe Reinforcement Learning (RL)-based auto-scaling policy agent that can efficiently adapt to traffic variations to ensure adherence to service specific latency requirements. We model the MEC environment using a Markov Decision Process (MDP). We demonstrate how latency requirements can be formally expressed in Linear Temporal Logic (LTL). The LTL specification acts as a guide to the policy agent to automatically learn auto-scaling decisions that maximize the probability of satisfying the LTL formula. We introduce a quantitative reward mechanism based on the LTL formula to tailor service specific latency requirements. We prove that our reward mechanism ensures convergence of standard Safe-RL approaches. We present experimental results in practical scenarios on a test-bed setup with real-world benchmark applications to show the effectiveness of our approach in comparison to other state-of-the-art methods in literature. Furthermore, we perform extensive simulated experiments to demonstrate the effectiveness of our approach in large scale scenarios.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {oct},
articleno = {109},
numpages = {33},
keywords = {safe reinforcement learning, auto-scaling, Multi-access edge computing}
}

@article{10.5555/3455716.3455883,
author = {Kallus, Nathan and Uehara, Masatoshi},
title = {Double Reinforcement Learning for Efficient Off-Policy Evaluation in Markov Decision Processes},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Off-policy evaluation (OPE) in reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration, which is often costly or otherwise infeasible. We consider for the first time the semiparametric efficiency limits of OPE in Markov decision processes (MDPs), where actions, rewards, and states are memoryless. We show existing OPE estimators may fail to be efficient in this setting. We develop a new estimator based on cross-fold estimation of q-functions and marginalized density ratios, which we term double reinforcement learning (DRL). We show that DRL is efficient when both components are estimated at fourth-root rates and is also doubly robust when only one component is consistent. We investigate these properties empirically and demonstrate the performance benefits due to harnessing memorylessness.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {167},
numpages = {63},
keywords = {semiparametric efficiency, double machine learning, off-policy evaluation, Markov decision processes}
}

@inproceedings{10.1145/3534678.3539416,
author = {Zhang, Weijia and Liu, Hao and Han, Jindong and Ge, Yong and Xiong, Hui},
title = {Multi-Agent Graph Convolutional Reinforcement Learning for Dynamic Electric Vehicle Charging Pricing},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539416},
doi = {10.1145/3534678.3539416},
abstract = {Electric Vehicles (EVs) have been emerging as a promising low-carbon transport target. While a large number of public charging stations are available, the use of these stations is often imbalanced, causing many problems to Charging Station Operators (CSOs). To this end, in this paper, we propose a Multi-Agent Graph Convolutional Reinforcement Learning (MAGC) framework to enable CSOs to achieve more effective use of these stations by providing dynamic pricing for each of the continuously arising charging requests with optimizing multiple long-term commercial goals. Specifically, we first formulate this charging station request-specific dynamic pricing problem as a mixed competitive-cooperative multi-agent reinforcement learning task, where each charging station is regarded as an agent. Moreover, by modeling the whole charging market as a dynamic heterogeneous graph, we devise a multi-view heterogeneous graph attention networks to integrate complex interplay between agents induced by their diversified relationships. Then, we propose a shared meta generator to generate individual customized dynamic pricing policies for large-scale yet diverse agents based on the extracted meta characteristics. Finally, we design a contrastive heterogeneous graph pooling representation module to learn a condensed yet effective state action representation to facilitate policy learning of large-scale agents. Extensive experiments on two real-world datasets demonstrate the effectiveness of MAGC and empirically show that the overall use of stations can be improved if all the charging stations in a charging market embrace our dynamic pricing policy.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2471–2481},
numpages = {11},
keywords = {multi-agent reinforcement learning, charging station dynamic pricing, graph neural networks, graph contrastive learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3555858.3563282,
author = {Wang, Ziqi and Liu, Jialin and Yannakakis, Georgios N.},
title = {The Fun Facets of Mario: Multifaceted Experience-Driven PCG via Reinforcement Learning},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3563282},
doi = {10.1145/3555858.3563282},
abstract = {The recently introduced EDRL framework approaches the experience-driven (ED) procedural generation of game content via a reinforcement learning (RL) perspective. EDRL has so far shown its effectiveness in generating novel platformer game levels endlessly in an online fashion. This paper extends the framework by integrating multiple facets of game creativity in the ED generation process. In particular, we employ EDRL on the creative facets of game level and gameplay design in Super Mario Bros. Inspired by Koster’s theory of fun, we formulate fun as moderate degrees of level or gameplay divergence and equip the algorithm with such reward functions. Moreover, we enable faster and more efficient game content generation through an episodic generative soft actor-critic algorithm. The resulting multifaceted EDRL is not only capable of generating fun levels efficiently, but it is also robust with respect to dissimilar playing styles and initial game level conditions.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {44},
numpages = {8},
keywords = {Experience-driven procedural content generation, procedural content generation via reinforcement learning, platformer games, online level generation, Super Mario Bros},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3580305.3599379,
author = {Zhang, Hancheng and Li, Guozheng and Liu, Chi Harold and Wang, Guoren and Tang, Jian},
title = {HiMacMic: Hierarchical Multi-Agent Deep Reinforcement Learning with Dynamic Asynchronous Macro Strategy},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599379},
doi = {10.1145/3580305.3599379},
abstract = {Multi-agent deep reinforcement learning (MADRL) has been widely used in many scenarios such as robotics and game AI. However, existing methods mainly focus on the optimization of agents' micro policies without considering the macro strategy. As a result, they cannot perform well in complex or sparse reward scenarios like the StarCraft Multi-Agent Challenge (SMAC) and Google Research Football (GRF). To this end, we propose a hierarchical MADRL framework called "HiMacMic" with dynamic asynchronous macro strategy. Spatially, HiMacMic determines a critical position by using a positional heat map. Temporally, the macro strategy dynamically decides its deadline and updates it asynchronously among agents. We validate HiMacMic in four widely used benchmarks, namely: Overcooked, GRF, SMAC and SMAC-v2 with nine chosen scenarios. Results show that HiMacMic not only converges faster and achieves higher results than ten existing approaches, but also shows its adaptability to different environment settings.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3239–3248},
numpages = {10},
keywords = {macro strategy, multi-agent deep reinforcement learning},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.5555/3545946.3598721,
author = {Haider, Tom and Roscher, Karsten and Schmoeller da Roza, Felippe and G\"{u}nnemann, Stephan},
title = {Out-of-Distribution Detection for Reinforcement Learning Agents with Probabilistic Dynamics Models},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reliability of reinforcement learning (RL) agents is a largely unsolved problem. Especially in situations that substantially differ from their training environment, RL agents often exhibit unpredictable behavior, potentially leading to performance loss, safety violations or catastrophic failure. Reliable decision making agents should therefore be able to cast an alert whenever they encounter situations they have never seen before and do not know how to handle. While the problem, also known as out-of-distribution (OOD) detection, has received considerable attention in other domains such as image classification or sensory data analysis, it is less frequently studied in the context of RL. In fact, there is not even a common understanding of what OOD actually means in RL. In this work, we want to bridge this gap and approach the topic of OOD in RL from a general perspective. For this, we formulate OOD in RL as severe perturbations of the Markov decision process (MDP). To detect such perturbations, we introduce a predictive algorithm utilizing probabilistic dynamics models and bootstrapped ensembles. Since existing benchmarks are sparse and limited in their complexity, we also propose a set of evaluation scenarios with OOD occurrences. A detailed analysis of our approach shows superior detection performance compared to existing baselines from related fields.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {851–859},
numpages = {9},
keywords = {sequential decision making, OOD detection, anomaly detection, reinforcement learning, AI safety},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3543712.3543722,
author = {N\'{e}meth, Marcell and Szundefinedcs, G\'{a}bor},
title = {Split Feature Space Ensemble Method Using Deep Reinforcement Learning for Algorithmic Trading},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543712.3543722},
doi = {10.1145/3543712.3543722},
abstract = {In the financial sector, machine learning is a promising tool, which can be utilized in stock trading as well. The aim of the research was to develop and refine deep reinforcement learning models to execute stock trading that maximizes revenue and minimizes investment risk. The main focus of this paper is on a new ensemble technique based on splitting the feature space and the optimization of decision-making by agents learning in parallel. As the consequence of the splitting, the space formed by all input features is replaced by subspaces, where each subspace is covered by a functional group of technical indicators. Based on the feature space splitting idea, we proposed a new ensemble method, called the Split Feature Space Ensemble Method, and developed three model variants of it. The experimental results show that the proposed method outperforms the standard ensemble approach.},
booktitle = {Proceedings of the 2022 8th International Conference on Computer Technology Applications},
pages = {188–194},
numpages = {7},
keywords = {ensemble method, deep reinforcement learning, algorithmic trading, Sharpe ratio, feature space},
location = {Vienna, Austria},
series = {ICCTA '22}
}

@inproceedings{10.1145/3487351.3490973,
author = {Antaris, Stefanos and Rafailidis, Dimitrios and Girdzijauskas, Sarunas},
title = {Meta-Reinforcement Learning via Buffering Graph Signatures for Live Video Streaming Events},
year = {2022},
isbn = {9781450391283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487351.3490973},
doi = {10.1145/3487351.3490973},
abstract = {In this study, we present a meta-learning model to adapt the predictions of the network's capacity between viewers who participate in a live video streaming event. We propose the MELANIE model, where an event is formulated as a Markov Decision Process, performing meta-learning on reinforcement learning tasks. By considering a new event as a task, we design an actor-critic learning scheme to compute the optimal policy on estimating the viewers' high-bandwidth connections. To ensure fast adaptation to new connections or changes among viewers during an event, we implement a prioritized replay memory buffer based on the Kullback-Leibler divergence of the reward/throughput of the viewers' connections. Moreover, we adopt a model-agnostic meta-learning framework to generate a global model from past events. As viewers scarcely participate in several events, the challenge resides on how to account for the low structural similarity of different events. To combat this issue, we design a graph signature buffer to calculate the structural similarities of several streaming events and adjust the training of the global model accordingly. We evaluate the proposed model on the link weight prediction task on three real-world datasets of live video streaming events. Our experiments demonstrate the effectiveness of our proposed model, with an average relative gain of 25\% against state-of-the-art strategies. For reproduction purposes, our evaluation datasets and implementation are publicly available at https://github.com/stefanosantaris/melanie},
booktitle = {Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {385–392},
numpages = {8},
keywords = {video streaming, graph signatures, meta-reinforcement learning},
location = {Virtual Event, Netherlands},
series = {ASONAM '21}
}

@inproceedings{10.1145/3604915.3608795,
author = {Zheng, Zhi and Sun, Ying and Song, Xin and Zhu, Hengshu and Xiong, Hui},
title = {Generative Learning Plan Recommendation for Employees: A Performance-Aware Reinforcement Learning Approach},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608795},
doi = {10.1145/3604915.3608795},
abstract = {With the rapid development of enterprise Learning Management Systems (LMS), more and more companies are trying to build enterprise training and course learning platforms for promoting the career development of employees. Indeed, through course learning, many employees have the opportunity to improve their knowledge and skills. For these systems, a major issue is how to recommend learning plans, i.e., a set of courses arranged in the order they should be learned, that can help employees improve their work performance. Existing studies mainly focus on recommending courses that users are most likely to click on by capturing their learning preferences. However, the learning preference of employees may not be the right fit for their career development, and thus it may not necessarily mean their work performance can be improved accordingly. Furthermore, how to capture the mutual correlation and sequential effects between courses, and ensure the rationality of the generated results, is also a major challenge. To this end, in this paper, we propose the Generative Learning plAn recommenDation (GLAD) framework, which can generate personalized learning plans for employees to help them improve their work performance. Specifically, we first design a performance predictor and a rationality discriminator, which have the same transformer-based model architecture, but with totally different parameters and functionalities. In particular, the performance predictor is trained for predicting the work performance of employees based on their work profiles and historical learning records, while the rationality discriminator aims to evaluate the rationality of the generated results. Then, we design a learning plan generator based on the gated transformer and the cross-attention mechanism for learning plan generation. We calculate the weighted sum of the output from the performance predictor and the rationality discriminator as the reward, and we use Self-Critical Sequence Training (SCST) based policy gradient methods to train the generator following the Generative Adversarial Network (GAN) paradigm. Finally, extensive experiments on real-world data clearly validate the effectiveness of our GLAD framework compared with state-of-the-art baseline methods and reveal some interesting findings for talent management.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {443–454},
numpages = {12},
keywords = {generative recommendation, learning management system, reinforcement learning},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@inproceedings{10.1145/3549737.3549808,
author = {Vouros, George},
title = {Tutorial on Explainable Deep Reinforcement Learning: One Framework, Three Paradigms and Many Challenges.},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549737.3549808},
doi = {10.1145/3549737.3549808},
abstract = {Interpretability, explainability and transparency are key issues to introducing Artificial Intelligence closed—box methods in many critical domains: This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability and fairness, and has important consequences towards keeping the human in the loop in high levels of automation, especially in critical cases for decision making. Reinforcement learning methods, and especially their deep versions, are closed-box methods that support agents to act autonomously in the real world. This tutorial will provide a formal specification of the deep reinforcement learning explainability problems, and will present the necessary components of a general explainable reinforcement learning framework. Based on this framework will provide distinct explainability paradigms towards solving explainability problems, with examples from state-of-the-art methods and real-world cases. The tutorial will conclude identifying open questions and important challenges. The tutorial is based on the survey paper on “Explainable Deep Reinforcement Learning” State of the Art and Challenges” [1].},
booktitle = {Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
articleno = {67},
numpages = {1},
keywords = {Interpretability, Transparency, Deep Learning, Explainability, Deep Reinforcement Learning},
location = {Corfu, Greece},
series = {SETN '22}
}

@article{10.1162/153244303765208377,
author = {Brafman, Ronen I. and Tennenholtz, Moshe},
title = {R-Max - a General Polynomial Time Algorithm for near-Optimal Reinforcement Learning},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
url = {https://doi.org/10.1162/153244303765208377},
doi = {10.1162/153244303765208377},
abstract = {R-MAX is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-MAX, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-MAX improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh's E3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the ``optimism under uncertainty'' bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {213–231},
numpages = {19},
keywords = {stochastic games, Markov decision processes, learning in games, reinforcement learning, provably efficient learning}
}

@inproceedings{10.5555/3581644.3581668,
author = {Ghnaya, Imed and Ahmed, Toufik and Mosbah, Mohamed and Aniss, Hasnaa},
title = {Maximizing Information Usefulness in Vehicular CP Networks Using Actor-Critic Reinforcement Learning},
year = {2023},
isbn = {9783903176515},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {Cooperative Perception (CP) allows Connected and Autonomous Vehicles (CAVs) to enhance their Environmental Awareness (EA) by sharing locally perceived objects through CP messages (CPMs). European Telecommunications Standards Institute (ETSI) has recently defined a set of CPM generation rules to achieve a trade-off between EA and Channel Busy Ratio (CBR) despite massive perception data. Nonetheless, these rules still lack the consideration of information usefulness, resulting in a considerable volume of useless information transmitted in the CP network. This limitation could increase CBR and thus decrease EA due to the loss of CPMs in the network. This paper introduces CloudAC-IU, a cloud-based deep reinforcement learning approach to lean CAVs to maximize perception information usefulness in the network. Simulation results highlight that the CloudAC-IU enhances EA by decreasing CBR and increasing CPM reception for CAVs compared to state-of-the-art works.},
booktitle = {Proceedings of the 18th International Conference on Network and Service Management},
articleno = {20},
numpages = {7},
keywords = {advantage actor-critic, V2X communications, reinforcement learning, cooperative perception, connected and autonomous vehicles},
location = {Thessaloniki, Greece},
series = {CNSM '22}
}

@inproceedings{10.5555/2888619.2888848,
author = {Rabe, Markus and Dross, Felix},
title = {A Reinforcement Learning Approach for a Decision Support System for Logistics Networks},
year = {2015},
isbn = {9781467397414},
publisher = {IEEE Press},
abstract = {This paper presents the architecture and working principles of a Decision Support System (DSS) for logistics networks. The system relies on a data-driven discrete-event simulation model. A brief introduction to Reinforcement Learning (RL) and an explanation of the adoption of RL to the concepts of the DSS is given. An illustration of the realization is presented using a specific aspect of a logistics network. The logistics network is described in a data model which is represented by database tables. The tables are used to dynamically instantiate the simulation model. The authors describe how SQL queries can be used to model actions of an RL agent. A Data Warehouse can be used to measure Key Performance Indicators on the simulation output data of the simulation model, which can be used as a reward criterion for the RL agent. The paper presents a basis for the ongoing development of an RL agent.},
booktitle = {Proceedings of the 2015 Winter Simulation Conference},
pages = {2020–2032},
numpages = {13},
location = {Huntington Beach, California},
series = {WSC '15}
}

@inproceedings{10.5555/3545946.3598759,
author = {Zhao, Rui and Liu, Xu and Zhang, Yizheng and Li, Minghao and Zhou, Cheng and Li, Shuai and Han, Lei},
title = {CraftEnv: A Flexible Collective Robotic Construction Environment for Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {CraftEnv is a flexible Collective Robotic Construction (CRC) environment for Multi-Agent Reinforcement Learning (MARL) research. CraftEnv can be used to study how artificial intelligent agents may learn to cooperate and solve complex real world tasks, such as collective construction and intelligent warehousing. The environment contains a set of collective construction tasks, which require a group of robotic vehicles to cooperate and learn to build different constructions efficiently. There are different elements in the CraftEnv, such as smartcars, blocks, and slopes. The smartcars can use the blocks and slopes to build different structures. The CraftEnv is highly flexible and simple to use, which enables creative and quick task-designs. The environment is written in python and can be rendered using PyBullet. The simulation is built based on real world robotic systems, designed with real-world constraints in mind. The learned policy can be transferred to the real world robotic system. CraftEnv is tailored for effective use by the research community and pushing forward collective intelligence and swarm technology.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1164–1172},
numpages = {9},
keywords = {multi-agent reinforcement learning, collective intelligence},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1109/WI-IAT.2012.161,
author = {Cilden, Erkin and Polat, Faruk},
title = {Abstraction in Model Based Partially Observable Reinforcement Learning Using Extended Sequence Trees},
year = {2012},
isbn = {9780769548807},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2012.161},
doi = {10.1109/WI-IAT.2012.161},
abstract = {Extended sequence tree is a direct method for automatic generation of useful abstractions in reinforcement learning, designed for problems that can be modelled as Markov decision process. This paper proposes a method to expand the extended sequence tree method over reinforcement learning to cover partial observability formalized via partially observable Markov decision process through belief state formalism. This expansion requires a reasonable approximation of information state. Inspired by statistical ranking, a simple but effective discretization schema over belief state space is defined. Extended sequence tree method is modified to make use of this schema under partial observability, and effectiveness of resulting algorithm is shown by experiments on some benchmark problems.},
booktitle = {Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {348–355},
numpages = {8},
keywords = {extended sequence tree, reinforcement learning, learning abstractions, partially observable markov decision process},
series = {WI-IAT '12}
}

@inproceedings{10.1145/1454115.1454122,
author = {Coons, Katherine E. and Robatmili, Behnam and Taylor, Matthew E. and Maher, Bertrand A. and Burger, Doug and McKinley, Kathryn S.},
title = {Feature Selection and Policy Optimization for Distributed Instruction Placement Using Reinforcement Learning},
year = {2008},
isbn = {9781605582825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1454115.1454122},
doi = {10.1145/1454115.1454122},
abstract = {Communication overheads are one of the fundamental challenges in a multiprocessor system. As the number of processors on a chip increases, communication overheads and the distribution of computation and data become increasingly important performance factors. Explicit Dataflow Graph Execution (EDGE) processors, in which instructions communicate with one another directly on a distributed substrate, give the compiler control over communication overheads at a fine granularity. Prior work shows that compilers can effectively reduce fine-grained communication overheads in EDGE architectures using a spatial instruction placement algorithm with a heuristic-based cost function. While this algorithm is effective, the cost function must be painstakingly tuned. Heuristics tuned to perform well across a variety of applications leave users with little ability to tune performance-critical applications, yet we find that the best placement heuristics vary significantly with the application.First, we suggest a systematic feature selection method that reduces the feature set size based on the extent to which features affect performance. To automatically discover placement heuristics, we then use these features as input to a reinforcement learning technique, called Neuro-Evolution of Augmenting Topologies (NEAT), that uses a genetic algorithm to evolve neural networks. We show that NEAT outperforms simulated annealing, the most commonly used optimization technique for instruction placement. We use NEAT to learn general heuristics that are as effective as hand-tuned heuristics, but we find that improving over highly hand-tuned general heuristics is difficult. We then suggest a hierarchical approach to machine learning that classifies segments of code with similar characteristics and learns heuristics for these classes. This approach performs closer to the specialized heuristics. Together, these results suggest that learning compiler heuristics may benefit from both improved feature selection and classification.},
booktitle = {Proceedings of the 17th International Conference on Parallel Architectures and Compilation Techniques},
pages = {32–42},
numpages = {11},
keywords = {neural networks, machine learning, instruction scheduling, genetic algorithms, compiler heuristics},
location = {Toronto, Ontario, Canada},
series = {PACT '08}
}

@inproceedings{10.3115/1220835.1220870,
author = {Tetreault, Joel R. and Litman, Diane J.},
title = {Comparing the Utility of State Features in Spoken Dialogue Using Reinforcement Learning},
year = {2006},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1220835.1220870},
doi = {10.3115/1220835.1220870},
abstract = {Recent work in designing spoken dialogue systems has focused on using Reinforcement Learning to automatically learn the best action for a system to take at any point in the dialogue to maximize dialogue success. While policy development is very important, choosing the best features to model the user state is equally important since it impacts the actions a system should make. In this paper, we compare the relative utility of adding three features to a model of user state in the domain of a spoken dialogue tutoring system. In addition, we also look at the effects of these features on what type of a question a tutoring system should ask at any state and compare it with our previous work on using feedback as the system action.},
booktitle = {Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics},
pages = {272–279},
numpages = {8},
location = {New York, New York},
series = {HLT-NAACL '06}
}

@inproceedings{10.1145/3343031.3350601,
author = {Sassatelli, Lucile and Winckler, Marco and Fisichella, Thomas and Aparicio, Ramon},
title = {User-Adaptive Editing for 360 Degree Video Streaming with Deep Reinforcement Learning},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350601},
doi = {10.1145/3343031.3350601},
abstract = {The development through streaming of 360degree videos is persistently hindered by how much bandwidth they require. Adapting spatially the quality of the sphere to the user's Field of View (FoV) lowers the data rate but requires to keep the playback buffer small, to predict the user's motion or to make replacements to keep the buffered qualities up to date with the moving FoV, all three being uncertain and risky. We have previously shown that opportunistically regaining control on the FoV with active attention-driving techniques makes for additional levers to ease streaming and improve Quality of Experience (QoE). Deep neural networks have been recently shown to achieve best performance for video streaming adaptation and head motion prediction. This demo presents a step ahead in the important investigation of deep neural network approaches to obtain user-adaptive and network-adaptive 360 degree video streaming systems. In this demo, we show how snap-changes, an attention-driving technique, can be automatically modulated by the user's motion to improve the streaming QoE. The control of snap-changes is made with a deep neural network trained on head motion traces with the Deep Reinforcement Learning strategy A3C.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {2208–2210},
numpages = {3},
keywords = {user attention, deep reinforcement learning, recurrent neural networks, film editing, motion prediction, 360 degree video streaming},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/3551349.3560429,
author = {Su, Jianzhong and Dai, Hong-Ning and Zhao, Lingjun and Zheng, Zibin and Luo, Xiapu},
title = {Effectively Generating Vulnerable Transaction Sequences in Smart Contracts with Reinforcement Learning-Guided Fuzzing},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560429},
doi = {10.1145/3551349.3560429},
abstract = {As computer programs run on top of blockchain, smart contracts have proliferated a myriad of decentralized applications while bringing security vulnerabilities, which may cause huge financial losses. Thus, it is crucial and urgent to detect the vulnerabilities of smart contracts. However, existing fuzzers for smart contracts are still inefficient to detect sophisticated vulnerabilities that require specific vulnerable transaction sequences to trigger. To address this challenge, we propose a novel vulnerability-guided fuzzer based on reinforcement learning, namely RLF, for generating vulnerable transaction sequences to detect such sophisticated vulnerabilities in smart contracts. In particular, we firstly model the process of fuzzing smart contracts as a Markov decision process to construct our reinforcement learning framework. We then creatively design an appropriate reward with consideration of both vulnerability and code coverage so that it can effectively guide our fuzzer to generate specific transaction sequences to reveal vulnerabilities, especially for the vulnerabilities related to multiple functions. We conduct extensive experiments to evaluate RLF’s performance. The experimental results demonstrate that our RLF outperforms state-of-the-art vulnerability-detection tools (e.g., detecting 8\%-69\% more vulnerabilities within 30 minutes).},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {36},
numpages = {12},
keywords = {Smart contract, Fuzzing, Reinforcement learning},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3534678.3542672,
author = {He, Weijie and Chen, Ting},
title = {Scalable Online Disease Diagnosis via Multi-Model-Fused Actor-Critic Reinforcement Learning},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3542672},
doi = {10.1145/3534678.3542672},
abstract = {For those seeking healthcare advice online, AI based dialogue agents capable of interacting with patients to perform automatic disease diagnosis are a viable option. This application necessitates efficient inquiry of relevant disease symptoms in order to make accurate diagnosis recommendations. This can be formulated as a problem of sequential feature (symptom) selection and classification for which reinforcement learning (RL) approaches have been proposed as a natural solution. They perform well when the feature space is small, that is, the number of symptoms and diagnosable disease categories is limited, but they frequently fail in assignments with a large number of features. To address this challenge, we propose a Multi-Model-Fused Actor-Critic (MMF-AC) RL framework that consists of a generative actor network and a diagnostic critic network. The actor incorporates a Variational AutoEncoder (VAE) to model the uncertainty induced by partial observations of features, thereby facilitating in making appropriate inquiries. In the critic network, a supervised diagnosis model for disease predictions is involved to precisely estimate the state-value function. Furthermore, inspired by the medical concept of differential diagnosis, we combine the generative and diagnosis models to create a novel reward shaping mechanism to address the sparse reward problem in large search spaces. We conduct extensive experiments on both synthetic and real-world datasets for empirical evaluations. The results demonstrate that our approach outperforms state-of-the-art methods in terms of diagnostic accuracy and interaction efficiency while also being more effectively scalable to large search spaces. Besides, our method is adaptable to both categorical and continuous features, making it ideal for online applications.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4695–4703},
numpages = {9},
keywords = {online disease diagnosis, self-diagnosis, reinforcement learning},
location = {Washington DC, USA},
series = {KDD '22}
}

