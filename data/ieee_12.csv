"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Reinforcement Learning for Altitude Hold and Path Planning in a Quadcopter","P. B. Karthik; K. Kumar; V. Fernandes; K. Arya","Dept. of Electronics and Communication, PES University, Bengaluru, India; Dept. of Electronics and Communication, KIET Group of Institutions, Ghaziabad, India; eYantra, Indian Institute of Technology, Powai, Mumbai, India; eYantra, Indian Institute of Technology, Powai, Mumbai, India","2020 6th International Conference on Control, Automation and Robotics (ICCAR)","4 Jun 2020","2020","","","463","467","The control and stability of drones is a challenging problem. There is need for a more dynamic and robust control that the drone can use to adjust itself to an unknown environment directly. This paper presents a framework for using reinforcement learning to control altitude of a drone. We use PID to stabilize x and y axis of the drone. The drone is trained using Q-learning of Reinforcement Learning in a simulated environment. The trained model is then tested in the real world. Furthermore, a comparative analysis of reinforcement learning and PID algorithm is presented. Finally, an application of way-point navigation from one given point to other in an environment filled with obstacles at different points formulated as a 3-dimensional grid is presented using Q-learning of Reinforcement Learning.","2251-2446","978-1-7281-6139-6","10.1109/ICCAR49639.2020.9108104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108104","reinforcement learning;Q-learning;ROS;PID;real world simulation;V-REP;path planning;navigation;localization;whycon;nano drone","","helicopters;learning (artificial intelligence);mobile robots;navigation;path planning;remotely operated vehicles;robust control;three-term control","reinforcement learning;dynamic control;robust control;drone;Q-learning;PID algorithm;altitude control;stability","","1","","12","IEEE","4 Jun 2020","","","IEEE","IEEE Conferences"
"A deep reinforcement learning algorithm based on modified Twin delay DDPG method for robotic applications","C. Vasquez-Jalpa; M. Nakano-Miyatake; E. Escamilla-Hernandez","Graduate Section, Mechanical and Engineering School, Instituto Politecnico Nacional, Mexico City, Mexico; Graduate Section, Mechanical and Engineering School, Instituto Politecnico Nacional, Mexico City, Mexico; Graduate Section, Mechanical and Engineering School, Instituto Politecnico Nacional, Mexico City, Mexico","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","743","748","This paper proposes a deep reinforcement learning algorithm for autonomous robotics, in which we modify twin delay deep deterministic policy gradient (TD3) to adapt for autonomous robots with higher degree freedom in movement. To provide a robot with free movement in the 2D space without collisions against some obstacles, such as wall, a robot is equipped with three cameras. The images captured by camera are used to train Convolutional Neural Networks (CNN) to understand environment with collisions or not-collisions. We added two additional parameters, observation’ O’, which are images obtained from cameras, and degrees of turns' deg’ into the original TD3’ s parameters composed of four values: [state's', reward ‘r’, action ‘a’ and next-state's' ‘]. To determine a next action with higher reward from the observation, two additional Neural Networks are constructed, being the first one determines an action from observation and the second one determines degree of turn from the observation and the action. The simulation results under three environments constructed by CoppeliaSim show a good performance of the proposed algorithm, reaching the target with higher rewards, even though the environments are unknown by robots.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649882","Deep Reinforcement Learning;Policy Gradient;Actor-Critic;Deep Q-Learning;Robot Vision","Simulation;Robot vision systems;Neural networks;Reinforcement learning;Cameras;Control systems;Delays","collision avoidance;convolutional neural nets;deep learning (artificial intelligence);gradient methods;mobile robots;reinforcement learning;robot vision","deep reinforcement learning algorithm;robotic applications;autonomous robotics;deep deterministic policy gradient;autonomous robots;higher degree freedom;free movement;cameras;convolutional neural networks;not-collisions;modified twin delay DDPG method;TD3;CoppeliaSim;CNN","","1","","10","","28 Dec 2021","","","IEEE","IEEE Conferences"
"A Compute-intensive Service Migration Strategy Based on Deep Reinforcement Learning Algorithm","Y. Cheng; X. Li","School of Electronic and Information Engineering of Beijing, Jiao Tong University, Beijing, China; School of Electronic and Information Engineering of Beijing, Jiao Tong University, Beijing, China","2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)","4 May 2020","2020","1","","1385","1388","With the rapid improvement of software service in recent years, compute-intensive services such as VR, AR, and face recognition have been developed drastically. It's hard for mobile devices with limited capacity computing and energy to meet the delay requirements of these services. With the cloud computing technology, the high computing demands for these compute-intensive services can be satisfied by migrating these compute-intensive services to powerful computation servers in the cloud. However, it will bring extra latency by long-distance communication between servers and mobile devices. Edge computing is emerging to reduces the extra latency coursed by sinking services to the edge. In this paper, we proposed an intelligent service migration algorithm model based on machine learning algorithms. Considering the dynamic of the bandwidth of network and the battery level of mobile devices, the proposed algorithm is aimed at optimizing the delay and energy consumption. The simulation shows that the performance the proposed service migration algorithm is better than the comparative algorithms.","","978-1-7281-4390-3","10.1109/ITNEC48623.2020.9085128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9085128","mobile edge computing;compute-intensive services;service migration;reinforcement learning","Performance evaluation;Energy consumption;Cloud computing;Machine learning algorithms;Heuristic algorithms;Computational modeling;Mobile handsets","cloud computing;file servers;learning (artificial intelligence);neural nets;power aware computing","energy consumption optimization;delay optimization;machine learning algorithms;software service;deep reinforcement learning algorithm;compute-intensive service migration strategy;intelligent service migration algorithm model;edge computing;computation servers;cloud computing technology","","1","","6","IEEE","4 May 2020","","","IEEE","IEEE Conferences"
"Locomotion Control Method for Humanoid Robot Based on United Hierarchical Reinforcement Learning","B. Liu; L. Ma; C. Liu; B. Xu","Shanghai Cultural Foundation; Department of Electronics and Information Engineering, Tongji University, Shanghai, China; Department of Electronics and Information Engineering, Tongji University, Shanghai, China; Department of Electronics and Information Engineering, Tongji University, Shanghai, China","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","1161","1166","This paper proposes a hierarchical reinforcement learning control method and applies it to the walking control of a humanoid robot. Firstly, a reinforcement learning algorithm called proximal policy optimization(PPO) is combined with central pattern generator(CPG). Thus an united hierarchical reinforcement learning (UHRL) is built which could cooperates with high and low levels control tasks. Secondly, the particle swarm optimization algorithm is used to obtain the initial parameter configuration of CPG. So that the robot can generate basic walking gait at the beginning of the experiment. The particle swarm variance fitness is used as the variation constraint to prevent the optimization process from falling into precocious convergence. Thirdly, the reward function of the high-level controller is designed to help the humanoid robot avoid deviation from the original path.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264548","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264548","","Robots;Reinforcement learning;Legged locomotion;Humanoid robots;Neurons;Hip;Optimization","gait analysis;humanoid robots;learning (artificial intelligence);legged locomotion;motion control;particle swarm optimisation;path planning;robot dynamics","locomotion control method;humanoid robot;united hierarchical reinforcement learning;hierarchical reinforcement learning control method;walking control;reinforcement learning algorithm;particle swarm optimization algorithm;particle swarm variance fitness;high-level controller;low level control tasks;high level control tasks;proximal policy optimization;central pattern generator;CPG;PPO;UHRL;basic walking gait generation","","1","","27","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Full-Actuation Rolling Locomotion with Tensegrity Robot via Deep Reinforcement Learning","Y. Guo; H. Peng","Department of Engineering Mechanics, State Key Laboratory of Structural Analysis for Industrial Equipment, Dalian University of Technology, Dalian, China; Department of Engineering Mechanics, State Key Laboratory of Structural Analysis for Industrial Equipment, Dalian University of Technology, Dalian, China","2021 5th International Conference on Robotics and Automation Sciences (ICRAS)","13 Jul 2021","2021","","","51","55","Tensegrity robots, entirely composed of elastic cables and rigid rods, have many excellent properties which have a wide application from complex co-robotics to planetary. Nevertheless, it is still difficult to control tensegrity robots because of their unconventional designs and highly coupled dynamics. Deep reinforcement learning algorithms have been used in a lot of robotic tasks due to their strong perception and decision-making capabilities. However, it often needs to collect a lot of samples, which limits its application. Model-based algorithms can learn with fewer samples but have a sub-optimal result because of the model error. In the paper, we proposed a hybrid method to achieve effective control of tensegrity robots. Firstly, we established the simulation platform via the framework of the positional formulation finite element method. And thens, we use a medium-sized neural network to fit the dynamic model and control the tensegrity robot via model prediction control (MPC). The controlled trajectories are used to initialize the parameters and memory of deep deterministic policy gradient (DDPG). We demonstrated that the hybrid algorithm can achieve efficient control of the tensegrity robot. In this work, we have realized full-actuation rolling with a tensegrity robot on a plane and 5° slope surface.","","978-1-6654-1226-1","10.1109/ICRAS52289.2021.9476651","National Key Research and Development Plan(grant numbers:2019YFB1706502); National Natural Science Foundation of China(grant numbers:11922203,11772074); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9476651","tensegrity;deep reinforcement learning;positional FEM;model prediction control","Heuristic algorithms;Neural networks;Decision making;Reinforcement learning;Predictive models;Prediction algorithms;Trajectory","cables (mechanical);control engineering computing;deep learning (artificial intelligence);finite element analysis;neurocontrollers;position control;predictive control;robot dynamics;rods (structures)","tensegrity robot;deep reinforcement learning algorithms;full-actuation rolling locomotion;elastic cables;rigid rods;decision-making;finite element method;model prediction control;medium-sized neural network;deep deterministic policy gradient","","1","","11","IEEE","13 Jul 2021","","","IEEE","IEEE Conferences"
"Ship Trajectory Tracking Using Improved Simulated Annealing and Reinforcement Learning","Y. Yu","School of Management and Engineering, Nanjing University, Nanjing, China","2018 IEEE International Conference on Information and Automation (ICIA)","26 Aug 2019","2018","","","1384","1389","It plays a vital role in the efficiency, safety and maneuverability of navigation to control ship's trajectory precisely. Based on reinforcement learning algorithm, compensation is made on the output of traditional PID controller. In order to save the time of adjusting the PID parameters, the paper introduces an improved simulated annealing algorithm. The designed controller can overcome the external interference, such as wind, wave and flow. Simulation results under the MOOS which was written by Paul Newman to support operations with autonomous marine vehicles in the MIT Ocean Engineering and the MIT Sea Grant programs, show that the ship course can be properly controlled even if changeable wave and wind exist. In addition, the algorithm is applicable for several unmanned systems.","","978-1-5386-8069-8","10.1109/ICInfA.2018.8812464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812464","Simulated Annealing;Reinforcement Learning;Trajectory Control;Neural Network","Marine vehicles;Reinforcement learning;Prediction algorithms;Simulated annealing;Trajectory;Biological neural networks;Annealing","autonomous underwater vehicles;control system synthesis;learning (artificial intelligence);marine navigation;mobile robots;ships;simulated annealing;three-term control;trajectory control","ship trajectory tracking;reinforcement learning algorithm;PID parameters;improved simulated annealing algorithm;external interference;autonomous marine vehicles;MIT Ocean Engineering;MIT Sea Grant programs;ship course;MOOS","","1","","16","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"Control design of two-level quantum systems with reinforcement learning","H. Yu; X. Xu; H. Ma; Z. Zhu; C. Chen","Department of Control and Systems Engineering, School of Management and Engineering Nanjing University, Nanjing, China; Department of Control and Systems Engineering, School of Management and Engineering Nanjing University, Nanjing, China; Department of Control and Systems Engineering, School of Management and Engineering Nanjing University, Nanjing, China; Department of Control and Systems Engineering, School of Management and Engineering Nanjing University, Nanjing, China; Department of Control and Systems Engineering, School of Management and Engineering Nanjing University, Nanjing, China","2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC)","9 Jul 2018","2018","","","922","927","In recent years, some experimental studies and simulations show that reinforcement learning (RL) is an effective learning control approach for solving certain quantum control problems. In this paper, Q-learning with different exploration strategies (e.g., ε-greedy and Softmax), probabilistic Q-learning (PQL) and quantum reinforcement learning (QRL) are applied to solve the state transition problem of two-level quantum systems (e.g., spin-1/2 systems), respectively. These reinforcement learning algorithms are introduced and analyzed regarding the learning control problem of the spin-1/2 system. According to the constraints of the control fields, two typical kinds of controllers, i.e., three-switch controller and Bang-Bang controller, are designed using reinforcement learning. The learning performance of the above RL algorithms for both of the three-switch control and Bang-Bang control of two-level quantum systems are demonstrated and analyzed.","","978-1-5386-7255-6","10.1109/YAC.2018.8406503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8406503","Quantum control;reinforcement learning;two-level quantum systems","Learning (artificial intelligence);Task analysis;Control design;Probabilistic logic;Bang-bang control;Probability distribution","bang-bang control;discrete systems;learning systems;quantum theory","state transition problem;two-level quantum systems;reinforcement learning algorithms;learning control problem;spin-1/2 system;control fields;Bang-Bang controller;learning performance;three-switch control;Bang-Bang control;control design;experimental studies;simulations;effective learning control approach;quantum control problems;quantum reinforcement learning","","1","","23","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"Meta Reinforcement Learning Based Underwater Manipulator Control","J. Moon; S. -H. Bae; M. Cashmore","Department of Electronics Engineering, Chosun University, Gwangju, Korea; REDONE TECHNOLOGIES CO., LTD, Jangseong, Korea; Computer and Information Sciences, The University of Strathclyde, Glasgow, UK","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","1473","1476","Robots have garnered significant attention owing to their advantages in terms of replacing human labor under hazardous environments. In particular, because underwater construction robots can perform various tasks that are highly dangerous under deep sea environments, the development of manipulator control technology for these underwater robots is crucial. In this study, we therefore introduce an underwater manipulator control method based on meta reinforcement learning. Specifically, we construct a real-world underwater robot manipulator environment using ROS Gazebo and conduct simulations for the testing and verification of the proposed method.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9650009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650009","Underwater robot;robotic manipulation;manipulator control;model based reinforcement learning;meta reinforcement learning","Learning systems;Autonomous underwater vehicles;Uncertainty;Computational modeling;Reinforcement learning;Predictive models;Manipulators","control engineering computing;manipulators;mobile robots;reinforcement learning;robot vision;underwater vehicles","meta reinforcement learning;real-world underwater robot manipulator environment;human labor;hazardous environments;underwater construction robots;deep sea environments;ROS Gazebo","","1","","5","","28 Dec 2021","","","IEEE","IEEE Conferences"
"UAV Reconnaissance Task Allocation with Reinforcement Learning and Genetic Algorithm","S. Gao; L. Zuo; S. X. Bao","National Lab of Radar Signal Processing, Xidian University, Xian, China; National Lab of Radar Signal Processing, Xidian University, Xian, China; National Lab of Radar Signal Processing, Xidian University, Xian, China","2022 International Conference on Automation, Robotics and Computer Engineering (ICARCE)","22 Feb 2023","2022","","","1","3","Unmanned air vehicle (UAV) reconnaissance task allocation is important in a total military combat system. The typical Genetic Algorithm (GA) is a common effective means to deal with the UAV task allocation problem. But when face with a large number of targets, the initial population has a huge influence on the performance of GA algorithms, which leads to instability on the solution accuracy. To overcome this limitation of heuristics algorithms, we propose a new algorithm combing reinforcement learning (RL) and the GA algorithms, named GA-RL. The RL is used to fast provide an initial population for GA, and then the GA algorithms further optimize this initial population to get the solution. Finally, the numerical simulation tests show that this algorithm can hugely improve the solving accuracy, especially in large tasks allocation problems.","","978-1-6654-7548-8","10.1109/ICARCE55724.2022.10046603","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046603","UAV;task allocation;reinforcement learning;genetic algorithm","Heuristic algorithms;Sociology;Reinforcement learning;Reconnaissance;Resource management;Task analysis;Statistics","autonomous aerial vehicles;genetic algorithms;reinforcement learning;remotely operated vehicles","algorithm combing reinforcement learning;common effective means;GA algorithms;heuristics algorithms;named GA-RL;tasks allocation problems;total military combat system;typical Genetic Algorithm;UAV reconnaissance task allocation;UAV task allocation problem;unmanned air vehicle reconnaissance task allocation","","1","","10","IEEE","22 Feb 2023","","","IEEE","IEEE Conferences"
"Option Discovery in Reinforcement Learning using Frequent Common Subsequences of Actions","S. Girgin; F. Polat","Department of Computer Engineering, Middle East Technical University, Ankara, Turkey; Department of Computer Engineering, Middle East Technical University, Ankara, Turkey","International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)","22 May 2006","2005","1","","371","376","Temporally abstract actions, or options, facilitate learning in large and complex domains by exploiting sub-tasks and hierarchical structure of the problem formed by these sub-tasks. In this paper, we study automatic generation of options using common sub-sequences derived from the state transition histories collected as learning progresses. The standard Q-learning algorithm is extended to use generated options transparently, and effectiveness of the method is demonstrated in Dietterich's Taxi domain","","0-7695-2504-0","10.1109/CIMCA.2005.1631294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631294","","Learning;History;State-space methods;Acceleration;Joining processes;Clustering algorithms;Partitioning algorithms;Topology;Computational intelligence;Computational modeling","learning (artificial intelligence);Markov processes;set theory","option discovery;reinforcement learning;frequent common subsequence;temporally abstract action;standard Q-learning algorithm;Dietterich Taxi domain","","1","","21","IEEE","22 May 2006","","","IEEE","IEEE Conferences"
"Optimal control design based on reinforcement learning for a class of nonlinear distributed systems","Z. He; Y. Liu","Nanjing University of Aeronautics and Astronautics, Nanjing, China; Nanjing University of Aeronautics and Astronautics, Nanjing, China","2013 10th IEEE International Conference on Control and Automation (ICCA)","22 Jul 2013","2013","","","384","389","This paper proposes an optimal control scheme for a class of non-affine nonlinear distributed systems. The research is conducted for a tethered parafoil system. The reference inputs are optimized by reinforcement learning method for two optimization goals respectively. A dynamic model approximation method is introduced to approximate the non-affine nonlinear terms. The tracking controller is designed and the stability analysis is given. The methodology is demonstrated by simulations.","1948-3457","978-1-4673-4708-2","10.1109/ICCA.2013.6565092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6565092","","Aerodynamics;Learning (artificial intelligence);Mathematical model;Approximation methods;Equations;Optimization;Nonlinear systems","control system synthesis;learning (artificial intelligence);nonlinear systems;optimal control;stability","optimal control design;reinforcement learning;nonaffine nonlinear distributed system;tethered parafoil system;optimization goal;dynamic model approximation;nonaffine nonlinear term;tracking controller;stability analysis","","1","","13","IEEE","22 Jul 2013","","","IEEE","IEEE Conferences"
"Scanning control of atomic force microscope based on deep reinforcement learning","H. Lv; H. Xu; L. Wang; H. Li","Changchun University of Science and Technology Electronic Information Engineering, Changchun, China; Changchun University of Science and Technology Electronic Information Engineering, Changchun, China; Changchun University of Science and Technology Electronic Information Engineering, Changchun, China; Changchun University of Science and Technology Electronic Information Engineering, Changchun, China","2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA)","2 Sep 2021","2021","","","19","22","Atomic force microscope can use the force between atoms to scan the morphology of samples at the micro-nano scale. However, the control problem of atomic force microscope faces the problems of complex control environment and high precision requirements. Therefore, an adaptive PID controller that introduces deep reinforcement learning technology is proposed. The control problem of the atomic force microscope is described as a Markov decision process, with the DDPG algorithm framework as the main body, and the reward function is designed according to the actual control requirements. DDPG algorithm takes error as observation input, PID parameter as action output, realizes adaptive controller design, and obtains the control parameters that meet the requirements after the training is completed the experimental results show that the controller can meet the control requirements during the learning process, improve the control accuracy of the atomic force microscope system, and improve the imaging quality. The research results can provide references for researchers in the same field.","","978-1-6654-3265-8","10.1109/AIEA53260.2021.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9525606","Nanometer measurement;atomic force microscope;deep reinforcement learning;parameter tuning","Training;Atomic force microscopy;Adaptation models;Adaptive systems;Shape;Force;Process control","adaptive control;control system synthesis;learning (artificial intelligence);Markov processes;three-term control","scanning control;control problem;complex control environment;adaptive PID controller;deep reinforcement learning technology;actual control requirements;adaptive controller design;control parameters;control accuracy;atomic force microscope system","","1","","6","IEEE","2 Sep 2021","","","IEEE","IEEE Conferences"
"Deep reinforcement learning-based robust missile guidance","J. Ahn; J. Shin; H. -G. Kim","Department of Mechanical Engineering, Chungbuk University, Cheongju, Korea; Department of Mechanical Engineering, Chungbuk University, Cheongju, Korea; Department of Mechanical Engineering, Incheon University, Incheon, Korea","2022 22nd International Conference on Control, Automation and Systems (ICCAS)","9 Jan 2023","2022","","","927","930","In missile guidance systems, uncertainties such as sensor noise degrade guidance performance. In order to handle the problem, an End-to-End missile guidance method based on deep reinforcement learning (DRL) is proposed. This study develops guidance commands for target intercept in a two-dimensional environment. In the environment, it is assumed that the interceptor measures a normalized, noisy relative observation value for the target. Finally, numerical simulations composed of several training and validation are performed to validate the performance of the proposed method.","2642-3901","978-89-93215-24-3","10.23919/ICCAS55662.2022.10003963","Defense Acquisition Program Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003963","missile guidance law;reinforcement learning;neural networks","Deep learning;Training;Solid modeling;Uncertainty;Neural networks;Reinforcement learning;Numerical simulation","control engineering computing;deep learning (artificial intelligence);missile guidance;numerical analysis;observers;reinforcement learning;robust control;target tracking","deep reinforcement learning-based robust missile guidance;DRL;end-to-end missile guidance method;guidance commands;noisy relative observation value;normalized observation value;sensor noise degrade guidance performance;two-dimensional environment","","1","","6","","9 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Sequential Controller for Mobile Robots with Obstacle Avoidance","S. Mashhouri; M. Rahmati; Y. Borhani; E. Najafi","Faculty of Mechanical Engineering, K.N. Toosi University of Technology, Tehran, Iran; Faculty of Mechanical Engineering, K.N. Toosi University of Technology, Tehran, Iran; Faculty of Mechanical Engineering, K.N. Toosi University of Technology, Tehran, Iran; Faculty of Mechanical Engineering, K.N. Toosi University of Technology, Tehran, Iran","2022 8th International Conference on Control, Instrumentation and Automation (ICCIA)","23 Mar 2022","2022","","","1","5","Obstacle avoidance and path planning play substantial roles in mobile robot applications. This paper has two main parts: first, an object detection algorithm based on YOLO-v4 with custom classes and depth camera in real-time has been implemented in Robot Operating System (ROS) to resolve robot obstacle avoidance issues. Then, controlling of the robot was discussed by dividing the path into three parts with their specific PID or PD controllers. The optimum parameters of each controller are then calculated by Reinforcement Learning (RL). For ensuring the precision of the obstacle avoidance method, the accuracy of distance is evaluated by the significant result of real thorough objects. Then for evaluating the controllers, the desired-and the traveled path were compared and the positional error was calculated.","","978-1-6654-9569-1","10.1109/ICCIA54998.2022.9737166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9737166","Mobile robot;PID;PD;Reinforcement Learning;Object Detection;Obstacle Avoidance;ROS;YOLO","Robot vision systems;Reinforcement learning;Object detection;Cameras;Turning;Trajectory;Mobile robots","collision avoidance;computer vision;mobile robots;object detection;PD control;reinforcement learning;robot programming;three-term control","depth camera;robot obstacle avoidance;reinforcement learning;path planning;sequential controller;mobile robots;object detection algorithm;robot operating system;YOLO-v4;robot control;PID controller;PD controller","","1","","14","IEEE","23 Mar 2022","","","IEEE","IEEE Conferences"
"Pursuit and evasion game between UVAs based on multi-agent reinforcement learning","G. Xu; Y. Zhao; H. Liu","School of Autontation, Shenyang Aerospace University, Shenyang, China; School of Autontation, Shenyang Aerospace University, Shenyang, China; School of Autontation, Shenyang Aerospace University, Shenyang, China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","1261","1266","Pursuit and evasion game between UVAs is a typical differential game. Differential games are usually difficult to obtain the optimal solutions because of the complex bilateral extremum problems. Reinforcement learning has superiorities in solving differential games with the advantages such as it does not need accurate controlled models and a lot of training data. In this paper, a multi-agent reinforcement learning model is established for UAV pursuit and evasion game. The relative motion state equation is used to describe the state to simplify the state set, and the pursuit and evasion game is transformed into a zero-sum game which is solved by Minimax-Q learning. The reinforcement learning model established in this paper reduces the complexity of solving problem and guarantees the convergence speed. Finally, the simulation results verify the rationality of the obtained control policy which makes both the pursuer and the evader tend to be advantageous to their own direction in the course of the countermeasures.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8997447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8997447","pursuit and evasion game;differential game;multi-agent reinforcement learning;Minimax-Q learning","Games;Learning (artificial intelligence);Mathematical model;Game theory;Convergence;Optimal control;Atmospheric modeling","autonomous aerial vehicles;differential games;learning (artificial intelligence);minimax techniques;multi-agent systems;set theory","differential games;complex bilateral extremum problems;multiagent reinforcement learning model;zero-sum game;training data;state set;minimax-Q learning;control policy;UVA pursuit and evasion game","","1","","17","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Goal-Oriented Navigation with Avoiding Obstacle based on Deep Reinforcement Learning in Continuous Action Space","P. X. Hien; G. -W. Kim","Department of Control and Robot Engineering, Chungbuk National University, South Korea; Department of Intelligent System and Robotics, Chungbuk National University, South Korea","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","8","11","Obstacle avoidance problems using Deep Reinforcement Learning (DRL) are becoming possible solutions for autonomous mobile robots. In real-world situations with stationary and moving obstacles, mobile robots must be able to navigate to a goal and safely avoid collisions. This work is an extension of ongoing research on the navigation approach for a mobile robot. We show that through the proposed DRL, a goal-oriented collision avoidance model can be trained end-to-end without manual turning or supervision by a human operator. We suggest performing the obstacle avoidance algorithm of the mobile robot in both simulated environments and continuous action space of the real world. Finally, we measure and evaluate the obstacle avoidance capability through data collection of hit ratio metrics during robot execution.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649898","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649898","deep reinforcement learning;Q-learning;obstacle avoidance;robot sensing systems;path planning","Measurement;Navigation;Reinforcement learning;Aerospace electronics;Robot sensing systems;Turning;Sensors","collision avoidance;deep learning (artificial intelligence);mobile robots;navigation;reinforcement learning","goal-oriented navigation;continuous action space;DRL;autonomous mobile robots;navigation approach;goal-oriented collision avoidance model;obstacle avoidance algorithm;obstacle avoidance capability;robot execution;deep reinforcement learning;hit ratio metrics","","1","","15","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Large-scale Autonomous Navigation and Path Planning of Lunar Rover via Deep Reinforcement Learning","T. Hu; T. Cao; B. Zheng; H. Zhang; M. Ni","Shanghai Aerospace Control Technology Institute, Shanghai Key Laboratory of Aerospace Intelligent Control Technology, Shanghai, China; Shanghai Aerospace Control Technology Institute, Shanghai Key Laboratory of Aerospace Intelligent Control Technology, Shanghai, China; Shanghai Aerospace Control Technology Institute, Shanghai Key Laboratory of Aerospace Intelligent Control Technology, Shanghai, China; Shanghai Aerospace Control Technology Institute, Shanghai Key Laboratory of Aerospace Intelligent Control Technology, Shanghai, China; School of Physics and Electronic Science, East China Normal University, Shanghai, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","2050","2055","Future lunar surface exploration needs to be carried out in areas with more complex terrain and richer scientific research value without prior information. Therefore, the patrol device needs to be able to autonomously detect terrain and obstacle avoidance when planning exploration tasks. However, Deep reinforcement learning has been successfully applied in various game-like environments, and it is a challenging task to apply deep reinforcement learning to the navigation and obstacle avoidance detection of the rover on lunar surface. We propose an autonomous decision-making method for lunar vehicle patrols based on deep reinforcement learning. The algorithm first uses lidar for simultaneous positioning and graphing. For the constructed static three-dimensional point cloud map of the lunar surface, it is described as a two-dimensional grid using octree-map projection The grid map is processed by the long-short time memory (LSTM) to process the two-dimensional grid obstacle information. Using dueling deep Q network (dueling DQN) with multi-step learning to train the planetary vehicle automatic path planning, and finally in the ROS static obstacle environment simulation verification experiment, the simulation results verify that the proposed method is effective and adaptable for the lunar rover in different terrain environments.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728075","lunar rover;deep reinforcement learning;path planning;lidar SLAM;navigation","Space vehicles;Point cloud compression;Laser radar;Moon;Decision making;Reinforcement learning;Path planning","collision avoidance;decision making;learning (artificial intelligence);lunar surface;mobile robots;optical radar;path planning;planetary rovers","lunar rover;scale autonomous navigation;deep reinforcement learning;future lunar surface exploration;terrain;obstacle avoidance detection;autonomous decision-making method;lunar vehicle patrols;two-dimensional grid obstacle information;dueling deep Q network;planetary vehicle automatic path planning","","1","","6","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"PSS Control of Multi Machine Power System Using Reinforcement Learning","X. Xie","Artificial Intelligence School, Wuchang University of Technology, Wuhan, China","2021 5th International Conference on Robotics and Automation Sciences (ICRAS)","13 Jul 2021","2021","","","132","135","Conventional power system stabilizer (CPSS) is used to damp low frequency oscillations (LFO) of interarea power system. But CPSS can't damp effectively LFO when there are big disturbances, such as three phase short circuit fault and changes between system structure & parameters. This paper proposes a novel adaptive PSS using reinforcement learning (RL) which has strong robustness to external perturbation. The Actor-Critic (AC) reinforcement learning is an important RL method. The Actor network can output a control signal and the Critic network is used to evaluate the control performance of Actor network. RBF neural network has better convergence than BP network. So the Actor network and Critic network are implemented by RBF network. The gradient descent method is used to tune RBF network weights online, so the presented RL-PSS has the adaptability to damp LFO invariable operation conditions. Simulation experiments have verified that the RL-PSS has better effectiveness and robustness than the CPSS.","","978-1-6654-1226-1","10.1109/ICRAS52289.2021.9476268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9476268","low frequency oscillation;power system stabilizer;reinforcement learning;RBF neural network","Systems operation;Simulation;Perturbation methods;Power system dynamics;Reinforcement learning;Radial basis function networks;Power system stability","backpropagation;control system synthesis;damping;gradient methods;learning (artificial intelligence);neurocontrollers;oscillations;power system control;power system stability;radial basis function networks;robust control","PSS control;multimachine power system;CPSS;low frequency oscillations;interarea power system;phase short circuit fault;system parameters;adaptive PSS;actor-critic reinforcement learning;RL method;actor network;control signal;critic network;control performance;RBF neural network;RBF network weights;LFO invariable operation conditions;RL-PSS;system structure","","1","","11","IEEE","13 Jul 2021","","","IEEE","IEEE Conferences"
"Optimizing the Area Coverage of Networked UAVs using Multi-Agent Reinforcement Learning","T. A. Tamba","Department of Electrical Engineering, Parahyangan Catholic University, Bandung, Indonesia","2021 International Conference on Instrumentation, Control, and Automation (ICA)","1 Dec 2021","2021","","","197","201","Wireless sensor networks (WSNs) have been widely used in various area coverage applications which require the monitoring and surveillance of systems with spatiotemporally varying variables or parameters. One important task in the implementation of WSNs for area coverage and monitoring purposes is the determination of the solution for the optimal coverage problem. This paper describes that the formulation of the area coverage problem can be modeled using Markov game modeling formalism whereas the optimal joint state-action policy for each agent which also takes into consideration the group objective can be computed using multi-agent Q-learning iterative processes using multi-agent reinforcement learning framework. Simulation results are presented to illustrate the proposed iterative learning-based area coverage solution approach.","2639-5045","978-1-6654-3295-5","10.1109/ICA52848.2021.9625676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9625676","area coverage problem;networked UAVs;multi-agent reinforcement learning","Wireless sensor networks;Simulation;Computational modeling;Surveillance;Reinforcement learning;Games;Markov processes","game theory;iterative methods;learning (artificial intelligence);learning systems;Markov processes;multi-agent systems;wireless sensor networks","networked UAVs;multiagent reinforcement learning;wireless sensor networks;WSNs;area coverage applications;monitoring purposes;optimal coverage problem;area coverage problem;Markov game modeling formalism;optimal joint state-action policy;multiagent Q-learning;iterative learning-based area coverage solution approach","","1","","19","IEEE","1 Dec 2021","","","IEEE","IEEE Conferences"
"Fuzzy policy gradient reinforcement learning for leader-follower systems","Dongbing Gu; Erfu Yang","Department of Computer Science, University of Essex, Colchester, UK; Department of Computer Science, University of Essex, Colchester, UK","IEEE International Conference Mechatronics and Automation, 2005","8 May 2006","2005","3","","1557","1561 Vol. 3","This paper presents a policy gradient multi-agent reinforcement learning algorithm for leader-follower systems. In this algorithm, cooperative dynamics of the leader-follower control is modelled as an incentive Stackelberg game. A linear incentive mechanism is used to connect the leader and follower policies. Policy gradient reinforcement learning explicitly explores policy parameter space to search the optimal policy. Fuzzy logic controllers are used as the policy. The parameters of fuzzy logic controllers can be improved by this policy gradient algorithm.","2152-744X","0-7803-9044-X","10.1109/ICMA.2005.1626787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1626787","","Fuzzy systems;Learning;Game theory;Fuzzy logic;Function approximation;Convergence;Heuristic algorithms;Stochastic processes;Minimax techniques;Multiagent systems","fuzzy control;learning (artificial intelligence);multi-agent systems;game theory;control engineering computing","leader-follower systems;fuzzy policy gradient reinforcement learning;cooperative dynamics;incentive Stackelberg game;linear incentive mechanism;fuzzy logic controllers","","1","","31","IEEE","8 May 2006","","","IEEE","IEEE Conferences"
"Velocity control in a right-turn across traffic scenario for autonomous vehicles using kernel-based reinforcement learning","Y. Zhang; B. Gao; L. Guo; H. Chen; J. Zhao","State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, PR China; State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, PR China; State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, PR China; Department of Control Science and Engineering, Jilin University, Changchun, PR China; Department of Control Science and Engineering, Jilin University, Changchun, PR China","2017 Chinese Automation Congress (CAC)","1 Jan 2018","2017","","","6211","6216","Recently, advanced control methods like machine leaning are increasingly applied to autonomous vehicle. This paper focuses on velocity control in a right-turn traffic scenario. A Markov Decision Processes(MDPs) is modeled and the actor-critic reinforcement learning architecture is employed. Then the kernel-based least squares policy iteration algorithm(KLSPI) is applied. Simulation results show that the proposed method can perform different policy in different cases, which preliminarily verify the rationality.","","978-1-5386-3524-7","10.1109/CAC.2017.8243896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8243896","autonomous vehicle;reinforcement learning (RL);kernel-based least squares policy iteration (KLSPI)","TV;Systems modeling","angular velocity control;control engineering computing;decision theory;iterative methods;learning (artificial intelligence);least squares approximations;Markov processes;mobile robots;motion control;road traffic control;road vehicles","actor-critic reinforcement learning architecture;velocity control;autonomous vehicle;machine leaning;right-turn traffic scenario;Markov Decision Processes;kernel-based reinforcement learning;kernel-based least squares policy iteration algorithm","","1","","9","IEEE","1 Jan 2018","","","IEEE","IEEE Conferences"
"Epersist: A Two-Wheeled Self Balancing Robot Using PID Controller And Deep Reinforcement Learning","G. S. Krishna; D. Sumith; G. Akshay","Department of Data Science and Artificial Intelligence, IIIT Naya Raipur, Chattisgarh, India; Department of Data Science and Artificial Intelligence, IIIT Naya Raipur, Chattisgarh, India; Department of Data Science and Artificial Intelligence, IIIT Naya Raipur, Chattisgarh, India","2022 22nd International Conference on Control, Automation and Systems (ICCAS)","9 Jan 2023","2022","","","1488","1492","A two-wheeled self-balancing robot is an example of an inverse pendulum and is an inherently non-linear, unstable system. The fundamental concept of the proposed framework “Epersist” is to overcome the challenge of counterbalancing an initially unstable system by delivering robust control mechanisms, Proportional Integral Derivative (PID), and Reinforcement Learning (RL). Moreover, the micro-controller NodeMCU ESP32 and inertial sensor in the Epersist employ fewer computational procedures to give accurate instruction regarding the spin of wheels to the motor driver, which helps control the wheels and balance the robot. This framework also consists of the mathematical model of the PID controller and a novel self-trained advantage actor-critic algorithm as the RL agent. After several experiments, control variable calibrations are made as the benchmark values to attain the angle of static equilibrium. This “Epersist” framework proposes PID and RL-assisted functional prototypes and simulations for better utility.","2642-3901","978-89-93215-24-3","10.23919/ICCAS55662.2022.10003940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003940","Two Wheeled Self-Balancing Robot;PID;Reinforcement Learning;NodeMCU ESP32","Robust control;Wheels;Prototypes;Reinforcement learning;Robot sensing systems;Mathematical models;Mobile robots","control system synthesis;deep learning (artificial intelligence);microcontrollers;mobile robots;nonlinear control systems;reinforcement learning;robust control;three-term control;wheels","actor-critic algorithm;control variable calibrations;deep reinforcement learning;Epersist framework;inertial sensor;inverse pendulum;microcontroller NodeMCU ESP32;PID controller;proportional integral derivative;RL agent;robust control mechanisms;two-wheeled self balancing robot","","1","","15","","9 Jan 2023","","","IEEE","IEEE Conferences"
"Real-time ai in xpilot using reinforcement learning","M. Allen; K. Dirmaier; G. Parker","Computer Science Department, Connecticut College, New London, CT, USA; Computer Science Department, Connecticut College, New London, CT, USA; Computer Science Department, Connecticut College, New London, CT, USA","2010 World Automation Congress","10 Dec 2010","2010","","","1","6","Reinforcement learning (RL) allows agents to learn a best-possible long-term course of action, based on immediate positive and negative rewards. This approach enables real-time learning, since the agent constantly adjusts the value of actions taken, eventually selecting that action with highest value in each environment-state it encounters. We investigate the use of the Q-learning RL technique in an agent that learns to intelligently navigate the Xpilot video game environment in real time. We compare learning performance for different reward and action models, and discuss the challenges of RL methods in such a reasonably complex domain.","2154-4824","978-1-889335-42-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5665403","Xpilot;Reinforcement Learning;Real-time Learning","Games;Real time systems;Learning;Marine vehicles;Computer crashes;Navigation","computer games;learning (artificial intelligence);real-time systems;software agents","real time AI;reinforcement learning;agent learning;Xpilot video game;Q-learning RL technique","","1","","11","","10 Dec 2010","","","IEEE","IEEE Conferences"
"Improved Robustness of Reinforcement Learning Based on Uncertainty and Disturbance Estimator","J. Choi; H. Park; J. Baek; S. Han","Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, Korea; Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, Korea; Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, Korea; Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, Korea","2022 22nd International Conference on Control, Automation and Systems (ICCAS)","9 Jan 2023","2022","","","267","272","This paper proposes a method to improve the robustness of RLs based on model-free uncertainty and disturbance estimator (RL-based UDE). In the real environment, instead of using optimal trajectory and control techniques to perform complex tasks, it learns through RL and supplements robustness by using uncertainty and disturbance estimator (UDE). From UDE, the robotics system can be improved the stability by appropriately canceling the uncertainty and disturbance without efforts to obtain model information; hence the UDE can compensate for the performance degradation of RL when system is non-stationary. In addition, the performance can be improved by reducing the sensor noise from low-pass filter of UDE. It is shown through an experiment that the proposed RL-based UDE provides robustness.","2642-3901","978-89-93215-24-3","10.23919/ICCAS55662.2022.10003692","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003692","Reinforcement learning;uncertainty and disturbance estimator;robust robotics control","Degradation;Uncertainty;Low-pass filters;Reinforcement learning;Robot sensing systems;Control systems;Robustness","low-pass filters;reinforcement learning;robust control;uncertain systems","complex tasks;control techniques;disturbance estimator;low-pass filter;model information;model-free uncertainty;optimal trajectory;reinforcement learning;RL-based UDE;robotics system;robustness;sensor noise","","1","","42","","9 Jan 2023","","","IEEE","IEEE Conferences"
"Research on reinforcement learning of the intelligent robot based on self-adaptive quantization","Zhang Rubo; Sun Yu; Wang Xingoe; Yang Guangmin; Gu Guochang","Harbin Engineering of Technology, Harbin, China; Harbin College, Harbin, China; NA; Harbin Engineering of Technology, Harbin, China; Harbin Engineering of Technology, Harbin, China","Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393)","6 Aug 2002","2000","2","","1226","1229 vol.2","The concept of the reinforcement learning comes from behavior psychology that takes behavior learning as trial and error, by which the states of the environment are mapped into corresponding actions. There is a question of how can the behaviourism be used to learn the actions in interaction with the environment in designing an intelligent robot. In the paper, the actions that the robot takes to avoid obstacles are taken as one class of behaviors and the reinforcement learning is used to realize behavior learning of obstacle avoidance. The quantization of the state space is very important in improving the robot's learning speed. The SOM neural network is adopted to get self-adaptive quantization of the state space. The self-organization characteristic of the SOM neural network makes it possible to solve the adaptation problem and is flexible in space quantization. The reinforcement learning is used to settle the robot learning of collision avoidance behavior based on quantization of the state space and satisfying results are obtained.","","0-7803-5995-X","10.1109/WCICA.2000.863438","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=863438","","Learning;Intelligent robots;Quantization","learning (artificial intelligence);robots;self-organising feature maps;state-space methods","reinforcement learning;intelligent robot;self-adaptive quantization;behavior psychology;behavior learning;behaviourism;obstacles avoidance;state space quantization;learning speed;SOM neural network;adaptation;avoidance","","1","","3","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Model-free LQ control for unmanned helicopters using reinforcement learning","D. J. Lee; H. Bang","Division of AeroSpace Engineering, School of Mechanical, AeroSpace & Systems Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Division of AeroSpace Engineering, School of Mechanical, AeroSpace & Systems Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","2011 11th International Conference on Control, Automation and Systems","19 Dec 2011","2011","","","117","120","This paper concerns with the autonomous flight control system of an unmanned helicopter. We adopt a model-free discrete linear quadratic regulation (LQR) architecture based on reinforcement learning algorithm by rewriting the Q-learning approach. From input and output data, the linear quadratic optimal gain is directly found without system identification procedure. Least square method is adopted in order to estimate the Q-value and the parameters related to optimal control gain. This methodology does not access to an exact model of the system and can be applied to full flight envelop maneuvering from hovering to aggressive flight with small modification. We constructed numerical simulations to evaluate the proposed algorithm with a discrete linear model of the unmanned helicopter.","2093-7121","978-89-93215-03-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106389","Linear Quadratic Regulation;Reinforcement Learning;Unmanned Helicopters","Helicopters;Learning;Feedback control;Mathematical model;Aerospace control;Adaptation models;Control systems","aircraft control;autonomous aerial vehicles;control engineering computing;discrete systems;estimation theory;helicopters;learning (artificial intelligence);least mean squares methods;least squares approximations;linear quadratic control;parameter estimation","model-free LQ control;unmanned helicopters;autonomous flight control system;model-free discrete linear quadratic regulation architecture;LQR architecture;reinforcement learning algorithm;Q-learning approach;linear quadratic optimal gain;system identification procedure;least square method;Q-value estimation;parameter estimation;optimal control gain;full flight envelop maneuvering;hovering;aggressive flight;discrete linear model","","","","10","","19 Dec 2011","","","IEEE","IEEE Conferences"
"Preview control with reinforcement learning for air combat","Changyou Liu; Ningquan Luo",Shenyang University of Technology; Shenyang University of Technology,"The 2002 International Conference on Control and Automation, 2002. ICCA. Final Program and Book of Abstracts.","8 Sep 2003","2002","","","192","192","","","0-7803-7412-6","10.1109/ICCA.2002.1229638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1229638","","Learning;Control systems;Aircraft;Aerospace control;Intelligent control;Fuzzy logic;Uncertainty;Feedback;Programmable control;Adaptive control","","","","","","","IEEE","8 Sep 2003","","","IEEE","IEEE Conferences"
"How to easily make a policy network of reinforcement learning robust without physical modeling","H. Park; S. Baek; S. Han","Department of Convergence IT Engineering, Pohang University of Science and Technology, Pohang, South Korea; Department of Convergence IT Engineering, Pohang University of Science and Technology, Pohang, South Korea; Department of Convergence IT Engineering, Pohang University of Science and Technology, Pohang, South Korea","2022 22nd International Conference on Control, Automation and Systems (ICCAS)","9 Jan 2023","2022","","","262","266","In this paper, we propose ensemble inverse model network based disturbance observer (EIMN-DOB) to improve the robustness of the policy network (PN) which is a training result of policy based reinforcement learning (RL), without physical modeling. EIMN-DOB uses the ensemble model of the inverse model network (IMN), which acts as a nominal inverse model, and can estimate and cancel model uncertainty and disturbance like a typical disturbance observer (DOB) without a physical modeling. Because EIMN is trained from the data used in training RL, the additional training data for expressing the inverse model are not required. The experiments in this paper appeared that the PN of soft actor critic(SAC) combined with EIMN-DOB maintains control performance even in the presence of disturbance in continuous control benchmark tasks based on Mujoco physics engine. When the trained PN is used with EIMN-DOB in the real environment, the control performance in simulator can be preserved in the real environment, and it is expected to be utilized to minimize the sim-to-real gap of RL.","2642-3901","978-89-93215-24-3","10.23919/ICCAS55662.2022.10003696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003696","Disturbance observer;Policy based reinforcement learning;Safe learning control;Ensemble","Training;Uncertainty;Training data;Reinforcement learning;Benchmark testing;Disturbance observers;Robustness","control system synthesis;neurocontrollers;observers;reinforcement learning;robust control","EIMN-DOB;ensemble inverse model network based disturbance observer;ensemble model;model uncertainty;Mujoco physics engine;nominal inverse model;PN;policy based reinforcement learning;policy network;RL;SAC;soft actor critic","","","","11","","9 Jan 2023","","","IEEE","IEEE Conferences"
"Multi-agent reinforcement learning based on quantum chaotic computer","Xiang-Ping Meng; Xin-Xin Wang; Jun Meng","Department of Electrical Engineering, Changchun Institute of Posts and Telecommunications, Changchun, Jilin, China; Information engineering College, Northeast Dianli University, Jilin, China; School of Computer Science&Engineering, Changchun University of Technology, Changchun, Jilin, China","2008 IEEE International Conference on Automation and Logistics","30 Sep 2008","2008","","","2486","2489","A novel learning policy in multi-agent reinforcement learning is presented, trying to find another tradeoff of exploration and exploitation efficiently, It use the output of the classical quantum computer as an input for chaotic dynamics amplifier, The novel amplifier consider the chaotic effect, it can amplify the initial value in polynomial time. It considers the action selection problem and argues that the problem, in principle, can be solved in polynomial time if it combines the quantum computer with the chaotic dynamics amplifier based on the logistic map.","2161-816X","978-1-4244-2502-0","10.1109/ICAL.2008.4636586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4636586","Reinforcement learning;Quantum computation;Chaotic dynamics;Logistic map","Quantum computing;Computers;Roads;Chaos;Learning;Logistics;Artificial neural networks","computational complexity;learning (artificial intelligence);multi-agent systems;quantum computing","multiagent reinforcement learning;quantum chaotic computer;chaotic dynamics amplifier;polynomial time","","","","14","IEEE","30 Sep 2008","","","IEEE","IEEE Conferences"
"Coaching: Human-assisted approach for reinforcement learning","N. Suppakun; T. Maneewarn","Institute of Fleld roBOtics (FIBO), King Mongkut's University of Technology Thonburi, Bangkok, Thailand; King Mongkut's University of Technology Thonburi, Bangkok, TH","2017 International Conference on Robotics and Automation Sciences (ICRAS)","19 Oct 2017","2017","","","177","181","The technique called ‘Coaching’ is proposed in this work. Coaching is a method to accelerate learning by employing a human knowledge at the early phase of learning. The human coach can guide a robot behavior by temporarily replacing the global goal with an intermediate target. During the coaching process, an action is chosen by a greedy policy such that it is most likely driving the robot to the intermediate target. When the intermediate target is reached, a normal pair of policy (f-greedy) and reward function is switched back. However, the global reward function is still used for updating the state-action value during both coaching and non-coaching periods. A human coach can guide the robot by using 8 verbal commands to place the intermediate target location relative to the agent current location. In this work, Q learning algorithm was used to test with the proposed method on 2 learning tasks: ball following, and obstacle avoidance. The proposed technique resulted in faster learning performance when compared to the traditional method of reinforcement learning.","","978-1-5386-3995-5","10.1109/ICRAS.2017.8071940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071940","reinforcement learning;human assisted learning;semi-supervised learning;robot coaching","Robots;Learning (artificial intelligence);Collision avoidance;Function approximation;Markov processes;Indexes","learning (artificial intelligence)","coaching process;greedy policy;global reward function;noncoaching periods;human coach;intermediate target location;Q learning algorithm;reinforcement learning;human-assisted approach;human knowledge;robot behavior","","","","11","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"A Robust Motion Planning Algorithm Based on Reinforcement Learning","Y. Xu; M. Gong; Y. Chen; S. Fan; Z. Qin; H. Wu","Software Development Center, China Information Technology, Design & Consulting Institute Co., Ltd., Beijing, China; Software Development Center, China Information Technology, Design & Consulting Institute Co., Ltd., Beijing, China; Software Development Center, China Information Technology, Design & Consulting Institute Co., Ltd., Beijing, China; Software Development Center, China Information Technology, Design & Consulting Institute Co., Ltd., Beijing, China; Software Development Center, China Information Technology, Design & Consulting Institute Co., Ltd., Beijing, China; Software Development Center, China Information Technology, Design & Consulting Institute Co., Ltd., Beijing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","5543","5547","Path planning is the basis for robots to perform all kinds of intelligent tasks. In this paper, we proposed a path generation algorithm based on reinforcement learning. It first uses the A algorithm, searches for an initial trajectory, generates a series of sub-target points, and then obtains a safe and smooth trajectory based on the SAC reinforcement learning algorithm. Compared to existing trajectory generation algorithms such as DWA, it is possible to adapt to more complex scenarios because it does not need to set the loss function manually. In this paper, simulation experiments are used to prove the practicability of the algorithm. And the proposed algorithm can generate safe and continuous trajectories in real-time.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055364","Path finding;A*;reinforcement learning;smooth tragectory","Training;Adaptation models;Trajectory planning;Reinforcement learning;Transforms;Real-time systems;Stability analysis","collision avoidance;mobile robots;path planning;reinforcement learning;trajectory control","A algorithm;continuous trajectories;existing trajectory generation algorithms;initial trajectory;intelligent tasks;path generation algorithm;path planning;robust motion planning algorithm;SAC reinforcement learning algorithm;safe trajectories;safe trajectory;smooth trajectory;sub-target points","","","","14","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Legged balance on moving table by reinforcement learning","W. Seol; Y. Jeon; K. Kim; S. Kim","Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Department of Mechanical Engineering, Koera Advanced Institute of Science and Technology, Daejeon, Republic of Korea","2020 20th International Conference on Control, Automation and Systems (ICCAS)","1 Dec 2020","2020","","","900","905","Balancing is one of the most essential ability for legged robots. From maintaining posture on unstable surfaces to withstanding sudden force disturbances, balancing is widely used in rescue robots. Among these various balancing issues, this study focused on balancing on moving plates. The study is applicable when the robot needs to perform an operation on an earthquake or shaking ship environment. In addition, since ZMP(Zero Moment Point) is used for the control, the performance is exhibited not only when an external force is applied such as wind force, but also an inertial force is exerted. In this study, reinforcement learning was used so that the proposed algorithm can avoid redundancy issues and kinematic singularities which cause problems in traditional method. The simulation results show how this new approach can improve the robot's balancing ability.","2642-3901","978-89-93215-20-5","10.23919/ICCAS50221.2020.9268268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268268","balancing;legged robot;kinematic singularity;dynamic simulation;reinforcement learning","Robots;Kinematics;Robot kinematics;Robot sensing systems;Reinforcement learning;Computational modeling;Legged locomotion","humanoid robots;learning (artificial intelligence);legged locomotion;motion control;robot dynamics","rescue robots;balancing issues;moving plates;earthquake;external force;wind force;inertial force;reinforcement learning;balancing ability;legged balance;moving table;legged robots;unstable surfaces;withstanding sudden force disturbances","","","","5","","1 Dec 2020","","","IEEE","IEEE Conferences"
"A reinforcement learning approach to problem solving for time-varying quadratic optimal control with unknown dynamics","Z. -Y. Liu; J. -H. Yu; F. Wei; Q. Zhang; X. -L. Wei","Beijing Electro-mechanical Engineering Institute, Beijing, China; Beijing Electro-mechanical Engineering Institute, Beijing, China; Beijing Electro-mechanical Engineering Institute, Beijing, China; Beijing Electro-mechanical Engineering Institute, Beijing, China; Beijing Electro-mechanical Engineering Institute, Beijing, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","1605","1610","The research of the linear quadratic regulator (LQR) problem of continuous-time linear systems with time-varying paramaters is carried out in this paper. As is known, the solution of the LQR problem is characterized by the induced differential Riccati equation (DRE), and the classical approaches to solving DRE problems usually presuppose a complete knowledge of system dynamics. However, this precondition does not alway s hold, as the system dynamics may be partially or completely unknown, or may vary over time. This paper aims to propose a model-free way to learn the optimal controller of continuous-time LTV systems from the date extracted from control inputs and system trajectories. More specifically, the process of solving the DRE is first converted to solving an iterative sequence of differential Lyapunov equations (DLE) via a policy iteration reinforcement learning mechanism. During each iteration, the Bézier control points technique is adopted to calculate the solution approximately associated with each DLE in a least-squares manner. The result given in this work shows that, with an appropriate choice of the Bézier function, the proposed off-policy reinforcement learning scheme guarantees convergence to an approximate optimal solution with an arbitrarily small neighborhood. Finally, a simulation example is used to illustrates the merits and effectiveness of the presented algorithm.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023565","Linear time-varying (LTV) system;linear quadratic regulator (LQR);reinforcement learning (RL);differential Riccati equation (DRE)","Regulators;System dynamics;Riccati equations;Optimal control;Reinforcement learning;Mathematical models;Trajectory","approximation theory;continuous time systems;differential equations;iterative methods;linear quadratic control;linear systems;Lyapunov methods;reinforcement learning;Riccati equations;time-varying systems","Bézier control points technique;continuous-time linear systems;continuous-time LTV systems;control inputs;differential Lyapunov equations;DLE;DRE problems;induced differential Riccati equation;iterative sequence;linear quadratic regulator problem;LQR problem;optimal controller;reinforcement learning mechanism;system dynamics;system trajectories;time-varying paramaters;time-varying quadratic optimal control","","","","5","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Electricity Price Controller in Smart Grids","Y. -H. Lin; W. -Y. Chiu","Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","1820","1824","Striking a balance between power supply and demand is the most imperative target for any electricity grid system. In order to address variability of renewable energy in the modern grid, a robust and elastic balancing scheme is required. Conventional model-based approaches can suffer from great performance degradation given the uncertainty induced by the renewable energy. As such, this study explores a model-free approach by proposing a reinforcement learning based pricing scheme that balances the power supply and demand. A price signal is considered as the control signal for the balance management. Case studies involving different market parameters and different time resolutions were conducted to show the effectiveness of the proposed methodology.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9650043","Ministry of Science and Technology of Taiwan(grant numbers:MOST 109-2221-E-007-020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650043","Reinforcement learning;price controller;smart grids","Renewable energy sources;Supply and demand;Uncertainty;Power supplies;Reinforcement learning;Pricing;Power markets","learning (artificial intelligence);power engineering computing;power generation control;power generation economics;power markets;pricing;smart power grids","electricity grid system;renewable energy;elastic balancing scheme;conventional model-based approaches;model-free approach;reinforcement learning based pricing scheme;power supply;control signal;electricity price controller;smart grid","","","","9","","28 Dec 2021","","","IEEE","IEEE Conferences"
"DeepTH: Chip Placement with Deep Reinforcement Learning Using a Three-Head Policy Network","D. Zhao; S. Yuan; Y. Sun; S. Tu; L. Xu","Dept. of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Dept. of Micro-Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Dept. of Micro-Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Dept. of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Dept. of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)","2 Jun 2023","2023","","","1","2","Modern very-large-scale integrated (VLSI) circuit placement with huge state space is a critical task for achieving layouts with high performance. Recently, reinforcement learning (RL) algorithms have made a promising breakthrough to dramatically save design time than human effort. However, the previous RL-based works either require a large dataset of chip placements for pre-training or produce illegal final placement solutions. In this paper, DeepTH, a three-head policy gradient placer, is proposed to learn from scratch without the need of pre-training, and generate superior chip floorplans. Graph neural network is initially adopted to extract the features from nodes and nets of chips for estimating the policy and value. To efficiently improve the quality of floorplans, a reconstruction head is employed in the RL network to recover the visual representation of the current placement, by enriching the extracted features of placement embedding. Besides, the reconstruction error is used as a bonus during training to encourage exploration while alleviating the sparse reward problem. Furthermore, the expert knowledge of floorplanning preference is embedded into the decision process to narrow down the potential action space. Experiment results on the ISPD 2005 benchmark have shown that our method achieves 19.02% HPWL improvement than the analytic placer DREAMPlace and 19.89% improvement at least than the state-of-the-art RL algorithms.","1558-1101","979-8-3503-9624-9","10.23919/DATE56975.2023.10137100","National Natural Science Foundation of China (NSFC)(grant numbers:62174110); National Key RD Program of China(grant numbers:2021YFA0717400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10137100","Chip placement;Reinforcement learning;Intrin-sic reward;Three head;Visual reconstruction","Training;Deep learning;Visualization;Layout;Reinforcement learning;Very large scale integration;Benchmark testing","deep learning (artificial intelligence);electronic engineering computing;graph neural networks;graph theory;integrated circuit design;integrated circuit layout;reinforcement learning;VLSI","chip placement;deep reinforcement learning;DeepTH;floorplanning preference;graph neural network;huge state space;RL network;superior chip floorplans;three-head policy gradient placer;three-head policy network;very-large-scale integrated circuit placement;VLSI circuit placement","","","","4","","2 Jun 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning Control of a 2-DOF Helicopter","Z. Zhao; W. He; T. Zou","Guangzhou University, Guangzhou, China; Guangzhou University, Guangzhou, China; Guangzhou University, Guangzhou, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","145","150","The article develops a reinforcement learning control scheme for a two-degree-of-freedom (2-DOF) helicopter system to achieve robust tracking. The control framework is divided into two parts: the critic neural network (NN) and the actor NN, which are designed to evaluate the control performance and estimate system uncertainties, respectively. Besides, the gradient descent method is exploited to update the weight of radial basis function NNs. Under the proposed control strategy, the rigorous stability of the closed-loop system is analyzed and demonstrated by Lyapunov’s stability theory. Finally, the Matlab simulation results are provided to verify the efficacy of the suggested scheme.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023707","Reinforcement learning;2-DOF helicopter system;the gradient descent.","Uncertainty;Simulation;Helicopters;Process control;Artificial neural networks;Reinforcement learning;Stability analysis","aerospace computing;aircraft control;closed loop systems;control engineering computing;gradient methods;helicopters;Lyapunov methods;neural nets;radial basis function networks;reinforcement learning;robust control","2DOF helicopter;actor neural network;actor NN;closed-loop system;critic neural network;critic NN;gradient descent method;Lyapunov stability theory;Matlab simulation;radial basis function NN;reinforcement learning control;robust tracking;system uncertainty estimation;two-degree-of-freedom helicopter","","","","11","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement learning with adaptive networks","T. Sasaki; S. Yamada","Department of Intelligent Mechanical Engineering, Okayama University of Science, Okayama, Japan; Department of Intelligent Mechanical Engineering, Okayama University of Science, Okayama, Japan","2017 International Conference on Robotics and Automation Sciences (ICRAS)","19 Oct 2017","2017","","","1","5","The reinforcement learning (RL) with the incremental normalized Gaussian networks (INGnet) using the state list was proposed in order to improve the learning efficiency. This learning system was applied to the garage parking control, the walking control of the stilt-type biped robot on the slope, and the wall avoidance control of Khepera robot. Since in the ordinary condition to add the new processing units they were not set in the central region of the garage parking field or around the states during the normal walking, RL with INGnet did not learn the control effectively. RL with INGnet using the state list set the processing units at the necessary positions and learned the garage parking control and the walking control of the stilt-type biped robot on the slope. However, since it is easy to learn the wall avoidance control, the state list was not needed for RL with INGnet: RL with INGnet without the state list was able to learn it.","","978-1-5386-3995-5","10.1109/ICRAS.2017.8071905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071905","reinforcement learning;INGnet;state list;garage parking;stilt-type biped robot;Khepera robot","Legged locomotion;Process control;Learning systems;Robot sensing systems","collision avoidance;gait analysis;Gaussian processes;learning (artificial intelligence);legged locomotion;traffic control","reinforcement learning;adaptive networks;RL;incremental normalized Gaussian networks;INGnet;state list;learning efficiency;learning system;garage parking control;walking control;stilt-type biped robot;wall avoidance control;Khepera robot;processing units;garage parking field;normal walking","","","","12","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"A new approach for structural credit assignment in distributed reinforcement learning systems","Zhong Yu; Gu Guochang; Zhang Rubo","School of Computer Science and Technology, Harbin Engineering of Technology, Harbin, Heilongjiang, China; School of Computer Science and Technology, Harbin Engineering of Technology, Harbin, Heilongjiang, China; School of Computer Science and Technology, Harbin Engineering University, Harbin City, Heilongjiang Province, P.R.China","2003 IEEE International Conference on Robotics and Automation (Cat. No.03CH37422)","10 Nov 2003","2003","1","","1215","1220 vol.1","Most existing algorithm for structural credit assignment are developed for competitive reinforcement learning systems. In competitive reinforcement learning system, agents are activated one by one, so there is only one active agent at a time and structural credit assignment could be implemented by some temporal credit assignment algorithms. In collaborated reinforcement learning systems, agents are activated simultaneously, so how to transform the global reinforcement signal fed back from the environment to a reinforcement vector is a crucial difficulty that could not be slide over. In this article, the first really feasible and efficient structural credit assignment difficulty in collaborated reinforcement learning systems is primarily solved. The experiments show that the algorithm converges very rapidly and the assignment result is quite satisfying.","1050-4729","0-7803-7736-2","10.1109/ROBOT.2003.1241758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241758","","Learning;International collaboration;Feeds;Computer science;Cities and towns;Multiagent systems;Computer hacking;Sun","unsupervised learning;multi-agent systems","structural credit assignment;distributed reinforcement learning systems;active agent;temporal credit assignment;reinforcement vector","","","","11","IEEE","10 Nov 2003","","","IEEE","IEEE Conferences"
"Research on Mobile Robot Path Planning Based on Deep Reinforcement Learning","X. Ye; S. Zhang","Harbin Engineering University, Harbin, Heilongjiang Province, China; Harbin Engineering University, Harbin, Heilongjiang Province, China","2020 IEEE International Conference on Mechatronics and Automation (ICMA)","26 Oct 2020","2020","","","1","6","Path planning based on deep reinforcement learning has been a hot topic in the field of mobile robots in recent years, but there are still many shortcomings in its application. Such as the lack of agents' generalization ability, the loss of targets in the process of exploration and the inefficiency of data exploration. In this paper, we propose an improved path planning algorithm to solve the above three problems. We take the Depth Deterministic Policy Gradient (DDPG) algorithm as the basic algorithm, and increase generalization ability of agents by adding adaptive gaussian noise, then, in order to solve the problem of losing target in exploration process, we redesign the reward functions based on the curiosity mechanism. We also add the memory units for agents to make the exploration process more efficient. Finally, we verify that the improved algorithm has good adaptability in different obstacle avoidance environments through simulation experiments.","2152-744X","978-1-7281-6416-8","10.1109/ICMA49215.2020.9233738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233738","Path planning;DDPG;Adaptive gaussian noise;Curiosity mechanism;Memory unit","","Gaussian noise;generalisation (artificial intelligence);learning (artificial intelligence);mobile robots;multi-agent systems;neurocontrollers;path planning","mobile robot path planning;deep reinforcement learning;adaptive Gaussian noise;depth deterministic policy gradient algorithm;agent generalization ability","","","","16","IEEE","26 Oct 2020","","","IEEE","IEEE Conferences"
"Application of Reinforcement Learning in PMLM Speed Control","G. Hong-xia; W. Jie; L. Yong-qiang","Electrical Power College, South China University of Technology, Guangzhou, China; Electrical Power College, South China University of Technology, Guangzhou, China; Electrical Power College, South China University of Technology, Guangzhou, China","2007 IEEE International Conference on Control and Automation","5 Nov 2007","2007","","","1823","1827","Reinforcement learning is used in permanent magnet linear motor control system in this paper. The proposed controller used the reinforcement signal from plant when the system model is unknown. The controller's parameters could be updated by use of adaptive heuristic assessment algorithm. The simulation results show that the proposed method can make the system have more dynamic performance and steady accuracy.","1948-3457","978-1-4244-0817-7","10.1109/ICCA.2007.4376676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376676","Reinforcement learning;adaptive heuristic assessment algorithm;action selection element;action critic element","Learning;Velocity control;Control systems;Permanent magnet motors;Heuristic algorithms;Stators;Automatic control;Programmable control;Adaptive control;Gears","angular velocity control;electric machine analysis computing;linear motors;machine control;permanent magnet motors","PMLM speed control;reinforcement learning;permanent magnet linear motor control system;adaptive heuristic assessment algorithm","","","","9","IEEE","5 Nov 2007","","","IEEE","IEEE Conferences"
"The apprentice modeling through reinforcement with a temporal analysis using the Q-learning algorithm","M. V. C. Guelpeli; B. S. de Oliveira; M. A. Pinto; R. C. dos Santos","Centro Universitário de Barra Mansa (UBM), Rio de Janeiro, Brazil; Centro Universitário de Barra Mansa (UBM), Rio de Janeiro, Brazil; Centro Universitário de Barra Mansa (UBM), Rio de Janeiro, Brazil; Centro Universitário de Barra Mansa (UBM), Rio de Janeiro, Brazil","2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE)","20 Aug 2012","2012","1","","296","300","This work aims to create the simulations by varying the alpha (a — Learning rate) and Gamma (y — Time reduction) values, such parameters found in the q-learning algorithm, which is possible to analyze the algorithms convergence, on what concerns the variations of these parameters. This work seeks to state that the parameters variations of Alpha and Gamma interfere on the convergence of Q-learning algorithm, thus, in the ITS learning.","","978-1-4673-0089-6","10.1109/CSAE.2012.6272601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272601","Machine learning;Learning reinforcement;Q-learning;intelligence tutoring system","Computational modeling;Analytical models;Convergence;Learning systems;Adaptation models;Machine learning","intelligent tutoring systems;learning (artificial intelligence)","apprentice modeling;temporal analysis;q-learning algorithm;alpha values;gamma values;ITS learning","","","","6","IEEE","20 Aug 2012","","","IEEE","IEEE Conferences"
"Optimal Observation Policy of Fault Diagnosis: A Reinforcement Learning Approach","J. Wang; P. Dai; W. Yu","School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of Mathematics, Southeast University, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","7206","7210","In this paper, a new approach is proposed to solve an optimization problem of sensor activation policy for fault diagnosis in discrete event systems. The case of dynamic observation is under consideration, where sensors are turned on or off to monitor the occurrence of observable events under a policy. To find the optimal sensor activation policy for diagnosis problem of numerical costs, a structure of most permissive observer is introduced to be a basic model to apply the reinforcement learning technique and a brief algorithm is also proposed to realized the method. The scalability and feasibility of our approach can be verified through simulation examples.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9326931","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326931","Policy optimization;fault diagnosis;discrete event system;reinforcement learning","Automata;Optimization;Fault diagnosis;Sensors;Monitoring;Reinforcement learning;Switches","discrete event systems;fault diagnosis;learning (artificial intelligence);observers;optimisation;sensors","optimal observation policy;fault diagnosis;reinforcement learning approach;optimization problem;discrete event systems;dynamic observation;observable events;optimal sensor activation policy;diagnosis problem;reinforcement learning technique;numerical costs;most permissive observer","","","","25","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Proactive Monitor Channel Allocation","T. Wu","School of Electronics and Information Technology, Sun Yat-sen University, Guangdong, China","2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","5 Apr 2021","2021","","","630","633","Reinforcement Learning algorithm has achieved lots of successful achievements in communication area and has been seen as a future technique for communication systems in 6G. We proposed a frequency allocation model to investigate the accuracy of this system based on reinforcement learning. We studied Deep Q Network and Dueling Deep Q Network to compare the efficiency of this two different algorithm. Applying Reinforcement Learning algorithm, we proposed A legitimate monitor to allocate serval channel and frequency to monitor several suspicious links to make sure the communication security in physical layer.","2689-6621","978-1-7281-8028-1","10.1109/IAEAC50856.2021.9391117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9391117","Reinforcement Learning(RL);Deep Q Network;Proactive Monitor;Security","Reinforcement learning;Channel allocation;Physical layer;Stability analysis;Security;Monitoring;Radio spectrum management","6G mobile communication;channel allocation;deep learning (artificial intelligence);frequency allocation;telecommunication computing;telecommunication security;wireless channels","proactive monitor channel allocation;reinforcement learning algorithm;frequency allocation model;dueling deep Q network;communication security;6G communication system","","","","4","IEEE","5 Apr 2021","","","IEEE","IEEE Conferences"
"Adaptive 3-D object classification with reinforcement learning","J. Garstka; G. Peters","Human-Computer Interaction, University of Hagen, Hagen, Germany; Human-Computer Interaction, University of Hagen, Hagen, Germany","2015 12th International Conference on Informatics in Control, Automation and Robotics (ICINCO)","10 Dec 2015","2015","02","","381","385","We propose an adaptive approach to 3-D object classification. In this approach appropriate 3-D feature descriptor algorithms for 3-D point clouds are selected via reinforcement learning depending on properties of the objects to be classified. This approach is supposed to be able to learn strategies for an advantageous selection of 3-D point cloud descriptor algorithms in an autonomous and adaptive way, and thus is supposed to yield higher object classification rates in unfamiliar environments than any of the single algorithms alone. In addition, we expect our approach to be able to adapt to subsequently added 3-D feature descriptor algorithms as well as to autonomously learn new object categories when encountered in the environment without further user assistance. We describe the 3-D object classification pipeline based on local 3-D features and its integration into the reinforcement learning environment.","","978-9-8975-8149-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7347796","3-D Object Classification;Reinforcement Learning","Learning (artificial intelligence);Three-dimensional displays;Pipelines;Histograms;Shape;Context;Object recognition","feature extraction;image classification;learning (artificial intelligence);object detection","adaptive 3D object classification approach;3D feature descriptor algorithms;3D point cloud descriptor algorithms;user assistance;3D object classification pipeline;reinforcement learning environment","","","","20","","10 Dec 2015","","","IEEE","IEEE Conferences"
"Adaptive Traffic Signal Control Through Time Period Division and Deep Reinforcement Learning","B. Gong; W. Zhu","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","4410","4414","In this paper, we propose an adaptive traffic signal control model combining time period division and deep reinforcement learning to improve the efficiency of traffic by dynamically changing the traffic phase duration according to the real-time situation. In our model, a day-time period is divided into two overlap period parts representing the morning situation and the evening situation, then the deep reinforcement learning algorithm-TD3 is selected to train the corresponding agent in each part, and finally a fuzzy method is used to coordinate these two agents at different time. In order to get better performance, we make some improvements in TD3. We improve the algorithm’s experience-replay mechanism and use some tricks in training. Simulation results shows that our model can effectively reduce vehicles’ accumulative waiting time, queue length and alleviate CO2 emission.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055131","Ministry of Education; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055131","traffic signal control;deep reinforcement learning;time period division","Deep learning;Training;Adaptation models;Simulation;Heuristic algorithms;Reinforcement learning;Real-time systems","adaptive control;control engineering computing;deep learning (artificial intelligence);fuzzy set theory;reinforcement learning;road traffic control;road vehicles;traffic engineering computing","adaptive traffic signal control;day-time period;deep reinforcement learning;evening situation;experience-replay mechanism;fuzzy method;morning situation;overlap period parts;TD3;time period division;traffic phase duration;vehicle accumulative waiting time;vehicle queue length","","","","10","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Towards training an agent in augmented reality world with reinforcement learning","V. V. R. M. K. R. Muvva; N. Adhikari; A. D. Ghimire","Department of Computer Science, Mississippi State University, Starkville, MS, USA; Department of Computer Science, Mississippi State University, Starkville, MS, USA; Department of Computer Science, Mississippi State University, Starkville, MS, USA","2017 17th International Conference on Control, Automation and Systems (ICCAS)","14 Dec 2017","2017","","","1884","1888","Reinforcement learning (RL) helps an agent to learn an optimal path within a specific environment while maximizing its performance. Reinforcement learning (RL) plays a crucial role on training an agent to accomplish a specific job in an environment. To train an agent an optimal policy, the robot must go through intensive training which is not cost-effective in the real-world. A cost-effective solution is required for training an agent by using a virtual environment so that the agent learns an optimal policy, which can be used in virtual as well as real environment for reaching the goal state. In this paper, a new method is purposed to train a physical robot to evade mix of physical and virtual obstacles to reach a desired goal state using optimal policy obtained by training the robot in an augmented reality (AR) world with one of the active reinforcement learning (RL) techniques, known as Q-learning.","","978-89-93215-14-4","10.23919/ICCAS.2017.8204283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8204283","Reinforcement Learning;Virtual Reality;Augmented Reality;Fiducial Markers","Artificial intelligence;Augmented reality;Robots","augmented reality;learning (artificial intelligence);multi-agent systems","augmented reality;Q-learning;active reinforcement learning techniques;virtual environment;cost-effective solution;intensive training;optimal policy;RL","","","","12","","14 Dec 2017","","","IEEE","IEEE Conferences"
"A Novel Multiagent Reinforcement Learning Algorithm Combination with Quantum Computation","Xiangping Meng; Yu Chen; Yuzhen Pi; Quande Yuan","Department of Electrical and Information Engineering, Changchun Institute of Posts and Telecommunications, Changchun, Jilin, China; Department of Information Engineering, Northeast Dianli University, Jilin, China; Department of Information Engineering, Northeast Dianli University, Jilin, China; Department of Information Engineering, Northeast Dianli University, Jilin, China","2006 6th World Congress on Intelligent Control and Automation","23 Oct 2006","2006","1","","2613","2617","A novel learning policy in multiagent reinforcement learning is presented, trying to find another tradeoff of exploration and exploitation efficiently, which is different from traditional greedy or softmax action selection method. The state and action of multiagent are represented with quantum superposition state, and probability amplitude is used to denote the probability of an action. Quantum search algorithm is adopted in multiagent action selection. The experiment results show that the new algorithm is effective and can help multiagent learn faster. This combination of quantum computing with multiagent reinforcement learning is an attempt, and the idea possibly brings more researches in multiagent reinforcement learning.","","1-4244-0332-4","10.1109/WCICA.2006.1712835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1712835","Multiagent;Stochastic games;Reinforcement learning;Quantum algorithm;Grover operator","Quantum computing;Learning;Stochastic processes;Game theory;Multiagent systems;Optimal control;Concurrent computing","learning (artificial intelligence);multi-agent systems;quantum computing","Multiagent;Stochastic games;Reinforcement learning;Quantum algorithm;Grover operator","","","","19","IEEE","23 Oct 2006","","","IEEE","IEEE Conferences"
"A Path Planning Algorithm Based on Deep Reinforcement Learning for Mobile Robots in Unknown Environment","H. Qin; B. Qiao; W. Wu; Y. Deng","College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, China","2022 IEEE 5th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)","26 Jan 2023","2022","5","","1661","1666","The path planning is one of the hottest issues of the mobile robotics, which is to find an optimized path for the robot to reach the target location from a start position. In this research a Proximal Policy Optimization (PPO) algorithm, which is by far the most widely used and efficient Deep Reinforcement Learning (DRL) algorithm, is applied to plan a safe and collision-free path from the initial position to the target one for the mobile robot in an unknown environment where a global map of the environment is unavailable for the robot during the planning. The mobile robot can only obtain the local information of the environment through the Lidar installed on it. The path planning of the mobile robots is modeled as a Partially Observed Markov Decision Problem (POMDP), of which the state space is represented by the position of the target relative to the mobile robot and the distance information obtained by the 2D-lidar, the action space is represented by the position for the mobile robot to reach in the next step. The execution of motion controller is only conducted for the robot to implement the learned strategy rather than conducted in the whole learning episodes during the training phase, which resulted in a great improvement in the training efficiency. Computer simulations demonstrated the effectiveness of the proposed approach.","2693-2776","978-1-6654-7968-4","10.1109/IMCEC55388.2022.10020025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020025","Mobile robot;Path planning;Deep reinforcement learning;Unknown environment;Proximal Policy Optimization","Training;Deep learning;Laser radar;Computer simulation;Reinforcement learning;Path planning;Planning","collision avoidance;learning (artificial intelligence);Markov processes;mobile robots;motion control;path planning","mobile robot;mobile robotics;path planning algorithm;unknown environment","","","","19","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"Vision-based Reinforcement learning: Moving object grasping with a Single Active-view Camera","S. Jang; H. Jeong; H. Yang","Department of Mechanical Engineering, Yonsei University, Seoul, Korea; Department of Mechanical Engineering, Yonsei University, Seoul, Korea; Department of Mechanical Engineering, Yonsei University, Seoul, Korea","2022 22nd International Conference on Control, Automation and Systems (ICCAS)","9 Jan 2023","2022","","","232","237","Traditional methods of grasping moving objects use visual information from camera that is statically mounted overlooking the task environment. However, when the camera view is hindered by an obstacle or a robot itself performing tasks, these methods cannot produce good results for grasping moving objects. This paper proposes a new moving object grasping method using only single active RGB camera attached to the robot with utilization of model free deep reinforcement learning strategy with Soft Actor-Critic(SAC) algorithm. Our proposed system uses RGB image data to train our proposed system and to further improve training and success rate, we integrate vision data with robot’s kinematic state information during training. A dense reward function is designed for efficient learning of the agent and training is done on simulation using a Ufactory xArm 7 manipulator. Experimental results demonstrate that with our proposed method, the manipulator can learn to autonomously grasp the moving object.","2642-3901","978-89-93215-24-3","10.23919/ICCAS55662.2022.10003899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003899","Vision-based Reinforcement Learning(RL);Soft Actor-Critic(SAC);Robotic grasping;Moving object","Training;Visualization;Robot vision systems;Force;Grasping;Reinforcement learning;Kinematics","cameras;image colour analysis;image sensors;learning (artificial intelligence);manipulators;mobile robots;robot vision","camera view;grasping moving objects;model free deep reinforcement learning strategy;moving object grasping method;RGB image data;single active RGB camera;single active-view camera;task environment;vision data;vision-based reinforcement learning;visual information","","","","40","","9 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning Controller Trained with Dynamic Performance Index and Its Application to 3-DOF Helicopter","Y. Chen; X. Yang; X. Zheng","Research Institute of Intelligent Control and Systems, Harbin Institute of Technology, Harbin, China; Research Institute of Intelligent Control and Systems, Harbin Institute of Technology, Harbin, China; Research Institute of Intelligent Control and Systems, Harbin Institute of Technology, Harbin, China","2018 Chinese Automation Congress (CAC)","24 Jan 2019","2018","","","3896","3901","In this paper, the dynamic performance index in the classical control theory is introduced into the reward of reinforcement learning. The incompatibility of large initial error with small operation error when using binary reward are solved. In the simulation experiment based on 3-DOF helicopter platform, the feasibility is proved, and the comparison result between this method and binary rewarding method is given. Experiments show that this method enables the 3-DOF helicopter to track time-varying signals. It can also cope with the large initial error and has better dynamic performance.","","978-1-7281-1312-8","10.1109/CAC.2018.8623597","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8623597","Reinforcement learning;continuous control;dynamic performance;3-DOF helicopters","Helicopters;Reinforcement learning;Vehicle dynamics;Performance analysis;Heuristic algorithms;Training;DC motors","aircraft control;control engineering computing;helicopters;learning (artificial intelligence);performance index;time-varying systems","dynamic performance index;reinforcement learning;initial error;operation error;binary reward;simulation experiment;binary rewarding method;time-varying signals;3-DOF helicopter platform","","","","9","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"K-Cluster Algorithm for Automatic Discovery of Subgoals in Reinforcement Learning","Ben-Nian Wang; Yang Gao; Zhao-Qian Chen; Jun-Yuan Xie; Shi-Fu Chen","National Laboratory for Novel Software Technology, Nanjing University, China; National Laboratory for Novel Software Technology, Nanjing University, China; National Laboratory for Novel Software Technology, Nanjing University, China; National Laboratory for Novel Software Technology, Nanjing University, China; National Laboratory for Novel Software Technology, Nanjing University, China","International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)","22 May 2006","2005","1","","658","662","Options have proven to be useful to accelerate agent's learning in many reinforcement learning tasks, determining useful subgoals is a key step for agent to create options. A K-cluster algorithm for automatic discovery of subgoals is presented in this paper. This algorithm can extract subgoals from the trajectories collected online in clustering way. The experiments show that the K-cluster algorithm can find subgoals more efficiently than the diverse density algorithm and that the reinforcement learning with this algorithm outperforms the one with the diverse density algorithm and flat Q-learning","","0-7695-2504-0","10.1109/CIMCA.2005.1631339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631339","","Learning;Clustering algorithms;Software algorithms;Laboratories;Computer science;Educational institutions;Accelerated aging;State-space methods;Computational intelligence;Computational modeling","learning (artificial intelligence);multi-agent systems;pattern clustering","K-cluster algorithm;automatic subgoal discovery;reinforcement learning;agent learning;diverse density algorithm;flat Q-learning","","","","8","IEEE","22 May 2006","","","IEEE","IEEE Conferences"
"RVI reinforcement learning for semi-Markov decision processes with average reward","Yanjie Li; Fang Cao","Division of Control and Mechatronics Engineering, Harbin Institute of Technology, Shenzhen Graduate School, Shenzhen, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China","2010 8th World Congress on Intelligent Control and Automation","23 Aug 2010","2010","","","1674","1679","Based on the sensitivity-based approach, we discuss the reinforcement learning problem of semi-Markov decision processes (SMDPs) with average reward. First, we provide a new Bellman optimality equation. On this basis, we propose a relative value iteration (RVI) reinforcement learning algorithm. The new RVI reinforcement learning algorithm may avoid the estimation of optimal average reward in the process of learning and has a good convergence rate.","","978-1-4244-6712-9","10.1109/WCICA.2010.5554785","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5554785","Semi-Markov decision processes;Performance potential;Relative value iteration;Reinforcement learning","Learning;Equations;Markov processes;Convergence;Estimation;Algorithm design and analysis;Heuristic algorithms","iterative methods;learning (artificial intelligence);Markov processes;optimisation","semiMarkov decision processes;sensitivity-based approach;reinforcement learning problem;SMDP;Bellman optimality equation;relative value iteration reinforcement learning algorithm;RVI reinforcement learning algorithm;optimal average reward;convergence rate","","","","22","IEEE","23 Aug 2010","","","IEEE","IEEE Conferences"
"Autonomous Navigation of UAV in Dynamic Unstructured Environments via Hierarchical Reinforcement Learning","K. Kou; G. Yang; W. Zhang; C. Wang; Y. Yao; X. Zhou","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China","2022 International Conference on Automation, Robotics and Computer Engineering (ICARCE)","22 Feb 2023","2022","","","1","5","Autonomous navigation of unmanned aerial vehicle (UAV) is one of the fundamental yet completely solved problems in automatic control. In this paper, an option-based hierarchical reinforcement learning approach is proposed for UAV autonomous navigation. Specifically, the proposed method consists of a high-level and two low-level model, where the high level behavior selection model learns a stable and reliable behavior selection strategy automatically, while the low-level obstacle avoidance model and target-driven control model implement two behavior strategies, obstacle avoidance and target approach, respectively, thus avoiding the dependence on manually designed control rules. Furthermore, the proposed model is pre-trained on large public dataset, allowing the model to converge quickly in various complex unstructured flight environments. Extensive experiments show that the proposed method indicates an overall advantage in various evaluation metrics, which indicating that the proposed method has a strong generalization capability in autonomous navigation task of UAV.","","978-1-6654-7548-8","10.1109/ICARCE55724.2022.10046655","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046655","Unmanned Aerial Vehicle (UAV);Autonomous Navigation;Hierarchical Reinforcement Learning","Training;Navigation;Reinforcement learning;Autonomous aerial vehicles;Reliability engineering;Behavioral sciences;Collision avoidance","autonomous aerial vehicles;collision avoidance;learning (artificial intelligence);mobile robots;navigation;path planning;reinforcement learning","automatic control;autonomous navigation task;behavior strategies;complex unstructured flight environments;dynamic unstructured environments;high level behavior selection model;low-level model;low-level obstacle avoidance model;manually designed control rules;option-based hierarchical reinforcement learning approach;reliable behavior selection strategy;solved problems;stable behavior selection strategy;target approach;target-driven control model;UAV autonomous navigation;unmanned aerial vehicle","","","","15","IEEE","22 Feb 2023","","","IEEE","IEEE Conferences"
"Towards reinforcement learning approach to energy-efficient control of server fans in data centres","Y. Berezovskaya; C. -W. Yang; V. Vyatkin","Luleå University of Technology, Luleå, Sweden; Luleå University of Technology, Luleå, Sweden; Luleå University of Technology, Luleå, Sweden","2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA )","30 Nov 2021","2021","","","1","4","Modern data centres require control, which aims to improve their energy efficiency and maintain their high availability. This work considers the implementation of a server fan agent, which is intended to minimise the power consumption of the corresponding server fan or group of fans. In the paper, the reinforcement learning approach to energy-efficient control of server fans is suggested. The reinforcement learning workflow is considered. The Simulink blocks simplifying the building of the environment for the reinforcement learning agent are developed. This work provides the framework for creating and training reinforcement learning agents of different types. As the paper is only a work-in-progress, possible type of agents and their training process is described, but training and deploying the agent is a work for the future.","","978-1-7281-2989-1","10.1109/ETFA45728.2021.9613639","Swedish Energy Agency(grant numbers:43090–2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613639","data centre;energy-efficient control;multi-agent control;reinforcement learning","Training;Fans;Data centers;Power demand;Software packages;Conferences;Reinforcement learning","computer centres;energy conservation;learning (artificial intelligence)","energy-efficient control;modern data centres;energy efficiency;server fan agent;reinforcement learning workflow;reinforcement learning agent;data centres","","","","15","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Optimal Tracking Control of Partial Unknown Continuous-Time Systems Using Integral Reinforcement Learning","W. Cheng; Z. Xiao; J. Li","School of Information and Control Engineering, Liaoning Shihua University, Fushun, P.R. China; School of Information and Control Engineering, Liaoning Shihua University, Fushun, P.R. China; School of Information and Control Engineering, Liaoning Shihua University, Fushun, P.R. China","2020 35th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","5 Feb 2021","2020","","","308","311","An integral reinforcement learning (IRL) algorithm is used for solving the optimal tracking control problem for partial unknown continuous-time systems that try to chase a polynomial reference signal. By using IRL to get the control input of the Bellman equation derived from the problem, the approximate controller is able to get without sufficient system dynamics. Firstly, the LQT problem is formulated. An augmented vector is defined, and an algebraic Riccati equation is obtained based on the dynamic programming method. Then, employing IRL yields the iterative Bellman equation and policy updating expression. And the approximate optimal tracking control policy is finally solved, under which the reference signal with higherorder polynomials and unknown model parameters can be tracked by a linear system with partial known model parameters, meanwhile the specific performance can be minimized. Finally, by a simulation example, the efficiency of the provided method is confirmed.","","978-1-7281-7684-0","10.1109/YAC51587.2020.9337613","National Natural Science Foundation of China(grant numbers:61673280); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9337613","Optimal tracking control;integral reinforcement learning;policy iteration","Linear systems;System dynamics;Simulation;Heuristic algorithms;Riccati equations;Reinforcement learning;Mathematical model","approximation theory;continuous time systems;discrete time systems;dynamic programming;iterative methods;learning (artificial intelligence);linear quadratic control;linear systems;neurocontrollers;nonlinear control systems;optimal control;Riccati equations;tracking","policy updating expression;dynamic programming;higher order polynomials;optimal tracking control;integral reinforcement learning;partial known model parameters;linear system;unknown model parameters;iterative Bellman equation;algebraic Riccati equation;LQT;polynomial reference signal;partial unknown continuous-time systems","","","","12","IEEE","5 Feb 2021","","","IEEE","IEEE Conferences"
"Optimal Operation of Integrated Energy System Based on Deep Reinforcement Learning","F. Zhang; F. Zhou; J. Zhao; W. Wang","Key Laboratory of Intelligent Control and Optimization for Industrial Equipment, Ministry of Education, Dalian University of Technology; Key Laboratory of Intelligent Control and Optimization for Industrial Equipment, Ministry of Education, Dalian University of Technology; Key Laboratory of Intelligent Control and Optimization for Industrial Equipment, Ministry of Education, Dalian University of Technology; Key Laboratory of Intelligent Control and Optimization for Industrial Equipment, Ministry of Education, Dalian University of Technology","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","149","154","Integrated energy system (IES) combining multi energies such as power, heat, cooling and gas, can be applied to improve the overall energy efficiency based on the energy cascade utilization. In order to enhance the operation efficiency of IES, an operation optimization method is proposed in this paper based on deep deterministic policy gradient (DDPG). Two categories of models of linear ones and nonlinear ones are established, then a nonlinear optimization model is constructed by taking the annual operating cost as the objective. With regard to the instability in its training process of DDPG, two improved strategies of scaled initial state spaces and dynamic noise compensation are presented. In the case study, the proposed method is employed to obtain the optimal operation strategies for a typical scenario of a practical IES. The experimental results demonstrate that the proposed one not only obtains the most economical strategies, but also exhibits the robustness of the algorithm.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9326839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326839","Integrated Energy System;Optimal Operation;Nonlinear Model;Deep Deterministic Policy Gradient","Optimization;Intelligent control;Linear programming;Gaussian noise","cost reduction;energy conservation;learning (artificial intelligence);optimisation;power engineering computing","economical strategies;dynamic noise compensation;IES;optimal operation strategies;annual operating cost;nonlinear optimization model;DDPG;deep deterministic policy gradient;operation optimization method;energy cascade utilization;energy efficiency;deep reinforcement learning;integrated energy system","","","","18","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Research on Online Reinforcement Learning Method Based on Experience-Replay","N. Hu; Z. Ge; X. Chen; C. Ding; H. Shi","China Electronic Product Reliability and Environmental Testing Research Institute, Guangzhou, China; China Electronic Product Reliability and Environmental Testing Research Institute, Guangzhou, China; China Electronic Product Reliability and Environmental Testing Research Institute, Guangzhou, China; China Electronic Product Reliability and Environmental Testing Research Institute, Guangzhou, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China","2018 IEEE International Conference on Information and Automation (ICIA)","26 Aug 2019","2018","","","1338","1343","As for standard reinforcement learning, the key is that the agent’s next step is directed by the instantaneous and delayed reporting from constant interaction with the environment and trial and error learning. But it makes the convergence rate slower for actual reinforcement learning; at the same time, inconsistency state will occur in the agent learning process. Therefore, it is necessary for the agent to remember what has been learned within the time specified to improve the convergence and robustness of decision making. With regard to the above-mentioned issues, this paper proposes to accelerate the convergence rate of reinforcement learning by using the function approximation ability of neural network and to improve the robustness of reinforcement learning by using the Memory-based Experience-Replay(ER) algorithm. The experimental results show the effectiveness of the proposed method.","","978-1-5386-8069-8","10.1109/ICInfA.2018.8812454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812454","Experience-Replay;Reinforcement Learning;Neural Network","Approximation algorithms;Reinforcement learning;Neural networks;Convergence;Decision making;Heuristic algorithms;Function approximation","decision making;function approximation;learning (artificial intelligence);multi-agent systems;neural nets","online reinforcement learning method;agent learning process;trial and error learning;decision making;function approximation ability;neural network;Experience-Replay algorithm","","","","17","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"Higher-order Polynomial Signal Tracking Control of Unknown Systems using Off-policy Integral Reinforcement Learning","W. Cheng; J. Li","School of Information and Control Engineering, Liaoning Shihua University, Liaoning, P.R. China; School of Information and Control Engineering, Liaoning Shihua University, Liaoning, P.R. China","2020 16th International Conference on Control, Automation, Robotics and Vision (ICARCV)","8 Jan 2021","2020","","","1077","1081","This paper aims at using an off-policy integral reinforcement learning (IRL) algorithm to solve the linear quadratic tracking (LQT) control problem of completely unknown continuous-time systems, such that an arbitrary higherorder polynomial signal can be followed via an optimal approach. Firstly, a linear continuous-time system with unknown model matrices is introduced with a target of tracking the reference signal with higher-order polynomials. Secondly, based on the knowledge of the on-policy IRL, an off-policy IRL algorithm is used to solve the derived iterative Bellman equation and update the control policy resulting in the optimal control policy. Finally, a simulation example is given to show the efficiency of the proposed approach.","","978-1-7281-7709-0","10.1109/ICARCV50220.2020.9305458","National Natural Science Foundation of China(grant numbers:61673280,62073158); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305458","","Heuristic algorithms;Trajectory;Mathematical model;Optimal control;Convergence;Control systems;Performance analysis","continuous time systems;dynamic programming;iterative methods;learning (artificial intelligence);linear quadratic control;linear systems;matrix algebra;nonlinear control systems;optimal control;parameter estimation;performance index;polynomials","unknown model matrices;reference signal;higher-order polynomials;on-policy IRL;off-policy IRL algorithm;optimal control policy;higher-order polynomial signal tracking control;unknown systems;off-policy integral reinforcement learning algorithm;linear quadratic tracking control problem;continuous-time systems;arbitrary higherorder polynomial signal;optimal approach;linear continuous-time system","","","","26","IEEE","8 Jan 2021","","","IEEE","IEEE Conferences"
"A Regionalization Navigation Method Based on Deep Reinforcement Learning","P. Li; X. Ruan; X. Zhu; J. Chai","Faculty of Information Technology, Beijing University of Technology; Faculty of Information Technology, Beijing University of Technology; Faculty of Information Technology, Beijing University of Technology; Faculty of Information Technology, Beijing University of Technology","2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","13 Feb 2020","2019","1","","803","807","Efficient navigation in complex environment is one of research hotspots in the field of robot control. In this paper, for the problems of navigation in distributed environment of a mobile robot, we propose a regionalization navigation method based on deep reinforcement learning. First of all, consider the characteristics of distributed environment, we use independent submodules learn control strategy in different region, and region model is built to integrate strategies to complete navigation in multi-area environment. Then, in order to improve learning efficiency, reward prediction and depth obstacles avoidance are added during training. Experiment result in single-area reveal the improvements of training method is helpful to enhance robot navigation performance. Moreover, by the proposed regionalization navigation studying in multi-area environment, our method shows the advantages in training time and reward that single model does not have, indicate that it can better deal with large-scale navigation.","2381-0947","978-1-7281-1907-6","10.1109/IAEAC47372.2019.8997645","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8997645","deep reinforcement learning;region model;distributed environment;reward prediction;depth obstacles avoidance","Navigation;Training;Robots;Neural networks;Predictive models;Games;Collision avoidance","collision avoidance;learning systems;mobile robots;navigation;path planning","learning efficiency;depth obstacles avoidance;robot navigation performance;multiarea environment;regionalization navigation method;deep reinforcement learning;robot control;distributed environment;mobile robot","","","","16","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Neuromuscular control of sagittal arm during repetitive movement by actor-critic reinforcement learning method","V. Golkhou; C. Lucas; M. Parnianpour","Department of Mechanical Engineering, Sharif University of Technology, Tehran, Iran; Department of Electrical and Computer Engineering, University of Tehran, Tehran, Iran; Department of Mechanical Engineering, Sharif University of Technology, Tehran, Iran","Proceedings World Automation Congress, 2004.","20 Jun 2005","2004","16","","371","376","In this study, we have used a single link system with a pair of muscles that are excited with alpha and gamma signals to achieve an oscillatory movement with variable amplitude and frequency. This paper proposes a reinforcement learning method with an Actor-Critic architecture instead of middle and low level of central nervous system (CNS). The Actor in this structure is a two layer feedforward neural network and the Critic is a model of the cerebellum. The Critic is trained by State-Action-Reward-State-Action (SARSA) method. The system showed excellent tracking capability and after 280 epochs the RMS error for position and velocity profiles were 0.02, 0.04 radian and radian/sec, respectively. ","","1-889335-21-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1438682","motor control;reinforcement learning;Actor-Critic;CMAC;Simulink","Neuromuscular;Muscles;Torque;Control systems;Central nervous system;Biological neural networks;Neural networks;Humans;Learning systems;Delay","","","","","","24","","20 Jun 2005","","","IEEE","IEEE Conferences"
"Intelligent Active Disturbance Rejection Control for Flight Vehicles Based on Deep Reinforcement Learning","H. Huang; Y. Zhu; B. Zheng; D. Zhang; X. Xu","Unmanned System Research Institute, Northwestern Polytechnical University, Xi’an, China; School of Marine Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Astronautics, Northwestern Polytechnical University, Xi’an, China; Shanghai Electro-Mechanical Engineering Institute, Shanghai, China; Shanghai Electro-Mechanical Engineering Institute, Shanghai, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","1549","1554","A control strategy for flight vehicles with input uncertainty and unknow external disturbance based on intelligent active disturbance rejection control (ADRC) is investigated in this paper. Firstly, a longitudinal short period model of flight vehicle with input uncertainty and unknown external disturbances is formulated, then the state feedback controller is proposed for the system based on ADRC. The extended state observer (ESO) is proposed to estimate and compensate the external disturbance and uncertainties. Lyapunov functional method is introduced to demonstrate the stability and robustness of closed-loop system. To improve the transient performance and simplify the design process, deep reinforcement learning is proposed to optimize the parameters of controller and filter. The feasibility of the proposed technique is further specifies based on numerical simulation results.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728383","Nature; Nature; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728383","Active Disturbance Rejection Control;Deep Reinforcement Learning;Extended State Observer;Uncertainties","Robust control;State feedback;Uncertainty;Process control;Reinforcement learning;Numerical simulation;Stability analysis","closed loop systems;control system synthesis;feedback;Lyapunov methods;nonlinear control systems;observers;robust control;stability;state feedback;uncertain systems","external disturbance;intelligent active disturbance rejection control;ADRC;longitudinal short period model;flight vehicle;input uncertainty;unknown external disturbances;state feedback controller;deep reinforcement learning;filter;control strategy","","","","14","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Hybrid control algorithm for humanoid robots walking based on episodic reinforcement learning","D. Katic; A. Rodic; E. Jose Bayro-Corrochanoy","Robotics Laboratory, Mihailo Pupin Institute University of Belgrde, Belgrade, Serbia; Robotics Laboratory, Mihailo Pupin Institute University of Belgrde, Belgrade, Serbia; CINVESTAV, Unidad Guadalajara, Department of Electrical Engineering and Computer Science, Zapopan, Jalisco, Mexico","World Automation Congress 2012","4 Oct 2012","2012","","","1","6","This paper presents a hybrid dynamic control approach to acquire biped walking of humanoid robots focussed on policy gradient episodic reinforcement learning with fuzzy evaluative feedback. The proposed structure of controller involves two feedback loops: conventional computed torque controller and episodic reinforcement learning controller. Reinforcement learning part includes fuzzy information about Zero-Moment Point errors. To demonstrate the effectiveness of our method, simulation tests using middle-size 36 DOFs humanoid robot MEXONE are performed.","2154-4824","978-1-889334-47-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320884","","Learning;Legged locomotion;Heuristic algorithms;Humanoid robots;Joints;Trajectory","","","","","","18","","4 Oct 2012","","","IEEE","IEEE Conferences"
"An End-to-End Deep Reinforcement Learning Method for UAV Autonomous Motion Planning","Y. Cui; X. Dong; D. Li; Z. Tu","Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China","2022 7th International Conference on Robotics and Automation Engineering (ICRAE)","8 Mar 2023","2022","","","100","104","UAVs rely on mapping the surroundings to gather real-time environmental information. The mapping outcome stands as the key prerequisite for further motion planning. Plenty of work has been done with sophisticated mapping algorithms. However, during UAV navigation, these algorithms are expected to be updated continuously, which consumes a significant amount of memory and computational resources in a large area. To address this limitation, in this paper, we propose an end-to-end method for UAV autonomous motion planning via Reinforcement Learning (RL). In particular, a deep RL network is built as the brain of the intelligent agent, which takes the depth image of the UAV visual feedback as input, and outputs the continuous action as a control decision. A convolutional neural network is employed to process the depth image. In order to implement and validate the proposed method, a high-fidelity 3D simulation environment is established in AirSim, which generates the real-time flight status and depth images during the UAV flight. As a result, the flight simulation demonstrates the effectiveness and efficiency of the RL-based motion planning algorithm in a complex environment. Importantly, the agent trained by the proposed IDDPG could get closer to the destination than that trained by DDPG by about 17 meters on average. Last, the computation time for each step is significantly reduced to 5 ms compared to the classical approach.","","978-1-6654-8918-8","10.1109/ICRAE56463.2022.10056204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10056204","Motion Planning;Reinforcement Learning;UAV","Meters;Visualization;Solid modeling;Three-dimensional displays;Navigation;Memory management;Reinforcement learning","autonomous aerial vehicles;deep learning (artificial intelligence);mobile robots;path planning;reinforcement learning;robot vision","deep RL network;depth image;depth images;end-to-end deep reinforcement learning method;end-to-end method;high-fidelity 3D simulation environment;real-time environmental information;real-time flight status;reinforcement learning;RL-based motion;sophisticated mapping algorithms;UAV autonomous motion planning;UAV flight;UAV navigation;UAV visual feedback","","","","15","IEEE","8 Mar 2023","","","IEEE","IEEE Conferences"
"Multi-Time Scale Voltage Control in Electrothermal Integrated Energy System Based on Deep Reinforcement Learning","M. Fu; K. Huang; H. Wang","Smart Energy Division, NARI Technology Co. Ltd, Nanjing, China; College of Electronic Science & Engineering, Southeast University, Nanjing, China; Smart Energy Division, NARI Technology Co. Ltd, Nanjing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","440","445","To solve a voltage safety issue during operation in electrothermal integrated energy systems (ETIES), a multi-time scale voltage control (VC) method is presented in this paper. Firstly, based on the mathematical models of combined heat and power (CHP), photovoltaic (PV) and electric boilers (EB), the target function and various restraints of system voltage safety are established and transformed into a Markov decision process. Secondly, a voltage regulation scheme that prioritizes reactive power compensation over active power reduction is used to enhance the system new power absorption capacity, and a deep reinforcement learning (DRL) based voltage regulation method for PV units and capacitor banks in multiple time scales is proposed. The optimal strategy of VC is obtained by agent training. Simulations are conducted to analyze in terms of reliability and efficiency of the scheme.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055579","Science and Technology Project of State Grid; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055579","integrated energy systems;deep reinforcement learning;voltage control;multiple timescales control","Deep learning;Training;Absorption;Cogeneration;Capacitors;Reinforcement learning;Inverters","boilers;cogeneration;deep learning (artificial intelligence);Markov processes;photovoltaic power systems;power generation control;reactive power;reactive power control;reinforcement learning;voltage control;voltage regulators","active power reduction;deep reinforcement learning based voltage regulation method;electrothermal integrated energy system;multiple time scales;multitime scale voltage control method;reactive power compensation;system voltage safety;voltage regulation scheme;voltage safety issue","","","","13","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Off-Policy Reinforcement Learning for Optimal Preview Tracking Control of Linear Discrete-Time systems with unknown dynamics","C. -R. Wang; H. -N. Wu","The Science and Technology on Aircraft Control Laboratory, Beihang University, Beijing, China; The Science and Technology on Aircraft Control Laboratory, Beihang University, Beijing, China","2018 Chinese Automation Congress (CAC)","24 Jan 2019","2018","","","1402","1407","In this paper., an off-policy reinforcement learning (RL) algorithm is presented to solve the optimal preview tracking control of discrete time systems with unknown dynamics. Firstly., an augmented state-space system that includes the available preview knowledge as a part of the state vector is constructed to cast the preview tracking control problem as a standard linear quadratic regulator (LQR) one. Secondly., the reinforcement learning technique is utilized to solve the algebraic Riccati equation (ARE) using online measurable data without requiring the a priori knowledge of the system matrices. Compared with the existing off-policy RL algorithm., the proposed scheme solves a preview tracking control problem. A numerical simulation example is given to verify the effectiveness of the proposed control scheme.","","978-1-7281-1312-8","10.1109/CAC.2018.8623077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8623077","off-policy reinforcement learning;optimal preview tracking control","Discrete-time systems;Reinforcement learning;Heuristic algorithms;Mathematical model;Symmetric matrices;Optimal control;System dynamics","continuous time systems;control system synthesis;discrete time systems;learning (artificial intelligence);linear quadratic control;linear systems;matrix algebra;predictive control;Riccati equations;state-space methods","optimal preview tracking control;linear discrete-time systems;off-policy reinforcement learning algorithm;augmented state-space system;preview tracking control problem;standard linear quadratic regulator;reinforcement learning technique;system matrices;algebraic Riccati equation","","","","17","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Decentralized reinforcement learning collaborative consensus algorithm for generation dispatch in virtual generation tribe","L. Qing; Z. Xiaoshun; P. Zhenning; T. Min; G. Lexin; Y. Tao; L. Qianjin; F. Yongkun","School of Electric Power, South China University of Technology, Guangzhou, China; School of Electric Power, South China University of Technology, Guangzhou, China; School of Electric Power, South China University of Technology, Guangzhou, China; School of Electric Power, South China University of Technology, Guangzhou, China; School of Electric Power, South China University of Technology, Guangzhou, China; School of Electric Power, South China University of Technology, Guangzhou, China; School of Electric Power, South China University of Technology, Guangzhou, China; Hunan Electric Power Transmission Construction Company, Changsha, China","2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","10 Mar 2016","2015","","","1197","1201","The article proposes a distributed reinforcement learning collaborative consensus algorithm for dynamic generation command dispatch of AGC in interconnected power grids under the framework of the virtual power generation tribes, in order to in response to the development of the EMS system in the Smart Grid from centralization to decentralized form. The simulation results of the Guangdong Grid show that: the algorithm can not only enhance the adaptive and dynamic performance of the system but also can reduce the adjustment cost as well as realizing the optimal allocation of automatic generation control.","","978-1-4799-1980-2","10.1109/IAEAC.2015.7428749","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7428749","virtual power tribes;collaborative reinforcement learning;stochastic optimization;AGC;decentralized autonomous","Power systems;Collaboration;Manganese;Learning (artificial intelligence);Optimization;Resource management;Companies","decentralised control;energy management systems;learning (artificial intelligence);power generation control;power generation dispatch;smart power grids","automatic generation control optimal allocation;Guangdong smart grid;EMS system;virtual power generation tribe;power grid interconnection;AGC;dynamic generation command dispatch;distributed reinforcement learning collaborative consensus algorithm;generation dispatch;decentralized reinforcement learning collaborative consensus algorithm","","","","7","IEEE","10 Mar 2016","","","IEEE","IEEE Conferences"
"Research on big data anomaly mining method for power grid operation and maintenance based on reinforcement learning algorithm","X. Wen","CSG EHV Power Transmission Company, Guangzhou, Guangdong, China","2022 9th International Forum on Electrical Engineering and Automation (IFEEA)","10 Feb 2023","2022","","","1055","1059","As the scale of development of power grids continues to expand, the issue of their safe operation has received much attention. The problem of low accuracy in the face of network attacks exists in the big data anomaly mining method of grid operation and maintenance. A big data anomaly mining method of grid operation and maintenance based on reinforcement learning algorithm is designed. The method is based on a reinforcement learning algorithm, which is used to obtain early warning data, develop an offline analysis function, calculate an increase matrix of key nodes, construct a big data anomaly identification model and optimise the mining process. Experimental results: The accuracy of the designed mining method is 11.92%, 14.57% and 13.02% higher than the average of the other three methods.","","978-1-6654-6421-5","10.1109/IFEEA57288.2022.10037865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10037865","Reinforcement learning algorithms;Grid operations and maintenance;Big data;anomaly mining;Grid security;Data objects","Electrical engineering;Sensitivity;Reinforcement learning;Big Data;Maintenance engineering;Power grids;Data models","Big Data;data mining;power grids","big data anomaly identification model;big data anomaly mining method;designed mining method;maintenance;optimise;power grid operation;power grids;reinforcement learning algorithm","","","","16","IEEE","10 Feb 2023","","","IEEE","IEEE Conferences"
"Neural Network and Reinforcement Learning based Energy Management Strategy for Battery/Supercapacitor HEV","J. Tao; G. Chen; R. Gao","School of Information Science and Engineering, NingboTech University, Ningbo, China; Zhejiang CRRC Electric Vehicle CO., LTD, Ningbo, China; School of Control Science and Engineering, Zhejiang University, Ningbo, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","5623","5628","A novel energy management strategy based on neural network (NN) and reinforcement learning is proposed to split the power of lithium battery and supercapacitor for hybrid energy vehicle (HEV) driving. First, the NN state-action model is trained by using traditional energy management controller. Then, new data is generated and combined with the original data to optimize the NN model in further. The action sequences are generated randomly and evaluated by NN model. Only the first action is adopted and the next action is re-predicted to reduce the cumulative error caused by the inaccurate model. Finally, four different driving patterns are selected and the comparison with deep Q-Network method is carried out to verify the effectiveness of the proposed energy management strategy.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728529","Energy management strategy;neural network;reinforcement learning","Training;Vehicle driving;Artificial neural networks;Reinforcement learning;Supercapacitors;Predictive models;Lithium batteries","battery powered vehicles;energy management systems;hybrid electric vehicles;learning (artificial intelligence);neural nets;secondary cells;supercapacitors","traditional energy management controller;NN model;action sequences;inaccurate model;deep Q-Network method;novel energy management strategy;reinforcement learning;lithium battery;hybrid energy vehicle;NN state-action model","","","","13","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"A Computational Offloading Strategy Based on Deep Reinforcement Learning in Small cell networks","Z. Li; W. Su; X. Li","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China","2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","5 Apr 2021","2021","5","","2328","2332","With the rapid development of communication technology in recent years, computing-intensive services such as virtual reality and face recognition are rapidly popularized. These services not only bring challenges to resource-constrained networks, but also cause a large amount of energy consumption and high delay on mobile devices with limited computing power and battery capacity. Mobile edge computing technology and small cell networks are the research hotspots in 5G era. Combining these two technologies can enable the network to have stronger coverage to provide users with more powerful computing power. In this paper, we propose a computing offloading algorithm model based on deep reinforcement learning in small cell networks. Considering the battery power of mobile devices and dynamic network bandwidth, the proposed algorithm aims to complete computing offload with a smaller overall overhead. The simulation shows that the performance of the proposed computational offloading algorithm is better than the comparative algorithm.","2689-6621","978-1-7281-8028-1","10.1109/IAEAC50856.2021.9391114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9391114","small cell networks;mobile edge computing;computing offloading;deep reinforcement learning","Energy consumption;Solid modeling;Heuristic algorithms;Reinforcement learning;Microcell networks;Mobile handsets;Delays","cloud computing;deep learning (artificial intelligence);mobile computing;power aware computing;resource allocation;virtual reality","computational offloading strategy;deep reinforcement learning;communication technology;computing-intensive services;virtual reality;face recognition;resource-constrained networks;mobile devices;battery capacity;mobile edge;battery power;dynamic network bandwidth;mobile edge computing technology","","","","7","IEEE","5 Apr 2021","","","IEEE","IEEE Conferences"
"Disturbance-Observer based Reinforcement Learning for Overhead Crane Systems","T. T. Bui; T. T. Cao; T. H. Nguyen; D. H. Le; H. H. Dao; P. N. Dao","Hanoi University of Science and Technology, Hanoi, Vietnam; Hanoi University of Science and Technology, Hanoi, Vietnam; Hanoi University of Science and Technology, Hanoi, Vietnam; Hanoi University of Science and Technology, Hanoi, Vietnam; Hanoi University of Science and Technology, Hanoi, Vietnam; Hanoi University of Science and Technology, Hanoi, Vietnam","2022 11th International Conference on Control, Automation and Information Sciences (ICCAIS)","30 Dec 2022","2022","","","19","23","In this work, a disturbance-observer based reinforcement learning control scheme is presented for the overhead crane system. First, the approximate/adaptive dynamic programming (ADP) method is applied to obtain the solution of a discounted optimal control problem. Here, we use only one neural network as a critic network. The weights of this network are updated iteratively using a novel updating rule law. A disturbance-observer is then designed to compensate the effect of the unknown input disturbance, therefore improve the robustness of the system. The convergence of each module as well as the stability of the whole closed-loop system is guaranteed by proving rigorously. Finally, numerical simulations are given to illustrate the effectiveness of the proposed method.","2475-7896","978-1-6654-5248-9","10.1109/ICCAIS56082.2022.9990215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9990215","Reinforcement Learning (RL);Overhead Crane;Lyapunov Stability Theory;Disturbance Observer (DO);Approximate/Adaptive dynamic programming (ADP)","Cranes;Simulation;Optimal control;Reinforcement learning;Observers;Numerical simulation;Stability analysis","closed loop systems;cranes;dynamic programming;learning systems;neurocontrollers;numerical analysis;observers;optimal control;reinforcement learning;stability","adaptive dynamic programming;ADP;approximate dynamic programming;closed-loop system;critic network;discounted optimal control problem;disturbance-observer based reinforcement learning control scheme;neural network;numerical simulations;overhead crane system;stability;unknown input disturbance;updating rule law","","","","7","IEEE","30 Dec 2022","","","IEEE","IEEE Conferences"
"Photovoltaic and energy storage control of partially observable distribution network based on deep reinforcement learning","Q. Bu; P. Lv; K. Zhang; X. Dou; F. Luo; X. Zhou","Economic Research Institution, State Grid Jiangsu Electric Power Co, Nanjing, China; Economic Research Institution, State Grid Jiangsu Electric Power Co, Nanjing, China; School of Electrical Engineering, Southeast University, Nanjing, China; School of Electrical Engineering, Southeast University, Nanjing, China; Economic Research Institution, State Grid Jiangsu Electric Power Co, Nanjing, China; ZIT (Nanjing) Techonology Co, Nanjing, China","2022 12th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","17 Oct 2022","2022","","","871","875","After a large number of distributed power sources are connected to the distribution network, the volatility and uncertainty brought by them may lead to the over-limit of the distribution network voltage and the increase of network losses; at the same time, the distribution network itself is also in a partially observable state. In view of these problems, photovoltaic and energy storage are selected as the control objects. In this paper, a photovoltaic energy storage linkage control technology based on deep reinforcement learning is designed, and an example is used to verify the feasibility and effectiveness of the method proposed in this paper.","2642-6633","978-1-6654-7267-8","10.1109/CYBER55403.2022.9907595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9907595","","Photovoltaic systems;Training;Couplings;Uncertainty;Distribution networks;Reinforcement learning;Real-time systems","distributed power generation;distribution networks;energy storage;learning (artificial intelligence);photovoltaic power systems;power distribution control;power distribution faults","network losses;partially observable state;control objects;photovoltaic energy storage linkage control technology;deep reinforcement learning;photovoltaic energy storage control;partially observable distribution network;distributed power sources;volatility;uncertainty;distribution network voltage","","","","15","IEEE","17 Oct 2022","","","IEEE","IEEE Conferences"
"A Resource Allocation Method of Deep Reinforcement Transfer Learning in WSN","D. Zhang; X. Li; Y. Shi","College of Computer and Information Engineering Bengbu University, Bengbu, Anhui, China; College of Computer and Information Engineering Bengbu University, Bengbu, Anhui, China; College of Computer and Information Engineering Bengbu University, Bengbu, Anhui, China","2021 International Conference on Computer Network, Electronic and Automation (ICCNEA)","19 Nov 2021","2021","","","220","224","In WSN environment, aiming at the problem of multi-objective and multi constraint resource allocation, a resource allocation mathematical model is established to seek the optimal solution. The deep reinforcement transfer learning (DRTL) method is adopted by introducing reliability and the idea of transfer learning. In the process of DRTL forward transmission and backward propagation, the maximum group is selected as the optimal allocation strategy. Finally, in the same environment, we compared with ant colony and discrete particle swarm algorithm, found that it has a significant impact on energy consumption, transmission time and reliability.","","978-1-6654-4486-6","10.1109/ICCNEA53019.2021.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9603801","Wireless Sensor Network;Resource Allocation;Deep Reinforcement Transfer Learning;Reliable Transmission","Training;Deep learning;Wireless communication;Wireless sensor networks;Energy consumption;Simulation;Transfer learning","optimisation;particle swarm optimisation;resource allocation;wireless sensor networks","resource allocation mathematical model;optimal solution;deep reinforcement transfer learning method;DRTL forward transmission;optimal allocation strategy;resource allocation method;WSN environment","","","","12","IEEE","19 Nov 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Adaptive Control for Tumor Reduction","I. Ferreira; J. M. Lemos; R. Cunha","Instituto Superior Técnico Univ., Lisbon, Portugal; INESC-ID, Instituto Superior Técnico Univ., Lisbon, Portugal; Institute for Systems and Robotics, Instituto Superior Técnico Univ., Lisbon, Portugal","2022 International Conference on Control, Automation and Diagnosis (ICCAD)","18 Aug 2022","2022","","","1","6","This work addresses the design of cancer therapy for tumour reduction using adaptive optimal control based on reinforcement learning. The approach proposed consists of defining a decreasing reference trajectory for the tumour size, that drives it to zero with a convenient rate, together with a regulation algorithm that adjusts the drug dose so that the tumor size tracks this reference. The motivation to use adaptive methods stems from the high variability of biomedical dynamics, both inter and intra-patient, together with the aim of providing the regulation controller with the ability to tune to the optimal solution when the tumor size decreases. The adaptation mechanism uses Q-learning and a quadratic cost, resulting in a model-free linear quadratic controller. Directional forgetting recursive least squares is used to estimate the coefficients of the quality function. Simulation results, with a logistic tumor model that incorporates the the effect of immunotherapy are presented.","2767-9896","978-1-6654-9794-7","10.1109/ICCAD55197.2022.9854034","Fundação para a Ciência e a Tecnologia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9854034","adaptive control;reinforcement learning;Q-learning;linear quadratic;cancer therapy","Drugs;Adaptation models;Q-learning;Medical treatment;Approximation algorithms;Regulation;Adaptive control","adaptive control;cancer;drugs;least squares approximations;linear quadratic control;medical control systems;neurocontrollers;reinforcement learning;tumours","intra-patient;regulation controller;optimal solution;Q-learning;model-free linear quadratic controller;logistic tumor model;reinforcement learning;tumor reduction;cancer therapy;adaptive optimal control;drug dose;biomedical dynamics","","","","23","IEEE","18 Aug 2022","","","IEEE","IEEE Conferences"
"Electric Vehicle Regulation Technology Based on Deep Reinforcement Learning","Z. Ou; X. Xiao; H. Yu; J. Yuan; X. Miao; D. Huang","Nantong Power Supply Branch, State Grid Jiangsu Electric Power Co., Ltd., Nantong, China; State Grid Jiangsu Electric Power Co., Ltd. Electric Power Research Institute, Nanjing, China; School of Electrical Engineering, Southeast University, Nanjing, China; Nantong Power Supply Branch, State Grid Jiangsu Electric Power Co., Ltd., Nantong, China; Nantong Power Supply Branch, State Grid Jiangsu Electric Power Co., Ltd., Nantong, China; Nantong Power Supply Branch, State Grid Jiangsu Electric Power Co., Ltd., Nantong, China","2022 12th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","17 Oct 2022","2022","","","755","758","The rapid development of electric vehicles(EV) has brought certain challenges to the stable operation of the power grid, but at the same time, EV can also be used as a distributed power supply network. This paper proposes an EV regulation method based on deep reinforcement learning(DRL). Through the management of charging and discharging of EV, while ensuring the stability of the voltage level of the distribution network, the network loss of the distribution network can be reduced. This method does not depend on the actual physical model and is more in line with the actual situation of the distribution network. Finally, the feasibility of the method is verified by simulation.","2642-6633","978-1-6654-7267-8","10.1109/CYBER55403.2022.9907128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9907128","","Power supplies;Distribution networks;Reinforcement learning;Power system stability;Electric vehicles;Current distribution;Regulation","deep learning (artificial intelligence);distribution networks;electric vehicles;losses;power engineering computing;power grids;reinforcement learning","electric vehicle regulation technology;power grid;distributed power supply network;EV regulation method;distribution network;network loss;deep reinforcement learning;DRL;actual physical model","","","","15","IEEE","17 Oct 2022","","","IEEE","IEEE Conferences"
"Optimal detection task allocation: A reinforcement learning approach","Q. Huang; Q. Bu; Z. Qin","China Electronics Technology Group Corp, Beijing, Beijing, CN; China Electronics Technology Group Corp, Beijing, Beijing, CN; China Electronics Technology Group Corp, Beijing, Beijing, CN","2017 Chinese Automation Congress (CAC)","1 Jan 2018","2017","","","369","374","Detection task allocation plays a significant role in many military applications. A better detection task allocation strategy can improve the operational efficiency of the military actions, such as improving tracking accuracy and attacking efficiency. However, the trajectory of the target is usually random which makes the allocation difficult to solve. In this paper, we consider the optimal detection task allocation (ODTA) problem to assign the detection task for each detection equipment in order to increase the overall detection efficiency of the detection system. We make the following contributions. First, this ODTA problem is formulated as a distributed Markov decision process (MDP) model. Each detection equipment only requires to know partial information of the other detection equipments which can save the computation burden. Second, a reinforcement learning approach combined with back-propagation network is proposed to solve this model in order to derive the optimal allocation strategy. Third, the numerical testing results demonstrate the proposed method can improve the overall detection efficiency of the system.","","978-1-5386-3524-7","10.1109/CAC.2017.8242794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8242794","Task allocation;optimization;Markov decision process;reinforcement learning","Resource management;Trajectory;Uncertainty;Markov processes;Learning (artificial intelligence);Linear programming","learning (artificial intelligence);Markov processes;military computing","optimal detection task allocation problem;distributed Markov decision process model;reinforcement learning approach;optimal allocation strategy;military applications;operational efficiency;ODTA problem;back-propagation network","","","","21","IEEE","1 Jan 2018","","","IEEE","IEEE Conferences"
"Adaptive Decision Fusion by Simple Reinforcement Learning","N. Mansouri; H. TabatabaeiYazdi","Electrical Engineering Department, Ferdwosi University (Mashhhad), Mashhad, Iran; Electrical Engineering Department, Ferdwosi University (Mashhhad), Mashhad, Iran","2003 4th International Conference on Control and Automation Proceedings","21 Feb 2006","2003","","","742","746","In the problem of optimal fusing decisions, the probability of detection (PD) and the probability of false alarm (PF) for each detector must be known, but this information is not always available practically. In this paper we presented an adaptive fusion model which estimates the PDand PFadaptively by a simple counting. Reference signals are not given, so the fused decision of all detectors is considered as the reference signal, the decision of a local detector is arbitrated by this fusion result","","0-7803-7777-X","10.1109/ICCA.2003.1595121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1595121","","Learning;Detectors","","","","","","9","IEEE","21 Feb 2006","","","IEEE","IEEE Conferences"
"Linear Optimal Control of Miniature Munition Based on Reinforcement Learning","J. Fangfan; X. Zhang","Beijing Key Laboratory of High Dynamic Navigation Technology, Beijing Information Science and Technology University, Beijing, China; Beijing Key Laboratory of High Dynamic Navigation Technology, Beijing Information Science and Technology University, Beijing, China","2018 IEEE International Conference on Information and Automation (ICIA)","26 Aug 2019","2018","","","506","510","An algorithm based on reinforcement learning is used to find the optimal regulator of miniature munitions control systems with partial knowledge of system dynamics. Firstly, a linear control model was established by analyzing the longitudinal dynamics of miniature munitions. Then, based on policy iteration technique, the optimal solution is found by alternating between the policy evaluation and policy update steps. Finally, the simulation results show the effectiveness of the proposed approach.","","978-1-5386-8069-8","10.1109/ICInfA.2018.8812402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812402","miniature munition;policy iteration;algebraic Riccati equation","Weapons;Mathematical model;Vehicle dynamics;Symmetric matrices;Optimal control;Cost function;Reinforcement learning","control engineering computing;iterative methods;learning (artificial intelligence);linear systems;military computing;military systems;optimal control;weapons","miniature munition;reinforcement learning;optimal regulator;partial knowledge;system dynamics;linear control model;longitudinal dynamics;policy iteration technique;optimal solution;policy evaluation;policy update steps;linear optimal control","","","","17","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"Train rescheduling method based on multi-agent reinforcement learning","Y. Cao; Z. Xu; M. Mei","School of Electronic and Information Engineering, Tongji University, Shanghai, China; School of Electronic and Information Engineering, Tongji University, Shanghai, China; School of Electronic and Information Engineering, Tongji University, Shanghai, China","2022 IEEE 6th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC )","9 Nov 2022","2022","","","301","305","Many multi-agent pathfinding algorithms have been raised to arrange trains' scheduling effectively and have reached excellent results. However, these algorithms usually focus on the fixed schedule and have a poor ability to deal with dynamic problems. This paper presents a train rescheduling method based on multi-agent reinforcement learning. A new observation is adopted for trains to better interact with the environment and other trains. The improved DQN network is implemented to train to obtain the best performance, such as avoiding conflicts, han-dling trains' breakdowns and generating new paths. According to simulation results, the model achieved an aggregate completion rate of over 70% of ten agents after training. Compared with the traditional multi-agent pathfinding algorithm CBS, this method was 20% higher in terms of completion rate when the malfunction rate was over 20%. Conclusively, the method has better handled unexpected situations and has excellent adaptability to problems such as sudden train breakdowns.","2689-6621","978-1-6654-5864-1","10.1109/IAEAC54830.2022.9929607","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9929607","multi-agent reinforcement learning;vehicle rescheduling;deep Q-learning;dynamic schedule","Training;Schedules;Adaptation models;Electric breakdown;Heuristic algorithms;Simulation;Reinforcement learning","deep learning (artificial intelligence);multi-agent systems;railways;reinforcement learning;scheduling","sudden train breakdowns;train rescheduling method;multiagent reinforcement learning;multiagent pathfinding algorithms;CBS;DQN network","","","","10","IEEE","9 Nov 2022","","","IEEE","IEEE Conferences"
"Reinforcement learning for natural gas pipeline pressure control","Z. Yang; J. Li; X. Lang","School of Information and Control Engineering, Liaoning Petrochemical University, Liaoning, P.R, China; School of Information and Control Engineering, Liaoning Petrochemical University, Liaoning, P.R, China; School of Information and Control Engineering, Liaoning Petrochemical University, Liaoning, P.R, China","2022 IEEE 17th International Conference on Control & Automation (ICCA)","25 Jul 2022","2022","","","790","794","In this research, an off-policy Q-learning algorithm is designed to solve the pressure tracking problem in gas pipelines by adjusting the rotating speed using only the observed data along the system trajectories. How to determine the most appropriate rotational speed by off-policy Q-learning and enforce the pipeline pressure to follow a desired value is very challenging due to nonlinear and unknown dynamics of gas pipeline pressure systems, as well as the requirement of variation of rotational speed. To this end, an optimal control problem of gas pipeline pressure tracking control is first formulated, then an off-policy Q-function based on iterative Bellman equation which is followed by an off-policy Q-learning algorithm used for searching the optimal rotational speed based on reinforcement learning technique and dynamic programming theory, such that the pressure can follow the desired value successfully. The simulation results are shown to validate the effectiveness of the proposed strategy.","1948-3457","978-1-6654-9572-1","10.1109/ICCA54724.2022.9831904","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831904","","Q-learning;System dynamics;Simulation;Pipelines;Velocity control;Search problems;Mathematical models","dynamic programming;learning (artificial intelligence);optimal control;pipelines;pressure control;stochastic systems","natural gas pipeline pressure control;off-policy Q-learning algorithm;pressure tracking problem;gas pipelines;rotating speed;system trajectories;appropriate rotational speed;desired value;nonlinear dynamics;unknown dynamics;gas pipeline pressure systems;optimal control problem;gas pipeline pressure tracking control;off-policy Q-function;optimal rotational speed;reinforcement learning technique","","","","16","IEEE","25 Jul 2022","","","IEEE","IEEE Conferences"
"Reliability deployment of service function chain based on multi-agent reinforcement learning","G. Liu; S. Huang; K. Li","College of Information Science and Technology, Shijiazhuang Tiedao University, Shijiazhuang, China; College of Information Science and Technology, Shijiazhuang Tiedao University, Shijiazhuang, China; College of Information Science and Technology, Shijiazhuang Tiedao University, Shijiazhuang, China","2022 IEEE 6th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC )","9 Nov 2022","2022","","","1574","1578","In order to overcome the shortcomings of traditional networks, Network Function Virtualization (NFV) is widely used. For meeting the diverse service needs of users, the deployment of service function chain based on NFV has become a current research focus. In the actual scenario that the network service may be interrupted due to the failure of the physical server, this paper proposes a noval reliable deployment method of service function chain based on multi-agent reinforcement learning. Firstly, an integer linear programming (ILP) model is established for the reliable deployment problem of service function chain. Secondly, an efficient MADDPG algorithm is designed to solve it. The experimental results show that the proposed algorithm reduces the cost of physical node computing resources and link bandwidth resources, and improves the acceptance rate of network service requests under satisfying reliability constraints with compared algorithm.","2689-6621","978-1-6654-5864-1","10.1109/IAEAC54830.2022.9929650","Natural science foundation of Hebei province(grant numbers:F2017210118); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9929650","service function chain;reliable;multi-agent reinforcement learning;ILP;MADDPG","Costs;Service function chaining;Computational modeling;Reinforcement learning;Bandwidth;Integer linear programming;Network function virtualization","cloud computing;integer programming;learning (artificial intelligence);linear programming;multi-agent systems;telecommunication network reliability;virtual machines;virtualisation","multiagent reinforcement learning;Network Function Virtualization;service function chain;noval reliable deployment method;reliable deployment problem;network service requests;reliability deployment","","","","10","IEEE","9 Nov 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning-SLAM for finding minimum cost path and mapping","N. Arana-Daniel; R. Rosales-Ochoa; C. López-Franco; E. Nuño","Department of Computer Science, CUCEI, University of Guadalajara, Guadalajara, Mexico; Department of Computer Science, CUCEI, University of Guadalajara, Guadalajara, Mexico; Department of Computer Science, CUCEI, University of Guadalajara, Guadalajara, Mexico; Department of Computer Science, CUCEI, University of Guadalajara, Guadalajara, Mexico","World Automation Congress 2012","4 Oct 2012","2012","","","1","6","In this work, we propose the integration of two of the most widely used approaches for the implementation of autonomous navigation systems: the reinforcement learning for path finding, along with SLAM (Simultaneous Localization and Mapping) type algorithms for localization and mapping of the environment. These two approaches are integrated to address the problem of how a robot should explore an unknown and dynamic environment while it collects perception features in order to locate itself and, at the same time, to obtain information clues about cost traversability of an area. So, when a robot is exploring and mapping with a SLAM algorithm it is also learning to associate perception features with costs and actions to find optimal paths from the starting point to the goal point in dynamical environments.","2154-4824","978-1-889334-47-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320898","","Simultaneous localization and mapping;Heuristic algorithms;Navigation;Path planning;Noise","","","","","1","19","","4 Oct 2012","","","IEEE","IEEE Conferences"
"Predictive Control of a Robot Manipulator with Deep Reinforcement Learning","E. Bejar; A. Morán","Engineering Department, Pontifical Catholic University of Peru, Lima, Peru; Engineering Department, Pontifical Catholic University of Peru, Lima, Peru","2021 7th International Conference on Control, Automation and Robotics (ICCAR)","25 Jun 2021","2021","","","127","130","This paper tackles the problem of trajectory following of a two-link rigid robot manipulator. The proposed controller bases its operation on the idea behind preview control in which the control law is divided in two parts: a feedback component that depends only on the present state of the system, and a predictive component that only uses future values of the reference trajectory. In this sense, the designed controller uses for training and control both present and future states of the system. Simulation results when following a test trajectory are presented to validate the proposed method and to show that the proposed controller exhibits better performance with respect to a neurocontroller that does not use a predictive component neither for training nor control.","2251-2454","978-1-6654-4986-1","10.1109/ICCAR52225.2021.9463462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463462","artificial intelligence;reinforcement learning;deep learning;robotics;control systems","Training;Neurocontrollers;Simulation;Reinforcement learning;Manipulators;Prediction algorithms;Trajectory","control engineering computing;control system synthesis;deep learning (artificial intelligence);feedback;manipulators;predictive control;trajectory control","reference trajectory;test trajectory;predictive component;predictive control;deep reinforcement learning;two-link rigid robot manipulator;preview control;feedback component","","","","18","IEEE","25 Jun 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning in Maximum Entropy Framework with Automatic Adjustment of Mixed Temperature Parameters for Path Planning","Y. Chen; F. Ying; X. Li; H. Liu","College of Information Science and Technology, Donghua University, China; College of Information Science and Technology, Donghua University, China; College of Information Science and Technology, Donghua University, China; College of Information Science and Technology, Donghua University, China","2023 7th International Conference on Robotics, Control and Automation (ICRCA)","5 Apr 2023","2023","","","78","82","Deep reinforcement learning in maximum entropy framework is sample-efficient and has a strong exploration capacity, making it effective and favorable to solve problems like path planning. Properly tuning the temperature parameters can improve the performance of policy learning, but manual tuning is inefficient. In this paper, we propose a mixed algorithm named SAC-M which is inspired by adaptive soft actor-critic (A-SAC) and soft actor-critic with automatic entropy (SAC-A). The proposed method achieves automatic adjustment of temperature parameters so that the entropy can vary among different states to control the degree of exploration, reducing the possibility of learning suboptimal policies to some extent. The experimental results illustrate that the proposed SAC-M outperforms A-SAC and SAC-A in path planning tasks in different scenes, especially when A-SAC and SAC-A are mixed in a proper ratio.","","979-8-3503-4578-0","10.1109/ICRCA57894.2023.10087467","Natural Science Foundation of Shanghai(grant numbers:21ZR1401100); Fundamental Research Funds for the Central Universities(grant numbers:2232022G-09); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10087467","deep reinforcement learning;path planning;obstacle avoidance;maximum entropy","Deep learning;Reinforcement learning;Manuals;Entropy;Path planning;Temperature control;Task analysis","deep learning (artificial intelligence);entropy;learning (artificial intelligence);maximum entropy methods;path planning;reinforcement learning","adaptive soft actor-critic;automatic adjustment;automatic entropy;deep reinforcement learning;manual tuning;maximum entropy framework;mixed algorithm;mixed temperature parameters;path planning;policy learning;sample-efficient;strong exploration capacity","","","","15","IEEE","5 Apr 2023","","","IEEE","IEEE Conferences"
"Fuzzy wavelet network with reinforcement learning: Application on underactuated system","I. S. Razo-Zapata; L. E. Ramos-Velasco; J. C. Ramos Fernández; M. A. Espejel-Rivera; J. Waissman-Vilanova","Departamento de Ingeniería Electrica y Computación, Instituto Tecnológico de Monterrey, Monterrey, Mexico; Centro de Investigación en Tecnologías de Informacion y Sistemas, Universidad Autónoma del Estado de Hidalgo, Hidalgo, Mexico; Universidad Politécnica de Pachuca, Hidalgo, Mexico; Universidad la Salle Pachuca, San Agustin, Mexico; Blvd. Encinas esquina con Rosales s/n, Universidad de Sonora, Hermosillo, Sonora, Mexico","World Automation Congress 2012","4 Oct 2012","2012","","","1","6","This paper presents a novel approach of reinforcement learning for continuous systems. The scheme is based in wavelet networks to approximating the continuous space of states. The structure of the wavelet network is dynamically generated accord to the explored regions and trained with a modified Q-Learning algorithm. The wavelet network include a fuzzy inference system which computes the value of the set of possible actions, in order to deal with continuous actions. This novel approach is called adaptive wavelet reinforcement learning control (AWRLC). Simulations of applying the proposed method to underactuated systems are performed to demonstrate the properties of the adaptive wavelet network controller.","2154-4824","978-1-889334-47-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320969","","Neurons;Learning;Control systems;Wavelet transforms;Electronic mail;Adaptive systems;Training","","","","","","22","","4 Oct 2012","","","IEEE","IEEE Conferences"
"Autonomous Surface Vehicle Control Method Using Deep Reinforcement Learning","S. Zhang; R. Yang; Z. Chen; M. Li","College of Engineering, Ocean University of China, Qingdao, China; College of Engineering, Ocean University of China, Qingdao, China; College of Engineering, Ocean University of China, Qingdao, China; College of Engineering, Ocean University of China, Qingdao, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","1664","1668","Autonomous Surface Vehicle (ASV) provides a new platform for marine exploration and environmental monitoring. It is very important for ASV to have learning ability in the unknown environment. Reinforcement learning is a branch of machine learning. ASV can obtain control behaviors and improve adaptability and autonomy through environmental exploration. This paper carries out dynamic modeling on the four-thruster ASV, especially designs a controller using the DDPG (deep deterministic policy gradient) algorithm. Simulation results show that the DDPG controller can control MIMO (multiple-input multiple-output) nonlinear systems. After training, the ASV can perform fixed-point control, sinusoidal trajectory tracking, and the ""reconfigurable"" experiments of multiple ASVs in the absence of water flow or water flow interference. The algorithm proposed in this paper has strong robustness and lays a foundation for the research of cooperative control of multiple ASVs.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327026","National Natural Science Foundation of China; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327026","ASV;DDPG;fixed-point control;trajectory tracking;reconfigurable","","control engineering computing;control system synthesis;cooperative systems;deep learning (artificial intelligence);MIMO systems;mobile robots;multi-robot systems;nonlinear control systems;tracking;trajectory control;unmanned surface vehicles","cooperative control;sinusoidal trajectory tracking;multiple-input multiple-output system;controller design;dynamic modeling;MIMO nonlinear systems;control behaviors;machine learning;unknown environment;learning ability;environmental monitoring;marine exploration;deep reinforcement learning;autonomous surface vehicle control method;water flow;multiple ASV;fixed-point control;DDPG controller;deep deterministic policy gradient;DDPG algorithm;four-thruster ASV;environmental exploration","","","","11","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Visual Surveillance using Deep Reinforcement Learning","K. -H. Choi; J. -E. Ha","Graduate School of Automotive Engineering, Seoul National University of Science and Technology, Seoul, Korea; Department of Mechanical and Automotive Engineering, Seoul National University of Science and Technology, Seoul, Korea","2020 20th International Conference on Control, Automation and Systems (ICCAS)","1 Dec 2020","2020","","","289","291","Visual surveillance aims a robust detection of foreground objects, and traditional algorithms usually use a background model image. A current is compared with the background model image. In this paper, we present a visual surveillance algorithm, which determines the parameters in Vibe using deep reinforcement learning. We apply DQN to determine three parameters in Vibe algorithm. We present a policy model which is composed of encoder and decoder type network. Experimental results shows the feasibility of the presented algorithm.","2642-3901","978-89-93215-20-5","10.23919/ICCAS50221.2020.9268429","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268429","Visual surveillance;Deep learning;GAN;BGS;Segmentation","Training;Visualization;Surveillance;Reinforcement learning;Frequency modulation;Computational modeling;Data models","computer vision;learning (artificial intelligence);neural nets;object detection;video surveillance","visual surveillance algorithm;deep reinforcement learning;Vibe algorithm;background model image;robust foreground object detection;encoder-decoder type network","","","","4","","1 Dec 2020","","","IEEE","IEEE Conferences"
"PI and PID Controller Tuning with Deep Reinforcement Learning","K. B. Trujillo; J. G. Álvarez; E. Cortés","SENA, Cali, Colombia; SENA, Cali, Colombia; SENA, Cali, Colombia","2022 IEEE International Conference on Automation/XXV Congress of the Chilean Association of Automatic Control (ICA-ACCA)","10 Jan 2023","2022","","","1","6","Two controllers, a PI and a PID, are tuned through the twin delayed deep deterministic policy gradient or TD3 algorithm, a reinforcement learning technique that can be applied to systems with continuous action states. For configuring the training environments, two mathematical models are identified, one for a temperature loop and the other for a flow loop, obtaining a first-order model and a FOPDT model, respectively. Furthermore, classical tuning methods are used as a reference to evaluate the performance of controllers tuned by means of an RL agent. The results show that, the algorithm tuning a PI controller with good performance, while the PID controller does not achieve outstanding results compared to traditional methods.","","978-1-6654-9408-3","10.1109/ICA-ACCA56767.2022.10006166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10006166","PID Controller;Deep Reinforcement Learning;TD3;FOPDT;Tuning","Training;Deep learning;PI control;Reinforcement learning;Mathematical models;Tuning","control engineering computing;control system synthesis;deep learning (artificial intelligence);gradient methods;mathematical analysis;multi-agent systems;PI control;reinforcement learning;three-term control","continuous action states;deep reinforcement learning;first-order model;flow loop;FOPDT;mathematical models;performance evaluation;PI controller tuning;PID controller tuning;RL agent;TD3 algorithm;temperature loop;training environments;twin delayed deep deterministic policy gradient","","","","0","IEEE","10 Jan 2023","","","IEEE","IEEE Conferences"
"The measurement of strategy convergence for reinforcement learning in discrete state space","Y. Gao; J. Yin; B. Wang; P. Qu; L. Zhou","Shandong Provincial Key Laboratory of Marine Ecology and Environment & Disaster Prevention and Mitigation North China Sea Branch of The State Oceanic Administration, Qingdao, Shandong, China; Shandong Provincial Key Laboratory of Marine Ecology and Environment & Disaster Prevention and Mitigation North China Sea Branch of The State Oceanic Administration, Qingdao, Shandong, China; Shandong Provincial Key Laboratory of Marine Ecology and Environment & Disaster Prevention and Mitigation North China Sea Branch of The State Oceanic Administration, Qingdao, Shandong, China; Shandong Provincial Key Laboratory of Marine Ecology and Environment & Disaster Prevention and Mitigation North China Sea Branch of The State Oceanic Administration, Qingdao, Shandong, China; Shandong Provincial Key Laboratory of Marine Ecology and Environment & Disaster Prevention and Mitigation North China Sea Branch of The State Oceanic Administration, Qingdao, Shandong, China","2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE)","20 Aug 2012","2012","2","","213","219","The concept of entropy is introduced into reinforcement learning. The definitions of the local strategy entropy and global strategy entropy are proposed. The global strategy entropy is proved to be the quantitative problem independent measurement of the learning progress, i.e. the convergence degree of the strategy. To improve the learning performance, reinforcement learning with self-adaptive learning rate is proposed based on the strategy entropy. The experimental results show that learning based on the local strategy entropy has better learning performance than those with fixed learning rates.","","978-1-4673-0089-6","10.1109/CSAE.2012.6272761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272761","reinforcement learning;strategy entropy;convergence;learning rate","Entropy;Learning;Uncertainty;Convergence;Random variables;Optimization;Markov processes","convergence;entropy;learning (artificial intelligence);performance evaluation;self-adjusting systems","strategy convergence measurement;reinforcement learning;discrete state space;local strategy entropy;global strategy entropy;learning performance improvement;self-adaptive learning rate","","","","13","IEEE","20 Aug 2012","","","IEEE","IEEE Conferences"
"Motion planning for a dynamically-coupled hyper-dynamic manipulator by reinforcement learning","M. Wada; A. Ming; M. Shimojo","Department of Mechanical Engineering and Intelligent Systems, T he University of Electro-Communications, Tokyo, Japan; Department of Mechanical Engineering and Intelligent Systems, T he University of Electro-Communications, Tokyo, Japan; Department of Mechanical Engineering and Intelligent Systems, T he University of Electro-Communications, Tokyo, Japan","2011 IEEE International Conference on Mechatronics and Automation","18 Aug 2011","2011","","","1822","1827","This paper proposes a new motion planning method for a hyper-dynamic robot which aims at realizing the motion control skills exhibited by professional golfers by a dexterous robot. The robot has similar distribution of actuators' capability to that of human, which is compact but requires dynamically-coupled driving for high speed motion. For such a robot with strong coupling and nonlinearity, to realize a motion including multi-boundary conditions, motion planning is very difficult. As a new method for the motion planning, the motion planning method using reinforcement learning is proposed in this paper. Simulation results show the effectiveness of the proposed motion planning method.","2152-744X","978-1-4244-8115-6","10.1109/ICMA.2011.5986256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5986256","Motion planning;dynamically-coupled drive;hyper dynamic manipulation;reinforcement Learning","","control engineering computing;control nonlinearities;dexterous manipulators;learning (artificial intelligence);motion control;path planning;sport","motion planning;dynamically-coupled hyperdynamic manipulator;reinforcement learning;hyperdynamic robot;motion control skill;professional golfer;dexterous robot;dynamically-coupled driving;high speed motion;coupling;nonlinearity","","","","24","IEEE","18 Aug 2011","","","IEEE","IEEE Conferences"
"Task Offloading based on Deep Reinforcement Learning with LSTM for Mobile Edge Computing","B. Xu; J. Chai; D. Liu; Z. Zhuang; Y. Zhao; X. Xu; J. Zhu; J. Qi","School of Internet of Things, Nanjing University of Posts and Telecommunication, Nanjing, China; School of Internet of Things, Nanjing University of Posts and Telecommunication, Nanjing, China; School of Internet of Things, Nanjing University of Posts and Telecommunication, Nanjing, China; School of Internet of Things, Nanjing University of Posts and Telecommunication, Nanjing, China; School of Internet of Things, Nanjing University of Posts and Telecommunication, Nanjing, China; School of Internet of Things, Nanjing University of Posts and Telecommunication, Nanjing, China; School of Internet of Things, Nanjing University of Posts and Telecommunication, Nanjing, China; School of Internet of Things, Nanjing University of Posts and Telecommunication, Nanjing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","4883","4888","Mobile devices may offload computationally intensive tasks to edge servers for processing in Mobile Edge Computing (MEC), thereby improving the quality of experience. Heuristic algorithms are feasible for MEC offloading decisions and resource allocation, but they are not suitable for high real-time MEC systems, ignoring the impact of channel dynamic changes on the computational offloading problem. In this paper, we construct a MEC system in a time-varying fading channel scenario and propose a deep reinforcement learning algorithm based on LSTM (DR-LSTM) to solve the joint optimization problem of task offloading decision and resource allocation. The DR-LSTM is combined with an order-preserving quantization algorithm to generate offloading decision, and a linear relaxation method is used to solve the resource allocation problem. Finally, it is verified through simulations that the DR-LSTM can effectively solve the task offloading and resource allocation problem under this model.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055708","China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055708","Mobile edge computing;Computation offloading;Deep reinforcement learning;LSTM","Deep learning;Quantization (signal);Heuristic algorithms;Computational modeling;Neural networks;Reinforcement learning;Relaxation methods","deep learning (artificial intelligence);edge computing;fading channels;mobile computing;optimisation;reinforcement learning;resource allocation;telecommunication scheduling","channel dynamic changes;computational offloading problem;computationally intensive tasks;deep reinforcement learning algorithm;DR-LSTM;heuristic algorithms;joint optimization problem;MEC offloading decisions;MEC system;Mobile devices;Mobile Edge Computing;order-preserving quantization algorithm;real-time MEC systems;resource allocation problem;task offloading decision;time-varying fading channel scenario","","","","15","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Use of the knowledge which is independence on reward in reinforcement learning","Y. Miyazaki; K. Kurashige","Muroran Institute of Technology, University of Computer Science and Systems Engineering, Hokkaido, Japan; Department of Computer Science & Systems Engineering, Muroran Institute of Technology, Hokkaido, Japan","2009 IEEE International Symposium on Computational Intelligence in Robotics and Automation - (CIRA)","1 Mar 2010","2009","","","114","119","Now, there are some techniques called machine learning, and reinforcement learning is one of the machine learning which often used for actual machine. In this study, we pay attention to the knowledge that does not depend on a reward in reinforcement learning, and we will improve learning efficiency by using it. Furthermore, we aim at letting agent coping with various tasks under environment where agent is put. In this paper, we propose the knowledge that does not depend on a reward, and we show utility by applying it to the problem that a task turns into under same environment.","","978-1-4244-4808-1","10.1109/CIRA.2009.5423225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5423225","","Robots;Machine learning;Humans;Learning systems;Neural networks;Genetic algorithms;Supervised learning;Computer errors;Computer science;Systems engineering and theory","control engineering computing;learning (artificial intelligence);robots","reinforcement learning;machine learning;agent coping","","","","11","IEEE","1 Mar 2010","","","IEEE","IEEE Conferences"
"Autonomous Drone Surveillance in a Known Environment Using Reinforcement Learning","M. Gaoi; X. Xing; D. E. Chang","School of Electrical Engineering, KAIST, Daejeon, Korea; School of Electrical Engineering, KAIST, Daejeon, Korea; School of Electrical Engineering, KAIST, Daejeon, Korea","2022 22nd International Conference on Control, Automation and Systems (ICCAS)","9 Jan 2023","2022","","","846","851","We utilize deep reinforcement learning to develop both single-agent and multi-agent methods that can accomplish autonomous drone surveillance tasks in a known indoor environment in this research. We combine the benefits of both visual and obstacle information to boost efficacy while ensuring low time consumption. And we devise a separate reinforcement learning training and test technique that both enhance training efficiency and ensure task completion. This method also creates a new field for sim-to-real transfer. Our experimental results show that the trained agents can detect all targets at a relatively fast speed while maintaining a high level of security, and the patrol completion rate is more than 98% in both single-agent and multi-agent tasks.","2642-3901","978-89-93215-24-3","10.23919/ICCAS55662.2022.10003796","K2; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003796","Drone Surveillance;Deep Reinforcement Learning;Patrol;Grid Map","Training;Visualization;Surveillance;Reinforcement learning;Sensor systems;Indoor environment;Sensors","autonomous aerial vehicles;collision avoidance;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;surveillance","autonomous drone surveillance tasks;deep reinforcement learning;known environment;known indoor environment;low time consumption;multiagent methods;multiagent tasks;separate reinforcement learning training;single-agent;task completion;test technique;trained agents;visual obstacle information","","","","16","","9 Jan 2023","","","IEEE","IEEE Conferences"
"Research on parameters of reinforcement learning on multi-agent system","Guoyu Zuo; Hongwei Zhang; Guangsheng Han; Jia Li","Beijing University of Technology, Beijing, China; Beijing University of Technology, Beijing, China; Beijing University of Technology, Beijing, China; Beijing University of Technology, Beijing, China","2008 7th World Congress on Intelligent Control and Automation","8 Aug 2008","2008","","","5406","5410","Reinforcement learning(RL) has been applied on the testbed of keepaway, a typical multi-agent system. To get the best performances of RL and figure out what influence the learning speed and the finally results, different values of the important variables had been selected in the emulator. The results intimated the influence of each parameter to the course of learning, and established the base of exploring the better values of parameters. This paper is also good for the researches on the effective solution to the problems of huge state spaces.","","978-1-4244-2113-8","10.1109/WCICA.2008.4594543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4594543","keepaway;multi-agent system;reinforcement learning;Robocup","Learning;Multiagent systems;Presses;Machine learning;Books;Information processing;Artificial intelligence","learning (artificial intelligence);multi-agent systems","reinforcement learning;multiagent system;keepaway","","","","6","IEEE","8 Aug 2008","","","IEEE","IEEE Conferences"
"Circuit Driving of RC Scale Cars using Reinforcement Learning","M. Kwon; Y. Eun","Department of Electrical Engineering and Computer Science, DGIST, Daegu, Korea; Department of Electrical Engineering and Computer Science, DGIST, Daegu, Korea","2022 22nd International Conference on Control, Automation and Systems (ICCAS)","9 Jan 2023","2022","","","217","221","This paper presents control of an RC scale car in a scale circuit using reinforcement learning. Experimental environment has been constructed with 1/27 scale remote controlled car, motion tracking system, and a computer that sends steering and thrust commands to the RC car based on feedback from the motion tracking system. The control consists of two layers. Low-level controller receives a desired velocity vector as a reference and do a basic PI control for thrust and P control for steering. High-level controller is trained by reinforcement learning that receives the car state and outputs the velocity command vector. The state include position, velocity, heading of the RC car, distances to surrounding boundaries of the circuit. The high-level controller takes the form of a recursive neural network, which is trained entirely in virtual environment. The car dynamics in the virtual environment is a bicycle model that includes tire slip force from the literature. With the resulting policy (high-level controller) the RC car successfully completes 10 laps in the actual environment of the circuit without colliding to the boundaries.","2642-3901","978-89-93215-24-3","10.23919/ICCAS55662.2022.10003730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003730","Reinforcement learning;Scale RC car;Circuit driving","Training;Tracking;Neural networks;Virtual environments;Reinforcement learning;Control systems;Trajectory","automobiles;bicycles;feedback;mobile robots;neural nets;PI control;position control;reinforcement learning;telecontrol;tyres;vehicle dynamics;velocity control","bicycle model;car dynamics;circuit driving;high-level controller;low-level controller;motion tracking system;P control;PI control;RC scale car;recursive neural network;reinforcement learning;scale circuit;thrust commands;tire slip force;velocity command vector;velocity vector;virtual environment","","","","7","","9 Jan 2023","","","IEEE","IEEE Conferences"
"Cognitive Optimal-Setting Control of AIoT Industrial Applications With Deep Reinforcement Learning","Y. -H. Lai; T. -C. Wu; C. -F. Lai; L. T. Yang; X. Zhou","Department of Computer Science and Information Engineering, National Taitung University, Taitung, Taiwan; Department of Engineering Science, National Cheng Kung University, Tainan, Taiwan; Department of Engineering Science, National Cheng Kung University, Tainan, Taiwan; Department of Computer Science, St. Francis Xavier University, Antigonish, NS, Canada; Faculty of Data Science, Shiga University, Hikone, Japan","IEEE Transactions on Industrial Informatics","4 Dec 2020","2021","17","3","2116","2123","For industrial applications of the artificial intelligence of things, mechanical control usually affects the overall product output and production schedule. Recently, more and more engineers have applied the deep reinforcement learning method to mechanical control to improve the company's profit. However, the problem of deep reinforcement learning training stage is that overfitting often occurs, which results in accidental control and increases the risk of overcontrol. In order to address this problem, in this article, an expected advantage learning method is proposed for moderating the maximum value of expectation-based deep reinforcement learning for industrial applications. With the tanh softmax policy of the softmax function, we replace the sigmod function with the tanh function as the softmax function activation value. It makes it so that the proposed expectation-based method can successfully decrease the value overfitting in cognitive computing. In the experimental results, the performance of the Deep Q Network algorithm, advantage learning algorithm, and propose expected advantage learning method were evaluated in every episodes with the four criteria: the total score, total step, average score, and highest score. Comparing with the AL algorithm, the total score of the proposed expected advantage learning method is increased by 6% in the same number of trainings. This shows that the action probability distribution of the proposed expected advantage learning method has better performance than the traditional soft-max strategy for the optimal setting control of industrial applications.","1941-0050","","10.1109/TII.2020.2986501","Ministry of Science and Technology of the Republic of China(grant numbers:108-2511-H-143 -001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9072609","Cognitive learning;deep reinforcement learning;expectation-based method;overfitting","Approximation algorithms;Optimization;Machine learning;Informatics;Cognitive systems;Acceleration","Internet of Things;learning (artificial intelligence);neural nets;optimal control;production control;production engineering computing;statistical distributions","expected advantage learning method;cognitive optimal-setting control;AIoT industrial applications;mechanical control;product output;production schedule;deep reinforcement learning training stage;expectation-based deep reinforcement learning;softmax function activation value;deep Q network algorithm;accidental control;tanh softmax policy;sigmod function;tanh function;cognitive computing;AL algorithm;action probability distribution","","21","","29","IEEE","20 Apr 2020","","","IEEE","IEEE Journals"
"Flexible Job-Shop Scheduling via Graph Neural Network and Deep Reinforcement Learning","W. Song; X. Chen; Q. Li; Z. Cao","Institute of Marine Science and Technology, Shandong University, Qingdao, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; Singapore Institute of Manufacturing Technology, Singapore","IEEE Transactions on Industrial Informatics","15 Dec 2022","2023","19","2","1600","1610","Recently, deep reinforcement learning (DRL) has been applied to learn priority dispatching rules (PDRs) for solving complex scheduling problems. However, the existing works face challenges in dealing with flexibility, which allows an operation to be scheduled on one out of multiple machines and is often required in practice. Such one-to-many relationship brings additional complexity in both decision making and state representation. This article considers the well-known flexible job-shop scheduling problem and addresses these issues by proposing a novel DRL method to learn high-quality PDRs end to end. The operation selection and the machine assignment are combined as a composite decision. Moreover, based on a novel heterogeneous graph representation of scheduling states, a heterogeneous-graph-neural-network-based architecture is proposed to capture complex relationships among operations and machines. Experiments show that the proposed method outperforms traditional PDRs and is computationally efficient, even on instances of larger scales and different properties unseen in training.","1941-0050","","10.1109/TII.2022.3189725","National Natural Science Foundation of China(grant numbers:62102228); Natural Science Foundation of Shandong Province(grant numbers:ZR2021QF063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826438","Deep reinforcement learning (DRL);flexible job-shop scheduling;graph neural network (GNN)","Job shop scheduling;Manufacturing;Scheduling;Processor scheduling;Optimal scheduling;Cloud computing;Informatics","decision making;deep learning (artificial intelligence);dispatching;graph theory;job shop scheduling;production engineering computing;reinforcement learning","complex scheduling problems;decision making;deep reinforcement learning;DRL method;flexible job-shop scheduling problem;heterogeneous-graph-neural-network-based architecture;high-quality PDR;multiple machines;operation selection;priority dispatching rules;state representation","","12","","46","IEEE","11 Jul 2022","","","IEEE","IEEE Journals"
"Augmenting Automated Game Testing with Deep Reinforcement Learning","J. Bergdahl; C. Gordillo; K. Tollmar; L. Gisslén","SEED - Electronic Arts (EA), Stockholm, Sweden; SEED - Electronic Arts (EA), Stockholm, Sweden; SEED - Electronic Arts (EA), Stockholm, Sweden; SEED - Electronic Arts (EA), Stockholm, Sweden","2020 IEEE Conference on Games (CoG)","20 Oct 2020","2020","","","600","603","General game testing relies on the use of human play testers, play test scripting, and prior knowledge of areas of interest to produce relevant test data. Using deep reinforcement learning (DRL), we introduce a self-learning mechanism to the game testing framework. With DRL, the framework is capable of exploring and/or exploiting the game mechanics based on a user-defined, reinforcing reward signal. As a result, test coverage is increased and unintended game play mechanics, exploits and bugs are discovered in a multitude of game types. In this paper, we show that DRL can be used to increase test coverage, find exploits, test map difficulty, and to detect common problems that arise in the testing of first-person shooter (FPS) games.","2325-4289","978-1-7281-4533-4","10.1109/CoG47356.2020.9231552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9231552","machine learning;game testing;automation;computer games;reinforcement learning","Games;Navigation;Testing;Training;Computer bugs;Heuristic algorithms;Task analysis","computer games;learning (artificial intelligence)","human play testers;play test scripting;relevant test data;deep reinforcement learning;DRL;self-learning mechanism;game testing framework;game mechanics;test coverage;unintended game play mechanics;game types;test map difficulty;first-person shooter games;augmenting automated game testing;general game testing","","11","","12","IEEE","20 Oct 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Residential HVAC Control with Consideration of Human Occupancy","E. McKee; Y. Du; F. Li; J. Munk; T. Johnston; K. Kurte; O. Kotevska; K. Amasyali; H. Zandi","The University of Tennessee, Knoxville, Tennessee, USA; The University of Tennessee, Knoxville, Tennessee, USA; The University of Tennessee, Knoxville, Tennessee, USA; Oak Ridge National Laboratory, Oak Ridge, Tennessee, USA; Oak Ridge National Laboratory, Oak Ridge, Tennessee, USA; Oak Ridge National Laboratory, Oak Ridge, Tennessee, USA; Oak Ridge National Laboratory, Oak Ridge, Tennessee, USA; Oak Ridge National Laboratory, Oak Ridge, Tennessee, USA; Oak Ridge National Laboratory, Oak Ridge, Tennessee, USA","2020 IEEE Power & Energy Society General Meeting (PESGM)","16 Dec 2020","2020","","","1","5","The Artificial Intelligence (AI) development described herein uses model-free Deep Reinforcement Learning (DRL) to minimize energy cost during residential heating, ventilation, and air conditioning (HVAC) operation. Building cooling loads and HVAC operation are difficult to accurately model due to complexity, lack of measurements and data, and model specific performance, so online machine learning is used to allow for real-time readjustment in performance. Energy costs for the multi-zone cooling unit shown in this work are minimized by scheduling on/off commands around dynamic prices. By taking advantage of precooling events that take place when the price is low, the agent is able to reduce operational cost without violating user comfort. The DRL controller was tested in simulation where the learner achieved a 43.89% cost reduction when compared to traditional, fixed-setpoint operation. The system is now ready for the next phase of testing in a live, real-time home environment.","1944-9933","978-1-7281-5508-1","10.1109/PESGM41954.2020.9281893","U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9281893","Automation;demand response;machine learning;transactive control;smart grid","HVAC;Atmospheric modeling;Reinforcement learning;Optimal scheduling;Real-time systems;Load modeling;Testing","building management systems;cost reduction;HVAC;learning (artificial intelligence);pricing","residential heating-ventilation-and-air conditioning operation;model-free deep reinforcement learning;AI;artificial intelligence development;residential HVAC control;fixed-setpoint operation;cost reduction;DRL controller;operational cost;multizone cooling unit;energy cost;online machine learning;HVAC operation;building cooling loads","","7","","18","IEEE","16 Dec 2020","","","IEEE","IEEE Conferences"
"Route optimization for autonomous bulldozer by distributed deep reinforcement learning","Y. Osaka; N. Odajima; Y. Uchimura","Shibaura Institute of Technology,Department of Mechanical Engineering,Tokyo,Japan; Shibaura Institute of Technology,Department of Mechanical Engineering,Tokyo,Japan; Shibaura Institute of Technology,Department of Mechanical Engineering,Tokyo,Japan","2021 IEEE International Conference on Mechatronics (ICM)","30 Mar 2021","2021","","","1","6","Since the publication showed DQN based reinforcement learning methods exceeds human's score in Atari 2600 video games, various deep reinforcement learning have bee researched. This paper proposes a method to control bulldozer autonomously by learning the sediment leveling route using PPO that enables distributed deep reinforcement learning. The simulator was originally developed that enables to reproduce the behavior of small and uniform sediment. By incorporating an LSTM that processes the input state as time-series data into the agent network, more than 95% of the sediment in the target area on average was achieved. In addition, the generalization performance for unknown condition was evaluated, by giving unlearned conditions were given as initial setups.","","978-1-7281-4442-9","10.1109/ICM46511.2021.9385686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9385686","Artificial intelligence;Machine Learning;Deep Reinforcement Learning;Machine Automation","Mechatronics;Conferences;Reinforcement learning;Games;Land vehicles;Sediments;Optimization","computer games;earthmoving equipment;learning (artificial intelligence)","route optimization;autonomous bulldozer;distributed deep reinforcement learning;DQN based reinforcement learning methods;Atari 2600 video games;sediment leveling route","","2","","14","IEEE","30 Mar 2021","","","IEEE","IEEE Conferences"
"RoSE: Robust Analog Circuit Parameter Optimization with Sampling-Efficient Reinforcement Learning","J. Gao; W. Cao; X. Zhang","Department of Electrical and Systems Engineering, Washington University, St. Louis, MO, USA; Department of Electrical and Systems Engineering, Washington University, St. Louis, MO, USA; Department of Electrical and Systems Engineering, Washington University, St. Louis, MO, USA","2023 60th ACM/IEEE Design Automation Conference (DAC)","15 Sep 2023","2023","","","1","6","Design automation of analog circuits has been a long-standing challenge in the integrated circuit field. Recently, multiple methods based on learning or optimization have demonstrated great promise in automating device sizing for analog circuits. However, they often ignore the strong susceptibility of analog circuits to process, voltage, and temperature (PVT) variations or suffer from low sampling efficiency to train algorithms. To address these critical limitations, this paper proposes RoSE, the first Robust analog circuit parameter optimization framework with high Sampling Efficience by synergistically combining Bayesian Optimization (BO) and reinforcement learning (RL). Its core is to use the fast convergence of BO to find an optimized starting point for the backbone RL agent to notably improve its sampling efficiency during the learning process. With this pre-optimization, we further leverage the RL’s superior optimization ability to achieve robust device sizing by incorporating sufficient features of PVT variations into the representation learning loop. Experimental results of our proposed method on exemplary circuits show 3.25×∼16× improvement of sampling efficiency and 6.8× ∼ 24× improvement of figure-of-merit (FoM, defined with design efficiency and design accuracy) as compared to prior methods.","","979-8-3503-2348-1","10.1109/DAC56929.2023.10247991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247991","","Training;Representation learning;Performance evaluation;Integrated circuits;Design automation;Reinforcement learning;Voltage","analogue circuits;Bayes methods;optimisation;reinforcement learning","automating device;Bayesian optimization;design accuracy;design automation;design efficiency;figure-of-merit;integrated circuit field;pre-optimization;process-voltage-temperature;PVT variations;representation learning loop;RLs superior optimization ability;robust analog circuit parameter optimization;RoSE;sampling-efficient reinforcement learning","","","","18","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
