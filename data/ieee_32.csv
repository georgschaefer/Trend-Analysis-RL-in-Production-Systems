"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Exposing Surveillance Detection Routes via Reinforcement Learning, Attack Graphs, and Cyber Terrain","L. Huang; T. Cody; C. Redino; A. Rahman; A. Kakkar; D. Kushwaha; C. Wang; R. Clark; D. Radke; P. Beling; E. Bowen","National Security Institute, Virginia Polytechnic University; National Security Institute, Virginia Polytechnic University; Deloitte & Touche LLP; Deloitte & Touche LLP; Deloitte & Touche LLP; Deloitte & Touche LLP; Deloitte & Touche LLP; Deloitte & Touche LLP; Deloitte & Touche LLP; National Security Institute, Virginia Polytechnic University; Deloitte & Touche LLP","2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)","23 Mar 2023","2022","","","1350","1357","Reinforcement learning (RL) operating on attack graphs leveraging cyber terrain principles are used to develop reward and state associated with determination of surveillance detection routes (SDR). This work extends previous efforts on developing RL methods for path analysis within enterprise networks. This work focuses on building SDR where the routes focus on exploring the network services while trying to evade risk. RL is utilized to support the development of these routes by building a reward mechanism that would help in realization of these paths. The RL algorithm is modified to have a novel warm-up phase which decides in the initial exploration which areas of the network are safe to explore based on the rewards and penalty scale factor.","","978-1-6654-6283-9","10.1109/ICMLA55696.2022.00282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10069285","attack graphs;reinforcement learning;surveillance detection routes;SDR;cyber terrain","Sensitivity;Surveillance;Buildings;MIMICs;Reinforcement learning;Production;Data models","computer network security;cyber-physical systems;graph theory;learning (artificial intelligence);reinforcement learning","attack graphs;cyber terrain principles;enterprise networks;network services;path analysis;reinforcement learning;reward mechanism;RL algorithm;RL methods;routes focus;SDR;surveillance detection routes","","2","","23","IEEE","23 Mar 2023","","","IEEE","IEEE Conferences"
"ReLeaSER: A Reinforcement Learning Strategy for Optimizing Utilization Of Ephemeral Cloud Resources","M. Handaoui; J. -E. Dartois; J. Boukhobza; O. Barais; L. d'Orazio","Univ Brest, Lab-STICC, CNRS, UMR 6285, Brest, France; Univ. Rennes, Inria, CNRS, IRISA; Univ Brest, Lab-STICC, CNRS, UMR 6285, Brest, France; Univ. Rennes, Inria, CNRS, IRISA; Univ. Rennes, Inria, CNRS, IRISA","2020 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)","26 Apr 2021","2020","","","65","73","Cloud data center capacities are over-provisioned to handle demand peaks and hardware failures which leads to low resources' utilization. One way to improve resource utilization and thus reduce the total cost of ownership is to offer unused resources (referred to as ephemeral resources) at a lower price. However, reselling resources needs to meet the expectations of its customers in terms of Quality of Service. The goal is so to maximize the amount of reclaimed resources while avoiding SLA penalties. To achieve that, cloud providers have to estimate their future utilization to provide availability guarantees. The prediction should consider a safety margin for resources to react to unpredictable workloads. The challenge is to find the safety margin that provides the best trade-off between the amount of resources to reclaim and the risk of SLA violations. Most state-of-the-art solutions consider a fixed safety margin for all types of metrics (e.g., CPU, RAM). However, a unique fixed margin does not consider various workloads variations over time which may lead to SLA violations or/and poor utilization. In order to tackle these challenges, we propose ReLeaSER, a Reinforcement Learning strategy for optimizing the ephemeral resources' utilization in the cloud. ReLeaSER dynamically tunes the safety margin at the host-level for each resource metric. The strategy learns from past prediction errors (that caused SLA violations). Our solution reduces significantly the SLA violation penalties on average by 2.7× and up to 3.4×. It also improves considerably the CPs' potential savings by 27.6% on average and up to 43.6%.","2330-2186","978-1-6654-0388-7","10.1109/CloudCom49646.2020.00009","French government(grant numbers:ANR-A0-AIRT-07); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407327","Cloud;Ephemeral Resources;Resource Optimization;SLA;Safety Margin;Reinforcement Learning","Measurement;Cloud computing;Stochastic processes;Random access memory;Reinforcement learning;Quality of service;Production","cloud computing;computer centres;contracts;learning (artificial intelligence);resource allocation","SLA violations;fixed safety margin;ReLeaSER;reinforcement learning strategy;resource metric;SLA violation penalties;ephemeral cloud resources;cloud data center capacities;resource utilization;cloud providers","","1","","30","IEEE","26 Apr 2021","","","IEEE","IEEE Conferences"
"Swarm Reinforcement Learning using DEEPSO-Q with Advantage for Operational Planning of Energy Plants","K. Takahashi; Y. Fukuyama","Meiji University, Tokyo, Japan; Meiji University, Tokyo, Japan","2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)","16 Apr 2020","2020","","","398","403","This paper proposes a new swarm reinforcement learning technique and its application to operational planning of energy plants (OPEPs) in buildings and factories. The proposed technique is differential evolutionary particle swarm optimization based q-learning with advantage (DEEPSO-Q w/A). The proposed DEEPSO-Q w/A based method can reduce engineering man-hour to develop the planning system and operational costs of the system. The results of the proposed method are compared with those of the conventional PSO-Q and DEEPSO-Q based methods. It is verified that the proposed method can reduce energy costs the most among the three applied methods.","","978-1-7281-4985-1","10.1109/ICAIIC48513.2020.9065286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065286","Swarm Reinforcement Learning;DEEPSO-Q with Advantage;Operational Planning of Energy Plants","Heating systems;Buildings;Production facilities;Learning (artificial intelligence);Generators;Planning;Power systems","evolutionary computation;learning (artificial intelligence);particle swarm optimisation;planning (artificial intelligence);power engineering computing;power plants","planning system;DEEPSO-Q based methods;swarm reinforcement learning technique;operational planning of energy plants;OPEPs;differential evolutionary particle swarm optimization based q-learning","","1","","13","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning Based Coded Computation for Mobile Ad Hoc Computing","B. Wang; J. Xie; K. Lu; Y. Wan; S. Fu","Department of Electrical and Computer Engineering, University of California, San Diego; Department of Electrical and Computer Engineering, San Diego State University, San Diego, CA; Department of Computer Science and Engineering, University of Puerto Rico at Mayagüez, Mayagüez, Puerto Rico; Department of Electrical Engineering, University of Texas at Arlington, Arlington, Texas; Department of Electrical Engineering, University of North Texas, Denton, Texas","ICC 2021 - IEEE International Conference on Communications","6 Aug 2021","2021","","","1","6","Mobile ad hoc computing (MAHC), which allows mobile devices to directly share their computing resources, is a promising solution to address the growing demands for computing resources required by mobile devices. However, offloading a computation task from a mobile device to other mobile devices is a challenging task due to frequent topology changes and link failures because of node mobility, unstable and unknown communication environments, and the heterogeneous nature of these devices. To address these challenges, in this paper, we introduce a novel coded computation scheme based on multi-agent reinforcement learning (MARL), which has many promising features such as adaptability to network changes, high efficiency and robustness to uncertain system disturbances, consideration of node heterogeneity, and decentralized load allocation. Comprehensive simulation studies demonstrate that the proposed approach can outperform state-of-the-art distributed computing schemes.","1938-1883","978-1-7281-7122-7","10.1109/ICC42927.2021.9500600","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9500600","","Uncertain systems;Computational modeling;Batch production systems;Reinforcement learning;Mobile handsets;Robustness;Topology","learning (artificial intelligence);mobile ad hoc networks;mobile computing;multi-agent systems;telecommunication network topology","multiagent reinforcement learning;mobile ad hoc computing;mobile device;computing resources;computation task;node mobility;computation scheme;state-of-the-art distributed computing schemes","","1","","24","IEEE","6 Aug 2021","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning in Cournot Games","Y. Shi; B. Zhang","Department of Electrical and Computer Engineering, University of Washington, Seattle, WA; Department of Electrical and Computer Engineering, University of Washington, Seattle, WA","2020 59th IEEE Conference on Decision and Control (CDC)","11 Jan 2021","2020","","","3561","3566","In this work, we study the interaction of strategic agents in continuous action Cournot games with limited information feedback. Cournot game is the essential market model for many socio-economic systems where agents learn and compete without the full knowledge of the system or each other. We consider the dynamics of the policy gradient algorithm, which is a widely adopted continuous control reinforcement learning algorithm, in concave Cournot games. We prove the convergence of policy gradient dynamics to the Nash equilibrium when the price function is linear or the number of agents is two. This is the first result (to the best of our knowledge) on the convergence property of learning algorithms with continuous action spaces that do not fall in the no-regret class.","2576-2370","978-1-7281-7447-1","10.1109/CDC42340.2020.9304089","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9304089","","Games;Convergence;Production;Nash equilibrium;Reinforcement learning;Heuristic algorithms;Standards","game theory;gradient methods;learning (artificial intelligence);multi-agent systems","continuous action Cournot games;information feedback;Cournot game;essential market model;socio-economic systems;policy gradient algorithm;continuous control reinforcement learning;concave Cournot games;policy gradient dynamics;multiagent reinforcement learning;strategic agents;Nash equilibrium","","1","","37","IEEE","11 Jan 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Grant-Free Mode Selection for O-RAN Systems","H. -W. Hsu; Y. -C. Lin; C. -W. Huang; P. Lin; S. -R. Yang","Department of Communication Engineering, National Central University, Taoyuan, Taiwan; Department of Communication Engineering, National Central University, Taoyuan, Taiwan; Department of Communication Engineering, National Central University, Taoyuan, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taiwan; Department of Computer Science and Institute of Communications Engineering, National Tsing Hua University, Taiwan","2023 International Wireless Communications and Mobile Computing (IWCMC)","21 Jul 2023","2023","","","1002","1007","As technology advancements are leading to the creation of 5G and next-generation base stations (BS) that offer improved performance and application integration, current solutions are mostly reliant on established technical standards. By incorporating intelligent wireless resource management technology, the current small cell system can be optimized and its transmission performance enhanced. The implementation of deep reinforcement learning was then added. By using indication reports as the state, the smart agent is able to dynamically select the optimal GF parameters to achieve high-efficiency transmission. In the context of ultra-reliable low latency communication (URLLC) applications, we have utilized 5G ns-3 simulation to simulate an IIoT factory scenario that diverges from traditional uplink methods. By implementing grant-free (GF) techniques, we can reduce delays while maintaining a suitable level of reliability. To dynamically select the most appropriate transmission mode under varying conditions, we have developed reinforcement learning (RL) methods. Our numerical results demonstrate a promising trend in the overall satisfaction rate.","2376-6506","979-8-3503-3339-8","10.1109/IWCMC58020.2023.10182990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10182990","Grant free mode;O-RAN;xApps;industrial IoT","Wireless communication;5G mobile communication;Reinforcement learning;Ultra reliable low latency communication;Production facilities;Numerical models;Resource management","5G mobile communication;deep learning (artificial intelligence);millimetre wave communication;reinforcement learning;resource allocation;telecommunication computing;telecommunication network reliability;telecommunication scheduling","5G NS-3 simulation;current small cell system;deep reinforcement learning;grant-free techniques;high-efficiency transmission;intelligent wireless resource management technology;next-generation base stations;o-RAN systems;optimal GF parameters;reinforcement learning methods;reinforcement learning-based grant-free mode selection;smart agent;transmission mode;transmission performance;ultra-reliable low latency communication applications","","","","13","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"An Imputation Reinforcement Learning Agent For Power Transformers Load Study","T. Houngbadji","METLAB Research Inc., Ecole Polytechnique de Montreal, Montreal, Canada","2022 IEEE International Conference on Power Systems Technology (POWERCON)","4 Nov 2022","2022","","","1","6","Among the challenges occurring when performing power transformers load study is the lack of data stemming from the loss of the heat-run factory test report, or the inability to properly collect the required inputs. To address the issue, this paper introduces a novel imputation strategy for the required attributes values. By adopting a data-driven approach, a reinforcement learning agent with little prior knowledge and using a multivariate random walks simulation of the state space is devised to navigate a thermal environment where unknown temperature rises are estimated against the normal life expectancy loading scenario. A performance evaluation of the imputation agent with a case study evidencing a natural cooled air transformer showcases prediction error within 5% range, signaling the efficacy of the imputation agent.","","978-1-6654-1775-4","10.1109/POWERCON53406.2022.9929886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9929886","power transformer loading;data imputation;reinforcement learning;random walk","Performance evaluation;Sensitivity analysis;Atmospheric modeling;Reinforcement learning;Production facilities;Power transformers;Thermal loading","cooling;data analysis;learning (artificial intelligence);power engineering computing;power transformer insulation;power transformer testing;power transformers;random processes","natural cooled air transformer;imputation agent;imputation reinforcement;power transformers load study;heat-run factory test report;required inputs;novel imputation strategy;required attributes values;data-driven approach;reinforcement learning agent;multivariate random walks simulation;normal life expectancy loading scenario;case study evidencing","","","","15","IEEE","4 Nov 2022","","","IEEE","IEEE Conferences"
"Distributed Emergent Agreements with Deep Reinforcement Learning","K. Schmid; R. Müller; L. Belzner; J. Tochtermann; C. Linhoff-Popien","Ludwig-Maximilians-Universität, München; Ludwig-Maximilians-Universität, München; Maiborn Wolff GmbH; Ludwig-Maximilians-Universität, München; Ludwig-Maximilians-Universität, München","2021 International Joint Conference on Neural Networks (IJCNN)","20 Sep 2021","2021","","","1","8","Building autonomous agents that are capable to cooperate with other machines is an essential step towards large scale application of AI systems. Especially systems comprised of multiple self-interested agents with general sum returns can profit from cooperative behavior as cooperation can help to increase the return from all agents simultaneously. A critical aspect that might undermine cooperation is given if agents cannot make credible threats or promises (called commitment problems). Inspired by this idea in this work we augment deep reinforcement learning agents with the capability to build agreements with one another, thereby enabling agents to autonomously learn at which time to cooperate with other agents. This approach, called distributed emergent agreement learning (DEAL), enables agents to commit to specific policies defined by the agreement. We evaluate DEAL with up to 16 agents, represented as Deep Q-Networks or instances of Proximal Policy Optimization in a factory domain and empirically show that agreements increase cooperation by improving both overall and agent individual returns.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9533333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533333","","Waste materials;Neural networks;Buildings;Reinforcement learning;Production facilities;Robustness;Autonomous agents","deep learning (artificial intelligence);multi-agent systems;optimisation","DEAL;Deep Q-Networks;autonomous agents;AI systems;self-interested agents;deep reinforcement learning;cooperative behavior;distributed emergent agreements learning;proximal policy optimization","","","","21","IEEE","20 Sep 2021","","","IEEE","IEEE Conferences"
"Comparison of Genetic and Reinforcement Learning Algorithms for Energy Cogeneration Optimization","G. Ghione; V. Randazzo; A. Recchia; E. Pasero; M. Badami","DET, Politecnico di Torino, Turin, Italy; DET, Politecnico di Torino, Turin, Italy; DENERG, Politecnico di Torino, Turin, Italy; DET, Politecnico di Torino, Turin, Italy; DENERG, Politecnico di Torino, Turin, Italy","2023 8th International Conference on Smart and Sustainable Technologies (SpliTech)","1 Aug 2023","2023","","","1","7","Large process plants generally require energy in different forms: mechanical, electrical, or thermal (in the form of steam or hot water). A commonly used source of energy is cogeneration, also defined as Combined Heat and Power (CHP). Cogeneration can offer substantial economic as well as energy savings; however, its real-time operation scheduling is still a challenge today. Multiple algorithms have been proposed for the CHP control problem in the literature, such as genetic algorithms (GAs), particle swarm optimization algorithms, artificial neural networks, fuzzy decision making systems and, most recently, reinforcement learning (RL) algorithms.This paper presents the comparison of a RL approach and a GA for the control of a cogenerator, using as a case study a thermal power plant serving a factory during the year 2021. The two methods were compared based on an earnings before interest, taxes, depreciation, and amortization (EBITDA) metric. The EBITDA that could be obtained using the RL algorithm, exceeds both the EBITDA that could be generated using a per-week genetic algorithm and the one from the manual scheduling of the CHP. Thus, the RL algorithm proves to be the most cost-effective strategy for the control of a CHP.","","978-953-290-128-3","10.23919/SpliTech58164.2023.10193518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193518","CHP;cogeneration optimization;EBITDA;energy cogeneration;genetic algorithm;neural networks;reinforcement learning","Thermal factors;Processor scheduling;Cogeneration;Manuals;Reinforcement learning;Scheduling;Production facilities","","","","","","30","","1 Aug 2023","","","IEEE","IEEE Conferences"
"Multi-Agent Distributed Reinforcement Learning Algorithm for Free-Model Economic-Environmental Power and CHP Dispatch Problems","S. Safiri; A. Nikoofard; M. Khosravy; T. Senjyu","Control Engineering Department, K.N. Toosi University of Tehran, Tehran, Iran; Control Engineering Department, K.N. Toosi University of Tehran, Tehran, Iran; Cross Labs, Cross Compass Ltd., Tokyo, Japan; Electrical and Electronics Engineering Department, University of the Ryukyus, Okinawa, Japan","IEEE Transactions on Power Systems","18 Aug 2023","2023","38","5","4489","4500","In conventional methods for Economic Dispatch Problem (EDP) and Economic-Environmental Dispatch Problem (EEDP), a full connection between the units and the control center is considered, while some faults in the communication system are possible in practical conditions. Hence, an intelligent and high-speed distributed method that is robust against disconnection is needed. In this paper, a Multi-Agent Distributed Reinforcement Learning (MADRL) algorithm based on consensus control for EDP and EEDP is presented. In this method, the incremental cost of units is optimized based on Lagrange method and the reinforcement learning algorithm. Thus, a performance index is defined for each agent in proposed algorithm to be independent of the unit model. The performance index of each agent is the sum of two terms, including i) the difference between the previous performance index value and local power and heat mismatch values and ii) the sum of the difference between the incremental cost of the unit with other neighboring units. In other words, optimizing the defined performance indexes of the agents eliminates the need for the system model. The MADRL method is tested on several grids and compared with other methods. The numerical results show an improvement in the algorithm speed and optimal point.","1558-0679","","10.1109/TPWRS.2022.3217905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9932023","Combined heat and power;consensus control;distributed systems;economic/environmental dispatch problem;multi-agent systems;reinforcement learning","Costs;Cogeneration;Reinforcement learning;Performance analysis;Production;Optimization;Consensus control","deep learning (artificial intelligence);multi-agent systems;optimisation;power generation dispatch;power generation economics;reinforcement learning","algorithm speed;CHP Dispatch problems;communication system;consensus control;control center;defined performance indexes;Economic Dispatch Problem;Economic-Environmental Dispatch Problem;EDP;EEDP;free-model Economic-Environmental power;heat mismatch values;high-speed distributed method;incremental cost;intelligent speed distributed method;Lagrange method;local power;MADRL method;MultiAgent Distributed Reinforcement Learning algorithm;neighboring units;practical conditions;previous performance index value;system model;unit model","","","","42","IEEE","28 Oct 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning of Heuristic EV Fleet Charging in a Day-Ahead Electricity Market","S. Vandael; B. Claessens; D. Ernst; T. Holvoet; G. Deconinck","Energyville, Genk, Belgium; Department of Energy, Research Institute Vlaamse Instelling Voor Technologisch Onderzoek, Mol, Belgium; Department of Electrical Engineering and Computer Science, University of Liège, Liège, Belgium; Department of Computer Science, Katholieke Universiteit Leuven, Leuven, Belgium; Energyville, Genk, Belgium","IEEE Transactions on Smart Grid","20 May 2017","2015","6","4","1795","1805","This paper addresses the problem of defining a day-ahead consumption plan for charging a fleet of electric vehicles (EVs), and following this plan during operation. A challenge herein is the beforehand unknown charging flexibility of EVs, which depends on numerous details about each EV (e.g., plug-in times, power limitations, battery size, power curve, etc.). To cope with this challenge, EV charging is controlled during opertion by a heuristic scheme, and the resulting charging behavior of the EV fleet is learned by using batch mode reinforcement learning. Based on this learned behavior, a cost-effective day-ahead consumption plan can be defined. In simulation experiments, our approach is benchmarked against a multistage stochastic programming solution, which uses an exact model of each EVs charging flexibility. Results show that our approach is able to find a day-ahead consumption plan with comparable quality to the benchmark solution, without requiring an exact day-ahead model of each EVs charging flexibility.","1949-3061","","10.1109/TSG.2015.2393059","DistriNet Research Group of the Department of Computer Science, Catholic University of Leuven; Vlaamse Instelling Voor Technologisch Onderzoek, Flemish Institute for Technological Research; Electrical Energy and Computing Architectures Research Group of the Department of Electrical Engineering, Catholic University of Leuven; Department of Electrical Engineering and Computer Science, University of Liège; Linear project; Institute for the Promotion of Innovation by Science and Technology in Flanders; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056534","Demand-side management;electric vehicles (EVs);reinforcement learning (RL);stochastic programming (SP);Demand-side management;electric vehicles (EVs);reinforcement learning (RL);stochastic programming (SP)","Schedules;Batteries;Learning (artificial intelligence);Electricity supply industry;Stochastic processes;Aerospace electronics;Benchmark testing","battery powered vehicles;learning (artificial intelligence);power engineering computing;power markets;power system planning;secondary cells;stochastic programming","batch mode reinforcement learning;heuristic EV fleet charging;day-ahead electricity market;cost-effective day-ahead consumption plan;plug-in times;power limitations;battery size;power curve;resulting charging behavior;multistage stochastic programming solution","","130","","28","IEEE","9 Mar 2015","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Strategic Bidding in Electricity Markets","Y. Ye; D. Qiu; M. Sun; D. Papadaskalopoulos; G. Strbac","Fetch.AI, St. John’s Innovation Centre, Cambridge, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.","IEEE Transactions on Smart Grid","19 Feb 2020","2020","11","2","1343","1355","Bi-level optimization and reinforcement learning (RL) constitute the state-of-the-art frameworks for modeling strategic bidding decisions in deregulated electricity markets. However, the former neglects the market participants' physical non-convex operating characteristics, while conventional RL methods require discretization of state and/or action spaces and thus suffer from the curse of dimensionality. This paper proposes a novel deep reinforcement learning (DRL) based methodology, combining a deep deterministic policy gradient (DDPG) method with a prioritized experience replay (PER) strategy. This approach sets up the problem in multi-dimensional continuous state and action spaces, enabling market participants to receive accurate feedback regarding the impact of their bidding decisions on the market clearing outcome, and devise more profitable bidding decisions by exploiting the entire action domain, also accounting for the effect of non-convex operating characteristics. Case studies demonstrate that the proposed methodology achieves a significantly higher profit than the alternative state-of-the-art methods, and exhibits a more favourable computational performance than benchmark RL methods due to the employment of the PER strategy.","1949-3061","","10.1109/TSG.2019.2936142","Horizon 2020 Framework Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805177","Bi-level optimization;deep neural networks;deep reinforcement learning;electricity markets;strategic bidding;unit commitment","Optimization;Electricity supply industry;Indexes;Reinforcement learning;Sun;Employment;Biological system modeling","learning (artificial intelligence);power engineering computing;power markets;profitability;tendering","action spaces;market participants;market clearing outcome;bi-level optimization;deep deterministic policy gradient method;deep reinforcement learning based methodology;deregulated electricity markets;strategic bidding decisions;benchmark RL methods;nonconvex operating characteristics;profitable bidding decisions","","129","","48","IEEE","19 Aug 2019","","","IEEE","IEEE Journals"
"Indirect Customer-to-Customer Energy Trading With Reinforcement Learning","T. Chen; W. Su","Department of Electrical and Computer Engineering, University of Michigan–Dearborn, Dearborn, MI, USA; Department of Electrical and Computer Engineering, University of Michigan–Dearborn, Dearborn, MI, USA","IEEE Transactions on Smart Grid","19 Jun 2019","2019","10","4","4338","4348","In this paper, we explore the role of emerging energy brokers (middlemen) in a localized event-driven market (LEM) at the distribution level for facilitating indirect customer-to-customer energy trading. This proposed LEM does not aim to replace any existing energy service or become the best market model; but instead to diversify the energy ecosystem at the edge of distribution networks. In light of this philosophy, the market mechanism will provide additional options for customers and prosumers who have the willingness to directly participate in the retail electricity market occasionally, on top of using existing utility services. It also helps in improving market efficiency and encouraging local-level power balance, while taking into account the characteristics of customers’ behavior. The energy trading process will be built as a Markov decision process with some reinforcement learning and data-driven methods applied. Some economic concepts, like search friction, related to this kind of typical search cost involved market model are also discussed.","1949-3061","","10.1109/TSG.2018.2857449","Socio-Cyber-Physical Framework of Future Retail Electricity Market, MCubed Program, 2016–2017; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8412581","Event-driven market;energy trading;Markov decision process;reinforcement learning","Electricity supply industry;Business;Ecosystems;Markov processes;Learning (artificial intelligence);Peer-to-peer computing;Buildings","cost reduction;customer services;learning (artificial intelligence);Markov processes;power engineering computing;power markets;smart power grids","energy ecosystem;market mechanism;customers;prosumers;retail electricity market;market efficiency;encouraging local-level power balance;energy trading process;reinforcement learning;typical search cost involved market model;indirect customer-to-customer energy trading;energy brokers;localized event-driven market;LEM;existing energy service;Markov decision process","","97","","31","IEEE","18 Jul 2018","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Joint Bidding and Pricing of Load Serving Entity","H. Xu; H. Sun; D. Nikovski; S. Kitamura; K. Mori; H. Hashimoto","Data Analytics Group, Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Data Analytics Group, Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Data Analytics Group, Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Advanced Technology Research and Development Center, Mitsubishi Electric Corporation, Amagasaki, Japan; Advanced Technology Research and Development Center, Mitsubishi Electric Corporation, Amagasaki, Japan; Data Analytics Group, Mitsubishi Electric Research Laboratories, Cambridge, MA, USA","IEEE Transactions on Smart Grid","21 Oct 2019","2019","10","6","6366","6375","In this paper, we address the problem of jointly determining the energy bid submitted to the wholesale electricity market (WEM) and the energy price charged in the retailed electricity market (REM) for a load serving entity (LSE). The joint bidding and pricing problem is formulated as a Markov decision process (MDP) with continuous state and action spaces in which the energy bid and the energy price are two actions that share a common objective. We apply the deep deterministic policy gradient (DDPG) algorithm to solve this MDP for the optimal bidding and pricing policies. Yet, the DDPG algorithm typically requires a significant number of state transition samples, which are costly in this application. To this end, we apply neural networks to learn dynamical bid and price response functions from historical data to model the WEM and the collective behavior of the end use customers (EUCs), respectively. These response functions explicitly capture the inter-temporal correlations of the WEM clearing results and the EUC responses and can be utilized to generate state transition samples without any cost. More importantly, the response functions also inform the choice of states in the MDP formulation. Numerical simulations illustrated the effectiveness of the proposed methodology.","1949-3061","","10.1109/TSG.2019.2903756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8662721","Electricity market;bidding;pricing;load serving entity;demand response;deep reinforcement learning","Pricing;Heuristic algorithms;Electricity supply industry;Power systems;ISO;Real-time systems;Energy consumption","","","","81","","28","IEEE","7 Mar 2019","","","IEEE","IEEE Journals"
"Local Energy Trading Behavior Modeling With Deep Reinforcement Learning","T. Chen; W. Su","Department of Electrical and Computer Engineering, University of Michigan-Dearborn, Dearborn, MI, USA; Department of Electrical and Computer Engineering, University of Michigan-Dearborn, Dearborn, MI, USA","IEEE Access","13 Nov 2018","2018","6","","62806","62814","In this paper, we model prosumers’ energy trading behavior, with the operation of an energy storage system, in a proposed event-driven local energy market. Through modeling local energy trading strategies of a prosumer in the proposed holistic market model, the prosumer’s decision-making process will be built as a Markov decision process with many continuous variables. Then, this decision-making process of local market participation will be solved by deep reinforcement learning technology with experience replay mechanism. Specifically, a deep Q-learning for local energy trading algorithm is modified from deep Q-network to facilitate such a decision-making within an intelligent energy system and promote prosumers’ willingness to participate in the localized energy ecosystem.","2169-3536","","10.1109/ACCESS.2018.2876652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8496766","Prosumer;energy trading;Markov decision process;deep reinforcement learning","Machine learning;Energy storage;Electricity supply industry;Companies;Decision making;Ecosystems;Learning (artificial intelligence)","decision making;learning (artificial intelligence);Markov processes;multi-agent systems;power engineering computing;power markets;power system management","holistic market model;Markov decision process;decision-making process;local market participation;deep reinforcement learning technology;deep Q-learning;local energy trading algorithm;deep Q-network;intelligent energy system;localized energy ecosystem;local energy trading behavior modeling;model prosumers;energy storage system;event-driven local energy market;local energy trading strategies","","71","1","26","OAPA","18 Oct 2018","","","IEEE","IEEE Journals"
"UAV Autonomous Target Search Based on Deep Reinforcement Learning in Complex Disaster Scene","C. Wu; B. Ju; Y. Wu; X. Lin; N. Xiong; G. Xu; H. Li; X. Liang","School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, China; School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, China; School of Public and Environmental Affairs, Indiana University, Bloomington, CO, USA; Department of Computer Science, Shanghai Normal University, Shanghai, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; State Key Laboratory for Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; School of Information, Kyoto University, Kyoto, CO, Japan","IEEE Access","30 Aug 2019","2019","7","","117227","117245","In recent years, artificial intelligence has played an increasingly important role in the field of automated control of drones. After AlphaGo used Intensive Learning to defeat the World Go Championship, intensive learning gained widespread attention. However, most of the existing reinforcement learning is applied in games with only two or three moving directions. This paper proves that deep reinforcement learning can be successfully applied to an ancient puzzle game Nokia Snake after further processing. A game with four directions of movement. Through deep intensive learning and training, the Snake (or self-learning Snake) learns to find the target path autonomously, and the average score on the Snake Game exceeds the average score on human level. This kind of Snake algorithm that can find the target path autonomously has broad prospects in the industrial field, such as: UAV oil and gas field inspection, Use drones to search for and rescue injured people after a complex disaster. As we all know, post-disaster relief requires careful staffing and material dispatch. There are many factors that need to be considered in the artificial planning of disaster relief. Therefore, we want to design a drone that can search and rescue personnel and dispatch materials. Current drones are quite mature in terms of automation control, but current drones require manual control. Therefore, the Snake algorithm proposed here to be able to find the target path autonomously is an attempt and key technology in the design of autonomous search and rescue personnel and material dispatching drones.","2169-3536","","10.1109/ACCESS.2019.2933002","National Basic Research Program of China (973 Program)(grant numbers:2018YFC0810204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8787847","Deep reinforcement learning;Markov decision;Monte Carlo;Q-learning","Reinforcement learning;Drones;Games;Training;Task analysis;Deep learning;Genetic algorithms","autonomous aerial vehicles;computer games;disasters;emergency management;emergency services;learning (artificial intelligence);rescue robots","self-learning snake;reinforcement learning;UAV autonomous target search;material dispatching drones;rescue personnel;autonomous search;automation control;post-disaster relief;gas field inspection;industrial field;Snake algorithm;Snake Game;average score;target path;deep intensive learning;ancient puzzle game Nokia Snake;artificial intelligence;complex disaster scene;deep reinforcement learning","","67","","39","CCBY","5 Aug 2019","","","IEEE","IEEE Journals"
"Multiagent Deep-Reinforcement-Learning-Based Resource Allocation for Heterogeneous QoS Guarantees for Vehicular Networks","J. Tian; Q. Liu; H. Zhang; D. Wu","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; Shandong Provincial Key Laboratory of Wireless Communication Technologies and the School of Control Science and Engineering, Shandong University, Jinan, China; Department of Computer Science and Engineering, University of Tennessee at Chattanooga, Chattanooga, TN, USA","IEEE Internet of Things Journal","24 Jan 2022","2022","9","3","1683","1695","Vehicle-to-vehicle communications can offer direct information interaction, including security-centered information and entertainment information. However, the rapid proliferation of vehicles and the diversity of communications services demand for a more intelligent and efficient resource allocation framework to enhance network performance. In this article, a multi-agent deep reinforcement learning-based resource allocation framework is developed to jointly optimize the channel allocation and power control to satisfy the heterogeneous Quality-of-Service (QoS) requirements in heterogeneous vehicular networks. In the proposed framework, the utility maximization problem is formulated by considering two types of traffics, i.e., the strict ultrareliable and low-latency requirements for safety-centric applications and the high-capacity requirements for entertainment applications. The utility of each vehicular users is formulated as a multicriterion objective function by taking into account the heterogeneous traffic requirements. To overcome the drawbacks of the traditional totally centralized and distributed deep reinforcement learning-based resource allocation approaches, we propose a multi-agent deep deterministic policy gradient algorithm with centralized learning and decentralized execution to solve the formulated optimization problem. The normalization of the input states and reward functions is introduced to speed up the training and learning progress of the proposed algorithm. Simulation results show the superiority of the proposed algorithm in terms of the convergence and system performance through the comparison with the other methods and schemes for the delay-sensitive applications and delay-tolerant applications.","2327-4662","","10.1109/JIOT.2021.3089823","Project of International Cooperation and Exchanges NSFC(grant numbers:61860206005); National Natural Science Foundation of China(grant numbers:61801278,61972237); Key Laboratory of Cognitive Radio and Information Processing, Ministry of Education (Guilin University of Electronic Technology)(grant numbers:CRKL190205); Shandong Provincial Scientific Research Programs in Colleges and Universities(grant numbers:J18KA310); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9456882","Deep reinforcement learning (DRL);heterogeneous applications;multi-agent deep deterministic policy gradient (MADDPG);resource allocation","Resource management;Quality of service;Optimization;Reinforcement learning;Training;Entertainment industry;Copper","channel allocation;learning (artificial intelligence);multi-agent systems;optimisation;power control;quality of service;resource allocation;telecommunication traffic","Quality-of-Service requirements;heterogeneous vehicular networks;utility maximization problem;strict ultrareliable requirements;low-latency requirements;safety-centric applications;high-capacity requirements;entertainment applications;vehicular users;heterogeneous traffic requirements;traditional totally centralized distributed deep reinforcement learning-based;allocation approaches;multiagent deep deterministic policy gradient algorithm;centralized learning;learning progress;multiagent deep-reinforcement-learning-based resource allocation;heterogeneous QoS guarantees;vehicle-to-vehicle communications;direct information interaction;security-centered information;entertainment information;communications services demand;intelligent resource allocation framework;efficient resource allocation framework;network performance;multiagent deep reinforcement learning-based resource allocation framework;channel allocation","","30","","44","IEEE","16 Jun 2021","","","IEEE","IEEE Journals"
"A Reinforcement Learning-Based Decision System for Electricity Pricing Plan Selection by Smart Grid End Users","T. Lu; X. Chen; M. B. McElroy; C. P. Nielsen; Q. Wu; Q. Ai","School of Electrical Engineering, Shandong University, Jinan, China; School of Electrical and Electronic Engineering, Huazhong University of Science and Technology, Wuhan, China; John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA; John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA; Department of Electrical Engineering, Center for Electric Power and Energy, Technical University of Denmark, Lyngby, Denmark; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Smart Grid","20 Apr 2021","2021","12","3","2176","2187","With the development of deregulated retail power markets, it is possible for end users equipped with smart meters and controllers to optimize their consumption cost portfolios by choosing various pricing plans from different retail electricity companies. This article proposes a reinforcement learning-based decision system for assisting the selection of electricity pricing plans, which can minimize the electricity payment and consumption dissatisfaction for individual smart grid end user. The decision problem is modeled as a transition probability-free Markov decision process (MDP) with improved state framework. The proposed problem is solved using a Kernel approximator-integrated batch Q-learning algorithm, where some modifications of sampling and data representation are made to improve the computational and prediction performance. The proposed algorithm can extract the hidden features behind the time-varying pricing plans from a continuous high-dimensional state space. Case studies are based on data from real-world historical pricing plans and the optimal decision policy is learned without a priori information about the market environment. Results of several experiments demonstrate that the proposed decision model can construct a precise predictive policy for individual user, effectively reducing their cost and energy consumption dissatisfaction.","1949-3061","","10.1109/TSG.2020.3027728","National Science Foundation China(grant numbers:51907066); Young Elite Scientists Sponsorship Program; Harvard Global Institute and Energy Foundation China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9209082","Smart grid end user;decision system;electricity market;value-based Q learning;demand response","Pricing;Companies;Prediction algorithms;Energy consumption;Electricity supply industry;Smart grids;Indexes","approximation theory;costing;decision theory;feature extraction;learning (artificial intelligence);Markov processes;power engineering computing;power markets;power system planning;pricing;probability;retailing;smart meters;smart power grids","precise predictive policy;cost reduction;market environment;continuous high-dimensional state space;hidden feature extraction;prediction performance;data representation;sampling modification;energy consumption dissatisfaction;decision model;optimal decision policy;real-world historical pricing plans;time-varying pricing plans;Kernel approximator-integrated batch Q-learning algorithm;improved state framework;transition probability-free Markov decision process;decision problem;electricity payment minimization;electricity pricing plans;retail electricity companies;consumption cost portfolio optimization;smart meters;deregulated retail power markets;smart grid end users;electricity pricing plan selection;reinforcement learning-based decision system","","27","","27","IEEE","29 Sep 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Bidding Strategy for EVAs in Local Energy Market Considering Information Asymmetry","Y. Tao; J. Qiu; S. Lai","School of Electrical and Infor-mation Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical and Infor-mation Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical and Infor-mation Engineering, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Industrial Informatics","21 Feb 2022","2022","18","6","3831","3842","With the increasing penetration of distributed energy resources (DERs) in smart grids, customers can be aggregated to participate in the local energy market (LEM). In the LEM, on the one hand, the aggregated customers can purchase electricity from local DERs at a price that may be lower than the electricity price from the utility. On the other hand, when there is abundant energy, the aggregated customers can sell them in the LEM at a higher price, supplementing the grid power supply with clean renewable energy. Therefore, the customers' dependency on the utility is reduced. In this context, this article presents a bidding strategy for electric vehicle aggregators (EVAs) based on data analytics and deep reinforcement learning (DRL). To achieve this goal, an asynchronous learning framework is put forward to help EVAs formulate bids, including bidding price and bidding volume. Compared with the conventional model-based strategy, the learning-based strategy shows advantages in rapid decision-making and reduced reliance on stochastic models. Besides, the EVAs can cope with the information asymmetry in the LEM by using the DRL method. A modified deep deterministic policy gradient methodology is utilized to speed up the online training to avoid high losses at the training stage. According to the simulation results, it can be concluded that the profit of the learning-based strategy is 63.3% higher than that of the random strategy. The coefficient of variation of the learning-based strategy is 76.4% lower than that of the random strategy. Therefore, the proposed learning-based method is effective.","1941-0050","","10.1109/TII.2021.3116275","ARC Research Hub(grant numbers:IH180100020); ARC Training Centre(grant numbers:IC200100023,LP200100056); Sir William Tyree Foundation-Distributed Power Generation Research Fund; Shenzhen Institute of Artificial Intelligence and Robotics for Society; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9552534","Data-driven bidding strategy;deep reinforcement learning (DRL);electric vehicle aggregators (EVAs);information asymmetry;local energy market (LEM)","Electricity supply industry;Mathematical models;Pricing;Optimization;Training;Reinforcement learning;Informatics","distributed power generation;electric vehicles;learning (artificial intelligence);power generation economics;power markets;pricing;smart power grids","deep reinforcement learning;bidding strategy;local energy market considering information asymmetry;distributed energy resources;smart grids;LEM;aggregated customers;local DERs;electricity price;abundant energy;higher price;grid power supply;clean renewable energy;electric vehicle aggregators;asynchronous learning framework;EVAs formulate bids;bidding price;bidding volume;conventional model-based strategy;learning-based strategy;modified deep deterministic policy gradient methodology;random strategy;learning-based method","","16","","29","IEEE","29 Sep 2021","","","IEEE","IEEE Journals"
"Resilience Microgrid as Power System Integrity Protection Scheme Element With Reinforcement Learning Based Management","L. Tightiz; H. Yang","Department of Computer Engineering, Sejong University, Seoul, South Korea; Department of Computer Engineering, Sejong University, Seoul, South Korea","IEEE Access","14 Jun 2021","2021","9","","83963","83975","The microgrid is a solution for integrating renewable energy resources into the power system. However, overcoming the randomness of these nature-based resources requires a robust control system. Moreover, electricity market participation and ancillary service provision for the utility grid are other aspects, although intensify microgrid penetration makes its environment interactions more complex. Reinforcement learning is a technique vastly applied to such an intricate environment. Hence, in this paper, we deployed deep deterministic policy gradient and soft-actor critic methods to solve the high-dimensional, continuous, and stochastic problem of the microgrid's energy management system and compared the performance of two methods. Additionally, we developed the microgrid interactions with the utility grid as a participant of system integrity protection schema responding promptly to the utility grid protection requirements based on its reliable available resources. Moreover, we applied actual data of Gasa Island microgrid in Korea to prove the efficiency of proposed method.","2169-3536","","10.1109/ACCESS.2021.3087491","Technology Development Program to Solve Climate Changes through the National Research Foundation of Korea (NRF) by the Ministry of Science and ICT(grant numbers:NRF-2021M1A2A2065447); Korea Electric Power Corporation(grant numbers:R17XA05-2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448198","Energy management system;deep deterministic policy gradient;soft actor-critic;system integrity protection schema","Microgrids;Energy management;Power generation;Uncertainty;Energy management systems;Batteries;Electricity supply industry","distributed power generation;energy management systems;learning (artificial intelligence);power engineering computing;power generation control;power generation protection;power grids;power markets;renewable energy sources;robust control;stochastic processes","electricity market participation;ancillary service provision;reinforcement learning;deep deterministic policy gradient;soft-actor critic methods;stochastic problem;Gasa Island microgrid;renewable energy resources;nature-based resources;robust control system;power system integrity protection scheme;utility grid protection;energy management system;Korea","","14","","30","CCBYNCND","8 Jun 2021","","","IEEE","IEEE Journals"
"Multi-Agent Deep Reinforcement Learning for Coordinated Energy Trading and Flexibility Services Provision in Local Electricity Markets","Y. Ye; D. Papadaskalopoulos; Q. Yuan; Y. Tang; G. Strbac","School of Electrical Engineering, Southeast University, Nanjing, China; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K; School of Electrical Engineering, Southeast University, Nanjing, China; School of Electrical Engineering, Southeast University, Nanjing, China; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K","IEEE Transactions on Smart Grid","21 Feb 2023","2023","14","2","1541","1554","Local electricity markets (LEM) have recently attracted great interest as an effective solution to the challenging problem of distributed energy resources’ (DER) management. However, LEM designs combining the market functions of local energy trading and flexibility services (FS) provision to wider system operators have not attracted sufficient attention. In the context of addressing this research gap, this paper firstly provides a new model-based system-centric formulation for the coordination of such a LEM, which provides a theoretical optimality benchmark. Compared to previous formulations, it considers the time-coupling operating characteristics of flexible DERs, and optimizes the two market functions simultaneously. Furthermore, this paper explores for the very first time a model-free prosumer-centric coordination approach for such a LEM, in order to address the practical limitations of model-based system-centric approaches. This is achieved through a new multi-agent deep reinforcement learning method which combines the beneficial properties of the multi-actor-attention-critic and the prioritized experience replay approaches. Case studies on a real-world, large-scale setting validate that the proposed LEM design successfully encapsulates the economic benefits of both local energy trading and FS provision functions, and demonstrate that the proposed learning method outperforms previous methods.","1949-3061","","10.1109/TSG.2022.3149266","National Natural Science Foundation of China(grant numbers:52207082); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20220842); 2021 Jiangsu Shuangchuang (Mass Innovation and Entrepreneurship) Talent Program(grant numbers:JSSCBS20210137); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9705504","Deep reinforcement learning;distributed energy resources;flexibility services;local electricity markets;multi-agent systems","Context modeling;Computer architecture;HVAC;Focusing;Electricity supply industry;Reinforcement learning;Costs","deep learning (artificial intelligence);optimisation;power distribution economics;power engineering computing;power markets;reinforcement learning","coordinated energy trading;distributed energy resources;flexibility service provision;flexible DER;LEM design;local electricity market functions;local energy trading;model-based system-centric approaches;model-based system-centric formulation;model-free prosumer-centric coordination approach;multiactor-attention-critic;multiagent deep reinforcement learning method;theoretical optimality benchmark;time-coupling operating characteristics","","13","","55","IEEE","7 Feb 2022","","","IEEE","IEEE Journals"
"Permissioned Blockchain and Deep Reinforcement Learning Enabled Security and Energy Efficient Healthcare Internet of Things","L. Liu; Z. Li","School of Political Science and Public Administration, East China University of Political Science and Law, Shanghai, China; School of International and Public Affairs, Shanghai Jiao Tong University, Shanghai, China","IEEE Access","24 May 2022","2022","10","","53640","53651","Recently, the Healthcare Internet of Things (H-IoT) has been widely applied to alleviate the global challenge of the coronavirus disease 2019 (COVID-19) pandemic. However, security and limited energy capacity issues remain the two main factors that prevent the large-scale application of the H-IoT. Therefore, a permissioned blockchain and deep reinforcement learning (DRL)-empowered H-IoT system is presented in this research to address these two issues. The proposed H-IoT system can provide real-time security and energy-efficient healthcare services to control the propagation of the COVID-19 pandemic. To address the security issue, a permissioned blockchain method is adopted to guarantee the security of the proposed H-IoT system. As for handling the limited energy constraint, we employ the mobile edge computing (MEC) method to offload the computing tasks to alleviate the computational burden and energy consumption of the proposed H-IoT system. We also adopt an energy harvesting method to improve performance. In addition, a DRL method is employed to jointly optimize both the security and energy efficiency performance of the proposed system. The simulation results demonstrate that the proposed solution can balance the requirements of security and energy efficiency issues and hence can better respond to the COVID-19 pandemic.","2169-3536","","10.1109/ACCESS.2022.3176444","National Natural Science Foundation of China(grant numbers:71974057,72071031); Ministry of Education through the Humanities and Social Science Project(grant numbers:19YJC630104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777961","Blockchain;deep reinforcement learning;mobile edge computing;energy harvesting;healthcare Internet of Things;COVID-19","COVID-19;Blockchains;Security;Pandemics;Medical services;Internet of Things;Industries","blockchains;diseases;energy conservation;energy consumption;energy harvesting;health care;Internet of Things;medical information systems;mobile computing;reinforcement learning","global challenge;energy efficient Healthcare Internet;deep reinforcement learning enabled security;energy efficiency issues;energy efficiency performance;DRL method;energy harvesting method;energy consumption;mobile edge computing method;energy constraint;permissioned blockchain method;security issue;COVID-19 pandemic;energy-efficient healthcare services;real-time security;deep reinforcement learning-empowered H-IoT system;limited energy capacity issues","","11","","56","CCBY","18 May 2022","","","IEEE","IEEE Journals"
"Approximating Nash Equilibrium in Day-ahead Electricity Market Bidding with Multi-agent Deep Reinforcement Learning","Y. Du; F. Li; H. Zandi; Y. Xue","University of Tennessee, Knoxville, USA; University of Tennessee, Knoxville, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA","Journal of Modern Power Systems and Clean Energy","19 Apr 2021","2021","9","3","534","544","In this paper, a day-ahead electricity market bidding problem with multiple strategic generation company (GEN-CO) bidders is studied. The problem is formulated as a Markov game model, where GENCO bidders interact with each other to develop their optimal day-ahead bidding strategies. Considering unobservable information in the problem, a model-free and data-driven approach, known as multi-agent deep deterministic policy gradient (MADDPG), is applied for approximating the Nash equilibrium (NE) in the above Markov game. The MAD-DPG algorithm has the advantage of generalization due to the automatic feature extraction ability of the deep neural networks. The algorithm is tested on an IEEE 30-bus system with three competitive GENCO bidders in both an uncongested case and a congested case. Comparisons with a truthful bidding strategy and state-of-the-art deep reinforcement learning methods including deep $Q$ network and deep deterministic policy gradient (DDPG) demonstrate that the applied MADDPG algorithm can find a superior bidding strategy for all the market participants with increased profit gains. In addition, the comparison with a conventional-model-based method shows that the MADDPG algorithm has higher computational efficiency, which is feasible for real-world applications.","2196-5420","","10.35833/MPCE.2020.000502","NSF(grant numbers:EEC-1041877,ECCS-1809458); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406572","Bidding strategy;day-ahead electricity market;deep reinforcement learning;Markov game;multi-agent deterministic policy gradient (MADDPG);Nash equilibrium (NE)","Games;Electricity supply industry;Markov processes;Computational modeling;Training;Nash equilibrium;Mathematical model","","","","9","","33","","19 Apr 2021","","","SGEPRI","SGEPRI Journals"
"A reinforcement learning algorithm for Agent-based Computational Economics (ACE) model of electricity markets","B. M. Radhakrishnan; D. Srinivasan; Y. F. A. Lau; B. G. Parasumanna; A. K. Rathore; S. K. Panda; A. Khambadkone","Department of Electrical and Computer Engineering, National University of Singapore; Department of Electrical and Computer Engineering, National University of Singapore; National University of Singapore, Singapore, SG; Department of Electrical and Computer Engineering, National University of Singapore; Department of Electrical and Computer Engineering, National University of Singapore; Department of Electrical and Computer Engineering, National University of Singapore; Department of Electrical and Computer Engineering, National University of Singapore","2015 IEEE Congress on Evolutionary Computation (CEC)","14 Sep 2015","2015","","","297","303","Electricity markets in countries around the world are being restructured in pursuit of economic efficiency through competition. However, unpredictability in electricity prices and previous occurrences of market failure have indicated a need to better understand the complex interactions between the various market participants as well as to design market rules that maximizes efficiency and security. In this paper, the techniques of Agent-based Computational Economics (ACE) are employed to simulate the behavior of the GenCo participants in the National Electricity Market of Singapore (NEMS). The use of Reinforcement Learning (RL) in the agent-based modeling will be a more realistic representation of the GenCo and will bring about more accurate electricity market simulation outcomes. Actor-Critics constitute an important aspect of RL and in this paper we propose an adaptive actor-critic mapping using Particle Swarm Optimization (PSO). A simulation platform is built with the proposed model for Genco's learning and is tested in the conditions of varying vesting contract levels. Simulation results indicate that the proposed learning algorithm is able to procure higher GenCo revenue when benchmarked with existing learning algorithms.","1941-0026","978-1-4799-7492-4","10.1109/CEC.2015.7256905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7256905","agent-based modeling;deregulated electricity market;particle swarm optimization;reinforcement learning","Electricity supply industry;Nanoelectromechanical systems;Adaptation models;Computational modeling;Contracts;Economics;Companies","learning (artificial intelligence);particle swarm optimisation;power engineering computing;power markets;power system economics","reinforcement learning algorithm;agent based computational economics;electricity market model;electricity market simulation;adaptive actor-critic mapping;particle swarm optimization","","6","","17","IEEE","14 Sep 2015","","","IEEE","IEEE Conferences"
"Proximal Policy Optimization Based Reinforcement Learning for Joint Bidding in Energy and Frequency Regulation Markets","M. Anwar; C. Wang; F. de Nijs; H. Wang","Faculty of Information Technology, Monash University, Melbourne, Australia; Department of Civil Engineering, Monash University, Melbourne, Australia; Department of Data Science and AI, Monash University, Melbourne, Australia; Department of Data Science and AI, Monash University, Melbourne, Australia","2022 IEEE Power & Energy Society General Meeting (PESGM)","27 Oct 2022","2022","","","1","5","Driven by the global decarbonization effort, the rapid integration of renewable energy into the conventional electricity grid presents new challenges and opportunities for the battery energy storage system (BESS) participating in the energy market. Energy arbitrage can be a significant source of revenue for the BESS due to the increasing price volatility in the spot market caused by the mismatch between renewable generation and electricity demand. In addition, the Frequency Control Ancillary Services (FCAS) markets established to stabilize the grid can offer higher returns for the BESS due to their capability to respond within milliseconds. Therefore, it is crucial for the BESS to carefully decide how much capacity to assign to each market to maximize the total profit under uncertain market conditions. This paper formulates the bidding problem of the BESS as a Markov Decision Process, which enables the BESS to participate in both the spot market and the FCAS market to maximize profit. Then, Proximal Policy Optimization, a model-free deep reinforcement learning algorithm, is employed to learn the optimal bidding strategy from the dynamic environment of the energy market under a continuous bidding scale. The proposed model is trained and validated using real-world historical data of the Australian National Electricity Market. The results demonstrate that our developed joint bidding strategy in both markets is significantly profitable compared to individual markets.","1944-9933","978-1-6654-0823-3","10.1109/PESGM48719.2022.9917082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9917082","","Renewable energy sources;Profitability;Heuristic algorithms;Reinforcement learning;Markov processes;Electricity supply industry;Regulation","","","","4","","23","IEEE","27 Oct 2022","","","IEEE","IEEE Conferences"
"Solving Unit Commitment Problems with Multi-step Deep Reinforcement Learning","J. Qin; N. Yu; Y. Gao","Department of Electrical and Computer Engineering, University of California, Riverside, Riverside, California, USA; Department of Electrical and Computer Engineering, University of California, Riverside, Riverside, California, USA; Department of Electrical and Computer Engineering, University of California, Riverside, Riverside, California, USA","2021 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)","13 Dec 2021","2021","","","140","145","Solving the unit commitment (UC) problem in a computationally efficient manner is a critical issue of electricity market operations. Optimization-based methods such as heuristics, dynamic programming, and mixed-integer quadratic programming (MIQP) often yield good solutions to the UC problem. However, the computation time of optimization-based methods grows exponentially with the number of generating units, which is a major bottleneck in practice. To address this issue, we formulate the UC problem as a Markov decision process and propose a novel multi-step deep reinforcement learning (RL)-based algorithm to solve the problem. We approximate the action-value function with neural networks and design an algorithm to determine the feasible action space. Numerical studies on a 5-generator test case show that our proposed algorithm significantly outperforms the deep Q-learning and yields similar level of performance as that of MIQP-based optimization in terms of optimality. The computation time of our proposed algorithm is much shorter than that of MIQP-based optimization methods.","","978-1-6654-1502-6","10.1109/SmartGridComm51999.2021.9632339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9632339","Unit commitment;Markov decision process;deep reinforcement learning;multi-step return","Q-learning;Conferences;Neural networks;Process control;Markov processes;Approximation algorithms;Electricity supply industry","dynamic programming;integer programming;learning (artificial intelligence);Markov processes;optimisation;power generation dispatch;power generation scheduling;power markets;quadratic programming","action-value function;feasible action space;5-generator test case;computation time;MIQP-based optimization methods;unit commitment problem;computationally efficient manner;electricity market operations;optimization-based methods;dynamic programming;mixed-integer quadratic programming;UC problem;generating units;Markov decision process;novel multistep deep reinforcement learning-based algorithm","","3","","19","IEEE","13 Dec 2021","","","IEEE","IEEE Conferences"
"Data-driven decision-making strategies for electricity retailers: A deep reinforcement learning approach","Y. Liu; D. Zhang; H. B. Gooi","Department of Electrical Engineering, Tsinghua University, Beijing, China; China Electric Power Research Institute, Beijing, China; Nanyang Technological University, Singapore, Singapore","CSEE Journal of Power and Energy Systems","23 Mar 2021","2021","7","2","358","367","With the continuous development of the electricity market, the electricity retailers, as the intermediaries between producers and consumers, have emerged in some of the liberalized electricity markets. Meanwhile, the electricity retailer faces many increasingly significant challenges from the complexities and uncertainties in both the supply and consumption sides. This paper applies a data-driven decision-making strategy via Advantage Actor-Critic (A2C) and Deep Q-Learning (DQN) for the electricity retailers. The retailers' profits and consumers' costs are both taken into account. This study verifies that the applied data-driven methods can handle the decision-making problem as well as promote the profitability of retailers in the electricity market. Furthermore, A2C is more appropriate than DQN in our simulation. The effectiveness of the applied datadriven methods is validated by using real-world data.","2096-0042","","10.17775/CSEEJPES.2019.02510","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9215156","Artificial intelligence;electricity market;demand response;smart grid","Decision making;Machine learning;Pricing;Electricity supply industry;Machine learning algorithms;Learning (artificial intelligence);Uncertainty","decision making;learning (artificial intelligence);power markets;profitability;retailing","Deep Q-learning;Advantage Actor-Critic;decision-making problem;applied data-driven methods;liberalized electricity markets;deep reinforcement learning approach;electricity retailer;data-driven decision-making strategy","","3","","43","","6 Oct 2020","","","CSEE","CSEE Journals"
"Intra-day Electricity Market Bidding for Storage Devices using Deep Reinforcement Learning","F. Verdaasdonk; S. Demir; N. G. Paterakis","Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands; Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands; Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands","2022 International Conference on Smart Energy Systems and Technologies (SEST)","28 Sep 2022","2022","","","1","6","This paper investigates the use of Deep Reinforcement Learning (DRL) to control a profit-seeking storage device trading in the European Continuous Intra-day Electricity Market (CIM). The main objective is to study whether model-free DRL can profitably trade on the CIM. Two DRL agents are compared: Twin Delayed Deep Deterministic Policy Gradients (TD3), and TD3 with behavior cloning. The agents are trained and evaluated in a simulated CIM environment, which uses historical market data to simulate other market participants. A Rolling Intrinsic (RI) algorithm is used as a benchmark. Results indicate that the agents are profitable and occasionally outperform RI, in one instance obtaining 162.03% of RI profit. However, none of the agents consistently outperforms the baseline. These results suggest that DRL has the potential to increase profitability considerably compared to RI, but that the observation provided to the agent is not descriptive enough of the CIM to learn a robust policy. Future research could use additional features in the observation, or model-based DRL to improve performance.","","978-1-6654-0557-7","10.1109/SEST53650.2022.9898405","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9898405","Deep Reinforcement learning;Continuous intra-day Market;Storage Device","Performance evaluation;Electric potential;Profitability;Cloning;Europe;Reinforcement learning;Electricity supply industry","deep learning (artificial intelligence);power engineering computing;power markets;profitability;reinforcement learning","twin delayed deep deterministic policy gradients;profitability analysis;model-based DRL;European continuous intra-day electricity market;deep reinforcement learning;rolling intrinsic algorithm;cloning behaviour;CIM environment","","2","","19","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Energy Procurement and Retail Pricing for Electricity Retailers via Deep Reinforcement Learning with Long Short-term Memory","H. Xu; J. Wen; Q. Hu; J. Shu; J. Lu; Z. Yang","State Key Laboratory of Advanced Electromagnetic Engineering and Technology School of Electrical and Electronic Engineering Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Advanced Electromagnetic Engineering and Technology School of Electrical and Electronic Engineering Huazhong University of Science and Technology, Wuhan, China; School of Electrical Engineering, Southeast University, Nanjing, China; NARI Group Corporation, Nanjing, China; NARI Group Corporation, Nanjing, China; NARI Group Corporation, Nanjing, China","CSEE Journal of Power and Energy Systems","26 Sep 2022","2022","8","5","1338","1351","The joint optimization problem of energy procurement and retail pricing for an electricity retailer is converted into separately determining the optimal procurement strategy and optimal pricing strategy, under the “price-taker” assumption. The aggregate energy consumption of end use customers (EUCs) is predicted to solve for the optimal procurement strategy vis a long short-term memory (LSTM)-based supervised learning method. The optimal retail pricing problem is formulated as a Markov decision process (MDP), which can be solved by using deep reinforcement learning (DRL) algorithms. However, the performance of existing DRL approaches may deteriorate due to their insufficient ability to extract discriminative features from the time-series vectors in the environmental states. We propose a novel deep deterministic policy gradient (DDPG) network structure with a shared LSTM-based representation network that fully exploits the Actor's and Critic's losses. The designed shared representation network and the joint loss function can enhance the environment perception capability of the proposed approach and further improve the optimization performance, resulting in a more profitable pricing strategy. Numerical simulations demonstrate the effectiveness of the proposed approach.","2096-0042","","10.17775/CSEEJPES.2021.04330","Natural Science Foundation of Jiangsu Province(grant numbers:BK20210002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9713968","Deep reinforcement learning;electricity market;energy procurement;long short-term memory;retail pricing","Procurement;Pricing;Optimization;Electricity supply industry;Energy consumption;System dynamics;Supervised learning","learning (artificial intelligence);Markov processes;optimisation;pricing;procurement;retailing","energy procurement;electricity retailer;long short-term memory;joint optimization problem;optimal procurement strategy;optimal pricing strategy;price-taker assumption;aggregate energy consumption;short-term memory-based supervised learning method;optimal retail pricing problem;deep reinforcement learning algorithms;deep deterministic policy gradient network structure;shared LSTM-based representation network;optimization performance;profitable pricing strategy","","2","","40","","14 Feb 2022","","","CSEE","CSEE Journals"
"Influence of Discrete and Continuous Action Spaces on Deep Reinforcement Learning-Based Pricing Strategy Optimization for Electricity Retailers","H. Xu; X. Cai; J. Shu; J. Lu","NARI Group Corporation (State Grid Electric Power Research Institue), NARI Research Institute, Nanjing, China; School of Computer Technology, Hohai University, Nanjing, China; NARI Group Corporation (State Grid Electric Power Research Institue), NARI Research Institute, Nanjing, China; NARI Group Corporation (State Grid Electric Power Research Institue), NARI Research Institute, Nanjing, China","2021 IEEE Sustainable Power and Energy Conference (iSPEC)","24 Mar 2022","2021","","","3843","3848","The pricing strategy optimization problem becomes important for electricity retailers in electricity market. Deep reinforcement learning (DRL) has been applied to solve the strategic decision-making problems in electricity market area. However, the influence of discrete and continuous action spaces on optimization results by using DRL-based methods to solve for optimal retail price is unknown. This paper applies two different DRL-based retail pricing strategies through deep Q network (DQN) and deep deterministic policy gradient (DDPG) for the electricity retailers. An in-depth comparative analysis between DQN and DDPG is conducted in terms of convergence and computational performance. The numerical results of optimal retail prices and responding loads show the influence of discrete and continuous actions space on optimization effect.","","978-1-6654-1439-5","10.1109/iSPEC53008.2021.9735962","Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9735962","deep reinforcement learning;electricity market;retail pricing;action space;demand response","Simulation;Decision making;Pricing;Reinforcement learning;Electricity supply industry;Linear programming;Stability analysis","decision making;deep learning (artificial intelligence);numerical analysis;optimisation;power engineering computing;power markets;pricing;reinforcement learning;retailing","electricity retailers;strategic decision-making problems;electricity market area;deep Q network;deep deterministic policy gradient;optimal retail prices;DRL-based retail pricing strategies;discrete action;deep reinforcement learning-based pricing strategy;optimization process;DDPG;in-depth comparative analysis;DQN;numerical results","","2","","20","IEEE","24 Mar 2022","","","IEEE","IEEE Conferences"
"Intelligent Bidding Strategies for Prosumers in Local Energy Markets Based on Reinforcement Learning","G. C. Okwuibe; J. Bhalodia; A. S. Gazafroudi; T. Brenner; P. Tzscheutschler; T. Hamacher","School of Engineering and Design, Technical University of Munich, Munich, Germany; OLI Systems GmbH, Harthausen, Germany; OLI Systems GmbH, Harthausen, Germany; OLI Systems GmbH, Harthausen, Germany; School of Engineering and Design, Technical University of Munich, Munich, Germany; School of Engineering and Design, Technical University of Munich, Munich, Germany","IEEE Access","3 Nov 2022","2022","10","","113275","113293","Local energy markets (LEMs) are proposed in recent years as a way to enable local prosumers and community to trade their electricity and have control over their electrical related resources by ensuring that electricity is traded closer to where it is produced. However, literature is still scarce with the most optimal and effective trading strategies for LEM design. In this work, we propose two reinforcement learning based intelligent bidding strategies for prosumers and consumers trading within an LEM. Our proposed models were evaluated of their performance by testing them in a German real case scenario. The simulation results show that intelligent bidding strategies create additional self sufficiency and market savings to the local community compared to the baseline strategy where the agents make their trading decision randomly without an intelligent agent. Moreover, modelling the intelligent agents to perform towards a common goal creates more share of individual savings for the prosumers and consumers compared to the classical intelligent bidding strategies employed in this work.","2169-3536","","10.1109/ACCESS.2022.3217497","Federal Ministry for Economic Affairs and Energy (BMWi), Germany; Blockchainbasiertes dezentrales Energiemarktdesign und Managementstruktur (BEST) Project(grant numbers:03EI4017D); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931013","Bidding strategy;energy community;local energy markets;Markov decision process;peer-to-peer;reinforcement learning","Electricity supply industry;Q-learning;Classification algorithms;Machine learning algorithms;Decision making;Data models;Optimization;Reinforcement  learning;Energy management;Markov processes","decision making;power engineering computing;power markets;reinforcement learning;tendering","local energy markets;reinforcement learning;LEM design;consumers trading;market savings;local community;intelligent bidding strategies;prosumers;trading strategies;German","","2","","42","CCBY","26 Oct 2022","","","IEEE","IEEE Journals"
"Generation of Game Stages With Quality and Diversity by Reinforcement Learning in Turn-Based RPG","S. -G. Nam; C. -H. Hsueh; K. Ikeda","School of Information Science, Japan Advanced Institute of Science and Technology, Ishikawa, Japan; School of Information Science, Japan Advanced Institute of Science and Technology, Ishikawa, Japan; School of Information Science, Japan Advanced Institute of Science and Technology, Ishikawa, Japan","IEEE Transactions on Games","14 Sep 2022","2022","14","3","488","501","Many recent studies in procedural content generation (PCG) are based on machine learning. One of the promising approaches is generative models, which have shown impressive results in generating new pictures and videos from existing ones. However, it is usually costly to collect sufficient content for training on PCG. To address this issue, we consider reinforcement learning (RL), which does not need to collect training data in advance but learns from its interaction with an environment. In this work, RL agents are trained to generate stages, which we define as series of events in turn-based role playing games. It is a challenging task since several events in a stage are usually highly correlated to each other. We first formulate the stage generation problem into a Markov decision process. A hand-crafted evaluation function, which simulates players’ enjoyment, is defined to evaluate generated stages. Two RL algorithms are selected in the experiments, which are deep Q-network for discrete action space and deep deterministic policy gradient for continuous action space. The generated stages from both models receive evaluation values indicating good quality. To solve the delayed reward problem and further improve the quality of the stages, we employ virtual simulations (VS) to give rewards to intermediate actions and get stages with higher average scores. In addition, we introduce noise to avoid generating similar stages while trying to keep the quality as high as possible. The proposed methods succeed in generating good and diverse stages.","2475-1510","","10.1109/TG.2021.3113313","JSPS KAKENHI(grant numbers:JP18H03347,JP17K00506,JP20K12121); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9541052","Diversity;machine learning;procedural content generation (PCG);quality;reinforcement learning (RL);turn-based role playing games (RPG)","Games;Genetic algorithms;Reinforcement learning;Training data;Markov processes;Entertainment industry;Training","computer games;decision theory;learning (artificial intelligence);Markov processes","sufficient content;PCG;reinforcement learning;training data;RL agents;turn-based role playing games;stage generation problem;hand-crafted evaluation function;generated stages;RL algorithms;similar stages;good stages;diverse stages;game stages;turn-based RPG;procedural content generation;machine learning;generative models","","2","","59","IEEE","20 Sep 2021","","","IEEE","IEEE Journals"
"A reinforcement learning algorithm developed to model GenCo strategic bidding behavior in multidimensional and continuous state and action spaces","A. Y. F. Lau; D. Srinivasan; T. Reindl","National University of Singapore, Singapore, SG; Department of Electrical Computer Engineering, National University of Singapore, Singapore; Solar Energy Research Institute of Singapore, National University of Singapore, Singapore","2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)","30 Sep 2013","2013","","","116","123","The electricity market has provided a complex economic environment, and consequently has increased the requirement for advancement of learning methods. In the agent-based modeling and simulation framework of this economic system, the generation company's decision-making is modeled using reinforcement learning. Existing learning methods that model the generation company's strategic bidding behavior are not adapted to the non-stationary and non-Markovian environment involving multidimensional and continuous state and action spaces. This paper proposes a reinforcement learning method to overcome these limitations. The proposed method discovers the input space structure through the self-organizing map, exploits learned experience through Roth-Erev reinforcement learning and explores through the actor critic map. Simulation results from experiments show that the proposed method outperforms Simulated Annealing Q-Learning and Variant Roth-Erev reinforcement learning. The proposed method is a step towards more realistic agent learning in Agent-based Computational Economics.","2325-1867","978-1-4673-5925-2","10.1109/ADPRL.2013.6614997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614997","reinforcement learning;strategic bidding behavior;agent-based modeling;electricity market","Learning (artificial intelligence);Electricity supply industry;Adaptation models;Vectors;Computational modeling;Simulated annealing;Schedules","decision making;learning (artificial intelligence);power engineering computing;power generation economics;power markets;self-organising feature maps","nonMarkovian environment;nonstationary environment;agent-based computational economics;actor critic map;self-organizing map;generation company;decision-making;economic system;agent-based simulation framework;agent-based modeling framework;complex economic environment;electricity market;action spaces;continuous state;multidimensional state;GenCo strategic bidding behavior;Roth-Erev reinforcement learning algorithm","","1","","31","IEEE","30 Sep 2013","","","IEEE","IEEE Conferences"
"Customized Rebate Pricing Mechanism for Virtual Power Plants Using a Hierarchical Game and Reinforcement Learning Approach","W. Chen; J. Qiu; J. Zhao; Q. Chai; Z. Y. Dong","School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Science and Engineering and the Shenzhen Institute of Artificial Intelligence and Robotics for Society, The Chinese University of Hong Kong, Shenzhen, China; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, NSW, Australia; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore","IEEE Transactions on Smart Grid","22 Dec 2022","2023","14","1","424","439","In the transition to a two-sided electricity market, energy users are turning into prosumers who own the flexible distributed energy resources (DERs) and have the potential to provide services to the power system. Virtual power plants (VPPs) aggregate DERs to join the electricity market and respond to system signals. It is urgent to develop a new pricing mechanism for VPPs to allocate the payoff from the electricity market to prosumers. This paper proposes a customized rebate package pricing mechanism for a VPP retailer to reward prosumers for supporting the power system. The retailer’s pricing strategies are determined based on a Stackelberg game, considering the heterogeneous prosumers’ dynamic selecting process based on an evolutionary game. The extended replicator dynamics is proposed to take the future payoff into account and guarantee the evolutionary equilibrium. Moreover, a new reinforcement learning algorithm based on the Cross learning model is developed to solve the evolutionary game with less computational effort. The simulation results verify the effectiveness of the proposed customized rebate package pricing mechanism, which can efficiently reward prosumers’ flexible resources in supporting the system while maximizing the retailer’s utility to achieve a win-win outcome.","1949-3061","","10.1109/TSG.2022.3185138","Australian Research Council (ARC) Research Hub(grant numbers:IH180100020); ARC Training Center(grant numbers:IC200100023); ARC linkage Project(grant numbers:LP200100056); ARC(grant numbers:DP220103881); Shenzhen Institute of Artificial Intelligence and Robotics for Society (AIRS); National Natural Science Foundation of China (Key Program)(grant numbers:71931003,72061147004); National Natural Science Foundation of China(grant numbers:72171206); Australian Government Research Training Program Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9803252","Customized rebate package pricing;evolutionary game theory;extended replicator dynamics;heterogeneous users;reinforcement learning;virtual power plant","Pricing;Games;Vehicle dynamics;Electricity supply industry;Heuristic algorithms;Behavioral sciences;Virtual power plants","distributed power generation;energy resources;evolutionary computation;game theory;learning (artificial intelligence);power markets;pricing","Cross learning model;customized rebate package pricing mechanism;customized rebate pricing mechanism;DERs;energy users;evolutionary game;flexible distributed energy resources;heterogeneous prosumers;hierarchical game;power system;reinforcement learning algorithm;reinforcement learning approach;retailer;Stackelberg game;system signals;two-sided electricity market;virtual power plants;VPPs","","1","","53","IEEE","22 Jun 2022","","","IEEE","IEEE Journals"
"Multi-Agent Reinforcement Learning Aided Resources Allocation Method in Vehicular Networks","Y. Ji; X. Zhang; Y. Wang; H. Gacanin; H. Sari; F. Adachi; G. Gui","College of Telecommunications and Information Engineering, NJUPT, Nanjing, China; College of Telecommunications and Information Engineering, NJUPT, Nanjing, China; College of Telecommunications and Information Engineering, NJUPT, Nanjing, China; Institute for Communication Technologies and Embedded Systems, RWTH Aachen University, Aachen, Germany; College of Telecommunications and Information Engineering, NJUPT, Nanjing, China; International Research Institute of Disaster Science (IRIDeS), Tohoku University, Sendai, Japan; College of Telecommunications and Information Engineering, NJUPT, Nanjing, China","2022 IEEE 96th Vehicular Technology Conference (VTC2022-Fall)","18 Jan 2023","2022","","","1","5","To address the problem of spectrum resources and transmitting power for vehicular networks, this paper proposes a resource allocation (RA) method based on dueling double deep-Q network (D3QN) reinforcement learning (RL). Due to the high mobility of the vehicle, the channel changes rapidly which makes it difficult to accurately collect high-accuracy channel state information at the base station and to perform centralized management. In response of this difficulty, we construct a multi-intelligence model, using Manhattan Grid Layout City Model as the basis of environment and with each vehicle-to-vehicle (V2V) link as an intelligence. They work together to interact with the environment, receive appropriate observations, get rewards, and finally learn to improve the allocation of power and spectrum to enable users to achieve a better entertainment experience and a safer driving environment. Experimental results demonstrate that with proper training mechanism and reward function construction, cooperation among multiple intelligence can be performed in a distributed manner, with improvements in both the capacity of total vehicle-to-infrastructure links and the effective payload delivery success rate of the V2V links compared to common Q-network.","2577-2465","978-1-6654-5468-1","10.1109/VTC2022-Fall57202.2022.10012735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10012735","Vehicle networks;dueling double deep-Q network;multi-agent reinforcement learning;resources allocation","Training;Vehicular and wireless technologies;Vehicle-to-infrastructure;Simulation;Urban areas;Entertainment industry;Vehicular ad hoc networks","deep learning (artificial intelligence);intelligent networks;mobility management (mobile radio);multi-agent systems;radio spectrum management;reinforcement learning;resource allocation;traffic engineering computing;vehicular ad hoc networks;wireless channels","base station;D3QN reinforcement learning;deep-Q network reinforcement learning;dueling double deep-Q network reinforcement learning;high-accuracy channel state information;Manhattan grid layout city model;multiagent reinforcement learning aided resources allocation method;multiintelligence model;resource allocation method;spectrum resources;V2V link;vehicle-to-infrastructure links;vehicle-to-vehicle link;vehicular networks","","","","22","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"Integrating distributed Bayesian inference and reinforcement learning for sensor management","C. Grappiolo; S. Whiteson; G. Pavlin; B. Bakker","ISLA, University of Amsterdam, Amsterdam, Netherlands; ISLA, University of Amsterdam, Amsterdam, Netherlands; D-CIS Laboratory, Thales Research and Technology, Delft, Netherlands; D-CIS Laboratory, Thales Research and Technology, Delft, Netherlands","2009 12th International Conference on Information Fusion","18 Aug 2009","2009","","","93","101","This paper introduces a sensor management approach that integrates distributed Bayesian inference (DBI) and reinforcement learning (RL). DBI is implemented using distributed perception networks (DPNs), a multiagent approach to performing efficient inference, while RL is used to automatically discover a mapping from the beliefs generated by the DPNs to the actions that enable active sensors to gather the most useful observations. The resulting method is evaluated on a simulation of a chemical leak localization task and the results demonstrate 1) that the integrated approach can learn policies that perform effective sensor management, 2) that inference based on a correct observation model, which the DPNs make feasible, is critical to performance, and 3) that the system scales to larger versions of the task.","","978-0-9824-4380-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5203741","Sensor management;distributed Bayesian inference;reinforcement learning;POMDPs","Bayesian methods;Learning;Chemical sensors;Sensor phenomena and characterization;Sensor fusion;Technology management;Sensor systems;Conference management;Chemical industry;Helicopters","belief networks;inference mechanisms;learning (artificial intelligence);multi-agent systems;sensor fusion","distributed Bayesian inference;reinforcement learning;sensor management;distributed perception network;multiagent approach;mapping discovery;belief generation;chemical leak localization;policy learning","","","","18","","18 Aug 2009","","","IEEE","IEEE Conferences"
"Reinforcement Learning-based Electricity Market Vulnerability Analysis under Cyber-Topology Attack","A. Dey; M. V. Salapaka","Department of Electrical and Computer Engineering, University of Minnesota Twin Cities; Department of Electrical and Computer Engineering, University of Minnesota Twin Cities","2023 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT)","22 Mar 2023","2023","","","1","5","In this article, we propose a reinforcement learning (RL)-based methodology to analyze vulnerability of real-time (RT) electricity market under grid topology attack. In RT electricity market, the electricity prices at different buses in the grid network are decided based on the locational marginal price (LMP). LMP is derived from the solution to DC optimal power flow (DCOPF) which depends on the grid topology, generation cost, and real-time demand. Hence, an attacker can manipulate the topology information to alter the solution to DCOPF leading to alteration of LMPs, thus harnessing monetary profit. Our analysis entails realistic cyber-topology attack, that is, the attacker can only manipulate the breaker status, but not the physical topology, and it has no knowledge of the topology prior to attack. Under such cyber-topology attack, our proposed RL-based methodology identifies the critical breakers in the power network that, if attacked, can lead to large deviation in LMP from the actual value and disrupt the electricity market. We instantiate our proposed technique in IEEE-39 and 300 bus network and establish that the critical branches, identified by our algorithm, are crucial in terms of maintaining the stability of RT market, hence must be protected by the grid operator.","2472-8152","978-1-6654-5355-4","10.1109/ISGT51731.2023.10066378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10066378","Cyber-security;electricity market;locational marginal price;reinforcement learning;topology attack","Costs;Network topology;Reinforcement learning;Power system stability;Electricity supply industry;Real-time systems;Stability analysis","cyber-physical systems;load flow;power engineering computing;power grids;power markets;power system security;pricing;reinforcement learning","DC optimal power flow;DCOPF;electricity prices;grid network;grid operator;grid topology attack;LMP;locational marginal price;physical topology;real-time electricity market;realistic cyber-topology attack;reinforcement learning-based electricity market vulnerability analysis;reinforcement learning-based methodology;RL-based methodology;RT electricity market;RT market;topology information","","","","15","IEEE","22 Mar 2023","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning Resources Allocation Method Using Dueling Double Deep Q-Network in Vehicular Networks","Y. Ji; Y. Wang; H. Zhao; G. Gui; H. Gacanin; H. Sari; F. Adachi","College of Telecommunications and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Telecommunications and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Telecommunications and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Telecommunications and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; Institute for Communication Technologies and Embedded Systems, RWTH Aachen University, Aachen, Germany; College of Telecommunications and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; International Research Institute of Disaster Science (IRIDeS), Tohoku University, Sendai, Japan","IEEE Transactions on Vehicular Technology","17 Oct 2023","2023","72","10","13447","13460","The communications between vehicle-to-vehicle (V2V) with high frequency, group sending, group receiving and periodic lead to serious collision of wireless resources and limited system capacity, and the rapid channel changes in high mobility vehicular environments preclude the possibility of collecting accurate instantaneous channel state information at the base station for centralized resource management. For the Internet of Vehicles (IoV), it is a fundamental challenge to achieve low latency and high reliability communication for real-time data interaction over short distances in a complex wireless propagation environment, as well as to attenuate and avoid inter-vehicle interference in the region through a reasonable spectrum allocation. To solve the above problems, this paper proposes a resource allocation (RA) method using dueling double deep Q-network reinforcement learning (RL) with low-dimensional fingerprints and soft-update architecture (D3QN-LS) while constructing a multi-agent model based on a Manhattan grid layout urban virtual environment, with communication links between V2V links acting as agents to reuse vehicle-to-infrastructure (V2I) spectrum resources. In addition, we extend the amount of transmitted data in our work, while adding scenarios where spectrum resources are relatively scarce, i.e. the number of V2V links is significantly larger than the amount of spectrum, to compensate for some of the shortcomings in existing literature studies. We demonstrate that the proposed D3QN-LS algorithm leads to a further improvement in the total capacity of V2I links and the success rate of periodic secure message transmission in V2V links.","1939-9359","","10.1109/TVT.2023.3275546","Key Project of Natural Science Foundation of the Higher Education Institutions of Jiangsu Province(grant numbers:22KJA510002); Jiangsu Provincial Key Research and Development Program(grant numbers:BE2020084-5); National Key Research and Development Program of China(grant numbers:2019YFB2103004); Future Network Scientific Research Fund Project(grant numbers:FNSRFP-2021-ZD-8,FNSRFP-2021-YB-31); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10123947","Internet of vehicles;transmit power;spectrum allocation;multi-agent reinforcement learning","Resource management;Reliability;Reinforcement learning;Safety;Entertainment industry;Deep learning;Communication system security","","","","","","46","IEEE","12 May 2023","","","IEEE","IEEE Journals"
"Exploring Deep Reinforcement Learning for Battling in Collectible Card Games","R. S. Vieira; A. R. Tavares; L. Chaimowicz","Dep. de Ciência da Computação, Univ. Federal de Minas Gerais, Belo Horizonte, Brazil; Instituto de Informática, Univ. Federal do Rio Grande do Sul, Porto Alegre, Brazil; Dep. de Ciência da Computação, Univ. Federal de Minas Gerais, Belo Horizonte, Brazil","2022 21st Brazilian Symposium on Computer Games and Digital Entertainment (SBGames)","28 Nov 2022","2022","","","1","6","Collectible card games (CCGs), such as Magic: the Gathering and Hearthstone, are a challenging domain where game-playing AI arguably has not yet reached human-level performance. We propose a deep reinforcement learning approach to battling in CCGs, using Legends of Code and Magic, a CCG designed for AI research, as a testbed. To do so, we formulate the battles as a Markov decision process, train agents to solve it, and evaluate them against two existing agents of different skill levels. Contrasting with the current state-of-the-art, our resulting agents act fast and can play many battles per second, despite their limited performance. We identify limitations and discuss several promising directions for improvement.","2159-6662","978-1-6654-6156-6","10.1109/SBGAMES56371.2022.9961110","CAPES; CNPq; Fapemig; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9961110","collectible card games;reinforcement learning;artificial intelligence","Deep learning;Video games;Codes;Entertainment industry;Games;Reinforcement learning;Markov processes","computer games;decision theory;deep learning (artificial intelligence);Markov processes;multi-agent systems;reinforcement learning","agent training;battling;CCGs;collectible card games;deep reinforcement learning;game-playing AI;human-level performance;Legends of Code and Magic;Markov decision process;skill levels","","","","30","IEEE","28 Nov 2022","","","IEEE","IEEE Conferences"
"Systematic choice of video game benchmarks in Deep Reinforcement Learning","É. Gomes; M. Souza","Institute of Mathematics and Statistics, Federal University of Bahia, UFBA, Salvador, Brazil; Institute of Mathematics and Statistics, Federal University of Bahia, UFBA, Salvador, Brazil","2021 20th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames)","20 Dec 2021","2021","","","162","171","Deep Reinforcement Learning has gained much attention due to results obtained by its methods to problems of high dimensionality, which were previously intractable or difficult to solve. In this context, video games have been widely used as experimental environments and benchmarks for the evaluation of reinforcement learning algorithms, as well as guiding the development of new methods. Although a lot has been done in Deep Reinforcement Learning since the proposal of its seminal work, little has been discussed about proper methodologies for constructing such evaluation benchmarks. This paper proposes to systematize the choice of video games to be used as a benchmark guaranteeing representativeness and diversity of learning environments based on the use of video game typologies proposed in the area of Game Design Research.","2159-6662","978-1-6654-0189-0","10.1109/SBGames54170.2021.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9637632","Deep Reinforcement Learning;Benchmark;video games","Systematics;Entertainment industry;Games;Reinforcement learning;Benchmark testing;Computational efficiency;Proposals","computer games;deep learning (artificial intelligence)","evaluation benchmarks;video game typologies;systematic choice;video game benchmarks;deep reinforcement learning;game design research","","","","13","IEEE","20 Dec 2021","","","IEEE","IEEE Conferences"
"Balancing the Performance of a FightingICE Agent using Reinforcement Learning and Skilled Experience Catalogue","A. Cherukuri; F. G. Glavin","Computer Science and Engineering, Indian Institute of Technology, Bombay; School of Computer Science, National University of Ireland, Galway","2022 IEEE Games, Entertainment, Media Conference (GEM)","20 Jan 2023","2022","","","1","6","Dynamic Difficulty Adjustment (DDA) is the process of changing the challenge offered dynamically based on the player's performance, as opposed to the player manually choosing the difficulty from a set of options. This helps in alleviating player frustration by having the opponents' skill match that of the player's. In this work, we propose a novel application of a DDA technique called Skilled Experience Catalogue (SEC) which has previously been used with success in First Person Shooter games. This approach uses experiential milestones of the learning process of an agent trained using Reinforcement Learning (RL). We have designed and implemented a custom SEC on top of the FightingICE platform that is used in the Fighting Game Artificial Intelligence (FTGAI) competition. We deployed our SEC agent against three fixed-strategy opponents and showed that we could successfully balance the game-play in two out of the three opponents over 150 games against each. Balancing was not achieved against the third opponent since the RL agent could not reach the required skill level after its initial training.","2766-6530","978-1-6654-6138-2","10.1109/GEM56474.2022.10017566","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10017566","Artificial Intelligence;Reinforcement Learning;Dynamic Difficulty Adjustment","Training;Garnets;Entertainment industry;Games;Reinforcement learning;Media;Real-time systems","computer games;reinforcement learning","DDA technique;dynamic difficulty adjustment;fighting game artificial intelligence;fightingICE agent;first person shooter games;FTGAI;game-play;reinforcement learning;RL agent;SEC agent;skilled experience catalogue","","","","24","IEEE","20 Jan 2023","","","IEEE","IEEE Conferences"
"Procedural Content Generation using Reinforcement Learning and Entropy Measure as Feedback","P. V. M. Dutra; S. M. Villela; R. F. Neto","Department of Computer Science, Federal University of Juiz de Fora, Juiz de Fora, Minas Gerais, Brazil; Department of Computer Science, Federal University of Juiz de Fora, Juiz de Fora, Minas Gerais, Brazil; Department of Computer Science, Federal University of Juiz de Fora, Juiz de Fora, Minas Gerais, Brazil","2022 21st Brazilian Symposium on Computer Games and Digital Entertainment (SBGames)","28 Nov 2022","2022","","","1","6","In this work, we investigate how we can approach procedural content generation with reinforcement learning and mixed-initiative design. A second question discussed here is how we can use metrics to evaluate the diversity of the generated level. Our proposal has as its main hypothesis to use scenario models, provided by an expert human level designer specialist, for the reinforcement learning agents in order to generate new scenarios. The levels provided by the specialist are separated into segments or blocks that are used to compose the new scenario structures. Also, a new reward function based on the use of entropy was proposed to measure the diversity of the generated scenarios. Initially, we trained our model for three different 2D Dungeon crawlers game environments. We analyzed our results through the value of the entropy, and it shows that our approach can generate wide levels with a diversity of segments.","2159-6662","978-1-6654-6156-6","10.1109/SBGAMES56371.2022.9961076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9961076","procedural content generation;reinforcement learning;entropy;machine learning","Measurement;Video games;Crawlers;Entertainment industry;Reinforcement learning;Games;Entropy","computer games;entropy;reinforcement learning","2D Dungeon crawlers game environments;entropy measure;expert human level designer specialist;mixed-initiative design;procedural content generation;reinforcement learning;scenario generation;scenario structures","","","","18","IEEE","28 Nov 2022","","","IEEE","IEEE Conferences"
"Energy Storage Arbitrage in Day-Ahead Electricity Market Using Deep Reinforcement Learning","T. Zonjee; S. S. Torbaghan","Environmental Technology, Wageningen University & Research, Wageningen, The Netherlands; Environmental Technology, Wageningen University & Research, Wageningen, The Netherlands","2023 IEEE Belgrade PowerTech","9 Aug 2023","2023","","","1","7","Large scale integration of renewable and distributed energy resources increases the need for flexibility on all levels of the energy value chain. Energy storage systems are considered as a major source of flexibility. They can help with maintaining a secure and reliable grid operation. The problem is that these technologies are capital intensive and therefore, there is a need for new algorithms that enable arbitrage while ensuring financial feasibility. To this end, in this research, we develop a constrained deep Q-learning based bidding algorithm to determine the optimal bidding strategy in the day-ahead electricity market. The proposed algorithm ensures compliance to energy storage system constraints. It takes imperfect, yet reasonably accurate, $\mathbf{24}$ -hour-ahead price forecast data as an input and returns the optimal bidding strategy as output. The numerical results and the sensitivity analysis show that the proposed algorithm effectively contains the impact of price forecast uncertainty to guarantee financial feasibility.","","978-1-6654-8778-8","10.1109/PowerTech55446.2023.10202674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10202674","Energy Storage;Energy Arbitrage;Deep Reinforcement Learning;Deep Q-Network;Day-Ahead Electricity Market","Renewable energy sources;Electric potential;Q-learning;Sensitivity analysis;Electricity supply industry;Forecast uncertainty;Large scale integration","deep learning (artificial intelligence);financial management;load forecasting;power engineering computing;power grids;power markets;pricing;reinforcement learning;sensitivity analysis;tendering","ahead price forecast data;constrained deep Q-learning based bidding algorithm;day-ahead electricity market;deep reinforcement learning;distributed energy resources;energy storage arbitrage;energy value chain;financial feasibility;reliable grid operation;renewable energy resources;secure grid operation;sensitivity analysis","","","","36","IEEE","9 Aug 2023","","","IEEE","IEEE Conferences"
"Market-based Coordinated Operation between Aggregators and Microgrids using Multi-agent Reinforcement Learning","W. Luan; B. Xu; B. Zhao","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China","2022 China International Conference on Electricity Distribution (CICED)","2 Nov 2022","2022","","","114","118","Local electricity market (LEM) has provided an effective solution to the coordination of demand-side resources (DSRs). However, network constraints violations and imbalanced budget problem hinder the application of LEM. To this end, this paper proposes a novel market based coordinated management approach. Firstly, flexible demand aggregators are modeled based on virtual battery model to facilitate the use of small-scale DSRs. Secondly, to deal with the extra expense of LEM caused by network losses, an improved pricing mechanism considering fair loss allocation is proposed. Thirdly, to realize distributed trading decision making while satisfying the network constraints, a multi-agent safe reinforcement learning method is adopted. Case studies under real-world settings validate the effectiveness of the proposed method for the safe and economic coordination of DSRs.","2161-749X","978-1-6654-5268-7","10.1109/CICED56215.2022.9928843","National Key Research and Development Program of China(grant numbers:2021YFB2401203); National Natural Science Foundation of China(grant numbers:National Natural Science Foundation of China); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9928843","demand side resource;local electricity market;multi-agent reinforcement learning;load aggregator;microgrid","Economics;Decision making;Reinforcement learning;Pricing;Microgrids;Electricity supply industry;Batteries;Resource management;Energy management","battery powered vehicles;decision making;demand side management;distributed power generation;multi-agent systems;power engineering computing;power markets;power system management;pricing;reinforcement learning","LEM;demand-side resources;network constraints violations;imbalanced budget problem;coordinated management approach;virtual battery model;network losses;multiagent safe reinforcement learning method;economic coordination;small-scale DSR;electricity market-based coordinated operation;pricing mechanism;distributed trading decision making;microgrid","","","","17","IEEE","2 Nov 2022","","","IEEE","IEEE Conferences"
"A Critical Study on Multi-agent System Based on Reinforcement Learning Theory and its Application in Research of Electricity Market Simulation","Y. Song; Z. Jing; Z. Gan","South China University of Technology, School of Electric Power Engineering, Guangzhou, China; South China University of Technology, School of Electric Power Engineering, Guangzhou, China; South China University of Technology, School of Electric Power Engineering, Guangzhou, China","2021 IEEE 4th International Electrical and Energy Conference (CIEEC)","17 Aug 2021","2021","","","1","7","Multi-agent system (MAS) based on reinforcement learning (RL) theory is an important means to design and evaluate electricity market rules. It can simulate operations of electricity market and activities of market subjects better. RL is a learning algorithm that maps uncertain information to behaviors through the perception of intelligent system environment. MAS is a complex system that transforms the economic model into a system composed of interacting agents. The research of reinforcement learning based multi-agent system (RLMAS) and its application has been paid more and more attention in the field of electricity market simulation. This paper first introduces basic ideas and algorithms of RLMAS, and then summarizes the main applications of RLMAS theory in market operation, strategy choices of market agents, electricity price forecasting and energy internet system. Finally, the prospect of its application in electricity market is prospected.","","978-1-7281-7149-4","10.1109/CIEEC50170.2021.9510406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9510406","artificial intelligence;reinforcement learning(RL);multi-agent system (MAS);electricity market simulation;energy internet system","Analytical models;Reinforcement learning;Learning (artificial intelligence);Transforms;Reliability theory;Electricity supply industry;Reliability engineering","learning (artificial intelligence);multi-agent systems;power engineering computing;power markets;pricing","MAS;reinforcement learning theory;RL;intelligent system environment;complex system;interacting agents;multiagent system;electricity market simulation;RLMAS theory;market operation;market agents;electricity price forecasting;energy internet system","","","","43","IEEE","17 Aug 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Online Resource Allocation in IoT Networks: Technology, Development, and Future Challenges","P. Cheng; Y. Chen; M. Ding; Z. Chen; S. Liu; Y. -P. P. Chen","La Trobe University, Australia; Fuzhou University, China; CSIRO DATA61, Australia; CSIRO DATA61, Australia; University of Sydney, Australia; La Trobe University, Australia","IEEE Communications Magazine","19 Jun 2023","2023","61","6","111","117","The growing number of complex and heterogeneous Internet of Things (IoT) applications has imposed a high demand for scarce communications and computing resources. To meet this stringent requirement, it is desirable to develop large-scale highly adaptive online resource allocation strategies to streamline existing network operations. Deep reinforcement learning (DRL), which combines the merits of reinforcement learning and deep learning, is capable of addressing complex decision-making tasks, thus enabling efficient online resource allocation. In this article, we present a DRL-based resource allocation framework. We begin a discussion on DRL basics and review its several recent applications. Then, we develop two new DRL algorithms that facilitate unlocking the potential of DRL and offer viable solutions to many more complex resource allocation problems. The first one tackles an optimization problem exposed to mixed (discrete and continuous) action spaces and bound by a number of highly non-linear quality-of-service (QoS) constraints. The second one extends the single-agent DRL to a more challenging multi-agent DRL by introducing a novel semi-distributed architecture. Finally, we discuss the challenges and future visions of applying DRL to real-world IoT networks.","1558-1896","","10.1109/MCOM.001.2200526","Australian Research Council (ARC)(grant numbers:DP210103410,DP220101634); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155733","","Deep learning;Industries;Decision making;Reinforcement learning;Quality of service;Computer architecture;Resource management","computer network security;decision making;deep learning (artificial intelligence);Internet of Things;multi-agent systems;optimisation;quality of service;reinforcement learning;resource allocation;telecommunication computing","complex decision-making tasks;complex Internet;complex resource allocation problems;computing resources;deep learning;deep reinforcement learning;DRL algorithms;DRL basics;DRL-based resource allocation framework;efficient online resource allocation;existing network operations;future visions;heterogeneous Internet;large-scale highly adaptive online resource allocation strategies;multiagent DRL;real-world IoT networks;scarce communications;single-agent DRL;stringent requirement;Things applications","","","","15","IEEE","19 Jun 2023","","","IEEE","IEEE Magazines"
"Deep Reinforcement Learning for Multi-Objective Resource Allocation in Multi-Platoon Cooperative Vehicular Networks","Y. Xu; K. Zhu; H. Xu; J. Ji","College of Computer and Information, Hohai University, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Electrical and Computer Engineering, University of Victoria, Victoria, Canada","IEEE Transactions on Wireless Communications","11 Sep 2023","2023","22","9","6185","6198","Grouping vehicles into platoons is a promising cooperative driving scenario to enhance the traffic safety and capacity of future vehicular networks. However, fast changing channel conditions in multi-platoon vehicular networks cause tremendous uncertainty for resource allocation. In addition, the unprecedented proliferation of various emerging vehicle-to-infrastructure (V2I) applications may result in some service demands with conflicting quality of experience. In this paper, we formulate a multi-objective resource allocation problem, which maximizes the transmission success ratio of intra-platoon communications and the mean opinion score (MOS) of V2I communication links. To efficiently solve this multi-objective optimization problem, we resort to a deep reinforcement learning (DRL) framework. Specifically, we divide it into a set of scalar optimization subproblems based on the weighted sum approach and model each one as a partially observable stochastic game (P-OSG), where each platoon acts as an agent and the actions taken by all platoons correspond to the resource allocation solution. We further propose a contribution-based dual-clip proximal policy optimization (CD-PPO) algorithm to deal with each subproblem, which is a DRL algorithm based on the actor-critic framework. The network parameters of all subproblems are then optimized collaboratively by using the proposed training algorithm and the neighborhood parameter transfer strategy. The desired Pareto front is obtained when all subproblems are solved. Simulation results reveal that the proposed algorithm can outperform other algorithms in terms of the MOS and transmission success ratio.","1558-2248","","10.1109/TWC.2023.3240425","National Natural Science Foundation of China(grant numbers:62071230,62061146002); Natural Science Foundation of Jiangsu Province(grant numbers:BK20211567); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10040608","Multi-platoon vehicular networks;resource allocation;deep reinforcement learning;dual-clip proximal policy optimization","Resource management;Optimization;Reinforcement learning;Reliability;Entertainment industry;Uncertainty;Streaming media","cooperative communication;deep learning (artificial intelligence);mobile radio;Pareto optimisation;reinforcement learning;resource allocation;stochastic games;telecommunication computing;vehicular ad hoc networks","CD-PPO;contribution-based dual-clip proximal policy optimization;cooperative driving scenario;deep reinforcement learning framework;DRL algorithm;intra-platoon communications;mean opinion score;multiobjective optimization problem;multiobjective resource allocation problem;multiplatoon vehicular networks;Pareto front;partially observable stochastic game;quality of experience;scalar optimization subproblems;service demands;traffic safety;transmission success ratio;V2I communication links;vehicle-to-infrastructure;weighted sum approach","","","","47","IEEE","7 Feb 2023","","","IEEE","IEEE Journals"
"Multi-agent Simulation for Strategic Bidding in Electricity Markets Using Reinforcement Learning","J. Wang; J. Wu; X. Kong","Key Laboratory of Smart Grid of Ministry of Education, Tianjin University, Tianiin, China; Key Laboratory of Smart Grid of Ministry of Education, Tianjin University, Tianiin, China; Key Laboratory of Smart Grid of Ministry of Education, Tianjin University, Tianiin, China","CSEE Journal of Power and Energy Systems","5 Jun 2023","2023","9","3","1051","1065","In this paper, a theoretical framework of Multi-agent Simulation (MAS) is proposed for strategic bidding in electricity markets using reinforcement learning, which consists of two parts: one is a MAS system used to simulate the competitive bidding of the actual electricity market; the other is an adaptive learning strategy bidding system used to provide agents with more intelligent bidding strategies. An Experience-Weighted Attraction (EWA) reinforcement learning algorithm (RLA) is applied to the MAS model and a new MAS method is presented for strategic bidding in electricity markets using a new Improved EWA (IEWA). From both qualitative and quantitative perspectives, it is compared with three other MAS methods using the Roth-Erev (RE), Q-learning and EWA. The results show that the performance of the MAS method using IEWA is proved to be better than the others. The four MAS models using four RLAs are built for strategic bidding in electricity markets. Through running the four MAS models, the rationality and correctness of the four MAS methods are verified for strategic bidding in electricity markets using reinforcement learning.","2096-0042","","10.17775/CSEEJPES.2020.02820","National Key Research and Development Program of China(grant numbers:2016YFB0901104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606939","Electricity market;multi-agent simulation;reinforcement learning;strategic bidding","Electricity supply industry;Reinforcement learning;Games;Economics;Heuristic algorithms;Prediction algorithms;Mathematical models","learning (artificial intelligence);multi-agent systems;power markets;reinforcement learning","actual electricity market;adaptive learning strategy bidding system;competitive bidding;electricity markets;Experience-Weighted Attraction reinforcement learning algorithm;intelligent bidding strategies;MAS method;MAS model;MAS system;Multiagent Simulation;strategic bidding","","","","53","","9 Nov 2021","","","CSEE","CSEE Journals"
"Gym Hero: A Research Environment for Reinforcement Learning Agents in Rhythm Games","R. F. F. Filho; Y. L. B. Nogueira; C. A. Vidal; J. B. Cavalcante-Neto; P. B. de Sousa Serafim","Teleinformatics Engineering Department (DETI), Federal University of Ceará (UFC), Fortaleza, Brazil; Department of Computing (DC), Federal University of Ceará (UFC), Fortaleza, Brazil; Department of Computing (DC), Federal University of Ceará (UFC), Fortaleza, Brazil; Department of Computing (DC), Federal University of Ceará (UFC), Fortaleza, Brazil; Instituto Atlântico, Fortaleza, Brazil","2021 20th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames)","20 Dec 2021","2021","","","87","96","This work presents a Reinforcement Learning environment, called Gym Hero, based on the game Guitar Hero. It consists of a similar game implementation, developed using the graphics engine PyGame, with four difficulty levels, and able to randomly generate tracks. On top of the game, we implemented a Gym environment to train and evaluate Reinforcement Learning agents. In order to assess the environment's capacity as a suitable learning tool, we ran a set of experiments to train three autonomous agents using Deep Reinforcement Learning. Each agent was trained on a different level using Deep Q-Networks, a technique that combines Reinforcement Learning with Deep Neural Networks. The input of the network is only the pixels of the screen. We show that the agents were capable of learning the expected behaviors to play the game. The obtained results validate the proposed environment as capable of evaluating autonomous agents on Reinforcement Learning tasks.","2159-6662","978-1-6654-0189-0","10.1109/SBGames54170.2021.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9637691","autonomous agents;reinforcement learning;deep learning;reinforcement learning environments;rhythm games;guitar hero","Training;Graphics;Electronic learning;Entertainment industry;Reinforcement learning;Games;Rhythm","computer games;learning (artificial intelligence)","reinforcement learning agents;rhythm games;game Guitar Hero;graphics engine PyGame;Gym environment;learning tool;autonomous agents;deep Q-networks;deep neural networks;Gym Hero;deep reinforcement learning","","","","29","IEEE","20 Dec 2021","","","IEEE","IEEE Conferences"
"A Framework For Critical Infrastructure Monitoring Based On Deep Reinforcement Learning Approach","K. Yunana; I. O. Oyefolahan; S. A. Bashir","Department of Computer Science, Nasarawa State University, Keffi, Nigeria; Department of Information Technology, Federal University of Technology, Minna, Nigeria; Department of Computer Science, Federal University of Technology, Minna, Nigeria","2022 5th Information Technology for Education and Development (ITED)","2 Mar 2023","2022","","","1","6","Critical Infrastructure (CI) are nowadays linked with IOT devices that communicate data through networks to achieve significant collaboration. With the progress in internet connectivity, IOT has disrupt numerous aspects of CI comprising communication systems, power plants, power grid, gas pipeline, and transportation systems. As a disruptive paradigm, the IOT and Cloud computing utilizing Smart IOT devices equipped with numerous sensors and actuating capabilities play significant roles when deployed in CI surroundings with the aim of monitoring vital observable figures consisting of flow rate, temperature, pressure, and lighting situations. Over the years, oil pipeline infrastructure have been the main economic means for conveying refined oil to assembly and distribution outlets. Though damages to the pipelines in this area by exclusion have influence the normal transport of refined oil to the outlets across the country like Nigeria which has influence the stream of income and damages to the environment. Reinforcement Learning (RL) approach for infrastructure reliability monitoring have receive numerous consideration by researchers denoting that RL centered policy reveals superior operation than regular traditional control systems strategies. Many of the studies utilised mainly algorithms for environment with discrete action and observation spaces unlike others with infinite state space. This study proposed a framework for critical infrastructure monitoring based on Deep Reinforcement Learning (DRL) for oil pipeline network and also developed a pipeline network monitoring (PNM) architecture with expression of the environment dynamics as Markov Decision Process. The sample observation space data and strategy for evaluation of the framework was also presented.","","978-1-6654-9370-3","10.1109/ITED56637.2022.10051520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10051520","Critical Infrastructure;Monitoring;Framework and Deep Reinforcement Learning","Temperature sensors;Deep learning;Oils;Pipelines;Transportation;Reinforcement learning;Critical infrastructure","cloud computing;computer network security;critical infrastructures;deep learning (artificial intelligence);Internet;Internet of Things;Markov processes;pipelines;power grids;reinforcement learning","assembly;CI comprising communication systems;CI surroundings;Cloud computing;critical infrastructure monitoring;Deep Reinforcement Learning approach;disruptive paradigm;distribution outlets;gas pipeline;infrastructure reliability monitoring;internet connectivity;main economic means;normal transport;numerous aspects;numerous consideration;numerous sensors;oil pipeline infrastructure;oil pipeline network;pipeline network monitoring architecture;pipelines;power grid;power plants;refined oil;regular traditional control systems strategies;sample observation space data;significant collaboration;significant roles;Smart IOT devices;transportation systems;vital observable figures","","","","32","IEEE","2 Mar 2023","","","IEEE","IEEE Conferences"
"Model-Based Reinforcement Learning via Stochastic Hybrid Models","H. Abdulsamad; J. Peters","Department of Electrical Engineering and Automation, Aalto University, Espoo, Finland; Department of Computer Science, Technical University of Darmstadt, Darmstadt, Germany","IEEE Open Journal of Control Systems","20 Jun 2023","2023","2","","155","170","Optimal control of general nonlinear systems is a central challenge in automation. Enabled by powerful function approximators, data-driven approaches to control have recently successfully tackled challenging applications. However, such methods often obscure the structure of dynamics and control behind black-box over-parameterized representations, thus limiting our ability to understand closed-loop behavior. This article adopts a hybrid-system view of nonlinear modeling and control that lends an explicit hierarchical structure to the problem and breaks down complex dynamics into simpler localized units. We consider a sequence modeling paradigm that captures the temporal structure of the data and derive an expectation-maximization (EM) algorithm that automatically decomposes nonlinear dynamics into stochastic piecewise affine models with nonlinear transition boundaries. Furthermore, we show that these time-series models naturally admit a closed-loop extension that we use to extract local polynomial feedback controllers from nonlinear experts via behavioral cloning. Finally, we introduce a novel hybrid relative entropy policy search (Hb-REPS) technique that incorporates the hierarchical nature of hybrid models and optimizes a set of time-invariant piecewise feedback controllers derived from a piecewise polynomial approximation of a global state-value function.","2694-085X","","10.1109/OJCSYS.2023.3277308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128705","Bayesian inference;behavioral cloning;expectation-maximization;hidden Markov models;hybrid models;piecewise feedback control;reinforcement learning;system identification","Hidden Markov models;Switches;Behavioral sciences;Bayes methods;Stochastic processes;Nonlinear dynamical systems;Reinforcement learning","closed loop systems;entropy;feedback;function approximation;nonlinear control systems;optimal control;polynomial approximation;polynomials;reinforcement learning","behavioral cloning;closed-loop behavior;closed-loop extension;complex dynamics;expectation-maximization algorithm;explicit hierarchical structure;function approximators;general nonlinear systems;global state-value function;hierarchical nature;hybrid relative entropy policy search;hybrid-system view;local polynomial feedback controllers;model-based reinforcement learning;nonlinear dynamics;nonlinear experts;nonlinear modeling;nonlinear transition boundaries;optimal control;over-parameterized representations;piecewise polynomial approximation;sequence modeling paradigm;simpler localized units;stochastic hybrid models;stochastic piecewise affine models;temporal structure;time-invariant piecewise feedback controllers;time-series models","","","","91","CCBY","17 May 2023","","","IEEE","IEEE Journals"
"An Edge-Cloud Integrated Solution for Buildings Demand Response Using Reinforcement Learning","X. Zhang; D. Biagioni; M. Cai; P. Graf; S. Rahman","Computational Science Center, U.S. National Renewable Energy Laboratory, Golden, CO, USA; Computational Science Center, U.S. National Renewable Energy Laboratory, Golden, CO, USA; Virginia Tech, Advanced Research Institute, Arlington, VA, USA; Computational Science Center, U.S. National Renewable Energy Laboratory, Golden, CO, USA; Virginia Tech, Advanced Research Institute, Arlington, VA, USA","IEEE Transactions on Smart Grid","21 Dec 2020","2021","12","1","420","431","Buildings, as major energy consumers, can provide great untapped demand response (DR) resources for grid services. However, their participation remains low in real-life. One major impediment for popularizing DR in buildings is the lack of cost-effective automation systems that can be widely adopted. Existing optimization-based smart building control algorithms suffer from high costs on both building-specific modeling and on-demand computing resources. To tackle these issues, this paper proposes a cost-effective edge-cloud integrated solution using reinforcement learning (RL). Beside RL's ability to solve sequential optimal decision-making problems, its adaptability to easy-to-obtain building models and the off-line learning feature are likely to reduce the controller's implementation cost. Using a surrogate building model learned automatically from building operation data, an RL agent learns an optimal control policy on cloud infrastructure, and the policy is then distributed to edge devices for execution. Simulation results demonstrate the control efficacy and the learning efficiency in buildings of different sizes. A preliminary cost analysis on a 4-zone commercial building shows the annual cost for optimal policy training is only 2.25% of the DR incentive received. Results of this study show a possible approach with higher return on investment for buildings to participate in DR programs.","1949-3061","","10.1109/TSG.2020.3014055","National Renewable Energy Laboratory, operated by Alliance for Sustainable Energy, LLC, for the U.S. Department of Energy (DOE)(grant numbers:DE-AC36-08GO28308); Assessment of Reinforcement Learning for Model NREL Problems Project; National Renewable Energy Laboratory’s Laboratory Directed Research and Development Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9161266","Demand response;reinforcement learning;smart building;air-conditioning;cloud computing","Buildings;Computational modeling;Optimal control;Cloud computing;Learning (artificial intelligence);Training;Load management","building management systems;cloud computing;control engineering computing;decision making;demand side management;learning (artificial intelligence);optimal control;optimisation","DR;cost-effective automation systems;existing optimization-based smart building control algorithms;building-specific modeling;on-demand computing resources;cost-effective edge-cloud integrated solution;reinforcement learning;sequential optimal decision-making problems;off-line learning feature;controller;surrogate building model;optimal control policy;learning efficiency;preliminary cost analysis;4-zone commercial building;annual cost;optimal policy training;buildings demand response;great untapped demand response resources","","62","","37","IEEE","6 Aug 2020","","","IEEE","IEEE Journals"
"Driving Decision and Control for Automated Lane Change Behavior based on Deep Reinforcement Learning","T. Shi; P. Wang; X. Cheng; C. -Y. Chan; D. Huang","Beijing Institute of Technology, Beijing; California PATH, University of California, Berkeley, Richmond, CA, US; Beijing Institute of Technology, Beijing; California PATH, University of California, Berkeley, Richmond, CA, US; Chinese Academy of Launch Vehicle Techonology","2019 IEEE Intelligent Transportation Systems Conference (ITSC)","28 Nov 2019","2019","","","2895","2900","To fulfill high-level automation, an automated vehicle needs to learn to make decisions and control its movement under complex scenarios. Due to the uncertainty and complexity of the driving environment, most classical rule-based methods cannot solve the problem of complicated decision tasks. Deep reinforcement learning has demonstrated impressive achievements in many fields such as playing games and robotics. However, a direct application of reinforcement learning algorithm for automated driving still face challenges in handling complex driving tasks. In this paper, we proposed a hierarchical reinforcement learning based architecture for decision making and control of lane changing situations. We divided the decision and control process into two correlated processes: 1) when to conduct lane change maneuver and 2) how to conduct the maneuver. To be specific, we first apply Deep Q-network (DQN) to decide when to conduct the maneuver based on the consideration of safety. Subsequently, we design a Deep Q-learning framework with quadratic approximator for deciding how to complete the maneuver in longitudinal direction (e.g. adjust to the selected gap or just follow the preceding vehicle). Finally, a polynomial lane change trajectory is generated and Pure Pursuit Control is implemented for path tracking for the lane change situation. We demonstrate the effectiveness of this framework in simulation, from both the decision-making and control layers.","","978-1-5386-7024-8","10.1109/ITSC.2019.8917392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917392","","Decision making;Machine learning;Task analysis;Learning (artificial intelligence);Robustness;Safety;Trajectory","behavioural sciences computing;collision avoidance;decision making;learning (artificial intelligence);neural nets;road safety;road traffic control;road vehicles;traffic engineering computing;trajectory control","control layers;automated lane change behavior;Deep reinforcement learning;high-level automation;automated vehicle;uncertainty;driving environment;classical rule-based methods;complicated decision tasks;reinforcement learning algorithm;automated driving;complex driving tasks;hierarchical reinforcement;decision making;lane changing situations;lane change maneuver;Deep Q-network;Deep Q-learning framework;polynomial lane change trajectory;lane change situation;decision-making;pure pursuit control","","38","","13","IEEE","28 Nov 2019","","","IEEE","IEEE Conferences"
"Optimization for Reinforcement Learning: From a single agent to cooperative agents","D. Lee; N. He; P. Kamalaruban; V. Cevher","Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana; Department of Industrial and Enterprise Systems Engineering and Coordinated Science Laboratory, University of Illinois at Urbana–Champaign; Postdoctoral Researcher, Ecole Polytechnique fédérale de Lausanne, Switzerland; Electrical and Computer Engineering Department, Rice University, Houston","IEEE Signal Processing Magazine","1 May 2020","2020","37","3","123","135","Fueled by recent advances in deep neural networks, reinforcement learning (RL) has been in the limelight because of many recent breakthroughs in artificial intelligence, including defeating humans in games (e.g., chess, Go, StarCraft), self-driving cars, smart-home automation, and service robots, among many others. Despite these remarkable achievements, many basic tasks can still elude a single RL agent. Examples abound, from multiplayer games, multirobots, cellular-antenna tilt control, traffic-control systems, and smart power grids to network management.","1558-0792","","10.1109/MSP.2020.2976000","NSF(grant numbers:NSF-CRII-1755829); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9084325","","Signal processing algorithms;Optimization;Approximation algorithms;Markov processes;Function approximation;Task analysis","learning (artificial intelligence);multi-agent systems;neural nets;optimisation","artificial intelligence;self-driving cars;smart-home automation;service robots;RL agent;multiplayer games;reinforcement learning;deep neural networks;cooperative agents;cellular-antenna tilt control;traffic-control systems;smart power grids;network management","","37","","60","IEEE","1 May 2020","","","IEEE","IEEE Magazines"
"Market-Based Model in CR-IoT: A Q-Probabilistic Multi-Agent Reinforcement Learning Approach","D. Wang; W. Zhang; B. Song; X. Du; M. Guizani","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Department of Computer and Information Sciences, Temple University, Philadelphia, USA; Department of Computer Science and Engineering, Qatar University, Doha, Qatar","IEEE Transactions on Cognitive Communications and Networking","6 Mar 2020","2020","6","1","179","188","The ever-increasing urban population and the corresponding material demands have brought unprecedented burdens to cities. To guarantee better QoS for citizens, smart cities leverage emerging technologies such as the Cognitive Radio Internet of Things (CR-IoT). However, resource allocation is a great challenge for CR-IoT, mainly because of the extremely numerous devices and users. Generally, the auction theory and game theory are applied to overcome the challenge. In this paper, we propose a multi-agent reinforcement learning (MARL) algorithm to learn the optimal resource allocation strategy in the oligopoly market model. Firstly, we model a multi-agent scenario with the primary users (PUs) as sellers and secondary users (SUs) as buyers. Then, we propose the Q-probabilistic multi-agent learning (QPML) and apply it to allocate resources in the market. In the multi-agent learning process, the PUs and SUs learn strategies to maximize their benefits and improve spectrum utilization. The performance of QPML is compared with Learning Automation (LA) through simulations. The experimental results show that our approach outperforms other approaches and performs well.","2332-7731","","10.1109/TCCN.2019.2950242","National Natural Science Foundation of China(grant numbers:61772387); Fundamental Research Funds of Ministry of Education and China Mobile(grant numbers:MCM20170202); Natural Science Foundation of Shaanxi Province(grant numbers:2019ZDLGY03-03); ISN State Key Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8887219","CR-IoT;MARL;resource allocation;market model","Resource management;Game theory;Games;Economics;Reinforcement learning;Smart cities;Cognitive radio","cognitive radio;game theory;Internet of Things;learning (artificial intelligence);learning automata;multi-agent systems;oligopoly;probability;resource allocation","cognitive radio Internet of Things;Q-probabilistic multiagent learning;learning automation;smart cities;material demands;urban population;CR-IoT;market-based model;multiagent learning process;secondary users;primary users;multiagent scenario;oligopoly market model;optimal resource allocation strategy;multiagent reinforcement learning algorithm;game theory;auction theory","","10","","29","IEEE","30 Oct 2019","","","IEEE","IEEE Journals"
"Utilizing Reinforcement Learning to Autonomously Mange Buffers in a Delay Tolerant Network Node","E. Harkavy; M. S. Net","Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA; Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Dr, Pasadena, CA","2020 IEEE Aerospace Conference","21 Aug 2020","2020","","","1","8","In order to effectively communicate with Earth from deep space there is a need for network automation similar to that of the Internet. The existing automated network protocols, such as TCP and IP, cannot work in deep space due to the assumptions under which they were designed. Specifically, protocols assume the existence of an end-to-end path between the source and destination for the entirety of a communication session and the path being traversable in a negligible amount of time. In contrast, a Delay Tolerant Network is a set of protocols that allows networking in environments where links suffer from high-delay or disruptions (e.g. Deep Space). These protocols rely on different assumptions such as time synchronization and suitable memory allocation. In this paper, we consider the problem of autonomously avoiding memory overflows in a Delay Tolerant Node. To that end, we propose using Reinforcement Learning to automate buffer management given that we can easily measure the relative rates of data coming in and out of the DTN node. In the case of detecting overflow, we let the autonomous agent choose between three actions: slowing down the client, requesting more resources from the Deep Space Network, or selectively dropping packets once the buffer nears capacity. Furthermore, we show that all of these actions can be realistically implemented in real-life operations given current and planned capabilities of Delay Tolerant Networking and the Deep Space Network. Similarly, we also show that using Reinforcement Learning for this problem is well suited to this application due to the number of possible states and variables, as well as the fact that large distances between deep space spacecraft and Earth prevent human-in-the-loop intervention.","1095-323X","978-1-7281-2734-7","10.1109/AERO47225.2020.9172453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172453","","Space vehicles;Earth;Protocols;Reinforcement learning;Switches;Delays;Synchronization","delay tolerant networks;learning (artificial intelligence);multi-agent systems;space communication links;telecommunication computing;telecommunication network management;transport protocols","Reinforcement Learning;Delay Tolerant Network Node;network automation;end-to-end path;communication session;suitable memory allocation;buffer management;DTN node;autonomous agent;Deep Space Network;Delay Tolerant Networking;deep space spacecraft;automated network protocols","","5","","14","IEEE","21 Aug 2020","","","IEEE","IEEE Conferences"
"Investigating Value of Curriculum Reinforcement Learning in Autonomous Driving Under Diverse Road and Weather Conditions","A. Ozturk; M. Burak Gunel; R. Dagdanov; M. Ekim Vural; F. Yurdakul; M. Dal; N. Kemal Ure","ITU Artificial Intelligence and Data Science Research Center and Department of Computer Engineering, Istanbul Technical University, Turkey; ITU Artificial Intelligence and Data Science Research Center and Department of Aeronautical Engineering, Istanbul Technical University, Turkey; ITU Artificial Intelligence and Data Science Research Center and Department of Aeronautical Engineering, Istanbul Technical University, Turkey; ITU Artificial Intelligence and Data Science Research Center and Department of Mechanical Engineering, Istanbul Technical University, Turkey; ITU Artificial Intelligence and Data Science Research Center and Department of Aeronautical Engineering, Istanbul Technical University, Turkey; Faculty of Computer Engineering, Bogazici University, Turkey; ITU Artificial Intelligence and Data Science Research Center and Department of Aeronautical Engineering, Istanbul Technical University, Turkey","2021 IEEE Intelligent Vehicles Symposium Workshops (IV Workshops)","10 Jan 2022","2021","","","358","363","Applications of reinforcement learning (RL) are popular in autonomous driving tasks. That being said, tuning the performance of an RL agent and guaranteeing the generalization performance across variety of different driving scenarios is still largely an open problem. In particular, getting good performance on complex road and weather conditions require exhaustive tuning and computation time. Curriculum RL, which focuses on solving simpler automation tasks in order to transfer knowledge to complex tasks, is attracting attention in RL community. The main contribution of this paper is a systematic study for investigating the value of curriculum reinforcement learning in autonomous driving applications. For this purpose, we setup several different driving scenarios in a realistic driving simulator, with varying road complexity and weather conditions. Next, we train and evaluate performance of RL agents on different sequences of task combinations and curricula. Results show that curriculum RL can yield significant gains in complex driving tasks, both in terms of driving performance and sample complexity. Results also demonstrate that different curricula might enable different benefits, which hints future research directions for automated curriculum training.","","978-1-6654-7921-9","10.1109/IVWorkshops54471.2021.9669203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9669203","","Training;Systematics;Roads;Conferences;Reinforcement learning;Complexity theory;Task analysis","computational complexity;mobile robots;reinforcement learning;road vehicles;traffic engineering computing","curriculum reinforcement learning;autonomous driving tasks;RL agent;generalization performance;driving scenarios;open problem;complex road;weather conditions;exhaustive tuning;computation time;curriculum RL;automation tasks;RL community;autonomous driving applications;realistic driving simulator;road complexity;task combinations;complex driving tasks;sample complexity;automated curriculum training","","1","","24","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"Towards Network Dynamics: Adaptive Buffer Management with Deep Reinforcement Learning","J. Zhu; D. Wang; S. Qin; G. Tao; H. Gui; F. Li; L. Ou","Purple Mountain Laboratories, Nanjing, China; Purple Mountain Laboratories, Nanjing, China; Purple Mountain Laboratories, Nanjing, China; Purple Mountain Laboratories, Nanjing, China; Shandong Future Network Research Institute, Jinan, China; China Academy of Information and Communications Technology, Beijing, China; China Telecom Research Institute, Guangzhou, China","GLOBECOM 2022 - 2022 IEEE Global Communications Conference","11 Jan 2023","2022","","","4935","4940","The prosperity of cloud computing and 5G/B5G is bringing a wide range of delay jitter-sensitive applications (e.g., professional audio/video streaming and industrial automation) to large-scale IP networks. Although various network-side techniques have been proposed to guarantee the quality of service (QoS), the client-side technique by introducing a receive buffer should never be neglected from the applications' perspective. In this paper, we revisit the buffer management problem to address the disadvantages of state-of-the-art studies, which as-sumed network characteristics known a prior with simplified or inaccurate network models and failed to adapt to network dynamics. Specifically, we propose adaptive buffer management with deep reinforcement learning, i.e., DRL-ABM. We first define a tradeoff value to measure the buffer management performance in terms of the start-up delay, underflow frequency and packet losses. Then we formulate the DRL model and design deep neural networks (DNNs) based on the advantage actor critic (A2C) algorithm. To evaluate the performance of DRL-ABM, we perform extensive simulations. Simulation results show that D RL-ABM can achieve better buffer management performance, i.e., reducing the tradeoff value by at least 20% when compared with the benchmarks TBM and ABM. Moreover, DRL-ABM reduces packet losses to approximately 0, indicating that a smaller receive buffer is sufficient if managed with DRL-ABM.","","978-1-6654-3540-6","10.1109/GLOBECOM48099.2022.10001602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10001602","Delay jitter-sensitive applications;Quality of service;Network dynamics;Adaptive buffer management;Deep reinforcement learning","Deep learning;Adaptation models;Simulation;Neural networks;Packet loss;Reinforcement learning;Quality of service","5G mobile communication;cloud computing;computer network management;deep learning (artificial intelligence);delays;IP networks;jitter;mobile computing;quality of service;reinforcement learning","A2C algorithm;adaptive buffer management;advantage actor critic algorithm;B5G;buffer management performance;client-side technique;cloud computing;deep neural networks;deep reinforcement learning;delay jitter-sensitive applications;DRL-ABM;industrial automation;large-scale IP networks;network characteristics;network dynamics;network-side techniques;packet losses;QoS;quality of service;start-up delay","","","","16","IEEE","11 Jan 2023","","","IEEE","IEEE Conferences"
"Efficient Reinforcement Learning Framework for Automated Logic Synthesis Exploration","Y. Qian; X. Zhou; H. Zhou; L. Wang","State Key Lab of ASIC and System, Fudan University; State Key Lab of ASIC and System, Fudan University; State Key Lab of ASIC and System, Fudan University; State Key Lab of ASIC and System, Fudan University","2022 International Conference on Field-Programmable Technology (ICFPT)","15 Dec 2022","2022","","","1","6","Logic synthesis is a crucial step in electronic design automation tools for integrated circuit design. In recent years, the development of reinforcement learning (RL) has enabled the designers to automatically explore the logic synthesis process. Existing RL based methods typically use conventional on-policy models, which leads to data inefficiency. Moreover, the exploration approach for FPGA technology mapping in recent works lacks the flexibility of the learning process. In this work, we propose ESE, a reinforcement learning based framework to efficiently learn the logic synthesis process. The framework supports the modeling for both the logic optimization and the FPGA technology mapping. The reward functions and terminal conditions in the RL environment are designed to efficiently guide the optimization of the metrics and execution time. For the modeling of FPGA mapping, the logic optimization and technology mapping are combined to be learned in a flexible way. Moreover, the Proximal Policy Optimization model is adopted to improve the utilization of samples. The proposed framework is evaluated on several common benchmarks. For the logic optimization on the EPFL benchmark, compared with previous works, the proposed method obtains an 11.3% improvement in the average quality (node-level-product) and reduces the execution time by 13.7%. For the FPGA technology mapping on the VTR benchmark, our method improves the average quality (LUT-level-product) by 14.8%, and reduces the execution time by 14.4% compared with the recent work.","","978-1-6654-5336-3","10.1109/ICFPT56656.2022.9974330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9974330","","Measurement;Integrated circuit synthesis;Reinforcement learning;Benchmark testing;Minimization;Integrated circuit modeling;Task analysis","circuit optimisation;field programmable gate arrays;logic design;reinforcement learning","automated logic synthesis exploration;efficient reinforcement learning framework;electronic design automation tools;execution time;exploration approach;FPGA mapping;FPGA technology mapping;integrated circuit design;learning process;logic optimization;logic synthesis process;node-level-product;on-policy models;proximal policy optimization model;reward functions;RL based methods;RL environment;terminal conditions;VTR benchmark","","","","11","IEEE","15 Dec 2022","","","IEEE","IEEE Conferences"
"Coverage Path Planning for Unmanned Aerial Vehicles in Complex 3D Environments with Deep Reinforcement Learning","J. Bialas; M. Doller","FH Kufstein Tirol-University of Applied Sciences, Kufstein, Austria; FH Kufstein Tirol University of Applied Sciences, Kufstein, Austria","2022 IEEE International Conference on Robotics and Biomimetics (ROBIO)","18 Jan 2023","2022","","","1080","1085","Coverage path planning (CPP) for unmanned aerial vehicles (UAVs) defines a vital role in the automation process of UAV-supported disaster management. While multiple algorithms exist to solve the CPP problem for planar areas, the proposed algorithm is the first to handle complex three-dimensional environments and also account for power constraints and changing environments. By applying proximal policy optimization to an advantage-based actor-critic deep reinforcement learning model, the proposed framework enables an agent to efficiently cover the target area (TA), considering the orientation of the observation sensor, avoiding collisions as well as no-flying zones (NFZ) and reacting to changing environments. Furthermore, a safe landing mechanism, based on the Dijkstra algorithm, expands the framework to guarantee a successful landing in the respective start and landing zone (SLZ) within the power constraints. The model is trained on real data to learn the optimal control policy. Additionally, the framework was tested and validated on hardware in a drone lab to confirm its effectiveness and capability to perform real-time path planning.","","978-1-6654-8109-0","10.1109/ROBIO55434.2022.10011936","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011936","","Three-dimensional displays;Heuristic algorithms;Reinforcement learning;Autonomous aerial vehicles;Hardware;Trajectory;Vehicle dynamics","autonomous aerial vehicles;collision avoidance;learning (artificial intelligence);mobile robots;optimal control;path planning;remotely operated vehicles","advantage-based actor-critic deep reinforcement learning model;automation process;coverage path planning;CPP problem;Dijkstra algorithm;landing zone;multiple algorithms;optimal control policy;planar areas;power constraints;proximal policy optimization;real-time path planning;respective start;safe landing mechanism;three-dimensional environments;UAV-supported disaster management;UAVs;unmanned aerial vehicles","","","","20","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"Applied Reinforcement Learning Method for Pressure Control in Managed Pressure Drilling","M. Usama; J. Cao; J. E. Gravdal; D. Sui","Department of Energy and Petroleum Technology, University of Stavanger, Stavanger, Norway; Department of Energy and Petroleum Technology, University of Stavanger, Stavanger, Norway; Department of Energy and Petroleum Technology, University of Stavanger, Stavanger, Norway; Department of Energy and Petroleum Technology, University of Stavanger, Stavanger, Norway","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","1","6","During drilling operations, maintaining an optimal bottom hole pressure (BHP) in this narrow pressure margin between the pore and fracture pressure gradient is critical to avoid damage to the formation and the well. Back-pressure Managed pressure drilling (MPD) is widely used in these unconventional drilling prospects as it controls the BHP by adjusting the surface pressure using a back-pressure choke manifold at the outlet. The standard technique used for automated control of the MPD choke is the proportional-integral-derivative (PID) controller. However, the physical system in the well during drilling operations is highly non-linear with respect to pressure and flow rates. Alternative control methods are therefore being explored. In this research, we explore a novel automation method for MPD by training an intelligent tuning agent for a PID control system. The objective is to explore an automated and optimized control strategy assisted by reinforcement learning algorithms and to verify the feasibility and efficiency of this approach.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10239724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10239724","Managed Pressure Drilling;Pressure Control;Reinforcement Learning","Drilling;Training;PI control;Reinforcement learning;Pressure control;PD control;Inductors","drilling (geotechnical);learning (artificial intelligence);optimal control;pressure control;three-term control","alternative control methods;applied reinforcement learning method;automated control strategy;back-pressure Managed pressure drilling;BHP;drilling operations;flow rates;fracture pressure gradient;MPD choke;narrow pressure margin;novel automation method;optimal bottom hole pressure;optimized control strategy;PID control system;pressure control;proportional-integral-derivative controller;reinforcement learning algorithms;surface pressure;unconventional drilling prospects","","","","22","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Decision-Making Strategy Using Multi-Agent Reinforcement Learning for Platoon Formation in Agreement-Seeking Cooperation","E. Hyeon; D. Karbowski; A. Rousseau","Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA","2023 IEEE Intelligent Vehicles Symposium (IV)","27 Jul 2023","2023","","","1","6","Among the four classes of cooperative driving automation defined in [1], agreement-seeking cooperation appears to be a promising option for achieving higher cooperation levels with general passenger vehicles. Because agreement-seeking cooperation allows connected and automated vehicles (CAVs) to decide whether or not to participate in cooperative driving, it is necessary for CAVs to have intelligent decision-making strategies. This work develops a farsighted, interaction-aware decision-making strategy using multi-agent reinforcement learning (MARL). A MARL system is formulated with unique state and action spaces reflecting agreement-seeking interactions. A state–action–reward–state–action (SARSA) algorithm is applied to learn the action-value function of each CAV. Simulation results show that using a MARL-based decision-making strategy increases agreement rates by 52% on average and cooperation time by 50%. The higher cooperation rates lead to higher energy efficiency: 5.5% more energy saving than heuristic decision-making.","2642-7214","979-8-3503-4691-6","10.1109/IV55152.2023.10186813","Vehicle Technologies Office; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10186813","","Energy consumption;Protocols;Costs;Intelligent vehicles;Simulation;Decision making;Reinforcement learning","decision making;mobile robots;multi-agent systems;reinforcement learning;road vehicles;traffic engineering computing","action-value function;agreement rates;agreement-seeking cooperation;agreement-seeking interactions;CAV;connected and automated vehicles;cooperation rates;cooperation time;cooperative driving automation;general passenger vehicles;intelligent decision-making strategies;interaction-aware decision-making strategy;MARL-based decision-making strategy;multiagent reinforcement learning;platoon formation;SARSA;state-action-reward-state-action algorithm","","","","28","IEEE","27 Jul 2023","","","IEEE","IEEE Conferences"
"A New Multipredictor Ensemble Decision Framework Based on Deep Reinforcement Learning for Regional GDP Prediction","Q. Li; C. Yu; G. Yan","College of Business and Trade, Hunan Industry Polytechnic, Changsha, China; Key Laboratory of Traffic Safety on Track of Ministry of Education, School of Traffic and Transportation Engineering, Central South University, Changsha, China; Key Laboratory of Traffic Safety on Track of Ministry of Education, School of Traffic and Transportation Engineering, Central South University, Changsha, China","IEEE Access","3 May 2022","2022","10","","45266","45279","Gross domestic product (GDP) can effectively reflect the situation of economic development and resource allocation in different regions. The high-precision GDP prediction technology lays a foundation for the sustainable development of regional resources and the proposal of economic management policies. To build an accurate GDP prediction model, this paper proposed a new multi-predictor ensemble decision framework based on deep reinforcement learning. Overall modeling consists of the following steps: Firstly, GRU, TCN, and DBN are the main predictors to train three GDP forecasting models with their characteristics. Then, the DQN algorithm effectively analyses the adaptability of these three neural networks to different GDP datasets to obtain an ensemble model. Finally, by adaptive optimization of the ensemble weight coefficients of these three neural networks, the DQN algorithm got the final GDP prediction results. Through three groups of experimental cases from China, the following conclusions can be drawn: (1) the DQN algorithm can obtain excellent experimental results in ensemble learning, which effectively improves the prediction performance of single predictors by more than 10 %. (2) The ensemble multi-predictor region GDP prediction framework based on deep reinforcement learning can achieve better prediction results than 18 benchmark models. In addition, the MAPE value of the proposed model is lower than 4.2% in all cases.","2169-3536","","10.1109/ACCESS.2022.3170905","Planning Subject for the 14th Five-Year Plan of Hunan Province Education Sciences(grant numbers:XJK21BZJ021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9764699","Deep reinforcement learning;GDP prediction;ensemble multipredictor framework","Economic indicators;Predictive models;Economics;Prediction algorithms;Forecasting;Data models;Reinforcement learning","deep learning (artificial intelligence);economic indicators;financial data processing;neural nets;reinforcement learning;resource allocation;sustainable development","DQN algorithm;ensemble learning;prediction performance;multipredictor region GDP prediction framework;deep reinforcement learning;regional GDP prediction;gross domestic product;economic development;resource allocation;high-precision GDP prediction technology;sustainable development;regional resources;economic management policies;accurate GDP prediction model;GDP forecasting models;neural networks;ensemble model;ensemble weight coefficients;final GDP prediction results;GDP datasets","","5","","66","CCBY","28 Apr 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning Based Antenna Selection in User-Centric Massive MIMO","X. Chai; H. Gao; J. Sun; X. Su; T. Lv; J. Zeng","Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Ministry of Industry and Information Technology, Institute of Telecommunications, Beijing, China; Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China","2020 IEEE 91st Vehicular Technology Conference (VTC2020-Spring)","30 Jun 2020","2020","","","1","6","In this paper, we consider a user-centric massive multiple-input multiple-output (UC-MMIMO) system, wherein the optimal antenna selection (AS) is very complicated, because of the huge number of deployed antennas. Traditional AS algorithms rely heavily on full and perfect channel state information (CSI). Thus, we propose a novel AS algorithm to achieve low-complexity and less CSI reliance for UC-MMIMO. The proposed AS algorithm consists of the selection stage and the adjustment stage. In the selection stage, antennas are selected by a reinforcement learning (RL) based algorithm in which input data are the locations of users. In the adjustment stage, an adjustment mechanism is designed to further improve the performance. Numerical results show that our algorithm achieves better performance with lower complexity compared with related traditional algorithms.","2577-2465","978-1-7281-5207-3","10.1109/VTC2020-Spring48590.2020.9129108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9129108","User-Centric Massive MIMO system;antenna selection;low complexity;antenna adjustment;reinforcement learning","Loaded antennas;Machine learning algorithms;Reinforcement learning;Antenna arrays;Massive MIMO","antenna arrays;learning (artificial intelligence);MIMO communication","UC-MMIMO;optimal antenna selection;perfect channel state information;AS algorithm;CSI reliance;selection stage;adjustment stage;reinforcement learning based algorithm;user-centric massive multiple-input multiple-output system","","5","","13","IEEE","30 Jun 2020","","","IEEE","IEEE Conferences"
"Digital Twin for Transportation Big Data: A Reinforcement Learning-Based Network Traffic Prediction Approach","L. Nie; X. Wang; Q. Zhao; Z. Shang; L. Feng; G. Li","School of Computer Science and Engineering, Macau University of Science and Technology, Macau, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Computer Science and Engineering, Macau University of Science and Technology, Macau, China; Acoustic Science and Technology Laboratory, Harbin Engineering University, Harbin, China; School of Computer Science and Engineering, Macau University of Science and Technology, Macau, China; Laboratory of BLOS Reliable Information Transmission, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Transactions on Intelligent Transportation Systems","","2023","PP","99","1","11","Vehicular Ad-Hoc Networks (VANETs), as the crucial support of Intelligent Transportation Systems (ITS), have received great attention in recent years. With the rapid development of VANETs, various services have generated a great deal of data that can be used for transportation planning and safe driving. Especially, with the advent of Coronavirus Disease 2019 (COVID-19), the transportation system has been impacted, thus novel modes of transportation planning and intelligent applications are necessary. Digital twins can provide powerful support for artificial intelligence applications in Transportation Big Data (TBD). The features of VANETs are varying, which arises the main challenge of digital twins applying in TBD. Network traffic prediction, as part of digital twins, is useful for network management and security in VANETs, such as network planning and anomaly detection. This paper proposes a network traffic prediction algorithm aiming at time-varying traffic flows with a large number of fluctuations. This algorithm combines Deep Q-Learning (DQN) and Generative Adversarial Networks (GAN) for network traffic feature extraction. DQN is leveraged to carry out network traffic prediction, in which GAN is involved to represent Q-network. Meanwhile, the generative network can increase the number of samples to improve the prediction error. We evaluate the performance of our method by implementing it on three real network traffic data sets. Finally, we compare the two state-of-the-art competing methods with our method.","1558-0016","","10.1109/TITS.2022.3232518","National Key Research and Development Program of China(grant numbers:2019YFC1511300); National Natural Science Foundation of China(grant numbers:U22A2006,62001073,62272075,62171378); National Natural Science Foundation of Chongqing(grant numbers:cstc2021ycjh-bgzxm0072); Science and Technology Research Program for Chongqing Municipal Education Commission(grant numbers:KJZD-M202200601); Science and Technology Development Fund, Macau, SAR(grant numbers:0037/2020/A1,0076/2022/A2,0093/2022/A2); Macao Young Scholars Program(grant numbers:AM2020014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021249","Digital twin;transportation big data;deep reinforcement learning;generative adversarial networks","COVID-19;Predictive models;Neural networks;Feature extraction;Transportation;Generative adversarial networks;Digital twins","","","","3","","","IEEE","18 Jan 2023","","","IEEE","IEEE Early Access Articles"
"Comfort-Oriented Motion Planning for Automated Vehicles Using Deep Reinforcement Learning","N. Rajesh; Y. Zheng; B. Shyrokau","Simulation and Test Solutions, Siemens Digital Industries Software B.V, Helmond, The Netherlands; Department of Cognitive Robotics, Delft University of Technology, Delft, The Netherlands; Department of Cognitive Robotics, Delft University of Technology, Delft, The Netherlands","IEEE Open Journal of Intelligent Transportation Systems","22 May 2023","2023","4","","348","359","Automated vehicles promise numerous advantages to their users. The proposed benefits could however be overshadowed by a rise in the susceptibility of passengers to motion sickness due to their engagement in non-driving tasks. Increasing attention is paid to designing vehicle motion to mitigate motion sickness. In this work, the deep reinforcement learning (DRL) method is used to plan vehicle trajectories, with a focus on minimizing low-frequency accelerations. These are known to be the primary cause of motion sickness. The goal is achieved by incorporating a frequency-weighted discomfort term into the reward function during training. The ability of the trained agent to target undesirable frequencies in accelerations is verified by comparing it with another agent trained for improving overall acceleration comfort. A reduction of 9.6% in frequency-weighted discomfort is achieved. The motion plan from the DRL agent is further compared with trajectories generated by human drivers in real-world scenarios. The results demonstrate comparable performance between the DRL agent and human drivers. Meanwhile, a significant reduction in online computation time has been observed when compared to a motion planner based on numerical optimization.","2687-7813","","10.1109/OJITS.2023.3275275","European Union Horizon 2020 Framework Program through Marie Skłodowska-Curie Actions(grant numbers:872907); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10123025","Automated driving;deep reinforcement learning;motion planning;motion sickness;proximal policy optimization","Motion sickness;Planning;Vehicles;Training;Trajectory;Reinforcement learning;Optimization","deep learning (artificial intelligence);learning (artificial intelligence);path planning;reinforcement learning","acceleration comfort;automated vehicles;comfort-oriented motion planning;deep reinforcement learning method;DRL agent;frequency-weighted discomfort term;low-frequency accelerations;motion plan;motion planner;motion sickness;trained agent;undesirable frequencies;vehicle motion;vehicle trajectories","","1","","48","CCBY","11 May 2023","","","IEEE","IEEE Journals"
"Minimizing expected loss for risk-avoiding reinforcement learning","Jung-Jung Yeh; Tsung-Ting Kuo; W. Chen; Shou-De Lin","National Taiwan University; National Taiwan University; Institute for Information Industry Taipei, Taiwan; National Taiwan University","2014 International Conference on Data Science and Advanced Analytics (DSAA)","12 Mar 2015","2014","","","11","17","This paper considers the design of a reinforcement learning (RL) agent that can strike a balance between return and risk. First, we discuss several favorable properties of an RL risk model, and then propose a definition of risk based on expected negative rewards. We also design a Q-decomposition-based framework that allows a reinforcement learning agent to control the balance between risk and profit. The results of experiments on both artificial and real-world stock datasets demonstrate that the proposed risk model satisfies the beneficial properties of an RL-based risk learning model, and also significantly outperforms other approaches in terms of avoiding risks.","","978-1-4799-6991-3","10.1109/DSAA.2014.7058045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058045","reinforcement learning;risk avoiding;risk model;profit model","Learning (artificial intelligence);Investment;Finance;Loss measurement;Reactive power;Legged locomotion","learning (artificial intelligence);multi-agent systems","expected loss minimization;risk-avoiding reinforcement learning;reinforcement learning agent;RL agent;RL risk model;expected negative rewards;Q-decomposition-based framework;RL-based risk learning model","","1","","20","IEEE","12 Mar 2015","","","IEEE","IEEE Conferences"
"A developmental actor-critic reinforcement learning approach for task-nonspecific robot","X. Li; Y. Yang; Y. Sun; L. Zhang","School of Computer Science and Technology, Northwestern Polytechnical University, Xi'an, Shaanxi, CHINA; School of Computer Science and Technology, Northwestern Polytechnical University, Xi'an, Shaanxi, CHINA; Air industry Xi'an aviation Computing Technology Research Institute, Xi'an, Shaanxi, CHINA; School of Computer Science and Technology, Northwestern Polytechnical University, Xi'an, Shaanxi, CHINA","2016 IEEE Chinese Guidance, Navigation and Control Conference (CGNCC)","23 Jan 2017","2016","","","2231","2237","For task-nonspecific robot work in changing environment, most researches on developmental learning based on cognitive psychology advocate a staged developmental process and an explicit hierarchical action model. Though progress has been made by existing developmental learning approaches, there are still two open-ended inevitable problems: a) when numerous tasks are involved, the learning speed is not always satisfactory; b) when these tasks are not specified in advance, the hierarchical action model is hard to design beforehand or learn automatically. In order to solve these two problems, this paper proposes a new developmental reinforcement learning approach presented with its model and algorithms. In our model, any one of actor-critic learning models is encapsulated as a learning infrastructure to build an implicit action model called reward-policy mapping, and a self-motivated module is used for autonomous robots. The proposed approach efficaciously supports the implementation of an autonomous, interactive, cumulative and online learning process of task-nonspecific robots. The simulation results show that, to learn to perform nearly twenty thousand tasks, the proposed approach just needs half of the time that its counterpart, the actor-critic learning algorithm encapsulated, needs.","","978-1-4673-8318-9","10.1109/CGNCC.2016.7829139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829139","","Robot kinematics;Psychology;Service robots;Computational modeling;Learning (artificial intelligence);Robot sensing systems","intelligent robots;learning (artificial intelligence);psychology","autonomous learning process;cumulative learning;online learning process;autonomous robots;self-motivated module;reward-policy mapping;actor-critic learning models;hierarchical action model;explicit hierarchical action model;cognitive psychology;changing environment;task-nonspecific robot;developmental actor-critic reinforcement learning","","1","","18","IEEE","23 Jan 2017","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Time-Energy Tradeoff Online Offloading in MEC-Enabled Industrial Internet of Things","X. Jiao; H. Ou; S. Chen; S. Guo; Y. Qu; C. Xiang; J. Shang","College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; Key Laboratory of Dynamic Cognitive System of Electromagnetic Spectrum Space, Ministry of Industry and Information Technology; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China","IEEE Transactions on Network Science and Engineering","","2023","PP","99","1","14","Mobile edge computing (MEC) has recently emerged as a promising technology to boost the integration ability of sensing, transmission and computation in industrial Internet of Things (IIoT). This paper investigates an MEC-enabled IIoT system, where multiple industrial devices may offload computation-intensive tasks to an edge server through wireless communication. We focus on the online offloading problem to optimize the tradeoff of the task accomplishing time and energy consumption. Time-varying wireless channels, random targeted task data sizes and dynamically changing residual energy as well as adaptively adjusted tradeoff weights make this problem highly challenging. Conventional optimization methods may lead to inefficient or even infeasible solutions. To efficiently tackle this problem, we leverage the deep reinforcement learning (DRL) technology to propose a time-energy tradeoff online offloading algorithm called TETO. In TETO, the online offloading decision policies are empirically learned via a well-designed DRL framework. TETO algorithm incorporates a stochastic strategy, the crossover and mutation technology and a novel feasible suboptimal offloading method to expand the offloading action search space with the provable feasibility guarantee. Extensive experimental results based on a real-world dataset show that, our TETO algorithm performs better than existing baseline algorithms, and obtains near-optimal performance with low CPU execution latency.","2327-4697","","10.1109/TNSE.2023.3263169","National Natural Science Foundation of China(grant numbers:62072064,62272069,62072303,62172063); Natural Science Foundation of Chongqing(grant numbers:CSTB2022NSCQ-MSX1017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10091913","Mobile edge computing;deep reinforcement learning;time-energy tradeoff;online offloading;industrial Internet of Things","Task analysis;Industrial Internet of Things;Wireless communication;Optimization;Energy consumption;Servers;Heuristic algorithms","","","","","","","IEEE","4 Apr 2023","","","IEEE","IEEE Early Access Articles"
"pDPoSt+sPBFT: A High Performance Blockchain-Assisted Parallel Reinforcement Learning in Industrial Edge-Cloud Collaborative Network","F. Yang; F. Xu; T. Feng; C. Qiu; C. Zhao","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; China National Tendering Center of Mechanical and Electrical Equipment, Ministry of Industry and Information Technology, Beijing, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Network and Service Management","9 Oct 2023","2023","20","3","2744","2759","With the increasing demand for resource scheduling efficiency in Industrial Internet of Things (IIoT), parallel reinforcement learning (PRL) based distributed edge-cloud collaborative resource scheduling scheme has attracted enormous attention. However, the computing and communication capacities, the security degree of massive distributed edge computing servers are different. It is difficult to make a large number of edge servers carry out security and efficiency PRL based edge-cloud collaboration resource scheduling scheme. Thus, in this paper, a large-scale distributed edge-cloud collaborative resource scheduling method based on picture delegated proof of state and suspicious practical byzantine fault tolerance (pDPoSt+sPBFT) consensus algorithm is proposed. To be specific, we first propose a collaborative edge-cloud industrial network architecture to support massive industrial intelligence tasks, then a distributed PRL based resource allocation scheme is utilized. Secondly, in order to improve the efficiency and security of distributed PRL training, we propose a server filtering strategy based on pDPoSt algorithm. Finally, a sPBFT algorithm is proposed to further realize security parameter aggregation of distributed PRL. Experimental results show that the proposed method has good efficiency and security performance compared with the traditional distributed edge-cloud collaborative resource scheduling algorithm. The proposed approach has great potential in complex IIoT scenarios.","1932-4537","","10.1109/TNSM.2022.3230208","Natural Science Foundation of China (NSFC), 2020 Industrial Internet Innovation and Development Project “Smart Energy Internet Security Situation Awareness Platform Project,” Beijing Natural Science Foundation-Haidian Frontier Project “Research on Key Technologies of wireless edge intelligent collaboration for industrial Internet scenarios (L202017),” and BUPT Excellent Ph.D. Students Foundation (CX2020214)(grant numbers:U1805262,U61971050); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9991258","Consensus algorithm;blockchain;security parallel reinforcement learning;edge-cloud collaborative resource scheduling","Servers;Training;Security;Task analysis;Job shop scheduling;Computational modeling;Blockchains","","","","","","55","IEEE","19 Dec 2022","","","IEEE","IEEE Journals"
"Com-DDPG: Task Offloading Based on Multiagent Reinforcement Learning for Information-Communication-Enhanced Mobile Edge Computing in the Internet of Vehicles","H. Gao; X. Wang; W. Wei; A. Al-Dulaimi; Y. Xu","School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer Science and Engineering, Xi'an University of Technology, Xi'an, China; R&D Department, EXFO, Montreal, Canada; School of Computer Science and Technology, Xidian University, Xi'an, China","IEEE Transactions on Vehicular Technology","","2023","PP","99","1","14","The emergence of the Internet of Vehicles (IoV) introduces challenges regarding computation-intensive and time-sensitive related services for data processing and communication. Limited resource availability increases the processing latency and may cause application interruption due to the mobility of vehicles. To address the real-time requirements of users and tasks, mobile edge computing (MEC), in which data are processed at the network edge, has been proposed to collaborate with the cloud to provide better performance. However, the offloading strategies proposed previously have some shortcomings in addressing issues such as task dependency and resource competition. In this paper, we propose a novel offloading strategy for MEC, Com-DDPG, in which multiagent reinforcement learning is used to enhance the offloading performance. Within the IoV transmission radius, multiple agents work together to learn the changes in the environment, such as the number of mobile devices and the queue of tasks, and take appropriate action in the form of a strategy for offloading to an edge server. First, we discuss models of task dependency, task priority, and resource consumption from the perspective of server clusters and multiple dependencies among tasks. In the proposed method, the communication behavior among multiple agents is formulated; then, the policy determined through reinforcement learning is executed as an offloading strategy to obtain the corresponding results. Second, to enhance the communication of information among multiple agents, a long short-term memory (LSTM) network is employed as an internal state predictor to provide a more complete environmental state, and a bidirectional recurrent neural network (BRNN) is used to learn and enhance the features obtained from the agents' communication. Finally, experiments carried out based on the Alibaba Cluster Dataset are presented. The results show that our method is superior to baseline methods in terms of energy consumption, load status and latency.","1939-9359","","10.1109/TVT.2023.3309321","National Key Research and Development Program of China(grant numbers:2022YFF0902500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10233027","Mobile Edge Computing;Multiagent Reinforcement Learning;Offloading Strategy;Wireless Communication;Internet of Vehicles","Task analysis;Servers;Reinforcement learning;Mobile handsets;Cloud computing;Performance evaluation;Energy consumption","","","","","","","IEEE","28 Aug 2023","","","IEEE","IEEE Early Access Articles"
"A Sample Efficiency Improved Method via Hierarchical Reinforcement Learning Networks","Q. Chen; E. Dallas; P. Shahverdi; J. Korneder; O. A. Rawashdeh; W. -Y. Geoffrey Louie","Intelligent Robotics Laboratory, Oakland University, MI, USA; Intelligent Robotics Laboratory, Oakland University, MI, USA; Intelligent Robotics Laboratory, Oakland University, MI, USA; Applied Behavior Analysis Clinic, Oakland University, MI, USA; Embedded Systems Research Lab, Oakland University, MI, USA; Intelligent Robotics Laboratory, Oakland University, MI, USA","2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","30 Sep 2022","2022","","","1498","1505","Learning from demonstration (LfD) approaches have garnered significant interest for teaching social robots a variety of tasks in healthcare, educational, and service domains after they have been deployed. These LfD approaches often require a significant number of demonstrations for a robot to learn a performant model from task demonstrations. However, requiring non-experts to provide numerous demonstrations for a social robot to learn a task is impractical in real-world applications. In this paper, we propose a method to improve the sample efficiency of existing learning from demonstration approaches via data augmentation, dynamic experience replay sizes, and hierarchical Deep Q-Networks (DQN). After validating our methods on two different datasets, results suggest that our proposed hierarchical DQN is effective for improving sample efficiency when learning tasks from demonstration. In the future, such a sample-efficient approach has the potential to improve our ability to apply LfD approaches for social robots to learn tasks in domains where demonstration data is limited, sparse, and imbalanced.","1944-9437","978-1-7281-8859-1","10.1109/RO-MAN53752.2022.9900738","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9900738","","Social robots;Education;Reinforcement learning;Medical services;Task analysis","control engineering computing;reinforcement learning;robot programming;social robots","social robot;sample efficiency;hierarchical reinforcement learning Networks;learning from demonstration;LfD;task demonstrations;dynamic experience replay;hierarchical DQN;data augmentation;hierarchical deep Q-networks","","","","33","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Magnetic Field Compensation Control for Spin-Exchange Relaxation-Free Comagnetometer Using Reinforcement Learning","F. Li; Z. Wang; R. Wang; S. Liu; B. Qin; Z. Liu; X. Zhou","School of Instrumentation Science and Optoelectronics Engineering, Beihang University, Beijing, China; School of Instrumentation Science and Optoelectronics Engineering and the Key Laboratory of the Ministry of Industry and Information Technology on Quantum Sensing Technology, Beihang University, Beijing, China; School of Instrumentation Science and Optoelectronics Engineering, Beihang University, Beijing, China; School of Instrumentation Science and Optoelectronics Engineering, Beihang University, Beijing, China; School of Instrumentation Science and Optoelectronics Engineering, Beihang University, Beijing, China; School of Instrumentation Science and Optoelectronics Engineering, Beihang University, Beijing, China; School of Instrumentation Science and Optoelectronics Engineering, Beihang University, Beijing, China","IEEE Transactions on Instrumentation and Measurement","16 Aug 2023","2023","72","","1","10","The triaxial drift magnetic field compensation (TDMFC) is a prerequisite to maintaining the excellent performance of the spin-exchange relaxation-free comagnetometer (SERFCM). In this article, we develop a previously undescribed controller design architecture running the proposed constrained dynamic action space  $Q$ -learning (CDA- $Q$ ) algorithm using reinforcement learning (RL) to solve the TDMFC problem. The architecture contains two parts: offline training and online deployment. Specifically, the CDA- $Q$  algorithm trains the agents with the simulated environment to produce the control strategies adopted in the online deployment. Numerical simulations verify the effectiveness of the obtained control strategies. Experimentally, the control strategies are deployed in the real-time control system achieving efficient and adaptive compensation of the triaxial drift magnetic field. Comparative experiments show that the proposed method is 67.56% more efficient than the existing method.","1557-9662","","10.1109/TIM.2023.3301054","National Natural Science Foundation of China(grant numbers:61673041,62122009); Special Project for Research and Development in Key areas of Guangdong Province(grant numbers:2021B0101410005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203035","Q-learning;real-time control system;reinforcement learning (RL);spin-exchange relaxation-free comagnetometer (SERFCM);triaxial drift magnetic field compensation (TDMFC)","Magnetometers;Laser excitation;Pump lasers;Magnetic shielding;Magnetic noise;Probes;Heuristic algorithms","compensation;control system synthesis;learning (artificial intelligence);magnetometers;reinforcement learning","adaptive compensation;CDA-Qalgorithm trains;constrained dynamic action spaceQ-learning algorithm;control strategies;efficient compensation;magnetic field compensation control;online deployment;real-time control system;reinforcement learning;spin-exchange relaxation-free comagnetometer;TDMFC problem;triaxial drift magnetic field compensation;undescribed controller design architecture","","","","34","IEEE","2 Aug 2023","","","IEEE","IEEE Journals"
"Multi-Agent Reinforcement Learning for UAVs 3D Trajectory Designing and Mobile Ground Users Scheduling with No-Fly Zones","Y. Gao; S. Wang; M. Liu; Y. Hu","School of Electronic Information, Wuhan University, Wuhan, China; Avic Xi’an Aircraft Industry (Group) Company ltd, Xian, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China","2023 IEEE/CIC International Conference on Communications in China (ICCC)","5 Sep 2023","2023","","","1","6","Unmanned aerial vehicle (UAV)-based aerial communication is considered a promising technology in future wireless systems. In this paper, we study a multi-UAV-assisted data transmission system in an urban environment, where a set of UAVs collect data from mobile ground users (GUs). We provide a design aiming to minimize the total data transmission time by jointly optimizing mobile GUs’ scheduling and the UAVs’ three-dimensional (3D) trajectory while satisfying the requirements of no-fly zones and collision avoidance. The formulated mixed-integer non-convex problem is difficult to address by utilizing traditional approaches, e.g., graph theory and successive convex approximation (SCA), due to the impacts of random GUs moving behaviors and the unpredictable UAV-GU channels. To tackle such challenges, we first transform the joint optimization problem into a Markov decision process. Then a joint optimizing scheme is proposed, including a multi-agent multi-step dueling double deep Q learning network (MAMD3QN) method for UAVs trajectory design and a greedy policy for mobile GUs scheduling. In particular, an improved DDQN network is utilized to optimize UAVs trajectory with dueling networks architecture and multi-step bootstrapping technique. Finally, simulation results show that the proposed design significantly outperforms the benchmark schemes, showcases the advantages of 3D trajectory design over two-dimensional (2D) cases, and highlights the robustness in terms of different NFZs and the mobility of GUs.","2377-8644","979-8-3503-4538-4","10.1109/ICCC57788.2023.10233375","National Natural Science Foundation of China; State Grid Hubei Electric Power Co; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10233375","Unmanned aerial vehicle;3D trajectory design;mobile ground users;no-fly zones;deep reinforcement learning","Wireless communication;Three-dimensional displays;Simulation;Urban areas;Two dimensional displays;Transforms;Autonomous aerial vehicles","autonomous aerial vehicles;collision avoidance;concave programming;control engineering computing;convex programming;deep learning (artificial intelligence);graph theory;integer programming;Markov processes;mobile communication;multi-agent systems;reinforcement learning;telecommunication control;telecommunication scheduling","3D trajectory design;dueling networks architecture;future wireless systems;joint optimization problem;MAMD3QN;mixed-integer nonconvex problem;mobile ground users scheduling;mobile GU scheduling;multi-agent multi-step dueling double deep Q learning network method;multiagent multistep dueling;multiagent reinforcement learning;multiUAV-assisted data transmission system;no-fly zones;random GU;successive convex approximation;three-dimensional trajectory;total data transmission time;UAV 3D trajectory designing;UAV trajectory design;unmanned aerial vehicle-based aerial communication;unpredictable UAV-GU channels;urban environment","","","","14","IEEE","5 Sep 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Multiple Access in Dynamic IoT Networks Using Bi-GRU","L. Lu; X. Gong; B. Ai; N. Wang; W. Chen","State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, China; School of Information Engineering, Zhengzhou University, Zhengzhou, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, China","ICC 2022 - IEEE International Conference on Communications","11 Aug 2022","2022","","","3196","3201","In the next-generation wireless communication systems, learning-based dynamic spectrum access strategy at the medium access control layer and physical layer shows its powerful capability of achieving optimal resources allocation, and it has become a hot research topic for the harmonious coexistence of heterogeneous wireless networks. In this paper, we propose a multiple access control method to achieve high network throughput by combining deep reinforcement learning and memory module. In specific, we introduce the bidirectional gated recurrent unit (Bi-GRU) in deep Q-learning (DQL) to utilize the information of varying environment observation at each time-step. Furthermore, we apply the method in a freeway scenario with real-world datasets, where the DQL node contends the same wireless channel with other nodes. Evaluated results demonstrate that the proposed approach learns an optimal policy without using complex mechanism or prior. Moreover, we consider realistic cases involving saturated or unsaturated uplink traffic flows of nodes on a freeway segment, and the on-line training strategies of the DQL node near the roadside facilities. The experimental results show that the proposed scheme leads to the highest throughput in all cases compared with the competing approaches.","1938-1883","978-1-5386-8347-7","10.1109/ICC45855.2022.9838614","Research and Development; Research and Development; State Key Laboratory of Rail Traffic Control and Safety; Fundamental Research Funds for the Central Universities; Royal Society; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9838614","Deep reinforcement learning;medium access control layer;Internet of Things","Access control;Training;Q-learning;Wireless networks;Memory modules;Traffic control;Throughput","cognitive radio;learning (artificial intelligence);radio networks;resource allocation;wireless channels","dynamic IoT networks;next-generation wireless communication systems;learning-based dynamic spectrum access strategy;medium access control layer;physical layer;optimal resources allocation;harmonious coexistence;heterogeneous wireless networks;multiple access control method;high network throughput;deep reinforcement learning;memory module;bidirectional gated recurrent unit;deep Q-learning;DQL node;wireless channel;optimal policy;on-line training strategies","","","","28","IEEE","11 Aug 2022","","","IEEE","IEEE Conferences"
"Quantum Reinforcement Learning","D. Dong; C. Chen; H. Li; T. -J. Tarn","Institute of Systems Science, Academy of Mathematics and Systems Science, Key Laboratory of Systems and Control, Chinese Academy and Sciences, Beijing, China; Department of Control and System Engineering, Nanjing University, Nanjing, China; Department of Manufacturing Engineering and Engineering Management, City University of Hong Kong, Kowloon Tong, Hong Kong, China; Department of Electrical and Systems Engineering, Washington University of Saint Louis, Saint Louis, MO, USA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","9 Sep 2008","2008","38","5","1207","1220","The key approaches for machine learning, particularly learning in unknown probabilistic environments, are new representations and computation mechanisms. In this paper, a novel quantum reinforcement learning (QRL) method is proposed by combining quantum theory and reinforcement learning (RL). Inspired by the state superposition principle and quantum parallelism, a framework of a value-updating algorithm is introduced. The state (action) in traditional RL is identified as the eigen state (eigen action) in QRL. The state (action) set can be represented with a quantum superposition state, and the eigen state (eigen action) can be obtained by randomly observing the simulated quantum state according to the collapse postulate of quantum measurement. The probability of the eigen action is determined by the probability amplitude, which is updated in parallel according to rewards. Some related characteristics of QRL such as convergence, optimality, and balancing between exploration and exploitation are also analyzed, which shows that this approach makes a good tradeoff between exploration and exploitation using the probability amplitude and can speedup learning through the quantum parallelism. To evaluate the performance and practicability of QRL, several simulated experiments are given, and the results demonstrate the effectiveness and superiority of the QRL algorithm for some complex problems. This paper is also an effective exploration on the application of quantum computation to artificial intelligence.","1941-0492","","10.1109/TSMCB.2008.925743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579244","Collapse;Grover iteration;probability amplitude;quantum reinforcement learning (QRL);state superposition;Collapse;Grover iteration;probability amplitude;quantum reinforcement learning (QRL);state superposition","Machine learning;Quantum computing;Quantum mechanics;Parallel processing;Artificial intelligence;Control systems;Systems engineering and theory;Machine learning algorithms;Convergence;Computational modeling","eigenvalues and eigenfunctions;learning (artificial intelligence);probability;quantum computing;quantum theory","quantum reinforcement learning;machine learning;quantum theory;state superposition principle;quantum parallelism;value-updating algorithm;quantum superposition state;eigen state;eigen action;quantum measurement;probability amplitude;quantum computation;artificial intelligence","Artificial Intelligence;Biomimetics;Computer Simulation;Humans;Models, Biological;Pattern Recognition, Automated;Quantum Theory;Reinforcement (Psychology)","201","2","53","IEEE","25 Jul 2008","","","IEEE","IEEE Journals"
"Ensemble Reinforcement Learning-Based Supervisory Control of Hybrid Electric Vehicle for Fuel Economy Improvement","B. Xu; X. Hu; X. Tang; X. Lin; H. Li; D. Rathod; Z. Filipi","Department of Automotive Engineering, Clemson University, Greenville, USA; Department of Automotive Engineering, Clemson University, Greenville, USA; Department of Automotive Engineering, Chongqing University, Chongqing, China; Department of Automotive, Mechanical and Manufacturing Engineering, Ontario Tech University, Oshawa, Canada; Department of Aerospace Engineering, University of Michigan, Ann Arbor, USA; Department of Automotive Engineering, Clemson University, Greenville, USA; Department of Automotive Engineering, Clemson University, Greenville, USA","IEEE Transactions on Transportation Electrification","19 Jun 2020","2020","6","2","717","727","This study proposes an ensemble reinforcement learning (RL) strategy to improve the fuel economy. A parallel hybrid electric vehicle model is first presented, followed by an introduction of ensemble RL strategy. The base RL algorithm is  $Q$ -learning, which is used to form multiple agents with different state combinations. Two common energy management strategies, namely, thermostatic strategy and equivalent consumption minimization strategy, are used as two single agents in the proposed ensemble agents. During the learning process, multiple RL agents make an action decision jointly by taking a weighted average. After each driving cycle iteration,  $Q$ -learning agents update their state-action values. A single RL agent is used as a reference for the proposed strategy. The results show that the fuel economy of the proposed ensemble strategy is 3.2% higher than that of the best single agent.","2332-7782","","10.1109/TTE.2020.2991079","National Natural Science Foundation of China(grant numbers:51705044,51875054); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9080091","Energy management strategy;hybrid electric vehicle (HEV);Q-learning;real-time implementation;reinforcement learning (RL)","State of charge;Supervisory control;Batteries;Hybrid electric vehicles;Engines;Fuel economy","energy management systems;fuel economy;hybrid electric vehicles;learning (artificial intelligence);multi-agent systems;power engineering computing;power system control;thermostats","ensemble reinforcement learning;fuel economy improvement;ensemble reinforcement learning strategy;parallel hybrid electric vehicle model;ensemble RL strategy;energy management strategies;thermostatic strategy;equivalent consumption minimization strategy;ensemble agents;learning process;multiple RL agents;Q-learning agents;single RL agent;ensemble strategy;supervisory control","","47","","36","IEEE","28 Apr 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Controller for SOC Management of Multi-Electrical Energy Storage System","F. Sanchez Gorostiza; F. M. Gonzalez-Longatt","Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, U.K.; Department of Electrical Engineering, Information Technology and Cybernetics, University of South-Eastern Norway, Porsgrunn, Norway","IEEE Transactions on Smart Grid","20 Oct 2020","2020","11","6","5039","5050","The ongoing reduction of the total rotational inertia in modern power systems brings about faster frequency dynamics that must be limited to maintain a secure and economical operation. Electrical energy storage systems (EESSs) have become increasingly attractive to provide fast frequency response services due to their response times. However, proper management of their finite energy reserves is required to ensure timely and secure operation. This paper proposes a deep reinforcement learning (DRL) based controller to manage the state of charge (SOC) of a Multi-EESS (M-EESS), providing frequency response services to the power grid. The proposed DRL agent is trained using an actor-critic method called Deep Deterministic Policy Gradients (DDPG) that allows for continuous action and smoother SOC control of the M-EESS. Deep neural networks (DNNs) are used to represent the actor and critic policies. The proposed strategy comprises granting the agent a constant reward for each time step that the SOC is within a specific band of its target value combined with a substantial penalty if the SOC reaches its minimum or maximum allowable values. The proposed controller is compared to benchmark DRL methods and other control techniques, i.e., Fuzzy Logic and a traditional PID control. Simulation results show the effectiveness of the proposed approach.","1949-3061","","10.1109/TSG.2020.2996274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9097915","Electrical energy storage systems;frequency response;state of charge control;reinforcement learning","State of charge;Frequency response;Frequency control;Energy storage;Time-frequency analysis;Power system stability","energy storage;frequency response;learning systems;neurocontrollers;power generation control;power grids;power system control;three-term control","benchmark DRL methods;PID control;SOC management;total rotational inertia;modern power systems;frequency dynamics;secure operation;electrical energy storage systems;fast frequency response services;finite energy reserves;deep reinforcement learning based controller;MultiEESS;M-EESS;power grid;DRL agent;actor-critic method;deep deterministic policy gradients;SOC control;deep neural networks","","42","","37","IEEE","21 May 2020","","","IEEE","IEEE Journals"
"Incremental Reinforcement Learning With Prioritized Sweeping for Dynamic Environments","Z. Wang; C. Chen; H. -X. Li; D. Dong; T. -J. Tarn","Department of Control and Systems Engineering, Nanjing University, Nanjing, China; Department of Control and Systems Engineering, Nanjing University, Nanjing, China; State Key Laboratory of High Performance Complex Manufacturing, Central South University, Changsha, China; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia; Department of Electrical and Systems Engineering, Washington University in St. Louis, St. Louis, MO, USA","IEEE/ASME Transactions on Mechatronics","16 Apr 2019","2019","24","2","621","632","In this paper, a novel incremental learning algorithm is presented for reinforcement learning (RL) in dynamic environments, where the rewards of state-action pairs may change over time. The proposed incremental RL (IRL) algorithm learns from the dynamic environments without making any assumptions or having any prior knowledge about the ever-changing environment. First, IRL generates a detector-agent to detect the changed part of the environment (drift environment) by executing a virtual RL process. Then, the agent gives priority to the drift environment and its neighbor environment for iteratively updating their state-action value functions using new rewards by dynamic programming. After the prioritized sweeping process, IRL restarts a canonical learning process to obtain a new optimal policy adapting to the new environment. The novelty is that IRL fuses the new information into the existing knowledge system incrementally as well as weakening the conflict between them. The IRL algorithm is compared to two direct approaches and various state-of-the-art transfer learning methods for classical maze navigation problems and an intelligent warehouse with multiple robots. The experimental results verify that IRL can effectively improve the adaptability and efficiency of RL algorithms in dynamic environments.","1941-014X","","10.1109/TMECH.2019.2899365","National Natural Science Foundation of China(grant numbers:71732003,61828303,61432008); National Key Research and Development Program of China(grant numbers:2016YFD0702100); RGC of Hong Kong(grant numbers:CityU: 11205615); Australian Research Council(grant numbers:DP190101566); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642342","Dynamic environments;environment drift;incremental reinforcement learning (IRL);intelligent warehouses;prioritized sweeping","Heuristic algorithms;Reinforcement learning;Knowledge based systems;Robots;Task analysis","dynamic programming;learning (artificial intelligence)","dynamic environments;drift environment;virtual RL process;neighbor environment;state-action value functions;prioritized sweeping process;canonical learning process;IRL algorithm;RL algorithms;incremental reinforcement learning;state-action pairs;incremental RL;incremental learning algorithm","","36","","43","IEEE","14 Feb 2019","","","IEEE","IEEE Journals"
"Intelligent Fault Diagnosis for Planetary Gearbox Using Time-Frequency Representation and Deep Reinforcement Learning","H. Wang; J. Xu; C. Sun; R. Yan; X. Chen","School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Mechanical Engineering, Xi'an Jiaotong University, Xi'an, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Mechanical Engineering, Xi'an Jiaotong University, Xi'an, China","IEEE/ASME Transactions on Mechatronics","15 Apr 2022","2022","27","2","985","998","Accurately and intelligently identifying faults of the planetary gearbox is essential in the safe and reliable operation and maintenance of the mechanical drive system. Recently, fault diagnosis of planetary gearbox has acquired tremendous progress, especially with the rising popularity of deep learning (DL). However, most methods are standard supervised learning where the input is directly mapped to a fault type, and with strong feedback. Also, their learning ways are static and unlike human learning that gradually acquires knowledge by interaction with the environment. To a certain extent, these deficiencies reduce the generalization and intelligence level of DL-based fault diagnosis methods. Besides, due to harsh working conditions, signals acquired often have strong noise and nonlinear features, leading to relatively low accuracy if raw signals are used as the input directly. Thus, this article proposes a new fault diagnosis method based on time-frequency representation and deep reinforcement learning (DRL). We first define fault diagnosis as a sequential decision-making problem in the classification Markov decision process. Next, the vibration signals are converted to uniform-sized TF maps by synchro-extracting transform to enhance the robustness of feature representation. Finally, a diagnosis agent is built and trained in the framework of DRL to learn the optimal classification policy automatically. Experimental results show that this method not only achieves better generalization and stability with an overall accuracy of over 99.5% in single-speed load cases but also outperforms others in multiwork conditions.","1941-014X","","10.1109/TMECH.2021.3076775","National Natural Science Foundation of China(grant numbers:51835009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9420305","Classification policy;deep learning (DL);deep reinforcement learning (DRL);fault diagnosis;planetary gearbox","Fault diagnosis;Games;Feature extraction;Vibrations;Decision making;Mechatronics","decision making;fault diagnosis;learning (artificial intelligence);Markov processes;vibrations","planetary gearbox;time-frequency representation;deep reinforcement learning;maintenance;mechanical drive system;deep learning;standard supervised learning;fault type;strong feedback;learning ways;human learning;intelligence level;DL-based fault diagnosis methods;strong noise;nonlinear features;raw signals;fault diagnosis method;classification Markov decision process;feature representation;diagnosis agent;intelligent fault diagnosis","","28","","49","IEEE","30 Apr 2021","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Relay Selection in Intelligent Reflecting Surface Assisted Cooperative Networks","C. Huang; G. Chen; Y. Gong; M. Wen; J. A. Chambers","School of Engineering, University of Leicester, Leicester, U.K.; School of Engineering, University of Leicester, Leicester, U.K.; Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, U.K.; National Engineering Technology Research Center for Mobile Ultrasonic Detection, South China University of Technology, Guangzhou, China; School of Engineering, University of Leicester, Leicester, U.K.","IEEE Wireless Communications Letters","7 May 2021","2021","10","5","1036","1040","This letter proposes a deep reinforcement learning (DRL) based relay selection scheme for cooperative networks with the intelligent reflecting surface (IRS). We consider a practical phase-dependent amplitude model in which the IRS reflection amplitudes vary with the discrete phase-shifts. Furthermore, we apply the relay selection to reduce the signal loss over distance in IRS-assisted networks. To solve the complicated problem of joint relay selection and IRS reflection coefficient optimization, we introduce DRL to learn from the environment to obtain the solution and reduce the computational complexity. Simulation results show that the throughput is significantly improved with the proposed DRL-based algorithm compared to random relay selection and random reflection coefficients methods.","2162-2345","","10.1109/LWC.2021.3056620","Engineering and Physical Sciences Research Council(grant numbers:EP/R006377/1 (“M3NET”)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9344820","Intelligent reflecting surface (IRS);relay selection;throughput;deep reinforcement learning","Relays;Optimization;Computational complexity;Throughput;Cooperative systems;Rician channels;Signal to noise ratio","computational complexity;cooperative communication;deep learning (artificial intelligence);optimisation;relay networks (telecommunication);telecommunication computing","random reflection coefficients methods;deep reinforcement learning based relay selection scheme;practical phase-dependent amplitude model;IRS reflection amplitudes;discrete phase-shifts;IRS-assisted networks;joint relay selection;IRS reflection coefficient optimization;DRL-based algorithm;intelligent reflecting surface assisted cooperative networks","","27","","21","IEEE","2 Feb 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Optimal Sensor Placement for Spatiotemporal Modeling","Z. Wang; H. -X. Li; C. Chen","Department of Systems Engineering and Engineering Management, City University of Hong Kong, Hong Kong, China; Department of Systems Engineering and Engineering Management, City University of Hong Kong, Hong Kong; Department of Control and Systems Engineering, Nanjing University, Nanjing, China","IEEE Transactions on Cybernetics","7 May 2020","2020","50","6","2861","2871","A reinforcement learning-based method is proposed for optimal sensor placement in the spatial domain for modeling distributed parameter systems (DPSs). First, a low-dimensional subspace, derived by Karhunen-Loève decomposition, is identified to capture the dominant dynamic features of the DPS. Second, a spatial objective function is proposed for the sensor placement. This function is defined in the obtained low-dimensional subspace by exploiting the time-space separation property of distributed processes, and in turn aims at minimizing the modeling error over the entire time and space domain. Third, the sensor placement configuration is mathematically formulated as a Markov decision process (MDP) with specified elements. Finally, the sensor locations are optimized through learning the optimal policies of the MDP according to the spatial objective function. The experimental results of a simulated catalytic rod and a real snap curing oven system are provided to demonstrate the feasibility and efficiency of the proposed method in solving the combinatorial optimization problems, such as optimal sensor placement.","2168-2275","","10.1109/TCYB.2019.2901897","GRF Project from the RGC of Hong Kong(grant numbers:11205615); National Basic Research Program of China (973 Program)(grant numbers:2016YFD0702100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668561","Distributed parameter systems (DPSs);Karhunen–Loève decomposition (KLD);optimal sensor placement;reinforcement learning (RL);spatiotemporal modeling","Spatiotemporal phenomena;Optimization;Linear programming;Modeling;Mathematical model;Nonlinear dynamical systems","distributed parameter systems;learning (artificial intelligence);Markov processes;optimisation;sensor placement","sensor locations;spatial objective function;sensor placement configuration;space domain;distributed processes;time-space separation property;Karhunen-Loève decomposition;distributed parameter systems;reinforcement learning-based method;spatiotemporal modeling;reinforcement learning-based optimal sensor placement;combinatorial optimization problems","","23","","58","IEEE","17 Mar 2019","","","IEEE","IEEE Journals"
"Transfer Deep Reinforcement Learning-Enabled Energy Management Strategy for Hybrid Tracked Vehicle","X. Guo; T. Liu; B. Tang; X. Tang; J. Zhang; W. Tan; S. Jin","School of Robot Engineering, Yangtze Normal University, Fuling, China; College of Automotive Engineering, Chongqing University, Chongqing, China; School of Intelligent Manufacturing Engineering, Chongqing University of Arts and Sciences, Chongqing, China; College of Automotive Engineering, Chongqing University, Chongqing, China; Department of Mechanical and Mechatronics Engineering, University of Waterloo, Waterloo, Canada; College of Automotive Engineering, Chongqing University, Chongqing, China; College of Automotive Engineering, Chongqing University, Chongqing, China","IEEE Access","18 Sep 2020","2020","8","","165837","165848","This paper proposes an adaptive energy management strategy for hybrid electric vehicles by combining deep reinforcement learning (DRL) and transfer learning (TL). This work aims to address the defect of DRL in tedious training time. First, an optimization control modeling of a hybrid tracked vehicle is built, wherein the elaborate powertrain components are introduced. Then, a bi-level control framework is constructed to derive the energy management strategies (EMSs). The upper-level is applying the particular deep deterministic policy gradient (DDPG) algorithms for EMS training at different speed intervals. The lower-level is employing the TL method to transform the pre-trained neural networks for a novel driving cycle. Finally, a series of experiments are executed to prove the effectiveness of the presented control framework. The optimality and adaptability of the formulated EMS are illuminated. The founded DRL and TL-enabled control policy is capable of enhancing energy efficiency and improving system performance.","2169-3536","","10.1109/ACCESS.2020.3022944","Chongqing Science and Technology Project(grant numbers:cstc2019jcyj-msxmX0636,cstc2019jcyj-msxmX0481); Research Startup Project of Yangtze Normal University(grant numbers:2016KYQD16); Chongqing Education Commission Project(grant numbers:KJ1712297,KJQN201901321); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9189810","Deep reinforcement learning;transfer learning;hybrid tracked vehicle;energy management strategy;deep deterministic policy gradient","Energy management;Batteries;Generators;Resistance;Mechanical power transmission;Engines;Training","energy management systems;hybrid electric vehicles;learning (artificial intelligence);neural nets;optimal control;tracked vehicles","transfer deep reinforcement learning-enabled energy management;optimization control modeling;training time;transfer learning;hybrid electric vehicles;adaptive energy management;energy efficiency;TL;DRL;adaptability;pre-trained neural networks;EMS training;deep deterministic policy gradient algorithms;bi-level control framework;powertrain components;hybrid tracked vehicle","","20","","38","CCBY","9 Sep 2020","","","IEEE","IEEE Journals"
"Buffer-Aided Relay Selection for Cooperative Hybrid NOMA/OMA Networks With Asynchronous Deep Reinforcement Learning","C. Huang; G. Chen; Y. Gong; P. Xu; Z. Han; J. A. Chambers","School of Engineering, University of Leicester, Leicester, U.K; School of Engineering, University of Leicester, Leicester, U.K; Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, U.K; Chongqing Key Laboratory of Mobile Communications Technology, School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; School of Engineering, University of Leicester, Leicester, U.K","IEEE Journal on Selected Areas in Communications","15 Jul 2021","2021","39","8","2514","2525","This paper investigates asynchronous reinforcement learning algorithms for joint buffer-aided relay selection and power allocation in the non-orthogonal-multiple-access (NOMA) relay network. With the hybrid NOMA/OMA transmission, we investigate joint relay selection and power allocation to maximize the throughput with the delay constraint. To solve this complicated high-dimensional optimization problem, we propose two asynchronous reinforcement learning-based schemes: the asynchronous deep Q-Learning network (ADQN)-based scheme and the asynchronous advantage actor-critic (A3C)-based scheme, respectively. The A3C-based scheme achieves better performance and robustness when the action space is large, while the ADQN-based scheme converges faster with a small action space. Moreover, a-prior information is exploited to improve the convergence of the proposed schemes. The simulation results show that the proposed asynchronous learning-based schemes can learn from the environment and achieve good convergence.","1558-0008","","10.1109/JSAC.2021.3087225","Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/R006377/1 (“M3NETs”)); Chongqing Natural Science Foundation Project(grant numbers:cstc2019jcyj-msxmX0032); National Natural Science Foundation of China(grant numbers:61701066,61971080); NSF(grant numbers:EARS-1839818,CNS1717454,CNS-1731424,CNS-1702850); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448155","Buffer-aided relay selection;NOMA;power allocation;asynchronous reinforcement learning","Relays;NOMA;Resource management;Delays;Throughput;Reinforcement learning;Convergence","cooperative communication;deep learning (artificial intelligence);nonorthogonal multiple access;optimisation;relay networks (telecommunication);resource allocation;telecommunication computing","a-prior information;A3C-based scheme;asynchronous deep Q-Learning network;delay constraint;cooperative hybrid NOMA-OMA networks;hybrid NOMA-OMA transmission;nonorthogonal-multiple-access relay network;joint buffer-aided relay selection;asynchronous deep reinforcement Learning;asynchronous learning-based schemes;ADQN-based scheme;asynchronous advantage actor-critic-based scheme;asynchronous reinforcement learning-based schemes;high-dimensional optimization problem;power allocation","","19","","41","IEEE","7 Jun 2021","","","IEEE","IEEE Journals"
"Incremental Reinforcement Learning in Continuous Spaces via Policy Relaxation and Importance Weighting","Z. Wang; H. -X. Li; C. Chen","Department of Control and Systems Engineering, School of Management and Engineering, Nanjing University, Nanjing, China; Department of Systems Engineering and Engineering Management, City University of Hong Kong, Hong Kong; Department of Control and Systems Engineering, School of Management and Engineering, Nanjing University, Nanjing, China","IEEE Transactions on Neural Networks and Learning Systems","2 Jun 2020","2020","31","6","1870","1883","In this paper, a systematic incremental learning method is presented for reinforcement learning in continuous spaces where the learning environment is dynamic. The goal is to adjust the previously learned policy in the original environment to a new one incrementally whenever the environment changes. To improve the adaptability to the ever-changing environment, we propose a two-step solution incorporated with the incremental learning procedure: policy relaxation and importance weighting. First, the behavior policy is relaxed to a random one in the initial learning episodes to encourage a proper exploration in the new environment. It alleviates the conflict between the new information and the existing knowledge for a better adaptation in the long term. Second, it is observed that episodes receiving higher returns are more in line with the new environment, and hence contain more new information. During parameter updating, we assign higher importance weights to the learning episodes that contain more new information, thus encouraging the previous optimal policy to be faster adapted to a new one that fits in the new environment. Empirical studies on continuous controlling tasks with varying configurations verify that the proposed method achieves a significantly faster adaptation to various dynamic environments than the baselines.","2162-2388","","10.1109/TNNLS.2019.2927320","General Research Fund Project from the Research Grant Council of Hong Kong SAR(grant numbers:11210719); Project from the City University of Hong Kong(grant numbers:7005092); National Basic Research Program of China (973 Program)(grant numbers:2016YFD0702100); Fundamental Research Funds for the Central Universities(grant numbers:011814380035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8786875","Continuous spaces;dynamic environments;importance weighting;incremental reinforcement learning (RL);policy relaxation","Task analysis;Learning systems;Heuristic algorithms;Function approximation;Robots;Navigation;Neural networks","learning (artificial intelligence)","incremental reinforcement learning;continuous spaces;policy relaxation;systematic incremental learning method;learning environment;learned policy;original environment;environment changes;importance weighting;behavior policy;initial learning episodes;optimal policy;continuous controlling tasks;dynamic environments","","17","","58","IEEE","5 Aug 2019","","","IEEE","IEEE Journals"
"Joint Trajectory and Passive Beamforming Design for Intelligent Reflecting Surface-Aided UAV Communications: A Deep Reinforcement Learning Approach","L. Wang; K. Wang; C. Pan; N. Aslam","School of Aerospace, Transport and Manufacturing, Cranfield University, Milton Keynes, U.K.; Department of Computer and Information Science, Northumbria University, Newcastle upon Tyne, U.K.; National Mobile Communications Research Laboratory, Southeast University, Nanjing, Jiangsu, China; Department of Computer and Information Science, Northumbria University, Newcastle upon Tyne, U.K.","IEEE Transactions on Mobile Computing","3 Oct 2023","2023","22","11","6543","6553","In this paper, the intelligent reflecting surface (IRS)-aided unmanned aerial vehicle (UAV) communication system is studied, where the UAV is deployed to serve the user equipment (UE) with the assistance of multiple IRSs mounted on several buildings to enhance the communication quality between UAV and UE. We aim to maximize the energy efficiency of the system, including the data rate of UE and the energy consumption of UAV via jointly optimizing the UAV's trajectory and the phase shifts of reflecting elements of IRS, when the UE moves and the selection of IRSs is considered for the energy saving purpose. Since the system is complex and the environment is dynamic, it is challenging to derive low-complexity algorithms by using conventional optimization methods. To address this issue, we first propose a deep Q-network (DQN)-based algorithm by discretizing the trajectory, which has the advantage of training time. Furthermore, we propose a deep deterministic policy gradient (DDPG)-based algorithm to tackle the case with continuous trajectory for achieving better performance. The experimental results show that the proposed algorithms achieve considerable performance compared to other traditional solutions.","1558-0660","","10.1109/TMC.2022.3200998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869783","Deep Reinforcement learning;UAV communications;intelligent reflecting surface","Trajectory;Autonomous aerial vehicles;Array signal processing;Wireless communication;Minimization;Energy efficiency;Rotors","","","","12","","40","IEEE","29 Aug 2022","","","IEEE","IEEE Journals"
"Delay-Constrained Buffer-Aided Relay Selection in the Internet of Things With Decision-Assisted Reinforcement Learning","C. Huang; G. Chen; Y. Gong","School of Engineering, University of Leicester, Leicester, U.K.; School of Engineering, University of Leicester, Leicester, U.K.; Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, U.K.","IEEE Internet of Things Journal","4 Jun 2021","2021","8","12","10198","10208","This article investigates the reinforcement learning for the relay selection in the delay-constrained buffer-aided networks. The buffer-aided relay selection significantly improves the outage performance but often at the price of higher latency. On the other hand, modern communication systems such as the Internet of Things often have strict requirement on latency. It is thus necessary to find relay selection policies to achieve good throughput performance in the buffer-aided relay network while stratifying the delay constraint. With the buffers employed at the relays and delay constraints imposed on the data transmission, obtaining the best relay selection becomes a complicated high-dimensional problem, making it hard for the reinforcement learning to converge. In this article, we propose the novel decision-assisted deep reinforcement learning to improve the convergence. This is achieved by exploring the a priori information from the buffer-aided relay system. The proposed approaches can achieve high throughput subject to delay constraints. Extensive simulation results are provided to verify the proposed algorithms.","2327-4662","","10.1109/JIOT.2021.3051239","Engineering and Physical Sciences Research Council(grant numbers:EP/R006377/1 (“M3NET”)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321455","Buffer-aided relay selection;deep reinforcement learning;delay constrained;Q-learning;Sarsa learning","Delays;Reinforcement learning;Relay networks (telecommunication);Throughput;Internet of Things;Data models;Convergence","convergence of numerical methods;decode and forward communication;inductive power transmission;Internet of Things;learning (artificial intelligence);power engineering computing;relay networks (telecommunication);telecommunication network reliability","delay-constrained buffer-aided relay selection;Internet of Things;decision-assisted reinforcement learning;communication systems;convergence method","","12","","39","IEEE","13 Jan 2021","","","IEEE","IEEE Journals"
"Minimalistic Attacks: How Little It Takes to Fool Deep Reinforcement Learning Policies","X. Qu; Z. Sun; Y. -S. Ong; A. Gupta; P. Wei","Computational Intelligence Lab, School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Data Science and Artificial Intelligence Research Centre, School of Computer Science and Engineering, Nanyang Technological University, Singapore; Singapore Institute of Manufacturing Technology, A*STAR, Singapore; Department of Computer Science, National University of Singapore, Singapore","IEEE Transactions on Cognitive and Developmental Systems","10 Dec 2021","2021","13","4","806","817","Recent studies have revealed that neural-network-based policies can be easily fooled by adversarial examples. However, while most prior works analyze the effects of perturbing every pixel of every frame assuming white-box policy access, in this article, we take a more restrictive view toward adversary generation—with the goal of unveiling the limits of a model’s vulnerability. In particular, we explore minimalistic attacks by defining three key settings: 1) Black-Box Policy Access: where the attacker only has access to the input (state) and output (action probability) of an RL policy; 2) Fractional-State Adversary: where only several pixels are perturbed, with the extreme case being a single-pixel adversary; and 3) Tactically Chanced Attack: where only significant frames are tactically chosen to be attacked. We formulate the adversarial attack by accommodating the three key settings, and explore their potency on six Atari games by examining four fully trained state-of-the-art policies. In Breakout, for example, we surprisingly find that: 1) all policies showcase significant performance degradation by merely modifying 0.01% of the input state and 2) the policy trained by DQN is totally deceived by perturbing only 1% frames.","2379-8939","","10.1109/TCDS.2020.2974509","National Research Foundation, Singapore under its AI Singapore Programme(grant numbers:AISG-RP-2018-004); Data Science and Artificial Intelligence Research Center at Nanyang Technological University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003391","Adversarial attack;reinforcement learning (RL)","Optimization;Neural networks;Games;Perturbation methods;Learning (artificial intelligence);Analytical models;Reinforcement learning;Generative adversarial networks","computer crime;deep learning (artificial intelligence);probability;reinforcement learning","minimalistic attacks;deep reinforcement learning policies;neural-network-based policies;adversarial examples;white-box policy access;adversary generation;action probability;RL policy;single-pixel adversary;adversarial attack;black-box policy access;fractional-state adversary;tactically chanced attack","","10","","47","IEEE","19 Feb 2020","","","IEEE","IEEE Journals"
"Reconfigurable and Traffic-Aware MAC Design for Virtualized Wireless Networks via Reinforcement Learning","A. Dalili Shoaei; M. Derakhshani; T. Le-Ngoc","Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, U.K.; Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada","IEEE Transactions on Communications","14 Aug 2019","2019","67","8","5490","5505","In this paper, we present a reconfigurable MAC scheme where the partition between contention-free and contention-based regimes in each frame is adaptive to the network status leveraging reinforcement learning. In particular, to support a virtualized wireless network consisting of multiple slices, each having heterogeneous and unsaturated devices, the proposed scheme aims to configure the partition for maximizing network throughput while maintaining the slice reservations. Applying complementary geometric programming and monomial approximations, an iterative algorithm is developed to find the optimal solution. For a large number of devices, a scalable algorithm with lower computational complexity is also proposed. The partitioning algorithm requires the knowledge of the device traffic statistics. In the absence of such knowledge, we develop a learning algorithm employing Thompson sampling to acquire packet arrival probabilities of devices. Furthermore, we model the problem as a thresholding multi-armed bandit and propose a threshold-based reconfigurable MAC algorithm, which is proved to achieve the optimal regret bound.","1558-0857","","10.1109/TCOMM.2019.2913413","Huawei Technologies; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8700494","Machine learning;network function virtualization;wireless networks","Partitioning algorithms;Wireless networks;Throughput;Machine-to-machine communications;Media Access Protocol;Approximation algorithms;Reinforcement learning","access protocols;approximation theory;computational complexity;geometric programming;learning (artificial intelligence);probability;radio networks;telecommunication traffic","traffic-aware MAC design;virtualized wireless network;reconfigurable MAC scheme;contention-based regimes;heterogeneous devices;unsaturated devices;slice reservations;complementary geometric programming;monomial approximations;iterative algorithm;scalable algorithm;partitioning algorithm;device traffic statistics;learning algorithm;threshold-based reconfigurable MAC algorithm;reinforcement learning;Thompson sampling;packet arrival probabilities;contention-free regimes;optimal regret bound","","9","","29","IEEE","26 Apr 2019","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Relay Selection in Delay-Constrained Secure Buffer-Aided CRNs","C. Huang; G. Chen; Y. Gong; P. Xu","Department of Engineering, University of Leicester, Leicester, UK; Department of Engineering, University of Leicester, Leicester, UK; School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, Leicestershire, UK; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing City, P.R. China","GLOBECOM 2020 - 2020 IEEE Global Communications Conference","25 Jan 2021","2020","","","1","6","In this paper, we investigate a Deep Reinforcement Learning based delay-constrained relay selection for secure buffer aided Cognitive Relay Networks (CRNs). We model the relay selection problem in secure butter-aided CRNs as a Markov Decision Process (MDP) problem, and introduce Deep Q-Learning to solve this MDP problem. In the proposed scheme, delay constraint is considered when the packets arriving at the receiver in CRNs. Moreover, we consider the security of data transmissions in butter-aided CRNs with an eavesdropper which can intercept the signals from the source and relays. Furthermore, we introduce ε-greedy strategy to balance the exploitation and exploration. The result shows compared with Max-Ratio scheme, the proposed scheme enhances the throughput with both delay and security constrained significantly in secure CRNs.","2576-6813","978-1-7281-8298-8","10.1109/GLOBECOM42002.2020.9322098","National Natural Science Foundation of China(grant numbers:61701066); EPSRC(grant numbers:EP/R006377/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9322098","Buffer-aided relay selection;cognitive relay networks;deep reinforcement learning;secure wireless communications;Q-Learning;throughput;delay-constrained","Relays;Delays;Throughput;Security;Reinforcement learning;Wireless communication;Relay networks (telecommunication)","cognitive radio;deep learning (artificial intelligence);greedy algorithms;Markov processes;radio receivers;relay networks (telecommunication);telecommunication computing;telecommunication security","cognitive relay networks;relay selection problem;Markov decision process problem;deep Q-learning;MDP problem;delay-constrained secure buffer-aided CRN;delay-constrained relay selection;deep reinforcement learning based delay-constrained relay selection;ε-greedy strategy;max-ratio scheme","","8","","26","IEEE","25 Jan 2021","","","IEEE","IEEE Conferences"
"Efficient Reinforcement Learning-Based Transmission Control for Mitigating Channel Congestion in 5G V2X Sidelink","L. -H. Nguyen; V. -L. Nguyen; J. -J. Kuo","Department of Computer Science and Information Engineering, National Chung Cheng University, Minhsiung, Chiayi, Taiwan; Department of Computer Science and Information Engineering, National Chung Cheng University, Minhsiung, Chiayi, Taiwan; Department of Computer Science and Information Engineering, National Chung Cheng University, Minhsiung, Chiayi, Taiwan","IEEE Access","16 Jun 2022","2022","10","","62268","62281","Channel congestion has been an open challenge for vehicular networks due to the limited resource of communication channels. Explosion of channel access requests from a massive number of transmitter vehicles can exhaust bandwidth and then degrade transmission quality. The rapid drop of messages (because of the high bit error rate in the transmission congestion condition) can threaten the safety of connected vehicles. Maintaining congestion-free communications is then essential to improve the reliability for vehicular networks, including Cellular-V2X (C-V2X)-based cooperative intelligent transport systems and road-safety applications. In this work, we present a novel intelligent transmission control model, namely DEEPCUT, to automatically adjust the message broadcasting rate of a transmitter vehicle. DEEPCUT works based on a Double Deep Q-learning Networks with Prioritized Experience Relay framework. DEEPCUT encourages the transmitter vehicle to (1) reduce its broadcasting rate if the vehicle is maintaining a safe distance from its neighbors and (2) increase the rate if the vehicle is approaching the others at a high-risk distance, all done by using reward/punish strategies. The evaluation results show that DEEPCUT can cut up 16% redundant data while increasing 22% packet reception rate compared with baseline models, particularly in crowd vehicular communications. Our risk-based transmission control can be an excellent complement to address the congestion when the channel cannot satisfy every vehicle’s resource requests. At best, the risk assessment-based approach in our congestion control method can provide a novel material to enhance Decentralized Congestion Control (DCC) for 5G V2X sidelink in the coming specifications.","2169-3536","","10.1109/ACCESS.2022.3182021","Ministry of Science and Technology of Taiwan(grant numbers:MOST 107-2218-E-194-016-MY3,MOST 108-2221-E-194-025-MY3,MOST 110-2811-E-194-501-MY2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793684","Vehicular network congestion;transmission control;reinforcement learning","Safety;Vehicle-to-everything;5G mobile communication;Broadcasting;Transmitters;Bandwidth;Vehicle dynamics","5G mobile communication;cooperative communication;deep learning (artificial intelligence);error statistics;intelligent transportation systems;mobile computing;reinforcement learning;relay networks (telecommunication);road safety;road vehicles;telecommunication congestion control;telecommunication traffic;vehicular ad hoc networks","DEEPCUT;Double Deep Q-learning Networks;Prioritized Experience Relay framework;transmitter vehicle;packet reception rate;crowd vehicular communications;risk assessment-based approach;congestion control method;Decentralized Congestion Control;channel congestion;5G V2X Sidelink;vehicular networks;communication channels;channel access requests;transmission quality;bit error rate;transmission congestion condition;intelligent transport systems;road-safety applications;intelligent transmission control model;message broadcasting rate;congestion-free communications;cellular-V2X-based cooperative intelligent transport systems;C-V2X-based cooperative intelligent transport systems;DCC;5G V2X sidelink","","4","","44","CCBYNCND","10 Jun 2022","","","IEEE","IEEE Journals"
"Pricing-Based Deep Reinforcement Learning for Live Video Streaming With Joint User Association and Resource Management in Mobile Edge Computing","P. -Y. Chou; W. -Y. Chen; C. -Y. Wang; R. -H. Hwang; W. -T. Chen","Research Center for Information Technology Innovation (CITI), Academia Sinica, Taipei City, Nankang, Taiwan; Research Center for Information Technology Innovation (CITI), Academia Sinica, Taipei City, Nankang, Taiwan; Research Center for Information Technology Innovation (CITI), Academia Sinica, Taipei City, Nankang, Taiwan; Department of Computer Science and Information Engineering, Advanced Institute of Manufacturing With High-Tech Innovations, National Chung Cheng University, Chiayi City, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan","IEEE Transactions on Wireless Communications","9 Jun 2022","2022","21","6","4310","4324","Mobile Edge Computing (MEC) is a promising technique in the 5G Era to improve the Quality of Experience (QoE) for online video streaming due to its ability to reduce the backhaul transmission by caching certain content. However, it still takes effort to address the user association and video quality selection problem under the limited resource of MEC to fully support the low-latency demand for live video streaming. We found the optimization problem to be a non-linear integer programming, which is impossible to obtain a globally optimal solution under polynomial time. In this paper, we formulate the problem and derive the closed-form solution in the form of Lagrangian multipliers; the searching of the optimal variables is formulated as a Multi-Arm Bandit (MAB) and we propose a Deep Deterministic Policy Gradient (DDPG) based algorithm exploiting the supply-demand interpretation of the Lagrange dual problem. Simulation results show that our proposed approach achieves significant QoE improvement, especially in the low wireless resource and high user number scenario compared to other baselines.","1558-2248","","10.1109/TWC.2021.3128741","Ministry of Science and Technology(grant numbers:MOST 108-2628-E-001-003-MY3); Academia Sinica under Thematic Research Grant(grant numbers:AS-TP-110-M07-2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9626650","Mobile edge computing (MEC);scalable video coding (SVC);live video streaming;deep deterministic policy gradient (DDPG);dual pricing approach","Streaming media;Quality of experience;Wireless communication;Pricing;Resource management;Reinforcement learning;Optimization","5G mobile communication;cache storage;computational complexity;deep learning (artificial intelligence);gradient methods;integer programming;mobile computing;nonlinear programming;pricing;quality of experience;telecommunication computing;video streaming","supply-demand interpretation;DDPG based algorithm;MAB;multiarm bandit;polynomial time;user association;quality of experience;pricing-based deep reinforcement learning;mobile edge computing;low wireless resource;QoE improvement;Lagrange dual problem;deep deterministic policy gradient based algorithm;optimal variables;closed-form solution;nonlinear integer programming;optimization problem;live video streaming;low-latency demand;video quality selection problem;backhaul transmission;online video streaming;5G Era;MEC;resource management;joint user association","","3","","40","IEEE","24 Nov 2021","","","IEEE","IEEE Journals"
"Mobility Prediction Based Vehicular Edge Caching: A Deep Reinforcement Learning Based Approach","K. An; X. Yan; T. Liang; W. Lu","The Sixty-third Research Institute, National University of Defense Technology, Nanjing, China; Guangxi Ship Digital Design and Advanced, Manufacturing Research Center of Engineering Technology, Beibu Gulf University, Qinzhou, China; The Sixty-third Research Institute, National University of Defense Technology, Nanjing, China; The Sixty-third Research In, National University of Defense Technology, Nanjing, China","2019 IEEE 19th International Conference on Communication Technology (ICCT)","2 Jan 2020","2019","","","1120","1125","Caching on edge nodes can effectively reduce the burden on the Internet of Vehicles (IoV) networks. However, the inherent limitations of IoV networks, such as restricted storage capability of cache nodes and high mobility of vehicles may cause poor quality of services. Accurate prediction could achieve seamless switching between edge servers, reduce pre-fetch redundancy, and improve data transmission efficiency. This paper investigates how to pre-cache packets at edge nodes to speed up services to improve the user experience. We consider the trade-off between the modelling accuracy and computational complexity, and design a Markov Deep Q-Learning (MDQL) model to formulate the caching strategy. The k-order Markov model is first used to predict the mobility of vehicles, and the prediction results are used as the input of deep reinforcement learning (DRL) for training. The MDQL model can reduce the size of the action space and the computational complexity of DRL while considering the balance between the cache hit rate and the cache replacement rate. Experimental results demonstrate the effectiveness of the proposed method.","2576-7828","978-1-7281-0535-2","10.1109/ICCT46805.2019.8947024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8947024","edge caching;mobility prediction;deep reinforcement learning;internet of vehicles","Training;Computational modeling;Reinforcement learning;Predictive models;Markov processes;Prediction algorithms;User experience","cache storage;computational complexity;Internet of Things;learning (artificial intelligence);Markov processes;quality of service;redundancy;vehicular ad hoc networks","seamless switching;edge servers;pre-fetch redundancy;data transmission efficiency;pre-cache packets;edge nodes;modelling accuracy;computational complexity;Markov Deep Q-Learning model;caching strategy;k-order Markov model;deep reinforcement learning;MDQL model;cache hit rate;cache replacement rate;IoV networks;restricted storage capability;cache nodes;Internet of Vehicles networks;mobility prediction based vehicular edge caching;quality of services","","2","","30","IEEE","2 Jan 2020","","","IEEE","IEEE Conferences"
"An Efficient Unified Approach Using Demonstrations for Inverse Reinforcement Learning","M. Hwang; W. -C. Jiang; Y. -J. Chen; K. -S. Hwang; Y. -C. Tseng","Department of Colorectal Surgery, Second Affiliated Hospital of Zhejiang University School of Medicine, Hangzhou, China; Department of Electrical Engineering, Tunghai University, Taichung, Taiwan; Department of Electrical Engineering, National Chung Cheng University, Chiayi City, Taiwan; Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Taiwan Semiconductor Manufacturing Company, Hsinchu, Taiwan","IEEE Transactions on Cognitive and Developmental Systems","9 Sep 2021","2021","13","3","444","452","A reinforcement learning (RF) agent is always equipped with a designed reward function to correct policies for optimal decision making through interactions with an environment. However, it is difficult to design a reward function appropriate for complex RF problems. To solve this difficulty, the inverse RF (IRL) is introduced to provide an efficient way to design a reward function based on input derived from knowledgeable experts. In the IRL, experts provide demonstrations so that the agents can imitate the behaviors accordingly. However, even incorrect demonstrations have merits, some of which are similar to correct ones, so as that the agents with these clues can endeavor to avoid the occurrence of that behavior. This article introduces an IRL method which considers two types of demonstrations, correct and incorrect, in function approximation of a reward function. Given the clues from two opposite demonstrations, agents can iteratively approximate a reward function that can guide them to like expert’s correct demonstrations and also, prevent them from making the same mistakes as the expert did. These incorrect demonstrations provide agents with some guidelines to avoid erroneous motions in the initial phase. Two simulated tasks, a labyrinth and robot soccer games are conducted to validate the proposed method. The simulation results show that the proposed method can achieve the objectives of generating an appropriate reward function to accomplish apprentice learning with an efficient learning time in IRL.","2379-8939","","10.1109/TCDS.2019.2957831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924683","Feature weight;incorrect demonstrations;inverse reinforcement learning (IRL);reinforcement learning (RF)","Trajectory;Learning (artificial intelligence);Robots;Electronic mail;Electrical engineering;Mathematical model;Task analysis","decision making;function approximation;iterative methods;learning (artificial intelligence);mobile robots;multi-robot systems;robots","efficient unified approach;inverse reinforcement learning;reinforcement learning agent;designed reward function;complex RF problems;inverse RF;IRL;incorrect demonstrations;correct ones;function approximation;opposite demonstrations;expert;appropriate reward function;efficient learning time","","2","","23","IEEE","5 Dec 2019","","","IEEE","IEEE Journals"
"Rapid Adaptation for Active Pantograph Control in High-Speed Railway via Deep Meta Reinforcement Learning","H. Wang; Z. Liu; Z. Han; Y. Wu; D. Liu","School of Electrical Engineering, Southwest Jiaotong University, Chengdu, China; School of Electrical Engineering, Southwest Jiaotong University, Chengdu, China; School of Electrical Engineering, Southwest Jiaotong University, Chengdu, China; School of Electrical Engineering, Southwest Jiaotong University, Chengdu, China; School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, Shenzhen, China","IEEE Transactions on Cybernetics","","2023","PP","99","1","13","Active pantograph control is the most promising technique for reducing contact force (CF) fluctuation and improving the train’s current collection quality. Existing solutions, however, suffer from two significant limitations: 1) they are incapable of dealing with the various pantograph types, catenary line operating conditions, changing operating speeds, and contingencies well and 2) it is challenging to implement in practical systems due to the lack of rapid adaptability to a new pantograph-catenary system (PCS) operating conditions and environmental disturbances. In this work, we alleviate these problems by developing a revolutionary context-based deep meta-reinforcement learning (CB-DMRL) algorithm. The proposed CB-DMRL algorithm combines Bayesian optimization (BO) with deep reinforcement learning (DRL), allowing the general agent to adapt to new tasks quickly and efficiently. We evaluated the CB-DMRL algorithm’s performance on a proven PCS model. The experimental results demonstrate that meta-training DRL policies with latent space swiftly adapt to new operating conditions and unknown perturbations. The meta-agent adapts quickly after two iterations with a high reward, which require only ten spans, approximately equal to 0.5 km of PCS interaction data. Compared with state-of-the-art DRL algorithms and traditional solutions, the proposed method can promptly traverse scenario changes and reduce CF fluctuations, resulting in an excellent performance.","2168-2275","","10.1109/TCYB.2023.3271900","National Natural Science Foundation of China(grant numbers:51977182); Science and Technology Innovation Talents of Sichuan Science and Technology Plan(grant numbers:2021JDRC0008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10124089","Active pantograph control;deep meta-reinforcement learning;high-speed railways","Task analysis;Adaptation models;Heuristic algorithms;Reinforcement learning;Rail transportation;Training;Vehicle dynamics","","","","1","","","IEEE","12 May 2023","","","IEEE","IEEE Early Access Articles"
"Deep Reinforcement Learning-Based Resource Management for Flexible Mobile Edge Computing: Architectures, Applications, and Research Issues","K. Wang; L. Wang; C. Pan; H. Ren","Department of Computer and Information Sciences, Northumbria University, Newcastle, U.K.; School of Aerospace, Transport and Manufacturing, Cranfield University, Cranfield, U.K.; National Mobile Communications Research Lab, Southeast University, Nanjing, China; National Mobile Communications Research Lab, Southeast University, Nanjing, China","IEEE Vehicular Technology Magazine","25 May 2022","2022","17","2","85","93","In this article, we introduce autonomous vehicle-assisted mobile edge computing (AV-MEC), including unmanned ground vehicle (UGV)- and unmanned aerial vehicle (UAV)-assisted MEC, where the UAV/UGV can be deployed and carry the computing server to serve ground mobile devices (MDs). We first discuss applications and main research problems. Then, deep reinforcement learning (DRL)-based solutions are introduced, explored, and demonstrated. We also discuss challenges and future research directions for an AV-MEC system with DRL being applied to it.","1556-6080","","10.1109/MVT.2022.3156745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9760711","","Reinforcement learning;Computer architecture;Trajectory;Resource management;Quality of service;Q-learning;Servers;Deep learning;Edge computing;Mobile communication","autonomous aerial vehicles;learning (artificial intelligence);mobile computing;mobile robots;remotely operated vehicles","flexible mobile edge computing;research issues;autonomous vehicle-assisted mobile edge;computing server;ground mobile devices;main research problems;deep reinforcement learning-based solutions;AV-MEC system;resource management","","","","15","IEEE","20 Apr 2022","","","IEEE","IEEE Magazines"
"Privacy Preserving Demand Side Management Method via Multi-Agent Reinforcement Learning","F. Zhang; Q. Yang; D. An","Faculty of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Faculty of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Faculty of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China","IEEE/CAA Journal of Automatica Sinica","13 Sep 2023","2023","10","10","1984","1999","The smart grid utilizes the demand side management technology to motivate energy users towards cutting demand during peak power consumption periods, which greatly improves the operation efficiency of the power grid. However, as the number of energy users participating in the smart grid continues to increase, the demand side management strategy of individual agent is greatly affected by the dynamic strategies of other agents. In addition, the existing demand side management methods, which need to obtain users' power consumption information, seriously threaten the users' privacy. To address the dynamic issue in the multi-microgrid demand side management model, a novel multi-agent reinforcement learning method based on centralized training and decentralized execution paradigm is presented to mitigate the damage of training performance caused by the instability of training experience. In order to protect users' privacy, we design a neural network with fixed parameters as the encryptor to transform the users' energy consumption information from low-dimensional to high-dimensional and theoretically prove that the proposed encryptor-based privacy preserving method will not affect the convergence property of the reinforcement learning algorithm. We verify the effectiveness of the proposed demand side management scheme with the real-world energy consumption data of Xi'an, Shaanxi, China. Simulation results show that the proposed method can effectively improve users' satisfaction while reducing the bill payment compared with traditional reinforcement learning (RL) methods (i.e., deep Q learning (DQN), deep deterministic policy gradient (DDPG), QMIX and multi-agent deep deterministic policy gradient (MADD PG)). The results also demonstrate that the proposed privacy protection scheme can effectively protect users' privacy while ensuring the performance of the algorithm.","2329-9274","","10.1109/JAS.2023.123321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10251086","Centralized training and decentralized execution;demand side management;multi-agent reinforcement learning;privacy preserving","Training;Privacy;Energy consumption;Demand side management;Power demand;Simulation;Power system dynamics","data privacy;deep learning (artificial intelligence);demand side management;distributed power generation;energy consumption;gradient methods;learning (artificial intelligence);multi-agent systems;power consumption;power engineering computing;power grids;reinforcement learning;smart power grids;telecommunication computing","centralized training;dynamic strategies;encryptor-based privacy preserving method;energy users;existing demand;individual agent;management method;management model;management scheme;management strategy;management technology;multiagent reinforcement learning;multimicrogrid demand;novel multiagent reinforcement;peak power consumption periods;power grid;privacy protection scheme;reinforcement learning algorithm;smart grid;traditional reinforcement learning methods;training experience;training performance","","","","49","","13 Sep 2023","","","IEEE","IEEE Journals"
"Composing Synergistic Macro Actions for Reinforcement Learning Agents","Y. -M. Chen; K. -Y. Chang; C. Liu; T. -C. Hsiao; Z. -W. Hong; C. -Y. Lee","Taiwan Semiconductor Manufacturing Company (TSMC), Hsinchu, Taiwan; Avery Design Systems Inc, Taipei, Taiwan; Faculty of Computer Science and Electrical Engineering, University of Rostock, Rostock, Germany; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","8","Macro actions have been demonstrated to be beneficial for the learning processes of an agent and have encouraged a variety of techniques to be developed for constructing more effective ones. However, previous techniques usually do not further consider combining macro actions to form a synergistic macro action ensemble, in which synergism exhibits when the constituent macro actions are favorable to be jointly used by an agent during evaluation. Such a synergistic macro action ensemble may potentially allow an agent to perform even better than the individual macro actions within it. Motivated by the recent advances of neural architecture search (NAS), in this brief, we formulate the construction of a synergistic macro action ensemble as a Markov decision process (MDP) and evaluate the constructed macro action ensemble as a whole. Such a problem formulation enables synergism to be taken into account by the proposed evaluation procedure. Our experimental results demonstrate that the proposed framework is able to discover the synergistic macro action ensembles. Furthermore, we also highlight the benefits of these macro action ensembles through a set of analytical cases.","2162-2388","","10.1109/TNNLS.2022.3213606","National Science and Technology Council (NSTC), Taiwan(grant numbers:MOST 111-2223-E-007-004-MY3,MOST 111-2628-E-007-010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9934932","Macro actions;Markov decision process (MDP);neural architecture search (NAS);reinforcement learning (RL);synergism","Task analysis;Reinforcement learning;Markov processes;Learning systems;Behavioral sciences;Artificial neural networks;Planning","","","","","","","IEEE","1 Nov 2022","","","IEEE","IEEE Early Access Articles"
"Multi-Agent Reinforcement Learning With Measured Difference Reward for Multi-Association in Ultra-Dense mmWave Network","X. Li; T. N. Guo; A. B. Mackenzie","Department of Electronics and Communication Engineering, Tennessee Tech University, Cookeville, TN, USA; Center for Manufacturing Research, Tennessee Technological University, Cookeville, TN, USA; Department of Electronics and Communication Engineering, Tennessee Tech University, Cookeville, TN, USA","IEEE Access","16 Nov 2022","2022","10","","118747","118758","Millimeter Wave (mmWave) communication technology is anticipated to play a vital role in meeting the growing demand for the scarce bandwidth in wireless communications. However, mmWave networks are highly susceptible to blockage. Thus, some mitigation techniques, such as multi-connectivity, need to be considered. Densely deploying mmWave base stations (mBSs) to form an ultra-dense network (UDN) also helps. With a mix of different technologies, optimally allocating resources becomes challenging. In this paper, we study mmWave user multi-association in a two-tier heterogeneous ultra-dense network (HetUDN) with a relatively large number of user equipments (UEs). We propose a framework of multi-agent reinforcement learning (MARL) to tackle the complicated optimization problem, leveraging its adaptivity to the communication environment. The proposed scheme considers mmWave beam-division based multi-connectivity and takes advantage of a macro base station (MBS) for indirect cooperation among agents (UEs). In particular, we borrow a credit-assignment technique called difference reward (DR) to deal with a relatively large MARL system with a large action space, which, to the best of our knowledge, is the first time to apply MARL with DR in user association. Furthermore, the proposed schemes are scalable mainly due to fixed observation dimensions and individual actions taken by UEs independently, ensuring that the operation is independent of the numbers of mBSs and UEs. Numerical results suggest that the two MARL schemes with measured DR could achieve a good balance between energy efficiency and QoS outage, and the one using extended DR (EDR) offers additional performance improvement.","2169-3536","","10.1109/ACCESS.2022.3221455","National Science Foundation(grant numbers:2135275); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9945955","Ultra-dense network (UDN);millimeter wave (mmWave);heterogeneous network;multi-association;multi-agent reinforcement learning (MARL)","Millimeter wave communication;Reinforcement learning;Base stations;Training;Optimization;Interference;Ultra-dense networks","millimetre wave communication;multi-agent systems;optimisation;radio equipment;radio networks;reinforcement learning;telecommunication computing","communication environment;complicated optimization problem;credit assignment technique;difference reward;HetUDN;macro base station;MARL schemes;measured difference reward;millimeter wave communication technology;mmWave base stations;mmWave beam division;mmWave user multiassociation;multiagent reinforcement learning;multiconnectivity;two tier heterogeneous ultra dense network;ultra dense mmWave network;user association;user equipments;wireless communications","","","","51","CCBY","10 Nov 2022","","","IEEE","IEEE Journals"
"Intelligent Method for UAV Navigation and De-confliction --Powered by Multi-Agent Reinforcement Learning","B. Xia; I. Mantegh; W. -F. Xie","Department of Mechanical and Industrial Engineering, Concordia University, Montreal, Canada; Aerospace Manufacturing Technologies Centre, National Research Council of Canada, Montreal, Canada; Department of Mechanical and Industrial Engineering, Concordia University, Montreal, Canada","2023 International Conference on Unmanned Aircraft Systems (ICUAS)","26 Jun 2023","2023","","","713","722","As Uncrewed Aircraft Systems (UAS) become more ubiquitous in urban airspace around the world, the need for reliable navigation and de-confliction technologies becomes paramount. In this paper, the authors improve the popular Deep Reinforcement Learning (RL) methods of Twin Delayed DDPG (TD3) and Proximal Policy Optimization (PPO) and propose two new integrated algorithms for de-confliction with single and multiple intruder UASs in different cases of fixed and variable altitudes. Based on the Actor-Critic method, new RL systems and reward functions are designed that enhance the training efficiency of the navigating UAS agent for the considered environment models. The simulation results show the capability of the trained agent to successfully navigate in a complex environment amid fixed and velocity obstacles. This research contributes to the development of autonomous navigation for UAS in urban airspace.","2575-7296","979-8-3503-1037-5","10.1109/ICUAS57906.2023.10156454","National Research Council Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10156454","Uncrewed Aircraft Systems;Deep Reinforcement Learning;Autonomous navigation","Training;Three-dimensional displays;Sensitivity;Simulation;Reinforcement learning;Autonomous aerial vehicles;Aircraft navigation","autonomous aerial vehicles;collision avoidance;control engineering computing;deep learning (artificial intelligence);gradient methods;learning (artificial intelligence);mobile robots;multi-agent systems;optimisation;reinforcement learning","Actor-Critic method;autonomous navigation;considered environment models;de-confliction technologies;fixed altitudes;fixed velocity obstacles;integrated algorithms;intelligent method;multiagent Reinforcement Learning;multiple intruder;navigating UAS agent;popular Deep Reinforcement;reliable navigation;reward functions;RL systems;single intruder;TD3;trained agent;Uncrewed Aircraft Systems;urban airspace;variable altitudes","","","","35","IEEE","26 Jun 2023","","","IEEE","IEEE Conferences"
