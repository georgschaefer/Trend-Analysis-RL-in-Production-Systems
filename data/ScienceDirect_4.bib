@article{ZHANG2023,
title = {H∞ tracking learning control for discrete-time Markov jump systems: A parallel off-policy reinforcement learning},
journal = {Journal of the Franklin Institute},
year = {2023},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2023.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0016003223006543},
author = {Xuewen Zhang and Jianwei Xia and Jing Wang and Xiangyong Chen and Hao Shen},
keywords = {Markov jump systems,  tracking control, Zero-sum game, Off-policy reinforcement learning},
abstract = {This paper deals with the H∞ tracking control problem for a class of linear discrete-time Markov jump systems, in which the knowledge of system dynamics is not required. First, combined with reinforcement learning, a novel Bellman equation and the augmented coupled game algebraic Riccati equation are presented to derived the optimal control policy for the augmented discrete-time Markov jump systems. Moreover, based on the augmented system, a newly constructed system is given to collect the input and output data, which solves the problem that the coupling term in the discrete-time Markov jump systems is difficult to solve. Subsequently, a novel model-free algorithm is designed that does not need the dynamic information of the original system. Finally, a numerical example is given to verify the effectiveness of the proposed approach.}
}
@article{SUNG202010493,
title = {Reinforcement Learning for Resource Constrained Project Scheduling Problem with Activity Iterations and Crashing},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {10493-10497},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2794},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320335588},
author = {Inkyung Sung and Bongjun Choi and Peter Nielsen},
keywords = {Job, activity scheduling, Intelligent decision support systems in manufacturing, Resource Allocation, Reinforcement learning control, Deep Q-learning, Activity Iteration, Crashing},
abstract = {Resource allocation is a key decision-making process in project management that assigns resources to activities of a project and determines the timing of the allocation in a cost and time effective manner. In this research, we address the resource allocation for a project, where iterations between activities of the project exist and the crashing, a method to shorten the duration of an activity by incorporating additional resources, is available. Considering the stochastic nature of project execution, we formulate the resource allocation as a Markov decision process and seek the best resource allocation policy using a deep reinforcement learning algorithm. The feasibility and performance of applying the algorithm to the resource allocation is then investigated by comparison with heuristic rules.}
}
@article{JIANG2023,
title = {NN-based reinforcement-learning optimal sliding mode control for drag-free and attitude of spacecraft with state constraints},
journal = {Advances in Space Research},
year = {2023},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2023.09.052},
url = {https://www.sciencedirect.com/science/article/pii/S0273117723007883},
author = {Changwu Jiang and Yuan Liu},
keywords = {Drag-free, Attitude, Nonlinear, Policy iteration, Optimal sliding mode},
abstract = {This paper investigates a tracking control issue for a class of drag-free spacecraft with state constraints caused by the optical assembly. Firstly, a nonlinear kinematics and dynamics relative equation of coupling system of position and attitude is devised by a 6-degree of freedom (DOF) model of rigid body. Secondly, an optimal control method with a terminal functional is designed to meet state constraints. To solve the Hamilton-Jacobi-Bellman (HJB) equation generated by the optimal control of the nonlinear model, a policy iteration ideal is presented and the final numerical solution of the iteration is obtained by a critic neural network (NN). Thirdly, to enhance the robustness of the closed-loop system, an optimal sliding mode control is proposed, which can ensure the optimal and robustness performance simultaneously. At last, the simulation results demonstrate the performance of the proposed methods and the contrast of precision and robustness of two methods are showcased.}
}
@article{SHI2023128174,
title = {Research on energy management of hydrogen electric coupling system based on deep reinforcement learning},
journal = {Energy},
volume = {282},
pages = {128174},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.128174},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223015682},
author = {Tao Shi and Chang Xu and Wenhao Dong and Hangyu Zhou and Awais Bokhari and Jiří Jaromír Klemeš and Ning Han},
keywords = {Deep reinforcement learning, Demand response, Hydrogen energy, Information uncertainty, Smart grid},
abstract = {In this paper, a deep reinforcement learning-based energy optimization management method for hydrogen-electric coupling system is proposed for the conversion and utilization and joint optimization operation of hydrogen, wind and solar energy forms considering information uncertainty on the demand side of smart grid. Based on the wind energy, photovoltaic energy generation and load forecast information, the method uses deep Q network to simulate the energy management strategy set of the hydrogen-electric coupling system, and obtains the optimal strategy through reinforcement learning to finally realize the optimal operation of the hydrogen-electric coupling system based on the demand response. Firstly, based on the energy management model, a research framework and equipment model for integrated energy systems is established. On the basis of fundamental theories of reinforcement learning framework, Q-learning algorithm and DQN algorithm, the empirical replay mechanism and freezing parameter mechanism to improve the performance of DQN are analyzed, and the energy management and optimization of integrated energy system is completed with the objective of economy. By comparing the performance of DQN algorithms with different parameters in integrated energy system energy management, the simulation results demonstrate the improvement of algorithm performance after inheriting the set of strategies, and verify the feasibility and superiority of deep reinforcement learning compared to genetic algorithm in integrated energy system energy management applications.}
}
@article{ZHANG2020404,
title = {EV charging bidding by multi-DQN reinforcement learning in electricity auction market},
journal = {Neurocomputing},
volume = {397},
pages = {404-414},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.08.106},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220304100},
author = {Yang Zhang and Zhengfeng Zhang and Qingyu Yang and Dou An and Donghe Li and Ce Li},
keywords = {Electric vehicle, Deep reinforcement learning, Multi-agent, Bidding},
abstract = {In this paper, we address the issue of optimal bidding strategy selection for Electric Vehicles (EVs) charging in an auction market. The problem of EV charging has attracted growing attention as EVs become more and more popular. We consider the scenario that EV owners submit their bids for charging to the charging station, and then charging station determines the winning EVs who are admitted to charge and the payments based on an online continuous progressive second price (OCPSP) auction mechanism. In light of this, how to formulate optimal bidding strategy and maximize the economic benefits is crucial for EV owners. To this end, we propose a Multi-Deep-Q-Network (Multi-DQN) reinforcement learning bidding strategy, in which, a value evaluation network and a target network are proposed for each agent to learn the optimal bidding strategy. The extensive experimental results show that our bidding strategy can achieve better economic benefits and help EV owners spend less time on charging compared to the Q-learning based approach and the random approach.}
}
@article{CHEN2021107738,
title = {An Effective Multi-population Grey Wolf Optimizer based on Reinforcement Learning for Flow Shop Scheduling Problem with Multi-machine Collaboration},
journal = {Computers & Industrial Engineering},
volume = {162},
pages = {107738},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107738},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221006422},
author = {Ronghua Chen and Bo Yang and Shi Li and Shilong Wang and Qingqing Cheng},
keywords = {Scheduling, Grey Wolf Optimizer (GWO), Multi-machine Collaboration, Reinforcement learning (RL), Multi-Population},
abstract = {This paper proposes the Flow Shop Scheduling Problem with Multi-machine Collaboration (FSSP-MC). In FSSP-MC, several machines can operate a single task simultaneously, so it is a coupling problem of resource composition and task sequencing, which is more difficult and has a larger scale solution space than the traditional Flow Shop Scheduling Problem (FSSP), therefore an optimization algorithm with higher efficient and accurate is demanded. However, most existing intelligent algorithms are easily trapped into local optima and have low precision on solving large-scale problems. To this end, an adaptive multi-objective Multi-population Grey Wolf Optimizer (AMPGWO) based on Reinforcement Learning (RL) is developed to address FSSP-MC with the goals of minimizing maximum completion time (makespan) and the total machine load. In AMPGWO, the whole population is divided into three subpopulations, and different search strategies are adopted in different subpopulations to enhance population diversity. Since the numbers of individuals in a subpopulations are pretty crucial for the performance of the algorithm, which needs to be reasonably determined and dynamically adjusted, so RL is applied to adaptively adjust the individual quantity of each subpopulation and strengthen the information exchange among different subpopulations. Finally, 20 instances of FSSP-MC with different sizes are used for three comparative experiments, in which the effectiveness of multi-population and RL mechanisms, effectiveness of mutation mechanism of AMPGWO are verified. Through results analysis, it can be seen that proposed AMPGWO is pretty effective and significantly outperforms its competitors in solving FSSP-MC.}
}
@article{PALOMBARINI201210251,
title = {SmartGantt – An intelligent system for real time rescheduling based on relational reinforcement learning},
journal = {Expert Systems with Applications},
volume = {39},
number = {11},
pages = {10251-10268},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.02.176},
url = {https://www.sciencedirect.com/science/article/pii/S0957417412004393},
author = {Jorge Palombarini and Ernesto Martínez},
keywords = {Manufacturing systems, Real-time rescheduling, Automated planning, Reinforcement learning, Information systems, Relational abstractions},
abstract = {With the current trend towards cognitive manufacturing systems to deal with unforeseen events and disturbances that constantly demand real-time repair decisions, learning/reasoning skills and interactive capabilities are important functionalities for rescheduling a shop-floor on the fly taking into account several objectives and goal states. In this work, the automatic generation and update through learning of rescheduling knowledge using simulated transitions of abstract schedule states is proposed. Deictic representations of schedules based on focal points are used to define a repair policy which generates a goal-directed sequence of repair operators to face unplanned events and operational disturbances. An industrial example where rescheduling is needed due to the arrival of a new/rush order, or whenever raw material delay/shortage or machine breakdown events occur are discussed using the SmartGantt prototype for interactive rescheduling in real-time. SmartGantt demonstrates that due date compliance of orders-in-progress, negotiating delivery conditions of new orders and ensuring distributed production control can be dramatically improved by means of relational reinforcement learning and a deictic representation of rescheduling tasks.}
}
@article{HUANG202050,
title = {Reinforcement Learning-Based Control for Nonlinear Discrete-Time Systems with Unknown Control Directions and Control Constraints},
journal = {Neurocomputing},
volume = {402},
pages = {50-65},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.03.061},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220304343},
author = {Miao Huang and Cong Liu and Xiaoqi He and Longhua Ma and Zheming Lu and Hongye Su},
keywords = {Neural networks, Reinforcement learning, Non-affine nonlinear systems, Output feedback, Unknown control directions},
abstract = {In this work, output-feedback control problems for a class of discrete-time non-affine nonlinear systems with unknown control directions and input constraints are considered by using reinforcement learning (RL) method. Two neural networks (NNs) implement the control: 1) a critic NN that estimates a non-quadratic strategic utility function (SUF) and 2) an action NN that generates optimized control input and minimizes the SUF. The implicit function theorem is applied to obtain the optimal control law since the control is appeared in a non-affine form. For the first time, the discrete Nussbaum gain is introduced to overcome the difficulty that the control directions are unknown and a non-quadratic SUF is used to deal with the control constraints in the RL-based control. The theoretical derivation of the uniformly ultimately boundedness of the NN weights and the closed-loop output tracking error is given. And two numerical examples have been supplied to valid the proposed method.}
}
@article{LI202269,
title = {Supervised assisted deep reinforcement learning for emergency voltage control of power systems},
journal = {Neurocomputing},
volume = {475},
pages = {69-79},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.12.043},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221018816},
author = {Xiaoshuang Li and Xiao Wang and Xinhu Zheng and Yuxin Dai and Zhihong Yu and Jun Jason Zhang and Guangquan Bu and Fei-Yue Wang},
keywords = {Deep reinforcement learning, Behavioral cloning, Dynamic demonstration, Emergency control},
abstract = {The increasing complexity of power systems makes existing deep reinforcement learning-based emergency voltage control methods face challenges in learning speed and data utilization efficiency. Meanwhile, the accumulated data containing expert experience and domain knowledge has not been fully utilized to improve the performance of the deep reinforcement learning methods. To address the above issues, a novel hybrid emergency voltage control method that combines expert experience and machine intelligence is proposed in this paper. Specifically, the expert experience in the off-line demonstration is extracted through a behavioral cloning model and the deep reinforcement learning method is applied to discover and learn new knowledge autonomously. A special supervised expert loss is designed to utilize the pre-trained behavioral cloning model to assist the self-learning process. The demonstration is dynamically updated during the training process such that the behavioral cloning model and the deep reinforcement learning model can facilitate each other continuously. Experiments are conducted on the open-source RLGC platform to validate the performance and the experimental results show that the proposed method can effectively improve the learning speed and the applicability of the model to different test situations.}
}
@article{PARK2021907,
title = {Reinforcement Learning for Control of Passive Heating and Cooling in Buildings⁎⁎This work was funded by National Science Foundation CBET-1804218.},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {20},
pages = {907-912},
year = {2021},
note = {Modeling, Estimation and Control Conference MECC 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.11.287},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321023314},
author = {Bumsoo Park and Alexandra R. Rempel and Alan K.L. Lai and Julianna Chiaramonte and Sandipan Mishra},
keywords = {Building automation, Reinforcement learning, Neural networks, Data-driven control},
abstract = {Mechanical space heating and cooling are responsible for over one-third of the greenhouse gases released by building operations globally. As a result, heating and cooling load reductions are high priorities in climate change mitigation efforts. Direct solar heating, natural ventilation, and shading are often able to condition indoor spaces “passively” using only climatic resources, but their performance is limited by the lack of effective and affordable controls for their operable elements: rule-based control strategies cannot anticipate changes in weather or adapt to seasonal changes, while model-based strategies require significant investment into the creation of customized thermal models. Here, we design and validate a model-free data-driven reinforcement learning approach by comparing tabular Q-learning and policy-gradient (REINFORCE) algorithms for passive heating and cooling. These algorithms are trained on a residential building simulated in EnergyPlus in Albany NY and evaluated on the basis of unmet heating and cooling loads in both the training climate and six others. We find that the learned operation of shading, night insulation, and window aperture opening, driven by indoor and outdoor air temperatures, window surface heat flux, and weather forecasts, reduces total loads by 47-76% compared to operation without passive systems. Additionally, the REINFORCE policy reduces loads by 13-64% over conventional rule-based control, with one exception. Together, these results show that reinforcement learning can improve passive heating and cooling performance substantially, ultimately reducing space heating and cooling energy requirements.}
}
@article{RUMMUKAINEN20191415,
title = {Practical Reinforcement Learning -Experiences in Lot Scheduling Application ⁎⁎This work was in part supported by Business Finland, through project Engineering Rulez.},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {1415-1420},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.397},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319313783},
author = {Hannu Rummukainen and Jukka K. Nurminen},
keywords = {Reinforcement learning, Stochastic economic lot scheduling, Learning control, Stochastic control, Monte Carlo simulation, Neural networks, Machine learning},
abstract = {With recent advances in deep reinforcement learning, it is time to take another look at reinforcement learning as an approach for discrete production control. We applied proximal policy optimization (PPO), a recently developed algorithm for deep reinforcement learning, to the stochastic economic lot scheduling problem. The problem involves scheduling manufacturing decisions on a single machine under stochastic demand, and despite its simplicity remains computationally challenging. We implemented two parameterized models for the control policy and value approximation, a linear model and a neural network, and used a modified PPO algorithm to seek the optimal parameter values. Benchmarking against the best known control policy for the test case, in which Paternina-Arboleda and Das (2005) combined a base-stock policy and an older reinforcement learning algorithm, we improved the average cost rate by 2 %. Our approach is more general, as we do not require a priori policy parameters such as base-stock levels, and the entire policy is learned.}
}
@article{LI2023108168,
title = {Optimization of oxygen system scheduling in hybrid action space based on deep reinforcement learning},
journal = {Computers & Chemical Engineering},
volume = {171},
pages = {108168},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108168},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423000376},
author = {Lijuan Li and Xue Yang and Shipin Yang and Xiaowei Xu},
keywords = {Oxygen system for steel enterprises, Hybrid action space, Deep Q network, Actor-critic algorithm},
abstract = {The simultaneous consideration of discrete and continuous variables including equipment start-up, shutdown, oxygen output and dissipation, is required in the optimal problem of oxygen system scheduling in steel enterprises. To solve this problem involving hybrid variables, a hybrid actor-critic (HAC) algorithm is proposed in this paper. The proposed algorithm subdivides the action space into a discrete and continuous action space and evaluates the policy through the improved Q function. Besides, the correlation matrix is constructed to address the correspondence between hybrid actions. As a result, the generation of spurious gradients that lead to suboptimal action selection is prevented. To verify the effectiveness of the proposed approach, experiments with multiple groups employing the real data from steel enterprises are carried out. The results demonstrate that such a practice-based solution successfully resolves the oxygen scheduling problem and simultaneously improves the reward and algorithm convergence speed.}
}
@article{GUO2023106613,
title = {Optimal navigation for AGVs: A soft actor–critic-based reinforcement learning approach with composite auxiliary rewards},
journal = {Engineering Applications of Artificial Intelligence},
volume = {124},
pages = {106613},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106613},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623007972},
author = {Haisen Guo and Zhigang Ren and Jialun Lai and Zongze Wu and Shengli Xie},
keywords = {AGVs, Optimal control, Motion planning, Reinforcement learning, Trajectory planning},
abstract = {In this paper, we address the problem of real-time navigation and obstacle avoidance for automated guided vehicles (AGVs) in dynamic environments, which is a primary research area in collaborative control systems for AGVs. To overcome the computational inefficiency of recalculating optimal paths every time, we propose an improved Soft Actor–Critic (SAC)-based reinforcement learning methodology. This methodology utilizes a novel composite auxiliary reward structure and sum-tree prioritized experience replay (SAC-SP) to achieve real-time optimal feedback control. First, we formulate the navigation task as a Markov Decision Process that considers both static and dynamic obstacles. To accelerate the active learning of AGVs, we propose a novel strategy that uses composite auxiliary rewards. Next, we train the AGVs using the proposed SAC-SP methodology to handle real-time navigation with the composite auxiliary reward structure. The well-trained policy network can generate effective on-board optimal feedback actions given obstacle positions, targets, and AGV states. Simulation experiments demonstrate that our proposed method can steer AGVs to the destination with high robustness to original conditions and various obstacle restrictions, generating optimal feedback actions in the shortest amount of time.}
}
@article{YAN2022102712,
title = {Reinforcement learning for logistics and supply chain management: Methodologies, state of the art, and future opportunities},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {162},
pages = {102712},
year = {2022},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2022.102712},
url = {https://www.sciencedirect.com/science/article/pii/S136655452200103X},
author = {Yimo Yan and Andy H.F. Chow and Chin Pang Ho and Yong-Hong Kuo and Qihao Wu and Chengshuo Ying},
keywords = {Reinforcement learning, Logistics, Supply chain, Markov decision process, Q-learning, Actor-critic methods, Neural network},
abstract = {With advances in technologies, data science techniques, and computing equipment, there has been rapidly increasing interest in the applications of reinforcement learning (RL) to address the challenges resulting from the evolving business and organisational operations in logistics and supply chain management (SCM). This paper aims to provide a comprehensive review of the development and applications of RL techniques in the field of logistics and SCM. We first provide an introduction to RL methodologies, followed by a classification of previous research studies by application. The state-of-the-art research is reviewed and the current challenges are discussed. It is found that Q-learning (QL) is the most popular RL approach adopted by these studies and the research on RL for urban logistics is growing in recent years due to the prevalence of E-commerce and last mile delivery. Finally, some potential directions are presented for future research.}
}
@article{YANG202219,
title = {Dynamic Path Planning for Mobile Robots with Deep Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {11},
pages = {19-24},
year = {2022},
note = {IFAC Workshop on Control for Smart Cities CSC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.08.042},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322011302},
author = {Laiyi Yang and Jing Bi and Haitao Yuan},
keywords = {Deep reinforcement learning, path planning, Soft Actor-Critic algorithm, continuous reward functions, mobile robots},
abstract = {Traditional path planning algorithms for mobile robots are not effective to solve high-dimensional problems, and suffer from slow convergence and complex modelling. Therefore, it is highly essential to design a more efficient algorithm to realize intelligent path planning of mobile robots. This work proposes an improved path planning algorithm, which is based on the algorithm of Soft Actor-Critic (SAC). It attempts to solve a problem of poor robot performance in complicated environments with static and dynamic obstacles. This work designs an improved reward function to enable mobile robots to quickly avoid obstacles and reach targets by using state dynamic normalization and priority replay buffer techniques. To evaluate its performance, a Pygame-based simulation environment is constructed. The proposed method is compared with a Proximal Policy Optimization (PPO) algorithm in the simulation environment. Experimental results demonstrate that the cumulative reward of the proposed method is much higher than that of PPO, and it is also more robust than PPO.}
}
@article{HU2023117288,
title = {Optimizing fuel economy and durability of hybrid fuel cell electric vehicles using deep reinforcement learning-based energy management systems},
journal = {Energy Conversion and Management},
volume = {291},
pages = {117288},
year = {2023},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2023.117288},
url = {https://www.sciencedirect.com/science/article/pii/S0196890423006349},
author = {Haowen Hu and Wei-Wei Yuan and Minghang Su and Kai Ou},
keywords = {Energy management strategy (EMS), Deep reinforcement learning, Pontryagin's minimum principle (PMP), Fuel economy, Battery degradation, Fuel cell degradation, Overall cost},
abstract = {An effective energy management strategy (EMS) is crucial for the reliable operation of fuel cell hybrid electric vehicles (FCHEVs). This study proposes a power distribution optimization strategy for FCHEVs that leverages deep reinforcement learning (DRL) and Pontryagin's minimum principle (PMP). The DRL algorithm effectively balances fuel economy, battery durability, and fuel cell durability objectives. The degradation mechanisms of battery and fuel cell under extreme working conditions are considered in the PMP optimization. A comprehensive evaluation framework is established with degradation and energy consumption models to serve as a reward for deep reinforcement learning to balance fuel economy and power sources' lifetime. Simulation results show that the proposed EMS framework reduces FC degradation by 18.4% and battery degradation by 71.1% compared to traditional PMP-based EMS under the NEDC driving condition. Hardware-in-the-loop (HIL) testing demonstrates that the proposed EMS framework has the potential for real-time application, with an average relative error between experiment and simulation of approximately 0.0203. This research highlights the significance of the proposed EMS framework in ensuring the reliable operation of FCHEVs with enhanced performance and reduced cost.}
}
@article{NIAN2020106886,
title = {A review On reinforcement learning: Introduction and applications in industrial process control},
journal = {Computers & Chemical Engineering},
volume = {139},
pages = {106886},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106886},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420300557},
author = {Rui Nian and Jinfeng Liu and Biao Huang},
keywords = {Reinforcement learning, Model predictive control, Optimal control, Machine learning, Process industry, Process control},
abstract = {In recent years, reinforcement learning (RL) has attracted significant attention from both industry and academia due to its success in solving some complex problems. This paper provides an overview of RL along with tutorials for practitioners who are interested in implementing RL solutions into process control applications. The paper starts by providing an introduction to different reinforcement learning algorithms. Then, recent successes of RL applications across different industries will be explored, with more emphasis on process control applications. A detailed RL implementation example will also be shown. Afterwards, RL will be compared with traditional optimal control methods, in terms of stability and computational complexity among other factors, and the current shortcomings of RL will be introduced. This paper is concluded with a summary of RL’s potential advantages and disadvantages.}
}
@article{KHAN2022108885,
title = {Designing the process designer: Hierarchical reinforcement learning for optimisation-based process design},
journal = {Chemical Engineering and Processing - Process Intensification},
volume = {180},
pages = {108885},
year = {2022},
issn = {0255-2701},
doi = {https://doi.org/10.1016/j.cep.2022.108885},
url = {https://www.sciencedirect.com/science/article/pii/S0255270122001039},
author = {Ahmad A. Khan and Alexei A. Lapkin},
keywords = {Process design, Process intensification, Reinforcement learning, Machine learning, Mathematical programming},
abstract = {Optimisation-based design is an established methodology that aims to achieve a globally optimal solution to a complex process design task by representing it as an optimisation problem. We propose a hybrid framework for decomposition-based process design, centred around hierarchical reinforcement learning and mathematical programming. The framework enables the agent to assemble processes, employ mathematical programming, and discover optimal designs without the need for a pre-defined process superstructure. The agent is composed of: (i) a higher level, that learns to construct the overall process by connecting process sections, and (ii) a lower level, that learns to build and solve sections by connecting and initialising unit operations. Such modularity allows for flexible and robust optimisation in constrained, nonlinear and nonconvex spaces. The framework is demonstrated in a case study of an intensified ethylene oxide production plant, yielding improved results compared to baseline designs reported in the open literature. The case study was implemented in Pyomo. Results reveal insights on the agent’s learning speed and ability to leverage process models.}
}
@article{LIU2020444,
title = {Reinforcement learning in free-form stamping of sheet-metals},
journal = {Procedia Manufacturing},
volume = {50},
pages = {444-449},
year = {2020},
note = {18th International Conference on Metal Forming 2020},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.08.081},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920317790},
author = {Shiming Liu and Zhusheng Shi and Jianguo Lin and Zhiqiang Li},
keywords = {Machine learning, Reinforcement learning, Q-learning, Sheet-metal stamping, Finite element analysis},
abstract = {Sheet-metal free-form stamping technology deforms sheet-metals with simple and low costs universal tools on a working bench, which is normally an anvil. This traditional forming method is praised for its high forming flexibility but complained due to its reliance on individual experience thus low repeatability. In this paper, a python-based overall learning algorithm, which incorporates a reinforcement learning (RL) algorithm, for a designed sheet-metal free-form stamping case is developed. A neural network system, known as deep Q-network (DQN), was used to approximate the action-value function (Q function) in the Deep Q-learning algorithm. The DQN was trained using mini-batch training method, with the computational experiment data provided through Finite Element (FE) simulations. The overall learning algorithm was instantiated and evaluated by training the RL model to convergence, which is able to predict the optimal forming route to achieve the desired shape. This algorithm achieves the intellectualisation of the traditional free-form sheet-metal stamping process for the first time, without prior expertise for guidance.}
}
@article{CHEN2022245,
title = {Distributed Reinforcement Learning Algorithm for Multi-Wave Fire Fighting Scheduling Problem},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {3},
pages = {245-250},
year = {2022},
note = {16th IFAC Symposium on Large Scale Complex Systems: Theory and Applications LSS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.05.043},
url = {https://www.sciencedirect.com/science/article/pii/S240589632200307X},
author = {Xiaoyu Chen and Junjie Fu and Jialing Zhou and Yuheng Li},
keywords = {Multi-agent systems, distributed reinforcement learning, FFE, UAV, fire fighting},
abstract = {This paper studies the distribution of FFEs (fire fighting equipments) carried by UAVs (unmanned aerial vehicles) from FFUs (fire fighting units) under the background of multi-wave forest fire. The objective is to allocate the FFEs of each FFU to minimize the sum of the probabilities of each fire site’s unsuccessful extinguishment. In order to solve the multi-wave equipment distribution problem of the FFUs, a distributed reinforcement learning algorithm is designed in this paper. In the algorithm, agents cooperate to find the optimal distribution of FFEs based on information exchange, and a local Q-function is established for each agent to find the optimal FFE distribution combination. Simulation results demonstrate the effectiveness of the algorithm.}
}
@article{ZHAO2023110761,
title = {Linear quadratic tracking control of unknown systems: A two-phase reinforcement learning method},
journal = {Automatica},
volume = {148},
pages = {110761},
year = {2023},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2022.110761},
url = {https://www.sciencedirect.com/science/article/pii/S0005109822006276},
author = {Jianguo Zhao and Chunyu Yang and Weinan Gao and Hamidreza Modares and Xinkai Chen and Wei Dai},
keywords = {Reinforcement learning, Linear quadratic tracking control, Discounted cost function, Singular perturbation theory},
abstract = {This paper considers the problem of linear quadratic tracking control (LQTC) with a discounted cost function for unknown systems. The existing design methods often require the discount factor to be small enough to guarantee the closed-loop stability. However, solving the discounted algebraic Riccati equation (ARE) may lead to ill-conditioned numerical issues if the discount factor is too small. By singular perturbation theory, we decompose the full-order discounted ARE into a reduced-order ARE and a Sylvester equation, which facilitate designing the feedback and feedforward control gains. The obtained controller is proved to be a stabilizing and near-optimal solution to the original LQTC problem. In the framework of reinforcement learning, both on-policy and off-policy two-phase learning algorithms are derived to design the near-optimal tracking control policy without knowing the discount factor. The advantages of the developed results are illustrated by comparative simulation results.}
}
@article{WEIGOLD202117,
title = {Method for the application of deep reinforcement learning for optimised control of industrial energy supply systems by the example of a central cooling system},
journal = {CIRP Annals},
volume = {70},
number = {1},
pages = {17-20},
year = {2021},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2021.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0007850621000214},
author = {Matthias Weigold and Heiko Ranzau and Sarah Schaumann and Thomas Kohne and Niklas Panten and Eberhard Abele},
keywords = {Machine learning, Energy efficiency, CO2 reduced production},
abstract = {This paper presents a method for data- and model-driven control optimisation for industrial energy supply systems (IESS) by means of deep reinforcement learning (DRL). The method consists of five steps, including system boundary definition and data accumulation, system modelling and validation, implementation of DRL algorithms, performance comparison and adaptation or application of the control strategy. The method is successfully applied to a simulation of an industrial cooling system using the PPO (proximal policy optimisation) algorithm. Significant reductions in electricity cost by 3% to 17% as well as reductions in CO2 emissions by 2% to 11% are achieved. The DRL-based control strategy is interpreted and three main reasons for the performance increase are identified. The DRL controller reduces energy cost by utilizing the storage capacity of the cooling system and moving electricity demand to times of lower prices. Additionally, the DRL-based control strategy for cooling towers as well as compression chillers reduces electricity cost and wear-related cost alike.}
}
@article{BLAIS2023226,
title = {Reinforcement learning for swarm robotics: An overview of applications, algorithms and simulators},
journal = {Cognitive Robotics},
volume = {3},
pages = {226-256},
year = {2023},
issn = {2667-2413},
doi = {https://doi.org/10.1016/j.cogr.2023.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S2667241323000241},
author = {Marc-Andrė Blais and Moulay A. Akhloufi},
keywords = {Swarm robotics, Reinforcement learning, Intelligent systems, Simulators, Drones},
abstract = {Robots such as drones, ground rovers, underwater vehicles and industrial robots have increased in popularity in recent years. Many sectors have benefited from this by increasing productivity while also decreasing costs and certain risks to humans. These robots can be controlled individually but are more efficient in a large group, also known as a swarm. However, an increase in the quantity and complexity of robots creates the need for an adequate control system. Reinforcement learning, an artificial intelligence paradigm, is an increasingly popular approach to control a swarm of unmanned vehicles. The quantity of reviews in the field of reinforcement learning-based swarm robotics is limited. We propose reviewing the various applications, algorithms and simulators on the subject to fill this gap. First, we present the current applications on swarm robotics with a focus on reinforcement learning control systems. Subsequently, we define important reinforcement learning terminologies, followed by a review of the current state-of-the-art in the field of swarm robotics utilizing reinforcement learning. Additionally, we review the various simulators used to train, validate and simulate swarms of unmanned vehicles. We finalize our review by discussing our findings and the possible directions for future research. Overall, our review demonstrates the potential and state-of-the-art reinforcement learning-based control systems for swarm robotics.}
}
@article{HUO2023106664,
title = {Reinforcement Learning-Based Fleet Dispatching for Greenhouse Gas Emission Reduction in Open-Pit Mining Operations},
journal = {Resources, Conservation and Recycling},
volume = {188},
pages = {106664},
year = {2023},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2022.106664},
url = {https://www.sciencedirect.com/science/article/pii/S0921344922004979},
author = {Da Huo and Yuksel Asli Sari and Ryan Kealey and Qian Zhang},
keywords = {Mining and Sustainability, Climate Change Mitigation, Carbon Emissions, Fleet Management, Reinforcement Learning},
abstract = {In typical mining operations, more than half of the direct greenhouse gas (GHG) emissions come from haulage fuel consumption. Smarter truck fleet dispatching is a feasible and manageable solution to reduce direct emissions with existing equipment. Conventional scheduling-based and human-led dispatching solutions often cause lower efficiency that wastes resources and elevates emissions. In this study, a simulated environment is developed to enable testing smarter real-time dispatching systems, Q-learning as a model-free reinforcement learning algorithm is used to improve fleet productivity, decrease waiting time and, consequently, reduce GHG emissions. The proposed algorithm trains the fleet to make better decisions based on payload, traffic, queueing, and maintenance conditions. Results show that this solution can reduce GHG emissions from haulage fuel consumption by over 30% while achieving the same production levels as compared to fixed scheduling. The proposed solution also shows advantages in handling operational randomness and balancing fleet size, productivity, and emissions.}
}
@article{LIU2022109191,
title = {Data-driven optimal tracking control for SMA actuated systems with prescribed performance via reinforcement learning},
journal = {Mechanical Systems and Signal Processing},
volume = {177},
pages = {109191},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.109191},
url = {https://www.sciencedirect.com/science/article/pii/S0888327022003466},
author = {Hongshuai Liu and Qiang Cheng and Jichun Xiao and Lina Hao},
keywords = {Shape memory alloy actuated systems, Data-driven optimization, Prescribed performance, Reinforcement learning},
abstract = {This article addresses the data-driven performance-prescribed continuous-time optimal tracking control problem for the shape memory alloy (SMA) actuated systems with completely unknown model knowledge. Firstly, the error constraint problem is transformed into the unconstrained error tracking problem by the prescribed performance control (PPC) method. Then, the optimal tracking control problem (OTCP) is pre-processed by establishing an unconstrained augmented system. Furthermore, the Hamilton–Jacobi–Bellman equation (HJBE) of the OTCP is solved iteratively by utilizing reinforcement learning (RL) without the SMA actuator model information requirement. The value function and execution strategy of the RL are approximated by two neural networks, acting as actor and critic, respectively, and the actor–critic based RL is implemented using the least-squares method. In addition, the Lyapunov method ensures the stability of the closed-loop system actuated by SMA, as well as the user-specified error constraints concerning the error convergence rate, overshoot, and tracking accuracy. Finally, the experimental results and comparisons illustrate the validity of the proposed method.}
}
@article{PINCIROLI2022752,
title = {Optimization of the Operation and Maintenance of renewable energy systems by Deep Reinforcement Learning},
journal = {Renewable Energy},
volume = {183},
pages = {752-763},
year = {2022},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2021.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S0960148121016347},
author = {Luca Pinciroli and Piero Baraldi and Guido Ballabio and Michele Compare and Enrico Zio},
keywords = {Renewable energy systems, Wind farm, Operation and maintenance, Prognostics and health management, Optimization, Deep reinforcement learning},
abstract = {Equipment of renewable energy systems are being supported by Prognostics & Health Management (PHM) capabilities to estimate their current health state and predict their Remaining Useful Life (RUL). The PHM health state estimates and RUL predictions can be used for the optimization of the systems Operation and Maintenance (O&M). This is an ambitious and challenging task, which requires to consider many factors, including the availability of maintenance crews, the variability of energy demand and production, the influence of the operating conditions on equipment performance and degradation and the long time horizons of renewable energy systems usage. We develop a novel formulation of the O&M optimization as a sequential decision problem and we resort to Deep Reinforcement Learning (DRL) to solve it. The proposed solution approach combines proximal policy optimization, imitation learning, for pre-training the learning agent, and a model of the environment which describes the renewable energy system behavior. The solution approach is tested by its application to a wind farm O&M problem. The optimal solution found is shown to outperform those provided by other DRL algorithms. Also, the approach does not require to select a-priori a maintenance strategy, but, rather, it discovers the best performing policy by itself.}
}
@article{YOU2022102,
title = {Integrating contrastive learning with dynamic models for reinforcement learning from images},
journal = {Neurocomputing},
volume = {476},
pages = {102-114},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.12.094},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221019500},
author = {Bang You and Oleg Arenz and Youping Chen and Jan Peters},
keywords = {Deep learning in robotics and automation, Reinforcement learning, Contrastive learning, Sensor-based control},
abstract = {Recent methods for reinforcement learning from images use auxiliary tasks to learn image features that are used by the agent’s policy or Q-function. In particular, methods based on contrastive learning that induce linearity of the latent dynamics or invariance to data augmentation have been shown to greatly improve the sample efficiency of the reinforcement learning algorithm and the generalizability of the learned embedding. We further argue, that explicitly improving Markovianity of the learned embedding is desirable and propose a self-supervised representation learning method which integrates contrastive learning with dynamic models to synergistically combine these three objectives: (1) We maximize the InfoNCE bound on the mutual information between the state- and action-embedding and the embedding of the next state to induce a linearly predictive embedding without explicitly learning a linear transition model, (2) we further improve Markovianity of the learned embedding by explicitly learning a non-linear transition model using regression, and (3) we maximize the mutual information between the two nonlinear predictions of the next embeddings based on the current action and two independent augmentations of the current state, which naturally induces transformation invariance not only for the state embedding, but also for the nonlinear transition model. Experimental evaluation on the Deepmind control suite shows that our proposed method achieves higher sample efficiency and better generalization than state-of-art methods based on contrastive learning or reconstruction.}
}
@article{LE2021108477,
title = {Reinforcement learning-based optimal complete water-blasting for autonomous ship hull corrosion cleaning system},
journal = {Ocean Engineering},
volume = {220},
pages = {108477},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2020.108477},
url = {https://www.sciencedirect.com/science/article/pii/S0029801820313846},
author = {Anh Vu Le and Phone Thiha Kyaw and Prabakaran Veerajagadheswar and M.A. Viraj J. Muthugala and Mohan Rajesh Elara and Madhu Kumar and Nguyen Huu {Khanh Nhan}},
keywords = {Ship maintenance industry, Corrosion cleaning, Benchmarking blasting quality, Reinforcement learning, Path planning},
abstract = {Routine cleaning of the corroded ship hulls in dry dock maintenance guarantees the smooth operation of the shipping industry. Deploying the autonomous system to remove the corrosion by water-blasting is a feasible approach to ease the burden in manual operation and to reduce water, time, and energy consumption. In this paper, the water-blasting framework is proposed for a novel robot platform named Hornbill with the adhesion mechanism by permanent magnetic, self-localization by sensor fusion to navigate smoothly on a vertical surface. Hence, we propose a complete waypoint path planning (CWPP) to re-blast the self-synthesizing deep convolutional neural network (DCNN) based corrosion heatmap by initial-blasting. The optimal CWPP problem, including the shortest travel distance and shortest travel time to save water, power while ensuring visiting all predefined waypoints by benchmarking output, is modeled as the classic Travel Salesman Problem (TSP). Further, the Pareto-optimal trajectory for given TSP has been driven by the reinforcement learning (RL) technique with a proposed reward function based on the robot's operation during blasting. From the experimental results at the shipyard site, the proposed RL-based CWPP generates the Pareto-optimal trajectory that enables the water-blasting robot to spend about 10% of energy and 9% of water less than the second-best evolutionary-based optimization method in various workspaces.}
}
@article{TAN2023105557,
title = {Bi-level optimization of charging scheduling of a battery swap station based on deep reinforcement learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {118},
pages = {105557},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105557},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622005474},
author = {Mao Tan and Zhuocen Dai and Yongxin Su and Caixue Chen and Ling Wang and Jie Chen},
keywords = {Deep reinforcement learning, Battery charging scheduling, Battery swap station, Electric vehicle},
abstract = {With the rapid increase of in the number of electric vehicle (EV), battery swapping is becoming a promising idea because of its short service waiting time. However, in the face of the uncertainty of the power grid and EV behavior, it is difficult to achieve a forward-looking and fast-response scheduling in a large scale battery swap station (BSS). A new bi-level scheduling model is proposed to solve this problem, in which the upper level is built on a deep reinforcement learning (DRL) framework to optimally allocate power among the chargers, and the lower level is modeled as a series of MILP subproblems for dispatching power among the batteries in a charger. A prediction module is included in the DRL framework improve the foresight of the algorithm, and a safety module is designed to avoid unsafe actions. Experimental results indicate that the proposed approach has excellent performance in large scale problem solving. It reduces the operating costs of the BSS significantly while satisfying the maximum power demand constraint. This is able to provide more economic benefits for the BSS and help peak shaving and valley filling for the power grid.}
}
@article{MA2020107016,
title = {Machine-learning-based simulation and fed-batch control of cyanobacterial-phycocyanin production in Plectonema by artificial neural network and deep reinforcement learning},
journal = {Computers & Chemical Engineering},
volume = {142},
pages = {107016},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.107016},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420300545},
author = {Yan Ma and Daniel A. Noreña-Caro and Alexandria J. Adams and Tyler B. Brentzel and José A. Romagnoli and Michael G. Benton},
keywords = {Deep reinforcement learning, Artificial neural network, Fed-batch control, C-phycocyanin, Plectonema},
abstract = {In this paper, a model-free deep reinforcement learning (DRL) strategy is presented with an artificial neural network (ANN) as reaction simulation environment, to obtain a fed-batch control strategy for an experimental bioreactor. The proposed method is a fundamental attempt to control reactions by employing state-of-the-art machine learning tools without the aid of well-established mechanistic understanding of the reaction system. This application utilizes the Asynchronous Advantage Actor-Critic (A3C) algorithm, a member of the DRL family, that takes advantage of actor-critic algorithm and asynchronous learning by parallel learning agents to achieve stability and efficiency of the learning process. The resulting controller demonstrates robust performance in the fed-batch bioreactor since it can be adjusted to meet varying constraining factors including nutrient limitations and culture lengths. Results are presented for a bioreactor that produces cyanobacterial-phycocyanin (C-PC) in Plectonema sp. UTEX 1541. Experimental validations show a 52.1% increase in the product yield, and a 20.1% increase in C-PC concentration compared to a control group with the same total nutrient input replenished in a non-optimized manner.}
}
@article{ZHONG2023105524,
title = {Hierarchical reinforcement learning based operational optimization for compressed air system},
journal = {Control Engineering Practice},
volume = {136},
pages = {105524},
year = {2023},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2023.105524},
url = {https://www.sciencedirect.com/science/article/pii/S096706612300093X},
author = {Lulu Zhong and Yang Liu and Jun Zhao and Wei Wang},
keywords = {compressed air system (CAS), Operational optimization, Hierarchical reinforcement learning (HRL), Adaptive parameter controller},
abstract = {The high proportion energy consumption of compressed air system (CAS) have drawn more and more attentions, and its energy conservation and emission reduction is of great significance to achieve the green transformation in an industrial park. In this study, a novel operational optimization method for CAS by hierarchical reinforcement learning (HRL) strategy is proposed, which involves two modules in terms of the operational optimization by combining the selectable of start–stop devices and operational mode set, the control optimization by adaptive parameter controller. Moreover, the close interaction between operational optimization and tracking control by using the hierarchical structure with Q-learning process. Besides, a feedback correction strategy is also proposed for reducing the negative effect of prediction output error and compressor physical limitation. Furthermore, a series of typical cases in an industrial park are selected for verifying the effectiveness of the proposed HRL, which shows that the proposed method achieves the best trade-off between complexity and time consuming.}
}
@article{MUGHEES2023121150,
title = {Reinforcement learning-based composite differential evolution for integrated demand response scheme in industrial microgrids},
journal = {Applied Energy},
volume = {342},
pages = {121150},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121150},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923005147},
author = {Neelam Mughees and Mujtaba Hussain Jaffery and Anam Mughees and Ejaz Ahmad Ansari and Abdullah Mughees},
keywords = {Integrated demand response, Industrial multi-energy microgrids, Differential evolution, Reinforcement learning, Battery degradation cost},
abstract = {The fourth industrial revolution is being propelled by the “Energy Internet,” which aims to encourage the integration of industrial multi-energy microgrids (MEMG) and renewable energy sources. The conventional demand response (DR) schemes only utilize a single energy source, which limits the industrial users (IUs) and prevents them from making full use of the demand side's communication capabilities. However, smart industrial multi-energy microgrids (MEMGs) give IUs additional options for meeting their energy needs by integrating diverse energy sources, including electricity, natural gas, and thermal power. This new approach to DR programs is known as “Integrated Demand Response” (IDR). This research work proposes a smart IDR program for a novel grid-connected industrial MEMG framework consisting of exhaust air wind turbines, concentrated photovoltaic-thermal panels, an electrical energy storage system (EES), a thermal energy storage (TES), a diesel generator, an indirect customer-to-customer energy trading platform, and typical electrical and thermal industrial loads. The proposed smart IDR considers the uncertainties of both wind and solar power generation and the buying and selling costs of electrical and thermal energy to automatically reduce the industry's total power consumption and battery EES degradation costs. A novel State-Action-Reward-State-Action (SARSA)-based composite different evolution (DE) method is proposed to solve a complex scenario-based non-convex optimization problem. It uses two selection strategies, three mutation strategies, and a positive feedback mechanism to change the states of the individuals. The strategies are coupled in pairs, resulting in a total of six distinct actions that may be performed by the SARSA agents. This allows an individual to not get stuck at a local optimum and adaptively benefit from all the mutation and parameter selection methods. Moreover, SARSA has introduced two more factors, the discount factor and the learning rate, which further improve the optimization performance. The proposed method is also compared with five other state-of-the-art methods to prove its effectiveness in minimizing industrial energy bills and battery degradation costs. The simulated results confirmed that the proposed SARSA-based composite DE algorithm has achieved the lowest total energy cost and battery degradation costs when compared with other state-of-the-art algorithms.}
}
@article{LI2021480,
title = {Complicated robot activity recognition by quality-aware deep reinforcement learning},
journal = {Future Generation Computer Systems},
volume = {117},
pages = {480-485},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330417},
author = {Xing Li and Junpei Zhong and M.M. Kamruzzaman},
keywords = {Human–computer interaction, Deep reinforcement learning, Quality model, Policy search, End-to-end learning},
abstract = {Automatic robot activity understanding plays an important role in human–computer interaction (HCI), especially in smart home service robots. Existing manipulator control methods, such as position control, vision-based control method, fail to meet the requirements of autonomous learning. Reinforcement learning can cope with the interaction of robot control and environment; however, the method should relearn the control method when the position of target object changes. To solve this problem, this paper proposes a quality model to utilize deep reinforcement learning scheme to achieve an end-to-end manipulator control. Specifically, we design a policy search algorithm to achieve automatic learning of manipulator. To avoid relearning of manipulator, we design convolutional neural network control scheme to remain the robustness of manipulator. Extensive experiment has shown the effectiveness of our proposed method.}
}
@article{VENKATASATISH202227646,
title = {Reinforcement learning based energy management systems and hydrogen refuelling stations for fuel cell electric vehicles: An overview},
journal = {International Journal of Hydrogen Energy},
volume = {47},
number = {64},
pages = {27646-27670},
year = {2022},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2022.06.088},
url = {https://www.sciencedirect.com/science/article/pii/S0360319922027008},
author = {R. Venkatasatish and C. Dhanamjayulu},
keywords = {Hydrogen refuelling station, Fuel cell hybrid electric vehicle, Energy storage systems, Energy management systems, Reinforcement learning, Deep Q learning},
abstract = {This paper examines the current state of the art of hydrogen refuelling stations-based production and storage systems for fuel cell hybrid electric vehicles (FCHEV). Nowadays, the emissions are increasing rapidly due to the usage of fossil fuels and the demand for hydrogen refuelling stations (HRS) is emerging to replace the conventional vehicles with FCHEVs. Hence, the availability of HRS and its economic aspects are discussed. In addition, a comprehensive study is presented on the energy storage systems such as batteries, supercapacitors and fuel cells which play a major role in the FCHEVs. An energy management system (EMS) is essential to meet the load requirement with effective utilisation of power sources with various optimizing techniques. A detailed comparative analysis is presented on the merits of Reinforcement learning (RL) for the FCHEVs. The significant challenges are discussed in depth with potential solutions for future work.}
}
@article{CHEN2022110581,
title = {Adaptive optimal output tracking of continuous-time systems via output-feedback-based reinforcement learning},
journal = {Automatica},
volume = {146},
pages = {110581},
year = {2022},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2022.110581},
url = {https://www.sciencedirect.com/science/article/pii/S0005109822004423},
author = {Ci Chen and Lihua Xie and Kan Xie and Frank L. Lewis and Shengli Xie},
keywords = {Reinforcement learning, Off-policy, Output tracking, Output feedback, Adaptive optimal control},
abstract = {Reinforcement learning provides a powerful tool for designing a satisfactory controller through interactions with the environment. Although off-policy learning algorithms were recently designed for tracking problems, most of these results either are full-state feedback or have bounded control errors, which may not be flexible or desirable for engineering problems in the real world. To address these problems, we propose an output-feedback-based reinforcement learning approach that allows us to find the optimal control solution using input–output data and ensure the asymptotic tracking control of continuous-time systems. More specifically, we first propose a dynamical controller revised from the standard output regulation theory and use it to formulate an optimal output tracking problem. Then, a state observer is used to re-express the system state. Consequently, we address the rank issue of the parameterization matrix and analyze the state re-expression error that are crucial for transforming the off-policy learning into an output-feedback form. A comprehensive simulation study is given to demonstrate the effectiveness of the proposed approach.}
}
@article{CHEN2023126871,
title = {Identifying critical nodes via link equations and deep reinforcement learning},
journal = {Neurocomputing},
volume = {562},
pages = {126871},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126871},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223009943},
author = {Peiyu Chen and Wenhui Fan},
keywords = {Identify critical nodes, Deep reinforcement learning, Information diffusion, Graph neural network},
abstract = {Identifying an optimal set of nodes that can maximize the spread of influence in a network is a crucial challenge in network science. It has numerous applications such as epidemic control and rumor containment. However, most existing techniques are limited by their high computational costs, making them impractical for graphs with millions of nodes. Moreover, the previous approaches have primarily focused on the structural characteristics of the network while the characteristics of information diffusion are ignored. This paper proposes a deep reinforcement learning framework, DeepELE, to bridge these gaps. DeepELE incorporates a graph embedding technique to represent the graph states and applies a deep reinforcement learning method to learn the policy automatically. Note that we assess the contribution of links to spreading processes and further account for the diffusion-related contribution along with the graph structure information into convolutional neural and the Q network. Extensive experiments on both synthetic and real-world networks validate the efficiency and efficacy of DeepELE. The results demonstrate that DeepELE significantly outperforms the state-of-the-art methods, especially for large-scale networks with millions of nodes.}
}
@article{YAN2023109136,
title = {A railway accident prevention method based on reinforcement learning – Active preventive strategy by multi-modal data},
journal = {Reliability Engineering & System Safety},
volume = {234},
pages = {109136},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109136},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023000510},
author = {Dongyang Yan and Keping Li and Qiaozhen Zhu and Yanyan Liu},
keywords = {Railway, Accident prediction, Artificial intelligence, Railway safety, Text data},
abstract = {Railway systems are entering an era of highly intelligent automation where stability and safety are becoming increasingly important. However, there is still a lack of intelligent and effective ways for railway accident prevention, especially active accident prevention strategies. This paper presents a railway accident prevention method based on the reinforcement learning model and multi-modal data to achieve active railway accident prevention strategies. Three metrics are designed to show the performance of active prevention methods. Based on the three metrics and the data from Federal Railroad Administration, the effectiveness of the proposed method is verified in the case study by introducing two methods as baselines. The results also show that nearly 30% of accidents can be effectively prevented through active preventive measures with the proposed method. Finally, this paper analyzes the influence of personal skills on the proposed model and makes relevant suggestions for improving railway safety based on the analysis of the results.}
}
@article{LI201299,
title = {Reinforcement learning for joint pricing, lead-time and scheduling decisions in make-to-order systems},
journal = {European Journal of Operational Research},
volume = {221},
number = {1},
pages = {99-109},
year = {2012},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2012.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S0377221712002135},
author = {Xueping Li and Jiao Wang and Rapinder Sawhney},
keywords = {Pricing, Reinforcement learning (RL), Scheduling, Q-learning, Simulation-based optimization, Semi-Markov Decision Problem (SMDP)},
abstract = {The paper investigates a problem faced by a make-to-order (MTO) firm that has the ability to reject or accept orders, and set prices and lead-times to influence demands. Inventory holding costs for early completed orders, tardiness costs for late delivery orders, order rejection costs, manufacturing variable costs, and fixed costs are considered. In order to maximize the expected profits in an infinite planning horizon with stochastic demands, the firm needs to make decisions from the following aspects: which orders to accept or reject, the trade-off between price and lead-time, and the potential for increased demand against capacity constraints. We model the problem as a Semi-Markov Decision Problem (SMDP) and develop a reinforcement learning (RL) based Q-learning algorithm (QLA) for the problem. In addition, we build a discrete-event simulation model to validate the performance of the QLA, and compare the experimental results with two benchmark policies, the First-Come-First-Serve (FCFS) policy and a threshold heuristic policy. It is shown that the QLA outperforms the existing policies.}
}
@article{DENG2023104955,
title = {Deep reinforcement learning for fuel cost optimization in district heating},
journal = {Sustainable Cities and Society},
volume = {99},
pages = {104955},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2023.104955},
url = {https://www.sciencedirect.com/science/article/pii/S2210670723005668},
author = {Jifei Deng and Miro Eklund and Seppo Sierla and Jouni Savolainen and Hannu Niemistö and Tommi Karhela and Valeriy Vyatkin},
keywords = {Deep reinforcement learning, Digital twin, District heating, Setpoint optimization},
abstract = {This study delves into the application of deep reinforcement learning (DRL) frameworks for optimizing setpoints in district heating systems, which experience hourly fluctuations in air temperature, customer demand, and fuel prices. The potential for energy conservation and cost reduction through setpoint optimization, involving adjustments to supply temperature and thermal energy storage utilization, is significant. However, the inherent nonlinear complexities of the system render conventional manual methods ineffective. To address these challenges, we introduce a novel learning framework with an expert knowledge module tailored for DRL techniques. The framework leverages system status information to facilitate learning. The training is performed by employing model-free DRL methods and a refined digital twin of the Espoo district heating system. The expert module, accounting for power plant capacities, ensures actionable directives aligned with operational feasibility. Empirical validation through comprehensive simulations demonstrates the efficacy of the proposed approach. Comparative analyses against manual methods and evolutionary techniques highlight the approach's superior ability to curtail fuel costs. This study advances the understanding of DRL in district heating optimization, offering a promising avenue for enhanced energy efficiency and cost savings.}
}
@article{ZOU2022108372,
title = {Robotic seam tracking system combining convolution filter and deep reinforcement learning},
journal = {Mechanical Systems and Signal Processing},
volume = {165},
pages = {108372},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108372},
url = {https://www.sciencedirect.com/science/article/pii/S0888327021007263},
author = {Yanbiao Zou and Tao Chen and Xiangzhi Chen and Jinchao Li},
keywords = {Seam tracking, Welding robot, Line laser sensor, Weld feature point localization, Convolution filter, Deep reinforcement learning},
abstract = {To perform automatic, real-time seam tracking tasks effectively, a robust and accurate seam tracking system must be designed. In this paper, we solve the seam tracking issue using a six-axis welding robot, a line laser sensor and an industrial computer. The processing of welding images is the core of the seam tracking system, which aims to determine the weld feature point in each image. We propose a two-stage weld feature point localization method that combines convolution filter and deep reinforcement learning (CF-DRL) to localize the weld feature point in each welding image robustly and accurately. In the first stage, the weld feature point is roughly tracked using a convolution filter tracker. But the position given by the convolution tracker is sometimes not accurate enough due to the natural gap between visual tracking and seam tracking. Consequently, in the second stage, the weld feature point should be further refined using our trained policy network. Using our two-stage weld feature point localization method, the weld feature points can be determined from noisy images in real time during the welding process. The 3D coordinate values of these points are obtained according to the structured light measurement principle to control the movement of the robot and the torch in real time. A robotic seam tracking system is established based on the equipment and methods mentioned above. Experimental results show that the welding torch runs smoothly with a strong arc light and splash interference. The mean tracking error of our experiments reaches 0.189 mm, which can fulfill actual welding requirements. Several comparison tests have been performed to illustrate the robustness and accuracy of our seam tracking system using our welding image dataset.}
}
@article{CHEN202264,
title = {A deep reinforcement learning based method for real-time path planning and dynamic obstacle avoidance},
journal = {Neurocomputing},
volume = {497},
pages = {64-75},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222005367},
author = {Pengzhan Chen and Jiean Pei and Weiqing Lu and Mingzhen Li},
keywords = {Soft Actor-Critic (SAC), Prioritized Experience Replay (PER), Dynamic obstacle avoidance, Path planning},
abstract = {In a dynamic environment, the moving obstacle makes the path planning of the manipulator very difficult. Therefore, this paper proposes a path planning with dynamic obstacle avoidance method of the manipulator based on a deep reinforcement learning algorithm soft actor-critic (SAC). To avoid the moving obstacle in the environment and make real-time planning, we design a comprehensive reward function of dynamic obstacle avoidance and target approach. Aiming at the problem of low sample utilization caused by random sampling, in this paper, prioritized experience replay (PER) is employed to change the weight of samples, and then improve the sampling efficiency. In addition, we carry out the simulation experiment and give the results. The result shows that this method can effectively avoid moving obstacles in the environment, and complete the planning task with a high success rate.}
}
@article{SYAFIIE201173,
title = {Model-free control based on reinforcement learning for a wastewater treatment problem},
journal = {Applied Soft Computing},
volume = {11},
number = {1},
pages = {73-82},
year = {2011},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2009.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S1568494609002087},
author = {S. Syafiie and F. Tadeo and E. Martinez and T. Alvarez},
keywords = {Wastewater treatment plants, Intelligent control, Oxidation–reduction potential},
abstract = {This article presents a proposal, based on the model-free learning control (MFLC) approach, for the control of the advanced oxidation process in wastewater plants. This is prompted by the fact that many organic pollutants in industrial wastewaters are resistant to conventional biological treatments, and the fact that advanced oxidation processes, controlled with learning controllers measuring the oxidation–reduction potential (ORP), give a cost-effective solution. The proposed automation strategy denoted MFLC-MSA is based on the integration of reinforcement learning with multiple step actions. This enables the most adequate control strategy to be learned directly from the process response to selected control inputs. Thus, the proposed methodology is satisfactory for oxidation processes of wastewater treatment plants, where the development of an adequate model for control design is usually too costly. The algorithm proposed has been tested in a lab pilot plant, where phenolic wastewater is oxidized to carboxylic acids and carbon dioxide. The obtained experimental results show that the proposed MFLC-MSA strategy can achieve good performance to guarantee on-specification discharge at maximum degradation rate using readily available measurements such as pH and ORP, inferential measurements of oxidation kinetics and peroxide consumption, respectively.}
}
@article{OUYANG20173476,
title = {Vibration Control Based on Reinforcement Learning for a Single-link Flexible Robotic Manipulator},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {3476-3481},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.932},
url = {https://www.sciencedirect.com/science/article/pii/S240589631731399X},
author = {Yuncheng Ouyang and Wei He and Xiajing Li and Jin-Kun Liu and Guang Li},
keywords = {Neural Networks, Vibration Control, Flexible Robotic Manipulator, Assumed Mode Method, Reinforcement Learning},
abstract = {In this paper, we focus on the reinforcement learning control of a single-link flexible manipulator and attempt to suppress the vibration due to its flexibility and lightweight structure. The assumed mode method (AMM) and the Lagrange’s equation are adopted in modeling to enhance the satisfaction of precision. Two radial basis function neural networks (RBFNNs) are employed in the designed control algorithm, actor neural network (NN) for generating a policy and critic NN for evaluating the cost-function. Rigorous stability of the system has been proven via Lyapunov’s direct method. According to the performance of simulation for the proposed control scheme, the superiority and feasibility of the proposed controller is verified.}
}
@incollection{ANANDAN20221093,
title = {Optimal Control Policies of a Crystallization Process Using Inverse Reinforcement Learning},
editor = {Ludovic Montastruc and Stephane Negny},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {51},
pages = {1093-1098},
year = {2022},
booktitle = {32nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-95879-0.50183-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323958790501831},
author = {Paul Danny Anandan and Chris D. Rielly and Brahim Benyahia},
keywords = {Apprenticeship Learning, Reinforcement Learning, Inverse Reinforcement Learning, Batch Crystallization},
abstract = {Crystallization is widely used in the pharmaceutical industry to purify reaction intermediates and final active pharmaceutical ingredients. This work presents a novel implementation of Inverse Reinforcement Learning (IRL) approach where an agent observes the expert’s optimal control policies of a crystallization process and attempts to mimic its performance. In essence, an Apprenticeship Learning (AL) setup was developed where the expert demonstrates the control task to the IRL agent to help attain effective control performance when compared to the expert. This is achieved through repeated execution of “exploitation policies” that simply maximizes the rewards over the consecutive IRL training episodes. The cooling crystallization of paracetamol is used as a case study and both proportional integral derivative (PID) and Model Predictive Control (MPC) strategies were considered as expert systems. A model based IRL technique is implemented to achieve effective trajectory tracking which ensures final crystal size, considered as the critical quality attributes, by reducing the deviation from the optimal reference trajectories namely process temperature, supersaturation, and particle size. The performance of the trained IRL agent was validated against the PID and MPC and tested in presence of noisy measurements and model uncertainties.}
}
@article{ZENG2021173,
title = {Deep-reinforcement-learning-based images segmentation for quantitative analysis of gold immunochromatographic strip},
journal = {Neurocomputing},
volume = {425},
pages = {173-180},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220305385},
author = {Nianyin Zeng and Han Li and Zidong Wang and Weibo Liu and Songming Liu and Fuad E. Alsaadi and Xiaohui Liu},
keywords = {Deep reinforcement learning, Image segmentation, Deep belief network, Multi-factor learning curve, Gold immunochromatographic strip},
abstract = {Gold immunochromatographic strip (GICS) is a widely used lateral flow immunoassay technique. A novel image segmentation method is developed in this paper for quantitative analysis of GICS based on the deep reinforcement learning (DRL), which can accurately distinguish the test line and the control line in the GICS images. The deep belief network (DBN) is employed in the deep Q network in our DRL algorithm. Meanwhile, the multi-factor learning curve is introduced in the DRL algorithm to dynamically adjust the capacity of the replay buffer and the sampling size, which leads to enhanced learning efficiency. It is worth mentioning that the states, actions, and rewards in the developed DRL algorithm are determined based on the characteristics of GICS images. Experiment results demonstrate the feasibility and reliability of the proposed DRL-based image segmentation method and show that the proposed new image segmentation method outperforms some existing image segmentation methods for quantitative analysis of GICS images.}
}
@article{NIKITA2021116171,
title = {Reinforcement learning based optimization of process chromatography for continuous processing of biopharmaceuticals},
journal = {Chemical Engineering Science},
volume = {230},
pages = {116171},
year = {2021},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2020.116171},
url = {https://www.sciencedirect.com/science/article/pii/S000925092030703X},
author = {Saxena Nikita and Anamika Tiwari and Deepak Sonawat and Hariprasad Kodamana and Anurag S. Rathore},
keywords = {Reinforcement learning, Optimization, BioSMB, Ion exchange chromatography, mAb},
abstract = {Process intensification in the form of continuous processing is presently being adopted by the biopharmaceutical industry as it offers significant advantages over conventional processing. Chromatographic steps form the core separation steps of a typical biopharma process due to their high selectivity and robustness. To this end, this paper proposes a novel approach based on reinforcement learning (RL), wherein a maximization problem is formulated for cation exchange chromatography for separation of charge variants by optimization of the process flowrate. Chromatography analysis and design toolkit have been used for process simulation and the optimum flow rate at which the yield is maximum and purity constraints are satisfied has been estimated based on the reward policy of RL. Results were experimentally validated and indicate that the proposed RL based approach is superior to the conventional trial and error method of optimizing flowrate in terms of both optimality and computational aspects (3X faster).}
}
@article{GUO2023109669,
title = {Collaborative planning of multi-tier sustainable supply chains: A reinforcement learning enhanced heuristic approach},
journal = {Computers & Industrial Engineering},
volume = {185},
pages = {109669},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109669},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223006939},
author = {Yuhan Guo and Tao Chen and Youssef Boulaksil and Linfan Xiao and Hamid Allaoui},
keywords = {Sustainability, Triple bottom-line, Collaboration, Multiple networks, Meta-heuristic, Markov Decision Process},
abstract = {Despite the growing importance of collaboration in achieving sustainability-related advantages for companies, existing studies lack a systematic framework to determine how multiple supply chains can jointly facilitate strategical decision-making to achieve the objectives in the triple-bottom-line (3BL). In this study, we suggest a comprehensive mixed-integer linear programming model for multi-network collaboration considering 3BL sustainability indicators and develop a heuristic approach enhanced by reinforcement learning to solve the model. The proposed model allows for optimal decision-making across multiple sustainable supply chains, simultaneously minimizing total costs and environmental impacts as well as maximizing social responsibility. The heuristic algorithm integrates a Markov decision-making process and information accumulation mechanism with the exploration of the solution space. It effectively learns from the solving process, and applies the most appropriate operator to iteratively improve the current solution according to the knowledge learnt. Extensive experiments based on real-world data are conducted and the results demonstrate that the proposed model and solution framework yield an effective collaborative supply chain design for each actor with superior efficiency and accuracy. Compared with CPLEX, the average solving times for medium-to-large instance scales are reduced by 16.34% to 87.59%, and 86.67% of the instances saw an improvement of solution quality, with an average improvement of 5.67%. Moreover, the inclusion of horizontal transportation in the proposed model provides a significant improvement of 51.24% in the economic bottom-line, as well as an improvement of 3.42% in the environmental bottom-line.}
}
@article{LI20221362,
title = {A novel milling parameter optimization method based on improved deep reinforcement learning considering machining cost},
journal = {Journal of Manufacturing Processes},
volume = {84},
pages = {1362-1375},
year = {2022},
issn = {1526-6125},
doi = {https://doi.org/10.1016/j.jmapro.2022.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S1526612522007885},
author = {Weiye Li and Bin Li and Songping He and Xinyong Mao and Chaochao Qiu and Yue Qiu and Xin Tan},
keywords = {Milling parameters optimization, Processing energy efficiency, Processing cost, Simulation environment, Deep reinforcement learning},
abstract = {The cutting parameters in the part machining process have a great impact on the energy efficiency and economy of the machining system. The cutting parameters in traditional machining are often selected according to the operator's experience, which lack attentions to energy saving and economy. Therefore, a milling process parameter optimization method based on deep reinforcement learning (DRL) is proposed in this paper. Taking the machining cost composed of cutting energy efficiency and machining time cost as the optimization goal, the spindle speed and feed speed under the combination of different cutting depth and cutting width parameters are optimized. Firstly, the machine tool energy consumption model is established by back propagation neural network (BPNN) regression method to realize the continuity of machine tool energy consumption state prediction, and the processing cost model is established as the optimization objective function. Then, the process parameter optimization problem is formally expressed as a Markov decision process (MDP), and the corresponding states, actions, reward functions and constraints are defined. Finally, combined with the machine tool power consumption model and machining cost model, the simulation environment is established, and the BP-TD3 method is proposed to solve the Markov decision problem of milling parameter optimization. Taking the machining center as an example, the aluminum alloy workpiece is milled. Compared with the classical optimization algorithm, the proposed method can save 95 % optimization calculation time, and ensure that the average processing cost after optimization is close to the minimum processing cost obtained by the classical optimization algorithm.}
}
@article{LIU2023103232,
title = {AdaBoost-Bagging deep inverse reinforcement learning for autonomous taxi cruising route and speed planning},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {177},
pages = {103232},
year = {2023},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2023.103232},
url = {https://www.sciencedirect.com/science/article/pii/S136655452300220X},
author = {Shan Liu and Ya Zhang and Zhengli Wang and Shiyi Gu},
keywords = {Cruising route planning, Cruising speed planning, Autonomous taxi, Inverse reinforcement learning, Ensemble learning},
abstract = {Taxi cruising route planning has attracted considerable attention, and relevant studies can be broadly categorized into three main streams: recommending one or multiple areas, providing a detailed cruising route, and deriving the optimal routing policy. However, these studies depend on accurate pick-up/drop-off information, and seldom pay attention to cruising speed planning. In view of the rapid development of autonomous taxis, this study proposes AdaBoost-Bagging maximum entropy deep inverse reinforcement learning to learn cruising policy from experienced taxi drivers’ trajectories. Moreover, we develop a trajectory-based self-attention bidirectional LSTM model to adjust cruising speeds on different roads. Numerical experiments using real taxi trajectories in Chengdu, China demonstrate the effectiveness of our approach in learning taxi drivers’ policies and improving taxis’ operational efficiency.}
}
@article{TANG2020102844,
title = {Online operations of automated electric taxi fleets: An advisor-student reinforcement learning framework},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {121},
pages = {102844},
year = {2020},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2020.102844},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X20307464},
author = {Xindi Tang and Meng Li and Xi Lin and Fang He},
keywords = {Automated vehicle, Electric taxi, Reinforcement learning, Online operation},
abstract = {Automation and electrification are inevitable trends in the development of intelligent vehicles. It is envisioned that automated electric taxis (AETs) will play an important role in future transportation systems for serving personalized travel demands. To tackle the operational challenges caused by the high spatiotemporal heterogeneity of customer demands entails novel online strategy to intelligently manage AET fleet. This study proposes an advisor-student reinforcement learning framework to solve the online operations problem of AET fleet through which the taxis are intelligently assigned to serve demands, dispatched to zones with excessive future demands, and forced to get refueled at charging stations. Extensive numerical experiments illustrate the advantages of the proposed framework over myopic and nearest distance greedy strategies, especially when vehicle relocation is highly needed.}
}
@article{SHONKWILER20231203,
title = {Deep reinforcement learning for stacking sequence optimization of composite laminates},
journal = {Manufacturing Letters},
volume = {35},
pages = {1203-1213},
year = {2023},
note = {51st SME North American Manufacturing Research Conference (NAMRC 51)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2023.08.133},
url = {https://www.sciencedirect.com/science/article/pii/S2213846323001943},
author = {Sara Shonkwiler and Xiang Li and Richard Fenrich and Sara McMains},
keywords = {Fiber-reinforced polymer composite, Composite stacking sequence, Deep reinforcement learning},
abstract = {Fiber reinforced polymer (FRP) composite laminates are increasingly used in a wide range of safety–critical products due to their excellent material properties. The stacking sequence of FRP composite laminates plays a critical role in the resulting part’s mechanical properties. Despite this, composite engineers still commonly fabricate parts with “classic” ply layups (e.g. four ply laminate with fiber orientation angles: [-45/0/45/90°]), which may have sub-optimal mechanical performance in the expected loading/use cases. Finding the composite stacking sequence that achieves the best material and mechanical properties possible is a challenging optimization problem characterized by the large domain space involved in solving an inverse design problem. This paper introduces a novel approach to optimizing stacking sequence for composite plate stiffness by applying off-policy deep reinforcement learning (DRL). We formulate the problem as a sequential decision making process. The state of the system is based on the stiffness of the composite for the currently selected stacking sequence and the action is to select a new stacking sequence using our reward function, formulated as the offset, normalized stiffness modulus. We compare our DRL model to two classical stacking sequences, a randomized baseline model, and a competitive genetic algorithm (NSGA-II). For maximizing longitudinal composite plate stiffness, the DRL model finds the optimum solution for all ply thicknesses and the genetic algorithm comes within 0.5% of the optimum. The DRL model determines the optimum stacking sequence significantly faster and is less sensitive to parameter tuning. The DRL model and the genetic algorithm both outperform the random baseline algorithm by over 5.7 standard deviations in the most conservative case. This research demonstrates the ability of DRL to effectively and efficiently optimize composite laminate stacking sequence.}
}
@article{BORYLO202389,
title = {A tutorial on reinforcement learning in selected aspects of communications and networking},
journal = {Computer Communications},
volume = {208},
pages = {89-110},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2023.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0140366423001858},
author = {Piotr Boryło and Edyta Biernacka and Jerzy Domżał and Bartosz Ka̧dziołka and Mirosław Kantor and Krzysztof Rusek and Maciej Skała and Krzysztof Wajda and Robert Wójcik and Wojciech Za̧bek},
keywords = {Reinforcement learning, Computer networks, Example-based tutorial},
abstract = {Telecommunication systems are increasingly complex, dynamic, and heterogeneous. Tools are needed to efficiently support and automate complex control and management processes. Reinforcement learning becomes one of the most attractive and popular solutions applicable to a wide variety of different aspects of communications and networking. Its development is further boosted by the favorable conditions created by both the existing IT infrastructure and evolving network architectures. The aim of this tutorial is twofold. Firstly, to provide fundamentals regarding the Reinforcement Learning (RL) method. Secondly, to comprehensively study the examples of applying RL-based solutions to solve problems in different aspects of communications and networking. Studies are supplemented with additional explanations, figures and critical considerations, including pros and cons of using selected methods for particular purposes. This part uniquely complements the tutorial one and facilitates in-depth understanding of RL. Based on the conducted studies, we draw a comparative analysis, summaries and expected future research topics and challenges of using RL in communications and networking.}
}
@article{ZAFEIROPOULOS2022102461,
title = {Reinforcement learning-assisted autoscaling mechanisms for serverless computing platforms},
journal = {Simulation Modelling Practice and Theory},
volume = {116},
pages = {102461},
year = {2022},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2021.102461},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X21001507},
author = {Anastasios Zafeiropoulos and Eleni Fotopoulou and Nikos Filinis and Symeon Papavassiliou},
keywords = {Serverless application, Serverless computing, Reinforcement learning, Autoscaling, Intelligent orchestration},
abstract = {Serverless computing is emerging as a cloud computing paradigm that provisions computing resources on demand, while billing is taking place based on the exact usage of the cloud resources. The responsibility for infrastructure management is undertaken by cloud providers, enabling developers to focus on the development of the business logic of their applications. For managing scalability, various autoscaling mechanisms have been proposed that try to optimize the provisioning of resources based on the posed workload. These mechanisms are configured and managed by the cloud provider, imposing non negligible administration overhead. A set of challenges are identified for introducing automation and optimizing the provisioning of resources, while in parallel respecting the agreed Service Level Agreement between cloud and application providers. To address these challenges, we have developed autoscaling mechanisms for serverless applications that are powered by Reinforcement Learning (RL) techniques. A set of RL environments and agents have been implemented (based on Q-learning, DynaQ+ and Deep Q-learning algorithms) for driving autoscaling mechanisms, able to autonomously manage dynamic workloads with Quality of Service (QoS) guarantees, while opting for efficient usage of resources. The produced environments and agents are evaluated in real and simulated environments, taking advantage of the Kubeless open-source serverless platform. The evaluation results validate the suitability of the proposed mechanisms to efficiently tackle scalability management for serverless applications.}
}
@article{LAU2002493,
title = {ADAPTIVE VECTOR QUANTIZATION FOR REINFORCEMENT LEARNING},
journal = {IFAC Proceedings Volumes},
volume = {35},
number = {1},
pages = {493-498},
year = {2002},
note = {15th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20020721-6-ES-1901.01068},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015394891},
author = {H.Y.K. Lau and K.L. Mak and I.S.K. Lee},
keywords = {Learning algorithm, intelligent control, vector quantization, robot navigation, automated guided vehicles},
abstract = {Dynamic programming methods are capable of solving reinforcement learning problems, in which an agent must improve its behavior through trial-and-error interactions with a dynamic environment. However, these computational algorithms suffer from the curse of dimensionality (Bellman, 1957) that the number of computational operations increases exponentially with the cardinality of the state space. In practice, this usually results in a very long training time and applications in continuous domain are far from trivial. In order to ease this problem, we propose the use of vector quantization to adaptively partition the state space based on the recent estimate of the action-value function. In particular, this state-space partitioning operation is performed incrementally to reflect the experience accumulated by the agent as it explores the underlying environment.}
}
@article{CHEN2023284,
title = {A collaborative scheduling method for cloud computing heterogeneous workflows based on deep reinforcement learning},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {284-297},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.11.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22004034},
author = {Genxin Chen and Jin Qi and Ying Sun and Xiaoxuan Hu and Zhenjiang Dong and Yanfei Sun},
keywords = {Heterogeneous workflows, Cloud computing, Deep reinforcement learning, High-dimensional objectives, Collaborative scheduling, Adaptive mechanism},
abstract = {Aiming at the problem of low overall service quality caused by the disordered collaboration of heterogeneous workflows and discontinuous task execution in cloud computing scenarios, this paper proposes a collaborative scheduling method for heterogeneous workflows in cloud computing based on deep reinforcement learning. The method optimizes workflow makespan, cost, fairness and continuity in cloud computing under the constraints of task execution continuity. First, the structure and time sequence features are extracted for the dynamic scheduling process, and a reasonable scheduling decision support feature set is constructed. Second, a time-step adaptive scheduling mechanism is designed to simplify redundant information in the scheduling process and enables the agent to achieve efficient learning. In addition, using equilibrium, priority and preference scheduling strategies, an immediate-lag compound reward mechanism and a scheduling-switching hybrid action are designed to achieve a unification of the agent’s learning objectives and actual scheduling requirements. Finally, by constructing a simulation platform and conducting comparative experiments with four other algorithms, the results show that the proposed method has advantages in collaborative optimization of high-dimensional objectives under task continuity constraints. Including the task loading strategy can optimize the makespan performance by 16.6% and improve the fairness index by 5.3%.}
}
@article{BECSI2018405,
title = {Policy gradient based Reinforcement learning control design of an electro-pneumatic gearbox actuator ⁎⁎The research reported in this paper was supported by the Higher Education Excellence Program of the Ministry of Human Capacities in the frame of Artificial Intelligence research area of Budapest University of Technology and Economics (BME FIKPMI/FM).EFOP-3.6.3-VEKOP-16-2017-00001: Talent management in autonomous vehicle control technologies- The Project is supported by the Hungarian Government and co-financed by the European Social Fund},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {22},
pages = {405-411},
year = {2018},
note = {12th IFAC Symposium on Robot Control SYROCO 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.577},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318332841},
author = {Tamás Bécsi and Szilárd Aradi and Ádám Szabó and Péter Gáspár},
keywords = {Reinforcement Learning Control, Mechatronic systems, Automotive sensors, actuators, Non-Linear Control Systems},
abstract = {The paper presents a reinforcement learning based solution for the control design problem of a gearbox actuator. The system is operated by an electro-pneumatic, three-state, floating piston cylinder. Besides the primary goals of positioning the piston, the nonlinear system’s quality objectives are to minimize switching time and overshoot. The control strategy based on the measurable parameters of the system is realized by a dense feedforward neural network. With the utilization of the policy based reinforcement learning architecture, the learning agent develops the optimal strategy for fast and smooth switching, under different and changing conditions.}
}
@article{XIAO202397,
title = {Deep reinforcement learning-driven smart and dynamic mass personalization},
journal = {Procedia CIRP},
volume = {119},
pages = {97-102},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004390},
author = {Ruxin Xiao and Yuchen Wang and Xinheng Wang and Ang Liu and Jinhua Zhang},
keywords = {Deep reinforcement learning, Smart mass personalization, Artificial intelligence-enhanced design, Real-time system},
abstract = {Smart mass personalization is becoming increasingly important to improve the competitiveness of products. In mass personalization, customers’ contextual data is characterized by complexity and fluctuation. Hence, designers must ensure the timeliness of smart mass personalization that can continuously satisfy customers’ demands. This paper proposes a deep reinforcement learning-driven system for dynamic and smart mass personalization. Besides, the system adopts deep Q-network as the training algorithm due to its compatibility with both off-policy and on-policy training. In the beginning, deep Q-network will get trained based on previous customers’ contextual data collected from purchase history and web services until it can generate the expected policy for concept generation. Then, the agent in deep Q-network will dynamically tune the algorithm by continuously interacting with incoming customers’ contextual data. Besides, this paper depicts a scenario of personalization for automobiles to illustrate this system. The contribution of this paper lies in the application of DRL to realize dynamic updates in smart mass personalization and the innovative dynamic action space generated from customer clusters.}
}
@article{LEE2021103737,
title = {Autonomous construction hoist system based on deep reinforcement learning in high-rise building construction},
journal = {Automation in Construction},
volume = {128},
pages = {103737},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103737},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521001886},
author = {Dongmin Lee and Minhoe Kim},
keywords = {Construction hoist, Autonomous hoist, Adaptive hoist control, Intelligent automation, Deep reinforcement learning, Deep Q-network (DQN)},
abstract = {Construction hoists at most building construction sites are manually controlled by human operators using their intuitions; as a result, unnecessary trips are often made when multiple hoists are operating simultaneously and/or when complicated hoist calls are requested. These trips increase a passenger's waiting time and lifting time, reducing the lifting performance of the hoists. To address this issue, the authors develop an autonomous hoist supported by a deep Q-network (DQN), a deep reinforcement learning method. The results show that the DQN algorithm can provide better control policy in complicated real-world hoist control situations than previous control algorithms, reducing the waiting time and lifting time of passengers by up to 86.7%. Such an automated hoist control system helps shorten the project schedule by increasing the lifting performance of multiple hoists at high-rise building construction sites.}
}
@incollection{MOWBRAY2022469,
title = {A Reinforcement Learning Approach to Online Scheduling of Single-Stage Batch Chemical Production Processes},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {469-474},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50078-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596500786},
author = {Max Mowbray and Dongda Zhang and Antonio Del Rio Chanona},
keywords = {Reinforcement Learning, Combinatorial Optimization, Production Scheduling, Machine Learning},
abstract = {The field of Reinforcement Learning (RL) has received a lot of attention for decision-making under uncertainty. Lately, much of this focus has been on the application of RL for combinatorial optimisation. Recent work has showcased the use of RL on a single-stage continuous chemical production scheduling problem. This work highlighted the potential of RL for optimal decision-making under uncertainty in the paradigm of (bio)chemical production scheduling. However, this novel approach is yet to be tested in the context of parallel unit operations and batch processing systems. In this work, we outline a framework for the use of RL to handle single-stage parallel, batch production. In particular, we incorporate elements such as uncertainties in the model data, limited batch size, sequencing constraints, and uncertainties in processing times and product demand, which make for a substantially harder problem. To handle the presence of precedence or succession constraints, by taking inspiration from approaches such as generalised disjunctive programming, we propose a novel methodology that identifies transformations of the control set available to the RL at each control interaction. Given that production typically operates under standard operating procedures, such transformations can be identified by logic. The efficacy of policy synthesis via evolutionary RL methods is benchmarked against mixed integer programming. The results of this study provide further support for the use of RL in online scheduling.}
}
@article{MATTERA2023200181,
title = {Shrinkage estimation with reinforcement learning of large variance matrices for portfolio selection},
journal = {Intelligent Systems with Applications},
volume = {17},
pages = {200181},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200181},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323000066},
author = {Giulio Mattera and Raffaele Mattera},
keywords = {Machine learning, Deep learning, Finance, Covariance estimation, Asset allocation, LSTM},
abstract = {A large amount of assets characterizes high-dimensional portfolio selection problems compared to temporal observation. In such a high-dimensional framework, the asset allocation is unfeasible because the covariance matrix obtained with the usual sample estimators cannot be inverted. This paper proposes a new shrinkage estimator based on reinforcement learning for large covariance matrices that is optimal in the context of portfolio selection. The resulting estimator is entirely data-driven since the optimal shrinkage intensity is given by optimizing neural network weights. This paper presents two different architectures: a standard fully connected network for a classical Policy Gradient Agent (PGA) and a Gated Recurrent Unit for a Recurrent Policy Gradient Agent (RPGA). To show the validity of the proposal, an application to asset allocation with Industry portfolios is provided. The results indicate that the RPGA-based approach in shrinkage estimation provides the best performance in out-of-sample comparison.}
}
@incollection{KUBOSAWA20221777,
title = {Practical Human Interface System for Transition Guidance in Chemical Plants using Reinforcement Learning},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {1777-1782},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50296-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596502967},
author = {Shumpei Kubosawa and Takashi Onishi and Yoshimasa Tsuruoka and Yasuo Fujisawa and Masanori Endo and Atsushi Uchimura and Masahiko Tatsumi and Norio Esaki and Gentaro Fukano and Tsutomu Kimura and Akihiko Imagawa and Takayasu Ikeda},
keywords = {chemical plant, reinforcement learning, optimisation, explainable AI},
abstract = {In chemical plants, transition operations, such as changing the production load from 100% to 80%, are commonly performed to satisfy production needs. As plant models used in conventional automatic control methods (e.g. step response models) cannot predict non-steady states, these transition operations warrant manual control. Previously, we proposed an automatic optimal control method using dynamic simulators and reinforcement learning, a machine learning method in artificial intelligence (AI), for transition operations. We implemented this existing AI system in an actual industrial plant and determined that further improvements were required in the interaction between the system and human operators for reliable and acceptable guidance. In this paper, we propose a human interface system for realising optimal transition operations by enabling AI to cooperate with human operators. To validate and authorise the AI-proposed manipulations performed by human operators, the interface system presented the entire procedure and sensors influencing the AI decision for online disturbance rejection prior to actual manipulations. The interface, coupled with the control method, was evaluated experimentally in an actual chemical plant. The proposed system demonstrated optimised transition operations for producing purity changes under abrupt heavy rain disturbance in terms of guidance.}
}
@article{SUN2023137,
title = {Persistent coverage of UAVs based on deep reinforcement learning with wonderful life utility},
journal = {Neurocomputing},
volume = {521},
pages = {137-145},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.11.091},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222014862},
author = {Zhaomei Sun and Nan Wang and Hong Lin and Xiaojun Zhou},
keywords = {Persistent coverage, Deep reinforcement learning, UAVs, BRNN, Wonderful life utility},
abstract = {The optimization problem of persistent coverage for a target region by using unmanned aerial vehicles (UAVs) is addressed in this study. A deep reinforcement learning algorithm (DRL) based on bidirectional recurrent neural networks (BRNN) is proposed to obtain the optimal control output policy of UAVs which manipulate the UAVs to periodically cover the whole target region and to minimize the maximum age of cells. The UAVs coordinate autonomously by using wonderful life utility (WLU) functions and BRNN. Because all control policies share parameters, the algorithm has strong robustness and scalability which enable individual UAV to freely join or leave the task without affecting the operation of the entire system. The algorithm uses consistent outputs to control multiple heterogeneous UAVs. Simulation results are given to illustrate the effectiveness of the proposed method.}
}
@article{MAK2023104376,
title = {Fair collaborative vehicle routing: A deep multi-agent reinforcement learning approach},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {157},
pages = {104376},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2023.104376},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X23003662},
author = {Stephen Mak and Liming Xu and Tim Pearce and Michael Ostroumov and Alexandra Brintrup},
keywords = {Collaborative vehicle routing, Deep multi-agent reinforcement learning, Negotiation, Gain sharing, Multi-agent systems, Machine learning},
abstract = {Collaborative vehicle routing occurs when carriers collaborate through sharing their transportation requests and performing transportation requests on behalf of each other. This achieves economies of scale, thus reducing cost, greenhouse gas emissions and road congestion. But which carrier should partner with whom, and how much should each carrier be compensated? Traditional game theoretic solution concepts are expensive to calculate as the characteristic function scales exponentially with the number of agents. This would require solving the vehicle routing problem (NP-hard) an exponential number of times. We therefore propose to model this problem as a coalitional bargaining game solved using deep multi-agent reinforcement learning, where – crucially – agents are not given access to the characteristic function. Instead, we implicitly reason about the characteristic function; thus, when deployed in production, we only need to evaluate the expensive post-collaboration vehicle routing problem once. Our contribution is that we are the first to consider both the route allocation problem and gain sharing problem simultaneously — without access to the expensive characteristic function. Through decentralised machine learning, our agents bargain with each other and agree to outcomes that correlate well with the Shapley value — a fair profit allocation mechanism. Importantly, we are able to achieve a reduction in run-time of 88%.}
}
@article{ZHANG2023103028,
title = {Robust safe reinforcement learning control of unknown continuous-time nonlinear systems with state constraints and disturbances},
journal = {Journal of Process Control},
volume = {128},
pages = {103028},
year = {2023},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2023.103028},
url = {https://www.sciencedirect.com/science/article/pii/S0959152423001154},
author = {Haoran Zhang and Chunhui Zhao and Jinliang Ding},
keywords = {Safe reinforcement learning, State constraints, Constrained optimal control, Unknown system dynamics},
abstract = {Industrial processes must operate safely and optimally in practice, which typically entails solving a constrained optimal control (COC) problem. Finding a control policy for such a problem, however, is challenging especially when the system dynamics are unknown. In this paper, a novel safe reinforcement learning (RL) algorithm is proposed to deal with the COC problem for the continuous-time nonlinear system with unknown dynamics and disturbances. The presented method can ensure that the system operates within a predefined safe region in the presence of system model uncertainties and external disturbances, which goes beyond the results of typical RL methods. To handle the system state constraints, the problem is first transformed into an unconstrained one via the proposed data-driven slack function approach. Then an improved model-based RL method is devised to learn a near-optimal and safe control policy in real-time. Finally, a robust compensator and the composite neural-network updating rule are designed to eliminate the influence of disturbances and uncertainties. Theoretical analysis based on the Lyapunov approach is conducted to prove the closed-loop system stability and the algorithm convergence, which further guarantees the satisfaction of safety constraints. The effectiveness of the proposed RL algorithm is verified through simulations.}
}
@article{KLAR20211,
title = {An implementation of a reinforcement learning based algorithm for factory layout planning},
journal = {Manufacturing Letters},
volume = {30},
pages = {1-4},
year = {2021},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2021.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S2213846321000651},
author = {Matthias Klar and Moritz Glatt and Jan C. Aurich},
keywords = {Reinforcement learning, Machine learning, Layout planning, Factory planning, Optimization},
abstract = {Factory layout planning is a recurring and time consuming process since multiple often conflicting planning objectives have to be considered simultaneously. Inadequately planned layouts however can significantly impede the operation of a factory. Recent studies from other disciplines have shown the potential of reinforcement learning to solve complex allocation problems. Consequently, this paper presents a reinforcement learning based approach for automated layout planning. In particular, a first implementation of the algorithm using Double Deep Q Learning is presented and used to solve an allocation problem with four functional units and the transportation time as an optimization criterion. The algorithm generated an optimized layout within 8,000 episodes of training and showed promising potential for more comprehensive applications in the future.}
}
@article{ZHANG2023257,
title = {Deep reinforcement learning for dynamic flexible job shop scheduling problem considering variable processing times},
journal = {Journal of Manufacturing Systems},
volume = {71},
pages = {257-273},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001917},
author = {Lu Zhang and Yi Feng and Qinge Xiao and Yunlang Xu and Di Li and Dongsheng Yang and Zhile Yang},
keywords = {Dynamic scheduling flexible job shop problem, Deep reinforcement learning, Variable processing times, Proximal policy optimization, Makespan},
abstract = {In recent years, the uncertainties and complexity in the production process, due to the boosted customized requirements, has dramatically increased the difficulties of Dynamic Flexible Job Shop Scheduling (DFJSP). This paper investigates a new DFJSP model taking into account the minimum completion time under the condition of machine processing time uncertainty, e.t. VPT-FJSP problem. In the formulated VPT-FJSP process, each workpiece needs to be processed by required machine at a certain time slot where Markov decision process (MDP) and reinforcement learning methods are adopted to solve VPT-FJSP. The agent designed in this paper employs the Proximal Policy Optimization(PPO) algorithm in deep reinforcement learning, which includes the Actor-Critic network. The input of the network is to extract the processing information matrix and to embed some advanced states in the workshop by graph neural network, which enables the agent to learn the complete state of the environment. Finally, we train and test the proposed framework on the canonical FJSP benchmark, and the experimental results show that our framework can make agent better than genetic algorithm and ant colony optimization in most cases, 94.29% of static scheduling. It is also shown superiority compared to the scheduling rules in dynamic environment and has demonstrated strong robustness in solving VPT-FJSP. Furthermore, this study conducted tests to assess the generalization capability of the agent on VPT-FJSP at different scales. In terms of exploring Makespan minimization, the agent outperformed four priority scheduling rules. These results indicate that the proposed dynamic scheduling framework and PPO algorithm are more effective in achieving superior solutions.}
}
@article{LI2023113722,
title = {Adaptive reinforcement learning fault-tolerant control for AUVs With thruster faults based on the integral extended state observer},
journal = {Ocean Engineering},
volume = {271},
pages = {113722},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.113722},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823001063},
author = {Zhifu Li and Ming Wang and Ge Ma and Tao Zou},
keywords = {Integral extended state observer (IESO), Autonomous underwater vehicles (AUVs), Fault-tolerant control (FTC), Thruster faults, Reinforcement learning (RL)},
abstract = {In this paper, a reinforcement learning (RL) fault-tolerant control (FTC) method is proposed for trajectory tracking of autonomous underwater vehicles (AUVs) with thruster faults. To deal with the thruster fault, unknown disturbance and model uncertainty, a new integral extended state observer (IESO) for fault diagnosis observation is proposed, which uses a conventional ESO to estimate the total system uncertainty, and introduces an integral mechanism to mitigate the effect of estimation error further. Thus, the problem that the estimation error caused by the traditional ESO leads to the decline of the fault-tolerant capability of the FTC system is solved. Then, to solve the problem of integral saturation due to the introduction of the integral term, the integral term is limited after the thruster fault of the AUV. Furthermore, based on the actor–critic structure of RL, a PD-like feedback controller is designed to realize the FTC of AUV in the face of thruster fault by using the total uncertainty of the IESO scheme. And the input saturation of the thruster is considered, and an auxiliary variable system is used to handle the control truncation between saturated and unsaturated inputs. Based on the Lyapunov method, the stability of the closed-loop system is analyzed and proved. Finally, the proposed method is verified to have good fault tolerance and robustness by simulation and underwater experiments.}
}
@article{WANG2023106095,
title = {Solving non-permutation flow-shop scheduling problem via a novel deep reinforcement learning approach},
journal = {Computers & Operations Research},
volume = {151},
pages = {106095},
year = {2023},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2022.106095},
url = {https://www.sciencedirect.com/science/article/pii/S0305054822003252},
author = {Zhenyu Wang and Bin Cai and Jun Li and Deheng Yang and Yang Zhao and Huan Xie},
keywords = {Non-permutation flow-shop scheduling problem, Deep reinforcement learning, Long short-term memory, Temporal difference},
abstract = {The non-permutation flow-shop scheduling problem (NPFS) is studied. We model it as a Markov decision process, creating a massive arena for reinforcement learning (RL) algorithms to work. While RL approaches with function approximation generate a significant number of sequences of highly linked states, few studies have examined the connection between the state sequences but merely shuffled their orders. To this end, this paper proposes a novel deep reinforcement learning algorithm, named LSTM-TD(0), to address NPFS. Specifically, we design fifteen state features to represent a production state at each scheduling point and fourteen actions to choose an unprocessed operation on a given machine. This study applies long short-term memory (LSTM) network to capture the intrinsic connection of the state sequences in RL-based scheduling approaches. Moreover, we enhance the LSTM model with the one-step temporal difference (TD(0)) algorithm to select each action impartially in relation to the state value, avoiding the frequent overestimation of action values in Q-learning. The proposed LSTM-TD(0) was trained using two LSTM networks and enhanced by redesigning the reward value. A series of comparative experiments were conducted between simple heuristic rules, metaheuristic rules, general DRL methods, and LSTM-TD(0) using a group of well-known benchmark problems with different scales. Comparative results have confirmed both the superiority and universality of LSTM-TD(0) over its competitors. Scalability tests reveal that our approach can generalize to instances of different sizes without retraining or knowledge transferring.}
}
@article{NGUYEN2021106574,
title = {RLTCP: A reinforcement learning approach to prioritizing automated user interface tests},
journal = {Information and Software Technology},
volume = {136},
pages = {106574},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106574},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000574},
author = {Vu Nguyen and Bach Le},
keywords = {Coverage graph, Test prioritization, Reinforcement learning, Automation testing},
abstract = {Context:
User interface testing validates the correctness of an application through visual cues and interactive events emitted in real-world usages. Performing user interface tests is a time-consuming process, and thus, many studies have focused on prioritizing test cases to help maintain the effectiveness of testing while reducing the need for full execution.
Objective:
This paper describes a novel test prioritization method called RLTCP whose goal is to maximize the number of test faults detected while reducing the amount of test.
Methods:
We define a weighted coverage graph to model the underlying association among test cases for the user interface testing. Our method combines Reinforcement Learning (RL) and the coverage graph to prioritize test cases. While RL is found to be suitable for rapidly changing projects with abundant historical data, the coverage graph considers in-depth the event-based aspects of user interface testing and provides a fine-grained level at which the RL system can gain more insights into individual test cases.
Results:
We experiment and assess the proposed method using nine data sets obtained from two mature web applications, finding that the method outperforms the six, including the state-of-the-art, methods.
Conclusions:
The use of both reinforcement learning and the underlying structure of user interface tests modeled via the coverage has the potential to improve the performance of test prioritization methods. Our study also shows the benefit of using the coverage graph to gain insights into test cases, their relationship and execution history.}
}
@article{ALAGUMUTHUKRISHNAN20231112,
title = {Reliable and Efficient Lane Changing Behaviour for Connected Autonomous Vehicle through Deep Reinforcement Learning},
journal = {Procedia Computer Science},
volume = {218},
pages = {1112-1121},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.090},
url = {https://www.sciencedirect.com/science/article/pii/S187705092300090X},
author = {S Alagumuthukrishnan and S Deepajothi and Rajasekar Vani and S Velliangiri},
keywords = {CAV, Deep Reinforcement Learning, Lane changine behaviour, Information fusion, Safety decision},
abstract = {The establishment of future intelligent transport systems is dependable on the reliable and seamless function of Connected and Autonomous Vehicles (CAV). Reinforcement learning (RL), which allows autonomous vehicles (AVs) to learn an ideal driving strategy through constant contact with the environment, plays a significant part in the decision-making process of autonomous driving (AD). The networking of CAV is advantageous since it allows for the transmission of traffic-related data to vehicles via Vehicle-to-External (V2X) communication. Recognition and anticipation of driving behaviour are critical for avoiding collisions because they can provide useful information to other drivers and vehicles. The fundamental challenge in developing CAV is the construction of an autonomous controller that can effectively perform close real-time control selections, such as a fast acceleration while merging onto a highway and rapid speed adjustments in stop-and-go traffic congestion. CAV driving behaviours can be considerably improved by utilizing shared information, resulting in more accountable, intelligent, and efficient driving. In the present work, a deep reinforcement learning approach is proposed that integrates the information gathered through connectivity capabilities and sensing from neighbour automobiles in the vicinity of CAV. The fused information is used for providing safe and cooperative lane-changing behaviour. The deployment of an algorithm in CAV is expected to improve the transportation safety of CAV driving behaviours.}
}
@article{LI2022117380,
title = {A reinforcement learning based RMOEA/D for bi-objective fuzzy flexible job shop scheduling},
journal = {Expert Systems with Applications},
volume = {203},
pages = {117380},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117380},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422007291},
author = {Rui Li and Wenyin Gong and Chao Lu},
keywords = {Fuzzy flexible job shop scheduling, Multi-objective optimization, Parameter adaption, Reinforcement learning, MOEA/D},
abstract = {The flexible job shop scheduling problem (FJSP) is significant for realistic manufacturing. However, the job processing time usually is uncertain and changeable during manufacturing. This paper presents a multi-objective FJSP with fuzzy processing time (MOFFJSP) for optimizing the makespan and total machine workload as objectives. To solve the MOFFJSP, a MOEA/D based on reinforcement learning named RMOEA/D is proposed. RMOEA/D can be featured as: (i) an initial strategy with three rules is used to get a high-quality initial population; (ii) a parameter adaption strategy based on Q-learning is proposed to guide the population choose the best parameter to increase diversity; (iii) a variable neighborhood search based on reinforcement learning is designed to lead the solution to choose the right local search method; and (iv) an elite archive is used to improve the usage rate of the abandoned historical solution. RMOEA/D is compared with five well-known realted methods, i.e., MOEA/D, NSGA-II, MOEA/D-M2M, NSGA-III and IAIS on three benchmark suites. The results show that RMOEA/D outperforms these five state-of-art algorithms.}
}
@article{HILLEBRAND2020266,
title = {A design methodology for deep reinforcement learning in autonomous systems},
journal = {Procedia Manufacturing},
volume = {52},
pages = {266-271},
year = {2020},
note = {System-Integrated Intelligence – Intelligent, Flexible and Connected Systems in Products and ProductionProceedings of the 5th International Conference on System-Integrated Intelligence (SysInt 2020), Bremen, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.11.044},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321879},
author = {Michael Hillebrand and Mohsin Lakhani and Roman Dumitrescu},
keywords = {Model-Based Systems Engineering, Autonomous Systems, Deep Reinforcement Learning, Hybrid Testbed},
abstract = {Autonomous systems such as mobile robots will play an important role in fields like industrial production, transportation or in hostile environments such as space. One of the most fundamental problem in autonomous mobile robotics is autonomous navigation. It is imperative for a mobile robot to learn to navigate in complex environments such as roads or buildings. The most popular approach to this problem is to utilize different algorithms for mapping the environment, self-localization in the map, planning a trajectory to the given goal and executing this trajectory. However, there are some drawbacks of these approaches. We often make assumptions about the environment such as no dynamic or transparent objects. Moreover, there is considerable overhead, they do not learn from failures and operation scenarios. This prompts us to search for alternative approaches for autonomous navigation, such as deep reinforcement learning. However, the application of deep reinforcement learning to a particular task involves a series of non-trivial design decisions. Previous work have failed to address the need for a design methodology for deep reinforcement learning systems. In this paper, we propose design methodology and discuss relevant design decisions for deep reinforcement learning in autonomous systems. We apply the methodology to the problem of autonomous navigation.}
}
@article{SSENGONZI2022100142,
title = {A survey of deep reinforcement learning application in 5G and beyond network slicing and virtualization},
journal = {Array},
volume = {14},
pages = {100142},
year = {2022},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2022.100142},
url = {https://www.sciencedirect.com/science/article/pii/S2590005622000133},
author = {Charles Ssengonzi and Okuthe P. Kogeda and Thomas O. Olwal},
keywords = {Machine learning, Reinforcement learning, Deep reinforcement learning, 5G, Multi-domain network slicing, Orchestration, Admission control, Prediction},
abstract = {The 5th Generation (5G) and beyond networks are expected to offer huge throughputs, connect large number of devices, support low latency and large numbers of business services. To realize this vision, there is a need for a paradigm shift in the way cellular networks are designed, built, and maintained. Network slicing divides the physical network infrastructure into multiple virtual networks to support diverse business services, enterprise applications and use cases. Multiple services and use cases with varying architectures and quality of service requirements on such shared infrastructure complicates the network environment. Moreover, the dynamic and heterogeneous nature of 5G and beyond networks will exacerbate network management and operations complexity. Inspired by the successful application of machine learning tools in solving complex mobile network decision making problems, deep reinforcement learning (Deep RL) methods provide potential solutions to address slice lifecycle management and operation challenges in 5G and beyond networks. This paper aims to bridge the gap between Deep RL and the 5G network slicing research, by presenting a comprehensive survey of their existing research association. First, the basic concepts of Deep RL framework are presented. 5G network slicing and virtualization principles are then discussed. Thirdly, we review challenges in 5G network slicing and the current research efforts to incorporate Deep RL in addressing them. Lastly, we present open research problems and directions for future research.}
}
@article{SCHREIBER2021110856,
title = {Monitoring data-driven Reinforcement Learning controller training: A comparative study of different training strategies for a real-world energy system},
journal = {Energy and Buildings},
volume = {239},
pages = {110856},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.110856},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821001407},
author = {Thomas Schreiber and Christoph Netsch and Marc Baranski and Dirk Müller},
keywords = {Reinforcement Learning, Building automation and control, Thermal energy systems, Demand response, Machine learning, Data-driven modeling, Offline training},
abstract = {With increasing complexity of building energy systems and rising shares of renewable energies in the grids, the requirements for building automation and control systems (BACS) are increasing. The use of storage systems enables the decoupling of energy demand and supply and to consider dynamic constraints in the control of the systems. The resulting optimization problem is very challenging to solve with the state-of-the-art rule-based-control (RBC) approach. Model Predictive Control (MPC) on the other hand allows a nearly optimal operation but comes with expensive modeling efforts and high computational costs. These drawbacks are contrasted by promising results from the field of Reinforcement Learning (RL). RL can be model-free, is highly adaptive and learns a policy by interacting with the controlled system. However, the literature also addresses a number of questions, to be answered before RL for BACS can be realized. One is the slow convergence of the training process, which makes the application of a pre-training strategy necessary. Therefore, we design and compare different pre-training work-flows for a real-world energy system, in a demand response scenario. We apply a data-driven approach, covering all aspects from raw monitoring data to the trained algorithm. The considered energy system consists of two compression chillers and an ice storage. The objective of the control task is to charge and discharge the storage with respect to dynamic constraints. We use machine learning models of the energy system to train and evaluate a state-of-the-art RL algorithm (DQN) under five different pre-training strategies. We compare, online and offline training and initialization of the RL controller together with a guiding RBC. We demonstrate that offline training with a guiding RBC provides stable learning and a RL controller that always outperforms this guiding RBC. Unguided exploration on the other hand leads to higher accumulated cost savings. Based on our findings, we derive recommendations for practical application and future research questions.}
}
@article{ALSOLAMI2023102466,
title = {Peer-to-peer trading in smart grid with demand response and grid outage using deep reinforcement learning},
journal = {Ain Shams Engineering Journal},
pages = {102466},
year = {2023},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2023.102466},
url = {https://www.sciencedirect.com/science/article/pii/S2090447923003556},
author = {Mohammed Alsolami and Ahmad Alferidi and Badr Lami and Sami {Ben Slama}},
keywords = {Artificial intelligence, Deep reinforcement learning, Peer-to-peer energy trading, Smart community, Photovoltaic-array, Home energy management},
abstract = {With the price of green energy now more reasonable, users can now produce enough electricity to meet their needs and make a profit by selling the surplus on the underground P2P energy market. For households, energy trading and demand management can reduce electricity costs. However, consumers generally obtain market offers based on their expectations and the forecasts of other households. However, the P2P exchange system is not able to quantify the gap between these offers and the best market. The objective of this paper is to apply deep reinforcement learning techniques to optimal energy trading and demand response (DR) methods within a peer-to-peer (P2P) market. The main objective is to maximize cost reductions. The best approach to achieve this objective was investigated as part of this project. The complexity of domestic energy trading and energy recovery is formally characterized as a partially observable Markov decision process (POMDP). Through decentralized training and performance-based learning, the strategy maximizes policy and value functions. In order to identify the most effective proactive solutions, a comparative analysis is carried out between the two parties. Based on the simulation results, it was observed that implementing the recommended reinforcement learning strategy to optimize peer-to-peer (P2P) energy exchange can lead to a significant improvement in the average household reward. Specifically, the average household reward can be increased by 7.6% and 12.08% by employing the aforementioned approach.}
}
@article{GIBERT2022102543,
title = {Enhancing the insertion of NOP instructions to obfuscate malware via deep reinforcement learning},
journal = {Computers & Security},
volume = {113},
pages = {102543},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102543},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821003679},
author = {Daniel Gibert and Matt Fredrikson and Carles Mateu and Jordi Planes and Quan Le},
keywords = {Malware classification, Assembly language source code, Obfuscation, Reinforcement learning, Deep Q-Network},
abstract = {Current state-of-the-art research for tackling the problem of malware detection and classification is centered on the design, implementation and deployment of systems powered by machine learning because of its ability to generalize to never-before-seen malware families and polymorphic mutations. However, it has been shown that machine learning models, in partidular deep neural networks, lack robustness against crafted inputs (adversarial examples). In this work, we have investigated the vulnerability of a state-of-the-art shallow convolutional neural network malware classifier against the deat code insertion technique. We propose a general framework powered by a Double Q-network to induce misclassification over malware families. The framework trains an agent through a convolutional neural network to select the optimal positions in a code sequence to insert dead code instructions so that the machine learning classifier mislabels the resulting executable. The experiments show that the proposed method significantly drops the classification accuracy of the classifier to 56.53% while having an evasion rate of 100% for the samples belonging to Kelihos_ver3, Simda, and Kelihos_ver1 families. In addition, the average number of instructions needed to mislabel malware in comparison to a random agent decreased by 33%.}
}
@article{GAO2022110366,
title = {Resilient reinforcement learning and robust output regulation under denial-of-service attacks},
journal = {Automatica},
volume = {142},
pages = {110366},
year = {2022},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2022.110366},
url = {https://www.sciencedirect.com/science/article/pii/S0005109822002163},
author = {Weinan Gao and Chao Deng and Yi Jiang and Zhong-Ping Jiang},
keywords = {Reinforcement learning, Robust output regulation, Hybrid iteration, Denial-of-service attacks},
abstract = {In this paper, we have proposed a novel resilient reinforcement learning approach for solving robust optimal output regulation problems of a class of partially linear systems under both dynamic uncertainties and denial-of-service attacks. Fundamentally different from existing works on reinforcement learning, the proposed approach rigorously analyzes both the resilience of closed-loop systems against attacks and the robustness against dynamic uncertainties. Moreover, we have proposed an original successive approximation approach, named hybrid iteration, to learn the robust optimal control policy, that converges faster than value iteration, and is independent of an initial admissible controller. Simulation results demonstrate the efficacy of the proposed approach.}
}
@incollection{MOWBRAY20221039,
title = {Safe Chance Constrained Reinforcement Learning for Batch Process Optimization and Control},
editor = {Ludovic Montastruc and Stephane Negny},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {51},
pages = {1039-1044},
year = {2022},
booktitle = {32nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-95879-0.50174-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323958790501740},
author = {Max Mowbray and Panagiotis Petsagkourakis and Antonio Del Rio Chanona and Dongda Zhang},
keywords = {Safe Reinforcement Learning, Optimal Control, Dynamic Optimization, Bioprocess Operation, Machine Learning},
abstract = {Reinforcement Learning (RL) has received interest within the context of decision making under uncertainty in the process industries. The primary benefit of RL arises from the formulation of the control problem as a Markov decision process (MDP), meaning that it inherits the benefits of accounting for uncertainty in a closed loop feedback control framework and models dynamics very generally via conditional probability density functions. This enables RL to handle problems with various types of exogenous and endogenous uncertainties. Despite this there has been little reported uptake of RL in the process industries. This is partly due to the inability to provide optimality guarantees under the model used for learning, but more importantly due to safety concerns. This has led to the development of RL algorithms in the context of ‘Safe RL’. Here, we present an algorithm that leverages the variance prediction of Gaussian process state space models to a) handle operational constraints and b) account for mismatch between the offline process model and the real online process. The algorithm is then benchmarked on an uncertain Lutein photo-production process against nonlinear model predictive control (NMPC) and several state-of-the-art Safe RL algorithms. Through definition of key performance indicators, we demonstrate the efficacy of the method with respect to objective performance and probabilistic constraint satisfaction.}
}
@article{LI2018313,
title = {Training a robust reinforcement learning controller for the uncertain system based on policy gradient method},
journal = {Neurocomputing},
volume = {316},
pages = {313-321},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218309226},
author = {Zhan Li and Shengri Xue and Weiyang Lin and Mingsi Tong},
keywords = {Robust controller, Reinforcement learning, Policy gradient},
abstract = {The target of this paper is to design a model-free robust controller for uncertain systems. The uncertainties of the control system mainly consists of model uncertainty and external disturbance, which widely exist in the practical utilization. These uncertainties will negatively influence the system performance and this motivates us to train a model-free controller to solve this problem. Reinforcement learning is an important branch of machine learning and is able to achieve well performed control results by optimizing a policy without the knowledge of mathematical plant model. In this paper, we construct a reward function module to describe the specific environment of the concerned system, taking uncertainties into account. Then we utilize a new policy gradient method to optimize the policy and implement this algorithm with the actor-critic structure neuro networks. These two networks are our reinforcement learning controllers. Finally, we illustrate the applicability and efficiency of the proposed method by applying it on an experimental helicopter platform model, which includes model uncertainties and external disturbances.}
}
@article{ZHANG2021107622,
title = {A model-based reinforcement learning approach for maintenance optimization of degrading systems in a large state space},
journal = {Computers & Industrial Engineering},
volume = {161},
pages = {107622},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107622},
url = {https://www.sciencedirect.com/science/article/pii/S036083522100526X},
author = {Ping Zhang and Xiaoyan Zhu and Min Xie},
keywords = {Maintenance optimization, Periodic inspection, Model-based reinforcement learning, Degrading system},
abstract = {Scheduling maintenance tasks based on the deteriorating process has often been established on degradation models. However, the formulas of the degradation processes are usually unknown and hard to be determined for a system working in practices. In this study, we develop a model-based reinforcement learning approach for maintenance optimization. The developed approach determines maintenance actions for each degradation state at each inspection time over a finite planning horizon, supposing that the degradation formula is known or unknown. At each inspection time, the developed approach attempts to learn an optimal assessment value for each maintenance action to be performed at each degradation state. The assessment value quantifies the goodness of each state-action pair in terms of minimizing the accumulated maintenance costs over the planning horizon. To optimize the assessment values when a well-defined degradation formula is known, we customize a Q-learning method with model-based acceleration. When the degradation formula is unknown or hard to be determined, we develop a Dyna-Q method with maintenance-oriented improvements, in which an environment model capturing the degradation pattern under different maintenance actions is learned at first; Then, the assessment values are optimized while considering the stochastic behavior of the system degradation. The final maintenance policy is acquired by performing the maintenance actions associated with the highest assessment values. Experimental studies are presented to illustrate the applications.}
}
@article{ZU2023419,
title = {A reinforcement learning algorithm acquires demonstration from the training agent by dividing the task space},
journal = {Neural Networks},
volume = {164},
pages = {419-427},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.04.042},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023002289},
author = {Lipeng Zu and Xiao He and Jia Yang and Lianqing Liu and Wenxue Wang},
keywords = {Reinforcement learning, Self-imitation learning, Task space division, Sparse reward function, Robotic grasping},
abstract = {Although reinforcement learning (RL) has made numerous breakthroughs in recent years, addressing reward-sparse environments remains challenging and requires further exploration. Many studies improve the performance of the agents by introducing the state-action pairs experienced by an expert. However, such kinds of strategies almost depend on the quality of the demonstration by the expert, which is rarely optimal in a real-world environment, and struggle with learning from sub-optimal demonstrations. In this paper, a self-imitation learning algorithm based on the task space division is proposed to realize an efficient high-quality demonstration acquire while the training process. To determine the quality of the trajectory, some well-designed criteria are defined in the task space for finding a better demonstration. The results show that the proposed algorithm will improve the success rate of robot control and achieve a high mean Q value per step. The algorithm framework proposed in this paper has illustrated a great potential to learn from a demonstration generated by using self-policy in sparse environments and can be used in reward-sparse environments where the task space can be divided.}
}
@article{HAO2021101470,
title = {URLLC resource slicing and scheduling for trustworthy 6G vehicular services: A federated reinforcement learning approach},
journal = {Physical Communication},
volume = {49},
pages = {101470},
year = {2021},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2021.101470},
url = {https://www.sciencedirect.com/science/article/pii/S187449072100207X},
author = {Min Hao and Dongdong Ye and Siming Wang and Beihai Tan and Rong Yu},
keywords = {6G, Vehicular edge computing, Resource slicing, Federated learning, Zero trust architecture},
abstract = {In the upcoming 6G era, vehicles will massively connect to the wireless network through edge access points such as roadside units (RSUs). The increasing number of connected vehicles and vehicular services will take 6G vehicular network to a new challenging security boundary, i.e., the so-called zero trust network. The traditional resource slicing and scheduling solution has to evolve to deal with the zero trust security problems. In this paper, we consider the trustworthy 6G vehicular services and focus on the typical scenario of vehicular task offloading with resource slicing and scheduling. In order to prevent vehicles from malicious attacks by untrusted edge access points, we exploit a subjective logic model to score the reputation of edge nodes. The vehicles will select the edge nodes with high reputation for task offloading. After that, we develop an federated asynchronous reinforcement learning algorithm to optimize the offloading problem. The simulation results show that our approach can efficiently schedule the slice resources and effectively protect the information security of vehicles.}
}
@article{YU202344,
title = {Hybrid attention-oriented experience replay for deep reinforcement learning and its application to a multi-robot cooperative hunting problem},
journal = {Neurocomputing},
volume = {523},
pages = {44-57},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S092523122201517X},
author = {Lingli Yu and Shuxin Huo and Zhengjiu Wang and Keyi Li},
keywords = {Deep reinforcement learning, Multi-robot, MADDPG, Prioritized experience replay, Attention mechanism},
abstract = {Multiple robots complete a cooperative hunting task by obtaining environmental information and autonomously learning hunting decision-making strategies. However, with the increase in the number of environment participants, it becomes difficult for robots to process a large amount of environmental information. Thus, a multi-robot cooperative hunting decision-making method called hybrid attention-oriented experience replay in multi-agent deep deterministic policy gradient (HAER-MADDPG) is proposed. First, a hybrid attention module is designed to pay greater attention to key information in a large amount of environmental information by integrating it with the multi-agent deep deterministic policy gradient (MADDPG). The method then combines hybrid attention and prioritized experience replay to improve the utilization of experience samples. Finally, the proposed algorithm is tested through a predator–prey game. The results show that the effectiveness, convergence speed, and scalability of the proposed algorithm are better than those of the baseline algorithms. In addition, HAER-MADDPG is effectively applied to a hunting task with real robots.}
}
@article{WU2022108433,
title = {Reinforcement learning approach to the control of heavy material handling manipulators for agricultural robots},
journal = {Computers and Electrical Engineering},
volume = {104},
pages = {108433},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108433},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622006425},
author = {Xiaoming Wu and Jing Chi and Xiao-Zheng Jin and Chao Deng},
keywords = {Agricultural robots, Policy iteration algorithm, Value iteration algorithm, Optimal control},
abstract = {In this paper, we consider the optimal control problem of heavy material handling manipulators for agricultural robots. Unlike the existing results on agricultural robots, the robot parameters may be unknown for the designer in this paper. To learn the linear quadratic control gain under unknown robot parameters, two reinforcement learning algorithms, i.e., policy iteration (PI) algorithm and value iteration (VI) algorithm, are proposed. Then, through combining the advantages of PI algorithm and VI algorithm, i.e., satisfactory convergence rate and without the restriction on feasibility initial control policy, respectively, a hybrid iteration (HI) algorithm is proposed, which can both achieve a satisfactory convergence rate and remove restrictions on feasibility initial control policy. It is shown that the convergence of the proposed HI algorithm can be achieved in theory. Finally, a simulation example is given to show that our designed HI algorithm can achieve a satisfactory simulation time.}
}
@article{HUANG2024102644,
title = {A novel robotic grasping method for moving objects based on multi-agent deep reinforcement learning},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {86},
pages = {102644},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102644},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523001199},
author = {Yu Huang and Daxin Liu and Zhenyu Liu and Ke Wang and Qide Wang and Jianrong Tan},
keywords = {Robotic grasping, Moving object, Reinforcement learning, Efficient sample, Multi-agent mechanism},
abstract = {To grasp the randomly moving objects in unstructured environment, a novel robotic grasping method based on multi-agent TD3 with high-quality memory (MA-TD3H) is proposed. During the grasping process, the MA-TD3H algorithm obtains the object's motion state from the vision detection module and outputs the velocity of the gripper. The quality of the sampled memory plays a crucial role in reinforcement learning models. In MA-TD3H, transitions are saved in the memory buffer and high-quality memory (H-memory) buffer respectively. When updating the actor network, transitions are adaptively sampled from the two buffers by a set ratio according to the current grasping success rate of the algorithm. Also, the multi-agent mechanism enables the MA-TD3H algorithm to control multiple agents for simultaneous training and experience sharing. In the simulation, MA-TD3H improves the success rate of grasping the moving object by around 25 percent, compared with TD3, DDPG and SAC. While in most cases, MA-TD3H spends 80 percent of the time of the other algorithms. In real-world experiments on grasping objects in different shapes and trajectories, the average grasping prediction success rate (GPSR) and grasping reaching success rate (GRSR) of MA-TD3H are above 90 percent and 80 percent respectively, and the average GRSR is improved by 20–30 percent compared with the other algorithms. In summary, simulated and real-world experiments validate that the MA-TD3H algorithm outperforms the other algorithms in robotic grasping for moving objects.}
}
@article{AUMJAUD2021100061,
title = {rl_reach: Reproducible reinforcement learning experiments for robotic reaching tasks},
journal = {Software Impacts},
volume = {8},
pages = {100061},
year = {2021},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2021.100061},
url = {https://www.sciencedirect.com/science/article/pii/S2665963821000099},
author = {Pierre Aumjaud and David McAuliffe and Francisco J. {Rodríguez Lera} and Philip Cardiff},
keywords = {Reinforcement learning, Experiments, Robotics, Artificial intelligence},
abstract = {Training reinforcement learning agents at solving a given task is highly dependent on identifying optimal sets of hyperparameters and selecting suitable environment input/output configurations. This tedious process could be eased with a straightforward toolbox allowing its user to quickly compare different training parameter sets. We present rl_reach, a self-contained, open-source and easy-to-use software package designed to run reproducible reinforcement learning experiments for customisable robotic reaching tasks. rl_reach packs together training environments, agents, hyperparameter optimisation tools and policy evaluation scripts, allowing its users to quickly investigate and identify optimal training configurations. rl_reach is publicly available at this URL: https://github.com/PierreExeter/rl_reach.}
}
@article{YANG2022109346,
title = {Online beam orbit correction of MEBT in CiADS based on multi-agent reinforcement learning algorithm},
journal = {Annals of Nuclear Energy},
volume = {179},
pages = {109346},
year = {2022},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2022.109346},
url = {https://www.sciencedirect.com/science/article/pii/S0306454922003814},
author = {Xuhui Yang and Youxin Chen and Jinqiang Wang and Hai Zheng and Haitao Liu and Detai Zhou and Yuan He and Zhijun Wang and Qingguo Zhou},
keywords = {Beam offset automatic correction, MEBT, Deep reinforcement learning, DDPG},
abstract = {The proton linac accelerator has been the mostly appropriate equipment applied to radioactive waste disposal and cancer treatment. However, beam offset is a particularly significant problem during proton acceleration, which not only produces a halo and causes orbital distortion of the beam to make the beam injection inefficient. In exceptionally complex accelerator systems, accurate correction of beam offset is very difficult. The traditional method is based on manual experience and manual adjustment to achieve beam trajectory correction, the automation level is low and the adjustment speed is slow. Therefore, this paper proposes a new method of beam orbit correction based on Multi-Agent Reinforcement Learning. In order to verify the feasibility of the algorithm in beam orbit correction, this method takes the MEBT section of the virtual accelerator as the control object, and uses three agents to control the beam orbit of Medium Energy Beam Transport (MEBT) in sections. Each agent is designed based on the Reinforcement Learning of the Deep Deterministic Policy Gradient (DDPG) algorithm, the deterministic strategy is used to explore the large state and action space to realize discretization, and the training model convergence is successfully realized. The experimental results show that the final calibrated average beam offset of this method is about 0.39 mm, which achieves the expected experimental goal. It can control the beam orbit of MEBT, and has application value and competitiveness in the beam orbit correction of linac.}
}
@article{BLAD2023120807,
title = {A laboratory test of an Offline-trained Multi-Agent Reinforcement Learning Algorithm for Heating Systems},
journal = {Applied Energy},
volume = {337},
pages = {120807},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.120807},
url = {https://www.sciencedirect.com/science/article/pii/S030626192300171X},
author = {C. Blad and S. Bøgh and C. Kallesøe and Paul Raftery},
abstract = {This paper presents a laboratory study of Offline-trained Reinforcement Learning (RL) control of a Heating Ventilation and Air-Conditioning (HVAC) system. We conducted the experiments on a radiant floor heating system consisting of two temperature zones located in Denmark. The buildings are subjected to real-world weather. A previous paper describes the algorithm we tested, which we summarize in this paper. First, we present a benchmarking test which we conducted during spring 2021 and winter 2021/2022. This data is used in the Offline RL framework to train and deploy the RL policy, which we then tested during winter 2021/2022 and spring 2022. An analysis of the data shows that the RL policy showed predictive control-like behavior, and reduced the oscillations of the system by a minimum of 40%. Additionally, we show that the RL policy is minimum 14% more cost-effective than the traditional control policy used in the benchmarking test.}
}
@article{GIRALDOPEREZ2023101174,
title = {A reinforcement learning based energy optimization approach for household fridges},
journal = {Sustainable Energy, Grids and Networks},
volume = {36},
pages = {101174},
year = {2023},
issn = {2352-4677},
doi = {https://doi.org/10.1016/j.segan.2023.101174},
url = {https://www.sciencedirect.com/science/article/pii/S2352467723001820},
author = {Juan Pablo Giraldo-Pérez and Ricardo Mejía-Gutiérrez and Jose Aguilar},
keywords = {Reinforcement learning, Renewable energy sources, Fridge control, Energy saving, Artificial Intelligence},
abstract = {The use of machine learning algorithms for the control of schedulable loads like Heating, ventilation, and air conditioning (HVAC), illumination, dryers and irrigation systems to optimize the use of RES and increase energy saving has obtained remarkable results in the last years. However, in the residential sector of tropical countries where HVAC systems are not necessary, these loads represent only a small percentage of the total energy consumption. In order to achieve a significant impact on energy savings and promote the use of RES, other residential loads must be taken into account in tropical countries. In the case of Colombia, for example, fridges account for 24% of residential energy consumption. This research proposes the use of RL for the development of a fridge energy management system capable of minimizing energy consumption and optimizing the use of RES for cooling. The fridge energy management system is based on an RL agent to control the fridge, and an artificial neural network to model the environment and assess the impact of its actions. Compared to the original fridge control, the RL-based control successfully reduced the total energy usage by 23% while also increasing the use of RES energy.}
}
@article{PITOMBEIRANETO2022100027,
title = {A reinforcement learning approach to the stochastic cutting stock problem},
journal = {EURO Journal on Computational Optimization},
volume = {10},
pages = {100027},
year = {2022},
issn = {2192-4406},
doi = {https://doi.org/10.1016/j.ejco.2022.100027},
url = {https://www.sciencedirect.com/science/article/pii/S219244062200003X},
author = {Anselmo R. Pitombeira-Neto and Arthur H.F. Murta},
keywords = {Cutting stock problem, Reinforcement learning, Approximate dynamic programming},
abstract = {We propose a formulation of the stochastic cutting stock problem as a discounted infinite-horizon Markov decision process. At each decision epoch, given current inventory of items, an agent chooses in which patterns to cut objects in stock in anticipation of the unknown demand. An optimal solution corresponds to a policy that associates each state with a decision and minimizes the expected total cost. Since exact algorithms scale exponentially with the state-space dimension, we develop a heuristic solution approach based on reinforcement learning. We propose an approximate policy iteration algorithm in which we apply a linear model to approximate the action-value function of a policy. Policy evaluation is performed by solving the projected Bellman equation from a sample of state transitions, decisions and costs obtained by simulation. Due to the large decision space, policy improvement is performed via the cross-entropy method. Computational experiments are carried out with the use of realistic data to illustrate the application of the algorithm. Heuristic policies obtained with polynomial and Fourier basis functions are compared with myopic and random policies. Results indicate the possibility of obtaining policies capable of adequately controlling inventories with an average cost up to 80% lower than the cost obtained by a myopic policy.}
}
@article{LENNARTSON2020485,
title = {Reinforcement Learning with Temporal Logic Constraints⁎⁎This work was supported by The Swedish Foundation for Strategic Research, through the Smart Assembly 4.0 project, within the Winquist Laboratory; SyTec - Systematic Testing of Cyber-Physical Systems, a Swedish Science Foundation grant for strong research environment; Wallenberg Artificial Intelligence, Autonomous Systems and Software Program (WASP) funded by Knut and Alice Wallenberg Foundation; NSFC 61673229 and the 111 International Collaboration Project of China (No. BP2018006). The support is gratefully acknowledged.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {4},
pages = {485-492},
year = {2020},
note = {15th IFAC Workshop on Discrete Event Systems WODES 2020 — Rio de Janeiro, Brazil, 11-13 November 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.04.044},
url = {https://www.sciencedirect.com/science/article/pii/S240589632100094X},
author = {Bengt Lennartson and Qing-Shan Jia},
keywords = {reinforcement learning, adaption, temporal logic specifications, modular systems},
abstract = {Reinforcement learning (RL) is an agent based AI learning method, where learning and optimization are combined. Dynamic programming is then performed iteratively, based on reward and next state observations from the system to be controlled. A brief survey of RL is given, followed by an evaluation of a recently proposed method to include temporal logic safety and liveness guarantees in RL, here combined with classical performance optimization. RL is based on Markov decision processes (MDPs), and to reduce the number of observations from the system, a modular MDP framework is proposed. In the learning process, it is then assumed that some parts of the system are represented by known MDP models, while other parts can be estimated by observations from the real system. Local information from the modular system may then be used to reduce the computational complexity, especially in the handling of safety properties.}
}
@article{LI2022300,
title = {SADRL: Merging human experience with machine intelligence via supervised assisted deep reinforcement learning},
journal = {Neurocomputing},
volume = {467},
pages = {300-309},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.09.064},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221014429},
author = {Xiaoshuang Li and Xiao Wang and Xinhu Zheng and Junchen Jin and Yanhao Huang and Jun Jason Zhang and Fei-Yue Wang},
keywords = {Deep reinforcement learning, Behavioral cloning, Dynamic demonstration, Double DQN},
abstract = {Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies in decision-making problems by directly interacting with environments. Meanwhile, supervised learning methods also show great capability of learning from data. However, how to combine DRL with supervised learning and leverage additional knowledge and data to assist the DRL agent remains difficult. This study proposes a novel Supervised Assisted Deep Reinforcement Learning (SADRL) framework integrating deep Q-learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC). Specifically, the proposed DQfDD-BC method leverages historical demonstrations to pre-train a behavioral cloning model and consistently update it by learning the dynamically updated demonstrations. A supervised expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach accelerates the learning processes, and meanwhile, adapts to different performance levels of demonstrations. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms using a BC model contribute to improving the learning convergence performance compared with the baseline models. We believe that SADRL provides an elegant framework and the proposed method can promote the integration of human experience and machine intelligence.}
}
@article{WANG2022109717,
title = {Multi-objective reinforcement learning framework for dynamic flexible job shop scheduling problem with uncertain events},
journal = {Applied Soft Computing},
volume = {131},
pages = {109717},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109717},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622007669},
author = {Hao Wang and Junfu Cheng and Chang Liu and Yuanyuan Zhang and Shunfang Hu and Liangyin Chen},
keywords = {Dynamic multi-objective flexible job shop scheduling problem, Real-time processing framework, Deep reinforcement learning, Local search algorithm},
abstract = {The economic benefits for manufacturing companies will be influenced by how it handles potential dynamic events and performs multi-objective real-time scheduling for existing dynamic events. Based on these, we propose a new dynamic multi-objective flexible job shop scheduling problem (DMFJSP) to simulate realistic production environment. Six dynamic events are involved in the problem including job insertion, job cancellation, job operation modification, machine addition, machine tool replacement and machine breakdown. As well as three objectives of longest job processing time (makespan), average machine utilization and average job processing delay rate with a set of constraints are also raised in the study. Then this research designs a novel dynamic multi-objective scheduling algorithm based on deep reinforcement learning. The algorithm uses two deep Q-learning networks and a real-time processing framework to process each dynamic event and generate complete scheduling scheme. In addition, an improved local search algorithm is adopted to further optimize the scheduling results and the idea of combination is used to make the scheduling rules more comprehensive. Experiments on 27 instances show the superiority and stability of our approach compared to each proposed combined rule, well-known scheduling rules and standard deep Q-learning based algorithms. Compared to the current optimal deep Q-learning method, the maximum performance improvement for our three objectives are approximately 57%, 164% and 28%.}
}
@article{LIU2023102131,
title = {Automated clash resolution for reinforcement steel design in precast concrete wall panels via generative adversarial network and reinforcement learning},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102131},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102131},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002598},
author = {Pengkun Liu and Hongtuo Qi and Jiepeng Liu and Liang Feng and Dongsheng Li and Jingjing Guo},
keywords = {Rebar Design, Precast Concrete Elements, Clash Resolution, Building Information Model, Deep Reinforcement Learning, Generative Adversarial Network},
abstract = {The increasing adoption of precast concrete elements (PCEs) is evident in civil infrastructure projects. The structural integrity of prefabricated structures relies on the quality of connections between adjacent components, necessitating the design of reinforcing steel bars (rebars) in PCEs. Dealing with PCEsand the complex design codes for rebars' arrangement can be labor-intensive, impractical, and error-prone when using traditional computer software due to their irregular shapes. This often results in frequent rebar clashes on construction sites. On the other hand, Building Information Modeling (BIM) technology, which is widely employed for structural design in the industry, offers potential solutions to these challenges. Several studies have proposed clash resolution strategies for moving components using optimization algorithms; these strategies are only suitable for regular reinforced concrete (RC) structures. The optimized path of rebars cannot be adjusted to avoid obstacles in PCEs. Due to strict design codes and large dimensions, existing studies do not possess the learning knowledge from design codes or drawings needed to generate clash-free rebar arrangements for real-world PCEs automatically. To overcome this limitation, we present a BIM-based framework that utilizes Generative Adversarial Network (GAN) and Deep Reinforcement Learning (DRL) to automatically generate clash-free rebar designs in prefabricated concrete wall panels (PCWPs). Our method employs GAN to learn from designers’ experiences from existing design drawings and generate 2D preliminary rebar designs, which are then transformed into mesh environments for DRL. The DRL engine incorporates state, action, and rewards designed according to buildability constraints and design codes. We conduct extensive experiments on real-world PCWPs to assess the efficacy of the proposed method. The results indicate that our framework can reduce engineering time for rebar designs by up to 80% compared to manual design, while achieving clash-free designs that adhere to design codes.}
}
@incollection{MOWBRAY2022445,
title = {Safe Chance Constrained Reinforcement Learning for Batch Process Optimization and Control},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {445-450},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50074-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596500749},
author = {Max Mowbray and Panogiotis Petsagkourakis and Antonio Del Rio Chanona and Dongda Zhang},
keywords = {Safe Reinforcement Learning, Optimal Control, Dynamic Optimization, Bioprocess Operation, Machine Learning},
abstract = {Reinforcement Learning (RL) has generated excitement within the process industries within the context of decision making under uncertainty. The primary benefit of RL is that it provides a flexible and general approach to handling systems subject to both exogenous and endogenous uncertainties. Despite this there has been little reported uptake of RL in the process industries. This is partly due to the inability to provide optimality guarantees under the model used for learning, but more importantly due to safety concerns. This has led to the development of RL algorithms in the context of ‘Safe RL’. In this work, we present an algorithm that leverages the variance prediction of Gaussian process state space models to a) handle operational constraints and b) account for mismatch between the offline process model and the real online process. The algorithm is then benchmarked on an uncertain Lutein photo-production process against nonlinear model predictive control (NMPC) and several state-of-the-art Safe RL algorithms. Through the definition of key performance indicators, we quantitatively demonstrate the efficacy of the method with respect to objective performance and probabilistic constraint satisfaction.}
}
@article{ZHANG2023105689,
title = {Deep reinforcement learning with domain randomization for overhead crane control with payload mass variations},
journal = {Control Engineering Practice},
volume = {141},
pages = {105689},
year = {2023},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2023.105689},
url = {https://www.sciencedirect.com/science/article/pii/S0967066123002587},
author = {Jianfeng Zhang and Chunhui Zhao and Jinliang Ding},
keywords = {Overhead cranes, Deep reinforcement learning, Domain randomization, Memory-augmented policy, Payload mass variations},
abstract = {Overhead cranes, as an important tool for loading and transporting, play an important role in modern industry. A key challenge in overhead crane control is payload mass variation: a policy learned to solve the overhead crane control in the fixed payload scenario often fails to solve the control task in the payload variation scenario. Therefore, from a practical perspective, this paper designs a novel deep reinforcement learning (DRL) control algorithm, domain randomization memory-augmented Beta proximal policy optimization (DR-MABPPO), which leverages the memory-augmented policy and incorporates the domain randomization (DR) training strategy to address the control problem of the overhead crane with payload masses variations. With the help of the DR training strategy and the memory-augmented policy, DR-MABPPO can learn a universal policy that is robust to the wide range of payload mass variations. As far as we know, this is the first time that the DRL technique is applied to solve the overhead crane control with payload mass variations. Simulation studies are conducted to demonstrate the effectiveness of the proposed method in the presence of payload mass variations, exhibiting satisfactory control performance when compared to PID and LQR.}
}
@article{QIN201889,
title = {An intelligent non-optimality self-recovery method based on reinforcement learning with small data in big data era},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {176},
pages = {89-100},
year = {2018},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2018.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169743918300662},
author = {Yan Qin and Chunhui Zhao and Furong Gao},
keywords = {Intelligent non-optimality self-recovery, Non-optimal diagnosis, Reinforcement learning, Actor-Critic structure, Big data era},
abstract = {Batch processes have attracted extensive attention as a crucial manufacturing way in modern industries. Although they are well equipped with control devices, batch processes may operate at a non-optimal status because of process disturbances, equipment aging, feedstock variations, etc. As a result, the quality indices or economic benefits may be undesirable using the pre-defined normal operation conditions. And this phenomenon is called non-optimality here. Therefore, it is indispensable to timely remedy the process to its optimal status without accurate models or amounts of data. To solve this problem, this study proposes an intelligent non-optimality self-recovery method based on reinforcement learning. First, the causal variables that lead to the non-optimality are identified by developing a status-degraded Fisher discriminant analysis with consideration of sparsity. Second, on the basis of self-learning mechanism, an intelligent self-recovery method is proposed using the reinforcement learning to automatically adjust the set-points of the causal controlled variables. The self-recovery action is taken iteratively through the Actor-Critic structure with two neural networks. In this way, effective actions are taken to remedy the process to its expected status which only require small data. Finally, the efficacy of the proposed method is illustrated by both numerical case and a typical batch-type manufacturing process, i.e., the injection molding process.}
}
@article{DUAN2023104956,
title = {Robot morphology evolution for automated HVAC system inspections using graph heuristic search and reinforcement learning},
journal = {Automation in Construction},
volume = {153},
pages = {104956},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104956},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523002169},
author = {Kangkang Duan and Christine Wun Ki Suen and Zhengbo Zou},
keywords = {Evolutionary robotics, Graph grammar, HVAC inspection, Reinforcement learning, Robot design},
abstract = {The building sector consumes more than 70% of the electricity produced in the U.S., with Heating Ventilation and Air Conditioning (HVAC) systems accounting for half of the electricity consumption. Leaks in HVAC systems have a significant impact on the energy efficiency of buildings, resulting in up to 33% of energy loss. However, the current manual approach to inspection is time-consuming and reactive, leaving room for automation. This paper presents a framework to automatically evolve robot morphology without requiring human intervention to suite any given HVAC and ceiling design. Robot morphologies are optimized using graph heuristic search based on tasks and environment designs, followed by testing of navigation abilities of the best-evolved robot in diverse ceiling environments using reinforcement learning. Tests conducted in robot simulation tools, utilizing realistic HVAC designs retrieved from Building Information Models (BIMs), demonstrate that effortless navigation in complex ceiling environments can be achieved by the evolved robots.}
}
@article{SELLAMI2022108957,
title = {Energy-aware task scheduling and offloading using deep reinforcement learning in SDN-enabled IoT network},
journal = {Computer Networks},
volume = {210},
pages = {108957},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.108957},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622001359},
author = {Bassem Sellami and Akram Hakiri and Sadok Ben Yahia and Pascal Berthou},
keywords = {Task scheduling, Deep reinforcing learning SDN, Fog computing, Internet of Things},
abstract = {The fifth-generation (5G) mobile network services have made tremendous growth in the Internet of Things (IoT) network. A counters number of battery-powered IoT devices are deployed to serve diverse scenarios, e.g., smart cities, autonomous farming, smart manufacturing, to name but a few. In this context, energy consumption became one of the most critical concerns in interconnecting smart IoT devices in such scenarios. Additionally, whenever these IoT devices are distributed in space and time-evolving, they are expected to deliver high volume data scalably/predictably while minimizing end-to-end latency. Furthermore, edge IoT nodes often face the biggest hurdle of performing optimal resource distribution and achieving high-performance levels while coping with the variability of task handling, energy conservation, and ultra-reliable low-latency. This paper investigates an energy-aware and low-latency oriented computing task scheduling problem in a Software-Defined Fog-IoT Network. First, we formulate the online task assignment and scheduling problem as an energy-constrained Deep Q-Learning process as a kickoff. The latter strives to minimize the network latency while ensuring energy efficiency by saving battery power under the constraints of application dependence. Then, given the task arrival process, we introduce a deep reinforcement learning (DRL) approach for dynamic task scheduling and assignment in the Software-Defined Networking (SDN)-enabled edge networks. We conducted comprehensive experiments and compared the introduced algorithm to three pioneering deep learning algorithms (i.e., deterministic, random, and A3C agents). Extensive simulation results demonstrated that our proposed solution outperforms these algorithms. Additionally, we highlight the characterizing feature of our design, energy-awareness, as it offers better energy-saving by up to 87% compared against the other approaches. We have shown that the offloading scheme could perform more task assignments with the available battery power by up to 50% less time delay. Our results back our claims that the solution we propose can readily be used to dynamically optimize task scheduling and assignment of complex jobs with task dependencies in distributed Fog IoT networks.}
}
@article{SINGH2020667,
title = {Reinforcement learning based control of batch polymerisation processes},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {1},
pages = {667-672},
year = {2020},
note = {6th Conference on Advances in Control and Optimization of Dynamical Systems ACODS 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.06.111},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301300},
author = {Vikas Singh and Hariprasad Kodamana},
keywords = {Reinforcement learning, Batch process, Risk sensitivity, Polymerisation process},
abstract = {Control of batch polymerization has been a challenging task. In this work, we have tried to use Reinforcement Learning (RL), and Deep Reinforcement Learning (DRL) based control on addressing the existing challenges. RL is a class of machine learning wherein an agent directly interacts with the environment and learns from its experience. The RL consist of an agent who takes an action, and the action changes the state of the environment. Based on old and new state agent gets a reward, which is reinforcement for its future actions. In this work, we have implemented RL and DRL based control for batch polymerization of Polymethyl methacrylate (PMMA). In both the controllers, the input variable considered was jacket temperature, while the reactor temperature was the output variable. Both the controllers have been found to achieve the given setpoint, while the DRL controller being faster than the RL controllers. Further, RL and DRL control with risk sensitivity were also carried out to accommodate the process constraints.}
}