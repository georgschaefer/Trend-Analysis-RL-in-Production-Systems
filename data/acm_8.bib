@inproceedings{10.1145/3523227.3551485,
author = {Grishanov, Alexey and Ianina, Anastasia and Vorontsov, Konstantin},
title = {Multiobjective Evaluation of Reinforcement Learning Based Recommender Systems},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523227.3551485},
doi = {10.1145/3523227.3551485},
abstract = {Movielens dataset has become a default choice for recommender systems evaluation. In this paper we analyze the best strategies of a Reinforcement Learning agent on Movielens (1M) dataset studying the balance between precision and diversity of recommendations. We found that trivial strategies are able to maximize ranking quality criteria, but useless for users of the recommendation system due to the lack of diversity in final predictions. Our proposed method stimulates the agent to explore the environment using the stochasticity of Ornstein-Uhlenbeck processes. Experiments show that optimization of the Ornstein-Uhlenbeck process drift coefficient improves the diversity of recommendations while maintaining high nDCG and HR criteria. To the best of our knowledge, the analysis of agent strategies in recommendation environments has not been studied excessively in previous works.},
booktitle = {Proceedings of the 16th ACM Conference on Recommender Systems},
pages = {622–627},
numpages = {6},
keywords = {Ornstein-Uhlenbeck processes, noise injection, Deep Reinforcement Learning, Recommendation Systems, Deep Deterministic Policy Gradient (DDPG)},
location = {Seattle, WA, USA},
series = {RecSys '22}
}

@article{10.5555/2789272.2886793,
author = {Sunehag, Peter and Hutter, Marcus},
title = {Rationality, Optimism and Guarantees in General Reinforcement Learning},
year = {2015},
issue_date = {January 2015},
publisher = {JMLR.org},
volume = {16},
number = {1},
issn = {1532-4435},
abstract = {In this article, we present a top-down theoretical study of general reinforcement learning agents. We begin with rational agents with unlimited resources and then move to a setting where an agent can only maintain a limited number of hypotheses and optimizes plans over a horizon much shorter than what the agent designer actually wants. We axiomatize what is rational in such a setting in a manner that enables optimism, which is important to achieve systematic explorative behavior. Then, within the class of agents deemed rational, we achieve convergence and finite-error bounds. Such results are desirable since they imply that the agent learns well from its experiences, but the bounds do not directly guarantee good performance and can be achieved by agents doing things one should obviously not. Good performance cannot in fact be guaranteed for any agent in fully general settings. Our approach is to design agents that learn well from experience and act rationally. We introduce a framework for general reinforcement learning agents based on rationality axioms for a decision function and an hypothesis-generating function designed so as to achieve guarantees on the number errors. We will consistently use an optimistic decision function but the hypothesis-generating function needs to change depending on what is known/assumed. We investigate a number of natural situations having either a frequentist or Bayesian flavor, deterministic or stochastic environments and either finite or countable hypothesis class. Further, to achieve sufficiently good bounds as to hold promise for practical success we introduce a notion of a class of environments being generated by a set of laws. None of the above has previously been done for fully general reinforcement learning environments.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1345–1390},
numpages = {46},
keywords = {optimality, optimism, rationality, reinforcement learning, error bounds}
}

@inproceedings{10.5555/2343576.2343644,
author = {Knox, W. Bradley and Stone, Peter},
title = {Reinforcement Learning from Simultaneous Human and MDP Reward},
year = {2012},
isbn = {0981738117},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {As computational agents are increasingly used beyond research labs, their success will depend on their ability to learn new skills and adapt to their dynamic, complex environments. If human users---without programming skills---can transfer their task knowledge to agents, learning can accelerate dramatically, reducing costly trials. The tamer framework guides the design of agents whose behavior can be shaped through signals of approval and disapproval, a natural form of human feedback. More recently, tamer+rl was introduced to enable human feedback to augment a traditional reinforcement learning (RL) agent that learns from a Markov decision process's (MDP) reward signal. We address limitations of prior work on tamer and tamer+rl, contributing in two critical directions. First, the four successful techniques for combining human reward with RL from prior tamer+rl work are tested on a second task, and these techniques' sensitivities to parameter changes are analyzed. Together, these examinations yield more general and prescriptive conclusions to guide others who wish to incorporate human knowledge into an RL algorithm. Second, tamer+rl has thus far been limited to a sequential setting, in which training occurs before learning from MDP reward. In this paper, we introduce a novel algorithm that shares the same spirit as tamer+rl but learns simultaneously from both reward sources, enabling the human feedback to come at any time during the reinforcement learning process. We call this algorithm simultaneous tamer+rl. To enable simultaneous learning, we introduce a new technique that appropriately determines the magnitude of the human model's influence on the RL algorithm throughout time and state-action space.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {475–482},
numpages = {8},
keywords = {interactive learning, human-agent interaction, shaping, human teachers, reinforcement learning},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@inproceedings{10.1145/3503181.3503203,
author = {Liu, Yong and Shen, Zhiqi and Zhang, Yinan and Cui, Lizhen},
title = {Diversity-Promoting Deep Reinforcement Learning for Interactive Recommendation},
year = {2022},
isbn = {9781450395540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503181.3503203},
doi = {10.1145/3503181.3503203},
abstract = {The interactive recommendation systems have recently attracted lots of research attentions, because they can model the interactions between the user and the recommender system. Most previous interactive recommendation methods only focus on optimizing recommendation accuracy. However, they usually overlook other important aspects of recommendation quality, e.g., the diversity of recommendation results. In this work, we propose a novel recommendation framework, called Diversity-promoting Deep Reinforcement Learning (D2RL), which aims to promote the diversity of interactive recommendation results. In D2RL, the Determinantal Point Process (DPP) is used to generate diverse, while relevant item recommendations. For each user, a personalized DPP kernel matrix is maintained, which is constructed from two parts: a fixed similarity matrix capturing item-item similarity, and the relevance of items dynamically learnt through an actor-critic reinforcement learning framework. To demonstrate the effectiveness of the proposed D2RL model, we have performed extensive offline experiments as well as simulated online experiments with real-world datasets.},
booktitle = {5th International Conference on Crowd Science and Engineering},
pages = {132–139},
numpages = {8},
keywords = {Reinforcement Learning, Interactive Recommendation, Determinantal Point Process, Diversity},
location = {Jinan, China},
series = {ICCSE '21}
}

@inproceedings{10.1145/3397271.3401467,
author = {Zhang, Weinan and Zhao, Xiangyu and Zhao, Li and Yin, Dawei and Yang, Grace Hui and Beutel, Alex},
title = {Deep Reinforcement Learning for Information Retrieval: Fundamentals and Advances},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401467},
doi = {10.1145/3397271.3401467},
abstract = {Information retrieval (IR) techniques, such as search, recommendation and online advertising, satisfying users' information needs by suggesting users personalized objects (information or services) at the appropriate time and place, play a crucial role in mitigating the information overload problem. Since the widely use of mobile applications, more and more information retrieval services have provided interactive functionality and products. Thus, learning from interaction becomes a crucial machine learning paradigm for interactive IR, which is based on reinforcement learning. With recent great advances in deep reinforcement learning (DRL), there have been increasing interests in developing DRL based information retrieval techniques, which could continuously update the information retrieval strategies according to users' real-time feedback, and optimize the expected cumulative long-term satisfaction from users. Our workshop aims to provide a venue, which can bring together academia researchers and industry practitioners (i) to discuss the principles, limitations and applications of DRL for information retrieval, and (ii) to foster research on innovative algorithms, novel techniques, and new applications of DRL to information retrieval.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2468–2471},
numpages = {4},
keywords = {information retrieval, deep reinforcement learning},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.5555/3545946.3598668,
author = {Sheng, Junjie and Wang, Xiangfeng and Jin, Bo and Li, Wenhao and Wang, Jun and Yan, Junchi and Chang, Tsung-Hui and Zha, Hongyuan},
title = {Learning Structured Communication for Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper investigates multi-agent reinforcement learning (MARL) communication mechanisms in large-scale scenarios. We propose a novel framework, Learning Structured Communication (LSC), that leverages a flexible and efficient communication topology. LSC enables adaptive agent grouping to create diverse hierarchical formations over episodes generated through an auxiliary task and a hierarchical routing protocol. We learn a hierarchical graph neural network with the formed topology that facilitates effective message generation and propagation between inter- and intra-group communications. Unlike state-of-the-art communication mechanisms, LSC possesses a detailed and learnable design for hierarchical communication. Numerical experiments on challenging tasks demonstrate that the proposed LSC exhibits high communication efficiency and global cooperation capability.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {436–438},
numpages = {3},
keywords = {multi-agent reinforcement learning, hierarchical structure, learning to communicate, graph neural networks},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3466184.3466305,
author = {van Heeswijk, Wouter and La Poutr\'{e}, Han},
title = {Deep Reinforcement Learning in Linear Discrete Action Spaces},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Problems in operations research are typically combinatorial and high-dimensional. To a degree, linear programs may efficiently solve such large decision problems. For stochastic multi-period problems, decomposition into a sequence of one-stage decisions with approximated downstream effects is often necessary, e.g., by deploying reinforcement learning to obtain value function approximations (VFAs). When embedding such VFAs into one-stage linear programs, VFA design is restricted by linearity. This paper presents an integrated simulation approach for such complex optimization problems, developing a deep reinforcement learning algorithm that combines linear programming and neural network VFAs. Our proposed method embeds neural network VFAs into one-stage linear decision problems, combining the nonlinear expressive power of neural networks with the efficiency of solving linear programs. As a proof of concept, we perform numerical experiments on a transportation problem. The neural network VFAs consistently outperform polynomial VFAs as well as other benchmarks, with limited design and tuning effort.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1063–1074},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/3582437.3587190,
author = {Chetitah, Mounsif and M\"{u}ller, Julian and Deserno, Lorenz and Waltmann, Maria and von Mammen, Sebastian},
title = {Gamification Framework for Reinforcement Learning-Based Neuropsychology Experiments},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3587190},
doi = {10.1145/3582437.3587190},
abstract = {Reinforcement learning (RL) is an adaptive process where an agent relies on its experience to improve the outcome of its performance. It learns by taking actions to maximize its rewards, and by minimizing the gap between predicted and received rewards. In experimental neuropsychology, RL algorithms are used as a conceptual basis to account for several aspects of human motivation and cognition. A number of neuropsychological experiments, such as reversal learning, sequential decision-making, and go-no-go tasks, are required to validate the decisive RL algorithms. The experiments are conducted in digital environments and are comprised of numerous trials that lead to participants’ frustration and fatigue. This paper presents a gamification framework for reinforcement-based neuropsychology experiments that aims to increase participant engagement and provide them with appropriate testing environments.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {51},
numpages = {4},
keywords = {reinforcement learning, gamification, neuropsychology, serious games},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3528535.3565249,
author = {Pan, Lichen and Qian, Jun and Xia, Wei and Mao, Hangyu and Yao, Jun and Li, Pengze and Xiao, Zhen},
title = {Optimizing Communication in Deep Reinforcement Learning with XingTian},
year = {2022},
isbn = {9781450393409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528535.3565249},
doi = {10.1145/3528535.3565249},
abstract = {Deep Reinforcement Learning (DRL) achieves great success in various domains. Communication in today's DRL algorithms takes non-negligible time compared to the computation. However, prior DRL frameworks usually focus on computation management while paying little attention to communication optimization, and fail to utilize the opportunity of the communication-computation overlap that hides the communication from the critical path of DRL algorithms. Consequently, communication can take more time than the computation in prior DRL frameworks. In this paper, we present XingTian, a novel DRL framework that co-designs the management of communication and computation in DRL algorithms. XingTian organizes the computation in DRL algorithms in a decentralized way and provides an asynchronous communication channel. XingTian makes the communication execute asynchronously and aggressively and takes advantage of the communication-computation overlapping opportunity from DRL algorithms. Experimental results show that XingTian improves data transmission efficiency and can transmit at least twice as much data per second as the state-of-the-art DRL framework RLLib. DRL algorithms based on XingTian achieve up to 70.71\% more throughput than RLLib-based ones with better or similar convergent performance. XingTian maintains high communication efficiency under different scale deployments and the XingTian-based DRL algorithm achieves 91.12\% higher throughput than the RLLib-based one when deployed in four machines. XingTian is open-sourced and publicly available at https://github.com/huawei-noah/xingtian.},
booktitle = {Proceedings of the 23rd ACM/IFIP International Middleware Conference},
pages = {255–268},
numpages = {14},
keywords = {communication-computation overlap, asynchronous communication, decentralized computation, deep reinforcement learning},
location = {Quebec, QC, Canada},
series = {Middleware '22}
}

@inproceedings{10.1145/3490354.3494415,
author = {Guan, Mao and Liu, Xiao-Yang},
title = {Explainable Deep Reinforcement Learning for Portfolio Management: An Empirical Approach},
year = {2022},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494415},
doi = {10.1145/3490354.3494415},
abstract = {Deep reinforcement learning (DRL) has been widely studied in the portfolio management task. However, it is challenging to understand a DRL-based trading strategy because of the black-box nature of deep neural networks. In this paper, we propose an empirical approach to explain the strategies of DRL agents for the portfolio management task. First, we use a linear model in hindsight as the reference model, which finds the best portfolio weights by assuming knowing actual stock returns in foresight. In particular, we use the coefficients of a linear model in hindsight as the reference feature weights. Secondly, for DRL agents, we use integrated gradients to define the feature weights, which are the coefficients between reward and features under a linear regression model. Thirdly, we study the prediction power in two cases, single-step prediction and multi-step prediction. In particular, we quantify the prediction power by calculating the linear correlations between the feature weights of a DRL agent and the reference feature weights, and similarly for machine learning methods. Finally, we evaluate a portfolio management task on Dow Jones 30 constituent stocks during 01/01/2009 to 09/01/2021. Our approach empirically reveals that a DRL agent exhibits a stronger multi-step prediction power than machine learning methods.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {50},
numpages = {9},
keywords = {integrated gradient, explainable deep reinforcement learning, portfolio management, linear model in hindsight},
location = {Virtual Event},
series = {ICAIF '21}
}

@inproceedings{10.5555/3545946.3598615,
author = {Jiang, Jiechuan and Lu, Zongqing},
title = {Adaptive Learning Rates for Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In multi-agent reinforcement learning (MARL), the learning rates of actors and critic are mostly hand-tuned and fixed. This not only requires heavy tuning but more importantly limits the learning. With adaptive learning rates according to gradient patterns, some optimizers have been proposed for general optimizations, which however do not take into consideration the characteristics of MARL. In this paper, we propose AdaMa to bring adaptive learning rates to cooperative MARL. AdaMa evaluates the contribution of actors' updates to the improvement of Q-value and adaptively updates the learning rates of actors to the direction of maximally improving the Q-value. AdaMa could also dynamically balance the learning rates between the critic and actors according to their varying effects on the learning. Moreover, AdaMa can incorporate the second-order approximation to capture the contribution of pairwise actors' updates and thus more accurately updates the learning rates of actors. Empirically, we show that AdaMa could accelerate learning and improve performance in a variety of multi-agent scenarios. More importantly, AdaMa does not require heavy hyperparameter tuning and thus significantly reduces the training cost. The visualizations of learning rates during training clearly explain how and why AdaMa works.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {23–30},
numpages = {8},
keywords = {adaptive learning rates, multi-agent reinforcement learning, reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.1145/2897824.2925881,
author = {Peng, Xue Bin and Berseth, Glen and van de Panne, Michiel},
title = {Terrain-Adaptive Locomotion Skills Using Deep Reinforcement Learning},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925881},
doi = {10.1145/2897824.2925881},
abstract = {Reinforcement learning offers a promising methodology for developing skills for simulated characters, but typically requires working with sparse hand-crafted features. Building on recent progress in deep reinforcement learning (DeepRL), we introduce a mixture of actor-critic experts (MACE) approach that learns terrain-adaptive dynamic locomotion skills using high-dimensional state and terrain descriptions as input, and parameterized leaps or steps as output actions. MACE learns more quickly than a single actor-critic approach and results in actor-critic experts that exhibit specialization. Additional elements of our solution that contribute towards efficient learning include Boltzmann exploration and the use of initial actor biases to encourage specialization. Results are demonstrated for multiple planar characters and terrain classes.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {81},
numpages = {12},
keywords = {physics-based characters, reinforcement learning}
}

@inproceedings{10.5555/2484920.2485093,
author = {Zhang, Chongjie and Lesser, Victor},
title = {Coordinating Multi-Agent Reinforcement Learning with Limited Communication},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Coordinated multi-agent reinforcement learning (MARL) provides a promising approach to scaling learning in large cooperative multi-agent systems. Distributed constraint optimization (DCOP) techniques have been used to coordinate action selection among agents during both the learning phase and the policy execution phase (if learning is off-line) to ensure good overall system performance. However, running DCOP algorithms for each action selection through the whole system results in significant communication among agents, which is not practical for most applications with limited communication bandwidth. In this paper, we develop a learning approach that generalizes previous coordinated MARL approaches that use DCOP algorithms and enables MARL to be conducted over a spectrum from independent learning (without communication) to fully coordinated learning depending on agents' communication bandwidth. Our approach defines an interaction measure that allows agents to dynamically identify their beneficial coordination set (i.e., whom to coordinate with) in different situations and to trade off its performance and communication cost. By limiting their coordination set, agents dynamically decompose the coordination network in a distributed way, resulting in dramatically reduced communication for DCOP algorithms without significantly affecting overall learning performance. Essentially, our learning approach conducts co-adaptation of agents' policy learning and coordination set identification, which outperforms approaches that sequence them.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1101–1108},
numpages = {8},
keywords = {multiagent learning, distributed constraint optimization, coordinated learning},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1145/3380446.3430619,
author = {Servadei, Lorenzo and Zheng, Jiapeng and Arjona-Medina, Jos\'{e} and Werner, Michael and Esen, Volkan and Hochreiter, Sepp and Ecker, Wolfgang and Wille, Robert},
title = {Cost Optimization at Early Stages of Design Using Deep Reinforcement Learning},
year = {2020},
isbn = {9781450375191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380446.3430619},
doi = {10.1145/3380446.3430619},
abstract = {With the increase in the complexity of the modern system on Chips(SoCs) and the demand for a lower time-to-market, automation becomes essential in hardware design. This is particularly relevant in complex/time-consuming tasks, as the optimization of design cost for a hardware component. Design cost, in fact, may depend on several objectives, as for the hardware-software trade-off. Given the complexity of this task, the designer often has no means to perform a fast and effective optimization in particular for larger and complex designs. In this paper, we introduce Deep Reinforcement Learning(DRL) for design cost optimization at the early stages of the design process. We first show that DRL is a perfectly suitable solution for the problem at hand. Afterward, by means of a Pointer Network, a neural network specifically applied for combinatorial problems, we benchmark three DRL algorithms towards the selected problem. Results obtained in different settings show the improvements achieved by DRL algorithms compared to conventional optimization methods. Additionally, by using reward redistribution proposed in the recently introduced RUDDER method, we obtain significant improvements in complex designs. Here, the obtained optimization is on average 15.18\% on the area as well as 8.25\% and 8.12\% on the application size and execution time on a dataset of industrial hardware/software interface design},
booktitle = {Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {37–42},
numpages = {6},
keywords = {machine learning, hardware-software co-design, design automation, reinforcement learning},
location = {Virtual Event, Iceland},
series = {MLCAD '20}
}

@inproceedings{10.1145/3341216.3342206,
author = {Shukla, Apoorv and Hudemann, Kevin Nico and Hecker, Artur and Schmid, Stefan},
title = {Runtime Verification of P4 Switches with Reinforcement Learning},
year = {2019},
isbn = {9781450368728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341216.3342206},
doi = {10.1145/3341216.3342206},
abstract = {We present the design and early implementation of p4rl, a system that uses reinforcement learning-guided fuzz testing to execute the verification of P4 switches automatically at runtime. p4rl system uses our novel user-friendly query language, p4q to conveniently specify the intended properties in simple conditional statements (if-else) and check the actual runtime behavior of the P4 switch against such properties. In p4rl, user-specified p4q queries with the control plane configuration, Agent, and the Reward System guide the fuzzing process to trigger runtime bugs automatically during Agent training. To illustrate the strength of p4rl, we developed and evaluated an early prototype of p4rl system that executes runtime verification of a P4 network device, e.g., L3 (Layer-3) switch. Our initial results are promising and show that p4rl automatically detects diverse bugs while outperforming the baseline approach.},
booktitle = {Proceedings of the 2019 Workshop on Network Meets AI \&amp; ML},
pages = {1–7},
numpages = {7},
keywords = {Network Verification, Fuzzing, P4, Machine Learning},
location = {Beijing, China},
series = {NetAI'19}
}

@inproceedings{10.5555/3091125.3091280,
author = {da Silva, Felipe Leno and Glatt, Ruben and Costa, Anna Helena Reali},
title = {Simultaneously Learning and Advising in Multiagent Reinforcement Learning},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reinforcement Learning has long been employed to solve sequential decision-making problems with minimal input data. However, the classical approach requires a large number of interactions with an environment to learn a suitable policy. This problem is further intensified when multiple autonomous agents are simultaneously learning in the same environment. The teacher-student approach aims at alleviating this problem by integrating an advising procedure in the learning process, in which an experienced agent (human or not) can advise a student to guide her exploration. Even though previous works reported that an agent can learn faster when receiving advice, their proposals require that the teacher is an expert in the learning task. Sharing successful episodes can also accelerate learning, but this procedure requires a lot of communication between agents, which is unfeasible for domains in which communication is limited. Thus, we here propose a multiagent advising framework where multiple agents can advise each other while learning in a shared environment. If in any state an agent is unsure about what to do, it can ask for advice to other agents and may receive answers from agents that have more confidence in their actuation for that state. We perform experiments in a simulated Robot Soccer environment and show that the learning process is improved by incorporating this kind of advice.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {1100–1108},
numpages = {9},
keywords = {transfer learning, multiagent reinforcement learning, autonomous advice taking, cooperative learning},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@inproceedings{10.1145/3460231.3478864,
author = {Antaris, Stefanos and Rafailidis, Dimitrios},
title = {Sequence Adaptation via Reinforcement Learning in Recommender Systems},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3478864},
doi = {10.1145/3460231.3478864},
abstract = {Accounting for the fact that users have different sequential patterns, the main drawback of state-of-the-art recommendation strategies is that a fixed sequence length of user-item interactions is required as input to train the models. This might limit the recommendation accuracy, as in practice users follow different trends on the sequential recommendations. Hence, baseline strategies might ignore important sequential interactions or add noise to the models with redundant interactions, depending on the variety of users’ sequential behaviours. To overcome this problem, in this study we propose the SAR model, which not only learns the sequential patterns but also adjusts the sequence length of user-item interactions in a personalized manner. We first design an actor-critic framework, where the RL agent tries to compute the optimal sequence length as an action, given the user’s state representation at a certain time step. In addition, we optimize a joint loss function to align the accuracy of the sequential recommendations with the expected cumulative rewards of the critic network, while at the same time we adapt the sequence length with the actor network in a personalized manner. Our experimental evaluation on four real-world datasets demonstrates the superiority of our proposed model over several baseline approaches. Finally, we make our implementation publicly available at https://github.com/stefanosantaris/sar.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {714–718},
numpages = {5},
keywords = {adaptive learning, reinforcement learning, sequential recommendation},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@inproceedings{10.1145/3406499.3418769,
author = {Mill\'{a}n-Arias, Cristian and Fernandes, Bruno and Cruz, Francisco and Dazeley, Richard and Fernandes, Sergio},
title = {A Robust Approach for Continuous Interactive Reinforcement Learning},
year = {2020},
isbn = {9781450380546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406499.3418769},
doi = {10.1145/3406499.3418769},
abstract = {Interactive reinforcement learning is an approach in which an external trainer helps an agent to learn through advice. A trainer is useful in large or continuous scenarios; however, when the characteristics of the environment change over time, it can affect the learning. Robust reinforcement learning is a reliable approach that allows an agent to learn a task, regardless of disturbances in the environment. In this work, we present an approach that addresses interactive reinforcement learning problems in a dynamic environment with continuous states and actions. Our results show that the proposed approach allows an agent to complete the cart-pole balancing task satisfactorily in a dynamic, continuous action-state domain.},
booktitle = {Proceedings of the 8th International Conference on Human-Agent Interaction},
pages = {278–280},
numpages = {3},
keywords = {interactive robust reinforcement learning, actor-critic, actor-disturber-critic, robust reinforcement learning, interactive reinforcement learning, reinforcement learning, policy-shaping},
location = {Virtual Event, USA},
series = {HAI '20}
}

@article{10.1145/3464389,
author = {Trummer, Immanuel and Wang, Junxiong and Wei, Ziyun and Maram, Deepak and Moseley, Samuel and Jo, Saehan and Antonakakis, Joseph and Rayabhari, Ankush},
title = {SkinnerDB: Regret-Bounded Query Evaluation via Reinforcement Learning},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/3464389},
doi = {10.1145/3464389},
abstract = {SkinnerDB uses reinforcement learning for reliable join ordering, exploiting an adaptive processing engine with specialized join algorithms and data structures. It maintains no data statistics and uses no cost or cardinality models. Also, it uses no training workloads nor does it try to link the current query to seemingly similar queries in the past. Instead, it uses reinforcement learning to learn optimal join orders from scratch during the execution of the current query. To that purpose, it divides the execution of a query into many small time slices. Different join orders are tried in different time slices. SkinnerDB merges result tuples generated according to different join orders until a complete query result is obtained. By measuring execution progress per time slice, it identifies promising join orders as execution proceeds.Along with SkinnerDB, we introduce a new quality criterion for query execution strategies. We upper-bound expected execution cost regret, i.e., the expected amount of execution cost wasted due to sub-optimal join order choices. SkinnerDB features multiple execution strategies that are optimized for that criterion. Some of them can be executed on top of existing database systems. For maximal performance, we introduce a customized execution engine, facilitating fast join order switching via specialized multi-way join algorithms and tuple representations.We experimentally compare SkinnerDB’s performance against various baselines, including MonetDB, Postgres, and adaptive processing methods. We consider various benchmarks, including the join order benchmark, TPC-H, and JCC-H, as well as benchmark variants with user-defined functions. Overall, the overheads of reliable join ordering are negligible compared to the performance impact of the occasional, catastrophic join order choice.},
journal = {ACM Trans. Database Syst.},
month = {sep},
articleno = {9},
numpages = {45},
keywords = {reinforcement learning, Query optimization, adaptive processing}
}

@inproceedings{10.5555/3545946.3598614,
author = {Yang, Jiachen and Mittal, Ketan and Dzanic, Tarik and Petrides, Socratis and Keith, Brendan and Petersen, Brenden and Faissol, Daniel and Anderson, Robert},
title = {Multi-Agent Reinforcement Learning for Adaptive Mesh Refinement},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Adaptive mesh refinement (AMR) is necessary for efficient finite element simulations of complex physical phenomenon, as it allocates limited computational budget based on the need for higher or lower resolution, which varies over space and time. We present a novel formulation of AMR as a fully-cooperative Markov game, in which each element is an independent agent who makes refinement and de-refinement choices based on local information. We design a novel deep multi-agent reinforcement learning (MARL) algorithm called Value Decomposition Graph Network (VDGN), which solves the two core challenges that AMR poses for MARL: posthumous credit assignment due to agent creation and deletion, and unstructured observations due to the diversity of mesh geometries. For the first time, we show that MARL enables anticipatory refinement of regions that will encounter complex features at future times, thereby unlocking entirely new regions of the error-cost objective landscape that are inaccessible by traditional methods based on local error estimators. Comprehensive experiments show that VDGN policies significantly outperform error threshold-based policies in global error and cost metrics. We show that learned policies generalize to test problems with physical features, mesh geometries, and longer simulation times that were not seen in training. We also extend VDGN with multi-objective optimization capabilities to find the Pareto front of the tradeoff between cost and error.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {14–22},
numpages = {9},
keywords = {multi-agent reinforcement learning, graph neural network, adaptive mesh refinement, numerical analysis},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3319619.3322044,
author = {Grbic, Djordje and Risi, Sebastian},
title = {Towards Continual Reinforcement Learning through Evolutionary Meta-Learning},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3322044},
doi = {10.1145/3319619.3322044},
abstract = {In continual learning, an agent is exposed to a changing environment, requiring it to adapt during execution time. While traditional reinforcement learning (RL) methods have shown impressive results in various domains, there has been less progress in addressing the challenge of continual learning. Current RL approaches do not allow the agent to adapt during execution but only during a dedicated training phase. Here we study the problem of continual learning in a 2D bipedal walker domain, in which the legs of the walker grow over its lifetime, requiring the agent to adapt. The introduced approach combines neuroevolution, to determine the starting weights of a deep neural network, and a version of deep reinforcement learning that is continually running during execution time. The proof-of-concept results show that the combined approach gives a better generalisation performance when compared to evolution or reinforcement learning alone. The hybridization of reinforcement learning and evolution opens up exciting new research directions for continually learning agents that can benefit from suitable priors determined by an evolutionary process.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {119–120},
numpages = {2},
keywords = {reinforcement learning, meta-learning, continual learning},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.5555/3545946.3598749,
author = {Chen, Yang and Zhang, Libo and Liu, Jiamou and Witbrock, Michael},
title = {Adversarial Inverse Reinforcement Learning for Mean Field Games},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Goal-based agents respond to environments and adjust behaviour accordingly to reach objectives. Understanding incentives of interacting agents from observed behaviour is a core problem in multi-agent systems. Inverse reinforcement learning (IRL) solves this problem, which infers underlying reward functions by observing the behaviour of rational agents. Despite IRL being principled, it becomes intractable when the number of agents grows because of the curse of dimensionality and the explosion of agent interactions. The formalism of Mean field games (MFGs) has gained momentum as a mathematically tractable paradigm for studying large-scale multi-agent systems. By grounding IRL in MFGs, recent research attempts to push the limits of the agent number in IRL. However, the study of IRL for MFGs is far from being mature as existing methods assume strong rationality, while real-world agents often exhibit bounded rationality due to the limited cognitive or computational capacity. Towards a more general and practical IRL framework for MFGs, this paper proposes Mean-Field Adversarial IRL, a novel framework capable of tolerating bounded rationality. We build it upon the maximum entropy principle, adversarial learning, and a new equilibrium concept for MFGs. We evaluate our machinery on simulated tasks with imperfect demonstrations resulting from bounded rationality. Experimental results demonstrate the superiority of MF-AIRL over existing methods in reward recovery.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1088–1096},
numpages = {9},
keywords = {mean field games, maximum entropy principle, inverse reinforcement learning, adversarial learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.1145/3563778,
author = {Baucum, Matt and Khojandi, Anahita and Myers, Carole and Kessler, Larry},
title = {Optimizing Substance Use Treatment Selection Using Reinforcement Learning},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3563778},
doi = {10.1145/3563778},
abstract = {Substance use disorder (SUD) exacts a substantial economic and social cost in the United States, and it is crucial for SUD treatment providers to match patients with feasible, effective, and affordable treatment plans. The availability of large SUD patient datasets allows for machine learning techniques to predict patient-level SUD outcomes, yet there has been almost no research on whether machine learning can be used to optimize or personalize which treatment plans SUD patients receive. We use contextual bandits (a reinforcement learning technique) to optimally map patients to SUD treatment plans, based on dozens of patient-level and geographic covariates. We also use near-optimal policies to incorporate treatments’ time-intensiveness and cost into our recommendations, to aid treatment providers and policymakers in allocating treatment resources. Our personalized treatment recommendation policies are estimated to yield higher remission rates than observed in our original dataset, and they suggest clinical insights to inform future research on data-driven SUD treatment matching.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {mar},
articleno = {13},
numpages = {30},
keywords = {substance use, Contextual bandits, reinforcement learning}
}

@inproceedings{10.5555/3535850.3535853,
author = {Agarwal, Mridul and Aggarwal, Vaneet and Lan, Tian},
title = {Multi-Objective Reinforcement Learning with Non-Linear Scalarization},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-Objective Reinforcement Learning (MORL) setup naturally arises in many places where an agent optimizes multiple objectives. We consider the problem of MORL where multiple objectives are combined using a non-linear scalarization. We combine the vector objectives with a concave scalarization function and maximize this scalar objective. To work with the non-linear scalarization, in this paper, we propose a solution using steady-state occupancy measures and long-term average rewards. We show that when the scalarization function is element-wise increasing, the optimal policy for the scalarization is also Pareto optimal. To maximize the scalarized objective, we propose a model-based posterior sampling algorithm. Using a novel Bellman error analysis for infinite horizon MDPs based proof, we show that the proposed algorithm obtains a regret bound of ~O(LKDS√A/T) for K objectives, and L-Lipschitz continous scalarization function for MDP with S states, A actions, and diameter D. Additionally, we propose policy-gradient and actor-critic algorithms for MORL. For the policy gradient actor, we obtain the gradient using chain rule, and we learn different critics for each of the K objectives. Finally, we implement our algorithms on multiple environments including deep-sea treasure, and network scheduling setups to demonstrate that the proposed algorithms can optimize non-linear scalarization of multiple objectives.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {9–17},
numpages = {9},
keywords = {actor-critic algorithms, regret analysis, multi-objective reinforcement learning, reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@article{10.1145/3359554,
author = {Lei, Yu and Li, Wenjie},
title = {Interactive Recommendation with User-Specific Deep Reinforcement Learning},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3359554},
doi = {10.1145/3359554},
abstract = {In this article, we study a multi-step interactive recommendation problem for explicit-feedback recommender systems. Different from the existing works, we propose a novel user-specific deep reinforcement learning approach to the problem. Specifically, we first formulate the problem of interactive recommendation for each target user as a Markov decision process (MDP). We then derive a multi-MDP reinforcement learning task for all involved users. To model the possible relationships (including similarities and differences) between different users’ MDPs, we construct user-specific latent states by using matrix factorization. After that, we propose a user-specific deep Q-learning (UDQN) method to estimate optimal policies based on the constructed user-specific latent states. Furthermore, we propose Biased UDQN (BUDQN) to explicitly model user-specific information by employing an additional bias parameter when estimating the Q-values for different users. Finally, we validate the effectiveness of our approach by comprehensive experimental results and analysis.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {oct},
articleno = {61},
numpages = {15},
keywords = {deep Q-learning, deep reinforcement learning, Interactive recommendation}
}

@inproceedings{10.1145/3583781.3590298,
author = {Ni, Yang and Abraham, Danny and Issa, Mariam and Kim, Yeseong and Mercati, Pietro and Imani, Mohsen},
title = {Efficient Off-Policy Reinforcement Learning via Brain-Inspired Computing},
year = {2023},
isbn = {9798400701252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583781.3590298},
doi = {10.1145/3583781.3590298},
abstract = {Reinforcement Learning (RL) has opened up new opportunities to enhance existing smart systems that generally include a complex decision-making process. However, modern RL algorithms, e.g., Deep Q-Networks (DQN), are based on deep neural networks, resulting in high computational costs. In this paper, we propose QHD, an off-policy value-based Hyperdimensional Reinforcement Learning, that mimics brain properties toward robust and real-time learning. QHD relies on a lightweight brain-inspired model to learn an optimal policy in an unknown environment. On both desktop and power-limited embedded platforms, QHD achieves significantly better overall efficiency than DQN while providing higher or comparable rewards. QHD is also suitable for highly-efficient reinforcement learning with great potential for online and real-time learning. Our solution supports a small experience replay batch size that provides 12.3 times speedup compared to DQN while ensuring minimal quality loss. Our evaluation shows QHD capability for real-time learning, providing 34.6 times speedup and significantly better quality of learning than DQN.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2023},
pages = {449–453},
numpages = {5},
keywords = {hyperdimensional computing, brain-inspired computing},
location = {Knoxville, TN, USA},
series = {GLSVLSI '23}
}

@inproceedings{10.1145/1273496.1273640,
author = {Zhang, Xinhua and Aberdeen, Douglas and Vishwanathan, S. V. N.},
title = {Conditional Random Fields for Multi-Agent Reinforcement Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273640},
doi = {10.1145/1273496.1273640},
abstract = {Conditional random fields (CRFs) are graphical models for modeling the probability of labels given the observations. They have traditionally been trained with using a set of observation and label pairs. Underlying all CRFs is the assumption that, conditioned on the training data, the labels are independent and identically distributed (iid). In this paper we explore the use of CRFs in a class of temporal learning algorithms, namely policy-gradient reinforcement learning (RL). Now the labels are no longer iid. They are actions that update the environment and affect the next observation. From an RL point of view, CRFs provide a natural way to model joint actions in a decentralized Markov decision process. They define how agents can communicate with each other to choose the optimal joint action. Our experiments include a synthetic network alignment problem, a distributed sensor network, and road traffic control; clearly outperforming RL methods which do not model the proper joint policy.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1143–1150},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@article{10.1145/3508028,
author = {Zhou, Xingyu},
title = {Differentially Private Reinforcement Learning with Linear Function Approximation},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3508028},
doi = {10.1145/3508028},
abstract = {Motivated by the wide adoption of reinforcement learning (RL) in real-world personalized services, where users' sensitive and private information needs to be protected, we study regret minimization in finite-horizon Markov decision processes (MDPs) under the constraints of differential privacy (DP). Compared to existing private RL algorithms that work only on tabular finite-state, finite-actions MDPs, we take the first step towards privacy-preserving learning in MDPs with large state and action spaces. Specifically, we consider MDPs with linear function approximation (in particular linear mixture MDPs) under the notion of joint differential privacy (JDP), where the RL agent is responsible for protecting users' sensitive data. We design two private RL algorithms that are based on value iteration and policy optimization, respectively, and show that they enjoy sub-linear regret performance while guaranteeing privacy protection. Moreover, the regret bounds are independent of the number of states, and scale at most logarithmically with the number of actions, making the algorithms suitable for privacy protection in nowadays large-scale personalized services. Our results are achieved via a general procedure for learning in linear mixture MDPs under changing regularizers, which not only generalizes previous results for non-private learning, but also serves as a building block for general private reinforcement learning.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {feb},
articleno = {8},
numpages = {27},
keywords = {linear function approximations, reinforcement learning, differential privacy}
}

@article{10.5555/3455716.3455867,
author = {Paul, Supratik and Chatzilygeroudis, Konstantinos and Ciosek, Kamil and Mouret, Jean-Baptiste and Osborne, Michael A. and Whiteson, Shimon},
title = {Robust Reinforcement Learning with Bayesian Optimisation and Quadrature},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables: state features that are unobservable and randomly determined by the environment in a physical setting but are controllable in a simulator. This article considers the problem of finding a robust policy while taking into account the impact of environment variables. We present alternating optimisation and quadrature (ALOQ), which uses Bayesian optimisation and Bayesian quadrature to address such settings. We also present transferable ALOQ (TALOQ), for settings where simulator inaccuracies lead to difficulty in transferring the learnt policy to the physical system. We show that our algorithms are robust to the presence of significant rare events, which may not be observable under random sampling but play a substantial role in determining the optimal policy. Experimental results across different domains show that our algorithms learn robust policies efficiently.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {151},
numpages = {31},
keywords = {Bayesian optimisation, Bayesian quadrature, reinforcement learning, environment variables, significant rare events}
}

@inproceedings{10.5555/3306127.3331848,
author = {Chen, Yujie and Qian, Yu and Yao, Yichen and Wu, Zili and Li, Rongqi and Zhou, Yinzhi and Hu, Haoyuan and Xu, Yinghui},
title = {Can Sophisticated Dispatching Strategy Acquired by Reinforcement Learning?},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper, we study a courier dispatching problem (CDP) raised from an online pickup-service platform of Alibaba. The CDP aims to assign a set of couriers to serve pickup requests with stochastic spatial and temporal arrival rate among urban regions. The objective is to maximize the revenue of served requests given a limited number of couriers over a period of time. Many online algorithms such as dynamic matching and vehicle routing strategy from existing literature could be applied to tackle this problem. However, these methods rely on appropriately predefined optimization objectives at each decision point, which is hard in dynamic situations. This paper formulates the CDP as a Markov decision process (MDP) and proposes a data-driven approach to derive the optimal dispatching rule-set under different scenarios. Our method stacks multi-layer images of the spatial-and-temporal map and apply multi-agent reinforcement learning (MARL) techniques to evolve dispatching models. This method solves the learning inefficiency caused by traditional centralized MDP modeling. Through comprehensive experiments on both artificial dataset and real-world dataset, we show: 1) By utilizing historical data and considering long-term revenue gains, MARL achieves better performance than myopic online algorithms; 2) MARL is able to construct the mapping between complex scenarios to sophisticated decisions such as the dispatching rule. 3) MARL has the scalability to adopt in large-scale real-world scenarios.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1395–1403},
numpages = {9},
keywords = {smart cities, courier dispatching problem, multi-agent reinforcement learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3508352.3549387,
author = {Li, Mengyuan and Kazemi, Arman and Laguna, Ann Franchesca and Hu, X. Sharon},
title = {Associative Memory Based Experience Replay for Deep Reinforcement Learning},
year = {2022},
isbn = {9781450392174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508352.3549387},
doi = {10.1145/3508352.3549387},
abstract = {Experience replay is an essential component in deep reinforcement learning (DRL), which stores the experiences and generates experiences for the agent to learn in real time. Recently, prioritized experience replay (PER) has been proven to be powerful and widely deployed in DRL agents. However, implementing PER on traditional CPU or GPU architectures incurs significant latency overhead due to its frequent and irregular memory accesses. This paper proposes a hardware-software co-design approach to design an associative memory (AM) based PER, AMPER, with an AM-friendly priority sampling operation. AMPER replaces the widely-used time-costly tree-traversal-based priority sampling in PER while preserving the learning performance. Further, we design an in-memory computing hardware architecture based on AM to support AMPER by leveraging parallel in-memory search operations. AMPER shows comparable learning performance while achieving 55\texttimes{} to 270\texttimes{} latency improvement when running on the proposed hardware compared to the state-of-the-art PER running on GPU.},
booktitle = {Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
articleno = {135},
numpages = {9},
location = {San Diego, California},
series = {ICCAD '22}
}

@inproceedings{10.1145/3313231.3352369,
author = {Kao, Sheng-Chun and Yang, Chao-Han Huck and Chen, Pin-Yu and Ma, Xiaoli and Krishna, Tushar},
title = {Reinforcement Learning Based Interconnection Routing for Adaptive Traffic Optimization},
year = {2019},
isbn = {9781450367004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313231.3352369},
doi = {10.1145/3313231.3352369},
abstract = {Applying Machine Learning (ML) techniques to design and optimize computer architectures is a promising research direction. Optimizing the runtime performance of a Network-on-Chip (NoC) necessitates a continuous learning framework. In this work, we demonstrate the promise of applying reinforcement learning (RL) to optimize NoC runtime performance. We present three RL-based methods for learning optimal routing algorithms. The experimental results show the algorithms can successfully learn a near-optimal solution across different environment states.},
booktitle = {Proceedings of the 13th IEEE/ACM International Symposium on Networks-on-Chip},
articleno = {17},
numpages = {2},
keywords = {scalable modeling, congestion control, intelligent physical systems, network-on-chips, reinforcement learning},
location = {New York, New York},
series = {NOCS '19}
}

@inproceedings{10.1145/3594300.3594314,
author = {Troch, Arne and Mannens, Erik and Mercelis, Siegfried},
title = {Solving the Storage Location Assignment Problem Using Reinforcement Learning},
year = {2023},
isbn = {9781450399982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594300.3594314},
doi = {10.1145/3594300.3594314},
abstract = {In this work, we deal with the Storage Location Assignment Problem, often referred to as the SLAP, in an E-commerce Distribution Center (EDC). With E-commerce steadily increasing in popularity over the past decades, it has become a key part of the logistics industry. Due to the direct link with the customer, EDC's are forced into a significantly more complex and dynamic order picking process compared to conventional Bulk Distribution Centers. As a result of these challenges, many traditional approaches such as genetic algorithms and rule-based methods reach only suboptimal solutions. We propose the use of Reinforcement Learning (RL) to solve the SLAP, leading to a solution that adapts to dynamically changing environment parameters during runtime. For this purpose, we define a model that transforms the SLAP into a sequential decision making problem. We validate this novel approach by training a state-of-the-art RL algorithm within this model and comparing its results with a benchmark genetic algorithm approach. We conclude that the RL algorithm achieves promising results, surpassing benchmark performance and nearing optimal performance in a small-scale warehouse environment.},
booktitle = {Proceedings of the 2023 8th International Conference on Mathematics and Artificial Intelligence},
pages = {89–95},
numpages = {7},
location = {Chongqing, China},
series = {ICMAI '23}
}

@inproceedings{10.5555/2691365.2691372,
author = {Lin, Xue and Wang, Yanzhi and Bogdan, Paul and Chang, Naehyuck and Pedram, Massoud},
title = {Reinforcement Learning Based Power Management for Hybrid Electric Vehicles},
year = {2014},
isbn = {9781479962778},
publisher = {IEEE Press},
abstract = {Compared to conventional internal combustion engine (ICE) propelled vehicles, hybrid electric vehicles (HEVs) can achieve both higher fuel economy and lower pollution emissions. The HEV consists of a hybrid propulsion system containing one ICE and one or more electric motors (EMs). The use of both ICE and EM increases the complexity of HEV power management, and therefore requires advanced power management policies to achieve higher performance and lower fuel consumption. Towards this end, our work aims at minimizing the HEV fuel consumption over any driving cycle (without prior knowledge of the cycle) by using a reinforcement learning technique. This is in clear contrast to prior work, which requires deterministic or stochastic knowledge of the driving cycles. In addition, the proposed reinforcement learning technique enables us to (partially) avoid reliance on complex HEV modeling while coping with driver specific behaviors. To our knowledge, this is the first work that applies the reinforcement learning technique to the HEV power management problem. Simulation results over real-world and testing driving cycles demonstrate the proposed HEV power management policy can improve fuel economy by 42\%.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Conference on Computer-Aided Design},
pages = {32–38},
numpages = {7},
keywords = {reinforcement learning, power management, hybrid electric vehicle (HEV)},
location = {San Jose, California},
series = {ICCAD '14}
}

@inproceedings{10.1145/3477495.3531716,
author = {Huang, Jin and Oosterhuis, Harrie and Cetinkaya, Bunyamin and Rood, Thijs and de Rijke, Maarten},
title = {State Encoders in Reinforcement Learning for Recommendation: A Reproducibility Study},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531716},
doi = {10.1145/3477495.3531716},
abstract = {Methods for reinforcement learning for recommendation are increasingly receiving attention as they can quickly adapt to user feedback. A typical RL4Rec framework consists of (1) a state encoder to encode the state that stores the users' historical interactions, and (2) an RL method to take actions and observe rewards. Prior work compared four state encoders in an environment where user feedback is simulated based on real-world logged user data. An attention-based state encoder was found to be the optimal choice as it reached the highest performance. However, this finding is limited to the actor-critic method, four state encoders, and evaluation-simulators that do not debias logged user data. In response to these shortcomings, we reproduce and expand on the existing comparison of attention-based state encoders (1) in the publicly available debiased RL4Rec SOFA simulator with (2) a different RL method, (3) more state encoders, and (4) a different dataset. Importantly, our experimental results indicate that existing findings do not generalize to the debiased SOFA simulator generated from a different dataset and a DQN-based method when compared with more state encoders.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2738–2748},
numpages = {11},
keywords = {reinforcement learning, recommendation, state encoders},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.5555/3535850.3535976,
author = {Ruan, Jingqing and Du, Yali and Xiong, Xuantang and Xing, Dengpeng and Li, Xiyun and Meng, Linghui and Zhang, Haifeng and Wang, Jun and Xu, Bo},
title = {GCS: Graph-Based Coordination Strategy for Multi-Agent Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Many real-world scenarios involve a team of agents that have to coordinate their policies to achieve a shared goal. Previous studies mainly focus on decentralized control to maximize a common reward and barely consider the coordination among control policies, which is critical in dynamic and complicated environments. In this work, we propose factorizing the joint team policy into graph generator and graph-based coordinated policy to enable coordinated behaviours among agents. The graph generator adopts an encoder-decoder framework that outputs directed acyclic graphs (DAGs) to capture the underlying dynamic decision structure. We also apply the DAGness-constrained and DAG depth-constrained optimization in the graph generator to balance efficiency and performance. The graph-based coordinated policy exploits the generated decision structure. The graph generator and coordinated policy are trained simultaneously to maximize the discounted return. Empirical evaluations on Collaborative Gaussian Squeeze, Cooperative Navigation, and Google Research Football demonstrate the superiority of the proposed method. The code is available at urlhttps://github.com/Amanda-1997/GCS_aamas337.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1128–1136},
numpages = {9},
keywords = {reinforcement learning, multi-agent systems, action coordination graph},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3573428.3573579,
author = {Guan, Weijie and Li, Zhufeng and Liang, Qimin},
title = {Non-Euclidean Space Exploration for Reinforcement Learning State Embedding},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573579},
doi = {10.1145/3573428.3573579},
abstract = {With the rapid increase of computer computing speed in recent years, mechanical learning is widely used in the field of artificial intelligence. Among them, the most popular one belongs to the field of deep learning in mechanical learning. The reason why the field of deep learning is attracting attention is the success and popularity of its application in industry. The success of graph neural networks in industry in recent years has made them a very popular direction nowadays. This paper is dedicated to exploring how to encode game states by means of graph neural networks and proposes a game state encoder combining Convolutional Neural Networks and Graph Neural Networks.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {834–838},
numpages = {5},
location = {Xiamen, China},
series = {EITCE '22}
}

@inproceedings{10.1109/CGO53902.2022.9741272,
author = {Kim, Minsu and Park, Jeong-Keun and Moon, Soo-Mook},
title = {Solving PBQP-Based Register Allocation Using Deep Reinforcement Learning},
year = {2022},
isbn = {9781665405843},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO53902.2022.9741272},
doi = {10.1109/CGO53902.2022.9741272},
abstract = {Irregularly structured registers are hard to abstract and allocate. Partitioned Boolean quadratic programming (PBQP) is a useful abstraction to represent complex register constraints, even those in highly irregular processors of automated test equipment (ATE) of DRAM memory chips. The PBQP problem is NP-hard, requiring a heuristic solution. If no spill is allowed as in ATE, however, we have to enumerate more to find a solution rather than to approximate, since a spill means a total compilation failure. We propose solving the PBQP problem with deep reinforcement learning (Deep-RL), more specifically, a model-based approach using Monte Carlo tree search and deep neural network as used in Alphazero, a proven Deep-RL technology. Through elaborate training with random PBQP graphs, our Deep-RL solver could cut the search space sharply, making an enumeration-based solution more affordable. Furthermore, by employing backtracking with a proper coloring order, Deep-RL can find a solution with modestly-trained neural networks with even less search space. Our experiments show that Deep-RL can successfully find a solution for 10 product-level ATE programs while searching much fewer (e.g., 1/3,500) states than the previous PBQP enumeration solver. Also, when applied to C programs in llvm-test-suite for regular CPUs, it achieves a competitive performance to the existing PBQP register allocator in LLVM.},
booktitle = {Proceedings of the 20th IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {230–241},
numpages = {12},
location = {Virtual Event, Republic of Korea},
series = {CGO '22}
}

@inproceedings{10.1145/3523227.3547418,
author = {Liaw, Richard and Bailey, Paige and Li, Ying and Dimakopoulou, Maria and Raimond, Yves},
title = {REVEAL 2022: Reinforcement Learning-Based Recommender Systems at Scale},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523227.3547418},
doi = {10.1145/3523227.3547418},
abstract = {Recommendation systems are increasingly modelled as a sequential decision making process, where the system decides which items to recommend to a given user. Each decision to recommend an item or slate of items has a significant impact on immediate and future user responses, long-term satisfaction or engagement with the system, and possibly valuable exposure for the item provider. The REVEAL workshop will focus on how to optimise this multi-step decision-making process, where a stream of interactions occurs between the user and the system. Deriving reward signals from these interactions, and creating a scalable, performant, and maintainable recommendation model to use for inference is a key challenge for machine learning teams, both in industry and academia. We will discuss the following challenges at the workshop: How can recommendation system models take into account the delayed effects of each recommendation? What are the right ways to reason and plan for longer-term user satisfaction? How can we leverage techniques such as Reinforcement Learning (RL) at scale?},
booktitle = {Proceedings of the 16th ACM Conference on Recommender Systems},
pages = {684–685},
numpages = {2},
keywords = {multi-armed bandits, off-policy, recommender systems, reinforcement learning},
location = {Seattle, WA, USA},
series = {RecSys '22}
}

@article{10.1145/3570959,
author = {Chen, Lei and Cao, Jie and Liang, Weichao and Wu, Jia and Ye, Qiaolin},
title = {Keywords-Enhanced Deep Reinforcement Learning Model for Travel Recommendation},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/3570959},
doi = {10.1145/3570959},
abstract = {Tourism is an important industry and a popular entertainment activity involving billions of visitors per annum. One challenging problem tourists face is identifying satisfactory products from vast tourism information. Most of travel recommendation methods regard the recommendation procedure as a static process and only focus on immediate rewards. Meanwhile, they often infer user intensions from click behaviors and ignore the informative keywords of the clicked products. To this end, in this article, we present a Keywords-enhanced Deep Reinforcement Learning model (KDRL) framework. Specifically, we formalize travel recommendation as a Markov Decision Process and implement it upon the Actor–Critic framework. It integrates keyword information into the reinforcement learning–(RL) based recommendation framework by devising novel state representation and reward function and learns the travel recommendation and keywords generation simultaneously. To the best of our knowledge, this is the first time that keywords are explicitly discussed and used in RL-based travel recommendations. Extensive experiments are performed on the real-world datasets and the results clearly show the superior performance of KDRL compared with the baseline methods.},
journal = {ACM Trans. Web},
month = {dec},
articleno = {5},
numpages = {21},
keywords = {reinforcement learning, attention mechanism, Recommender system, deep neural network}
}

@inproceedings{10.1145/3238147.3238206,
author = {Wan, Yao and Zhao, Zhou and Yang, Min and Xu, Guandong and Ying, Haochao and Wu, Jian and Yu, Philip S.},
title = {Improving Automatic Source Code Summarization via Deep Reinforcement Learning},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238206},
doi = {10.1145/3238147.3238206},
abstract = {Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {397–407},
numpages = {11},
keywords = {Code summarization, comment generation, deep learning, reinforcement learning},
location = {Montpellier, France},
series = {ASE '18}
}

@article{10.1145/3584949,
author = {Rezaei, Yoones and Khan, Talha and Lee, Stephen and Moss\'{e}, Daniel},
title = {Solar-Powered Parking Analytics System Using Deep Reinforcement Learning},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3584949},
doi = {10.1145/3584949},
abstract = {Advances in deep vision techniques and the ubiquity of smart cameras will drive the next generation of video analytics. However, video analytics applications consume vast amounts of energy as deep learning techniques are power-hungry. In this article, we focus on a parking video analytics platform and propose RL-CamSleep, a deep reinforcement learning-based technique, to actuate the cameras to reduce the energy footprint while retaining the system’s utility. Our key insight is that many video-analytics applications do not need to be always operational, and we can design policies to activate video analytics only when necessary. We design two modes of operation for the reinforcement learning (RL) controller: (i) cloud-based mode and (ii) grid-isolated solar-powered mode. In the cloud-based mode, the controller runs on the cloud to control the cameras, whereas, in the solar-powered mode, the RL controller is constrained by the energy produced by solar. We evaluate our approach on a city-scale parking dataset having 76 streets spread across a city. Our analysis shows RL-CamSleep can learn an adaptive policy that reduces the average energy consumption by 76\% and achieves an average accuracy of 98\%. For the grid-isolated mode, RL-CamSleep outperforms other baseline techniques demonstrating the need for adaptive policy in energy-constrained environments.},
journal = {ACM Trans. Sen. Netw.},
month = {apr},
articleno = {75},
numpages = {27},
keywords = {Deep reinforcement learning, parking analytic system, ML, deep learning, energy efficiency}
}

@inproceedings{10.1145/3441417.3441433,
author = {Liu, Xia and Duan, Shihong and Pei, Fei and Chen, Qianwen and Liu, Bisong and Qiao, Feng},
title = {Safety Risk Assessment for Children's Products Based on Reinforcement Learning},
year = {2021},
isbn = {9781450387842},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441417.3441433},
doi = {10.1145/3441417.3441433},
abstract = {The prediction accuracy rate of the types of multi-factor associated injuries in the risk assessment model has been tested with 122 test data, resulting in that the correct recognition rate of the injury consequences of the adjusted model was about 75\%, the identification accuracy rate of two kinds of risk factor-related events in the categories of the mechanical injuries and inorganic poison hazards reached 78\%, and that of the other eight categories of risk factor-related hazard events was higher than 75\%, which confirms that the test results of the risk assessment model are satisfying. After reinforcement learning, the test accuracy of the strong classifier has been improved to 81\%, indicating that the reinforcement learning module has increased the generalization ability of the model at a certain extent, and the Bayesian network method combined with reinforcement learning may have stronger applicability.In the structure learning, the conditional probabilities as shown in Table 5, Table 6, and Table 7 are obtained, and the relationship that different factors cause a certain hazard is revealed, consequently to confirm how the key factors of a certain injury that is occurred.},
booktitle = {Proceedings of the 4th International Conference on Advances in Artificial Intelligence},
pages = {35–40},
numpages = {6},
keywords = {Reinforcement Learning, Assessment Model, Safety Risk, Children's Products},
location = {London, United Kingdom},
series = {ICAAI '20}
}

@inproceedings{10.1145/2742060.2742078,
author = {Lu, Shiting (Justin) and Tessier, Russell and Burleson, Wayne},
title = {Reinforcement Learning for Thermal-Aware Many-Core Task Allocation},
year = {2015},
isbn = {9781450334747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742060.2742078},
doi = {10.1145/2742060.2742078},
abstract = {To maintain reliable operation, task allocation for many-core processors must consider the heat interaction of processor cores and network-on-chip routers in performing task assignment. Our approach employs reinforcement learning, machine learning algorithm that performs task allocation based on current core and router temperatures and a prediction of which assignment will minimize maximum temperature in the future. The algorithm updates prediction models after each allocation based on feedback regarding the accuracy of previous predictions. Our new algorithm is verified via detailed many-core simulation which includes on-chip routing. Our results show that the proposed technique is fast (scheduling performed in &lt;1 ms) and can efficiently reduce peak temperature by up to 8°C in a 49-core processor (4.3°C on average) versus a competing task allocation approach for a series of SPLASH-2 benchmarks.},
booktitle = {Proceedings of the 25th Edition on Great Lakes Symposium on VLSI},
pages = {379–384},
numpages = {6},
keywords = {thermal aware, task allocation, reinforcement learning},
location = {Pittsburgh, Pennsylvania, USA},
series = {GLSVLSI '15}
}

@inproceedings{10.1145/3449726.3459462,
author = {Ecoffet, Paul and Fontbonne, Nicolas and Andr\'{e}, Jean-Baptiste and Bredeche, Nicolas},
title = {Reinforcement Learning with Rare Significant Events: Direct Policy Search vs. Gradient Policy Search},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3459462},
doi = {10.1145/3449726.3459462},
abstract = {This paper shows that the CMAES direct policy search method fares significantly better than PPO gradient policy search for a reinforcement learning task where significant events are rare.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {97–98},
numpages = {2},
keywords = {PPO, continuous state and action spaces, gradient policy search, on-policy, on-line, rare significant events, direct policy search, reinforcement learning, evolutionary algorithms, CMAES},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.5555/3539845.3540004,
author = {Elfar, Mahmoud and Liang, Tung-Che and Chakrabarty, Krishnendu and Pajic, Miroslav},
title = {Adaptive Droplet Routing for MEDA Biochips via Deep Reinforcement Learning},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Digital microfluidic biochips (DMFBs) based on a micro-electrode-dot-array (MEDA) architecture provide finegrained control and sensing of droplets in real-time. However, excessive actuation of microelectrodes in MEDA biochips can lead to charge trapping during bioassay execution, causing the failure of microelectrodes and erroneous bioassay outcomes. A recently proposed enhancement to MEDA allows run-time measurement of microelectrode health information, thereby enabling synthesis of adaptive routing strategies for droplets. However, existing synthesis solutions are computationally infeasible for large MEDA biochips that have been commercialized. In this paper, we propose a synthesis framework for adaptive droplet routing in MEDA biochips via deep reinforcement learning (DRL). The framework utilizes the real-time microelectrode health feedback to synthesize droplet routes that proactively minimize the likelihood of charge trapping. We show how the adaptive routing strategies can be synthesized using DRL. We implement the DRL agent, the MEDA simulation environment, and the bioassay scheduler using the OpenAI Gym environment. Our framework obtains adaptive routing policies efficiently for COVID-19 testing protocols on large arrays that reflect the sizes of commercial MEDA biochips available in the marketplace, significantly increasing probabilities of successful bioassay completion compared to existing methods.},
booktitle = {Proceedings of the 2022 Conference \&amp; Exhibition on Design, Automation \&amp; Test in Europe},
pages = {640–645},
numpages = {6},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3551349.3560507,
author = {Ferdous, Raihana and Kifetew, Fitsum and Prandi, Davide and Susi, Angelo},
title = {Towards Agent-Based Testing of 3D Games Using Reinforcement Learning},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560507},
doi = {10.1145/3551349.3560507},
abstract = {Computer game is a billion-dollar industry and is booming. Testing games has been recognized as a difficult task, which mainly relies on manual playing and scripting based testing. With the advances in technologies, computer games have become increasingly more interactive and complex, thus play-testing using human participants alone has become unfeasible. In recent days, play-testing of games via autonomous agents has shown great promise by accelerating and simplifying this process. Reinforcement Learning solutions have the potential of complementing current scripted and automated solutions by learning directly from playing the game without the need of human intervention. This paper presented an approach based on reinforcement learning for automated testing of 3D games. We make use of the notion of curiosity as a motivating factor to encourage an RL agent to explore its environment. The results from our exploratory study are promising and we have preliminary evidence that reinforcement learning can be adopted for automated testing of 3D games.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {211},
numpages = {8},
keywords = {reinforcement learning, functional coverage, game play testing},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3278186.3278191,
author = {Vuong, Thi Anh Tuyet and Takada, Shingo},
title = {A Reinforcement Learning Based Approach to Automated Testing of Android Applications},
year = {2018},
isbn = {9781450360531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278186.3278191},
doi = {10.1145/3278186.3278191},
abstract = {In recent years, researchers have actively proposed tools to automate testing for Android applications. Their techniques, however, still encounter major difficulties. First is the difficulty of achieving high code coverage because applications usually have a large number of possible combinations of operations and transitions, which makes testing all possible scenarios time-consuming and ineffective for large systems. Second is the difficulty of achieving a wide range of application functionalities, because some functionalities can only be reached through a specific sequence of events. Therefore they are tested less often in random testing. Facing these problems, we apply a reinforcement learning algorithm called Q-learning to take advantage of both random and model-based testing. A Q-learning agent interacts with the Android application, builds a behavioral model gradually and generates test cases based on the model. The agent explores the application in an optimal way that reveals as much functionalities of the application as possible. The exploration using Q-learning improves code coverage in comparison to random and model-based testing and is able to detect faults in applications under test.},
booktitle = {Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation},
pages = {31–37},
numpages = {7},
keywords = {Q-Learning, Android, Test Input Generation, Reinforcement Learning},
location = {Lake Buena Vista, FL, USA},
series = {A-TEST 2018}
}

@inproceedings{10.5555/3586210.3586434,
author = {Kov\'{a}cs, Benjamin and Tassel, Pierre and Gebser, Martin and Seidel, Georg},
title = {A Customizable Reinforcement Learning Environment for Semiconductor Fab Simulation},
year = {2023},
publisher = {IEEE Press},
abstract = {Reinforcement learning based methods are increasingly used to solve NP-hard combinatorial optimization problems. By learning from the problem structure, or the characteristics of instances, the approach has high potential compared to alternative techniques solving all instances from scratch. This work introduces a novel framework for creating (deep) reinforcement learning environments simulating up to real-world scale semiconductor fab scheduling problem instances. The highly configurable framework supports creating single- and multi-agent environments where the simulation factory is either partially or fully controlled by the learning agents. The action and observation spaces and the reward function are customizable based on pre-defined features. Our toolkit creates environments with a standard interface that can be integrated with various algorithms in a few minutes. The simulated datasets may involve challenging features like downtimes, batching, rework, and sequence-dependent setups. These can also be turned off and simulated datasets be automatically downscaled during the prototyping phase.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2663–2674},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@article{10.5555/2627435.2627456,
author = {Moore, Brett L. and Pyeatt, Larry D. and Kulkarni, Vivekanand and Panousis, Periklis and Padrez, Kevin and Doufas, Anthony G.},
title = {Reinforcement Learning for Closed-Loop Propofol Anesthesia: A Study in Human Volunteers},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Clinical research has demonstrated the efficacy of closed-loop control of anesthesia using the bispectral index of the electroencephalogram as the controlled variable. These controllers have evolved to yield patient-specific anesthesia, which is associated with improved patient outcomes. Despite progress, the problem of patient-specific anesthesia remains unsolved. A variety of factors confound good control, including variations in human physiology, imperfect measures of drug effect, and delayed, hysteretic response to drug delivery. Reinforcement learning (RL) appears to be uniquely equipped to overcome these challenges; however, the literature offers no precedent for RL in anesthesia. To begin exploring the role RL might play in improving anesthetic care, we investigated the method's application in the delivery of patient-specific, propofol-induced hypnosis in human volunteers. When compared to performance metrics reported in the anesthesia literature, RL demonstrated patient-specific control marked by improved accuracy and stability. Furthermore, these results suggest that RL may be considered a viable alternative for solving other difficult closed-loop control problems in medicine. More rigorous clinical study, beyond the confines of controlled human volunteer studies, is needed to substantiate these findings.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {655–696},
numpages = {42},
keywords = {propofol, closed-loop control, hypnosis, reinforcement learning, anesthesia, bispectral index}
}

