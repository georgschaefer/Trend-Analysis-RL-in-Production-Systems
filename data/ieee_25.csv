"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Energy-Efficient IoT Sensor Calibration With Deep Reinforcement Learning","A. Ashiquzzaman; H. Lee; T. -W. Um; J. Kim","School of Electronics and Computer Engineering, Chonnam National University, Gwangju, South Korea; Human IT Convergence Research Center, Korea Electronics Technology Institute, Seoul, South Korea; Department of Cyber Security, College of Science and Technology, Duksung Women’s University, Seoul, South Korea; School of Electronics and Computer Engineering, Chonnam National University, Gwangju, South Korea","IEEE Access","2 Jun 2020","2020","8","","97045","97055","The modern development of ultra-durable and energy-efficient IoT based communication sensors has much application in modern telecommunication and networking sectors. Sensor calibration to reduce power usage is beneficial to minimizing energy consumption in sensors as well as improve the efficiency of devices. Reinforcement learning (RL) has been received much attention from researchers and now widely applied in many study fields to achieve intelligent automation. Though various types of sensors have been widely used in the field of IoT, rare researches were conducted in resource optimizing. In this novel research, a new style of power conservation has been explored with the help of RL to make a new generation of IoT devices with calibrated power sources to maximize resource utilization. A closed grid multiple power source based control for sensor resource utilization has been introduced. Our proposed model using Deep Q learning (DQN) enables IoT sensors to maximize its resource utilization. This research focuses solely on the energy-efficient sensor calibration and simulation results show promising performance of the proposed method.","2169-3536","","10.1109/ACCESS.2020.2992853","National Research Foundation of Korea (NRF) grant funded by the Korean Government (MSIT)(grant numbers:2018R1A2B2003774); Information Technology Research Center (ITRC); IITP (Institute for Information & Communications Technology Planning & Evaluation)(grant numbers:IITP-2020-2016-0-00314); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9087857","Algorithm design and analysis;optimization;computational and artificial intelligence;battery management systems;simulation;electronic design automation and methodology;deeplearning;reinforcement learning","Deep learning;Reinforcement learning;Resource management;Neural networks;Machine learning algorithms;Calibration","energy conservation;Internet of Things;learning (artificial intelligence);optimisation;telecommunication computing;telecommunication power management;wireless sensor networks","RL;IoT devices;calibrated power sources;closed grid multiple power source;sensor resource utilization;deep Q learning;IoT sensors;energy-efficient sensor calibration;energy-efficient IoT sensor calibration;modern development;communication sensors;modern telecommunication;networking sectors;power usage;energy consumption;study fields;rare researches;resource optimizing;power conservation;deep reinforcement learning","","10","","36","CCBY","6 May 2020","","","IEEE","IEEE Journals"
"Federated Reinforcement Learning-Based Resource Allocation for D2D-Aided Digital Twin Edge Networks in 6G Industrial IoT","Q. Guo; F. Tang; N. Kato","Graduate School of Information Sciences, Tohoku University, Sendai, Japan; School of Computer Science and Engineering, Central South University, ChangSha, China; Graduate School of Information Sciences, Tohoku University, Sendai, Japan","IEEE Transactions on Industrial Informatics","4 May 2023","2023","19","5","7228","7236","The sixth generation (6G) is conceived to address the expected high level of requirements (such as ultra-high-data-transmission rate, support for the highest moving speed and seamless connection, etc.) in the next decade and beyond. In the context of 6G, a large number of Industrial Internet of Things (IoT) (IIoT) devices may access the network, and thanks to the rapid development of artificial intelligence make smart manufacturing has the opportunity to be realized. However, a large number of IoT devices, the tremendous volume of data, the heterogeneous nature of devices, and the increasing concerns of privacy challenge the efficient management and quality of services in IIoT. To address these problems, in this article, a device-to-device (D2D) communication-aided digital twin edge network is proposed, where edge computing is introduced to bring computing and storage resources near to the end devices, and digital twin is utilized to fill the gap between physical and virtual space and D2D communication is applied to assist resource limited IoT devices to achieve normal communication. Moreover, digital twin-empowered federated reinforcement learning is leveraged to provide privacy awareness and decentralized resource allocation strategy training on D2D communication links to further improve network performance. The simulation results show that the proposal achieves significant network performance compared with baseline algorithms.","1941-0050","","10.1109/TII.2022.3227655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9976231","Device-to-device (D2D) communication;digital twin (DT);edge computing;federated learning (FL);industrial Internet of Things (IIoT);resource allocation;sixth generation (6G)","Industrial Internet of Things;Device-to-device communication;Resource management;Digital twins;6G mobile communication;Edge computing;Training","6G mobile communication;artificial intelligence;blockchains;computer network security;data privacy;digital twins;edge computing;Internet of Things;mobile radio;production engineering computing;reinforcement learning;resource allocation","6G Industrial IoT;D2D-;device-to-device communication-aided digital twin edge network;digital twin edge networks;edge computing;federated reinforcement learning-based resource allocation;highest moving speed;IIoT;improve network performance;IoT devices;resource allocation strategy training;significant network performance;storage resources;Things devices;twin-empowered federated reinforcement learning;ultra-high-data-transmission rate","","1","","29","IEEE","8 Dec 2022","","","IEEE","IEEE Journals"
"Optimizing Nitrogen Management with Deep Reinforcement Learning and Crop Simulations","J. Wu; R. Tao; P. Zhao; N. F. Martin; N. Hovakimyan","University of Illinois at Urbana-Champaign, Urbana, IL, USA; University of Illinois at Urbana-Champaign, Urbana, IL, USA; University of Illinois at Urbana-Champaign, Urbana, IL, USA; University of Illinois at Urbana-Champaign, Urbana, IL, USA; University of Illinois at Urbana-Champaign, Urbana, IL, USA","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","23 Aug 2022","2022","","","1711","1719","Nitrogen (N) management is critical to sustain soil fertility and crop production while minimizing the negative environmental impact, but is challenging to optimize. This paper proposes an intelligent N management system using deep reinforcement learning (RL) and crop simulations with Decision Support System for Agrotechnology Transfer (DSSAT). We first formulate the N management problem as an RL problem. We then train management policies with deep Q-network and soft actor-critic algorithms, and the Gym-DSSAT interface that allows for daily interactions between the simulated crop environment and RL agents. According to the experiments on the maize crop in both Iowa and Florida in the US, our RL-trained policies outperform previous empirical methods by achieving higher or similar yield while using less fertilizers.","2160-7516","978-1-6654-8739-9","10.1109/CVPRW56347.2022.00178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857484","","Decision support systems;Training;Crops;Reinforcement learning;Soil;Data models;Nitrogen","agricultural engineering;crops;decision support systems;deep learning (artificial intelligence);nitrogen;production engineering computing","deep reinforcement learning;soil fertility;negative environmental impact;intelligent N management system;decision support system;agrotechnology transfer;deep Q-network;simulated crop environment;fertilizers;nitrogen management system;Florida;N","","4","","31","IEEE","23 Aug 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Routing in IP Media Broadcast Networks: Feasibility and Performance","P. Amaral; D. Simões","Departamento de Engenharia Electrotécnica e de Computadores, Faculdade de Ciências e Tecnologia (FCT), Universidade Nova de Lisboa, Caparica, Portugal; Departamento de Engenharia Electrotécnica e de Computadores, Faculdade de Ciências e Tecnologia (FCT), Universidade Nova de Lisboa, Caparica, Portugal","IEEE Access","20 Jun 2022","2022","10","","62459","62470","The media broadcast industry has evolved from Serial Digital Interface (SDI) based infrastructures to IP networks. While IP based video broadcast is well established in the data plane, the use of IP networks to transport media flows still poses challenges in terms of resource management and orchestration. Software Defined Networking (SDN) based orchestration architectures have emerged in the industry that use SDN to route the media flows of a broadcast service across the provider IP network. Several approaches to multimedia flow routing in IP based SDN networks have been proposed in the context of streaming applications over the Internet. These range from model based linear optimization solutions that have high complexity to simple shortest path based routing with either Static Link Costs (SLC) or Dynamic Link Costs (DLC). More recently model-free optimization methods such as Deep Reinforcement Learning (DRL) have been proposed for routing and Traffic Engineering (TE) of multimedia flows in SDN networks. The media broadcast scenario however has specific requirements, with services like Master Control Room (MCR) operation and live broadcasting of events, and it has been rarely addressed in the literature. In this work we propose a DRL based routing method for this scenario and compare it to SLC and DLC algorithms based on Dijkstra shortest paths. This is, to our knowledge, the first work to follow this approach in the context of media broadcast services in IP infrastructures. The algorithm is designed considering the specifications and capabilities of one of the leading SDN orchestrators in the market and considers the more common Service Level Agreement (SLA) requirements in the industry. Three different DRL algorithms are implemented and compared and we evaluate them using a real service provider network topology. The results indicate that DRL based routing is applicable in real production scenarios and that it achieves considerable performance gains when compared to the SLC and DLC shortest path algorithms commonly used today.","2169-3536","","10.1109/ACCESS.2022.3182009","Fundação para a Ciência e Tecnologia/Ministério da Ciência Tecnologia e Ensino Superior (FCT/MCTES) through National Funds and When Applicable Co-Funded EU Funds(grant numbers:UIDB/50008/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793657","Media broadcast networks;artificial intelligence;deep reinforcement learning;network orchestration;routing;software defined networks","Routing;Media;Heuristic algorithms;IP networks;Costs;Optimization;Network topology","broadcast communication;Internet;IP networks;multimedia communication;optimisation;quality of service;reinforcement learning;software defined networking;telecommunication network routing;telecommunication network topology;telecommunication traffic","deep reinforcement;media broadcast industry;Serial Digital Interface based infrastructures;IP networks;video broadcast;media flows;resource management;broadcast service;provider IP network;multimedia flow;SDN networks;simple shortest path;Static Link Costs;SLC;Dynamic Link Costs;model-free optimization methods;Deep Reinforcement;media broadcast scenario;DRL based routing;media broadcast services;IP infrastructures;leading SDN orchestrators;common Service Level Agreement requirements;service provider network topology;DLC shortest path algorithms;software defined networking based orchestration architectures;IP media","","2","","29","CCBY","10 Jun 2022","","","IEEE","IEEE Journals"
"Sim-to-Real Transfer for Visual Reinforcement Learning of Deformable Object Manipulation for Robot-Assisted Surgery","P. M. Scheikl; E. Tagliabue; B. Gyenes; M. Wagner; D. Dall'Alba; P. Fiorini; F. Mathis-Ullrich","Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Computer Science, University of Verona, Verona, Italy; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Department for General, Visceral and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany; Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany","IEEE Robotics and Automation Letters","20 Dec 2022","2023","8","2","560","567","Automation holds the potential to assist surgeons in robotic interventions, shifting their mental work load from visuomotor control to high level decision making. Reinforcement learning has shown promising results in learning complex visuomotor policies, especially in simulation environments where many samples can be collected at low cost. A core challenge is learning policies in simulation that can be deployed in the real world, thereby overcoming the sim-to-real gap. In this letter, we bridge the visual sim-to-real gap with an image-based reinforcement learning pipeline based on pixel-level domain adaptation and demonstrate its effectiveness on an image-based task in deformable object manipulation. We choose a tissue retraction task because of its importance in clinical reality of precise cancer surgery. After training in simulation on domain-translated images, our policy requires no retraining to perform tissue retraction with a 50% success rate on the real robotic system using raw RGB images. Furthermore, our sim-to-real transfer method makes no assumptions on the task itself and requires no paired images. This letter introduces the first successful application of visual sim-to-real transfer for robotic manipulation of deformable objects in the surgical field, which represents a notable step towards the clinical translation of cognitive surgical robotics.","2377-3766","","10.1109/LRA.2022.3227873","Karlsruhe House of Young Scientists; Helmholtz Association; European Research Council(grant numbers:742671); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9976185","Computer vision for medical robotics;reinforcement learning;surgical robotics: Laparoscopy","Task analysis;Training;Robots;Visualization;Grippers;Surgery;Medical robotics","cancer;cognition;decision making;image colour analysis;image sensors;manipulators;reinforcement learning;robot vision;surgical robots","cognitive surgical robotics;complex visuomotor policies;core challenge;deformable object manipulation;deformable objects;domain-translated images;high level decision making;image-based reinforcement learning pipeline;image-based task;mental work load;paired images;pixel-level domain adaptation;precise cancer surgery;raw RGB images;robot-assisted surgery;robotic interventions;robotic manipulation;robotic system;sim-to-real gap;sim-to-real transfer method;simulation environments;tissue retraction task;visual reinforcement learning;visuomotor control","","3","","30","IEEE","8 Dec 2022","","","IEEE","IEEE Journals"
"Performance Optimization for Blockchain-Enabled Industrial Internet of Things (IIoT) Systems: A Deep Reinforcement Learning Approach","M. Liu; F. R. Yu; Y. Teng; V. C. M. Leung; M. Song","Beijing Key Laboratory of Space-ground Interconnection and Convergence, Beijing University of Posts and Telecommunications, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Beijing Key Laboratory of Space-ground Interconnection and Convergence, Beijing University of Posts and Telecommunications, Beijing, China; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Beijing Key Laboratory of Space-ground Interconnection and Convergence, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Industrial Informatics","13 Jun 2019","2019","15","6","3559","3570","Recent advances in the industrial Internet of things (IIoT) provide plenty of opportunities for various industries. To address the security and efficiency issues of the massive IIoT data, blockchain is widely considered as a promising solution to enable data storing/processing/sharing in a secure and efficient way. To meet the high throughput requirement, this paper proposes a novel deep reinforcement learning (DRL)-based performance optimization framework for blockchain-enabled IIoT systems, the goals of which are threefold: 1) providing a methodology for evaluating the system from the aspects of scalability, decentralization, latency, and security; 2) improving the scalability of the underlying blockchain without affecting the system's decentralization, latency, and security; and 3) designing a modulable blockchain for IIoT systems, where the block producers, consensus algorithm, block size, and block interval can be selected/adjusted using the DRL technique. Simulations results show that our proposed framework can effectively improve the performance of blockchain-enabled IIoT systems and well adapt to the dynamics of the IIoT.","1941-0050","","10.1109/TII.2019.2897805","National Key R&D Program of China(grant numbers:2018YFB1201500); National Natural Science Foundation of China(grant numbers:61771072); Beijing Natural Science Foundation(grant numbers:L171011); Beijing Major Science and Technology Special Projects(grant numbers:Z181100003118012); China Scholarship Council(grant numbers:201706470059); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636255","Blockchain;deep reinforcement learning (DRL);industrial Internet of Things (IIoT);performance optimization","Blockchain;Security;Scalability;Optimization;Internet of Things;Throughput;Performance analysis","cryptography;distributed databases;Internet of Things;learning (artificial intelligence);neural nets;optimisation;production engineering computing","blockchain-enabled IIoT systems;data storing;blockchain-enabled industrial Internet of Things systems;deep reinforcement learning;performance optimization;data processing;data sharing;DRL","","212","","52","IEEE","6 Feb 2019","","","IEEE","IEEE Journals"
"Cloud–Edge Collaborative SFC Mapping for Industrial IoT Using Deep Reinforcement Learning","S. Xu; Y. Li; S. Guo; C. Lei; D. Liu; X. Qiu","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Information and Communication Academy, State Grid Information and Telecommunication Group, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Industrial Informatics","21 Feb 2022","2022","18","6","4158","4168","The industrial Internet of Things (IIoT) and 5G have been served as the key elements to support the reliable and efficient operation of Industry 4.0. By integrating burgeoning network function virtualization (NFV) technology with cloud computing and mobile edge computing, an NFV-enabled cloud–edge collaborative IIoT architecture can efficiently provide flexible service for the massive IIoT traffic in the form of a service function chain (SFC). However, the efficient cloud–edge collaboration, the reasonable comprehensive resource consumption, and different quality of services are still key problems to be solved. Thus, to balance the quality of IIoT services, as well as computational and communicational resource consumption, a multiobjective SFC deployment model is designed to characterize the diverse service requirements and specific network environment for the IIoT. Then, a deep-$Q$-learning-based online SFC deployment algorithm is presented, which can efficiently learn the relationship between the SFC deployment scheme and its performance through the iterative training. Simulation results demonstrate that our proposed approach outperforms others in balancing the resource consumption, accepting more SFC requests, as well as providing differentiated services for delay-sensitive IIoT traffic and resource-intensive IIoT traffic.","1941-0050","","10.1109/TII.2021.3113875","National Key R&D Program of China(grant numbers:2020YFB2104503); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9541048","Deep  $Q$ -learning (DQL);industrial Internet of Things (IIoT);mobile edge computing (MEC);network function virtualization (NFV);5G","Industrial Internet of Things;Cloud computing;Delays;Servers;Computer architecture;Quality of service;5G mobile communication","5G mobile communication;cloud computing;Internet of Things;iterative methods;production engineering computing;quality of service;reinforcement learning;virtualisation","resource-intensive IIoT traffic;delay-sensitive IIoT traffic;differentiated services;SFC requests;SFC deployment scheme;SFC deployment algorithm;specific network environment;diverse service requirements;multiobjective SFC deployment model;communicational resource consumption;computational resource consumption;IIoT services;reasonable comprehensive resource consumption;efficient cloud-edge collaboration;service function chain;massive IIoT traffic;flexible service;NFV-enabled cloud-edge collaborative IIoT architecture;mobile edge computing;cloud computing;burgeoning network function virtualization;reliable operation;deep reinforcement learning;industrial IoT;cloud-edge collaborative SFC mapping","","10","","36","IEEE","20 Sep 2021","","","IEEE","IEEE Journals"
"Continuous Reinforcement Learning With Knowledge-Inspired Reward Shaping for Autonomous Cavity Filter Tuning","Z. Wang; Y. Ou; X. Wu; W. Feng","Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Yongsheng Ou and Xinyu Wu are with Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Yongsheng Ou and Xinyu Wu are with Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China","2018 IEEE International Conference on Cyborg and Bionic Systems (CBS)","17 Jan 2019","2018","","","53","58","Reinforcement Learning has achieved a great success in recent decades when applying to the fields such as finance, robotics, and multi-agent games. A variety of traditional manual tasks are facing upgrading, and reinforcement learning opens the door to a whole new world for improving these tasks. In this paper, we focus on the task called Cavity Filter Tuning, a traditionally manual work in communication industries which not only consumes time, but also highly depends on human knowledge. We present a framework based on Deep Deterministic Policy Gradient for automatically tuning cavity filters, and design appropriate reward functions inspired by human expertise in the tuning task. Simulation experiments are conducted to validate the applicability of our algorithm. Our proposed method is able to autonomously tune a detuned filter to meet the design specifications from any random starting positions.","","978-1-5386-7355-3","10.1109/CBS.2018.8612197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8612197","","Tuning;Cavity resonators;Scattering parameters;Fasteners;Reinforcement learning;Task analysis;Loss measurement","cavity resonator filters;learning (artificial intelligence);microwave filters;multi-agent systems;production engineering computing","reward functions;random starting position;detuned filter;tuning task;cavity filters;Deep Deterministic Policy Gradient;multiagent games;reinforcement learning;autonomous cavity filter tuning;knowledge-inspired reward;continuous reinforcement","","9","1","14","IEEE","17 Jan 2019","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Cooperative Partial Task Offloading and Resource Allocation for IIoT Applications","F. Zhang; G. Han; L. Liu; M. Martínez-García; Y. Peng","College of Internet of Things Engineering, Hohai University, Changzhou, China; College of Internet of Things Engineering, Hohai University, Changzhou, China; College of Internet of Things Engineering, Hohai University, Changzhou, China; Department of Aeronautical and Automotive Engineering, Loughborough University, Loughborough, U.K.; Artificial Intelligence Institute, Shanghai University, Shanghai, China","IEEE Transactions on Network Science and Engineering","20 Sep 2023","2023","10","5","2991","3006","The Industrial Internet of Things (IIoT) has been regarded as one of the pillars supporting the conceptual paradigm of the Industry 4.0. Compared with traditional cloud computing schemes, edge computing provides an effective solution towards easing congestion in backhaul links and core networks, while meeting real-time, security and reliability demands of compute-intensive and delay-sensitive IIoT applications. Many existing studies only optimize end-edge-cloud cooperative task offloading, and neglect the optimization of the communication and computation resource allocation. In this paper, the cooperative partial task offloading and resource allocation (CPTORA) framework is designed, which jointly considers cooperation among various IIoT devices, local edge computing servers (ECSs), non-local ECSs, and cloud computing servers – for balancing the workload of the ECSs and increasing the resource utilization rate. Then, considering the complex dynamics and unpredictability in IIoT environments, the joint optimization problem is modeled as a constrained Markov decision process. Furthermore, we propose an improved soft actor-critic-based CPTORA (ISAC-CPTORA) algorithm, able to make task offloading and resource allocation decisions for each IIoT device. This algorithm innovatively introduces the idea of distributional reinforcement learning to the soft actor-critic, which can effectively reduce Q-value overestimations or underestimations. Meanwhile, this algorithm employs the prioritized experience replay to enhance its learning efficiency. Extensive laboratory experiments indicate that our CPTORA framework and ISAC-CPTORA algorithm efficiently decrease the total system costs (i.e., latency costs and energy costs), in contrast to various baseline frameworks and algorithms.","2327-4697","","10.1109/TNSE.2022.3167949","National Key Research and Development Program of China(grant numbers:2017YFE0125300); National Natural Science Foundation of China; Guangdong Joint Fund(grant numbers:U1801264); Jiangsu Provincial Key Research and Development Program(grant numbers:BE2019648); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9760005","Cooperative partial offloading;IIoT;improved soft actor-critic;resource allocation","Task analysis;Industrial Internet of Things;Resource management;Optimization;Costs;Edge computing;Delays","cloud computing;deep learning (artificial intelligence);edge computing;Internet of Things;Markov processes;mobile computing;optimisation;production engineering computing;reinforcement learning;resource allocation","backhaul links;baseline frameworks;computation resource allocation;conceptual paradigm;constrained Markov decision process;core networks;CPTORA framework;deep reinforcement learning;delay-sensitive IIoT applications;distributional reinforcement;ECS;edge computing;end-edge-cloud cooperative task offloading;IIoT device;IIoT environments;industrial Internet of things;ISAC-CPTORA algorithm;joint optimization problem;learning efficiency;partial task offloading;resource allocation decisions;resource utilization rate;security;traditional cloud computing schemes","","4","","48","IEEE","19 Apr 2022","","","IEEE","IEEE Journals"
"Nexus of Deep Reinforcement Learning and Leader–Follower Approach for AIoT Enabled Aerial Networks","G. Raja; S. Essaky; A. Ganapathisubramaniyan; Y. Baskar","NGNLab, Department of Computer Technology, Anna University, MIT Campus, Chennai, India; NGNLab, Department of Computer Technology, Anna University, MIT Campus, Chennai, India; Foreign Exchange and Local Markets Group, Citicorp Services India Private Ltd., Chennai, India; NGNLab, Department of Computer Technology, Anna University, MIT Campus, Chennai, India","IEEE Transactions on Industrial Informatics","13 Jul 2023","2023","19","8","9165","9172","The Industrial Internet of Things (IIoT) is a new industrial 4.0 paradigm that combines IoT, robotics, cyber-physical systems, and other future industrial advancements. Unmanned aerial vehicles (UAVs), part of the IIoT infrastructure, have a significant potential for civil and military purposes. Through the artificial intelligence of things (AIoT), a well-organized group of UAVs outperforms a single large UAV in terms of device scalability, maintenance, and expense. Therefore, the UAV swarm with industry 4.0 intelligence can be used for a wide range of 24/7 security and remote monitoring applications. Though multi-UAV systems are beneficial, their application has many challenges. There is a high risk of collision in the multi-UAV system without coordination. This article proposes an AIoT-based navigation and formation control (AIoT-NFC) mechanism to scale down the collision risk by combining deep reinforcement learning (DRL) with the leader–follower approach. In AIoT-NFC, a deep deterministic policy gradient (DDPG) based algorithm is proposed to navigate UAVs in remote surveillance without colliding with obstacles and other UAVs. Furthermore, the AIoT-NFC system incorporates a fault tolerance mechanism that can handle the scenario of a leader's failure due to actuator malfunction. Experimental results show that the AIoT-NFC achieves faster convergence with a lower collision rate. AIoT-NFC reduced the collision rate by 14.99% compared to existing navigation methods in successful formation without colliding with the other UAVs.","1941-0050","","10.1109/TII.2022.3226529","NGNLab, Department of Computer Technology, Anna University, Chennai, India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9969936","Artificial Intelligence of Things (AIoT);deep reinforcement learning (DRL);industrial internet of things (IIoT);navigation and formation control (NFC);remote surveillance;unmanned aerial vehicles (UAV)","Navigation;Autonomous aerial vehicles;Surveillance;Industrial Internet of Things;Monitoring;Mathematical models;Collision avoidance","actuators;aircraft navigation;artificial intelligence;autonomous aerial vehicles;collision avoidance;cyber-physical systems;deep learning (artificial intelligence);fault tolerance;gradient methods;Internet of Things;mobile robots;multi-robot systems;production engineering computing;reinforcement learning","AIoT enabled aerial networks;AIoT-based navigation and formation control;AIoT-NFC system;Artificial Intelligence of Things;collision risk;cyber-physical systems;cyberphysical systems;DDPG based algorithm;deep deterministic policy gradient based algorithm;deep reinforcement learning;fault tolerance mechanism;IIoT infrastructure;Industrial 4.0 intelligence;Industrial Internet of Things;leader-follower approach;multiUAV system;remote monitoring applications;robotics;UAV navigation;UAV swarm;unmanned aerial vehicles","","","","33","IEEE","5 Dec 2022","","","IEEE","IEEE Journals"
"Internet of Things Communication protocols optimization using Blockchain Technology integrated with Reinforcement Learning","M. Kumari; D. M. Gaikwad; D. S. A. Chavhan","ETC Dept, G H Raisoni University; G H Raisoni College of Engineering; GWCET","2022 International Conference on Emerging Trends in Engineering and Medical Sciences (ICETEMS)","11 Apr 2023","2022","","","441","447","Under the IoT vision, conventional items become sophisticated and self-contained. This ideal has become an actuality due to its technological breakthroughs, although there are still challenges to face. Especially in the field of security, like data accuracy. All academia and commerce are curious about the combined study of block chain and computational modelling (ML) because it may offer significant advantages for achieving decentralized, safe, intelligent, and advanced information management operations and administration. Considering the expected IoT will have to establish relationships in this massive incoming data stream as it evolves in the coming years Data sharing will change as a result of the crucial software known as the blockchain. A scientific innovation that can transform several industries, such as the Internet of Things, is the capacity to make connections within dispersed systems without the need for power. As a result, the controller operates well and may be modified to fit into different, dynamic contexts. Additionally, since greater-level systems are taught with deep reinforcement learning, the machine can expect to study even after it is put into use, which makes it perfect for practical uses. Based on their personal experience with the employment of an agent, learning through Reinforcement: S (State), A (Action), and R (Reaction) are the parameters (Reward Under the IoT vision, ordinary items become clever and self-contained. This paper investigates this relationship, analyses issues with blockchain and IoT systems, and assesses the most pertinent studies in order to establish how blockchain with reinforcement learning may enhance IoT. Depending on the finding, the study then suggests using supervised learning techniques to address some of the major problems faced by blockchain-enabled IIoT systems, such as block time reduction and operations throughput development. There will be a thorough case study that demonstrates how a Q-learning strategy can be used to minimize latency problems for a miner and hence lower the likelihood of forking events.","","978-1-6654-6112-2","10.1109/ICETEMS56252.2022.10093387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10093387","Internet of Thing(IOT);Blockchain;Reinforcement Learning","Technological innovation;Webcams;Supervised learning;Transforms;Big Data;Throughput;Blockchains","blockchains;computer network security;deep learning (artificial intelligence);information management;Internet of Things;production engineering computing;protocols;reinforcement learning;supervised learning","block time reduction;blockchain technology;blockchain-enabled IIoT systems;computational modelling;crucial software;deep reinforcement learning;dynamic contexts;greater-level systems;information management operations;Internet of Things communication protocols optimization;IoT systems;IoT vision;massive incoming data stream;ML;ordinary items;Q-learning strategy;reinforcement learning;scientific innovation;supervised learning techniques","","","","89","IEEE","11 Apr 2023","","","IEEE","IEEE Conferences"
"When Moving Target Defense Meets Attack Prediction in Digital Twins: A Convolutional and Hierarchical Reinforcement Learning Approach","T. Zhang; C. Xu; Y. Lian; H. Tian; J. Kang; X. Kuang; D. Niyato","School of Software Engineering, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Automation, Guangdong University of Technology, Guangzhou, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Journal on Selected Areas in Communications","","2023","PP","99","1","1","With rapid development of emerging technologies for Internet of Things (IoT), digital twins (DT) have been proposed to support a wide variety of applications. A mobile network is expected to be integrated with DT to form a DT mobile network (DTMN). Unfortunately, DTMN still faces security threats, which have attracted great research attention. Current defense mechanisms are mostly static, i.e., responding after attacks happening. To solve the aforementioned problem, moving target defense (MTD) has been proposed as an innovative solution. However, there exist three major challenges when applying MTD into DTMN. Firstly, less emphasis was paid to collaborative scheduling between multiple MTD schemes, which can improve the security of DTMN. Secondly, MTD schemes require lots of network resources, but few works focus on the time allocation of multiple MTD schemes to reduce network resource consumption. Thirdly, existing defense strategies only rely on current information, but do not consider future information. In this paper, we propose a collaborative mutation-based MTD (CM-MTD) in DTMN. We mainly consider two MTD schemes called host address mutation (HAM) and route mutation (RM), respectively, which adjust network properties and invalidate different stages of cyber kill chain. We firstly formulate a semi-Markov decision process (SMDP) to model time-varying security events and dynamic deployment of multiple MTD schemes. Then, security events are predicted by long short-term memory (LSTM), which are regarded as network states in SMDP. Next, infeasible actions that do not satisfy network constraints will be removed from the action space of the SMDP. Lastly, we design a hierarchical deep reinforcement learning algorithm for collaborative scheduling. Simulation results highlight the effectiveness of CM-MTD compared with baseline solutions.","1558-0008","","10.1109/JSAC.2023.3310072","National Natural Science Foundation of China(grant numbers:62225105,61871048,62102099,U22A2054); Advanced Research Program in the 14th Five-year Plan(grant numbers:JZX6Y202211010822); Guangzhou Basic Research Program(grant numbers:2023A04J1699); National Research Foundation; Future Communications Research & Development Programme; DSO National Laboratories under the AI Singapore Programme(grant numbers:AISG2-RP-2020-019); Energy Research Test-Bed and Industry Partnership Funding Initiative, Energy Grid (EG) 2.0 programme, DesCartes and the Campus for Research Excellence and Technological Enterprise (CREATE) programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234402","Digital Twins;Moving Target Defense;Attack Prediction;Hierarchical Deep Reinforcement Learning","Security;IP networks;Collaboration;Switches;Servers;Reinforcement learning;Laboratories","","","","","","","IEEE","30 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Online Partial Offloading and Task Scheduling in SDN-Fog Networks With Deep Recurrent Reinforcement Learning","J. Baek; G. Kaddoum","Department of Electrical Engineering, École de Technologie Supérieure, Montreal, QC, Canada; Department of Electrical Engineering, École de Technologie Supérieure, Montreal, QC, Canada","IEEE Internet of Things Journal","17 Jun 2022","2022","9","13","11578","11589","Smart industries enabling automation and data exchange in manufacturing technologies demanding real-time processing, nearby storage, and reliability, all of which can be satisfied by the fog computing architecture. With the emergence of smart devices coupled with a diverse range of application requirements, it is essential to have an intelligent fog network where intelligence is spread across all network segments, taking network nodes self-aware and self-decision making. In fog networks, an optimal distribution decision faces challenges due to uncertainties associated with user workload and available resources at the fog nodes and also the wide range of node’s computing power. Given this challenge, a computational offloading and CPU resource scheduling method for minimizing energy consumption is proposed. To investigate the characteristics for offloading and optimizing their allocation, we consider two types of tasks, namely, offloadable and nonoffloadable tasks. The independent fog nodes adopt the same strategy without prior knowledge of the dynamic statistics and global observations, aiming to maximize a common goal with cooperative behaviors. Then, the deep recurrent  $Q$ -network (DRQN) is applied to deal with the partial-observability from limited information. The proposed DRQN-based method requires comparatively less computational complexity than the conventional  $Q$ -learning algorithm. The simulation results show that the proposed method can effectively deal with both transmission and CPU energy consumptions while guaranteeing convergence in a limited time.","2327-4662","","10.1109/JIOT.2021.3130474","Natural Sciences and Engineering Research Council of Canada (NSERC)(grant numbers:RDCPJ 501617-16); Fonds de recherche du Québec under Doctoral Research Scholarships 2020–2021; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9626146","Computation offloading;deep Q-network (DQN);deep recurrent Q-network (DRQN);deep-reinforcement learning;fog computing;Industrial IoT (IIoT);resource scheduling;software-defined networking (SDN)","Task analysis;Computational modeling;Energy consumption;Resource management;Processor scheduling;Edge computing;Delays","deep learning (artificial intelligence);distributed processing;Internet of Things;recurrent neural nets;reinforcement learning;resource allocation;software defined networking;telecommunication scheduling","online partial offloading;task scheduling;SDN-fog networks;deep recurrent reinforcement learning;data exchange;manufacturing technologies;real-time processing;nearby storage;fog computing architecture;smart devices;application requirements;intelligent fog network;network segments;network nodes self-aware;self-decision making;optimal distribution decision;user workload;computational offloading;CPU resource scheduling method;energy consumption;offloadable tasks;nonoffloadable tasks;independent fog nodes;deep recurrent Q-network;partial-observability;DRQN-based method;computational complexity;conventional Q-learning algorithm","","13","","38","IEEE","24 Nov 2021","","","IEEE","IEEE Journals"
"A Heuristically Assisted Deep Reinforcement Learning Approach for Network Slice Placement","J. J. Alves Esteves; A. Boubendir; F. Guillemin; P. Sens","Orange Labs, Chatilon, France; Orange Labs, Chatilon, France; Orange Labs, Chatilon, France; LIP6—Inria, Sorbonne University, Paris, France","IEEE Transactions on Network and Service Management","1 Feb 2023","2022","19","4","4794","4806","Network Slice placement with the problem of allocation of resources from a virtualized substrate network is an optimization problem which can be formulated as a multi-objective Integer Linear Programming (ILP) problem. However, to cope with the complexity of such a continuous task and seeking for optimality and automation, the use of Machine Learning (ML) techniques appear as a promising approach. We introduce a hybrid placement solution based on Deep Reinforcement Learning (DRL) and a dedicated optimization heuristic based on the “Power of Two Choices” principle. The DRL algorithm uses the so-called Asynchronous Advantage Actor Critic (A3C) algorithm for fast learning, and Graph Convolutional Networks (GCN) to automate feature extraction from the physical substrate network. The proposed Heuristically-Assisted DRL (HA-DRL) allows for the acceleration of the learning process and substantial gain in resource usage when compared against other state-of-the-art approaches, as evidenced by evaluation results.","1932-4537","","10.1109/TNSM.2021.3132103","European Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9632824","Network slicing;optimization;automation;deep reinforcement learning;placement;large scale","Optimization;Convergence;Training;Reinforcement learning;Feature extraction;Bandwidth;Random access memory","5G mobile communication;deep learning (artificial intelligence);feature extraction;integer programming;learning (artificial intelligence);linear programming;reinforcement learning;telecommunication computing;telecommunication networks;virtualisation","5G networks;A3C algorithm;asynchronous advantage actor critic algorithm;dedicated optimization heuristic;feature extraction;GCN;graph convolutional networks;HA-DRL;heuristically assisted deep reinforcement learning;heuristically-assisted DRL;hybrid placement solution;multiobjective integer linear programming problem;network function virtualization;network slice placement;NFV;physical substrate network;telecommunications network;virtualized substrate network","","10","","35","IEEE","2 Dec 2021","","","IEEE","IEEE Journals"
"Cost Optimization at Early Stages of Design Using Deep Reinforcement Learning","L. Servadei; J. Zheng; J. Arjona-Medina; M. Werner; V. Esen; S. Hochreiter; W. Ecker; R. Wille",Johannes Kepler University Linz; Technical University of Munich; Johannes Kepler University Linz; Technical University of Munich; Infineon Technologies AG; Johannes Kepler University Linz; Technical University of Munich; Software Competence Center Hagenberg GmbH (SCCH),"2020 ACM/IEEE 2nd Workshop on Machine Learning for CAD (MLCAD)","9 Apr 2021","2020","","","37","42","With the increase in the complexity of the modern System on Chips (SoCs) and the demand for a lower time-to-market, automation becomes essential in hardware design. This is particularly relevant in complex/time-consuming tasks, as the optimization of design cost for a hardware component. Design cost, in fact, may depend on several objectives, as for the hardware-software trade-off. Given the complexity of this task, the designer often has no means to perform a fast and effective optimization-in particular for larger and complex designs. In this paper, we introduce Deep Reinforcement Learning (DRL) for design cost optimization at the early stages of the design process. We first show that DRL is a perfectly suitable solution for the problem at hand. Afterwards, by means of a Pointer Network, a neural network specifically applied for combinatorial problems, we benchmark three DRL algorithms towards the selected problem. Results obtained in different settings show the improvements achieved by DRL algorithms compared to conventional optimization methods. Additionally, by using reward redistribution proposed in the recently introduced RUDDER method, we obtain significant improvements in complex designs.","","978-1-4503-7519-1","10.1145/3380446.3430619","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9394673","Machine Learning;Reinforcement Learning;Design Automation;Hardware-Software co-design","Neural networks;Reinforcement learning;Benchmark testing;Hardware;Complexity theory;Task analysis;Optimization","circuit complexity;circuit optimisation;deep learning (artificial intelligence);integrated circuit design;system-on-chip","time-to-market;hardware design;hardware component;hardware-software trade-off;design cost optimization;DRL algorithms;deep reinforcement learning;system on chips;SoC;pointer network;neural network;RUDDER method","","1","","17","","9 Apr 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Cyber-Physical Systems","X. Liu; H. Xu; W. Liao; W. Yu","Dept. of Computer and Information Sciences, Towson University, Towson, MD, USA; Dept. of Computer and Information Sciences, Towson University, Towson, MD, USA; Dept. of Computer and Information Sciences, Towson University, Towson, MD, USA; Dept. of Computer and Information Sciences, Towson University, Towson, MD, USA","2019 IEEE International Conference on Industrial Internet (ICII)","19 May 2020","2019","","","318","327","Cyber-Physical Systems (CPS), including smart industrial manufacturing, smart transportation, and smart grids, among others, are envisioned to convert traditionally isolated automated critical systems into modern interconnected intelligent systems via interconnected human, system, and physical assets, as well as providing significant economic and societal benefits. The characteristics of CPS include complexity, dynamic variability, and heterogeneity, arising from interactions between cyber and physical subsystems. These characteristics introduce critical challenges in addition to existing and vital safety and reliability requirements from traditional critical systems. To overcome these challenges, Artificial Intelligence (AI) and Machine Learning (ML) schemes, which have proven effective in numerous fields (robotics, automation, prediction, etc.), can be leveraged as solutions for CPS. In particular, reinforcement learning can make precise decisions automatically to maximize cumulative reward via systematic trial and error in an unknown environment. Yet, challenges still remain for integrating complex reinforcement learning systems with dynamic and diverse CPS domains. In this paper, we conduct a thorough investigation of existing research on reinforcement learning for CPS, and propose a framework for future research. In addition, we carry out two case studies on reinforcement learning in transportation CPS and industrial CPS to validate the effectiveness of reinforcement learning in targeted applications. Using realistic simulation platforms, we validate the effectiveness of reinforcement learning for decision making in routing for transportation CPS and production control for industrial CPS. Finally, we outline some future research challenges that remain.","","978-1-7281-2977-8","10.1109/ICII.2019.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065113","Reinforcement-learning,-Cyber-Physical-Systems,-Internet-of-Things","Learning (artificial intelligence);Computational modeling;Robots;Transportation;Routing;Machine learning;Process control","cyber-physical systems;decision making;learning (artificial intelligence);optimisation","reinforcement learning;cyber-physical systems;physical assets;physical subsystems;safety requirements;reliability requirements;complex reinforcement;dynamic CPS domains;diverse CPS domains;transportation CPS;industrial CPS;automated critical systems;interconnected intelligent systems;artificial intelligence;cumulative reward maximization;machine learning;realistic simulation platforms;routing decision making;production control","","11","","88","IEEE","19 May 2020","","","IEEE","IEEE Conferences"
"SCHEMA: Service Chain Elastic Management with Distributed Reinforcement Learning","A. Dalgkitsis; L. A. Garrido; P. -V. Mekikis; K. Ramantas; L. Alonso; C. Verikoukis","Iquadrat Informatica S.L., Barcelona, Spain; Iquadrat Informatica S.L., Barcelona, Spain; Iquadrat Informatica S.L., Barcelona, Spain; Iquadrat Informatica S.L., Barcelona, Spain; Signal Theory & Communications Dept., Universitat Politécnica de Catalunya (UPC), Barcelona, Spain; Telecommunications Technological Centre of Catalonia (CTTC/CERCA), Castelldefels, Barcelona, Spain","2021 IEEE Global Communications Conference (GLOBECOM)","2 Feb 2022","2021","","","01","06","As the demand for Network Function Virtualization accelerates, service providers are expected to advance the way they manage and orchestrate their network services to offer lower latency services to their future users. Modern services require complex data flows between Virtual Network Functions, placed in separate network domains, risking an increase in latency that compromises the offered latency constraints. This shift requires high levels of automation to deal with the scale and load of future networks. In this paper, we formulate the Service Function Chaining (SFC) placement problem and then we tackle it by introducing SCHEMA, a Distributed Reinforcement Learning (RL) algorithm that performs complex SFC orchestration for low latency services. We combine multiple RL agents with a Bidding Mechanism to enable scalability on multi-domain networks. Finally, we use a simulation model to evaluate SCHEMA, and we demonstrate its ability to obtain a 60.54% reduction of average service latency when compared to a centralised RL solution.","","978-1-7281-8104-2","10.1109/GLOBECOM46510.2021.9685162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9685162","Zero-touch Orchestration;Network Function Virtualization;SFC Placement;Distributed Reinforcement Learning","Automation;Service function chaining;Scalability;Conferences;Reinforcement learning;Ultra reliable low latency communication;Network function virtualization","cloud computing;reinforcement learning;virtualisation","multidomain networks;SCHEMA;service chain elastic management;network function virtualization;network services;lower latency services;service function chaining placement problem;distributed reinforcement learning algorithm;SFC orchestration;multiple RL agents","","4","","14","IEEE","2 Feb 2022","","","IEEE","IEEE Conferences"
"Multivariable Anesthesia Control Using Reinforcement Learning","N. Sadati; A. Aflaki; M. Jahed","Electrical Engineering Departmen Intelligent Systems Laboratory, t, Sharif University of Technology, Tehran, Iran; Electrical Engineering Department, Sharif University of Technology, Tehran, Iran; Electrical Engineering Department, Sharif University of Technology, Tehran, Iran","2006 IEEE International Conference on Systems, Man and Cybernetics","16 Jul 2007","2006","6","","4563","4568","The anesthesia community has recently witnessed numerous advances in monitoring of the anesthetic state. This development has spurred a renewed interest in the automation of the clinical anesthesia. The absence of a precise model in biomedical field has motivated the authors to propose a novel approach capable of handling the uncertainties present in the model and moreover, to design a new approach which is subjective and can be adopted to patient specific requirements. In this regard a study has been conducted in this paper, in order to fit the intelligence of fuzzy controllers based on reinforcement learning in this clinical application. This study suggests that the chosen approach fits the scenario of controlling two anesthetic drugs; Atracurium and Isoflurane, to achieve a surgical anesthetic state, in terms of muscle relaxation (paralysis) and blood pressure (mean arterial pressure (MAP)) as monitoring indices.","1062-922X","1-4244-0099-6","10.1109/ICSMC.2006.384865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274631","","Anesthesia;Learning;Anesthetic drugs;Biomedical monitoring;Blood pressure;Automatic control;Automation;Uncertainty;Fuzzy control;Pressure control","drugs;fuzzy control;learning (artificial intelligence);medical computing;medical control systems;multivariable systems;patient treatment","multivariable anesthesia control;reinforcement learning;clinical anesthesia;biomedical field;fuzzy control;anesthetic drugs;Atracurium;Isoflurane;surgical anesthetic state;muscle relaxation;blood pressure","","1","","19","IEEE","16 Jul 2007","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Analog Circuit Sizing with an Electrical Design Space and Sparse Rewards","Y. Uhlmann; M. Essich; L. Bramlage; J. Scheible; C. Curio","Electronics & Drives, Reutlingen University, Reutlingen, Germany; Cognitive Systems, Reutlingen University, Reutlingen, Germany; Cognitive Systems, Reutlingen University, Reutlingen, Germany; Electronics & Drives, Reutlingen University, Reutlingen, Germany; Cognitive Systems, Reutlingen University, Reutlingen, Germany","2022 ACM/IEEE 4th Workshop on Machine Learning for CAD (MLCAD)","29 Sep 2022","2022","","","21","26","There is still a great reliance on human expert knowledge during the analog integrated circuit sizing design phase due to its complexity and scale, with the result that there is a very low level of automation associated with it. Current research shows that reinforcement learning is a promising approach for addressing this issue. Similarly, it has been shown that the convergence of conventional optimization approaches can be improved by transforming the design space from the geometrical domain into the electrical domain. Here, this design space transformation is employed as an alternative action space for deep reinforcement learning agents. The presented approach is based entirely on reinforcement learning, whereby agents are trained in the craft of analog circuit sizing without explicit expert guidance. After training and evaluating agents on circuits of varying complexity, their behavior when confronted with a different technology, is examined, showing the applicability, feasibility as well as transferability of this approach.","","978-1-6654-9729-9","10.1109/MLCAD55463.2022.9900083","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9900083","analog circuit sizing;reinforcement learning;neural networks","Training;Solid modeling;Automation;Conferences;Reinforcement learning;Complexity theory;Behavioral sciences","analogue integrated circuits;deep learning (artificial intelligence);electronic engineering computing;optimisation;reinforcement learning","analog circuit sizing;electrical design space;sparse rewards;human expert knowledge;analog integrated circuit sizing design phase;optimization approaches;geometrical domain;electrical domain;design space transformation;deep reinforcement learning agents","","","","34","IEEE","29 Sep 2022","","","IEEE","IEEE Conferences"
"Deer in The Headlights: Short Term Planning via Reinforcement Learning Algorithms for Autonomous Vehicles","L. Hishmeh; F. Awad","Department of Network Engineering & Security, Jordan University of Science & Technology, Irbid, Jordan; Department of Network Engineering & Security, Jordan University of Science & Technology, Irbid, Jordan","2020 11th International Conference on Information and Communication Systems (ICICS)","27 Apr 2020","2020","","","255","260","Reinforcement learning has seen a resurgence recently in tandem with the resurgence of deep neural networks. Recent work done by DeepMind and others has shown us the enormous potential of reinforcement learning when applied to certain problem domains. Of these domains, one of them is of particular contemporary relevance; Autonomous Vehicles. Several multinational corporations are pouring enormous resources into developing cars which can autonomously drive themselves. Autonomous Vehicles are expected to bring about the largest revolution in the automobile industry, since the invention of the internal combustion engine. One of the key components of autonomous vehicles is reinforcement learning. In this paper, we explored the performance of a Reinforcement Learning agent, known as Dueling DQN, to a simulated driving environment, presenting a short term planner to be used as a lane following and obstacle avoidance system as part of a larger, integrated system.","2573-3346","978-1-7281-6227-0","10.1109/ICICS49469.2020.239561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079075","Deep Reinforcement Learning;Autonomous Vehicles;CARLA;Deep Q-Learning Networks;Neural Networks","Technological innovation;Q-learning;Pipelines;Neural networks;Internal combustion engines;Reinforcement learning;Planning","automobile industry;automobiles;collision avoidance;internal combustion engines;learning (artificial intelligence);neural nets","short term planning;autonomous vehicles;resurgence;deep neural networks;reinforcement learning agent;internal combustion engine;Dueling DQN;lane following;obstacle avoidance system","","","","10","IEEE","27 Apr 2020","","","IEEE","IEEE Conferences"
"DRL-cloud: Deep reinforcement learning-based resource provisioning and task scheduling for cloud service providers","M. Cheng; J. Li; S. Nazarian","Duke University, Durham, NC, USA; University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA","2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC)","22 Feb 2018","2018","","","129","134","Cloud computing has become an attractive computing paradigm in both academia and industry. Through virtualization technology, Cloud Service Providers (CSPs) that own data centers can structure physical servers into Virtual Machines (VMs) to provide services, resources, and infrastructures to users. Profit-driven CSPs charge users for service access and VM rental, and reduce power consumption and electric bills so as to increase profit margin. The key challenge faced by CSPs is data center energy cost minimization. Prior works proposed various algorithms to reduce energy cost through Resource Provisioning (RP) and/or Task Scheduling (TS). However, they have scalability issues or do not consider TS with task dependencies, which is a crucial factor that ensures correct parallel execution of tasks. This paper presents DRL-Cloud, a novel Deep Reinforcement Learning (DRL)-based RP and TS system, to minimize energy cost for large-scale CSPs with very large number of servers that receive enormous numbers of user requests per day. A deep Q-learning-based two-stage RP-TS processor is designed to automatically generate the best long-term decisions by learning from the changing environment such as user request patterns and realistic electric price. With training techniques such as target network, experience replay, and exploration and exploitation, the proposed DRL-Cloud achieves remarkably high energy cost efficiency, low reject rate as well as low runtime with fast convergence. Compared with one of the state-of-the-art energy efficient algorithms, the proposed DRL-Cloud achieves up to 320% energy cost efficiency improvement while maintaining lower reject rate on average. For an example CSP setup with 5,000 servers and 200,000 tasks, compared to a fast round-robin baseline, the proposed DRL-Cloud achieves up to 144% runtime reduction.","2153-697X","978-1-5090-0602-1","10.1109/ASPDAC.2018.8297294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8297294","Deep reinforcement learning;deep Q-learning;cloud computing;resource provisioning;task scheduling;cloud resource management","Servers;Task analysis;Cloud computing;Data centers;Computational modeling;Runtime;Micromechanical devices","cloud computing;computer centres;energy conservation;learning (artificial intelligence);scheduling;virtual machines","Cloud computing;cloud service providers;service access;data center energy cost minimization;task scheduling;deep Q-learning;two-stage RP-TS processor;high energy cost efficiency;DRL-cloud;deep reinforcement learning-based resource provisioning;profit-driven CSPs;energy cost efficiency;Virtual Machines;virtualization technology","","111","","33","IEEE","22 Feb 2018","","","IEEE","IEEE Conferences"
"Model-based reinforcement learning approach for deformable linear object manipulation","H. Han; G. Paul; T. Matsubara","Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), Nara, Japan; Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), Nara, Japan; Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), Nara, Japan","2017 13th IEEE Conference on Automation Science and Engineering (CASE)","15 Jan 2018","2017","","","750","755","Deformable Linear Object (DLO) manipulation has wide application in industry and in daily life. Conventionally, it is difficult for a robot to manipulate a DLO to achieve the target configuration due to the absence of the universal model that specifies the DLO regardless of the material and environment. Since the state variable of a DLO can be very high dimensional, identifying such a model may require a huge number of samples. Thus, model-based planning of DLO manipulation would be impractical and unreasonable. In this paper, we explore another approach based on reinforcement learning. To this end, our approach is to apply a sample-efficient model-based reinforcement learning method, so-called PILCO [1], to resolve the high dimensional planning problem of DLO manipulation with a reasonable number of samples. To investigate the effectiveness of our approach, we developed an experimental setup with a dual-arm industrial robot and multiple sensors. Then, we conducted experiments to show that our approach is efficient by performing a DLO manipulation task.","2161-8089","978-1-5090-6781-7","10.1109/COASE.2017.8256194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8256194","","Learning (artificial intelligence);Planning;Service robots;Probabilistic logic;Reactive power","industrial manipulators;learning (artificial intelligence);path planning;planning (artificial intelligence);sensors","high dimensional planning problem;dual-arm industrial robot;DLO manipulation task;deformable linear object manipulation;target configuration;universal model;model-based reinforcement learning approach;model-based planning;sample-efficient model-based reinforcement learning method;PILCO;sensor","","16","","16","IEEE","15 Jan 2018","","","IEEE","IEEE Conferences"
"Object Detection-Based Reinforcement Learning for Autonomous Point-to-Point Navigation","T. Lewis; A. Ibarra; M. Jamshidi","Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX; Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX; Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX","2022 World Automation Congress (WAC)","8 Nov 2022","2022","","","394","399","Autonomous navigation has been a fundamental area of research for real-world mobile robotic applications, having widespread utility across many industries from warehouse package delivery to residential cleaning services. Because of the complex nature of the robot’s environment, several challenges have prevented effectively implementing reinforcement learning-based algorithms trained in simulation. While difficulties can arise from the virtual environment lacking the sophistication to represent such a large and complex state space based on data-heavy sensor observations, the variance in MDP representations across related studies biases their fair comparison, performance, and repeatability. In this study, it is found that the design of the reward function used for training a vision-based mobile agent to perform collision-free point-goal navigation in simulation plays a significant role in overall performance. A novel approach is introduced where reward is also granted for successfully detecting a target object scaled according to prediction confidence. This strategy was found to significantly improve the point-goal navigation behavior compared to simpler reward function designs seen in similar related studies.","2154-4824","979-8-88831-444-9","10.23919/WAC55640.2022.9934448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9934448","Deep Reinforcement Learning;Autonomous Navigation;Robot Simulation;Object Detection;Computer Vision;Proximal Policy Optimization;Markov Decision Process;Unmanned Ground Vehicles","Training;Navigation;Service robots;Mobile agents;Virtual environments;Reinforcement learning;Switches","collision avoidance;learning (artificial intelligence);Markov processes;mobile robots;navigation;object detection;robot vision","autonomous navigation;collision-free point-goal navigation;complex nature;complex state space;data-heavy sensor observations;large state space;MDP representations;object detection-based reinforcement;point-goal navigation behavior;point-to-point navigation;real-world mobile robotic applications;reinforcement learning-based algorithms;residential cleaning services;robot;similar related studies;simpler reward function designs;target object;virtual environment;vision-based mobile agent;warehouse package delivery;widespread utility","","1","","29","","8 Nov 2022","","","IEEE","IEEE Conferences"
"MultiROS: ROS-Based Robot Simulation Environment for Concurrent Deep Reinforcement Learning","J. Kapukotuwa; B. Lee; D. Devine; Y. Qiao","Software Research Institute, Technological University of the Shannon: Midlands Midwest, Co Westmeath, Athlone, Ireland; Software Research Institute, Technological University of the Shannon: Midlands Midwest, Co Westmeath, Athlone, Ireland; Materials Research Institute, Technological University of the Shannon: Midlands Midwest, Co Westmeath, Athlone, Ireland; Software Research Institute, Technological University of the Shannon: Midlands Midwest, Co Westmeath, Athlone, Ireland","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","1098","1103","On the journey of true autonomous robotics, applying deep reinforcement learning (DRL) techniques to solve complex robotics tasks has been a growing interest in academics and the industry. Currently, numerous simulation frameworks exist for evaluating DRL algorithms with robots, and they usually come with prebuilt tasks or provide tools to create custom environments. Among these, one of the highly sought approaches is using Robot Operating System (ROS) based DRL frameworks for simulation and deployment in the real world. The current ROS-based DRL simulation frameworks like openai_ros or Gym-gazebo provide a framework for creating environments; however, they do not support training with vectorised environments for speeding up the training process and parallel simulations for testing and evaluating meta-learning, multi-task learning and transfer learning approaches. Therefore, we present MultiROS, a 3D robotic simulation framework with a collection of prebuilt environments for deep reinforcement learning (DRL) research to overcome these limitations. This package interfaces with the Gazebo robotic simulator using ROS and provides a modular structure to create ROS-based RL environments. Unlike the others, MultiROS provides support for training with multiple environments in parallel and simultaneously accessing data from each simulation. Furthermore, since MultiROS uses the popular OpenAI Gym interface, it is compatible with most OpenAI Gym based reinforcement learning algorithms that use third-party python frameworks.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926475","Science Foundation Ireland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926475","","Training;Solid modeling;Three-dimensional displays;Service robots;Operating systems;Transfer learning;Reinforcement learning","deep learning (artificial intelligence);mobile robots;operating systems (computers);reinforcement learning;robot programming","MultiROS;concurrent deep reinforcement learning;autonomous robotics;deep reinforcement learning techniques;complex robotics tasks;DRL algorithms;openai_ros;vectorised environments;meta-learning;transfer learning approaches;3D robotic simulation framework;Gazebo robotic simulator;ROS-based RL environments;OpenAI Gym interface;OpenAI Gym based reinforcement;third-party python frameworks;ROS-based DRL simulation frameworks;robot operating system based DRL frameworks;ROS-based robot simulation environment","","","","24","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"Standard Cell Routing with Reinforcement Learning and Genetic Algorithm in Advanced Technology Nodes","H. Ren; M. Fojtik","NVIDIA Corporation, Austin, TX; NVIDIA Corporation, Durham, NC","2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC)","11 Mar 2021","2021","","","684","689","Standard cell layout in advanced technology nodes are done manually in the industry today. Automating standard cell layout process, in particular the routing step, are challenging because of the constraints of enormous design rules. In this paper we propose a machine learning based approach that applies genetic algorithm to create initial routing candidates and uses reinforcement learning (RL) to fix the design rule violations incrementally. A design rule checker feedbacks the violations to the RL agent and the agent learns how to fix them based on the data. This approach is also applicable to future technology nodes with unseen design rules. We demonstrate the effectiveness of this approach on a number of standard cells. We have shown that it can route a cell which is deemed unroutable manually, reducing the cell size by 11%.","2153-697X","978-1-4503-7999-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9371660","","Layout;Reinforcement learning;Routing;Reliability;Task analysis;Standards;Genetic algorithms","","","","","","18","","11 Mar 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Smart Home Energy Management","L. Yu; W. Xie; D. Xie; Y. Zou; D. Zhang; Z. Sun; L. Zhang; Y. Zhang; T. Jiang","Key Laboratory of Broadband Wireless Communication and Sensor Network Technology of Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China; Key Laboratory of Broadband Wireless Communication and Sensor Network Technology of Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China; Key Laboratory of Broadband Wireless Communication and Sensor Network Technology of Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China; Key Laboratory of Broadband Wireless Communication and Sensor Network Technology of Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China; Jiangsu Key Laboratory of Broadband Wireless Communication and Internet of Things, School of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China; Key Laboratory of Broadband Wireless Communication and Sensor Network Technology of Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China; Key Laboratory of Broadband Wireless Communication and Sensor Network Technology of Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Engineering, University of Leicester, Leicester, U.K.; National Laboratory for Optoelectronics, School of Electronics Information and Communications, Huazhong University of Science and Technology, Wuhan, China","IEEE Internet of Things Journal","14 Apr 2020","2020","7","4","2751","2762","We investigate an energy cost minimization problem for a smart home in the absence of a building thermal dynamics model with the consideration of a comfortable temperature range. Due to the existence of model uncertainty, parameter uncertainty (e.g., renewable generation output, nonshiftable power demand, outdoor temperature, and electricity price), and temporally coupled operational constraints, it is very challenging to design an optimal energy management algorithm for scheduling heating, ventilation, and air conditioning systems and energy storage systems in the smart home. To address the challenge, we first formulate the above problem as a Markov decision process, and then propose an energy management algorithm based on deep deterministic policy gradients. It is worth mentioning that the proposed algorithm does not require the prior knowledge of uncertain parameters and building the thermal dynamics model. The simulation results based on real-world traces demonstrate the effectiveness and robustness of the proposed algorithm.","2327-4662","","10.1109/JIOT.2019.2957289","National Natural Science Foundation of China(grant numbers:61972214,61671253,61631020,91738201,61571241,61872423,61972208,61672299,61771258); Natural Science Foundation of Jiangsu Province(grant numbers:BK20171446); Scientific Research Fund of Nanjing University of Posts and Telecommunications(grant numbers:NY219062); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919976","Deep reinforcement learning (DRL);energy cost;energy management;energy storage systems (ESSs);heating, ventilation, and air conditioning (HVAC) systems;smart home;thermal comfort","Smart homes;Energy management;Temperature distribution;Internet of Things;Heuristic algorithms;Load modeling","building management systems;decision theory;energy conservation;energy management systems;energy storage;gradient methods;home automation;HVAC;learning (artificial intelligence);Markov processes;neural nets;power engineering computing;renewable energy sources","renewable generation output;nonshiftable power demand;outdoor temperature;electricity price;operational constraints;optimal energy management algorithm;heating ventilation and air conditioning systems;energy storage systems;Markov decision process;deep deterministic policy gradients;uncertain parameters;deep reinforcement learning;parameter uncertainty;model uncertainty;energy cost minimization problem;smart home energy management","","193","","43","IEEE","3 Dec 2019","","","IEEE","IEEE Journals"
"Autonomous Navigation via Deep Reinforcement Learning for Resource Constraint Edge Nodes Using Transfer Learning","A. Anwar; A. Raychowdhury","Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA","IEEE Access","11 Feb 2020","2020","8","","26549","26560","Smart and agile drones are fast becoming ubiquitous at the edge of the cloud. The usage of these drones is constrained by their limited power and compute capability. In this paper, we present a Transfer Learning (TL) based approach to reduce on-board computation required to train a deep neural network for autonomous navigation via value-based Deep Reinforcement Learning for a target algorithmic performance. A library of 3D realistic meta-environments is manually designed using Unreal Gaming Engine and the network is trained end-to-end. These trained meta-weights are then used as initializers to the network in a test environment and fine-tuned for the last few fully connected layers. Variation in drone dynamics and environmental characteristics is carried out to show robustness of the approach. Using NVIDIA GPU profiler, it was shown that the energy consumption and training latency is reduced by 3.7$\times$ and 1.8$\times$ respectively without significant degradation in the performance in terms of average distance traveled before crash i.e. Mean Safe Flight (MSF). The approach is also tested on a real environment using DJI Tello drone and similar results were reported. The code for the approach can be found on GitHub: https://github.com/aqeelanwar/DRLwithTL.","2169-3536","","10.1109/ACCESS.2020.2971172","Semiconductor Research Corporation; Defense Advanced Research Projects Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978577","Autonomous navigation;transfer learning;deep reinforcement learning;drone","Training;Drones;Autonomous robots;Task analysis;Reinforcement learning;Artificial neural networks","autonomous aerial vehicles;cloud computing;computer games;control engineering computing;learning (artificial intelligence);navigation;neural nets;robot vision","Unreal Gaming Engine;drone dynamics;NVIDIA GPU profiler;energy consumption;autonomous navigation;resource constraint edge nodes;transfer learning;agile drones;on-board computation;deep neural network;value-based deep reinforcement learning;3D realistic meta-environments;TL;mean safe flight;MSF;DJI Tello drone;smart drones;cloud","","46","","35","CCBY","3 Feb 2020","","","IEEE","IEEE Journals"
"Optimization strategy based on deep reinforcement learning for home energy management","Y. Liu; D. Zhang; H. B. Gooi","China Electric Power Research Institute, Beijing, China; China Electric Power Research Institute, Beijing, China; Nanyang Technological University, Singapore, Singapore","CSEE Journal of Power and Energy Systems","6 Aug 2020","2020","6","3","572","582","With the development of a smart grid and smart home, massive amounts of data can be made available, providing the basis for algorithm training in artificial intelligence applications. These continuous improving conditions are expected to enable the home energy management system (HEMS) to cope with the increasing complexities and uncertainties in the enduser side of the power grid system. In this paper, a home energy management optimization strategy is proposed based on deep Q-learning (DQN) and double deep Q-learning (DDQN) to perform scheduling of home energy appliances. The applied algorithms are model-free and can help the customers reduce electricity consumption by taking a series of actions in response to a dynamic environment. In the test, the DDQN is more appropriate for minimizing the cost in a HEMS compared to DQN. In the process of method implementation, the generalization and reward setting of the algorithms are discussed and analyzed in detail. The results of this method are compared with those of Particle Swarm Optimization (PSO) to validate the performance of the proposed algorithm. The effectiveness of applied data-driven methods is validated by using a real-world database combined with the household energy storage model.","2096-0042","","10.17775/CSEEJPES.2019.02890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9057008","Deep reinforcement learning;demand response;home energy management system;smart grid","Energy management;Machine learning;Optimization;Home appliances;Heuristic algorithms;Energy storage;Feature extraction","artificial intelligence;domestic appliances;energy management systems;energy storage;home automation;learning (artificial intelligence);particle swarm optimisation;power engineering computing;power grids;smart power grids","particle swarm optimization;applied data-driven methods;household energy storage model;deep reinforcement learning;smart grid;smart home;algorithm training;applied algorithms;home energy appliances;DDQN;double deep Q-learning;home energy management optimization strategy;power grid system;HEMS;home energy management system;artificial intelligence applications","","28","","","","6 Apr 2020","","","CSEE","CSEE Journals"
"Home Energy Recommendation System (HERS): A Deep Reinforcement Learning Method Based on Residents’ Feedback and Activity","S. S. Shuvo; Y. Yilmaz","Department of Electrical Engineering, University of South Florida, Tampa, FL, USA; Department of Electrical Engineering, University of South Florida, Tampa, FL, USA","IEEE Transactions on Smart Grid","21 Jun 2022","2022","13","4","2812","2821","Smart home appliances can take command and act intelligently, making them suitable for implementing optimization techniques. Artificial intelligence (AI) based control of these smart devices enables demand-side management (DSM) of electricity consumption. By integrating human feedback and activity in the decision process, this work proposes a deep Reinforcement Learning (RL) method for managing smart devices to optimize electricity cost and comfort residents. Our contributions are twofold. Firstly, we incorporate human feedback in the objective function of our DSM technique that we name Home Energy Recommendation System (HERS). Secondly, we include human activity data in the RL state definition to enhance the energy optimization performance. We perform comprehensive experimental analyses to compare the proposed deep RL approach with existing approaches that lack the aforementioned critical decision-making features. The proposed model is robust to varying resident activities and preferences and applicable to a broad spectrum of homes with different resident profiles.","1949-3061","","10.1109/TSG.2022.3158814","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733029","Home energy management;demand side management;customer comfort;residents’ activity label;deep reinforcement learning;artificial intelligence","Smart homes;Costs;Smart devices;Real-time systems;Pricing;Batteries;Activity recognition","artificial intelligence;decision making;deep learning (artificial intelligence);demand side management;domestic appliances;home automation;optimisation;power engineering computing;power generation economics;reinforcement learning","home energy recommendation system;deep reinforcement learning method;smart home appliances;optimization techniques;artificial intelligence based control;smart devices;demand-side management;electricity consumption;human feedback;electricity cost;DSM technique;RL state definition;energy optimization performance;deep RL approach;HERS;decision process;critical decision-making features","","18","","43","IEEE","11 Mar 2022","","","IEEE","IEEE Journals"
"Real-Time Scheduling of Operational Time for Smart Home Appliances Based on Reinforcement Learning","M. Khan; J. Seo; D. Kim","School of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; School of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; School of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea","IEEE Access","30 Jun 2020","2020","8","","116520","116534","The scheduling of the operational time of household appliances requires several parameters to be tuned according to the available energy supplied to a smart home. However, scheduling of operational time of multiple appliances in a smart home itself is the NP-hard problem and thus requires an intelligent, heuristic method to be solved in polynomial time. In this research work, we propose Real-time Scheduling of Operational Time of Household Appliances based on the well-known value iterative reinforcement learning called Quality learning (RSOTHA-QL). The proposed RSOTHA-QL scheme operates in two phases. In the first phase, the agents of the Q learning act by interacting with the smart home environment and obtain a reward. The reward value is further utilized to schedule the operational time of household appliances in the next state ensuring minimum energy consumption. In the second phase, the dissatisfaction arises due to scheduling of operational time of the household appliances of the home user is maintained by categorizing the household appliances into three groups: 1) deferrable, 2) non-deferrable, and 3) controllable. Besides, using the shared memory synchronization phenomenon, the agents attached to each appliance of the smart home become coordinated. The simulation and experiments are performed in a smart home scenario comprised of a single user and multiple appliances. As compared with our previous research work using the Least Slack Time (LST) scheduling algorithm and scheduling based on demand-response strategy, it is revealed that the operational time of the household appliances is efficiently scheduled to reduce the energy consumption and dissatisfaction level of the home users significantly.","2169-3536","","10.1109/ACCESS.2020.3004151","BK21 Plus Project (SW Human Resource Development Program for Supporting Smart Life) funded by the Ministry of Education, School of Computer Science and Engineering, Kyungpook National University, South Korea(grant numbers:21A20131600005); Korea Research Fellowship Program funded by the Ministry of Science and ICT through the National Research Foundation of Korea(grant numbers:2019H1D3A1A01102987); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9122531","Energy consumption;Q~learning;scheduling;smart home","Home appliances;Smart homes;Smart grids;Optimal scheduling;Schedules;Real-time systems;Energy consumption","computational complexity;control engineering computing;domestic appliances;home automation;iterative methods;learning (artificial intelligence);power engineering computing;scheduling","operational time;Real-time Scheduling;home user;smart home Appliances;quality learning;value iterative reinforcement learning;RSOTHA-QL scheme;NP-hard problem;minimum energy consumption","","13","","39","CCBY","22 Jun 2020","","","IEEE","IEEE Journals"
"Door opening by joining reinforcement learning and intelligent control","B. Nemec; L. Žlajpah; A. Ude","Humanoid and Cognitive Robotics Lab, Jozef Stefan Institute, Ljubljana, Slovenia; Humanoid and Cognitive Robotics Lab, Jozef Stefan Institute, Ljubljana, Slovenia; Humanoid and Cognitive Robotics Lab, Jozef Stefan Institute, Ljubljana, Slovenia","2017 18th International Conference on Advanced Robotics (ICAR)","31 Aug 2017","2017","","","222","228","In this paper we address a problem of how to open the doors with an articulated robot. We propose a novel algorithm, that combines widely used reinforcement learning approach with intelligent control algorithms. In order to speed up learning, we formed more structured search, which exploits physical constraints of the problem to be solved. The underlying controller, which acts as a policy search agent, generates movements along the admissible directions defined by physical constraints of the task. This way we can efficiently solve many practical problems such as door opening without almost any previous knowledge of the environment. The approach was verified in simulation as well as with real robot experiment.","","978-1-5386-3157-7","10.1109/ICAR.2017.8023522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8023522","Reinforcement Learning;Intelligent Control;Autonomous Exploration","Learning (artificial intelligence);Intelligent control;Robot sensing systems;Force;Collision avoidance;Quaternions","home automation;intelligent robots;learning (artificial intelligence);service robots","door opening;policy search agent;physical constraints;articulated robot;joint reinforcement learning-intelligent control","","12","","27","IEEE","31 Aug 2017","","","IEEE","IEEE Conferences"
"Autonomous Household Energy Management Using Deep Reinforcement Learning","N. Tsang; C. Cao; S. Wu; Z. Yan; A. Yousefi; A. Fred-Ojala; I. Sidhu","Department of Civil and Environmental Engineering, University of California, Berkeley, Berkeley, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, USA; School of Economics and Management, Tsinghua University, Beijing, China; Department of Industrial Engineering and Operations Research, University of California, Berkeley, Berkeley, USA; Department of Industrial Engineering and Operations Research, University of California, Berkeley, Berkeley, USA; Department of Industrial Engineering and Operations Research, University of California, Berkeley, Berkeley, USA","2019 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)","12 Aug 2019","2019","","","1","7","With the massive growth of smart meters and availability of data, there is a unique opportunity to offer novel services to utility customers. The emergence of new technologies can be used to learn a customer's electrical consumption patterns and to offer customized solutions for minimizing electricity cost. One such technology is deep reinforcement learning (DRL), which is explored in this paper. The proposed approach for autonomous household energy management utilizes deep Q-learning with a novel method to deal with potential scalability issues that involves grouping dependent electrical devices. The defined reward function addresses the problems of previous research by adding more scenarios for the penalty and reward. Transfer learning using an adviser agent is also utilized to improve training time. By grouping dependent devices, we are able to resolve the problem of scalability. The results of these experiments demonstrate that DRL can be utilized to autonomously control household energy consumption.","","978-1-7281-3401-7","10.1109/ICE.2019.8792636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792636","artificial intelligence;autonomous systems;energy management;reinforcement learning;q-learning","Buildings;Training;Reinforcement learning;Biological neural networks;Energy management;Adaptation models;Machine learning algorithms","building management systems;energy consumption;energy management systems;home automation;learning (artificial intelligence);power consumption","customized solutions;electricity cost;deep reinforcement learning;autonomous household energy management;potential scalability issues;massive growth;smart meters;utility customers;household energy consumption;electrical devices;deep Q-learning;reward function","","9","","10","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Reinforcement learning aided smart-home decision-making in an interactive smart grid","D. Li; S. K. Jayaweera","Department of Electrical and Computer Engineering, University of New Mexico, Albuquerque, USA; Department of Electrical and Computer Engineering, University of New Mexico, Albuquerque, USA","2014 IEEE Green Energy and Systems Conference (IGESC)","26 Jan 2015","2014","","","1","6","In this paper, a complete hierarchical architecture is presented for the Utility-customer interaction, which tightly connect several important research topics, such as customer load prediction, renewable generation integration, power-load balancing and demand response. The complete interaction cycle consists of two stages: (1) Initial interaction (long-term planning) and (2) Real-time interaction (short-term planning). A hidden mode Markov decision process (HM-MDP) model is developed for customer real-time decision making, which outperforms the conventional Markov decision process (MDP) model in handling the non-stationary environment. To obtain a low-complexity, real-time algorithm, that allows to adaptively incorporate new observations as the environment changes, we resort to Q-learning based approximate dynamic programming (ADP). Without requiring specific starting and ending points of the scheduling period, the Q-learning algorithm offers more flexibility in practice. Performance analysis of both exact and approximate algorithms are presented with simulation results, in comparison with other decision making strategies.","","978-1-4799-7333-0","10.1109/IGESC.2014.7018632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018632","Smart-home;hidden mode Markov decision process (HM-MDP);Baum-Welch algorithm;approximate dynamic programming (ADP);Q-learning","Hidden Markov models;Markov processes;Heuristic algorithms;Decision making;Scheduling;Real-time systems;Approximation algorithms","approximation theory;decision making;dynamic programming;game theory;home automation;learning (artificial intelligence);Markov processes;power engineering computing;power system planning;smart power grids","reinforcement learning aided smart-home decision-making;interactive smart grid;utility-customer interaction;complete hierarchical architecture;customer load prediction;renewable generation integration;power-load balancing;demand response;long-term planning;short-term planning;hidden mode Markov decision process model;HM-MDP model;customer real-time decision making;Q-learning based approximate dynamic programming;ADP","","9","","17","IEEE","26 Jan 2015","","","IEEE","IEEE Conferences"
"Adaptive Learning: A New Decentralized Reinforcement Learning Approach for Cooperative Multiagent Systems","M. -L. Li; S. Chen; J. Chen","College of Intelligence Science and Technology, National University of Defence Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defence Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defence Technology, Changsha, China","IEEE Access","4 Jun 2020","2020","8","","99404","99421","Multiagent systems (MASs) have received extensive attention in a variety of domains, such as robotics and distributed control. This paper focuses on how independent learners (ILs, structures used in decentralized reinforcement learning) decide on their individual behaviors to achieve coherent joint behavior. To date, Reinforcement learning(RL) approaches for ILs have not guaranteed convergence to the optimal joint policy in scenarios in which communication is difficult. Especially in a decentralized algorithm, the proportion of credit for a single agent’s action in a multiagent system is not distinguished, which can lead to miscoordination of joint actions. Therefore, it is highly significant to study the mechanisms of coordination between agents in MASs. Most previous coordination mechanisms have been carried out by modeling the communication mechanism and other agent policies. These methods are applicable only to a particular system, so such algorithms do not offer generalizability, especially when there are dozens or more agents. Therefore, this paper mainly focuses on the MAS contains more than a dozen agents. By combining the method of parallel computation, the experimental environment is closer to the application scene. By studying the paradigm of centralized training and decentralized execution(CTDE), a multi-agent reinforcement learning algorithm for implicit coordination based on TD error is proposed. The new algorithm can dynamically adjust the learning rate by deeply analyzing the dissonance problem in the matrix game and combining it with a multiagent environment. By adjusting the dynamic learning rate between agents, coordination of the agents’ strategies can be achieved. Experimental results show that the proposed algorithm can effectively improve the coordination ability of a MAS. Moreover, the variance of the training results is more stable than that of the hysteretic Q learning(HQL) algorithm. Hence, the problem of miscoordination in a MAS can be avoided to some extent without additional communication. Our work provides a new way to solve the miscoordination problem for reinforcement learning algorithms in the scale of dozens or more number of agents. As a new IL structure algorithm, our results should be extended and further studied.","2169-3536","","10.1109/ACCESS.2020.2997899","National Natural Science Foundation of China(grant numbers:61702528); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9102277","Reinforcement learning;multiagent system;intelligent control","Learning (artificial intelligence);Training;Multi-agent systems;Heuristic algorithms;Roads;Urban areas;Games","learning (artificial intelligence);multi-agent systems","adaptive learning;decentralized reinforcement learning approach;cooperative multiagent systems;distributed control;individual behaviors;coherent joint behavior;optimal joint policy;single agent action;communication mechanism;MAS;dozen agents;multiagent reinforcement learning algorithm;implicit coordination;multiagent environment;dynamic learning rate;IL structure algorithm;coordination mechanisms;independent learners;centralized training and decentralized execution;CTDE;dissonance problem;matrix game;hysteretic Q learning;HQL;robotics","","9","","53","CCBY","27 May 2020","","","IEEE","IEEE Journals"
"Smart Home’s Energy Management Through a Clustering-Based Reinforcement Learning Approach","I. Zenginis; J. Vardakas; N. E. Koltsaklis; C. Verikoukis","Department of R&D, Iquadrat Informatica S.L., Barcelona, Spain; Department of R&D, Iquadrat Informatica S.L., Barcelona, Spain; Faculty of Electrical Engineering, Czech Technical University, Prague, Czech Republic; CEID, University of Patras, Patras, Greece","IEEE Internet of Things Journal","24 Aug 2022","2022","9","17","16363","16371","Smart homes that contain renewable energy sources, storage systems, and controllable loads will be key components of the future smart grid. In this article, we develop a reinforcement-learning (RL)-based scheme for the real-time energy management of a smart home that contains a photovoltaic system, a storage device, and a heating, ventilation, and air conditioning (HVAC) system. The objective of the proposed scheme is to minimize the smart home’s electricity cost and the residents’ thermal discomfort by appropriately scheduling the storage device and the HVAC system on a daily basis. The problem is formulated as a Markov decision process, which is solved using the deep deterministic policy gradient (DDPG) algorithm. The main contribution of our study compared to the existing literature on RL-based energy management is the development of a clustering process that partitions the training data set into more homogeneous training subsets. Different DDPG agents are trained based on the data included in the derived subsets, while in real time, the test days are assigned to the appropriate agent, which is able to achieve more efficient energy schedules when compared to a single DDPG agent that is trained based on a unified training data set.","2327-4662","","10.1109/JIOT.2022.3152586","PROGRESSUS Project; Electronic Components and Systems for European Leadership Joint Undertaking(grant numbers:876868); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9716893","Clustering;energy management;reinforcement learning (RL);smart home","HVAC;Energy management;Costs;Training;Internet of Things;Real-time systems;Smart homes","energy management systems;home automation;HVAC;Markov processes;pattern clustering;photovoltaic power systems;power engineering computing;power generation scheduling;real-time systems;reinforcement learning;renewable energy sources;smart power grids","smart home energy management;renewable energy sources;controllable loads;smart grid;reinforcement-learning-based scheme;real-time energy management;photovoltaic system;storage device;heating-ventilation-air conditioning system;HVAC system;Markov decision process;deep deterministic policy gradient algorithm;RL-based energy management;clustering process;smart home electricity cost;resident thermal discomfort;DDPG;homogeneous training subsets","","9","","42","IEEE","18 Feb 2022","","","IEEE","IEEE Journals"
"DRL-HEMS: Deep Reinforcement Learning Agent for Demand Response in Home Energy Management Systems Considering Customers and Operators Perspectives","A. A. Amer; K. Shaban; A. M. Massoud","Electrical Engineering Department, Qatar University, Doha, Qatar; Computer Science and Engineering Department, Qatar University, Doha, Qatar; Electrical Engineering Department, Qatar University, Doha, Qatar","IEEE Transactions on Smart Grid","22 Dec 2022","2023","14","1","239","250","With the smart grid and smart homes development, different data are made available, providing a source for training algorithms, such as deep reinforcement learning (DRL), in smart grid applications. These algorithms allowed the home energy management systems (HEMSs) to deal with the computational complexities and the uncertainties at the end-user side. This article proposes a multi-objective DRL-HEMS: a data-driven solution, which is a trained DRL agent in a HEMS to optimize the energy consumption of a household with different appliances, an energy storage system, a photovoltaic system, and an electric vehicle. The proposed solution reduces the electricity cost considering the resident’s comfort level and the loading level of the distribution transformer. The distribution transformer load is optimized by optimizing its loss-of-life. The performance of DRL-HEMS is evaluated using real-world data, and results show that it can optimize multiple appliances operation, reduce electricity bill cost, dissatisfaction cost, and the transformer loading condition.","1949-3061","","10.1109/TSG.2022.3198401","Qatar National Research Fund (a Member of Qatar Foundation)(grant numbers:NPRP11S-1202-170052); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9855534","Deep reinforcement learning;multi-objective deep reinforcement learning;demand response;home energy management","Home appliances;Costs;Transformers;Load modeling;Optimization;Schedules;Reinforcement learning","building management systems;computational complexity;deep learning (artificial intelligence);demand side management;domestic appliances;electric vehicles;energy consumption;energy storage;home automation;photovoltaic power systems;power engineering computing;power transformers;reinforcement learning;smart power grids","computational complexities;customers perspectives;data-driven solution;deep reinforcement learning agent;demand response;dissatisfaction cost;distribution transformer load;electric vehicle;electricity bill cost;energy consumption;energy storage system;home energy management systems;multiobjective DRL-HEMS;multiple appliances operation;operators perspectives;photovoltaic system;smart grid applications;smart homes development;training algorithms","","8","","35","IEEE","15 Aug 2022","","","IEEE","IEEE Journals"
"Mixed Deep Reinforcement Learning Considering Discrete-continuous Hybrid Action Space for Smart Home Energy Management","C. Huang; H. Zhang; L. Wang; X. Luo; Y. Song","State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, S. A. R., China; Department of Electrical and Computer Engineering, State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, S. A. R., China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; Department of Electrical and Computer Engineering, State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, S. A. R., China","Journal of Modern Power Systems and Clean Energy","16 May 2022","2022","10","3","743","754","This paper develops deep reinforcement learning (DRL) algorithms for optimizing the operation of home energy system which consists of photovoltaic (PV) panels, battery energy storage system, and household appliances. Model-free DRL algorithms can efficiently handle the difficulty of energy system modeling and uncertainty of PV generation. However, discrete-continuous hybrid action space of the considered home energy system challenges existing DRL algorithms for either discrete actions or continuous actions. Thus, a mixed deep reinforcement learning (MDRL) algorithm is proposed, which integrates deep Q-learning (DQL) algorithm and deep deterministic policy gradient (DDPG) algorithm. The DQL algorithm deals with discrete actions, while the DDPG algorithm handles continuous actions. The MDRL algorithm learns optimal strategy by trial-and-error interactions with the environment. However, unsafe actions, which violate system constraints, can give rise to great cost. To handle such problem, a safe-MDRL algorithm is further proposed. Simulation studies demonstrate that the proposed MDRL algorithm can efficiently handle the challenge from discrete-continuous hybrid action space for home energy management. The proposed MDRL algorithm reduces the operation cost while maintaining the human thermal comfort by comparing with benchmark algorithms on the test dataset. Moreover, the safe-MDRL algorithm greatly reduces the loss of thermal comfort in the learning stage by the proposed MDRL algorithm.","2196-5420","","10.35833/MPCE.2021.000394","National Natural Science Foundation of China(grant numbers:62002016); Science and Technology Development Fund, Macau S.A. R.(grant numbers:0137/2019/A3); Beijing Natural Science Foundation(grant numbers:9204028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9682649","Demand response;deep reinforcement learning;discrete-continuous action space;home energy management;safe reinforcement learning","Home appliances;HVAC;Reinforcement learning;Costs;Aerospace electronics;Renewable energy sources;Task analysis","deep learning (artificial intelligence);domestic appliances;home automation;photovoltaic power systems;power engineering computing;reinforcement learning;solar cells","mixed deep reinforcement learning considering discrete-continuous hybrid action space;smart home energy management;deep reinforcement learning algorithms;battery energy storage system;model-free DRL algorithms;home energy system;discrete actions;continuous actions;mixed deep reinforcement learning algorithm;deep Q-learning algorithm;deep deterministic policy gradient algorithm;DQL algorithm deals;DDPG algorithm;unsafe actions;system constraints;safe-MDRL algorithm;benchmark algorithms;learning stage","","7","","44","","14 Jan 2022","","","SGEPRI","SGEPRI Journals"
"Potential Impacts of Smart Homes on Human Behavior: A Reinforcement Learning Approach","S. Suman; A. Etemad; F. Rivest","Department of Electrical and Computer Engineering, Queen’s University, Kingston, ON, Canada; Department of Electrical and Computer Engineering and Ingenuity Labs Research Institute, Queen’s University, Kingston, ON, Canada; Department of Mathematics and Computer Science, Royal Military College, Kingston, ON, Canada","IEEE Transactions on Artificial Intelligence","21 Jul 2022","2022","3","4","567","580","Smart homes are becoming increasingly popular as a result of advances in machine learning and cloud computing. Devices, such as smart thermostats and speakers, are now capable of learning from user feedback and adaptively adjust their settings to human preferences. Nonetheless, these devices might in turn impact human behavior. To investigate the potential impacts of smart homes on human behavior, we simulate a series of hierarchical-reinforcement learning-based human models capable of performing various activities—namely, setting temperature and humidity for thermal comfort inside a Q-Learning-based smart home model. We then investigate the possibility of the human models’ behaviors being altered as a result of the smart home and the human model adapting to one another. For our human model, the activities are based on hierarchical-reinforcement learning. This allows the human to learn how long it must continue a given activity and decide when to leave it. We then integrate our human model in the environment along with the smart home model and perform rigorous experiments considering various scenarios involving a model of a single human and models of two different humans with the smart home. Our experiments show that with the smart home, the human model can exhibit unexpected behaviors such as frequent changing of activities and an increase in the time required to modify the thermal preferences. With two human models, we interestingly observe that certain combinations of models result in normal behaviors, while other combinations exhibit the same unexpected behaviors as those observed from the single human experiment.","2691-4581","","10.1109/TAI.2021.3127483","BMO Bank of Montreal, Mitacs; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612040","AI-human interaction;hierarchical reinforce ment learning;smart home;Q-Learning","Smart homes;Artificial intelligence;Biological system modeling;Adaptation models;Reinforcement learning;Humidity;Atmospheric modeling","cloud computing;home automation;learning (artificial intelligence);thermostats","reinforcement Learning approach;smart thermostats;human preferences;impact human behavior;hierarchical-reinforcement learning-based human models;smart home model;human model;different humans;unexpected behaviors;models result;single human experiment","","6","","49","IEEE","11 Nov 2021","","","IEEE","IEEE Journals"
"Fast Adaptation with Meta-Reinforcement Learning for Trust Modelling in Human-Robot Interaction","Y. Gao; E. Sibirtseva; G. Castellano; D. Kragic","Department of Information and Technology, Uppsala University, Sweden; Robotics, Perception and Learning Lab, EECS at KTH Royal Institute of Technology, Stockholm, Sweden; Department of Information and Technology, Uppsala University, Sweden; Robotics, Perception and Learning Lab, EECS at KTH Royal Institute of Technology, Stockholm, Sweden","2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","28 Jan 2020","2019","","","305","312","In socially assistive robotics, an important research area is the development of adaptation techniques and their effect on human-robot interaction. We present a meta-learning based policy gradient method for addressing the problem of adaptation in human-robot interaction and also investigate its role as a mechanism for trust modelling. By building an escape room scenario in mixed reality with a robot, we test our hypothesis that bi-directional trust can be influenced by different adaptation algorithms. We found that our proposed model increased the perceived trustworthiness of the robot and influenced the dynamics of gaining human's trust. Additionally, participants evaluated that the robot perceived them as more trustworthy during the interactions with the meta-learning based adaptation compared to the previously studied statistical adaptation model.","2153-0866","978-1-7281-4004-9","10.1109/IROS40897.2019.8967924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967924","","","gradient methods;human-robot interaction;learning (artificial intelligence);service robots;social aspects of automation","human trust;perceived trustworthiness;escape room scenario;statistical adaptation model;meta-learning based adaptation;bi-directional trust;meta-learning based policy gradient method;adaptation techniques;socially assistive robotics;human-robot interaction;trust modelling;meta-reinforcement learning","","5","","44","IEEE","28 Jan 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Home Energy Management System for Resiliency","N. S. Raman; N. Gaikwad; P. Barooah; S. P. Meyn","University of Florida, Gainesville, Florida, USA; University of Florida, Gainesville, Florida, USA; University of Florida, Gainesville, Florida, USA; University of Florida, Gainesville, Florida, USA","2021 American Control Conference (ACC)","28 Jul 2021","2021","","","1358","1364","With increase in the frequency of natural disasters such as hurricanes that disrupt the supply from the grid, there is a greater need for resiliency in electric supply. Rooftop solar photovoltaic (PV) panels along with batteries can provide resiliency to a house in a blackout due to a natural disaster. Our previous work showed that intelligence can reduce the size of a PV+battery system for the same level of post-blackout service compared to a conventional system that does not employ intelligent control. The intelligent controller proposed is based on model predictive control (MPC), which has two main challenges. One, it requires simple yet accurate models as it involves real-time optimization. Two, the discrete actuation for residential loads (on/off) makes the underlying optimization problem a mixed-integer program (MIP) which is challenging to solve. An attractive alternative to MPC is reinforcement learning (RL) as the real-time control computation is both model-free and simple. These points of interest accompany certain trade-offs; RL requires computationally expensive offline learning, and its performance is sensitive to various design choices. In this work, we propose an RL-based controller. We compare its performance with the MPC controller proposed in our prior work and a non-intelligent baseline controller. The RL controller is found to provide a resiliency performance - by commanding critical loads and batteries-similar to MPC with a significant reduction in computational effort.","2378-5861","978-1-6654-4197-1","10.23919/ACC50511.2021.9483162","NSF(grant numbers:1646229,1934322); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9483162","","Computational modeling;Simulation;Refrigerators;Reinforcement learning;Real-time systems;Robustness;Batteries","control engineering computing;disasters;energy management systems;home automation;home computing;integer programming;intelligent control;learning (artificial intelligence);photovoltaic power systems;power engineering computing;power system control;predictive control;roofs","rooftop solar PV panels;computational expensive offline learning;home energy management system;resiliency performance;MPC;mixed-integer program;optimization problem;residential loads;model predictive control;intelligent controller;post-blackout service;PV+battery system;rooftop solar photovoltaic panels;electric supply;natural disaster;reinforcement learning","","4","","25","","28 Jul 2021","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Algorithm for Warehousing Multi-AGV Path Planning","G. Shen; R. Ma; Z. Tang; L. Chang","School of Information, Beijing Wuzi University, Beijing, China; School of Information, Beijing Wuzi University, Beijing, China; School of Information, Beijing Wuzi University, Beijing, China; School of Information, Beijing Wuzi University, Beijing, China","2021 International Conference on Networking, Communications and Information Technology (NetCIT)","11 Mar 2022","2021","","","421","429","The problem of AGV path planning has become a key technical problem in the fields of cargo transportation and rapid sorting. Due to the continuous expansion of the scale of application scenarios, more AGV cooperation is required. Traditional planning models are difficult to coordinate path planning between multiple AGVs. Aiming at the warehouse layout scenario of the Kiva system, the article proposes an A3C algorithm combined with the Attention mechanism of the MAA3C algorithm. The advantage function and entropy are used for training to better solve the path planning problem of the unequal number of picking shelves in the storage multi-pick station, MAAC (Uniform), MADDPG, MADDPG+SAC, and COMA+SAC algorithms for comparison. The simulation results show that the MAA3C algorithm has a better convergence effect than other algorithms, the convergence time is 1/3 of other algorithms, and the average reward is increased by about 25%. Therefore, this algorithm can effectively solve the optimization problem of picking path planning under the ""goods to people"" mode, and improve the collaboration and scalability of multiple AGVs in the warehouse.","","978-1-6654-0070-1","10.1109/NetCIT54147.2021.00090","National Natural Science Foundation of China; Beijing Key Laboratory of Intelligent Logistics System; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730895","intelligent storage;path optimization;deep reinforcement learning;asynchronous advantage actor-critic;attention mechanism","Training;Scalability;Simulation;Layout;Warehousing;Collaboration;Transportation","automatic guided vehicles;learning (artificial intelligence);mobile robots;optimisation;path planning;warehouse automation;warehousing","deep reinforcement;multiAGV path planning;AGV path planning;key technical problem;cargo transportation;rapid sorting;continuous expansion;AGV cooperation;traditional planning models;multiple AGVs;warehouse layout scenario;MAA3C;advantage function;entropy;path planning problem;storage multipick station;optimization problem","","2","","16","IEEE","11 Mar 2022","","","IEEE","IEEE Conferences"
"Multi-objective Reinforcement Learning based approach for User-Centric Power Optimization in Smart Home Environments","S. Gupta; S. Bhambri; K. Dhingra; A. B. Buduru; P. Kumaraguru","Indraprastha Institute of Information Technology, Delhi, India; Delhi Technological University, Delhi, India; Indraprastha Institute of Information Technology, Delhi, India; Indraprastha Institute of Information Technology, Delhi, India; Indraprastha Institute of Information Technology, Delhi, India","2020 IEEE International Conference on Smart Data Services (SMDS)","24 Dec 2020","2020","","","89","96","Smart homes require every device inside them to be connected with each other at all times, which leads to a lot of power wastage on a daily basis. As the devices inside a smart home increase, it becomes difficult for the user to control or operate every individual device optimally. Therefore, users generally rely on power management systems for such optimization but often are not satisfied with the results. In this paper, we present a novel multi-objective reinforcement learning framework with two-fold objectives of minimizing power consumption and maximizing user satisfaction. The framework explores the trade-off between the two objectives and converges to a better power management policy when both objectives are considered while finding an optimal policy. We experiment on real-world smart home data, and show that the multi-objective approaches: i) establish trade-off between the two objectives, ii) achieve better combined user satisfaction and power consumption than single-objective approaches. We also show that the devices that are used regularly and have several fluctuations in device modes at regular intervals should be targeted for optimization, and the experiments on data from other smart homes fetch similar results, hence ensuring transfer-ability of the proposed framework.","","978-1-7281-8777-8","10.1109/SMDS49396.2020.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288505","power management;multi-objective reinforcement learning (MORL);sequential decision-making","Performance evaluation;Power demand;Fluctuations;Power system management;Smart homes;Reinforcement learning;Optimization","buildings (structures);home automation;learning (artificial intelligence);optimisation;power aware computing","user-centric power optimization;power wastage;power management systems;multiobjective reinforcement learning;two-fold objectives;power consumption;user satisfaction maximization;power management policy;optimal policy;smart home data;single-objective approaches","","1","","15","IEEE","24 Dec 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning Driven Physical Synthesis : (Invited Paper)","Z. He; L. Zhang; P. Liao; Y. Ma; B. Yu","Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong","2020 IEEE 15th International Conference on Solid-State & Integrated Circuit Technology (ICSICT)","21 Dec 2020","2020","","","1","4","Physical synthesis has emerged as a core component in a modern circuit design flow. Large-scale optimization problem is often involved in the process, which requires substantial efforts to solve and no optimality is guaranteed. Reinforcement learning provides one direction to deal with the above issue by automatically acquiring knowledge through experience, which has shown great success in various applications. In this paper, we introduce the foundation of and the progress in reinforcement learning, and review some recent approaches in applying reinforcement learning to physical synthesis. We hope to inspire more work and to see more talented ideas in this field.","","978-1-7281-6235-5","10.1109/ICSICT49897.2020.9278350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9278350","","Reinforcement learning;Mathematical model;Training;Optimization;Monte Carlo methods;Routing;Prediction algorithms","electronic design automation;knowledge acquisition;learning (artificial intelligence);network synthesis","reinforcement learning;physical synthesis;circuit design flow;large-scale optimization problem","","1","","37","IEEE","21 Dec 2020","","","IEEE","IEEE Conferences"
"Altruism and Selfishness in Believable Game Agents: Deep Reinforcement Learning in Modified Dictator Games","D. Daylamani-Zad; M. C. Angelides","College of Engineering, Design and Physical Sciences, Brunel University London, London, U.K.; College of Engineering, Design and Physical Sciences, Brunel University London, London, U.K.","IEEE Transactions on Games","15 Sep 2021","2021","13","3","229","238","This article focuses on using deep reinforcement learning, specifically proximity policy optimization, to train agents in a social dilemma game, modified dictator game, in order to investigate the effect of selfishness and altruism on the believability of the game agents. We present the design and implementation of the training environment, including the reward functions which are based on the findings of established empirical research, with three agent profiles mapped to the three standard constant elasticity of substitution (CES) utility functions, i.e., selfish, perfect substitutes, and Leontief, which measure different levels of selfishness/altruism. The trained models are validated and then used in a sample game, which is used to evaluate the believability of the three agent profiles using the agent believability metrics. The results indicate that players find altruistic behaviour more believable and consider selfishness less so. Analysis of the results indicates that human-like behavior resulting from the application of artificial intelligence evolves from perceived human behavior rather than the observed. The analysis also indicates that selfishness/altruism may be considered as an extra dimension to be included in the believability metrics.","2475-1510","","10.1109/TG.2020.2989636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079189","Agents;believability;deep reinforcement learning;dictator game;proximal policy optimization (PPO)","Games;Optimization;Neural networks;Mathematical model;Training;Reinforcement learning","behavioural sciences computing;computer games;game theory;learning (artificial intelligence);social aspects of automation;social sciences","believability metrics;believable game agents;deep reinforcement learning;modified dictator game;specifically proximity policy optimization;social dilemma game;training environment;reward functions;agent profiles;standard constant elasticity;selfish substitutes;perfect substitutes;trained models;sample game","","1","","49","CCBY","27 Apr 2020","","","IEEE","IEEE Journals"
"Contrastive Learning Methods for Deep Reinforcement Learning","D. Wang; M. Hu","Department of Mechanical and Industrial Engineering, University of Illinois at Chicago, Chicago, IL, USA; Department of Mechanical and Industrial Engineering, University of Illinois at Chicago, Chicago, IL, USA","IEEE Access","13 Sep 2023","2023","11","","97107","97117","Deep reinforcement learning (DRL) has shown promising performance in various application areas (e.g., games and autonomous vehicles). Experience replay buffer strategy and parallel learning strategy are widely used to boost the performances of offline and online deep reinforcement learning algorithms. However, state-action distribution shifts lead to bootstrap errors. Experience replay buffer learns policies with elder experience trajectories, limiting its application to off-policy algorithms. Balancing the new and the old experience is challenging. Parallel learning strategies can train policies with online experiences. However, parallel environmental instances organize the agent pool inefficiently with higher simulation or physical costs. To overcome these shortcomings, we develop four lightweight and effective DRL algorithms, instance-actor, parallel-actor, instance-critic, and parallel-critic methods, to contrast different-age trajectory experiences. We train the contrast DRL according to the received rewards and proposed contrast loss, which is calculated by designed positive/negative keys. Our benchmark experiments using PyBullet robotics environments show that our proposed algorithm matches or is better than the state-of-the-art DRL algorithms.","2169-3536","","10.1109/ACCESS.2023.3312383","National Science Foundation(grant numbers:CMMI 1634738); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10242114","Contrastive learning;deep reinforcement learning;different-age experience;experience replay buffer;parallel learning","Reinforcement learning;Deep learning;Trajectory;Training;Neural networks;Estimation;Dictionaries","control engineering computing;deep learning (artificial intelligence);mobile robots;reinforcement learning","benchmark experiments;buffer strategy;contrast DRL;contrastive learning methods;different-age trajectory experiences;effective DRL algorithms;elder experience trajectories;lightweight DRL algorithms;off-policy algorithms;online deep reinforcement learning algorithms;parallel environmental instances;parallel learning strategies;parallel-actor method;parallel-critic methods;state-action distribution shifts","","","","51","CCBYNCND","6 Sep 2023","","","IEEE","IEEE Journals"
"Matching and generation method of virtual terminals in smart substation based on reinforcement learning and semantic analysis","X. Peng; D. Xiaobin; P. Ye; C. Xu; L. Hongjun; L. Jin","Power Dispatching and Control, Center of China Southern Power Grid, Guangzhou, China; Power Dispatching and Control, Center of China Southern Power Grid, Guangzhou, China; Power Dispatching and Control, Center of China Southern Power Grid, Guangzhou, China; Power Dispatching and Control, Center of China Southern Power Grid, Guangzhou, China; CYG SUNRI Co., Ltd., Shenzhen, China; CYG SUNRI Co., Ltd., Shenzhen, China","2021 International Conference on Power System Technology (POWERCON)","8 Feb 2022","2021","","","2434","2439","Aiming at the problems of long time, heavy workload and error prone in virtual terminal connection of secondary equipment in smart substation, an automatic matching and generation method of virtual terminal in smart substation based on reinforcement learning and semantic analysis is proposed. Firstly, according to the similarity of input and output virtual terminals in data format and text description, the semantic similarity of virtual connections is calculated through historical virtual connection data, and the automatic matching model of virtual terminals between secondary devices is established; Secondly, the reinforcement learning method is introduced into the semantic similarity calculation model to automatically group the text, and the reinforcement learning algorithm is used on the semantic similarity calculation Siamese network model. It improves the semantic loss faced by the long-term and short-term memory model when extracting the semantics of sentences; Finally, combined with the actual situation of virtual terminal connection, the evaluation index for testing the virtual terminal automatic connection method is designed, and the example of a 220 kV smart substation in South China Power Grid shows that this method can effectively reduce the virtual terminal with wrong connection, reduce the manual workload, and effectively improve the virtual terminal connection efficiency.","2642-6226","978-1-6654-0737-3","10.1109/POWERCON53785.2021.9697701","China Southern Power Grid; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9697701","smart substation;secondary system;virtual terminal;semantic analysis;typical interval;virtual connection semantic similarity;reinforcement learning","Wiring;Substations;Semantics;Memory management;Reinforcement learning;Voltage;Data models","power engineering computing;power grids;recurrent neural nets;reinforcement learning;substation automation;substations","reinforcement learning;semantic similarity calculation Siamese network model;short-term memory;smart substation;virtual terminal connection;semantic analysis;long-term memory;virtual terminal matching;virtual terminal generation;South China power grid","","","","12","IEEE","8 Feb 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Architectures for Dynamic Generation of Smart Home Services","M. Qiu; E. Najm; R. Sharrock; B. Traverson","EDF R&D, Télécom Paris, Palaiseau, France; Télécom Paris, Palaiseau, France; Télécom Paris, Palaiseau, France; EDF R&D, Palaiseau, France","2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)","23 Mar 2023","2022","","","7","14","A smart home system is realized by implementing various services. However, the design and deployment of smart home services are challenging due to their complexity and the large number of connected objects. Existing approaches to the smart home system to create services either require complex input from the inhabitant or can only work if the inhabitant specifies regulation solutions rather than targets. In addition, smart home services may conflict if they access the same actuators. Learning methods to dynamically generate smart home services are promising ways to solve the above problems. In this paper, depending on the ability to consider the composition of services and their mutual influence, we propose several reinforcement learning-based architectures for a smart home system to dynamically generate services. The expected advantages are, first, that the smart home services can propose the states of the actuators by considering the target values of the controllable environment states given by the inhabitant or by interacting with the inhabitant in a simple and natural way; and second, that there is no conflict between these propositions. We compare the performance of the proposed architectures using several simulated smart home environments with different services and select the architectures with the best performance concerning our predefined metrics.","","978-1-6654-6283-9","10.1109/ICMLA55696.2022.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10069910","service orientation;reinforcement learning;multi-agent;smart home","Measurement;Learning systems;Actuators;Architecture;Knowledge based systems;Smart homes;Reinforcement learning","home automation;reinforcement learning","dynamically generate services;reinforcement learning-based architectures;simulated smart home environments;smart home service dynamic generation;smart home system","","","","20","IEEE","23 Mar 2023","","","IEEE","IEEE Conferences"
"Energy Efficient Scheduling in Smart Home using Deep Reinforcement Learning","A. Roslann; F. A. Asuhaimi; K. N. Z. Ariffin","Department of Electronic Engineering, Universiti Sains Islam Malaysia, Nilai, Malaysia; Department of Electronic Engineering, Universiti Sains Islam Malaysia, Nilai, Malaysia; Department of Electronic Engineering, Universiti Sains Islam Malaysia, Nilai, Malaysia","2022 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET)","9 Nov 2022","2022","","","1","6","In a smart home, the scheduling of the period of time that household appliances are allowed to be operational necessarily requires the adjustment of multiple parameters in accordance with the amount of available energy. Nevertheless, the scheduling of the operational time of multiple appliances in a smart home itself is a difficult problem, and as a result, it requires an intelligent, heuristic method in order to be solved in polynomial time. In this piece of research, we propose scheduling of household appliances based on a well-known value iterative reinforcement learning technique called Quality learning. This technique is used to learn values over time. The proposed method will be carried out in two stages. The first step in the Q learning process involves the agents interacting with the environment of the smart home in order to earn a reward for their efforts. The value of the reward is then used to schedule the operating times of various household appliances in the subsequent state so that the total amount of energy consumed is kept to a minimum. In the second phase, the user's dissatisfaction is maintained due to the scheduling of the household appliances. This is accomplished by classifying the household appliances into two groups: shiftable and non-shiftable. In addition, by making use of the phenomenon of shared memory synchronisation, the agents that are connected to each individual appliance in a smart home become synchronised. The simulations are carried out in a model of a smart home that consists of a single person and a number of different types of appliances. It has come to our attention that, in contrast to manual scheduling algorithm and scheduling that was based on a demand-response strategy, the operational time of the household appliances has been revealed to be effectively scheduled in order to reduce the amount of energy that is consumed.","","978-1-6654-6837-4","10.1109/IICAIET55139.2022.9936765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9936765","smart home;deep reinforcement learning;scheduling;smart meter;energy efficiency","Home appliances;Schedules;Q-learning;Scheduling algorithms;Smart homes;Load management;Scheduling","building management systems;deep learning (artificial intelligence);demand side management;domestic appliances;home automation;iterative methods;power engineering computing;reinforcement learning;scheduling;smart meters","demand-response strategy;Q learning process;quality learning;operating times;iterative reinforcement learning technique;deep reinforcement learning;energy efficient scheduling;manual scheduling algorithm;smart home;household appliances","","","","14","IEEE","9 Nov 2022","","","IEEE","IEEE Conferences"
"Control-Logic Synthesis of Fully Programmable Valve Array Using Reinforcement Learning","X. Huang; H. Cai; W. Guo; G. Liu; T. -Y. Ho; K. Chakrabarty; U. Schlichtmann","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, Hong Kong; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, USA; Chair of Electronic Design Automation, Technical University of Munich, Germany","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2023","PP","99","1","1","Fully programmable valve array (FPVA) biochips have emerged as a promising alternative for traditional application-specific microfluidic platforms thanks to their advantages in terms of flexibility and reconfigurability. By regularly deploying microvalves along vertical and horizontal flow channels, microfluidic modules with different sizes and shapes can be constructed dynamically on the chip, thereby enabling the automatic execution of various assay procedures in biology and biochemistry. The above advantages, however, result largely from the large-scale integration of valves as well as accurate control of their switchings, leading to very complicated control-logic design of such chips. In this paper, we propose a reinforcement learning-based synthesis flow for the control-logic design of FPVA biochips, taking multichannel switching and control-cost minimization into consideration simultaneously. By employing a double deep Q-network and two Boolean-logic simplification techniques, control logics with both high switching efficiency and low fabrication cost can be constructed automatically. Furthermore, the solution space of multichannel-switching combinations is reduced to improve the search efficiency of the proposed method. Experimental results on multiple benchmarks demonstrate that the proposed synthesis flow leads to better design solutions compared with the state-of-the-art techniques.","1937-4151","","10.1109/TCAD.2023.3309740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10233701","Boolean logic simplification;control logic;fully programmable valve array;microfluidics;reinforcement learning","Valves;Control systems;Switches;Multiplexing;Cells (biology);Mixers;Image color analysis","","","","","","","IEEE","29 Aug 2023","","","IEEE","IEEE Early Access Articles"
"An Automatic Configuration Memetic Algorithm with Reinforcement Learning Mechanism for Real-value Optimization Problems","Z. Wang; Y. Li; P. Fu","School of Computer and Communication Technology, Lanzhou University of Technology, Lanzhou, China; School of Computer and Communication Technology, Lanzhou University of Technology, Lanzhou, China; School of Computer and Communication Technology, Lanzhou University of Technology, Lanzhou, China","2023 26th International Conference on Computer Supported Cooperative Work in Design (CSCWD)","22 Jun 2023","2023","","","131","136","With the development of reinforcement learning technology, reinforcement learning is utilized in many areas. The real-value optimization problem is common and difficult in manufacturing. In this study, a memetic algorithm (called ACMA) with reinforcement learning mechanism is designed for solving real-value optimization problems. The multi-heuristic pool which contains various heuristic algorithms is employed in ACMA to realize the automatic configuration of global and local search algorithms via Q-learning. The process of ACMA is mainly divided into two phases, including the training phase and the testing phase. The effective heuristic algorithms are selected from the multi-heuristic pool during the training phase, and effective heuristic algorithms are learned in the testing phase. The ACMA, standard MA, and other algorithms are utilized to address CEC 2017 benchmark problems. The results show that ACMA is a potential algorithm to address real-value optimization problems.","2768-1904","979-8-3503-3168-4","10.1109/CSCWD57460.2023.10152810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10152810","Memetic algorithm;Reinforcement learning;Real-value optimization problem;Automatic configuration","Memetics;Training;Q-learning;Federated learning;Heuristic algorithms;Benchmark testing;Manufacturing","evolutionary computation;learning (artificial intelligence);optimisation;reinforcement learning;search problems","ACMA;automatic configuration memetic algorithm;effective heuristic algorithms;global search algorithms;local search algorithms;multiheuristic pool;potential algorithm;Q-learning;real-value optimization problem;reinforcement learning mechanism;testing phase;training phase","","","","25","IEEE","22 Jun 2023","","","IEEE","IEEE Conferences"
"Intelligent Farm Based on Deep Reinforcement Learning for optimal control","H. M. Yassine; K. Roufaida; V. P. Shkodyrev; M. Abdelhak; L. Zarour; R. Khaled","World-class Scientific Center “Advanced Digital Technologies”, Peter the Great St. Petersburg Polytechnic University (SPbPU), St. Petersburg, RUSSIA; Computer Sciences Departement, Linfi Laboratory, Mohamed Khider University of Biskra, Biskra, ALGERIA; World-class Scientific Center “Advanced Digital Technologies”, Peter the Great St. Petersburg Polytechnic University (SPbPU), St. Petersburg, RUSSIA; Computer Sciences Departement, Linfi Laboratory, Mohamed Khider University of Biskra, Biskra, ALGERIA; Higher School of Cyber physical Systems and Management, Peter the Great St. Petersburg Polytechnic University (SPbPU), St. Petersburg, RUSSIA; Computer Sciences Departement, Linfi Laboratory, Mohamed Khider University of Biskra, Biskra, ALGERIA","2022 International Symposium on iNnovative Informatics of Biskra (ISNIB)","28 Mar 2023","2022","","","1","6","Smart agriculture is the use of automation, sensors, and data to improve crop yields, reduce costs and increase crop quality. Predictive analytics, machine learning, and artificial intelligence are used. Another benefit of using artificial intelligence in agriculture is the ability to optimize the consumption of resources such as water and fertilizer to achieve the greatest yield. However, to date, the technology has not been used to optimize the management of agricultural operations. The objective of this paper is to introduce the concept of smart farming and discuss applying deep reinforcement learning for optimal management in the field. We propose a smart farming system that uses Deep Reinforcement Learning to optimize the operation of a farm. The system uses Deep Learning to recognize patterns in a large dataset of real-world agricultural data, such as crop yields and weather, and uses this knowledge to recommend actions. It also dynamically learns optimal control policies for a variety of crops, depending on the farmer's goals and current conditions.","","979-8-3503-2065-7","10.1109/ISNIB57382.2022.10076088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10076088","Deep Reinforcement Learning;Optimal Control Systems;Smart Farming;Multi-Agent System;Cognitive System","Deep learning;Smart agriculture;Crops;Optimal control;Reinforcement learning;Production;Reliability","agriculture;artificial intelligence;crops;deep learning (artificial intelligence);farming;fertilisers;learning (artificial intelligence);optimal control;reinforcement learning","agricultural operations;artificial intelligence;crop quality;crop yields;deep reinforcement learning;greatest yield;intelligent farm;machine learning;optimal control policies;optimal management;real-world agricultural data;smart agriculture;smart farming system","","","","16","IEEE","28 Mar 2023","","","IEEE","IEEE Conferences"
"DRSAL: Deep Reinforcement Skin Cancer Diagnosis with Active Learning Technique","R. G; A. Senthilselvi","Dept of CSE, SRM Institute of Science and Technology, Chennai, India; Dept of CSE, SRM Institute of Science and Technology, Chennai, India","2022 Third International Conference on Smart Technologies in Computing, Electrical and Electronics (ICSTCEE)","18 Apr 2023","2022","","","1","7","Malignancy is the most dangerous disease that causes higher fatality rate among humans. Albeit, different malignancy found in recent times, skin cancer is considered as highly contagious and affects human life in a slower pace. Current development of skin imaging modality such as Dermoscopy, clinicians can able to diagnose the disease quickly. With less availability of experts around the world, automating the disease diagnosis helps to start the treatment earlier. Advancement of deep learning techniques helps this automation procedure for disease diagnosis. Main problem behind this algorithm to get good accuracy, is that it needs more amount of labelled input data. Labelling huge input data is a time-consuming process and the chance for getting experts in medical field to label the input data is very less. We propose a novel algorithm which can train themselves with less data available and predicts and annotate the remaining dataset available with less intervention of human experts. This is possible with the improvement of deep reinforcement learning algorithm which smartly analyses the given data and diagnose the disease by providing a smaller number of datasets with active learning technique. Multi-Agent in the reinforcement algorithm helps to learn the important features from the given class and based on the similarity it aligns the input data of individual class accordingly. Each agent shares the gained information to other agent in order to improvise the learning method in a quicker way and start labelling or predicting the remaining unknown dataset. The proposed method Deep Reinforcement Skin Cancer Diagnosis with Active Learning Technique (DRSAL) achieves higher accuracy of 94.2%in diagnosing the disease compared with other popular deep learning and active learning algorithms.","","978-1-6654-5664-7","10.1109/ICSTCEE56972.2022.10100191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10100191","Deep Reinforcement learning;Deep Learning;Skin Cancer;Melanoma","Deep learning;Industries;Training data;Reinforcement learning;Prediction algorithms;Skin;Medical diagnosis","cancer;deep learning (artificial intelligence);diseases;learning (artificial intelligence);medical computing;medical image processing;patient diagnosis;reinforcement learning;skin","active learning algorithms;active learning technique;dangerous disease;deep learning techniques;deep reinforcement learning algorithm;Deep Reinforcement Skin Cancer Diagnosis;different malignancy;disease diagnosis;higher fatality rate;human experts;human life;labelled input data;labelling huge input data;learning method;popular deep learning;reinforcement algorithm;time-consuming process","","","","55","IEEE","18 Apr 2023","","","IEEE","IEEE Conferences"
"Evolving internal reinforcers for an intrinsically motivated reinforcement-learning robot","M. Schembri; M. Mirolli; G. Baldassarre","Laboratory of Autonomous Robotics and Artificial Life, Consiglio Nazionale delle Ricerche (LARAL-ISTC-CNR), Istituto di Scienze e Tecnologie della Cognizione, Rome, Italy; Laboratory of Autonomous Robotics and Artificial Life, Consiglio Nazionale delle Ricerche (LARAL-ISTC-CNR), Istituto di Scienze e Tecnologie della Cognizione, Rome, Italy; Laboratory of Autonomous Robotics and Artificial Life, Consiglio Nazionale delle Ricerche (LARAL-ISTC-CNR), Istituto di Scienze e Tecnologie della Cognizione, Rome, Italy","2007 IEEE 6th International Conference on Development and Learning","22 Oct 2007","2007","","","282","287","Intrinsically motivated reinforcement learning (IMRL) has been proposed as a framework within which agents exploit ""internal reinforcement"" to acquire general-purpose building-block behaviors (""skills"") which can be later combined for solving several specific tasks. The architectures so far proposed within this framework are limited in that: (1) they use hardwired ""salient events"" to form and train skills, and this limits agents' autonomy; (2) they are applicable only to problems with abstract states and actions, as grid-world problems. This paper proposes solutions to these problems in the form of a hierarchical reinforcement-learning architecture that: (1) exploits evolutionary robotics techniques so to allow the system to autonomously discover ""salient events""; (2) uses neural networks so to allow the system to cope with continuous states and noisy environments. The viability of the proposed approach is demonstrated with a simulated robotic scenario.","2161-9476","978-1-4244-1115-3","10.1109/DEVLRN.2007.4354052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4354052","Intrinsically Motivated Reinforcement Learning;Evolutionary Robotics;Actor-Critic;Surprise;Neural Networks","Robots;Organisms;Machine learning;Robotic assembly;Neural networks;Erbium;Testing;Proposals;Contracts;Neuroscience","genetic algorithms;intelligent robots;learning (artificial intelligence);neural nets","intrinsically motivated reinforcement-learning robot;general-purpose building-block behaviors;evolutionary robotics technique;actor-critic neural network architecture;internal reinforcement;genetic algorithm","","45","","22","IEEE","22 Oct 2007","","","IEEE","IEEE Conferences"
"RLScheduler: An Automated HPC Batch Job Scheduler Using Reinforcement Learning","D. Zhang; D. Dai; Y. He; F. S. Bao; B. Xie","Computer Science Department, University of North Carolina at Charlotte; Computer Science Department, University of North Carolina at Charlotte; Computer Science Department, Iowa State University; Computer Science Department, Iowa State University; Oak Ridge National Laboratory, Oak Ridge Leadership Computing Facility","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021","2020","","","1","15","Today's high-performance computing (HPC) platforms are still dominated by batch jobs. Accordingly, effective batch job scheduling is crucial to obtain high system efficiency. Existing HPC batch job schedulers typically leverage heuristic priority functions to prioritize and schedule jobs. But, once configured and deployed by the experts, such priority functions can hardly adapt to the changes of job loads, optimization goals, or system settings, potentially leading to degraded system efficiency when changes occur. To address this fundamental issue, we present RLScheduler, an automated HPC batch job scheduler built on reinforcement learning. RLScheduler relies on minimal manual interventions or expert knowledge, but can learn high-quality scheduling policies via its own continuous `trial and error'. We introduce a new kernel-based neural network structure and trajectory filtering mechanism in RLScheduler to improve and stabilize the learning process. Through extensive evaluations, we confirm that RLScheduler can learn high-quality scheduling policies towards various workloads and various optimization goals with relatively low computation cost. Moreover, we show that the learned models perform stably even when applied to unseen workloads, making them practical for production use.","","978-1-7281-9998-6","10.1109/SC41405.2020.00035","National Science Foundation; Oak Ridge National Laboratory; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355253","","Schedules;Processor scheduling;Neural networks;Reinforcement learning;Production;Trajectory;Optimization","learning (artificial intelligence);neural nets;optimisation;parallel processing;scheduling","RLScheduler;automated HPC batch job scheduler;reinforcement learning;high-performance computing platforms;HPC batch job schedulers;high-quality scheduling policies;kernel-based neural network structure;trajectory filtering mechanism;optimization goals","","31","","46","IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Reinforcement learning and co-operation in a simulated multi-agent system","K. Kostiadis; Huosheng Hu","Department of Computer Science, University of Essex, Colchester, UK; Department of Computer Science, University of Essex, Colchester, UK","Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289)","6 Aug 2002","1999","2","","990","995 vol.2","The complexity of most multi-agent systems prohibits a hand-coded approach to decision-making. In addition to that a complex, dynamic, adversarial environment like the one of a football game makes decision-making and cooperation even more difficult. This paper addresses these problems by using machine learning techniques and agent technology. By gathering useful experience from earlier stages, an agent can significantly improve performance. The method used requires no previous knowledge regarding the environment. Since cooperation in adversarial domains is a very challenging task, the proposed learning algorithm assigns each agent a role to play to achieve a certain goal. By distributing the responsibilities among the agents and linking their goals, an efficient way of cooperation emerges.","","0-7803-5184-3","10.1109/IROS.1999.812809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=812809","","Learning;Multiagent systems;Robotic assembly;Orbital robotics;Computational modeling;Computer simulation;Decision making;Joining processes;Robots;State-space methods","digital simulation;mobile robots;multi-robot systems;sport;games of skill;multi-agent systems;learning (artificial intelligence)","reinforcement learning;cooperation;simulated multi-agent system;robot football game;machine learning;agent technology;adversarial domains;robot soccer","","11","","18","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Composite Experience Replay-Based Deep Reinforcement Learning With Application in Wind Farm Control","H. Dong; X. Zhao","Intelligent Control & Smart Energy (ICSE) Research Group, School of Engineering, University of Warwick, Coventry, U.K.; Intelligent Control & Smart Energy (ICSE) Research Group, School of Engineering, University of Warwick, Coventry, U.K.","IEEE Transactions on Control Systems Technology","13 Apr 2022","2022","30","3","1281","1295","In this article, a deep reinforcement learning (RL)-based control approach with enhanced learning efficiency and effectiveness is proposed to address the wind farm control problem. Specifically, a novel composite experience replay (CER) strategy is designed and embedded in the deep deterministic policy gradient (DDPG) algorithm. CER provides a new sampling scheme that can mine the information of stored transitions in-depth by making a tradeoff between rewards and temporal difference (TD) errors. Modified importance-sampling weights are introduced to the training process of neural networks (NNs) to deal with the distribution mismatching problem induced by CER. Then, our CER-DDPG approach is applied to optimizing the total power production of wind farms. The main challenge of this control problem comes from the strong wake effects among wind turbines and the stochastic features of environments, rendering it intractable for conventional control approaches. A reward regularization process is designed along with the CER-DDPG, which employs an additional NN to handle the bias of rewards caused by the stochastic wind speeds. Tests with a dynamic wind farm simulator (WFSim) show that our method achieves higher rewards with less training costs than conventional deep RL-based control approaches, and it has the ability to increase the total power generation of wind farms with different specifications.","1558-0865","","10.1109/TCST.2021.3102476","U.K. Engineering and Physical Sciences Research Council(grant numbers:EP/S001905/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512381","Intelligent control;model-free control;neural networks (NNs);reinforcement learning (RL);wind farm control","Wind farms;Training;Production;Heuristic algorithms;Artificial neural networks;Wind turbines;Wind speed","learning (artificial intelligence);neural nets;power generation control;wakes;wind power plants;wind turbines","composite experience replay-based deep reinforcement learning;deep reinforcement learning-based control approach;enhanced learning efficiency;wind farm control problem;novel composite experience replay strategy;deep deterministic policy gradient algorithm;sampling scheme;stored transitions in-depth;temporal difference errors;modified importance-sampling weights;distribution mismatching problem;CER-DDPG approach;wind farms;strong wake effects;wind turbines;conventional control approaches;reward regularization process;stochastic wind speeds;dynamic wind farm simulator;conventional deep RL-based control approaches","","10","","43","IEEE","12 Aug 2021","","","IEEE","IEEE Journals"
"Intelligent Information Fusion for Conflicting Evidence Using Reinforcement Learning and Dempster-Shafer Theory","F. Huang; Y. Zhang; W. Jiang; Y. He; X. Deng","School of Electronics and Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China","2021 IEEE International Conference on Unmanned Systems (ICUS)","22 Dec 2021","2021","","","190","195","Multi-sensor information fusion is a information fusion technology to improve system performance, which plays a key role in actual production and application. Dempster-Shafer theory (DST) can achieve information fusion due to the effectiveness of processing uncertain information without prior probabilities. However, when the evidence is conflicting, it may produce counter-intuitive judgments. In addition, the existing methods need to obtain all sensors information to deal with the conflict, which make them impossible to realize real-time fusion of online information in practice. In order to solve the above problems, we propose an intelligent information fusion method based on the reinforcement learning (RL) and DST, named the DST-RL method. Specifically, the introduction of artificial intelligence technology to realize adaptive conflict processing, which can achieve effective removal of inaccuracy information and avoid the inaccuracy caused by human intervention. Then the Dempster's combination rule (DCR) is adopted to achieve effective fusion of multi-sensor information. On the one hand, the DST-RL method can realize efficient multi-sensor information fusion. On the other hand, it can reduce the complexity of the system when the amount of information is large. Numerical example and application simulation show that our proposed intelligent information fusion method can achieve significant performance superiority in processing online conflicting information.","","978-1-6654-3885-8","10.1109/ICUS52573.2021.9641305","National Natural Science Foundation of China(grant numbers:61671384); Northwestern Polytechnical University(grant numbers:CX2021078); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9641305","multi-sensor information fusion;Dempster-Shafer theory;reinforcement learning;Dempster's combination rule","Deep learning;Fuses;System performance;Simulation;Reinforcement learning;Production;Sensor fusion","probability;reinforcement learning;sensor fusion;uncertainty handling","Dempster-Shafer theory;sensors information;real-time fusion;online information;reinforcement learning;DST-RL method;artificial intelligence technology;adaptive conflict processing;inaccuracy information;conflicting information;intelligent information fusion;multisensor information fusion;Dempster combination rule;DCR;online conflicting information processing","","2","","20","IEEE","22 Dec 2021","","","IEEE","IEEE Conferences"
"Implementing Reinforcement Learning Datacenter Congestion Control in NVIDIA NICs","B. Fuhrer; Y. Shpigelman; C. Tessler; S. Mannor; G. Chechik; E. Zahavi; G. Dalal",NVIDIA Networking; NVIDIA Networking; TNVIDIA Research; TNVIDIA Research; TNVIDIA Research; NVIDIA Networking; TNVIDIA Research,"2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","10 Jul 2023","2023","","","331","343","As communication protocols evolve, datacenter network utilization increases. As a result, congestion is more frequent, causing higher latency and packet loss. Combined with the increasing complexity of workloads, manual design of congestion control (CC) algorithms becomes extremely difficult. This calls for the development of AI approaches to replace the human effort. Unfortunately, it is currently not possible to deploy AI models on network devices due to their limited computational capabilities. Here, we offer a solution to this problem by building a computationally-light solution based on a recent reinforcement learning CC algorithm [1, RL-CC]. We reduce the inference time of RL-CC by x500 by distilling its complex neural network into decision trees. This transformation enables real-time inference within the μ-sec decision-time requirement, with a negligible effect on quality. We deploy the transformed policy on NVIDIA NICs in a live cluster. Compared to popular CC algorithms used in production, RL-CC is the only method that performs well on all benchmarks tested over a large range of number of flows. It balances multiple metrics simultaneously: bandwidth, latency, and packet drops. These results suggest that data-driven methods for CC are feasible, challenging the prior belief that handcrafted heuristics are necessary to achieve optimal performance.","","979-8-3503-0119-9","10.1109/CCGrid57682.2023.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10171517","datacenter networks;reinforcement learning;distillation;congestion control;gradient boosting trees;RDMA","Protocols;Neural networks;Clustering algorithms;Packet loss;Reinforcement learning;Production;Manuals","computer centres;decision trees;deep learning (artificial intelligence);protocols;reinforcement learning;telecommunication congestion control","AI approaches;AI models;communication protocols;complex neural network;computational capabilities;computationally-light solution;datacenter network utilization increases;decision trees;higher latency;implementing reinforcement learning datacenter congestion control;inference time;manual design;network devices;NVIDIA NICs;packet drops;packet loss;popular CC algorithms;real-time inference;recent reinforcement;RL-CC;transformed policy;μ-sec decision-time requirement","","1","","50","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"A Reinforcement-Learning-based Agent to discover Safety-Critical States in Smart Grid Environments","A. Santorsola; A. Maci; P. Delvecchio; A. Coscia","CyberSecurity Lab BV TECH S.p.A., Milan, Italy; CyberSecurity Lab BV TECH S.p.A., Milan, Italy; CyberSecurity Lab BV TECH S.p.A., Milan, Italy; CyberSecurity Lab BV TECH S.p.A., Milan, Italy","2023 3rd International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)","22 Sep 2023","2023","","","1","7","The complexity of industrial systems and processes has grown significantly in recent years, due to the integration of Information Technology (IT) and Operational Technology (OT) to monitor and control interconnected equipment in critical infrastructures, improving their production processes. Smart Grids are one of the possible examples of technology enabled by IT/OT integration. However, such energy distribution systems are exposed to several vulnerabilities that make them particularly susceptible to cyber threats with critical implications for human safety. Sophisticated attacks against OT infrastructures show few observable indicators in a large timeframe and traditional fault detection methods are ineffective in discovering safety-critical states, especially in large observation and action spaces.This paper presents a methodology to identify safety-critical command patterns within a Smart Grid ICS network. In particular, a modular simulation framework that embeds physical processes simulation, industrial-specific control protocols virtualization, as well as a Reinforcement Learning algorithm has been developed. The preliminary results in training and test cases have demonstrated the modeling and learning capability of the proposed approach.","","979-8-3503-2297-2","10.1109/ICECCME57830.2023.10252540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10252540","Smart Grid;Mathematical Modelling;Reinforcement Learning;Cyber Security & Safety;Energy Transmission;and Distribution","Training;Q-learning;Protocols;Process control;Production;Smart grids;Safety","critical infrastructures;cyber-physical systems;fault diagnosis;multi-agent systems;power engineering computing;reinforcement learning;safety;smart power grids","action spaces;critical infrastructures;energy distribution systems;industrial-specific control protocols virtualization;information technology;interconnected equipment;IT-OT integration;modular simulation framework;operational technology;physical processes simulation;reinforcement-learning-based agent;safety-critical command patterns;smart grid ICS network;traditional fault detection methods","","","","36","IEEE","22 Sep 2023","","","IEEE","IEEE Conferences"
"Optimal Order Acceptance and Scheduling via Deep Reinforcement Learning","J. Qian; C. Chen; K. Wu; L. Yu","College of Systems Engineering, National University of Defense Technology, Changsha, Hunan; College of Systems Engineering, National University of Defense Technology, Changsha, Hunan; College of Systems Engineering and College of Electronic Sciences, National University of Defense Technology, Changsha, Hunan; Ningbo Lensee Intelligent Technology Co., Ltd, Ningbo, Zhejiang","2022 6th International Symposium on Computer Science and Intelligent Control (ISCSIC)","14 Mar 2023","2022","","","63","68","Order acceptance and scheduling (OAS) in a vital problem for the management of production, storage, transportation processes. In an OAS problem, a decision maker should determine whether or not to accept a newly arrived order, and upon an order accomplished, it should further determine which order will be produced subsequently. This paper proposes an MDP for modeling the OAS problem, where a decision is made by jointly considering multiple features including price, quantity and latest delivery date, delayed delivery cost, and others. Furthermore, a deep reinforcement learning algorithm is developed for solving the optimal OAS policy, which outperforms existing schemes in numerical simulations.","","978-1-6654-5488-9","10.1109/ISCSIC57216.2022.00024","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10063650","Order acceptance and scheduling;Markov decision process;Deep reinforcement learning","Deep learning;Neural networks;Transportation;Process control;Reinforcement learning;Production;Numerical simulation","decision making;deep learning (artificial intelligence);learning (artificial intelligence);Markov processes;optimisation;order processing;reinforcement learning;scheduling","decision maker;deep reinforcement learning algorithm;delayed delivery cost;latest delivery date;newly arrived order;OAS problem;optimal OAS policy;optimal order acceptance;transportation processes;vital problem","","","","17","IEEE","14 Mar 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning based Cloud-native Network Function Placement in Private 5G Networks","J. Kim; J. Lee; T. Kim; S. Pack","School of Electrical Engineering, Korea University, Seoul, Korea; School of Electrical Engineering, Korea University, Seoul, Korea; School of Electrical Engineering, Korea University, Seoul, Korea; School of Electrical Engineering, Korea University, Seoul, Korea","2020 IEEE Globecom Workshops (GC Wkshps","5 Mar 2021","2020","","","1","6","With the advantages of satisfying service requirements and providing high security, standalone private fifth generation (5G) network is perceived as a promising technology for vertical industries. However, to manage the cloud-native network functions (CNFs) in an effective manner, a sophisticated control plane management scheme should be designed in standalone private 5G networks. In this paper, we propose a deep Q-network based CNF placement algorithm (DQN-CNFPA), that jointly minimizes the cost occurred in launching and operating CNFs on edge clouds and the back-haul control traffic overhead. In addition, DQN-CNFPA learns spatiotemporal patterns in service requests and places CNFs in consideration of future cost leveraged by the previous CNF placement strategy. Evaluation results demonstrate that DQN-CNFPA can reduce the cost per hour up to 11.2% compared to the scheme without learning spatiotemporal service request patterns.","","978-1-7281-7307-8","10.1109/GCWkshps50303.2020.9367481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9367481","","Industries;Fluctuations;5G mobile communication;Reinforcement learning;Spatiotemporal phenomena;Security;Optimization","5G mobile communication;cloud computing;deep learning (artificial intelligence);mobility management (mobile radio);telecommunication computing;telecommunication security;telecommunication traffic;virtualisation","standalone private fifth generation network;sophisticated control plane management scheme;standalone private 5G networks;DQN-CNFPA;edge clouds;back-haul control traffic overhead;service requests;spatiotemporal service request patterns;deep reinforcement learning based cloud-native network function placement;CNF placement strategy;deep Q-network based CNF placement algorithm","","5","","15","IEEE","5 Mar 2021","","","IEEE","IEEE Conferences"
"Blockchain-Enabled Computing Resource Trading: A Deep Reinforcement Learning Approach","Z. Xie; R. Wu; M. Hu; H. Tian","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","2020 IEEE Wireless Communications and Networking Conference (WCNC)","19 Jun 2020","2020","","","1","8","Driven by the vision of the Internet of Things (IoT) under the fifth-generation (5G) wireless network, computing resource trading attracts numerous attention from both academia and industry. Prior works mainly focus on the design of auction mechanisms to implement pricing and resource allocation. However, it is still a challenging problem because of the following three aspects: 1) How to ensure that the auction mechanism runs fairly? An auction mechanism is vulnerable and questionable since the auctioneer may fail the orders matching operation or collude with a few peers. 2) It’s hard to assign the computing resources of providers to customers and guarantee reasonable rewards for each participator. 3) How to make bidding strategies for each participator? Each participator has its willingness to selWuy, which are time-variant and private. To address the above issues, we build a blockchain-enabled computing resource trading system that takes both pricing and bidding strategies into consideration, on which providers and customers can trade computing resources securely, safely and willingly. Next, we formulate a decision-making problem in the continuous double auction (CDA) to maximize their payoffs. Then, we propose a universal model-free Deep Reinforcement Learning (DRL) framework for both computing resource providers and customers. We conduct extensive experiments to evaluate the performance of our DRL framework. Simulation results show that our solution outperforms others in both static and dynamic scenarios. Our DRL framework can achieve higher rewards than others by at least 35%. Furthermore, the average trading price from our DRL framework is less volatile than that from the compared methods. The DRL framework promotes trading and brings larger trading quantities, thus resulting in higher social welfare by at least 25% than the compared schemes.","1558-2612","978-1-7281-3106-1","10.1109/WCNC45663.2020.9120521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9120521","Computing resource trading;continuous double auction;deep reinforcement learning.","Learning systems;Industries;Simulation;Wireless networks;Conferences;Decision making;Reinforcement learning","computer networks;cryptocurrencies;decision making;game theory;learning (artificial intelligence);pricing;resource allocation","bidding strategies;blockchain-enabled computing resource trading;continuous double auction mechanism;computing resource providers;average trading price;DRL framework;larger trading quantities;fifth-generation wireless network;resource allocation;decision-making problem;universal model-free deep reinforcement learning","","5","","22","IEEE","19 Jun 2020","","","IEEE","IEEE Conferences"
"Exploring market power using deep reinforcement learning for intelligent bidding strategies","A. J. M. Kell; M. Forshaw; A. Stephen McGough","Department of Chemical Engineering, Imperial College London, London, U.K; School of Computing, Newcastle University, Newcastle upon Tyne, UK; School of Computing, Newcastle University, Newcastle upon Tyne, UK","2020 IEEE International Conference on Big Data (Big Data)","19 Mar 2021","2020","","","4402","4411","Decentralized electricity markets are often dominated by a small set of generator companies who control the majority of the capacity. In this paper, we explore the effect of the total controlled electricity capacity by a single, or group, of generator companies can have on the average electricity price. We demonstrate this through the use of ElecSim, a simulation of a country-wide energy market. We develop a strategic agent, representing a generation company, which uses a deep deterministic policy gradient reinforcement learning algorithm to bid in a uniform pricing electricity market. A uniform pricing market is one where all players are paid the highest accepted price. ElecSim is parameterized to the United Kingdom for the year 2018. This work can help inform policy on how to best regulate a market to ensure that the price of electricity remains competitive.We find that capacity has an impact on the average electricity price in a single year. If any single generator company, or a collaborating group of generator companies, control more than ~11% of generation capacity and bid strategically, prices begin to increase by ~25%. The value of ~25% and may vary between market structures and ~11% countries. For instance, different load profiles may favour a particular type of generator or a different distribution of generation capacity. Once the capacity controlled by a generator company, which bids strategically, is higher than ~35%, prices increase exponentially. We observe that the use of a market cap of approximately double the average market price has the effect of significantly decreasing this effect and maintaining a competitive market. A fair and competitive electricity market provides value to consumers and enables a more competitive economy through the utilisation of electricity by both industry and consumers.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9378137","Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9378137","deep reinforcement learning;bidding strategy;multi-agent system;electricity markets","Government;Companies;Reinforcement learning;Pricing;Big Data;Electricity supply industry;Generators","deep learning (artificial intelligence);gradient methods;learning (artificial intelligence);power engineering computing;power markets;pricing","intelligent bidding strategies;decentralized electricity markets;total controlled electricity capacity;average electricity price;ElecSim;country-wide energy market;strategic agent;deep deterministic policy gradient reinforcement learning algorithm;uniform pricing electricity market;single generator company;generation capacity;market structures;market cap;competitive electricity market;United Kingdom","","4","","21","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"Evaluating the Performance of Various Deep Reinforcement Learning Algorithms for a Conversational Chatbot","R. Rajamalli Keerthana; G. Fathima; L. Florence","Department of Computer Science and Engineering, Adhiyamaan College of Engineering, Hosur, India.; Department of Computer Science and Engineering, Adhiyamaan College of Engineering, Hosur, India.; Department of Computer Science and Engineering, Adhiyamaan College of Engineering, Hosur, India.","2021 2nd International Conference for Emerging Technology (INCET)","22 Jun 2021","2021","","","1","8","Conversational agents are the most popular AI technology in IT trends. Domain specific chatbots are now used by almost every industry in order to upgrade their customer service. The Proposed paper shows the modelling and performance of one such conversational agent created using deep learning. The proposed model utilizes NMT (Neural Machine Translation) from the TensorFlow software libraries. A BiRNN (Bidirectional Recurrent Neural Network) is used in order to process input sentences that contain large number of tokens (20-40 words). In order to understand the context of the input sentence attention model is used along with BiRNN. The conversational models usually have one drawback, that is, they sometimes provide irrelevant answer to the input. This happens quite often in conversational chatbots as the chatbot doesn't realize that it is answering without context. This drawback is solved in the proposed system using Deep Reinforcement Learning technique. Deep reinforcement Learning follows a reward system that enables the bot to differentiate between right and wrong answers. Deep Reinforcement Learning techniques allows the chatbot to understand the sentiment of the query and reply accordingly. The Deep Reinforcement Learning algorithms used in the proposed system is Q-Learning, Deep Q Neural Network (DQN) and Distributional Reinforcement Learning with Quantile Regression (QR-DQN). The performance of each algorithm is evaluated and compared in this paper in order to find the best DRL algorithm. The dataset used in the proposed system is Cornell Movie-dialogs corpus and CoQA (A Conversational Question Answering Challenge). CoQA is a large dataset that contains data collected from 8000+ conversations in the form of questions and answers. The main goal of the proposed work is to increase the relevancy of the chatbot responses and to increase the perplexity of the conversational chatbot.","","978-1-7281-7029-9","10.1109/INCET51464.2021.9456321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9456321","Deep Reinforcement Learning;Chatbot;BiRNN;Attention model;NMT;Q-Learning;DQN;QR-DQN","Measurement;Industries;Deep learning;Software libraries;Recurrent neural networks;Reinforcement learning;Chatbot","learning (artificial intelligence);natural language processing;recurrent neural nets;software libraries","various Deep Reinforcement Learning algorithms;conversational chatbot;Conversational agents;popular AI technology;domain specific chatbots;modelling performance;conversational agent;deep learning;Neural Machine Translation;TensorFlow software libraries;BiRNN;Bidirectional Recurrent Neural Network;input sentences;input sentence attention model;conversational models;irrelevant answer;Deep Reinforcement Learning technique;Q-Learning;Deep Q Neural Network;Distributional Reinforcement Learning;Conversational Question Answering Challenge;chatbot responses","","3","","20","IEEE","22 Jun 2021","","","IEEE","IEEE Conferences"
"A Multi-Agent Reinforcement Learning Architecture for Network Slicing Orchestration","F. Mason; G. Nencioni; A. Zanella","Dep. of Information Engineering, University of Padova, Padova, Italy; Dep. of Electrical Engineering and Computer Science, University of Stavanger, Stavanger, Norway; Dep. of Information Engineering, University of Padova, Padova, Italy","2021 19th Mediterranean Communication and Computer Networking Conference (MedComNet)","9 Aug 2021","2021","","","1","8","The Network Slicing (NS) paradigm is one of the pillars of the future 5G networks and is gathering great attention from both industry and scientific communities. In a NS scenario, physical and virtual resources are partitioned among multiple logical networks, named slices, with specific characteristics. The challenge consists in finding efficient strategies to dynamically allocate the network resources among the different slices according to the user requirements. In this paper, we tackle the target problem by exploiting a Deep Reinforcement Learning approach. Our framework is based on a distributed architecture, where multiple agents cooperate towards a common goal. The agent training is carried out following the Advantage Actor Critic algorithm, which makes it possible to handle continuous action spaces. By means of extensive simulations, we show that our strategy yields better performance than an efficient empirical algorithm, while ensuring high adaptability to different scenarios without the need for additional training.","","978-1-6654-3590-1","10.1109/MedComNet52149.2021.9501279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9501279","Network slicing;resource allocation;distributed machine learning;deep reinforcement learning","Training;Industries;Adaptation models;Network topology;5G mobile communication;Network slicing;Reinforcement learning","5G mobile communication;deep learning (artificial intelligence);multi-agent systems;resource allocation;telecommunication computing","physical resources;virtual resources;multiple logical networks;network resource allocation;deep reinforcement learning approach;distributed architecture;multiple agents;agent training;advantage actor critic algorithm;empirical algorithm;network slicing paradigm;future 5G networks;multiagent reinforcement learning architecture","","3","","26","IEEE","9 Aug 2021","","","IEEE","IEEE Conferences"
"SAC: A Novel Multi-hop Routing Policy in Hybrid Distributed IoT System based on Multi-agent Reinforcement Learning","W. Zhang; T. Liu; M. Xie; J. Zhang; C. Pan","Department of Computing Sciences, Texas A&M University-Corpus Christi, Corpus Christi, TX; Department of Math and Computer Science, Lawrence Technological University, Southfield, MI; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX; Department of Computer Science, Harvard University, Cambridge, MA; Department of Computing Sciences, Texas A&M University-Corpus Christi, Corpus Christi, TX","2021 22nd International Symposium on Quality Electronic Design (ISQED)","10 May 2021","2021","","","129","134","Energy harvesting (EH) IoT devices have attracted vast attention in both academia and industry as they can work sustainably by harvesting energy from the ambient environment. However, due to the weak and transient nature of harvesting power, EH technology is unable to support power-intensive IoT devices such as IoT edge servers. Therefore, the hybrid IoT system where the EH IoT devices and non-EH IoT devices co-exist is forthcoming. This paper explored the routing problem in such a hybrid distributed IoT system. We first proposed a comprehensive multi-hop routing mechanism of this hybrid system. After that, we proposed a distributed multi-agent deep reinforcement learning algorithm, known as spatial asynchronous advantage actor-critic (SAC), to optimize the system routing policy and energy allocation while maximizing the total amount of transmitted data and the overall data delivery to the sink node. The experiments indicate that SAC can averagely complete at least $\sim 1.5 \times$ transmission rate and $\sim 12.9\times$ Sink packet delivery rate compared with the baselines.","1948-3287","978-1-7281-7641-3","10.1109/ISQED51717.2021.9424255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9424255","","Industries;Distributed databases;Reinforcement learning;Routing;Systems simulation;Servers;Resource management","energy harvesting;Internet of Things;learning (artificial intelligence);multi-agent systems;optimisation;routing protocols;telecommunication network routing;wireless sensor networks","novel multihop routing policy;hybrid distributed IoT system;multiagent reinforcement learning;energy harvesting IoT devices;vast attention;weak nature;transient nature;harvesting power;EH technology;power-intensive IoT devices;IoT edge servers;hybrid IoT system;EH IoT devices;nonEH IoT devices;routing problem;comprehensive multihop routing mechanism;hybrid system;multiagent deep reinforcement learning algorithm;spatial asynchronous advantage actor-critic;system routing policy","","3","","14","IEEE","10 May 2021","","","IEEE","IEEE Conferences"
"RiverFuzzRL - an open-source tool to experiment with reinforcement learning for fuzzing","C. Paduraru; M. Paduraru; A. Stefanescu","Department of Computer Science, University of Bucharest, Romania; Department of Computer Science, University of Bucharest, Romania; Department of Computer Science, University of Bucharest, Romania","2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)","24 May 2021","2021","","","430","435","Combining fuzzing techniques and reinforcement learning could be an important direction in software testing. However, there is a gap in support for experimentation in this field, as there are no open-source tools to let academia and industry to perform experiments easily. The purpose of this paper is to fill this gap by introducing a new framework, named RiverFuzzRL, on top of our already mature frame-work for AI-guided fuzzing, River. We provide out-of-the-box implementations for users to choose from or customize for their test target. The work presented here is performed on testing binaries and does not require access to the source code, but it can be easily adapted to other types of software testing as well. We also discuss the challenges faced, opportunities, and factors that are important for performance, as seen in the evaluation.","2159-4848","978-1-7281-6836-4","10.1109/ICST49551.2021.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438567","fuzzing;reinforcement learning;binary analysis;symbolic execution;open-source tool","Industries;Protocols;Conferences;Reinforcement learning;Tools;Fuzzing;Search problems","fuzzy set theory;learning (artificial intelligence);program testing;public domain software","source code;software testing;open-source tool;reinforcement learning;important direction;frame-work;test target;testing binaries;RiverFuzzRL","","2","","25","IEEE","24 May 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning with Adjustments","H. Khorasgani; H. Wang; C. Gupta; S. Serita","Hitachi Industrial AI Lab, Santa Clara, USA; Hitachi Industrial AI Lab, Santa Clara, USA; Hitachi Industrial AI Lab, Santa Clara, USA; Hitachi Industrial AI Lab, Santa Clara, USA","2021 IEEE 19th International Conference on Industrial Informatics (INDIN)","11 Oct 2021","2021","","","1","8","Deep reinforcement learning (RL) algorithms can learn complex policies to optimize agent operation over time. RL algorithms have shown promising results in solving complicated problems in recent years. However, their application on real-world physical systems remains limited. Despite the advancements in RL algorithms, the industries often prefer traditional control strategies. Traditional methods are simple, computationally efficient and easy to adjust. In this paper, we first propose a new Q-learning algorithm for continuous action space, which can bridge the control and RL algorithms and bring us the best of both worlds. Our method can learn complex policies to achieve long-term goals and at the same time it can be easily adjusted to address short-term requirements without retraining. Next, we present an approximation of our algorithm which can be applied to address short-term requirements of any pre-trained RL algorithm. The case studies demonstrate that both our proposed method as well as its practical approximation can achieve short-term and long-term goals without complex reward functions.","","978-1-7281-4395-8","10.1109/INDIN45523.2021.9557543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9557543","","Robust control;Industries;Bridges;Uncertainty;Conferences;Reinforcement learning;Predictive models","approximation theory;learning (artificial intelligence);multi-agent systems","deep reinforcement learning algorithms;complex policies;RL algorithms;real-world physical systems;traditional control strategies;Q-learning algorithm;long term goals;pretrained RL algorithm;short term requirements","","1","","19","IEEE","11 Oct 2021","","","IEEE","IEEE Conferences"
"Security and 5G: Attack mitigation using Reinforcement Learning in SDN networks","J. Á. Fernández-Carrasco; L. Segurola-Gil; F. Zola; R. Orduna-Urrutia","Fundación Vicomtech, Basque Research and Technology Alliance (BRTA), Donostia-San Sebastian, Kingdom of Spain; Fundación Vicomtech, Basque Research and Technology Alliance (BRTA), Donostia-San Sebastian, Kingdom of Spain; Fundación Vicomtech, Basque Research and Technology Alliance (BRTA), Donostia-San Sebastian, Kingdom of Spain; Fundación Vicomtech, Basque Research and Technology Alliance (BRTA), Donostia-San Sebastian, Kingdom of Spain","2022 IEEE Future Networks World Forum (FNWF)","8 Mar 2023","2022","","","622","627","5G ecosystem is shaping the future of communication networks enabling innovation and digital transformation not only for individual users but also for companies, industries, and communities. In this scenario, technologies such as Software Defined Networking (SDN) represent a solution for telecommunications providers to create agile, scalable, efficient platforms capable of meeting the requirements in the 5G ecosystem. However, as network environments and systems become increasingly complex, both in terms of size and dynamic behavior, the number of vulnerabilities in them can be very high. In addition, hackers are continuously improving intrusion methods, which are becoming more difficult to detect. For this reason, in this study, we deploy a system based on a Reinforcement Learning (RL) agent capable of applying different countermeasures to defend a network against intrusion and DDoS attacks using SDN. The approach is drawn like a serious game in which a defender and an attacker carry out actions based on the observations they get from the environment, i.e., network current status. In this study, defenders and attackers are trained using the Deep Q-Learning (DQN) algorithm with some variations, like Prioritized Replay, Dueling, and Double DQN, comparing their results in order to get the best strategy for attack mitigation. The results of this paper show that RL algorithms can be successfully used to create more versatile agents able of interpreting and adapting themselves to different situations and so run the best countermeasure to protect the network. According to the results, it is also shown that the Complete strategy, which includes the three DQN variations analyzed, is the one that allows obtaining agents with the best decision making to respond to attacks.","2770-7679","978-1-6654-6250-1","10.1109/FNWF55208.2022.00114","Spanish Centre for the Development of Industrial Technology (CDTI)(grant numbers:MIG-20211019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10056620","5G;SDN;Reinforcement Learning;DQN Algorithm;Cybersecurity","Industries;Technological innovation;Q-learning;5G mobile communication;Digital transformation;Ecosystems;Games","5G mobile communication;computer network security;decision making;deep learning (artificial intelligence);learning (artificial intelligence);multi-agent systems;reinforcement learning;security of data;software defined networking;telecommunication computing","agile platforms;attack mitigation;communication networks;DDoS attacks;Deep Q-Learning algorithm;different countermeasures;digital transformation;Double DQN;DQN variations;dynamic behavior;efficient platforms;innovation;intrusion methods;network current status;network environments;Reinforcement Learning;RL algorithms;scalable platforms;SDN networks;serious game;Software Defined Networking;telecommunications providers;versatile agents","","1","","28","IEEE","8 Mar 2023","","","IEEE","IEEE Conferences"
"Online 3D Bin Packing Reinforcement Learning Solution with Buffer","A. V. Puche; S. Lee","Artificial Intelligence School, Sungkyunkwan University, Suwon, South Korea; Artificial Intelligence School, Sungkyunkwan University, Suwon, South Korea","2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","26 Dec 2022","2022","","","8902","8909","The 3D Bin Packing Problem (3D-BPP) is one of the most demanded yet challenging problems in industry, where an agent must pack variable size items delivered in sequence into a finite bin with the aim to maximize the space utilization. It represents a strongly NP-Hard optimization problem such that no solution has been offered to date with high performance in space utilization. In this paper, we present a new reinforcement learning (RL) framework for a 3D-BPP solution for improving performance. First, a buffer is introduced to allow multi-item action selection. By increasing the degree of freedom in action selection, a more complex policy that results in better packing performance can be derived. Second, we propose an agnostic data augmentation strategy that exploits both bin item symmetries for improving sample efficiency. Third, we implement a model-based RL method adapted from the popular algorithm AlphaGo, which has shown superhuman performance in zero-sum games. Our adaptation is capable of working in single-player and score based environments. In spite of the fact that AlphaGo versions are known to be computationally heavy, we manage to train the proposed framework with a single thread and GPU, while obtaining a solution that outperforms the state-of-the-art results in space utilization.","2153-0866","978-1-6654-7927-1","10.1109/IROS47612.2022.9982095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982095","","Industries;Three-dimensional displays;Instruction sets;Computational modeling;Graphics processing units;Reinforcement learning;Games","bin packing;computational complexity;game theory;learning (artificial intelligence);optimisation","3D Bin Packing Problem;3D-BPP solution;agnostic data augmentation strategy;bin item symmetries;finite bin;model-based RL method;multiitem action selection;NP-Hard optimization problem;online 3D Bin Packing reinforcement learning solution;packing performance;reinforcement learning framework;space utilization;superhuman performance;variable size items","","1","","24","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Domain Adaptation of Reinforcement Learning Agents based on Network Service Proximity","K. Dey; S. K. Perepu; P. Dasgupta; A. Das","Ericsson Research (Artificial Intelligence), India; Ericsson Research (Artificial Intelligence), India; Indian Institute of Technology, Kharagpur, India; Indian Institute of Technology, Kharagpur, India","2023 IEEE 9th International Conference on Network Softwarization (NetSoft)","13 Jul 2023","2023","","","152","160","The dynamic and evolutionary nature of service requirements in wireless networks has motivated the telecom industry to consider intelligent self-adapting Reinforcement Learning (RL) agents for controlling the growing portfolio of network services. Infusion of many new types of services is anticipated with future adoption of 6G networks, and sometimes these services will be defined by applications that are external to the network. An RL agent trained for managing the needs of a specific service type may not be ideal for managing a different service type without domain adaptation. We provide a simple heuristic for evaluating a measure of proximity between a new service and existing services, and show that the RL agent of the most proximal service rapidly adapts to the new service type through a well defined process of domain adaptation. Our approach enables a trained source policy to adapt to new situations with changed dynamics without retraining a new policy, thereby achieving significant computing and cost-effectiveness. Such domain adaptation techniques may soon provide a foundation for more generalized RL-based service management under the face of rapidly evolving service types.","2693-9789","979-8-3503-9980-6","10.1109/NetSoft57336.2023.10175507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10175507","wireless networks;service management;reinforcement learning","6G mobile communication;Training;Industries;Wireless networks;Reinforcement learning;Telecommunication services;Portfolios","6G mobile communication;multi-agent systems;reinforcement learning;telecommunication computing;wireless channels","domain adaptation techniques;dynamic nature;evolutionary nature;generalized RL-based service management;intelligent self-adapting Reinforcement learning agents;network service proximity;network services;proximal service;RL agent;service requirements;specific service type;wireless networks","","1","","24","IEEE","13 Jul 2023","","","IEEE","IEEE Conferences"
"Prehensile Robotic pick-and-place in clutter with Deep Reinforcement Learning","M. B. Imtiaz; Y. Qiao; B. Lee","Software Research Institute, Technological University of Shannon: Midlands Midwest, Athlone, Ireland; Software Research Institute, Technological University of Shannon: Midlands Midwest, Athlone, Ireland; Software Research Institute, Technological University of Shannon: Midlands Midwest, Athlone, Ireland","2022 International Conference on Electrical, Computer and Energy Technologies (ICECET)","9 Sep 2022","2022","","","1","6","In this paper, we present a self-learning deep reinforcement learning-based framework for industrial pick-and-place tasks in a cluttered environment through intelligent prehensile robotic grasping. This approach aims to enable agents learn and perform pick and place regular and irregular objects in clutter through robotic grasping in order to enhance both quantity and quality in various industries. In order to do so, we design a Markov decision process (MDP) and deploy a model-free off-policy temporal difference algorithm Q-learning. We utilize end-to-end DenseNet-121 architecture fully convolutional network (FCN) in extended format for Q-function approximation. A pixelwise parameterization scheme is designed to calculate the pixelwise maps of action values. Rewards are allocated according to the success of the action performed. The proposed approach doesn’t require any domain specifications, geometrical knowledge of objects or any extraordinary resources such as huge datasets or memory requirements. We have presented the training and testing results of our approach compared to its different variants and random density clutter sizes.","","978-1-6654-7087-2","10.1109/ICECET55527.2022.9873426","Science Foundation Ireland; European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9873426","deep reinforcement learning;prehensile robotic grasping;Markov decision process;Q-learning;fully convolutional network;Q-function;DenseNet-121","Training;Industries;Q-learning;Service robots;Memory management;Grasping;Markov processes","function approximation;learning (artificial intelligence);Markov processes","deep reinforcement learning-based framework;-place tasks;cluttered environment;intelligent prehensile robotic grasping;irregular objects;Markov decision process;model-free off-policy temporal difference algorithm Q-learning;DenseNet-121 architecture fully convolutional network;pixelwise parameterization scheme;pixelwise maps;random density clutter","","1","","56","IEEE","9 Sep 2022","","","IEEE","IEEE Conferences"
"DeepDLP: Deep Reinforcement Learning based Framework for Dynamic Liner Trade Pricing","X. Li; Y. Hu; Y. Bai; X. Gao; G. Chen","Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China","2023 17th International Conference on Ubiquitous Information Management and Communication (IMCOM)","8 Feb 2023","2023","","","1","8","Liner trade differs from other traffic flow in that it is characterized by substantially higher traffic volumes due to its low cost and high capacity. In the liner trade, pricing models should be distinguished by their ability to adapt to changing market conditions and provide a realistic picture of market trends. However, the prevalent manual pricing model used today in the industry can hardly adapt to the changing environment. We design a Deep Reinforcement Learning (RL) Framework for Liner Trade Problem (DeepDLP) to identify price strategy based on the characteristics of liner trade environment. We address four key problems with DeepDLP that significantly affect pricing. First, to address the issue of RL's challenging convergence due to current market variances, we use the Long-Short Time Memory (LSTM) to recognize environmental changes. Secondly, to avoid accumulated bias over time in LSTM, DeepDLP uses the price adjustment result of RL as the input of the Dynamic Price Prediction Module. Thirdly, in order to reduce noise and take past external information into account, we employ a historical information sensitive network in the Dynamic Price Prediction Module. Last but not least, we designed a balanced reward that allows the model to take into account both revenue maximization per unit of time and the sale of extra capacity. We first perform data wrangling and analysis on the actual data of a top liner trade company to identify the major routes and affecting elements in order to demonstrate the effectiveness and efficiency of DeepDLP. Next, we simulate real liner trade scenarios and conduct several experiments. The outcome shows that DeepDLP outperforms baselines and validates the significance of the designs.","","978-1-6654-5348-6","10.1109/IMCOM56909.2023.10035599","National Natural Science Foundation of China(grant numbers:62272302,62172276); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035599","Liner Shipping;Reinforcement Learning;Time-series Model","Deep learning;Industries;Adaptation models;Costs;Pricing;Reinforcement learning;Manuals","deep learning (artificial intelligence);pricing;recurrent neural nets;reinforcement learning","changing environment;current market variances;Deep Reinforcement;DeepDLP;Dynamic Liner Trade pricing;Dynamic Price Prediction Module;environmental changes;historical information sensitive network;liner trade company;Liner trade differs;liner trade environment;Liner Trade Problem;liner trade scenarios;Long-Short Time Memory;LSTM;market conditions;market trends;prevalent manual pricing model;price strategy;pricing models;RL's challenging convergence;substantially higher traffic volumes;traffic flow","","","","22","IEEE","8 Feb 2023","","","IEEE","IEEE Conferences"
"RL-KDA: A K-degree Anonymity Algorithm Based on Reinforcement Learning","X. Ma; N. Xiang; Y. Gao","School of Computer Science, Inner Mongolia University, Hohhot, China; School of Computer Science, Inner Mongolia University, Hohhot, China; School of Computer Science, Inner Mongolia University, Hohhot, China","2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC)","2 Aug 2023","2023","","","729","734","K-degree anonymity is one of the main techniques for data privacy and has gained attention in academia, industry, and government. Many social network data publishing algorithms based on K-anonymity techniques have been proposed, but most studies focus on static social networks. Compared to static social networks, dynamic social networks suffer from problems such as higher information loss and lower data utility. To address the existing problem of dynamic social networks, we propose a K-degree anonymity dynamic data publishing algorithm based on reinforcement learning. The algorithm ends with two phases: anonymization sequence and graph modification. In the anonymous sequence phase, this paper combines the idea of reinforcement learning and the characteristics of dynamic data change to build a reinforcement learning model for anonymous sequences. In this way, an ideal anonymous sequence can be created. We also propose a new strategy for graph modification, which selects edges according to degree centrality to generate anonymous graphs. Finally, experiments on real datasets show the effectiveness of our algorithm.","0730-3157","979-8-3503-2697-0","10.1109/COMPSAC57700.2023.00100","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10196930","Privacy Protection;K-Degree Anonymity;Reinforcement Learning;Social Graphs","Industries;Data privacy;Privacy;Social networking (online);Publishing;Heuristic algorithms;Reinforcement learning","data privacy;graph theory;reinforcement learning;social networking (online)","anonymization sequence;anonymous graphs;anonymous sequence phase;anonymous sequences;data privacy;degree centrality;dynamic data change;dynamic social networks;graph modification;higher information loss;ideal anonymous sequence;K-anonymity techniques;K-degree anonymity algorithm;K-degree anonymity dynamic data publishing algorithm;lower data utility;reinforcement learning model;social network data publishing algorithms;static social networks","","","","18","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"PA-FEAT: Fast Feature Selection for Structured Data via Progress-Aware Multi-Task Deep Reinforcement Learning","J. Zhang; Z. Luo; Q. Xu; M. Zhang","Tangshan Research Institute, BIT; National University of Singapore; OceanBase; Tangshan Research Institute, BIT","2023 IEEE 39th International Conference on Data Engineering (ICDE)","26 Jul 2023","2023","","","394","407","Feature selection is an effective technique for structured data analytics, aiming to eliminate redundant features and irrelevant features for downstream tasks (e.g., classification). With the deepening of data-driven decision-making applications in various industries, the demand for real-time structured data analysis is constantly increasing. At this time, high requirements are placed on the time cost of feature selection. However, existing feature selection methods may easily fall into the dilemma of efficiency and effectiveness when faced with this situation due to the huge feature space. In this paper, we study a novel fast feature selection scenario, which is to generalize the knowledge of feature selection from historical structured data analytics tasks (seen tasks) and then quickly apply it to the process of feature selection for future structured data analytics tasks (unseen tasks). We propose a novel Progress-Aware multi-task deep reinforcement learning method for Fast fEAture selecTion (PA-FEAT), which makes full use of various progress-related information generated during the knowledge generalization process to achieve efficiency and effectiveness simultaneously. Extensive results on eight real-world datasets show that PA-FEAT consistently outperforms eight baselines in terms of efficiency and effectiveness.","2375-026X","979-8-3503-2227-9","10.1109/ICDE55515.2023.00037","National Key Research and Development Program of China; National Natural Science Foundation of China; Ant Group; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10184534","Data Analytics;Feature Selection;Multi-Task Learning;Deep Reinforcement Learning","Deep learning;Industries;Data analysis;Decision making;Reinforcement learning;Feature extraction;Multitasking","data analysis;decision making;deep learning (artificial intelligence);feature extraction;feature selection;learning (artificial intelligence);reinforcement learning","data-driven decision-making applications;existing feature selection methods;future structured data analytics tasks;huge feature space;irrelevant features;novel fast feature selection scenario;novel Progress-Aware multitask deep reinforcement learning method;PA-FEAT;real-time structured data analysis;redundant features","","","","47","IEEE","26 Jul 2023","","","IEEE","IEEE Conferences"
"Process Proportional-Integral PI Control with Deep Reinforcement Learning","T. Tiong; I. Saad; K. T. K. Teo; H. Bin Lago","Electrical and Computer Engineering, Curtin University Malaysia, Miri Sarawak, Malaysia; Electrical and Electronics Engineering, University Malaysia Sabah, Kota Kinabalu, Sabah, Malaysia; Electrical and Electronics Engineering, University Malaysia Sabah, Kota Kinabalu, Sabah, Malaysia; Electrical and Electronics Engineering, University Malaysia Sabah, Kota Kinabalu, Sabah, Malaysia","2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC)","18 Apr 2023","2023","","","0093","0101","Advanced model-based controllers in process industries require regular maintenance to maintain acceptable performance. Controller performance is continuously monitored and remedial model reidentification procedure initiated when performance degraded. These procedures are typically complicated and often cause expensive interruptions to normal operations. In this paper, deep reinforcement learning (DRL) is used to develop an adaptive, model-free controller for general discrete-time processes. The proposed DRL controller is a data-based controller that learns the control policy in real time by merely interacting with the process. The effectiveness of the DRL Proportional-Integral-Derivative (PID) controller is demonstrated through many simulations.","","979-8-3503-3286-5","10.1109/CCWC57344.2023.10099286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10099286","deep reinforcement learning;actor-critic;twin-delayed DDPG;process control;proportional-integral control","Industries;Deep learning;Training;Adaptation models;Computational modeling;Process control;Reinforcement learning","control system synthesis;deep learning (artificial intelligence);learning (artificial intelligence);maintenance engineering;PI control;reinforcement learning;three-term control","adaptive model-free controller;advanced model-based controllers;control policy;controller performance;data-based controller;deep reinforcement learning;discrete-time processes;DRL controller;DRL Proportional-Integral-Derivative controller;expensive interruptions;normal operations;process Proportional-Integral PI control;regular maintenance;remedial model reidentification procedure","","","","48","IEEE","18 Apr 2023","","","IEEE","IEEE Conferences"
"Multi-agent Federated Reinforcement Learning for Resource Allocation in UAV-enabled Internet of Medical Things Networks","A. M. Seid; A. Erbad; H. N. Abishu; A. Albaseer; M. Abdallah; M. Guizani","College of Science and Engineering, Division of Information and Computing Technology, Hamad Bin Khalifa University, Doha, Qatar; College of Science and Engineering, Division of Information and Computing Technology, Hamad Bin Khalifa University, Doha, Qatar; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, CHINA; College of Science and Engineering, Division of Information and Computing Technology, Hamad Bin Khalifa University, Doha, Qatar; College of Science and Engineering, Division of Information and Computing Technology, Hamad Bin Khalifa University, Doha, Qatar; Machine Learning Department, Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE","IEEE Internet of Things Journal","","2023","PP","99","1","1","In the 5G/B5G network paradigms, intelligent medical devices known as the Internet of Medical Things (IoMT) have been used in the healthcare industry to monitor remote users’ health status, such as elderly monitoring, injuries, stress, and patients with chronic diseases. Since IoMT devices have limited resources, mobile edge computing (MEC) has been deployed in 5G networks to enable them to offload their tasks to the nearest computational servers for processing. However, when IoMTs are far from network coverage or the computational servers at the terrestrial MEC are overloaded/emergencies occur, these devices cannot access computing services, potentially risking the lives of patients. In this context, unmanned aerial vehicles (UAVs) are considered a prominent aerial connectivity solution for healthcare systems. In this paper, we propose a multi-agent federated reinforcement learning (MAFRL)-based resource allocation framework for a multi-UAV-enabled healthcare system. We formulate the computation offloading and resource allocation problems as a Markov decision process game in federated learning with multiple participants. Then, we propose a MAFRL algorithm to solve the formulated problem, minimize latency and energy consumption, and ensure the quality of service. Finally, extensive simulation results on a real-world heartbeat dataset prove that the proposed MAFRL algorithm significantly minimizes the cost, preserves privacy, and improves accuracy compared to the baseline learning algorithms.","2327-4662","","10.1109/JIOT.2023.3283353","Qatar National Research Fund(grant numbers:NPRP13S-0205-200265); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10144626","Federated learning;MARL;Internet of Medical Things;UAV;Emergency;Healthcare","Medical services;Resource management;Internet of Things;Monitoring;Computational modeling;Industries;Privacy","","","","","","","IEEE","6 Jun 2023","","","IEEE","IEEE Early Access Articles"
"A Reinforcement Learning Approach for Network Slicing in 5G Networks","I. Amonarriz-Pagola; J. A. Fernandez-Carrasco","Vicomtech, Basque Research and Technology Alliance (BRTA), Donostia/San Sebastian, Spain; Vicomtech, Basque Research and Technology Alliance (BRTA), Donostia/San Sebastian, Spain","2023 JNIC Cybersecurity Conference (JNIC)","7 Aug 2023","2023","","","1","7","The emergence of the 5G ecosystem has revolutionized the landscape of communication networks, acting as a catalyst for digital transformation for individuals, companies, and industries. Efficient resource and slice management are vital in 5G networks to ensure the quality of service. To achieve this, a Reinforcement Learning (RL) approach is presented, which trains an intelligent agent to allocate slices in a 5G environment. Different RL algorithms such as Soft Actor Critic (SAC) and Deep Q-Networks (DQN) with some variants such as Double DQN, Dueling DQN, and Prioritized Experience Replay are used to optimize the allocation of slices based on the network state. The performance of the agent is compared with random allocation and heuristic-based methods. The objective is for the results to show that the proposed RL approach outperforms these methods, demonstrating the effectiveness of using RL for network slicing in 5G networks.","","978-84-8158-971-9","10.23919/JNIC58574.2023.10205800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205800","Reinforcement Learning;Network Slicing;5G;DQN","Industries;5G mobile communication;Network slicing;Digital transformation;Ecosystems;Reinforcement learning;Quality of service","","","","","","28","","7 Aug 2023","","","IEEE","IEEE Conferences"
"Analysis of Resource Management Methods Based on Reinforcement Learning","M. Xing; Z. Wang; Z. Xiao","Department of Computer Science, Peking University, Beijing, China; Faculty of Science, National University of Singapore, Singapore, Singapore; Department of Computer Science, Peking University, Beijing, China","2021 International Conference on High Performance Big Data and Intelligent Systems (HPBD&IS)","3 Jan 2022","2021","","","27","31","As the scale of service-based applications rapidly grows over recent years, tremendous amount of user data is being generated on a daily basis, which needs to be processed by computing jobs. Distributed computing frameworks are extensively applied to efficiently process large-scale data using finite resources, which has consequently placed resource management at the center of attention for many researchers. Traditional heuristic-based resource management algorithms are widely used in the industry, while often require experts with rich experience to design and tune rules, which is usually a time- consuming process and difficult to be generalized to computing jobs with distinct natures and scales. With the immense successes of reinforcement learning (RL) in the fields of games, auto- driving, and robotics, researchers begin to model and learn the task of resource management through the perspectives of RL, which has been proven to outperform conventional methods by experimental results. In this paper, we aim to summarize the relevant background, introduce both the heuristic-based and RL- based algorithms and propose a few areas of improvement for future work to come.","","978-1-6654-1327-5","10.1109/HPBDIS53214.2021.9658350","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9658350","Deep Learning;Reinforcement Learning;Re- source Management;Graph Neural Networks","Industries;Service robots;Heuristic algorithms;Computational modeling;Neural networks;Distributed databases;Reinforcement learning","heuristic programming;reinforcement learning;resource allocation","resource management methods;reinforcement learning;service-based applications;user data;computing jobs;distributed computing frameworks;large-scale data;finite resources;heuristic-based resource management algorithms;large-scale data processing;heuristic-based algorithm;RL-based algorithm","","","","40","IEEE","3 Jan 2022","","","IEEE","IEEE Conferences"
"A Novel Entropy-Maximizing TD3-based Reinforcement Learning for Automatic PID Tuning","M. A. Chowdhury; Q. Lu","Department of Chemical Engineering, Texas Tech University, Lubbock, TX, USA; Department of Chemical Engineering, Texas Tech University, Lubbock, TX, USA","2023 American Control Conference (ACC)","3 Jul 2023","2023","","","2763","2768","Proportional-integral-derivative (PID) controllers have been widely used in the process industry. However, the satisfactory control performance of a PID controller depends strongly on the tuning parameters. Conventional PID tuning methods require extensive knowledge of the system model, which is not always known especially in the case of complex dynamical systems. In contrast, reinforcement learning-based PID tuning has gained popularity since it can treat PID tuning as a black-box problem and deliver the optimal PID parameters without requiring explicit process models. In this paper, we present a novel entropy-maximizing twin-delayed deep deterministic policy gradient (EMTD3) method for automating the PID tuning. In the proposed method, an entropy-maximizing stochastic actor is employed at the beginning to encourage the exploration of the action space. Then a deterministic actor is deployed to focus on local exploitation and discover the optimal solution. The incorporation of the entropy-maximizing term can significantly improve the sample efficiency and assist in fast convergence to the global solution. Our proposed method is applied to the PID tuning of a second-order system to verify its effectiveness in improving the sample efficiency and discovering the optimal PID parameters compared to traditional TD3.","2378-5861","979-8-3503-2806-6","10.23919/ACC55779.2023.10156246","Texas Tech University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10156246","","Training;Industries;Uncertainty;Stochastic processes;Process control;Reinforcement learning;Robustness","deep learning (artificial intelligence);entropy;gradient methods;optimisation;reinforcement learning;three-term control","automatic PID;conventional PID tuning methods;entropy-maximizing stochastic actor;entropy-maximizing term;explicit process models;Novel Entropy-Maximizing TD3-based Reinforcement Learning;novel entropy-maximizing twin-delayed deep deterministic policy gradient method;optimal PID parameters;PID controller;proportional-integral-derivative controllers;reinforcement learning-based PID tuning;satisfactory control performance;tuning parameters","","","","20","","3 Jul 2023","","","IEEE","IEEE Conferences"
"Teach Reinforcement Learning in Robotics Class: two case studies","Y. Zhang; W. Qu; Y. Xiao; G. Zhong","Normal College, ShenZhen University, ShenZhen, China; Normal College, ShenZhen University, ShenZhen, China; Normal College, ShenZhen University, ShenZhen, China; Normal College, ShenZhen University, ShenZhen, China","2022 IEEE International Conference on Teaching, Assessment and Learning for Engineering (TALE)","14 Jun 2023","2022","","","218","222","Reinforcement learning (RL), as a promising approach to realize Artificial Intelligence (AI), has been widely applied in developing robots. However, the current K12 Robotics Course is still designed to focus on traditional robot technology without RL. In order to foster the transition between education and industries, a robot course integrating RL is designed in our research. We implemented the course and carried out case studies in a robot course in a school and an experience class for K12 students. The results show that most students can successfully complete the robot task based on RL under reasonable course difficulty, and most students agree the proposal of teaching RL in K12 Robotics Course. This suggests researchers and policy makers to work on updating K12 Robotics Course by adding RL section, thus to keep the AI curriculum system and content modern.","2470-6698","978-1-6654-9117-4","10.1109/TALE54877.2022.00043","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148363","K12 Robotics Course;Artificial Intelligence education;Reinforcement Learning;curriculum design","Industries;Service robots;Education;Reinforcement learning;Proposals;Artificial intelligence;Task analysis","computer aided instruction;computer science education;control engineering computing;control engineering education;educational courses;educational institutions;reinforcement learning;teaching","AI curriculum system;artificial intelligence curriculum system;experience class;K12 robotics course;reinforcement learning teaching;RL section;robot course;robotics class;school","","","","15","IEEE","14 Jun 2023","","","IEEE","IEEE Conferences"
"Optimal IPT Core Design for Wireless Electric Vehicles by Reinforcement Learning","M. S. Jeong; J. H. Jang; E. S. Lee","School of Electrical Engineering, Hanyang University ERICA Campus, Ansan, South Korea; School of Electrical Engineering, Hanyang University ERICA Campus, Ansan, South Korea; School of Electrical Engineering, Hanyang University ERICA Campus, Ansan, South Korea","IEEE Transactions on Power Electronics","22 Sep 2023","2023","38","11","13262","13272","In this article, optimal inductive power transfer (IPT) core structures for wireless electric vehicle (WEV), which can be derived by optimal reinforcement learning (RL) algorithms, are newly proposed. Because the IPT cannot be theoretically analyzed to find a maximum value of mutual inductance for the optimal core structure design, intuitive and iterative process based on finite element method analysis are usually implemented. This conventional method, however, is not preferred due to numerous possible combinations and computation times. For this reason, RL algorithms are designed to optimize nonlinear system design, enabling the WEV IPT to be efficiently designed with high mutual inductance, even in the presence of severe misalignment conditions. Contrary to the conventional RL algorithm for the IPT core design, the proposed RL algorithm can follow higher mutual inductance by shorter episodes; hence, 50% of computation time reduction and 2% of maximum mutual inductance were achieved. A prototype of WEV IPT system designed by the proposed RL algorithm was fabricated, satisfying the standard J2954 of the society of automotive engineers for WPT3/Z3 case. As a result, it is found that the proposed WEV IPT can be manufactured, considering the desired number of cores for reasonable cost and weight of the vehicle assembly.","1941-0107","","10.1109/TPEL.2023.3297740","Korea Institute of Energy Technology Evaluation and Planning(grant numbers:20224000000160); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190144","ϵ-greedy algorithms;artificial intelligence (AI);inductive power transfer (IPT);machine learning (ML);misalignment tolerance;neural network (NN);optimal core design;reinforcement learning (RL);wireless electric vehicle (WEV)","Inductance;Artificial neural networks;Finite element analysis;Magnetic cores;Prediction algorithms;Reinforcement learning;Wireless communication","automotive engineering;cores;electric vehicles;finite element analysis;inductance;inductive power transmission;iterative methods;power engineering computing;prototypes;reinforcement learning","automotive engineering;computation time reduction;computation times;finite element method analysis;intuitive process;iterative process;mutual inductance;nonlinear system design;optimal core structure design;optimal inductive power transfer core structures;optimal IPT core design;optimal reinforcement learning algorithms;vehicle assembly;WEV IPT system;wireless electric vehicle","","","","20","IEEE","21 Jul 2023","","","IEEE","IEEE Journals"
"Deep reinforcement learning for proactive spectrum defragmentation in elastic optical networks","E. Etezadi; C. Natalino; R. Diaz; A. Lindgren; S. Melin; L. Wosinska; P. Monti; M. Furdek","Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Telia Company, Solna, Sweden; Telia Company, Solna, Sweden; Telia Company, Solna, Sweden; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden","Journal of Optical Communications and Networking","28 Sep 2023","2023","15","10","E86","E96","The immense growth of Internet traffic calls for advanced techniques to enable the dynamic operation of optical networks, efficient use of spectral resources, and automation. In this paper, we investigate the proactive spectrum defragmentation (SD) problem in elastic optical networks and propose a novel deep reinforcement learning-based framework DeepDefrag to increase spectral usage efficiency. Unlike the conventional, often threshold-based heuristic algorithms that address a subset of the defragmentation-related tasks and have limited automation capabilities, DeepDefrag jointly addresses the three main aspects of the SD process: determining when to perform defragmentation, which connections to reconfigure, and which part of the spectrum to reallocate them to. By considering service attributes, the spectrum occupancy state expressed by several different fragmentation metrics, and the reconfiguration cost, DeepDefrag is able to consistently select appropriate reconfiguration actions over the network lifetime and adapt to changing conditions. Extensive simulation results reveal superior performance of the proposed scheme over a scenario with exhaustive defragmentation and a well-known benchmark heuristic from the literature, achieving lower blocking probability at a smaller defragmentation overhead.","1943-0639","","10.1364/JOCN.489577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10266913","","Measurement;Optical fiber networks;Optical scattering;Heuristic algorithms;Task analysis;Mathematical models;Entropy","deep learning (artificial intelligence);Internet;optical fibre networks;probability;reinforcement learning;telecommunication computing;telecommunication traffic","appropriate reconfiguration actions;automation capabilities;conventional threshold-based heuristic algorithms;deep reinforcement learning-based framework DeepDefrag;defragmentation-related tasks;dynamic operation;elastic optical networks;exhaustive defragmentation;immense growth;Internet traffic calls;network lifetime;often threshold-based heuristic algorithms;proactive spectrum defragmentation problem;SD process;smaller defragmentation overhead;spectral resources;spectral usage efficiency;spectrum occupancy state","","","","","","28 Sep 2023","","","IEEE","IEEE Journals"
"When Multi-access Edge Computing Meets Multi-area Intelligent Reflecting Surface: A Multi-agent Reinforcement Learning Approach","S. Zhuang; Y. He; F. R. Yu; C. Gao; W. Pan; Z. Ming","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China","2022 IEEE/ACM 30th International Symposium on Quality of Service (IWQoS)","5 Jul 2022","2022","","","1","10","In recent years, multi-access edge computing (MEC) is emerging to provide computation and storage capabilities to the Internet of things (IoT) devices to improve the quality of service (QoS) of IoT applications. In addition, intelligent reflecting surface (IRS) techniques have attracted great interests from both academia and industry to improve the communication efficiency. Although existing works leverage the IRS technique in MEC networks, they mainly focus on the single-IRS single-area scenario. However, in practice, multi-IRS will be deployed in multi-area scenarios in future networks. Consequently, considering the single-IRS single-area scenario will have inferior performance. In this paper, to address the aforementioned issue, we propose an efficient resource provisioning scheme for multi-IRS multi-area scenarios in MEC networks. We first model the problem as a cooperative multi-agent reinforcement learning process, where each agent manages one area and all agents share the network bandwidth and computation resources. Then, we propose a multi-agent actor-critic method with an attention mechanism for resource management with latency guarantee. Finally, we conduct extensive simulations to verify the effectiveness of the proposed scheme. Our scheme can reduce the required computation resources by up to 11.84% when compared with the benchmark works. It is also shown that our proposed scheme can improve the efficiency of resource allocation and scale well with the increasing demand from IoT devices.","1548-615X","978-1-6654-6824-4","10.1109/IWQoS54832.2022.9812883","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812883","Intelligent reflecting surface;Multi-agent reinforcement learning;Resource allocation","Industries;Multi-access edge computing;Computational modeling;Quality of service;Reinforcement learning;Bandwidth;Benchmark testing","Internet of Things;multi-access systems;multi-agent systems;quality of service;reinforcement learning;resource allocation","multiaccess edge computing;multiarea intelligent reflecting surface;storage capabilities;Internet of things devices;intelligent reflecting surface techniques;IRS technique;MEC networks;single-IRS single-area scenario;efficient resource provisioning scheme;multiIRS multiarea scenarios;multiagent reinforcement learning process;network bandwidth;multiagent actor-critic method;computation resources","","","","26","IEEE","5 Jul 2022","","","IEEE","IEEE Conferences"
"Model Predictive Control Guided Reinforcement Learning Control Scheme","H. Xie; X. Xu; Y. Li; W. Hong; J. Shi","Department of Chemical and Biochemical Engineering, Xiamen University, Xiamen, China; Department of Chemical and Biochemical Engineering, Xiamen University, Xiamen, China; Department of Earth Science and Engineering, Imperial College London, London, UK; Department of Chemical and Biochemical Engineering, Xiamen University, Xiamen, China; Department of Chemical and Biochemical Engineering, Xiamen University, Xiamen, China","2020 International Joint Conference on Neural Networks (IJCNN)","28 Sep 2020","2020","","","1","8","Deep Reinforcement Learning (DRL) is an artificial intelligence technology that can complete decision-making tasks by interaction. It has been successfully applied to various games. However, there are still many challenges when this technique is applied to the industrial process control due to the low sample efficiency and the inability to deal with large time delay. In this paper, a novel Model Predictive Control (MPC) guided Reinforcement Learning Control (MP-RLC) scheme is proposed for the process control. In this scheme, Model predictive control is directly combined with Reinforcement Learning (RL) to guide the training process, thus greatly improving the sample efficiency of reinforcement learning and effectively solving the problem of time delay. The simulation results on both a third-order linear system and a nonlinear continuous stirred tank reactor (CSTR) system with large time delay demonstrate that this scheme can not only accelerate the training process but also improve the control performance, which is superior to both standalone RL and MPC schemes. The proposed approach may help to pave the way for DRL applied to industrial processes.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9207398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207398","Deep reinforcement learning;Model predictive control;Time delay;Process control","Training;Linear systems;Delay effects;Simulation;Training data;Reinforcement learning;Prediction algorithms","learning (artificial intelligence);learning systems;manufacturing processes;nonlinear control systems;predictive control;process control","MPC schemes;industrial process control;time delay;model predictive control;control performance;reinforcement learning control scheme;deep reinforcement learning;third-order linear system;nonlinear continuous stirred tank reactor system;CSTR system","","8","","29","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
"Reconfigurable Intelligent Surface Assisted Multiuser MISO Systems Exploiting Deep Reinforcement Learning","C. Huang; R. Mo; C. Yuen","Engineering Product Development (EPD) Pillar, Singapore University of Technology and Design, Singapore; Engineering Product Development (EPD) Pillar, Singapore University of Technology and Design, Singapore; Engineering Product Development (EPD) Pillar, Singapore University of Technology and Design, Singapore","IEEE Journal on Selected Areas in Communications","19 Aug 2020","2020","38","8","1839","1850","Recently, the reconfigurable intelligent surface (RIS), benefited from the breakthrough on the fabrication of programmable meta-material, has been speculated as one of the key enabling technologies for the future six generation (6G) wireless communication systems scaled up beyond massive multiple input multiple output (Massive-MIMO) technology to achieve smart radio environments. Employed as reflecting arrays, RIS is able to assist MIMO transmissions without the need of radio frequency chains resulting in considerable reduction in power consumption. In this paper, we investigate the joint design of transmit beamforming matrix at the base station and the phase shift matrix at the RIS, by leveraging recent advances in deep reinforcement learning (DRL). We first develop a DRL based algorithm, in which the joint design is obtained through trial-and-error interactions with the environment by observing predefined rewards, in the context of continuous state and action. Unlike the most reported works utilizing the alternating optimization techniques to alternatively obtain the transmit beamforming and phase shifts, the proposed DRL based algorithm obtains the joint design simultaneously as the output of the DRL neural network. Simulation results show that the proposed algorithm is not only able to learn from the environment and gradually improve its behavior, but also obtains the comparable performance compared with two state-of-the-art benchmarks. It is also observed that, appropriate neural network parameter settings will improve significantly the performance and convergence rate of the proposed algorithm.","1558-0008","","10.1109/JSAC.2020.3000835","A*STAR under its RIE2020 Advanced Manufacturing and Engineering (AME) Industry Alignment Fund–Pre Positioning (IAF-PP)(grant numbers:A19D6a0053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9110869","Reconfigurable intelligent surface;Massive MIMO;6G;smart radio environment;beamforming matrix;phase shift matrix;deep reinforcement learning","Array signal processing;MIMO communication;Optimization;Wireless communication;MISO communication;Antenna arrays;Receivers","antenna arrays;array signal processing;learning (artificial intelligence);matrix algebra;MIMO communication;optimisation;radio receivers;wireless channels","programmable meta-material;future six generation wireless communication systems;massive multiple input multiple output;smart radio environments;RIS;MIMO transmissions;radio frequency chains;transmit beamforming matrix;base station;phase shift matrix;DRL based algorithm;phase shifts;DRL neural network;reconfigurable intelligent surface assisted multiuser MISO systems exploiting deep reinforcement learning","","428","","43","IEEE","8 Jun 2020","","","IEEE","IEEE Journals"
"GaDQN-IDS: A Novel Self-Adaptive IDS for VANETs Based on Bayesian Game Theory and Deep Reinforcement Learning","J. Liang; M. Ma; X. Tan","College of Software Engineering, Shenzhen Institute of Information Technology, Shenzhen, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; College of Software Engineering, Shenzhen Institute of Information Technology, Shenzhen, China","IEEE Transactions on Intelligent Transportation Systems","10 Aug 2022","2022","23","8","12724","12737","Due to the nature of high mobility and dynamic network topology, Intrusion Detection Systems (IDSs) in Vehicular Ad-hoc Networks (VANETs) face lots of challenges, especially in balancing the accuracy and efficiency of detection. Current researches about the deployment of IDSs in VANETs mainly focus on a tradeoff between the effectiveness and efficiency, but few efforts have been done about the adaptability of the tradeoff in the changeable networks. Thus, we address two crucial problems: 1) how to perceive the environmental change in the perspective of an IDS? 2) how to make the IDS adaptive in different scenarios? In this paper, a Bayesian Game theory and Deep Q-learning Network-based IDS is proposed for VANETs, called GaDQN-IDS. The interactions between an IDS and attackers are formulated as a dynamic intrusion detection game, in which the IDS decides either to just adjust the tradeoff between the accuracy and efficiency or to be retrained completely when its detection capacity has declined. The Nash Equilibria (NE) of the game is derived to reveal how the optimal decision of the IDS depends on the detection performance and road conditions. Moreover, a Deep Q-learning Network (DQN)-Adjustment is proposed to realize the self-adaptation of the IDS in the dynamic game, while an Error Priority Learning (EPL) is further designed for IDS retraining in changing VANETs. Simulation results show that the GaDQN-IDS has better performance than other existing IDSs with higher detection rate as well as lower detection time and overhead.","1558-0016","","10.1109/TITS.2021.3117028","A*STAR through its RIE2020 Advanced Manufacturing and Engineering (AME) Industry Alignment Fund C PrePositing (IAF-PP)(grant numbers:A19D6a0053); Program of Guangdong Innovative Research Team(grant numbers:2020KCXTD040); Pengcheng Scholar Funded Scheme; Basic Research Project of Science and Technology Plan of Shenzhen(grant numbers:SZIITWDZC2021A02,JCYJ20200109141218676); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9573515","IDSs;VANETs;GaDQN-IDS;Nash equilibria;DQN-adjustment;EPL","Vehicle dynamics;Monitoring;Roads;Intrusion detection;Games;Bayes methods;Game theory","Bayes methods;game theory;reinforcement learning;telecommunication network topology;telecommunication security;vehicular ad hoc networks","novel self-adaptive IDS;dynamic network topology;Intrusion Detection Systems;Vehicular Ad-hoc Networks;changeable networks;IDS adaptive;called GaDQN-IDS;dynamic intrusion detection game;Deep Q-learning Network-Adjustment;dynamic game;changing VANETs","","5","","33","IEEE","14 Oct 2021","","","IEEE","IEEE Journals"
"A Cooperative Multiagent Reinforcement Learning Framework for Droplet Routing in Digital Microfluidic Biochips","C. Jiang; R. Yang; Q. Xu; H. Yao; T. -Y. Ho; B. Yuan","Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; School of Microelectronics, University of Science and Technology of China, Hefei, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","21 Aug 2023","2023","42","9","3007","3020","Digital microfluidic biochips (DMFBs) have shown great advantages in automatically executing biochemical protocols through manipulating discrete nano/picoliter droplets which are transported in parallel to achieve high-throughput outcomes. However, because of electrode degradations, the droplet transportation may fail, causing incorrect fluidic operations. To perform safety-critical bio-protocols, the reliability of droplet transportation becomes an utmost concern for DMFBs. It has been shown by the previous works that a reliable transportation policy can be learned using reinforcement learning (RL)-based methods by capturing the underlying health conditions of electrodes and making online decisions. However, previous RL methods may fail to accomplish routing tasks with multiple droplets, because there is a lack of cooperation among different agents (each agent represents one droplet). To deal with this problem and scale RL methods to many droplets, this article proposes a new cooperative centralized learning and distributed execution multiagent RL (MARL) framework for droplet routing in DMFBs using value-decomposition networks (VDNs). Moreover, to speed up the training and decision process as well as apply our method in large biochips, we use a partial observation space where agents can only observe environment in a limited field of view (FOV) centered around themselves. Compared with the state-of-the-art approach, the superior performance of the proposed approach is demonstrated on different DMFBs in terms of success rate and average completion time. We also validate our method on large biochips (e.g.,  $\mathbf {50\times 50}$  DMFBs) with more droplets than state-of-the-art approach (e.g., ten droplets).","1937-4151","","10.1109/TCAD.2022.3233019","National Natural Science Foundation of China(grant numbers:61976111,62141415); Guangdong Provincial Key Laboratory(grant numbers:2020B121201001); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X386); JC STEM Lab of Intelligent Design Automation; The Hong Kong Jockey Club Charities Trust; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004216","Digital microfluidic biochips (DMFBs);droplet routing;field of view (FOV);multiagent reinforcement learning (RL);value-decomposition networks (VDNs)","Electrodes;Routing;Biochips;Reinforcement learning;Transportation;Microfluidics;Task analysis","bioMEMS;drops;lab-on-a-chip;learning (artificial intelligence);microfluidics;multi-agent systems;reinforcement learning","50 DMFBs;biochemical protocols;centralized learning;different DMFBs;digital microfluidic biochips;droplet routing;droplet transportation;electrode degradations;execution multiagent RL framework;incorrect fluidic operations;making online decisions;multiagent reinforcement learning framework;multiple droplets;previous RL methods;reinforcement learning-based methods;reliable transportation policy;routing tasks;safety-critical bio-protocols","","","","49","IEEE","30 Dec 2022","","","IEEE","IEEE Journals"
"Actor-Critic Deep Reinforcement Learning for Solving Job Shop Scheduling Problems","C. -L. Liu; C. -C. Chang; C. -J. Tseng","Department of Industrial Engineering and Management, National Chiao Tung University, Hsinchu, Taiwan; Department of Industrial Engineering and Management, National Chiao Tung University, Hsinchu, Taiwan; Department of Industrial Engineering and Management, National Chiao Tung University, Hsinchu, Taiwan","IEEE Access","27 Apr 2020","2020","8","","71752","71762","In the past decades, many optimization methods have been devised and applied to job shop scheduling problem (JSSP) to find the optimal solution. Many methods assumed that the scheduling results were applied to static environments, but the whole environments in the real world are always dynamic. Moreover, many unexpected events such as machine breakdowns and material problems may be present to adversely affect the initial job scheduling. This work views JSSP as a sequential decision making problem and proposes to use deep reinforcement learning to cope with this problem. The combination of deep learning and reinforcement learning avoids handcraft features as used in traditional reinforcement learning, and it is expected that the combination will make the whole learning phase more efficient. Our proposed model comprises actor network and critic network, both including convolution layers and fully connected layer. Actor network agent learns how to behave in different situations, while critic network helps agent evaluate the value of statement then return to actor network. This work proposes a parallel training method, combining asynchronous update as well as deep deterministic policy gradient (DDPG), to train the model. The whole network is trained with parallel training on a multi-agent environment and different simple dispatching rules are considered as actions. We evaluate our proposed model on more than ten instances that are present in a famous benchmark problem library - OR library. The evaluation results indicate that our method is comparative in static JSSP benchmark problems, and achieves a good balance between makespan and execution time in dynamic environments. Scheduling score of our method is 91.12% in static JSSP benchmark problems, and 80.78% in dynamic environments.","2169-3536","","10.1109/ACCESS.2020.2987820","Ministry of Science and Technology, Taiwan(grant numbers:MOST 107-2221-E-009-109-MY2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066984","Job shop scheduling problem (JSSP);deep reinforcement learning;actor-critic network;parallel training","Job shop scheduling;Machine learning;Benchmark testing;Dynamic scheduling;Learning (artificial intelligence);Training;Optimization","decision making;job shop scheduling;learning (artificial intelligence);multi-agent systems;optimisation;problem solving;production engineering computing","dispatching rules;DDPG;convolution layers;actor-critic deep reinforcement learning;sequential decision making problem;material problems;machine breakdowns;static environments;job shop scheduling problem;optimization methods;dynamic environments;static JSSP benchmark problems;multiagent environment;deep deterministic policy gradient;parallel training method;critic network;actor network agent","","95","","44","CCBY","14 Apr 2020","","","IEEE","IEEE Journals"
"Dynamic Multichannel Access Based on Deep Reinforcement Learning in Distributed Wireless Networks","Q. Cui; Z. Zhang; Y. Shi; W. Ni; M. Zeng; M. Zhou","National Engineering Laboratory for Mobile Network Technologies, Beijing University of Posts and Telecommunications, Beijing, China; National Engineering Laboratory for Mobile Network Technologies, Beijing University of Posts and Telecommunications, Beijing, China; National Engineering Laboratory for Mobile Network Technologies, Beijing University of Posts and Telecommunications, Beijing, China; Digital Productivity and Services Flagship, Commonwealth Scientific and Industrial Research Organization, Sydney, NSW, Australia; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Baicells Technologies Company, Ltd., Plano, TX, USA","IEEE Systems Journal","9 Dec 2022","2022","16","4","5831","5834","With the emergence of innovative applications in vertical industries such as smart home and industrial automation, machine communication has shown a spurt of development. Different from the traditional human-oriented cellular communication, machine communication is characterized by strong uncertainty and abruptness, large-scale concurrent device connection as well as uneven and unsaturated data traffic. This article investigates the dynamic multiple-devices multiple-channels access for unsaturated traffic with retransmission mechanism, which is aimed at reducing the long-term data packet loss resulting from buffer overflows and transmission failure. The instant channel selection will lead to a non-negligible impact on the future decision, motivating us to model this problem as a Markov decision process. Limited by the unknown environment knowledge, we proposed a dynamic access policy based on deep reinforcement learning algorithm to optimally select the channel for transmission or keep silent for Internet-of-Things devices. Simulation results confirm that our proposed channel access strategy can reduce the collision and the packet loss of network. Furthermore, it can also work well when coexisting with the devices that adopt time division multiple access (TDMA) or ALOHA protocol.","1937-9234","","10.1109/JSYST.2021.3134820","Beijing Natural Science Foundation(grant numbers:L182038); National Natural Science Foundation of China(grant numbers:61971066); National Youth Top-notch Talent Support Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665388","Deep Q-network (DQN);dynamic multiple-channel access;deep reinforcement learning (DRL)","Costs;Time division multiple access;Q-learning;Packet loss;Performance evaluation;Heuristic algorithms;Markov processes","access protocols;cellular radio;computer network reliability;deep learning (artificial intelligence);Internet of Things;Markov processes;multiuser channels;reinforcement learning;telecommunication traffic;time division multiple access;wireless channels","ALOHA protocol;buffer overflows;channel access strategy;channel selection;data traffic;deep reinforcement learning algorithm;distributed wireless networks;dynamic access policy;dynamic multichannel access;human-oriented cellular communication;industrial automation;Internet-of-Things devices;long-term data packet loss;machine communication;Markov decision process;multiple-devices multiple-channels access;retransmission mechanism;smart home;TDMA;time division multiple access;transmission failure","","2","","13","IEEE","29 Dec 2021","","","IEEE","IEEE Journals"
"An Online Control Approach for Forging Machine Using Reinforcement Learning and Taboo Search","D. Zhang; Z. Gao; Z. Lin","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Faculty of Engineering and Environment, University of Northumbria, Newcastle upon Tyne, U.K.; School of Electrical Engineering, Tianjin University of Technology, Tianjin, China","IEEE Access","9 Sep 2020","2020","8","","158666","158678","It is noticed that offline-training and online-implementation method is dominant in the data-driven control. However, the inconsistence existing in offline data and online data may degrade the control performance. To address the aforementioned issue, an online control strategy is developed so that the control parameters can be updated online based on the real-time data measured to ensure satisfactory control performance in this study. Specifically, an online control algorithm is addressed to control the pressing-down speed of the forging machine based on the framework of the reinforcement learning that has a capability of building a complete mapping from state space to action space only according to the neighbour samples. Rather than using the way of trials and errors which is too slow to be online implementation, a taboo search is addressed to speed up the learning-working process by directly searching the control on the current states, followed by the stability conditions, derived from Lyapunov stability theory. A coarse model that is limited to get the cost information of the reinforcement learning is used to make the best of mechanism information, which prevents the occurrence of the invalid states that do not conform to system characteristics. The effectiveness of the algorithm is demonstrated by an ultra-low forging machine, which outperforms the conventional approaches such as PID and neural network control approaches. The proposed algorithm has advantages in parameter adjustments so that it is easier to implement in a practical system.","2169-3536","","10.1109/ACCESS.2020.3020550","National Science Foundation of China(grant numbers:61873180); Tianjin University Innovation Fund(grant numbers:2020XT-0024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9181549","Online control;reinforcement learning;taboo search;forging machine","Valves;Training;Learning (artificial intelligence);Process control;Aerospace electronics;Servomotors","control engineering computing;forging;learning (artificial intelligence);Lyapunov methods;pressing;production engineering computing;search problems;stability","online-implementation;ultra-low forging machine;Lyapunov stability theory;learning-working process;online control strategy;online data;offline data;data-driven control;offline-training;taboo search;reinforcement learning","","2","","31","CCBY","31 Aug 2020","","","IEEE","IEEE Journals"
"Roadmap for development of skills in Artificial Intelligence by means of a Reinforcement Learning model using a DeepRacer autonomous vehicle","J. L. Cota; J. A. T. Rodríguez; B. G. Alonso; C. V. Hurtado","School of Engineering and Sciences, Tecnológico de Monterrey, Monterrey, Mexico; School of Engineering and Sciences, Tecnológico de Monterrey, Monterrey, Mexico; School of Engineering and Sciences, Tecnológico de Monterrey, Monterrey, Mexico; School of Engineering and Sciences, Tecnológico de Monterrey, Monterrey, Mexico","2022 IEEE Global Engineering Education Conference (EDUCON)","11 May 2022","2022","","","1355","1364","Using Deepracer, through experimentation and simulation, theoretical concepts can be applied to a practical application of reinforcement learning (RL) in a real-life problem. It was considered as a highly useful tool to develop many direct and transversal competences that students need to work within the field of artificial intelligence (AI). Nowadays the combination of Hardware, Computer Vision methods and Machine Learning (ML) algorithms for the development of controllers for vehicle driving automation have facilitated the development of solutions for this problem. The intention of this work is to show a Roadmap that was formulated to learn AI, ML and RL competencies required to prepare undergraduate students for the industry of this area, following a structured mostly practical learning plan using Deepracer AWS platform and local alternatives for training; and a physical vehicle as primary tools that have made an incredibly compact setup process and reduced complexity in the educational researching field related to learning autonomous vehicles (AV) software development process. The AWS DeepRacer framework includes all needed hardware based on a two front-camera vehicle for stereo vision and a LiDAR sensor, besides it provides a powerful computation computer for high performance in scale autonomous vehicles. The roadmap was executed testing and comparing different RL models over the modalities that Deepracer provides (by comparing both local and AWS console training) documentation of the execution of the generated Roadmap is shown to ensure that should be considered as a stable learning system that could be followed by college programs. Finally, models were tested on a physical track built, and limitations, considerations and improvements for this Roadmap are explained as a contribution for future work.","2165-9567","978-1-6654-4434-7","10.1109/EDUCON52537.2022.9766659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9766659","Deep Learning;Reinforcement Learning;Artificial Intelligence;Educational Innovation;Professional Education","Training;Machine learning algorithms;Computational modeling;Vehicle driving;Reinforcement learning;Hardware;Software","control engineering computing;control engineering education;driver information systems;educational institutions;learning (artificial intelligence);mobile robots;remotely operated vehicles;robot vision;stereo image processing","compact setup process;stable learning system;generated roadmap;local AWS console training;powerful computation computer;stereo vision;front-camera vehicle;AWS DeepRacer framework;autonomous vehicles software development process;educational researching field;physical vehicle;local alternatives;practical learning plan;undergraduate students;RL competencies;vehicle driving automation;computer vision methods;transversal competences;direct competences;real-life problem;DeepRacer autonomous vehicle;reinforcement learning model;artificial intelligence","","2","","24","IEEE","11 May 2022","","","IEEE","IEEE Conferences"
"Blockchain-Enabled Software-Defined Industrial Internet of Things With Deep Reinforcement Learning","J. Luo; Q. Chen; F. R. Yu; L. Tang","Key Laboratory of Mobile Communication Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; Key Laboratory of Mobile Communication Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada; Key Laboratory of Mobile Communication Technology, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Internet of Things Journal","12 Jun 2020","2020","7","6","5466","5480","Recently, software-defined Industrial Internet of Things (SDIIoT), the integration of software-defined networking (SDN) and Industrial Internet of Things (IIoT), has emerged. It is perceived as an effective way to manage IIoT dynamically. Aiming to improve the scalability and flexibility of SDIIoT, multi-SDN has been applied to form a physically distributed control plane to handle a large amount of data generated by industrial devices. However, as the core of multi-SDN, reaching consensus among multiple SDN controllers is a thorny issue. To meet the required design principle, this article proposes a blockchain-enabled distributed SDIIoT to synchronize local views between distinct SDN controllers and finally reach the consensus of the global view. On the other hand, both the cryptographic operations of blockchain and the noncryptographic tasks have access to the same computational resource pool of mobile edge cloud (MEC). In order to optimize the system energy efficiency, we adaptively allocate computational resources and the batch size of the block by jointly considering the trust features of SDN controllers and the resource requirements of noncryptographic operations. To implement the truly distributed manner of blockchain, we describe our problem as a partially observable Markov decision process (POMDP) and propose a novel deep reinforcement learning (DRL) approach to solve it. In the simulation results, we compare three different protocols of blockchain and show the effectiveness of our scheme in each of them.","2327-4662","","10.1109/JIOT.2020.2978516","National Natural Science Foundation of China(grant numbers:61571073); Science and Technology Research Program of Chongqing Municipal Education Commission(grant numbers:KJZD-M201800601); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025213","Blockchain;deep reinforcement learning (DRL);Industrial Internet of Things (IIoT);software-defined networking (SDN)","Internet of Things;Protocols;Machine learning;Scalability;Computer architecture;Throughput","cloud computing;cryptography;Internet of Things;learning (artificial intelligence);Markov processes;production engineering computing;software defined networking","SDN controllers;design principle;industrial devices;physically distributed control plane;multiSDN;IIoT dynamically;software-defined networking;SDIIoT;blockchain-enabled software-defined Industrial Internet;deep reinforcement learning approach;resource requirements;computational resources;computational resource pool","","48","","39","IEEE","5 Mar 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Computation Offloading in Fog Enabled Industrial Internet of Things","Y. Ren; Y. Sun; M. Peng","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Industrial Informatics","5 Apr 2021","2021","17","7","4978","4987","Fog computing is seen as a key enabler to meet the stringent requirements of industrial Internet of Things (IIoT). Specifically, lower latency and IIoT devices’ energy consumption can be achieved by offloading computation-intensive tasks to fog access points (F-APs). However, traditional computation offloading optimization methods often possess high complexity, making them inapplicable in practical IIoT. To overcome this issue, this article proposes a deep reinforcement learning (DRL) based approach to minimize long-term system energy consumption in a computation offloading scenario with multiple IIoT devices and multiple F-APs. The proposal features a multi-agent setting to deal with the curse of dimensionality of the action space by creating a DRL model for each IIoT device, which identifies its serving F-AP based on network and device states. After F-AP selection is finished, a low complexity greedy algorithm is executed at each F-AP under a computation capability constraint to determine which offloading requests are further forwarded to the cloud. By conducting offline training in the cloud and then making decisions online, iterative online optimization procedures are avoided and, hence, F-APs can quickly adjust F-AP selection for each device with trained DRL models. Via simulation, the impact of batch size on system performance is demonstrated and the proposed DRL-based approach shows competitive performance compared to various baselines including exhaustive search and genetic algorithm based approaches. In addition, the generalization capability of the proposal is verified as well.","1941-0050","","10.1109/TII.2020.3021024","National Natural Science Foundation of China(grant numbers:62001053,61925101,61831002); State Major Science and Technology Special Project(grant numbers:2018ZX03001023); Beijing Natural Science Foundation(grant numbers:JQ18016); National Program for Special Support of Eminent Professionals; Fundamental Research Funds for the Central Universities(grant numbers:24820202020RC11); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9183960","Computation offloading;fog computing;industrial Internet of Things (IIoT);multi-agent deep reinforcement learning (DRL)","Task analysis;Energy consumption;Computational modeling;Complexity theory;Internet of Things;Optimization;Proposals","cloud computing;computational complexity;deep learning (artificial intelligence);energy consumption;genetic algorithms;greedy algorithms;Internet of Things;iterative methods;multi-agent systems;power aware computing;production engineering computing;search problems","generalization capability;iterative online optimization procedures;exhaustive search;batch size;online decision making;offline training;curse of dimensionality;long-term system energy consumption minimization;IIoT device energy consumption;fog enabled industrial Internet of Things;offloading requests;computation capability constraint;low complexity greedy algorithm;DRL model;multiagent;multiple IIoT devices;practical IIoT;traditional computation offloading optimization methods;fog access points;computation-intensive tasks;fog computing;genetic algorithm based approaches;DRL-based approach;system performance;trained DRL models;F-AP selection","","37","","36","IEEE","1 Sep 2020","","","IEEE","IEEE Journals"
"Adaptive Laser Welding Control: A Reinforcement Learning Approach","G. Masinelli; T. Le-Quang; S. Zanoli; K. Wasmer; S. A. Shevchik","Laboratory for Advanced Materials Processing, Swiss Federal Laboratories for Materials Science and Technology (EMPA), Thun, Switzerland; Laboratory for Advanced Materials Processing, Swiss Federal Laboratories for Materials Science and Technology (EMPA), Thun, Switzerland; Embedded Systems Laboratory, Swiss Federal Institute of Technology in Lausanne (EPFL), Lausanne, Switzerland; Laboratory for Advanced Materials Processing, Swiss Federal Laboratories for Materials Science and Technology (EMPA), Thun, Switzerland; Laboratory for Advanced Materials Processing, Swiss Federal Laboratories for Materials Science and Technology (EMPA), Thun, Switzerland","IEEE Access","11 Jun 2020","2020","8","","103803","103814","Despite extensive research efforts in the field of laser welding, the imperfect repeatability of the weld quality still represents an open topic. Indeed, the inherent complexity of the underlying physical phenomena prevents the implementation of an effective controller using conventional regulators. To close this gap, we propose the application of Reinforcement Learning for closed-loop adaptive control of welding processes. The presented system is able to autonomously learn a control law that achieves a predefined weld quality independently from the starting conditions and without prior knowledge of the process dynamics. Specifically, our control unit influences the welding process by modulating the laser power and uses optical and acoustic emission signals as sensory input. The algorithm consists of three elements: a smart agent interacting with the process, a feedback network for quality monitoring, and an encoder that retains only the quality critic events from the sensory input. Based on the data representation provided by the encoder, the smart agent decides the output laser power accordingly. The corresponding input signals are then analyzed by the feedback network to determine the resulting process quality. Depending on the distance to the targeted quality, a reward is given to the agent. The latter is designed to learn from its experience by taking the actions that maximize not just its immediate reward, but the sum of all the rewards that it will receive from that moment on. Two learning schemes were tested for the agent, namely ${Q}$ -Learning and Policy Gradient. The required training time to reach the targeted quality was 20 min for the former technique and 33 min for the latter.","2169-3536","","10.1109/ACCESS.2020.2998052","Swiss Federal Laboratories for Materials Science and Technology (EMPA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9102251","Laser welding;laser material processing;reinforcement learning;policy gradient;Q-learning;closed-loop control","Welding;Optical sensors;Process control;Power lasers;Laser modes","adaptive control;adaptive systems;closed loop systems;control engineering computing;feedback;large-scale systems;laser beam welding;learning (artificial intelligence);multi-agent systems;production engineering computing","adaptive laser welding control;reinforcement learning approach;extensive research efforts;imperfect repeatability;open topic;inherent complexity;underlying physical phenomena;effective controller;conventional regulators;closed-loop adaptive control;welding process;control law;predefined weld quality;process dynamics;control unit;optical emission signals;acoustic emission signals;sensory input;smart agent;feedback network;quality monitoring;quality critic events;output laser power;corresponding input signals;resulting process quality;targeted quality;learning schemes;policy gradient","","25","","47","CCBY","27 May 2020","","","IEEE","IEEE Journals"
"Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks","G. Schoettler; A. Nair; J. A. Ojea; S. Levine; E. Solowjow",Siemens; UC Berkeley; Siemens; UC Berkeley; Siemens,"2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","10 Feb 2021","2020","","","9728","9735","Robotic insertion tasks are characterized by contact and friction mechanics, making them challenging for conventional feedback control methods due to unmodeled physical effects. Reinforcement learning (RL) is a promising approach for learning control policies in such settings. However, RL can be unsafe during exploration and might require a large amount of real-world training data, which is expensive to collect. In this paper, we study how to use meta-reinforcement learning to solve the bulk of the problem in simulation by solving a family of simulated industrial insertion tasks and then adapt policies quickly in the real world. We demonstrate our approach by training an agent to successfully perform challenging real-world insertion tasks using less than 20 trials of real-world experience.","2153-0866","978-1-7281-6212-6","10.1109/IROS45743.2020.9340848","Siemens; Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340848","","Connectors;Training;Adaptation models;Service robots;Training data;Reinforcement learning;Task analysis","control engineering computing;feedback;friction;industrial robots;learning (artificial intelligence);mechanical contact;multi-agent systems;production engineering computing;robot dynamics","robotic industrial insertion tasks;friction mechanics;feedback control;learning control policies;contatct mechanics;metareinforcement learning;agent training","","18","","42","IEEE","10 Feb 2021","","","IEEE","IEEE Conferences"
"Adaptive Service Function Chain Scheduling in Mobile Edge Computing via Deep Reinforcement Learning","T. Wang; J. Zu; G. Hu; D. Peng","Institute of Command and Control Engineering, Army Engineering University, Nanjing, China; Institute of Command and Control Engineering, Army Engineering University, Nanjing, China; Institute of Command and Control Engineering, Army Engineering University, Nanjing, China; Institute of Command and Control Engineering, Army Engineering University, Nanjing, China","IEEE Access","17 Sep 2020","2020","8","","164922","164935","MEC (Mobile Edge Computing) provides both IT service environment and cloud computation on the edge of the network. This technology not only minimizes the end-to-end latency but also increases the efficiency of computing. Some latency-sensitive applications, such as cloud video, online game, and augmented reality, take advantage of the MEC system to provide fast and stable services. Several new network techniques, including the implementation of NFV (Network Function Virtualization), the placement of VNF (Virtual Network Function) and the scheduling of SFC (Service Function Chain), should be considered to be applied in the MEC system. In this paper, we focus on the research about the scheduling of SFC in the NFV enabled MEC system and propose a solution accordingly. First, we make reasonable assumptions on the settings of MEC systems and model the SFC scheduling problem into a flexible job-shop scheduling problem. Since minimizing the latency can significantly improve the quality of service (QoS) and increase the revenue of Internet Service Providers, our optimization goal is to minimize the overall scheduling latency. To solve this optimization problem, a deep reinforcement learning based algorithm DQS is proposed. DQS can detect the variation of the MEC system's environment and perform adaptive scheduling for SFC requests. As the results of the simulation indicate, DQS works better than the other off-the-shelf algorithms in two key indexes: overall scheduling latency and average resource usage. Moreover, DQS can shorten the decision time and schedule SFCs stably with high performance. It is suitable to be extended to an online scheduling algorithm.","2169-3536","","10.1109/ACCESS.2020.3022038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187253","Service function chain;mobile edge computing;scheduling optimization;deep reinforcement learning","Cloud computing;Job shop scheduling;Delays;Processor scheduling;Optimization;Servers","augmented reality;cloud computing;job shop scheduling;learning (artificial intelligence);optimisation;production engineering computing;quality of service;virtualisation","adaptive service function chain scheduling;mobile edge computing;schedule SFC;online scheduling algorithm;adaptive scheduling;DQS;scheduling latency;Internet Service Providers;flexible job-shop scheduling problem;SFC scheduling problem;virtual network function;network function virtualization;NFV;network techniques;stable services;fast services;MEC system;cloud video;latency-sensitive applications;end-to-end latency;cloud computation;service environment;deep reinforcement learning","","11","","42","CCBY","7 Sep 2020","","","IEEE","IEEE Journals"
"Concurrent Order Dispatch for Instant Delivery with Time-Constrained Actor-Critic Reinforcement Learning","B. Guo; S. Wang; Y. Ding; G. Wang; S. He; D. Zhang; T. He",Southeast University; Southeast University; Alibaba Group; Rutgers University; University of Connecticut; Rutgers University; Southeast University,"2021 IEEE Real-Time Systems Symposium (RTSS)","7 Dec 2021","2021","","","176","187","Instant delivery has developed rapidly in recent years and significantly changed the lifestyle of people due to its timeliness and convenience. In instant delivery, the order dispatch process is concurrent. Couriers take new orders continuously and deliver multiple orders in a delivery trip (i.e., a batch). The delivery time of orders in a batch is often overlapped and interlinked with each other. The pickup and delivery sequence of the existing orders in a batch changes dynamically due to time constraints and real-time overdue possibility (i.e., the rate of deliveries that are not finished in promised time). Most of existing order dispatch mechanisms are designed for independent order dispatch or concurrent delivery without strict time constraints, hence are incapable of handling real-time concurrent dispatch with strict time constraints in on-demand instant delivery.To address the challenge, we propose a Time-Constrained Actor-Critic Reinforcement learning based concurrent dispatch system called TCAC-Dispatch to enhance the long-term overall revenue and reduce the overdue rate. Specifically, we design a deep matching network (DMN) with a variable action space, which integrates the state embedding (including route behaviors encoding) and actions embedding features into a long-term matching value. Then the Actor-Critic model tackles the concurrent order dispatch problem considering strict time constraints and stochastic demand-supply in instant delivery. An estimated-time based action pruning module is designed to ensure time constraints guarantee and accelerate the training as well as dispatching processes. We evaluate the TCAC-Dispatch with one-month data involved with 36.48 million orders and 42,000 couriers collected from one of the largest instant delivery companies in China, i.e., Eleme. Empirical experiments are conducted on a data-driven emulator deployed on the development environment of Eleme and results show that our method achieves 22% of the increase in total revenue and reduces the overdue rate by 21.6%.","2576-3172","978-1-6654-2802-6","10.1109/RTSS52674.2021.00026","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622337","Instant delivery;Concurrent Order Dispatch;Reinforcement Learning","Training;Data analysis;Stochastic processes;Reinforcement learning;Companies;Real-time systems;Encoding","goods distribution;learning (artificial intelligence);order picking;production engineering computing;stochastic processes","multiple orders;on-demand instant delivery;concurrent order dispatch problem;estimated-time based action;time-constrained actor-critic reinforcement learning;deep matching network;stochastic demand-supply;pruning module;China;data-driven emulator","","9","","40","IEEE","7 Dec 2021","","","IEEE","IEEE Conferences"
"An Efficient Parallel Reinforcement Learning Approach to Cross-Layer Defense Mechanism in Industrial Control Systems","K. Zhong; Z. Yang; G. Xiao; X. Li; W. Yang; K. Li","College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Hunan Province Key Laboratory of Industrial Internet Technology and Security, Changsha University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022","2022","33","11","2979","2990","The ongoing digitalization enables stable control processes and smooth operations of Industrial Control Systems (ICSs). A direct consequence of the highly interconnected architecture of ICSs is the introduced cyber vulnerability and increasing cyber security threats to ICSs. Numerous researches pay attention to the security problem of ICSs. However, most current studies face two challenges. First, the interaction problem between the cyber layer and the physical layer of ICSs may result in incorrect attack response strategies. Second, ICSs are real-time systems, but existing defense decision algorithms based on game theory or reinforcement learning techniques have high computational complexity, which prevents them from making decisions quickly. In this paper, we design a new multi-attribute based method for quantifying rewards and propose a multi-attribute based Q-learning algorithm to resolve the interaction problem. In addition, to overcome the limitation of slow convergence, we develop an effective parallel Q-learning (PQL) algorithm to quickly find the optimal strategy. The experimental results show the effectiveness of the PQL algorithm. Compared with the Q-learning algorithm (QL) and the deep Q-network (DQN) algorithm, our proposed solution can reduce the average completion time by 12.5 to 37 percent.","1558-2183","","10.1109/TPDS.2021.3135412","NSFC(grant numbers:61772182,62172146,62172157,61802032,61802444); NSFC(grant numbers:61661146006); Hunan Province Key Laboratory of Industrial Internet Technology and Security(grant numbers:2019TP1011); National Key Research and Development Program of China(grant numbers:2021YFF0901001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650577","Industrial control system (ICS);interaction;multiple attributes;parallel q-learning;stochastic game","Games;Q-learning;Security;Integrated circuit modeling;Process control;Physical layer;Stochastic processes","control engineering computing;industrial control;production engineering computing;reinforcement learning;security of data","parallel reinforcement learning;cross-layer defense mechanism;industrial control systems;ICSs;cyber vulnerability;cyber layer;physical layer;parallel Q-learning;cyber security threats;multi-attribute based method","","7","","42","IEEE","14 Dec 2021","","","IEEE","IEEE Journals"
"Scheduling Storage Process of Shuttle-Based Storage and Retrieval Systems Based on Reinforcement Learning","L. Luo; N. Zhao; G. Lodewijks","School of Mechanical Engineering, University of Science and Technology Beijing, Beijing, China; School of Mechanical Engineering, University of Science and Technology Beijing, Beijing, China; School of Aviation, University of New South Wales, Sydney, Australia","Complex System Modeling and Simulation","29 Jul 2021","2021","1","2","131","144","The Shuttle-Based Storage and Retrieval System (SBS/RS) has been widely studied because it is currently the most efficient automated warehousing system. Most of the related existing studies are focused on the prediction and improvement of the efficiency of such a system at the design stage. Hence, the control of existing SBS/RSs has been rarely investigated. In existing SBS/RSs, some empirical rules, such as storing loads column by column, are used to control or schedule the storage process. The question is whether or not the control of the storage process in an existing system can be improved further by using a different approach. The storage process is controlled to minimize the makespan of storing a series of loads into racks. Empirical storage rules are easy to control, but they do not reach the minimum makespan. In this study, the performance of a control system that uses reinforcement learning to schedule the storage process of an SBS/RS with fixed configurations is evaluated. Specifically, a reinforcement learning algorithm called the actor-critic algorithm is used. This algorithm is made up of two neural networks and is effective in making decisions and updating itself. It can also reduce the makespan relative to the existing empirical rules used to improve system performance. Experiment results show that in an SBS/RS comprising six columns and six tiers and featuring a storage capacity of 72 loads, the actor-critic algorithm can reduce the makespan by 6.67% relative to the column-by-column storage rule. The proposed algorithm also reduces the makespan by more than 30% when the number of loads being stored is in the range of 7-45, which is equal to 9.7%-62.5% of the systems' storage capacity.","2096-9929","","10.23919/CSMS.2021.0013","National Natural Science Foundation of China(grant numbers:52075036); Natural Science Foundation of Beijing Municipality(grant numbers:L191011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9502052","Shuttle-Based Storage and Retrieval System (SBS/RS);reinforcement learning;scheduling","Performance evaluation;Schedules;System performance;Neural networks;Warehousing;Process control;Reinforcement learning","decision making;learning (artificial intelligence);neural nets;production engineering computing;scheduling;warehousing","actor-critic algorithm;column-by-column storage rule;automated warehousing system;empirical storage rules;reinforcement learning algorithm;storage process scheduling;shuttle-based storage and retrieval systems;SBS-RS;storage process control;neural network;decision making;control system performance","","5","","22","","29 Jul 2021","","","TUP","TUP Journals"
