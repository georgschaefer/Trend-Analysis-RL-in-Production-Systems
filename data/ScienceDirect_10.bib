@article{SANTOS201228,
title = {Dyna-H: A heuristic planning reinforcement learning algorithm applied to role-playing game strategy decision systems},
journal = {Knowledge-Based Systems},
volume = {32},
pages = {28-36},
year = {2012},
note = {New Trends on Intelligent Decision Support Systems},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2011.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950705111002097},
author = {Matilde Santos and José Antonio {Martín H.} and Victoria López and Guillermo Botella},
keywords = {Decision-making, Path-finding, Heuristic-search, A-star, Reinforcement-learning},
abstract = {In a role-playing game, finding optimal trajectories is one of the most important tasks. In fact, the strategy decision system becomes a key component of a game engine. Determining the way in which decisions are taken (e.g. online, batch or simulated) and the consumed resources in decision making (e.g. execution time, memory) will influence, to a major degree, the game performance. When classical search algorithms such as A∗ can be used, they are the very first option. Nevertheless, such methods rely on precise and complete models of the search space so there are many interesting scenarios where its application is not possible, and hence, model free methods for sequential decision making under uncertainty are the best choice. In this paper, we propose a heuristic planning strategy to incorporate, into a Dyna agent, the ability of heuristic-search in path-finding. The proposed Dyna-H algorithm selects branches more likely to produce outcomes than other branches, just as A∗ does. However, unlike A∗, it has the advantages of a model-free online reinforcement learning algorithm. We evaluate our proposed algorithm against the one-step Q-learning and Dyna-Q algorithms and found that the Dyna-H, with its advantages, produced clearly superior results.}
}
@article{QI2022123826,
title = {Generalization ability of hybrid electric vehicle energy management strategy based on reinforcement learning method},
journal = {Energy},
volume = {250},
pages = {123826},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.123826},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222007290},
author = {Chunyang Qi and Chuanxue Song and Feng Xiao and Shixin Song},
keywords = {Generalization ability, Energy management, Reinforcement learning, Hybrid electric vehicle},
abstract = {Energy management is a fundamental task of a hybrid electric vehicle. However, dealing with multiple hybrid electric vehicles would be very time consuming, and developing a separate management strategy for each model is a huge workload to. Based on the above problems, this paper investigates the generalization capability of energy management strategies for hybrid electric vehicles. To improve the generalization of energy management strategies, a multi-agent reinforcement learning algorithm is proposed. To achieve this goal, the first analysis from the state values of reinforcement learning in the state selection, if all the typical features of the vehicle operation are added to the reinforcement learning algorithm, then it will make the model have a certain generalization ability. Then, with the help of the auxiliary agent, the reward value of reinforcement learning can be improved by using KL-divergence. The training and validation results show that the strategy can also achieve the training effect when tested on new models. In addition, a new driving cycle is selected for environmental testing, and the results show that the method also has strong generalization ability.}
}
@article{MARAVALL2013661,
title = {Coordination of communication in robot teams by reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {61},
number = {7},
pages = {661-666},
year = {2013},
note = {Collective and Social Autonomous Robots},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2012.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0921889012001704},
author = {Darío Maravall and Javier {de Lope} and Raúl Domínguez},
keywords = {Multi-agent systems, Multi-robot systems, Dynamics of artificial languages, Computational semiotics, Reinforcement learning, Self-collective coordination, Language games, Signaling games},
abstract = {In multi-agent systems, the study of language and communication is an active field of research. In this paper we present the application of Reinforcement Learning (RL) to the self-emergence of a common lexicon in robot teams. By modeling the vocabulary or lexicon of each agent as an association matrix or look-up table that maps the meanings (i.e. the objects encountered by the robots or the states of the environment itself) into symbols or signals we check whether it is possible for the robot team to converge in an autonomous, decentralized way to a common lexicon by means of RL, so that the communication efficiency of the entire robot team is optimal. We have conducted several experiments aimed at testing whether it is possible to converge with RL to an optimal Saussurean Communication System. We have organized our experiments alongside two main lines: first, we have investigated the effect of the team size centered on teams of moderated size in the order of 5 and 10 individuals, typical of multi-robot systems. Second, and foremost, we have also investigated the effect of the lexicon size on the convergence results. To analyze the convergence of the robot team we have defined the team’s consensus when all the robots (i.e. 100% of the population) share the same association matrix or lexicon. As a general conclusion we have shown that RL allows the convergence to lexicon consensus in a population of autonomous agents.}
}
@article{ZHU2022107658,
title = {Alleviating parameter-tuning burden in reinforcement learning for large-scale process control},
journal = {Computers & Chemical Engineering},
volume = {158},
pages = {107658},
year = {2022},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.107658},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422000035},
author = {Lingwei Zhu and Go Takami and Mizuo Kawahara and Hiroaki Kanokogi and Takamitsu Matsubara},
keywords = {Reinforcement learning, Process control, Vinyl acetate monomer, Monotonic improvement},
abstract = {Modern process controllers necessitate high quality models and remedial system re-identification upon performance degradation. Reinforcement Learning (RL) can be a promising replacement for those laborious manual procedures. However, in realistic scenarios time is limited, algorithms that can robustly learn with reduced human-agent interactions or self-exploration e.g. parameter tuning are desired. In practice, a great portion of time in setting up an RL algorithm to properly work is spent on those trial-and-error interactions. To reduce the interaction time, we propose a principled framework to ensure monotonic policy improvement even with underperforming parameters, enhancing the robustness of RL process against parameter setting. We incorporate key ingredients such as random features and factorial policy into monotonic improvement mechanism for learning cautiously in large-scale process control problems. We demonstrate in challenging control problems on the simulated vinyl acetate monomer process that the proposed method robustly learns meaningful policy within a short, fixed learning horizon given various parameter configurations that simulate the interactions, comparing to the other method that can only show good performance specific to a narrow range of parameters.}
}
@article{BYUN2023111260,
title = {Embedding active learning in batch-to-batch optimization using reinforcement learning},
journal = {Automatica},
volume = {157},
pages = {111260},
year = {2023},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2023.111260},
url = {https://www.sciencedirect.com/science/article/pii/S0005109823004211},
author = {Ha-Eun Byun and Boeun Kim and Jay H. Lee},
keywords = {Batch process optimization, Batch-to-batch optimization, Reinforcement learning, Active learning, Bayes-Adaptive Markov decision process, Hyper-state, Optimization under uncertainty, Model-plant mismatch},
abstract = {Batch-to-batch (B2B) or run-to-run (R2R) optimization refers to the strategy of updating the operating parameters of a batch run based on the results of previous runs and exploits the repetitive nature of batch process operation. Although B2B optimization uses feedback from previous batch runs to learn about model uncertainty and improve the operation of future runs, the standard techniques have the limitations of passive learning and being myopic in making adjustments. This work proposes a novel way to use the reinforcement learning approach to embed the active learning feature into B2B optimization. For this, the B2B optimization problem is formulated as a maximization of a long-term performance of repeated batch runs, which are modeled as a stochastic process with uncertain parameters. To solve the resulting Bayes-Adaptive Markov decision process (BAMDP) problem in a near-optimal manner, a policy gradient reinforcement learning algorithm is employed. Through case studies, the behavior and effectiveness of the proposed B2B optimization method are examined by comparing it with the traditional certainty equivalence based B2B optimization method with passive learning.}
}
@article{YAN2023364,
title = {Reinforcement learning-based integrated active fault diagnosis and tracking control},
journal = {ISA Transactions},
volume = {132},
pages = {364-376},
year = {2023},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2022.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0019057822003299},
author = {Zichen Yan and Feng Xu and Junbo Tan and Houde Liu and Bin Liang},
keywords = {Active fault diagnosis, Fault-tolerant control, Constrained reinforcement learning, Maximum mean discrepancy},
abstract = {Reliable and real-time active diagnosis of system faults with uncertainties is strongly dependent on the input design. This paper establishes a data-driven framework for integrated design of active fault diagnosis and control while ensuring the tracking performance. To be specific, the input design is formulated as a constrained optimization problem that can be solved with the aid of constrained reinforcement learning algorithms. Moreover, based on the maximum mean discrepancy metric, a novel active fault isolation scheme is proposed to implement model discrimination using system outputs. At the end, the effectiveness of the proposed approach is evaluated in two case studies in the presence of probabilistic disturbances and uncertainties.}
}
@article{JIA2023128928,
title = {A novel health-aware deep reinforcement learning energy management for fuel cell bus incorporating offline high-quality experience},
journal = {Energy},
volume = {282},
pages = {128928},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.128928},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223023228},
author = {Chunchun Jia and Hongwen He and Jiaming Zhou and Jianwei Li and Zhongbao Wei and Kunang Li},
keywords = {Fuel cell buses, Energy management strategy, Deep reinforcement learning, High-quality learning experience, Vehicular energy systems durability},
abstract = {Data-driven intelligent energy management strategy (EMS) helps to further improve the performance and efficiency of fuel cell hybrid electric bus (FCHEB). However, most deep reinforcement learning (DRL) algorithms suffer from disadvantages such as overestimation and poor training stability, which limit the optimization effectiveness of the strategy. In addition, DRL-based EMSs tend to achieve good control only for the set optimization objectives and cannot be generalized to optimization objectives beyond the reward function. To solve the above problems, a novel health-aware DRL energy management for FCHEB is proposed in this paper. Firstly, based on the actual collected city bus driving cycles, a large amount of high-quality learning experience containing health-aware information is obtained through an advanced model predictive control strategy. Secondly, the state-of-the-art Twin Delayed Deep Deterministic policy gradient (TD3) algorithm is combined with offline high-quality learning experience to address the inherent shortcomings during the “cold-start phase” and to enhance the generalization capability of the proposed strategy. Finally, validation results showed that the proposed EMS improves training efficiency by 61.85% and fuel economy by 7.45%, extends fuel cell life by 4% and battery life by 19.4% compared to the conventional TD3-based EMS.}
}
@article{MAHAZABEEN2023100276,
title = {Enhancing EV charger resilience with reinforcement learning aided control},
journal = {e-Prime - Advances in Electrical Engineering, Electronics and Energy},
volume = {5},
pages = {100276},
year = {2023},
issn = {2772-6711},
doi = {https://doi.org/10.1016/j.prime.2023.100276},
url = {https://www.sciencedirect.com/science/article/pii/S2772671123001717},
author = {Maliha Mahazabeen and Ali Jafarian Abianeh and Shayan Ebrahimi and Hisham Daoud and Farzad Ferdowsi},
keywords = {Electric vehicle, Battery storage, Dual active bridge converter, PI Controller, Reinforcement learning},
abstract = {This study aims to improve the performance of sustainable electric vehicle chargers in the face of unpblackictable/unpreventable disturbances. Over the past few years, Dual Active Bridge (DAB) DC-DC Converters are procuring substantial recognition for electric vehicle charging applications due to their superior characteristics such as higher power density, bidirectional mode of operation, and higher efficiency. Unexpected disturbances and fault scenarios at both source and load sides can deteriorate DAB converters’ performance. In this study, the performance of a single-phase shifted DAB converter is enhanced to achieve desiblack output current under several disturbance conditions for electric vehicle (EV) charging applications. A Reinforcement Learning (RL) based Deep Deterministic Policy Gradient (DDPG) algorithm is deployed to proactively tune control parameters when the DAB undergoes certain unexpected disturbances including short circuit faults at the source and battery sides. Results show that the RL-tuned PI controller improves the rate of current overshoot significantly compablack with the manually-tuned PI controller. The method and results are validated through simulations in MATLAB/Simulink environment.}
}
@article{CAO2022110377,
title = {Finding the optimal multilayer network structure through reinforcement learning in fault diagnosis},
journal = {Measurement},
volume = {188},
pages = {110377},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2021.110377},
url = {https://www.sciencedirect.com/science/article/pii/S0263224121012707},
author = {Jie Cao and Jialin Ma and Dailin Huang and Ping Yu},
keywords = {Neural architecture search, Pareto efficiency, Reinforcement learning, Fault diagnosis},
abstract = {Deep learning (DL) is an important method in industrial fault diagnosis. However, DL’s network structure needs to be designed with experience. To simplify the design of network structures, we propose the neural architecture search network with Pareto efficiency reward and insert replay buffer (NAS-PERIRB) algorithm. In this paper, the early stopping and insert replay buffer (IRB) are used to improving the training efficiency of the samples. In addition, we design the Pareto efficiency reward function to optimize the goals and design a network search space to perform effective searches. What is more, we evaluate the NAS-PERIRB under two datasets. Results show that the two datasets have reached 99% accuracy in various situations, which means the NAS-PERIRB can achieve the purpose of designing the network structure independently.}
}
@article{WANG2022e11319,
title = {Multi-task dispatch of shared autonomous electric vehicles for Mobility-on-Demand services – combination of deep reinforcement learning and combinatorial optimization method},
journal = {Heliyon},
volume = {8},
number = {11},
pages = {e11319},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e11319},
url = {https://www.sciencedirect.com/science/article/pii/S240584402202607X},
author = {Ning Wang and Jiahui Guo},
keywords = {Sustainable transportation, Recharging task assignment, Repositioning task assignment, Bipartite graph matching, Maximum flow problem},
abstract = {The Autonomous Mobility-on-Demand system is an emerging green and sustainable transportation system providing on-demand mobility services for urban residents. To achieve the best recharging, delivering, and repositioning task assignment decision-making process for shared autonomous electric vehicles, this paper formulates the fleet dynamic operating process into a multi-agent multi-task dynamic dispatching problem based on Markov Decision Process. Specifically, the decision-making process at each time step is divided into 3 sub-processes, among which recharging and delivery task assignment processes are transformed into a maximum weight matching problem of bipartite graph respectively, and the repositioning task assignment process is quantified as a maximum flow problem. Kuhn-Munkres Algorithm and Edmond-Karp Algorithm are adopted to solve the above two mathematical problems to achieve the optimal task allocation policy. To further improve the dispatching performance, a new instant reward function balancing order income with trip satisfaction is designed, and a state-value function estimated by Back Propagation-Deep Neural Network is defined as a matching degree between each shared autonomous electric vehicle and each delivery task. The numerical results show that: (i) a reward function focusing on income and satisfaction can increase total revenue by 33.2%, (ii) the introduction of task allocation repositioning increases total revenue by 50.0%, (iii) a re-estimated state value function leads to a 2.8% increase in total revenue, (iv) the combination of charging and task repositioning can reduce user waiting time and significantly improve user satisfaction with the trip.}
}
@article{PANG2021107585,
title = {RL-DARTS: Differentiable neural architecture search via reinforcement-learning-based meta-optimizer},
journal = {Knowledge-Based Systems},
volume = {234},
pages = {107585},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107585},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121008479},
author = {Dong Pang and Xinyi Le and Xinping Guan},
keywords = {Neural architecture search, Reinforcement learning, Meta optimizer, Deep learning, Bi-level optimization},
abstract = {Differentiable search approaches have attracted extensive attention recently due to their advantages in effectively finding novel neural architectures. However, these methods suffer from shortcomings on heavy computation consumption and low robustness in some cases. In this work, we propose a novel differentiable search method based on reinforcement learning, to further improve the computation efficiency, network precision, and robustness in the neural architecture search area. Our method constructs a reinforcement learning-based meta-optimizer to solve the architecture-parameter optimization problem, which is superior in properties of adaptability and robustness to fixed optimizers. This learnable meta-optimizer can alter its model parameters along with the search process to adapt the optimization procedure, making it possible to find out better structures and parameters with less time. Specifically, we formulate a double-loop algorithm to address the optimization problem in the searched super-network. Through switching between the external and internal loops, our method alternately optimizes the super-network and the meta-optimizer, which converges to the optimal location more rapidly and robustly.}
}
@article{XIN2022126537,
title = {Online reinforcement learning multiplayer non-zero sum games of continuous-time Markov jump linear systems},
journal = {Applied Mathematics and Computation},
volume = {412},
pages = {126537},
year = {2022},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2021.126537},
url = {https://www.sciencedirect.com/science/article/pii/S0096300321006214},
author = {Xilin Xin and Yidong Tu and Vladimir Stojanovic and Hai Wang and Kaibo Shi and Shuping He and Tianhong Pan},
keywords = {Reinforcement learning, Markov jump linear systems, Multiplayer non-zero sum games, Coupled algebraic Riccati equations},
abstract = {In this paper, a novel online mode-free integral reinforcement learning algorithm is proposed to solve the multiplayer non-zero sum games. We first collect and learn the subsystems information of states and inputs; then we use the online learning to compute the corresponding N coupled algebraic Riccati equations. The policy iterative algorithm proposed in this paper can solve the coupled algebraic Riccati equations corresponding to the multiplayer non-zero sum games. Finally, the effectiveness and feasibility of the design method of this paper is proved by simulation example with three players.}
}
@article{YANG2021100030,
title = {Reinforcement learning for fluctuation reduction of wind power with energy storage},
journal = {Results in Control and Optimization},
volume = {4},
pages = {100030},
year = {2021},
issn = {2666-7207},
doi = {https://doi.org/10.1016/j.rico.2021.100030},
url = {https://www.sciencedirect.com/science/article/pii/S2666720721000199},
author = {Zhen Yang and Xiaoteng Ma and Li Xia and Qianchuan Zhao and Xiaohong Guan},
keywords = {Wind power, Battery energy storage, Fluctuation reduction, Sensitive-based optimization, Reinforcement learning},
abstract = {The intrinsic randomness of renewable energy has a negative impact on the safety of power grid. In this paper, we aim at decreasing large fluctuations of the power output from a wind farm integrated with a battery energy storage system (BESS), so as to improve the stability and quality of the power system. The control method is to dynamically charge or discharge the BESS, coordinated with limited wind curtailment. The fluctuation of total power output is measured by variance, which reflects the risk to the safety of grid. The difficulty is that this dynamic optimization problem does not meet the requirement of a standard Markov decision process (MDP) model, since the variance metric is not additive. To solve this problem, we first propose the sensitivity-based optimization method and derive a difference formula to quantify the variance metrics of the system. Then we implement the optimization approach as reinforcement learning algorithms, in a mode of data-driven. We develop the Q-learning algorithm so that it can be executed online and generate improved policies repeatedly with observed data. Furthermore, we implement Deep Q-Networks (DQN) to handle the difficulty of continuous states. The performance of the proposed algorithms is verified with real data, which demonstrates the effectiveness of our algorithms.}
}
@article{ZHOU2021369,
title = {ReinforceNet: A reinforcement learning embedded object detection framework with region selection network},
journal = {Neurocomputing},
volume = {443},
pages = {369-379},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.02.073},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221003295},
author = {Man Zhou and Rujing Wang and Chengjun Xie and Liu Liu and Rui Li and Fangyuan Wang and Dengshan Li},
keywords = {Reinforcement learning, Convolutional neural network, Region selection network, Object detection},
abstract = {In recent years, researchers have explored reinforcement learning based object detection methods. However, existing methods always suffer from barely satisfactory performance. The main reasons are that current reinforcement learning based methods generate a sequence of inaccurate regions without a reasonable reward function, and regard the non-optimal one at the final step as the detection result for lack of an effective region selection and refinement strategy. To tackle the above problems, we propose a novel reinforcement learning based object detection framework, namely ReinforceNet, possessing the capability of the region selection and refinement by integrating reinforcement learning agents’ action space with Convolutional Neural Network based feature space. In ReinforceNet, we redevelop a reward function that enables the agent to be trained effectively and provide more accurate region proposals. In order to further optimize them, we design Convolutional Neural Network based region selection network (RS-net) and bounding box refinement network (BBR-net). Particularly, the former consists of two sub-networks: Intersection-of-Union network (IoU-net) and Completeness network (CPL-net) which are employed jointly for selecting the optimal region proposal. The latter aims to refine the selected one as the final result. Extensive experimental results on two standard datasets PASCAL VOC 2007 and VOC 2012 demonstrate that ReinforceNet is capable of improving the region selection and learning better agent action representations for reinforcement learning, resulting in the state-of-the-art performance.}
}
@article{SHI202214,
title = {GPM: A graph convolutional network based reinforcement learning framework for portfolio management},
journal = {Neurocomputing},
volume = {498},
pages = {14-27},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.04.105},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222005021},
author = {Si Shi and Jianjun Li and Guohui Li and Peng Pan and Qi Chen and Qing Sun},
keywords = {Portfolio management, Reinforcement learning, Graph convolutional network, Company relationship},
abstract = {Portfolio management is a decision-making process of periodically reallocating a certain amount of funds into a portfolio of assets, with the objective of maximizing the profits constrained to a given risk level. Due to its nature of learning from dynamic interactions and planning for long-run performance, reinforcement learning (RL) recently has received much attention in portfolio management. However, most of existing RL-based PM approaches in general only consider price changes of portfolio assets and the implicit correlations between price changes, while ignoring the rich relations between companies in the market, such as two assets in the same sector or two companies have a supplier-customer relation, which are very important for trading decision making. To address these limitations, in this paper, we propose GPM, a novel graph convolutional network-based reinforcement learning framework for portfolio management, which first employs Relational Graph Convolutional Network (R-GCN) to extract asset relational features, and then combines relational features with multi-scale temporal features to make trading decisions for better performance. We also introduce softmax with temperature to increase portfolio diversity, which leads to further increase in profits. Experimental results on two real-world datasets: NASDAQ and NYSE, validate the effectiveness of GPM over state-of-the-art PM methods. Further, more experiments are conducted to evaluate GPM with different graph neural networks and different number of network layers, to explore proper settings for different markets.}
}
@article{CHEN2023110562,
title = {Searching High-value Edges Attack sequence through deep Reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {272},
pages = {110562},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110562},
url = {https://www.sciencedirect.com/science/article/pii/S095070512300312X},
author = {Libin Chen and Luyao Wang and Chengyi Zeng and Hongfu Liu and Jing Chen},
keywords = {Complex network, Edge attack, Deep reinforcement learning, GNN},
abstract = {It is of great significance to search high-value attack sequence in complex networks, which help us easily destroy the harmful network such as crime network. Most research focuses on searching high-value nodes attack sequence, while the edge is more fragile and difficult to defend. In this paper, we deem the attacking process as a Markov process, and design a framework SHEAR (Searching High-value Edges Attack sequence through deep Reinforcement learning). During training, in the encoding module, considering that the edges are numerous and more difficult to measure, Graph neural network (GNN) is applied to automatically obtain the node embedding, and then, according to the mechanism of edge connection in the network, we propose and verify a mapping function: Hadamard, which can enlarge the similar dimensions between nodes and reduce the different dimensions between nodes, so as to retain more network information. Then the edge vector and network vector are used as the input of deep reinforcement learning (DRL). In the decoding module, we approximate the state–action value function with Q network. SHEAR can be trained on the same type of small networks and then the same model can be migrated to a wide variety of scenarios with extraordinary performance and fast speed. Compared with the best results obtained by the other four classical methods, in scenarios where attack cost is not considered, the performance of SHEAR can be improved by 33.52% in synthetic networks and 30.74% in real networks. In scenarios where removal costs are equal to edge betweenness value, SHEAR can be improved by 56.53% in synthetic networks and 38.93% in real networks. For networks with random edge weights, SHEAR can be improved by 53.87% in synthetic networks and 29.30% in real networks. In addition, we conducted further experiments to evaluate the impact of the use of graph neural networks and reinforcement learning on model performance, and found that improvements in graph representation capabilities and decision-making capabilities lead to better model performance.}
}
@article{SIVASHANGARAN2021218,
title = {Intelligent Autonomous Navigation of Car-Like Unmanned Ground Vehicle via Deep Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {20},
pages = {218-225},
year = {2021},
note = {Modeling, Estimation and Control Conference MECC 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.11.178},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321022229},
author = {Shathushan Sivashangaran and Minghui Zheng},
keywords = {Actor-Critic Network, Autonomous Vehicles, Deep Deterministic Policy Gradient, Deep Reinforcement Learning, Intelligent Navigation, Motion Planning},
abstract = {In this paper, a car-like Unmanned Ground Vehicle (UGV) is simulated and trained as an intelligent agent to navigate and exit unknown obstacle filled environments given no prior knowledge of environment characteristics, using a Reinforcement Learning (RL) algorithm tailored for continuous action spaces. This is achieved using Deep Deterministic Policy Gradient (DDPG), an Actor-Critic network that combines multiple cutting-edge Artificial Intelligence methods including continuous Deep-Q learning, policy gradient methods and actor-critic networks. A combination of two feedforward neural networks with Rectified Linear Units (ReLU) is used for the critic and actor representations which combine both policy and value based methods to learn continuous action space policies via approximation functions. The role of the actor network in this architecture is to decide linear and angular velocity outputs from a continuous action space given current state inputs, to then be evaluated by the critic network to learn and estimate Q-values by minimizing a loss function. The proposed DDPG RL network is trained and evaluated in two obstacle filled environments for a car-like UGV with wheelbase, l of 0.3 m. During the 10,000 episode training period, the agent converges to a maximum reward value of 180 after 1100 training episodes in the first environment, and a maximum reward value of 80 after 7500 training episodes in the second, more complex environment. The agent is shown to exhibit intelligent human-like learning behavior to learn optimal policies and adapt to new environments at the end of each training period with no changes to network architecture.}
}
@article{ZHU2021102272,
title = {Power system structure optimization based on reinforcement learning and sparse constraints under DoS attacks in cloud environments},
journal = {Simulation Modelling Practice and Theory},
volume = {110},
pages = {102272},
year = {2021},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2021.102272},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X21000034},
author = {Zhiqin Zhu and Fancheng Zeng and Guanqiu Qi and Yuanyuan Li and Hou Jie and Neal Mazur},
keywords = {Structure optimization, Reinforcement learning, Neural networks, Sparse constraints, Power system, Power grids, DoS attacks, Cloud computing},
abstract = {Due to the complex structure of a distributed energy resources system (DES) and a large amount of sensor data, local computers cannot provide enough computing resources to process the related data in a short time. Moreover, network integration causes a power system vulnerable to denial of service (DoS) attacks. DoS attacks result in the loss of partial sensor data, which affects the control performance of local computers on a power system. Therefore, this paper proposes a power system structure optimization strategy based on both sparse constraint optimization and cloud computing to solve the lack of computing power from local computers and prevent DoS attacks. Cloud computing is introduced to provide powerful computing resources for processing the related data in the proposed solution. The blocking probability of sensor data caused by DoS attacks is reduced by optimizing the sensor layout of a power system and reducing the transmission of sensor data. This paper also proposes a control strategy based on actor–critic reinforcement learning (RL) to maintain the stability of a power system during the structure optimization process. Three IEEE bus test systems are used to verify the effectiveness of the proposed structure optimization method and control strategy. The experimental results confirm that the proposed structure optimization method and control strategy can maintain the stability of a power system under DoS attacks.}
}
@article{FARIA2024107256,
title = {A data-driven tracking control framework using physics-informed neural networks and deep reinforcement learning for dynamical systems},
journal = {Engineering Applications of Artificial Intelligence},
volume = {127},
pages = {107256},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107256},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623014409},
author = {R.R. Faria and B.D.O. Capron and A.R. Secchi and M.B. {De Souza}},
keywords = {Reinforcement learning, Predictive control, Deep neural networks},
abstract = {This paper addresses how physical knowledge can improve machine learning in process control. A data-driven tracking control framework using physics-informed neural networks (PINNs) and deep reinforcement learning (DRL) is proposed for dynamical systems, which is particularly important when iterations or repetitions of data collection experiments are limited or even not allowed. An indirect control approach is followed to accomplish this. First, a new process model is identified using PINNs trained from a model representing the experimental data collected from the plant. Then, the DRL-based control agent utilizes the identified PINN model to deliver an offline policy. We evaluate our framework for data-driven tracking control of nonisothermal CSTR, considering measurement noise and stochastic dynamics for closed-loop control. Our approach performed comparably to an NMPC without requiring a model to predict the process dynamics.}
}
@article{JI2022105257,
title = {Online reinforcement learning for the shape morphing adaptive control of 4D printed shape memory polymer},
journal = {Control Engineering Practice},
volume = {126},
pages = {105257},
year = {2022},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2022.105257},
url = {https://www.sciencedirect.com/science/article/pii/S096706612200123X},
author = {Qinglei Ji and Xi Vincent Wang and Lihui Wang and Lei Feng},
keywords = {Closed loop control, Reinforcement learning, Q-learning, 4D printing, Shape memory polymer},
abstract = {Combining 3D printing and smart materials, 4D printing technologies enable the printed actuators to further change their shapes or other properties after prototyping. However, the shape morphing of 4D printed actuators suffers from poor controllability and low precision. One of the main challenges is that the 4D printed actuators are hard to be modeled and it is difficult to develop an appropriate controller for them. In this study, various popular reinforcement learning (RL) methods are applied to address the problem of online and adaptive model-free control of 4D printed shape memory polymer (SMP). Their training efficiencies are compared and an adaptive LQR controller based on Q learning is developed to realize efficient online learning. The RL controller achieves precise and quick shape control within 2−−3 learning episodes and is adaptive to the changing properties of SMP. The RL controller performance is then compared with a model-based LQR controller and shows high control precision and excellent adaptability to the varying control plant.}
}
@article{FULPAGARE2022112542,
title = {Optimal energy management for air cooled server fans using Deep Reinforcement Learning control method},
journal = {Energy and Buildings},
volume = {277},
pages = {112542},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112542},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822007137},
author = {Yogesh Fulpagare and Kuei-Ru Huang and Ying-Hao Liao and Chi-Chuan Wang},
keywords = {Air cooling, Control, Data center, Deep reinforcement learning, Energy-saving},
abstract = {The current study proposed the Deep Reinforcement Learning (DRL) AI control algorithm for energy-saving optimization in 1U air-cooled server mockup for different configurations and scenarios. The proposed algorithm can handle the server's complex thermal environment, including number of heat sources and their locations, type of heat sink, airflow, fan and bypass airflow. Based on the analysis and test results, it is found that controlling a single fan individually and the fans in different zones can offer the reduction of fan energy by 39–47% compared with the controlling all fans collectively. After the modification in observation characteristics, the fan usage rate was reduced by 45% compared with the single fan control model. Further, five different control models were developed and tested with experiments for different function variables and reward functions on three server configurations with different algorithmic settings for energy saving optimizations. The heat source of the three server configurations was close to the upper limit temperature and maintained stably by saving about 12% to 50% fan energy consumption. The algorithm can judge the system status and successfully provide real-time decision-making action thereby the energy consumption of the fan is significantly reduced.}
}
@article{QUANG2022479,
title = {Smart closed-loop control of laser welding using reinforcement learning},
journal = {Procedia CIRP},
volume = {111},
pages = {479-483},
year = {2022},
note = {12th CIRP Conference on Photonic Technologies [LANE 2022]},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.08.074},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122009532},
author = {Tri Le Quang and Bastian Meylan and Giulio Masinelli and Fatemeh Saeidi and Sergey A. Shevchik and Farzad Vakili Farahani and Kilian Wasmer},
keywords = {Closed-loop control, artificiel intelligence, AI, self-learning, reinforcement learning, laser welding},
abstract = {The present work demonstrates an adaptive closed-loop control for laser welding processes. Based on feedback signals from a sensing system, the controller interacts with the laser to adapt the processing parameters to achieve or maintain the target welding quality. The controller is constructed based on a model-free reinforcement learning approach, namely Q-learning. This algorithm allows autonomous learning of the control law independently from the starting conditions as well as any prior knowledge of the process dynamics. The controller's performance is demonstrated in both a well-controlled lab environment and more unpredictable industrial situations. For the demonstration, the control system is allowed to vary the laser power, and the feedback signal is given by an industrial laser process control unit (Coherent SmartSense+) using an optical sensor. The time needed to train the control system is approximately five and twenty minutes for the well-controlled and the industrial situations, respectively.}
}
@article{LIU2022108865,
title = {Attitude control for hypersonic reentry vehicles: An efficient deep reinforcement learning method},
journal = {Applied Soft Computing},
volume = {123},
pages = {108865},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108865},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622002514},
author = {Yiheng Liu and Honglun Wang and Tiancai Wu and Yuebin Lun and Jiaxuan Fan and Jianfa Wu},
keywords = {Deep reinforcement learning, Value function approximation, Attitude control, Anti-disturbance control, Hypersonic reentry vehicle},
abstract = {Aiming at the attitude control problem of hypersonic reentry vehicles (HRVs), a deep reinforcement learning (DRL) based anti-disturbance control method is proposed. First, a compound control framework consisting of a DRL-based auxiliary controller and a fixed-time anti-disturbance controller is proposed to improve the control performance under the premise of ensuring stability. Then, a novel value function approximation mechanism, named experience-based value expansion (EVE), is proposed to modify the value function update equation based on a two-dimensional replay buffer, which solves the DRL convergence problem brought by the HRV’s strong nonlinearities, tight coupling, and big flight envelope. Furthermore, a result-oriented encoder (ROE) is proposed to solve the DRL generalization problem brought by the HRV’s high uncertainties and unavailable real training environment. A bottleneck shape neural network structure is used for the DRL’s network structure to extract high-dimensional features and prevent overfitting to the training environment. Finally, abundant numerical comparative simulations demonstrate the effectiveness of the proposed efficient DRL algorithms and the DRL-based attitude controller.}
}
@article{LINS2019212,
title = {Deep reinforcement learning applied to the k-server problem},
journal = {Expert Systems with Applications},
volume = {135},
pages = {212-218},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419304154},
author = {Ramon Augusto Sousa Lins and Adrião Duarte Neto Dória and Jorge Dantas de Melo},
keywords = {Deep reinforcement learning, Online problem, The -server problem, Combinatorial optimization, Competitive location},
abstract = {The reinforcement learning paradigm has been shown to be an effective approach in solving the k-server problem. However, this approach is based on the Q-learning algorithm, being subjected to the curse of dimensionality problem, since the action-value function (Q-function) grows exponentially with the increase in the number of states and actions. In this work, a new algorithm based on the deep reinforcement learning paradigm is proposed. For this, the Q-function is defined by a multilayer perceptron neural network that extracts the information of the environment from images that encode the dynamics of the problem. The applicability of the proposed algorithm is illustrated in a case study in which different nodes and servers problem configurations are considered. The agents behavior is analyzed during the training phase and its efficiency is evaluated from performance tests that quantify the quality of the generated server displacement policies. The results obtained provide a new algorithm promising view as an alternative solution to the k-server problem.}
}
@article{HAN2020102247,
title = {A novel reinforcement learning method for improving occupant comfort via window opening and closing},
journal = {Sustainable Cities and Society},
volume = {61},
pages = {102247},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102247},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720304686},
author = {Mengjie Han and Ross May and Xingxing Zhang and Xinru Wang and Song Pan and Yan Da and Yuan Jin},
keywords = {Markov decision processes, Reinforcement learning, Window control, Indoor comfort, Occupant},
abstract = {An occupant's window opening and closing behaviour can significantly influence the level of comfort in the indoor environment. Such behaviour is, however, complex to predict and control conventionally. This paper, therefore, proposes a novel reinforcement learning (RL) method for the advanced control of window opening and closing. The RL control aims at optimising the time point for window opening/closing through observing and learning from the environment. The theory of model-free RL control is developed with the objective of improving occupant comfort, which is applied to historical field measurement data taken from an office building in Beijing. Preliminary testing of RL control is conducted by evaluating the control method’s actions. The results show that the RL control strategy improves thermal and indoor air quality by more than 90% when compared with the actual historically observed occupant data. This methodology establishes a prototype for optimally controlling window opening and closing behaviour. It can be further extended by including more environmental parameters and more objectives such as energy consumption. The model-free characteristic of RL avoids the disadvantage of implementing inaccurate or complex models for the environment, thereby enabling a great potential in the application of intelligent control for buildings.}
}
@article{XIE2023108209,
title = {Reinforcement learning for soft sensor design through autonomous cross-domain data selection},
journal = {Computers & Chemical Engineering},
volume = {173},
pages = {108209},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108209},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423000789},
author = {Junyao Xie and Oguzhan Dogru and Biao Huang and Chris Godwaldt and Brett Willms},
keywords = {Soft sensor, Reinforcement learning, Data selection, Domain adaptation},
abstract = {Data-driven soft sensors have been extensively applied in the process industry for quality variable estimation. It is challenging to build reliable soft sensors for complex industrial processes under new operating conditions where available data are limited. To overcome this issue, we leverage samples from source domains, formulate the sample selection and soft sensor problem of the target domain as a Markov decision process, and solve this cross-domain soft sensor problem by proposing a reinforcement learning framework. Specifically, we propose an asynchronous advantage selector-actor-critic method for cross-domain sample selection and soft sensor design. The transferability of source-domain samples to the target domain is determined by the proposed method. The correlation and estimation error metrics are incorporated into the reward function for the performance-driven design. An extension to feature data selection is also proposed. The applicability of the proposed methods is demonstrated via a simulation study and an industrial case study.}
}
@article{FELICE2022115,
title = {Deep reinforcement learning for closed-loop blood glucose control: two approaches},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {40},
pages = {115-120},
year = {2022},
note = {1st IFAC Workshop on Control of Complex Systems COSY 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.01.058},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323000630},
author = {Francesco Di Felice and Alessandro Borri and Maria Domenica Di Benedetto},
keywords = {Adaptive, Learning Systems, Modelling, Control of Biomedical Systems, Reinforcement learning control, Numerical simulation},
abstract = {Reinforcement learning, thanks to the observation-action approach, represents a useful control tool, in particular when the dynamics are characterized by strong non-linearity and complexity. In this sense, it has a natural application in the biological systems field where the complexity of the dynamics makes the automatic control particularly challenging. This paper presents a combined application of neural networks and reinforcement learning, in the so-called field of deep reinforcement learning, for the glucose regulation problem in patients with diabetes mellitus. The glucose control problem is solved through the Deep Deterministic Policy Gradient (DDPG) and the Soft Actor-Critic (SAC) algorithms, where the environment exploited for the agent's interactions is represented by a glucose model that is completely unknown to agents. Preliminary results show that the DDPG and SAC agents can suitably control the glucose dynamics, making the proposed approach promising for further investigations. The comparison between the two agents shows a better behaviour of SAC algorithm.}
}
@article{SREENIVAS2022200105,
title = {Safe deployment of a reinforcement learning robot using self stabilization},
journal = {Intelligent Systems with Applications},
volume = {16},
pages = {200105},
year = {2022},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2022.200105},
url = {https://www.sciencedirect.com/science/article/pii/S2667305322000436},
author = {Nanda Kishore Sreenivas and Shrisha Rao},
keywords = {Safety in robotics, Reinforcement learning, Self stabilization},
abstract = {In toy environments like video games, a reinforcement learning agent is deployed and operates within the same state space in which it was trained. However, in robotics applications such as industrial systems or autonomous vehicles, this cannot be guaranteed. A robot can be pushed out of its training space by some unforeseen perturbation, which may cause it to go into an unknown state from which it has not been trained to move towards its goal. While most prior work in the area of RL safety focuses on ensuring safety in the training phase, this paper focuses on ensuring the safe deployment of a robot that has already been trained to operate within a safe space. This work defines a condition on the state and action spaces, that if satisfied, guarantees the robot’s recovery to safety independently. We also propose a strategy and design that facilitate this recovery within a finite number of steps after perturbation. This is implemented and tested against a standard RL model, and the results indicate a significant improvement in performance.}
}
@article{CHAHARSOOGHI2008949,
title = {A reinforcement learning model for supply chain ordering management: An application to the beer game},
journal = {Decision Support Systems},
volume = {45},
number = {4},
pages = {949-959},
year = {2008},
note = {Information Technology and Systems in the Internet-Era},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2008.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167923608000560},
author = {S. Kamal Chaharsooghi and Jafar Heydari and S. Hessameddin Zegordi},
keywords = {Supply chain, Ordering policy, Multi-agent systems, Beer game, Reinforcement learning},
abstract = {A major challenge in supply chain ordering management is the coordination of ordering policies adopted by each level of the chain, so as to minimize inventory costs. This paper describes a new approach to decide on ordering policies of supply chain members in an integrated manner. In the first step supply chain ordering management has been considered as a multi-agent system and formulated as a reinforcement learning (RL) model. In the final step a Q-learning algorithm is proposed to solve the RL model. Results show that the reinforcement learning ordering mechanism (RLOM) is better than two other known algorithms.}
}
@article{FENG2023330,
title = {Approximating Nash equilibrium for anti-UAV jamming Markov game using a novel event-triggered multi-agent reinforcement learning},
journal = {Neural Networks},
volume = {161},
pages = {330-342},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.12.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022005226},
author = {Zikai Feng and Mengxing Huang and Yuanyuan Wu and Di Wu and Jinde Cao and Iakov Korovin and Sergey Gorbachev and Nadezhda Gorbacheva},
keywords = {Anti-jamming Markov game, Event-triggered multi-agent deep reinforcement learning, Beta strategy, Nash equilibrium},
abstract = {In the downlink communication, it is currently challenging for ground users to cope with the uncertain interference from aerial intelligent jammers. The cooperation and competition between ground users and unmanned aerial vehicle (UAV) jammers leads to a Markov game problem of anti-UAV jamming. Therefore, a model-free method is adopted based on multi-agent reinforcement learning (MARL) to handle the Markov game. However, the benchmark MARL strategies suffer from dimension explosion and local optimal convergence. To solve these issues, a novel event-triggered multi-agent proximal policy optimization algorithm with Beta strategy (ETMAPPO) is proposed in this paper, which aims to reduce the dimension of information transmission and improve the efficiency of policy convergence. In this event-triggering mechanism, agents can learn to obtain appropriate observation in different moment, thereby reducing the transmission of valueless information. Beta operator is used to optimize the action search. It expands the search scope of policy space. Ablation simulations show that the proposed strategy achieves better global benefits with fewer dimension of information than benchmark algorithms. In addition, the convergence performance verifies that the well-trained ETMAPPO has the capability to achieve stable jamming strategies and stable anti-jamming strategies. This approximately constitutes the Nash equilibrium of the anti-jamming Markov game.}
}
@article{GAUTRON2022107182,
title = {Reinforcement learning for crop management support: Review, prospects and challenges},
journal = {Computers and Electronics in Agriculture},
volume = {200},
pages = {107182},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107182},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922004999},
author = {Romain Gautron and Odalric-Ambrym Maillard and Philippe Preux and Marc Corbeels and Régis Sabbadin},
keywords = {reinforcement learning, multi-armed bandit, machine learning, decision support system, crop management},
abstract = {Reinforcement learning (RL), including multi-armed bandits, is a branch of machine learning that deals with the problem of sequential decision-making in uncertain and unknown environments through learning by practice. While best known for being the core of the artificial intelligence (AI) world’s best Go game player, RL has a vast range of potential applications. RL may help to address some of the criticisms leveled against crop management decision support systems (DSS): it is an interactive, geared towards action, contextual tool to evaluate series of crop operations faced with uncertainties. A review of RL use for crop management DSS reveals a limited number of contributions. We profile key prospects for a human-centered, real-world, interactive RL-based system to face tomorrow’s agricultural decisions, and theoretical and ongoing practical challenges that may explain its current low uptake. We argue that a joint research effort from the RL and agronomy communities is necessary to explore RL’s full potential.}
}
@article{KIM2023101177,
title = {Optimal continuous control of refrigerator for electricity cost minimization—Hierarchical reinforcement learning approach},
journal = {Sustainable Energy, Grids and Networks},
volume = {36},
pages = {101177},
year = {2023},
issn = {2352-4677},
doi = {https://doi.org/10.1016/j.segan.2023.101177},
url = {https://www.sciencedirect.com/science/article/pii/S2352467723001856},
author = {Bongseok Kim and Jihwan An and Min K. Sim},
keywords = {Hierarchical reinforcement learning, Refrigerator, Energy management, Optimal continuous control, Time-of-use},
abstract = {A refrigerator is a commonly used household appliance; however, limited research has focused on optimizing temperature control policy with the consideration of Time-of-Use (ToU) electricity price. This paper introduces a novel framework that utilizes hierarchical reinforcement learning (HRL) to control the intensity of refrigerator motors. The objective is to achieve both temperature regulation and cost savings under ToU and stochastic usage patterns. The problem is tackled by two HRL agents. The high-level agent is responsible for determining temperature reference based on ToU pricing, while the low-level agent adjusts the motor intensity to meet this temperature reference. To tackle non-stationarity in HRL, the high-level agent employs hindsight action transition and reward function approximation, while the low-level agent employs hindsight goal transition. Through the experimental evaluation, we found that the proposed method outperforms the conventional control methods and standard reinforcement learning approaches. It achieves the lowest total costs, resulting in a significant cost reduction of 5%–24%.}
}
@article{ZHANG20221,
title = {A leader-following paradigm based deep reinforcement learning method for multi-agent cooperation games},
journal = {Neural Networks},
volume = {156},
pages = {1-12},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S089360802200346X},
author = {Feiye Zhang and Qingyu Yang and Dou An},
keywords = {Multi-agent systems, Deep reinforcement learning, Centralized training with decentralized execution, Cooperative games},
abstract = {Multi-agent deep reinforcement learning algorithms with centralized training with decentralized execution (CTDE) paradigm has attracted growing attention in both industry and research community. However, the existing CTDE methods follow the action selection paradigm that all agents choose actions at the same time, which ignores the heterogeneous roles of different agents. Motivated by the human wisdom in cooperative behaviors, we present a novel leader-following paradigm based deep multi-agent cooperation method (LFMCO) for multi-agent cooperative games. Specifically, we define a leader as someone who broadcasts a message representing the selected action to all subordinates. After that, the followers choose their individual action based on the received message from the leader. To measure the influence of leader’s action on followers, we introduced a concept of information gain, i.e., the change of followers’ value function entropy, which is positively correlated with the influence of leader’s action. We evaluate the LFMCO on several cooperation scenarios of StarCraft2. Simulation results confirm the significant performance improvements of LFMCO compared with four state-of-the-art benchmarks on the challenging cooperative environment.}
}
@article{MORITA2016110,
title = {Corticostriatal circuit mechanisms of value-based action selection: Implementation of reinforcement learning algorithms and beyond},
journal = {Behavioural Brain Research},
volume = {311},
pages = {110-121},
year = {2016},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2016.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0166432816302923},
author = {Kenji Morita and Jenia Jitsev and Abigail Morrison},
keywords = {Corticostriatal, Action selection, Reinforcement learning, Reward prediction error, Winner-take-all, Nonlinear dynamics},
abstract = {Value-based action selection has been suggested to be realized in the corticostriatal local circuits through competition among neural populations. In this article, we review theoretical and experimental studies that have constructed and verified this notion, and provide new perspectives on how the local-circuit selection mechanisms implement reinforcement learning (RL) algorithms and computations beyond them. The striatal neurons are mostly inhibitory, and lateral inhibition among them has been classically proposed to realize “Winner-Take-All (WTA)” selection of the maximum-valued action (i.e., ‘max’ operation). Although this view has been challenged by the revealed weakness, sparseness, and asymmetry of lateral inhibition, which suggest more complex dynamics, WTA-like competition could still occur on short time scales. Unlike the striatal circuit, the cortical circuit contains recurrent excitation, which may enable retention or temporal integration of information and probabilistic “soft-max” selection. The striatal “max” circuit and the cortical “soft-max” circuit might co-implement an RL algorithm called Q-learning; the cortical circuit might also similarly serve for other algorithms such as SARSA. In these implementations, the cortical circuit presumably sustains activity representing the executed action, which negatively impacts dopamine neurons so that they can calculate reward-prediction-error. Regarding the suggested more complex dynamics of striatal, as well as cortical, circuits on long time scales, which could be viewed as a sequence of short WTA fragments, computational roles remain open: such a sequence might represent (1) sequential state-action-state transitions, constituting replay or simulation of the internal model, (2) a single state/action by the whole trajectory, or (3) probabilistic sampling of state/action.}
}
@article{ERGUN2022102869,
title = {Reinforcement learning based reliability-aware routing in IoT networks},
journal = {Ad Hoc Networks},
volume = {132},
pages = {102869},
year = {2022},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2022.102869},
url = {https://www.sciencedirect.com/science/article/pii/S1570870522000658},
author = {Kazim Ergun and Raid Ayoub and Pietro Mercati and Tajana Rosing},
keywords = {IoT networks, Routing, Reliability},
abstract = {The unprecedented scale and ubiquity of the Internet of Things (IoT) introduce a maintainability challenge. IoT networks operate in diverse and harsh environments that impose thermal stress on IoT devices. The lifetime of these networks can be limited by hardware failures resulting from exacerbated reliability degradation mechanisms at high temperatures. In this paper, we propose a novel adaptive and distributed reliability-aware routing protocol based on reinforcement learning to mitigate the reliability degradation of IoT devices and improve the network Mean Time to Failure (MTTF). Through routing, we curb the utilization of quickly degrading devices, which helps to lower the device power dissipation and temperature, thus reducing the effect of temperature-driven failure mechanisms. To quantify and optimize networking performance besides reliability, we incorporate Expected Transmission Count (ETX) in our formulations as a measure of communication link quality. Our proposed algorithm adapts routing decisions based on the current reliability status of the devices, the amount of degradation they are likely to experience due to communication activity, and network performance goals. We extend the ns-3 network simulator to support our reliability models and evaluate the routing performance by comparing with state-of-the-art approaches. Our results show up to a 73.2% improvement in reliability for various communication data rates and the number of nodes in the network while delivering comparable performance.}
}
@article{CARRERAS2002469,
title = {HIGH-LEVEL CONTROL OF AUTONOMOUS ROBOTS USING A BEHAVIOR-BASED SCHEME AND REINFORCEMENT LEARNING},
journal = {IFAC Proceedings Volumes},
volume = {35},
number = {1},
pages = {469-474},
year = {2002},
note = {15th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20020721-6-ES-1901.01303},
url = {https://www.sciencedirect.com/science/article/pii/S147466701539724X},
author = {M. Carreras and J. Yuh and J. Batlle},
keywords = {Autonomous vehicles, decentralized control, learning algorithms, neural networks, robot navigation},
abstract = {This paper proposes a behavior-based scheme for high-level control of autonomous robots. Two main characteristics can be highlighted in the control scheme. Behavior coordination is done through a hybrid methodology, which takes in advantages of the robustness and modularity in competitive approaches, as well as optimized trajectories in cooperative ones. As a second feature, behavior state/action mapping is learnt by means of Reinforcement Learning (RL). A new continuous approach of the Q_learning algorithm, implemented with a multi-layer neural network, is used. The behavior-based scheme attempts to fulfill simple missions in which several behaviors/tasks compete for the vehicle's control. This paper is centered in the RL-based behaviors. In order to test the feasibility of the proposed Neural-Q_learning scheme, real experiments with the underwater robot ODIN in a target following behavior were done. Results showed the convergence of the behavior into an optimal state/action mapping. Discussion about the proposed approach is given, as well as an overall description of the high level control scheme.}
}
@article{YANG2023305,
title = {A model-based deep reinforcement learning approach to the nonblocking coordination of modular supervisors of discrete event systems},
journal = {Information Sciences},
volume = {630},
pages = {305-321},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.02.033},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523002256},
author = {Junjun Yang and Kaige Tan and Lei Feng and Zhiwu Li},
keywords = {Deep reinforcement learning, Discrete event system, Local modular control, Supervisory control theory},
abstract = {Modular supervisory control may lead to conflicts among the modular supervisors for large-scale discrete event systems. The existing methods for ensuring nonblocking control of modular supervisors either exploit favorable structures in the system model to guarantee the nonblocking property of modular supervisors or employ hierarchical model abstraction methods for reducing the computational complexity of designing a nonblocking coordinator. The nonblocking modular control problem is, in general, NP-hard. This study integrates supervisory control theory and a model-based deep reinforcement learning method to synthesize a nonblocking coordinator for the modular supervisors. The deep reinforcement learning method significantly reduces the computational complexity by avoiding the computation of synchronization of multiple modular supervisors and the plant models. The supervisory control function is approximated by the deep neural network instead of a large-sized finite automaton. Furthermore, the proposed model-based deep reinforcement learning method is more efficient than the standard deep Q network algorithm.}
}
@article{POWELL2020107077,
title = {Real-time optimization using reinforcement learning},
journal = {Computers & Chemical Engineering},
volume = {143},
pages = {107077},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.107077},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420305500},
author = {By Kody M. Powell and Derek Machalek and Titus Quah},
abstract = {This work introduces a novel methodology for real-time optimization (RTO) of process systems using reinforcement learning (RL), where optimal decisions in response to external stimuli become embedded into a neural network. This is in contrast to the conventional RTO methodology, where a process model is solved repeatedly for optimality. This reinforcement learning real-time optimization methodology (RL-RTO) utilizes an actor-critic architecture similar to that being used in dynamic control research. However, the methodology presented here is purely for steady-state optimization, which is a novel feature of this work. This work also presents a novel, hybrid training methodology, where a gradient-based optimization solver is used for the training the value network (or critic) and a meta-heuristic optimization algorithm (particle swarm optimization or PSO) is used for training the policy network (or actor). Using this novel training algorithm, the neural networks representing the RL application can be updated in real-time or by using a batch-online training methodology. This technique allows for a solver to utilize the entire data set and attempt to find a global optimum, rather than by taking smaller, incremental update steps after each new data point is collected. As the process system runs and more data becomes available, the critic and the actor networks can be updated in sequence so that the RL-RTO application continually updates itself and gets closer to approaching true optimality. A process system (a chemical reactor) is used as a demonstration case study and also to compare the performance of RL-RTO to a conventional RTO methodology, which uses a near-perfect first principles model of the system, combined with a nonlinear programming (NLP) optimization technique. Each of these methods is compared to a brute force operational methodology in which the system's product throughput is maximized. The RL-RTO application demonstrates promise, as it improves the reactor's annual profit by 9.6%. By comparison, the first principles plus NLP method improves the profit by 17.2%. These RL-RTO results, while promising, indicate that there is still more development needed for RL-RTO to be a viable competitor to the conventional methods.}
}
@article{SEYYEDABBASI2023103411,
title = {A reinforcement learning-based metaheuristic algorithm for solving global optimization problems},
journal = {Advances in Engineering Software},
volume = {178},
pages = {103411},
year = {2023},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2023.103411},
url = {https://www.sciencedirect.com/science/article/pii/S0965997823000030},
author = {Amir Seyyedabbasi},
keywords = {Metaheuristic algorithm, Reinforcement learning algorithm, Sand cat swarm optimization, Q-learning, Machine learning},
abstract = {The purpose of this study is to utilize reinforcement learning in order to improve the performance of the Sand Cat Swarm Optimization algorithm (SCSO). In this paper, we propose a novel algorithm for the solution of global optimization problems that is called RLSCSO. In this method, metaheuristic algorithm is combined with reinforcement learning techniques to form a hybrid metaheuristic algorithm. This study aims to provide search agents with the opportunity to perform efficient exploration of the search space in order to find a global optimal solution by using efficient exploration and exploitation to find optimal solutions within a given search space. A comprehensive evaluation of the RLSCSO has been conducted on 20 benchmark functions and 100-digit challenge basic test functions. Additionally, the proposed algorithm is applied to the problem of localizing mobile sensor nodes, which is NP-hard (nondeterministic polynomial time). Several extensive analyses have been conducted in order to determine the effectiveness and efficiency of the proposed algorithm in solving global optimization problems. In terms of cost values, the RLSCSO algorithm provides the optimal solution, along with tradeoffs between exploration and exploitation.}
}
@article{CALS2021107221,
title = {Solving the online batching problem using deep reinforcement learning},
journal = {Computers & Industrial Engineering},
volume = {156},
pages = {107221},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107221},
url = {https://www.sciencedirect.com/science/article/pii/S036083522100125X},
author = {Bram Cals and Yingqian Zhang and Remco Dijkman and Claudy {van Dorst}},
keywords = {Deep reinforcement learning, Order batching, Sequential decision making, Machine learning, Warehousing, E-commerce},
abstract = {In e-commerce markets, on-time delivery is of great importance to customer satisfaction. In this paper, we present a Deep Reinforcement Learning (DRL) approach, together with a heuristic, for deciding how and when arrived orders should be batched and picked in a warehouse to minimize the number of tardy orders. In particular, the technique facilitates making decisions on whether an order should be picked individually (pick-by-order) or picked in a batch with other orders (pick-by-batch), and if so, with which other orders. We approach the problem by formulating it as a semi-Markov decision process and developing a vector-based state representation that includes the characteristics of the warehouse system. This allows us to create a deep reinforcement learning solution that learns a strategy by interacting with the environment and solve the problem with a proximal policy optimization algorithm. We evaluate the performance of the proposed DRL approach by comparing it with several batching and sequencing heuristics in different problem settings. The results show that the DRL approach can develop a strategy that produces consistent, good solutions and performs better than the proposed heuristics in most of the tested cases. We show that the strategy learned by DRL is different from the hand-crafted heuristics. In this paper, we demonstrate that the benefits from recent advancements of Deep Reinforcement Learning can be transferred to solve sequential decision-making problems in warehousing operations.}
}
@article{GORGES2019218,
title = {Distributed Adaptive Linear Quadratic Control using Distributed Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {11},
pages = {218-223},
year = {2019},
note = {5th IFAC Conference on Intelligent Control and Automation Sciences ICONS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.09.144},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319307748},
author = {Daniel Görges},
keywords = {Linear quadratic regulator, distributed control, adaptive control, large-scale systems, reinforcement learning, Q learning},
abstract = {In this paper distributed adaptive linear quadratic control of discrete-time linear large-scale systems with unknown dynamics using distributed reinforcement learning is studied. Linear quadratic control based on dynamic programming (specifically policy iteration) and adaptive linear quadratic control based on reinforcement learning (especially Q learning) are reviewed first. Then distributed adaptive linear quadratic control is addressed. Two Q functions exploiting the quadratic structure of the value function and leading to a decentralized and a distributed policy are proposed and a decentralized as well as a distributed Q learning algorithm are presented. Finally the concepts are evaluated in a simulation study. The simulation results indicate that the distributed policy is near-optimal.}
}
@article{ZHANG2023136800,
title = {Energy management strategy of a novel parallel electric-hydraulic hybrid electric vehicle based on deep reinforcement learning and entropy evaluation},
journal = {Journal of Cleaner Production},
volume = {403},
pages = {136800},
year = {2023},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2023.136800},
url = {https://www.sciencedirect.com/science/article/pii/S0959652623009587},
author = {Zhen Zhang and Tiezhu Zhang and Jichao Hong and Hongxin Zhang and Jian Yang},
keywords = {Deep reinforcement learning, Energy management, Hybrid vehicle, Electric-hydraulic, Entropy},
abstract = {An excellent energy management strategy is paramount to the new energy vehicle safety, durability, and reliability, which invariably affects the driving experience. This paper proposes a novel parallel electric-hydraulic hybrid electric vehicle (PEHHEV), which has the characteristics of multiple working modes and power sources. Aiming at the defects of outmoded control strategies, this paper applies the long short-term memory (LSTM) neural network to the proximal policy optimization (PPO) algorithm. A PPO-LSTM-based energy management strategy for PEHHEV to achieve optimal working mode switching is established. To avoid paying too much attention to the economy and neglecting other performance parameters, we design a local sample Shannon entropy to realize dynamic evaluation for performance parameters. Through offline training, online testing, and entropy evaluation, the energy consumption rate under WLTC and NEDC increased by 18.51% and 15.74% respectively. It is indicated that PEHHEV can maintain dynamic performance and obtain lower energy consumption under the PPO-LSTM-based energy management strategy, which verifies its feasibility and robustness. The investigation in this paper can improve energy management performance and fill the literature gap, which has more preponderance than other strategies. This is the first of its kind to apply PPO-LSTM and entropy evaluation to developing control strategies for modern vehicles.}
}
@article{CHIVKULA2022339,
title = {Curriculum-based reinforcement learning for path tracking in an underactuated nonholonomic system},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {37},
pages = {339-344},
year = {2022},
note = {2nd Modeling, Estimation and Control Conference MECC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.11.207},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322028506},
author = {Prashanth Chivkula and Colin Rodwell and Phanindra Tallapragada},
keywords = {Nonholonomic Systems, Reinforcement Learning, Path Tracking},
abstract = {Underactuated mechanical systems with nonholonomic constraints find applications in bioinspired robotics, such as snake-like robots and more recently in fish-like aquatic robots. Animal locomotion suggests that in such bioinspired robots, gaits or cyclic changes in kinematics or shape variables lead to efficient and agile motion. Path tracking in such nonholonomic systems that are not purely kinematic can be a challenging problem. In this paper we consider the problem of path tracking by a modified Chaplygin sleigh with a ‘tail’ which is a four degree of freedom nonholonomic system, possessing a single internal reaction wheel as an actuator. We develop a curriculum based deep Reinforcement Learning (RL) optimal control approach for simultaneous velocity and path tracking for this system. The curriculum based learning approach first leads to a policy for optimal tracking of limit cycles in a reduced velocity space and then in a next step to track a path. This curriculum approach allows an RL agent to learn the ’mechanics on invariant manifolds’ of the system and can be a useful approach in the motion control of high degree of freedom robots with increasing model complexity.}
}
@article{WANG2024108618,
title = {Control policy transfer of deep reinforcement learning based intelligent forced heat convection control},
journal = {International Journal of Thermal Sciences},
volume = {195},
pages = {108618},
year = {2024},
issn = {1290-0729},
doi = {https://doi.org/10.1016/j.ijthermalsci.2023.108618},
url = {https://www.sciencedirect.com/science/article/pii/S1290072923004799},
author = {Yi-Zhe Wang and Jiang-Zhou Peng and Nadine Aubry and Yu-Bai Li and Zhi-Hua Chen and Wei-Tao Wu},
keywords = {Deep reinforcement learning, Active thermal control, Forced heat convection, Heat transfer enhancement, Machine learning},
abstract = {Deep reinforcement learning (DRL) has gradually emerged as a novel and effective method for intelligent control of conjugate heat transfer. Through proper training, DRL agent usually can find a better control strategy than the one optimized manually. For numerous numerical simulation-based investigations, the promising results have only been obtained on 2-dimensional models due to the heavy burden of data acquisition. This paper proposes a novel strategy of transferring the control policy learned in 2-dimensional environment into a new 3-dimensional environment. PDD-DQN (Prioritized Dueling Double Deep Q-Networks) algorithm is used to recognize the underlying relationship between the forced fluid flow and heat transfer performance to produce a control strategy. The DRL controller is first trained on a simple 2-dimensional cavity with one heat source, and the DRL controller is able to reduce the maximum temperature to 315 K which is 2 K lower than the manually optimized control strategy; then the control strategy is transferred to 3-dimensional models where the maximum temperature is further reduced to 308 K; and compared to training the DRL agent directly on 3-dimensional model, the policy-transfer strategy requires only 10% computational expenditure. The proposed strategy is then applied to a more complex testbed with multiple heat sources for further investigating its ability; in both 2D and 3D environment, the DRL controller can cool the maximum temperature to be 307 K which is 8 K lower than the manually optimized control policy; and the computational cost drop to be 3%. The generalization ability of the trained DRL agent in inter-dimensional geometries is confirmed. For the cases with huge grid size, the policy-transfer strategy can economize exponential computational cost, which is of great significance for applying DRL methods to practical thermal control problems.}
}
@article{REDDY2023120663,
title = {Intelligent deep reinforcement learning based SMCF boundaries—An effective assessment of integrated network operating state},
journal = {Expert Systems with Applications},
volume = {230},
pages = {120663},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120663},
url = {https://www.sciencedirect.com/science/article/pii/S095741742301165X},
author = {Kadapa Harinadha Reddy},
keywords = {Integrated network, Deep reinforcement learning, Boundary limits, Sensitive modulation, Operating state},
abstract = {In this paper, applied energy system with multiple distributed generation (DG) units like solar PV, wind firm and biomass energy has been assessed and its operating state is estimated. Deep reinforcement learning (dRFL) based sensitive modulation control factor (SMCF) is proposed for assessment of entire applied energy system operating state during the utility grid connected or grid isolated mode/islanding mode. Grid isolated/island state identification is essentially required in integrated power grid when operating with DG to avoid the hazardous to human begins, as well as power electronic components. Time interval of distorted signal due to disturbance in integrated power grid has been taken for SMCF control. Variations in voltage (VRV), variations in frequency (VRF) beyond their limits i.e. lower limit (LL) and higher limit (HL) and these are considered to obtain SMCF in-phase component and quadrature phase component of circuit. Instant of SM control, tsm for a UV or OV are calculated and it is updated by the change in VRV. Obviously VRF also updates the instant of SM control and hence SMCF control factor is obtained. In-phase control variable of SMCF circuit i.e. αs_d with boundary limits i.e. αs_dLL and αs_dHL are used for the every 10 ms interval period/range to identify data of event i.e. either in island event (ISE) or non island event (NOIE). Now the dRFL is used for αsd training with a deterministic policy gradient (DMPG) evaluation. For a HL targets, two segment limits i.e. τ1, τ2 are taken for error minimization in dRFL training. According to first target; error Eα1 is validated the test condition of τ1<τ2 in a proposed dRFL training. Also error Eα2 is validated the test condition of τ1≥ τ2 for second target in a proposed dRFL training. The completion of dRFL training gives the αHL and hence minimization of EαHL as per objective will be obtained. Error minimization of reinforcement learning (RFL) has been obtained by DMPG for every target and action of energy system operating state. Effective dRFL based SMCF control algorithm introduced to validate every ISE, NOIE including lower mismatch scenarios and avoid the false tripping signal.}
}
@article{WANG2023117442,
title = {A comparative study of deep reinforcement learning based energy management strategy for hybrid electric vehicle},
journal = {Energy Conversion and Management},
volume = {293},
pages = {117442},
year = {2023},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2023.117442},
url = {https://www.sciencedirect.com/science/article/pii/S0196890423007884},
author = {Zexing Wang and Hongwen He and Jiankun Peng and Weiqi Chen and Changcheng Wu and Yi Fan and Jiaxuan Zhou},
keywords = {Energy management, Deep reinforcement learning, Hybrid electric vehicle, Power battery health},
abstract = {Energy management strategies (EMSs) are essential for hybrid electric vehicles (HEVs), as they can further exploit the potential of HEVs to save energy and reduce emissions. Research on deep reinforcement learning (DRL)-based EMSs is developing rapidly. However, most studies have ignored the impact of uniform test benchmarks on the performance of DRL-based EMS and focus too much on fuel economy improvement resulting in a single optimization objective. In this study, four DRL-based EMSs are designed for HEVs with a multi-objective optimization reward function that considers battery health furtherly. The optimal learning rates and weight coefficients of the four EMSs are determined first. Based on this, the monetary cost, fuel cost, and battery health of each EMS are intensively studied under nine driving cycles. The EMSs perform better in high-speed conditions and worse in suburban conditions are initially concluded. A comparative analysis under unlearned mixed driving cycles validates this conclusion and shows that the SAC-based EMS achieves a fuel consumption of 4.218L per 100 km and 99.96 % battery health, which are the lowest of the four EMSs. This paper can provide a theoretical basis for the parametric and driving cycle study of DRL-based EMSs.}
}
@article{CONRADIE20051,
title = {Development of neurocontrollers with evolutionary reinforcement learning},
journal = {Computers & Chemical Engineering},
volume = {30},
number = {1},
pages = {1-17},
year = {2005},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2005.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0098135405001870},
author = {Alex v.E. Conradie and Chris Aldrich},
keywords = {Neurocontrol, Reinforcement learning},
abstract = {The growth in intelligent control is among other fuelled by the realization that nonlinear control theory is not yet able to provide practical solutions to present day control challenges. Overdesign is therefore often used as a means to avoid highly nonlinear regions of operation, despite the risk of significant economic penalties both in terms of capital and operating costs. The Symbiotic Adaptive Neuro-Evolution (SANE) algorithm combines the design and controller development functions into a single coherent step through the use of evolutionary reinforcement learning. SANE locates the optimum operating steady state and develops a neurocontroller based on maximising economic considerations. In this paper, the use of SANE to optimize and control a bioreactor at its economically optimal steady state is discussed. The developed neurocontroller was found to be robust in the presence of significant process uncertainty, as a result of the generalization afforded by the learning algorithm. More autonomous control is thus achieved in operating regions of greater complexity and uncertainty. Overdesign in the process industries may thus be limited by the use of the SANE algorithm.}
}
@article{NOAEEN2022116830,
title = {Reinforcement learning in urban network traffic signal control: A systematic literature review},
journal = {Expert Systems with Applications},
volume = {199},
pages = {116830},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116830},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422002858},
author = {Mohammad Noaeen and Atharva Naik and Liana Goodman and Jared Crebo and Taimoor Abrar and Zahra Shakeri Hossein Abad and Ana L.C. Bazzan and Behrouz Far},
keywords = {Reinforcement learning, Traffic light control, Urban network, Multi-agent system, Intelligent transportation system, Artificial intelligence},
abstract = {Improvement of traffic signal control (TSC) efficiency has been found to lead to improved urban transportation and enhanced quality of life. Recently, the use of reinforcement learning (RL) in various areas of TSC has gained significant traction; thus, we conducted a systematic literature review as a systematic, comprehensive, and reproducible review to dissect all the existing research that applied RL in the network-level TSC domain, called as RL in NTSC or RL-NTSC for brevity. The review only targeted the network-level articles that tested the proposed methods in networks with two or more intersections. This review covers 160 peer-reviewed articles from 30 countries published from 1994 to March 2020. The goal of this study is to provide the research community with statistical and conceptual knowledge, summarize existence evidence, characterize RL applications in NTSC domains, explore all applied methods and major first events in the defined scope, and identify areas for further research based on the explored research problems in current research. We analyzed the extracted data from the included articles in the following seven categories: (i) publication and authors’ data, (ii) method identification and analysis, (iii) environment attributes and traffic simulation, (iv) application domains of RL-NTSC, (v) major first events of RL-NTSC and authors’ key statements, (vi) code availability, and (vii) evaluation. This paper provides a comprehensive view of the past 26 years of research on applying RL to NTSC. It also reveals the role of advancing deep learning methods in the revival of the research area, the rise of using non-commercial microscopic traffic simulators, a lack of interaction between traffic and transportation engineering practitioners and researchers, and a lack of proposal and creation of testbeds which can likely bring different communities together around common goals.}
}
@article{GOSAVI2004654,
title = {Reinforcement learning for long-run average cost},
journal = {European Journal of Operational Research},
volume = {155},
number = {3},
pages = {654-674},
year = {2004},
note = {Traffic and Transportation Systems Analysis},
issn = {0377-2217},
doi = {https://doi.org/10.1016/S0377-2217(02)00874-3},
url = {https://www.sciencedirect.com/science/article/pii/S0377221702008743},
author = {Abhijit Gosavi},
keywords = {Stochastic processes, Reinforcement learning, Two time scales},
abstract = {A large class of sequential decision-making problems under uncertainty can be modeled as Markov and semi-Markov decision problems (SMDPs), when their underlying probability structure has a Markov chain. They may be solved by using classical dynamic programming (DP) methods. However, DP methods suffer from the curse of dimensionality and break down rapidly in face of large state-spaces. In addition, DP methods require the exact computation of the so-called transition probabilities, which are often hard to obtain and are hence said to suffer from the curse of modeling as well. In recent years, a simulation-based method, called reinforcement learning (RL), has emerged in the literature. It can, to a great extent, alleviate stochastic DP of its curses by generating ‘near-optimal’ solutions to problems having large state-spaces and complex transition mechanisms. In this paper, a simulation-based algorithm that solves Markov and SMDPs is presented, along with its convergence analysis. The algorithm involves a step-size based transformation on two-time scales. Its convergence analysis is based on a recent result on asynchronous convergence of iterates on two-time scales. We present numerical results from the new algorithm on a classical preventive maintenance case study of a reasonable size, where results on the optimal policy are also available. In addition, we present a tutorial that explains the framework of RL in the context of long-run average cost SMDPs.}
}
@article{HU20216490,
title = {A model-free distributed cooperative frequency control strategy for MT-HVDC systems using reinforcement learning method},
journal = {Journal of the Franklin Institute},
volume = {358},
number = {13},
pages = {6490-6507},
year = {2021},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2021.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0016003221003562},
author = {Zhong-Jie Hu and Zhi-Wei Liu and Chaojie Li and Tingwen Huang and Xiong Hu},
abstract = {The performance of the existing frequency control strategies for MT-HVDC systems relies on the accuracy of the mathematical models. However, it is hard to obtain the exact mathematical models of MT-HVDC systems. To deal with this challenge, this paper presents a distributed cooperative frequency control strategy for MT-HVDC systems by using a reinforcement learning method, i.e., value iteration algorithm. Specifically, the proposed control strategy is data driven and hence model free. Besides, the proposed control strategy is distributed in the sense that each AC area only requires the local and neighboring information, rather than the information of all the AC areas. Moreover, the proposed control strategy makes the connected AC areas compensate load disturbances together by sharing their power reserves via HVDC grids, which greatly decreases the operation costs of MT-HVDC systems. The performance of the proposed control strategy is validated by cases study.}
}
@incollection{MODARES2018313,
title = {Chapter 14 - Adaptive H∞ Tracking Control of Nonlinear Systems Using Reinforcement Learning},
editor = {Danilo Comminiello and José C. Príncipe},
booktitle = {Adaptive Learning Methods for Nonlinear System Modeling},
publisher = {Butterworth-Heinemann},
pages = {313-333},
year = {2018},
isbn = {978-0-12-812976-0},
doi = {https://doi.org/10.1016/B978-0-12-812976-0.00018-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012812976000018X},
author = {Hamidreza Modares and Bahare Kiumarsi and Kyriakos G. Vamvoudakis and Frank L. Lewis},
keywords = { control, Optimal tracking, Reinforcement learning},
abstract = {This chapter presents online solutions to the optimal H∞ tracking of nonlinear systems to attenuate the effect of disturbance on the performance of the systems. To obviate the requirement of the complete knowledge of the system dynamics, reinforcement learning (RL) is used to learn the solutions to the Hamilton–Jacobi–Isaacs equations arising from solving the H∞ tracking problem. Off-policy RL algorithms are designed for continuous-time systems, which allows the reuse of data for learning and consequently leads to data efficient RL algorithms. A solution is first presented for the H∞ optimal tracking control of affine nonlinear systems. It is then extended to a special class of nonlinear nonaffine systems. It is shown that for the nonaffine systems existence of a stabilizing solution depends on the performance function. A performance function is designed to assure the existence of the solution to a class of nonaffine system, while taking into account the input constraints.}
}
@article{BETANCOURT2021114002,
title = {Deep reinforcement learning for portfolio management of markets with a dynamic number of assets},
journal = {Expert Systems with Applications},
volume = {164},
pages = {114002},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114002},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420307776},
author = {Carlos Betancourt and Wen-Hui Chen},
keywords = {Reinforcement learning, Deep learning, Portfolio management, Transaction costs, Multiple assets},
abstract = {This work proposes a novel portfolio management method using deep reinforcement learning on markets with a dynamic number of assets. This problem is especially important in cryptocurrency markets, which already support the trading of hundreds of assets with new ones being added every month. A novel neural network architecture is proposed, which is trained using deep reinforcement learning. Our architecture considers all assets in the market, and automatically adapts when new ones are suddenly introduced, making our method more general and sample-efficient than previous methods. Further, transaction cost minimization is considered when formulating the problem. For this purpose, a novel algorithm to compute optimal transactions given a desired portfolio is integrated into the architecture. The proposed method was tested on a dataset of one of the largest cryptocurrency markets in the world, outperforming state-of-the-art methods, achieving average daily returns of over 24%.}
}
@article{ZHANG2023677,
title = {Guided probabilistic reinforcement learning for sampling-efficient maintenance scheduling of multi-component system},
journal = {Applied Mathematical Modelling},
volume = {119},
pages = {677-697},
year = {2023},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2023.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X23001269},
author = {Yiming Zhang and Dingyang Zhang and Xiaoge Zhang and Lemiao Qiu and Felix T.S. Chan and Zili Wang and Shuyou Zhang},
keywords = {Deep Reinforcement Learning, Multi-component System, Probabilistic Machine Learning, Maintenance Scheduling, Sampling-Efficient Learning},
abstract = {In recent years, multi-agent deep reinforcement learning has progressed rapidly as reflected by its increasing adoptions in industrial applications. This paper proposes a Guided Probabilistic Reinforcement Learning (Guided-PRL) model to tackle maintenance scheduling of multi-component systems in the presence of uncertainty with the goal of minimizing the overall life-cycle cost. The proposed Guided-PRL is deeply rooted in the Actor-Critic (AC) scheme. Since traditional AC falls short in sampling efficiency and suffers from getting stuck in local minima in the context of multi-agent reinforcement learning, it is thus challenging for the actor network to converge to a solution of desirable quality even when the critic network is properly configured. To address these issues, we develop a generic framework to facilitate effective training of the actor network, and the framework consists of environmental reward modeling, degradation formulation, state representation, and policy optimization. The convergence speed of the actor network is significantly improved with a guided sampling scheme for environment exploration by exploiting rules-based domain expert policies. To handle data scarcity, the environmental modeling and policy optimization are approximated with Bayesian models for effective uncertainty quantification. The Guided-PRL model is evaluated using the simulations of a 12-component system as well as GE90 and CFM56 engines. Compared with four alternative deep reinforcement learning schemes, the Guided-PRL lowers life-cycle cost by 34.92% to 88.07%. In comparison with rules-based expert policies, the Guided-PRL decreases the life-cycle cost by 23.26% to 51.36%.}
}
@article{BAI2023102,
title = {Energy-efficient power control strategy of the delay tolerable service based on the reinforcement learning},
journal = {Computer Communications},
volume = {210},
pages = {102-115},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2023.07.034},
url = {https://www.sciencedirect.com/science/article/pii/S0140366423002670},
author = {Mengmeng Bai and Rui Zhu and Jianxin Guo and Feng Wang and Hangjie Zhu and Yushuai Zhang},
keywords = {Energy efficiency, Approximate statistical dynamic programming, Deep reinforcement learning, Deep Q network, Deep deterministic policy gradient, Proximal policy optimization, Outage probability},
abstract = {In recent years, the rapid development of Internet technology and its applications has led to an exponential growth in the number of Internet users and wireless terminal devices, resulting in a corresponding increase in energy consumption. This has necessitated the need to reduce energy consumption while maintaining the quality of communication services. To this end, we investigate the possibility of improving energy efficiency (EE) of delay tolerable (DT) services by allocating resources based on the time-domain water-filling algorithm. We first transform the non-convex problem of maximizing EE into a convex problem of minimizing transmission power to obtain the optimal solution, and then use the greedy algorithm to obtain an upper bound. Furthermore, to capture a more realistic scenario, an Approximate Statistical Dynamic Programming (ASDP) algorithm is introduced, but its effect on enhancing EE is limited. To overcome this limitation, three Deep Reinforcement Learning (DRL) algorithms are implemented. The simulations results show that the settings of maximum transmit power and SNR during agent training have an impact on the performance of the agent. Finally, by comparing the mean values of transmission power, outage probability, equilibrium power and performance improvement percentage of several algorithms, we conclude that the Deep Deterministic Policy Gradient (DDPG) algorithm produces the best agent performance in the environment with a fixed SNR of 2 (dB).}
}
@article{MARTINEZ2017295,
title = {Relational reinforcement learning with guided demonstrations},
journal = {Artificial Intelligence},
volume = {247},
pages = {295-312},
year = {2017},
note = {Special Issue on AI and Robotics},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000284},
author = {David Martínez and Guillem Alenyà and Carme Torras},
keywords = {Active learning, Learning guidance, Planning excuse, Reinforcement learning, Robot learning, Teacher demonstration, Teacher guidance},
abstract = {Model-based reinforcement learning is a powerful paradigm for learning tasks in robotics. However, in-depth exploration is usually required and the actions have to be known in advance. Thus, we propose a novel algorithm that integrates the option of requesting teacher demonstrations to learn new domains with fewer action executions and no previous knowledge. Demonstrations allow new actions to be learned and they greatly reduce the amount of exploration required, but they are only requested when they are expected to yield a significant improvement because the teacher's time is considered to be more valuable than the robot's time. Moreover, selecting the appropriate action to demonstrate is not an easy task, and thus some guidance is provided to the teacher. The rule-based model is analyzed to determine the parts of the state that may be incomplete, and to provide the teacher with a set of possible problems for which a demonstration is needed. Rule analysis is also used to find better alternative models and to complete subgoals before requesting help, thereby minimizing the number of requested demonstrations. These improvements were demonstrated in a set of experiments, which included domains from the international planning competition and a robotic task. Adding teacher demonstrations and rule analysis reduced the amount of exploration required by up to 60% in some domains, and improved the success ratio by 35% in other domains.}
}
@article{KIM2021107465,
title = {Model-based reinforcement learning and predictive control for two-stage optimal control of fed-batch bioreactor},
journal = {Computers & Chemical Engineering},
volume = {154},
pages = {107465},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107465},
url = {https://www.sciencedirect.com/science/article/pii/S009813542100243X},
author = {Jong Woo Kim and Byung Jun Park and Tae Hoon Oh and Jong Min Lee},
keywords = {Fed-batch bioreactor, Dynamic optimization, Reinforcement learning, Model predictive control},
abstract = {In this study, we propose a two-stage optimal control framework for a fed-batch bioreactor. The high-level controller aims to obtain the optimal feed trajectory that maximizes the final time productivity and yield using a nominal model. By contrast, the low-level controller maintains the high-level performance in the presence of the model-plant mismatch and real-time disturbances. This two-stage decomposition can perform the closed-loop operation with less online recomputation. To solve the high-level optimization, differential dynamic programming (DDP), a model-based reinforcement learning that employs the derivatives of the model is applied. Three types of low-level controllers are proposed: DDP controller, a model predictive control (MPC) that tracks the high-level trajectory, and an economic MPC. We first validate that DDP yields as good result as the direct method. Second, we compare the three low-level controllers and verify the necessity of the two-stage decomposition through the studies on a bioreactor.}
}
@article{HU2023104383,
title = {Iterative reward shaping for non-overshooting altitude control of a wing-in-ground craft based on deep reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {163},
pages = {104383},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104383},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000222},
author = {Huan Hu and Guiyong Zhang and Lichao Ding and Kuikui Jiao and Zhifan Zhang and Ji Zhang},
keywords = {Wing-in-ground craft, Non-overshoot, Deep reinforcement learning, Reward shaping, Path following},
abstract = {When a wing-in-ground craft (WIG) adjusts its flying altitude, overshooting behavior may occur, which weakens the safety and stealth ability. In previous studies on path following, cross-track error was used in company with other indicators to indirectly suppress overshoot. This paper proposes a method for direct and gradual suppression of the overshoot via deep reinforcement learning (DRL), which iterates the reward function by introducing a partial one based on the current overshoot magnitude. Each time the overshoot is obtained by DRL, a function about this overshoot is added to the reward function for retraining. The function is defined as a type of cross-track error within a range of the current overshoot magnitude to the target altitude, and it counts the partial reward before the WIG gets the worse overshoot during training. The methodological feasibility is proved by mathematical reasoning, and an example of a virtual WIG changing the altitude is taken to validate the method. Assuming that the added partial function is in a basic 1-order fractional form of cross-track error and multiplied by a factor, the implementation of iterative reward shaping decreases overshoot to a minimal level, with the overshoot down to over 99.8% when compared to the initial one. Moreover, when introducing the partial reward function in the first iteration, influence of the factor on overshoot is analyzed. For a WIG’s adjustment of altitude, the method can monotonically reduce overshoot within tolerance.}
}
@article{ALIBEKOV2016285,
title = {Policy Derivation Methods for Critic-Only Reinforcement Learning in Continuous Action Spaces},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {5},
pages = {285-290},
year = {2016},
note = {4th IFAC Conference on Intelligent Control and Automation SciencesICONS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.07.127},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316303305},
author = {Eduard Alibekov and Jiri Kubalik and Robert Babuska},
keywords = {reinforcement learning, continuous actions, multi-variable systems, optimal control, policy derivation},
abstract = {State-of-the-art critic-only reinforcement learning methods can deal with a small discrete action space. The most common approach to real-world problems with continuous actions is to discretize the action space. In this paper a method is proposed to derive a continuous-action policy based on a value function that has been computed for discrete actions by using any known algorithm such as value iteration. Several variants of the policy-derivation algorithm are introduced and compared on two continuous state-action benchmarks: double pendulum swing-up and 3D mountain car.}
}
@article{TSIANIKAS2021116778,
title = {A storage expansion planning framework using reinforcement learning and simulation-based optimization},
journal = {Applied Energy},
volume = {290},
pages = {116778},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116778},
url = {https://www.sciencedirect.com/science/article/pii/S030626192100283X},
author = {Stamatis Tsianikas and Nooshin Yousefi and Jian Zhou and Mark D. Rodgers and David Coit},
keywords = {Investment planning, Decision-making, Reinforcement learning, Microgrids},
abstract = {In the wake of the highly electrified future ahead of us, the role of energy storage is crucial wherever distributed generation is abundant, such as in microgrid settings. Given the variety of storage options that are becoming more and more economical, determining which type of storage technology to invest in, along with the appropriate timing and capacity becomes a critical research question. It is inevitable that these problems will continue to become increasingly relevant in the future and require strategic planning and holistic and modern frameworks in order to be solved. Reinforcement Learning algorithms have already proven to be successful in problems where sequential decision-making is inherent. In the operations planning area, these algorithms are already used but mostly in short-term problems with well-defined constraints. On the contrary, we expand and tailor these techniques to long-term planning by utilizing model-free algorithms combined with simulation-based models. A model and expansion plan have been developed to optimally determine microgrid designs as they evolve to dynamically react to changing conditions and to exploit energy storage capabilities. We show that it is possible to derive better engineering solutions that would point to the types of energy storage units which could be at the core of future microgrid applications. Another key finding is that the optimal storage capacity threshold for a system depends heavily on the price movements of the available storage units. By utilizing the proposed approaches, it is possible to model inherent problem uncertainties and optimize the whole streamline of sequential investment decision-making.}
}
@article{KIM2018257,
title = {Deep reinforcement learning based finite-horizon optimal tracking control for nonlinear system},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {25},
pages = {257-262},
year = {2018},
note = {9th IFAC Symposium on Robust Control Design ROCOND 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.115},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318327769},
author = {Jong Woo Kim and Byung Jun Park and Haeun Yoo and Jay H. Lee and Jong Min Lee},
keywords = {Reinforcement learning, Approximate dynamic programming, Deep learning, Globalized dual heuristic programming, Optimal control, Optimal tracking},
abstract = {Reinforcement learning (RL) can be used to obtain an approximate numerical solution to the Hamilton-Jacobi-Bellman (HJB) equation. Recent advances in machine learning community enable the use of deep neural networks (DNNs) to approximate high-dimensional nonlinear functions as those that occur in RL, accurately without any domain knowledge. In the standard RL setting, both system and cost structures are unknown, and the amount of data needed to obtain an accurate approximation can be impractically large. Meanwhile, when the structures are known, they can be used to solve the HJB equation efficiently. Herein, the model-based globalized dual heuristic programming (GDHP) is proposed, in which the HJB equation is separated into value, costate, and policy functions. A particular class of interest in this research is finite horizon optimal tracking control (FHOC) problem. Additional issues that arise, such as time-varying functions, terminal constraints, and delta-input formulation, are addressed in the context of FHOC. The DNN structure and training algorithm suitable for FHOC are presented. A benchmark continuous reactor example is provided to illustrate the proposed approach.}
}
@article{KAZMI2018159,
title = {Gigawatt-hour scale savings on a budget of zero: Deep reinforcement learning based optimal control of hot water systems},
journal = {Energy},
volume = {144},
pages = {159-168},
year = {2018},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2017.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S0360544217320388},
author = {Hussain Kazmi and Fahad Mehmood and Stefan Lodeweyckx and Johan Driesen},
keywords = {Deep reinforcement learning, Domestic hot water, Optimal control, Energy efficiency, Smart grid},
abstract = {Energy consumption for hot water production is a major draw in high efficiency buildings. Optimizing this has typically been approached from a thermodynamics perspective, decoupled from occupant influence. Furthermore, optimization usually presupposes existence of a detailed dynamics model for the hot water system. These assumptions lead to suboptimal energy efficiency in the real world. In this paper, we present a novel reinforcement learning based methodology which optimizes hot water production. The proposed methodology is completely generalizable, and does not require an offline step or human domain knowledge to build a model for the hot water vessel or the heating element. Occupant preferences too are learnt on the fly. The proposed system is applied to a set of 32 houses in the Netherlands where it reduces energy consumption for hot water production by roughly 20% with no loss of occupant comfort. Extrapolating, this translates to absolute savings of roughly 200 kWh for a single household on an annual basis. This performance can be replicated to any domestic hot water system and optimization objective, given that the fairly minimal requirements on sensor data are met. With millions of hot water systems operational worldwide, the proposed framework has the potential to reduce energy consumption in existing and new systems on a multi Gigawatt-hour scale in the years to come.}
}
@article{HUANG2023103697,
title = {Collective reinforcement learning based resource allocation for digital twin service in 6G networks},
journal = {Journal of Network and Computer Applications},
volume = {217},
pages = {103697},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103697},
url = {https://www.sciencedirect.com/science/article/pii/S1084804523001169},
author = {Zhongwei Huang and Dagang Li and Jun Cai and Hua Lu},
keywords = {Digital twin, Resource allocation, Internet of Things, Collective reinforcement learning},
abstract = {The 6th generation (6G) mobile communications technology will realize the interconnection of humans, machines, things as well as virtual space. The development of digital twins (DTs) and 6G has accelerated the Internet of Things (IoT) in an unprecedented way. The combination of DTs and edge intelligence (EI) enables powerful digital space synchronized with the real world constructed in the intelligent edge, bringing real-time, and adaptive services delivery of IoT. However, the dynamic features and heterogeneous resources in 6G-enabled IoT make the resource allocation for computation-intensive and delay-sensitive DTs services more challenging. In this paper, we first define the DTs implementation process as a DT service function chain (DTSFC) and address the resource allocation problem of DTs-empowered networks in form of dynamic DTSFCs orchestration. We further propose a novel collective reinforcement learning (CRL) method which is inspired by human collaboration, to realize the effective resource allocation of DTSFCs. Numerical results verify that the proposed CRL algorithm improves the learning efficiency and generalization ability compared with the benchmarks.}
}
@article{KOPF20201555,
title = {Deep Decentralized Reinforcement Learning for Cooperative Control},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {1555-1562},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2181},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320328342},
author = {Florian Köpf and Samuel Tesfazgi and Michael Flad and Sören Hohmann},
keywords = {Reinforcement Learning, Deep Learning, Learning Control, Shared Control, Decentralized Control, Machine Learning, Non-stationary Systems, Nonlinear Control},
abstract = {In order to collaborate efficiently with unknown partners in cooperative control settings, adaptation of the partners based on online experience is required. The rather general and widely applicable control setting, where each cooperation partner might strive for individual goals while the control laws and objectives of the partners are unknown, entails various challenges such as the non-stationarity of the environment, the multi-agent credit assignment problem, the alter-exploration problem and the coordination problem. We propose new, modular deep decentralized Multi-Agent Reinforcement Learning mechanisms to account for these challenges. Therefore, our method uses a time-dependent prioritization of samples, incorporates a model of the system dynamics and utilizes variable, accountability-driven learning rates and simulated, artificial experiences in order to guide the learning process. The effectiveness of our method is demonstrated by means of a simulated, nonlinear cooperative control task.}
}
@article{XU2022108030,
title = {Real-time fast charging station recommendation for electric vehicles in coupled power-transportation networks: A graph reinforcement learning method},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {141},
pages = {108030},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2022.108030},
url = {https://www.sciencedirect.com/science/article/pii/S0142061522000746},
author = {Peidong Xu and Jun Zhang and Tianlu Gao and Siyuan Chen and Xiaohui Wang and Huaiguang Jiang and Wenzhong Gao},
keywords = {Coupled power-transportation network, DQN(λ), Electric vehicle, Fast charging station recommendation, Graph attention networks},
abstract = {With the increasing penetration rate of electric vehicles, the fast charging demands of electric vehicles will have a significant influence on the operation of coupled power-transportation networks. To promote the interests of the coupled system, fast charging stations, and electric vehicle users, in this paper, a multi-objective system-level fast charging station recommendation method is proposed to dynamically allocate electric vehicles to suitable stations. The recommendation problem is formulated as a sequential decision-making problem and a deep reinforcement learning method is adopted. To deal with the network-structure coupled system states, graph attention networks are introduced. Considering the heterogeneity between entities, we propose a physical connection-based graph formulation method with feature projection to integrate multi-dimensional information from charging stations, traffic nodes, and power grid buses into a graph. The graph convolution of coupled system states can then be realized to promote environment perception. Besides, to address the long time-delay action execution in recommendation problem, a double-prioritized DQN(λ) training mechanism is developed to update the guidance strategy, where an attention-prioritized cache construction method is proposed to improve the training efficiency cooperated with prioritized experience replay. The proposed graph reinforcement learning method is trained and evaluated in a joint power-transportation simulation platform. Simulation results show that the proposed strategy can promote the interest of multiple facets in coupled power-transportation networks by handling the requests in a real-time manner. Its feasibility and robustness in the urban transportation systems are also demonstrated.}
}
@article{SHI2011105,
title = {Artificial emotion model based on reinforcement learning mechanism of neural network},
journal = {The Journal of China Universities of Posts and Telecommunications},
volume = {18},
number = {3},
pages = {105-109},
year = {2011},
issn = {1005-8885},
doi = {https://doi.org/10.1016/S1005-8885(10)60071-4},
url = {https://www.sciencedirect.com/science/article/pii/S1005888510600714},
author = {Xue-fei SHI and Zhi-liang WANG and An PING and Li-kun ZHANG},
keywords = {artificial emotion model, reinforcement learning, hierarchical structure, neural network},
abstract = {A hierarchical-processed frame construction of artificial emotion model for intelligent system is proposed in the paper according to the basic conclusion of emotional psychology. The general method of emotion processing, which considers only one single layer, has been changed in the presented construction. An artificial emotional development model is put forward based on reinforcement learning mechanism of neural network. The new model takes the emotion itself as reinforcement signal and describes its different influences on action learning efficiency corresponding to different individualities. In the end, simulation result based on child playmate robot is discussed and the effectiveness of the model is verified.}
}
@article{OLIVEIRA2021114228,
title = {Q-Managed: A new algorithm for a multiobjective reinforcement learning},
journal = {Expert Systems with Applications},
volume = {168},
pages = {114228},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114228},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420309490},
author = {Thiago Henrique Freire de Oliveira and Luiz Paulo de Souza Medeiros and Adrião Duarte Dória Neto and Jorge Dantas Melo},
keywords = {Multiobjective reinforcement learning, ε-constraint, Q-Learning, Pareto dominance, Single-policy approach, Hypervolume},
abstract = {Multi-objective reinforcement learning (MORL) involves the use of reinforcement learning techniques to address problems with multiple objectives, conflicting or not. Among the main techniques used to treat this class of problems, we saw that they are limited by some factors, such as the Pareto Front shape and computational cost. This paper proposes a new iterative algorithm based on the single-policy approach, called Q-Managed. We use a hybrid multi-objective optimization (MOO) method that provides the mathematical guarantee that all policies belonging to the Pareto Front can be found, regardless of whether it is concave, convex or a mixture of both. Another important aspect that is worth mentioning is that its simplicity and performance are from a single-policy algorithms. To validate our proposal, we use the traditional MORL benchmarks and with different configurations of the Pareto Front. The Q-Managed shows success in finding all the optimal policies in all environments, surpassing all the single-policy algorithms in the literature in terms of policy quality. Based on the used benchmarks, its effectiveness can also be equated to the best multi-policy algorithms. The hypervolume metric was used to compare the quality of the policies found by our algorithm with those found in the state of the art. Extensions for non-episodic environments and stochastic transition functions are also introduced.}
}
@article{CORACI2023117303,
title = {Effective pre-training of a deep reinforcement learning agent by means of long short-term memory models for thermal energy management in buildings},
journal = {Energy Conversion and Management},
volume = {291},
pages = {117303},
year = {2023},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2023.117303},
url = {https://www.sciencedirect.com/science/article/pii/S0196890423006490},
author = {Davide Coraci and Silvio Brandi and Alfonso Capozzoli},
keywords = {Deep reinforcement learning, LSTM neural network, Data-driven building modelling, Real-time deployment, Heating system, Energy management},
abstract = {Recently, deep reinforcement learning has emerged as a popular approach for enhancing thermal energy management in buildings due to its flexibility and model-free nature. However, the time-consuming convergence of deep reinforcement learning poses a challenge. To address this, offline pre-training of deep reinforcement learning controllers using physics-based simulation environments has been commonly employed. However, developing these models requires significant effort and expertise. Alternatively, data-driven models offer a promising solution by emulating building dynamics, but they struggle to predict previously unseen patterns. Therefore, this paper introduces a strategy to effectively train and deploy a deep reinforcement learning controller by means of long short-term memory neural networks. The experiments were carried out using an EnergyPlus simulation environment as a proxy of a real building. An automatic and recursive procedure is designed to determine the minimum amount of historical data required to train a robust data-driven model which mimics building dynamics. The trained deep reinforcement learning agent meets safety requirements in the simulation environment after two and a half months of training. Additionally, it reduces indoor temperature violations by 80% while consuming the same amount of energy as a baseline rule-based controller.}
}
@article{HE2023104089,
title = {Autonomous anomaly detection on traffic flow time series with reinforcement learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {150},
pages = {104089},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2023.104089},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X23000785},
author = {Dan He and Jiwon Kim and Hua Shi and Boyu Ruan},
keywords = {Anomaly detection, Traffic flow data, Reinforcement learning, Deep learning, Kernel density estimation},
abstract = {This study develops an autonomous artificial intelligence (AI) agent to detect anomalies in traffic flow time series data, which can learn anomaly patterns from data without supervision, requiring no ground-truth labels for model training or knowledge of a threshold for anomaly definition. Specifically, our model is based on reinforcement learning, where an agent is built by a Long-Short-Term-Memory (LSTM) model and Q-learning algorithm to incorporate sequential information in time series data into policy optimization. The key contribution of our model is the development of a novel unsupervised reward learning algorithm that automatically learns the reward for an action taken by the agent based on the distribution of data, without requiring a manual specification of a reward function. To test the performance of our model, we conduct a comprehensive set of experimental study on both real-world data from Brisbane city, Australia, and synthetic data simulated according to the distribution of real-world data. We compare the performance of our model against three state-of-the-art models, and the experimental results show that our model outperforms the other models in different parameter settings, with around 90% precision, 80% recall, and 85% F1 score.}
}
@article{YANG2023735,
title = {Energy scheduling for DoS attack over multi-hop networks: Deep reinforcement learning approach},
journal = {Neural Networks},
volume = {161},
pages = {735-745},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.028},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000916},
author = {Lixin Yang and Jie Tao and Yong-Hua Liu and Yong Xu and Chun-Yi Su},
keywords = {Multi-hop networks, DoS attack, Kalman filtering, Markov decision process, Dueling double Q-network},
abstract = {This paper studies the energy scheduling for Denial-of-Service (DoS) attack against remote state estimation over multi-hop networks. A smart sensor observes a dynamic system, and transmits its local state estimate to a remote estimator. Due to the limited communication range of the sensor, some relay nodes are employed to deliver data packets from the sensor to the remote estimator, which constitutes a multi-hop network. To maximize the estimation error covariance with energy constraint, a DoS attacker needs to determine the energy level implemented on each channel. This problem is formulated as an associated Markov decision process (MDP), and the existence of an optimal deterministic and stationary policy (DSP) is proved for the attacker. Besides, a simple threshold structure of the optimal policy is obtained, which significantly reduces the computational complexity. Furthermore, an up-to-date deep reinforcement learning (DRL) algorithm, dueling double Q-network (D3QN), is introduced to approximate the optimal policy. Finally, a simulation example illustrates the developed results and verifies the effectiveness of D3QN for optimal DoS attack energy scheduling.}
}
@article{AHMED2018233,
title = {Comparison of Model Predictive and Reinforcement Learning Methods for Fault Tolerant Control},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {24},
pages = {233-240},
year = {2018},
note = {10th IFAC Symposium on Fault Detection, Supervision and Safety for Technical Processes SAFEPROCESS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.583},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318322924},
author = {Ibrahim Ahmed and Hamed Khorasgani and Gautam Biswas},
keywords = {Reinforcement learning control, model predictive control, fault tolerance, model-based control, hierarchical reinforcement learning},
abstract = {A desirable property in fault-tolerant controllers is adaptability to system changes as they evolve during systems operations. An adaptive controller does not require optimal control policies to be enumerated for possible faults. Instead it can approximate one in real-time. We present two adaptive fault-tolerant control schemes for a discrete time system based on hierarchical reinforcement learning. We compare their performance against a model predictive controller in presence of sensor noise and persistent faults. The controllers are tested on a fuel tank model of a C-130 plane. Our experiments demonstrate that reinforcement learning-based controllers perform more robustly than model predictive controllers under faults, partially observable system models, and varying sensor noise levels.}
}
@article{TREESATAYAPUN2023101381,
title = {Fault-tolerant control based on reinforcement learning and sliding event-triggered mechanism for a class of unknown discrete-time systems},
journal = {Nonlinear Analysis: Hybrid Systems},
volume = {50},
pages = {101381},
year = {2023},
issn = {1751-570X},
doi = {https://doi.org/10.1016/j.nahs.2023.101381},
url = {https://www.sciencedirect.com/science/article/pii/S1751570X23000523},
author = {C. Treesatayapun},
keywords = {Reinforcement learning, Fault tolerant control, Sliding event-trigger, Discrete-time systems, Fuzzy rules emulated networks},
abstract = {This paper presents an adaptive fault-tolerant control (FTC) system based on reinforcement learning using an even-triggered mechanism. The even-triggered mechanism is established through a justifiable sliding surface and triggered function, without the need for any fault detection or observer. The learning laws are derived to ensure the convergence of internal signals and tracking error, and an actor–critic architecture is designed accordingly. To validate the proposed scheme, an experimental system is constructed and tested using five typical actuator faults. The results indicate a positive closed-loop performance and a reduction of approximately 25% in data transmission for both cases with and without faults.}
}
@article{TIWARI20211,
title = {DAPath: Distance-aware knowledge graph reasoning based on deep reinforcement learning},
journal = {Neural Networks},
volume = {135},
pages = {1-12},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S089360802030410X},
author = {Prayag Tiwari and Hongyin Zhu and Hari Mohan Pandey},
keywords = {Knowledge graph reasoning, Reinforcement learning, Graph self-attention, GRU},
abstract = {Knowledge graph reasoning aims to find reasoning paths for relations over incomplete knowledge graphs (KG). Prior works may not take into account that the rewards for each position (vertex in the graph) may be different. We propose the distance-aware reward in the reinforcement learning framework to assign different rewards for different positions. We observe that KG embeddings are learned from independent triples and therefore cannot fully cover the information described in the local neighborhood. To this effect, we integrate a graph self-attention (GSA) mechanism to capture more comprehensive entity information from the neighboring entities and relations. To let the model remember the path, we incorporate the GSA mechanism with GRU to consider the memory of relations in the path. Our approach can train the agent in one-pass, thus eliminating the pre-training or fine-tuning process, which significantly reduces the problem complexity. Experimental results demonstrate the effectiveness of our method. We found that our model can mine more balanced paths for each relation.}
}
@article{ZAMFIRACHE2022162,
title = {Policy Iteration Reinforcement Learning-based control using a Grey Wolf Optimizer algorithm},
journal = {Information Sciences},
volume = {585},
pages = {162-175},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.11.051},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521011737},
author = {Iuliu Alexandru Zamfirache and Radu-Emil Precup and Raul-Cristian Roman and Emil M. Petriu},
keywords = {Grey Wolf Optimizer, NN training, Optimal reference tracking control, Policy iteration, Reinforcement learning, Servo systems},
abstract = {This paper presents a new Reinforcement Learning (RL)-based control approach that uses the Policy Iteration (PI) and a metaheuristic Grey Wolf Optimizer (GWO) algorithm to train the Neural Networks (NNs). Due to an efficient tradeoff to exploration and exploitation, the GWO algorithm shows good results in NN training and solving complex optimization problems. The proposed approach is compared to the classical PI RL-based control approach using the Gradient Descent (GD) algorithm, and with the RL-based control approach which uses the metaheuristic Particle Swarm Optimization (PSO) algorithm. The experiments are conducted using a nonlinear servo system laboratory equipment. Each approach evaluated on how well it solves the optimal reference tracking control for an experimental servo system position control system. The policy NNs specific to all three approaches are implemented as state feedback with integrator controllers to remove the steady-state control errors and thus ensure the convergence of the objective function. Because of the random nature of metaheuristic algorithms, the experiments for GWO and PSO algorithms are run multiple times and the results are averaged before the conclusions are presented. The experimental results shows that for the control objective considered in this paper, the GWO algorithm represents a better solution compared to GD and PSO algorithms.}
}
@article{LI2023110034,
title = {A path selection scheme for detecting malicious behavior based on deep reinforcement learning in SDN/NFV-Enabled network},
journal = {Computer Networks},
volume = {236},
pages = {110034},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110034},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623004796},
author = {Man Li and Shuangxing Deng and Huachun Zhou and Yajuan Qin},
keywords = {Software defined network (SDN), Network function virtualization (NFV), Deep reinforcement learning (DRL), Service function chain (SFC)},
abstract = {The SDN/NFV network is prone to different types of attacks. The Distributed Denial of Service (DDoS) attack has the most severe impact as it can overwhelm the critical components of SDN/NFV to degrade its performance. We propose a closed-loop security architecture (SFCSA) and virtualize detection methods as network service functions in this article. Combining the detection methods forms detection paths, in which different detection paths affect security performance differently. Further, we model the path selection problem as a Markov Decision Process, where the reward balances the malicious traffic detection capability and end-to-end latency. Then, an integrated deep reinforcement learning and convolution neural network path selection algorithm (CNNQ) is proposed. Furthermore, we define a total path malicious traffic detection capability metric. The defined metrics and common metrics are applied to evaluate the building prototype, with the corresponding experimental results demonstrating that the detection performance when combining multiple detection modules outperforms a single detection-based module. Besides, we verify the effectiveness of the CNNQ method under various DDoS attacks scenarios and present the fine-grained classification results of the selected detection modules.}
}
@article{HAN2019101748,
title = {A review of reinforcement learning methodologies for controlling occupant comfort in buildings},
journal = {Sustainable Cities and Society},
volume = {51},
pages = {101748},
year = {2019},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.101748},
url = {https://www.sciencedirect.com/science/article/pii/S2210670719307589},
author = {Mengjie Han and Ross May and Xingxing Zhang and Xinru Wang and Song Pan and Da Yan and Yuan Jin and Liguo Xu},
keywords = {Reinforcement learning, Control, Building, Indoor comfort, Occupant},
abstract = {Classical building control systems are becoming vulnerable with increasing complexities in contemporary built environments and energy systems. Due to this, the reinforcement learning (RL) method is becoming more distinctive and applicable in control networks for buildings. This paper, therefore, conducts a comprehensive review of RL techniques applied in control systems for occupant comfort in indoor built environments. The empirical applications of RL-based control systems are presented, depending on comfort objectives (thermal comfort, indoor air quality, and lighting) along with other objectives which invariably includes energy consumption. The class of RL algorithms and implementation details regarding how the value functions have been represented and how the policies are improved are also illustrated. This paper shows there are limited works for which RL has been explored for controlling occupant comfort, especially in indoor air quality and lighting. Relatively few of the reviewed works incorporate occupancy patterns and/or occupant feedback into the control loop. Moreover, this paper identifies a gap with regard to the performance of implementing cooperative multi-agent RL (MARL). Based on our findings, current challenges and further opportunities are discussed. We expect to clarify the feasible theory and functions of RL for building control systems, which would promote their wider-spread application in built environments.}
}
@article{GLAVIC20176918,
title = {Reinforcement Learning for Electric Power System Decision and Control: Past Considerations and Perspectives},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {6918-6927},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.1217},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317317238},
author = {Mevludin Glavic and Raphaël Fonteneau and Damien Ernst},
keywords = {Electric power system, reinforcement learning, control, decision},
abstract = {In this paper, we review past (including very recent) research considerations in using reinforcement learning (RL) to solve electric power system decision and control problems. The RL considerations are reviewed in terms of specific electric power system problems, type of control and RL method used. We also provide observations about past considerations based on a comprehensive review of available publications. The review reveals the RL is considered as viable solutions to many decision and control problems across different time scales and electric power system states. Furthermore, we analyse the perspectives of RL approaches in light of the emergence of new-generation, communications, and instrumentation technologies currently in use, or available for future use, in power systems. The perspectives are also analysed in terms of recent breakthroughs in RL algorithms (Safe RL, Deep RL and path integral control for RL) and other, not previously considered, problems for RL considerations (most notably restorative, emergency controls together with so-called system integrity protection schemes, fusion with existing robust controls, and combining preventive and emergency control).}
}
@article{YANG2019158,
title = {Event-trigger-based robust control for nonlinear constrained-input systems using reinforcement learning method},
journal = {Neurocomputing},
volume = {340},
pages = {158-170},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.02.034},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219302486},
author = {Dongsheng Yang and Ting Li and Huaguang Zhang and Xiangpeng Xie},
keywords = {Event-triggered control, Robust  control, Hamilton–Jacobi–Isaacs (HJI) equation, Neural networks, Input constrains},
abstract = {In this paper, an online integral reinforcement learning strategy is proposed to deal with robust constrained control problems using event-triggered mechanism for nonlinear Continuous-Time (C-T) systems with external disturbances. The novel design of constrained control law is addressed together with the adaptive event-triggered condition by guaranteeing the optimal performance and system stability. An adaptive online actor-critic Neural Network (NN) reinforcement learning scheme is developed to approximate the optimal solution of the complicated Hamilton–Jacobi–Isaacs equation. Meanwhile, the convergence of NN weight errors and the event-triggered closed-loop system stability are demonstrated to be uniform ultimate bounded by Lyapunov analysis under the proposed triggering condition. Moreover, event-triggered H∞ tracking control with input constrains and limited network communication is also presented by establishing an augmented system. Finally, simulation results are provided to show the algorithm validity.}
}
@article{BACHRACH2020103356,
title = {Negotiating team formation using deep reinforcement learning},
journal = {Artificial Intelligence},
volume = {288},
pages = {103356},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2020.103356},
url = {https://www.sciencedirect.com/science/article/pii/S0004370220301077},
author = {Yoram Bachrach and Richard Everett and Edward Hughes and Angeliki Lazaridou and Joel Z. Leibo and Marc Lanctot and Michael Johanson and Wojciech M. Czarnecki and Thore Graepel},
keywords = {Multi-agent systems, Team formation, Coalition formation, Reinforcement learning, Deep learning, Cooperative games, Shapley value},
abstract = {When autonomous agents interact in the same environment, they must often cooperate to achieve their goals. One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it. However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement. Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols. More general methods usually require human input or domain-specific data, and so do not scale. To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning. Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven. We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory. Additionally, we investigate how the physical location of agents influences negotiation outcomes.}
}
@article{BERNASCONI2023101974,
title = {Dealer markets: A reinforcement learning mean field game approach},
journal = {The North American Journal of Economics and Finance},
volume = {68},
pages = {101974},
year = {2023},
issn = {1062-9408},
doi = {https://doi.org/10.1016/j.najef.2023.101974},
url = {https://www.sciencedirect.com/science/article/pii/S1062940823000979},
author = {Martino Bernasconi and E. Vittori and F. Trovò and M. Restelli},
keywords = {Dealer markets, Market making, Mean field games, Game theory, Reinforcement learning},
abstract = {We study the problem of finding an equilibrium strategy in an Over The Counter (OTC) market populated by multiple strategic dealers quoting the bid–ask prices. The need for an equilibrium strategy comes from the assumption that each dealer adapts their behavior by learning the optimal quoting strategy. Hence, we model the market as a game between many agents competing for resources. Based on this framework, we propose an efficient numerical procedure using Reinforcement Learning and Mean Field Games which learns an approximate equilibrium. Through an experimental campaign, we validate the proposed method in a realistic market-making scenario against strategic dealers trained to exploit our weaknesses and evaluate their performance against non-strategic agents.}
}
@article{ZHAO2023119707,
title = {Asset correlation based deep reinforcement learning for the portfolio selection},
journal = {Expert Systems with Applications},
volume = {221},
pages = {119707},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119707},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423002087},
author = {Tianlong Zhao and Xiang Ma and Xuemei Li and Caiming Zhang},
keywords = {Portfolio selection, Deep reinforcement learning, Asset correlations},
abstract = {Portfolio selection is an important application of AI in the financial field, which has attracted considerable attention from academia and industry alike. One of the great challenges in this application is modeling the correlation among assets in the portfolio. However, current studies cannot deal well with this challenge because it is difficult to analyze complex nonlinearity in the correlation. This paper proposes a policy network that models the nonlinear correlation by utilizing the self-attention mechanism to better tackle this issue. In addition, a deterministic policy gradient recurrent reinforcement learning method based on Monte Carlo sampling is constructed with the objective function of cumulative return to train the policy network. In most existing reinforcement learning-based studies, the state transition probability is generally regarded as unknown, so the value function of the policy can only be estimated. Based on financial backtest experiments, we analyze that the state transition probability is known in the portfolio, and value function can be directly obtained by sampling, further theoretically proving the optimality of the proposed reinforcement learning method in the portfolio. Finally, the superiority and generality of our approach are demonstrated through comprehensive experiments on the cryptocurrency dataset, S&P 500 stock dataset, and ETF dataset.}
}
@article{YAO2022317,
title = {Graph and dynamics interpretation in robotic reinforcement learning task},
journal = {Information Sciences},
volume = {611},
pages = {317-334},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.08.041},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522009276},
author = {Zonggui Yao and Jun Yu and Jian Zhang and Wei He},
keywords = {Graph neural networks (GNNs), Dynamics estimations, Robotic controls, Robotic force transmission, Trajectory following, Reinforcement learning},
abstract = {Robot control tasks are typically solved by reinforcement learning approaches in a circular way of trial and learn. A recent trend of the research on robotic reinforcement learning is the employment of the deep learning methods. Existing deep learning methods achieve the control by training the approximation models of the dynamic function, value function or the policy function in the control algorithms. However, these methods usually handle the modeling from a statistical perspective without considering the physics characteristics of the robot’s motion. One of the typical problems is the force transmission through different parts of a robot and the calculations of the robotic dynamics quantities are prone to be ignored. To this end, we propose to use the force transmission graph to interpret the force transmission mechanism obeyed by the motion of the robot and estimate the dynamics quantities of the robot’s motion with a quadratic model. Following this concern, we propose a model-based reinforcement learning framework for robotic control in which the dynamic model comprises two components, i.e. the Graph Convolution Network (GCN) and the Two-Layer Perception (TLP) network. The GCN serves as a parameter estimator of the force transmission graph and a structural feature extractor. The TLP network approximates the quadratic model that should be able to estimate the dynamics quantities of the robot’s motion. For this reason, the proposed framework is named as GCN of Dynamics estimation in Reinforcement Learning method (GDRL for short). The deployed method interprets the intrinsic mechanism of robotic force transmissions through robot limbs, therefore the model is highly interpretable. Experimental results show that GDRL can predict the gesture and location of the robot for the next move well such that the performance of our method surpasses that of the previous methods in robot control task in our task setting of multiple types of robots. We also designed to compare with the previous model-free methods in our task setting, and the results are outstanding, which is owing to the interpretation of the physical characteristics.}
}
@article{TREESATAYAPUN2023541,
title = {Discrete-time robust event-triggered actuator fault-tolerant control based on adaptive networks and reinforcement learning},
journal = {Neural Networks},
volume = {166},
pages = {541-554},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023004227},
author = {C. Treesatayapun},
keywords = {Reinforcement learning, Fault tolerant control, Active and passive actuator faults, Discrete-time systems, Fuzzy rules emulated networks},
abstract = {This paper focuses on the topic of fault-tolerant control for discrete-time systems with nonlinear uncertainties and actuator faults. It considers both passive and active faults as part of the analysis and design. The proposed adaptive controller, based on a nonlinear electronic circuit, handles offset-biasing, sensitivity variation, and dead-zone effects. An event-triggered mechanism, utilizing a sliding surface, enhances robustness and reduces data transmission. Adaptive networks called MiFRENs are employed, trained using reinforcement learning. Theoretical analysis guarantees boundedness of internal signals and tracking error. Experimental results validate the scheme, demonstrating required conditions, reduced data transmission, and robust performance. Comparative evaluations confirm its superiority}
}
@article{VAMPLEW201726,
title = {Steering approaches to Pareto-optimal multiobjective reinforcement learning},
journal = {Neurocomputing},
volume = {263},
pages = {26-38},
year = {2017},
note = {Multiobjective Reinforcement Learning: Theory and Applications},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.08.152},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217311013},
author = {Peter Vamplew and Rustam Issabekov and Richard Dazeley and Cameron Foale and Adam Berry and Tim Moore and Douglas Creighton},
keywords = {Multiobjective reinforcement learning, Non-stationary policies, Geometric steering, Interactive reinforcement learning, Pareto optimality},
abstract = {For reinforcement learning tasks with multiple objectives, it may be advantageous to learn stochastic or non-stationary policies. This paper investigates two novel algorithms for learning non-stationary policies which produce Pareto-optimal behaviour (w-steering and Q-steering), by extending prior work based on the concept of geometric steering. Empirical results demonstrate that both new algorithms offer substantial performance improvements over stationary deterministic policies, while Q-steering significantly outperforms w-steering when the agent has no information about recurrent states within the environment. It is further demonstrated that Q-steering can be used interactively by providing a human decision-maker with a visualisation of the Pareto front and allowing them to adjust the agent’s target point during learning. To demonstrate broader applicability, the use of Q-steering in combination with function approximation is also illustrated on a task involving control of local battery storage for a residential solar power system.}
}
@article{BAYIZ20145393,
title = {Nonlinear Disturbance Compensation and Reference Tracking via Reinforcement Learning with Fuzzy Approximators},
journal = {IFAC Proceedings Volumes},
volume = {47},
number = {3},
pages = {5393-5398},
year = {2014},
note = {19th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20140824-6-ZA-1003.02511},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016424535},
author = {Y. Efe Bayiz and Robert Babuska},
keywords = {reinforcement learning, nonlinear disturbance compensation, fuzzy modeling, model-based learning control},
abstract = {Reinforcement Learning (RL) algorithms can learn optimal control laws for nonlinear dynamic systems without relying on a mathematical model of the system to be controlled. While RL can in principle discover control laws from scratch, by solely interacting with the process, in practice this does not yield any significant advantages. Learning control laws from scratch is lengthy and may lead to system damage due to the trial and error nature of the learning process. In this paper, we adopt a different and largely unexplored approach: a nominal control law is used to achieve reasonable, yet suboptimal, performance and a RL agent is trained to act as a nonlinear compensator whose task is to improve upon the performance of the nominal controller. The RL agent learns by means of an actor-critic algorithm using a plant model acquired on-line, alongside the critic and actor. Fuzzy approximators are employed to represent all the adjustable components of the learning scheme. One advantage of fuzzy approximators is the straightforward way in which they allow for the inclusion of prior knowledge. The proposed control scheme is applied to a reference tracking problem of 1-DOF robot arm influenced by an unknown payload disturbance due to gravity. The nominal controller is a PD controller, which is unable to properly compensate the effect of the disturbance considered. Simulation results indicate that the novel method is able to learn to compensate the disturbance for any reference angle varying throughout the experiment.}
}
@article{SAMI2024255,
title = {LearnChain: Transparent and cooperative reinforcement learning on Blockchain},
journal = {Future Generation Computer Systems},
volume = {150},
pages = {255-271},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003370},
author = {Hani Sami and Rabeb Mizouni and Hadi Otrok and Shakti Singh and Jamal Bentahar and Azzam Mourad},
keywords = {Cooperative artificial intelligence (AI), Blockchain, Ethereum, Quorum, Reinforcement learning, Transparency, Trust, Vehicular edge computing},
abstract = {We consider multi-agent reinforcement learning (MARL) with the popular paradigm of centralized training and decentralized execution (CTDE). CTDE empowers sharing knowledge from agents in different environments for updating a shared model. A wide range of applications is supported through CTDE in MARL, such as self-driving vehicle coordination, traffic lights synchronization, or cooperation in various aspects of the Internet of Things (IoT), including resource management. Despite the drawbacks of relying on a central authority for handling model updates, incorporating multiple sources of data raises concerns about the trustworthiness of the process. For instance, participating agents could provide data in the favor of their experiences to shift the model towards certain behaviors. Similarly, sending falsified data for updates could lead to adversarial attacks. To overcome these challenges, it is essential to integrate the Ethereum Blockchain technology to handle model updates in the CTDE paradigm by achieving decentralized storage and consensus mechanism for model updates. In the literature, there exist multiple efforts that propose using reinforcement learning (RL) on Blockchain; however, none of them have considered updating MARL of CTDE on-chain, allowing transparent and auditable record of the training process. Therefore, we propose LearnChain, a framework that offers an integration between the CTDE mechanism and a Consortium Blockchain built between authorized participants, thus avoiding gas costs. At the core of LearnChain, RL is integrated with Quorum, offering separate smart contracts for deployment, data handling with incentive mechanisms, training, target update, and inference. Based on a real use-case entailing management of Vehicular Edge Computing tasks through multi-agent synchronization, we implement LearnChain and evaluate its performance and cost in different settings. Our results show the ability to improve learning from shared experiences and to adapt to environment changes on the Quorum BlockChain.}
}
@article{HOU2024214,
title = {Energy efficient task scheduling based on deep reinforcement learning in cloud environment: A specialized review},
journal = {Future Generation Computer Systems},
volume = {151},
pages = {214-231},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003771},
author = {Huanhuan Hou and Siti Nuraishah {Agos Jawaddi} and Azlan Ismail},
keywords = {Task scheduling, Deep reinforcement learning, Energy efficient, Markov decision process, Cloud computing},
abstract = {The expanding scale of cloud data centers and the diversification of user services have led to an increase in energy consumption and greenhouse gas emissions, resulting in long-term detrimental effects on the environment. To address this issue, scheduling techniques that reduce energy usage have become a hot topic in cloud computing and cluster management. The Deep Reinforcement Learning (DRL) approach, which combines the advantages of Deep Learning and Reinforcement Learning, has shown promise in resolving scheduling problems in cloud computing. However, reviews of the literature on task scheduling that employ DRL techniques for reducing energy consumption are limited. In this paper, we survey and analyze energy consumption models used for scheduling goals, provide an overview of the DRL algorithms used in the literature, and quantitatively compare the model differences of Markov Decision Process elements. We also summarize the experimental platforms, datasets, and neural network structures used in the DRL algorithm. Finally, we analyze the research gap in DRL-based task scheduling and discuss existing challenges as well as future directions from various aspects. This paper contributes to the correlation perspective on the task scheduling problem with the DRL approach and provides a reference for in-depth research on the direction of DRL-based task scheduling research. Our findings suggest that DRL-based scheduling techniques can significantly reduce energy consumption in cloud data centers, making them a promising area for further investigation.}
}
@article{WU2020142,
title = {Adaptive stock trading strategies with deep reinforcement learning methods},
journal = {Information Sciences},
volume = {538},
pages = {142-158},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.05.066},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520304692},
author = {Xing Wu and Haolei Chen and Jianjia Wang and Luigi Troiano and Vincenzo Loia and Hamido Fujita},
keywords = {Stock trading strategy, Gated recurrent unit, Deep Q-learning, Deep deterministic policy gradient},
abstract = {The increasing complexity and dynamical property in stock markets are key challenges of the financial industry, in which inflexible trading strategies designed by experienced financial practitioners fail to achieve satisfactory performance in all market conditions. To meet this challenge, adaptive stock trading strategies with deep reinforcement learning methods are proposed. For the time-series nature of stock market data, the Gated Recurrent Unit (GRU) is applied to extract informative financial features, which can represent the intrinsic characteristics of the stock market for adaptive trading decisions. Furthermore, with the tailored design of state and action spaces, two trading strategies with reinforcement learning methods are proposed as GDQN (Gated Deep Q-learning trading strategy) and GDPG (Gated Deterministic Policy Gradient trading strategy). To verify the robustness and effectiveness of GDQN and GDPG, they are tested both in the trending and in the volatile stock market from different countries. Experimental results show that the proposed GDQN and GDPG not only outperform the Turtle trading strategy but also achieve more stable returns than a state-of-the-art direct reinforcement learning method, DRL trading strategy, in the volatile stock market. As far as the GDQN and the GDPG are compared, experimental results demonstrate that the GDPG with an actor-critic framework is more stable than the GDQN with a critic-only framework in the ever-evolving stock market.}
}
@article{CHENG2018303,
title = {ThermalNet: A deep reinforcement learning-based combustion optimization system for coal-fired boiler},
journal = {Engineering Applications of Artificial Intelligence},
volume = {74},
pages = {303-311},
year = {2018},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2018.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0952197618301477},
author = {Yin Cheng and Yuexin Huang and Bo Pang and Weidong Zhang},
keywords = {Combustion optimization, DQN, LSTM, Reinforcement learning, DCS},
abstract = {This paper presents a combustion optimization system for coal-fired boilers that includes a trade-off between emissions control and boiler efficiency. Designing an optimizer for this nonlinear, multiple-input multiple-output problem is challenging. This paper describes the development of an integrated combustion optimization system called ThermalNet, which is based on a deep Q-network (DQN) and a long short-term memory (LSTM) module. ThermalNet is a highly automated system consisting of an LSTM–ConvNet predictor and a DQN optimizer. The LSTM–ConvNet extracts the features of boiler behavior from the distributed control system (DCS) operational data of a supercritical thermal plant. The DQN reinforcement learning optimizer contributes to the online development of policies based on static and dynamic states. ThermalNet establishes a sequence of control actions that both reduce emissions and simultaneously enhance fuel utilization. The internal structure of the DQN optimizer demonstrates a greater representation capacity than does the shallow multilayer optimizer. The presented experiments indicate the effectiveness of the proposed optimization system.}
}
@article{GUISI2016855,
title = {Reinforcement Learning with Multiple Shared Rewards},
journal = {Procedia Computer Science},
volume = {80},
pages = {855-864},
year = {2016},
note = {International Conference on Computational Science 2016, ICCS 2016, 6-8 June 2016, San Diego, California, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.376},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916308511},
author = {Douglas M. Guisi and Richardson Ribeiro and Marcelo Teixeira and Andre P. Borges and Fabricio Enembreck},
keywords = {Adaptive Agents, Shared Rewards, Interaction, Learning, Coordination.},
abstract = {A major concern in multi-agent coordination is how to select algorithms that can lead agents to learn together to achieve certain goals. Much of the research on multi-agent learning relates to reinforcement learning (RL) techniques. One element of RL is the interaction model, which describes how agents should interact with each other and with the environment. Discrete, continuous and objective-oriented interaction models can improve convergence among agents. This paper proposes an approach based on the integration of multi-agent coordination models designed for reward-sharing policies. By taking the best features from each model, better agent coordination is achieved. Our experimental results show that this approach improves convergence among agents even in large state-spaces and yields better results than classical RL approaches.}
}
@article{DEMOSTHENOUS2022117311,
title = {Deep reinforcement learning for improving competitive cycling performance},
journal = {Expert Systems with Applications},
volume = {203},
pages = {117311},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117311},
url = {https://www.sciencedirect.com/science/article/pii/S095741742200673X},
author = {Giorgos Demosthenous and Marios Kyriakou and Vassilis Vassiliades},
keywords = {Reinforcement learning, Recommendation systems, Competitive cycling},
abstract = {Developing expert systems that make use of artificial intelligence (AI) to provide predictive analytics as well as targeted recommendations for decision support has been gaining momentum in recent years. Both academia and industry are looking into creating such systems to solve real-world problems and tackle specific challenges. In our work, we investigate the potential application of different machine learning approaches to solutions around competitive cycling. Specifically, we build and evaluate prediction models that are capable of accurately predicting a cyclist’s speed and heart rate using sensory information collected during bike rides. In addition, we create a recommendation module that is able to provide real-time action suggestions to cyclists regarding their posture with the goal of improving their overall performance. We achieve this using a combination of model-based reinforcement learning (RL) and deep RL. In particular, we use model-based RL to learn a “simulator” of bike rides using the prediction models and action profiles extracted from sensors placed on the cyclists’ back. We then use deep Q-learning in the simulator to extract policies that improve a cyclist’s behavior during a bike ride. Our evaluation shows that by recommending specific actions throughout the ride, cyclists can increase their overall average speed with only a minimal impact on their heart rate. The results presented in this paper constitute clear evidence that advanced AI techniques are a prime candidate for further developing intelligent solutions in competitive cycling and other similar areas.}
}
@incollection{PERE20221459,
title = {The Impact of Reward Shaping in Reinforcement Learning for Agent-based Microgrid Control},
editor = {Ludovic Montastruc and Stephane Negny},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {51},
pages = {1459-1464},
year = {2022},
booktitle = {32nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-95879-0.50244-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323958790502447},
author = {Valentin Père and Fabien Baillon and Mathieu Milhe and Jean-Louis Dirion},
keywords = {Microgrid, Reinforcement Learning, Control, Reward},
abstract = {In order to reduce CO2 emissions, electricity networks must increasingly integrate renewable energies. Microgrids are distributed electrical networks with their own generation and load, often supported by an electrical storage system. It can be connected to the external electrical network or isolated. Since electricity consumption, price and renewable production are stochastic phenomena, the control of microgrids must adapt to uncertainties. Data-driven models and in particular reinforcement learning (RL) have become efficient algorithms in high-level microgrid control. RL are agent-based algorithms, which interact with their environment and learn with a numerical reward signal. A certain behavior can implicitly be expected when the reward system is formulated. For example, a reward system that encourages the agent to interact as little as possible with the external network will explicitly increase the autonomy of the microgrid. Implicitly, it can be expected to schedule the battery to maximize the ratio of renewable energy used to the amount producible. Q-learning algorithm has been used due to its performance in discrete action space, which simplified the benchmark complexity. An agent is trained with different reward functions commonly found in the literature related to data-driven microgrid control algorithms. The agent parameters do not vary from one case study to another. Indicators are set up to evaluate the agent behavior. They are based on implicit behavioral criteria in the definition of the reward system such as the ratio of renewable energy used, the amount of energy stored during peak hours, etc. This study enables to find a way to rationalize the choice of a reward system to control in a near-optimal way microgrid while meeting implicit secondary objectives. It could lead to a choice on weighting coefficient in a combination of reward functions.}
}
@article{DATTA2021329,
title = {Reinforcement learning in surgery},
journal = {Surgery},
volume = {170},
number = {1},
pages = {329-332},
year = {2021},
issn = {0039-6060},
doi = {https://doi.org/10.1016/j.surg.2020.11.040},
url = {https://www.sciencedirect.com/science/article/pii/S0039606020308254},
author = {Shounak Datta and Yanjun Li and Matthew M. Ruppert and Yuanfang Ren and Benjamin Shickel and Tezcan Ozrazgat-Baslanti and Parisa Rashidi and Azra Bihorac},
abstract = {Patients and physicians make essential decisions regarding diagnostic and therapeutic interventions. These actions should be performed or deferred under time constraints and uncertainty regarding patients’ diagnoses and predicted response to treatment. This may lead to cognitive and judgment errors. Reinforcement learning is a subfield of machine learning that identifies a sequence of actions to increase the probability of achieving a predetermined goal. Reinforcement learning has the potential to assist in surgical decision making by recommending actions at predefined intervals and its ability to utilize complex input data, including text, image, and temporal data, in the decision-making process. The algorithm mimics a human trial-and-error learning process to calculate optimum recommendation policies. The article provides insight regarding challenges in the development and application of reinforcement learning in the medical field, with an emphasis on surgical decision making. The review focuses on challenges in formulating reward function describing the ultimate goal and determination of patient states derived from electronic health records, along with the lack of resources to simulate the potential benefits of suggested actions in response to changing physiological states during and after surgery. Although clinical implementation would require secure, interoperable, livestreaming electronic health record data for use by virtual model, development and validation of personalized reinforcement learning models in surgery can contribute to improving care by helping patients and clinicians make better decisions.}
}
@article{MATE2023100131,
title = {Simultaneous tuning of multiple PID controllers for multivariable systems using deep reinforcement learning},
journal = {Digital Chemical Engineering},
pages = {100131},
year = {2023},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2023.100131},
url = {https://www.sciencedirect.com/science/article/pii/S2772508123000492},
author = {Sammyak Mate and Pawankumar Pal and Anshumali Jaiswal and Sharad Bhartiya},
keywords = {Nonlinear multivariable system, Simultaneous tuning of multiple PIDs, Deep reinforcement learning, Actor-critic network, Non-minimum phase},
abstract = {Traditionally, tuning of PID controllers is based on linear approximation of the dynamics between the manipulated input and the controlled output. The tuning is performed one loop at a time and interaction effects between the multiple single-input-single-output (SISO) feedback control loops is ignored. It is also well-known that if the plant operates over a wide operating range, the dynamic behaviour changes thereby rendering the performance of an initially tuned PID controller unacceptable. The design of PID controllers, in general, is based on linear models that are obtained by linearizing a nonlinear system around a steady state operating point. For example, in peak seeking control, the sign of the process gain changes around the peak value, thereby invalidating the linear model obtained at the other side of the peak. Similarly, at other operating points, the multivariable plant may exhibit new dynamic features such as inverse response. This work proposes to use deep reinforcement learning (DRL) strategies to simultaneously tune multiple SISO PID controllers using a single DRL agent while enforcing interval constraints on the tuning parameter values. This ensures that interaction effects between the loops are directly factored in the tuning. Interval constraints also ensure safety of the plant during training by ensuring that the tuning parameter values are bounded in a stable region. Moreover, a trained agent when deployed, provides operating condition based PID parameters on the fly ensuring nonlinear compensation in the PID design. The methodology is demonstrated on a quadruple tank benchmark system via simulations by simultaneously tuning two PI level controllers. The same methodology is then adopted to tune PI controllers for the operating condition under which the plant exhibits a right half plane multivariable direction zero. Comparisons with PI controllers tuned with standard methods suggest that the proposed method is a viable approach, particularly when simulators are available for the plant dynamics.}
}
@article{YANG202289,
title = {Path planning of UAV base station based on deep reinforcement learning},
journal = {Procedia Computer Science},
volume = {202},
pages = {89-104},
year = {2022},
note = {International Conference on Identification, Information and Knowledge in the internet of Things, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922005476},
author = {Siming Yang and Zheng Shan and Jiang Cao and Yuan Gao and Yang Guo and Ping Wang and Xiaonan Wang and Jing Wang and Tingting Zhang and Jiayu Guo},
keywords = {UAV path planning, Wireless communication coverage, Deep learning, Deep reinforcement learning},
abstract = {UAV base station platform has become the current research hotspot of assisting ground base station for wireless coverage.At present, the most important issue is how to make path planning to provide the stable communication guarantee for multiple mobile users. In this article, we model the air-to-ground channel to describe the path loss between the UAV platform and the user and build a simulation environment for training based on the OpenAI-GYM architecture. In addition, this paper proposes a reinforcement learning algorithm based on intrinsic rewards, which uses the mean square error of the state prediction results to quantify the novelty of the state. Algorithms enable agents to efficiently carry out strategy iterations. Experiments results showed that our algorithm has a higher score and takes less time.}
}
@article{GALLEGO2023100241,
title = {Maintaining flexibility in smart grid consumption through deep learning and deep reinforcement learning},
journal = {Energy and AI},
volume = {13},
pages = {100241},
year = {2023},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2023.100241},
url = {https://www.sciencedirect.com/science/article/pii/S2666546823000137},
author = {Fernando Gallego and Cristian Martín and Manuel Díaz and Daniel Garrido},
keywords = {Multi-agent based system, Smart grid, Distributed energy resources},
abstract = {The smart grid concept is key to the energy revolution that has been taking place in recent years. Smart Grids have been present in energy research since their emergence. However, the scarcity of data from different energy sources, hardware power, or co-simulation environments has hindered their development. With advances in multi-agent-based systems, the possibility of simulating the behavior of different energy sources, combining real building consumption, and simulated data, storage batteries and vehicle charging points, has opened up. This development has resulted in much research published using both simulated and physical data. All these investigations show that the main problem is that the machine learning algorithms do not fully match the real behavior, it is complex to use them to replicate the different actions to be performed. This paper aims to combine the approach of behavior prediction with state-of-the-art techniques, such as deep learning and deep reinforcement learning, to simulate unknown or critical system scenarios. A very important element in smart grids is the possibility of maintaining consumption within specific ranges (flexibility). For this purpose, we have made use of Tensorflow libraries that predict energy consumption and deep reinforcement learning to select the optimal actions to be performed in our system. The developed platform is flexible enough to include new technologies such as smart batteries, electric vehicles, etc., and it is oriented to real-time operation, being applied in an on-going real project such as the European ebalance-plus project.11Ebalance-plus: https://www.ebalanceplus.eu.}
}
@article{ARCHIBALD2023112238,
title = {A stochastic maximum principle approach for reinforcement learning with parameterized environment},
journal = {Journal of Computational Physics},
volume = {488},
pages = {112238},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112238},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123003339},
author = {Richard Archibald and Feng Bao and Jiongmin Yong},
keywords = {Reinforcement learning, Optimal control, Stochastic maximum principle, Parameter estimation, Optimal filtering},
abstract = {In this work, we introduce a stochastic maximum principle (SMP) approach for solving the reinforcement learning problem with the assumption that the unknowns in the environment can be parameterized based on physics knowledge. For the development of numerical algorithms, we apply an effective online parameter estimation method as our exploration technique to estimate the environment parameter during the training procedure, and the exploitation for the optimal policy is achieved by an efficient backward action learning method for policy improvement under the SMP framework. Numerical experiments are presented to demonstrate that the SMP approach for reinforcement learning can produce reliable control policy, and the gradient descent type optimization in the SMP solver requires less training episodes compared with the standard dynamic programming principle based methods.}
}
@article{JIA20196158,
title = {Advanced Building Control via Deep Reinforcement Learning},
journal = {Energy Procedia},
volume = {158},
pages = {6158-6163},
year = {2019},
note = {Innovative Solutions for Energy Transitions},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2019.01.494},
url = {https://www.sciencedirect.com/science/article/pii/S187661021930517X},
author = {Ruoxi Jia and Ming Jin and Kaiyu Sun and Tianzhen Hong and Costas Spanos},
keywords = {smart building, building control, reinforcement learning, energy efficiency, cyber-physical systems, optimization},
abstract = {Building control is a challenging task, not least because of complex building dynamics ad multiple control objectives that are often conflicting. To tackle this challenge, we explore an end-to-end deep reinforcement learning paradigm, which learns an optimal control strategy to reduce energy consumption and to enhance occupant comfort from the data of building-controller interactions. Because real-world control policies need to be interpretable and efficient in learning, this work makes the following key contributions: (1) we investigated a systematic approach to encode expert knowledge in reinforcement learning through “experience replay” and/or “expert policy guidance”; (2) we proposed to regulate the smoothness property of the neural network to penalize the erratic behavior, which is found to dramatically stabilize the learning process and lead to interpretable control laws; (3) we established a virtual testbed for building control by combining the state-of-the-art building energy simulator EnergyPlus with a python environment to provide a systematic evaluation and comparison platform, which will not only further our understanding of the strengths and weaknesses of existing building control algorithms, but also suggest directions for future research. We experimentally verified our proposed deep reinforcement learning paradigm on the virtual testbed in case studies, which demonstrated promising results.}
}
@article{WANG2023110355,
title = {Mobile agent path planning under uncertain environment using reinforcement learning and probabilistic model checking},
journal = {Knowledge-Based Systems},
volume = {264},
pages = {110355},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110355},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123001053},
author = {Xia Wang and Jun Liu and Chris Nugent and Ian Cleland and Yang Xu},
keywords = {Expected reward, Mobile agent, Uncertain environment, Probabilistic model checking, -},
abstract = {The major challenge in mobile agent path planning, within an uncertain environment, is effectively determining an optimal control model to discover the target location as quickly as possible and evaluating the control system’s reliability. To address this challenge, we introduce a learning-verification integrated mobile agent path planning method to achieve both the effectiveness and the reliability. More specifically, we first propose a modified Q-learning algorithm (a popular reinforcement learning algorithm), called QEA−learning algorithm, to find the best Q-table in the environment. We then determine the location transition probability matrix, and establish a probability model using the assumption that the agent selects a location with a higher Q-value. Secondly, the learnt behaviour of the mobile agent based on QEA−learning algorithm, is formalized as a Discrete-time Markov Chain (DTMC) model. Thirdly, the required reliability requirements of the mobile agent control system are specified using Probabilistic Computation Tree Logic (PCTL). In addition, the DTMC model and the specified properties are taken as the input of the Probabilistic Model Checker PRISM for automatic verification. This is preformed to evaluate and verify the control system’s reliability. Finally, a case study of a mobile agent walking in a grids map is used to illustrate the proposed learning algorithm. Here we have a special focus on the modelling approach demonstrating how PRISM can be used to analyse and evaluate the reliability of the mobile agent control system learnt via the proposed algorithm. The results show that the path identified using the proposed integrated method yields the largest expected reward.}
}
@article{KWON2023114100,
title = {A hybrid decision support system for adaptive trading strategies: Combining a rule-based expert system with a deep reinforcement learning strategy},
journal = {Decision Support Systems},
pages = {114100},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114100},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623001756},
author = {Yuhee Kwon and Zoonky Lee},
keywords = {Intelligent hybrid trading system, Intelligent decision support system, Adaptive trading strategy, Reinforcement learning, Rule-based system},
abstract = {Stock trading strategies pose challenging applications of machine learning for significant commercial yields in the finance industry, drawing the attention of both economists and computer scientists. Until now, many researchers have proposed various methods to implement intelligent trading strategy systems that can support decisions regarding stock trading. Some studies have shown that the problem of trading strategies can be successfully addressed by applying hybrid approaches. Motivated by this, we propose a hybrid decision support system for adaptive trading strategies that combines a rule-based system with deep reinforcement learning to self-improve by learning with human expertise. This study overcomes the limitations of previous hybrid models that mainly have focused on optimizing trading decisions and improving forecasting accuracy. The proposed hybrid model combines decision-making information from a rule-based model to enable the agent of reinforcement learning to capture more trading opportunities. In addition, the investor's available balance states facilitate adaptive learning by interacting with the environment. Moreover, the proposed trading mechanism adjusts the volume size using the policy gradient algorithm's action probabilities, resulting in improved risk-adjusted returns. The proposed hybrid model has the potential to be a reliable trading system in real-world applications through its ability to adapt to different market scenarios, withstand stressful market conditions, reduce transaction costs, scale to various index funds, and extend the proposed hybrid structure. This study highlights the applicability of more advanced machine learning in financial areas, and we also suggest expanding this approach to adaptive decision-making systems in other fields.}
}
@article{SU2023103955,
title = {EMVLight: A multi-agent reinforcement learning framework for an emergency vehicle decentralized routing and traffic signal control system},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {146},
pages = {103955},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103955},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22003680},
author = {Haoran Su and Yaofeng D. Zhong and Joseph Y.J. Chow and Biswadip Dey and Li Jin},
keywords = {Emergency vehicle management, Traffic signal control, Deep reinforcement learning, Multi-agent system},
abstract = {Emergency vehicles (EMVs) play a crucial role in responding to time-critical calls such as medical emergencies and fire outbreaks in urban areas. Existing methods for EMV dispatch typically optimize routes based on historical traffic-flow data and design traffic signal pre-emption accordingly; however, we still lack a systematic methodology to address the coupling between EMV routing and traffic signal control. In this paper, we propose EMVLight, a decentralized reinforcement learning (RL) framework for joint dynamic EMV routing and traffic signal pre-emption. We adopt the multi-agent advantage actor–critic method with policy sharing and spatial discounted factor. This framework addresses the coupling between EMV navigation and traffic signal control via an innovative design of multi-class RL agents and a novel pressure-based reward function. The proposed methodology enables EMVLight to learn network-level cooperative traffic signal phasing strategies that not only reduce EMV travel time but also shortens the travel time of non-EMVs. Simulation-based experiments indicate that EMVLight enables up to a 42.6% reduction in EMV travel time as well as an 23.5% shorter average travel time compared with existing approaches.}
}