@inproceedings{10.5555/3535850.3535950,
author = {McCalmon, Joe and Le, Thai and Alqahtani, Sarra and Lee, Dongwon},
title = {CAPS: Comprehensible Abstract Policy Summaries for Explaining Reinforcement Learning Agents},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {As reinforcement learning (RL) continues to improve and be applied in situations alongside humans, the need to explain the learned behaviors of RL agents to end-users becomes more important. Strategies for explaining the reasoning behind an agent's policy, called policy-level explanations, can lead to important insights about both the task and the agent's behaviors. Following this line of research, in this work, we propose a novel approach, named as CAPS, that summarizes an agent's policy in the form of a directed graph with natural language descriptions. A decision tree based clustering method is utilized to abstract the state space of the task into fewer, condensed states which makes the policy graphs more digestible to end-users. This abstraction allows the users to control the size of the policy graph to achieve their desired balance between comprehensibility and accuracy. In addition, we develop a heuristic optimization method to find the most explainable graph policy and present it to the users. Finally, we use the user-defined predicates to enrich the abstract states with semantic meaning. We test our approach on five RL tasks, using both deterministic and stochastic policies, and show that our method is: (1) agnostic to the algorithms used to train the policies, and (2) comparable in accuracy and superior in explanation capabilities to existing baselines. Especially, when provided with our explanation graph, end-users are able to accurately interpret policies of trained RL agents 80\% of the time, compared to 10\% when provided with the next best baseline. We make our code and datasets available to ensure the reproducibility of our research findings: https://github.com/mccajl/CAPS},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {889–897},
numpages = {9},
keywords = {policy-level explanations, explainable machine learning, user study, reinforcement learning, policy summary, explainable reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/1102256.1102277,
author = {Wada, Atsushi and Takadama, Keiki and Shimohara, Katsunori},
title = {Learning Classifier System Equivalent with Reinforcement Learning with Function Approximation},
year = {2005},
isbn = {9781450378000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1102256.1102277},
doi = {10.1145/1102256.1102277},
abstract = {We present an experimental comparison of the reinforcement process between Learning Classifier System (LCS) and Reinforcement Learning (RL) with function approximation (FA) method, regarding their generalization mechanisms. To validate our previous theoretical analysis that derived equivalence of reinforcement process between LCS and RL, we introduce a simple test environment named Gridworld, which can be applied to both LCS and RL with three different classes of generalization: (1) tabular representation; (2) state aggregation; and (3) linear approximation. From the simulation experiments comparing LCS with its GA-inactivated and corresponding RL method, all the cases regarding the class of generalization showed identical results with the criteria of performance and temporal difference (TD) error, thereby verifying the equivalence predicted from the theory.},
booktitle = {Proceedings of the 7th Annual Workshop on Genetic and Evolutionary Computation},
pages = {92–93},
numpages = {2},
keywords = {genetic-based machine learning, function approximation, learning classifier systems, reinforcement learning},
location = {Washington, D.C.},
series = {GECCO '05}
}

@inproceedings{10.5555/3398761.3398927,
author = {Wang, Haozhe and Zhou, Jiale and He, Xuming},
title = {Learning Context-Aware Task Reasoning for Efficient Meta Reinforcement Learning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Despite recent success of deep network-based Reinforcement Learning (RL), it remains elusive to achieve human-level efficiency in learning novel tasks. While previous efforts attempt to address this challenge using meta-learning strategies, they typically suffer from sampling inefficiency with on-policy RL algorithms or meta-overfitting with off-policy learning. In this work, we propose a novel meta-RL strategy to address those limitations. In particular, we decompose the meta-RL problem into three sub-tasks, task-exploration, task-inference and task-fulfillment, instantiated with two deep network agents and a task encoder. During meta-training, our method learns a task-conditioned actor network for task-fulfillment, an explorer network with a self-supervised reward shaping that encourages task-informative experiences in task-exploration, and a context-aware graph-based task encoder for task inference. We validate our approach with extensive experiments on several public benchmarks and the results show that our algorithm effectively performs exploration for task inference, improves sample efficiency during both training and testing, and mitigates the meta-overfitting problem.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1440–1448},
numpages = {9},
keywords = {deep reinforcement learning, multitask learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@article{10.1145/3611679,
author = {Sandygulova, Anara and Amir, Aida and Oralbayeva, Nurziya and Telisheva, Zhansaule and Zhanatkyzy, Aida and Shakerimov, Aidar and Sarmonov, Shamil and Aimysheva, Arna},
title = {QWriter: A Reinforcement Learning-Based Robot for Early Literacy Acquisition},
year = {2023},
issue_date = {Fall 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3611679},
doi = {10.1145/3611679},
abstract = {Robot-assisted language learning produces comparable results to human tutors in a long-term study with elementary school children.},
journal = {XRDS},
month = {oct},
pages = {10–15},
numpages = {6}
}

@inproceedings{10.1145/3318265.3318294,
author = {Zhou, Yinda and Liu, Weiming and Li, Bin},
title = {Two-Stage Population Based Training Method for Deep Reinforcement Learning},
year = {2019},
isbn = {9781450366380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318265.3318294},
doi = {10.1145/3318265.3318294},
abstract = {Deep reinforcement learning (DRL) methods has been widely applied on more and more challenging learning tasks, and achieved excellent performance. However, the efficiency of deep reinforcement learning is notoriously sensitive to their own hyperparameter configuration. The optimization process of deep reinforcement learning is highly dynamic and non-stationary, rather than a simple fitting process. So, its optimal hyperparameter should be adaptively adjusted according to the current learning process, rather than using a fixed set of hyperparameter configurations from beginning to end. DeepMind innovatively proposed a population based training (PBT) method for deep reinforcement learning, which achieved hyperparameter adaptation and made the model better trained. However, we assume that at the early stage when the learning model has little knowledge of the environment, frequent hyperparameter change will not be helpful for the model to learn efficiently, while learning with a reasonable fixed hyperparameter configuration will help the model obtain necessary knowledge as quick as possible, which we consider is more important for reinforcement learning at early stage. In this paper, we verified our hypothesis through experiments, and a Two-Stage Population Based Training (TS-PBT) method is proposed, which is a more efficient population based training method for deep reinforcement learning. Experiments show that at the same computational budget, our TS-PBT method makes the final performance of the model significantly better than the PBT method. TS-PBT achieved 40\%, 310\%, 2\%, 53\%, 30\% and 38\% performance improvement over PBT separately in six test environments.},
booktitle = {Proceedings of the 3rd International Conference on High Performance Compilation, Computing and Communications},
pages = {38–44},
numpages = {7},
keywords = {two-stage, hyperparameter adaptation, deep reinforcement learning, game, PBT},
location = {Xi'an, China},
series = {HP3C '19}
}

@article{10.1145/3358230,
author = {Tran, Hoang-Dung and Cai, Feiyang and Diego, Manzanas Lopez and Musau, Patrick and Johnson, Taylor T. and Koutsoukos, Xenofon},
title = {Safety Verification of Cyber-Physical Systems with Reinforcement Learning Control},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3358230},
doi = {10.1145/3358230},
abstract = {This paper proposes a new forward reachability analysis approach to verify safety of cyber-physical systems (CPS) with reinforcement learning controllers. The foundation of our approach lies on two efficient, exact and over-approximate reachability algorithms for neural network control systems using star sets, which is an efficient representation of polyhedra. Using these algorithms, we determine the initial conditions for which a safety-critical system with a neural network controller is safe by incrementally searching a critical initial condition where the safety of the system cannot be established. Our approach produces tight over-approximation error and it is computationally efficient, which allows the application to practical CPS with learning enable components (LECs). We implement our approach in NNV, a recent verification tool for neural networks and neural network control systems, and evaluate its advantages and applicability by verifying safety of a practical Advanced Emergency Braking System (AEBS) with a reinforcement learning (RL) controller trained using the deep deterministic policy gradient (DDPG) method. The experimental results show that our new reachability algorithms are much less conservative than existing polyhedra-based approaches. We successfully determine the entire region of the initial conditions of the AEBS with the RL controller such that the safety of the system is guaranteed, while a polyhedra-based approach cannot prove the safety properties of the system.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {oct},
articleno = {105},
numpages = {22},
keywords = {reinforcement learning, Formal methods, verification}
}

@inproceedings{10.5555/2484920.2485084,
author = {Gehring, Clement and Precup, Doina},
title = {Smart Exploration in Reinforcement Learning Using Absolute Temporal Difference Errors},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Exploration is still one of the crucial problems in reinforcement learning, especially for agents acting in safety-critical situations. We propose a new directed exploration method, based on a notion of state controlability. Intuitively, if an agent wants to stay safe, it should seek out states where the effects of its actions are easier to predict; we call such states more controllable. Our main contribution is a new notion of controlability, computed directly from temporal-difference errors. Unlike other existing approaches of this type, our method scales linearly with the number of state features, and is directly applicable to function approximation. Our method converges to correct values in the policy evaluation setting. We also demonstrate significantly faster learning when this exploration strategy is used in large control problems.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1037–1044},
numpages = {8},
keywords = {temporal-difference error, reinforcement learning, exploration},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1145/3578245.3585427,
author = {Russo Russo, Gabriele},
title = {Using Reinforcement Learning to Control Auto-Scaling of Distributed Applications},
year = {2023},
isbn = {9798400700729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578245.3585427},
doi = {10.1145/3578245.3585427},
abstract = {Modern distributed systems can benefit from the availability of large-scale and heterogeneous computing infrastructures. However, the complexity and dynamic nature of these environments also call for self-adaptation abilities, as guaranteeing efficient resource usage and acceptable service levels through static configurations is very difficult.In this talk, we discuss a hierarchical auto-scaling approach for distributed applications, where application-level managers steer the overall process by supervising component-level adaptation managers. Following a bottom-up approach, we first discuss how to exploit model-free and model-based reinforcement learning to compute auto-scaling policies for each component. Then, we show how Bayesian optimization can be used to automatically configure the lower-level auto-scalers based on application-level objectives. As a case study, we consider distributed data stream processing applications, which process high-volume data flows in near real-time and cope with varying and unpredictable workloads.},
booktitle = {Companion of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {137–138},
numpages = {2},
keywords = {auto-scaling, reinforcement learning},
location = {Coimbra, Portugal},
series = {ICPE '23 Companion}
}

@inproceedings{10.1145/3533271.3561704,
author = {Liu, Chunli and Ventre, Carmine and Polukarov, Maria},
title = {Synthetic Data Augmentation for Deep Reinforcement Learning in Financial Trading},
year = {2022},
isbn = {9781450393768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533271.3561704},
doi = {10.1145/3533271.3561704},
abstract = {Despite the eye-catching advances in the area, deploying Deep Reinforcement Learning (DRL) in financial markets remains a challenging task. Model-based techniques often fall short due to epistemic uncertainty, whereas model-free approaches require large amount of data that is often unavailable. Motivated by the recent research on the generation of realistic synthetic financial data, we explore the possibility of using augmented synthetic datasets for training DRL agents without direct access to the real financial data. With our novel approach, termed synthetic data augmented reinforcement learning for trading (SDARL4T), we test whether the performance of DRL for financial trading can be enhanced, by attending to both profitability and generalization abilities. We show that DRL agents trained with SDARL4T make a profit which is comparable, and often much larger, than that obtained by the agents trained on real data, while guaranteeing similar robustness. These results support the adoption of our framework in real-world uses of DRL for trading.},
booktitle = {Proceedings of the Third ACM International Conference on AI in Finance},
pages = {343–351},
numpages = {9},
keywords = {Generative Model, Financial Trading., Quantitative Finance, Markov Decision Process, Deep Reinforcement Learning, Synthetic data},
location = {New York, NY, USA},
series = {ICAIF '22}
}

@inproceedings{10.1145/3290480.3290494,
author = {Wu, Cangshuai and Shi, Jiangyong and Yang, Yuexiang and Li, Wenhua},
title = {Enhancing Machine Learning Based Malware Detection Model by Reinforcement Learning},
year = {2018},
isbn = {9781450365673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290480.3290494},
doi = {10.1145/3290480.3290494},
abstract = {Malware detection is getting more and more attention due to the rapid growth of new malware. As a result, machine learning (ML) has become a popular way to detect malware variants. However, machine learning models can also be cheated. Through reinforcement learning (RL), we can generate new malware samples which can bypass the detection of machine learning. In this paper, a RL model on malware generation named gym-plus is designed. Gym-plus is built based on gym-malware with some improvements. As a result, the probability of evading machine learning based static PE malware detection models is increased by 30\%. Based on these newly generated samples, we retrain our detecting model to detect unknown threats. In our test, the detection accuracy of malware increased from 15.75\% to 93.5\%.},
booktitle = {Proceedings of the 8th International Conference on Communication and Network Security},
pages = {74–78},
numpages = {5},
keywords = {machine learning, malware evasion, reinforcement learning, static analysis},
location = {Qingdao, China},
series = {ICCNS '18}
}

@inproceedings{10.1145/3551901.3556493,
author = {Abdul, Naiju Karim and Antony, George and Rao, Rahul M. and Skariah, Suriya T.},
title = {Scan Chain Clustering and Optimization with Constrained Clustering and Reinforcement Learning},
year = {2022},
isbn = {9781450394864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551901.3556493},
doi = {10.1145/3551901.3556493},
abstract = {Scan chains are used in design for test by providing controllability and observability at each register. Scan optimization is run during physical design after placement where scannable elements are re-ordered along the chain to reduce total wirelength (and power). In this paper, we present a machine learning based technique that leverages constrained clustering and reinforcement learning to obtain a wirelength efficient scan chain solution. Novel techniques like next-min sorted assignment, clustered assignment, node collapsing, partitioned Q-Learning and in-context start-end node determination are introduced to enable improved wire length while honoring design-for-test constraints. The proposed method is shown to provide up to 24\% scan wirelength reduction over a traditional algorithmic optimization technique across 188 moderately sized blocks from an industrial 7nm design.},
booktitle = {Proceedings of the 2022 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {83–90},
numpages = {8},
keywords = {reinforcement learning, scan chain, constrained clustering, machine learning, physical design},
location = {Virtual Event, China},
series = {MLCAD '22}
}

@inproceedings{10.1145/3489525.3511685,
author = {de Goede, Danilo and Kampert, Duncan and Varbanescu, Ana Lucia},
title = {The Cost of Reinforcement Learning for Game Engines: The AZ-Hive Case-Study},
year = {2022},
isbn = {9781450391436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489525.3511685},
doi = {10.1145/3489525.3511685},
abstract = {Although utilising computers to play board games has been a topic of research for many decades, the recent rapid developments in the field of reinforcement learning - like AlphaZero and variants - brought unprecedented progress in games such as chess and Go. However, the efficiency of this process remains unknown. In this work, we analyse the cost and efficiency of the AlphaZero approach when building a new game engine. Thus, we present our experience building AZ-Hive, an AlphaZero-based playing engine for the game of Hive. Using only the rules of the game and a quality of play assessment, AZ-Hive learns to play the game from scratch. Getting AZ-Hive up and running requires encoding the game in AlphaZero, i.e., capturing the board, the game state, the rules and the assessment of play-quality. And different encodings lead to significantly different AZ-Hive engines, with very different performance results. Thus, we propose a design space for configuring AZ-Hive, and demonstrate the costs and benefits of different configurations in this space. We find that different configurations lead to a less or more competitive playing-engine, but the training and evaluation for different such engines is prohibitively expensive. Moreover, no systematic, efficient exploration or pruning of the space is possible. In turn, an exhaustive exploration can easily take tens of training-years.},
booktitle = {Proceedings of the 2022 ACM/SPEC on International Conference on Performance Engineering},
pages = {145–152},
numpages = {8},
keywords = {energy efficiency, computational cost, design space exploration, alphazero: hive, game-playing engines, reinforcement learning},
location = {Beijing, China},
series = {ICPE '22}
}

@inproceedings{10.1145/3589010.3594888,
author = {Theodoropoulos, Theodoros and Kafetzis, Dimitrios and Violos, John and Makris, Antonios and Tserpes, Konstantinos},
title = {Multi-Agent Deep Reinforcement Learning for Weighted Multi-Path Routing},
year = {2023},
isbn = {9798400701641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589010.3594888},
doi = {10.1145/3589010.3594888},
abstract = {Traditional multi-path routing methods distribute evenly traffic across multiple paths in a network, which can lead to inefficient use of resources if some paths are significantly longer or less reliable than others. Weighted multi-path routing addresses this issue by introducing weights to appropriately distribute traffic across the available paths based on their state. This paper proposes a novel approach to weighted multi-path routing using a multi-agent actor-critic framework, in a manner that is aligned with the need to keep up with the Quality of Service requirements of contemporary, bandwidth-intensive applications.},
booktitle = {Proceedings of the 3rd Workshop on Flexible Resource and Application Management on the Edge},
pages = {7–11},
numpages = {5},
keywords = {deep reinforcement learning, actor-critic, weighted multi-path routing, multi-agent, bandwidth},
location = {Orlando, FL, USA},
series = {FRAME '23}
}

@article{10.1145/3425500,
author = {Asseman, Alexis and Antoine, Nicolas and Ozcan, Ahmet S.},
title = {Accelerating Deep Neuroevolution on Distributed FPGAs for Reinforcement Learning Problems},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1550-4832},
url = {https://doi.org/10.1145/3425500},
doi = {10.1145/3425500},
abstract = {Reinforcement learning, augmented by the representational power of deep neural networks, has shown promising results on high-dimensional problems, such as game playing and robotic control. However, the sequential nature of these problems poses a fundamental challenge for computational efficiency. Recently, alternative approaches such as evolutionary strategies and deep neuroevolution demonstrated competitive results with faster training time on distributed CPU cores. Here we report record training times (running at about 1 million frames per second) for Atari 2600 games using deep neuroevolution implemented on distributed FPGAs. Combined hardware implementation of the game console, image preprocessing and the neural network in an optimized pipeline, multiplied with the system level parallelism enabled the acceleration. These results are the first application demonstration on the IBM Neural Computer, which is a custom designed system that consists of 432 Xilinx FPGAs interconnected in a 3D mesh network topology. In addition to high performance, experiments also showed improvement in accuracy for all games compared to the CPU implementation of the same algorithm.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = {apr},
articleno = {21},
numpages = {17},
keywords = {reinforcement learning, neuroevolution, field programmable gate array, artificial neural network, Genetic algorithm}
}

@inproceedings{10.1145/1102351.1102454,
author = {\c{S}im\c{s}ek, \"{O}zg\"{u}r and Wolfe, Alicia P. and Barto, Andrew G.},
title = {Identifying Useful Subgoals in Reinforcement Learning by Local Graph Partitioning},
year = {2005},
isbn = {1595931805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1102351.1102454},
doi = {10.1145/1102351.1102454},
abstract = {We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs---those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek---states that lie between two densely-connected regions of the state space while producing an algorithm with low computational cost.},
booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
pages = {816–823},
numpages = {8},
location = {Bonn, Germany},
series = {ICML '05}
}

@inproceedings{10.1145/3269206.3272021,
author = {Jin, Junqi and Song, Chengru and Li, Han and Gai, Kun and Wang, Jun and Zhang, Weinan},
title = {Real-Time Bidding with Multi-Agent Reinforcement Learning in Display Advertising},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3272021},
doi = {10.1145/3269206.3272021},
abstract = {Real-time advertising allows advertisers to bid for each impression for a visiting user. To optimize specific goals such as maximizing revenue and return on investment (ROI) led by ad placements, advertisers not only need to estimate the relevance between the ads and user's interests, but most importantly require a strategic response with respect to other advertisers bidding in the market. In this paper, we formulate bidding optimization with multi-agent reinforcement learning. To deal with a large number of advertisers, we propose a clustering method and assign each cluster with a strategic bidding agent. A practical Distributed Coordinated Multi-Agent Bidding (DCMAB) has been proposed and implemented to balance the tradeoff between the competition and cooperation among advertisers. The empirical study on our industry-scaled real-world data has demonstrated the effectiveness of our methods. Our results show cluster-based bidding would largely outperform single-agent and bandit approaches, and the coordinated bidding achieves better overall objectives than purely self-interested bidding agents.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {2193–2201},
numpages = {9},
keywords = {multi-agent reinforcement learning, real-time bidding, bid optimization, display advertising},
location = {Torino, Italy},
series = {CIKM '18}
}

@article{10.1145/3502868,
author = {Romdhana, Andrea and Merlo, Alessio and Ceccato, Mariano and Tonella, Paolo},
title = {Deep Reinforcement Learning for Black-Box Testing of Android Apps},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3502868},
doi = {10.1145/3502868},
abstract = {The state space of Android apps is huge, and its thorough exploration during testing remains a significant challenge. The best exploration strategy is highly dependent on the features of the app under test. Reinforcement Learning (RL) is a machine learning technique that learns the optimal strategy to solve a task by trial and error, guided by positive or negative reward, rather than explicit supervision. Deep RL is a recent extension of RL that takes advantage of the learning capabilities of neural networks. Such capabilities make Deep RL suitable for complex exploration spaces such as one of Android apps. However, state-of-the-art, publicly available tools only support basic, Tabular RL. We have developed ARES, a Deep RL approach for black-box testing of Android apps. Experimental results show that it achieves higher coverage and fault revelation than the baselines, including state-of-the-art tools, such as TimeMachine and Q-Testing. We also investigated the reasons behind such performance qualitatively, and we have identified the key features of Android apps that make Deep RL particularly effective on them to be the presence of chained and blocking activities. Moreover, we have developed FATE to fine-tune the hyperparameters of Deep RL algorithms on simulated apps, since it is computationally expensive to carry it out on real apps.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {65},
numpages = {29},
keywords = {Deep reinforcement learning, Android testing}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00071,
author = {Lu, Chengjie},
title = {Evolutionary Computation and Reinforcement Learning for Cyber-Physical System Design},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00071},
doi = {10.1109/ICSE-Companion58688.2023.00071},
abstract = {Cyber-physical systems (CPSs) are designed to integrate computation and physical processes through constantly interacting with the physical environment. The complexity and uncertainty of the environment often come up with unpredictable situations, which place high demands on the dynamic adaptability of CPSs. Further, as the environment evolves, the CPS needs to constantly evolve itself to adapt to the changing environment. This paper presents a research plan that aims to develop a novel framework to address CPS design challenges under uncertain environments. We propose to utilize evolutionary computation and reinforcement learning techniques to design control policies that can adapt to the dynamic changes and uncertainties of the environment. Further, novel testing and evaluation approaches that can generate test cases while adapting to dynamic changes in the system and the environment will be explored.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {264–266},
numpages = {3},
keywords = {uncertainty, cyber-physical system, reinforcement learning, evolutionary computation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3457682.3457685,
author = {GAO, Ni and HE, Yiyue and JIAO, Yuming and CHANG, Zhuo},
title = {Online Optimal Investment Portfolio Model Based on Deep Reinforcement Learning},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457685},
doi = {10.1145/3457682.3457685},
abstract = {Combining deep reinforcement learning with portfolio management is one of the research hotspots in the field of quantitative investment. In recent years, quantitative investment has become one of the research hotspots in the field of deep reinforcement learning. Combining portfolio management with deep reinforcement learning has become a hot research direction. The model in this paper combines the perception ability of deep learning and the online decision-making ability of reinforcement learning to construct an intelligent model of portfolio management based on deep reinforcement learning DDPG algorithm. This model addresses issues such as the limited ability of reinforcement learning models to perceive complex environments and the strong correlation between the internal features of state data, introducing the convolutional neural network into the DDPG model. The use of convolutional neural networks to process multi-dimensional state data improves the perception ability of the reinforcement learning model and solves the problem of data correlation. Therefore, the model in this paper can obtain the dynamic optimal investment portfolio within the specified trading time range. At the same time, this paper introduces the action vector into the state value representing the stock market environment, and solves the problem of static model. Experiments show that our model improves the income acquisition ability of the portfolio management model.},
booktitle = {Proceedings of the 2021 13th International Conference on Machine Learning and Computing},
pages = {14–20},
numpages = {7},
keywords = {convolutional neural network, DDPG, deep reinforcement learning, portfolio management, Stock market},
location = {Shenzhen, China},
series = {ICMLC '21}
}

@inproceedings{10.5555/2936924.2937185,
author = {Li, Zhuoru and Narayan, Akshay and Leong, Tze-Yun},
title = {A Core Task Abstraction Approach to Hierarchical Reinforcement Learning: (Extended Abstract)},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We propose a new, core task abstraction (CTA) approach to learning the relevant transition functions in model-based hierarchical reinforcement learning. CTA exploits contextual independences of the state variables conditional on the task-specific actions; its promising performance is demonstrated through a set of benchmark problems.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {1411–1412},
numpages = {2},
keywords = {hierarchical reinforcement learning, core task abstraction},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@article{10.1145/3414523,
author = {Rakesh Kumar, S. and Muthuramalingam, S. and Al-Turjman, Fadi},
title = {Multimodal News Feed Evaluation System with Deep Reinforcement Learning Approaches},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3414523},
doi = {10.1145/3414523},
abstract = {Multilingual and multimodal data analysis is the emerging news feed evaluation system. News feed analysis and evaluations are interrelated processes, which are useful in understanding the news factors. The news feed evaluation system can be implemented for single or multilingual language models. Classification techniques used on multilingual news analysis require deep layered learning techniques rather than conventional approaches. In this proposed work, a hierarchical structure of deep learning algorithms is implemented for making an effective complex news evaluation system. Deep learning techniques such as the Deep Cooperative Multilingual Reinforcement Learning Model, the Multidimensional Genetic Algorithm, and the Multilingual Generative Adversarial Network are developed to evaluate a vast number of news feeds. The proposed tech-niques collaborate in a pipeline order to build a deep news feed evaluation system. The implementation details project that the newly proposed system performs 5\% to 12\% better than the other news evaluation systems.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {8},
numpages = {12},
keywords = {RL techniques, DL techniques, multilingual news and analysis, News feeds}
}

@inproceedings{10.1145/3400302.3415710,
author = {Pan, Zhixin and Sheldon, Jennifer and Mishra, Prabhat},
title = {Test Generation Using Reinforcement Learning for Delay-Based Side-Channel Analysis},
year = {2020},
isbn = {9781450380263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400302.3415710},
doi = {10.1145/3400302.3415710},
abstract = {Reliability and trustworthiness are dominant factors in designing System-on-Chips (SoCs) for a variety of applications. Malicious implants, such as hardware Trojans, can lead to undesired information leakage or system malfunction. To ensure trustworthy computing, it is critical to develop efficient Trojan detection techniques. While existing delay-based side-channel analysis is promising, it is not effective due to two fundamental limitations: (i) The difference in path delay between the golden design and Trojan inserted design is negligible compared with environmental noise and process variations. (ii) Existing approaches rely on manually crafted rules for test generation, and require a large number of simulations, making it impractical for industrial designs. In this paper, we propose a novel test generation method using reinforcement learning for delay-based Trojan detection. This paper makes three important contributions.1) Unlike existing methods that rely on the delay difference of a few gates, our proposed approach utilizes critical path analysis to generate test vectors that can maximize the side-channel sensitivity.2) To the best of our knowledge, our approach is the first attempt in applying reinforcement learning for efficient test generation to detect Trojans using delay-based analysis. 3) Our experimental results demonstrate that our method can significantly improve both side-channel sensitivity (59\% on average) and test generation time (17x on average) compared to state-of-the-art test generation techniques.},
booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
articleno = {109},
numpages = {7},
keywords = {reinforcement learning, test generation, side-channel analysis},
location = {Virtual Event, USA},
series = {ICCAD '20}
}

@article{10.1145/3539223,
author = {Malviya, Shrikant and Kumar, Piyush and Namasudra, Suyel and Tiwary, Uma Shanker},
title = {Experience Replay-Based Deep Reinforcement Learning for Dialogue Management Optimisation},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3539223},
doi = {10.1145/3539223},
abstract = {Dialogue policy is a crucial component in task-oriented Spoken Dialogue Systems (SDSs). As a decision function, it takes the current dialogue state as input and generates appropriate system’s response. In this paper, we explore the reinforcement learning approaches to solve this problem in an Indic language scenario. Recently, Deep Reinforcement Learning (DRL) has been used to optimise the dialogue policy. However, many DRL approaches are not sample-efficient. Hence, particular attention is given to actor-critic methods based on off-policy reinforcement learning that utilise the Experience Replay (ER) technique for reducing the bias and variance to achieve high sample efficiency. ER based actor-critic methods, such as Advantage Actor-Critic Experience Replay (A2CER) are proven to deliver competitive results in gaming environments that are fully observable and have a very small action-set. While, in SDSs, the states are not fully observable and often have to deal with the large action space. Describing the limitations of traditional methods, i.e., value-based and policy-based methods, such as high variance, low sample-efficiency, and often converging to local optima, we firstly explore the use of A2CER in dialogue policy learning. It is shown to beat the current state-of-the-art deep learning methods for SDS. Secondly, to handle the issues of early-stage performance, we utilise a demonstration corpus to pre-train the models prior to on-line policy learning. We thus experiment with the A2CER on a larger action space and find it significantly faster than the current state-of-the-art. Combining both approaches, we present a novel DRL based dialogue policy optimisation method, A2CER and its effectiveness for a task-oriented SDS in the Indic language.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
keywords = {Spoken dialogue systems, deep reinforcement learning, dialogue management}
}

@inproceedings{10.1145/3386527.3406729,
author = {Ding, Xinyi and Larson, Eric C.},
title = {Automatic RNN Cell Design for Knowledge Tracing Using Reinforcement Learning},
year = {2020},
isbn = {9781450379519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386527.3406729},
doi = {10.1145/3386527.3406729},
abstract = {Empirical results have shown that deep neural networks achieve superior performance in the application of Knowledge Tracing. However, the design of recurrent cells like long short term memory (LSTM) cells or gated recurrent units (GRU) is influenced largely by applications in natural language processing. They were proposed and evaluated in the context of sequence to sequence modeling, like machine translation. Even though the LSTM cell works well for knowledge tracing, it is unknown if its architecture is ideally suited for knowledge tracing. Despite the fact that there are several recurrent neural network based architectures proposed for knowledge tracing, the methodologies rely on empirical observations and trial and error, which may not be efficient or scalable. In this study, we investigate using reinforcement learning for the automatic design of recurrent neural network cells for knowledge tracing, showing improved performance compared to the LSTM cell. We also discuss a potential method for model regularization using neural architecture search.},
booktitle = {Proceedings of the Seventh ACM Conference on Learning @ Scale},
pages = {285–288},
numpages = {4},
keywords = {neural architecture search, reinforcement learning, regularization, knowledge tracing, recurrent neural network},
location = {Virtual Event, USA},
series = {L@S '20}
}

@inproceedings{10.1145/3209889.3209890,
author = {Ortiz, Jennifer and Balazinska, Magdalena and Gehrke, Johannes and Keerthi, S. Sathiya},
title = {Learning State Representations for Query Optimization with Deep Reinforcement Learning},
year = {2018},
isbn = {9781450358286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209889.3209890},
doi = {10.1145/3209889.3209890},
abstract = {We explore the idea of using deep reinforcement learning for query optimization. The approach is to build queries incrementally by encoding properties of subqueries using a learned representation.In this paper, we focus specifically on the state representation problem and the formation of the state transition function. We show preliminary results and discuss how we can use the state representation to improve query optimization using reinforcement learning.},
booktitle = {Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning},
articleno = {4},
numpages = {4},
location = {Houston, TX, USA},
series = {DEEM'18}
}

@inproceedings{10.5555/3398761.3398863,
author = {McKee, Kevin R. and Gemp, Ian and McWilliams, Brian and Du\`{e}\~{n}ez-Guzm\'{a}n, Edgar A. and Hughes, Edward and Leibo, Joel Z.},
title = {Social Diversity and Social Preferences in Mixed-Motive Reinforcement Learning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent research on reinforcement learning in pure-conflict and pure-common interest games has emphasized the importance of population heterogeneity. In contrast, studies of reinforcement learning in mixed-motive games have primarily leveraged homogeneous approaches. Given the defining characteristic of mixed-motive games--the imperfect correlation of incentives between group members--we study the effect of population heterogeneity on mixed-motive reinforcement learning. We draw on interdependence theory from social psychology and imbue reinforcement learning agents with Social Value Orientation (SVO), a flexible formalization of preferences over group outcome distributions. We subsequently explore the effects of diversity in SVO on populations of reinforcement learning agents in two mixed-motive Markov games. We demonstrate that heterogeneity in SVO generates meaningful and complex behavioral variation among agents similar to that suggested by interdependence theory. Empirical results in these mixed-motive dilemmas suggest agents trained in heterogeneous populations develop particularly generalized, high-performing policies relative to those trained in homogeneous populations.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {869–877},
numpages = {9},
keywords = {social value orientation, population heterogeneity, multi-agent reinforcement learning, mixed-motive games, interdependence theory},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3599957.3606236,
author = {Hadi, Abir Mohammad and Jang, Youngsun and Won, Kwanghee},
title = {Deep Reinforcement Learning Agent for Dynamic Pruning of Convolutional Layers},
year = {2023},
isbn = {9798400702280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3599957.3606236},
doi = {10.1145/3599957.3606236},
abstract = {Convolutional neural networks have become ubiquitous in image classification tasks. The state-of-the-art models for image classifications use convolutional layers in one way or another. There is a need for deploying deep learning models, especially the real-time vision models, in the edge devices to get better latency. But deploying such models in edge devices are becoming critical as the networks are becoming deeper and more dense. An overparameterized network is not necessarily required in many of the use cases of such deployment. This led researcher to develop technique for optimizing smaller and shallower networks, network architecture search techniques, and deep learning model compression techniques. In this research, we proposed a framework that utilizes deep determinisitic policy gradient, a class of deep reinforcement learning algorithm, to the learn the best set of filters considering the intrinsic dimensionality of the dataset, feature of each layer and the criteria based on which the filters of a convolutional layer will be ranked. By learning this relationship, we can prune off unnecessary filters which will reduce both computational and memory requirement for the model without losing too much accuracy. Our method showed that the model can prune off 66\% filters overall.},
booktitle = {Proceedings of the 2023 International Conference on Research in Adaptive and Convergent Systems},
articleno = {33},
numpages = {6},
keywords = {Neural network, reinforcement learning, compression, pruning, complexity},
location = {Gdansk, Poland},
series = {RACS '23}
}

@article{10.5555/2627435.2627443,
author = {Wilson, Aaron and Fern, Alan and Tadepalli, Prasad},
title = {Using Trajectory Data to Improve Bayesian Optimization for Reinforcement Learning},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Recently, Bayesian Optimization (BO) has been used to successfully optimize parametric policies in several challenging Reinforcement Learning (RL) applications. BO is attractive for this problem because it exploits Bayesian prior information about the expected return and exploits this knowledge to select new policies to execute. Effectively, the BO framework for policy search addresses the exploration-exploitation tradeoff. In this work, we show how to more effectively apply BO to RL by exploiting the sequential trajectory information generated by RL agents. Our contributions can be broken into two distinct, but mutually beneficial, parts. The first is a new Gaussian process (GP) kernel for measuring the similarity between policies using trajectory data generated from policy executions. This kernel can be used in order to improve posterior estimates of the expected return thereby improving the quality of exploration. The second contribution, is a new GP mean function which uses learned transition and reward functions to approximate the surface of the objective. We show that the model-based approach we develop can recover from model inaccuracies when good transition and reward models cannot be learned. We give empirical results in a standard set of RL benchmarks showing that both our model-based and model-free approaches can speed up learning compared to competing methods. Further, we show that our contributions can be combined to yield synergistic improvement in some domains.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {253–282},
numpages = {30},
keywords = {reinforcement learning, optimization, Bayesian, policy search, Markov decision process, MDP}
}

@inproceedings{10.1145/3472735.3473389,
author = {Dugan, Kevin and Harb, Maher and Rice, Daniel},
title = {A Reinforcement Learning Framework for Optimizing Throughput in DOCSIS Networks},
year = {2021},
isbn = {9781450386340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472735.3473389},
doi = {10.1145/3472735.3473389},
abstract = {The capacity in a communication network is restricted by the famous Shannon-Hartley theorem, which establishes a relationship between maximum achievable capacity, channel bandwidth, and signal-to-noise ratio of the channel. The state-of-the-art in pushing the achievable capacity close to the theoretical limit revolves around coming up with ever more efficient error correction algorithms combined with assigning the proper modulation and encoding scheme to match the conditions of the spectrum at any given point in time. In cable broadband networks, which operate under the DOCSIS protocol, a Profile Management Application (PMA) system uses telemetry collected from cable modems and cable modem termination systems (CMTSs) to dynamically assign DOCSIS profiles that constitute a combination of Forward Error Correction (FEC) configuration, a Quadrature Amplitude Modulation (QAM) level, and other protocol-based configurations. The objective behind this dynamic assignment is twofold: maximizing capacity and keeping the uncorrectable error rate at a minimal level. The current PMA implementation, adopts a rule-based approach, where pre-defined thresholds govern the decisions for adjusting the profiles. This approach, while proven to be successful, limits opportunities to fully realize optimal DOCSIS configurations to bring system performance closer to the Shannon limit. Through a reinforcement learning (RL) implementation of PMA, it is possible to substitute the pre-defined rules for a system that learns to select the optimal configuration at each decision point, based on past outcomes and potential future rewards. In this paper, we focus on designing an RL-based PMA system to manage DOCSIS 3.0 upstream configurations.},
booktitle = {Proceedings of the 4th FlexNets Workshop on Flexible Networks Artificial Intelligence Supported Network Flexibility and Agility},
pages = {50–55},
numpages = {6},
location = {Virtual Event, USA},
series = {FlexNets '21}
}

@article{10.1145/3467979,
author = {Zheng, Bolong and Ming, Lingfeng and Hu, Qi and L\"{u}, Zhipeng and Liu, Guanfeng and Zhou, Xiaofang},
title = {Supply-Demand-Aware Deep Reinforcement Learning for Dynamic Fleet Management},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3467979},
doi = {10.1145/3467979},
abstract = {Online ride-hailing platforms have reduced significantly the amounts of the time that taxis are idle and that passengers spend on waiting. As a key component of these platforms, the fleet management problem can be naturally modeled as a Markov Decision Process, which enables us to use the deep reinforcement learning. However, existing studies are proposed based on simplified problem settings that fail to model the complicated supply-dynamics and restrict the performance in the real traffic environment. In this article, we propose a supply-demand-aware deep reinforcement learning algorithm for taxi dispatching, where we use a deep Q-network with action sampling policy, called AS-DQN, to learn an optimal dispatching policy. Furthermore, we utilize a dueling network architecture, called AS-DDQN, to improve the performance of AS-DQN. Extensive experiments on real-world datasets offer insight into the performance of our model and show that it is capable of outperforming the baseline approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jan},
articleno = {37},
numpages = {19},
keywords = {Trajectory, fleet management, deep reinforcement learning}
}

@inproceedings{10.5555/3535850.3536010,
author = {Yang, Jiachen and Wang, Ethan and Trivedi, Rakshit and Zhao, Tuo and Zha, Hongyuan},
title = {Adaptive Incentive Design with Multi-Agent Meta-Gradient Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Critical sectors of human society are progressing toward the adoption of powerful artificial intelligence (AI) agents, which are trained individually on behalf of self-interested principals but deployed in a shared environment. Short of direct centralized regulation of AI, which is as difficult an issue as regulation of human actions, one must design institutional mechanisms that indirectly guide agents' behaviors to safeguard and improve social welfare in the shared environment. Our paper focuses on one important class of such mechanisms: the problem of adaptive incentive design, whereby a central planner intervenes on the payoffs of an agent population via incentives in order to optimize a system objective. To tackle this problem in high-dimensional environments whose dynamics may be unknown or too complex to model, we propose a model-free meta-gradient method to learn an adaptive incentive function in the context of multi-agent reinforcement learning. Via the principle of online cross-validation, the incentive designer explicitly accounts for its impact on agents' learning and, through them, the impact on future social welfare. Experiments on didactic benchmark problems show that the proposed method can induce selfish agents to learn near-optimal cooperative behavior and significantly outperform learning-oblivious baselines. When applied to a complex simulated economy, the proposed method finds tax policies that achieve better trade-off between economic productivity and equality than baselines, a result that we interpret via a detailed behavioral analysis.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1436–1445},
numpages = {10},
keywords = {incentive design, multi-agent reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3194085.3194088,
author = {Spryn, Mitchell and Sharma, Aditya and Parkar, Dhawal and Shrimal, Madhur},
title = {Distributed Deep Reinforcement Learning on the Cloud for Autonomous Driving},
year = {2018},
isbn = {9781450357395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194085.3194088},
doi = {10.1145/3194085.3194088},
abstract = {This paper proposes an architecture for leveraging cloud computing technology to reduce training time for deep reinforcement learning models for autonomous driving by distributing the training process across a pool of virtual machines. By parallelizing the training process, careful design of the reward function and use of techniques like transfer learning, we demonstrate a decrease in training time for our example autonomous driving problem from 140 hours to less than 1 hour. We go over our network architecture, job distribution paradigm, reward function design and report results from experiments on small sized cluster (1--6 training nodes) of machines. We also discuss the limitations of our approach when trying to scale up to massive clusters.},
booktitle = {Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous Systems},
pages = {16–22},
numpages = {7},
keywords = {deep reinforcement learning, autonomous driving, distributed machine learning, cloud computing, simulation},
location = {Gothenburg, Sweden},
series = {SEFAIS '18}
}

@article{10.1145/3520434,
author = {Pozzi, Matteo G. and Herbert, Steven J. and Sengupta, Akash and Mullins, Robert D.},
title = {Using Reinforcement Learning to Perform Qubit Routing in Quantum Compilers},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/3520434},
doi = {10.1145/3520434},
abstract = {‘‘Qubit routing” refers to the task of modifying quantum circuits so that they satisfy the connectivity constraints of a target quantum computer. This involves inserting SWAP gates into the circuit so that the logical gates only ever occur between adjacent physical qubits. The goal is to minimise the circuit depth added by the SWAP gates.In this article, we propose a qubit routing procedure that uses a modified version of the deep Q-learning paradigm. The system is able to outperform the qubit routing procedures from two of the most advanced quantum compilers currently available (Qiskit and t ( | ) ket ( rangle ) ), on both random and realistic circuits, across a range of near-term architecture sizes (with up to 50 qubits).},
journal = {ACM Transactions on Quantum Computing},
month = {may},
articleno = {10},
numpages = {25},
keywords = {deep learning, tket, qubit mapping, Q-learning, machine learning, qiskit, qubit routing, simulated annealing, neural networks, quantum circuits}
}

@inproceedings{10.1145/3319619.3326882,
author = {Ji, Hong-Ming and Chen, Liang-Yu and Hsiao, Tzu-Chien},
title = {Real-Time Detection of Internet Addiction Using Reinforcement Learning System},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3326882},
doi = {10.1145/3319619.3326882},
abstract = {1Since Internet addiction (IA) was reported in 1996, research on IA assessment has attracted considerable interest. The development of a real-time detector system can help communities, educational institutes, or clinics immediately assess the risk of IA in Internet users. However, current questionnaires were designed to ask Internet users to self-report their Internet experiences for at least 6 months. Physiological measurements were used to assist questionnaires in the short-term assessment of IA, but physiological properties cannot assess IA in real-time due to a lack of algorithms. Therefore, the real-time detection of IA is still a work in progress. In this study, we adopted an extended classifier system with continuous real-coded variables (XCSR), which can solve the non-Markovian problem with continuous real-values to produce optimal policy, and determine high-risk and low-risk IA using Chen Internet addiction scale (CIAS) data or respiratory instantaneous frequency (IF) components of Internet users as input information. The result shows that the classification accuracy of XCSR can reach close to 100\%. We also used XCSR to verify the items of CIAS and extract important respiratory indexes to assess IA. We expect that a real-time detector that immediately assesses the risk of IA may be designed in this way.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1280–1288},
numpages = {9},
keywords = {internet addiction, extended classifier system with continuous real-coded variables, reinforcement learning system, instantaneous respiratory frequency},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.1145/3591106.3592288,
author = {Alexoudi, Panagiota and Mademlis, Ioannis and Pitas, Ioannis},
title = {Escaping Local Minima in Deep Reinforcement Learning for Video Summarization},
year = {2023},
isbn = {9798400701788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591106.3592288},
doi = {10.1145/3591106.3592288},
abstract = {State-of-the-art deep neural unsupervised video summarization methods mostly fall under the adversarial reconstruction framework. This employs a Generative Adversarial Network (GAN) structure and Long Short-Term Memory (LSTM) autoencoders during its training stage. The typical result is a selector LSTM that sequentially receives video frame representations and outputs corresponding scalar importance factors, which are then used to select key-frames. This basic approach has been augmented with an additional Deep Reinforcement Learning (DRL) agent, trained using the Discriminator’s output as a reward, which learns to optimize the selector’s outputs. However, local minima are a well-known problem in DRL. Thus, this paper presents a novel regularizer for escaping local loss minima, in order to improve unsupervised key-frame extraction. It is an additive loss term employed during a second training phase, that rewards the difference of the neural agent’s parameters from those of a previously found good solution. Thus, it encourages the training process to explore more aggressively the parameter space in order to discover a better local loss minimum. Evaluation performed on two public datasets shows considerable increases over the baseline and against the state-of-the-art.},
booktitle = {Proceedings of the 2023 ACM International Conference on Multimedia Retrieval},
pages = {530–534},
numpages = {5},
keywords = {deep reinforcement learning, unsupervised learning, key-frame extraction, video summarization},
location = {Thessaloniki, Greece},
series = {ICMR '23}
}

@inproceedings{10.1145/3109859.3109914,
author = {Zhang, Yang and Zhang, Chenwei and Liu, Xiaozhong},
title = {Dynamic Scholarly Collaborator Recommendation via Competitive Multi-Agent Reinforcement Learning},
year = {2017},
isbn = {9781450346528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109859.3109914},
doi = {10.1145/3109859.3109914},
abstract = {In an interdisciplinary environment, scientific collaboration is becoming increasingly important. Helping scholars make a right choice of potential collaborators is essential in achieving scientific success. Intuitively, the generation of collaboration relationship is a dynamic process. For instance, one scholar may first choose to work with Scholar A, and then work with Scholar B after accumulating additional academic credits. To address this property, we propose a novel dynamic collaboration recommendation method by adapting the multi-agent reinforcement learning technique to the coauthor network analysis. The collaborator selection is optimized from several different scholar similarity measurements. Unlike prior studies, the proposed method characterizes scholarly competition, a.k.a. different scholars will compete for potential collaborator at each iteration. An evaluation with the ACM data shows that multi-agent reinforcement learning plus scholarly competition modeling can be significant for collaboration recommendation.},
booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
pages = {331–335},
numpages = {5},
keywords = {reinforcement learning, multi-agent, collaborator recommendation, dynamic, competition},
location = {Como, Italy},
series = {RecSys '17}
}

@inproceedings{10.1145/1502650.1502700,
author = {Atrash, Amin and Pineau, Joelle},
title = {A Bayesian Reinforcement Learning Approach for Customizing Human-Robot Interfaces},
year = {2009},
isbn = {9781605581682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1502650.1502700},
doi = {10.1145/1502650.1502700},
abstract = {Personal robots are becoming increasingly prevalent, which raises a number of interesting issues regarding the design and customization of interfaces to such platforms. The particular problem addressed by this paper is the use of learning methods to improve the quality and effectiveness of human-machine interaction onboard a robotic wheelchair. In support of this, we present a method for learning and adapting probabilistic models with the aid of a human operator. We use a Bayesian reinforcement learning framework, that allows us to mix learning and execution, as well as take advantage of prior information about the world. We address the problems of learning, handling a partially observable environment, and limiting the number of action requests. We demonstrate empirical feasibility of our approach on an interface for an autonomous wheelchair.},
booktitle = {Proceedings of the 14th International Conference on Intelligent User Interfaces},
pages = {355–360},
numpages = {6},
keywords = {intelligent interfaces for ubiquitous computing, intelligent assistants, activity \&amp; plan recognition},
location = {Sanibel Island, Florida, USA},
series = {IUI '09}
}

@inproceedings{10.1145/3545008.3545012,
author = {Lu, Kai and Li, Guokuan and Wan, Jiguang and Ma, Ruixiang and Zhao, Wei},
title = {ADSTS: Automatic Distributed Storage Tuning System Using Deep Reinforcement Learning},
year = {2023},
isbn = {9781450397339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545008.3545012},
doi = {10.1145/3545008.3545012},
abstract = {Modern distributed storage systems with the immense number of configurations, unpredictable workloads and difficult performance evaluation pose higher requirements to parameter tuning. Providing an automatic parameter tuning solution for distributed storage systems is in demand. Lots of researches have attempted to build automatic tuning systems based on deep reinforcement learning (RL). However, they have several limitations in the face of these requirements, including lack of parameter spaces processing, less advanced RL models and time-consuming and unstable training process. In this paper, we present and evaluate the ADSTS, which is an automatic distributed storage tuning system based on deep reinforcement learning. A general preprocessing guideline is first proposed to generate standardized tunable parameter domain. Thereinto, Recursive Stratified Sampling without the nonincremental nature is designed to sample huge parameter spaces and Lasso regression is adopted to identify important parameters. Besides, the twin-delayed deep deterministic policy gradient method is utilized to find the optimal values of tunable parameters. Finally, Multi-processing Training and Workload-directed Model Fine-tuning are adopted to accelerate the model convergence. ADSTS is implemented on Park and is used in the real-world system Ceph. The evaluation results show that ADSTS can recommend near-optimal configurations and improve system performance by 1.5 \texttimes{} ∼2.5 \texttimes{} with acceptable overheads.},
booktitle = {Proceedings of the 51st International Conference on Parallel Processing},
articleno = {25},
numpages = {13},
keywords = {Parameter Identification, Auto-tuning, Reinforcement Learning, Distributed Storage System},
location = {Bordeaux, France},
series = {ICPP '22}
}

@inproceedings{10.1145/3331184.3331370,
author = {Chen, Cen and Fu, Chilin and Hu, Xu and Zhang, Xiaolu and Zhou, Jun and Li, Xiaolong and Bao, Forrest Sheng},
title = {Reinforcement Learning for User Intent Prediction in Customer Service Bots},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331370},
doi = {10.1145/3331184.3331370},
abstract = {A customer service bot is now a necessary component of an e-commerce platform. As a core module of the customer service bot, user intent prediction can help predict user questions before they ask. A typical solution is to find top candidate questions that a user will be interested in. Such solution ignores the inter-relationship between questions and often aims to maximize the immediate reward such as clicks, which may not be ideal in practice. Hence, we propose to view the problem as a sequential decision making process to better capture the long-term effects of each recommendation in the list. Intuitively, we formulate the problem as a Markov decision process and consider using reinforcement learning for the problem. With this approach, questions presented to users are both relevant and diverse. Experiments on offline real-world dataset and online system demonstrate the effectiveness of our proposed approach.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1265–1268},
numpages = {4},
keywords = {customer service bots, user intent prediction, reinforcement learning},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.5555/3581644.3581726,
author = {Corcoran, Diarmuid and Kreuger, Per and Boman, Magnus},
title = {A Sample Efficient Multi-Agent Approach to Continuous Reinforcement Learning},
year = {2023},
isbn = {9783903176515},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {As design, deployment and operation complexity increase in mobile systems, adaptive self-learning techniques have become essential enablers in mitigation and control of the complexity problem. Artificial intelligence and, in particular, reinforcement learning has shown great potential in learning complex tasks through observations. The majority of ongoing reinforcement learning research activities focus on single-agent problem settings with an assumption of accessibility to a globally observable state and action space. In many real-world settings, such as LTE or 5G, decision making is distributed and there is often only local accessibility to the state space. In such settings, multi-agent learning may be preferable, with the added challenge of ensuring that all agents collaboratively work towards achieving a common goal. We present a novel cooperative and distributed actor-critic multi-agent reinforcement learning algorithm. We claim the approach is sample efficient, both in terms of selecting observation samples and in terms of assignment of credit between subsets of collaborating agents.},
booktitle = {Proceedings of the 18th International Conference on Network and Service Management},
articleno = {65},
numpages = {7},
keywords = {radio resource scheduling, machine learning},
location = {Thessaloniki, Greece},
series = {CNSM '22}
}

@article{10.1145/3058592,
author = {Wang, Hongbign and Chen, Xin and Wu, Qin and Yu, Qi and Hu, Xingguo and Zheng, Zibin and Bouguettaya, Athman},
title = {Integrating Reinforcement Learning with Multi-Agent Techniques for Adaptive Service Composition},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3058592},
doi = {10.1145/3058592},
abstract = {Service-oriented architecture is a widely used software engineering paradigm to cope with complexity and dynamics in enterprise applications. Service composition, which provides a cost-effective way to implement software systems, has attracted significant attention from both industry and research communities. As online services may keep evolving over time and thus lead to a highly dynamic environment, service composition must be self-adaptive to tackle uninformed behavior during the evolution of services. In addition, service composition should also maintain high efficiency for large-scale services, which are common for enterprise applications. This article presents a new model for large-scale adaptive service composition based on multi-agent reinforcement learning. The model integrates reinforcement learning and game theory, where the former is to achieve adaptation in a highly dynamic environment and the latter is to enable agents to work for a common task (i.e., composition). In particular, we propose a multi-agent Q-learning algorithm for service composition, which is expected to achieve better performance when compared with the single-agent Q-learning method and multi-agent SARSA (State-Action-Reward-State-Action) method. Our experimental results demonstrate the effectiveness and efficiency of our approach.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {may},
articleno = {8},
numpages = {42},
keywords = {Service composition, multi-agent system, reinforcement learning, game theory}
}

@inproceedings{10.1145/3401071.3401657,
author = {Guo, Runsheng Benson and Daudjee, Khuzaima},
title = {Research Challenges in Deep Reinforcement Learning-Based Join Query Optimization},
year = {2020},
isbn = {9781450380294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401071.3401657},
doi = {10.1145/3401071.3401657},
abstract = {The order in which relations are joined and the physical join operators used are two aspects of query plans which have a significant impact on the execution latency of join queries. However, the set of valid query plans grows exponentially with the number of relations to be joined. Hence, it becomes computationally expensive to enumerate all such plans for a complex join query. Recently, several deep reinforcement learning (DRL) based approaches propose using neural networks to construct a query plan. They demonstrate that efficient query plans can be found without exhaustively enumerating the search space. We integrated our implementation of a DRL-based solution to optimize join order and operators into the PostgreSQL query optimizer. In practice, we found limitations in the quality of the query plans chosen which are not addressed in existing approaches. In this paper we highlight some of these limitations and propose future research challenges along with potential solutions.},
booktitle = {Proceedings of the Third International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {3},
numpages = {6},
location = {Portland, Oregon},
series = {aiDM '20}
}

@inproceedings{10.1109/WIIAT.2008.77,
author = {Gomes, Eduardo Rodrigues and Kowalczyk, Ryszard},
title = {Non-Symmetric Preferences in the IPA Market with Reinforcement Learning},
year = {2008},
isbn = {9780769534961},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WIIAT.2008.77},
doi = {10.1109/WIIAT.2008.77},
abstract = {Machine Learning has been proposed to support and optimize market-based resource allocation. In particular, Reinforcement Learning (RL) has been used to improve the allocation in terms of the utility received by resource requesting agents in the Iterative Price Adjustment (IPA) mechanism. In such an approach, utility functions describe the agents' preferences for resource attributes and are the basis for RL to learn demand functions that are optimized for the market. It has been shown that the reward functions based on the individual utility of the agents and the social welfare of the allocation can deliver similar social results when the market consists only of learning agents with symmetric preferences. In this paper we investigate the IPA market-based resource allocation with RL for the case of agents with non-symmetric preferences. We show through experimental investigation that the results observed above are also held in this case. In particular, we show that the individual-based reward function is able to approximate the solution to the fairest Pareto-Optimal allocation in situations where the social-based reward function fails.},
booktitle = {Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {424–430},
numpages = {7},
keywords = {Market-based Resource Allocation, Iterative Price Adjustment, Social Rewards, Reinforcement Learning, Individual},
series = {WI-IAT '08}
}

@inproceedings{10.5555/3091125.3091207,
author = {Veeriah, Vivek and van Seijen, Harm and Sutton, Richard S.},
title = {Forward Actor-Critic for Nonlinear Function Approximation in Reinforcement Learning},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-step methods are important in reinforcement learn- ing (RL). Eligibility traces, the usual way of handling them, works well with linear function approximators. Recently, van Seijen (2016) had introduced a delayed learning approach, without eligibility traces, for handling the multi-step λ-return with nonlinear function approximators. However, this was limited to action-value methods. In this paper, we extend this approach to handle n-step returns, generalize this approach to policy gradient methods and empirically study the effect of such delayed updates in control tasks. Specifically, we introduce two novel forward actor- critic methods and empirically investigate our proposed methods with the conventional actor-critic method on mountain car and pole-balancing tasks. From our experiments, we observe that forward actor-critic dramatically outperforms the conventional actor-critic in these standard control tasks. Notably, this forward actor-critic method has produced a new class of multi-step RL algorithms without eligibility traces.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {556–564},
numpages = {9},
keywords = {policy gradient, incremental learning, nonlinear function approximation, actor-critic, reinforcement learning},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@inproceedings{10.5555/2380943.2380946,
author = {Papangelis, Alexandros},
title = {A Comparative Study of Reinforcement Learning Techniques on Dialogue Management},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Adaptive Dialogue Systems are rapidly becoming part of our everyday lives. As they progress and adopt new technologies they become more intelligent and able to adapt better and faster to their environment. Research in this field is currently focused on how to achieve adaptation, and particularly on applying Reinforcement Learning (RL) techniques, so a comparative study of the related methods, such as this, is necessary. In this work we compare several standard and state of the art online RL algorithms that are used to train the dialogue manager in a dynamic environment, aiming to aid researchers/developers choose the appropriate RL algorithm for their system. This is the first work, to the best of our knowledge, to evaluate online RL algorithms on the dialogue problem and in a dynamic environment.},
booktitle = {Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics},
pages = {22–31},
numpages = {10},
location = {Avignon, France},
series = {EACL '12}
}

@inproceedings{10.5555/3463952.3464053,
author = {Lyu, Xueguang and Xiao, Yuchen and Daley, Brett and Amato, Christopher},
title = {Contrasting Centralized and Decentralized Critics in Multi-Agent Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Centralized Training for Decentralized Execution, where agents are trained offline using centralized information but execute in a decentralized manner online, has gained popularity in the multi-agent reinforcement learning community. In particular, actor-critic methods with a centralized critic and decentralized actors are a common instance of this idea. However, the implications of using a centralized critic in this context are not fully discussed and understood even though it is the standard choice of many algorithms. We therefore formally analyze centralized and decentralized critic approaches, providing a deeper understanding of the implications of critic choice. Because our theory makes unrealistic assumptions, we also empirically compare the centralized and decentralized critic methods over a wide set of environments to validate our theories and to provide practical advice. We show that there exist misconceptions regarding centralized critics in the current literature and show that the centralized critic design is not strictly beneficial, but rather both centralized and decentralized critics have different pros and cons that should be taken into account by algorithm designers.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {844–852},
numpages = {9},
keywords = {policy gradient, multi-agent system, reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/2576768.2598358,
author = {Koutn\'{\i}k, Jan and Schmidhuber, Juergen and Gomez, Faustino},
title = {Evolving Deep Unsupervised Convolutional Networks for Vision-Based Reinforcement Learning},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598358},
doi = {10.1145/2576768.2598358},
abstract = {Dealing with high-dimensional input spaces, like visual input, is a challenging task for reinforcement learning (RL). Neuroevolution (NE), used for continuous RL problems, has to either reduce the problem dimensionality by (1) compressing the representation of the neural network controllers or (2) employing a pre-processor (compressor) that transforms the high-dimensional raw inputs into low-dimensional features. In this paper, we are able to evolve extremely small recurrent neural network (RNN) controllers for a task that previously required networks with over a million weights. The high-dimensional visual input, which the controller would normally receive, is first transformed into a compact feature vector through a deep, max-pooling convolutional neural network (MPCNN). Both the MPCNN preprocessor and the RNN controller are evolved successfully to control a car in the TORCS racing simulator using only visual input. This is the first use of deep learning in the context evolutionary RL.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {541–548},
numpages = {8},
keywords = {games, neuroevolution, reinforcement learning, deep learning, vision-based torcs},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.1145/3292500.3330933,
author = {Shang, Wenjie and Yu, Yang and Li, Qingyang and Qin, Zhiwei and Meng, Yiping and Ye, Jieping},
title = {Environment Reconstruction with Hidden Confounders for Reinforcement Learning Based Recommendation},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330933},
doi = {10.1145/3292500.3330933},
abstract = {Reinforcement learning aims at searching the best policy model for decision making, and has been shown powerful for sequential recommendations. The training of the policy by reinforcement learning, however, is placed in an environment. In many real-world applications, however, the policy training in the real environment can cause an unbearable cost, due to the exploration in the environment. Environment reconstruction from the past data is thus an appealing way to release the power of reinforcement learning in these applications. The reconstruction of the environment is, basically, to extract the casual effect model from the data. However, real-world applications are often too complex to offer fully observable environment information. Therefore, quite possibly there are unobserved confounding variables lying behind the data. The hidden confounder can obstruct an effective reconstruction of the environment. In this paper, by treating the hidden confounder as a hidden policy, we propose a deconfounded multi-agent environment reconstruction (DEMER) approach in order to learn the environment together with the hidden confounder. DEMER adopts a multi-agent generative adversarial imitation learning framework. It proposes to introduce the confounder embedded policy, and use the compatible discriminator for training the policies. We then apply DEMER in an application of driver program recommendation. We firstly use an artificial driver program recommendation environment, abstracted from the real application, to verify and analyze the effectiveness of DEMER. We then test DEMER in the real application of Didi Chuxing. Experiment results show that DEMER can effectively reconstruct the hidden confounder, and thus can build the environment better. DEMER also derives a recommendation policy with a significantly improved performance in the test phase of the real application.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {566–576},
numpages = {11},
keywords = {imitation learning, recommendation, hidden confounder, reinforcement learning, environment reconstruction},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3405837.3411396,
author = {Jog, Suraj and Liu, Zikun and Franques, Antonio and Fernando, Vimuth and Hassanieh, Haitham and Abadal, Sergi and Torrellas, Josep},
title = {Millimeter Wave Wireless Network on Chip Using Deep Reinforcement Learning},
year = {2021},
isbn = {9781450380485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405837.3411396},
doi = {10.1145/3405837.3411396},
abstract = {Wireless Network-on-Chip (NoC) has emerged as a promising solution to scale chip multi-core processors to hundreds of cores. However, traditional medium access protocols fall short here since the traffic patterns on wireless NoCs tend to be very dynamic and can change drastically across different cores, different time intervals and different applications. In this work, we present NeuMAC, a unified approach that combines networking, architecture and AI to generate highly adaptive medium access protocols that can learn and optimize for the structure, correlations and statistics of the traffic patterns on the NoC. Our results show that NeuMAC can quickly adapt to NoC traffic to provide significant gains in terms of latency and overall execution time, improving the execution time by up to 1.69X - 3.74X.},
booktitle = {Proceedings of the SIGCOMM '20 Poster and Demo Sessions},
pages = {70–72},
numpages = {3},
keywords = {millimeter wave, deep reinforcement learning, wireless network-on-chip},
location = {Virtual event},
series = {SIGCOMM '20}
}

@inproceedings{10.5555/3398761.3398858,
author = {Ma, Jinming and Wu, Feng},
title = {Feudal Multi-Agent Deep Reinforcement Learning for Traffic Signal Control},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reinforcement learning (RL) is a promising technique for optimizing traffic signal controllers that dynamically respond to real-time traffic conditions. Recent efforts that applied Multi-Agent RL (MARL) to this problem have shown remarkable improvement over centralized RL, with the scalability to solve large problems by distributing the global control to local RL agents. Unfortunately, it is also easy to get stuck in local optima because each agent only has partial observability of the environment with limited communication. To tackle this, we borrow ideas from feudal RL and propose a novel MARL approach combining with the feudal hierarchy. Specifically, we split the traffic network into several regions, where each region is controlled by a manager agent and the agents who control the traffic signals are its workers. In our method, managers coordinate their high-level behaviors and set goals for their workers in the region, while each lower-level worker controls traffic signals to fulfill the managerial goals. By doing so, we are able to coordinate globally while retain scalability. We empirically evaluate our method both in a synthetic traffic grid and real-world traffic network using the SUMO simulator. Our experimental results show that our approach outperforms the state-of-the-art in almost all evaluation metrics commonly used for traffic signal control.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {816–824},
numpages = {9},
keywords = {multi-agent reinforcement learning, deep reinforcement learning, traffic signal control, feudal reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

