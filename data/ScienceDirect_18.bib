@article{ZHANG2020102861,
title = {Multi-vehicle routing problems with soft time windows: A multi-agent reinforcement learning approach},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {121},
pages = {102861},
year = {2020},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2020.102861},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X20307610},
author = {Ke Zhang and Fang He and Zhengchao Zhang and Xi Lin and Meng Li},
keywords = {Reinforcement learning, Vehicle routing problem, Attention mechanism, Computational efficiency, Multi-agent},
abstract = {Multi-vehicle routing problem with soft time windows (MVRPSTW) is an indispensable constituent in urban logistics distribution systems. Over the past decade, numerous methods for MVRPSTW have been proposed, but most are based on heuristic rules that require a large amount of computation time. With the current rapid increase of logistics demands, traditional methods incur the dilemma between computational efficiency and solution quality. To efficiently solve the problem, we propose a novel reinforcement learning algorithm called the Multi-Agent Attention Model that can solve routing problem instantly benefit from lengthy offline training. Specifically, the vehicle routing problem is regarded as a vehicle tour generation process, and an encoder-decoder framework with attention layers is proposed to generate tours of multiple vehicles iteratively. Furthermore, a multi-agent reinforcement learning method with an unsupervised auxiliary network is developed for the model training. By evaluated on four synthetic networks with different scales, the results demonstrate that the proposed method consistently outperforms Google OR-Tools and traditional methods with little computation time. In addition, we validate the robustness of the well-trained model by varying the number of customers and the capacities of vehicles.}
}
@article{MA2021107204,
title = {Adaptive model-free fault-tolerant control based on integral reinforcement learning for a highly flexible aircraft with actuator faults},
journal = {Aerospace Science and Technology},
volume = {119},
pages = {107204},
year = {2021},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2021.107204},
url = {https://www.sciencedirect.com/science/article/pii/S1270963821007148},
author = {Jianjun Ma and Chi Peng},
keywords = {Model-free, Fault-tolerant control, Integral reinforcement learning, Highly flexible aircraft, Observer-like reference model},
abstract = {This article presents an adaptive model-free fault-tolerant control scheme based on integral reinforcement learning (IRL) technique for tracking control of a highly flexible aircraft (HFA) with actuator faults. To begin, the integral of the tracking error is introduced as a new state to construct an augmented system for the control design. Following that, the off-policy IRL method is applied to obtain the optimal feedback control law online in order to solve the tracking problem without system knowledge. Furthermore, in order to effectively handle system uncertainties, external disturbances, and actuator faults, an adaptive model-free fault-tolerant controller with an observer-like reference model is developed. The designed controller can guarantee that the closed-loop system is uniformly bounded and the tracking error asymptotically approaches zero by choosing design parameters appropriately. Finally, numerical simulations demonstrate the desirable fault accommodation capability of the proposed fault-tolerant strategy.}
}
@article{WALTZ2023634,
title = {Spatial–temporal recurrent reinforcement learning for autonomous ships},
journal = {Neural Networks},
volume = {165},
pages = {634-653},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300326X},
author = {Martin Waltz and Ostap Okhrin},
keywords = {Deep reinforcement learning, Recurrency, Autonomous surface vehicle, COLREG},
abstract = {This paper proposes a spatial–temporal recurrent neural network architecture for deep Q-networks that can be used to steer an autonomous ship. The network design makes it possible to handle an arbitrary number of surrounding target ships while offering robustness to partial observability. Furthermore, a state-of-the-art collision risk metric is proposed to enable an easier assessment of different situations by the agent. The COLREG rules of maritime traffic are explicitly considered in the design of the reward function. The final policy is validated on a custom set of newly created single-ship encounters called ‘Around the Clock’ problems and the commonly used Imazu (1987) problems, which include 18 multi-ship scenarios. Performance comparisons with artificial potential field and velocity obstacle methods demonstrate the potential of the proposed approach for maritime path planning. Furthermore, the new architecture exhibits robustness when it is deployed in multi-agent scenarios and it is compatible with other deep reinforcement learning algorithms, including actor-critic frameworks.}
}
@article{WANG2022931,
title = {Simultaneous task and energy planning using deep reinforcement learning},
journal = {Information Sciences},
volume = {607},
pages = {931-946},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522005977},
author = {Di Wang and Mengqi Hu and Jeffery D. Weir},
keywords = {Simultaneous task and energy planning, Neural combinatorial optimization, Deep reinforcement learning, End-to-end learning, Sequence-to-sequence decision},
abstract = {To improve energy awareness of unmanned autonomous vehicles, it is critical to co-optimize task planning and energy scheduling. To the best of our knowledge, most of the existing task planning algorithms either ignore energy constraints or make energy scheduling decisions based on simple rules. To bridge these research gaps, we propose a combinatorial optimization model for the simultaneous task and energy planning (STEP) problem. In this paper, we propose three variants of STEP problems (i) the vehicle can visit stationary charging stations multiple times at various locations; (ii) the vehicle can efficiently coordinate with mobile charging stations to achieve zero waiting time for recharging, and (iii) the vehicle can maximally harvest solar energy by considering time variance in solar irradiance. Besides, in order to obtain fast and reliable solutions to STEP problems, we propose a neural combinatorial optimizer using the deep reinforcement learning algorithm with a proposed link information filter. The near-optimal solutions can be obtained very fast without solving the problem from scratch when environments change. Our simulation results demonstrate that (i) our proposed neural optimizer can find solutions close to the optimum and outperform the exact and heuristic algorithms in terms of computational cost; (ii) the end-to-end learning (directly mapping from perceptions to control) model outperforms the traditional learning (mapping from perception to prediction to control) model.}
}
@article{OVERENG2021109433,
title = {Dynamic Positioning using Deep Reinforcement Learning},
journal = {Ocean Engineering},
volume = {235},
pages = {109433},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109433},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821008398},
author = {Simen Sem Øvereng and Dong Trong Nguyen and Geir Hamre},
keywords = {Dynamic Positioning, Deep Reinforcement Learning, Proximal policy optimization, Reward shaping},
abstract = {This paper demonstrates the implementation and performance testing of a Deep Reinforcement Learning based control scheme used for Dynamic Positioning of a marine surface vessel. The control scheme encapsulated motion control and control allocation by using a neural network, which was trained on a digital twin without having any prior knowledge of the system dynamics, using the Proximal Policy Optimization learning algorithm. By using a multivariate Gaussian reward function for rewarding small errors between the vessel and the various setpoints, while encouraging small actuator outputs, the proposed Deep Reinforcement Learning based control scheme showed good positioning performance while being energy efficient. Both simulations and model scale sea trials were carried out to demonstrate performance compared to traditional methods, and to evaluate the ability of neural networks trained in simulation to perform on real life systems.}
}
@article{CHEN2021108186,
title = {Edge intelligence computing for mobile augmented reality with deep reinforcement learning approach},
journal = {Computer Networks},
volume = {195},
pages = {108186},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108186},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621002425},
author = {Miaojiang Chen and Wei Liu and Tian Wang and Anfeng Liu and Zhiwen Zeng},
keywords = {Beyond fifth-generation, Mobile augmented reality, Markov decision process, Deep reinforcement learning, Artificial intelligence},
abstract = {Convergence of Augmented Reality (AR) and Next Generation Internet-of-Things (NG-IoT) can create new opportunities in many emerging areas, where the real-time data can be visualized on the devices. Integrated NG-IoT network, AR can improve efficiency in many fields such as mobile computing, smart city, intelligent transportation and telemedicine. However, limited by capability of mobile device, the reliability and latency requirements of AR applications is difficult to meet by local processing. To solve this problem, we study a binary offloading scheme for AR edge computing. Based on the proposed model, the parts of AR computing can offload to edge network servers, which is extend the computing capability of mobile AR devices. Moreover, a deep reinforcement learning offloading model is considered to acquire B5G network resource allocation and optimally AR offloading decisions. First, this offloading model does not need to solve combinatorial optimization, which is greatly reduced the computational complexity. Then the wireless channel gains and binary offloading states is modeled as a Markov decision process, and solved by deep reinforcement learning. Numerical results show that our scheme can achieve better performance compared with existing optimization methods.}
}
@article{BO2023108413,
title = {Control invariant set enhanced safe reinforcement learning: Improved sampling efficiency, guaranteed stability and robustness},
journal = {Computers & Chemical Engineering},
volume = {179},
pages = {108413},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108413},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423002831},
author = {Song Bo and Bernard T. Agyeman and Xunyuan Yin and Jinfeng Liu},
keywords = {Advanced process control, Robustness control invariant set, Reinforcement learning, Closed-loop stability, Sampling efficiency},
abstract = {Reinforcement learning (RL) is an area of significant research interest, and safe RL in particular is attracting attention due to its ability to handle safety-driven constraints that are crucial for real-world applications. This work proposes a novel approach to RL training, called control invariant set (CIS) enhanced RL, which leverages the advantages of utilizing the explicit form of CIS to improve stability guarantees and sampling efficiency. Furthermore, the robustness of the proposed approach is investigated in the presence of uncertainty. The approach consists of two learning stages: offline and online. In the offline stage, CIS is incorporated into the reward design, initial state sampling, and state reset procedures. This incorporation of CIS facilitates improved sampling efficiency during the offline training process. In the online stage, RL is retrained whenever the predicted next step state is outside of the CIS, which serves as a stability criterion, by introducing a Safety Supervisor to examine the safety of the action and make necessary corrections. The stability analysis is conducted for both cases, with and without uncertainty. To evaluate the proposed approach, we apply it to a simulated chemical reactor. The results show a significant improvement in sampling efficiency during offline training and closed-loop stability guarantee in the online implementation, with and without uncertainty.}
}
@article{ZHOU2021120118,
title = {A novel energy management strategy of hybrid electric vehicle via an improved TD3 deep reinforcement learning},
journal = {Energy},
volume = {224},
pages = {120118},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.120118},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221003674},
author = {Jianhao Zhou and Siwu Xue and Yuan Xue and Yuhui Liao and Jun Liu and Wanzhong Zhao},
keywords = {Hybrid electric vehicle, Energy management strategy, Deep reinforcement learning, TD3},
abstract = {The formulation of high-efficient energy management strategy (EMS) for hybrid electric vehicles (HEVs) becomes the most crucial task owing to the variation of electrified powertrain topology and uncertainty of driving scenarios. In this study, a deep reinforcement learning (DRL) algorithm, namely TD3, is leveraged to derivate intelligent EMS for HEV. A heuristic rule-based local controller (LC) is embedded within the DRL loop to eliminate irrational torque allocation with considering the characteristics of powertrain components. In order to resolve the influence of environmental disturbance, a hybrid experience replay (HER) method is proposed based on a mixed experience buffer (MEB) consisting of offline computed optimal experience and online learned experience. The results indicate that improved TD3 based EMS obtained the best fuel optimality, fastest convergence speed and highest robustness in comparison to typical value-based and policy-based DRL EMSs under various driving cycles. LC leads to a boosting effect on the convergence speed of TD3-based EMS wherein a “warm” start of exploring is exhibited. Meanwhile, by incorporating HER coupled with MEB, the impact of environmental disturbance including load mass and road gradient, as an increase of input observations, can be negligible to the performance of TD3-based EMS.}
}
@article{GAMA2021105357,
title = {A reinforcement learning approach to the orienteering problem with time windows},
journal = {Computers & Operations Research},
volume = {133},
pages = {105357},
year = {2021},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2021.105357},
url = {https://www.sciencedirect.com/science/article/pii/S0305054821001349},
author = {Ricardo Gama and Hugo {L. Fernandes}},
keywords = {Machine learning, Combinatorial optimization},
abstract = {The Orienteering Problem with Time Windows (OPTW) is a combinatorial optimization problem where the goal is to maximize the total score collected from different visited locations. The application of neural network models to combinatorial optimization has recently shown promising results in dealing with similar problems, like the Travelling Salesman Problem. A neural network allows learning solutions using reinforcement learning or supervised learning, depending on the available data. After the learning stage, it can be generalized and quickly fine-tuned to further improve performance and personalization. The advantages are evident since, for real-world applications, solution quality, personalization, and execution times are all important factors that should be taken into account. This study explores the use of Pointer Network models trained using reinforcement learning to solve the OPTW problem. We propose a modified architecture that leverages Pointer Networks to better address problems related with dynamic time-dependent constraints. Among its various applications, the OPTW can be used to model the Tourist Trip Design Problem (TTDP). We train the Pointer Network with the TTDP problem in mind, by sampling variables that can change across tourists visiting a particular instance-region: starting position, starting time, available time, and the scores given to each point of interest. Once a model-region is trained, it can infer a solution for a particular tourist using beam search. We based the assessment of our approach on several existing benchmark OPTW instances. We show that it generalizes across different tourists that visit each region and that it generally outperforms the most commonly used heuristic, while computing the solution in realistic times.}
}
@article{HONG2023105805,
title = {Autonomous calibration of EFDC for predicting chlorophyll-a using reinforcement learning and a real-time monitoring system},
journal = {Environmental Modelling & Software},
volume = {168},
pages = {105805},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105805},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223001913},
author = {Seok Min Hong and Ather Abbas and Soobin Kim and Do Hyuck Kwon and Nakyung Yoon and Daeun Yun and Sanguk Lee and Yakov Pachepsky and JongCheol Pyo and Kyung Hwa Cho},
keywords = {Autonomous calibration, Cyanobacteria, Environmental fluid dynamics code, Real-time monitoring, Reinforcement learning},
abstract = {Cyanobacterial blooms cause critical damage to aquatic ecosystems and water resources. Therefore, numerical models have been utilized to simulate cyanobacteria by calibrating model parameters for accurate simulation. While conventional calibration, which uses fixed water quality parameters throughout the simulation period, is commonly utilized, it may lead to inaccurate modeling results. To address it, this study proposed a reinforcement learning and environmental fluid dynamics code (EFDC-RL) model that uses real-time pontoon monitoring data and hyperspectral images to autonomously control water quality parameters. The EFDC-RL model showed impressive performance, with an R2 value of 0.7406 and 0.4126 for the training and test datasets, respectively. In comparison, the Chlorophyll-a simulation of conventional calibration had an R2 of 0.2133 and 0.0220, respectively. This study shows that the EFDC-RL model is a suitable framework for autonomous calibration of water quality parameters and real-time spatiotemporal simulation of cyanobacteria distribution.}
}
@article{OBAYASHI201452,
title = {Assist-as-needed robotic trainer based on reinforcement learning and its application to dart-throwing},
journal = {Neural Networks},
volume = {53},
pages = {52-60},
year = {2014},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608014000240},
author = {Chihiro Obayashi and Tomoya Tamei and Tomohiro Shibata},
keywords = {Assistive robotics, Assist-as-needed, Motor skill learning, Reinforcement learning},
abstract = {This paper proposes a novel robotic trainer for motor skill learning. It is user-adaptive inspired by the assist-as-needed principle well known in the field of physical therapy. Most previous studies in the field of the robotic assistance of motor skill learning have used predetermined desired trajectories, and it has not been examined intensively whether these trajectories were optimal for each user. Furthermore, the guidance hypothesis states that humans tend to rely too much on external assistive feedback, resulting in interference with the internal feedback necessary for motor skill learning. A few studies have proposed a system that adjusts its assistive strength according to the user’s performance in order to prevent the user from relying too much on the robotic assistance. There are, however, problems in these studies, in that a physical model of the user’s motor system is required, which is inherently difficult to construct. In this paper, we propose a framework for a robotic trainer that is user-adaptive and that neither requires a specific desired trajectory nor a physical model of the user’s motor system, and we achieve this using model-free reinforcement learning. We chose dart-throwing as an example motor-learning task as it is one of the simplest throwing tasks, and its performance can easily be and quantitatively measured. Training experiments with novices, aiming at maximizing the score with the darts and minimizing the physical robotic assistance, demonstrate the feasibility and plausibility of the proposed framework.}
}
@article{HAYASHI2022101512,
title = {Graph-based reinforcement learning for discrete cross-section optimization of planar steel frames},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101512},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101512},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002603},
author = {Kazuki Hayashi and Makoto Ohsaki},
keywords = {Machine learning, Reinforcement learning, Graph embedding, Structural optimization, Cross-section optimization, Steel frame},
abstract = {A combined method of graph embedding (GE) and reinforcement learning (RL) is developed for discrete cross-section optimization of planar steel frames, in which the section size of each member is selected from a prescribed list of standard sections. The RL agent aims to minimize the total structural volume under various practical constraints. GE is a method for extracting features from data with irregular connectivity. While most of the existing GE methods aim at extracting node features, an improved GE formulation is developed for extracting features of edges associated with members in this study. Owing to the proposed GE operations, the agent is capable of grasping the structural property of columns and beams considering their connectivity in a frame with an arbitrary size as feature vectors of the same size. Using the feature vectors, the agent is trained to estimate the accurate return associated with each action and to take proper actions on which members to reduce or increase their size using an RL algorithm. The applicability of the proposed method is versatile because various frames different in the numbers of nodes and members can be used for both training and application phases. In the numerical examples, the trained agents outperform a particle swarm optimization method as a benchmark in terms of both computational cost and design quality for cross-sectional design changes; the agents successfully assign reasonable cross-sections considering the geometry, connectivity, and support and load conditions of the frames.}
}
@incollection{GOTTL20221555,
title = {Using Reinforcement Learning in a Game-like Setup for Automated Process Synthesis without Prior Process Knowledge},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {1555-1560},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50259-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596502591},
author = {Quirin Göttl and Dominik G. Grimm and Jakob Burger},
keywords = {Automated Process Synthesis, Flowsheet Synthesis, Artificial Intelligence, Machine Learning, Reinforcement Learning},
abstract = {The present work uses reinforcement learning (RL) for automated flowsheet synthesis. The task of synthesizing a flowsheet is reformulated into a two-player game, in which an agent learns by self-play without prior knowledge. The hierarchical RL scheme developed in our previous work (Göttl et al., 2021b) is coupled with an improved training process. The training process is analyzed in detail using the synthesis of ethyl tert-butyl ether (ETBE) as an example. This analysis uncovers how the agent’s evolution is driven by the two-player setup.}
}
@article{HUANG201910,
title = {Deep reinforcement learning-based joint task offloading and bandwidth allocation for multi-user mobile edge computing},
journal = {Digital Communications and Networks},
volume = {5},
number = {1},
pages = {10-17},
year = {2019},
note = {Artificial Intelligence for Future Wireless Communications and Networking},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2018.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2352864818301469},
author = {Liang Huang and Xu Feng and Cheng Zhang and Liping Qian and Yuan Wu},
keywords = {Mobile edge computing, Joint computation offloading and resource allocation, Deep-Q network},
abstract = {The rapid growth of mobile internet services has yielded a variety of computation-intensive applications such as virtual/augmented reality. Mobile Edge Computing (MEC), which enables mobile terminals to offload computation tasks to servers located at the edge of the cellular networks, has been considered as an efficient approach to relieve the heavy computational burdens and realize an efficient computation offloading. Driven by the consequent requirement for proper resource allocations for computation offloading via MEC, in this paper, we propose a Deep-Q Network (DQN) based task offloading and resource allocation algorithm for the MEC. Specifically, we consider a MEC system in which every mobile terminal has multiple tasks offloaded to the edge server and design a joint task offloading decision and bandwidth allocation optimization to minimize the overall offloading cost in terms of energy cost, computation cost, and delay cost. Although the proposed optimization problem is a mixed integer nonlinear programming in nature, we exploit an emerging DQN technique to solve it. Extensive numerical results show that our proposed DQN-based approach can achieve the near-optimal performance.}
}
@article{PENG2023108977,
title = {Supplemented with reinforcement learning to improve the detection of passive remote sensing devices},
journal = {Signal Processing},
volume = {209},
pages = {108977},
year = {2023},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2023.108977},
url = {https://www.sciencedirect.com/science/article/pii/S0165168423000518},
author = {Hong Peng and Rui Guo},
keywords = {Remote sensing, Passive remote sensing device, Reinforcement learning, Policy iteration, Non-quadratic form},
abstract = {In this paper, a policy iterative algorithm based on reinforcement learning is proposed to improve the balanced detection quality of passive remote sensing equipment. Firstly, the basic structure of the policy iteration algorithm is given for nonlinear affine systems, and stability proof is given. The whole process of detection information of passive remote sensing equipment is equivalent to the absorption of electric energy by the explored object. A nonlinear affine model for the PI solution is established by using this equivalent model. The working quality based on various information sensors is equivalent to the discrete-time cost function. Finally, a simulation experiment is carried out for a class of passive remote sensing equipment with 8 kinds of characteristic information and a virtual self-powered device is considered. The effectiveness of the algorithm is proven.}
}
@incollection{GOTTL2021209,
title = {Automated Process Synthesis Using Reinforcement Learning},
editor = {Metin Türkay and Rafiqul Gani},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {50},
pages = {209-214},
year = {2021},
booktitle = {31st European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-88506-5.50034-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323885065500346},
author = {Quirin Göttl and Dominik Grimm and Jakob Burger},
keywords = {process synthesis, machine learning, reinforcement learning, automated method},
abstract = {A novel method for automated flowsheet synthesis based on reinforcement learning (RL) is presented. Using the interaction with a process simulator as the learning environment, an agent is trained to solve the task of synthesizing process flowsheets without any heuristics or prior knowledge. The developed RL method models the task as a competitive two-player game that the agent plays against itself during training. The concept is proven to work along an example with a quaternary mixture that is processed using a reactor or distillation units.}
}
@article{ESCANDELLMONTERO201447,
title = {Optimization of anemia treatment in hemodialysis patients via reinforcement learning},
journal = {Artificial Intelligence in Medicine},
volume = {62},
number = {1},
pages = {47-60},
year = {2014},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2014.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0933365714000840},
author = {Pablo Escandell-Montero and Milena Chermisi and José M. Martínez-Martínez and Juan Gómez-Sanchis and Carlo Barbieri and Emilio Soria-Olivas and Flavio Mari and Joan Vila-Francés and Andrea Stopper and Emanuele Gatti and José D. Martín-Guerrero},
keywords = {Reinforcement learning, Markov decision processes, Fitted Q iteration, Chronic kidney disease, Renal anemia, Darbepoietin alfa},
abstract = {Objective
Anemia is a frequent comorbidity in hemodialysis patients that can be successfully treated by administering erythropoiesis-stimulating agents (ESAs). ESAs dosing is currently based on clinical protocols that often do not account for the high inter- and intra-individual variability in the patient's response. As a result, the hemoglobin level of some patients oscillates around the target range, which is associated with multiple risks and side-effects. This work proposes a methodology based on reinforcement learning (RL) to optimize ESA therapy.
Methods
RL is a data-driven approach for solving sequential decision-making problems that are formulated as Markov decision processes (MDPs). Computing optimal drug administration strategies for chronic diseases is a sequential decision-making problem in which the goal is to find the best sequence of drug doses. MDPs are particularly suitable for modeling these problems due to their ability to capture the uncertainty associated with the outcome of the treatment and the stochastic nature of the underlying process. The RL algorithm employed in the proposed methodology is fitted Q iteration, which stands out for its ability to make an efficient use of data.
Results
The experiments reported here are based on a computational model that describes the effect of ESAs on the hemoglobin level. The performance of the proposed method is evaluated and compared with the well-known Q-learning algorithm and with a standard protocol. Simulation results show that the performance of Q-learning is substantially lower than FQI and the protocol. When comparing FQI and the protocol, FQI achieves an increment of 27.6% in the proportion of patients that are within the targeted range of hemoglobin during the period of treatment. In addition, the quantity of drug needed is reduced by 5.13%, which indicates a more efficient use of ESAs.
Conclusion
Although prospective validation is required, promising results demonstrate the potential of RL to become an alternative to current protocols.}
}
@article{HUSSAIN2023100313,
title = {Energy management of buildings with energy storage and solar photovoltaic: A diversity in experience approach for deep reinforcement learning agents},
journal = {Energy and AI},
pages = {100313},
year = {2023},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2023.100313},
url = {https://www.sciencedirect.com/science/article/pii/S266654682300085X},
author = {Akhtar Hussain and Petr Musilek},
keywords = {Battery energy storage, Building demand management, Deep reinforcement learning, Diversity in experience, Energy management},
abstract = {Deep reinforcement learning (DRL) is a suitable approach to handle uncertainty in managing the energy consumption of buildings with energy storage systems. Conventionally, DRL agents are trained by randomly selecting samples from a data set, which can result in overexposure to some data categories and under/no exposure to other data categories. Thus, the trained model may be biased toward some data groups and underperform (provide suboptimal results) for data groups to which it was less exposed. To address this issue, diversity in experience-based DRL agent training framework is proposed in this study. This approach ensures the exposure of agents to all types of data. The proposed framework is implemented in two steps. In the first step, raw data are grouped into different clusters using the K-means clustering method. The clustered data is then arranged by stacking the data of one cluster on top of another. In the second step, a selection algorithm is proposed to select data from each cluster to train the DRL agent. The frequency of selection from each cluster is in proportion to the number of data points in that cluster and therefore named the proportional selection method. To analyze the performance of the proposed approach and compare the results with the conventional random selection method, two indices are proposed in this study: the flatness index and the divergence index. The model is trained using different data sets (1-year, 3-year, and 5-year) and also with the inclusion of solar photovoltaics. The simulation results confirmed the superior performance of the proposed approach to flatten the building’s load curve by optimally operating the energy storage system.}
}
@article{ZHANG2023121777,
title = {Two-timescale autonomous energy management strategy based on multi-agent deep reinforcement learning approach for residential multicarrier energy system},
journal = {Applied Energy},
volume = {351},
pages = {121777},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121777},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923011418},
author = {Bin Zhang and Weihao Hu and Amer M.Y.M. Ghias and Xiao Xu and Zhe Chen},
keywords = {Machine learning, Multiagent systems, Residential energy management, Deep reinforcement learning},
abstract = {In the residential multicarrier energy system (RMES), autonomous energy management (AEM) is beneficial for prosumers' costs that actively controls generation, energy conversion, and storage in real-time. Conventional model-based AEM methods rely on forecast models of distributed energy resources and proper system parameters, which are difficult for practical application. This article proposes a novel model-free two-timescale real-time AEM strategy for the RMES. The energy management problem in the RMES is time-decomposed into two timescales (hourly-ahead external energy trading and 15-min ahead internal energy conversion), formulated as Markov Games. Then, defining a multi-agent system not only enables separate energy management diagrams but also accelerates learning. Intelligent agents account for optimizing energy trading and energy conversion to minimize daily costs under the training of a deep deterministic policy gradient algorithm. In order to learn a collaborative strategy, all agents are trained centrally while being executed based on local information in a decentralized manner for fast response. A deterministic study indicates that the proposed AEM strategy can effectively and flexibly schedule the operation of components with respect to different price signals and load profiles. In the stochastic study that considers the error thresholds of solar panels generation and loads, the difference between the energy cost of test scenarios with a trained strategy and the no-regret learning method is equal to 0.32%, which is appropriate. In addition, the proposed method not only achieves a reduction in power imbalance but also exhibits lower energy costs when compared to various benchmark methods.}
}
@article{HU2023101387,
title = {Deep reinforcement learning assisted co-evolutionary differential evolution for constrained optimization},
journal = {Swarm and Evolutionary Computation},
volume = {83},
pages = {101387},
year = {2023},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2023.101387},
url = {https://www.sciencedirect.com/science/article/pii/S2210650223001608},
author = {Zhenzhen Hu and Wenyin Gong and Witold Pedrycz and Yanchi Li},
keywords = {Constraint handling technique, Deep reinforcement learning, Differential evolution, Co-evolution, Evolutionary operator},
abstract = {Solving constrained optimization problems (COPs) with evolutionary algorithms (EAs) is a popular research direction due to its potential and diverse applications. One of the key issues in solving COPs is the choice of constraint handling techniques (CHTs), as different CHTs can lead to different evolutionary directions. Combining EAs with deep reinforcement learning (DRL) is a promising and emerging approach for solving COPs. Although DRL can help solve the problem of pre-setting operators in EAs, neural networks need to obtain diverse training data within a limited number of evaluations in EAs. Based on the above considerations, this work proposes a DRL assisted co-evolutionary differential evolution, named CEDE-DRL, which can effectively use DRL to help EAs solve COPs. (1) This method incorporates co-evolution into the extraction of training data for the first time, ensuring the diversity of samples and improving the accuracy of the neural network model through information exchange between multiple populations. (2) Multiple CHTs are used for offspring selection to ensure the algorithm’s generality and flexibility. (3) DRL is used to evaluate the population state, taking into account feasibility, convergence, and diversity in the state setting and using the overall degree of improvement as a reward. The neural network selects suitable parent populations and corresponding archives for mutation. Finally, (4) to avoid premature convergence and local optima, an adaptive operator selection and individual archive elimination mechanism is added. Comparisons with state-of-the-art algorithms on benchmark functions CEC2010 and CEC2017 show that the proposed method performs competitively and produced robust solutions. The results of the application test set CEC2020 show that the proposed algorithm is also effective in real-world problems.}
}
@article{HU2022107731,
title = {Constrained evolutionary optimization based on reinforcement learning using the objective function and constraints},
journal = {Knowledge-Based Systems},
volume = {237},
pages = {107731},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107731},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121009709},
author = {Zhenzhen Hu and Wenyin Gong},
keywords = {Constrained optimization, Reinforcement learning, Adaptive operator selection, Differential evolution},
abstract = {Solving constrained optimization problems (COPs) with evolutionary algorithms is highly active in the evolutionary computation community. Combining evolutionary algorithms with the learning techniques is an efficient way to obtain promising performance for the COPs. Based on this consideration, we propose a differential evolution assisted by reinforcement learning (RL), namely RL-CORCO, to effectively solve the COPs. The proposed method can be featured as (i) the Q-learning in RL is used for adaptive operator selection; (ii) the hierarchical population is set as a state to find the feasible optimal solution; and (iii) the correlation between constraints and the objective function is utilized. The RL-CORCO is tested on 18 benchmark problems in the CEC 2010 competition and 28 benchmark problems in the CEC 2017 competition. Experimental results show that in CEC2017, RL-CORCO performed better than others on 12 problems in 50 dimensions and 14 problems in 100 dimensions. The results of the Friedman’s test demonstrate the efficacy of the algorithm, which is able to obtain highly competitive results compared with other related methods.}
}
@article{JING2024121373,
title = {Automated cryptocurrency trading approach using ensemble deep reinforcement learning: Learn to understand candlesticks},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121373},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121373},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423018754},
author = {Liu Jing and Yuncheol Kang},
keywords = {Automated trading, Candlestick images, Cryptocurrency, Deep reinforcement learning, Ensemble approach},
abstract = {Despite their high risk, cryptocurrencies have gained popularity as viable trading options. Cryptocurrencies are digital assets that experience significant fluctuations in a market operating 24 h a day. Recently, considerable attention has been paid to developing trading bots using machine-learning-based artificial intelligence. Previous studies have employed machine learning techniques to predict financial market trends or make trading decisions, primarily using numerical data extracted from candlesticks. However, these data often overlook the temporal and spatial information of candlesticks, leading to a limited understanding of their significance. In this study, we utilize multi-resolution candlestick images containing temporal and spatial information. Our rationale for using visual information from candlestick charts is to replicate the decision-making processes of human trading experts. To achieve this, we employ deep reinforcement learning algorithms to generate trading signals based on a state vector that includes embedded candlestick-chart images. The trading signal is generated using a multi-agent weighted voting ensemble approach. We test the proposed approach on two BTC/USDT datasets under both bullish and bearish market scenarios. Additionally, we use an attention-based technique to identify significant areas in the candlestick images targeted by the proposed approach. Our findings demonstrate that models using candlestick images 'as-is', outperform those using raw numeric data and other baseline models.}
}
@article{HIRASHIMA20116976,
title = {A New Reinforcement Learning System for Train Marshaling with Selectable Desired Layout},
journal = {IFAC Proceedings Volumes},
volume = {44},
number = {1},
pages = {6976-6981},
year = {2011},
note = {18th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20110828-6-IT-1002.01616},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016447257},
author = {Yoichi Hirashima},
keywords = {Marshaling, Freight Train, Reinforcement Learning, Scheduling, Grouping},
abstract = {Abstract
In this paper, a new reinforcement learning method for transfer scheduling of freight cars in a train is proposed. In the proposed method, the number of freight-movements in order to line freights in the desired order is reflected by corresponding evaluation value for each pair of freight-layout and removal-destination in a freight yard. Evaluation values are obtained by the Q-Learning method. The best transfer scheduling can be derived by selecting the removal-action of freight that has the best evaluation value at each freight-layout. By using the proposed method, the number of cars moved in each removal-action, the layout of desirable line, the removal order of freight cars and the destination of removed cars are simultaneously optimized by autonomous learning, so that the total number of removal actions is minimized. In the problem, the number of layouts of freight cars increases by the exponential rate with increase of total count of cars. Therefore, conventional methods have great difficulties to determine desirable movements of freight cars in order to reduce the run time for assembling a train.}
}
@article{LATHUILIERE201961,
title = {Neural network based reinforcement learning for audio–visual gaze control in human–robot interaction},
journal = {Pattern Recognition Letters},
volume = {118},
pages = {61-71},
year = {2019},
note = {Cooperative and Social Robots: Understanding Human Activities and Intentions},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2018.05.023},
url = {https://www.sciencedirect.com/science/article/pii/S0167865518302113},
author = {Stéphane Lathuilière and Benoit Massé and Pablo Mesejo and Radu Horaud},
keywords = {Reinforcement learning, Human–robot interaction, Robot gaze control, Neural networks, Transfer learning, Multimodal data fusion},
abstract = {This paper introduces a novel neural network-based reinforcement learning approach for robot gaze control. Our approach enables a robot to learn and to adapt its gaze control strategy for human–robot interaction neither with the use of external sensors nor with human supervision. The robot learns to focus its attention onto groups of people from its own audio-visual experiences, independently of the number of people, of their positions and of their physical appearances. In particular, we use a recurrent neural network architecture in combination with Q-learning to find an optimal action-selection policy; we pre-train the network using a simulated environment that mimics realistic scenarios that involve speaking/silent participants, thus avoiding the need of tedious sessions of a robot interacting with people. Our experimental evaluation suggests that the proposed method is robust in terms of parameter configuration, i.e. the selection of the parameter values employed by the method do not have a decisive impact on the performance. The best results are obtained when both audio and visual information is jointly used. Experiments with the Nao robot indicate that our framework is a step forward towards the autonomous learning of a socially acceptable gaze behavior.}
}
@article{MODARES2014193,
title = {Integral reinforcement learning and experience replay for adaptive optimal control of partially-unknown constrained-input continuous-time systems},
journal = {Automatica},
volume = {50},
number = {1},
pages = {193-202},
year = {2014},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2013.09.043},
url = {https://www.sciencedirect.com/science/article/pii/S0005109813004767},
author = {Hamidreza Modares and Frank L. Lewis and Mohammad-Bagher Naghibi-Sistani},
keywords = {Integral reinforcement learning, Experience replay, Optimal control, Neural networks, Input constraints},
abstract = {In this paper, an integral reinforcement learning (IRL) algorithm on an actor–critic structure is developed to learn online the solution to the Hamilton–Jacobi–Bellman equation for partially-unknown constrained-input systems. The technique of experience replay is used to update the critic weights to solve an IRL Bellman equation. This means, unlike existing reinforcement learning algorithms, recorded past experiences are used concurrently with current data for adaptation of the critic weights. It is shown that using this technique, instead of the traditional persistence of excitation condition which is often difficult or impossible to verify online, an easy-to-check condition on the richness of the recorded data is sufficient to guarantee convergence to a near-optimal control law. Stability of the proposed feedback control law is shown and the effectiveness of the proposed method is illustrated with simulation examples.}
}
@article{KAISER2020352,
title = {Bayesian decomposition of multi-modal dynamical systems for reinforcement learning},
journal = {Neurocomputing},
volume = {416},
pages = {352-359},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.12.132},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220305026},
author = {Markus Kaiser and Clemens Otte and Thomas A. Runkler and Carl Henrik Ek},
keywords = {Bayesian machine learning, Gaussian processes, Hierarchical gaussian processes, Reinforcement learning, Model-based reinforcement learning, Stochastic policy search, Data-efficiency},
abstract = {In this paper, we present a model-based reinforcement learning system where the transition model is treated in a Bayesian manner. The approach naturally lends itself to exploit expert knowledge by introducing priors to impose structure on the underlying learning task. The additional information introduced to the system means that we can learn from small amounts of data, recover an interpretable model and, importantly, provide predictions with an associated uncertainty. To show the benefits of the approach, we use a challenging data set where the dynamics of the underlying system exhibit both operational phase shifts and heteroscedastic noise. Comparing our model to NFQ and BNN+LV, we show how our approach yields human-interpretable insight about the underlying dynamics while also increasing data-efficiency.}
}
@article{NGO2023101936,
title = {Does reinforcement learning outperform deep learning and traditional portfolio optimization models in frontier and developed financial markets?},
journal = {Research in International Business and Finance},
volume = {65},
pages = {101936},
year = {2023},
issn = {0275-5319},
doi = {https://doi.org/10.1016/j.ribaf.2023.101936},
url = {https://www.sciencedirect.com/science/article/pii/S0275531923000624},
author = {Vu Minh Ngo and Huan Huu Nguyen and Phuc {Van Nguyen}},
keywords = {, , , , },
abstract = {Advancements in machine learning have opened up a wide range of new possibilities for using advanced computer algorithms, such as reinforcement learning in portfolio risk management. However, very little evidence has been provided on the superior performance of reinforcement learning models over traditional optimization models following the mean-variance framework in different financial market settings. This study uses two experiments with data from the Vietnamese and U.S. securities markets to justify whether advanced machine learning models could outperform traditional portfolios' cumulative returns while optimizing the Sharpe ratio. The results suggest that reinforcement learning consistently outperforms the established methods and benchmarks in both experiments, even when using a very similar degree of diversification in portfolio construction and the same input data. This study confirms the ability of reinforcement learning to provide dynamic responses to market conditions and redefine the risk-return standard in the financial system.}
}
@article{LUO2022109844,
title = {Reinforcement learning-based modified cuckoo search algorithm for economic dispatch problems},
journal = {Knowledge-Based Systems},
volume = {257},
pages = {109844},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109844},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122009388},
author = {Wenguan Luo and Xiaobing Yu},
keywords = {Economic dispatch problem, Reinforcement learning, Cuckoo search algorithm},
abstract = {One of the crucial tasks of social development is carbon neutrality, which is mainly due to the emission of greenhouse gases. However, thermal power, which produces plenty of harmful gases, is still the main component of electric energy. Therefore, Economic Dispatch (ED) is proposed to utilize energy resources more efficiently and reduce the cost of power generation. ED is a nonlinear and nonconvex-constrained optimization problem that is difficult to optimize. In this paper, we propose a Reinforcement Learning-based Modified Cuckoo Search algorithm (RLMCS) to solve ED problems. The proposed algorithm employs the concept of Reinforcement Learning (RL) and develops an RL-based method to process population obtained from the explorative phase. The RL-based method can dynamically enhance the population based on cumulative rewards and the current environmental state. Thus, the comprehensive search ability of RLMCS has been well improved. Moreover, some proven technologies, i.e., Gaussian random walk, quasi-opposition learning, and adaptive switch parameter, are introduced to further enhance the efficiency of RLMCS. The performance of the RLMCS is tested on standard ED problems (6 and 11 units) and ED problems with valve-point effects (10, 14, and 40 units). RLMCS is also compared with some well-established CS variants. The experimental results have demonstrated that RLMCS is more competitive and robust.}
}
@article{HOU2023106703,
title = {Subtask-masked curriculum learning for reinforcement learning with application to UAV maneuver decision-making},
journal = {Engineering Applications of Artificial Intelligence},
volume = {125},
pages = {106703},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106703},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623008874},
author = {Yueqi Hou and Xiaolong Liang and Maolong Lv and Qisong Yang and Yang Li},
keywords = {Unmanned Aerial Vehicle, Maneuver decision-making, Reinforcement learning, Curriculum learning, Knowledge transfer},
abstract = {Unmanned Aerial Vehicle (UAV) maneuver strategy learning remains a challenge when using Reinforcement Learning (RL) in this sparse reward task. In this paper, we propose Subtask-Masked curriculum learning for RL (Sub Mas-RL), an efficient RL paradigm that implements curriculum learning and knowledge transfer for UAV maneuver scenarios involving multiple missiles. First, this study introduces a novel concept known as subtask mask to create source tasks from a target task by masking partial subtasks. Then, a subtask-masked curriculum generation method is proposed to generate a sequenced curriculum by alternately conducting task generation and task sequencing. To establish efficient knowledge transfer and avoid negative transfer, this paper employs two transfer techniques, policy distillation and policy reuse, along with an explicit transfer condition that masks irrelevant knowledge. Experimental results demonstrate that our method achieves a 94.8% success rate in the UAV maneuver scenario, where the direct use of reinforcement learning always fails. The proposed RL framework Sub Mas-RL is expected to learn an effective policy in complex tasks with sparse rewards.}
}
@article{KARIMI2022193,
title = {Task offloading in vehicular edge computing networks via deep reinforcement learning},
journal = {Computer Communications},
volume = {189},
pages = {193-204},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422001104},
author = {Elham Karimi and Yuanzhu Chen and Behzad Akbari},
keywords = {Vehicular networks, Multi-access edge computing, Central cloud, Resource allocation, Task offloading, Response time, Deep reinforcement learning},
abstract = {Given the rapid increase of various applications in vehicular networks, it is crucial to consider a flexible architecture to improve the Quality of Service (QoS). Utilizing Multi-access Edge Computing (MEC) as a distributed paradigm, with resource capabilities closer to the vehicles, would be a promising solution to reduce response time in such a network. However, MEC suffers from limited resources and is deprived of handling high mobilities with many diverse applications. This paper proposes cooperation between MEC and central cloud decisions for different vehicular application offloading. We formulate a new resource allocation problem to guarantee the required response time. To solve such an NP-hard problem, we utilize deep reinforcement learning, a proper computational model, to automatically learn the dynamics of the network state and rapidly capture an optimal solution. Extensive numerical analysis and results illustrate how our proposed scheme can achieve a high acceptance rate with a low response time.}
}
@article{JAISWAL20236006,
title = {Reinforcement learning based Islanding detection technique in distributed generation},
journal = {Energy Reports},
volume = {9},
pages = {6006-6019},
year = {2023},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2023.05.069},
url = {https://www.sciencedirect.com/science/article/pii/S2352484723008120},
author = {Aashish Jaiswal and Shubhash Chandra and Anurag Priyadarshi and Sulabh Sachan and Sanchari Deb},
keywords = {ID (Islanding Detection), DG technology, Point of Connection (PoC), VMD (Variable Mode Decomposition), PLL (Phase Locked Loop) and microgrid},
abstract = {As the scientific, technical, and economic developments of the world continue to advance, there will be an increased requirement for Distribution Generation(DG) technology. There is a widespread disruption in the primary power grid, the procedure known as islanding involves the construction of a power island that operates in a manner analogous to a section of the utility system. The Islanding Detection (ID), in which the development of islanding, actions carried out during islanding, and the approaches utilized to recognize islanding are detailed, is entirely necessary for this study. In this paper the author used reinforcement learning method which is a technique for introducing machines to learn by rewarding appropriate conduct and/or penalizing inappropriate behavior. The Remote technique, the Local approach, and the Hybrid approach were all observable in the Islanding detection methodologies. Passive islanding detection algorithms for inverter-oriented distributed generation systems based on Variational Mode Decomposition (VMD) and the microgrid approach are implemented throughout this research attempt. According to the comparison results of the study, the proposed system is more reliable and better than the compared technique. The comparison results show the detection time of suggested model which is 0.06 s. In​ contrast to active techniques, it is capable of functioning normally and without disrupting the usual operation of the system. As a result, it can be used effectively for real-time applications.}
}
@article{CHEN2022122123,
title = {Deep reinforcement learning-based multi-objective control of hybrid power system combined with road recognition under time-varying environment},
journal = {Energy},
volume = {239},
pages = {122123},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.122123},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221023719},
author = {Jiaxin Chen and Hong Shu and Xiaolin Tang and Teng Liu and Weida Wang},
keywords = {Hybrid electric vehicle, Road recognition network, Deep reinforcement learning, Multi-objective control network, Energy management strategy},
abstract = {Aiming at promoting the intelligent development of control technology for new energy vehicles and showing the outstanding advantages of deep reinforcement learning (DRL), this paper trained a VGG16-based road recognition convolutional neural network (CNN) at first. Lots of high-definition images of five typical roads are collected by the racing game Dust Rally 2.0, including dry asphalt, wet asphalt, snow, dry cobblestone, and wet cobblestone. Then, a time-varying driving environment model was established, involving driving images, road slope, longitudinal speed, and the number of passengers. Finally, a stereoscopic control network suitable for nine-dimensional state space and three-dimensional action space was built, and for parallel hybrid electric vehicles (HEVs) with the P3 structure, a deep q-network (DQN)-based energy management strategy (EMS) achieving multi-objective control was proposed, including the fine-tuning strategy of the motor speed to maintain the optimal slip rate during braking, the engine power control strategy and the continuously variable transmission (CVT) gear ratio control strategy. Simulation results show under the influence of some factors such as tree shade and image compression, the road recognition network has the highest accuracy for snow roads and wet asphalt roads. Three types of control strategies learned simultaneously by the stereoscopic control network not only maintain the near-optimal slip rate in the braking period but also achieve a fuel consumption of 4788.93 g, while dynamic programming (DP)-based EMS gets a fuel consumption of 4295.61 g. Moreover, even DP-based EMS only contains three states and two actions, the time consumed for DP-based EMS and DQN-based EMS to run the speed cycle of 3602s is about 4911s and 10s, respectively. Therefore, the optimization and real-time performance of DRL-based EMS can be guaranteed.}
}
@article{ZHANG2022116454,
title = {Twin delayed deep deterministic policy gradient-based deep reinforcement learning for energy management of fuel cell vehicle integrating durability information of powertrain},
journal = {Energy Conversion and Management},
volume = {274},
pages = {116454},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.116454},
url = {https://www.sciencedirect.com/science/article/pii/S0196890422012328},
author = {Yuanzhi Zhang and Caizhi Zhang and Ruijia Fan and Shulong Huang and Yun Yang and Qianwen Xu},
keywords = {Deep reinforcement learning, Energy management strategy, Fuel cell vehicle, Fuel economy, Lifespan durability, Twin delayed deep deterministic policy gradient},
abstract = {Deep reinforcement learning (DRL)-based energy management strategy (EMS) is attractive for fuel cell vehicle (FCV). Nevertheless, the fuel economy and lifespan durability of proton exchange membrane fuel cell (PEMFC) stack and lithium-ion battery (LIB) may not be synchronously optimized since transient degradation variations of PEMFC stack and LIB are not generally regarded for DRL-based EMSs. Furthermore, the inappropriate action space and the overestimated value function of DRL can lead to suboptimal EMS for on-line control. To this end, the objective of this research endeavors to formulate a twin delayed deep deterministic policy gradient (TD3)-based EMS integrating durability information of PEMFC stack and LIB, which can interact with the vehicle operating states to continuously control the hybrid powertrain and limit the overestimation of DRL value function for ensuring maximum multi-objective reward at each moment. Unlike traditional DRL-based EMSs, the multi-objective reward function for this study is enlarged to incorporate the hydrogen consumption, state of charge (SOC)-sustaining penalty and transient lifespan degradation information of PEMFC stack and LIB in off-line training and on-line control. The results demonstrate that the proposed EMS can drastically lessen the training time and computational burden. Meanwhile, in contrast with deep Q-network (DQN)-based and deep deterministic policy gradient (DDPG)-based EMSs in the various real-world urban and standard driving cycles, the proposed EMS can achieve hydrogen abatement at least 9.76% and 1.07%, and slow down total powertrain degradation at least 9.11% and 2.62%, respectively.}
}
@article{SAMADI2020106211,
title = {Decentralized multi-agent based energy management of microgrid using reinforcement learning},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {122},
pages = {106211},
year = {2020},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2020.106211},
url = {https://www.sciencedirect.com/science/article/pii/S0142061520304877},
author = {Esmat Samadi and Ali Badri and Reza Ebrahimpour},
keywords = {Distributed energy resources, Microgrid energy management system, Multi-agent systems, Reinforcement learning},
abstract = {This paper proposes a multi-agent based decentralized energy management approach in a grid-connected microgrid (MG). The MG comprises of wind and photovoltaic resources, diesel generator, electrical energy storage, and combined heat and power generations to serve electrical and thermal loads at the lower-level of energy management system (EMS). All distributed energy resources (DERs) and customers are modelled as self-interested agents who adopt reinforcement learning to optimize their behaviours and operation costs. Based on this algorithm, agents have the capability to interact with each other in a distributed manner and find the best strategy in competitive environment. At the upper-level of EMS, there is an energy management agent that gathers the information of agents of lower-level and clears the MG electrical and thermal energy market in line with predetermined goals. Utilizing energy availability from different DERs and variety of customers’ consumption patterns, considering uncertainty of renewable generation and load consumption and taking into account technical constraint of DERs are the strengths of the presented framework. Performance of the proposed algorithm is investigated under different conditions of agents learning and using ε-greedy, soft-max and upper confidence bound methods. The simulation results verify efficacy of the proposed approach.}
}
@article{ANCILLOTTI20171,
title = {A reinforcement learning-based link quality estimation strategy for RPL and its impact on topology management},
journal = {Computer Communications},
volume = {112},
pages = {1-13},
year = {2017},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2017.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0140366417305704},
author = {Emilio Ancillotti and Carlo Vallati and Raffaele Bruno and Enzo Mingozzi},
keywords = {RPL, Topology and mobility management, Link quality estimation, Experimental evaluation},
abstract = {Over the last few years, standardisation efforts are consolidating the role of the Routing Protocol for Low-Power and Lossy Networks (RPL) as the standard routing protocol for IPv6-based Wireless Sensor Networks (WSNs). Although many core functionalities are well defined, others are left implementation dependent. Among them, the definition of an efficient link-quality estimation (LQE) strategy is of paramount importance, as it influences significantly both the quality of the selected network routes and nodes’ energy consumption. In this paper, we present RL-Probe, a novel strategy for link quality monitoring in RPL, which accurately measures link quality with minimal overhead and energy waste. To achieve this goal, RL-Probe leverages both synchronous and asynchronous monitoring schemes to maintain up-to-date information on link quality and to promptly react to sudden topology changes, e.g. due to mobility. Our solution relies on a reinforcement learning model to drive the monitoring procedures in order to minimise the overhead caused by active probing operations. The performance of the proposed solution is assessed by means of simulations and real experiments. Results demonstrated that RL-Probe helps in effectively improving packet loss rates, allowing nodes to promptly react to link quality variations as well as to link failures due to node mobility.}
}
@article{XIANG2020118597,
title = {Energy emergency supply chain collaboration optimization with group consensus through reinforcement learning considering non-cooperative behaviours},
journal = {Energy},
volume = {210},
pages = {118597},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.118597},
url = {https://www.sciencedirect.com/science/article/pii/S0360544220317059},
author = {Liu Xiang},
keywords = {Energy emergency, Supply chain collaboration, Optimization, Consensus, Non-cooperative behaviours, Reinforcement learning},
abstract = {As a response to emergency events occurred frequently around the world, energy emergency supply chain collaboration has becomes a business imperative with multiple energy trading organizations to respond it by group consensus. However, managing agile energy emergency supply chain collaboration, with the minimum energy recovery time regarding energy supply shortage driven by urgent events such as earthquake, is confronted with a difficult task: govern non-cooperative behaviours of energy emergency supply chain collaboration that identifies the irrational causes underlying deviations from neoclassical utility-maximizing economic decisions. In this paper, develop a smart model for energy emergency supply chain collaboration that the work bridges the divide between emergency supply chain collaboration optimization with group consensus and reinforcement learning. It sets up collaboration consensus with scenarios learning algorithm driven by the satisfaction-level combination of generalising past experience and future scenarios to new local energy supply shortage emergency situations to govern non-cooperative irrational behaviours, resulting in the response process with the minimum energy recovery time, cost and CO2 emissions. Simulations results show that proposed model has a significantly lower running time by 40%, and reduces minimisation of cost for energy restoration by 7% and minimisation of CO2 emissions by 10.8% on average.}
}
@article{XIA2023410,
title = {A multi-agent convolution deep reinforcement learning network for aeroengine fleet maintenance strategy optimization},
journal = {Journal of Manufacturing Systems},
volume = {68},
pages = {410-425},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523000791},
author = {Xiangzhao Xia and Xuyun Fu and Shisheng Zhong and Zhen Li and Song Fu and Zhengfeng Bai and Xueyun Liu},
keywords = {Deep reinforcement learning, Maintenance strategy optimization, Multi-agent, Engine fleet, Aeroengine},
abstract = {The core task of aeroengine fleet maintenance strategy optimization is to realize the collaborative optimization of each engine and fleet resources. However, the existing maintenance strategy method cannot precisely coordinate the maintenance plan of each engine in the fleet, especially when the state space dimension of the fleet is high and the maintenance action is complex. To solve these problems, a multi-agent convolution deep reinforcement learning network, which is a variant of the traditional deep Q-learning network, is proposed to accurately optimize the maintenance strategy of engine fleet. Firstly, the convolutional deep learning network is used as the Q-value computing network in the deep Q-learning method to extract the low-dimensional features that can better represent the high-dimensional state of the engine fleet. Secondly, in order to effectively make decisions on the complex maintenance actions of each engine, the single-agent in the deep Q-learning method is extended to a multi-agent structure. Each agent uses an independent Q-value calculation network and combined with constraints to make decisions only on the maintenance actions of its corresponding engine. At the same time, reinforcement learning mechanism is used to complete the collaborative optimization of engine maintenance strategies. Finally, through two groups of simulation experiments, the rough optimization and fine optimization of fleet maintenance strategy are realized respectively, and compared with some extensive optimization methods. The experimental results show the superiority of the proposed method in aeroengine fleet maintenance strategy optimization.}
}
@article{GARNIER2021104973,
title = {A review on deep reinforcement learning for fluid mechanics},
journal = {Computers & Fluids},
volume = {225},
pages = {104973},
year = {2021},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2021.104973},
url = {https://www.sciencedirect.com/science/article/pii/S0045793021001407},
author = {Paul Garnier and Jonathan Viquerat and Jean Rabault and Aurélien Larcher and Alexander Kuhnle and Elie Hachem},
keywords = {Deep reinforcement learning, Fluid mechanics},
abstract = {Deep reinforcement learning (DRL) has recently been adopted in a wide range of physics and engineering domains for its ability to solve decision-making problems that were previously out of reach due to a combination of non-linearity and high dimensionality. In the last few years, it has spread in the field of computational mechanics, and particularly in fluid dynamics, with recent applications in flow control and shape optimization. In this work, we conduct a detailed review of existing DRL applications to fluid mechanics problems. In addition, we present recent results that further illustrate the potential of DRL in Fluid Mechanics. The coupling methods used in each case are covered, detailing their advantages and limitations. Our review also focuses on the comparison with classical methods for optimal control and optimization. Finally, several test cases are described that illustrate recent progress made in this field. The goal of this publication is to provide an understanding of DRL capabilities along with state-of-the-art applications in fluid dynamics to researchers wishing to address new problems with these methods.}
}
@article{KOKSALAHMED2021269,
title = {Reinforcement learning-enabled genetic algorithm for school bus scheduling},
journal = {Journal of Intelligent Transportation Systems},
volume = {26},
number = {3},
pages = {269-283},
year = {2021},
issn = {1547-2450},
doi = {https://doi.org/10.1080/15472450.2020.1852082},
url = {https://www.sciencedirect.com/science/article/pii/S1547245022003619},
author = {Eda {Köksal Ahmed} and Zengxiang Li and Bharadwaj Veeravalli and Shen Ren},
keywords = {Combinatorial optimization, genetic algorithm, multi-objective optimization, reinforcement learning, vehicle scheduling},
abstract = {In this paper, we focus on a bi-objective school bus scheduling optimization problem, which is a subset of vehicle fleet scheduling problems to transport students distributed across a designated area to the relevant schools. The problem being proven as NP-hard in the literature, we propose an algorithm that seamlessly integrates a reinforcement learning approach with a genetic algorithm. Our proposed algorithm utilizes the processed data supplied by our intelligent transportation system framework to decide the genetic algorithm parameters on-the-fly with the aid of reinforcement learning. With the active guidance of reinforcement learning, the efficiency of the genetic algorithm is improved, and the near-optimal schedule can be achieved in a shorter duration. To evaluate the model, we conducted experiments on a geospatial dataset comprising road networks, trip trajectories of buses, and the address of students. Results indicate that the genetic algorithm improves the travel distance and time compared to the existing schedule. Reinforcement learning-enabled genetic algorithm improves the performance and the objective function significantly, furthermore with a fewer number of generations compared to various state-of-the-art evolutionary algorithms. The saving by reinforcement learning-enabled genetic algorithm compared to the schedule by initial state generation process is 8.63% and 16.92% for the travel distance for buses and students, respectively, and 14.95% and 26.58% for the travel time for buses and students, respectively.}
}
@article{PASSALIS201937,
title = {Deep reinforcement learning for controlling frontal person close-up shooting},
journal = {Neurocomputing},
volume = {335},
pages = {37-47},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.01.046},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219300724},
author = {Nikolaos Passalis and Anastasios Tefas},
keywords = {Reinforcement learning, Deep learning, Drone-based cinematography},
abstract = {Drones, also known as Unmanned Aerial Vehicles, are capable of capturing spectacular aerial shots and can be used to aid several cinematography-oriented tasks. However, flying drones in a professional setting requires the cooperation of several people, increasing the production cost and possibly reducing the quality of the obtained shots. In this paper, a generic way for formulating cinematography-oriented control objectives, that can be used for training any RL agent to automate the drone and camera control processes, is proposed. To increase the convergence speed and learn more accurate deep RL agents, a hint-based reward function is also employed. Two simulation environments, one for drone control and one for camera control, were developed and used for training and evaluating the proposed methods. The proposed method can be combined both with methods capable of performing discrete control, as well as with continuous control methods. It was experimentally demonstrated that the proposed method improves the control accuracy over both handcrafted control techniques and deep RL models trained with other reward functions.}
}
@article{SOMAN2022104069,
title = {Automating look-ahead schedule generation for construction using linked-data based constraint checking and reinforcement learning},
journal = {Automation in Construction},
volume = {134},
pages = {104069},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104069},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005203},
author = {Ranjith K. Soman and Miguel Molina-Solana},
keywords = {Look-ahead planning, Lean construction, Linked-data, Reinforcement learning, Scheduling, Resource constrained project scheduling problem (RCPSP), Look ahead schedule (LAS), Q-learning},
abstract = {Look-ahead planning is the stage in construction planning where information from diverse sources is integrated and plans developed for the next six/eight weeks. Poor planning of construction site activities at this stage often results in cost overruns and schedule delays. This work presents a novel Look-Ahead Schedule (LAS) generation method, which uses reinforcement learning and linked-data based constraint checking within the reward, to address the issues associated with manual look-ahead planning and help construction professionals efficiently plan construction activities at this stage. Our proposal can generate conflict-free LAS significantly faster than conventional methods, demonstrating its capability as a decision support tool during look-ahead planning meetings. Therefore, this paper extends existing knowledge in the construction informatics domain by demonstrating the application of reinforcement learning to aid data-driven look-ahead planning.}
}
@article{ZHANG2021121492,
title = {A novel asynchronous deep reinforcement learning model with adaptive early forecasting method and reward incentive mechanism for short-term load forecasting},
journal = {Energy},
volume = {236},
pages = {121492},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121492},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221017400},
author = {Wenyu Zhang and Qian Chen and Jianyong Yan and Shuai Zhang and Jiyuan Xu},
keywords = {Load forecasting, Deep reinforcement learning, Deep learning, Deep deterministic policy gradient},
abstract = {Accurate load forecasting is challenging due to the significant uncertainty of load demand. Deep reinforcement learning, which integrates the nonlinear fitting ability of deep learning with the decision-making ability of reinforcement learning, has obtained effective solutions to various optimization problems. However, no study has been reported, which used deep reinforcement learning for short-term load forecasting because of the difficulties in handling the high temporal correlation and high convergence instability. In this study, a novel asynchronous deep reinforcement learning model is proposed for short-term load forecasting by addressing the above difficulties. First, a new asynchronous deep deterministic policy gradient method is proposed to disrupt the temporal correlation of different samples to reduce the overestimation of the expected total discount reward of the agent. Further, a new adaptive early forecasting method is proposed to reduce the time cost of model training by adaptively judging the training situation of the agent. Moreover, a new reward incentive mechanism is proposed to stabilize the convergence of model training by taking into account the trend of agent actions at different time steps. The experimental results show that the proposed model achieves higher forecasting accuracy, less time cost, and more stable convergence compared with eleven baseline models.}
}
@article{WEI2021786,
title = {Reinforcement learning-based QoE-oriented dynamic adaptive streaming framework},
journal = {Information Sciences},
volume = {569},
pages = {786-803},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521004588},
author = {Xuekai Wei and Mingliang Zhou and Sam Kwong and Hui Yuan and Shiqi Wang and Guopu Zhu and Jingchao Cao},
keywords = {MPEG-DASH, Quality of experience, Machine learning, Reinforcement learning},
abstract = {Dynamic adaptive streaming over the HTTP (DASH) standard has been widely adopted by many content providers for online video transmission and greatly improve the performance. Designing an efficient DASH system is challenging because of the inherent large fluctuations characterizing both encoded video sequences and network traces. In this paper, a reinforcement learning (RL)-based DASH technique that addresses user quality of experience (QoE) is constructed. The DASH adaptive bitrate (ABR) selection problem is formulated as a Markov decision process (MDP) problem. Accordingly, an RL-based solution is proposed to solve the MDP problem, in which the DASH clients act as the RL agent, and the network variation constitutes the environment. The proposed user QoE is used as the reward by jointly considering the video quality and buffer status. The goal of the RL algorithm is to select a suitable video quality level for each video segment to maximize the total reward. Then, the proposed RL-based ABR algorithm is embedded in the QoE-oriented DASH framework. Experimental results show that the proposed RL-based ABR algorithm outperforms state-of-the-art schemes in terms of both temporal and visual QoE factors by a noticeable margin while guaranteeing application-level fairness when multiple clients share a bottlenecked network.}
}
@article{DENG2021102676,
title = {Event-triggered output-feedback adaptive tracking control of autonomous underwater vehicles using reinforcement learning},
journal = {Applied Ocean Research},
volume = {113},
pages = {102676},
year = {2021},
issn = {0141-1187},
doi = {https://doi.org/10.1016/j.apor.2021.102676},
url = {https://www.sciencedirect.com/science/article/pii/S014111872100153X},
author = {Yingjie Deng and Tao Liu and Dingxuan Zhao},
keywords = {Event-triggered control (ETC), Autonomous underwater vehicle (AUV), Event-triggered adaptive neural observer, Reinforcement learning (RL), Jumps of virtual control laws},
abstract = {This paper investigates the event-triggered tracking control of fully actuated autonomous underwater vehicles (AUVs) in the vertical plane. Specifically, this paper studies the ETC in the sensor-to-controller channel, where the X-Z coordinates and the pitch angle are transmitted in an event-triggered manner. The greater communication saving is ensured. Additionally, the recently-raised problem of “jumps of virtual control laws” is solved by establishing an event-triggered adaptive neural observer. This observer can offer the benefit of state recovery, which conforms to the nautical practice. As the observer is co-located with the controller, a succinct triggering condition is devised to avoid the “Zeno” behavior and co-located with the sensors. To achieve the optimization of the long-term tracking performance, the reinforcement learning (RL) technique is performed by using the critic-actor method with the radial basis function (RBF) neural networks (NNs). The critic NN approximates the performance index and is transferred as the reinforcement signal to the actor NN, which accounts for the uncertainties. The closed-loop stability is analyzed based on the observer-based tracking errors, and all of them are proved to be semi-globally uniformly ultimately bounded (SGUUB). Finally, a numerical experiment substantiates the effectiveness of the proposed scheme.}
}
@article{WANG2022108939,
title = {Towards an energy-efficient Data Center Network based on deep reinforcement learning},
journal = {Computer Networks},
volume = {210},
pages = {108939},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.108939},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622001220},
author = {Yang Wang and Yutong Li and Ting Wang and Gang Liu},
keywords = {Data center network, Power conservation, Deep reinforcement learning},
abstract = {Data Center Network (DCN) plays a crucial role in orchestrating the physical or virtual resources in data centers to meet the requirements of Internet of Things and Cloud Computing. The energy efficiency should be seriously considered for DCNs with large-scale switch devices which support numerous realtime network flow demands, especially for enormous flow demands from IoT devices. Typically, the network energy conservation can be achieved by optimizing routing and flow scheduling with energy awareness, targeting at powering off as many idle and low-loaded network devices as possible. For energy efficiency objective, in this paper we address a combinatorial optimization problem, named Multi-Commodity Flow (MCF) problem which optimizes the bandwidth allocation and routing to reduce the energy consumption. We propose a framework which has the lookahead ability of predicting flow demands in DCNs to dynamically feed the MCF problem as inputs. A Long Short-Term Memory (LSTM) network is exploited for flow demand prediction in DCNs and a Deep Reinforcement Learning (DRL) algorithm is tailored for solving the MCF problem. In experiments, we evaluate the predicted flow demands which simulate real flow demands and conduct a comparison between our DRL scheme with the baseline and optimizer to show the advantage of the DRL solution in optimality and efficiency.}
}
@article{MUTEBA2020315,
title = {Deep Reinforcement Learning Based Resource Allocation For Narrowband Cognitive Radio-IoT Systems},
journal = {Procedia Computer Science},
volume = {175},
pages = {315-324},
year = {2020},
note = {The 17th International Conference on Mobile Systems and Pervasive Computing (MobiSPC),The 15th International Conference on Future Networks and Communications (FNC),The 10th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.07.046},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920317270},
author = {K.F. Muteba and K Djouani and T.O. Olwal},
keywords = {Narrowband-Cognitive IoT, LPWA, spectrum allocation, 3GPP, Q-learning, Deep Q-learning},
abstract = {Narrowband Internet-of-Things (NB-IoT) is a low-power wide area (LPWA) technology developed by the Third-generation Partnership Project (3GPP) with objective to enable a wide range of IoT devices, low cost device and low power in the 5G era. As the number of IoT devices continue to increase, the demand for the spectrum allocation grows proportionately. The NB-IoT spectrum allocation is limited from 180 KHz to 200 KHz and is not sufficient to accomodate the exponential surge in the size of the NB-IoT devices.Thus, the need to efficiently allocate the available spectrum to the NB-IoT devices. Furthermore, in an attempt to enhance the coverage in NB-IoT network, recent relevant studies (3GPP release 13) have introduced the concept of repeated transmission. Since repeated transmissions ensure coverage enhancement but cause spectrum wastage, the traditional resource allocation is not appropriate for NB-IoT network. Motivated by this research gap we propose a NB-Cognitive Radio-IoT (NB-CR-IoT) technique which integrates Cognitive Radio (CR) techniques into the operation of the conventional NB-IoT. The resulting architecture seeks to foster an efficient opportunistic spectrum access in distributed heterogeneous networks.We further formulate the resource allocation problem as a deep Q-learning solved by reducing the number of repeated transmissions and allocating more IoT devices in NB-IoT network. The results in this contribution indicate that DQN outperforms the traditional Q-learning algorithm.}
}
@article{KUBALIK20174162,
title = {Optimal Control via Reinforcement Learning with Symbolic Policy Approximation},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {4162-4167},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.805},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317312594},
author = {Jiří Kubalík and Eduard Alibekov and Robert Babuška},
keywords = {reinforcement learning, value iteration, symbolic regression, genetic programming, nonlinear model-based control, optimal control},
abstract = {Model-based reinforcement learning (RL) algorithms can be used to derive optimal control laws for nonlinear dynamic systems. With continuous-valued state and input variables, RL algorithms have to rely on function approximators to represent the value function and policy mappings. This paper addresses the problem of finding a smooth policy based on the value function represented by means of a basis-function approximator. We first show that policies derived directly from the value function or represented explicitly by the same type of approximator lead to inferior control performance, manifested by non-smooth control signals and steady-state errors. We then propose a novel method to construct a smooth policy represented by an analytic equation, obtained by means of symbolic regression. The proposed method is illustrated on a reference-tracking problem of a 1-DOF robot arm operating under the influence of gravity. The results show that the analytic control law performs at least equally well as the original numerically approximated policy, while it leads to much smoother control signals. In addition, the analytic function is readable (as opposed to black-box approximators) and can be used in further analysis and synthesis of the closed loop.}
}
@article{SYAFIIE2005127,
title = {MODEL-FREE INTELLIGENT CONTROL USING REINFORCEMENT LEARNING AND TEMPORAL ABSTRACTION-APPLIED TO pH CONTROL},
journal = {IFAC Proceedings Volumes},
volume = {38},
number = {1},
pages = {127-132},
year = {2005},
note = {16th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20050703-6-CZ-1902.00242},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016362541},
author = {S. Syafiie and F. Tadeo and E. Martinez},
keywords = {learning control, intelligent control, online learning, agents, process control, neutralization process, pH control, temporal abstraction, model free},
abstract = {This article presents a solution to pH control based on model-free intelligent control (MFIC) using reinforcement learning. This control technique is proposed because the algorithm gives a general solution for acid-base system, yet simple enough for its implementation in existing control hardware. In standard reinforcement learning, the interaction between an agent and the environment is based on a fixed time scale: during learning, the agent can select several primitive actions depending on the system state. A novel solution is presented, using multi-step actions (MSA): actions on multiple time scales consist of several identical primitive actions. This solves the problem of determining a suitable fixed time scale to select control actions so as to trade off accuracy in control against learning complexity. The application of multi-step actions on a simulated pH process shows that the proposed MFIC learns to control adequately the neutralization process.}
}
@article{CHEN20211,
title = {Deep reinforcement learning for computation offloading in mobile edge computing environment},
journal = {Computer Communications},
volume = {175},
pages = {1-12},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001729},
author = {Miaojiang Chen and Tian Wang and Shaobo Zhang and Anfeng Liu},
keywords = {Internet of things (IoT), Reinforcement learning, Markov decision process, Computation offloading, Deep learning, Mobile edge computing},
abstract = {Recently, in order to distribute computing, networking resources, services, near terminals, mobile fog is gradually becoming the mobile edge computing (MEC) paradigm. In a mobile fog environment, the quality of service affected by offloading speeds and the fog processing, however the traditional fog method to solve the problem of computation resources allocation is difficult because of the complex network states distribution environment (that is, F-AP states, AP states, mobile device states and code block states). In this paper, to improve the fog resource provisioning performance of mobile devices, the learning-based mobile fog scheme with deep deterministic policy gradient (DDPG) algorithm is proposed. An offloading block pulsating discrete event system is modeled as a Markov Decision Processes (MDPs), which can realize the offloading computing without knowing the transition probabilities among different network states. Furthermore, the DDPG algorithm is used to solve the issue of state spaces explosion and learn an optimal offloading policy on distributed mobile fog computing. The simulation results show that our proposed scheme achieves 20%, 37%, 46% improvement on related performance compared with the policy gradient (PG), deterministic policy gradient (DPG) and actor–critic (AC) methods. Besides, compared with the traditional fog provisioning scheme, our scheme shows better cost performance of fog resource provisioning under different locations number and different task arrival rates.}
}
@article{JAIN20225708,
title = {Cybertwin-driven resource allocation using deep reinforcement learning in 6G-enabled edge environment},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part B},
pages = {5708-5720},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822000386},
author = {Vibha Jain and Bijendra Kumar and Aditya Gupta},
keywords = {Cybertwin, 6G, Resource allocation, Computation offloading, Deep reinforcement learning},
abstract = {The recent emergence of sixth-generation (6G) enabled wireless communication technology has resulted in the rapid proliferation of a wide range of real-time applications. These applications are highly data-computation intensive and generate huge data traffic. Cybertwin-driven edge computing emerges as a promising solution to satisfy massive user demand, but it also introduces new challenges. One of the most difficult challenges in edge networks is efficiently offloading tasks while managing computation, communication, and cache resources. Traditional statistical optimization methods are incapable of addressing the offloading problem in a dynamic edge computing environment. In this work, we propose a joint resource allocation and computation offloading scheme by integrating deep reinforcement learning in Cybertwin enabled 6G wireless networks. The proposed system uses the potential of the MATD3 algorithm to provide QoS to end-users by minimizing the overall latency and energy consumption with better management of cache resources. As these edge resources are deployed in inaccessible locations, therefore, we employ secure authentication mechanism for Cybertwins. The proposed system is implemented in a simulated environment, and the results are calculated for different performance metrics with previous benchmark methodologies such as RRA, GRA, and MADDPG. The comparative analysis reveals that the proposed MATD3 reduces end-to-end latency and energy consumption by 13.8% and 12.5% respectively over MADDPG with a 4% increase in successful task completion.}
}
@article{FRENAY20091494,
title = {QL2, a simple reinforcement learning scheme for two-player zero-sum Markov games},
journal = {Neurocomputing},
volume = {72},
number = {7},
pages = {1494-1507},
year = {2009},
note = {Advances in Machine Learning and Computational Intelligence},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2008.12.022},
url = {https://www.sciencedirect.com/science/article/pii/S0925231209000150},
author = {Benoît Frénay and Marco Saerens},
keywords = {Reinforcement learning, -Learning, Markov games, Two-player zero-sum games, Multi-agent},
abstract = {Markov games is a framework which can be used to formalise n-agent reinforcement learning (RL). Littman (Markov games as a framework for multi-agent reinforcement learning, in: Proceedings of the 11th International Conference on Machine Learning (ICML-94), 1994.) uses this framework to model two-agent zero-sum problems and, within this context, proposes the minimax-Q algorithm. This paper reviews RL algorithms for two-player zero-sum Markov games and introduces a new, simple, fast, algorithm, called QL2. QL2 is compared to several standard algorithms (Q-learning, Minimax and minimax-Q) implemented with the Qash library written in Python. The experiments show that QL2 converges empirically to optimal mixed policies, as minimax-Q, but uses a surprisingly simple and cheap updating rule.}
}
@article{YANG2022100974,
title = {Evolutionary reinforcement learning via cooperative coevolutionary negatively correlated search},
journal = {Swarm and Evolutionary Computation},
volume = {68},
pages = {100974},
year = {2022},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2021.100974},
url = {https://www.sciencedirect.com/science/article/pii/S221065022100136X},
author = {Peng Yang and Hu Zhang and Yanglong Yu and Mingjia Li and Ke Tang},
keywords = {Evolutionary algorithms, Deep reinforcement learning, Cooperative coevolution},
abstract = {Evolutionary algorithms (EAs) have been successfully applied to optimize the policies for Reinforcement Learning (RL) tasks due to their exploration ability. The recently proposed Negatively Correlated Search (NCS) provides a distinct parallel exploration search behavior and is expected to facilitate RL more effectively. Considering that the commonly adopted neural policies usually involves millions of parameters to be optimized, the direct application of NCS to RL may face a great challenge of the large-scale search space. To address this issue, this paper presents an NCS-friendly Cooperative Coevolution (CC) framework to scale-up NCS while largely preserving its parallel exploration search behavior. The issue of traditional CC that can deteriorate NCS is also discussed. Empirical studies on 10 popular Atari games show that the proposed method can significantly outperform three state-of-the-art deep RL methods with 50% less computational time by effectively exploring a 1.7 million-dimensional search space.}
}
@article{KRISHNAN2021103223,
title = {Reinforcement learning-based dynamic routing using mobile sink for data collection in WSNs and IoT applications},
journal = {Journal of Network and Computer Applications},
volume = {194},
pages = {103223},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103223},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521002241},
author = {Muralitharan Krishnan and Yongdo Lim},
keywords = {Wireless sensor networks, Internet of Things, Clustering, Reinforcement learning, Q-Learning, Mobile sink, Routing},
abstract = {Energy is one of the most critical resources for sensor devices that decides the network lifetime of the wireless sensor networks. In many circumstances, sensor devices consume more energy for data transmission, reception, and forwarding operations. The major challenge is to increase the network lifetime by implementing the latest research models to reduce the deployment and operational cost. Many existing methods address the application of the static sink with multi-hop routing. But most of them suffer from energy-hole issues and inefficient data collection due to the early death of sensor nodes. Most of the existing methods of learning require massive data with feature engineering which eventually increases the learning complexity. In order to avoid these issues, a robust reinforcement learning-based mobile sink model is proposed for dynamic routing with efficient data collection. In addition, the Q-Learning approach is implemented to induce automatic learning through the shortest route. Combining these strategies preserves network stability and efficiently improves the routing performance as well as the reward. The simulation results reveal that the proposed reinforcement learning-based mobile sink model extends the network lifetime, provides an improved learning time with more reward, and results in high efficiency when compared with existing methods.}
}
@incollection{TAYLOR2014217,
title = {Chapter 9 - Cerebellar and Prefrontal Cortex Contributions to Adaptation, Strategies, and Reinforcement Learning},
editor = {Narender Ramnani},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {210},
pages = {217-253},
year = {2014},
booktitle = {Cerebellar Learning},
issn = {0079-6123},
doi = {https://doi.org/10.1016/B978-0-444-63356-9.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444633569000091},
author = {Jordan A. Taylor and Richard B. Ivry},
keywords = {cerebellum, prefrontal cortex, basal ganglia, sensorimotor learning, adaptation, reinforcement learning, systems interaction, error-based learning, ataxia},
abstract = {Traditionally, motor learning has been studied as an implicit learning process, one in which movement errors are used to improve performance in a continuous, gradual manner. The cerebellum figures prominently in this literature given well-established ideas about the role of this system in error-based learning and the production of automatized skills. Recent developments have brought into focus the relevance of multiple learning mechanisms for sensorimotor learning. These include processes involving repetition, reinforcement learning, and strategy utilization. We examine these developments, considering their implications for understanding cerebellar function and how this structure interacts with other neural systems to support motor learning. Converging lines of evidence from behavioral, computational, and neuropsychological studies suggest a fundamental distinction between processes that use error information to improve action execution or action selection. While the cerebellum is clearly linked to the former, its role in the latter remains an open question.}
}
@article{WU2021212,
title = {Distributed reinforcement learning algorithm of operator service slice competition prediction based on zero-sum markov game},
journal = {Neurocomputing},
volume = {439},
pages = {212-222},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.01.061},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221000898},
author = {Guomin Wu and Guoping Tan and Jinxin Deng and Defu Jiang},
keywords = {Network slicing, Operator competition, Multi-agent, Zero-sum markov game, Reinforcement learning},
abstract = {As a key enabling technology in the emerging network, network slicing can dynamically provide on-demand service with distinct logical slice instance. While most related studies have mainly focused on resource management, this study targets solving business competition between two operator slices using artificial intelligence. In this competition, each operator slice tries to maximize its own payoff, meanwhile its opponent strives to minimize it. Moreover, two operators update their marketing strategies over time. Therefore, predicting its result is a challenge. After the zero-sum Markov game is modeled for the research problem, we present the min–max Q learning algorithm. In each market state, each slice attains its temporary optimal strategy using the min–max algorithm. In the Markov decision process, Q value is dynamically modified under different market states, and the final Q value presents predictive result for this competition. Finally, a mass of numerical results prove that the min–max Q learning algorithm outperforms the repeated game, in which market state is invariable over time.}
}
@article{XIA20161,
title = {Neural inverse reinforcement learning in autonomous navigation},
journal = {Robotics and Autonomous Systems},
volume = {84},
pages = {1-14},
year = {2016},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2016.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0921889015301652},
author = {Chen Xia and Abdelkader {El Kamel}},
keywords = {Inverse reinforcement learning, Learning from demonstration, Neural network, Autonomous navigation, Markov decision processes, Dynamic environments},
abstract = {Designing intelligent and robust autonomous navigation systems remains a great challenge in mobile robotics. Inverse reinforcement learning (IRL) offers an efficient learning technique from expert demonstrations to teach robots how to perform specific tasks without manually specifying the reward function. Most of existing IRL algorithms assume the expert policy to be optimal and deterministic, and are applied to experiments with relatively small-size state spaces. However, in autonomous navigation tasks, the state spaces are frequently large and demonstrations can hardly visit all the states. Meanwhile the expert policy may be non-optimal and stochastic. In this paper, we focus on IRL with large-scale and high-dimensional state spaces by introducing the neural network to generalize the expert’s behaviors to unvisited regions of the state space and an explicit policy representation is easily expressed by neural network, even for the stochastic expert policy. An efficient and convenient algorithm, Neural Inverse Reinforcement Learning (NIRL), is proposed. Experimental results on simulated autonomous navigation tasks show that a mobile robot using our approach can successfully navigate to the target position without colliding with unpredicted obstacles, largely reduce the learning time, and has a good generalization performance on undemonstrated states. Hence prove the robot intelligence of autonomous navigation transplanted from limited demonstrations to completely unknown tasks.}
}
@article{CHEON2022100,
title = {Reinforcement Learning based Multi‐Step Look‐Ahead Bayesian Optimization},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {7},
pages = {100-105},
year = {2022},
note = {13th IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems DYCOPS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.07.428},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322008291},
author = {Mujin Cheon and Haeun Byun and Jay H. Lee},
keywords = {Black box optimization, Bayesian optimization, surrogate modeling, acquisition function, reinforcement learning, dynamic programming, sequential decision making},
abstract = {This paper considers the situation where data‐based optimization is to be performed but data sampling is limited due to high cost and time. Such situations demand highly efficient data‐sampling and utilization and Bayesian optimization (BO) is the most commonly used method as it allows users to balance between exploration and exploitation in deciding where to sample next in the design space. However, the standard acquisition functions used in Bayesian optimization such as the expected improvement have been criticized for being greedy and myopic in many situations. To address the limitation of the standard acquisition functions of BO due to its near-sighted nature, this paper suggests a novel reinforcement learning based method which enables multi‐step lookahead Bayesian optimization. Several benchmark functions are tested to compare the performance of the RL based method against the traditional BO methods using expected improvement and its rollout-based extensions. The proposed method outperformed popular Bayesian optimization methods in the case study.}
}
@article{SHANG2021708,
title = {Deep reinforcement learning with reference system to handle constraints for energy-efficient train control},
journal = {Information Sciences},
volume = {570},
pages = {708-721},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.04.088},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521004291},
author = {Mengying Shang and Yonghua Zhou and Hamido Fujita},
keywords = {Deep reinforcement learning, Energy saving, Train control, Constraint handling},
abstract = {Train energy-efficient control involves complicated optimization processes subject to constraints such as speed, time, position and comfort requirements. Conventional optimization techniques are not apt at accumulating numerous solution instances into decision intelligence by learning for consecutively confronted new problems. Deep reinforcement learning (DRL), which can directly output control decisions based on current states, has shown great potentials for next-generation intelligent control. However, if the DRL is directly applied to energy-efficient train control, the received results are almost unsatisfactory. The reason lies in that the agent may get into confusion about how to trade off those constraints, and spend great computational time performing a large number of meaningless explorations. This article attempts to propose an approach of DRL with a reference system (DRL-RS) for proactive constraint handling, where the reference system deals with checking and correcting the agent’s learning progresses to avoid stepping farther and farther onto the erroneous road. The proposed approach is evaluated by the numerical experiments on train control in metro lines. The experimental results demonstrate that the DRL-RS can achieve faster learning convergence, compared with the directly applied DRL. Furthermore, it is possible to reduce more energy consumption than the commonly used genetic algorithm.}
}
@article{LEITE2023111027,
title = {Solving an energy resource management problem with a novel multi-objective evolutionary reinforcement learning method},
journal = {Knowledge-Based Systems},
volume = {280},
pages = {111027},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111027},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123007773},
author = {G.M.C. Leite and S. Jiménez-Fernández and S. Salcedo-Sanz and C.G. Marcelino and C.E. Pedreira},
keywords = {Multi-objective reinforcement learning, Policy search, Neuroevolution, Microgrids, Energy management},
abstract = {Microgrids have become popular candidates for integrating diverse energy sources into the power grid as means of reducing fossil fuel usage. Energy Resource Management (ERM) is a type of Unit Commitment problem, where a player operates a microgrid with diverse renewable generators integrated with an external supplier. Calculating the economic dispatch of each committed unit on a planning horizon is an NP-hard problem, and therefore, finding an exact solution is difficult. This paper presents a multi-objective solution to the ERM problem from the perspective of battery operation and external supplier dispatch. First, a novel multi-objective decision problem modeling is proposed that considers three objectives: cost, greenhouse gas emissions, and battery degradation. This framework involves a learning agent that controls the depth of discharge of a Lithium-Ion battery. To address the proposed problem, a new multi-objective algorithm called Multi-Objective Evolutionary Policy Search (MEPS) is introduced. The proposed algorithm uses NeuroEvolution of Augmenting Topologies structure to evolve artificial neural networks for estimating action-preference values considering multi-objective rewards. The MEPS performance is evaluated on both standard and newly-proposed benchmark problems, using the hypervolume as the evaluation metric. When compared to standard deep reinforcement learning, results showed that MEPS provides cost-effective, environmentally friendly, and efficient energy storage management solutions. Furthermore, MEPS effectively solves the proposed ERM problem by finding neural networks with a small number of nodes and connections, which are suitable for use in embedded control systems. Overall, MEPS proved to be a promising multi-objective approach in the transition to clean energy resources.}
}
@article{LIU2023126381,
title = {Policy ensemble gradient for continuous control problems in deep reinforcement learning},
journal = {Neurocomputing},
volume = {548},
pages = {126381},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126381},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223005040},
author = {Guoqiang Liu and Gang Chen and Victoria Huang},
keywords = {Robust policy gradient, Deep reinforcement learning, Policy ensemble gradient},
abstract = {Policy gradient algorithms for reinforcement learning (RL) have successfully tackled a broad range of high-dimensional continuous RL problems, including many challenging robotic control problems. These algorithms can be largely divided into two categories, i.e., on-policy algorithms and off-policy algorithms. Off-policy deep RL (DRL) algorithms enjoy better sample efficiency than and often outperform on-policy algorithms. However, cutting-edge off-policy algorithms still suffer from the low-quality estimation of policy gradients, resulting in compromised learning performance and high sensitivity to hyper-parameter settings. To address this issue, we propose a new concept of robust policy gradient (RPG). Driven by RPG, this paper further develops a new policy ensemble gradient (PEG) algorithm for DRL, inspired by the recent success of several ensemble DRL algorithms. PEG efficiently and effectively estimates RPG by using multiple policy gradients obtained respectively from several off-policy base learners in an ensemble. The estimated RPG is then utilized for training all base learners simultaneously. Comprehensive experiments have been performed on six Mujoco benchmark problems. Compared to four state-of-the-art off-policy algorithms and four cutting-edge ensemble policy gradient algorithms, our new PEG algorithm achieved highly competitive stability, performance and sample efficiency. Further analysis shows that PEG is insensitive to varied hyper-parameter settings, confirming the positive role of RPG in building reliable and effective off-policy DRL algorithms.}
}
@article{LEOTTAU2018130,
title = {Decentralized Reinforcement Learning of Robot Behaviors},
journal = {Artificial Intelligence},
volume = {256},
pages = {130-159},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217301674},
author = {David L. Leottau and Javier Ruiz-del-Solar and Robert Babuška},
keywords = {Reinforcement learning, Multi-agent systems, Decentralized control, Autonomous robots, Distributed artificial intelligence},
abstract = {A multi-agent methodology is proposed for Decentralized Reinforcement Learning (DRL) of individual behaviors in problems where multi-dimensional action spaces are involved. When using this methodology, sub-tasks are learned in parallel by individual agents working toward a common goal. In addition to proposing this methodology, three specific multi agent DRL approaches are considered: DRL-Independent, DRL Cooperative-Adaptive (CA), and DRL-Lenient. These approaches are validated and analyzed with an extensive empirical study using four different problems: 3D Mountain Car, SCARA Real-Time Trajectory Generation, Ball-Dribbling in humanoid soccer robotics, and Ball-Pushing using differential drive robots. The experimental validation provides evidence that DRL implementations show better performances and faster learning times than their centralized counterparts, while using less computational resources. DRL-Lenient and DRL-CA algorithms achieve the best final performances for the four tested problems, outperforming their DRL-Independent counterparts. Furthermore, the benefits of the DRL-Lenient and DRL-CA are more noticeable when the problem complexity increases and the centralized scheme becomes intractable given the available computational resources and training time.}
}
@article{CHEN2021119,
title = {Delay-aware model-based reinforcement learning for continuous control},
journal = {Neurocomputing},
volume = {450},
pages = {119-128},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221005427},
author = {Baiming Chen and Mengdi Xu and Liang Li and Ding Zhao},
keywords = {Model-based reinforcement learning, Markov decision process, Continuous control, Delayed system},
abstract = {Action delays degrade the performance of reinforcement learning in many real-world systems. This paper proposes a formal definition of delay-aware Markov Decision Process and proves it can be transformed into standard MDP with augmented states using the Markov reward process. We develop a delay-aware model-based reinforcement learning framework that can incorporate the multi-step delay into the learned system models without learning effort. Experiments with the Gym and MuJoCo platforms show that the proposed delay-aware model-based algorithm is more efficient in training and transferable between systems with various durations of delay compared with state-of-the-art model-free reinforcement learning methods.}
}
@article{LU2023112507,
title = {An active equalization method for redundant battery based on deep reinforcement learning},
journal = {Measurement},
volume = {210},
pages = {112507},
year = {2023},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2023.112507},
url = {https://www.sciencedirect.com/science/article/pii/S0263224123000714},
author = {Chenlei Lu and Jianlong Chen and Cong Chen and Yin Huang and Dongji Xuan},
keywords = {Deep reinforcement learning, Active equalization, Redundant battery, Switch control policy},
abstract = {Battery equalization is essential in the battery management system. In this paper, an active equalization method based on redundant battery is proposed. The equalization circuit consists of a battery string composed of multiple batteries connected in series and a redundant battery. During the discharging process, one cell in the string is selected by the switch controller to be paralleled with redundant cells for equalization purposes. On this basis, an optimal switch control strategy based on deep reinforcement learning (DRL) is proposed, which takes into account the battery's state of charge (SOC), state of health (SOH), and current distribution during parallel connection. The proposed optimal switching control strategy can achieve equalization with the least number of switching times. Simulation shows that, compared with the greedy algorithm and the rule algorithm, the strategy proposed in this paper can reduce the SOC inconsistency of the battery string to less than 1% with the minimum number of switching times.}
}
@article{PENG20231,
title = {Task offloading in Multiple-Services Mobile Edge Computing: A deep reinforcement learning algorithm},
journal = {Computer Communications},
volume = {202},
pages = {1-12},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2023.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0140366423000385},
author = {Ziyu Peng and Gaocai Wang and Wang Nong and Yu Qiu and Shuqiang Huang},
keywords = {Mobile edge computing, Service caching, Task offloading, Resource allocation, Deep reinforcement learning},
abstract = {Multiple-Services Mobile Edge Computing enables task-relate services cached in edge server to be dynamically updated, and thus provides great opportunities to offload tasks to edge server for execution. However, the requirements and popularity of services, the computing requirement and the amount of data transferred from users to edge server are dynamic with time. How to adaptively adjust the subset of total service types in the resource-limited edge server and determine the task offloading destination and resource allocation decisions to improve the overall system performance is a challenge problem. To solve this challenge, we firstly convert it into a Markov decision process, then propose a soft actor–critic deep reinforcement learning-based algorithm, called DSOR, to jointly determine not only the discrete decisions of service caching and task offloading but also the continuous allocation of bandwidth and computing resource. To improve the accuracy of our algorithm, we employ an efficient trick of converting the discrete action selection into a continuous space to deal with the key design challenge that arises from continuous-discrete hybrid action space. Additionally, to improve resource utilization, a novel reward function is integrated to our algorithm to speed up the convergence of training while making full use of system resources. Extensive numerical results show that compared with other baseline algorithms, our algorithm can effectively reduce the long-term average completion delay of tasks while accessing excellent performance in terms of stability.}
}
@article{ZHOU202246,
title = {Decentralized optimal large scale multi-player pursuit-evasion strategies: A mean field game approach with reinforcement learning},
journal = {Neurocomputing},
volume = {484},
pages = {46-58},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.01.141},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221015769},
author = {Zejian Zhou and Hao Xu},
keywords = {Approximate dynamic programming, Optimal control, Mean field game theory, Pursuit-evasion game, Reinforcement learning},
abstract = {In this paper, the intelligent design for the pursuit-evasion game with large scale multi-pursuer and multi-evader has been investigated. Due to the vast number of agents, the notorious ”Curse of Dimensionality” can seriously challenge the traditional design in multi-player pursuit-evasion game, especially under harsh environment with limited communication resource to support information exchange among multi-players. To address this intractable challenge, the emerging Mean Field Games (MFG) theory has been utilized to solve the optimal pursuit-evasion strategies based on a new form of probability density function (PDF) instead of detailed information from all the other players/agents. As such, not only the information exchange is reduced, but also the computation dimension for the optimal strategy derivation is decreased. Specifically, the MFG has been integrated into the pursuit-evasion game to generate a hierarchical structure where the pursuers and the evaders form two mean field groups separately. To online solve the mean field equations, i.e., two coupled partial differential equations, the actor-critic reinforcement learning mechanism is adopted and further extended to a novel actor-critic-mass-opponent (ACMO) approach. In ACMO, the actor neural network estimates the optimal control, the critic neural network approximates the optimal cost function, the mass neural network learns the agent’s group PDF, and the opponent neural network predicts the opponents’ average states in the form of PDF that causes maximum cost for the agent’s group. The Lyapunov theory is utilized to provide the convergence analysis for all neural networks and the stability analysis for the closed-loop system. Eventually, a series of numerical simulations are conducted to demonstrate the effectiveness of the developed scheme.}
}
@article{PARTALAS20091900,
title = {Pruning an ensemble of classifiers via reinforcement learning},
journal = {Neurocomputing},
volume = {72},
number = {7},
pages = {1900-1909},
year = {2009},
note = {Advances in Machine Learning and Computational Intelligence},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2008.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0925231208003184},
author = {Ioannis Partalas and Grigorios Tsoumakas and Ioannis Vlahavas},
keywords = {Reinforcement learning, Ensemble selection},
abstract = {This paper studies the problem of pruning an ensemble of classifiers from a reinforcement learning perspective. It contributes a new pruning approach that uses the Q-learning algorithm in order to approximate an optimal policy of choosing whether to include or exclude each classifier from the ensemble. Extensive experimental comparisons of the proposed approach against state-of-the-art pruning and combination methods show very promising results. Additionally, we present an extension that allows the improvement of the solutions returned by the proposed approach over time, which is very useful in certain performance-critical domains.}
}
@article{SARDARMEHNI2021197,
title = {Sub-optimal tracking in switched systems with fixed final time and fixed mode sequence using reinforcement learning},
journal = {Neurocomputing},
volume = {420},
pages = {197-209},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220314168},
author = {Tohid Sardarmehni and Xingyong Song},
keywords = {Optimal control, Tracking, Switched systems, Fixed mode sequence},
abstract = {Approximate dynamic programming is used to solve optimal tracking problems in switched systems with controlled subsystems and fixed mode sequence. Two feedback control solutions are generated such that the system tracks a desired reference signal, and the optimal switching instants are sought. Simulation results are provided to illustrate the effectiveness of the solutions.}
}
@article{ZHOU2023102955,
title = {CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning},
journal = {Journal of Systems Architecture},
volume = {142},
pages = {102955},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2023.102955},
url = {https://www.sciencedirect.com/science/article/pii/S1383762123001340},
author = {Ti Zhou and Man Lin},
keywords = {Energy management for small devices, Reinforcement learning with temporal encoding, Soft-deadline constrained application},
abstract = {Small devices are frequently used in IoT and smart-city applications to perform periodic dedicated tasks with soft deadlines. This work focuses on developing methods to derive efficient power-management methods for periodic tasks on small devices. We first study the limitations of the existing Linux built-in methods used in small devices. We illustrate three typical workload/system patterns that are challenging to manage with Linux’s built-in solutions. We develop a reinforcement-learning-based technique with temporal encoding to derive an effective DVFS governor even with the presence of the three system patterns. The derived governor uses only one performance counter, the same as the built-in Linux mechanism, and does not require an explicit task model for the workload. We implemented a prototype system on the Nvidia Jetson Nano Board and experimented with it with six applications, including two self-designed and four benchmark applications. Under different deadline constraints, our approach can quickly derive a DVFS governor that can adapt to performance requirements and outperform the built-in Linux approach in energy saving. On Mibench workloads, with performance slack ranging from 0.04 s to 0.4 s, the proposed method can save 3%–11% more energy compared to Ondemand. AudioReg and FaceReg applications tested have 5%–14% energy-saving improvement. We have open-sourced the implementation of our in-kernel quantized neural network engine. The codebase can be found at: https://github.com/coladog/tinyagent.}
}
@article{ZHAO2023811,
title = {Federated multi-objective reinforcement learning},
journal = {Information Sciences},
volume = {624},
pages = {811-832},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.12.083},
url = {https://www.sciencedirect.com/science/article/pii/S002002552201578X},
author = {Fangyuan Zhao and Xuebin Ren and Shusen Yang and Peng Zhao and Rui Zhang and Xinxin Xu},
keywords = {Reinforcement learning, Multi-objective optimization, Graph model, Federated learning, Differential privacy},
abstract = {Multi-objective reinforcement Learning (MORL) has significant potential for solving complex decision problems with conflicting objectives. Desiring sufficient training samples, it is promising to achieve federated MORL in large-scale distributed settings. However, itstill suffers from poor efficiency and high privacy risks. To mitigate the inefficiency issue, we first propose a novel probablistic algorithm PMORL that can seek an optimal policy via the expectation maximization (EM) algorithm with high efficiency. To extend PMORL to distributed settings with privacy protection, we then present the first federated MORL algorithm Fed-PMORL with client-level differential privacy (DP). In Fed-PMORL, personalized actors are trained and maintained at local clients whereas critics are aggregated and sanitized at the central server. Extensive experimental results in benchmark MORL environments demonstrate that Fed-PMORL under DP guarantees can achieve superior performance with high efficiency. In particular, compared with the state-of-the-art methods, PMORL and Fed-PMORL can save up to 50% training episodes for achieving the same model utility. With a sufficient number of clients (e.g., 1000 clients), Fed-PMORL with a formal DP guarantee shows utility comparable to that of the non-private algorithm.}
}
@article{ACHBANY20082507,
title = {Tuning continual exploration in reinforcement learning: An optimality property of the Boltzmann strategy},
journal = {Neurocomputing},
volume = {71},
number = {13},
pages = {2507-2520},
year = {2008},
note = {Artificial Neural Networks (ICANN 2006) / Engineering of Intelligent Systems (ICEIS 2006)},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2007.11.040},
url = {https://www.sciencedirect.com/science/article/pii/S0925231208002130},
author = {Youssef Achbany and François Fouss and Luh Yen and Alain Pirotte and Marco Saerens},
keywords = {Reinforcement learning, Markov decision processes, Exploration and exploitation, Maximum entropy, Shortest-path problems, Randomized strategy},
abstract = {This paper presents a model allowing to tune continual exploration in an optimal way by integrating exploration and exploitation in a common framework. It first quantifies exploration by defining the degree of exploration of a state as the entropy of the probability distribution for choosing an admissible action in that state. Then, the exploration/exploitation tradeoff is formulated as a global optimization problem: find the exploration strategy that minimizes the expected cumulated cost, while maintaining fixed degrees of exploration at the states. In other words, maximize exploitation for constant exploration. This formulation leads to a set of nonlinear iterative equations reminiscent of the value-iteration algorithm and demonstrates that the Boltzmann strategy based on the Q-value is optimal in this sense. Convergence of those equations to a local minimum is proved for a stationary environment. Interestingly, in the deterministic case, when there is no exploration, these equations reduce to the Bellman equations for finding the shortest path. Furthermore, if the graph of states is directed and acyclic, the nonlinear equations can easily be solved by a single backward pass from the destination state. Stochastic shortest-path problems and discounted problems are also studied, and links between our algorithm and the SARSA algorithm are examined. The theoretical results are confirmed by simple simulations showing that the proposed exploration strategy outperforms the ε-greedy strategy.}
}
@article{LU2019937,
title = {Incentive-based demand response for smart grid with reinforcement learning and deep neural network},
journal = {Applied Energy},
volume = {236},
pages = {937-949},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.12.061},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918318798},
author = {Renzhi Lu and Seung Ho Hong},
keywords = {Artificial intelligence, Reinforcement learning, Deep neural network, Incentive-based demand response, Smart grid},
abstract = {Balancing electricity generation and consumption is essential for smoothing the power grids. Any mismatch between energy supply and demand would increase costs to both the service provider and customers and may even cripple the entire grid. This paper proposes a novel real-time incentive-based demand response algorithm for smart grid systems with reinforcement learning and deep neural network, aiming to help the service provider to purchase energy resources from its subscribed customers to balance energy fluctuations and enhance grid reliability. In particular, to overcome the future uncertainties, deep neural network is used to predict the unknown prices and energy demands. After that, reinforcement learning is adopted to obtain the optimal incentive rates for different customers considering the profits of both service provider and customers. Simulation results show that this proposed incentive-based demand response algorithm induces demand side participation, promotes service provider and customers profitabilities, and improves system reliability by balancing energy resources, which can be regarded as a win-win strategy for both service provider and customers.}
}
@article{LI201820,
title = {Adaptive neural network tracking control-based reinforcement learning for wheeled mobile robots with skidding and slipping},
journal = {Neurocomputing},
volume = {283},
pages = {20-30},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.12.051},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217319136},
author = {Shu Li and Liang Ding and Haibo Gao and Chao Chen and Zhen Liu and Zongquan Deng},
keywords = {Wheeled mobile robot, Adaptive tracking control, Reinforcement learning, Neural network},
abstract = {To track the desired trajectories of the wheeled mobile robot (WMR) with time-varying forward direction, a reinforcement learning-based adaptive neural tracking algorithm is proposed for the nonlinear discrete-time (DT) dynamic system of the WMR with skidding and slipping. And, the typical model is transformed into an affine nonlinear DT system, the constraint of the coupling robot input torque is extended to pseudo dead zone (PDZ) control input. Three neural networks (NNs) are introduced as action NNs to approximate the unknown modeling item, the skidding and the slipping item and the PDZ item, whereas another NN is employed as critic NN to approximate the strategy utility function. Then, the critic and action NN adaptive laws are designed through the standard gradient-based adaptation method. The uniform ultimate boundedness (UUB) of all signals in the affine nonlinear DT WMR system can be ensured, while the tracking error converging to a small compact set by zero. Numerical simulations are conduced to validate the proposed method.}
}
@article{LI2023492,
title = {Task offloading mechanism based on federated reinforcement learning in mobile edge computing},
journal = {Digital Communications and Networks},
volume = {9},
number = {2},
pages = {492-504},
year = {2023},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000554},
author = {Jie Li and Zhiping Yang and Xingwei Wang and Yichao Xia and Shijian Ni},
keywords = {Mobile edge computing, Task offloading, QoS, Deep reinforcement learning, Federated learning},
abstract = {With the arrival of 5G, latency-sensitive applications are becoming increasingly diverse. Mobile Edge Computing (MEC) technology has the characteristics of high bandwidth, low latency and low energy consumption, and has attracted much attention among researchers. To improve the Quality of Service (QoS), this study focuses on computation offloading in MEC. We consider the QoS from the perspective of computational cost, dimensional disaster, user privacy and catastrophic forgetting of new users. The QoS model is established based on the delay and energy consumption and is based on DDQN and a Federated Learning (FL) adaptive task offloading algorithm in MEC. The proposed algorithm combines the QoS model and deep reinforcement learning algorithm to obtain an optimal offloading policy according to the local link and node state information in the channel coherence time to address the problem of time-varying transmission channels and reduce the computing energy consumption and task processing delay. To solve the problems of privacy and catastrophic forgetting, we use FL to make distributed use of multiple users’ data to obtain the decision model, protect data privacy and improve the model universality. In the process of FL iteration, the communication delay of individual devices is too large, which affects the overall delay cost. Therefore, we adopt a communication delay optimization algorithm based on the unary outlier detection mechanism to reduce the communication delay of FL. The simulation results indicate that compared with existing schemes, the proposed method significantly reduces the computation cost on a device and improves the QoS when handling complex tasks.}
}
@article{MADANI2014732,
title = {A game theory–reinforcement learning (GT–RL) method to develop optimal operation policies for multi-operator reservoir systems},
journal = {Journal of Hydrology},
volume = {519},
pages = {732-742},
year = {2014},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2014.07.061},
url = {https://www.sciencedirect.com/science/article/pii/S0022169414005952},
author = {Kaveh Madani and Milad Hooshyar},
keywords = {Game theory, Reinforcement learning, Reservoir operation, Conflict resolution, Optimization, Evolutionary algorithm},
abstract = {Summary
Reservoir systems with multiple operators can benefit from coordination of operation policies. To maximize the total benefit of these systems the literature has normally used the social planner’s approach. Based on this approach operation decisions are optimized using a multi-objective optimization model with a compound system’s objective. While the utility of the system can be increased this way, fair allocation of benefits among the operators remains challenging for the social planner who has to assign controversial weights to the system’s beneficiaries and their objectives. Cooperative game theory provides an alternative framework for fair and efficient allocation of the incremental benefits of cooperation. To determine the fair and efficient utility shares of the beneficiaries, cooperative game theory solution methods consider the gains of each party in the status quo (non-cooperation) as well as what can be gained through the grand coalition (social planner’s solution or full cooperation) and partial coalitions. Nevertheless, estimation of the benefits of different coalitions can be challenging in complex multi-beneficiary systems. Reinforcement learning can be used to address this challenge and determine the gains of the beneficiaries for different levels of cooperation, i.e., non-cooperation, partial cooperation, and full cooperation, providing the essential input for allocation based on cooperative game theory. This paper develops a game theory–reinforcement learning (GT–RL) method for determining the optimal operation policies in multi-operator multi-reservoir systems with respect to fairness and efficiency criteria. As the first step to underline the utility of the GT–RL method in solving complex multi-agent multi-reservoir problems without a need for developing compound objectives and weight assignment, the proposed method is applied to a hypothetical three-agent three-reservoir system.}
}
@article{ZHAO202256,
title = {Deep reinforcement learning guided graph neural networks for brain network analysis},
journal = {Neural Networks},
volume = {154},
pages = {56-67},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.06.035},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022002507},
author = {Xusheng Zhao and Jia Wu and Hao Peng and Amin Beheshti and Jessica J.M. Monaghan and David McAlpine and Heivet Hernandez-Perez and Mark Dras and Qiong Dai and Yangyang Li and Philip S. Yu and Lifang He},
keywords = {Brain network, Network representation learning, Graph neural network, Deep reinforcement learning},
abstract = {Modern neuroimaging techniques enable us to construct human brains as brain networks or connectomes. Capturing brain networks’ structural information and hierarchical patterns is essential for understanding brain functions and disease states. Recently, the promising network representation learning capability of graph neural networks (GNNs) has prompted related methods for brain network analysis to be proposed. Specifically, these methods apply feature aggregation and global pooling to convert brain network instances into vector representations encoding brain structure induction for downstream brain network analysis tasks. However, existing GNN-based methods often neglect that brain networks of different subjects may require various aggregation iterations and use GNN with a fixed number of layers to learn all brain networks. Therefore, how to fully release the potential of GNNs to promote brain network analysis is still non-trivial. In our work, a novel brain network representation framework, BN-GNN, is proposed to solve this difficulty, which searches for the optimal GNN architecture for each brain network. Concretely, BN-GNN employs deep reinforcement learning (DRL) to automatically predict the optimal number of feature propagations (reflected in the number of GNN layers) required for a given brain network. Furthermore, BN-GNN improves the upper bound of traditional GNNs’ performance in eight brain network disease analysis tasks.}
}
@article{LIN202046,
title = {Deep reinforcement learning and LSTM for optimal renewable energy accommodation in 5G internet of energy with bad data tolerant},
journal = {Computer Communications},
volume = {156},
pages = {46-53},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.03.024},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419321103},
author = {Lin Lin and Xin Guan and Benran Hu and Jun Li and Ning Wang and Di Sun},
keywords = {5G internet of energy, Renewable energy accommodation, Deep reinforcement learning, Demand response, LSTM},
abstract = {With the high penetration of large scale distributed renewable energy generations, there is a serious curtailment of wind and solar energy in 5G internet of energy. A reasonable assessment of large scale renewable energy grid-connected capacities under random scenarios is critical to promote the efficient utilization of renewable energy and improve the stability of power systems. To assure the authenticity of the data collected by the terminals and describe data characteristics precisely are crucial problems in assessing the accommodation capability of renewable energy. To solve these problems, in this paper, we propose an L-DRL algorithm based on deep reinforcement learning (DRL) to maximize renewable energy accommodation in 5G internet of energy. LSTM as a bad data tolerant mechanism provides real state value for the solution of accommodation strategy, which ensures the accurate assessment of renewable energy accommodation capacity. DDPG is used to obtain optimal renewable energy accommodation strategies in different scenarios. In the numerical results, based on real meteorological data, we validate the performance of the proposed algorithm. Results show considering the energy storage system and demand response mechanism can improve the capacity of renewable energy accommodation in 5G internet of energy.}
}
@article{RILEY20211061,
title = {Utilising Assured Multi-Agent Reinforcement Learning within Safety-Critical Scenarios},
journal = {Procedia Computer Science},
volume = {192},
pages = {1061-1070},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.109},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015970},
author = {Joshua Riley and Radu Calinescu and Colin Paterson and Daniel Kudenko and Alec Banks},
keywords = {Reinforcement Learning, Multi-Agent Systems, Quantitative Verification, Assurance, Multi-Agent Reinforcement Learning, Safety-Critical Scenarios, Safe Multi-Agent Reinforcement Learning, Assured Multi-Agent Reinforcement Learning},
abstract = {Multi-agent reinforcement learning allows a team of agents to learn how to work together to solve complex decision-making problems in a shared environment. However, this learning process utilises stochastic mechanisms, meaning that its use in safety-critical domains can be problematic. To overcome this issue, we propose an Assured Multi-Agent Reinforcement Learning (AMARL) approach that uses a model checking technique called quantitative verification to provide formal guarantees of agent compliance with safety, performance, and other non-functional requirements during and after the reinforcement learning process. We demonstrate the applicability of our AMARL approach in three different patrolling navigation domains in which multi-agent systems must learn to visit key areas by using different types of reinforcement learning algorithms (temporal difference learning, game theory, and direct policy search). Furthermore, we compare the effectiveness of these algorithms when used in combination with and without our approach. Our extensive experiments with both homogeneous and heterogeneous multi-agent systems of different sizes show that the use of AMARL leads to safety requirements being consistently satisfied and to better overall results than standard reinforcement learning.}
}
@article{LI2021107784,
title = {Towards learning behavior modeling of military logistics agent utilizing profit sharing reinforcement learning algorithm},
journal = {Applied Soft Computing},
volume = {112},
pages = {107784},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107784},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621007055},
author = {Xiong Li and Wei Pu and Xiaodong Zhao},
keywords = {Reinforcement learning, Profit sharing reinforcement learning algorithm, Learning behavior modeling, Agent-based modeling, Logistics system},
abstract = {Agent-based modeling has become a beneficial tool in describing the complex and intelligent decision-making behaviors of military logistics entities, which is essential in exploring military logistics system. A challenging task in this field is the learning behavior modeling of military logistics agents. Profit sharing (PS) reinforcement learning algorithm is a representative exploitation-oriented method describing empirical reinforcement learning mechanism, and has been successfully applied to a variety of real-world problems. However, constructing the learning behavior model of military logistics agents is difficult by merely using the original PS algorithm. This difficulty is due to the actual characteristics of equipment support operations and military requirements, such as experience sharing, cooperative action, and hierarchical control. To address this issue, we propose an improved PS algorithm by introducing cooperative task reward correction parameters, experience sharing learning function, and superior command controlled function. We use the research methodology centering on the basic process of the improved PS algorithm as basis to construct the architecture of the learning behavior model of military logistics agents and its corresponding model of elements. Furthermore, we design the implementation algorithm of the learning behavior model. Lastly, we conduct a case study of a tactical military industrial logistics simulation system, thereby verifying the feasibility and effectiveness of the learning behavior model. We find that the improved PS algorithm and corresponding learning behavior model have more advantages than the original PS algorithm.}
}
@article{TAGHAVI2023119160,
title = {A reinforcement learning model for the reliability of blockchain oracles},
journal = {Expert Systems with Applications},
volume = {214},
pages = {119160},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.119160},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422021789},
author = {Mona Taghavi and Jamal Bentahar and Hadi Otrok and Kaveh Bakhtiyari},
keywords = {Smart contract, Blockchain oracle, Multi-armed bandit, Reinforcement learning, Reputation model},
abstract = {Smart contracts struggle with the major limitation of operating on data that is solely residing on the blockchain network. The need of recruiting third parties, known as oracles, to assist smart contracts has been recognized with the emergence of blockchain technology. Oracles could be deviant and commit ill-intentioned behaviors, or be selfish and hide their actual available resources to gain optimal profit. Current research proposals employ oracles as trusted entities with no robust assessment mechanism, which entails a risk of turning them into centralized points of failure. The need for an effective method to select the most economical and rewarding oracles that are self-interested and act independently is somehow neglected. Thus, this paper proposes a Bayesian Bandit Learning Oracles Reliability (BLOR) mechanism to identify trustless and cost-efficient oracles. Within BLOR, we learn the behavior of oracles by formulating a Bayesian cost-dependent reputation model and utilize reinforcement learning (knowledge gradient algorithm) to guide the learning process. BLOR enables all the blockchain validators to verify the obtained results while running the algorithm at the same time by dealing with the randomness issue within the limited blockchain structure. We implement and experiment with BLOR using Python and the Solidity language on Ethereum. BLOR is benchmarked against several models where it proved to be highly efficient in selecting the most reliable and economical oracles with a fair balance.}
}
@article{ROKHFOROZ2023120010,
title = {Multi-agent reinforcement learning with graph convolutional neural networks for optimal bidding strategies of generation units in electricity markets},
journal = {Expert Systems with Applications},
volume = {225},
pages = {120010},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120010},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423005122},
author = {Pegah Rokhforoz and Mina Montazeri and Olga Fink},
keywords = {Graph convolutional neural network, Reinforcement learning, Multi agent, Power market},
abstract = {Finding optimal bidding strategies for generation units in electricity markets would result in higher profit. However, it is a challenging problem due to the system uncertainty which is due to the lack of knowledge of the strategies of other generation units. Distributed optimization, where each entity or agent decides on its bid individually, has become state of the art. However, it cannot overcome the challenges of system uncertainties. Deep reinforcement learning is a promising approach to learning the optimal strategy in uncertain environments. Nevertheless, it is not able to integrate the information on the spatial system topology into the learning process. This paper proposes a semi-distributed learning algorithm based on deep reinforcement learning (DRL) combined with a graph convolutional neural network (GCN). In fact, the proposed framework helps the generation units to update their decisions by getting feedback from the environment so that they can overcome the challenges of uncertainties. In this proposed algorithm, the state and connection between nodes are the inputs of the GCN, which can make generation units aware of the network structure of the system. This information on the system topology helps the generation units learn to improve their bidding strategies and increase their profit. We evaluate the proposed algorithm on the IEEE 30-bus system under different scenarios. Also, to investigate the generalization ability of the proposed approach, we test the trained model on the IEEE 39-bus system. The results show that the proposed algorithm has a better generalization ability compared to the DRL and can result in a higher profit when changing the topology of the system.}
}
@article{SHAHMARDAN2020106134,
title = {Truck scheduling in a multi-door cross-docking center with partial unloading – Reinforcement learning-based simulated annealing approaches},
journal = {Computers & Industrial Engineering},
volume = {139},
pages = {106134},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2019.106134},
url = {https://www.sciencedirect.com/science/article/pii/S0360835219306035},
author = {Amin Shahmardan and Mohsen S. Sajadieh},
keywords = {Logistics, Cross docking, Truck scheduling, Simulated annealing, Reinforcement learning},
abstract = {In this paper, a truck scheduling problem at a cross-docking center is investigated where inbound trucks are also used as outbound. Moreover, inbound trucks do not need to unload and reload the demand of allocated destination, i.e. they can be partially unloaded. The problem is modeled as a mixed integer program to find the optimal dock-door and destination assignments as well as the scheduling of trucks to minimize makespan. Due to model complexity, a hybrid heuristic-simulated annealing is developed. A number of generic and tailor-made neighborhood search structures are also developed to efficiently search solution space. Moreover, some reinforcement learning methods are applied to intellectually learn more suitable neighborhood search structures in different situations. Finally, the numerical study shows that partial unloading of compound trucks has a crucial impact on makespan reduction.}
}
@article{TCHAMITCHIAN2005131,
title = {DAILY TEMPERATURE OPTIMISATION IN GREENHOUSE BY REINFORCEMENT LEARNING},
journal = {IFAC Proceedings Volumes},
volume = {38},
number = {1},
pages = {131-136},
year = {2005},
note = {16th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20050703-6-CZ-1902.02112},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016381241},
author = {Marc Tchamitchian and Constantin Kittas and Thomas Bartzanas and Christos Lykas},
keywords = {Greenhouse, Climate control, Rose, Temperature, Heating, Reinforcement learning},
abstract = {The goal of this study is to show the usefulness of reinforcement learning (RL) to solve a common greenhouse climate optimisation problem. The problem is to minimise the daily heating cost while achieving simultaneously two agronomic goals, namely maintaining a good crop growth and an appropriate development rate. The complexity of the problem is due to the very different time constants of these two biological processes. First, a simple model for greenhouse roses is presented that simulates the daily crop growth and development. Second, the RL method is presented, in its application to this problem. Finally, optimisation results are presented and discussed.}
}
@article{LIU2022102816,
title = {The flying sidekick traveling salesman problem with stochastic travel time: A reinforcement learning approach},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {164},
pages = {102816},
year = {2022},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2022.102816},
url = {https://www.sciencedirect.com/science/article/pii/S1366554522002034},
author = {Zeyu Liu and Xueping Li and Anahita Khojandi},
keywords = {Drone, Dynamic vehicle routing problem, Traveling salesman problem, Markov decision process, Deep reinforcement learning, Artificial neural network},
abstract = {As a novel urban delivery approach, the coordinated operation of a truck–drone pair has gained increasing popularity, where the truck takes a traveling salesman route and the drone launches from the truck to deliver packages to nearby customers. Previous studies have referred to this problem as the flying sidekick traveling salesman problem (FSTSP) and have proposed numerous algorithms to solve it. However, few studies have considered the stochasticity of the travel time on the road network, mainly caused by traffic congestion, harsh weather conditions, etc, which heavily impacts the speed of the truck, thus affecting the drone’s operations and overall delivery routine. In this study, we extend the FSTSP with stochastic travel times and formulate the problem into a Markov decision process (MDP). The model is solved using reinforcement learning (RL) algorithms including the deep Q-network (DQN) and the Advantage Actor-Critic (A2C) algorithm to overcome the curse of dimensionality. Using an artificially generated dataset that was widely accepted as benchmarks in the literature, we show that the reinforcement learning algorithms also perform well as approximate optimization algorithms, outperforming a mixed integer programming (MIP) model and a local search heuristic algorithm on the original FSTSP without the stochastic travel time. On the FSTSP with stochastic travel time, the reinforcement learning algorithms obtain flexible policies that make dynamic decisions based on different traffic conditions on the roads, saving up to 28.65% on delivery time compared with the MIP model and a dynamic local search (DLS) algorithm. We also conduct a case study using real-time traffic data collected in a middle-sized city in the U.S. using Google Map API. Compared with a benchmark calculated by the DLS, the DRL approach saves 32.68% total delivery time in the case study, showing great potential for future practical adoption.}
}
@article{HUANG2023119011,
title = {A novel policy based on action confidence limit to improve exploration efficiency in reinforcement learning},
journal = {Information Sciences},
volume = {640},
pages = {119011},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119011},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523005960},
author = {Fanghui Huang and Xinyang Deng and Yixin He and Wen Jiang},
keywords = {Reinforcement learning, Exploration policy, Action confidence limit, Uncertainty of action, Deep auto-encoder network},
abstract = {Reinforcement learning has been used to solve many intelligent decision-making problems. However, reinforcement learning still faces a challenge of the low exploration efficiency problem in practice, limiting its widespread application. To address this issue, in this paper, a novel exploration policy based on Q value and exploration value is proposed. The exploration value adopts action confidence limit to measure the uncertainty of the action, which guides the agent to adaptively explore the uncertainty region of the environment. This method can improve exploration efficiency and is beneficial for the agent to make optimal decisions. Then, in order to make our proposed policy applicable to discrete and continuous environments, we combine the proposed policy with classic reinforcement learning algorithms (Q-learning and deep Q-network), and propose two novel algorithms, respectively. Moreover, the convergence of the algorithm is analyzed. Furthermore, a deep auto-encoder network model is used to establish the mapping relationship on state-action in discrete environments, which can avoid a large number of state-action pairs stored in Q-learning stage. Our proposed method can achieve adaptive and effective exploration, which is beneficial for the agent to make intelligent decisions. Finally, the results are verified in discrete and continuous simulation environments. Experimental results demonstrate that our method improves the average reward value and reduces the number of catastrophic actions.}
}
@article{WU2018799,
title = {Continuous reinforcement learning of energy management with deep Q network for a power split hybrid electric bus},
journal = {Applied Energy},
volume = {222},
pages = {799-811},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.03.104},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918304422},
author = {Jingda Wu and Hongwen He and Jiankun Peng and Yuecheng Li and Zhanjiang Li},
keywords = {Energy management strategy, Continuous reinforcement learning, Deep Q learning, Dynamic programming, Hybrid electric bus},
abstract = {Reinforcement learning is a new research hotspot in the artificial intelligence community. Q learning as a famous reinforcement learning algorithm can achieve satisfactory control performance without need to clarify the complex internal factors in controlled objects. However, discretization state is necessary which limits the application of Q learning in energy management for hybrid electric bus (HEB). In this paper the deep Q learning (DQL) is adopted for energy management issue and the strategy is proposed and verified. Firstly, the system modeling of bus configuration are described. Then, the energy management strategy based on deep Q learning is put forward. Deep neural network is employed and well trained to approximate the action value function (Q function). Furthermore, the Q learning strategy based on the same model is mentioned and applied to compare with deep Q learning. Finally, a part of trained decision network is analyzed separately to verify the effectiveness and rationality of the DQL-based strategy. The training results indicate that DQL-based strategy makes a better performance than that of Q learning in training time consuming and convergence rate. Results also demonstrate the fuel economy of proposed strategy under the unknown driving condition achieves 89% of dynamic programming-based method. In addition, the technique can finally learn to the target state of charge under different initial conditions. The main contribution of this study is to explore a novel reinforcement learning methodology into energy management for HEB which solve the curse of state variable dimensionality, and the techniques can be adopted to solve similar problems.}
}
@article{FEE2011152,
title = {A hypothesis for basal ganglia-dependent reinforcement learning in the songbird},
journal = {Neuroscience},
volume = {198},
pages = {152-170},
year = {2011},
note = {Function and Dysfunction of the Basal Ganglia},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2011.09.069},
url = {https://www.sciencedirect.com/science/article/pii/S0306452211011754},
author = {M.S. Fee and J.H. Goldberg},
keywords = {neural sequences, vocal learning, motor learning, striatum, direct pathway, indirect pathway},
abstract = {Most of our motor skills are not innately programmed, but are learned by a combination of motor exploration and performance evaluation, suggesting that they proceed through a reinforcement learning (RL) mechanism. Songbirds have emerged as a model system to study how a complex behavioral sequence can be learned through an RL-like strategy. Interestingly, like motor sequence learning in mammals, song learning in birds requires a basal ganglia (BG)-thalamocortical loop, suggesting common neural mechanisms. Here, we outline a specific working hypothesis for how BG-forebrain circuits could utilize an internally computed reinforcement signal to direct song learning. Our model includes a number of general concepts borrowed from the mammalian BG literature, including a dopaminergic reward prediction error and dopamine-mediated plasticity at corticostriatal synapses. We also invoke a number of conceptual advances arising from recent observations in the songbird. Specifically, there is evidence for a specialized cortical circuit that adds trial-to-trial variability to stereotyped cortical motor programs, and a role for the BG in “biasing” this variability to improve behavioral performance. This BG-dependent “premotor bias” may in turn guide plasticity in downstream cortical synapses to consolidate recently learned song changes. Given the similarity between mammalian and songbird BG-thalamocortical circuits, our model for the role of the BG in this process may have broader relevance to mammalian BG function. This article is part of a Special Issue entitled: Function and Dysfunction of the Basal Ganglia.}
}
@article{SUN2020107230,
title = {MARVEL: Enabling controller load balancing in software-defined networks with multi-agent reinforcement learning},
journal = {Computer Networks},
volume = {177},
pages = {107230},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107230},
url = {https://www.sciencedirect.com/science/article/pii/S138912861931566X},
author = {Penghao Sun and Zehua Guo and Gang Wang and Julong Lan and Yuxiang Hu},
keywords = {Multi-agent reinforcement learning, Neural networks, Software-defined networking, Switch migration},
abstract = {The control plane plays a significant role in Software-Defined Networking (SDN). A large SDN usually implements its control plane with several distributed controllers, each controlling a subset of switches and synchronizing with other controllers to maintain a consistent network view. Under the fluctuating network traffic, a static controller-switch mapping relationship could lead to imbalanced workload allocation. Controllers may getoverloaded and reject new requests, eventually reducing the control plane's request processing ability. Most existing schemes have relied heavily on iterative optimization algorithms to manipulate the mapping relationship between controllers and switches, which are either time-consuming or less satisfactory in terms of performance. In this paper, we propose a dynamic controller workload balancing scheme, that is termed MARVEL, based on multi-agent reinforcement learning for generation of switch migration actions. MARVEL works in two phases: offline training and online decision making. In the training phase, each agent learns how to migrate switches through interacting with the network. In the online phase, MARVEL is deployed to make decisions on migrating switches. Experimental results show that MARVEL outperforms competing existing schemes by improving the control plane's request processing ability at least 27.3% while using 25% less processing time.}
}
@article{MA2017278,
title = {Cooperative two-engine multi-objective bee foraging algorithm with reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {133},
pages = {278-293},
year = {2017},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0950705117303489},
author = {Lianbo Ma and Shi Cheng and Xingwei Wang and Min Huang and Hai Shen and Xiaoxian He and Yuhui Shi},
keywords = {Bee foraging, Multi-objective optimization, Indicator, Pareto},
abstract = {This paper proposes a novel multi-objective bee foraging algorithm (MOBFA) based on two-engine co-evolution mechanism for solving multi-objective optimization problems. The proposed MOBFA aims to handle the convergence and diversity separately via evolving two cooperative search engines with different evolution rules. Specifically, in the colony-level interaction, the primary concept is to first assign two different performance evaluation principles (i.e., Pareto-based measure and indicator-based measure) to the two engines for evolving each archive respectively, and then use the comprehensive learning mechanism over the two archives to boost the population diversity. In the individual-level foraging, the neighbor-discount-information (NDI) learning based on reinforcement learning (RL) is integrated into the single-objective searching to adjust the flight trajectories of foraging bee. By testing on a suit of benchmark functions, the proposed MOBFA is verified experimentally to be superior or at least comparable to its competitors in terms of two commonly used metrics IGD and SPREAD.}
}
@article{OCHOA2022119067,
title = {Multi-agent deep reinforcement learning for efficient multi-timescale bidding of a hybrid power plant in day-ahead and real-time markets},
journal = {Applied Energy},
volume = {317},
pages = {119067},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119067},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922004603},
author = {Tomás Ochoa and Esteban Gil and Alejandro Angulo and Carlos Valle},
keywords = {Multi-view artificial neural networks, Multi-agent deep reinforcement learning, Energy management system, Solar generation, Energy storage, Electricity market bidding, Multi-timescale electricity markets},
abstract = {Effective bidding on multiple electricity products under uncertainty would allow a more profitable market participation for hybrid power plants with variable energy resources and storage systems, therefore aiding the decarbonization process. This study deals with the effective bidding of a photovoltaic plant with an energy storage system (PV-ESS) participating in multi-timescale electricity markets by providing energy and ancillary services (AS) products. The energy management system (EMS) aims to maximize the plant’s profits by efficiently bidding in the day-ahead and real-time markets while considering the awarded products’ adequate delivery. EMS’s bidding decisions are usually obtained from traditional mathematical optimization frameworks. However, since the addressed problem is a multi-stage stochastic program, it is often intractable and suffers the curse of dimensionality. This paper presents a novel multi-agent deep reinforcement learning (MADRL) framework for efficient multi-timescale bidding. Two agents based on multi-view artificial neural networks with recurrent layers (MVANNs) are adjusted to map environment observations to actions. Such mappings use as inputs available information related to electricity market products, bidding decisions, solar generation, stored energy, and time representations to bid in both electricity markets. Sustained by a price-taker assumption, the physically and financially constrained EMS’s environment is simulated by employing historical data. A shared cumulative reward function with a finite time horizon is used to adjust both MVANNs’ weights simultaneously during the learning phase. We compare the proposed MADRL framework against scenario-based two-stage robust and stochastic optimization methods. Results are provided for one-year-round market participation of the hybrid plant at a 1-minute resolution. The proposed method achieved statistically significant higher profits, less variable incomes from both electricity markets, and better provision of awarded products by achieving smaller and less variable energy imbalances through time.}
}
@article{GAO2022118762,
title = {Model-augmented safe reinforcement learning for Volt-VAR control in power distribution networks},
journal = {Applied Energy},
volume = {313},
pages = {118762},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.118762},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922002148},
author = {Yuanqi Gao and Nanpeng Yu},
keywords = {Volt-VAR control, Data-driven, Deep reinforcement learning, Pathwise derivative, Safe exploration},
abstract = {Volt-VAR control (VVC) is a critical tool to manage voltage profiles and reactive power flow in power distribution networks by setting voltage regulating and reactive power compensation device status. To facilitate the adoption of VVC, many physical model-based and data-driven algorithms have been proposed. However, most of the physical model-based methods rely on distribution network parameters, whereas the data-driven algorithms lack safety guarantees. In this paper, we propose a data-driven safe reinforcement learning (RL) algorithm for the VVC problem. We introduce three innovations to improve the learning efficiency and the safety. First, we train the RL agent using a learned environment model to improve the sample efficiency. Second, a safety layer is added to the policy neural network to enhance operational constraint satisfactions for both initial exploration phase and convergence phase. Finally, to improve the algorithm’s performance when learning from limited data, we propose a novel mutual information regularization neural network for the safety layer. Simulation results on IEEE distribution test feeders show that the proposed algorithm improves constraint satisfactions compared to existing data-driven RL methods. With a modest amount of historical data, it is able to approximately maintain constraint satisfactions during the entire course of training. Asymptotically, it also yields similar level of performance of an ideal physical model-based benchmark. One possible limitation is that the proposed framework assumes a time-invariant distribution network topology and zero load transfer from other circuits. This is also an opportunity for future research.}
}
@article{KOU2020114772,
title = {Safe deep reinforcement learning-based constrained optimal control scheme for active distribution networks},
journal = {Applied Energy},
volume = {264},
pages = {114772},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.114772},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920302841},
author = {Peng Kou and Deliang Liang and Chen Wang and Zihao Wu and Lin Gao},
keywords = {Active distribution network, Constraint satisfaction, Deep deterministic policy gradient, Optimal voltage control, Smart transformer},
abstract = {Reinforcement learning-based schemes are being recently applied for model-free voltage control in active distribution networks. However, existing reinforcement learning methods face challenges when it comes to continuous state and action spaces problems or problems with operation constraints. To address these limitations, this paper proposes an optimal voltage control scheme based on the safe deep reinforcement learning. In this scheme, the optimal voltage control problem is formulated as a constrained Markov decision process, in which both state and action spaces are continuous. To solve this problem efficiently, the deep deterministic policy gradient algorithm is utilized to learn the reactive power control policies, which determine the optimal control actions from the states. In contrast to existing reinforcement learning methods, deep deterministic policy gradient is naturally capable of addressing control problems with continuous state and action spaces. This is due to the utilization of deep neural networks to approximate both value function and policy. In addition, in order to handle the operation constraints in active distribution networks, a safe exploration approach is proposed to form a safety layer, which is composed directly on top the deep deterministic policy gradient actor network. This safety layer predicts the change in the constrained states and prevents the violation of active distribution networks operation constraints. Numerical simulations on modified IEEE test systems demonstrate that the proposed scheme successfully maintains all bus voltage within the allowed range, and reduces the system loss by 15% compared to the no control case.}
}
@article{QIN2022103625,
title = {Energy optimization for regional buildings based on distributed reinforcement learning},
journal = {Sustainable Cities and Society},
volume = {78},
pages = {103625},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103625},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721008891},
author = {Yude Qin and Ji Ke and Biao Wang and Gennady Fedorovich Filaretov},
keywords = {Distributed reinforcement learning, Soft Actor-Critic strategy, Building energy optimization, Comprehensive evaluation},
abstract = {Model-free control approaches, such as Reinforcement Learning (RL), can be trained using historical data and therefore have the advantage of low cost and scalability. However, RL does not provide efficient, coordinated control for regional buildings, which leads to inter-building energy coupling, and thus resulting in higher energy consumption. This paper presents energy optimization strategies based on Distributed Reinforcement Learning (DRL) to reduce energy consumption in regional buildings while maintaining human comfort. The proposed strategy's system learns to regulate procedures to reduce building energy consumption through parameter sharing and coordination optimization. The energy optimization strategies are validated in this research by utilizing nine campus buildings as a case analysis. The results show that the system achieves the lowest total energy consumption with the employed strategies against the Rule-Based Control (RBC), Soft Actor-Critic (SAC) strategy, Model Predictive Control (MPC) and Non-dominated Sorting Genetic Algorithm Ⅱ (NSGA-Ⅱ). Furthermore, the proposed energy optimization strategies demonstrated good accuracy and robustness with a comprehensive evaluation of multi-building energy consumption in error analysis, load factor, power demand, and net power consumption.}
}
@article{HARROLD2022119151,
title = {Renewable energy integration and microgrid energy trading using multi-agent deep reinforcement learning},
journal = {Applied Energy},
volume = {318},
pages = {119151},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119151},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922005256},
author = {Daniel J.B. Harrold and Jun Cao and Zhong Fan},
keywords = {Deep deterministic policy gradients, Energy arbitrage, Hybrid energy storage systems, Multi-agent systems, Renewable energy},
abstract = {To reduce global greenhouse gas emissions, the world must find intelligent solutions to maximise the utilisation of carbon-free renewable energy sources. In this paper, multi-agent reinforcement learning is used to control a microgrid in a mixed cooperative and competitive setting. The agents observe fluctuating energy demand, dynamic wholesale energy prices, and intermittent renewable energy sources to control a hybrid energy storage system to maximise the utilisation of the renewables to reduce the energy costs of the grid. In addition, an aggregator agent trades with external microgrids competing against one another and the aggregator to reduce their own energy bills. For this, the algorithm deep deterministic policy gradients (DDPG) and multi-agent DDPG (MADDPG) are used to compare the use of a single global controller versus multiple distributed agents, along with the single and multi-agent variants of distributional DDPG (D3PG) and twin delayed DDPG (TD3). The research found it is significantly more profitable for the primary microgrid to sell energy on its own terms rather than selling back to the utility grid, and is also beneficial for the external microgrids as they also reduce their own energy bills. The methods that produced the greatest profits were the multi-agent approaches where each agent has its own reward function based on the principle of marginal contribution from game theory. The multi-agent approaches were better able to evaluate their performance controlling individual components of the environment which allowed them to develop their own unique policies for the different types of energy storage system.}
}
@article{ZHANG2023116647,
title = {Multi-agent deep reinforcement learning based distributed control architecture for interconnected multi-energy microgrid energy management and optimization},
journal = {Energy Conversion and Management},
volume = {277},
pages = {116647},
year = {2023},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.116647},
url = {https://www.sciencedirect.com/science/article/pii/S019689042201425X},
author = {Bin Zhang and Weihao Hu and Amer {M.Y.M. Ghias} and Xiao Xu and Zhe Chen},
keywords = {Multiagent deep reinforcement learning, Energy management, Energy Internet, Bottom-up, Distributed control},
abstract = {Environmental and climate change concerns are pushing the rapid development of new energy resources (DERs). The Energy Internet (EI), with the power-sharing functionality introduced by energy routers (ERs), offers an appealing alternative for DER systems. However, previous centralized control schemes for EI systems that follow a top-down architecture are unreliable for future power systems. This study proposes a distributed control scheme for bottom-up EI architecture. Second, model-based distributed control methods are not sufficiently flexible to deal with the complex uncertainties associated with multi-energy demands and DERs. A novel model-free/data-driven multiagent deep reinforcement learning (MADRL) method is proposed to learn the optimal operation strategy for the bottom-layer microgrid (MG) cluster. Unlike existing single-agent deep reinforcement learning methods that rely on homogeneous MG settings, the proposed MADRL adopts a form of decentralized execution, in which agents operate independently to meet local customized energy demands while preserving privacy. Third, an attention mechanism is added to the centralized critic, which can effectively accelerate the learning speed. Considering the bottom-layer power exchange request and the predicted electricity price, model predictive control of the upper layer determines the optimal power dispatching between the ERs and main grid. Simulations with other alternatives demonstrate the effectiveness of the proposed control scheme.}
}
@article{HUANG2022119353,
title = {Battery health-aware and naturalistic data-driven energy management for hybrid electric bus based on TD3 deep reinforcement learning algorithm},
journal = {Applied Energy},
volume = {321},
pages = {119353},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119353},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922006985},
author = {Ruchen Huang and Hongwen He and Xuyang Zhao and Yunlong Wang and Menglin Li},
keywords = {Hybrid electric bus, Energy management, Battery health, Driving cycle construction, Twin delayed deep deterministic policy gradient (TD3)},
abstract = {Energy management is critical to reduce energy consumption and extend the service life of hybrid power systems. This article proposes an energy management strategy based on deep reinforcement learning with awareness of battery health for an urban power-split hybrid electric bus. In this article, a specific driving cycle of the test bus route is constructed through a naturalistic data-driven method to evaluate the practical operating costs of the hybrid electric bus accurately. Furthermore, an energy management strategy based on twin delayed deep deterministic policy gradient algorithm considering battery health is innovatively designed to minimize the total operating cost with a tradeoff between fuel consumption and battery degradation. Finally, the superiority of the proposed strategy over other state-of-the-art deep reinforcement learning-based strategies including deep deterministic policy gradient and double deep Q-learning is validated. Simulation results show that the constructed driving cycle can effectively reflect the real traffic conditions of the test bus route, and the proposed strategy can reduce the total operating cost while extending the battery life efficiently. This article makes contribution to the reliable evaluation of the practical operating costs and the extension of the battery life for urban hybrid electric buses through deep reinforcement learning methods.}
}
@article{CHU2022108617,
title = {Optimal home energy management strategy: A reinforcement learning method with actor-critic using Kronecker-factored trust region},
journal = {Electric Power Systems Research},
volume = {212},
pages = {108617},
year = {2022},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2022.108617},
url = {https://www.sciencedirect.com/science/article/pii/S0378779622006915},
author = {Yunfei Chu and Zhinong Wei and Guoqiang Sun and Haixiang Zang and Sheng Chen and Yizhou Zhou},
keywords = {Appliance scheduling, Demand response, Deep reinforcement learning, Home energy management, Markov decision process},
abstract = {The global development trend of building decentralized and low-carbon energy systems has given rise to challenges in the demand side energy management. With the overwhelming increase in the domestic electricity demand and distributed energy penetration, scheduling residential energy consumption has become essential for decreasing environmental impact. To this end, the paper proposes an energy scheduling strategy optimization framework for a home energy management system devised with photovoltaic and storage to realize the optimal scheduling of household appliances. Uncertainties in energy usage behaviour and real-time electricity prices were considered and described using a Markov decision process model. Subsequently, to search for the optimal appliance scheduling policy in a complex and changing environment, a model-free energy scheduling approach was proposed based on actor-critic using the Kronecker-Factored Trust Region (ACKTR). Unlike conventional deep reinforcement learning methods, the proposed ACKTR-based approach has high sampling efficiency and can deal with both discrete and continuous control actions to jointly optimize the scheduling strategies of various types of appliances. Finally, the numerical results of case studies validate the effectiveness and demonstrate that the proposed approach can significantly reduce costs while maintaining satisfaction, with a remarkable 25.37% cost saving on a typical test day.}
}
@article{CHEN2022118394,
title = {GPDS: A multi-agent deep reinforcement learning game for anti-jamming secure computing in MEC network},
journal = {Expert Systems with Applications},
volume = {210},
pages = {118394},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118394},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422015044},
author = {Miaojiang Chen and Wei Liu and Ning Zhang and Junling Li and Yingying Ren and Meng Yi and Anfeng Liu},
keywords = {Deep reinforcement learning, Multi-agent, Secure computing, Decision-making, Mobile Edge Computing (MEC)},
abstract = {The openness of Mobile Edge Computing (MEC) networks makes them vulnerable to interference attacks by malicious jammers, which endangers the communication quality of mobile users. To achieve secure computing, the conventional method is that the mobile device reduces the attacker’s malicious interference by increasing the transmission power. However, the cost of power defense is unacceptable in MEC with resource shortages. Therefore, this paper considers a novel defense strategy based on time-varying channel and describes the malicious interference countermeasure process as a multi-user intelligent game model. Because the interference model and interference strategy are unknown, this paper proposes a deep reinforcement learning multi-user random Game with Post-Decision State (named GPDS) to intelligently resist intelligent attackers. In the GPDS algorithm, mobile users need to obtain the communication quality, spectrum availability, and jammer’s strategy from the state of the blocked channel. The reward of the optimal decision strategy is defined as the expected value of the maximum channel throughput, and the potential optimal channel selection strategy is obtained through Nash equilibrium. After GPDS training, mobile users can learn the optimal channel switching strategy after multi-step training. The experimental results show that the proposed GPDS achieves better anti-jamming performance, compared with SOTA algorithms.}
}
@article{KOSLOVSKI2024354,
title = {DAG-based workflows scheduling using Actor–Critic Deep Reinforcement Learning},
journal = {Future Generation Computer Systems},
volume = {150},
pages = {354-363},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003485},
author = {Guilherme Piêgas Koslovski and Kleiton Pereira and Paulo Roberto Albuquerque},
keywords = {Scheduling, Actor–critic, Deep reinforcement learning, DAG, Tasks, Jobs, Workflow},
abstract = {High-Performance Computing (HPC) is essential to support the advance in multiple research and industrial fields. Despite the recent growth in processing and networking power, the HPC Data Centers (DCs) are finite, and should be carefully managed to host multiple jobs. The scheduling of tasks (composing a job) is a crucial and complex task, once the reflexes of the scheduler’s decisions are perceptible both for users (e.g., slowdown) and for infrastructure administrators (e.g., use of resources and queue length). In fact, the process of scheduling workflows atop a DC can be modeled as a graph mapping problem. While an undirected graph is used to represent the DC, a Directed Acyclic Graph (DAG) is used to express the tasks dependencies. Each vertex and edge from both graphs can have weights associated with them, denoting the residual capacities for DC resources, as well as computing and networking demands for workflows. Motivated by the combinatorial explosion of the aforementioned scheduling problem, the integration of Machine Learning (ML) for generating or improving scheduling policies is a reality, however the proposals in the specialized literature opt, mostly, for using simplified models to reduce the search space or are trained to specific scenarios, which leads to policies that eventually fall short of real DCs expectations. Given this challenge, this work applies Actor–Critic (AC) Reinforcement Learning (RL) to schedule DAG-based workflows. Instead of proposing a new policy, the AC RL is used to select the appropriated scheduling policy from a pool of consolidated algorithms, guided by the DAGs workload and DC usage. The AC RL-based scheduler analyzes the DAGs queue and the DC status to define which algorithms are better suited to improve the overall performance indicators in each scenario instance. The simulation protocol comprises multiple analysis with distinct workload configurations, number of jobs, queue ordering polices and strategies to select the target DC servers. The results demonstrated that the AC RL selects the scheduling policy which fits the current workload and DC status.}
}
@article{LI20216054,
title = {An optimal coordinated proton exchange membrane fuel cell heat management method based on large-scale multi-agent deep reinforcement learning},
journal = {Energy Reports},
volume = {7},
pages = {6054-6068},
year = {2021},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S2352484721008192},
author = {Jiawen Li and Yaping Li and Tao Yu},
keywords = {Distributed deep reinforcement learning, Cooperative exploration strategy large-scale multi-agent twin-delay deep policy gradient (CESL-MATD3), proton exchange membrane fuel cell (PEMFC), coordinated control of stack temperature, Stack heat management system},
abstract = {To improve the operating efficiency of proton exchange membrane fuel cells (PEMFCs), an optimal coordinated control strategy for addressing the poor coordination problem between the water pump and radiator in a PEMFC stack heat management system is proposed in this paper. To this end, a cooperative exploration strategy large-scale multiagent twin-delay deep policy gradient (CESL-MATD3) algorithm has been developed for this control strategy. In this algorithm, both the water pump and radiator are treated as individual agents, and the strategies of centralized training and decentralized execution are applied; thus, coordinated control over the two agents is realized. Moreover, the concepts of curriculum learning, imitation learning, and various novel parallel computing techniques are incorporated into the design of this algorithm, resulting in enhanced training efficiency; thus, a coordinated control strategy with better robustness is obtained. According to the experimental results, compared with other advanced control algorithms, this coordinated control strategy-based algorithm achieves better performance and robustness for PEMFC stack temperature management. The proposed method can effectively improve the response speed of the controllers, reduce the fluctuation and oscillation of the stack temperature and the temperature difference between the stack outlet and inlet (stack temperature difference) during heat management, and reduce the maximum overshoot of the stack temperature by 99.12% and of the stack temperature difference by 97.97%.}
}
@article{BECSI2018429,
title = {Highway Environment Model for Reinforcement Learning ⁎⁎The research reported in this paper was supported by the Higher Education Excellence Program of the Ministry of Human Capacities in the frame of Artificial Intelligence research area of Budapest University of Technology and Economics (BME FIKPMI/FM).EFOP-3.6.3-VEKOP-16-2017-00001: Talent management in au-tonomous vehicle control technologies- The Project is supported by the Hungarian Government and co-financed by the European Social Fund},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {22},
pages = {429-434},
year = {2018},
note = {12th IFAC Symposium on Robot Control SYROCO 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.596},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318333032},
author = {Tamás Bécsi and Szilárd Aradi and Árpád Fehér and János Szalay and Péter Gáspár},
keywords = {Machine Learning, Reinforcement Learning Control, Autonomous Vehicles, Road traffic},
abstract = {The paper presents a microscopic highway simulation model, built as an environment for the development of different machine learning based autonomous vehicle controllers. The environment is based on the popular OpenAI Gym framework, hence it can be easily integrated into multiple projects. The traffic flow is operated by classic microscopic models, while the agent’s vehicle uses a rigid kinematic single-track model, with either continuous or discrete action spaces. The environment also provides a simple high-level sensor model, where the state of the agent and its surroundings are part of the observation. To aid the learning process, multiple reward functions are also provided.}
}