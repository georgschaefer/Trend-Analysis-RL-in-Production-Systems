@inproceedings{10.1145/3277644.3277796,
author = {Luo, Jieliang and Green, Sam},
title = {Bridging Reinforcement Learning and Creativity: Implementing Reinforcement Learning in Processing},
year = {2018},
isbn = {9781450360265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277644.3277796},
doi = {10.1145/3277644.3277796},
abstract = {Artists are underrepresented in the reinforcement learning (RL) community due to the steep learning curve involved in in-depth understanding of RL algorithms. However, artists can play an important role in the RL community by defining innovative problems, designing creative environments, and creating novel applications. As a popular tool for artists to experiment with programming, Processing has been highly adapted by many artists as their entry point to programming. Given the popularity of Processing in the creative community, we use this tutorial as a steppingstone to bridge RL and creativity by introducing RL core concepts in Processing. The purpose of this workshop is twofold: 1) to attract more artists to the RL community by demonstrating RL demos in their familiar IDE; 2) to demystify RL problems by implementing them in a high-level language without any external libraries. Importantly, this tutorial is not about introducing a specific programming language, but will focus on how to analyze, frame, and solve RL problems.},
booktitle = {SIGGRAPH Asia 2018 Courses},
articleno = {3},
numpages = {78},
location = {Tokyo, Japan},
series = {SA '18}
}

@inproceedings{10.1145/180139.181019,
author = {Fiechter, Claude-Nicolas},
title = {Efficient Reinforcement Learning},
year = {1994},
isbn = {0897916557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/180139.181019},
doi = {10.1145/180139.181019},
abstract = {In this paper we propose a new formal model for studying reinforcement learning, based on Valiant's PAC framework.In our model the learner does not have direct access to every state of the environment. Instead, every sequence of experiments starts in a fixed initial state and the learner is provided with a “reset” operation that interrupts the current sequence of experiments and starts a new one (from the initial state).We do not require the agent to learn the optimal policy but only a good approximation of it with high probability. More precisely, we require the learner to produce a policy whose expected value from the initial state is ε-close to that of the optimal policy, with probability no less than 1−δ.For this model, we describe an algorithm that produces such an (ε,δ)-optimal policy for any environment, in time polynomial in N,K,1/ε,1/δ,1/(1−β) and rmax, where N is the number of states of the environment, K is the maximum number of actions in a state, β is the discount factor and rmax is the maximum reward on any transition.},
booktitle = {Proceedings of the Seventh Annual Conference on Computational Learning Theory},
pages = {88–97},
numpages = {10},
location = {New Brunswick, New Jersey, USA},
series = {COLT '94}
}

@inproceedings{10.5555/3306127.3331809,
author = {Leibo, Joel Z. and Perolat, Julien and Hughes, Edward and Wheelwright, Steven and Marblestone, Adam H. and Du\'{e}\~{n}ez-Guzm\'{a}n, Edgar and Sunehag, Peter and Dunning, Iain and Graepel, Thore},
title = {Malthusian Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Here we explore a new algorithmic framework for multi-agent reinforcement learning, called Malthusian reinforcement learning, which extends self-play to include fitness-linked population size dynamics that drive ongoing innovation. In Malthusian RL, increases in a subpopulation's average return drive subsequent increases in its size, just as Thomas Malthus argued in 1798 was the relationship between preindustrial income levels and population growth. Malthusian reinforcement learning harnesses the competitive pressures arising from growing and shrinking population size to drive agents to explore regions of state and policy spaces that they could not otherwise reach. Furthermore, in environments where there are potential gains from specialization and division of labor, we show that Malthusian reinforcement learning is better positioned to take advantage of such synergies than algorithms based on self-play.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1099–1107},
numpages = {9},
keywords = {intrinsic motivation, demography, artificial general intelligence, adaptive radiation, evolution},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/1390156.1390194,
author = {Epshteyn, Arkady and Vogel, Adam and DeJong, Gerald},
title = {Active Reinforcement Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390194},
doi = {10.1145/1390156.1390194},
abstract = {When the transition probabilities and rewards of a Markov Decision Process (MDP) are known, an agent can obtain the optimal policy without any interaction with the environment. However, exact transition probabilities are difficult for experts to specify. One option left to an agent is a long and potentially costly exploration of the environment. In this paper, we propose another alternative: given initial (possibly inaccurate) specification of the MDP, the agent determines the sensitivity of the optimal policy to changes in transitions and rewards. It then focuses its exploration on the regions of space to which the optimal policy is most sensitive. We show that the proposed exploration strategy performs well on several control and planning problems.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {296–303},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.5555/3466184.3466363,
author = {Feldkamp, Niclas and Bergmann, Soeren and Strassburger, Steffen},
title = {Simulation-Based Deep Reinforcement Learning for Modular Production Systems},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Modular production systems aim to supersede the traditional line production in the automobile industry. The idea here is that highly customized products can move dynamically and autonomously through a system of flexible workstations without fixed production cycles. This approach has challenging demands regarding planning and organization of such systems. Since each product can define its way through the system freely and individually, implementing rules and heuristics that leverage the flexibility in the system in order to increase performance can be difficult in this dynamic environment. Transport tasks are usually carried out by automated guided vehicles (AGVs). Therefore, integration of AI-based control logics offer a promising alternative to manually implemented decision rules for operating the AGVs. This paper presents an approach for using reinforcement learning (RL) in combination with simulation in order to control AGVs in modular production systems. We present a case study and compare our approach to heuristic rules.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1596–1607},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.5555/2343576.2343607,
author = {Lau, Qiangfeng Peter and Lee, Mong Li and Hsu, Wynne},
title = {Coordination Guided Reinforcement Learning},
year = {2012},
isbn = {0981738117},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper, we propose to guide reinforcement learning (RL) with expert coordination knowledge for multi-agent problems managed by a central controller. The aim is to learn to use expert coordination knowledge to restrict the joint action space and to direct exploration towards more promising states, thereby improving the overall learning rate. We model such coordination knowledge as constraints and propose a two-level RL system that utilizes these constraints for online applications. Our declarative approach towards specifying coordination in multi-agent learning allows knowledge sharing between constraints and features (basis functions) for function approximation. Results on a soccer game and a tactical real-time strategy game show that coordination constraints improve the learning rate compared to using only unary constraints. The two-level RL system also outperforms existing single-level approach that utilizes joint action selection via coordination graphs.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {215–222},
numpages = {8},
keywords = {factored Markov decision process, guiding exploration, coordination constraints, reinforcement learning},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@inproceedings{10.5555/3398761.3398946,
author = {Zhang, Shangtong and Boehmer, Wendelin and Whiteson, Shimon},
title = {Deep Residual Reinforcement Learning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We revisit residual algorithms in both model-free and model-based reinforcement learning settings. We propose the bidirectional target network technique to stabilize residual algorithms, yielding a residual version of DDPG that significantly outperforms vanilla DDPG in the DeepMind Control Suite benchmark. Moreover, we find the residual algorithm an effective approach to the distribution mismatch problem in model-based planning. Compared with the existing TD(k) method, our residual-based method makes weaker assumptions about the model and yields a greater performance boost.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1611–1619},
numpages = {9},
keywords = {reinforcement learning, residual algorithms},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3439706.3446882,
author = {Taylor, Matthew E.},
title = {Reinforcement Learning for Electronic Design Automation: Successes and Opportunities},
year = {2021},
isbn = {9781450383004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439706.3446882},
doi = {10.1145/3439706.3446882},
abstract = {Reinforcement learning is a machine learning technique that has been applied in many domains, including robotics, game playing, and finance. This talk will briefly introduce reinforcement learning with two use cases related to compiler optimization and chip design. Interested participants will also have materials suggested to learn a more at a technical or non-technical level about this exciting tool.},
booktitle = {Proceedings of the 2021 International Symposium on Physical Design},
pages = {3},
numpages = {1},
keywords = {applied machine learning, problem formulation, reinforcement learning},
location = {Virtual Event, USA},
series = {ISPD '21}
}

@inproceedings{10.1145/3589883.3589920,
author = {Morgado, Ana C. and Souper, Tomas and Marques, Ana and Silva, In\^{e}s and Rosado, Lu\'{\i}s},
title = {Reinforcement Learning to Improve Color Adjustments in the Ceramic Industry},
year = {2023},
isbn = {9781450398329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589883.3589920},
doi = {10.1145/3589883.3589920},
abstract = {The ceramic industry is a highly competitive millenary sector with a substantial economic impact in several countries translated into a high global volume business, especially in exports. Reliability in the color development process is critical, ensuring that the produced pieces achieve the defined requirements. Nevertheless, current strategies for color formulation or adjustment are quite manual and subjective, essentially based on a trial-error process. These conventional procedures lead to the development of unnecessary pieces until the target color is achieved, which translates into a waste of raw materials and working time. In this paper, we present an automated approach based on Reinforcement Learning (RL) to improve color adjustments in the ceramic industry. By using the spectral data of the available components (pigments and glazes), the proposed algorithm provides the best formula to achieve the target color, i.e. the list of components for the mixture and corresponding quantities. Two datasets were used: the NTU Watercolor Pigments Spectral Measurement dataset; and the Matcer\^{a}mica Ceramics Spectral Measurement dataset. Three RL algorithms were trained and compared for benchmarking purposes: Deep Q-learning. (DQN), Advantage Actor-Critic (A2C) and Continual RL Without Conflict (OWL). The A2C and OWL models obtained similar performances for the NTU dataset with a mean of 0.668 and 0.733, respectively. For the Matcer\^{a}mica dataset, OWL yielded better results with a mean of 3.258. These results demonstrate the potential of the proposed approach to be integrated into an AI-powered software solution that optimizes the iterative process of color (re)creation in ceramic glazes.},
booktitle = {Proceedings of the 2023 8th International Conference on Machine Learning Technologies},
pages = {240–248},
numpages = {9},
keywords = {Process Optimization, Color Adjustment, Ceramics, Reinforcement Learning},
location = {Stockholm, Sweden},
series = {ICMLT '23}
}

@inproceedings{10.5555/3199700.3199814,
author = {Li, Hongjia and Wei, Tianshu and Ren, Ao and Zhu, Qi and Wang, Yanzhi},
title = {Deep Reinforcement Learning: Framework, Applications, and Embedded Implementations},
year = {2017},
publisher = {IEEE Press},
abstract = {The recent breakthroughs of deep reinforcement learning (DRL) technique in Alpha Go and playing Atari have set a good example in handling large state and actions spaces of complicated control problems. The DRL technique is comprised of (i) an offline deep neural network (DNN) construction phase, which derives the correlation between each state-action pair of the system and its value function, and (ii) an online deep Q-learning phase, which adaptively derives the optimal action and updates value estimates.In this paper, we first present the general DRL framework, which can be widely utilized in many applications with different optimization objectives. This is followed by the introduction of three specific applications: the cloud computing resource allocation problem, the residential smart grid task scheduling problem, and building HVAC system optimal control problem. The effectiveness of the DRL technique in these three cyber-physical applications have been validated. Finally, this paper investigates the stochastic computing-based hardware implementations of the DRL framework, which consumes a significant improvement in area efficiency and power consumption compared with binary-based implementation counterparts.},
booktitle = {Proceedings of the 36th International Conference on Computer-Aided Design},
pages = {847–854},
numpages = {8},
keywords = {optimal control, deep reinforcement learning, cyber-physical systems, stochastic computing},
location = {Irvine, California},
series = {ICCAD '17}
}

@inproceedings{10.1145/3439706.3446883,
author = {Goldie, Anna and Mirhoseini, Azalia},
title = {Reinforcement Learning for Placement Optimization},
year = {2021},
isbn = {9781450383004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439706.3446883},
doi = {10.1145/3439706.3446883},
abstract = {In the past decade, computer systems and chips have played a key role in the success of artificial intelligence (AI). Our vision in Google Brain's Machine Learning for Systems team is to use AI to transform the way in which computer systems and chips are designed. Many core problems in systems and hardware design are combinatorial optimization or decision making tasks with state and action spaces that are orders of magnitude larger than that of standard AI benchmarks in robotics and games. In this talk, we will describe some of our latest learning based approaches to tackling such large-scale optimization problems. We will discuss our work on a new domain-transferable reinforcement learning (RL) method for optimizing chip placement [1], a long pole in hardware design. Our approach is capable of learning from past experience and improving over time, resulting in more optimized placements on unseen chip blocks as the RL agent is exposed to a larger volume of data. Our objective is to minimize power, performance, and area. We show that, in under six hours, our method can generate placements that are superhuman or comparable on modern accelerator chips, whereas existing baselines require human experts in the loop and can take several weeks.},
booktitle = {Proceedings of the 2021 International Symposium on Physical Design},
pages = {5},
numpages = {1},
keywords = {deep learning, reinforcement learning, reinforcement learning for combinatorial optimization, device placement, placement optimization},
location = {Virtual Event, USA},
series = {ISPD '21}
}

@inproceedings{10.1145/3510455.3512782,
author = {Pritchard, Shadow and Nagaraju, Vidhyashree and Fiondella, Lance},
title = {Automating Staged Rollout with Reinforcement Learning},
year = {2022},
isbn = {9781450392242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510455.3512782},
doi = {10.1145/3510455.3512782},
abstract = {Staged rollout is a strategy of incrementally releasing software updates to portions of the user population in order to accelerate defect discovery without incurring catastrophic outcomes such as system wide outages. Some past studies have examined how to quantify and automate staged rollout, but stop short of simultaneously considering multiple product or process metrics explicitly. This paper demonstrates the potential to automate staged rollout with multi-objective reinforcement learning in order to dynamically balance stakeholder needs such as time to deliver new features and downtime incurred by failures due to latent defects.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {16–20},
numpages = {5},
keywords = {software reliability, DevOps, reinforcement learning, staged rollout},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-NIER '22}
}

@inproceedings{10.1145/3306618.3314259,
author = {Peysakhovich, Alexander},
title = {Reinforcement Learning and Inverse Reinforcement Learning with System 1 and System 2},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314259},
doi = {10.1145/3306618.3314259},
abstract = {Inferring a person's goal from their behavior is an important problem in applications of AI (e.g. automated assistants, recommender systems). The workhorse model for this task is the rational actor model - this amounts to assuming that people have stable reward functions, discount the future exponentially, and construct optimal plans. Under the rational actor assumption techniques such as inverse reinforcement learning (IRL) can be used to infer a person's goals from their actions. A competing model is the dual-system model. Here decisions are the result of an interplay between a fast, automatic, heuristic-based system 1 and a slower, deliberate, calculating system 2. We generalize the dual system framework to the case of Markov decision problems and show how to compute optimal plans for dual-system agents. We show that dual-system agents exhibit behaviors that are incompatible with rational actor assumption. We show that naive applications of rational-actor IRL to the behavior of dual-system agents can generate wrong inference about the agents' goals and suggest interventions that actually reduce the agent's overall utility. Finally, we adapt a simple IRL algorithm to correctly infer the goals of dual system decision-makers. This allows us to make interventions that help, rather than hinder, the dual-system agent's ability to reach their true goals.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {409–415},
numpages = {7},
keywords = {dual system model, behavioral economics, reinforcement learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3600211.3604698,
author = {Gilbert, Thomas Krendl and Lambert, Nathan and Dean, Sarah and Zick, Tom and Snoswell, Aaron and Mehta, Soham},
title = {Reward Reports for Reinforcement Learning},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604698},
doi = {10.1145/3600211.3604698},
abstract = {Building systems that are good for society in the face of complex societal effects requires a dynamic approach. Recent approaches to machine learning (ML) documentation have demonstrated the promise of discursive frameworks for deliberation about these complexities. However, these developments have been grounded in a static ML paradigm, leaving the role of feedback and post-deployment performance unexamined. Meanwhile, recent work in reinforcement learning has shown that the effects of feedback and optimization objectives on system behavior can be wide-ranging and unpredictable. In this paper we sketch a framework for documenting deployed and iteratively updated learning systems, which we call Reward Reports. Taking inspiration from technical concepts in reinforcement learning, we outline Reward Reports as living documents that track updates to design choices and assumptions behind what a particular automated system is optimizing for. They are intended to track dynamic phenomena arising from system deployment, rather than merely static properties of models or data. After presenting the elements of a Reward Report, we discuss a concrete example: Meta’s BlenderBot 3 chatbot. Several others for game-playing (DeepMind’s MuZero), content recommendation (MovieLens), and traffic control (Project Flow) are included in the appendix.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {84–130},
numpages = {47},
keywords = {documentation, Reward function, ethical considerations, reporting, disaggregated evaluation},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@article{10.1145/3582560,
author = {Sun, Shuo and Wang, Rundong and An, Bo},
title = {Reinforcement Learning for Quantitative Trading},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3582560},
doi = {10.1145/3582560},
abstract = {Quantitative trading (QT), which refers to the usage of mathematical models and data-driven techniques in analyzing the financial market, has been a popular topic in both academia and financial industry since 1970s. In the last decade, reinforcement learning (RL) has garnered significant interest in many domains such as robotics and video games, owing to its outstanding ability on solving complex sequential decision making problems. RL’s impact is pervasive, recently demonstrating its ability to conquer many challenging QT tasks. It is a flourishing research direction to explore RL techniques’ potential on QT tasks. This paper aims at providing a comprehensive survey of research efforts on RL-based methods for QT tasks. More concretely, we devise a taxonomy of RL-based QT models, along with a comprehensive summary of the state of the art. Finally, we discuss current challenges and propose future research directions in this exciting field.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {mar},
articleno = {44},
numpages = {29},
keywords = {survey, stock market, Reinforcement learning, quantitative finance}
}

@article{10.1145/3360567,
author = {Chen, Jia and Wei, Jiayi and Feng, Yu and Bastani, Osbert and Dillig, Isil},
title = {Relational Verification Using Reinforcement Learning},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360567},
doi = {10.1145/3360567},
abstract = {Relational verification aims to prove properties that relate a pair of programs or two different runs of the same program. While relational properties (e.g., equivalence, non-interference) can be verified by reducing them to standard safety, there are typically many possible reduction strategies, only some of which result in successful automated verification. Motivated by this problem, we propose a novel relational verification algorithm that learns useful reduction strategies using reinforcement learning. Specifically, we show how to formulate relational verification as a Markov Decision Process (MDP) and use reinforcement learning to synthesize an optimal policy for the underlying MDP. The learned policy is then used to guide the search for a successful verification strategy. We have implemented this approach in a tool called Coeus and evaluate it on two benchmark suites. Our evaluation shows that Coeus solves significantly more problems within a given time limit compared to multiple baselines, including two state-of-the-art relational verification tools.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {141},
numpages = {30},
keywords = {reinforcement learning, proof search, verification, relational property, policy gradient, neural network}
}

@article{10.5555/1314498.1390329,
author = {Ghavamzadeh, Mohammad and Mahadevan, Sridhar},
title = {Hierarchical Average Reward Reinforcement Learning},
year = {2007},
issue_date = {12/1/2007},
publisher = {JMLR.org},
volume = {8},
issn = {1532-4435},
abstract = {Hierarchical reinforcement learning (HRL) is a general framework for scaling reinforcement learning (RL) to problems with large state and action spaces by using the task (or action) structure to restrict the space of policies. Prior work in HRL including HAMs, options, MAXQ, and PHAMs has been limited to the discrete-time discounted reward semi-Markov decision process (SMDP) model. The average reward optimality criterion has been recognized to be more appropriate for a wide class of continuing tasks than the discounted framework. Although average reward RL has been studied for decades, prior work has been largely limited to flat policy representations.In this paper, we develop a framework for HRL based on the average reward optimality criterion. We investigate two formulations of HRL based on the average reward SMDP model, both for discrete-time and continuous-time. These formulations correspond to two notions of optimality that have been previously explored in HRL: hierarchical optimality and recursive optimality. We present algorithms that learn to find hierarchically and recursively optimal average reward policies under discrete-time and continuous-time average reward SMDP models.We use two automated guided vehicle (AGV) scheduling tasks as experimental testbeds to study the empirical performance of the proposed algorithms. The first problem is a relatively simple AGV scheduling task, in which the hierarchically and recursively optimal policies are different. We compare the proposed algorithms with three other HRL methods, including a hierarchically optimal discounted reward algorithm and a recursively optimal discounted reward algorithm on this problem. The second problem is a larger AGV scheduling task. We model this problem using both discrete-time and continuous-time models. We use a hierarchical task decomposition in which the hierarchically and recursively optimal policies are the same for this problem. We compare the performance of the proposed algorithms with a hierarchically optimal discounted reward algorithm and a recursively optimal discounted reward algorithm, as well as a non-hierarchical average reward algorithm. The results show that the proposed hierarchical average reward algorithms converge to the same performance as their discounted reward counterparts.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {2629–2669},
numpages = {41}
}

@inproceedings{10.1145/3526241.3530379,
author = {Sarihi, Amin and Patooghy, Ahmad and Jamieson, Peter and Badawy, Abdel-Hameed A.},
title = {Hardware Trojan Insertion Using Reinforcement Learning},
year = {2022},
isbn = {9781450393225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526241.3530379},
doi = {10.1145/3526241.3530379},
abstract = {This paper utilizes Reinforcement Learning (RL) as a means to automate the Hardware Trojan (HT) insertion process to eliminate the inherent human biases that limit the development of robust HT detection methods. An RL agent explores the design space and finds circuit locations that are best for keeping inserted HTs hidden. To achieve this, a digital circuit is converted to an environment in which an RL agent inserts HTs such that the cumulative reward is maximized. Our toolset can insert combinational HTs into the ISCAS-85 benchmark suite with variations in HT size and triggering conditions. Experimental results show that the toolset achieves high input coverage rates (100\% in two benchmark circuits) that confirms its effectiveness. Also, the inserted HTs have shown a minimal footprint and rare activation probability.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2022},
pages = {139–142},
numpages = {4},
keywords = {reinforcement learning, automated benchmarks, hardware Trojan},
location = {Irvine, CA, USA},
series = {GLSVLSI '22}
}

@inproceedings{10.1145/3474369.3486877,
author = {Kujanp\"{a}\"{a}, Kalle and Victor, Willie and Ilin, Alexander},
title = {Automating Privilege Escalation with Deep Reinforcement Learning},
year = {2021},
isbn = {9781450386579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474369.3486877},
doi = {10.1145/3474369.3486877},
abstract = {AI-based defensive solutions are necessary to defend networks and information assets against intelligent automated attacks. Gathering enough realistic data for training machine learning-based defenses is a significant practical challenge. An intelligent red teaming agent capable of performing realistic attacks can alleviate this problem. However, there is little scientific evidence demonstrating the feasibility of fully automated attacks using machine learning. In this work, we exemplify the potential threat of malicious actors using deep reinforcement learning to train automated agents. We present an agent that uses a state-of-the-art reinforcement learning algorithm to perform local privilege escalation. Our results show that the autonomous agent can escalate privileges in a Windows~7 environment using a wide variety of different techniques depending on the environment configuration it encounters. Hence, our agent is usable for generating realistic attack sensor data for training and evaluating intrusion detection systems.},
booktitle = {Proceedings of the 14th ACM Workshop on Artificial Intelligence and Security},
pages = {157–168},
numpages = {12},
keywords = {post-exploitation, autonomous cyber defense, attack automation, pomdp, actor-critic, red teaming, neural networks, a2c, privilege escalation, deep reinforcement learning, autonomous malware},
location = {Virtual Event, Republic of Korea},
series = {AISec '21}
}

@inproceedings{10.5555/3522802.3522831,
author = {Woo, Jong Hun and Cho, Young In and Nam, So Hyun and Nam, Jong-Ho},
title = {Development of a Reinforcement Learning-Based Adaptive Scheduling Algorithm for Block Assembly Production Line},
year = {2022},
publisher = {IEEE Press},
abstract = {Rule-based heuristic algorithms and meta-heuristic algorithms have been studied to solve the scheduling problems of production systems. In recent research, reinforcement learning-based adaptive scheduling algorithms have been applied to solve complex problems with high-dimensional and vast state space. A production system in shipyards is a high-variable system, in which various production factors such as space, workforce, and resources are related. Thus, adaptive scheduling according to the changes in the production system and surrounding environment must be performed in shipyards. In this study, a basic reinforcement learning model for scheduling problems of shipyards was developed. A simplified model of the panel block shop in shipyards was assumed and the optimal policy for determining the input sequence of blocks was learned to reduce the flow time. The open source-based discrete event simulation (DES) kernel SimPy was incorporated into the environment of the reinforcement learning model.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {29},
numpages = {12},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3322645.3322693,
author = {Li, Meng-Jhe and Li, An-Hong and Huang, Yu-Jung and Chu, Shao-I},
title = {Implementation of Deep Reinforcement Learning},
year = {2019},
isbn = {9781450361033},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322645.3322693},
doi = {10.1145/3322645.3322693},
abstract = {Reinforcement Learning (RL) is different from supervised learning, which is learning from a training set of labeled examples provided by a knowledgable external supervisor. RL is also different from unsupervised learning, which is typically about finding structure hidden in collections of unlabeled data. A Deep-Q-Network (DQN) RL system relied heavily on GPUs to accelerate computation. However, it is challenging to implement and deploy an RL model in an embedded system which has limited computing units and programming capacity. PYNQ with CPU-FPGA heterogeneous architecture is a platform that aims at developing embedded systems based on FPGA. This paper aims at constructing a fast FPGA prototyping framework for Cart-Pole problem on PYNQ platform.},
booktitle = {Proceedings of the 2nd International Conference on Information Science and Systems},
pages = {232–236},
numpages = {5},
keywords = {FPGA, Reinforcement Learning, Neural Network},
location = {Tokyo, Japan},
series = {ICISS '19}
}

@inproceedings{10.5555/2936924.2937079,
author = {Shiarlis, Kyriacos and Messias, Joao and Whiteson, Shimon},
title = {Inverse Reinforcement Learning from Failure},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Inverse reinforcement learning (IRL) allows autonomous agents to learn to solve complex tasks from successful demonstrations. However, in many settings, e.g., when a human learns the task by trial and error, failed demonstrations are also readily available. In addition, in some tasks, purposely generating failed demonstrations may be easier than generating successful ones. Since existing IRL methods cannot make use of failed demonstrations, in this paper we propose inverse reinforcement learning from failure (IRLF) which exploits both successful and failed demonstrations. Starting from the state-of-the-art maximum causal entropy IRL method, we propose a new constrained optimisation formulation that accommodates both types of demonstrations while remaining convex. We then derive update rules for learning reward functions and policies. Experiments on both simulated and real-robot data demonstrate that IRLF converges faster and generalises better than maximum causal entropy IRL, especially when few successful demonstrations are available.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {1060–1068},
numpages = {9},
keywords = {inverse reinforcement learning, machine learning., social navigation, learning from demonstration, robotics},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.1145/375735.376302,
author = {Makar, Rajbala and Mahadevan, Sridhar and Ghavamzadeh, Mohammad},
title = {Hierarchical Multi-Agent Reinforcement Learning},
year = {2001},
isbn = {158113326X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375735.376302},
doi = {10.1145/375735.376302},
abstract = {In this paper we investigate the use of hierarchical reinforcement learning to speed up the acquisition of cooperative multi-agent tasks. We extend the MAXQ framework to the multi-agent case. Each agent uses the same MAXQ hierarchy to decompose a task into sub-tasks. Learning is decentralized, with each agent learning three interrelated skills: how to perform subtasks, which order to do them in, and how to coordinate with other agents. Coordination skills among agents are learned by using joint actions at the highest level(s) of the hierarchy. The Q nodes at the highest level(s) of the hierarchy are configured to represent the joint task-action space among multiple agents. In this approach, each agent only knows what other agents are doing at the level of sub-tasks, and is unaware of lower level (primitive) actions. This hierarchical approach allows agents to learn coordination faster by sharing information at the level of sub-tasks, rather than attempting to learn coordination taking into account primitive joint state-action values. We apply this hierarchical multi-agent reinforcement learning algorithm to a complex AGV scheduling task and compare its performance and speed with other learning approaches, including flat multi-agent, single agent using MAXQ, selfish multiple agents using MAXQ (where each agent acts independently without communicating with the other agents), as well as several well-known AGV heuristics like "first come first serve", "highest queue first" and "nearest station first". We also compare the tradeoffs in learning speed vs. performance of modeling joint action values at multiple levels in the MAXQ hierarchy.},
booktitle = {Proceedings of the Fifth International Conference on Autonomous Agents},
pages = {246–253},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {AGENTS '01}
}

@inproceedings{10.1145/3404835.3462813,
author = {Kuhnle, Alexander and Aroca-Ouellette, Miguel and Basu, Anindya and Sensoy, Murat and Reid, John and Zhang, Dell},
title = {Reinforcement Learning for Information Retrieval},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462813},
doi = {10.1145/3404835.3462813},
abstract = {There is strong interest in leveraging reinforcement learning (RL) for information retrieval (IR) applications including search, recommendation, and advertising. Just in 2020, the term "reinforcement learning" was mentioned in more than 60 different papers published by ACM SIGIR. It has also been reported that Internet companies like Google and Alibaba have started to gain competitive advantages from their RL-based search and recommendation engines. This full-day tutorial gives IR researchers and practitioners who have no or little experience with RL the opportunity to learn about the fundamentals of modern RL in a practical hands-on setting. Furthermore, some representative applications of RL in IR systems will be introduced and discussed. By attending this tutorial, the participants will acquire a good knowledge of modern RL concepts and standard algorithms such as REINFORCE and DQN. This knowledge will help them better understand some of the latest IR publications involving RL, as well as prepare them to tackle their own practical IR problems using RL techniques and tools. Please refer to the tutorial website (https://rl-starterpack.github.io/) for more information.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2669–2672},
numpages = {4},
keywords = {actor-critic methods, recommender systems, deep Q-networks, Markov decision process, computational advertising, search engines, policy gradient},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.5555/3398761.3399004,
author = {Goindani, Mahak and Neville, Jennifer},
title = {Cluster-Based Social Reinforcement Learning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Social Reinforcement Learning considers multi-agent systems with large number of agents and relatively few interactions between them, which is challenging due to high-dimensional search space, inter-agent dependencies that increase computational complexity. Moreover sparse agent interactions produce insufficient data to capture higher-order relations (interactions) for learning accurate policies. To overcome these challenges, we present a dynamic cluster-based Social RL approach that utilizes the properties of the social network structure, agent interactions, and correlations to obtain a compact model to represent network dynamics.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1852–1854},
numpages = {3},
keywords = {social networks, multi-agent learning, reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.5555/3539845.3540117,
author = {Synthesis, Structure and Zhao, Zhenxin and Zhang, Lihong},
title = {Deep Reinforcement Learning for Analog Circuit},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {This paper presents a novel deep-reinforcement-learning-based method for analog circuit structure synthesis. It behaves like a designer, who learns from trials, derives design knowledge and experience, and evolves gradually to eventually figure out a way to construct circuit structures that can meet the given design specifications. Necessary design rules are defined and applied to set up the specialized environment of reinforcement learning in order to reasonably construct circuit structures. The produced circuit structures are then verified by the simulation-in-loop sizing. In addition, hash table and symbolic analysis techniques are employed to significantly promote the evaluation efficiency. Our experimental results demonstrate the sound efficiency, strong reliability, and wide applicability of the proposed method.},
booktitle = {Proceedings of the 2022 Conference \&amp; Exhibition on Design, Automation \&amp; Test in Europe},
pages = {1157–1160},
numpages = {4},
keywords = {deep reinforcement learning, hash table, analog circuit synthesis},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3511808.3557245,
author = {Zhang, Ruitong and Peng, Hao and Dou, Yingtong and Wu, Jia and Sun, Qingyun and Li, Yangyang and Zhang, Jingyi and Yu, Philip S.},
title = {Automating DBSCAN via Deep Reinforcement Learning},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557245},
doi = {10.1145/3511808.3557245},
abstract = {DBSCAN is widely used in many scientific and engineering fields because of its simplicity and practicality. However, due to its high sensitivity parameters, the accuracy of the clustering result depends heavily on practical experience. In this paper, we first propose a novel Deep Reinforcement Learning guided automatic DBSCAN parameters search framework, namely DRL-DBSCAN. The framework models the process of adjusting the parameter search direction by perceiving the clustering environment as a Markov decision process, which aims to find the best clustering parameters without manual assistance. DRL-DBSCAN learns the optimal clustering parameter search policy for different feature distributions via interacting with the clusters, using a weakly-supervised reward training policy network. In addition, we also present a recursive search mechanism driven by the scale of the data to efficiently and controllably process large parameter spaces. Extensive experiments are conducted on five artificial and real-world datasets based on the proposed four working modes. The results of offline and online tasks show that the DRL-DBSCAN not only consistently improves DBSCAN clustering accuracy by up to 26\% and 25\% respectively, but also can stably find the dominant parameters with high computational efficiency. The code is available at https://github.com/RingBDStack/DRL-DBSCAN.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {2620–2630},
numpages = {11},
keywords = {density-based clustering, hyperparameter search, recursive mechanism, deep reinforcement learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.5555/2946645.3007020,
author = {Barreto, Andr\'{e} M. S. and Precup, Doina and Pineau, Joelle},
title = {Practical Kernel-Based Reinforcement Learning},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Kernel-based reinforcement learning (KBRL) stands out among approximate reinforcement learning algorithms for its strong theoretical guarantees. By casting the learning problem as a local kernel approximation, KBRL provides a way of computing a decision policy which converges to a unique solution and is statistically consistent. Unfortunately, the model constructed by KBRL grows with the number of sample transitions, resulting in a computational cost that precludes its application to large-scale or on-line domains. In this paper we introduce an algorithm that turns KBRL into a practical reinforcement learning tool. Kernel-based stochastic factorization (KBSF) builds on a simple idea: when a transition probability matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix, potentially much smaller than the original, which retains some fundamental properties of its precursor. KBSF exploits such an insight to compress the information contained in KBRL's model into an approximator of fixed size. This makes it possible to build an approximation considering both the difficulty of the problem and the associated computational cost. KBSF's computational complexity is linear in the number of sample transitions, which is the best one can do without discarding data. Moreover, the algorithm's simple mechanics allow for a fully incremental implementation that makes the amount of memory used independent of the number of sample transitions. The result is a kernel-based reinforcement learning algorithm that can be applied to large-scale problems in both off-line and on-line regimes. We derive upper bounds for the distance between the value functions computed by KBRL and KBSF using the same data. We also prove that it is possible to control the magnitude of the variables appearing in our bounds, which means that, given enough computational resources, we can make KBSF's value function as close as desired to the value function that would be computed by KBRL using the same set of sample transitions. The potential of our algorithm is demonstrated in an extensive empirical study in which KBSF is applied to difficult tasks based on real-world data. Not only does KBSF solve problems that had never been solved before, but it also significantly outperforms other state-of-the-art reinforcement learning algorithms on the tasks studied.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2372–2441},
numpages = {70},
keywords = {Markov decision processes, dynamic programming, kernel-based approximation, reinforcement learning, stochastic factorization}
}

@inproceedings{10.1145/1963564.1963578,
author = {Yadav, Anil Kumar and Shrivastava, Shaillendra Kumar},
title = {Evaluation of Reinforcement Learning Techniques},
year = {2011},
isbn = {9781450304085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963564.1963578},
doi = {10.1145/1963564.1963578},
abstract = {Reinforcement learning is became one of the most important approaches to machine intelligence. Now RL is widely use by different research field as intelligent control, robotics and neuroscience. It provides us possible solution within unknown environment, but at the same time we have to take care of its decision because RL can independently learn without prior knowledge or training and it take decision by learning experience through trial-and-error interaction with its environment. In recent time many research works was done for RL and researchers has also proposed various algorithm and model such as SARSA [2], TDN [3] which tries to solve sequential decision making problems of continuous state and action space.In this paper we proposed Q-learning algorithm and evaluation of RL techniques (Reinforcement learning architecture, algorithms for making training matrix in the form of state-action pair Q-table) containing learner (decision making agent) that takes actions in an environment and receive reward for (or penalty) its actions in trying to solves a problems. Learning agent, the fundamental element of reinforcement learning, there is a decision maker that receive and select an action for the system.In reinforcement learning technique especially in Query base self learning the learner (Agent) required a lot of training input of execution cycle. In order to assess and comparison of QA and TDN based reinforcement learning, we found that QA is better in the context of discount rate, learning time, memory usage.},
booktitle = {Proceedings of the First International Conference on Intelligent Interactive Technologies and Multimedia},
pages = {88–92},
numpages = {5},
keywords = {learning agent, environment, reward, reinforcement learning, state-action, Q-learning},
location = {Allahabad, India},
series = {IITM '10}
}

@inproceedings{10.5555/3545946.3599024,
author = {Dohmen, Taylor and Trivedi, Ashutosh},
title = {Reinforcement Learning with Depreciating Assets},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {A basic assumption of traditional reinforcement learning is that the value of a reward does not change once it is received by an agent. The present work forgoes this assumption and considers the situation where the value of a reward decays proportionally to the time elapsed since it was obtained. Emphasizing the inflection point occurring at the time of payment, we use the term asset to refer to a reward that is currently in the possession of an agent. Adopting this language, we initiate the study of depreciating assets within the framework of infinite-horizon quantitative optimization. In particular, we propose a notion of asset depreciation, inspired by classical exponential discounting, where the value of an asset is scaled by a fixed discount factor at each time step after it is obtained by the agent. We formulate an equational characterization of optimality in this context, establish that optimal values and policies can be computed efficiently, and develop a model-free reinforcement learning approach to obtain optimal policies.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2628–2630},
numpages = {3},
keywords = {temporal discounting, asset depreciation, reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3306127.3332032,
author = {Mern, John and Sadigh, Dorsa and Kochenderfer, Mykel},
title = {Object Exchangability in Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Although deep reinforcement learning has advanced significantly over the past several years, sample efficiency remains a major challenge. Careful choice of input representations can help improve efficiency depending on the structure present in the problem. In this work, we present an attention-based method to project inputs into an efficient representation space that is invariant under changes to input ordering. We show that our proposed representation results in a search space that is a factor of m! smaller for inputs of m objects. Our experiments demonstrate improvements in sample efficiency for policy gradient methods on a variety of tasks. We show that our representation allows us to solve problems that are otherwise intractable when using naive approaches.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2126–2128},
numpages = {3},
keywords = {reasoning, knowledge representation, reinforcement learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inbook{10.1145/3502398.3502405,
title = {Reinforcement Learning and Affective Computing},
year = {2022},
isbn = {9781450395908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502398.3502405},
abstract = {Affective computing is a nascent field situated at the intersection of artificial intelligence with social and behavioral science. It studies how human emotions are perceived and expressed, which then informs the design of intelligent agents and systems that can either mimic this behavior to improve their intelligence or incorporate such knowledge to effectively understand and communicate with their human collaborators. Affective computing research has recently seen significant advances and is making a critical transformation from exploratory studies to real-world applications in the emerging research area known as applied affective computing.This book offers readers an overview of the state-of-the-art and emerging themes in affective computing, including a comprehensive review of the existing approaches to affective computing systems and social signal processing. It provides in-depth case studies of applied affective computing in various domains, such as social robotics and mental well-being. It also addresses ethical concerns related to affective computing and how to prevent misuse of the technology in research and applications. Further, this book identifies future directions for the field and summarizes a set of guidelines for developing next-generation affective computing systems that are effective, safe, and human-centered.For researchers and practitioners new to affective computing, this book will serve as an introduction to the field to help them in identifying new research topics or developing novel applications. For more experienced researchers and practitioners, the discussions in this book provide guidance for adopting a human-centered design and development approach to advance affective computing},
booktitle = {Applied Affective Computing}
}

@inproceedings{10.5555/3237383.3237450,
author = {Spooner, Thomas and Fearnley, John and Savani, Rahul and Koukorinis, Andreas},
title = {Market Making via Reinforcement Learning},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Market making is a fundamental trading problem in which an agent provides liquidity by continually offering to buy and sell a security. The problem is challenging due to inventory risk, the risk of accumulating an unfavourable position and ultimately losing money. In this paper, we develop a high-fidelity simulation of limit order book markets, and use it to design a market making agent using temporal-difference reinforcement learning. We use a linear combination of tile codings as a value function approximator, and design a custom reward function that controls inventory risk. We demonstrate the effectiveness of our approach by showing that our agent outperforms both simple benchmark strategies and a recent online learning approach from the literature.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {434–442},
numpages = {9},
keywords = {market making, td learning, tile coding, limit order books},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/1143844.1143955,
author = {Strehl, Alexander L. and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L.},
title = {PAC Model-Free Reinforcement Learning},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143955},
doi = {10.1145/1143844.1143955},
abstract = {For a Markov Decision Process with finite state (size S) and action spaces (size A per state), we propose a new algorithm---Delayed Q-Learning. We prove it is PAC, achieving near optimal performance except for \~{O}(SA) timesteps using O(SA) space, improving on the \~{O}(S2 A) bounds of best previous algorithms. This result proves efficient reinforcement learning is possible without learning a model of the MDP from experience. Learning takes place from a single continuous thread of experience---no resets nor parallel sampling is used. Beyond its smaller storage and experience requirements, Delayed Q-learning's per-experience computation cost is much less than that of previous PAC algorithms.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {881–888},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}

@inproceedings{10.5555/3306127.3331712,
author = {Yu, Chao and Wang, Xin and Hao, Jianye and Feng, Zhanbo},
title = {Reinforcement Learning for Cooperative Overtaking},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper solves the cooperative overtaking problem in autonomous driving using reinforcement learning techniques. Learning in such a situation is challenging due to vehicular mobility, which renders a continuously changing environment for each learning vehicle. Without no explicit coordination mechanisms, inefficient behaviors among vehicles might cause fatal uncoordinated outcomes. To solve this issue, we propose two basic coordination models to enable distributed learning of cooperative overtaking maneuvers in a group of vehicles. Extension mechanisms are then presented to make these models workable in more complex and realistic settings with any number of vehicles. Experiments verify that, by capturing the underlying consistency of identities or positions during vehicles' movement, efficient coordinated behaviors can be achieved simply through vehicles' local learning interactions.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {341–349},
numpages = {9},
keywords = {autonomous driving, cooperative overtaking, coordination graph, multiagent learning, reinforcement learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.5555/3545946.3598674,
author = {Zang, Yifan and He, Jinmin and Li, Kai and Fu, Haobo and Fu, Qiang and Xing, Junliang},
title = {Sequential Cooperative Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Cooperative multi-agent reinforcement learning (MARL) aims to coordinate the actions of multiple agents via a shared team reward. The complex interactions among agents make this problem extremely difficult. The mainstream of MARL methods often implicitly learn an inexplicable value decomposition from the shared reward into individual utilities, failing to give insights into how well each agent acts and lacking direct policy optimization guidance. This paper presents a sequential MARL framework that factorizes and simplifies the complex interaction analysis into a sequential evaluation process for more effective and efficient learning. We explicitly formulate this factorization via a novel sequential advantage function to evaluate each agent's actions, which achieves an explicable credit assignment and substantially facilitates policy optimization. We realize the sequential credit assignment (SeCA) by dynamically adjusting the sequence in light of agents' contributions to the team. Extensive experimental validations on a challenging set of StarCraft II micromanagement tasks verify SeCA's effectiveness.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {485–493},
numpages = {9},
keywords = {sequential evaluation, sequential credit assignment, cooperative multi-agent reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3545946.3599090,
author = {Shperberg, Shahaf S. and Liu, Bo and Stone, Peter},
title = {Relaxed Exploration Constrained Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This extended abstract introduces a novel setting of reinforcement learning with constraints, called Relaxed Exploration Constrained Reinforcement Learning (RECRL). As in standard constrained reinforcement learning (CRL), the aim is to find a policy that maximizes environmental return subject to a set of constraints. However, in RECRL there is an initial training phase in which the constraints are relaxed, thus the agent can explore the environment more freely. When training is done, the agent is deployed in the environment and is required to fully satisfy all constraints. As an initial approach to RECRL problems, we introduce a curriculum-based approach, named CLiC, that can be applied to existing CRL algorithms to improve their exploration during the training phase while allowing them to gradually converge to a policy that satisfies the full set of constraints. Empirical evaluation shows that CLiC produces policies with a higher return during deployment than policies learned when training is done using only the strict set of constraints.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2821–2823},
numpages = {3},
keywords = {constrained reinforcement learning, curriculum learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3545946.3598801,
author = {Bettini, Matteo and Shankar, Ajay and Prorok, Amanda},
title = {Heterogeneous Multi-Robot Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Cooperative multi-robot tasks can benefit from heterogeneity in the robots' physical and behavioral traits. In spite of this, traditional Multi-Agent Reinforcement Learning (MARL) frameworks lack the ability to explicitly accommodate policy heterogeneity, and typically constrain agents to share neural network parameters. This enforced homogeneity limits application in cases where the tasks benefit from heterogeneous behaviors. In this paper, we crystallize the role of heterogeneity in MARL policies. Towards this end, we introduce Heterogeneous Graph Neural Network Proximal Policy Optimization (HetGPPO), a paradigm for training heterogeneous MARL policies that leverages a Graph Neural Network for differentiable inter-agent communication. HetGPPO allows communicating agents to learn heterogeneous behaviors while enabling fully decentralized training in partially observable environments. We complement this with a taxonomical overview that exposes more heterogeneity classes than previously identified. To motivate the need for our model, we present a characterization of techniques that homogeneous models can leverage to emulate heterogeneous behavior, and show how this "apparent heterogeneity" is brittle in real-world conditions. Through simulations and real-world experiments, we show that: (i) when homogeneous methods fail due to strong heterogeneous requirements, HetGPPO succeeds, and, (ii) when homogeneous methods are able to learn apparently heterogeneous behaviors, HetGPPO achieves higher resilience to both training and deployment noise.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1485–1494},
numpages = {10},
keywords = {multi-agent reinforcement learning, multi-robot systems, heterogeneity},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.5555/3013545.3013551,
author = {Moriarty, David E. and Schultz, Alan C. and Grefenstette, John J.},
title = {Evolutionary Algorithms for Reinforcement Learning},
year = {1999},
issue_date = {July 1999},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {11},
number = {1},
issn = {1076-9757},
abstract = {There are two distinct approaches to solving reinforcement learning problems, namely, searching in value function space and searching in policy space. Temporal difference methods and evolutionary algorithms are well-known examples of these approaches. Kaelbling, Littman and Moore recently provided an informative survey of temporal difference methods. This article focuses on the application of evolutionary algorithms to the reinforcement learning problem, emphasizing alternative policy representations, credit assignment methods, and problem-specific genetic operators. Strengths and weaknesses of the evolutionary approach to reinforcement learning are presented, along with a survey of representative applications.},
journal = {J. Artif. Int. Res.},
month = {jul},
pages = {241–276},
numpages = {36}
}

@inproceedings{10.1145/1329125.1329248,
author = {Taylor, Matthew E. and Stone, Peter},
title = {Towards Reinforcement Learning Representation Transfer},
year = {2007},
isbn = {9788190426275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1329125.1329248},
doi = {10.1145/1329125.1329248},
abstract = {Transfer learning problems are typically framed as leveraging knowledge learned on a source task to improve learning on a related, but different, target task. Current transfer methods are able to successfully transfer knowledge between agents in different reinforcement learning tasks, reducing the time needed to learn the target. However, the complimentary task of representation transfer, i.e. transferring knowledge between agents with different internal representations, has not been well explored. The goal in both types of transfer problems is the same: reduce the time needed to learn the target with transfer, relative to learning the target without transfer. This work introduces one such representation transfer algorithm which is implemented in a complex multiagent domain. Experiments demonstrate that transferring the learned knowledge between different representations is both possible and beneficial.},
booktitle = {Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems},
articleno = {100},
numpages = {3},
location = {Honolulu, Hawaii},
series = {AAMAS '07}
}

@inproceedings{10.1145/1570256.1570375,
author = {Konen, Wolfgang and Bartz-Beielstein, Thomas},
title = {Reinforcement Learning for Games: Failures and Successes},
year = {2009},
isbn = {9781605585055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1570256.1570375},
doi = {10.1145/1570256.1570375},
abstract = {We apply CMA-ES, an evolution strategy with covariance matrix adaptation, and TDL (Temporal Difference Learning) to reinforcement learning tasks. In both cases these algorithms seek to optimize a neural network which provides the policy for playing a simple game (TicTacToe). Our contribution is to study the effect of varying learning conditions on learning speed and quality. Certain initial failures with wrong fitness functions lead to the development of new fitness functions, which allow fast learning. These new fitness functions in combination with CMA-ES reduce the number of required games needed for training to the same order of magnitude as TDL.The selection of suitable features is also of critical importance for the learning success. It could be shown that using the raw board position as an input feature is not very effective -- and it is orders of magnitudes slower than different feature sets which exploit the symmetry of the game. We develop a measure "feature set utility", FU, which allows to characterize a given feature set in advance. We show that the lower bound provided by FU is largely in accordance with the results from our repeated experiments for very different learning algorithms, CMA-ES and TDL.},
booktitle = {Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers},
pages = {2641–2648},
numpages = {8},
keywords = {games, learning, evolution strategies, failures},
location = {Montreal, Qu\'{e}bec, Canada},
series = {GECCO '09}
}

@inproceedings{10.1145/3520304.3533983,
author = {Tessari, Michele and Iacca, Giovanni},
title = {Reinforcement Learning Based Adaptive Metaheuristics},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3533983},
doi = {10.1145/3520304.3533983},
abstract = {Parameter adaptation, that is the capability to automatically adjust an algorithm's hyperparameters depending on the problem being faced, is one of the main trends in evolutionary computation applied to numerical optimization. While several handcrafted adaptation policies have been proposed over the years to address this problem, only few attempts have been done so far at apply machine learning to learn such policies. Here, we introduce a general-purpose framework for performing parameter adaptation in continuous-domain metaheuristics based on state-of-the-art reinforcement learning algorithms. We demonstrate the applicability of this framework on two algorithms, namely Covariance Matrix Adaptation Evolution Strategies (CMA-ES) and Differential Evolution (DE), for which we learn, respectively, adaptation policies for the step-size (for CMA-ES), and the scale factor and crossover rate (for DE). We train these policies on a set of 46 benchmark functions at different dimensionalities, with various inputs to the policies, in two settings: one policy per function, and one global policy for all functions. Compared, respectively, to the Cumulative Step-size Adaptation (CSA) policy and to two well-known adaptive DE variants (iDE and jDE), our policies are able to produce competitive results in the majority of cases, especially in the case of DE.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1854–1861},
numpages = {8},
keywords = {reinforcement learning, adaptation, algorithm configuration, evolutionary algorithms},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.5555/3535850.3535980,
author = {Senadeera, Manisha and Karimpanal, Thommen George and Gupta, Sunil and Rana, Santu},
title = {Sympathy-Based Reinforcement Learning Agents},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {As artificial agents become increasingly prevalent in our daily lives, it becomes imperative to equip them with an awareness of societal norms; specifically, the ability to account for and be considerate towards others they may cohabit with. In this work, we explore the ability for an agent trained through reinforcement learning to exhibit sympathetic behaviours towards another (independent) agent in the environment. We propose to achieve such behaviours by first inferring the reward function of the independent agent, through inverse reinforcement learning, and subsequently learning a policy based on a sympathetic reward function - a convex combination of the inferred rewards and the agent's own rewards. The corresponding weighting is determined by a sympathy function which is computed based on the estimated return of the agent's current action relative to that of all possible actions it could have taken. We evaluate our approach on adversarial as well as assistive environment settings, and demonstrate the ability of our sympathetic agent to perform well at its own goal, while simultaneously giving due consideration to another agent in its environment. We also empirically examine and report the sensitivity of our agent's performance to the hyperparameters introduced in our proposed framework.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1164–1172},
numpages = {9},
keywords = {inverse reinforcement learning, sympathy, empathy, reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.5555/1402383.1402427,
author = {Taylor, Matthew E. and Kuhlmann, Gregory and Stone, Peter},
title = {Autonomous Transfer for Reinforcement Learning},
year = {2008},
isbn = {9780981738109},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent work in transfer learning has succeeded in making reinforcement learning algorithms more efficient by incorporating knowledge from previous tasks. However, such methods typically must be provided either a full model of the tasks or an explicit relation mapping one task into the other. An autonomous agent may not have access to such high-level information, but would be able to analyze its experience to find similarities between tasks. In this paper we introduce Modeling Approximate State Transitions by Exploiting Regression (MASTER), a method for automatically learning a mapping from one task to another through an agent's experience. We empirically demonstrate that such learned relationships can significantly improve the speed of a reinforcement learning algorithm in a series of Mountain Car tasks. Additionally, we demonstrate that our method may also assist with the difficult problem of task selection for transfer.},
booktitle = {Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {283–290},
numpages = {8},
keywords = {reinforcement learning, transfer learning},
location = {Estoril, Portugal},
series = {AAMAS '08}
}

@inproceedings{10.5555/2936924.2936991,
author = {El Asri, Layla and Piot, Bilal and Geist, Matthieu and Laroche, Romain and Pietquin, Olivier},
title = {Score-Based Inverse Reinforcement Learning},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper reports theoretical and empirical results obtained for the score-based Inverse Reinforcement Learning (IRL) algorithm. It relies on a non-standard setting for IRL consisting of learning a reward from a set of globally scored trajectories. This allows using any type of policy (optimal or not) to generate trajectories without prior knowledge during data collection. This way, any existing database (like logs of systems in use) can be scored a posteriori by an expert and used to learn a reward function. Thanks to this reward function, it is shown that a near-optimal policy can be computed. Being related to least-square regression, the algorithm (called SBIRL) comes with theoretical guarantees that are proven in this paper. SBIRL is compared to standard IRL algorithms on synthetic data showing that annotations do help under conditions on the quality of the trajectories. It is also shown to be suitable for real-world applications such as the optimisation of a spoken dialogue system.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {457–465},
numpages = {9},
keywords = {learning from demonstration, inverse reinforcement learning, spoken dialogue systems, reinforcement learning, markov decision processes},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.1145/3340531.3417448,
author = {Khurana, Udayan and Samulowitz, Horst},
title = {Autonomous Predictive Modeling via Reinforcement Learning},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3417448},
doi = {10.1145/3340531.3417448},
abstract = {Building a robust predictive model requires an array of steps such as data imputation, feature transformations, estimator selection, hyper-parameter search, ensemble construction, amongst others. Due to this vast, complex and heterogeneous space of operations, off-the-shelf optimization methods offer infeasible solutions for realistic response time requirements. In practice, much of the predictive modeling process is conducted by experienced data scientists, who selectively make use of available tools. Over time, they develop an understanding of the behavior of operators, and perform serial decision making under uncertainty, colloquially referred to as educated guesswork. With an unprecedented demand for application of supervised machine learning, there is a call for solutions that automatically search for a suitable combination of operators across these tasks while minimize the modeling error. We introduce a novel system called APRL (Autonomous Predictive modeler via Reinforcement Learning), that uses past experience through reinforcement learning to optimize sequential decision making from within a set of diverse actions under a budget constraint. Our experiments demonstrate the superiority of the proposed approach over known AutoML systems that utilize Bayesian optimization or genetic algorithms.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
pages = {3285–3288},
numpages = {4},
keywords = {automated machine learning, reinforcement learning, data science automation},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/375735.376334,
author = {Isbell, Charles and Shelton, Christian R. and Kearns, Michael and Singh, Satinder and Stone, Peter},
title = {A Social Reinforcement Learning Agent},
year = {2001},
isbn = {158113326X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375735.376334},
doi = {10.1145/375735.376334},
abstract = {We report on our reinforcement learning work on Cobot, a software agent that resides in the well-known online chat community LambdaMOO. Our initial work on Cobot~cite{cobotaaai} provided him with the ability to collect {em social statistics/} and report them to users in a reactive manner. Here we describe our application of reinforcement learning to allow Cobot to proactively take actions in this complex social environment, and adapt his behavior from multiple sources of human reward. After 5 months of training, Cobot received 3171 reward and punishment events from 254 different Lambda-MOO users, and learned nontrivial preferences for a number of users. Cobot modifies his behavior based on his current state in an attempt to maximize reward. Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment.},
booktitle = {Proceedings of the Fifth International Conference on Autonomous Agents},
pages = {377–384},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {AGENTS '01}
}

@inproceedings{10.1145/3325730.3325743,
author = {Shen, Xiangxiang and Yin, Chuanhuan and Hou, Xinwen},
title = {Self-Attention for Deep Reinforcement Learning},
year = {2019},
isbn = {9781450362580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325730.3325743},
doi = {10.1145/3325730.3325743},
abstract = {Reinforcement learning is concerned with how software agents ought to take actions according to the state of the environment so as to maximize some notion of cumulative reward. Therefore, in-depth study and mining of the state of the environment will be more conducive to the agent to make better decisions. Motivated by the advantages of self-attention mechanism in machine translation, this paper presents a new scheme. In this scheme, the state in deep reinforcement learning algorithms can be combined with self-attention mechanism. After that agents will pay more attention to the internal structure of state especially in a complex game environment, like real-time strategy game StarCraft. StarCraft is a huge challenge platform for AI researchers because of its huge state spaces and action spaces. Some baseline agents of reinforcement learning provided by DeepMind for mini-games in StarCraft II have not reached the level of an amateur player. Our agents use fewer features than DeepMind's baseline agents and have made significant improvement.},
booktitle = {Proceedings of the 2019 4th International Conference on Mathematics and Artificial Intelligence},
pages = {71–75},
numpages = {5},
keywords = {Deep reinforcement learning, A3C, StarCraft II mini-games, Self-Attention},
location = {Chegndu, China},
series = {ICMAI '19}
}

@inproceedings{10.1145/3532213.3532318,
author = {Tang, Siqi and Han, Conying and Guo, Tiande and Li, Mingqiang},
title = {Reinforcement Learning of Graph Matching},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532318},
doi = {10.1145/3532213.3532318},
abstract = {Graph matching not only has widespread application prospect and practical value, but also provides a broader perspective and new technologies for fundamental researches. The recently presented idea to learn heuristics or approximation algorithms often require significant specialized knowledge and trial-and-error. Therefore, we need better models and better ways to meet practical implementation. In this paper, a unique feature extraction combined with reinforcement learning procedure is proposed to tackle this challenge. There are three main contributions: 1. we introduce matrix symmetric compression to obtain global feature and Bi-directional Recurrent Neural Network (Bi-RNN) to extract local feature; 2. we transform graph matching to sequence-to-sequence problem based on the above feature; 3. we optimize parameters using Actor-Critic framework. Our experiments on synthetic and real databases reveal that reinforcement learning compares favorably to supervised case and traditional methods, both in terms of efficiency and quality.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {689–697},
numpages = {9},
keywords = {Quadratic Assignment Problem (QAP), Graph Matching, Reinforcement Learning, Matrix Symmetric Compression, Sequence-to-Sequence},
location = {Tianjin, China},
series = {ICCAI '22}
}

@article{10.5555/2627435.2670324,
author = {Tziortziotis, Nikolaos and Dimitrakakis, Christos and Blekas, Konstantinos},
title = {Cover Tree Bayesian Reinforcement Learning},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {This paper proposes an online tree-based Bayesian approach for reinforcement learning. For inference, we employ a generalised context tree model. This defines a distribution on multivariate Gaussian piecewise-linear models, which can be updated in closed form. The tree structure itself is constructed using the cover tree method, which remains efficient in high dimensional spaces. We combine the model with Thompson sampling and approximate dynamic programming to obtain effective exploration policies in unknown environments. The flexibility and computational simplicity of the model render it suitable for many reinforcement learning problems in continuous state spaces. We demonstrate this in an experimental comparison with a Gaussian process model, a linear model and simple least squares policy iteration.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2313–2335},
numpages = {23},
keywords = {Bayesian inference, non-parametric statistics, reinforcement learning}
}

