@inproceedings{10.1145/3511808.3557551,
author = {Xia, Wei and Liu, Weiwen and Liu, Yifan and Tang, Ruiming},
title = {Balancing Utility and Exposure Fairness for Integrated Ranking with Reinforcement Learning},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557551},
doi = {10.1145/3511808.3557551},
abstract = {Integrated ranking is critical in industrial recommendation systems and has attracted increasing attention. In an integrated ranking system, items from multiple channels are merged together and form an integrated list. During this process, apart from optimizing the system's utility like the total number of clicks, a fair allocation of the exposure opportunities over different channels also needs to be satisfied. To address this problem, we propose an integrated ranking model called <u>I</u>ntegrated <u>D</u>eep-<u>Q</u> <u>N</u>etwork (iDQN), which jointly considers user preferences, the platform's utility, and the exposure fairness. Extensive offline experiments validate the effectiveness of iDQN in managing the tradeoff between utility and fairness. Moreover, iDQN also has been deployed onto the online AppStore platform in Huawei, where the online A/B test shows iDQN outperforms the baseline by 1.87\% and 2.21\% in terms of utility and fairness, respectively.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {4590–4594},
numpages = {5},
keywords = {integrated ranking, reinforcement learning, recommender systems},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1109/WI-IAT.2012.154,
author = {Teng, Teck-Hou and Tan, Ah-Hwee},
title = {Knowledge-Based Exploration for Reinforcement Learning in Self-Organizing Neural Networks},
year = {2012},
isbn = {9780769548807},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2012.154},
doi = {10.1109/WI-IAT.2012.154},
abstract = {Exploration is necessary during reinforcement learning to discover new solutions in a given problem space. Most reinforcement learning systems, however, adopt a simple strategy, by randomly selecting an action among all the available actions. This paper proposes a novel exploration strategy, known as Knowledge-based Exploration, for guiding the exploration of a family of self-organizing neural networks in reinforcement learning. Specifically, exploration is directed towards unexplored and favorable action choices while steering away from those negative action choices that are likely to fail. This is achieved by using the learned knowledge of the agent to identify prior action choices leading to low $Q$-values in similar situations. Consequently, the agent is expected to learn the right solutions in a shorter time, improving overall learning efficiency. Using a Pursuit-Evasion problem domain, we evaluate the efficacy of the knowledge-based exploration strategy, in terms of task performance, rate of learning and model complexity. Comparison with random exploration and three other heuristic-based directed exploration strategies show that Knowledge-based Exploration is significantly more effective and robust for reinforcement learning in real time.},
booktitle = {Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {332–339},
numpages = {8},
keywords = {Rule-Based System, Self-Organizing Neural Network, Directed Exploration, Reinforcement Learning},
series = {WI-IAT '12}
}

@inproceedings{10.5555/3545946.3598957,
author = {Khanna, Pranav and Tennenholtz, Guy and Merlis, Nadav and Mannor, Shie and Tessler, Chen},
title = {Never Worse, Mostly Better: Stable Policy Improvement in Deep Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In recent years, there has been significant progress in applying deep reinforcement learning (RL) for solving challenging problems across a wide variety of domains. Nevertheless, convergence of various methods has been shown to suffer from inconsistencies, due to algorithmic instability and variance, as well as stochasticity in the benchmark environments. Particularly, despite the fact that the agent's performance may be improving on average, it may abruptly deteriorate at late stages of training. In this work, we study methods for enhancing the agent's learning process, by providing conservative updates with respect to either the obtained history or a reference benchmark policy. Our method, termed EVEREST, obtains high confidence improvements via confidence bounds of a reference policy. Through extensive empirical analysis we demonstrate the benefit of our approach in terms of both performance and stabilization, with significant improvements in continuous control and Atari benchmarks.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2430–2432},
numpages = {3},
keywords = {reinforcement learning, deep learning, stability},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3545946.3598807,
author = {Shi, Yufeng and Feng, Mingxiao and Wang, Minrui and Zhou, Wengang and Li, Houqiang},
title = {Multi-Agent Reinforcement Learning with Safety Layer for Active Voltage Control},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The main goal of active voltage control is to keep the voltage of each bus in the grid within a safe range. With the increasing penetration of renewable and distributed energy sources in the grid, growing complexity, increasing uncertainty, and aggravating volatility bring great challenges to voltage control in modern power systems. Traditional algorithms can hardly guarantee real-time safe control to cope with these challenges. In recent years, substantial attention has been paid to the application of multi-agent reinforcement learning algorithms (MARL) to coordinate the control units in each area of the grid in real time for active voltage control in complex scenarios. However, these MARL algorithms do not explicitly guarantee that the power system satisfies the security constraints. There is a little in-depth study on safe multi-agent policy learning in multi-agent-based voltage control, especially the direct correction of unsafe actions. In this paper, we formalize the active voltage control problem as a Constrained Markov Game and approach it with a centralized data-driven safety layer that requires global observations and maps unsafe actions to safe actions. In order to make the policy network rely on local observations for decentralized execution, we introduce two novel components into the policy network: action correction penalty loss and action correction sub-networks. Notably, our approaches are easily extendable to other MARL algorithms for continuous actions. In the experiments, we evaluate our methods in the power distribution network simulation environment and demonstrate the capability of the safety layer to correct unsafe actions and the effectiveness of the safety layer to improve the performance of the policy itself.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1533–1541},
numpages = {9},
keywords = {active voltage control, multi-agent deep reinforcement learning, safe exploration},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3545946.3598667,
author = {Wang, Xuefeng and Li, Xinran and Shao, Jiawei and Zhang, Jun},
title = {AC2C: Adaptively Controlled Two-Hop Communication for Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Learning communication strategies in cooperative multi-agent reinforcement learning (MARL) has recently attracted intensive attention. Early studies typically assumed a fully-connected communication topology among agents, which induces high communication costs and may not be feasible. Some recent works have developed adaptive communication strategies to reduce communication overhead, but these methods cannot effectively obtain valuable information from agents that are beyond the communication range. In this paper, we consider a realistic communication model where each agent has a limited communication range, and the communication topology dynamically changes. To facilitate effective agent communication, we propose a novel communication protocol calledAdaptively Controlled Two-Hop Communication (AC2C). After an initial local communication round, AC2C employs an adaptive two-hop communication strategy to enable long-range information exchange among agents to boost performance, which is implemented by a communication controller. This controller determines whether each agent should ask for two-hop messages and thus helps to reduce the communication overhead during distributed execution. We evaluate AC2C on three cooperative multi-agent tasks, and the experimental results show that it outperforms relevant baselines with lower communication costs.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {427–435},
numpages = {9},
keywords = {two-hop communication, adaptive controller, reinforcement learning, multi-agent system},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3557915.3561005,
author = {Arasteh, Fazel and SheikhGarGar, Soroush and Papagelis, Manos},
title = {Network-Aware Multi-Agent Reinforcement Learning for the Vehicle Navigation Problem},
year = {2022},
isbn = {9781450395298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3557915.3561005},
doi = {10.1145/3557915.3561005},
abstract = {Traffic congestion is characterized by longer trip times, and increased air pollution. In a static road network, the travel time to a destination is constant and can be computed using the shortest path first algorithm (SPF). However, road network conditions are dynamic, rendering the SPF to perform sub-optimally at times. In addition, in a realistic multiple-vehicle scenario, the SPF routing algorithm can cause congestion by routing all vehicles through the same shortest path. In this paper, we propose a network-aware multi-agent reinforcement learning model for addressing this problem. Our key idea is to assign an RL agent to intersections. Each RL agent operates as a router agent and is responsible for providing routing instructions to approaching vehicles. When a vehicle reaches an intersection, it submits a routing query to the RL agent consisting of its final destination. The RL agent generates a routing response based on (i) the destination, (ii) the current state of the road network, and (iii) routing policies learned by cooperating with other neighboring RL agents. Our experimental evaluation shows that the proposed MARL model outperforms the SPF algorithm by (up to) 20.2\% in average travel time.},
booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
articleno = {69},
numpages = {4},
keywords = {multi-agent reinforcement learning, graph attention network (GAT), intelligent transportation systems, adaptive vehicle navigation},
location = {Seattle, Washington},
series = {SIGSPATIAL '22}
}

@inproceedings{10.1145/3297280.3297371,
author = {Ossenkopf, Marie and Jorgensen, Mackenzie and Geihs, Kurt},
title = {Hierarchical Multi-Agent Deep Reinforcement Learning to Develop Long-Term Coordination},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297371},
doi = {10.1145/3297280.3297371},
abstract = {Multi-agent systems need to communicate to coordinate a shared task. We show that a recurrent neural network (RNN) can learn a communication protocol for coordination, even if the actions to coordinate lie outside of the communication range. We also show that a single RNN is unable to do this if there is an independent action sequence necessary before the coordinated action can be executed. We propose a hierarchical deep reinforcement learning model for multi-agent systems that separates the communication and coordination task from the action picking through a hierarchical policy. As a testbed, we propose the Dungeon Lever Game and we extend the Differentiable Inter-Agent Learning (DIAL) framework [3]. First we prove that the agents need a hierarchical policy to learn communication and actions and, second, we present results from our successful model of the Dungeon Lever Game.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {922–929},
numpages = {8},
keywords = {multi-agent systems, deep reinforcement learning, hierarchical learning, agent communication},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/2739480.2754783,
author = {Ja\'{s}kowski, Wojciech and Szubert, Marcin and Liskowski, Pawe\l{} and Krawiec, Krzysztof},
title = {High-Dimensional Function Approximation for Knowledge-Free Reinforcement Learning: A Case Study in SZ-Tetris},
year = {2015},
isbn = {9781450334723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739480.2754783},
doi = {10.1145/2739480.2754783},
abstract = {SZ-Tetris, a restricted version of Tetris, is a difficult reinforcement learning task. Previous research showed that, similarly to the original Tetris, value function-based methods such as temporal difference learning, do not work well for SZ-Tetris. The best performance in this game was achieved by employing direct policy search techniques, in particular the cross-entropy method in combination with handcrafted features. Nonetheless, a simple heuristic hand-coded player scores even higher. Here we show that it is possible to equal its performance with CMA-ES (Covariance Matrix Adaptation Evolution Strategy). We demonstrate that further improvement is possible by employing systematic n-tuple network, a knowledge-free function approximator, and VD-CMA-ES, a linear variant of CMA-ES for high dimension optimization. Last but not least, we show that a large systematic n-tuple network (involving more than 4 million parameters) allows the classical temporal difference learning algorithm to obtain similar average performance to VD-CMA-ES, but at 20 times lower computational expense, leading to the best policy for SZ-Tetris known to date. These results enrich the current understanding of difficulty of SZ-Tetris, and shed new light on the capabilities of particular search paradigms when applied to representations of various characteristics and dimensionality.},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {567–573},
numpages = {7},
keywords = {n-tuple system, video games, covariance matrix adaptation, vd-cma, cma-es, knowledge-free representations, function approximation, reinforcement learning},
location = {Madrid, Spain},
series = {GECCO '15}
}

@inproceedings{10.1145/3447928.3456707,
author = {Bernini, Nicola and Bessa, Mikhail and Delmas, R\'{e}mi and Gold, Arthur and Goubault, Eric and Pennec, Romain and Putot, Sylvie and Sillion, Fran\c{c}ois},
title = {A Few Lessons Learned in Reinforcement Learning for Quadcopter Attitude Control},
year = {2021},
isbn = {9781450383394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447928.3456707},
doi = {10.1145/3447928.3456707},
abstract = {In the context of developing safe air transportation, our work is focused on understanding how Reinforcement Learning methods can improve the state of the art in traditional control, in nominal as well as non-nominal cases. The end goal is to train provably safe controllers, by improving both training and verification methods. In this paper, we explore this path for controlling the attitude of a quadcopter: we discuss theoretical as well as practical aspects of training neural nets for controlling a crazyflie 2.0 drone. In particular we describe thoroughly the choices in training algorithms, neural net architecture, hyperparameters, observation space etc. We also discuss the robustness of the obtained controllers, both to partial loss of power for one rotor and to wind gusts. Finally, we measure the performance of the approach by using a robust form of a signal temporal logic to quantitatively evaluate the vehicle's behavior.},
booktitle = {Proceedings of the 24th International Conference on Hybrid Systems: Computation and Control},
articleno = {27},
numpages = {11},
location = {Nashville, Tennessee},
series = {HSCC '21}
}

@inproceedings{10.1145/3511808.3557105,
author = {Chen, Xiusi and Jiang, Jyun-Yu and Jin, Kun and Zhou, Yichao and Liu, Mingyan and Brantingham, P. Jeffrey and Wang, Wei},
title = {ReLiable:  Offline Reinforcement Learning for Tactical Strategies in Professional Basketball Games},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557105},
doi = {10.1145/3511808.3557105},
abstract = {Professional basketball provides an intriguing example of a dynamic spatio-temporal game that incorporates both hidden strategy policies and situational decision making. During a game, the coaches and players are assumed to follow a general game plan, but players are also forced to make spur-of-the-moment decisions based on immediate conditions on the court. However, because it is challenging to process heterogeneous signals on the court and the space of potential actions and outcomes is massive, it is hard for players to find an optimal strategy on the fly given a short amount of time to observe conditions and take action. In this work, we present ReLiable (ReinforcemEnt Learning In bAsketBaLl gamEs). Specifically, we investigate the possibility of using reinforcement learning (RL) to guide player decisions. We train an offline deep Q-network (DQN) on historical National Basketball Association (NBA) game data from 2015-2016. The data include play-by-play and player movement sensor data. We apply our trained agent to games that it has not seen. Our method is able to propose potentially smarter tactical strategies, compared with replay gameplay data, producing expected final game scores comparable to elite NBA teams. Our approach can be useful for learning strategy policies from other game-like domains characterized by competing groups and sequential spatio-temporal event data.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {3023–3032},
numpages = {10},
keywords = {sports, reinforcement learning, policy learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3450268.3453517,
author = {Kang, Zhuangwei and Barve, Yogesh D. and Bao, Shunxing and Dubey, Abhishek and Gokhale, Aniruddha},
title = {Configuration Tuning for Distributed IoT Message Systems Using Deep Reinforcement Learning: Poster Abstract},
year = {2021},
isbn = {9781450383547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450268.3453517},
doi = {10.1145/3450268.3453517},
abstract = {Distributed messaging systems (DMSs) are often equipped with a large number of configurable parameters that enable users to define application run-time behaviors and information dissemination rules. However, the resulting high-dimensional configuration space makes it difficult for users to determine the best configuration that can maximize application QoS under a variety of operational conditions. This poster introduces a novel, automatic knob tuning framework called DMSConfig. DMSConfig explores the configuration space by interacting with a data-driven environment prediction model(a DMS simulator), which eliminates the prohibitive cost of conducting online interactions with the production environment. DMSConfig employs the deep deterministic policy gradient (DDPG) method and a custom reward mechanism to learn and make configuration decisions based on predicted DMS states and performance. Our initial experimental results, conducted on a single-broker Kafka cluster, show that DMSConfig significantly outperforms the default configuration and has better adaptability to CPU and bandwidth-limited environments. We also confirm that DMSConfig produces fewer violations of latency constraints than three prevalent parameter tuning tools.},
booktitle = {Proceedings of the International Conference on Internet-of-Things Design and Implementation},
pages = {273–274},
numpages = {2},
keywords = {System Configuration, Publish/Subscribe Middleware, Policy-based RL Algorithm},
location = {Charlottesvle, VA, USA},
series = {IoTDI '21}
}

@inproceedings{10.1145/3508352.3549458,
author = {Zhuang, Yan and Zhang, Zhihao and Liu, Dajiang},
title = {Towards High-Quality CGRA Mapping with Graph Neural Networks and Reinforcement Learning},
year = {2022},
isbn = {9781450392174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508352.3549458},
doi = {10.1145/3508352.3549458},
abstract = {Coarse-Grained Reconfigurable Architectures (CGRA) is a promising solution to accelerate domain applications due to its good combination of energy-efficiency and flexibility. Loops, as computation-intensive parts of applications, are often mapped onto CGRA and modulo scheduling is commonly used to improve the execution performance. However, the actual performance using modulo scheduling is highly dependent on the mapping ability of the Data Dependency Graph (DDG) extracted from a loop. As existing approaches usually separate routing exploration of multi-cycle dependence from mapping for fast compilation, they may easily suffer from poor mapping quality. In this paper, we integrate the routing explorations into the mapping process and make it have more opportunities to find a globally optimized solution. Meanwhile, with a reduced resource graph defined, the searching space of the new mapping problem is not greatly increased. To efficiently solve the problem, we introduce graph neural network based reinforcement learning to predict a placement distribution over different resource nodes for all operations in a DDG. Using the routing connectivity as the reward signal, we optimize the parameters of neural network to find a valid mapping solution with a policy gradient method. Without much engineering and heuristic designing, our approach achieves 1.57\texttimes{} mapping quality, as compared to the state-of-the-art heuristic.},
booktitle = {Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
articleno = {61},
numpages = {9},
keywords = {reinforcement learning, graph neural network, modulo scheduling, CGRA},
location = {San Diego, California},
series = {ICCAD '22}
}

@article{10.1145/3197517.3201315,
author = {Liu, Libin and Hodgins, Jessica},
title = {Learning Basketball Dribbling Skills Using Trajectory Optimization and Deep Reinforcement Learning},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3197517.3201315},
doi = {10.1145/3197517.3201315},
abstract = {Basketball is one of the world's most popular sports because of the agility and speed demonstrated by the players. This agility and speed makes designing controllers to realize robust control of basketball skills a challenge for physics-based character animation. The highly dynamic behaviors and precise manipulation of the ball that occur in the game are difficult to reproduce for simulated players. In this paper, we present an approach for learning robust basketball dribbling controllers from motion capture data. Our system decouples a basketball controller into locomotion control and arm control components and learns each component separately. To achieve robust control of the ball, we develop an efficient pipeline based on trajectory optimization and deep reinforcement learning and learn non-linear arm control policies. We also present a technique for learning skills and the transition between skills simultaneously. Our system is capable of learning robust controllers for various basketball dribbling skills, such as dribbling between the legs and crossover moves. The resulting control graphs enable a simulated player to perform transitions between these skills and respond to user interaction.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {142},
numpages = {14},
keywords = {human simulation, physics-based characters, deep deterministic policy gradient, basketball, motion control}
}

@article{10.1145/3532625,
author = {Zhang, Xiaoyu and Gao, Wei and Li, Ge and Jiang, Qiuping and Cong, Runmin},
title = {Image Quality Assessment–driven Reinforcement Learning for Mixed Distorted Image Restoration},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3532625},
doi = {10.1145/3532625},
abstract = {Due to the diversity of the degradation process that is difficult to model, the recovery of mixed distorted images is still a challenging problem. The deep learning model trained under certain degradation declines significantly in other degradation situations. In this article, we explore ways to use a combination of tools to deal with the mixed distortion. First, we illustrate the limitations of a single deep network in dealing with multiple distortion types and then introduce a hierarchical toolkit with distinguished powerful tools. Second, we investigate how an efficient representation of images combined with a reinforcement learning (RL) paradigm helps to deal with tool noise in continuous restoration. The proposed method can accurately capture the distortion preferences for selecting the optimal recovery tools by RL agent. Finally, to fully utilize random tools for unknown distortion combinations, we adopt the exploration scheme with various quality evaluation methods to achieve more quality improvements. Experimental results demonstrate that the peak signal-to-noise ratio of the proposed method is 3.30 dB higher than other state-of-the-art RL-based methods on the CSIQ single distortion dataset and 0.95 dB higher on the DIV2K mixed distortion dataset.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {feb},
articleno = {42},
numpages = {23},
keywords = {Image restoration, reinforcement learning, deep learning}
}

@inproceedings{10.1145/1143844.1143901,
author = {Keller, Philipp W. and Mannor, Shie and Precup, Doina},
title = {Automatic Basis Function Construction for Approximate Dynamic Programming and Reinforcement Learning},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143901},
doi = {10.1145/1143844.1143901},
abstract = {We address the problem of automatically constructing basis functions for linear approximation of the value function of a Markov Decision Process (MDP). Our work builds on results by Bertsekas and Casta\~{n}on (1989) who proposed a method for automatically aggregating states to speed up value iteration. We propose to use neighborhood component analysis (Goldberger et al., 2005), a dimensionality reduction technique created for supervised learning, in order to map a high-dimensional state space to a low-dimensional space, based on the Bellman error, or on the temporal difference (TD) error. We then place basis function in the lower-dimensional space. These are added as new features for the linear function approximator. This approach is applied to a high-dimensional inventory control problem.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {449–456},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}

@inproceedings{10.1145/3404835.3462859,
author = {Kaiser, Magdalena and Saha Roy, Rishiraj and Weikum, Gerhard},
title = {Reinforcement Learning from Reformulations in Conversational Question Answering over Knowledge Graphs},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462859},
doi = {10.1145/3404835.3462859},
abstract = {The rise of personal assistants has made conversational question answering (ConvQA) a very popular mechanism for user-system interaction. State-of-the-art methods for ConvQA over knowledge graphs (KGs) can only learn from crisp question-answer pairs found in popular benchmarks. In reality, however, such training data is hard to come by: users would rarely mark answers explicitly as correct or wrong. In this work, we take a step towards a more natural learning paradigm - from noisy and implicit feedback via question reformulations. A reformulation is likely to be triggered by an incorrect system response, whereas a new follow-up question could be a positive signal on the previous turn's answer. We present a reinforcement learning model, termed CONQUER, that can learn from a conversational stream of questions and reformulations. CONQUER models the answering process as multiple agents walking in parallel on the KG, where the walks are determined by actions sampled using a policy network. This policy network takes the question along with the conversational context as inputs and is trained via noisy rewards obtained from the reformulation likelihood. To evaluate CONQUER, we create and release ConvRef, a benchmark with about 11k natural conversations containing around 205k reformulations. Experiments show that CONQUER successfully learns from noisy reward signals, significantly improving over a state-of-the-art baseline.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {459–469},
numpages = {11},
keywords = {feedback, knowledge graphs, question answering, conversations},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/1075405.1075413,
author = {Dowling, Jim and Cahill, Vinny},
title = {Self-Managed Decentralised Systems Using K-Components and Collaborative Reinforcement Learning},
year = {2004},
isbn = {1581139896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1075405.1075413},
doi = {10.1145/1075405.1075413},
abstract = {Components in a decentralised system are faced with uncertainty as how to best adapt to a changing environment to maintain or optimise system performance. How can individual components learn to adapt to recover from faults in an uncertain environment? How can a decentralised system coordinate the adaptive behaviour of its components to realise system optimisation goals given problems establishing consensus in dynamic environments? This paper introduces a self-adaptive component model, called K-Components, that enables individual components adapt to a changing environment and a decentralised coordination model, called collaborative reinforcement learning, that enables groups of components to learn to collectively adapt their behaviour to establish and maintain system-wide properties in a changing environment.},
booktitle = {Proceedings of the 1st ACM SIGSOFT Workshop on Self-Managed Systems},
pages = {39–43},
numpages = {5},
keywords = {collaborative reinforcement learning, architectural reflection, decentralised self-adaptive systems},
location = {Newport Beach, California},
series = {WOSS '04}
}

@article{10.1109/TASLP.2021.3138670,
author = {Li, Qian and Peng, Hao and Li, Jianxin and Wu, Jia and Ning, Yuanxing and Wang, Lihong and Yu, Philip S. and Wang, Zheng},
title = {Reinforcement Learning-Based Dialogue Guided Event Extraction to Exploit Argument Relations},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3138670},
doi = {10.1109/TASLP.2021.3138670},
abstract = {Event extraction is a ftask for natural language processing. Finding the roles of event arguments like event participants is essential for event extraction. However, doing so for real-life event descriptions is challenging because an argument’s role often varies in different contexts. While the relationship and interactions between multiple arguments are useful for settling the argument roles, such information is largely ignored by existing approaches. This paper presents a better approach for event extraction by explicitly utilizing the relationships of event arguments. We achieve this through a carefully designed task-oriented dialogue system. To model the argument relation, we employ reinforcement learning and incremental learning to extract multiple arguments via a multi-turned, iterative process. Our approach leverages knowledge of the already extracted arguments of the same sentence to determine the role of arguments that would be difficult to decide individually. It then uses the newly obtained information to improve the decisions of previously extracted arguments. This two-way feedback process allows us to exploit the argument relations to effectively settle argument roles, leading to better sentence understanding and event extraction. Experimental results show that our approach consistently outperforms seven state-of-the-art event extraction methods for the classification of events and argument role and argument identification.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {520–533},
numpages = {14}
}

@inproceedings{10.5555/3586210.3586293,
author = {Issabakhsh, Mona and Lee, Seokgi},
title = {A Hierarchical Deep Reinforcement Learning Approach for Outpatient Primary Care Scheduling},
year = {2023},
publisher = {IEEE Press},
abstract = {Primary care clinics suffer from high patient no-shows and late cancellation rates. Admitting walk-in patients to primary care setting helps improving clinic's utilization rates and accessibility, therefore, following an efficient walk-in patient admission policy is highly prominent. This research applies a learning-based outpatient management system investigating patient admission and assignment policies to improve the operational efficiency in a general outpatient clinic with high no-show and cancellation rates and daily walk-in requests. Contrary to the general outpatient literature, our results show that only 30\% of the walk-in requests should be admitted to minimize the wait time of already admitted patients and providers' over time. Our results also suggest assigning more than 50\% of the available slots of a clinic session to punctual patients who have an appointment, to minimize long-run costs. The model and the results, however, are generated based on specific data and parameters, and cannot be directly generalized to other clinics.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {997–1008},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3442381.3449934,
author = {Zhang, Weijia and Liu, Hao and Wang, Fan and Xu, Tong and Xin, Haoran and Dou, Dejing and Xiong, Hui},
title = {Intelligent Electric Vehicle Charging Recommendation Based on Multi-Agent Reinforcement Learning},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449934},
doi = {10.1145/3442381.3449934},
abstract = {Electric Vehicle&nbsp;(EV) has become a preferable choice in the modern transportation system due to its environmental and energy sustainability. However, in many large cities, EV drivers often fail to find the proper spots for charging, because of the limited charging infrastructures and the spatiotemporally unbalanced charging demands. Indeed, the recent emergence of deep reinforcement learning provides great potential to improve the charging experience from various aspects over a long-term horizon. In this paper, we propose a framework, named Multi-Agent Spatio-Temporal Reinforcement Learning&nbsp;(Master), for intelligently recommending public accessible charging stations by jointly considering various long-term spatiotemporal factors. Specifically, by regarding each charging station as an individual agent, we formulate this problem as a multi-objective multi-agent reinforcement learning task. We first develop a multi-agent actor-critic framework with the centralized attentive critic to coordinate the recommendation between geo-distributed agents. Moreover, to quantify the influence of future potential charging competition, we introduce a delayed access strategy to exploit the knowledge of future charging competition during training. After that, to effectively optimize multiple learning objectives, we extend the centralized attentive critic to multi-critics and develop a dynamic gradient re-weighting strategy to adaptively guide the optimization direction. Finally, extensive experiments on two real-world datasets demonstrate that Master achieves the best comprehensive performance compared with nine baseline approaches.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1856–1867},
numpages = {12},
keywords = {multi-agent reinforcement learning, Charging station recommendation, multi-objective optimization},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3494885.3494896,
author = {Zhang, Tao and Feng, Jun and Lu, Jiamin},
title = {Distant Supervision Relation Extraction via Reinforcement Learning with Potential Energy Function},
year = {2021},
isbn = {9781450390675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494885.3494896},
doi = {10.1145/3494885.3494896},
abstract = {Distant supervision has become an essential method for relation extraction. Although distant supervision is very effective, there is a large amount of noise in the dataset produced by distant supervision. To filter out the noise in the dataset, most recent models try to handle it by reinforcement learning, which require waiting for all sentences in the bags to be selected when calculating the reward, resulting in large time consumption in model training. To solve the problem, this paper proposes a method called distant supervision relation extraction via Reinforcement Learning with Potential Energy Function (PEF-RL). The model calculates the semantic similarity of sentences and relations through the relational alias table, and we pass the semantic similarity of sentences and relations as an additional reward to the potential energy function to guide the decision making process of the instance selector during reinforcement learning, which improves the learning efficiency of the reinforcement learning relation extraction model. Experiments show that our model has significant improvements in several dimensions compared to existing models.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Software Engineering},
pages = {61–66},
numpages = {6},
keywords = {Potential Energy Function, Reinforcement learning, Distant supervision, Relation Extraction},
location = {Singapore, Singapore},
series = {CSSE '21}
}

@inproceedings{10.1145/3336191.3371858,
author = {Liu, Feng and Guo, Huifeng and Li, Xutao and Tang, Ruiming and Ye, Yunming and He, Xiuqiang},
title = {End-to-End Deep Reinforcement Learning Based Recommendation with Supervised Embedding},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371858},
doi = {10.1145/3336191.3371858},
abstract = {The research of reinforcement learning (RL) based recommendation method has become a hot topic in recommendation community, due to the recent advance in interactive recommender systems. The existing RL recommendation approaches can be summarized into a unified framework with three components, namely embedding component (EC), state representation component (SRC) and policy component (PC). We find that EC cannot be nicely trained with the other two components simultaneously. Previous studies bypass the obstacle through a pre-training and fixing strategy, which makes their approaches unlike a real end-to-end fashion. More importantly, such pre-trained and fixed EC suffers from two inherent drawbacks: (1) Pre-trained and fixed embeddings are unable to model evolving preference of users and item correlations in the dynamic environment; (2) Pre-training is inconvenient in the industrial applications. To address the problem, in this paper, we propose an End-to-end Deep Reinforcement learning based Recommendation framework (EDRR). In this framework, a supervised learning signal is carefully designed for smoothing the update gradients to EC, and three incorporating ways are introduced and compared. To the best of our knowledge, we are the first to address the training compatibility between the three components in RL based recommendations. Extensive experiments are conducted on three real-world datasets, and the results demonstrate the proposed EDRR effectively achieves the end-to-end training purpose for both policy-based and value-based RL models, and delivers better performance than state-of-the-art methods.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {384–392},
numpages = {9},
keywords = {recommendation, supervised embedding, end-to-end, reinforcement learning},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.1145/3501712.3529736,
author = {Dietz, Griffin and King Chen, Jennifer and Beason, Jazbo and Tarrow, Matthew and Hilliard, Adriana and Shapiro, R. Benjamin},
title = {ARtonomous: Introducing Middle School Students to Reinforcement Learning Through Virtual Robotics},
year = {2022},
isbn = {9781450391979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501712.3529736},
doi = {10.1145/3501712.3529736},
abstract = {Typical educational robotics approaches rely on imperative programming for robot navigation. However, with the increasing presence of AI in everyday life, these approaches miss an opportunity to introduce machine learning (ML) techniques grounded in an authentic and engaging learning context. Furthermore, the needs for costly specialized equipment and ample physical space are barriers that limit access to robotics experiences for all learners. We propose ARtonomous, a relatively low-cost, virtual alternative to physical, programming-only robotics kits. With ARtonomous, students employ reinforcement learning (RL) alongside code to train and customize virtual autonomous robotic vehicles. Through a study evaluating ARtonomous, we found that middle-school students developed an understanding of RL, reported high levels of engagement, and demonstrated curiosity for learning more about ML. This research demonstrates the feasibility of an approach like ARtonomous for 1) eliminating barriers to robotics education and 2) promoting student learning and interest in RL and ML.},
booktitle = {Proceedings of the 21st Annual ACM Interaction Design and Children Conference},
pages = {430–441},
numpages = {12},
keywords = {middle school, robotics, AI, reinforcement learning, education},
location = {Braga, Portugal},
series = {IDC '22}
}

@inproceedings{10.1145/3292500.3330668,
author = {Zou, Lixin and Xia, Long and Ding, Zhuoye and Song, Jiaxing and Liu, Weidong and Yin, Dawei},
title = {Reinforcement Learning to Optimize Long-Term User Engagement in Recommender Systems},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330668},
doi = {10.1145/3292500.3330668},
abstract = {Recommender systems play a crucial role in our daily lives. Feed streaming mechanism has been widely used in the recommender system, especially on the mobile Apps. The feed streaming setting provides users the interactive manner of recommendation in never-ending feeds. In such a manner, a good recommender system should pay more attention to user stickiness, which is far beyond classical instant metrics and typically measured by long-term user engagement. Directly optimizing long-term user engagement is a non-trivial problem, as the learning target is usually not available for conventional supervised learning methods. Though reinforcement learning~(RL) naturally fits the problem of maximizing the long term rewards, applying RL to optimize long-term user engagement is still facing challenges: user behaviors are versatile to model, which typically consists of both instant feedback (eg. clicks) and delayed feedback (eg. dwell time, revisit); in addition, performing effective off-policy learning is still immature, especially when combining bootstrapping and function approximation. To address these issues, in this work, we introduce a RL framework --- FeedRec to optimize the long-term user engagement. FeedRec includes two components: 1)~a Q-Network which designed in hierarchical LSTM takes charge of modeling complex user behaviors, and 2)~a S-Network, which simulates the environment, assists the Q-Network and voids the instability of convergence in policy learning. Extensive experiments on synthetic data and a real-world large scale data show that FeedRec effectively optimizes the long-term user engagement and outperforms state-of-the-arts.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2810–2818},
numpages = {9},
keywords = {reinforcement learning, recommender system, long-term user engagement},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.5555/3545946.3598864,
author = {Qian, Junqi and Weng, Paul and Tan, Chenmien},
title = {Learning Rewards to Optimize Global Performance Metrics in Deep Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {When applying reinforcement learning (RL) to a new problem, reward engineering is a necessary, but often difficult and error-prone task a system designer has to face. To avoid this step, we propose LR4GPM, a novel (deep) RL method that can optimize a global performance metric, which is supposed to be available as part of the problem description. LR4GPM alternates between two phases: (1) learning a (possibly vector) reward function used to fit the performance metric, and (2) training a policy to optimize an approximation of this performance metric based on the learned rewards. Such RL training is not straightforward since both the reward function and the policy are trained using non-stationary data. To overcome this issue, we propose several training tricks. We demonstrate the efficiency of LR4GPM on several domains. Notably, LR4GPM outperforms the winner of a recent autonomous driving competition organized at DAI'2020.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1951–1960},
numpages = {10},
keywords = {deep reinforcement learning, reward learning, non-linear performance metric},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/2484920.2485188,
author = {Kraemer, Landon and Banerjee, Bikramjit},
title = {Concurrent Reinforcement Learning as a Rehearsal for Decentralized Planning under Uncertainty},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Dec-POMDPs are a powerful tool for modeling multi-agent planning and decision-making under uncertainty. Prevalent Dec-POMDP solution techniques require centralized computation given full knowledge of the underlying model. Recently, reinforcement learning (RL) based approaches have been proposed for distributed solution of Dec-POMDPs without full prior knowledge of the model. These methods assume that agents have only local information available to them during the learning process, i.e. that conditions during learning and policy execution are identical. However, in practical scenarios this may not be the case, and agents may have difficulty learning under such unnecessary constraints. We propose a novel RL approach in which agents are allowed to emph{rehearse} with information that will not be available during policy execution. The key is for the agents to learn policies that do not explicitly rely on this information. We show experimentally that incorporating such information can ameliorate the difficulties faced by non-rehearsal-based learners, and demonstrate fast, (near) optimal performance on many existing benchmark Dec-POMDP problems. We also propose a new benchmark domain that is less abstract than existing domains and is designed to be particularly challenging to RL-based solvers, as a target for current and future research on RL solutions to Dec-POMDPs.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1291–1292},
numpages = {2},
keywords = {multi-agent reinforcement learning, decentralized partially-observable markov decision processes},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1145/3580305.3599839,
author = {Sun, Qian and Zhang, Le and Yu, Huan and Zhang, Weijia and Mei, Yu and Xiong, Hui},
title = {Hierarchical Reinforcement Learning for Dynamic Autonomous Vehicle Navigation at Intelligent Intersections},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599839},
doi = {10.1145/3580305.3599839},
abstract = {Recent years have witnessed the rapid development of the Cooperative Vehicle Infrastructure System (CVIS), where road infrastructures such as traffic lights (TL) and autonomous vehicles (AVs) can share information among each other and work collaboratively to provide safer and more comfortable transportation experience to human beings. While many efforts have been made to develop efficient and sustainable CVIS solutions, existing approaches on urban intersections heavily rely on domain knowledge and physical assumptions, preventing them from being practically applied. To this end, this paper proposes NavTL, a learning-based framework to jointly control traffic signal plans and autonomous vehicle rerouting in mixed traffic scenarios where human-driven vehicles and AVs co-exist. The objective is to improve travel efficiency and reduce total travel time by minimizing congestion at the intersections while guiding AVs to avoid the temporally congested roads. Specifically, we design a graph-enhanced multi-agent decentralized bi-directional hierarchical reinforcement learning framework by regarding TLs as manager agents and AVs as worker agents. At lower temporal resolution timesteps, each manager sets a goal for the workers within its controlled region. Simultaneously, managers learn to take the signal actions based on the observation from the environment as well as an intention information extracted from its workers. At higher temporal resolution timesteps, each worker makes rerouting decisions along its way to the destination based on its observation from the environment, an intention-enhanced manager state representation, and a goal from its present manager. Finally, extensive experiments on one synthetic and two real-world network-level datasets demonstrate the effectiveness of our proposed framework in terms of improving travel efficiency.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4852–4861},
numpages = {10},
keywords = {reinforcement learning, traffic signal control, intelligent transportation system, multi-agent system, dynamic vehicle navigation, mixed autonomy traffic control},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.5555/3545946.3599162,
author = {Nafi, Nasik Muhammad},
title = {Learning Representations and Robust Exploration for Improved Generalization in Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Deep Reinforcement Learning agents typically aim to learn a task through interacting in a particular environment. However, training on such singleton RL tasks, where the agent interacts with the same environment in every episode, implicitly leads to overfitting. Thus, the agent fails to generalize to minor changes in the environment, especially in image-based observation. Generalization is one of the main contemporary research challenges and recently proposed environments that enable diversified episode generation opens up the possibility to investigate generalization. My initial work towards this objective includes representation learning through the partial decoupling of policy and value networks and hyperbolic discounting in a single-agent setting. Efficient exploration is another crucial aspect of achieving generalization when learning from limited data. My dissertation would focus on proposing and evaluating methods that enable better representation learning and exploration for unseen scenarios. Another key objective is to extend my work to multi-agent generalization which is comparatively less studied.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {3032–3034},
numpages = {3},
keywords = {representation learning, generalization, exploration, multi-agent systems, reinforcement learning, discounting},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3582099.3582133,
author = {Parnichkun, Manukid},
title = {Multiple Robots Path Planning Based on Reinforcement Learning for Object Transportation},
year = {2023},
isbn = {9781450398749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582099.3582133},
doi = {10.1145/3582099.3582133},
abstract = {This paper proposes reinforcement learning methods to perform an object transportation task for multiple robots. This task consists of two main subtasks, path planning and motion control task. Double deep Q-learning (DDQN) model is selected to achieve path planning for an unknown environment. To increase the capability of reinforcement learning model, semi-supervised method by A* algorithm is applied during the training process. In motion control task, reinforcement learning model is designed to control a movement of a differential wheeled mobile robot. The actions of mobile robot consisting of linear and angular velocities are computed by agent. The models for motion control task are separately trained for two different purposes. The first agent is trained to deal with the path following task and the other agent is trained to handle the point following task. The agent of the point following task is utilized to control the group of robots to move with a specific formation. Proximal policy optimization (PPO) is selected for the path following task and deep deterministic policy gradient (DDPG) is selected for the point following task. Eventually, the integration of the proposed reinforcement learning models can accomplish the object transportation task for multiple robots successfully both in simulation and experiment.},
booktitle = {Proceedings of the 2022 5th Artificial Intelligence and Cloud Computing Conference},
pages = {229–234},
numpages = {6},
keywords = {Deep reinforcement learning, Object transportation, Multiple robots cooperation},
location = {Osaka, Japan},
series = {AICCC '22}
}

@inproceedings{10.1145/3472163.3472176,
author = {Sharma, Vishal and Dyreson, Curtis and Flann, Nicholas},
title = {MANTIS: Multiple Type and Attribute Index Selection Using Deep Reinforcement Learning},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472176},
doi = {10.1145/3472163.3472176},
abstract = {DBMS performance is dependent on many parameters, such as index selection, cache size, physical layout, and data partitioning. Some combinations of these parameters can lead to optimal performance for a given workload but selecting an optimal or near-optimal combination is challenging, especially for large databases with complex workloads. Among the hundreds of parameters, index selection is arguably the most critical parameter for performance. We propose a self-administered framework, called the Multiple Type and Attribute Index Selector (MANTIS), that automatically selects near-optimal indexes. The framework advances the state-of-the-art index selection by considering both multi-attribute and multiple types of indexes within a bounded storage size constraint, a combination not previously addressed. MANTIS combines supervised and reinforcement learning, a Deep Neural Network recommends the type of index for a given workload while a Deep Q-Learning network recommends the multi-attribute aspect. MANTIS is sensitive to storage cost constraints and incorporates noisy rewards in its reward function for better performance. Our experimental evaluation shows that MANTIS outperforms the current state-of-art methods by an average of 9.53\% QphH@size.},
booktitle = {Proceedings of the 25th International Database Engineering \&amp; Applications Symposium},
pages = {56–64},
numpages = {9},
keywords = {Deep Reinforcement Learning, Priority Experience Replay, Database Index Selection, Markovian Decision Process},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.1145/3368926.3369713,
author = {Kuvshinova, Tatiana and Khritankov, Anton},
title = {Improving a Language Model Evaluator for Sentence Compression Without Reinforcement Learning},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369713},
doi = {10.1145/3368926.3369713},
abstract = {We consider sentence compression as a binary classification task on tokens. In this paper we improve on a language model evaluator model by incorporating a score from a neural language model directly into the loss function instead of resorting to reinforcement learning. As a result, the model learns to remove individual tokens and to preserve readability at the same time while maintaining the desired level of compression. We compare our model with a state-of-the-art model, which uses a policy-based reinforcement learning method for evaluating compressed sentences on readability. We perform automatic evaluation and evaluation with humans. Experiments demonstrate that we were able to improve on the strong baselines. We also provide human-evaluation of 200 gold compressions from Google dataset setting a baseline for human-evaluation in upcoming studies.},
booktitle = {Proceedings of the 10th International Symposium on Information and Communication Technology},
pages = {92–97},
numpages = {6},
keywords = {summarization, sentence compression, neural networks, language model},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT '19}
}

@inproceedings{10.1145/3543622.3573136,
author = {Xu, Zhenyu and Yu, Miaoxiang and Yang, Qing and Jeong, Yeonho and Wei, Tao},
title = {A Novel FPGA Simulator Accelerating Reinforcement Learning-Based Design of Power Converters},
year = {2023},
isbn = {9781450394178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543622.3573136},
doi = {10.1145/3543622.3573136},
abstract = {High-efficiency energy conversion systems have become increasingly important due to their wide use in all electronic systems such as data centers, smart mobile devices, E-vehicles, medical instruments, and so forth. Complex and interdependent parameters make optimal designs of power converters challenging to get. Recent research has shown that reinforcement learning (RL) shows great promise in the design of such converter circuits. A trained RL agent can search for optimal design parameters for power conversion circuit topologies under targeted application requirements. Training an RL agent requires numerous circuit simulations. As a result, they may take days to complete, primarily because of the slow time-domain circuit simulation.This abstract proposes a new FPGA architecture that accelerates the circuit simulation and hence substantially speeds up the RL-based design method for power converters. Our new architecture supports all power electronic circuit converters and their variations. It substantially improves the training speed of RL-based design methods. High-level synthesis (HLS) was used to build the accelerator on Amazon Web Service (AWS) F1 instance. An AWS virtual PC hosts the training algorithm. The host interacts with the FPGA accelerator by updating the circuit parameters, initiating simulation, and collecting the simulation results during training iterations. A script was created on the host side to facilitate this design method to convert a netlist containing circuit topology and parameters into core matrices in the FPGA accelerator. Experimental results showed 60x overall speedup of our RL-based design method in comparison with using a popular commercial simulator, PowerSim.},
booktitle = {Proceedings of the 2023 ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
pages = {51},
numpages = {1},
keywords = {simulation, reinforcement learning, fpga accelerator, power electronics},
location = {Monterey, CA, USA},
series = {FPGA '23}
}

@inproceedings{10.5555/3545946.3598862,
author = {Grooten, Bram and Sokar, Ghada and Dohare, Shibhansh and Mocanu, Elena and Taylor, Matthew E. and Pechenizkiy, Mykola and Mocanu, Decebal Constantin},
title = {Automatic Noise Filtering with Dynamic Sparse Training in Deep Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Tomorrow's robots will need to distinguish useful information from noise when performing different tasks. A household robot for instance may continuously receive a plethora of information about the home, but needs to focus on just a small subset to successfully execute its current chore. Filtering distracting inputs that contain irrelevant data has received little attention in the reinforcement learning literature. To start resolving this, we formulate a problem setting in reinforcement learning called theextremely noisy environment (ENE), where up to 99\% of the input features are pure noise. Agents need to detect which features provide task-relevant information about the state of the environment. Consequently, we propose a new method termed Automatic Noise Filtering (ANF), which uses the principles of dynamic sparse training in synergy with various deep reinforcement learning algorithms. The sparse input layer learns to focus its connectivity on task-relevant features, such that ANF-SAC and ANF-TD3 outperform standard SAC and TD3 by a large margin, while using up to 95\% fewer weights. Furthermore, we devise a transfer learning setting for ENEs, by permuting all features of the environment after 1M timesteps to simulate the fact that other information sources can become relevant as the world evolves. Again, ANF surpasses the baselines in final performance and sample complexity. Our code is available at https://github.com/bramgrooten/automatic-noise-filtering.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1932–1941},
numpages = {10},
keywords = {deep reinforcement learning, sparse training, noise filtering},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3219819.3220122,
author = {Chen, Shi-Yong and Yu, Yang and Da, Qing and Tan, Jun and Huang, Hai-Kuan and Tang, Hai-Hong},
title = {Stabilizing Reinforcement Learning in Dynamic Environment with Application to Online Recommendation},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220122},
doi = {10.1145/3219819.3220122},
abstract = {Deep reinforcement learning has shown great potential in improving system performance autonomously, by learning from iterations with the environment. However, traditional reinforcement learning approaches are designed to work in static environments. In many real-world problems, the environments are commonly dynamic, in which the performance of reinforcement learning approaches can degrade drastically. A direct cause of the performance degradation is the high-variance and biased estimation of the reward, due to the distribution shifting in dynamic environments. In this paper, we propose two techniques to alleviate the unstable reward estimation problem in dynamic environments, the stratified sampling replay strategy and the approximate regretted reward, which address the problem from the sample aspect and the reward aspect, respectively. Integrating the two techniques with Double DQN, we propose the Robust DQN method. We apply Robust DQN in the tip recommendation system in Taobao online retail trading platform. We firstly disclose the highly dynamic property of the recommendation application. We then carried out online A/B test to examine Robust DQN. The results show that Robust DQN can effectively stabilize the value estimation and, therefore, improves the performance in this real-world dynamic environment.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {1187–1196},
numpages = {10},
keywords = {reinforcement learning, dynamic environment, recommendation, approximate regretted reward, stratified sampling replay},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.5555/3522802.3522929,
author = {Kim, Taehyung and Kim, Hyeongook and Lee, Tae-eog and Morrison, James Robert and Kim, Eungjin},
title = {On Scheduling a Photolithograhy Toolset Based on a Deep Reinforcement Learning Approach with Action Filter},
year = {2022},
publisher = {IEEE Press},
abstract = {Production scheduling of semiconductor manufacturing tools is a challenging problem due to the complexity of the equipment and systems in modern wafer fabs. In our study, we focus on the photolithography toolset and consider it as a non-identical parallel machine scheduling problem with random lot arrivals and auxiliary resource constraints. The proposed methodology strives to learn a near optimal scheduling policy by incorporating WIP, masks, and the tardiness of jobs. An Action Filter (AF) is proposed as a methodology to eliminate illogical actions and speed the learning process of agents. The proposed model was evaluated in a simulation environment inspired by practical photolithography scheduling problems across various settings with reticle and qualification constraints. Our experiments demonstrated improved performance compared to typical rule-based strategies. Relative to our learning methods, weighted shortest processing time (WSPT) and apparent tardiness cost with setups (ATCS) rules perform 28\% and 32\% worse for weighted tardiness, respectively.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {156},
numpages = {10},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3404397.3404466,
author = {Wang, Haoyu and Shen, Haiying and Liu, Qi and Zheng, Kevin and Xu, Jie},
title = {A Reinforcement Learning Based System for Minimizing Cloud Storage Service Cost},
year = {2020},
isbn = {9781450388160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404397.3404466},
doi = {10.1145/3404397.3404466},
abstract = {Currently, many web applications are deployed on cloud storage service provided by cloud service providers (CSPs). A CSP offers different types of storage including hot, cold and archive storage and sets unit prices for these different types, which vary substantially. By properly assigning the data files of a web application to different types of storage based on their usage profiles and the CSP’s pricing policy, a cloud customer potentially can achieve substantial cost savings and minimize the payment to the CSP. However, no previous research handles this problem. Towards this goal, we present a Markov Decision Process formulation for the cost minimization problem, and then develop a reinforcement learning based approach to effectively solve the problem, which changes the type of storage of each data file periodically to minimize money cost in long term. We then propose a method to aggregate concurrently requested data files to further reduce the cloud storage service payment for a web application. Our experiments with Wikipedia traces show the effectiveness of the proposed methods for minimizing cloud customer cost in comparison with other methods.},
booktitle = {Proceedings of the 49th International Conference on Parallel Processing},
articleno = {30},
numpages = {10},
location = {Edmonton, AB, Canada},
series = {ICPP '20}
}

@inproceedings{10.1145/3563357.3564055,
author = {Mottahedi, Sam and Pavlak, Gregory S.},
title = {Constrained Differentiable Cross-Entropy Method for Safe Model-Based Reinforcement Learning},
year = {2022},
isbn = {9781450398909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563357.3564055},
doi = {10.1145/3563357.3564055},
abstract = {Reinforcement learning agents must explore their environments to learn optimal policies through trial and error. Due to challenges in simulating the complexities of the real world, there is a growing trend of training reinforcement learning (RL) agents directly in the real world instead of mostly or entirely in simulation. Safety concerns are paramount when training RL agents directly in the real world. This paper proposes MPC-CDCEM, a model-based reinforcement algorithm (RL) that allows the agent to safely interact with the environment and explore without additional assumptions on system dynamics. The algorithm uses a Model Predictive Control (MPC) framework with a differentiable cross-entropy optimizer, which induces a differentiable policy that considers the constraints while addressing the objective mismatch problem in model-based RL algorithms. We evaluate our algorithm in Safety Gym environments and on a practical building energy optimization problem. In addition, we showed that in both experiments, our algorithms have the lowest number of constraint violations and achieve comparable rewards compared to baseline constrained RL algorithms.},
booktitle = {Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {40–48},
numpages = {9},
keywords = {differentiable convex optimization, limited multi-label classification, constrained Markov decision process, cross-entropy methods, reinforcement learning},
location = {Boston, Massachusetts},
series = {BuildSys '22}
}

@inproceedings{10.1145/3383455.3422570,
author = {Karpe, Micha\"{e}l and Fang, Jin and Ma, Zhongyao and Wang, Chen},
title = {Multi-Agent Reinforcement Learning in a Realistic Limit Order Book Market Simulation},
year = {2021},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383455.3422570},
doi = {10.1145/3383455.3422570},
abstract = {Optimal order execution is widely studied by industry practitioners and academic researchers because it determines the profitability of investment decisions and high-level trading strategies, particularly those involving large volumes of orders. However, complex and unknown market dynamics pose significant challenges for the development and validation of optimal execution strategies. In this paper, we propose a model-free approach by training Reinforcement Learning (RL) agents in a realistic market simulation environment with multiple agents. First, we configure a multi-agent historical order book simulation environment for execution tasks built on an Agent-Based Interactive Discrete Event Simulation (ABIDES) [6]. Second, we formulate the problem of optimal execution in an RL setting where an intelligent agent can make order execution and placement decisions based on market microstructure trading signals in High Frequency Trading (HFT). Third, we develop and train an RL execution agent using the Double Deep Q-Learning (DDQL) algorithm in the ABIDES environment. In some scenarios, our RL agent converges towards a Time-Weighted Average Price (TWAP) strategy. Finally, we evaluate the simulation with our RL agent by comparing it with a market replay simulation using real market Limit Order Book (LOB) data.},
booktitle = {Proceedings of the First ACM International Conference on AI in Finance},
articleno = {30},
numpages = {7},
keywords = {high-frequency trading, market simulation, limit order book, optimal execution, multi-agent reinforcement learning},
location = {New York, New York},
series = {ICAIF '20}
}

@inproceedings{10.1145/3440840.3440856,
author = {Philezwini Sithungu, Siphesihle and Marie Ehlers, Elizabeth},
title = {A Reinforcement Learning-Based Classification Symbiont Agent for Dynamic Difficulty Balancing},
year = {2021},
isbn = {9781450388085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440840.3440856},
doi = {10.1145/3440840.3440856},
abstract = {AdaptiveSGA is a mechanism for achieving Adaptive Game AI-based Dynamic Difficulty Balancing in games. AdaptiveSGA is based on the Symbiotic Game Agent model and, therefore, leverages the advantages of biological symbiosis. Within the AdaptiveSGA architecture, the classification symbiont agent is responsible for the dynamic difficulty balancing component. Current work proposes the use of a classification symbiont agent that makes use of reinforcement learning to optimise dynamic difficulty balancing in order to match the opponent's skill. Current work also introduces three different types of decision-making algorithms that can be used by decision-making symbiont agents to display different kinds of behaviour. The ability to reproduce different kinds of NPC behaviour forms the adaptive game AI component of AdaptiveSGA. Experimental results showed that the reinforcement learning-based classification symbiont agent can achieve an even game with opponents and can further help minimise the number of draws.},
booktitle = {Proceedings of the 2020 3rd International Conference on Computational Intelligence and Intelligent Systems},
pages = {15–23},
numpages = {9},
keywords = {q-learning, dynamic difficulty balancing, classification symbiont agent, symbiotic game agent},
location = {Tokyo, Japan},
series = {CIIS '20}
}

@inproceedings{10.1145/3477495.3531969,
author = {Wu, Junda and Xie, Zhihui and Yu, Tong and Zhao, Handong and Zhang, Ruiyi and Li, Shuai},
title = {Dynamics-Aware Adaptation for Reinforcement Learning Based Cross-Domain Interactive Recommendation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531969},
doi = {10.1145/3477495.3531969},
abstract = {Interactive recommender systems (IRS) have received wide attention in recent years. To capture users' dynamic preferences and maximize their long-term engagement, IRS are usually formulated as reinforcement learning (RL) problems. Despite the promise to solve complex decision-making problems, RL-based methods generally require a large amount of online interaction, restricting their applications due to economic considerations. One possible direction to alleviate this issue is cross-domain recommendation that aims to leverage abundant logged interaction data from a source domain (e.g., adventure genre in movie recommendation) to improve the recommendation quality in the target domain (e.g., crime genre). Nevertheless, prior studies mostly focus on adapting the static representations of users/items. Few have explored how the temporally dynamic user-item interaction patterns transform across domains.Motivated by the above consideration, we propose DACIR, a novel Doubly-Adaptive deep RL-based framework for Cross-domain Interactive Recommendation. We first pinpoint how users behave differently in two domains and highlight the potential to leverage the shared user dynamics to boost IRS. To transfer static user preferences across domains, DACIR enforces consistency of item representation by aligning embeddings into a shared latent space. In addition, given the user dynamics in IRS, DACIR calibrates the dynamic interaction patterns in two domains via reward correlation. Once the double adaptation narrows the cross-domain gap, we are able to learn a transferable policy for the target recommender by leveraging logged data. Experiments on real-world datasets validate the superiority of our approach, which consistently achieves significant improvements over the baselines.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {290–300},
numpages = {11},
keywords = {cross-domain recommendation, interactive recommender systems, reinforcement learning},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3543507.3583856,
author = {Li, Zhenyang and Dong, Yancheng and Gao, Chen and Zhao, Yizhou and Li, Dong and Hao, Jianye and Zhang, Kai and Li, Yong and Wang, Zhi},
title = {Breaking Filter Bubble: A Reinforcement Learning Framework of Controllable Recommender System},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583856},
doi = {10.1145/3543507.3583856},
abstract = {In the information-overloaded era of the Web, recommender systems that provide personalized content filtering are now the mainstream portal for users to access Web information. Recommender systems deploy machine learning models to learn users’ preferences from collected historical data, leading to more centralized recommendation results due to the feedback loop. As a result, it will harm the ranking of content outside the narrowed scope and limit the options seen by users. In this work, we first conduct data analysis from a graph view to observe that the users’ feedback is restricted to limited items, verifying the phenomenon of centralized recommendation. We further develop a general simulation framework to derive the procedure of the recommender system, including data collection, model learning, and item exposure, which forms a loop. To address the filter bubble issue under the feedback loop, we then propose a general and easy-to-use reinforcement learning-based method, which can adaptively select few but effective connections between nodes from different communities as the exposure list. We conduct extensive experiments in the simulation framework based on large-scale real-world datasets. The results demonstrate that our proposed reinforcement learning-based control method can serve as an effective solution to alleviate the filter bubble and the separated communities induced by it. We believe the proposed framework of controllable recommendation in this work can inspire not only the researchers of recommender systems, but also a broader community concerned with artificial intelligence algorithms’ impact on humanity, especially for those vulnerable populations on the Web.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {4041–4049},
numpages = {9},
keywords = {Filter Bubble, Reinforcement Learning, Controllable Recommendation},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3469213.3470265,
author = {Chen, Yuting and Han, Xuehui},
title = {Four-Rotor Aerobatic Flight of Inverted Pendulum Based on Reinforcement Learning},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470265},
doi = {10.1145/3469213.3470265},
abstract = {Using the four-rotor inverted pendulum system as the research object, the control method of four-rotor is studied in order to realize the aerobatic flight in the air; According to the different states of the rod in the air, the motion of the four-rotor is analyzed; In view of the complex changes of the path, the deep reinforcement learning algorithm is used to train the quadrotor maneuver strategy, and the path tracking and receiving tasks are completed. The tracking error is improved, and the parameter selection is completed by the algorithm, which reduces the number and difficulty of experiments.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {64},
numpages = {5},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@article{10.14778/3611540.3611576,
author = {Yan, Zhengtong and Uotila, Valter and Lu, Jiaheng},
title = {Join Order Selection with Deep Reinforcement Learning: Fundamentals, Techniques, and Challenges},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611576},
doi = {10.14778/3611540.3611576},
abstract = {Join Order Selection (JOS) is a fundamental challenge in query optimization, as it significantly affects query performance. However, finding an optimal join order is an NP-hard problem due to the exponentially large search space. Despite the decades-long effort, traditional methods still suffer from limitations. Deep Reinforcement Learning (DRL) approaches have recently gained growing interest and shown superior performance over traditional methods. These DRL-based methods could leverage prior experience through the trial-and-error strategy to automatically explore the optimal join order. This tutorial will focus on recent DRL-based approaches for join order selection by providing a comprehensive overview of the various approaches. We will start by briefly introducing the core concepts of join ordering and the traditional methods for JOS. Next, we will provide some preliminary knowledge about DRL and then delve into DRL-based join order selection approaches by offering detailed information on those methods, analyzing their relationships, and summarizing their weaknesses and strengths. To help the audience gain a deeper understanding of DRL approaches for JOS, we will present two open-source demonstrations and compare their differences. Finally, we will identify research challenges and open problems to provide insights into future research directions. This tutorial will provide valuable guidance for developing more practical DRL approaches for JOS.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3882–3885},
numpages = {4}
}

@inproceedings{10.1145/3412841.3441953,
author = {Jiang, Shuo and Amato, Christopher},
title = {Multi-Agent Reinforcement Learning with Directed Exploration and Selective Memory Reuse},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441953},
doi = {10.1145/3412841.3441953},
abstract = {Many tasks require cooperation and coordination of multiple agents. Multi-agent reinforcement learning (MARL) can effectively learn solutions to these problems, but exploration and local optima problems are still open research topics. In this paper, we propose a new multi-agent policy gradient method called decentralized exploration and selective memory policy gradient (DecESPG) that addresses these issues. DecESPG consists of two additional components built on policy gradient: 1) an exploration bonus component that directs agents to explore novel observations and actions and 2) a selective memory component that records past trajectories to reuse valuable experience and reinforce cooperative behavior. Experimental results verify that the proposed method learns faster and outperforms state-of-the-art MARL algorithms.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {777–784},
numpages = {8},
keywords = {multi-agent reinforcement learning, gaussian process regression},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1109/WI-IAT.2012.21,
author = {Bromuri, Stefano},
title = {A Tensor Factorization Approach to Generalization in Multi-Agent Reinforcement Learning},
year = {2012},
isbn = {9780769548807},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2012.21},
doi = {10.1109/WI-IAT.2012.21},
abstract = {Reinforcement learning (RL) and multi-agent reinforcement learning (MARL) are disciplines concerned with defining automatically the behaviour of an agent, or a set of interacting agents, by means of reward mechanisms coming from the environment. An important research issue in the context of RL and MARL is the definition of approaches to combine the knowledge of multiple learning agents to improve the overall performance of the multi-agent system (MAS). This paper illustrates how to improve RL and MARL algorithms by utilizing results from multi-linear algebra such as tensors and tensor factorizations. In particular, the focus is on showing how to modify existing algorithms from literature to include a tensor factorization step applied to the Q-Tables learned by the individual agents to generalize the knowledge about the actions performed in the environment. The modified algorithms are then evaluated in three RL and MARL scenarios against their unmodified version to show the benefits of the tensor factorization step.},
booktitle = {Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {274–281},
numpages = {8},
keywords = {Algorithms, Dynamic Programming, Learning Systems, Software Agents},
series = {WI-IAT '12}
}

@inproceedings{10.1145/3449726.3463198,
author = {Bishop, Jordan T. and Gallagher, Marcus and Browne, Will N.},
title = {A Genetic Fuzzy System for Interpretable and Parsimonious Reinforcement Learning Policies},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3463198},
doi = {10.1145/3449726.3463198},
abstract = {Reinforcement learning (RL) is experiencing a resurgence in research interest, where Learning Classifier Systems (LCSs) have been applied for many years. However, traditional Michigan approaches tend to evolve large rule bases that are difficult to interpret or scale to domains beyond standard mazes. A Pittsburgh Genetic Fuzzy System (dubbed Fuzzy MoCoCo) is proposed that utilises both multiobjective and cooperative coevolutionary mechanisms to evolve fuzzy rule-based policies for RL environments. Multiobjectivity in the system is concerned with policy performance vs. complexity. The continuous state RL environment Mountain Car is used as a testing bed for the proposed system. Results show the system is able to effectively explore the trade-off between policy performance and complexity, and learn interpretable, high-performing policies that use as few rules as possible.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1630–1638},
numpages = {9},
keywords = {genetic fuzzy systems, multiobjective optimisation, cooperative coevolution, reinforcement learning, direct policy search},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.5555/3539845.3539865,
author = {Saeidi, Seyyed Amirhossein and Fallah, Forouzan and Barmaki, Soroush and Farbeh, Hamed},
title = {A Novel Neuromorphic Processors Realization of Spiking Deep Reinforcement Learning for Portfolio Management},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {The process of constantly reallocating budgets into financial assets, aiming to increase the anticipated return of assets and minimizing the risk, is known as portfolio management. Processing speed and energy consumption of portfolio management have become crucial as the complexity of their real-world applications increasingly involves high-dimensional observation and action spaces and environment uncertainty, which their limited onboard resources cannot offset. Emerging neuromorphic chips inspired by the human brain increase processing speed by up to 500 times and reduce power consumption by several orders of magnitude. This paper proposes a spiking deep reinforcement learning (SDRL) algorithm that can predict financial markets based on unpredictable environments and achieve the defined portfolio management goal of profitability and risk reduction. This algorithm is optimized for Intel's Loihi neuromorphic processor and provides 186x and 516x energy consumption reduction compared to a high-end processor and GPU, respectively. In addition, a 1.3x and 2.0x speed-up is observed over the high-end processors and GPUs, respectively. The evaluations are performed on cryptocurrency market benchmark between 2016 and 2021.},
booktitle = {Proceedings of the 2022 Conference \&amp; Exhibition on Design, Automation \&amp; Test in Europe},
pages = {68–71},
numpages = {4},
keywords = {deep reinforcement learning, portfolio management, neuromorphic computing, loihi},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3286978.3286991,
author = {Ning, Xiaodong and Yao, Lina and Wang, Xianzhi and Benatallah, Boualem and Salim, Flora and Haghighi, Pari Delir},
title = {Predicting Citywide Passenger Demand via Reinforcement Learning from Spatio-Temporal Dynamics},
year = {2018},
isbn = {9781450360937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286978.3286991},
doi = {10.1145/3286978.3286991},
abstract = {The global urbanization imposes unprecedented pressure on urban infrastructure and public resources. The population explosion has made it challenging to satisfy the daily needs of urban residents. 'Smart City' is a solution that utilizes different types of data collection sensors to help manage assets and resources intelligently and more efficiently. Under the Smart City umbrella, the primary research initiative in improving the efficiency of car-hailing services is to predict the citywide passenger demand to address the imbalance between the demand and supply. However, predicting the passenger demand requires analysis on various data such as historical passenger demand, crowd outflow, and weather information, and it remains challenging to discover the latent relationships among these data. To address this challenge, we propose to improve the passenger demand prediction via learning the salient spatial-temporal dynamics within a reinforcement learning framework. Our model employs an information selection mechanism to focus on the most distinctive data in historical observations. This mechanism can automatically adjust the information zone according to the prediction performance to find the optimal choice. It also ensures the prediction model to take full advantage of the available data by introducing the positive and excluding the negative correlations. We have conducted experiments on a large-scale real-world dataset that covers 1.5 million people in a major city in China. The results show our model outperforms state-of-the-art and a series of baselines by a large margin.},
booktitle = {Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {19–28},
numpages = {10},
keywords = {Reinforcement Learning, passenger demand prediction, spatial-temporal dynamics},
location = {New York, NY, USA},
series = {MobiQuitous '18}
}

@inproceedings{10.1145/3501710.3524734,
author = {Balakrishnan, Anand and Jaksic, Stefan and Aguilar, Edgar and Nickovic, Dejan and Deshmukh, Jyotirmoy},
title = {Poster Abstract: Model-Free Reinforcement Learning for Symbolic Automata-Encoded Objectives},
year = {2022},
isbn = {9781450391962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501710.3524734},
doi = {10.1145/3501710.3524734},
abstract = {In this work, we propose the use of symbolic automata as formal specifications for reinforcement learning agents. The use of symbolic automata serves as a generalization of both bounded-time temporal logic-based specifications and deterministic finite automata, allowing us to describe input alphabets over metric spaces. Furthermore, our use of symbolic automata allows us to define non-sparse potential-based rewards which empirically shape the reward surface, leading to better convergence during RL. We also show that our potential-based rewarding strategy still allows us to obtain the policy that maximizes the satisfaction of the given specification.},
booktitle = {Proceedings of the 25th ACM International Conference on Hybrid Systems: Computation and Control},
articleno = {26},
numpages = {2},
location = {Milan, Italy},
series = {HSCC '22}
}

@inproceedings{10.1145/3313149.3313369,
author = {Hahn, Ernst Moritz and Perez, Mateo and Schewe, Sven and Somenzi, Fabio and Trivedi, Ashutosh and Wojtczak, Dominik},
title = {Limit Reachability for Model-Free Reinforcement Learning of ω-Regular Objectives},
year = {2019},
isbn = {9781450366977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313149.3313369},
doi = {10.1145/3313149.3313369},
abstract = {We have recently solved the model-free reinforcement learning of ω-regular objectives for Markov decision processes. We outline our constructive reduction from the almost-sure satisfaction of ω-regular objectives to an almost-sure reachability problem, and extend this technique to learning how to control an unknown model so that the chance of satisfying the objective is maximized. A key feature of our technique is the compilation of ω-regular properties into limit-deterministic B\"{u}chi automata, which sidesteps difficulties associated with the traditional Rabin automata. Our approach allows us to apply model-free, off-the-shelf reinforcement learning algorithms to compute optimal strategies from the observations of the Markov decision process.},
booktitle = {Proceedings of the Fifth International Workshop on Symbolic-Numeric Methods for Reasoning about CPS and IoT},
pages = {16–18},
numpages = {3},
keywords = {omega-regular objectives, reinforcement learning},
location = {Montreal, Quebec, Canada},
series = {SNR '19}
}

