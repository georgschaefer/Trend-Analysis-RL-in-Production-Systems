"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Policy gradient reinforcement learning for fast quadrupedal locomotion","N. Kohl; P. Stone","Department of Computer Sciences, University of Texas, Austin, Austin, TX, USA; Department of Computer Sciences, University of Texas, Austin, Austin, TX, USA","IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004","6 Jul 2004","2004","3","","2619","2624 Vol.3","This paper presents a machine learning approach to optimizing a quadrupedal trot gait for forward speed. Given a parameterized walk designed for a specific robot, we propose using a form of policy gradient reinforcement learning to automatically search the set of possible parameters with the goal of finding the fastest possible walk. We implement and test our approach on a commercially available quadrupedal robot platform, namely the Sony Aibo robot. After about three hours of learning, all on the physical robots and with no human intervention other than to change the batteries, the robots achieved a gait faster than any previously known gait known for the Aibo, significantly outperforming a variety of existing hand-coded and learned solutions.","1050-4729","0-7803-8232-3","10.1109/ROBOT.2004.1307456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307456","","Legged locomotion;Robotics and automation;Machine learning;Leg;Humans;Testing;Robot control;Stability;Friction;Hardware","learning (artificial intelligence);gradient methods;legged locomotion","policy gradient reinforcement learning;fast quadrupedal locomotion;machine learning;quadrupedal trot gait;Sony Aibo robot","","286","3","17","IEEE","6 Jul 2004","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Based Dynamic Walking Control","Y. Mao; J. Wang; P. Jia; S. Li; Z. Qiu; Le Zhang; Z. Han","State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China; State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China; State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China; State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China; State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China; State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China; State Key Laboratory of Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing, China","Proceedings 2007 IEEE International Conference on Robotics and Automation","21 May 2007","2007","","","3609","3614","A quasi-passive dynamic walking robot is built to study natural and energy-efficient biped walking. The robot is actuated by MACCEPA actuators. A reinforcement learning based control method is proposed to enhance the robustness and stability of the robot's walking. The proposed method first learns the desired gait for the robot's walking on a flat floor. Then a fuzzy advantage learning method is used to control it to walk on uneven floor. The effectiveness of the method is verified by simulation results.","1050-4729","1-4244-0601-3","10.1109/ROBOT.2007.364031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4209649","passive dynamic walking;reinforcement learning;fuzzy advantage learning;biped robot","Learning;Legged locomotion;Robust stability;Knee;Robot sensing systems;Robotics and automation;Robot control;Leg;Actuators;Energy efficiency","fuzzy control;learning (artificial intelligence);learning systems;legged locomotion;robot dynamics;robust control","reinforcement learning;dynamic walking control;quasipassive dynamic walking robot;biped walking;MACCEPA actuators;robustness;stability;fuzzy advantage learning method","","15","","9","IEEE","21 May 2007","","","IEEE","IEEE Conferences"
"Application of Reinforcement Learning to UR10 Positioning for Prioritized Multi-Step Inspection in NVIDIA Omniverse","F. Bahrpeyma; A. Sunilkumar; D. Reichelt","Dresden University of Applied Sciences, Dresden, Germany; Dresden University of Applied Sciences, Dresden, Germany; Dresden University of Applied Sciences, Dresden, Germany","2023 IEEE Symposium on Industrial Electronics & Applications (ISIEA)","15 Aug 2023","2023","","","1","6","Sequential multi-step operations have long played an important role in manufacturing systems. In high level multi-step manufacturing processes, multiple operations are carried out in sequence on different machines/robots, while in low level applications, multi-step operations are performed by a single robot. This paper in particular studies the multi-step inspection problem with focus on robot-positioning. Traditionally, multi-step robot positioning was implemented in engineering cycles under a set of predetermined conditions. Nonetheless, due to recent trends towards mass-customization, the dynamic nature of working environments coupled with uncertainty has imposed multi-step robot positioning tasks with challenges that transcend static engineering cycles and require an automated adaptation process when dealing new products. Based on the requirements associated with multi-step inspection processes (in the context of mass-customization), and the associated dynamicity and uncertainty, this paper investigates the use of reinforcement learning in bringing automation into the development of inspection systems by focusing on the multi-step robot positioning problem. The simulations have been implemented for a set of state of the art reinforcement learning algorithm including DDPG, TD3, TRPO and PPO via NVIDIA Omniverse, Isaac Sim environment. Our experiments indicated that TRPO demonstrated the overall best performance for the accuracy in visiting the inspection points and job rejection rate, whereas PPO, as a theoretically more capable method, exhibited a lower performance.","2472-7660","979-8-3503-4749-4","10.1109/ISIEA58478.2023.10212317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10212317","Multi-step robotic tasks;robotics;Reinforcement Learning;PPO;TROP;TD3","Uncertainty;Sensitivity;Trajectory planning;Heuristic algorithms;Reinforcement learning;Inspection;Robot sensing systems","control engineering computing;industrial robots;inspection;manufacturing processes;mass production;position control;product customisation;production engineering computing;reinforcement learning;solid modelling","automated adaptation process;DDPG;high level multistep manufacturing processes;inspection systems;Isaac Sim environment;job rejection rate;manufacturing systems;mass-customization;multistep inspection processes;multistep robot positioning problem;NVIDIA Omniverse;PPO;prioritized multistep inspection;reinforcement learning;sequential multistep operations;static engineering cycles;TD3;TRPO;UR10 positioning","","1","","16","IEEE","15 Aug 2023","","","IEEE","IEEE Conferences"
"Reinforcement adaptive learning neural network based friction compensation for high speed and precision","Y. H. Kim; F. L. Lewis","Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA; Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA","Proceedings of the 37th IEEE Conference on Decision and Control (Cat. No.98CH36171)","6 Aug 2002","1998","1","","1064","1069 vol.1","There is an increasing number of applications in high precision motion control systems in manufacturing, i.e., ultra-precision machining, assembly of small components and micro devices. It is very difficult to assure such accuracy due to many factors affecting the precision of motion, such as friction and backlash in the drive system. The standard PID (proportional-integral-derivative) type servo control algorithms are not capable of delivering the desired precision under the influence of friction and backlash. In the paper, the friction and the disturbance are identified by a neural network. The weight adaptation rule, defined as a reinforcement adaptive-learning rule, is derived from the Lyapunov stability theory. Therefore the proposed scheme can be applicable to a wide class of mechanical systems. The simulation results on 1-DOF mechanical system verify the effectiveness of the proposed algorithm.","0191-2216","0-7803-4394-8","10.1109/CDC.1998.760838","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=760838","","Adaptive systems;Neural networks;Friction;Mechanical systems;Motion control;Manufacturing;Machining;Assembly systems;Three-term control;Servosystems","friction;compensation;motion control;learning (artificial intelligence);neurocontrollers;Lyapunov methods;manufacturing processes;mechanical variables control","reinforcement adaptive learning neural network based friction compensation;high precision motion control systems;weight adaptation rule;Lyapunov stability theory","","8","1","15","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Graph-based Reinforcement Learning meets Mixed Integer Programs: An application to 3D robot assembly discovery","N. Funk; S. Menzenbach; G. Chalvatzaki; J. Peters","Department of Computer Science, Technical University of Darmstadt; Department of Computer Science, Technical University of Darmstadt; Department of Computer Science, Technical University of Darmstadt; Department of Computer Science, Technical University of Darmstadt","2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","26 Dec 2022","2022","","","10215","10222","Robot assembly discovery (RAD) is a challenging problem that lives at the intersection of resource allocation and motion planning. The goal is to combine a predefined set of objects to form something new while considering task execution with the robot-in-the-loop. In this work, we tackle the problem of building arbitrary, predefined target structures entirely from scratch using a set of Tetris-like building blocks and a robotic manipulator. Our novel hierarchical approach aims at efficiently decomposing the overall task into three feasible levels that benefit mutually from each other. On the high level, we run a classical mixed-integer program for global optimization of block-type selection and the blocks' final poses to recreate the desired shape. Its output is then exploited to efficiently guide the exploration of an underlying reinforcement learning (RL) policy. This RL policy draws its generalization properties from a flexible graph-based representation that is learned through Q-learning and can be refined with search. Moreover, it accounts for the necessary conditions of structural stability and robotic feasibility that cannot be effectively reflected in the previous layer. Lastly, a grasp and motion planner transforms the desired assembly commands into robot joint movements. We demonstrate our proposed method's performance on a set of competitive simulated RAD environments, showcase real-world transfer, and report performance and robustness gains compared to an unstructured end-to-end approach.","2153-0866","978-1-6654-7927-1","10.1109/IROS47612.2022.9981784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981784","","Three-dimensional displays;Shape;Transforms;Manipulators;Robustness;Planning;Structural engineering","graph theory;integer programming;intelligent robots;learning (artificial intelligence);mobile robots;multi-robot systems;path planning;resource allocation;robot dynamics;robotic assembly","block-type selection;desired assembly commands;feasible levels;flexible graph-based representation;graph-based reinforcement learning;mixed integer programs;mixed-integer program;motion planning;predefined set;predefined target structures;Q-learning;resource allocation;RL policy;robot assembly discovery;robot joint movements;robot-in-the-loop;robotic feasibility;robotic manipulator;task execution;Tetris-like building blocks;underlying reinforcement learning policy","","4","","35","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Model-Based Reinforcement Learning via Proximal Policy Optimization","Y. Sun; X. Yuan; W. Liu; C. Sun","School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","4736","4740","Proximal policy optimization (PPO) is the state-of the-art most effective model-free reinforcement learning algorithm. Its powerful policy search ability allows an agent to find the optimal policy by trial and error but leads to high computation and low data-efficiency. Model-based algorithms can make the most efficient use of data by learning a forward model from observation, but face the challenge of model error. In this paper, we combine the strengths of both algorithms and introduce a data-efficient model-based approach called PIPPO (probabilistic inference via PPO). It makes online probabilistic dynamic model inference based on Gaussian process regression and executes offline policy improvement using PPO on the inferred model. Empirical evaluation on the pendulum benchmark problem shows that the proposed PIPPO algorithm has comparable performance and less interaction with the environment compared with traditional PPO.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8996875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8996875","reinforcement learning;proximal policy optimization;Gaussian process regression;data-efficiency","Computational modeling;Probabilistic logic;Data models;Heuristic algorithms;Gaussian processes;Optimization;Ground penetrating radar","Gaussian processes;learning (artificial intelligence);pendulums;regression analysis","model-based reinforcement learning;proximal policy optimization;PPO;model-free reinforcement;powerful policy search ability;optimal policy;low data-efficiency;model-based algorithms;forward model;model error;data-efficient model-based approach;online probabilistic dynamic model inference;policy improvement;inferred model;PIPPO algorithm","","5","","13","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Scheduling of Continuous Annealing With a Multi-Objective Differential Evolution Algorithm Based on Deep Reinforcement Learning","T. Li; Y. Meng; L. Tang","National Frontiers Science Center for Industrial Intelligence and Systems Optimization and the Key Laboratory of Data Analytics and Optimization for Smart Industry (Northeastern University), Ministry of Education, Shenyang, China; National Frontiers Science Center for Industrial Intelligence and Systems Optimization and the Key Laboratory of Data Analytics and Optimization for Smart Industry (Northeastern University), Ministry of Education, Shenyang, China; National Frontiers Science Center for Industrial Intelligence and Systems Optimization, Shenyang, China","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","14","This paper studies a multi-objective scheduling problem in a continuous annealing operation of the steel industry, which is to simultaneously determine the production batch sizes for coils and their schedules so as to minimize the cost of setup, earliness, and tardiness. A large batch can reduce the setup costs but lead to a cost increase in earliness and tardiness. Thus, the conflict objectives of minimizing setup cost, earliness, and tardiness can be formulated separately. In this paper, we formulate a multi-objective optimization model for the problem and develop an adaptive multi-objective differential evolutionary based on deep reinforcement learning (AMODE-DRL) for effectively obtaining the Pareto solutions. In the proposed AMODE-RDL, DRL is integrated into the MODE algorithm as a controller, which can adaptively select mutation operators and parameters according to different search domains. Computational results on randomly generated instances and the practical problem instances show that DRL can effectively guide MODE to select mutation operators and parameters. The proposed algorithm can obtain better solutions compared to other powerful multi-objective evolutionary and adaptive MODE algorithms. Note to Practitioners—Setup usually leads to a decrease in production capacity and an increase in production costs. Similarly, earliness and tardiness costs are also crucial. These costs correspond to production, customer demand, and inventory, which are common objectives in production systems. However, the three objectives are generally conflicting, and it is very hard for practitioners to make appropriate decisions with manual experience. The multi-objective optimization methods can provide different scheduling for practitioners who may choose the suitable one according to current working conditions. Accordingly, the proposed model and algorithm can extend to other fields with similar characteristic problems.","1558-3783","","10.1109/TASE.2023.3244331","Major Program of National Natural Science Foundation of China(grant numbers:72192830,72192831); National Natural Science Foundation of China(grant numbers:72002028); Higher Education Discipline Innovation Project(grant numbers:B16009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049395","Continuous annealing batch scheduling;multi-objective optimization;differential evolution;deep reinforcement learning","Costs;Annealing;Production;Coils;Optimization;Job shop scheduling;Steel","","","","2","","","IEEE","22 Feb 2023","","","IEEE","IEEE Early Access Articles"
"Self-adaptive multi-objective optimization method design based on agent reinforcement learning for elevator group control systems","Fanlin Zeng; Qun Zong; Zhengya Sun; Liqian Dou","College of Electric and Automation Engineering, Tianjin University, TJU, Tianjin, China; College of Electric and Automation Engineering, Tianjin University, TJU, Tianjin, China; College of Electric and Automation Engineering, Tianjin University, TJU, Tianjin, China; College of Electric and Automation Engineering, Tianjin University, TJU, Tianjin, China","2010 8th World Congress on Intelligent Control and Automation","23 Aug 2010","2010","","","2577","2582","This paper study the multi-objective optimization problem of elevator group control systems by using the Markov Decision Process model. Define the Agent to be the leaner and decision-maker of the MDP model. And then using reinforcement learning Algorithm combined with generic method defines the elements of this model. Moreover we use SARSA(λ ) value iteration algorithm which was selected to iterative estimation the utility function combined with tile coding function approximation to design an optimization algorithm, and then prove that the solution of this algorithm will converges to a bounded domain which is given in this paper. The effect for dynamic optimization objective function of proposed approach was validated by virtual simulation environment of elevator group control systems.","","978-1-4244-6712-9","10.1109/WCICA.2010.5554696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5554696","elevator group control systems;adaptive multi-objective optimization;optimizing parameters of the evaluation function;reinforcement learning;Agent;SARSA(λ ) Algorithm","Tiles;Encoding;Markov processes;Control systems;Approximation algorithms;Algorithm design and analysis;Heuristic algorithms","control system synthesis;decision making;function approximation;iterative methods;learning (artificial intelligence);lifts;Markov processes;optimisation","self adaptive multiobjective optimization method design;agent reinforcement learning;elevator group control system;Markov decision process model;decision making;SARSA;iteration algorithm;iterative estimation;utility function;tile coding function approximation;virtual simulation environment","","2","","","IEEE","23 Aug 2010","","","IEEE","IEEE Conferences"
"Output Feedback Reinforcement Q-learning for Optimal Quadratic Tracking Control of Unknown Discrete-Time Linear Systems and Its Application","G. Zhao; W. Sun; H. Cai; Y. Peng","College of Automation Science and Engineering, South China University of Technology, Guangzhou, P. R. China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, P. R. China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, P. R. China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, P. R. China","2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)","20 Dec 2018","2018","","","750","755","In this paper, a novel output feedback solution based on the Q-learning algorithm using the measured data is proposed for the linear quadratic tracking (LQT) problem of unknown discrete-time systems. To tackle this technical issue, an augmented system composed of the original controlled system and the linear command generator is first constructed. Then, by using the past input, output, and reference trajectory data of the augmented system, the output feedback Q-learning scheme is able to learn the optimal tracking controller online without requiring any knowledge of the augmented system dynamics. Learning algorithms including both policy iteration (PI) and value iteration (VI) algorithms are developed to converge to the optimal solution. Finally, simulation results are provided to verify the effectiveness of the proposed scheme.","","978-1-5386-9582-1","10.1109/ICARCV.2018.8581252","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8581252","","Heuristic algorithms;Trajectory;System dynamics;Mathematical model;Output feedback;Generators;Approximation algorithms","discrete time systems;feedback;iterative methods;learning systems;linear quadratic control;linear systems;trajectory control","Q-learning algorithm;linear quadratic tracking problem;discrete-time systems;original controlled system;linear command generator;reference trajectory data;VI algorithm;value iteration algorithm;PI algorithm;policy iteration algorithm;output feedback solution;unknown discrete-time linear systems;optimal quadratic tracking control;output feedback reinforcement Q-learning;optimal solution;learning algorithms;augmented system dynamics;optimal tracking controller;output feedback Q-learning scheme","","1","","19","IEEE","20 Dec 2018","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-based Swing-Free Trajectories Planning Algorithm for UAV with a Suspended Load","R. Li; F. Yang; Y. Xu; W. Yuan; Q. Lu","HDU-ITMO Joint Institute, Hangzhou Dianzi University, Hangzhou, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","6149","6154","UAV transportation has great application prospects, and can be used to transport some materials that need to be delivered quickly, such as food, medicine, and even organ transplantation, in the face of many special situations available using rope suspended load, but, when using suspension transport, the swing of the load will increase the instability of the flight. To address this situation, we propose a method based on Deep Q-Networks to generate the minimum swing trajectory of UAV suspended load. First, a training environment is established using the kinematic model of UAV suspended load to discretize the action, which will greatly accelerate the training speed; Finally, a step-by-step regional approximation exploration reward function is designed, and the final training simulation results verify the feasibility of the method, and the suspended load oscillation is controlled within a certain range during the whole transportation process.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055333","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055333","Deep Q-Networks;UAV;Suspended load;Path planning","Training;Trajectory planning;Simulation;Transportation;Kinematics;Rendering (computer graphics);Trajectory","autonomous aerial vehicles;control engineering computing;deep learning (artificial intelligence);learning (artificial intelligence);motion control;path planning;position control;reinforcement learning;suspensions;trajectory control","Deep Q-Networks;Deep reinforcement learning-based swing-free trajectories;final training simulation results;great application prospects;minimum swing trajectory;organ transplantation;special situations available using rope;step-by-step regional approximation exploration reward function;suspended load oscillation;suspension transport;training environment;training speed;transportation process;UAV transportation","","","","18","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Method for Quadrotor Attitude Control Based on Expert Information","Y. Zhu; S. Lian; W. Zhong; W. Meng","School of Automation, Guangdong University of Technology, Guangdong, China; School of Automation, Guangdong University of Technology, Guangdong, China; School of Automation, Guangdong University of Technology, Guangdong, China; School of Automation, Guangdong University of Technology, Guangdong, China","2023 8th International Conference on Automation, Control and Robotics Engineering (CACRE)","8 Aug 2023","2023","","","281","286","In this paper, a model-free reinforcement learning(RL) method of training a nonlinear attitude controller of a quadrotor is proposed. For the problem that the attitude controller is uncontrolled when trained by RL directly, the proposed method utilizes an expert to provide the prior information, i.e. the action’s judgement and suggestion, to guide the updating process. For the problem that the policy falls in local optima by the limitation of the expert, the proposed method maximize the entropy of the strategy to increase the exploratory behavior of the nonlinear attitude controller approximator. Furthermore, We employ the Proximal policy optimization algorithm (PPO) as the RL model and PID algorithm as the expert model to approach an exact attitude controller of a quadrotor based on the proposed method. Finally, the simulations experiments has been conducted to verify that our proposed method can train a true nonlinear attitude controller which has a better performance than the expert.","","979-8-3503-0277-6","10.1109/CACRE58689.2023.10208497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10208497","Attitude control;Expert information;Entropy;Proximal policy optimization;PID;Quadrotor","Training;Attitude control;Heuristic algorithms;Process control;Switches;Reinforcement learning;Approximation algorithms","attitude control;autonomous aerial vehicles;helicopters;learning (artificial intelligence);nonlinear control systems;optimisation;reinforcement learning;three-term control","exact attitude controller;expert information;expert model;nonlinear attitude controller approximator;Proximal policy optimization algorithm;quadrotor attitude control;reinforcement learning method","","","","15","IEEE","8 Aug 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-Based Control of Bicycle Robots on Rough Terrain","X. Zhu; X. Zheng; Y. Deng; Z. Chen; B. Liang; Y. Liu","School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China; School of Automation, Beijing University of Posts and Telecommunications, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China","2023 9th International Conference on Control, Automation and Robotics (ICCAR)","21 Jun 2023","2023","","","103","108","The bicycle robot is a kind of unmanned mobile robot with great potentials. However, the control of such robots on rough terrain under model uncertainties and disturbances is challenging due to the lateral instability and underactuated char-acterisitcs. In this paper, a controller based on deep reinforcement learning and Stanley algorithm is designed to achieve the path tracking and balancing control for a bicycle robot on rough terrain. First, the outputs of the controller are compensated by residual reinforcement learning to reduce the gap between the training environment and the test environment. Then, in balancing control, the piecewise curriculum learning is designed to accelerate the training process. In path tracking control, Stanley algorithm is used. The proposed algorithm is tested in Gazebo simulation environment. The results showed that the proposed algorithm achieved good performance.","2251-2454","979-8-3503-2251-4","10.1109/ICCAR57134.2023.10151769","National Natural Science Foundation of China(grant numbers:62203252,52205008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10151769","bicycle robot;path tracking;balancing control;reinforcement learning;curriculum learning","Training;Uncertainty;Tracking;Shape;Process control;Reinforcement learning;Bicycles","bicycles;control engineering computing;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;path planning;reinforcement learning","balancing control;bicycle robot;deep reinforcement learning;path tracking control;piecewise curriculum learning;residual reinforcement learning;rough terrain;Stanley algorithm;underactuated char-acterisitcs;unmanned mobile robot","","","","23","IEEE","21 Jun 2023","","","IEEE","IEEE Conferences"
"Data-driven Reinforcement Learning for Linear Quadratic Control of Unknown Systems over Lossy Channels","L. Zhang; L. Xiao; J. Wang; M. Z. Q. Chen","School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China","2022 IEEE 5th International Conference on Automation, Electronics and Electrical Engineering (AUTEEE)","2 Jan 2023","2022","","","191","195","This research deals with the linear quadratic (LQ) optimal control and suboptimal control of networked systems with packet losses and unknown system dynamics. Three system models including two kinds of Transmission Control protocol (TCP) and one User Datagram protocol (UDP) are discussed. The LQ control problems corresponding to these cases are solved respectively by the model-free methods. In the infinite time domain optimal regulation problem, the model-free method only uses the input and output data to design the controller. A numerical example illustrates the validity of the theoretical results.","2831-4549","978-1-6654-7197-8","10.1109/AUTEEE56487.2022.9994506","National Natural Science Foundation of China(grant numbers:61903194,61873129); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9994506","networked systems;linear quadratic control;reinforcement learning;packet dropout","Electrical engineering;Protocols;Q-learning;System dynamics;Optimal control;Packet loss;Data models","approximation theory;learning (artificial intelligence);linear quadratic control;optimal control;suboptimal control;transport protocols","data-driven reinforcement learning;infinite time domain optimal regulation problem;linear quadratic Control;linear quadratic optimal control;lossy channels;LQ control problems;model-free method;networked systems;output data;packet losses;suboptimal control;system models;Transmission Control protocol;unknown system dynamics;unknown systems;User Datagram protocol","","","","15","IEEE","2 Jan 2023","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach for Robust Restoration of Generators in Power System","H. Li; X. Yang; C. Zhai","School of Automation, China University of Geosciences Wuhan Campus; Singapore-ETH Centre CREATE Campus, Future Resilient Systems, Singapore; School of Automation, China University of Geosciences Wuhan Campus","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","1651","1656","In the past decades, cascading blackouts have caused serious damages to power systems and affected the normal operation of society, so it is crucial to quickly restore the damaged power system to normal state. In this paper, a reinforcement learning (RL) approach is developed to achieve the robust restoration of generators in power systems. Firstly, the performance recovery of damaged power system is modeled as markov decision process, and constraints such as line voltage and active power output of generators are considered. Secondly, a RL algorithm is proposed to search the optimal control strategy for generator units recovery. Based on the proposed restoration approach, Q-learning algorithm is employed to obtain the optimal strategy for power supply of generaors in power grids. Finally, numerical simulations are carried out on IEEE 9-bus system under different scenarios of external disturbances and certainties. The simulation results demonstrate the effectiveness and feasibility of the proposed approach.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055589","Fundamental Research Funds for the Central Universities; China University of Geosciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055589","Cascading blackouts;Reinforcement learning;Generator units recovery;Q-learning;IEEE 9-bus system","Q-learning;Power supplies;Simulation;Power system protection;Generators;Explosions;Power systems","learning (artificial intelligence);Markov processes;optimal control;power engineering computing;power grids;reinforcement learning","damaged power system;generator units recovery;IEEE 9-bus system;power grids;Q-learning algorithm;reinforcement learning approach;restoration approach;serious damages","","","","24","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Data-Driven MPC for Linear Systems using Reinforcement Learning","Z. Sun; Q. Wang; J. Pan; Y. Xia","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","394","399","This paper proposes a novel scheme to solve the optimal control problem for unknown linear systems in a data driven manner. The method doesn’t require any prior knowledge of the system, and only utilizes past input-output trajectories to describe the system features implicitly and realize the prediction on the basis of behavioral systems theory. Meanwhile, we adopt reinforcement learning to update the terminal cost function online to ensure the closed-loop stability. The merit of the proposed scheme is the avoiding of the system identification process and the complex design process of terminal cost, terminal set and terminal controller in the standard MPC. We verify the performance of the algorithm by simulation.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728233","Model predictive control (MPC);reinforcement learning (RL);data-driven method","Linear systems;Costs;Reinforcement learning;Predictive models;Prediction algorithms;Stability analysis;Trajectory","closed loop systems;control system synthesis;learning (artificial intelligence);linear systems;nonlinear control systems;optimal control;predictive control;stability","system features;behavioral systems theory;reinforcement learning;terminal cost function;closed-loop stability;system identification process;complex design process;terminal set;terminal controller;standard MPC;data-driven MPC;optimal control problem;unknown linear systems;data driven manner;input-output trajectories","","","","20","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Machine Learning (Reinforcement Learning)-Based Steel Stock Yard Planning Algorithm","J. Hun Woo; Y. In Cho; S. Hyeon Yu; S. Hyun Nam; H. Zhu; D. Hoon Kwak; J. -H. Nam","Dept. of Naval Architecture and Ocean Eng., Seoul National University, Seoul, SOUTH KOREA; Dept. of Naval Architecture and Ocean Eng., Seoul National University, Seoul, SOUTH KOREA; Dept. of Naval Architecture and Ocean Eng., Seoul National University, Seoul, SOUTH KOREA; Dept. of Naval Architecture and Ocean Eng., Seoul National University, Seoul, SOUTH KOREA; Dept. of Naval Architecture and Ocean Eng., Seoul National University, Seoul, SOUTH KOREA; Dept. of Naval Architecture and Ocean Eng., Seoul National University, Seoul, SOUTH KOREA; Div. of Naval Architecture and Ocean Syst. Eng., Korea Maritime & Ocean University, Busan, SOUTH KOREA","2020 Winter Simulation Conference (WSC)","29 Mar 2021","2020","","","1560","1571","Steel plates are supplied to shipyards without adhering to the processing schedule because they are ordered in bulk according to the steel market condition and the mid- to long-term production strategy. Hence, steel plates are stacked up in the steel stock yard until they are input to the processing system and then supplied sequentially according to the processing start date. Currently, a steel stock yard, is operated by the experience of field workers, and inefficiencies such as excessive crane use are occurring, so efficient management techniques are required. However, the conventional optimization algorithm has limitations because the input timing of the steel is random. In this study, a study was conducted to determine the order of steel input into steel stock yard using reinforcement learning algorithm. Effective algorithm(A3C) can be identified through tests, and it was validated that proposed method is effective for problems of actual size steel stock yard.","1558-4305","978-1-7281-9499-8","10.1109/WSC48552.2020.9384049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384049","","Schedules;Machine learning algorithms;Cranes;Transportation;Reinforcement learning;Timing;Steel","cranes;learning (artificial intelligence);optimisation;production engineering computing;scheduling;shipbuilding industry;steel;steel industry","steel plates;steel market condition;processing start date;steel input;reinforcement learning algorithm;actual size steel stock yard;A3C;mid-to long-term production strategy;field workers;shipyards","","","","11","IEEE","29 Mar 2021","","","IEEE","IEEE Conferences"
"AlphaSOC: Reinforcement Learning-based Cybersecurity Automation for Cyber-Physical Systems","R. Silva; C. Hickert; N. Sarfaraz; J. Brush; J. Silbermann; T. Sookoor",Johns Hopkins University Applied Physics Laboratory; Johns Hopkins University Applied Physics Laboratory; Johns Hopkins University Applied Physics Laboratory; Johns Hopkins University Applied Physics Laboratory; Johns Hopkins University Applied Physics Laboratory; Johns Hopkins University Applied Physics Laboratory,"2022 ACM/IEEE 13th International Conference on Cyber-Physical Systems (ICCPS)","24 Jun 2022","2022","","","290","291","Achieving agile and resilient autonomous capabilities for cyber defense requires moving past indicators and situational awareness into automated response and recovery capabilities. The objective of the AlphaSOC project is to use state of the art sequential decision-making methods to automatically investigate and mitigate attacks on cyber physical systems (CPS). To demonstrate this, we developed a simulation environment that models the distributed navigation control system and physics of a large ship with two rudders and thrusters for propulsion. Defending this control network requires processing large volumes of cyber and physical signals to coordi-nate defensive actions over many devices with minimal disruption to nominal operation. We are developing a Reinforcement Learning (RL)-based approach to solve the resulting sequential decision-making problem that has large observation and action spaces.","","978-1-6654-0967-4","10.1109/ICCPS54341.2022.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797597","Reinforcement Learning;Cybersecurity;Cyber-Physical System","Navigation;Decision making;Process control;Reinforcement learning;Cyber-physical systems;Propulsion;Control systems","decision making;learning (artificial intelligence);security of data","cyber-physical systems;agile capabilities;resilient autonomous capabilities;cyber defense;situational awareness;automated response;recovery capabilities;AlphaSOC project;cyber physical systems;distributed navigation control system;physical signals;coordi-nate defensive actions;sequential decision-making problem;reinforcement learning-based cybersecurity automation","","1","","5","IEEE","24 Jun 2022","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Driven Artificial Bee Colony Algorithm for Distributed Heterogeneous No-Wait Flowshop Scheduling Problem With Sequence-Dependent Setup Times","F. Zhao; Z. Wang; L. Wang","School of Computer and Communication Technology, Lanzhou University of Technology, Lanzhou, China; School of Computer and Communication Technology, Lanzhou University of Technology, Lanzhou, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Automation Science and Engineering","5 Oct 2023","2023","20","4","2305","2320","The distributed heterogeneous factory system is a typical scenario in the manufacturing industry. A distributed heterogeneous no-wait flowshop scheduling problem with sequence-dependent setup times (DHNWFSP-SDST) is studied in this paper. The differences in factory configuration and transportation time are considered in DHNWFSP-SDST. A mixed-integer linear programming (MILP) model is constructed and an artificial bee colony algorithm (ABC) with Q-learning (QABC) is proposed to address the DHNWFSP-SDST. Heuristic methods named NEH_H and DHHS are designed to construct potential initial candidates for the population. The neighborhood structures based on the job blocks are introduced in QABC to explore the solution space during the evolution processes. The Q-learning mechanism is employed to select neighborhood structures via empirical knowledge in the operation processes. The speed-up methods to accelerate the evaluation of the obtained neighborhood are designed to reduce the computation time of the QABC. The experimental results show that the QABC is a potential algorithm to address the DHNWFSP-SDST. Note to Practitioners—Distributed manufacturing under the heterogeneous environment generally exists in real manufacturing systems. The scheduling problem refers to the reasonable arrangement of production orders to optimize certain indicators under limited time, resources, and computing costs. Distributed heterogeneous no-wait flowshop scheduling problem with setup times is an important industrial scheduling problem in distributed heterogeneous manufacturing systems. The problem takes into account the differences in factory configuration and transportation time in distributed regions. The problem is a kind of NP-hard problem as the solution space is huge. A reinforcement learning driven artificial bee colony algorithm is designed to address the problem. Heuristic methods are utilized to construct potential initial solutions. Neighborhood structures are designed to further optimize the initial solution. The effective empirical guidance is provided by Q-learning for the selection of the neighborhood structure of the algorithm to avoid invalid search. The experimental results show that the QABC obtains a high-quality scheduling scheme in a reasonable time.","1558-3783","","10.1109/TASE.2022.3212786","National Natural Science Foundation of China(grant numbers:62063021,62273193); Key Talent Project of Gansu Province(grant numbers:ZZ2021G50700016); Key Research Programs of Science and Technology Commission Foundation of Gansu Province(grant numbers:21YF5WA086); Lanzhou Science Bureau Project(grant numbers:2018-rc-98); Project of Gansu Natural Science Foundation(grant numbers:21JR7RA204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9934830","Heterogeneous flowshop scheduling;sequence-dependent setup times;artificial bee colony algorithm;Q-learning;neighborhood structure","Production facilities;Job shop scheduling;Q-learning;Artificial bee colony algorithm;Metaheuristics;Heuristic algorithms;Production","","","","2","","31","IEEE","1 Nov 2022","","","IEEE","IEEE Journals"
"Flexible Job Shop Scheduling Based on Deep Reinforcement Learning","Y. Feng; L. Zhang; Z. Yang; Y. Guo; D. Yang","School of Control Science and Engineering, Dalian University of Technology, Dalian, China; School of Control Science and Engineering, Dalian University of Technology, Dalian, China; Chinese Academy of Sciences, Shenzhen Institute of Advanced Technology, Shenzhen, China; Chinese Academy of Sciences, Shenzhen Institute of Advanced Technology, Shenzhen, China; Intelligent Electrical Science and Technology Research Institute, Northeastern University, Shenyang, China","2021 5th Asian Conference on Artificial Intelligence Technology (ACAIT)","14 Mar 2022","2021","","","660","666","With the rise of industry 4.0 and the establishment of intelligent factories, how to use artificial intelligence algorithm to solve flexible job shop scheduling problem (FJSP) is one of the hot research. FJSP is proved to be a NP-hard problem with a large solution space. Compared with the job-shop scheduling problem (JSP), FJSP should not only know the processing time of the workpiece process, but also need the machine to optimize the objective function. Therefore, this paper considers the maximum completion time as the objective function, and proposes a deep reinforcement learning (DRL) algorithm to solve FJSP. Compared with heuristic rule and genetic algorithm, the numerical results show that DRL algorithm has better searching ability and better effect in solving FJSP problem, which provides a new idea for solving shop scheduling problem by artificial intelligence algorithm.","","978-1-6654-2630-5","10.1109/ACAIT53529.2021.9731322","Research and Development; Innovation Fund; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9731322","Deep reinforcement learning algorithm;flexible job shop scheduling problem;maximum completion time","Job shop scheduling;NP-hard problem;Heuristic algorithms;Reinforcement learning;Linear programming;Search problems;Production facilities","computational complexity;deep learning (artificial intelligence);factory automation;job shop scheduling;optimisation;production engineering computing;production facilities;reinforcement learning;search problems","NP-hard problem;workpiece process;objective function;maximum completion time;deep reinforcement learning algorithm;genetic algorithm;DRL algorithm;FJSP problem;artificial intelligence algorithm;intelligent factories;flexible job shop scheduling problem;industry 4.0;heuristic rule;searching ability","","3","","14","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Control and Networking Co-Design for Industrial Internet of Things","H. Xu; X. Liu; W. Yu; D. Griffith; N. Golmie","Department of Computer and Information Sciences, Towson University, Towson, USA; Department of Computer and Information Sciences, Towson University, Towson, USA; Department of Computer and Information Sciences, Towson University, Towson, USA; National Institute of Standards and Technology (NIST), Gaithersburg, USA; National Institute of Standards and Technology (NIST), Gaithersburg, USA","IEEE Journal on Selected Areas in Communications","6 May 2020","2020","38","5","885","898","Industrial Internet-of-Things (IIoT), also known as Industry 4.0, is the integration of Internet of Things (IoT) technology into the industrial manufacturing system so that the connectivity, efficiency, and intelligence of factories and plants can be improved. From a cyber physical system (CPS) perspective, multiple systems (e.g., control, networking and computing systems) are synthesized into IIoT systems interactively to achieve the operator’s design goals. The interactions among different systems is a non-negligible factor that affects the IIoT design and requirements, such as automation, especially under dynamic industrial operations. In this paper, we leverage reinforcement learning techniques to automatically configure the control and networking systems under a dynamic industrial environment. We design three new policies based on the characteristics of industrial systems so that the reinforcement learning can converge rapidly. We implement and integrate the reinforcement learning-based co-design approach on a realistic wireless cyber-physical simulator to conduct extensive experiments. Our experimental results demonstrate that our approach can effectively and quickly reconfigure the control and networking systems automatically in a dynamic industrial environment.","1558-0008","","10.1109/JSAC.2020.2980909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9044436","Industry 4.0;Internet of Things;reinforcement learning;control and networking co-design","Control systems;Reinforcement learning;Wireless sensor networks;Wireless communication;Sensor systems","cyber-physical systems;industrial plants;Internet of Things;learning (artificial intelligence);manufacturing systems;production engineering computing","Industry 4.0;dynamic industrial operations;nonnegligible factor;operator;IIoT systems;cyber physical system perspective;industrial manufacturing system;Industrial Internet-of-Things;networking co-design;reinforcement learning-based control;networking systems;wireless cyber-physical simulator;industrial systems;dynamic industrial environment","","40","","42","IEEE","23 Mar 2020","","","IEEE","IEEE Journals"
"Obstacle avoidance of multi mobile robots based on behavior decomposition reinforcement learning","Linan Zu; Peng Yang; Lingling Chen; Xueping Zhang; Yantao Tian","School of Electrical Engineering and Automation, Hebei University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Hebei University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Hebei University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Hebei University of Technology, Tianjin, China; College of Communication Engineering, Jilin University, Changchun, China","2007 IEEE International Conference on Robotics and Biomimetics (ROBIO)","16 May 2008","2007","","","1018","1023","A reinforcement learning method based on behavior decomposition was proposed for obstacle avoidance of multi mobile robots. It decomposed the complicated behaviors into a series of simple sub-behaviors which were learned independently. The learning structures, parameters and reinforcement functions of every behavior are designed. Then, the fusion for learning results of all behaviors was optimized by learning. This learning algorithm could reduce the status space and predigest the design of reinforcement functions so as to improve the learning speed and the veracity of learning results. Finally, this learning method was adopted to realize the self-adaptation action fusion of mobile robots in the task of obstacle avoidance. And its efficiency was validated by simulation results.","","978-1-4244-1761-2","10.1109/ROBIO.2007.4522303","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4522303","Reinforcement learning;Q-learning;obstacle avoidance;behavior decomposition","Mobile robots;Learning systems;Robotics and automation;Partitioning algorithms;Orbital robotics;Robustness;Biomimetics;Algorithm design and analysis;Educational institutions;Mobile communication","collision avoidance;learning (artificial intelligence);mobile robots;multi-robot systems","obstacle avoidance;multimobile robot;behavior decomposition reinforcement learning","","2","","13","IEEE","16 May 2008","","","IEEE","IEEE Conferences"
"Decentralized reinforcement social learning based on cooperative policy exploration in multi-agent systems","C. Wang; X. Chen","School of Automation, Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China; School of Automation, Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China","2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","30 Nov 2017","2017","","","1575","1580","Coordination problems including miscoordination and relative overgeneralization are difficult to overcome especially in dynamic and stochastic environments. In the practical scenario, there may be a large number of agents, and the interactions between agents may be sparse and unfixed. In this paper, we study the coordination problems and stochastic rewards under the social learning framework where there are a group of agents and each agent behaves independently and interacts with another agent randomly chosen from the group. We are looking for a learning technique that makes all agents learn a consistent optimal policy in a two agents game with pathology of coordination problems or stochastic rewards under such a framework. A new algorithm named Decentralized concurrent learning and cooperative policy exploration (DCL-CPE) is contributed, which possesses the ability to overcome the coordination problems and the stochastic rewards via local interaction under the social learning framework. Empirical results for several cooperative games are presented to show the superiority of our algorithm.","","978-1-5386-1645-1","10.1109/SMC.2017.8122839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8122839","Multi-agent systems;Multi-agent coordination;Social learning;Concurrent learning;Cooperative games","Games;Nash equilibrium;Space exploration;Convergence;Multi-agent systems;Automation;Pathology","game theory;learning (artificial intelligence);multi-agent systems;stochastic processes","Decentralized reinforcement social learning;multiagent systems;coordination problems;stochastic rewards;learning technique;agents game;Decentralized concurrent learning;cooperative policy exploration;optimal policy","","1","","16","IEEE","30 Nov 2017","","","IEEE","IEEE Conferences"
"Control of Five-qubit System Based on Quantum Reinforcement Learning","D. Dong; C. Chen; Z. Chen; C. Zhang","Department of Automation, University of Science and Technology, Hefei, Anhui, China; Department of Automation, University of Science and Technology, Hefei, Anhui, China; Department of Automation, University of Science and Technology, Hefei, Anhui, China; Department of Automation, University of Science and Technology, Hefei, Anhui, China","2006 International Conference on Computational Intelligence and Security","29 Jan 2007","2006","1","","164","167","Controlling the multi-qubit system is a key task for practical quantum information processing. In this paper, the control problem of five-qubit is studied. A novel quantum reinforcement learning algorithm based on quantum superposition principle is proposed for the quantum control problem. The simulated result shows that quantum reinforcement learning can effectively find the optimal control sequence through fast learning","","1-4244-0604-8","10.1109/ICCIAS.2006.294113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4072066","","Control systems;Learning;Quantum mechanics;Automatic control;Automation;Optimal control;Quantum computing;Nuclear magnetic resonance;Information processing;Information theory","control engineering computing;discrete systems;learning (artificial intelligence);optimal control;quantum theory","five-qubit system control;quantum reinforcement learning;quantum information processing;quantum superposition;quantum control;optimal control","","","","32","IEEE","29 Jan 2007","","","IEEE","IEEE Conferences"
"Minerva: A reinforcement learning-based technique for optimal scheduling and bottleneck detection in distributed factory operations","T. E. Thomas; J. Koo; S. Chaterji; S. Bagchi","School of Electrical and Computer Engineering, Purdue University; School of Electrical and Computer Engineering, Purdue University; School of Electrical and Computer Engineering, Purdue University; School of Electrical and Computer Engineering, Purdue University","2018 10th International Conference on Communication Systems & Networks (COMSNETS)","2 Apr 2018","2018","","","129","136","In manufacturing systems, the term bottleneck refers to a component that limits the entire throughput of a system. A number of approaches have attempted bottleneck detection. However, existing solutions have limitations, leaving the bottleneck identification still no trivial task. To address the shortcomings of prior works, we study Job Shop Scheduling Problems (JSSP) with the realistic extension that jobs are enqueued periodically, and propose a machine learning based solution to such a problem, named Minerva. Minerva first finds the optimal resource scheduling for a target interval, based on a model-free reinforcement learning technique. Then, using an artificial neural network classifier, Minerva identifies the constrained resources for each target interval. Minerva is evaluated on two representative benchmarks with the key result being that Minerva is able to detect the system bottleneck(s) with a high accuracy of 95.2%, which is almost 25% better than the best-in-class bottleneck identification methods.","2155-2509","978-1-5386-1182-1","10.1109/COMSNETS.2018.8328189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8328189","","Throughput;Optimal scheduling;Production facilities;Job shop scheduling;Learning (artificial intelligence);Neural networks;Communication systems","job shop scheduling;learning (artificial intelligence);manufacturing systems;neural nets;production engineering computing;production facilities","model-free reinforcement learning technique;best-in-class bottleneck identification methods;artificial neural network classifier;machine learning based solution;Minerva;job shop scheduling problems;manufacturing systems;reinforcement learning-based technique;optimal resource scheduling;distributed factory operations;bottleneck detection;optimal scheduling","","19","","37","IEEE","2 Apr 2018","","","IEEE","IEEE Conferences"
"Virtual Commissioning Simulation as OpenAI Gym - A Reinforcement Learning Environment for Control Systems","F. Jaensch; L. Klingel; A. Verl","ISW University of Stuttgart, Stuttgart, Germany; ISW University of Stuttgart, Stuttgart, Germany; ISW University of Stuttgart, Stuttgart, Germany","2022 5th International Conference on Artificial Intelligence for Industries (AI4I)","15 May 2023","2022","","","64","67","Manual development of control systems’ software is time-consuming and error-prone. Thus, high costs are already incurred in this phase of mechatronic system development. Virtual prototypes have so far only been used for testing purposes, such as virtual commissioning, but not for the automated creation of the control. A good test environment can also be extended to a learning environment with appropriate trial and error based algorithms. This work shows an approach to extend an industrial software tool for virtual commissioning as a standardized OpenAI gym environment. Thereby, established reinforcement learning algorithms can be used more easily and a step towards an industrial application of self-learning control systems can be made. The goal of this work is to provide industry and research with a platform for easy entry into the field of reinforcement learning.","2770-4718","978-1-6654-5961-7","10.1109/AI4I54798.2022.00023","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10123668","reinforcement learning;virtual commissioning;simulation;digital twin","Industries;Mechatronics;Costs;Software algorithms;Prototypes;Reinforcement learning;Manuals","control engineering computing;learning (artificial intelligence);mechatronics;power engineering computing;public domain software;reinforcement learning;software tools","appropriate trial;automated creation;error based algorithms;error-prone;established reinforcement learning algorithms;good test environment;industrial software tool;learning environment;manual development;mechatronic system development;self-learning control systems;standardized OpenAI gym environment;testing purposes;virtual commissioning simulation;virtual prototypes","","1","","14","IEEE","15 May 2023","","","IEEE","IEEE Conferences"
"Multi-agent Reinforcement Learning in a Large Scale Environment via Supervisory Network and Curriculum Learning","S. Do; C. Lee","Defense ICT Conversion Research Section, Electronics and Telecommunications Research Institute (ETRI), Daejeon, Korea; Defense ICT Conversion Research Section, Electronics and Telecommunications Research Institute (ETRI), Daejeon, Korea","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","207","210","Multi-agent reinforcement learning is essential for learning optimal policy for collaboration and competition environments. However, as the action space of the agent increases, the number of state-action pairs which have to be explored increases exponentially. As a result, increasing search space causes difficulty to converge the learning. To solve this problem, we propose a supervisory network. To achieve the global goal, the supervisory network creates a sub-goal and assigns the goals to the agents so that the agents can effectively learn the optimal policy with a small action space. In addition, we adapt the curriculum learning method to learn a large-scale environment. As a consequence, the agent can explore the environment in which the complexity increases gradually. Although a baseline network was learned in the same environment to compare with our model, the baseline fails to learn an optimal policy while our model successes to learn in the large-scale environment.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649915","Electronics and Telecommunications Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649915","Deep reinforcement learning;action planning;task assignment;curriculum learning","Learning systems;Adaptation models;Automation;Collaboration;Reinforcement learning;Control systems;Complexity theory","deep learning (artificial intelligence);multi-agent systems;reinforcement learning","multiagent reinforcement learning;supervisory network;optimal policy learning;competition environments;action space;state-action pairs;curriculum learning;large-scale environment;search space;baseline network;deep reinforcement learning","","3","","9","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning-based Learning from Demonstrations for Collaborative Robots","W. D. Li","School of Logistics Engineering, Wuhan University of Technology, China","2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)","5 Oct 2021","2021","","","1642","1647","Learning from Demonstrations (LfD) can support a human operator to control a collaborative robot (cobot) in an intuitive means. Gaussian Mixture Model and Gaussian Mixture Regression (GMM and GMR) are useful tools for implementing such a LfD approach. However, well-performed GMM/GMR require a series of demonstrations without trembling and jerky features, which is challenging to achieve in practical applications. To address this issue, in this paper, an improved Reinforcement Learning (RL)-based approach for GMM/GMR is devised to carry out a variety of complex tasks. The innovations of the research are twofold: firstly, a Gaussian noise strategy is designed to scatter demonstrations with trembling and jerky features to better support the optimization of GMM/GMR; Secondly, an improved RL-based optimization algorithm is developed to eliminate potential under-lover-fitting GMM/GMR. A cases study was conducted to verify the approach. Experimental results and comparative analyses showed that this developed approach exhibited good performances in computational efficiency and solution quality.","2161-8089","978-1-6654-1873-7","10.1109/CASE49439.2021.9551596","National Natural Science Foundation of China(grant numbers:51975444); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551596","","Technological innovation;Service robots;Gaussian noise;Collaboration;Scattering;Tools;Manufacturing","Gaussian noise;industrial robots;learning (artificial intelligence);mixture models;multi-robot systems","improved RL-based optimization algorithm;scatter demonstrations;Gaussian noise strategy;trembling jerky features;LfD approach;Gaussian Mixture Regression;Gaussian Mixture Model;intuitive means;human operator;collaborative robot;Reinforcement Learning","","1","","14","IEEE","5 Oct 2021","","","IEEE","IEEE Conferences"
"Efficient Policy Learning for General Robotic Tasks with Adaptive Dual-memory Hindsight Experience Replay Based on Deep Reinforcement Learning","M. Dong; F. Ying; X. Li; H. Liu","College of Information Science and Technology, Donghua University, China; College of Information Science and Technology, Donghua University, China; College of Information Science and Technology, Donghua University, China; College of Information Science and Technology, Donghua University, China","2023 7th International Conference on Robotics, Control and Automation (ICRCA)","5 Apr 2023","2023","","","62","66","Deep reinforcement learning (DRL) features powerful ability of perception and decision-making, which is reward-driven and learns strategies through the interaction between the agent and the environment. However, the discrete reward mechanism makes it difficult for DRL to obtain positive feedback in the early stage of interaction, resulting in low learning efficiency. The hindsight experience replay (HER) mechanism can improve the deficiency of the discrete reward, but it also causes a lot of data redundancy. This paper proposes an adaptive dual-memory hindsight experience replay structure. The success rate of the algorithm can be improved while the training efficiency can be ensured by using the dual-memory bank structure to split the empirical data and adjusting the proportion of HER mechanism. The proposed method is applied to the DRL algorithm and verified on a 7-DoF robot, and experimental results show that the algorithm has good performance.","","979-8-3503-4578-0","10.1109/ICRCA57894.2023.10087824","Natural Science Foundation of Shanghai(grant numbers:21ZR1401100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10087824","deep reinforcement learning;robot;hindsight experience replay (HER);policy learning","Training;Deep learning;Automation;Redundancy;Decision making;Reinforcement learning;Manipulators","decision making;deep learning (artificial intelligence);learning (artificial intelligence);reinforcement learning","adaptive dual-memory hindsight experience replay structure;decision-making;deep reinforcement learning;discrete reward mechanism;DRL algorithm;dual-memory bank structure;efficient policy learning;general robotic tasks;hindsight experience replay mechanism;low learning efficiency;reward-driven","","","","16","IEEE","5 Apr 2023","","","IEEE","IEEE Conferences"
"Infrared Image Captioning Based on Unsupervised Learning and Reinforcement Learning","C. Gao; G. Bian; Y. Dong; X. Yuan; H. Liu","School of Physics and Electronic Information, Yantai University, Yantai, China; School of Physics and Electronic Information, Yantai University, Yantai, China; School of Physics and Electronic Information, Yantai University, Yantai, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","2022 International Conference on Automation, Robotics and Computer Engineering (ICARCE)","22 Feb 2023","2022","","","1","4","When sufficient prior knowledge is lacking or manual annotation is difficult, solving the problem directly based on training samples of unknown category can greatly reduce the time cost. Therefore, we add unsupervised learning to the preliminary groundwork of image captioning for efficient image domain conversion to achieve batch generation of the required images. At the same time, more and more infrared images are being applied to assist decision making and environment perception. Generating more diverse and discriminative image captions in similar scenes will be effective in enhancing decision making and perception capabilities. Our infrared image caption model trained with reinforcement learning has satisfactory results both in terms of quantitative scores and in real scene tests.","","978-1-6654-7548-8","10.1109/ICARCE55724.2022.10046598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046598","unsupervised learning;reinforcement learning;infrared image processing;image captioning","Training;Costs;Automation;Annotations;Decision making;Redundancy;Reinforcement learning","decision making;infrared imaging;reinforcement learning;unsupervised learning","batch generation;decision making;discriminative image captions;diverse image captions;efficient image domain conversion;image captioning;infrared image caption model;infrared images;manual annotation;reinforcement learning;required images;unknown category;unsupervised learning","","","","13","IEEE","22 Feb 2023","","","IEEE","IEEE Conferences"
"Deep reinforcement learning for high precision assembly tasks","T. Inoue; G. De Magistris; A. Munawar; T. Yokoya; R. Tachibana","IBM Research - Tokyo, IBM Japan, Japan; IBM Research - Tokyo, IBM Japan, Japan; IBM Research - Tokyo, IBM Japan, Japan; Tsukuba Research Laboratory, Yaskawa electric corporation, Japan; IBM Research - Tokyo, IBM Japan, Japan","2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","14 Dec 2017","2017","","","819","825","The high precision assembly of mechanical parts requires precision that exceeds that of robots. Conventional part-mating methods used in the current manufacturing require numerous parameters to be tediously tuned before deployment. We show how a robot can successfully perform a peg-in-hole task with a tight clearance through training a recurrent neural network with reinforcement learning. In addition to reducing manual effort, the proposed method also shows a better fitting performance with a tighter clearance and robustness against positional and angular errors for the peg-in-hole task. The neural network learns to take the optimal action by observing the sensors of a robot to estimate the system state. The advantages of our proposed method are validated experimentally on a 7-axis articulated robot arm.","2153-0866","978-1-5386-2682-5","10.1109/IROS.2017.8202244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8202244","","Robot sensing systems;Service robots;Neural networks;Programming","industrial manipulators;learning systems;neurocontrollers;observers;optimal control;position control;recurrent neural nets;robotic assembly;robust control","manufacturing;part-mating methods;neural network learning;optimal action;system state estimation;7-axis articulated robot arm;angular errors;positional errors;robustness;fitting performance;recurrent neural network;tight clearance;peg-in-hole task;robots;mechanical parts;high precision assembly tasks;deep reinforcement learning","","176","1","21","IEEE","14 Dec 2017","","","IEEE","IEEE Conferences"
"Application of Reinforcement Learning to a Mining System","A. X. Fidêncio; T. Glasmachers; D. Naro","Technical University of Dortmund, Dortmund, Germany; Ruhr University Bochum, Bochum, Germany; thyssenkrupp Industrial Solutions, Essen, Germany","2021 IEEE 19th World Symposium on Applied Machine Intelligence and Informatics (SAMI)","22 Mar 2021","2021","","","000111","000118","Automation techniques have been widely applied in different industry segments, among others, to increase both productivity and safety. In the mining industry, with the usage of such systems, the operator can be removed from hazardous environments without compromising task execution and it is possible to achieve more efficient and standardized operation. In this work a study case on the application of machine learning algorithms to a mining system example is presented, in which reinforcement learning algorithms were used to solve a control problem. As an example, a machine chain consisting of a Bucket Wheel Excavator, a Belt Wagon and a Hopper Car was used. This system has two material transfer points that need to remain aligned during operation in order to allow continuous material flow. To keep the alignment, the controller makes use of seven degrees of freedom given by slewing, luffing and crawler drives. Experimental tests were done in a simulated environment with two state-of-the-art algorithms, namely Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC). The trained agents were evaluated in terms of episode return and length, as well as alignment quality and action values used. Results show that, for the given task, the PPO agent performs quantitatively and qualitatively better than the SAC agent. However, none of the agents were able to completely solve the proposed testing task.","","978-1-7281-8053-3","10.1109/SAMI50585.2021.9378663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9378663","Machine Learning;Reinforcement Learning;Control Applications;Mining Industry;Industrial Application","Productivity;Machine learning algorithms;Wheels;Reinforcement learning;Safety;Task analysis;Testing","belts;excavators;learning (artificial intelligence);mechanical testing;mining equipment;mining industry;production engineering computing;productivity;safety;wheels","automation techniques;productivity;safety;mining industry;hazardous environments;reinforcement learning algorithms;bucket wheel excavator;hopper car;material transfer points;crawler drives;proximal policy optimization;soft actor-critic;PPO agent;belt wagon;machine learning algorithms;experimental tests;SAC","","3","","27","IEEE","22 Mar 2021","","","IEEE","IEEE Conferences"
"Industrial Load Management using Multi-Agent Reinforcement Learning for Rescheduling","M. Roesch; C. Linder; C. Bruckdorfer; A. Hohmann; G. Reinhart","Fraunhofer Research Institution for Casting, Composite and Processing Technology, Augsburg, Germany; Fraunhofer Research Institution for Casting, Composite and Processing Technology, Augsburg, Germany; Fraunhofer Research Institution for Casting, Composite and Processing Technology, Augsburg, Germany; Fraunhofer Research Institution for Casting, Composite and Processing Technology, Augsburg, Germany; Fraunhofer Research Institution for Casting, Composite and Processing Technology, Augsburg, Germany","2019 Second International Conference on Artificial Intelligence for Industries (AI4I)","9 Mar 2020","2019","","","99","102","Industrial load management plays an important role in the balance of energy consumption and electricity generation, which is increasingly fluctuating due to the growing share of renewable energies. Manufacturing companies are able to adapt their energy consumption by considering energy aspects in their production schedule. It may even be beneficial to temporarily force production resources into idle states and to thus reduce energy demand for a limited period. However, the resulting scheduling problem is very complex and at the same time should be executed in real-time. This paper presents an approach for industrial load management using multi-agent reinforcement learning for energy-oriented rescheduling. A simulation study serves to validate the approach. The results show good solutions and at the same time low computational expense compared to a metaheuristic approach using simulated annealing.","","978-1-7281-4087-2","10.1109/AI4I46381.2019.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9027801","Industrial load management, rescheduling, multi-agent reinforcement learning","Load management;Schedules;Energy consumption;Learning (artificial intelligence);Job shop scheduling;Real-time systems","energy consumption;learning (artificial intelligence);load management;multi-agent systems;scheduling;simulated annealing","industrial load management;multiagent reinforcement learning;energy consumption;renewable energies;energy aspects;production schedule;energy demand;energy-oriented rescheduling;simulated annealing","","3","","17","IEEE","9 Mar 2020","","","IEEE","IEEE Conferences"
"Optimal Design of Flexible Job Shop Scheduling Under Resource Preemption Based on Deep Reinforcement Learning","Z. Chen; L. Zhang; X. Wang; P. Gu","School of Automation Science and Eletrical Engineering, Beihang University, Beijing, China; School of Automation Science and Eletrical Engineering, Beihang University, Beijing, China; School of Automation Science and Eletrical Engineering, Beihang University, Beijing, China; School of Automation Science and Eletrical Engineering, Beihang University, Beijing, China","Complex System Modeling and Simulation","26 Jul 2022","2022","2","2","174","185","With the popularization of multi-variety and small-batch production patterns, the flexible job shop scheduling problem (FJSSP) has been widely studied. The sharing of processing resources by multiple machines frequently occurs due to space constraints in a flexible shop, which results in resource preemption for processing workpieces. Resource preemption complicates the constraints of scheduling problems that are otherwise difficult to solve. In this paper, the flexible job shop scheduling problem under the process resource preemption scenario is modeled, and a two-layer rule scheduling algorithm based on deep reinforcement learning is proposed to achieve the goal of minimum scheduling time. The simulation experiments compare our scheduling algorithm with two traditional metaheuristic optimization algorithms among different processing resource distribution scenarios in static scheduling environment. The results suggest that the two-layer rule scheduling algorithm based on deep reinforcement learning is more effective than the meta-heuristic algorithm in the application of processing resource preemption scenarios. Ablation experiments, generalization, and dynamic experiments are performed to demonstrate the excellent performance of our method for FJSSP under resource preemption.","2096-9929","","10.23919/CSMS.2022.0007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9841531","flexible job shop scheduling;resource preemption;deep reinforcement learning;two-level scheduling","Job shop scheduling;Scheduling algorithms;Heuristic algorithms;Metaheuristics;Reinforcement learning;Production;Dynamic scheduling","deep learning (artificial intelligence);job shop scheduling;metaheuristics;production engineering computing","process resource preemption scenario;deep reinforcement learning;scheduling time;scheduling algorithm;traditional metaheuristic optimization algorithms;different processing resource distribution scenarios;static scheduling environment;processing resource preemption scenarios;small-batch production patterns;flexible job shop scheduling problem;flexible shop;scheduling problems;optimal design;FJSSP;metaheuristic algorithm","","1","","28","","26 Jul 2022","","","TUP","TUP Journals"
"Demand Side Management and Peer-to-Peer Energy Trading for Industrial Users Using Two-Level Multi-Agent Reinforcement Learning","J. Wang; D. K. Mishra; L. Li; J. Zhang","School of Electrical and Data Engineering, University of Technology Sydney, Broadway, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Broadway, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Broadway, NSW, Australia; Department of Automotive Engineering, Clemson University, Greenville, SC, USA","IEEE Transactions on Energy Markets, Policy and Regulation","14 Mar 2023","2023","1","1","23","36","Over the past few years, with more concerns on energy equity, energy security and environmental sustainability, peer-to-peer (P2P) energy trading market has increasingly attracted attention to energy users. With the rising electricity price and the growing public doubt to unsustainable development, more efforts are given to the integration between P2P energy trading and industrial practice. To date, the current research work has emphasized electricity cost reduction using demand-side management (DSM) and P2P energy trading market to promote energy sharing and create a win–win situation for all industrial market participants. However, less attention has been given to evaluating industrial production. With this attention, this paper formulates a cost minimization problem for multiple discrete manufacturers, where DSM and P2P energy trading market are integrated to optimize manufacturing productivity and energy consumption. Moreover, this paper proposes a two-level reinforcement learning (RL) method to obtain the optimal DSM and energy trading strategies for discrete manufacturers. Unlike the existing RL approaches, the proposed method depicts a two-level learning approach, where the upper-level program suggests a production plan for the manufacturer to improve the cost return, while the lower-level program optimizes the net benefits. Further, to verify the performance of the proposed method, simulations are conducted based on four different case studies, including different weather and load conditions. Numerical results demonstrate that the proposed algorithm provides a better production plan with enhanced learning speed and economic benefits compared to the existing RL approaches.","2771-9626","","10.1109/TEMPR.2023.3239989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10026665","Bilateral energy trading;demand side manage- ment;discrete manufacturing;hybrid reinforcement learning","Costs;Production;Peer-to-peer computing;Optimization;Decision making;Schedules;Reinforcement learning","cost reduction;demand side management;energy consumption;learning (artificial intelligence);multi-agent systems;peer-to-peer computing;power engineering computing;power generation economics;power markets;pricing;production planning;reinforcement learning","electricity cost reduction;energy consumption;energy equity;energy security;energy sharing;energy trading strategies;energy users;industrial market participants;industrial practice;industrial production;industrial users;manufacturing productivity;P2P energy trading market;peer-to-peer energy trading market;two-level learning approach;two-level multiagent reinforcement;two-level reinforcement learning method","","2","","32","IEEE","26 Jan 2023","","","IEEE","IEEE Journals"
"Parameter design of grid-tied inverter using reinforcement learning","Z. Wang; W. Wang","Beijing Sifang Automation Co., Ltd, Beijing, China; Beijing Sifang Automation Co., Ltd, Beijing, China","2021 Annual Meeting of CSEE Study Committee of HVDC and Power Electronics (HVDC 2021)","25 Mar 2022","2021","2021","","116","120","Grid-tied inverters have been widely adopted in distributed generation systems. The output current distortion and oscillation occur under certain conditions especially in the weak grid and multi-parallel inverters. To maintain the stability and ensure the dynamic and steady performance of current tracking, the controller parameters required iterative design and evaluation. Taking single phase inverter as an example, traditional design approaches are discussed first. Then, considering the difficulties in the parameter design process, deep reinforcement learning (DRL) is introduced to explore the optimal combination of controller parameters, by which the parameters of controllers can be automatically tuned. The realization of the DRL approach is discussed in detail and simulation validates the correctness of the attempt.","","978-1-83953-636-6","10.1049/icp.2021.2547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9741508","","","distributed power generation;invertors;learning (artificial intelligence);power grids","controller parameters;single phase inverter;traditional design approaches;parameter design process;deep reinforcement learning;grid-tied inverter;distributed generation systems;output current distortion;oscillation;weak grid;multiparallel inverters;dynamic performance;steady performance;current tracking","","","","","","25 Mar 2022","","","IET","IET Conferences"
"Reinforcement Learning on Variable Impedance Controller for High-Precision Robotic Assembly","J. Luo; E. Solowjow; C. Wen; J. A. Ojea; A. M. Agogino; A. Tamar; P. Abbeel","University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA; Technion, Haifa, Israel; University of California, Berkeley, CA, USA","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","3080","3087","Precise robotic manipulation skills are desirable in many industrial settings, reinforcement learning (RL) methods hold the promise of acquiring these skills autonomously. In this paper, we explicitly consider incorporating operational space force/torque information into reinforcement learning; this is motivated by humans heuristically mapping perceived forces to control actions, which results in completing high-precision tasks in a fairly easy manner. Our approach combines RL with force/torque information by incorporating a proper operational space force controller; where we also exploit different ablations on processing this information. Moreover, we propose a neural network architecture that generalizes to reasonable variations of the environment. We evaluate our method on the open-source Siemens Robot Learning Challenge, which requires precise and delicate force-controlled behavior to assemble a tight-fit gear wheel set.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793506","","Task analysis;Gears;Aerospace electronics;Robots;Reinforcement learning;Trajectory;Neural networks","assembling;control engineering computing;force control;industrial robots;learning (artificial intelligence);neural net architecture;position control;robotic assembly;wheels","high-precision robotic assembly;precise robotic manipulation skills;industrial settings;reinforcement learning methods;RL;perceived forces;high-precision tasks;proper operational space force controller;open-source Siemens Robot Learning Challenge;precise force-controlled behavior;delicate force-controlled behavior;variable impedance controller","","81","","38","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Path planning of mobile robot based on deep reinforcement learning with transfer learning strategy","J. Zhu; C. Yang; Z. Liu; C. Yang","School of Automation and Electrical Engineering, Linyi university, Linyi, China; School of Information Science and Engineering, Linyi university, Linyi, China; School of Automation and Electrical Engineering, Linyi university, Linyi, China; School of Information Science and Engineering, Linyi university, Linyi, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","1242","1246","Under complex environments, mobile robots can decision-making, autonomous learning, intelligent obstacle avoidance, and complete the task from start point to endpoint. This paper designed the mobile robot, excluding planners and unknown maps, which can successfully reach the target by autonomously learning and navigating in the unknown environment. By applying deep reinforcement learning to the path planning of mobile robots, the robot can collect data and conduct training on its own, and improve it autonomously without manual supervision. Consequently, it can complete the path planning task. The application of transfer learning improves the adaptive efficiency of the mobile robot to the environment. Finally, the results are verified by comparative experiments in three simulation environments.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023708","deep reinforcement learning;mobile robots;obstacle avoidance","Training;Deep learning;Navigation;Transfer learning;Supervised learning;Reinforcement learning;Path planning","collision avoidance;learning (artificial intelligence);mobile robots;path planning","autonomous learning;deep reinforcement learning;mobile robot;path planning task;transfer learning strategy","","","","21","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Time-Aware Shaping (IEEE 802.1Qbv) in Time-Sensitive Networks","A. Roberty; S. B. H. Said; F. Ridouard; H. Bauer; A. Geniet","CEA, List, University Paris-Saclay, Palaiseau, France; CEA, List, University Paris-Saclay, Palaiseau, France; ISAE-ENSMA - LIAS, Futuroscope Cedex, France; ISAE-ENSMA - LIAS, Futuroscope Cedex, France; ISAE-ENSMA - LIAS, University of Poitiers, Poitiers, France","2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)","12 Oct 2023","2023","","","1","4","Industry 4.0 involves the networking of production equipment. This can be achieved thanks to the Time-Sensitive Networking (TSN) set of network standards. However, this new paradigm brings new challenges because TSN features optimization relies on the dynamic characteristics of the underlying communication network (e.g., network topology, routing strategy, critical flows requirements, etc.). This paper focuses on the case of the IEEE 802.1Qbv standard by exploring the applicability of a Deep Reinforcement Learning (DRL) approach in order to reduce the configuration time of the TSN-specific parameters, compared to exact or heuristic methods.","1946-0759","979-8-3503-3991-8","10.1109/ETFA54631.2023.10275566","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275566","","Deep learning;Network topology;Production equipment;Reinforcement learning;Routing;Fourth Industrial Revolution;Communication networks","","","","","","19","IEEE","12 Oct 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning Algorithms in Global Path Planning for Mobile Robot","V. N. Sichkar","Department of Control Systems and Robotics, ITMO University, Saint-Petersburg, Russia","2019 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)","24 Jun 2019","2019","","","1","5","The paper is devoted to the research of two approaches for global path planning for mobile robots, based on Q-Learning and Sarsa algorithms. The study has been done with different adjustments of two algorithms that made it possible to learn faster. The implementation of two Reinforcement Learning algorithms showed differences in learning time and the methods of building path to avoid obstacles and to reach a destination point. The analysis of obtained results made it possible to select optimal parameters of the considered algorithms for the tested environments. Experiments were performed in virtual environments where algorithms learned which steps to choose in order to get a maximum payoff and reach the goal avoiding obstacles.","","978-1-5386-8119-0","10.1109/ICIEAM.2019.8742915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8742915","reinforcement learning;Q-Learning algorithm;Sarsa algorithm;path planning;mobile agent","Mobile agents;Virtual environments;Reinforcement learning;Training;Industrial engineering;Manufacturing;Task analysis","collision avoidance;learning (artificial intelligence);mobile robots;robot programming","global path planning;mobile robot;Sarsa algorithms;reinforcement learning algorithms;Q-Learning;obstacle avoidance","","16","","33","IEEE","24 Jun 2019","","","IEEE","IEEE Conferences"
"Cooperative behavior acquisition of multiple autonomous mobile robots by an objective-based reinforcement learning system","Kunikazu Kobayashi; Koji Nakano; Takashi Kuremoto; Masanao Obayashi","Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Ube, Japan; Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Ube, Japan; Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Japan; Division of Computer Science & Design Engineering, Yamaguchi University, Ube, Ube, Japan","2007 International Conference on Control, Automation and Systems","26 Dec 2007","2007","","","777","780","The present paper proposes an objective-based reinforcement learning system for multiple autonomous mobile robots to acquire cooperative behavior. The proposed system employs profit sharing (PS) as a learning method. A major characteristic of the system is using two kinds of PS tables. One is to learn cooperative behavior using information on other agents' positions and the other is to learn how to control basic movements. Through computer simulation and real robot experiment using a garbage-collection problem, the performance of the proposed system is evaluated. As a result, it is verified that agents select the most available garbage for cooperative behavior using visual information in an unknown environment and move to the target avoiding obstacles.","","978-89-950038-6-2","10.1109/ICCAS.2007.4407004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407004","objective-based reinforcement learning;multiagent system;cooperative behavior;autonomous mobile robot;profit sharing","Mobile robots;Learning systems;Control systems;Computer simulation;Automatic control;Design automation;Robotics and automation;Computer science;Design engineering;Multiagent systems","collision avoidance;cooperative systems;learning (artificial intelligence);mobile robots;multi-robot systems","cooperative behavior acquisition;multiple autonomous mobile robots;objective-based reinforcement learning system;profit sharing;garbage-collection problem","","2","","11","","26 Dec 2007","","","IEEE","IEEE Conferences"
"RL-Sizer: VLSI Gate Sizing for Timing Optimization using Deep Reinforcement Learning","Y. -C. Lu; S. Nath; V. Khandelwal; S. K. Lim","School of ECE, Georgia Institute of Technology, Atlanta, GA; Synopsys Inc., Mountain View, CA; Synopsys Inc., Hillsboro, OR; School of ECE, Georgia Institute of Technology, Atlanta, GA","2021 58th ACM/IEEE Design Automation Conference (DAC)","8 Nov 2021","2021","","","733","738","Gate sizing for timing optimization is performed extensively throughout electronic design automation (EDA) flows. However, increasing design sizes and time-to-market pressure force EDA tools to maintain pseudo-linear complexity, thereby limiting the global exploration done by the underlying sizing algorithms. Furthermore, high-performance low-power designs are pushing the envelope on power, performance and area (PPA), creating a need for last mile PPA closure using more powerful algorithms. Reinforcement learning (RL) is a disruptive paradigm that achieves high-quality optimization results beyond traditional algorithms. In this paper, we formulate gate sizing as an RL process, and propose RL-Sizer, an autonomous gate sizing agent, which performs timing optimization in an unsupervised manner. In the experiments, we demonstrate that RL-Sizer can improve the native sizing algorithms of an industry-leading EDA tool, Synopsys IC-Compiler II (ICC2), on 6 commercial designs in advanced process nodes (5 – 16nm). RL-Sizer delivers significantly better total negative slack (TNS) and number of violating endpoints (NVEs) on 4 designs with negligible power overhead, while achieving parity on athe others.","0738-100X","978-1-6654-3274-0","10.1109/DAC18074.2021.9586138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586138","","Design automation;Limiting;Force;Reinforcement learning;Logic gates;Tools;Very large scale integration","deep learning (artificial intelligence);electronic design automation;optimisation;reinforcement learning;VLSI","negligible power overhead;VLSI gate sizing;timing optimization;deep reinforcement learning;electronic design automation;time-to-market pressure force EDA tools;pseudolinear complexity;sizing algorithms;high-performance low-power designs;high-quality optimization;RL process;autonomous gate sizing agent;native sizing algorithms;industry-leading EDA tool;RL-sizer;PPA closure;total negative slack;size 5.0 nm to 16.0 nm","","13","","14","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Learning Coordination in Multi-Agent Systems Using Influence Value Reinforcement Learning","D. Barrios-Aranibar; L. M. G. Goncalves","Department of Computing Engineering and Automation, Federal University of Rio Grande do Norte, Natal, Brazil; Department of Computing Engineering and Automation, Federal University of Rio Grande do Norte, Natal, Brazil","Seventh International Conference on Intelligent Systems Design and Applications (ISDA 2007)","27 Nov 2007","2007","","","471","478","In this paper authors propose a new paradigm for learning coordination in multi-agent systems. This approach is based on social interaction of people, specially in the fact that people communicate each other what they think about their actions and this opinion can influence the behavior of each other. It is proposed a model in which agents, into a multi-agent system, learns to coordinate actions giving opinions about actions of other agents and also being influenced with opinions of other agents about their actions. This paradigm was used to develop a modified version of the Q-learning algorithm. This algorithm was tested and compared with independent learning (IL) and joint action learning (JAL) in two single state problems with two agents. This approach shows to have more probability to converge to an optimal equilibrium than IL and JAL Q-learning algorithms. Also, it does not need to make an entire model of all joint actions like JAL algorithms.","2164-7151","978-0-7695-2976-9","10.1109/ISDA.2007.136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389653","","Multiagent systems;Learning;Nash equilibrium;Collaborative work;Intelligent systems;Design engineering;Design automation;Testing;Robot kinematics;Artificial intelligence","learning (artificial intelligence);multi-agent systems","learning coordination;multi-agent systems;influence value reinforcement learning;people social interaction;Q-learning algorithm;independent learning;joint action learning","","4","","10","IEEE","27 Nov 2007","","","IEEE","IEEE Conferences"
"Reinforcement learning-based feature learning for object tracking","Fang Liu; Jianbo Su","Research Center of Intelligent Robotics & Department of Automation, ShangHai JiaoTong University, Shanghai, China; Research Center of Intelligent Robotics & Department of Automation, ShangHai JiaoTong University, Shanghai, China","Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.","20 Sep 2004","2004","2","","748","751 Vol.2","Feature learning in object tracking is important because the choice of the features significantly affects system's performance. A novel online feature learning approach based on reinforcement learning is proposed. Reinforcement learning has been extensively used as a generative model of sequential decision-making that interacts with uncertain environment. We extend this technique to feature selection for object tracking, and further add human-computer interaction to reinforcement learning to reduce the learning complexity and speed the convergence rate. Experiments of the object tracking are provided to verify the effectiveness of the proposed approach.","1051-4651","0-7695-2128-2","10.1109/ICPR.2004.1334367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1334367","","Target tracking;Intelligent robots;Robotics and automation;Convergence;Infrared detectors;Machine learning;Human computer interaction;Decision making;Computer vision;Cameras","learning (artificial intelligence);tracking;decision making;feature extraction;human computer interaction","object tracking;reinforcement learning;online feature learning approach;sequential decision-making;feature selection;human-computer interaction","","2","1","10","IEEE","20 Sep 2004","","","IEEE","IEEE Conferences"
"ART-R: a novel reinforcement learning algorithm using an ART module for state representation","L. Brignone; M. Howarth","IFREMER, France; Sheffield Hallam University, Sheffield, UK","2003 IEEE XIII Workshop on Neural Networks for Signal Processing (IEEE Cat. No.03TH8718)","2 Aug 2004","2003","","","829","838","The work introduces a neural network (NN) algorithm capable of merging the fast and stable learning behaviour offered by the adaptive resonance theory (ART) and the advantageous properties of a reinforcement learning agent. The result is ART-R a neural algorithm particularly suited to learning state-action mappings in control applications. A real time example addressing a typical problem found in autonomous robotic assembly is discussed to highlight the achievement of unsupervised and fast learning of an optimal behaviour.","1089-3555","0-7803-8177-7","10.1109/NNSP.2003.1318082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318082","","Learning;Subspace constraints;Backpropagation;Biological neural networks;Resonance;Pattern recognition;Space exploration;Neural networks;Computer architecture;Merging","ART neural nets;learning (artificial intelligence);robotic assembly","ART-R;adaptive resonance theory;reinforcement learning algorithm;state representation;neural network algorithm;state-action mapping learning;autonomous robotic assembly","","1","","16","IEEE","2 Aug 2004","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Smart Grid Operations: Algorithms, Applications, and Prospects","Y. Li; C. Yu; M. Shahidehpour; T. Yang; Z. Zeng; T. Chai","School of Artificial Intelligence and Automation, Autonomous Intelligent Unmanned System Engineering Research Center, Key Laboratory of Image Processing and Intelligence Control, Ministry of Education of China, and the Hubei Key Laboratory of Brain-Inspired Intelligent Systems and the Belt and Road Joint Laboratory on Measurement and Control Technology, Huazhong University of Science and Technology, Wuhan, China; China-EU Institute for Clean and Renewable Energy, Huazhong University of Science and Technology, Wuhan, China; Robert W. Galvin Center for Electricity Innovation, Illinois Institute of Technology, Chicago, IL, USA; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; School of Artificial Intelligence and Automation, Autonomous Intelligent Unmanned System Engineering Research Center, Key Laboratory of Image Processing and Intelligence Control, Ministry of Education of China, and the Hubei Key Laboratory of Brain-Inspired Intelligent Systems and the Belt and Road Joint Laboratory on Measurement and Control Technology, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China","Proceedings of the IEEE","14 Sep 2023","2023","111","9","1055","1096","With the increasing penetration of renewable energy and flexible loads in smart grids, a more complicated power system with high uncertainty is gradually formed, which brings about great challenges to smart grid operations. Traditional optimization methods usually require accurate mathematical models and parameters and cannot deal well with the growing complexity and uncertainty. Fortunately, the widespread popularity of advanced meters makes it possible for smart grid to collect massive data, which offers opportunities for data-driven artificial intelligence methods to address the optimal operation and control issues. Therein, deep reinforcement learning (DRL) has attracted extensive attention for its excellent performance in operation problems with high uncertainty. To this end, this article presents a comprehensive literature survey on DRL and its applications in smart grid operations. First, a detailed overview of DRL, from fundamental concepts to advanced models, is conducted in this article. Afterward, we review various DRL techniques as well as their extensions developed to cope with emerging issues in the smart grid, including optimal dispatch, operational control, electricity market, and other emerging areas. In addition, an application-oriented survey of DRL in smart grid is presented to identify difficulties for future research. Finally, essential challenges, potential solutions, and future research directions concerning the DRL applications in smart grid are also discussed.","1558-2256","","10.1109/JPROC.2023.3303358","National Key Research and Development Program of China(grant numbers:2021ZD0201300); National Natural Science Foundation of China(grant numbers:62073148); Key Project of National Natural Science Foundation of China(grant numbers:62233006); Smart Grid Joint Key Project of National Natural Science Foundation of China and the State Grid Corporation of China(grant numbers:U2066202); Major Program of National Natural Science Foundation of China(grant numbers:61991400); 2020 Science and Technology Major Project of Liaoning Province(grant numbers:2020JH1/10100008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10241311","Deep reinforcement learning (DRL);electricity market;operational control;optimal dispatch;smart grid (SG)","Smart grids;Reinforcement learning;Artificial intelligence;Surveys;Power system stability;Deep learning;Uncertainty;Renewable energy sources;Flexible electronics;Computer applications;Power markets;Electricity supply industry deregulation;Power system control","control engineering computing;deep learning (artificial intelligence);optimisation;power engineering computing;power generation control;power generation dispatch;power markets;reinforcement learning;smart power grids","application-oriented survey;data-driven artificial intelligence methods;deep reinforcement learning;DRL;electricity market;flexible loads;operational control;optimal dispatch;optimal operation;power system;renewable energy;smart grid operations","","","","268","IEEE","5 Sep 2023","","","IEEE","IEEE Journals"
"A Cooperative Control Method Based on Reinforcement Learning","Z. Zhuang; Y. Xu; L. Zou; W. Zhang; Q. Zhao; R. Yao","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2018 Chinese Automation Congress (CAC)","24 Jan 2019","2018","","","1586","1591","With the increase of system dimensions, the difficulty of decoupling increases exponentially and the controller becomes more difficult to design. A reinforcement learning framework combined with tradition control method is presented to get rid of decoupling trouble and simplified controller design. A Markov decision process of basis weight and moisture control is explicitly defined. A first-order Pade approximation is applied to solve the time delay problem in reinforcement learning. With the proposed design, the control scheme could obtain a good performance of the control system as same as the PID & feedforward decoupling control through cooperative control. Numerical simulations are given to demonstrate the effectiveness of the proposed method.","","978-1-7281-1312-8","10.1109/CAC.2018.8623799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8623799","Reinforcement Learning;Deep Learning;Basis Weight and moisture control;Forward Decoupling;Pade Approximation;PID;time delay","Moisture;Transfer functions;Reinforcement learning;Automation;Couplings;Difference equations;Moisture control","approximation theory;control system synthesis;delays;feedforward;learning (artificial intelligence);Markov processes;moisture control;three-term control","cooperative control method;reinforcement learning;Markov decision process;moisture control;first-order Pade approximation;time delay problem;control system;controller design;PID;feedforward decoupling control","","1","","17","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Comparison of Model-Based and Model-Free Reinforcement Learning for Real-World Dexterous Robotic Manipulation Tasks","D. Valencia; J. Jia; R. Li; A. Hayashi; M. Lecchi; R. Terezakis; T. Gee; M. Liarokapis; B. A. MacDonald; H. Williams","Centre for Automation and Robotic Engineering Science, The University of Auckland, New Zealand; Centre for Automation and Robotic Engineering Science, The University of Auckland, New Zealand; Centre for Automation and Robotic Engineering Science, The University of Auckland, New Zealand; New Dexterity Research Group, The University of Auckland, New Zealand; New Dexterity Research Group, The University of Auckland, New Zealand; New Dexterity Research Group, The University of Auckland, New Zealand; Centre for Automation and Robotic Engineering Science, The University of Auckland, New Zealand; New Dexterity Research Group, The University of Auckland, New Zealand; Centre for Automation and Robotic Engineering Science, The University of Auckland, New Zealand; Centre for Automation and Robotic Engineering Science, The University of Auckland, New Zealand","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","871","878","Model Free Reinforcement Learning (MFRL) has shown significant promise for learning dexterous robotic manipulation tasks, at least in simulation. However, the high number of samples, as well as the long training times, prevent MFRL from scaling to complex real-world tasks. Model- Based Reinforcement Learning (MBRL) emerges as a potential solution that, in theory, can improve the data efficiency of MFRL approaches. This could drastically reduce the training time of MFRL, and increase the application of RL for real- world robotic tasks. This article presents a study on the feasibility of using the state-of-the-art MBRL to improve the training time for two real-world dexterous manipulation tasks. The evaluation is conducted on a real low-cost robot gripper where the predictive model and the control policy are learned from scratch. The results indicate that MBRL is capable of learning accurate models of the world, but does not show clear improvements in learning the control policy in the real world as prior literature suggests should be expected.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160983","","Training;Automation;Reinforcement learning;Predictive models;Data models;Proposals;Task analysis","dexterous manipulators;grippers;reinforcement learning","low-cost robot gripper;MBRL;MFRL approaches;model free reinforcement learning;model- based reinforcement learning;real-world dexterous robotic manipulation tasks","","","","53","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Online Dispatching Policy in Real-Time Train Timetable Rescheduling","P. Yue; Y. Jin; X. Dai; Z. Feng; D. Cui","State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; School of Computer Science and Electronic Engineering, University of Surrey, Guildford, U.K.; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China","IEEE Transactions on Intelligent Transportation Systems","","2023","PP","99","1","13","Train Timetable Rescheduling (TTR) is a crucial task in the daily operation of high-speed railways to maintain punctuality and efficiency in the presence of unexpected disturbances. However, it is challenging to promptly create a rescheduled timetable in real time. In this study, we propose a reinforcement-learning-based method for real-time rescheduling of high-speed trains. The key innovation of the proposed method is to learn a well-generalized dispatching policy from a large amount of samples, which can be applied to the TTR task directly. At first, the problem is transformed into a multi-stage decision process, and the decision agent is designed to predict dispatching rules. To enhance the training efficiency, we generate a small yet good-quality action set to reduce invalid explorations. Besides, we propose an action sampling strategy for action selection, which implements forward planning with consideration of evaluation uncertainty, thus improving search efficiency. Extensive experimental results demonstrate the effectiveness and competitiveness of the proposed method. It has been proven that the local policies trained by the proposed method can be applied to numerous problem instances directly, rendering it unnecessary to use human-designed rules.","1558-0016","","10.1109/TITS.2023.3305074","National Natural Science Foundation of China(grant numbers:61790574,61773111,U1834211); Science and Technology Project of China National Railway Group Corporation Ltd.(grant numbers:N2019G020); Alexander von Humboldt Professorship for Artificial Intelligence Endowed by the Germany Federal Ministry of Education and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10226458","Reinforcement learning;train timetable rescheduling;high-speed trains;real-time rescheduling","Delays;Real-time systems;Dispatching;Rail transportation;Metaheuristics;Learning systems;Task analysis","","","","","","","IEEE","22 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Arena-Rosnav: Towards Deployment of Deep-Reinforcement-Learning-Based Obstacle Avoidance into Conventional Autonomous Navigation Systems","L. Kästner; T. Buiyan; L. Jiao; T. A. Le; X. Zhao; Z. Shen; J. Lambrecht","Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany","2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","16 Dec 2021","2021","","","6456","6463","Recently, mobile robots have become important tools in various industries, especially in logistics. Deep reinforcement learning emerged as an alternative planning method to replace overly conservative approaches and promises more efficient and flexible navigation. However, deep reinforcement learning approaches are not suitable for long-range navigation due to their proneness to local minima and lack of long term memory, which hinders its widespread integration into industrial applications of mobile robotics. In this paper, we propose a navigation system incorporating deep-reinforcement-learning- based local planners into conventional navigation stacks for long-range navigation. Therefore, a framework for training and testing the deep reinforcement learning algorithms along with classic approaches is presented. We evaluated our deep-reinforcement-learning-enhanced navigation system against various conventional planners and found that our system outperforms them in terms of safety, efficiency and robustness.","2153-0866","978-1-6654-1714-3","10.1109/IROS51168.2021.9636226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636226","","Training;Navigation;Service robots;Semantics;Reinforcement learning;Robustness;Safety","collision avoidance;control engineering computing;deep learning (artificial intelligence);industrial robots;logistics;mobile robots;navigation;production engineering computing;reinforcement learning","mobile robots;alternative planning method;long-range navigation;industrial mobile robotics;deep-reinforcement-learning-based obstacle avoidance;autonomous navigation systems;Arena-Rosnav;logistics;deep reinforcement learning- based local planners","","15","","22","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning-based Box Unloading Sequence Planning for Robotic Container-Unloading System","H. Park; G. R. Cho; E. -J. Jung; S. Park; J. Bae; M. -G. Kim","Intelligent Robotics R&D Division, Korea Institute of Robotics & Technology Convergence, Seoul, Korea; Autonomous Systems Center, Intelligent Robotics R&D Division, Korea Institute of Robotics & Technology Convergence, Pohang, Korea; Autonomous Systems Center, Intelligent Robotics R&D Division, Korea Institute of Robotics & Technology Convergence, Pohang, Korea; Autonomous Systems Center, Intelligent Robotics R&D Division, Korea Institute of Robotics & Technology Convergence, Pohang, Korea; Autonomous Systems Center, Intelligent Robotics R&D Division, Korea Institute of Robotics & Technology Convergence, Pohang, Korea; Autonomous Systems Center, Intelligent Robotics R&D Division, Korea Institute of Robotics & Technology Convergence, Pohang, Korea","2021 18th International Conference on Ubiquitous Robots (UR)","30 Jul 2021","2021","","","371","374","The need for robotic warehouse automation is emerging in the logistics industry because of the risk of manual labor-related employee injuries during truck unloading. The automatic unloading process for boxes in freight containers proceeds in the following order of steps: vision recognition, selection of box to be unloaded, and picking and placing the box on the conveyor belt using a robot manipulator. In this paper, we propose a box unloading sequence plan for robotic-logistics systems. First, it is assumed that the shape of the boxes stacked on the truck is pre-recognized by the visual system. Then, the box unloading sequence plan is generated by reinforcement learning. The performance of the proposed method was verified by comparing it with that of the heuristic method by numerical operation.","2325-033X","978-1-6654-3899-5","10.1109/UR52253.2021.9494633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9494633","","Service robots;Shape;Reinforcement learning;Manuals;Visual systems;Manipulators;Planning","conveyors;freight containers;industrial robots;learning (artificial intelligence);logistics;personnel;unloading;warehouse automation","reinforcement learning-based box unloading sequence planning;robotic container-unloading system;robotic warehouse automation;manual labor-related employee injuries;truck unloading;automatic unloading process;freight containers proceeds;robot manipulator;sequence plan;robotic-logistics systems","","1","","12","IEEE","30 Jul 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Accelerator using Q-Learning Algorithm with Optimized Bit Precision","N. Sutisna; A. M. R. Ilmy; H. N. Setiawan; I. Syafalni; R. Mulyawan; N. Ahmadi; T. Adiono",School of Electrical Engineering and Informatics; School of Electrical Engineering and Informatics; School of Electrical Engineering and Informatics; School of Electrical Engineering and Informatics; School of Electrical Engineering and Informatics; School of Electrical Engineering and Informatics; School of Electrical Engineering and Informatics,"2022 8th International Conference on Wireless and Telematics (ICWT)","7 Nov 2022","2022","","","1","5","This paper presents a Reinforcement Learning (RL) accelerator using Q-learning algorithm with optimized bit precision. In this work, we perform evaluation of the employed bit width of the data path subject to accuracy of the Q-values. The designed RL accelerator is implementing the Q-Learning algorithm that comprises several blocks: Q-Value memories, Q-Updater, Policy Generator and, Environment block. In addition, we also present the corresponding architecture and implement the design in the FPGA. Experimental results show that the number of bits can be reduced from 32 bits to 16 bits without sacrificing the accuracy. The accuracy can be maintained at around 88% when employing 16 bits data path with 10 bits fraction. Moreover, the designed 16 bits RL accelerator design size offers reduction of LUTs and FFs compared to 32 bits implementation by around 40% and 14 %, respectively. Hence, the optimized accelerator can be useful for low-complexity systems or limited resources such as in robot automation for smart navigation and smart mapping.","","978-1-6654-5122-2","10.1109/ICWT55831.2022.9935371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9935371","Reinforcement Learning;Q-Learning;Approximate Computing;Low Complexity;Bits Precision","Wireless communication;Q-learning;Automation;Navigation;Computer architecture;Telematics;Generators","electronic engineering computing;field programmable gate arrays;reinforcement learning","optimized bit precision;Q-learning algorithm;data path subject;RL accelerator design;reinforcement learning accelerator design;Q-value memories;Q-updater;policy generator;environment block;FPGA;smart navigation;smart mapping;robot automation;word length 10.0 bit;word length 16.0 bit to 32.0 bit","","","","9","IEEE","7 Nov 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning of Robotic Precision Insertion Skill Accelerated by Demonstrations","X. Wu; D. Zhang; F. Qin; D. Xu","School of Artificial Intelligence, University of Chinese Academy of Science, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Science, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Science, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Science, Beijing, China","2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)","19 Sep 2019","2019","","","1651","1656","Automatic high precision assembly of millimeter sized objects is a challenging task. Traditional methods for precision assembly rely on explicit programming with real robot system, and require complex parameter-tuning work. In this paper, we realize deep reinforcement learning of precision insertion skill learning, based on prioritized dueling deep Q-network (DQN). The Q-function is represented by the long short term memory (LSTM) neural network, whose input and output are the raw 6D force-torque feedback and the Q-value, respectively. According to the Q values conditioned on the current state, the skill model selects a 6 degree-of-freedom action from the predefined action set. To accelerate the learning process, the data from demonstrations is used to pre-train the model before the DQN starts. In order to improve the insertion efficiency and safety, insertion step length is modulated based on the instant reward. Our proposed method is validated with the peg-in-hole insertion experiments on a precision assembly robot.","2161-8089","978-1-7281-0356-3","10.1109/COASE.2019.8842940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8842940","","Robotic assembly;Neural networks;Modulation;Reinforcement learning;Programming;Data models;Safety","force control;force sensors;industrial manipulators;learning (artificial intelligence);position control;recurrent neural nets;robotic assembly","DQN;insertion step length;peg-in-hole insertion experiments;precision assembly robot;deep reinforcement learning;robot system;prioritized dueling deep Q-network;long short term memory neural network;force-torque feedback;Q-value;skill model;learning process;skill learning;robotic precision insertion skill","","11","","18","IEEE","19 Sep 2019","","","IEEE","IEEE Conferences"
"Dynamic power management of an embedded sensor network based on actor-critic reinforcement based learning","P. Sridhar; T. Nanayakkara; A. M. Madni; Mo Jamshidi","Department of Electrical Engineering, University of Texas, San Antonio, San Antonio, USA; Department of Mechanical Engineering, University of Moratuwa, Sri Lanka; Crocker Capital, Los Angeles, USA; Department of Electrical Engineering, University of Texas, San Antonio, San Antonio, USA","2007 Third International Conference on Information and Automation for Sustainability","17 Jun 2008","2007","","","76","81","Wireless sensor networks (WSNs) have gained tremendous popularity in recent years due to the wide range of applications envisioned - ranging from aerospace and defense to industrial and commercial. Although limited by communication and energy constraints, the low cost, small sensor nodes lend themselves to be deployed in large numbers to form a network with high spatial distribution. The overall effectiveness of the sensor network depends on how well the mutually contradicting objectives of conserving the limited on-board battery power and keeping the sensors awake for stimuli, are managed. In this paper, we have proposed an actor-critic based reinforcement learning mechanism that can be practically implemented on an embedded sensor with limited memory and processing power. Specifically, the contribution of this paper is the development of the value function (or critic/reinforcement function) that is implemented on each sensor node which aids in dynamic power scheduling based on different situations. The effectiveness of the proposed method has been demonstrated with real world experiments.","2151-1810","978-1-4244-1899-2","10.1109/ICIAFS.2007.4544783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544783","","Energy management;Aerodynamics;Wireless sensor networks;Aerospace industry;Defense industry;Costs;Battery management systems;Learning;Job shop scheduling;Dynamic scheduling","intelligent sensors;learning (artificial intelligence);power aware computing;scheduling;telecommunication computing;wireless sensor networks","dynamic power management;embedded wireless sensor network;actor-critic reinforcement based learning;spatial distribution;dynamic power scheduling","","2","","13","IEEE","17 Jun 2008","","","IEEE","IEEE Conferences"
"Attention Based Large Scale Multi-agent Reinforcement Learning","X. Wang; L. Ke; G. Zhang; D. Zhu","State Key Laboratory for Manufacturing Systems Engineering, School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory for Manufacturing Systems Engineering, School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory for Manufacturing Systems Engineering, School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory for Manufacturing Systems Engineering, School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China","2022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD)","12 Jul 2022","2022","","","112","117","Learning in large scale Multi-Agent Reinforcement Learning is fundamentally difficult due to the curse of dimensionality. In homogeneous multi-agent setting, mean field theory provides an effective way of scaling MARL to environments with many agents by abstracting other agents to a virtual mean agent, which assumes the impact of each player on the outcome is equal and infinitesimal. However, in some real scenarios, it is only several neighboring agents that affect the decision-making of an agent, need not all other agents. In addition, different neighboring agents may have different degrees of influence on the decision-making of an agent. In this paper, not restricted to homogeneous setting, we propose Adaptive Mean Field Multi-Agent Reinforcement Learning (AMF-MARL), which is based on the attention mechanism and can be used to deal with many agent scenarios in which there may be different influence relationships among agents. Specifically, we firstly derive the mean field approximation with adaptive weight. Then, we propose the Adaptive Mean Field Q-learning (AMF-Q) approach, and describe how to obtain the adaptive weight. Finally, we conduct experiment to study the learning effectiveness of proposed approach.","","978-1-6654-9913-2","10.1109/ICAIBD55127.2022.9820093","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820093","large scale;multi-agent reinforcement learning;adaptive mean field approximation","Q-learning;Decision making;Games;Learning (artificial intelligence);Big Data;Approximation algorithms;Mean field theory","decision making;multi-agent systems;reinforcement learning","learning effectiveness;mean field theory;virtual mean agent;decision-making;different neighboring agents;scale multiagent reinforcement learning;adaptive mean field Q-learning approach;adaptive mean field multiagent reinforcement learning;homogeneous multi-agent setting;AMF-MARL","","2","","20","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Adaptive Optimal Control for Nonlinear Systems With Asymmetric Hysteresis","L. Zheng; Z. Liu; Y. Wang; C. L. P. Chen; Y. Zhang; Z. Wu","School of Automation and the Guangdong-HongKong-Macao Joint Laboratory for Smart Discrete Manufacturing, Guangdong University of Technology, Guangzhou, China; School of Automation and the Guangdong-HongKong-Macao Joint Laboratory for Smart Discrete Manufacturing, Guangdong University of Technology, Guangzhou, China; College of Electrical and Information Engineering and the National Engineering Laboratory for Robot Visual Perception and Control, Hunan University, Changsha, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation and the Guangdong-HongKong-Macao Joint Laboratory for Smart Discrete Manufacturing, Guangdong University of Technology, Guangzhou, China; School of Automation and the Guangdong-HongKong-Macao Joint Laboratory for Smart Discrete Manufacturing, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","10","This article investigates the adaptive optimal tracking problem for a class of nonlinear affine systems with asymmetric Prandtl–Ishlinskii (PI) hysteresis nonlinearities based on actor-critic (A-C) learning mechanisms. Considering the huge obstacles arising from the uncertainty of hysteresis nonlinearity in actuators, we develop a scheme for the conflict between the construction of Hamilton functions and hysteresis nonlinearity. The actuator hysteresis forces the input into a hysteresis delay, thus preventing the Hamilton function from getting the current moment’s input instantly and thus making optimization impossible. In the first step, an inverse model is constructed to compensate for the hysteresis model with a shift factor. In the second step, we compensate for the control input by designing a feedback controller and incorporating the estimation and approximation errors into the Hamilton error. Optimal control, the other part of the actual control input, is obtained by taking partial derivatives of the Hamiltonian function after the nonlinearities have been circumvented. At the end, a simulation is given to validate the developed solution.","2162-2388","","10.1109/TNNLS.2023.3289978","National Key Research and Development Program of China(grant numbers:2020AAA0108303); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2022A1515010151); Guangdong Province Universities and Colleges Pearl River Scholar Funded Scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10195877","Actor-critic (A-C) learning mechanism;asymmetric Prandtl–Ishlinskii (PI) hysteresis;inverse compensators;neural networks","Hysteresis;Optimal control;Learning systems;Mathematical models;Estimation;Density functional theory;Backstepping","","","","","","","IEEE","26 Jul 2023","","","IEEE","IEEE Early Access Articles"
"A Target-coupled Multiagent Reinforcement Learning Approach for Teams of Mobile Sensing Robots","X. Wang; C. Zang; S. Xu; P. Zeng","Key Laboratory of Networked Control Systems, Chinese Academy of Sciences, Shenyang, China; Key Laboratory of Networked Control Systems, Chinese Academy of Sciences, Shenyang, China; Key Laboratory of Networked Control Systems, Chinese Academy of Sciences, Shenyang, China; Key Laboratory of Networked Control Systems, Chinese Academy of Sciences, Shenyang, China","2021 3rd International Conference on Industrial Artificial Intelligence (IAI)","30 Nov 2021","2021","","","1","6","A mobile sensing robot team (MSRT) is a typical application of multiagent system, which faces the huge challenge of environment dynamism that the change of action selection of one robot may influence behaviors of the other robots. In this paper, we investigate reinforcement learning methods for MSRT problem. A target-coupled multiagent reinforcement learning approach is proposed to solve the MSRT problem by taking advantage of both the knowledge of each agent and the local environment information sensed by the agent for achieving a shared goal in a common environment. We show the strength of our approach compared to the existed decentralized Q-learning in MSRT problem, where sensing robots are able to meet targets coverage requirement by discovering a coordinated strategy.","","978-1-6654-3517-8","10.1109/IAI53119.2021.9619378","Research and Development; State Key Laboratory of Robotics; National Natural Science Foundation of China; Natural Science Foundation of Liaoning Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619378","","Robot kinematics;Simulation;Reinforcement learning;Robot sensing systems;Sensors;Task analysis;Finishing","control engineering computing;mobile robots;multi-agent systems;multi-robot systems;reinforcement learning","MSRT;mobile sensing robot team;multiagent system;target-coupled multiagent reinforcement learning","","","","16","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Digital Twin-Driven Collaborative Scheduling for Heterogeneous Task and Edge-End Resource via Multi-Agent Deep Reinforcement Learning","C. Xu; Z. Tang; H. Yu; P. Zeng; L. Kong","State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Journal on Selected Areas in Communications","","2023","PP","99","1","1","With the interdisciplinary advances of mobile communication and edge computing, massive heterogeneous tasks are accessing wireless networks and competing for the edge-end computing and communication resources. Digital twin (DT), which establishes the digital models of physical objects for simulation, analysis and optimization, provides a promising method for network scheduling and management. This paper proposes a DT-driven edge-end collaborative scheduling algorithm for heterogeneous tasks and heterogeneous computing/communication resources. Specifically, multiple end devices (EDs) cooperate with each other to accomplish a complex job, where each ED can offload individual task to multiple edge servers (ESs) for parallel computing. By fully considering deadline requirements of heterogeneous tasks, maximum computing capabilities of ESs and EDs, computing resource estimation deviations of DT, maximum transmit powers of EDs and tolerable peak interference powers to coexisting EDs, we formulate a job completion time minimization problem to jointly optimize the edge-end task division, transmit power control, computing resource type matching and allocation. To solve this non-convex problem, we first reformulate it by multi-agent Markov decision process, where a compound reward leveraging latency reward and deadline reward according to the task criticality is designed. Then, we propose a multi-agent deep reinforcement learning-based scheduling algorithm, where Actor-Critic framework with estimation and target networks is designed for policy and value iterations. Meanwhile, a step-by-step ϵ-greedy algorithm is proposed to balance exploration and exploitation, avoiding local optimal trap. Through offline centralized training by DT and online distributed execution by EDs, we realize edge-end collaborative computing for heterogeneous tasks. Experimental results demonstrate that, comparing with typical benchmark algorithms, the proposed algorithm converges with the highest reward and achieves the smallest job completion time, where the deadlines of heterogeneous tasks can be well satisfied respectively.","1558-0008","","10.1109/JSAC.2023.3310066","National Key Research and Development Program of China(grant numbers:2020YFB1710900); National Natural Science Foundation of China(grant numbers:62173322,92267108); Science and Technology Program of Liaoning Province(grant numbers:2023JH3/10200004,2022JH25/10100005); Youth Innovation Promotion Association(grant numbers:2019202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234400","Digital twin;collaborative scheduling;edge computing;task offloading;multi-agent deep reinforcement learning","Task analysis;Computational modeling;Processor scheduling;Energy consumption;Collaboration;Minimization;Resource management","","","","","","","CCBY","30 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Model-based Reinforcement Learning Variable Impedance Control for 1-DOF PAM Elbow Exokeleton","Z. Zhao; J. Xiao; S. Wang; M. Liu; X. Li; L. Hao","School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China; School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China; School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China; School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China; State Key Laboratory of Syntherical Automation for Process Industries, Northeastern University, Shenyang, China; School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","2369","2374","Providing effective physical human-robot interactive control is one of the most important considerations in designing an exoskeleton robot. The purpose of the study is to develop a variable impedance control method for providing a reliable approach to different users to operate the exoskeleton by using interactive torque and a spring-damper model. The proposed method based on a model-based reinforcement learning structure and model predictive control frame, designed a neural network dynamic model to replace the state-action value function and using a linear estimate model to fit the action generated from the agent. Analysis and experiment results indicate that the designed method has less interactive torque and tracking error than the traditional impedance control method. Meanwhile, the results also illustrate the changing process of impedance parameters. The proposed control method sufficiently applies to the 1-DOF PAM elbow exoskeleton system.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728717","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728717","Exoskeleton;PAM;Reinforcement learning;Variable impedance control","Torque;Exoskeletons;Neural networks;Reinforcement learning;Predictive models;Impedance;Reliability","control engineering computing;control system synthesis;human-robot interaction;medical robotics;predictive control;reinforcement learning;torque control;wearable robots","exoskeleton robot;impedance parameters;interactive torque;linear estimate model;model predictive control frame;neural network dynamic model;PAM elbow exoskeleton system;physical human robot interactive control;reinforcement learning structure;spring damper model;state action value function;traditional impedance control method;variable impedance control method","","","","28","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Battery Management for Warehouse Robots via Average-Reward Reinforcement Learning","Y. Mu; Y. Li; K. Lin; K. Deng; Q. Liu","Department of Control Science and Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Department of Control Science and Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Department of Control Science and Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Department of Control Science and Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Department of Control Science and Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China","2022 IEEE International Conference on Robotics and Biomimetics (ROBIO)","18 Jan 2023","2022","","","253","258","In automated warehouses, the battery management strategy of Automated Guided Vehicles (AGVs) can affect the throughput and operational efficiency of the warehouse. In this paper, we first model the battery management problem as a Markov Decision Process (MDP) and adopt the deep reinforcement learning (DRL) algorithm as the battery management strategy. However, discounted reward DRL algorithms ignore long-term benefits, which are not suitable for the strategy since orders arriving at the warehouse at every moment are important and should be treated. In order to solve the above problems, we then introduce the average reward DRL algorithm to focus more on long-term benefits. But the existing average reward DRL algorithms have the problems of low sample utilization and unstable training. Therefore, we present a practical algorithm called average reward TD3 (ARTD3) that learns faster and is more stable. Finally, we conduct extensive experiments to confirm that ARTD3 outperforms discounted reward DRL algorithm and rule-based methods.","","978-1-6654-8109-0","10.1109/ROBIO55434.2022.10011784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011784","Automated Warehouses;Battery Management;Average-Reward Reinforcement Learning","Training;Measurement;Deep learning;Remotely guided vehicles;Battery management systems;Reinforcement learning;Markov processes","automatic guided vehicles;battery management systems;control engineering computing;deep learning (artificial intelligence);industrial robots;Markov processes;production engineering computing;reinforcement learning;warehouse automation","AGV;ARTD3;automated guided vehicles;automated warehouses;average reward DRL;average reward TD3;average-reward reinforcement learning;battery management;deep reinforcement learning;Markov decision process;warehouse robots","","","","24","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"A deep reinforcement learning based multiple meta-heuristic methods approach for resource constrained multi-project scheduling problem","Z. Han; Y. Yang; H. Ye","School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China","2022 7th International Conference on Intelligent Computing and Signal Processing (ICSP)","24 May 2022","2022","","","26","29","In order to solve resource-constrained multi-project scheduling problem (RCMPSP) more efficiently, this paper proposes a deep reinforcement learning algorithm based multiple meta-heuristic methods that combines the advantages of discrete cuckoo search (DCS) and particle swarm optimization (PSO). In the process of population evolution, Deep reinforcement learning was used to select the most suitable meta-heuristic algorithm (DCS and PSO) according to the diversity and quality of the population, and the reward was designed according to the evolution effect, so as to guide the algorithm to update the solutions more effectively and find the optimal solution quickly. In addition, the key steps of the CS algorithm, Levy flight and random walk, are redefined in this paper. The task movement, reverse mutation, task list recombination and adaptive and repairable swap mutation are used to make it suitable for solving discrete RCMPSP problems and improve the convergence speed of DCS algorithm. Experimental results on the latest data set (MPLIB) demonstrate the effectiveness of the proposed algorithm.","","978-1-6654-7857-1","10.1109/ICSP54964.2022.9778702","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9778702","Resource-constrained multi-project scheduling problem;Deep reinforcement learning;Multiple meta-heuristic methods;Discrete cuckoo search;Particle swarm optimization","Sociology;Signal processing algorithms;Reinforcement learning;Search problems;Scheduling;Task analysis;Statistics","deep learning (artificial intelligence);genetic algorithms;particle swarm optimisation;production engineering computing;project management;scheduling;search problems","multiple meta-heuristic methods approach;resource-constrained multiproject scheduling problem;deep reinforcement learning algorithm;particle swarm optimization;PSO;population evolution;suitable meta-heuristic algorithm;evolution effect;optimal solution;CS algorithm;discrete RCMPSP problems;DCS algorithm","","","","14","IEEE","24 May 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning for the Control of Bacterial Growth Bioprocess","G. Bujgoi; D. Sendrescu; D. Gagiu","Department of Automation and Electronics, University of Craiova, Craiova, Romania; Department of Automation and Electronics, University of Craiova, Craiova, Romania; Department of Automation and Electronics, University of Craiova, Craiova, Romania","2021 25th International Conference on System Theory, Control and Computing (ICSTCC)","17 Nov 2021","2021","","","525","529","paper deals with the application of the reinforcement learning technique (from the well-known field of machine learning) for the control of a biotechnological process – the bacterial growth. The mathematical model of the bioprocess consists of a set of nonlinear differential equations. This type of systems is difficult to control because they are strongly nonlinear, with time-varying parameters and very sensitive to small changes in initial conditions. The paper presents a control strategy based on an actor-critic structure from the reinforcement learning approach that is a combination of optimal control (based on state feedback) and adaptive control. The adaptive part of the controller is a neural network used for value function approximation. The performance of the algorithm is analyzed using numerical simulations.","2372-1618","978-1-6654-1496-8","10.1109/ICSTCC52150.2021.9607291","European Social Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607291","machine learning;reinforcement learning;bacterial growth bioprocess","Microorganisms;Biological system modeling;Computational modeling;Neural networks;Process control;Optimal control;Reinforcement learning","adaptive control;biotechnology;control engineering computing;function approximation;neurocontrollers;nonlinear differential equations;optimal control;production engineering computing;reinforcement learning;state feedback;time-varying systems","bacterial growth bioprocess;reinforcement learning technique;machine learning;biotechnological process;mathematical model;nonlinear differential equations;time-varying parameters;initial conditions;control strategy;actor-critic structure;reinforcement learning approach;optimal control;adaptive control;value function approximation","","","","9","IEEE","17 Nov 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Control of a Shape Memory Alloy-based Bionic Robotic Hand","M. Liu; L. Hao; W. Zhang; Y. Chen; J. Chen","School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China; School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China; School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China; State Key Laboratory of Intelligent Technology and Systems, Tsinghua University, China; Sino-Dutch Biomedical and Information Engineering School, Northeastern University, Shenyang, China","2019 IEEE 9th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","16 Apr 2020","2019","","","969","973","Shape Memory Alloy (SMA) with the advantage of high power-to-weight ratio as a promising alternative to traditional actuators is widely applied to robotic systems. Due to the merits of low noise, driving voltages and small size SMA is used in robotic hand. Hysteresis phenomenon of SMA and dynamic model of the SMA-based robotic hand are complexities in control system. In this paper, we apply a reinforcement learning (RL) algorithm to the robotic hand actuated by SMA for motion control. The proposed hand can achieve the desired bending state and grasp the object with desired bending state effectively and steadily.","2379-7711","978-1-7281-0770-7","10.1109/CYBER46603.2019.9066775","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066775","shape memory alloy;reinforcement learning;bionic robotic hand","Temperature sensors;Robot sensing systems;Thumb;Temperature measurement;Actuators","actuators;bending;control engineering computing;learning (artificial intelligence);manipulators;motion control;shape memory effects","SMA-based robotic hand;control system;reinforcement learning algorithm;motion control;reinforcement learning control;high power-to-weight ratio;traditional actuators;robotic systems;shape memory alloy-based bionic robotic hand","","4","","20","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Optimizing Earth Moving Operations Via Reinforcement Learning","V. Shitole; J. Louis; P. Tadepalli","Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA","2019 Winter Simulation Conference (WSC)","20 Feb 2020","2019","","","2954","2965","Earth moving operations are a critical component of construction and mining industries with a lot of potential for optimization and improved productivity. In this paper we combine discrete event simulation with reinforcement learning (RL) and neural networks to optimize these operations that tend to be cyclical and equipment-intensive. One advantage of RL is that it can learn near-optimal policies from the simulators with little human guidance. We compare three different RL methods including Q-learning, actor-critic, and trust region policy optimization and show that they all converge to significantly better policies than human-designed heuristics. We conclude that RL is a promising approach to automate and optimize earth moving and other similar expensive operations in construction, mining, and manufacturing industries.","1558-4305","978-1-7281-3283-9","10.1109/WSC40007.2019.9004935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9004935","","Learning (artificial intelligence);Earth;Markov processes;Neural networks;Linear programming;Routing;Optimization","construction equipment;construction industry;discrete event simulation;earthmoving equipment;learning (artificial intelligence);mechanical engineering computing;mining industry;neural nets;optimisation;productivity","earth moving operations;reinforcement learning;critical component;mining industries;productivity;discrete event simulation;Q-learning method;actor-critic method;trust region policy optimization;construction industries;neural networks;human-designed heuristics;manufacturing industries","","2","","16","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"Inverse Reinforcement Learning in Tracking Control Based on Inverse Optimal Control","W. Xue; P. Kolaric; J. Fan; B. Lian; T. Chai; F. L. Lewis","State Key Laboratory of Synthetical Automation for Process Industries and International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; UTA Research Institute, University of Texas at Arlington, Arlington, TX, USA; State Key Laboratory of Synthetical Automation for Process Industries and International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; UTA Research Institute, University of Texas at Arlington, Arlington, TX, USA; State Key Laboratory of Synthetical Automation for Process Industries and International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; UTA Research Institute, University of Texas at Arlington, Arlington, TX, USA","IEEE Transactions on Cybernetics","19 Sep 2022","2022","52","10","10570","10581","This article provides a novel inverse reinforcement learning (RL) algorithm that learns an unknown performance objective function for tracking control. The algorithm combines three steps: 1) an optimal control update; 2) a gradient descent correction step; and 3) an inverse optimal control (IOC) update. The new algorithm clarifies the relation between inverse RL and IOC. It is shown that the reward weight of an unknown performance objective that generates a target control policy may not be unique. We characterize the set of all weights that generate the same target control policy. We develop a model-based algorithm and, further, two model-free algorithms for systems with unknown model information. Finally, simulation experiments are presented to show the effectiveness of the proposed algorithms.","2168-2275","","10.1109/TCYB.2021.3062856","National Natural Science Foundation of China (NSFC)(grant numbers:61991400,61991404,61991403,61533015); Naval Research under (ONR)(grant numbers:N00014-18-1-2221); National Science Foundation (NSF)(grant numbers:ECCS-1839804); Army Research Office (ARO)(grant numbers:W911NF-20-1-0132); Higher Education Discipline Innovation Project(grant numbers:B08015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9409781","Inverse optimal control (IOC);inverse reinforcement learning (RL);optimal control;RL;tracking control","Optimal control;Trajectory;Target tracking;Control systems;Heuristic algorithms;Mathematical model;Symmetric matrices","gradient methods;learning (artificial intelligence);optimal control","target control policy;model-based algorithm;model-free algorithms;unknown model information;tracking control;novel inverse reinforcement learning algorithm;RL;unknown performance objective function;gradient descent correction step;inverse optimal control update","Algorithms;Computer Simulation;Learning;Reinforcement, Psychology;Reward","17","","36","IEEE","20 Apr 2021","","","IEEE","IEEE Journals"
"Inverse Reinforcement Q-Learning Through Expert Imitation for Discrete-Time Systems","W. Xue; B. Lian; J. Fan; P. Kolaric; T. Chai; F. L. Lewis","State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; UTA Research Institute, University of Texas at Arlington, Arlington, TX, USA; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; UTA Research Institute, University of Texas at Arlington, Arlington, TX, USA; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; UTA Research Institute, University of Texas at Arlington, Arlington, TX, USA","IEEE Transactions on Neural Networks and Learning Systems","2 May 2023","2023","34","5","2386","2399","In inverse reinforcement learning (RL), there are two agents. An expert target agent has a performance cost function and exhibits control and state behaviors to a learner. The learner agent does not know the expert’s performance cost function but seeks to reconstruct it by observing the expert’s behaviors and tries to imitate these behaviors optimally by its own response. In this article, we formulate an imitation problem where the optimal performance intent of a discrete-time (DT) expert target agent is unknown to a DT Learner agent. Using only the observed expert’s behavior trajectory, the learner seeks to determine a cost function that yields the same optimal feedback gain as the expert’s, and thus, imitates the optimal response of the expert. We develop an inverse RL approach with a new scheme to solve the behavior imitation problem. The approach consists of a cost function update based on an extension of RL policy iteration and inverse optimal control, and a control policy update based on optimal control. Then, under this scheme, we develop an inverse reinforcement Q-learning algorithm, which is an extension of RL Q-learning. This algorithm does not require any knowledge of agent dynamics. Proofs of stability, convergence, and optimality are given. A key property about the nonunique solution is also shown. Finally, simulation experiments are presented to show the effectiveness of the new approach.","2162-2388","","10.1109/TNNLS.2021.3106635","NSFC(grant numbers:61991400,61991404,61991403,61533015); 111 Project(grant numbers:B08015); 2020 Science and Technology Major Project of Liaoning Province(grant numbers:2020JH1/10100008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537731","Data-based control;inverse optimal control (IOC);inverse reinforcement learning (RL);Q-learning","Cost function;Optimal control;Costs;Trajectory;Heuristic algorithms;Convergence;System dynamics","control engineering computing;discrete time systems;feedback;iterative methods;learning (artificial intelligence);optimal control;reinforcement learning","agent dynamics;behavior imitation problem;control policy update;cost function update;discrete-time expert target agent;discrete-time systems;DT Learner agent;exhibits control;expert imitation;inverse optimal control;inverse reinforcement learning;inverse reinforcement Q-learning algorithm;inverse RL approach;observed expert;optimal feedback gain;optimal performance intent;optimal response;performance cost function;state behaviors","","9","","39","IEEE","14 Sep 2021","","","IEEE","IEEE Journals"
"Inverse Reinforcement Learning for Trajectory Imitation Using Static Output Feedback Control","W. Xue; B. Lian; J. Fan; T. Chai; F. L. Lewis","State Key Laboratory of Synthetical Automation for Process Industries and International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; UTA Research Institute, University of Texas at Arlington, Arlington, TX, USA; State Key Laboratory of Synthetical Automation for Process Industries and International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries and International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; UTA Research Institute, University of Texas at Arlington, Arlington, TX, USA","IEEE Transactions on Cybernetics","","2023","PP","99","1","13","This article studies the trajectory imitation control problem of linear systems suffering external disturbances and develops a data-driven static output feedback (OPFB) control-based inverse reinforcement learning (RL) approach. An Expert-Learner structure is considered where the learner aims to imitate expert’s trajectory. Using only measured expert’s and learner’s own input and output data, the learner computes the policy of the expert by reconstructing its unknown value function weights and thus, imitates its optimally operating trajectory. Three static OPFB inverse RL algorithms are proposed. The first algorithm is a model-based scheme and serves as basis. The second algorithm is a data-driven method using input-state data. The third algorithm is a data-driven method using only input–output data. The stability, convergence, optimality, and robustness are well analyzed. Finally, simulation experiments are conducted to verify the proposed algorithms.","2168-2275","","10.1109/TCYB.2023.3241015","Science and Technology Major Project 2020 of Liaoning Province(grant numbers:2020JH1/10100008); NSFC(grant numbers:61991404,61991403,61991400); 111 Project(grant numbers:B08015); LiaoNing Revitalization Talents Program(grant numbers:XLYC2007135); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10045775","Data-driven control;inverse optimal control (IOC);inverse reinforcement learning (RL);static output feedback (OPFB) control;trajectory imitation","Trajectory;Optimal control;Costs;Standards;Output feedback;State feedback;Reinforcement learning","","","","","","","IEEE","16 Feb 2023","","","IEEE","IEEE Early Access Articles"
"Transferring Human Manipulation Knowledge to Robots with Inverse Reinforcement Learning","E. B. Hansen; R. E. Andersen; S. Madsen; S. Bøgh","Department of Materials and Production, Aalborg University, Denmark; Department of Materials and Production, Aalborg University, Denmark; Department of Materials and Production, Aalborg University, Denmark; Department of Materials and Production, Aalborg University, Denmark","2020 IEEE/SICE International Symposium on System Integration (SII)","9 Mar 2020","2020","","","933","937","The need for adaptable models, e.g. reinforcement learning, have in recent years been more present within the industry. In this paper, we show how two versions of inverse reinforcement learning can be used to transfer task knowledge from a human expert to a robot in a dynamic environment. Moreover, a second method called Principal Component Analysis weighting is presented and discussed. The method shows potential in the use case but requires some more research.","2474-2325","978-1-7281-6667-4","10.1109/SII46433.2020.9025873","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025873","","Learning (artificial intelligence);Trajectory;Task analysis;Machine learning;Mathematical model;Service robots","intelligent robots;learning (artificial intelligence);principal component analysis","human manipulation knowledge;inverse reinforcement learning;task knowledge transfer;human expert;robots;principal component analysis weighting","","4","","21","IEEE","9 Mar 2020","","","IEEE","IEEE Conferences"
"Comparative Study for Deep Reinforcement Learning with CNN, RNN, and LSTM in Autonomous Navigation","Z. TAN; M. KARAKOSE","Erzincan Binali Yıldırım University, Erzincan, Turkey; Firat University, Elazığ, Turkey","2020 International Conference on Data Analytics for Business and Industry: Way Towards a Sustainable Economy (ICDABI)","20 Jan 2021","2020","","","1","5","Reinforcement learning algorithms are one of the popular machine learning methods in recent years. Unlike deep learning (DL) algorithms, it does not require a data set during the training phase, increasing its popularity. Today, it offers successful results especially in the navigation of autonomous robots and in solving complex problems such as video games. The feedback process is also known as a reward or called a penalty. Given Agents and environment, it is determined which action to take. In this article, the performance of three different DL algorithms has been compared using the PyGame simulator. In the simulator created using CNN, RNN and LSTM deep learning algorithms, it is aimed that the representative will learn to move without hitting four different fixed obstacles. While creating the training environment, the movement of an autonomous robot in the field without getting stuck in obstacles was simulated. Separate results of each algorithm were reported in the training results. As a result of these reports, it has been observed that the LSTM algorithm is more successful than the others.","","978-1-7281-9675-6","10.1109/ICDABI51230.2020.9325622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9325622","Deep Reinforcement Learning;Autonomous Navigations;Deep Q-Learning;PyGame","Reinforcement learning;Training;Long short term memory;Games;Robots;Libraries;Industries","collision avoidance;convolutional neural nets;deep learning (artificial intelligence);mobile robots;recurrent neural nets","reinforcement learning algorithms;machine learning;autonomous robot;video games;DL algorithms;PyGame simulator;LSTM deep learning algorithms;deep reinforcement learning;CNN;RNN;autonomous navigation","","3","","24","IEEE","20 Jan 2021","","","IEEE","IEEE Conferences"
"Efficient DER Voltage Control Using Ensemble Deep Reinforcement Learning","J. Obert; R. D. Trevizan; A. Chavez","Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA","2022 5th International Conference on Artificial Intelligence for Industries (AI4I)","15 May 2023","2022","","","55","58","To meet the challenges oflow-carbon power generation, distributed energy resources (DERs) such as solar and wind power generators are now widely integrated into the power grid. Because of the autonomous nature of DERs, ensuring properly regulated output voltages of the individual sources to the grid distribution system poses a technical challenge to grid operators. Stochastic, model-free voltage regulations methods such as deep reinforcement learning (DRL) have proven effective in the regulation of DER output voltages; however, deriving an optimal voltage control policy using DRL over a large state space has a large computational time complexity. In this paper we illustrate a computationally efficient method for deriving an optimal voltage control policy using a parallelized DRL ensemble. Additionally, we illustrate the resiliency of the control ensemble when random noise is introduced by a cyber adversary.","2770-4718","978-1-6654-5961-7","10.1109/AI4I54798.2022.00021","Sandia National Laboratories; National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10123519","deep reinforcement learning;distributed energy resources;double Q-learning;policy gradients learning;voltage control.","Deep learning;Industries;Stochastic processes;Reinforcement learning;Wind power generation;Regulation;Power grids","deep learning (artificial intelligence);distributed power generation;power engineering computing;power generation control;power grids;reinforcement learning;voltage control;wind power plants","autonomous nature;challenges oflow-carbon power generation;computationally efficient method;control ensemble;DER output voltages;DERs;distributed energy resources;efficient DER voltage control;ensemble deep reinforcement learning;grid distribution system;grid operators;individual sources;model-free voltage regulations methods;optimal voltage control policy;parallelized DRL ensemble;power grid;solar wind power generators;technical challenge","","","","10","IEEE","15 May 2023","","","IEEE","IEEE Conferences"
"Proximity-Based Reward System and Reinforcement Learning for Path Planning","M. -A. Blais; M. A. Akhloufi","Dept of Computer Science, Université de Moncton Perception, Robotics, and Intelligent Machines Research Group (PRIME), Moncton, NB, Canada; Dept of Computer Science, Université de Moncton Perception, Robotics, and Intelligent Machines Research Group (PRIME), Moncton, NB, Canada","2023 15th International Conference on Computer and Automation Engineering (ICCAE)","3 May 2023","2023","","","1","5","Path planning is an important and complex task in the field of robotics and automation. It consists of finding the optimal path given a starting location, obstacles and a final destination. Reinforcement learning is a trial and error approach that has seen success in the field of path planning. Multiple reinforcement learning algorithms such as Q-learning and SARSA exist and have achieved great results. These algorithms typically use a uniform reward system such that every move, collision and goal return a specific reward. We propose a proximity-based reward system for classical reinforcement learning algorithms on path planning scenarios. We compare our reward systems combined with different optimization techniques and algorithms for path planning. These approaches are compared using the total completion rate for the mazes and average training time. We achieved interesting results with our reward systems and optimization techniques allowing us to decrease the training time.","2154-4360","979-8-3503-9622-5","10.1109/ICCAE56788.2023.10111485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10111485","reinforcement learning;path planning;robotics;proximity;reward","Training;Automation;Q-learning;Path planning;Task analysis;Optimization","learning (artificial intelligence);mobile robots;optimisation;path planning;reinforcement learning","classical reinforcement;multiple reinforcement learning algorithms;optimal path;path planning scenarios;proximity-based reward system;reward systems;specific reward;uniform reward system","","","","16","IEEE","3 May 2023","","","IEEE","IEEE Conferences"
"Path Tracking Control of Hybrid-driven Robotic Fish Based on Deep Reinforcement Learning","L. Ma; Z. Yue; R. Zhang","School of International Education, Zhengzhou University of Light Industry, Zhengzhou, Henan Province, China; School of International Education, Zhengzhou University of Light Industry, Zhengzhou, Henan Province, China; School of mechanical engineering, Tianjin University, Tianjin, China","2020 IEEE International Conference on Mechatronics and Automation (ICMA)","26 Oct 2020","2020","","","815","820","Hybrid-driven robotic fish (HRF) is a new type of marine robot with long endurance. With the development of artificial intelligence and deep reinforcement learning, hybrid-driven robotic fish are becoming more intelligent and autonomous. This article is based on autonomous learning and autonomous decision-making control technology, drawing on human learning and decision-making processes, so that the aircraft can accumulate past control experience in a complex marine environment, acquire knowledge, and constantly improve its own performance and adaptability to achieve path following purpose. Firstly, the movement pattern of HRF was analyzed, and then the process of Deep Reinforcement Learning was analyzed. In addition, the Deep Reinforcement Learning method was improved based on the HRF movement pattern, and a pool experiment was performed. The experimental results show that the accuracy of the HRF path following control phase based on Deep Reinforcement Learning is improved by about 3.79%, compared with the traditional PID control method. It also indicates the Deep Reinforcement Learning control method has a better path following ability. Furthermore, it is of great significance to the swarm HRFs control and application.","2152-744X","978-1-7281-6416-8","10.1109/ICMA49215.2020.9233667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233667","Hybrid-driven robotic fish;path tracking control;Deep Reinforcement Learning","Training;PI control;Tracking;Decision making;Process control;Reinforcement learning;Fish","decision making;learning (artificial intelligence);marine robots;mobile robots;motion control;neurocontrollers;path planning","marine robot;autonomous learning;path tracking control;HRF path following control;deep reinforcement learning;autonomous decision making control;hybrid driven robotic fish;artificial intelligence","","3","","15","IEEE","26 Oct 2020","","","IEEE","IEEE Conferences"
"Continuous Control with Deep Reinforcement Learning for Mobile Robot Navigation","J. Xiang; Q. Li; X. Dong; Z. Ren","School of Automation Science and Electrical Engineering, Beihang University, Beijing, P.R. China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, P.R. China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, P.R. China; Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, P.R. China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","1501","1506","Autonomous navigation is one of the focuses in the field of mobile robot research. The traditional method usually consists of two parts: building the map of environment, localization of mobile robot and path planning. However, these traditional methods usually rely on high-precision sensor information. At the same time, mobile robots have no intelligent understanding of autonomous navigation. In this article, a deep reinforcement learning method, i.e. soft actor critic, is used to navigate in a mapless environment. It takes laser scanning data and information of the target as input, outputs linear velocity and angular velocity in continuous space. The simulation shows that this learning-based end-to-end autonomous navigation method can accomplish tasks as well as traditional methods.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8996652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8996652","Mobile Robot;Deep Reinforcement Learning;Soft Actor Critic;Autonomous navigation","Mobile robots;Machine learning;Navigation;Autonomous robots;Entropy;Lasers","angular velocity control;control engineering computing;learning (artificial intelligence);mobile robots;navigation;path planning;robot vision","continuous control;mobile robot navigation;mobile robot research;path planning;high-precision sensor information;mobile robots;intelligent understanding;deep reinforcement learning method;mapless environment;laser scanning data;continuous space;learning-based end-to-end autonomous navigation method","","14","","17","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"A greedy navigation and subtle obstacle avoidance algorithm for USV using reinforcement learning","X. Wang; X. Liu; T. Shen; W. Zhang","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Mechanical Engineering, Sophia University, Tokyo, Japan; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","770","775","As the Unmanned Surface Vessels (USV) having been applied in diverse and complex environments, it is extremely important to improve autonomous navigation. Considering this background, a greedy navigation and subtle obstacle avoidance algorithm is proposed on the basis of actor-critic architecture to achieve the goal with very little training cost. Markov process is established elaborately to fit the kinematics equation and the reward function with behavioral priori provides benefits in both training and testing. Compared to the analytical approach, the proposed algorithm has the features of conciseness, adaptability and extendibility. Four different scenarios are designed and adopted to demonstrate the effectiveness and practicalbility of our algorithm.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8996917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8996917","USV;reinforcement learning;path planning;obstacle avoidance","Collision avoidance;Mathematical model;Learning (artificial intelligence);Training;Surges;Navigation;Path planning","collision avoidance;control engineering computing;learning (artificial intelligence);Markov processes;mobile robots;navigation;path planning","USV;reinforcement learning;unmanned surface vessels;autonomous navigation;greedy navigation;subtle obstacle avoidance algorithm;actor-critic architecture;Markov process","","5","","12","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Multi-objective reinforcement learning algorithm for MOSDMP in unknown environment","Yun Zhao; Q. Chen; W. Hu","Department of Automation, University of Science and Technology, Nanjing, Jiangsu, China; Department of Automation, University of Science and Technology, Nanjing, Jiangsu, China; Department of Automation, University of Science and Technology, Nanjing, Jiangsu, China","2010 8th World Congress on Intelligent Control and Automation","23 Aug 2010","2010","","","3190","3194","In this paper, a new multi-objective reinforcement learning algorithm for multi-objective sequential decision making problems in unknown environment is proposed. The salient characters of the algorithm are: 1) decision maker's objective preference is introduced to guide learning direction; 2) a new measure of comparing action decisions under several objectives based on the fuzzy inference system is defined; 3) fast learning speed can be achieved. Simulation results demonstrate that the proposed algorithm has a good learning performance.","","978-1-4244-6712-9","10.1109/WCICA.2010.5553980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5553980","Reinforcement learning;Markov decision processes (MDP);Fuzzy inference system;Action decision","Learning;Markov processes;Silicon;Inference algorithms;Delta modulation;Optimization;Algorithm design and analysis","decision making;fuzzy reasoning;learning (artificial intelligence)","MOSDMP;multiobjective reinforcement learning algorithm;multiobjective sequential decision making problem;fuzzy inference system;learning speed","","3","","11","IEEE","23 Aug 2010","","","IEEE","IEEE Conferences"
"Energy Optimization Management of Multi-microgrid using Deep Reinforcement Learning","T. Zhang; D. Yue; N. Zhao","dept. Institute of Advanced Technology, Nanjing University of Posts and Telecommunications (of Aff.) Automation Nanjing University of Posts and Telecommunication (of Aff.), Nanjing, China; dept. Institute of Advanced Technology, Nanjing University of Posts and Telecommunications (of Aff.) Automation Nanjing University of Posts and Telecommunication (of Aff.), Nanjing, China; dept. Institute of Advanced Technology, Nanjing University of Posts and Telecommunications (of Aff.) Automation Nanjing University of Posts and Telecommunication (of Aff.), Nanjing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","4049","4053","The paper investigates the energy optimization management problem for multi-microgrid (MMG) in the island case. MMG system could achieve greater consumption of distributed energy and a more stable power supply. While, due to the existence of uncertainties such as wind turbines, photovoltaics and loads, it is challenging to design an accurate energy optimization model to control power flow. Notably, conventional methods also appear to be inapplicable. To address the problem of MMG, a deep reinforcement learning optimal algorithm is applied in this paper. The simulation verifies that the proposed method could achieve the power balance within MMG and could minimize the total cost under uncertainties.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9326507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326507","Multi-microgrid;Energy optimization management;Deep reinforcement learning;Power balance","Microgrids;Optimization;Power generation;Batteries;Uncertainty;Real-time systems;Telecommunications","cost reduction;distributed power generation;energy management systems;learning (artificial intelligence);load flow control;optimisation;power generation economics;power system simulation","power balance;multimicrogrid;energy optimization management problem;island case;MMG system;distributed energy consumption;stable power supply;control power flow;deep reinforcement learning optimal algorithm;total cost","","2","","13","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning for Active Voltage Control on Multi-Hybrid Microgrid Interconnection System","J. Yang; C. Yuan; F. Meng","College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","4700","4704","Under the new trend of decarbonization, a large number of renewable distributed generations are integrated into the distribution network to form a multi-hybrid microgrid interconnection system. Aiming at the problem of bus voltage fluctuation caused by the uncertainty of source and load in hybrid microgrid, a voltage stability control method based on multi-agent reinforcement learning is proposed in this paper. The distributed power and energy storage devices in the interconnected system of multi-hybrid microgrid are used to alleviate power congestion and improve voltage quality. The bus voltage stability control problem is transformed into a Markov decision game process, and the reward function is designed according to the voltage stability. The voltage stability control model is constructed with the framework of centralized training and decentralized execution. This method does not require accurate power flow modeling, and constantly updates the neural network parameters through the continuous interaction between the agent and the environment to control voltage dynamic stability. Finally, the effectiveness of the proposed method is verified by numerical simulation, and the characteristics of different multi-agent reinforcement learning algorithms are compared and summarized.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10054852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054852","hybrid microgrid group;distributed generation;multi-agent reinforcement learning;active voltage control;power quality","Training;Uncertainty;Microgrids;Reinforcement learning;Interconnected systems;Power system stability;Stability analysis","distributed power generation;distribution networks;learning (artificial intelligence);Markov processes;multi-agent systems;power generation control;power system stability;reinforcement learning;voltage control","active voltage control;bus voltage fluctuation;bus voltage stability control problem;different multiagent reinforcement learning algorithms;distributed power;distribution network;energy storage devices;hybrid microgrid;interconnected system;multihybrid microgrid interconnection system;renewable distributed generations;voltage dynamic stability;voltage quality;voltage stability control method;voltage stability control model","","","","11","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Encoding Topology Information for Deep Reinforcement Learning with Continuous Action Space","Q. Wang; Y. Gao; W. Liu","Dept. of Automation, Shanghai Jiao Tong University, Shanghai, China; MoE Key Lab of Articial Intelligence Dept. of Automation, Shanghai Jiao Tong University, Shanghai, China; Dept. of Automation, Shanghai Jiao Tong University, Shanghai, China","2019 4th International Conference on Robotics and Automation Engineering (ICRAE)","23 Mar 2020","2019","","","158","162","In the context of reinforcement learning, the training efficiency can decay exponentially with the size of the state space. Therefore, designing easily-optimized state space representation has remained an open problem. In this paper, we focus on a general and challenging scenario, i.e. reinforcement learning with continuous action spaces. We propose a new representation framework by explicitly encoding topology information such as the geometrical and the kinematic relations among different parts of the agent to make the representation more informative, which results in effective optimization. Extensive experiments were conducted on three settings to demonstrate that our method can remarkably stabilize and speed up the training process.","","978-1-7281-4740-6","10.1109/ICRAE48301.2019.9043778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9043778","deep reinforcement learning;state space representation;continuous action space","Robot kinematics;Reinforcement learning;Training;Aerospace electronics;Task analysis;Manipulators","learning (artificial intelligence);optimisation;state-space methods;topology","continuous action space;easily-optimized state space representation;representation framework;effective optimization;deep reinforcement learning;topology information encoding;kinematic relations;geometrical relations","","","","18","IEEE","23 Mar 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning with Auxiliary Localization Task for Mapless Navigation","C. He; W. Zhang; T. Wang","Department of Automation, Southeast University, Nanjing, China; Department of Automation, Southeast University, Nanjing, China; Department of Automation, Southeast University, Nanjing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","3069","3073","With the development of deep learning, many researchers utilize deep reinforcement learning (DRL) to solve robot navigation in complex environments [1], [2]. These approaches are attractive as they learn navigation polices directly from sensory data without needing to construct a map. However, learning to navigate from reinforcement learning poses the challenge that rewards are sparsely distributed in environments, which results in low sample efficiency and slow convergence. To tackle the challenge, we design a novel auxiliary robot localization task. The localization task combined with a widely employed depth prediction task is jointly learned with the goal-driven deep reinforcement problem to facilitate efficient feature representation learning. We test our approach on both static-goal and random-goal maze environments. Experiment results validate the effectiveness of our auxiliary localization task in improving both final reward and convergence speed.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327835","Deep Reinforcement Learning;Robot Navigation;Auxiliary Localization","Task analysis;Navigation;Location awareness;Robots;Reinforcement learning;Convergence;Training","control engineering computing;control system synthesis;deep learning (artificial intelligence);mobile robots;navigation;path planning;robot programming","mapless navigation;deep reinforcement learning;robot navigation;feature representation learning;auxiliary robot localization task;goal-driven deep reinforcement;static-goal maze environment;random-goal maze environment","","","","18","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Learning to Discover Task-Relevant Features for Interpretable Reinforcement Learning","Q. Zhang; X. Ma; Y. Yang; C. Li; J. Yang; Y. Liu; B. Liang","Harbin Institute of Technology, Harbin, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Harbin Institute of Technology, Harbin, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Robotics and Automation Letters","21 Jul 2021","2021","6","4","6601","6607","Reinforcement Learning (RL) agents are often fed with large-dimensional observations to achieve the ideal performance in complex environments. Unfortunately, the massive observation space usually contains useless or even adverse features, which leads to low sample efficiency. Existing methods rely on domain knowledge and cross-validation to discover efficient features which are informative for decision-making. To minimize the impact of prior knowledge, we propose a temporal-adaptive feature attention algorithm (TAFA). We adopt a non-linear attention module, automatically choosing task-relevant components of hand-crafted state features without any domain knowledge. Our experiments on MuJoCo and TORCS tasks show that the agent achieves competitive performance with state-of-the-art methods while successfully identifying the most task-relevant features for free. We believe our work takes a step towards the interpretability of RL. Our code is available at https://github.com/QiyuanZhang19/Temporal-Adaptive-Feature-Attention/tree/master.","2377-3766","","10.1109/LRA.2021.3091885","National Natural Science Foundation of China(grant numbers:61903215); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463791","Non-linear attention;task-relevant features;interpretable reinforcement learning","Task analysis;Feature extraction;Trajectory;Reinforcement learning;Data mining;Predictive models;Decision making","decision making;learning (artificial intelligence)","temporal-adaptive feature attention;nonlinear attention module;task-relevant components;hand-crafted state features;task-relevant features;interpretable reinforcement Learning;reinforcement learning agents;large-dimensional observations;decision-making;TAFA;MuJoCo;TORCS","","3","","20","IEEE","23 Jun 2021","","","IEEE","IEEE Journals"
"Distributed neural network-based policy gradient reinforcement learning for multi-robot formations","Wen Shang; Dong Sun","Department of Manufacturing Engineering and Engineering Management,Suzhou Research Institute, City University of Hong Kong, Suzhou, Jiangsu, China; Department of Manufacturing Engineering and Engineering Management, City University of Hong Kong, Hong Kong, China","2008 International Conference on Information and Automation","26 Aug 2008","2008","","","113","118","Multi-robot learning is a challenging task not only because of large and continuous state/action spaces, but also uncertainty and partial observability during learning. This paper presents a distributed policy gradient reinforcement learning (PGRL) methodology of a multi-robot system using neural network as the function approximator. This distributed PGRL algorithm enables each robot to independently decide its policy, which is, however, affected by all the other robots. Neural network is used to generalize over continuous state space as well as discrete/continuous action spaces. A case study on leader-follower formation application is performed to demonstrate the effectiveness of the proposed learning method.","","978-1-4244-2183-1","10.1109/ICINFA.2008.4607978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4607978","","Artificial neural networks;Robots;Lead;History;Indexes;Learning;Function approximation","learning (artificial intelligence);multi-robot systems;neural nets;state-space methods","distributed neural network;distributed policy gradient reinforcement learning;multirobot formations;multirobot learning;PGRL;function approximator;continuous state space;leader-follower formation","","2","","16","IEEE","26 Aug 2008","","","IEEE","IEEE Conferences"
"A reinforcement learning approach to obstacle avoidance of mobile robots","K. Macek; I. Petrovic; N. Peric","Faculty of Electrical Engineering and Computing, Department of Control and Computer Engineering in Automation, University of Zagreb, Croatia; Faculty of Electrical Engineering and Computing, Department of Control and Computer Engineering in Automation, University of Zagreb, Croatia; Faculty of Electrical Engineering and Computing, Department of Control and Computer Engineering in Automation, University of Zagreb, Croatia","7th International Workshop on Advanced Motion Control. Proceedings (Cat. No.02TH8623)","7 Nov 2002","2002","","","462","466","One of the basic issues in the navigation of autonomous mobile robots is the obstacle avoidance task that is commonly achieved using a reactive control paradigm where a local mapping from perceived states to actions is acquired. A control strategy with learning capabilities in an unknown environment can be obtained using reinforcement learning where the learning agent is given only sparse reward information. This credit assignment problem includes both temporal and structural aspects. While the temporal credit assignment problem is solved using core elements of the reinforcement learning agent, solution of the structural credit assignment problem requires an appropriate internal state space representation of the environment. In this paper, a discrete coding of the input space using a neural network structure is presented as opposed to the commonly used continuous internal representation. This enables a faster and more efficient convergence of the reinforcement learning process.","","0-7803-7479-7","10.1109/AMC.2002.1026964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1026964","","Learning;Mobile robots;Control engineering computing;Automatic control;Neural networks;Fuzzy logic;Path planning;Robotics and automation;Navigation;State-space methods","mobile robots;collision avoidance;learning (artificial intelligence);state-space methods;neurocontrollers;control system synthesis","mobile robot obstacle avoidance;reinforcement learning approach;control design;temporal credit assignment problem;state space representation;neural network structure;continuous internal representation","","25","1","13","IEEE","7 Nov 2002","","","IEEE","IEEE Conferences"
"Research on autonomous maneuvering decision of UCAV based on deep reinforcement learning","Y. Zhang; W. Zu; Y. Gao; H. Chang","University of Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China","2018 Chinese Control And Decision Conference (CCDC)","9 Jul 2018","2018","","","230","235","In order to improve the intelligent level of UCAV in one-to-one air combat, an autonomous maneuvering decision algorithm based on deep reinforcement learning is proposed. UCAV learns strategies by sensing the environment, performing maneuvering actions, and getting feedback. In this way, we can avoid the limitations of existing theories and human operations. Firstly an environment is modeled to simulate the real-time situation of air combat. Then a situation assessment method based on Energy-Maneuverability theory is utilized to design the reward functions. Finally model based on deep reinforcement learning is created for UCAV to learn strategies to gain the advantage for the opponent.","1948-9447","978-1-5386-1244-6","10.1109/CCDC.2018.8407136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8407136","Air Combat;Autonomous Maneuvering Decision;Deep Reinforcement Learning","Machine learning;Neural networks;Turning;Atmospheric modeling;Learning (artificial intelligence);Automation;Electronic mail","aircraft control;autonomous aerial vehicles;control engineering computing;decision making;learning (artificial intelligence);military aircraft;mobile robots;path planning","UCAV;deep reinforcement learning;air combat;autonomous maneuvering decision algorithm;maneuvering actions;one-to-one air combat;feedback;Energy-Maneuverability theory;reward functions;unmanned combat aerial vehicle;intelligent level","","7","","18","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"Cooperative and Adaptive Optimal Output Regulation of Discrete-Time Multi-Agent Systems Using Reinforcement Learning","W. Gao; Y. Liu; A. Odekunle; Z. -P. Jiang; Y. Yu; P. Lu","Department of Electrical Engineering, Georgia Southern University, Statesboro, GA, USA; Chinese Academy of Science, Shenyang Institute of Automation, China; Department of Electrical Engineering, Georgia Southern University, Statesboro, GA, USA; Department of Electrical and Computer Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Automation Engineering, Nanchang University, 330029, China; School of Automation, Beijing Institute of Technology, China","2018 IEEE International Conference on Real-time Computing and Robotics (RCAR)","24 Jan 2019","2018","","","348","353","This paper proposes an original data-driven intelligent control solution to the cooperative output regulation problem of discrete-time multi-agent systems. Based on the combination of internal model principle and reinforcement learning, a distributed suboptimal controller is learned realtime via online input-state data collected from system trajectories. Rigorous theoretical analysis guarantees the convergence of the proposed algorithm and the asymptotic stability of the closed-loop system. Numerical results validate the effectiveness of the proposed control methodology.","","978-1-5386-6869-6","10.1109/RCAR.2018.8621852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8621852","","Multi-agent systems;Optimal control;Adaptation models;Reinforcement learning;Closed loop systems;Automation;Decentralized control","adaptive control;asymptotic stability;closed loop systems;discrete time systems;distributed control;intelligent control;learning (artificial intelligence);linear systems;multi-agent systems;suboptimal control","online input-state data;closed-loop system;adaptive optimal output regulation;discrete-time multiagent systems;reinforcement learning;internal model principle;distributed suboptimal controller;data-driven intelligent control","","2","","28","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"A modification of gradient policy in reinforcement learning procedure","M. Abas; T. Skripčák","Institute of Applied Informatics, Automation and Mathematics, Faculty of Materials Science and Technology in Trnava, Slovak University of Technology in Bratislava, Trnava, Slovak Republic; Institute of Applied Informatics, Automation and Mathematics, Faculty of Materials Science and Technology in Trnava, Slovak University of Technology in Bratislava, Trnava, Slovak Republic","2012 15th International Conference on Interactive Collaborative Learning (ICL)","7 Jan 2013","2012","","","1","2","The gradient of a scalar function is frequently used in various areas of mathematics. In informatics it can be used, for example, in the process of learning procedure of many control systems. The key observation is that gradient, if it is a non-zero vector, is a vector in the direction of greatest rate of the scalar function. In this contribution we show a method how to determine the direction(s) even if the gradient is zero vector. We show that this can be done with the knowledge which students have it their stage of study.","","978-1-4673-2427-4","10.1109/ICL.2012.6402200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6402200","neuron networks;control system;gradient policy;direction of greatest rate","Vectors;Learning;Educational institutions;Robots;Control systems;Informatics;Automation","gradient methods;intelligent robots;learning (artificial intelligence);vectors","gradient policy modification;scalar function gradient;mathematics;informatics;reinforcement learning procedure;control system;zero vector gradient;nonzero vector gradient;robot learning","","","","4","IEEE","7 Jan 2013","","","IEEE","IEEE Conferences"
"A Novel Reinforcement Learning-based Unsupervised Fault Detection for Industrial Manufacturing Systems","A. Acernese; A. Yerudkar; C. Del Vecchio","Department of Engineering, University of Sannio, Benevento, Italy; Department of Engineering, University of Sannio, Benevento, Italy; Department of Engineering, University of Sannio, Benevento, Italy","2022 American Control Conference (ACC)","5 Sep 2022","2022","","","2650","2655","With the advent of industry 4.0, machine learning (ML) methods have mainly been applied to design condition-based maintenance strategies to improve the detection of failure precursors and forecast degradation. However, in real-world scenarios, relevant features unraveling the actual machine conditions are often unknown, posing new challenges in addressing fault diagnosis problems. Moreover, ML approaches generally need ad-hoc feature extractions, involving the development of customized models for each case study. Finally, the early substitution of key mechanical components to avoid costly breakdowns challenge the collection of sizable significant data sets to train fault detection (FD) systems. To address these issues, this paper proposes a new unsupervised FD method based on double deep-Q network (DDQN) with prioritized experience replay (PER). We validate the effectiveness of the proposed algorithm on real steel plant data. Lastly, we compare the performance of our method with other FD methods showing its viability.","2378-5861","978-1-6654-5196-3","10.23919/ACC53348.2022.9867763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9867763","","Fault diagnosis;Degradation;Electric breakdown;Fault detection;Machine learning;Maintenance engineering;Feature extraction","condition monitoring;fault diagnosis;feature extraction;learning (artificial intelligence);maintenance engineering;manufacturing systems;production engineering computing","novel reinforcement;unsupervised fault detection;industrial manufacturing systems;machine learning methods;condition-based maintenance strategies;failure precursors;forecast degradation;actual machine conditions;fault diagnosis problems;ML approaches;ad-hoc feature extractions;customized models;early substitution;key mechanical components;costly breakdowns;sizable significant data sets;fault detection systems;unsupervised FD method;steel plant data;FD methods","","1","","20","","5 Sep 2022","","","IEEE","IEEE Conferences"
"Improving Build Quality in Laser Powder Bed Fusion Using High Dynamic Range Imaging and Model-Based Reinforcement Learning","C. Knaak; L. Masseling; E. Duong; P. Abels; A. Gillner","Fraunhofer-Institute for Laser Technology ILT, Aachen, Germany; Fraunhofer-Institute for Laser Technology ILT, Aachen, Germany; Fraunhofer-Institute for Laser Technology ILT, Aachen, Germany; Fraunhofer-Institute for Laser Technology ILT, Aachen, Germany; Fraunhofer-Institute for Laser Technology ILT, Aachen, Germany","IEEE Access","13 Apr 2021","2021","9","","55214","55231","In laser-based additive manufacturing (AM) of metal parts from powder bed, information about actual part quality obtained during build is essential for cost-efficient production and high product quality. Reliable and effective monitoring strategies for laser powder bed fusion (LPBF) therefore remain in high demand and are the subject of current research. To address this demand, a novel analysis approach using high dynamic range (HDR) optical imaging in combination with convolutional neural networks (CNN) is proposed for spatially resolved and layer-wise prediction of the surface roughness of LPBF parts. In a further step, the predicted surface roughness maps are used as a feedback signal for a reinforcement learning technique that employs a dynamics model to subsequently identify optimal process parameters under varying and uncertain conditions. The proposed approach ultimately combines the estimation of the local surface roughness based on image texture and model-based reinforcement learning to an in-situ optimization framework for LPBF processes. In addition, the relationship between the layer surface roughness of the part and the overall part density is discussed on the basis of experimental data, which also indicate the applicability of the proposed method in industrial environments. This preliminary study is a first step towards highly adaptive and intelligent machines in the field of automated laser powder bed fusion with the primary goals of reducing production costs and improving the environmental fingerprint as well as print quality.","2169-3536","","10.1109/ACCESS.2021.3067302","European Union’s Horizon 2020 research and innovation programme(grant numbers:825030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9381862","Laser powder bed fusion;convolutional neural networks;model-based reinforcement learning;high dynamic range imaging;surface roughness optimization","Surface roughness;Rough surfaces;Surface treatment;Surface emitting lasers;Surface topography;Surface texture;Optimization","convolutional neural nets;image texture;manufacturing processes;optimisation;product quality;production engineering computing;rapid prototyping (industrial);surface roughness","in-situ optimization framework;surface roughness maps;print quality;production costs;automated laser powder bed fusion;intelligent machines;highly adaptive machines;layer surface roughness;LPBF processes;image texture;dynamics model;reinforcement learning technique;LPBF parts;layer-wise prediction;high dynamic range optical imaging;monitoring strategies;product quality;cost-efficient production;part quality;metal parts;laser-based additive manufacturing;model-based reinforcement learning;high dynamic range imaging;build quality","","22","","76","CCBY","19 Mar 2021","","","IEEE","IEEE Journals"
"Configuration-Adaptive Wireless Visual Sensing System With Deep Reinforcement Learning","S. Zhou; D. Van Le; R. Tan; J. Q. Yang; D. Ho","HP-NTU Digital Manufacturing Corporate Lab, Nanyang Technological University, Singapore; HP-NTU Digital Manufacturing Corporate Lab, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; HP-NTU Digital Manufacturing Corporate Lab, Nanyang Technological University, Singapore; HP Inc., Palo Alto, CA, USA","IEEE Transactions on Mobile Computing","4 Aug 2023","2023","22","9","5078","5091","Visual sensing has been increasingly employed in various industrial applications including manufacturing process monitoring and worker safety monitoring. This paper presents the design and implementation of a wireless camera system, namely, EFCam, which uses low-power wireless communications and edge-fog computing to achieve cordless and energy-efficient visual sensing. The camera performs image pre-processing and offloads the data to a resourceful fog node for advanced processing using deep models. EFCam admits dynamic configurations of several parameters that form a configuration space. It aims to adapt the configuration to maintain desired visual sensing performance of the deep model at the fog node with minimum energy consumption of the camera in image capture, pre-processing, and data communications, under dynamic variations of the monitored process, the application requirement, and wireless channel conditions. However, the adaptation is challenging due to the complex relationships among the involved factors. To address the complexity, we apply deep reinforcement learning to learn the optimal adaptation policy when a fog node supports one or more wireless cameras. Extensive evaluation based on trace-driven simulations and experiments show that EFCam complies with the accuracy and latency requirements with lower energy consumption for a real industrial product object tracking application, compared with five baseline approaches incorporating hysteresis-based and event-triggered adaptation.","1558-0660","","10.1109/TMC.2022.3175182","Industry Alignment Fund – Industry Collaboration(grant numbers:RIE2020); HP Inc.; HP-NTU Digital Manufacturing Corporate Lab.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9775607","Wireless visual sensing;fog computing;computation offloading;deep reinforcement learning","Cameras;Wireless communication;Wireless sensor networks;Sensors;Visualization;Ad hoc networks;Energy consumption","cameras;deep learning (artificial intelligence);distributed processing;energy conservation;energy consumption;feature extraction;image capture;object tracking;process monitoring;radiocommunication;reinforcement learning;telecommunication power management;wireless channels","advanced processing;application requirement;configuration space;configuration-adaptive wireless visual sensing system;cordless energy-efficient;data communications;deep model;deep reinforcement learning;dynamic configurations;edge-fog computing;EFCam complies;event-triggered adaptation;image capture;industrial applications;industrial product object tracking application;lower energy consumption;manufacturing process monitoring;minimum energy consumption;monitored process;optimal adaptation policy;resourceful fog node;visual sensing performance;wireless camera system;wireless cameras;wireless channel conditions;worker safety monitoring","","3","","31","IEEE","16 May 2022","","","IEEE","IEEE Journals"
"Multi-Agent and Cooperative Deep Reinforcement Learning for Scalable Network Automation in Multi-Domain SD-EONs","B. Li; R. Zhang; X. Tian; Z. Zhu","School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Research Center for Industrial Internet, Zhejiang Lab, Hangzhou, Zhejiang, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China","IEEE Transactions on Network and Service Management","9 Dec 2021","2021","18","4","4801","4813","The service provisioning in multi-domain software-defined elastic optical networks (SD-EONs) is an interesting but difficult problem to tackle, because the basic problem of lightpath provisioning, i.e., the routing and spectrum assignment (RSA), is  $\mathcal {NP}$ -hard, and each domain is owned and operated by a different carrier. Therefore, even though numerous RSA heuristics have been proposed, there does not exist a universal winner that can always achieve the lowest blocking probability in all the scenarios of a multi-domain SD-EON. This motivates us to revisit the inter-domain provisioning problem in this paper by leveraging deep reinforcement learning (DRL). Specifically, we propose DeepCoop, which is an inter-domain service framework that uses multiple cooperative DRL agents to achieve scalable network automation in a multi-domain SD-EON. DeepCoop employs a DRL agent in each domain to optimize intra-domain service provisioning, while a domain-level path computation element (PCE) is introduced to obtain the sequence of the domains to go through for each lightpath request. By sharing a restricted amount of information among each other, the DRL agents can make their decisions distributedly. To ensure scalability and universality, we design the action space of each DRL agent based on well-known RSA heuristics, and architect the agents based on the soft actor-critic (SAC) scenario. We run extensive simulations to evaluate DeepCoop, and the results show that DeepCoop can adapt to the dynamic environment in a multi-domain SD-EON to always select the best RSA heuristic for minimizing blocking probability, and it outperforms the existing algorithms on inter-domain provisioning in various scenarios. Moreover, we verify that the distributed training implemented in DeepCoop ensures its universality and scalability (i.e., its training and operation do not depend on the topology of the SD-EON).","1932-4537","","10.1109/TNSM.2021.3102621","NSFC(grant numbers:61871357); Zhejiang Lab Research Fund(grant numbers:2019LE0AB01); SPR Program of CAS(grant numbers:XDC02070300); Fundamental Funds for Central Universities(grant numbers:WK3500000006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507559","Multi-agent system;deep reinforcement learning (DRL);software-defined networking (SDN);elastic optical networks (EONs);multi-domain;network automation","Optical fiber networks;Topology;Scalability;Routing;Heuristic algorithms;Automation;Training","optical fibre networks;optimisation;probability;reinforcement learning;software defined networking;telecommunication network routing;telecommunication network topology","scalable network automation;multidomain SD-EON;DeepCoop;DRL agent;intra-domain service provisioning;domain-level path computation element;multidomain software-defined elastic optical networks;inter-domain provisioning problem;inter-domain service framework;routing and spectrum assignment;NP-hard;deep reinforcement learning","","5","","58","IEEE","5 Aug 2021","","","IEEE","IEEE Journals"
"Nonlinear Approximate Optimal Control Based on Integral Reinforcement Learning","F. Tian; F. Liu","Ministry of education, Key Laboratory of Advanced Process Control for Light Industry, Jiangnan University, Wuxi; Ministry of education, Key Laboratory of Advanced Process Control for Light Industry, Jiangnan University, Wuxi","2022 IEEE 11th Data Driven Control and Learning Systems Conference (DDCLS)","26 Aug 2022","2022","","","1273","1278","This paper aims to find the optimal control solution of an affine nonlinear continuous-time system with unknown input dynamic. Based on Critic-Actor neural network, an online integral reinforcement learning algorithm has been proposed. The algorithm solves the Bellman equation online, while Critic neural network is used to approximate the value function and Actor neural network is used for policy improvement. The policy evaluation and policy improvement of integral reinforcement learning are performed alternately until the performance of control systems no longer improves. By using Lyapunov function theory, all the weights of Critic-Actor neural network and the states of the system are guaranteed to be locally uniformly ultimately bounded. The simulation results show the effectiveness of the developed method.","2767-9861","978-1-6654-9675-9","10.1109/DDCLS55054.2022.9858347","National Natural Science Foundation (NNSF) of China(grant numbers:61833007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858347","Integral reinforcement learning (IRL);optimal control;neural network;IRL-Bellman equation","Heuristic algorithms;Simulation;Neural networks;Optimal control;Reinforcement learning;Approximation algorithms;Cost function","adaptive control;approximation theory;closed loop systems;continuous time systems;control system analysis computing;Lyapunov methods;neurocontrollers;nonlinear control systems;optimal control;reinforcement learning","nonlinear approximate optimal control;optimal control solution;affine nonlinear continuous-time system;unknown input dynamic;online integral reinforcement learning algorithm;Bellman equation online;critic neural network;value function;policy improvement;policy evaluation;control systems;critic-actor neural network","","1","","17","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Multi-connectivity Resource Allocation in Factory Automation Systems","M. Farzanullah; H. V. Vu; T. Le-Ngoc","Department of Electrical & Computer Engineering, McGill University, Montréal, QC, Canada; Huawei Technologies Canada, Ottawa, ON, Canada; Department of Electrical & Computer Engineering, McGill University, Montréal, QC, Canada","2022 IEEE 96th Vehicular Technology Conference (VTC2022-Fall)","18 Jan 2023","2022","","","1","5","We propose joint user association, channel assignment and power allocation for mobile robot Ultra-Reliable and Low Latency Communications (URLLC) based on multi-connectivity and reinforcement learning. The mobile robots require control messages from the central guidance system at regular intervals. We use a two-phase communication scheme where robots can form multiple clusters. The robots in a cluster are close to each other and can have reliable Device-to-Device (D2D) communications. In Phase I, the APs transmit the combined payload of a cluster to the cluster leader within a latency constraint. The cluster leader broadcasts this message to its members in Phase II. We develop a distributed Multi-Agent Reinforcement Learning (MARL) algorithm for joint user association and resource allocation (RA) for Phase I. The cluster leaders use their local Channel State Information (CSI) to decide the APs for connection along with the sub-band and power level. The cluster leaders utilize multi-connectivity to connect to multiple APs to increase their reliability. The objective is to maximize the successful payload delivery probability for all robots. Illustrative simulation results indicate that the proposed scheme can approach the performance of the centralized algorithm and offer a substantial gain in reliability as compared to single-connectivity (when cluster leaders are able to connect to 1 AP).","2577-2465","978-1-6654-5468-1","10.1109/VTC2022-Fall57202.2022.10012900","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10012900","Multi-connectivity;Factory automation;Resource allocation;Reinforcement Learning","Vehicular and wireless technologies;Clustering algorithms;Reinforcement learning;Channel allocation;Ultra reliable low latency communication;Resource management;Reliability","channel allocation;factory automation;mobile radio;mobile robots;multi-agent systems;reinforcement learning;resource allocation;telecommunication computing;telecommunication network performance;telecommunication network reliability","central guidance system;channel assignment;cluster leader;Device-to-Device communications;distributed MultiAgent Reinforcement Learning algorithm;factory automation systems;joint user association;local Channel State Information;MARL;mobile robots;Multiconnectivity resource allocation;power allocation;single-connectivity;two-phase communication scheme;Ultra-Reliable and Low Latency Communications;URLLC","","","","17","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Tracking Control for a Class of Discrete-Time Systems With Actuator Fault","Y. Liu; Z. Wang","State Key Laboratory of Synthetical Automation for Process Industries and the College of Information Science and Engineering, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries and the College of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Transactions on Circuits and Systems II: Express Briefs","1 Jun 2022","2022","69","6","2827","2831","Tracking control problem is a common problem and widely used in many fields. In this brief, the data-based tracking control with actuator fault is studied. To deal with the fault, many detection mechanisms and fault-tolerant tracking (FTT) controllers have been studied. However, the existing detection mechanisms cannot detect the systems with non-zero initial value. Furthermore, the existing FTT controllers require some designed parameters to obtain the fault. To solve above problems, a detection mechanism based on expanded time horizon and FTT controller based on reinforcement learning are proposed in this brief. The proposed detection mechanism is appropriate for arbitrary initial value, which consists of the initial value and past data. Besides, a nested calculation method is presented to obtain the fault information of FTT controller, which only uses the system data. Hence, the proposed FTT controller avoids the design of additional parameters for fault. Finally, the effectiveness of proposed methods is verified by simulation example.","1558-3791","","10.1109/TCSII.2021.3131360","National Natural Science Foundation of China(grant numbers:61973070); Liaoning Revitalization Talents Program(grant numbers:XLYC1802010); SAPI Fundamental Research Funds(grant numbers:2018ZCX22); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9628163","Fault-tolerant tracking;reinforcement learning;actuator fault;detection mechanism;expanded time horizon;nested calculation","Circuit faults;Mathematical models;Actuators;Fault detection;Reinforcement learning;Circuits and systems;Time measurement","actuators;control system synthesis;discrete time systems;fault tolerant control;nonlinear control systems;reinforcement learning;tracking","discrete-time systems;actuator fault;control problem;detection mechanism;nonzero initial value;expanded time horizon;FTT controller;fault information;system data;reinforcement learning-based tracking control","","","","26","IEEE","30 Nov 2021","","","IEEE","IEEE Journals"
"Multiagent Deep Reinforcement Learning for Joint Multichannel Access and Task Offloading of Mobile-Edge Computing in Industry 4.0","Z. Cao; P. Zhou; R. Li; S. Huang; D. Wu","School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Hubei Engineering Research Center on Big Data Security, School of Cyber Science and Engineering, Huazhong University of Science & Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Physical Science and Technology, Central China Normal University, Wuhan, China; Department of Electrical and Computer Engineering, University of Florida, Gainesville, USA","IEEE Internet of Things Journal","10 Jul 2020","2020","7","7","6201","6213","Industry 4.0 aims to create a modern industrial system by introducing technologies, such as cloud computing, intelligent robotics, and wireless sensor networks. In this article, we consider the multichannel access and task offloading problem in mobile-edge computing (MEC)-enabled industry 4.0 and describe this problem in multiagent environment. To solve this problem, we propose a novel multiagent deep reinforcement learning (MADRL) scheme. The solution enables edge devices (EDs) to cooperate with each other, which can significantly reduce the computation delay and improve the channel access success rate. Extensive simulation results with different system parameters reveal that the proposed scheme could reduce computation delay by 33.38% and increase the channel access success rate by 14.88% and channel utilization by 3.24% compared to the traditional single-agent reinforcement learning method.","2327-4662","","10.1109/JIOT.2020.2968951","National Basic Research Program of China (973 Program)(grant numbers:2016YFB0800402,2016QY01W0202); National Natural Science Foundation of China(grant numbers:61972448,U1836204,U1936108,61572221,61433006,U1401258); Major Projects of the National Social Science Foundation(grant numbers:16ZDA092); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9037194","Machine-to-machine (M2M) communications;mobile-edge computing (MEC);multiagent deep reinforcement learning (MADRL);task offloading","Task analysis;Reinforcement learning;Heuristic algorithms;Servers;Industries;Edge computing","learning (artificial intelligence);mobile computing;multi-access systems;multi-agent systems;multiuser channels;neural nets;production engineering computing","channel access;single-agent reinforcement learning method;joint multichannel access;introducing technologies;cloud computing;intelligent robotics;wireless sensor networks;mobile-edge computing-enabled industry 4;edge devices;multiagent deep reinforcement learning scheme;MADRL scheme","","82","","43","IEEE","16 Mar 2020","","","IEEE","IEEE Journals"
"Optimizing Federated Learning With Deep Reinforcement Learning for Digital Twin Empowered Industrial IoT","W. Yang; W. Xiang; Y. Yang; P. Cheng","School of Automation and Information Engineering, Xi’an University of Technology, Xi’an, China; School of Computing, Engineering and Mathematical Sciences, La Trobe University, Melbourne, Australia; School of Automation and Information Engineering, Xi’an University of Technology, Xi’an, China; School of Computing, Engineering and Mathematical Sciences, La Trobe University, Melbourne, VIC, Australia","IEEE Transactions on Industrial Informatics","15 Dec 2022","2023","19","2","1884","1893","The accelerated development of the Industrial Internet of Things (IIoT) is catalyzing the digitalization of industrial production to achieve Industry 4.0. In this article, we propose a novel digital twin (DT) empowered IIoT (DTEI) architecture, in which DTs capture the properties of industrial devices for real-time processing and intelligent decision making. To alleviate data transmission burden and privacy leakage, we aim to optimize federated learning (FL) to construct the DTEI model. Specifically, to cope with the heterogeneity of IIoT devices, we develop the DTEI-assisted deep reinforcement learning method for the selection process of IIoT devices in FL, especially for selecting IIoT devices with high utility values. Furthermore, we propose an asynchronous FL scheme to address the discrete effects caused by heterogeneous IIoT devices. Experimental results show that our proposed scheme features faster convergence and higher training accuracy compared to the benchmark.","1941-0050","","10.1109/TII.2022.3183465","Shaanxi Innovation Capability Support project(grant numbers:2021TD-25); Natural Science Basic Research Program of Shaanxi(grant numbers:2021JQ-478); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815106","Deep reinforcement learning (DRL);digital twin (DT);federated learning (FL);Industrial Internet of Things (IIoT);learning efficiency;real time","Industrial Internet of Things;Training;Data models;Real-time systems;Digital twins;Collaborative work;Task analysis","data privacy;decision making;Internet of Things;learning (artificial intelligence);production engineering computing","accelerated development;asynchronous FL scheme;data transmission burden;digital twin;digitalization;DTEI model;DTEI-assisted deep reinforcement learning method;federated learning;heterogeneous IIoT devices;IIoT architecture;industrial devices;Industrial IoT;industrial production;intelligent decision making;privacy leakage;real-time processing;selection process","","19","","28","CCBY","4 Jul 2022","","","IEEE","IEEE Journals"
"Investigation on Transient Stability Enhancement of Multi-VSG System Incorporating Resistive SFCLs Based on Deep Reinforcement Learning","L. Chen; J. Tang; X. Qiao; H. Chen; J. Zhu; Y. Jiang; Z. Zhao; R. Hu; X. Deng","Hubei Engineering and Technology Research Center for AC/DC Intelligent Distribution Network, School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; Hubei Engineering and Technology Research Center for AC/DC Intelligent Distribution Network, School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; Hubei Engineering and Technology Research Center for AC/DC Intelligent Distribution Network, School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; Hubei Engineering and Technology Research Center for AC/DC Intelligent Distribution Network, School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; China Electric Power Research Institute, Beijing, China; Hubei Engineering and Technology Research Center for AC/DC Intelligent Distribution Network, School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; Hubei Engineering and Technology Research Center for AC/DC Intelligent Distribution Network, School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; Hubei Engineering and Technology Research Center for AC/DC Intelligent Distribution Network, School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; Hubei Engineering and Technology Research Center for AC/DC Intelligent Distribution Network, School of Electrical Engineering and Automation, Wuhan University, Wuhan, China","IEEE Transactions on Industry Applications","","2023","PP","99","1","13","Virtual synchronous generator (VSG) strategy is an effective means for renewable energy sources to be connected to power grids. Meanwhile, a greater focus should be on the multi-VSG system's (MVS) transient stability issue. Regarding the MVS with resistive superconducting fault current limiters (R-SFCLs), this paper proposes a control method based on deep reinforcement learning (DRL) agent for increasing the transient stability. Firstly, the theoretical model of the MVS incorporating R-SFCLs is established, and R-SFCLs are used to limit the fault current in VSGs and keep the fault ride-through (FRT) operation. Then, the working mechanism of the proposed method is elaborated, by designing the Markov decision process (MDP) model of transient control, and applying the improved softmax deep deterministic policy gradients (SD2) algorithm to train the DRL agent. The advanced time series feature extraction network (TSFEN) based on the convolutional neural network (CNN) and gate recurrent unit (GRU) is suggested to enhance the actor network and critic network of the SD2. A detailed simulation model is created using MATLAB, and a comparison with the traditional VSG control, only R-SFCL, deep deterministic policy gradients (DDPG) algorithm, and improved power loop scheme is conducted. From multiple cases, the proposed approach can satisfactorily boost the transient stability of the MVS, and the generalization ability of the DRL agent under different untrained scenarios is validated. The proposed method's validity and suitability are well-confirmed.","1939-9367","","10.1109/TIA.2023.3321264","National Natural Science Foundation of China(grant numbers:51877154); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10268991","Deep reinforcement learning;multi-vsg system;resistive superconducting fault current limiter;transient stability;virtual synchronous generator","Power system stability;Transient analysis;Stability criteria;Mathematical models;Damping;Synchronous generators;Reinforcement learning","","","","","","","IEEE","2 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Value-Approximation based Deep Reinforcement Learning Techniques: An Overview","M. Sewak; S. K. Sahay; H. Rathore","Department of CS & IS, Goa Campus BITS, Pilani, Goa, India; Department of CS & IS, Goa Campus BITS, Pilani, Goa, India; Department of CS & IS, Goa Campus BITS, Pilani, Goa, India","2020 IEEE 5th International Conference on Computing Communication and Automation (ICCCA)","10 Nov 2020","2020","","","379","384","Deep Reinforcement Learning (DRL) combines the power of Deep Leaning and Reinforcement learning, and has started gaining a lot of attraction in various domains. Also, it is empowering the artificial intelligence based agents which could surpass human-level performance even the tasks which was earlier thought to be best performed by humans only. In Industry 4.0, DRL based agents are enabling applications ranging from autonomous fleets and automatic process control to dynamic scheduling for complex production lines. In this context, the study of value-approximation based DRL techniques gains significance as these techniques are instrumental in enabling the concept of General Artificial Intelligence. Therefore, to understand the value-based DRL and to apply it optimally, we present an overview of the value-approximation based DRL techniques and explained how DRL built on Markov Decision Process and the Bellman equation can further improve the general applicability of the DQN model for different applications and achieve better results over the existing ones. We also explain the setup of a Reinforcement learning problem and describe its environment, state, reward-function and agents.","2642-7354","978-1-7281-6324-6","10.1109/ICCCA49541.2020.9250787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9250787","Deep Reinforcement Learning;Value Approximation;Markov Decision Process;Bellman Equation;Deep Q Network","Process control;Reinforcement learning;Production;Markov processes;Mathematical model;Artificial intelligence;Task analysis","approximation theory;decision theory;dynamic scheduling;learning (artificial intelligence);Markov processes;multi-agent systems;process control;production engineering computing","artificial intelligence based agents;human-level performance;DRL based agents;value-approximation based DRL techniques;general artificial intelligence;deep reinforcement learning techniques;Markov decision process;Bellman equation;Industry 4.0;autonomous fleets;automatic process control;dynamic scheduling;complex production lines;DQN model","","10","","17","IEEE","10 Nov 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning based Actor-Critic Framework for Decision-Making Actions in Production Scheduling","A. K. Elsayed; E. K. Elsayed; K. A. Eldahshan","Department of Information Systems Higher Institute for Specific Studies (HISS) Department of Mathematics, Faculty of Science (Girls), AL-Azhar University, Cairo, Egypt; School of Computers Science Canadian International College(CIC) Department of Mathematics, Faculty of Science(Girls), AL-Azhar University, Cairo, Egypt; Department of Mathematics, Faculty of Science, AL-Azhar University, Cairo, Egypt","2021 Tenth International Conference on Intelligent Computing and Information Systems (ICICIS)","3 Feb 2022","2021","","","32","40","Smart manufacturers system is faced with the planning and scheduling production challenge to achieve high performance. This research regards flexible job shop scheduling as a sequential decision-making problem and deep reinforcement learning-based Actor-Critic framework is proposed to cope with this problem. The proposed model can extract features from the input data using a convolutional network. In each optimal solution, we regard each operation of a job as a decision that contains information; and each decision as a function of five times in job processing which is classified using information from dispatch rules. For the learning by steps, we run a one-step actor-critic and improve the returns by repeated more steps. We compare different learning rates and discount factors with the generated returns to show the performance of decisions taken by the learning agent. Finally, we experiment proposed model on a case study and benchmark dataset with different values of random seeds to ensure the proposed model framework is more effective over a long time. The results indicate that our proposed model has more effective to solve Flexible job shop scheduling problems with big data set that has more than 100 jobs and 100 machines.","","978-1-6654-4076-9","10.1109/ICICIS52592.2021.9694207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9694207","Job Shop Scheduling;Decision Making;Deep Reinforcement Learning;Actor-Critic Network","Job shop scheduling;Processor scheduling;Decision making;Training data;Production;Benchmark testing;Feature extraction","decision making;job shop scheduling;learning (artificial intelligence);manufacturing systems;scheduling","job processing;different learning rates;learning agent;Flexible job shop scheduling problems;decision-making actions;smart manufacturers system;planning;scheduling production challenge;sequential decision-making problem;deep reinforcement learning-based Actor-Critic framework","","1","","20","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"The Vector Control Scheme for Amphibious Spherical Robots Based on Reinforcement Learning","H. Yin; S. Guo; L. Shi; M. Zhou; X. Hou; Z. Li; D. Xia","Key Laboratory of Convergence Medical Engineering System and Healthcare Technology, the Ministry of Industry and Information Technology, School of Life Science, Beijing Institute of Technology, Beijing, China; Key Laboratory of Convergence Medical Engineering System and Healthcare Technology, the Ministry of Industry and Information Technology, School of Life Science, Beijing Institute of Technology, Beijing, China; Key Laboratory of Convergence Medical Engineering System and Healthcare Technology, the Ministry of Industry and Information Technology, School of Life Science, Beijing Institute of Technology, Beijing, China; Key Laboratory of Convergence Medical Engineering System and Healthcare Technology, the Ministry of Industry and Information Technology, School of Life Science, Beijing Institute of Technology, Beijing, China; Key Laboratory of Convergence Medical Engineering System and Healthcare Technology, the Ministry of Industry and Information Technology, School of Life Science, Beijing Institute of Technology, Beijing, China; Key Laboratory of Convergence Medical Engineering System and Healthcare Technology, the Ministry of Industry and Information Technology, School of Life Science, Beijing Institute of Technology, Beijing, China; Key Laboratory of Convergence Medical Engineering System and Healthcare Technology, the Ministry of Industry and Information Technology, School of Life Science, Beijing Institute of Technology, Beijing, China","2021 IEEE International Conference on Mechatronics and Automation (ICMA)","27 Aug 2021","2021","","","594","599","Due to variable underwater working conditions and unfavorable environments, it is difficult to design a controller suitable for underwater robots. This paper uses the adaptive ability of reinforcement learning to propose a two-layer network framework based on reinforcement learning to realize the control of amphibious spherical robots. The upper planning layer mainly plans the total torque of the robot at each moment according to the desired position and speed. The lower control layer mainly configures the parameters of the four machine legs according to the planning instructions of the upper planning layer. Through the cooperation of the planning layer and the control layer, the adaptive motion control of the amphibious spherical robot can finally be realized. Finally, the proposed scheme was verified on a simulated amphibious spherical robot.","2152-744X","978-1-6654-4101-8","10.1109/ICMA52036.2021.9512624","National Natural Science Foundation of China(grant numbers:61773064,61503028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512624","Reinforcement learning;Amphibious spherical robot;Motion control","Legged locomotion;Torque;Mechatronics;Reinforcement learning;Unmanned underwater vehicles;Stability analysis;Planning","adaptive control;control engineering computing;control system synthesis;learning (artificial intelligence);legged locomotion;motion control;position control","vector control;reinforcement learning;variable underwater working conditions;underwater robots;two-layer network framework;planning instructions;adaptive motion control;simulated amphibious spherical robot;machine legs","","1","","18","IEEE","27 Aug 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Maintenance Scheduling for Resource Constrained Flow Line System","Z. Ling; X. Wang; F. Qu","School of Safety Engineering, Shenyang Aerospace University, Shenyang, China; School of Safety Engineering, Shenyang Aerospace University, Shenyang, China; School of Safety Engineering, Shenyang Aerospace University, Shenyang, China","2018 IEEE 4th International Conference on Control Science and Systems Engineering (ICCSSE)","30 May 2019","2018","","","364","369","This paper considers the problem of condition-based maintenance for a two-machine flow line which consists of an upstream production machine, a downstream production machine and an immediate buffer with maintenance resource constraints. Due to the constrained maintenance resource, the maintenance actions are imperfect, and they are purposefully initiated when the machines are operating on a deteriorated quality states represented by multiple decreasing yield levels. A continuous-time semi-Markov decision processes model is formulated to describe the machine degradation processes. The distributed model-free average reward reinforcement learning algorithm, semi-Markov average reward technique algorithm is used to implement within each machine and its adjacent buffer and determine the optimal maintenance policy of the total system. The numerical results show that the approach can converge to the approximate optimal solution. Also, the effects of the variation of some parameters on the optimal policy and on the average cost rate are given.","","978-1-5386-7887-9","10.1109/CCSSE.2018.8724807","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8724807","degradation quality states;semi-Markov decision process;constrained resource;multi-agent reinforcement learning;condition-based maintenance","Preventive maintenance;Reinforcement learning;Heuristic algorithms;Job shop scheduling;Adaptation models","decision theory;learning (artificial intelligence);maintenance engineering;Markov processes;production equipment;scheduling","reinforcement learning-based maintenance scheduling;resource constrained flow line system;two-machine flow line;upstream production machine;downstream production machine;semiMarkov average reward technique algorithm;machine degradation process;numerical analysis","","4","","11","IEEE","30 May 2019","","","IEEE","IEEE Conferences"
"Integration of Deep Reinforcement Learning and Discrete-Event Simulation for Real-Time Scheduling of a Flexible Job Shop Production","S. Lang; F. Behrendt; N. Lanzerath; T. Reggelin; M. Müller","Fraunhofer Institute for Factory Operation and Automation (IFF), Magdeburg, GERMANY; Fraunhofer Institute for Factory Operation and Automation (IFF), Magdeburg, GERMANY; Institute of Logistics and Material Handling Systems (ILM) Otto von Guericke University Magdeburg, Magdeburg, GERMANY; Institute of Logistics and Material Handling Systems (ILM) Otto von Guericke University Magdeburg, Magdeburg, GERMANY; Institute of Logistics and Material Handling Systems (ILM) Otto von Guericke University Magdeburg, Magdeburg, GERMANY","2020 Winter Simulation Conference (WSC)","29 Mar 2021","2020","","","3057","3068","The following paper presents the application of Deep Q-Networks (DQN) for solving a flexible job shop problem with integrated process planning. DQN is a deep reinforcement learning algorithm, which aims to train an agent to perform a specific task. In particular, we train two DQN agents in connection with a discrete-event simulation model of the problem, where one agent is responsible for the selection of operation sequences, while the other allocates jobs to machines. We compare the performance of DQN with the GRASP metaheuristic. After less than one hour of training, DQN generates schedules providing a lower makespan and total tardiness as the GRASP algorithm. Our first investigations reveal that DQN seems to generalize the training data to other problem cases. Once trained, the prediction and evaluation of new production schedules requires less than 0.2 seconds.","1558-4305","978-1-7281-9499-8","10.1109/WSC48552.2020.9383997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383997","","Training;Schedules;Process planning;Training data;Reinforcement learning;Real-time systems;Task analysis","deep learning (artificial intelligence);discrete event simulation;job shop scheduling;metaheuristics;multi-agent systems;process planning","real-time scheduling;flexible job shop production;deep Q-networks;flexible job shop problem;process planning;DQN agents;discrete-event simulation;GRASP metaheuristic;production schedules;deep reinforcement learning;operation sequences;job allocation;makespan;total tardiness","","20","","33","IEEE","29 Mar 2021","","","IEEE","IEEE Conferences"
"Research on Distributed Intelligent Protection Strategy of distribution network based on Reinforcement Learning","S. Cui; P. Zeng; E. Hu","State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; State Grid Shaoxing Power Supply Company, Shaoxing, China","2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)","3 Feb 2022","2021","2","","1069","1073","In the context of building a new power system with new energy as the main part, the operation mode of distribution network is more complex, and the traditional distribution network protection strategy can hardly meet the demand, so this paper proposes a distributed intelligent protection strategy for distribution network based on reinforcement learning. This paper analyzes the impact of new energy resources access on distribution network relay protection, and presents an intelligent protection algorithm model based on deep reinforcement learning. The structure of intelligent protection device is designed, and the distribution network distributed protection scheme based on intelligent protection device is proposed. Finally, the algorithm training process is given, and the experimental environment is built.","","978-1-6654-2877-4","10.1109/ICIBA52610.2021.9688080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688080","New power system;distribution network protection;deep reinforcement learning;intelligent protection device;distributed protection strategy","Training;Uncertainty;Power supplies;Protective relaying;Distribution networks;Reinforcement learning;Power system stability","deep learning (artificial intelligence);power distribution protection;power engineering computing;reinforcement learning;relay protection;renewable energy sources","intelligent protection device;distributed intelligent protection strategy;distribution network relay protection;intelligent protection algorithm model;deep reinforcement learning;distribution network protection strategy;algorithm training process;energy resources","","","","13","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
