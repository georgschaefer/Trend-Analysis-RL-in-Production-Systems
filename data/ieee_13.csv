"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Invited: Efficient Reinforcement Learning for Automating Human Decision-Making in SoC Design","S. Sadasivam; J. Lee; Z. Chen; R. Jain",Qualcomm Technologies Inc.; Qualcomm Technologies Inc.; Carnegie Mellon University; Qualcomm Technologies Inc.,"2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)","20 Sep 2018","2018","","","1","6","The exponential growth in PVT corners due to Moore's law scaling, and the increasing demand for consumer applications and longer battery life in mobile devices, has ushered in significant cost and power-related challenges for designing and productizing mobile chips within a predictable schedule. Two main reasons for this are the reliance on human decision-making to achieve the desired performance within the target area and power budget, and significant increases in complexity of the human decision-making space. The problem is that to-date human design experience has not been replaced by design automation tools, and tasks requiring experience of past designs are still being performed manually.In this paper we investigate how machine learning may be applied to develop tools that learn from experience just like human designers, thus automating tasks that still require human intervention. The potential advantage of the machine learning approach is the ability to scale with increasing complexity and therefore hold the design-time constant with same manpower.Reinforcement Learning (RL) is a machine learning technique that allows us to mimic a human designers' ability to learn from experience and automate human decision-making, without loss in quality of the design, while making the design time independent of the complexity. In this paper we show how manual design tasks can be abstracted as RL problems. Based on the experience with applying RL to one of these problems, we show that RL can automatically achieve results similar to human designs, but in a predictable schedule. However, a major drawback is that the RL solution can require a prohibitively large number of iterations for training. If efficient training techniques can be developed for RL, it holds great promise to automate tasks requiring human experience. In this paper we present a Bayesian Optimization technique for reducing the RL training time.","","978-1-5386-4114-9","10.1109/DAC.2018.8465874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8465874","","Tools;Task analysis;Learning (artificial intelligence);Tuning;Games;Decision making;Design automation","Bayes methods;learning (artificial intelligence);optimisation;system-on-chip","automating human decision-making;SoC design;Moore's law scaling;power-related challenges;human decision-making space;design automation tools;human designers;human intervention;machine learning approach;RL;machine learning technique;design time;human designs;automate tasks;human experience;human decision-making;human design experience","","","","18","IEEE","20 Sep 2018","","","IEEE","IEEE Conferences"
"RLBrowse: Generating Realistic Packet Traces with Reinforcement Learning","A. Griessel; M. Stephan; M. Mieth; W. Kellerer; P. Krämer","Technical University of Munich, Munich, Germany; Technical University of Munich, Munich, Germany; Ipoque GmbH - A Rohde&Schwarz Company, Leipzig, Germany; Technical University of Munich, Munich, Germany; Technical University of Munich, Munich, Germany","NOMS 2022-2022 IEEE/IFIP Network Operations and Management Symposium","9 Jun 2022","2022","","","1","6","Automated Web Browsing tools, such as Selenium and headless browsers, are used to collect traffic traces from networked applications, with which statistical models describing the traffic are obtained. However, we show that traces from Selenium and headless browsers have markedly different traffic characteristics than human generated traces, with potential impact on the quality of the obtained models. To overcome this limitation, we propose RLBrowse, an automated web automation framework that imitates human browsing habits by separating web automation from the browser using reinforcement learning. By separating the browser and automation tool, RLBrowse improves on 9 out of the 13 traffic trace features tested. The distribution of packet sizes in a trace improves the most, with a nearly 400 % improvement. We test RLBrowse by collecting a corpus of network packet traces on a set of human-navigated website browsing sessions, and by RLBrowse and Selenium. In the subsequent analysis, we identify key differences in the resulting packet traces.","2374-9709","978-1-6654-0601-7","10.1109/NOMS54207.2022.9789851","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9789851","","Visualization;Automation;Navigation;Reinforcement learning;Data collection;Data models;Browsers","Internet;online front-ends;reinforcement learning","RLBrowse;reinforcement learning;Selenium;headless browsers;traffic traces;statistical models;traffic characteristics;human generated traces;human browsing habits;browser;automation tool;packet sizes;network packet traces;human-navigated Web site browsing sessions;traffic trace features;automated Web automation framework;Web browsing tools;realistic packet traces","","","","28","IEEE","9 Jun 2022","","","IEEE","IEEE Conferences"
"Encoder-Decoder Neural Network Architecture for solving Job Shop Scheduling Problems using Reinforcement Learning","R. Magalhães; M. Martins; S. Vieira; F. Santos; J. Sousa","IDMEC lnstituto Superior Técnico, Lisbon, Portugal; IDMEC lnstituto Superior Técnico, Lisbon, Portugal; IDMEC lnstituto Superior Técnico, Lisbon, Portugal; IDMEC lnstituto Superior Técnico, Lisbon, Portugal; IDMEC lnstituto Superior Técnico, Lisbon, Portugal","2021 IEEE Symposium Series on Computational Intelligence (SSCI)","24 Jan 2022","2021","","","01","08","This paper proposes an Encoder-Decoder neural network architecture with Attention Mechanism for solving the DRC-FJSSP using Deep Q-Learning. In the DRC-FJSSP the number of operations to schedule is problem dependent. Current state-of-the-art reinforcement learning methods arbitrarily simplify the input information to a fixed-size feature input vector. This way, they end up losing relevant problem information for a large enough number of operations. Furthermore, on the one hand, human schedulers tend to optimize production schedules by moving operations individually into more adequate positions in the schedule. On the other hand, the aforementioned state-of-the-art methods apply heuristics recurrently as their optimization procedure. These limitations come as the cost of the neural network architecture, which is limited to fixed-size inputs and outputs. The architecture proposed in this paper is a Recurrent Neural Network, which enables it to work with inputs and outputs of variable sizes. This decisive feature makes it possible for the agent to move a specific operation to a more adequate position in the schedule and receive explicit problem information, such as the processing times of all operations. In the end, this approach proved to be competitive with a state-of-the-art metaheuristic method, the KGFOA. This promising results come even with a limitation in the available computational resources, which only allowed the development of scarcely trained agent.","","978-1-7281-9048-8","10.1109/SSCI50451.2021.9659849","FCT, through IDMEC, under LAETA(grant numbers:UIDP/50022/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9659849","Job-Shop Scheduling;Reinforcement Learning;Deep Q-Learning;Encoder-Decoder;Attention Mechanism;Pointer Networks","Training;Schedules;Q-learning;Recurrent neural networks;Metaheuristics;Computer architecture;Artificial neural networks","deep learning (artificial intelligence);heuristic programming;job shop scheduling;metaheuristics;optimisation;production control;production engineering computing;recurrent neural nets;reinforcement learning","encoder-decoder neural network architecture;job shop scheduling problems;reinforcement learning;DRC-FJSSP;human schedulers;production schedules;recurrent neural network;state-of-the-art metaheuristic method;deep Q-learning;state-of-the-art reinforcement learning methods;fixed-size feature input vector;heuristics model;optimization process;KGFOA","","","","22","IEEE","24 Jan 2022","","","IEEE","IEEE Conferences"
"Sample-Efficient Reinforcement Learning for Pose Regulation of a Mobile Robot","W. Brescia; L. De Cicco; S. Mascolo","Politecnico di Bari, Bari, Italy; Politecnico di Bari, Bari, Italy; Politecnico di Bari, Bari, Italy","2022 11th International Conference on Control, Automation and Information Sciences (ICCAIS)","30 Dec 2022","2022","","","42","47","Reinforcement Learning (RL) has gained interest in the control and automation communities thanks to its encouraging results in many challenging control problems without requiring a model of the system and of the environment. Yet, it is well-known that employing such a learning-based approach in real scenarios may be problematic, as a prohibitive amount of data might be required to converge to an optimal control policy. In this work, we equip a popular RL algorithm with two tools to improve exploration effectiveness and sample efficiency: the Episodic Noise, that helps useful subsets of actions emerge already in the first few training episodes, and the Difficulty Manager, that generates goals proportioned to the current agent’s capabilities. We demonstrate the effectiveness of such proposed tools on a pose regulation task of a four wheel steering four wheel driving robot, suitable for a wide range of industrial scenarios. The resulting agent learns effective sets of actions in just a few hundreds training epochs, reaching satisfactory performance during tests.","2475-7896","978-1-6654-5248-9","10.1109/ICCAIS56082.2022.9990480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9990480","Deep Reinforcement Learning;Mobile Robots;TD3;Sample Efficiency","Training;Automation;Service robots;Wheels;Optimal control;Reinforcement learning;Regulation","learning (artificial intelligence);mobile robots;multi-agent systems;optimal control","automation communities thanks;challenging control problems;current agent;Episodic Noise;exploration effectiveness;industrial scenarios;learning-based approach;mobile robot;optimal control policy;popular RL algorithm;pose regulation;regulation task;resulting agent;sample efficiency;sample-efficient reinforcement Learning;training episodes;wheel driving robot","","","","16","IEEE","30 Dec 2022","","","IEEE","IEEE Conferences"
"Implementation and evaluation of NoisyNets to reinforcement learning of automated ICT system design","T. Zhou; Y. Yakuwa; N. Okamura; T. Kuroda; I. E. Yairi","Graduate School of Science and Technology, Sophia University, 7-1 Kioi-cho, Chiyoda-ku, Tokyo, 102-8554, Japan; NEC Corporation, 5-7-1, Shiba, Minato-ku, Tokyo, Japan; Graduate School of Science and Technology, Sophia University, 7-1 Kioi-cho, Chiyoda-ku, Tokyo, 102-8554, Japan; NEC Corporation, 5-7-1, Shiba, Minato-ku, Tokyo, Japan; Graduate School of Science and Technology, Sophia University, 7-1 Kioi-cho, Chiyoda-ku, Tokyo, 102-8554, Japan","IEICE Communications Express","","2023","PP","99","1","4","This paper proposes to apply the method with an additional noisy layer to the structure of the graph neural network for reinforcement learning in automated design technology for information and communication systems. The automated design technology has an elementary problem of huge learning time caused by overestimating a specific configuration because of hardly ever rewards despite huge exploration space with a vast combination of selections, arrangements, and connections. The parametric noise applied to the network is learned with gradient descent and the remaining network weights to reduce this harmful overestimation during learning and increase the design exploration efficiency. The evaluation result showed that using the proposed algorithm for our automated design technology in development could shorten 15% of the episodes needed for learning to converge.","2187-0136","","10.23919/comex.2023XBL0101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10287235","network system design;design automation;machine learning;reinforcement learning","Noise measurement;Reinforcement learning;System analysis and design;Topology;Convergence;Artificial neural networks;Graph neural networks","","","","","","","","17 Oct 2023","","","IEICE","IEICE Early Access Articles"
"Controlled Deep Reinforcement Learning for Optimized Slice Placement","J. J. Alves Esteves; A. Boubendir; F. Guillemin; P. Sens","Orange Labs, France; Orange Labs, France; Orange Labs, France; Sorbonne Universit&#x00E9; / CNRS / Inria, LIP6, France","2021 IEEE International Mediterranean Conference on Communications and Networking (MeditCom)","23 Dec 2021","2021","","","20","22","We present a hybrid ML-heuristic approach that we name ""Heuristically Assisted Deep Reinforcement Learning (HA-DRL)"" to solve the problem of Network Slice Placement Optimization. The proposed approach leverages recent works on Deep Reinforcement Learning (DRL) for slice placement and Virtual Network Embedding (VNE) and uses a heuristic function to optimize the exploration of the action space by giving priority to reliable actions indicated by an efficient heuristic algorithm. The evaluation results show that the proposed HA-DRL algorithm can accelerate the learning of an efficient slice placement policy improving slice acceptance ratio when compared with state-of-the-art approaches that are based only on reinforcement learning.","","978-1-6654-4505-4","10.1109/MeditCom49071.2021.9647662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647662","Network Slicing;Optimization;Automation;Deep Reinforcement Learning;Placement;Large Scale","Training;Scalability;Heuristic algorithms;Conferences;Reinforcement learning;Parallel processing;Collaborative work","computer network reliability;deep learning (artificial intelligence);optimisation;reinforcement learning;virtualisation","hybrid ML-heuristic approach;network slice placement optimization;virtual network embedding;heuristic function;heuristic algorithm;HA-DRL algorithm;slice acceptance ratio;slice placement policy;optimized slice placement;deep reinforcement learning","","","","5","IEEE","23 Dec 2021","","","IEEE","IEEE Conferences"
"Learning to Grasp on the Moon from 3D Octree Observations with Deep Reinforcement Learning","A. Orsula; S. Bøgh; M. Olivares-Mendez; C. Martinez","Space Robotics Research Group (SpaceR), Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg; Department of Materials and Production, Robotics & Automation Group, Aalborg University, Denmark; Space Robotics Research Group (SpaceR), Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg; Space Robotics Research Group (SpaceR), Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg","2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","26 Dec 2022","2022","","","4112","4119","Extraterrestrial rovers with a general-purpose robotic arm have many potential applications in lunar and planetary exploration. Introducing autonomy into such systems is desirable for increasing the time that rovers can spend gathering scientific data and collecting samples. This work investigates the applicability of deep reinforcement learning for vision-based robotic grasping of objects on the Moon. A novel simulation environment with procedurally-generated datasets is created to train agents under challenging conditions in unstructured scenes with uneven terrain and harsh illumination. A model-free off-policy actor-critic algorithm is then employed for end-to-end learning of a policy that directly maps compact octree observations to continuous actions in Cartesian space. Experimental evaluation indicates that 3D data representations enable more effective learning of manipulation skills when compared to traditionally used image-based observations. Domain randomization improves the generalization of learned policies to novel scenes with previously unseen objects and different illumination conditions. To this end, we demonstrate zero-shot sim-to-real transfer by evaluating trained agents on a real robot in a Moon-analogue facility. The source code and datasets are available at https://github.com/AndrejOrsula/drl_grasping.","2153-0866","978-1-6654-7927-1","10.1109/IROS47612.2022.9981661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981661","","Deep learning;Three-dimensional displays;Source coding;Moon;Octrees;Lighting;Reinforcement learning","data visualisation;learning (artificial intelligence);manipulators;mobile robots;octrees;planetary rovers;robot vision","3D data representations;3D octree observations;collecting samples;deep reinforcement learning;different illumination conditions;effective learning;end-to-end learning;extraterrestrial rovers;general-purpose robotic arm;grasp;learned policies;maps compact octree observations;model-free off-policy actor-critic;Moon-analogue facility;planetary exploration;procedurally-generated datasets;scientific data;traditionally used image-based observations","","2","","42","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"The Study of a Textile Punching Robot Based on Combined Deep Reinforcement Learning","W. Li; D. Chen; J. Dai; J. Le","School of Computer Science and Technology, Donghua University, Shanghai, China; School of Computer Science and Technology, Donghua University, Shanghai, China; School of Mechanical Engineering, Donghua University, Shanghai, China; School of Computer Science and Technology, Donghua University, Shanghai, China","2018 International Conference on Cloud Computing, Big Data and Blockchain (ICCBB)","11 Jul 2019","2018","","","1","8","Punching of textile uppers is an important process in the shoe production line, which determines the accuracy of positioning and affects all subsequent shoemaking steps. At present, there are few researches on the 3-DOF punching robot in this field. The critical value of the punching force of the end mechanism is not clear, and the punching path planning is not adopted with a suitable algorithm, resulting in large power consumption and low work efficiency. Therefore, this paper describes the design of a punching robot to meet the action requirements of punching the textile uppers and complete the path planning to improve efficiency. The first step is to model a 3-DOF robot for textile upper punching including rail type and articulated type to meet the needs of punching movements. The second step is to gain the lowest critical value of the pressure with ANSYS by simulating deformation process when the punch punches in the vamp until breaks it. And this outcome is the theory basis for motor selection and robot design. The third step is multi-point punching path planning, where a combined deep reinforcement learning (DRL) method is proposed. The DRL pre-training is performed through asynchronous advantage actor-critic (A3C). Training data is adopted to optimize the recurrent neural networks (RNN) that parameterizes the stochastic policy. Then the inspire planning (IP) algorithm is conducted to get the optimal path. The experimental results show that this method can jump out of the local optimal solution and outperform other state-of-the-art methods.","7281-1277","978-1-7281-1277-0","10.1109/ICCBB.2018.8756483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8756483","3-DOF robot;Dynamic simulation of punching;Path planning;Combined deep reinforcement learning;Asynchronous advantage actor-critic","Punching;Textiles;Stress;Robots;Finite element analysis;Path planning;Strain","control engineering computing;footwear industry;industrial manipulators;learning (artificial intelligence);manipulator dynamics;mobile robots;path planning;production engineering computing;punching;recurrent neural nets","shoe production line;punching force;punching movements;multipoint punching path planning;textile punching robot;deep reinforcement learning;rail type;textile upper punching;shoemaking steps;3-DOF punching robot;ANSYS;deformation process simulation;motor selection;robot design;asynchronous advantage actor-critic;recurrent neural networks;inspire planning","","","","19","IEEE","11 Jul 2019","","","IEEE","IEEE Conferences"
"Solving Multi-objective Reinforcement Learning Problems by EDA-RL - Acquisition of Various Strategies","H. Handa","Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan","2009 Ninth International Conference on Intelligent Systems Design and Applications","28 Dec 2009","2009","","","426","431","EDA-RL, estimation of distribution algorithms for reinforcement learning problems, have been proposed by us recently. The EDA-RL can improve policies by EDA scheme: First, select better episodes. Secondly, estimate probabilistic models, i.e., policies, and finally, interact with the environment for generating new episodes. In this paper, the EDA-RL is extended for multi-objective reinforcement learning problems, where reward is given by several criteria. By incorporating the notions in evolutionary multi-objective optimization, the proposed method is enable to acquire various strategies by a single run.","2164-7151","978-1-4244-4735-0","10.1109/ISDA.2009.92","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364903","Estimation of Distribution Algorithms;Reinforcement Learning Problems;Evolutionary Multi-Objective Optimisation","Learning;Electronic design automation and methodology;Evolutionary computation;Safety;Markov random fields;Intelligent systems;Optimization methods;State estimation;Probability distribution;Design optimization","evolutionary computation;learning (artificial intelligence);optimisation;probability","multiobjective reinforcement learning problems;distribution algorithms;probabilistic models;evolutionary multi-objective optimization","","11","","6","IEEE","28 Dec 2009","","","IEEE","IEEE Conferences"
"Falling avoidance control of acrobat robot by reinforcement learning","T. Kochiya; M. Yamakita","Department of Mechanical and Control Systems Eng, Tokyo Institute of Technology, Tokyo, Japan; RIKEN Institute of Physical and Chemical Research, Nagoya, Japan","SICE Annual Conference 2007","7 Jan 2008","2007","","","322","326","In this study a landing control of an acrobat robot is considered and Q-learning method is applied for falling avoidance control. Since the dynamics of the system is changed according to contact conditions to the ground, the system is a typical variable constraint and hybrid system. The state space for the Q-learning consists of discrete mode variable and continuous states. It is shown by numerical simulations that taking a step motion is automatically generated and falling down is avoided properly.","","978-4-907764-27-2","10.1109/SICE.2007.4421000","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4421000","Variable constraint system;Q-learning;Hybrid system","Robot control;Learning;Control systems;Chemical technology;Mechanical variables control;Numerical simulation;Orbital robotics;State-space methods;Robotics and automation;Motion control","collision avoidance;learning (artificial intelligence);mobile robots","falling avoidance control;acrobat robot;reinforcement learning;landing control;Q-learning method;discrete mode variable","","6","","6","","7 Jan 2008","","","IEEE","IEEE Conferences"
"Hardware design of autonomous snake-like robot for reinforcement learning based on environment","K. Ito; A. Takayama; T. Kobayashi","Hosei University, Koganei, Tokyo, Japan; DAIHATSU MOTOR Company Limited, Ikeda, Osaka, Japan; Hosei University, Koganei, Tokyo, Japan","2009 IEEE/RSJ International Conference on Intelligent Robots and Systems","15 Dec 2009","2009","","","2622","2627","In this paper, we propose the design of a robot with a snake-like body based on a test environment. We explore the abstraction of state-action spaces for reinforcement learning. Additionally, we discuss the versatility of the proposed mechanism by showing that different tasks can be completed by simply changing the reward of the reinforcement learning. Finally, we mention the importance of a body design based on an environment by considering the concept of ecological niches.","2153-0866","978-1-4244-3803-7","10.1109/IROS.2009.5354817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5354817","","Hardware;Learning;Orbital robotics;Intelligent robots;Indium tin oxide;Robot control;USA Councils;Testing;Automation","learning (artificial intelligence);mobile robots","autonomous snake-like robot;reinforcement learning;ecological niches concept;robot hardware design;test environment learning;state-action space abstraction","","5","","18","IEEE","15 Dec 2009","","","IEEE","IEEE Conferences"
"A reinforcement learning approach for robot control in an unknown environment","Nan-Feng Xiao; S. Nahavandi","School of Engineering and Technology, Deakin University, Geelong, VIC, Australia; School of Engineering and Technology, Deakin University, Geelong, VIC, Australia","2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.","2 Apr 2003","2002","2","","1096","1099 vol.2","In this paper, a control approach based on reinforcement learning is present for a robot to complete a dynamic task in an unknown environment. First, a temporal difference-based reinforcement learning algorithm and its evaluation function are used to make the robot learn with its trials and errors as well as experiences. Second, the simulation are carried out to adjust the parameters of the learning algorithm and determine an optimal policy by using the models of a robot. Last, the effectiveness of the present approach is demonstrated by balancing an inverse pendulum in the unknown environment.","","0-7803-7657-9","10.1109/ICIT.2002.1189324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1189324","","Learning;Robot control;Robot kinematics;Humans;Robotics and automation;Robot sensing systems;Australia;Legged locomotion;Animals;Leg","robots;learning (artificial intelligence);uncertain systems;optimal control","reinforcement learning approach;robot control;unknown environment;temporal difference-based reinforcement learning algorithm;evaluation function;optimal policy;inverse pendulum","","4","","4","IEEE","2 Apr 2003","","","IEEE","IEEE Conferences"
"A study of reinforcement learning with knowledge sharing -Applications to real mobile robots-","K. Ito; Y. Imoto; H. Taguchi; A. Gofuku","Department or Laboratory, Okayama University, Japan; Department or Laboratory, Okayama University, Japan; Department or Laboratory, Okayama University, Japan; Department or Laboratory, Okayama University, Japan","2004 IEEE International Conference on Robotics and Biomimetics","24 Oct 2005","2004","","","175","180","In this paper, we consider multi-agent system in which every agents have own tasks that differs each other. We propose a method that decreases learning time of reinforcement learning by using the model of environment. In the proposed algorithm, the model is created by sharing the experiences of agents each other. To demonstrate the effectiveness of the proposed method, simulations of a puddle world and experiments of a maze world have been carried out. As a result effective behaviors have been obtained quickly.","","0-7803-8614-8","10.1109/ROBIO.2004.1521772","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521772","","Learning;Mobile robots;Multiagent systems;Robotics and automation;Costs;Indium tin oxide;Laboratories;Automatic control;Robot control","learning (artificial intelligence);mobile robots;multi-agent systems","reinforcement learning;knowledge sharing;multiagent system;puddle world simulation;maze world simulation","","4","","19","IEEE","24 Oct 2005","","","IEEE","IEEE Conferences"
"Reinforcement learning based on modular fuzzy model with gating unit","T. Watanabe; T. Wada","Faculty of Engineering, Osaka Electro Communication University, Neyagawa, Osaka, Japan; Graduate School of Engineering, Osaka Electro Communication University, Neyagawa, Osaka, Japan","2008 IEEE International Conference on Systems, Man and Cybernetics","7 Apr 2009","2008","","","1806","1811","In order to realize intelligent agent such as autonomous mobile robots, reinforcement learning is one of necessary techniques in behavior control system. However, applying the reinforcement learning to actual sized problem, the ldquocurse of dimensionalityrdquo problem in partition of sensory states should be avoided maintaining computational efficiency. Furthermore the robot task is desired to be decomposed automatically in learning process for achievement of good performance. We tackle these two issues by applying modular fuzzy model with gating unit to reinforcement learning. The modular fuzzy model extending SIRMs architecture is formulated to apply Q-learning algorithm. The gating unit that is constructed as a neural network model or simple learning parameters is installed to switch the use of the modular model for task decomposition. Through numerical examples, we found that the proposed method has fair convergence property of learning compared with the conventional model structure.","1062-922X","978-1-4244-2383-5","10.1109/ICSMC.2008.4811551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4811551","reinforcement learning;Q-learning;modular fuzzy model;modular neural network;neural network","Learning;Switches;Intelligent agent;Mobile robots;Automatic control;Control systems;Computational efficiency;Robot sensing systems;Robotics and automation;Computer architecture","fuzzy set theory;learning (artificial intelligence);mobile robots;neurocontrollers","reinforcement learning;modular fuzzy model;gating unit;intelligent agents;autonomous mobile robots;behavior control system;curse of dimensionality problem;SIRM architecture;Q-learning algorithm;task decomposition","","3","","23","IEEE","7 Apr 2009","","","IEEE","IEEE Conferences"
"Incorporating fuzzy logic to reinforcement learning [mobile robot navigation]","G. Faria; R. A. F. Romero","SCE-TCMC-USP, Sao Paulo, Brazil; SCE-TCMC-USP, Sao Paulo, Brazil","Ninth IEEE International Conference on Fuzzy Systems. FUZZ- IEEE 2000 (Cat. No.00CH37063)","6 Aug 2002","2000","2","","847","852 vol.2","Proposes a sensor-based navigation method that utilizes fuzzy logic in reinforcement learning algorithms for navigation of a mobile robot in uncertain environments. The sonar readings are codified in distance notions by fuzzy sets and a modification in the R-learning algorithm by incorporating fuzzy logic is proposed. Fuzzy logic is used for weighting the immediate reward value, that is a variable present in most reinforcement learning algorithms. The effectiveness of the modified algorithm, R'-learning, is verified in several tests and compared to the performance of the R-learning algorithm.","1098-7584","0-7803-5877-5","10.1109/FUZZY.2000.839142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839142","","Fuzzy logic;Robot sensing systems;Mobile robots;Robot programming;Machine learning;Fuzzy sets;Sonar navigation;Robotics and automation;Artificial intelligence;Large Hadron Collider","learning (artificial intelligence);fuzzy logic;path planning;mobile robots;fuzzy set theory;optimal control;Markov processes;decision theory;sonar","reinforcement learning;mobile robot navigation;sensor-based navigation method;uncertain environments;sonar readings;R-learning algorithm;immediate reward value;R'-learning algorithm","","3","","12","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Online Support Vector Regression based value function approximation for Reinforcement Learning","D. -H. Lee; V. V. Quang; S. Jo; J. -J. Lee","Robotics Program, KAIST, Daejeon, Korea; School of Electrical Engineering and Computer Science, KAIST, Daejeon, Korea; KAIST, School of Electrical Engineering and Computer Science, Daejeon, Korea; KAIST, School of Electrical Engineering and Computer Science, Daejeon, Korea","2009 IEEE International Symposium on Industrial Electronics","25 Aug 2009","2009","","","449","454","This paper proposes the online Support Vector Regression (SVR) based value function approximation method for Reinforcement Learning (RL). This approach conserves the Support Vector Machine (SVM)'s good property, the generalization which is a key issue of function approximation. Online SVR can do incremental learning and automatically track variation of environment with time-varying characteristics. Using the online SVR, we can obtain the fast and good estimation of value function and achieve RL objective efficiently. Throughout simulation tests, the feasibility and usefulness of the proposed approach is demonstrated by comparison with SARSA and Q-learning.","2163-5145","978-1-4244-4347-5","10.1109/ISIE.2009.5222726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5222726","","Function approximation;Learning;Support vector machines;State estimation;Electronic mail;Quadratic programming;Industrial electronics;Computer science;Robotics and automation;Testing","function approximation;learning (artificial intelligence);support vector machines","online support vector regression;value function approximation;reinforcement learning;Q-learning","","2","","9","IEEE","25 Aug 2009","","","IEEE","IEEE Conferences"
"Automatic Path Search for Roving Robot Using Reinforcement Learning","S. Miyata; H. Nakamura; A. Yanou; S. Takehara","Kinki University, Higashihiroshima, Japan; Kinki University, Higashihiroshima, Japan; Okayama University, Japan; Kinki University, Higashihiroshima, Japan","2009 Fourth International Conference on Innovative Computing, Information and Control (ICICIC)","17 Feb 2010","2009","","","169","172","Rapid advances in robot technology have been made in recent years. In connection with these advances, robots are expected to be utilized in a variety of places and environments. This study describes, (1) a method which allows a robot to measure the location of its destination in the real world based on an image obtained from a single camera, and (2) a method of navigating a robot to a destination which is selected by a user on a display showing the forward robot view. Consideration is also given to cases in which there are obstacles between the robot and the destination. Through the use of reinforcement learning, which is considered a promising candidate among autonomous control techniques, the roving robot tries to find the shortest way to the destination based on information concerning the locations of obstacles and the destination. This study also describes an image-based method of measuring a selected location, the results from a simulation of path finding using reinforcement learning, and the results from an experiment of navigation in a real environment. Finally, a summary of the main conclusions is provided.","","978-1-4244-5544-7","10.1109/ICICIC.2009.121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5412489","","Robotics and automation;Learning;Robot kinematics;Cameras;Navigation;Displays;Robot sensing systems;Robot vision systems;Lenses;Automatic control","collision avoidance;learning (artificial intelligence);mobile robots;navigation;robot vision","automatic path search;roving robot;reinforcement learning;robot navigation;camera;obstacles","","2","","5","IEEE","17 Feb 2010","","","IEEE","IEEE Conferences"
"Distributed form closure for convex planar objects through reinforcement learning with local information","A. H. Elahibakhsh; M. N. Ahmadabadi; F. J. Sharifi; B. N. Araabi","School of Cognitive Sciences, Institute for Studies on Theoretical Physics and Mathematics, Tehran, Iran; School of Cognitive Sciences, Institute for Studies on Theoretical Physics and Mathematics, Tehran, Iran; NA; School of Cognitive Sciences, Institute for Studies on Theoretical Physics and Mathematics, Tehran, Iran","2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)","14 Feb 2005","2004","4","","3170","3175 vol.4","Many real world applications would involve grasp of large objects in unstructured environments. Agent-based approach to multi-robot grasp of objects would prove useful under the above circumstances. In this paper, the problem of form closure grasp for planar convex objects by multiple robots is tackled. Contrary to the previous approaches, no a priori information about the shape of the object is assumed, and the robots are not allowed to fully communicate among themselves. A distributed multi-agent based approach using Q-learning is proposed. The state space, action set and learning algorithm are formulated. The results are verified through simulations using a developed Q-learning test bed.","","0-7803-8463-6","10.1109/IROS.2004.1389905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1389905","","Learning;Orbital robotics;Intelligent robots;Shape control;Robot sensing systems;Testing;Artificial intelligence;Cognitive robotics;Robotics and automation;Automatic control","learning (artificial intelligence);multi-robot systems;multi-agent systems;grippers;state-space methods","distributed form closure;convex planar object;reinforcement learning;local information;multirobot grasp;distributed multiagent system;Q-learning;state space algorithm;learning algorithm","","2","","16","IEEE","14 Feb 2005","","","IEEE","IEEE Conferences"
"Efficient Accelerator/Network Co-Search With Circular Greedy Reinforcement Learning","Z. Liu; G. Li; J. Cheng","Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Circuits and Systems II: Express Briefs","27 Jun 2023","2023","70","7","2615","2619","Recently, accelerator/network co-search has shown great promise in reducing the complexity of co-design and achieving higher model accuracy. Unlike the manual design methodology, it automatically learns the optimal network architecture and corresponding accelerator under the given constraints. However, prior works do not consider the direct feedback of searched accelerators on the network search in each step, which leads to low converge speed and sub-optimal solutions. To address this issue, we propose DAN, a reinforcement learning-based framework for fast and accurate accelerator/network co-search. The fundamental idea is to model the co-search as interleaved network-aware accelerator search (AS) and accelerator-aware network search (NS) using separate RL agents, which improves the performance of AS and NS, and encourages a tight interaction between AS and NS. Experimental results show that our proposed method consistently outperforms single-agent based method in terms of converge speed and performance.","1558-3791","","10.1109/TCSII.2023.3237912","STI 2030-Major Projects(grant numbers:2021ZD0201504); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDB32050200); Jiangsu Key Research and Development Plan(grant numbers:BE2021012-2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020186","Accelerator/network co-search;reinforcement learning;performance estimation;multi-objective optimization","Network architecture;Search problems;Hardware;Energy consumption;Training;Optimization;Accelerated aging","convolutional neural nets;deep learning (artificial intelligence);greedy algorithms;multi-agent systems;reinforcement learning","accelerator-aware network search;AS;circular greedy reinforcement learning;interleaved network-aware accelerator search;NS;reinforcement learning-based framework;RL agents;searched accelerators","","2","","23","IEEE","18 Jan 2023","","","IEEE","IEEE Journals"
"Approximating the value function for continuous space reinforcement learning in robot control","S. Buck; M. Beetz; T. Schmitt","Munich University of Technology, Germany; Munich University of Technology, Germany; Munich University of Technology, Germany","IEEE/RSJ International Conference on Intelligent Robots and Systems","10 Dec 2002","2002","1","","1062","1067 vol.1","Many robot learning tasks are very difficult to solve: their state spaces are high dimensional, variables and command parameters are continuously valued, and system states are only partly observable. In this paper, we propose to learn a continuous space value function for reinforcement learning using neural networks trained from data of exploration runs. The learned function is guaranteed to be a lower bound for, and reproduces the characteristic shape of, the accurate value function. We apply our approach to two robot navigation tasks, discuss how to deal with possible problems occurring in practice, and assess its performance.","","0-7803-7398-7","10.1109/IRDS.2002.1041532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041532","","Learning;Robot control;State-space methods;Orbital robotics;Shape;Navigation;Robotics and automation;Upper bound;Neural networks;Acceleration","learning (artificial intelligence);state-space methods;mobile robots;neural nets","robot learning;reinforcement learning;neural networks;continuous space value function;robot navigation;state spaces;autonomous robot skills","","2","","20","IEEE","10 Dec 2002","","","IEEE","IEEE Conferences"
"Fuzzy-based reinforcement learning of a robot force control skill","R. Araujo; U. Nunes; A. T. de Almeida","Institute of Systems and Robotics (ISR), Electrical Engineering Department, University of Coimbra, Coimbra, Portugal; Institute of Systems and Robotics (ISR), Electrical Engineering Department, University of Coimbra, Coimbra, Portugal; Institute of Systems and Robotics (ISR), Electrical Engineering Department, University of Coimbra, Coimbra, Portugal","Proceedings of IEEE International Symposium on Industrial Electronics","6 Aug 2002","1996","1","","453","458 vol.1","Humans perform many tasks with relative ease. In spite of this, many tasks are difficult to model explicitly and it is difficult to design and program automatic control algorithms for them. The development, improvement, and application of learning techniques taking advantage of sensory information would enable the acquisition of new robot skills and avoid some of the difficulties of explicit programming. This paper describes an approach for the generation of skills for the control of multidegree of freedom robotic systems. In the method, the acquisition of skills is done online by self-learning. Instead of generating skills by explicit programming of a perception to action mapping, they are generated by trial and error learning, guided by a performance evaluation feedback function. The structure of the controller consists of two fuzzy subsystems both implemented by feedforward multilayer neural networks. The action fuzzy subsystem has the purpose of generating command actions for the system under control. The evaluation-prediction fuzzy subsystem predicts the future value, a function that evaluates the performance of the controller. Simulation results concerning the application of the approach to learning a robot manipulator force control skill are presented.","","0-7803-3334-9","10.1109/ISIE.1996.548531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=548531","","Learning;Force control;Robot sensing systems;Fuzzy control;Robotics and automation;Robot programming;Control systems;Multi-layer neural network;Humans;Algorithm design and analysis","manipulators","robot force control skill;fuzzy-based reinforcement learning;automatic control algorithms;learning techniques;multidegree of freedom;control design;skills acquisition;online self-learning;trial and error learning;control simulation;performance evaluation feedback function;fuzzy subsystems;feedforward multilayer neural networks;robot manipulator","","1","","10","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Towards Efficient Mapless Navigation Using Deep Reinforcement Learning with Parameter Space Noise","X. Liu; Q. Zhou; H. Wang; Y. Yang","Qian Xuesen Laboratory of Space Technology, Beijing, China; Qian Xuesen Laboratory of Space Technology, Beijing, China; Qian Xuesen Laboratory of Space Technology, Beijing, China; Qian Xuesen Laboratory of Space Technology, Beijing, China","2019 Chinese Control Conference (CCC)","17 Oct 2019","2019","","","8833","8837","This paper presents a deep reinforcement learning framework that is capable of training the mapless motion planner end-to-end by taking the laser range findings as input and the continuous steering commands as output. Major improvements are introduced in our Deep Deterministic Policy Gradient algorithm (DDPG): parameter space noise is used to encourage exploration and an efficient exploration strategy is designed to boost navigation performance. The proposed learning framework is implemented to train the mobile robot in the Gym-gazebo simulation environment. The simulation study shows that the proposed mapless motion planner can navigate the nonholonomic mobile robot effectively without collisions.","1934-1768","978-9-8815-6397-2","10.23919/ChiCC.2019.8866029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866029","Reinforcement learning;neural networks;parameter space noise for exploration;mobile robot;path planning","Robots;Conferences;Automation;Nickel;Economic indicators;Machine learning;Handheld computers","collision avoidance;gradient methods;laser ranging;learning (artificial intelligence);mobile robots;navigation;parameter space methods;robot vision","parameter space noise;deep reinforcement learning framework;mapless motion planner;laser range findings;continuous steering commands;deep deterministic policy gradient algorithm;navigation performance;Gym-gazebo simulation environment;mapless navigation;exploration strategy;nonholonomic mobile robot","","1","","22","","17 Oct 2019","","","IEEE","IEEE Conferences"
"Learning to Drive via Apprenticeship Learning and Deep Reinforcement Learning","W. Huang; F. Braghin; Z. Wang","Industrial and Information Engineering, Politecnico Di Milano, Milan, Italy; Industrial and Information Engineering, Politecnico Di Milano, Milan, Italy; School of Communication Engineering, Xidian University, Xi'an, China","2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)","13 Feb 2020","2019","","","1536","1540","With the implementation of reinforcement learning (RL) algorithms, current state-of-art autonomous vehicle technology have the potential to get closer to full automation. However, most of the applications have been limited to game domains or discrete action space which are far from the real world driving. Moreover, it is very tough to tune the parameters of reward mechanism since the driving styles vary a lot among the different users. For instance, an aggressive driver may prefer driving with high acceleration whereas some conservative drivers prefer a safer driving style. Therefore, we propose an apprenticeship learning in combination with deep reinforcement learning approach that allows the agent to learn the driving and stopping behaviors with continuous actions. We use gradient inverse reinforcement learning (GIRL) algorithm to recover the unknown reward function and employ REINFORCE as well as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal policy. The performance of our method is evaluated in simulation-based scenario and the results demonstrate that the agent performs human like driving and even better in some aspects after training.","2375-0197","978-1-7281-3798-8","10.1109/ICTAI.2019.00220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8995417","Reinforcement Learning Application;Inverse Reinforcment Learning;Autonomous Driving","","driver information systems;gradient methods;learning (artificial intelligence)","apprenticeship learning;reinforcement learning algorithms;autonomous vehicle technology;game domains;discrete action space;reward mechanism;driving styles;aggressive driver;conservative drivers;safer driving style;deep reinforcement learning approach;continuous actions;gradient inverse reinforcement learning algorithm;unknown reward function;deep deterministic policy gradient algorithm","","1","","21","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Simulation of a Quadcopter Stabilization System Using a Neural Reinforcement Learning Algorithm","A. V. Khrenov; S. A. K. Diane","Lab. of System Integration of Control Technologies, Institute of Control Problems of Russian Academy of Sciences, Moscow, Russia; Lab. of System Integration of Control Technologies, Institute of Control Problems of Russian Academy of Sciences, Moscow, Russia","2022 15th International Conference Management of large-scale system development (MLSD)","9 Nov 2022","2022","","","1","5","Setting up classical PID controllers works for systems with a priori known properties and parameters. After changing the characteristics of an object, such regulators may not be able to stabilize its control variables and break the specified limits e.g. when the angular velocity is exceeded as a result of an engine malfunction. This paper presents an algorithm for the neural stabilization of a quadcopter. We use reinforcement learning method to enable adaptation of the agent during the control process. The quadcopter is simulated in a virtual environment and trained in a series of episodes with sequential assessment of its movement efficiency and tuning neural network parameters. A comparison of the transient characteristics of the quadcopter in various conditions showed that the neural network controller surpasses the classical PID controller by several significant criteria.","","978-1-6654-9701-5","10.1109/MLSD55143.2022.9934441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9934441","quadcopter simulation;flight stabilization;neural network;reinforcement learning","Automation;Atmospheric modeling;Helicopters;Neural networks;Software algorithms;Virtual environments;Reinforcement learning","aerospace control;control engineering computing;helicopters;neural nets;reinforcement learning;three-term control","quadcopter stabilization system;neural reinforcement learning algorithm;classical PID controller;control variables;angular velocity;engine malfunction;neural stabilization;reinforcement learning method;movement efficiency;tuning neural network parameters;transient characteristics;neural network controller;virtual environment;neural network parameters.","","","","11","IEEE","9 Nov 2022","","","IEEE","IEEE Conferences"
"Grouped Intersection-based Routing using Reinforcement Learning for Urban VANETs","Q. Yang; S. -J. Yoo","Electrical and Computer Engineering, INHA University, Incheon, Korea; Electrical and Computer Engineering, INHA University, Incheon, Korea","2022 13th International Conference on Information and Communication Technology Convergence (ICTC)","25 Nov 2022","2022","","","1855","1858","Under the rapid upgrowth of internet of vehicles (IoV), routing in vehicular ad-hoc networks (VANETs) has gained a great amount of attention in the past few years in academic and industry communities. Due to the complexity of urban territory and the scale of vehicular mobility, infrastructure resources are widely used in VANETs to improve network performance. We propose a grouped intersection-assisted routing protocol in VANETs using the Q-learning algorithm for an urban environment. The simulation results show our method can dramatically decrease the communication complexity of the learning procedure and improve the convergence speed compared to the conventional Q-learning algorithm without grouping.","2162-1241","978-1-6654-9939-2","10.1109/ICTC55196.2022.9952627","IITP (Institute for Information & Communications Technology Planning & Evaluation); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9952627","geographic routing;intersection-based routing;Q-learning;reinforcement learning;vehicular ad-hoc networks","Q-learning;Simulation;Urban areas;Routing;Routing protocols;Ad hoc networks;Complexity theory","communication complexity;learning (artificial intelligence);mobility management (mobile radio);routing protocols;telecommunication computing;vehicular ad hoc networks","academic industry communities;communication complexity;grouped intersection;industry communities;infrastructure resources;Internet of Vehicle;IoV;l Q-learning algorithm;learning procedure;network performance;reinforcement learning;urban environment;urban territory;urban VANET;vehicular mobility","","","","5","IEEE","25 Nov 2022","","","IEEE","IEEE Conferences"
"Collaborative Path Planning of Multiple Carrier-based Aircraft Based on Multi-agent Reinforcement Learning","Z. Shang; Z. Mao; H. Zhang; M. Xu","School of Computer and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; School of Computer and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; School of Computer and Artificial Intelligence, Zhengzhou University, Zhengzhou, China; School of Computer and Artificial Intelligence, Zhengzhou University, Zhengzhou, China","2022 23rd IEEE International Conference on Mobile Data Management (MDM)","25 Aug 2022","2022","","","512","517","Path planning of carrier-based aircraft is of great significance to improve the scheduling efficiency on the aircraft carrier deck. However, it is not easy to find the optimal paths for multiple carrier-based aircraft since the environment of carrier deck is highly dynamic and complex. To overcome this issue, we propose a collaborative path planning model based on multi-agent reinforcement learning. The collaborative path planning for multiple carrier-based aircraft is modeled as a multi-agent reinforcement learning problem, and we build a model based on the state and action space of the carrier-based aircraft. Then we train the model in the simulated gird environment of USS Ford. Finally, the performance of the proposed model is evaluated by experiments under three fixed scenarios and ten random scenarios, and results are shown in the form of simulation visualization. The experimental results show that compared with RRT-Star algorithm, PSO algorithm and deep reinforcement learning DQN model, the proposed model has lower response time, higher completion rate and shorter average path length.","2375-0324","978-1-6654-5176-5","10.1109/MDM55031.2022.00108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9861143","carrier-based aircraft;path planning;reinforcement learning;MADDPG","Atmospheric modeling;Collaboration;Reinforcement learning;Path planning;Aircraft manufacture;Time factors;Aircraft","aerospace computing;aircraft control;control engineering computing;multi-agent systems;particle swarm optimisation;path planning;reinforcement learning;scheduling","multiple carrier-based aircraft;aircraft carrier deck;collaborative path planning model;multiagent reinforcement learning problem;simulated grid environment;USS Ford;RRT-Star algorithm;PSO algorithm;deep reinforcement learning DQN model","","","","25","IEEE","25 Aug 2022","","","IEEE","IEEE Conferences"
"Signal Integrity and Power Leakage Optimization for 3D X-Point Memory Operation using Reinforcement Learning","K. Son; K. Kim; G. Park; D. Lho; H. Park; B. Sim; T. Shin; J. Park; H. Kim; J. Kim; K. Gong","School of Electrical Engineering, KAIST, Daejeon, South Korea; School of Electrical Engineering, KAIST, Daejeon, South Korea; School of Electrical Engineering, KAIST, Daejeon, South Korea; School of Electrical Engineering, KAIST, Daejeon, South Korea; School of Electrical Engineering, KAIST, Daejeon, South Korea; School of Electrical Engineering, KAIST, Daejeon, South Korea; School of Electrical Engineering, KAIST, Daejeon, South Korea; School of Electrical Engineering, KAIST, Daejeon, South Korea; School of Electrical Engineering, KAIST, Daejeon, South Korea; School of Electrical Engineering, KAIST, Daejeon, South Korea; Mixed Design Team, SK Hynix, Icheon, South Korea","2022 IEEE 31st Conference on Electrical Performance of Electronic Packaging and Systems (EPEPS)","22 Nov 2022","2022","","","1","3","As the signal integrity (SI) issues become critical with high bandwidth and density applications, the SI analysis and optimization are necessary. The SI optimization loop including design, modeling, simulation, analysis and revision is repetitive and confined to specific applications. To overcome the recurrent issues, we proposed reinforcement learning (RL) model for SI and power leakage optimization in 3D X-Point memory operation. We defined the MDP components to reflect the optimization problem and the RL model shows learning convergence. The optimal design shows 6.2 % of crosstalk, 17.7 % of IR drop and 25.3 % of power leakage improvement than original design.","2165-4115","978-1-6654-5075-1","10.1109/EPEPS53828.2022.9947197","Research Foundation of Korea (NRF)(grant numbers:NRF-2020M3F3A2A01081587,NRF-2022M3I7A4072293); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9947197","3D X-Point memory;power leakage;reinforcement learning;signal integrity (SI)","Solid modeling;Analytical models;Three-dimensional displays;Integrated circuit interconnections;Reinforcement learning;Manufacturing;Integrated circuit modeling","electronic engineering computing;integrated circuit design;optimisation;random-access storage;reinforcement learning","3D X-Point memory operation;density applications;optimal design;optimization problem;power leakage improvement;power leakage optimization;reinforcement learning model;RL model;SI;signal integrity issues","","","","6","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"Efficient Training Management for Mobile Crowd-Machine Learning: A Deep Reinforcement Learning Approach","T. T. Anh; N. C. Luong; D. Niyato; D. I. Kim; L. -C. Wang","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan","IEEE Wireless Communications Letters","14 Oct 2019","2019","8","5","1345","1348","In this letter, we consider the concept of mobile crowd-machine learning (MCML) for a federated learning model. The MCML enables mobile devices in a mobile network to collaboratively train neural network models required by a server while keeping data on the mobile devices. The MCML thus addresses data privacy issues of traditional machine learning. However, the mobile devices are constrained by energy, CPU, and wireless bandwidth. Thus, to minimize the energy consumption, training time, and communication cost, the server needs to determine proper amounts of data and energy that the mobile devices use for training. However, under the dynamics and uncertainty of the mobile environment, it is challenging for the server to determine the optimal decisions on mobile device resource management. In this letter, we propose to adopt a deep $Q$ -learning algorithm that allows the server to learn and find optimal decisions without any a priori knowledge of network dynamics. Simulation results show that the proposed algorithm outperforms the static algorithms in terms of energy consumption and training latency.","2162-2345","","10.1109/LWC.2019.2917133","A*STAR-NTU-SUTD Joint Research Grant Call on Artificial Intelligence for the Future of Manufacturing(grant numbers:RGANS1906); WASP/NTU(grant numbers:M4082187 (4080)); Singapore MOE Tier 1(grant numbers:2017-T1-002-007 RG122/17); MOE Tier 2(grant numbers:MOE2014-T2-2-015 ARC4/15); Singapore(grant numbers:NRF2015-NRF-ISF001-2277); Singapore EMA Energy Resilience(grant numbers:NRF2017EWT-EP003-041); National Research Foundation of Korea; Korean Government(grant numbers:2014R1A5A1011478); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8716527","Mobile crowd;federated learning;deep reinforcement learning","Mobile handsets;Servers;Training;Data models;Energy consumption;Heuristic algorithms;Machine learning","data privacy;learning (artificial intelligence);mobile computing;neural nets","mobile devices;MCML;energy consumption;mobile environment;mobile device resource management;learning algorithm;efficient training management;mobile crowd-machine learning;federated learning model;mobile network;neural network models","","70","","7","IEEE","16 May 2019","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Practical Phase Shift Optimization in RIS-assisted Networks over Short Packet Communications","R. Hashemi; S. Ali; E. M. Taghavi; N. H. Mahmood; M. Latva-Aho","Centre for Wireless Communications (CWC), University of Oulu, Oulu, Finland; Centre for Wireless Communications (CWC), University of Oulu, Oulu, Finland; Centre for Wireless Communications (CWC), University of Oulu, Oulu, Finland; Centre for Wireless Communications (CWC), University of Oulu, Oulu, Finland; Centre for Wireless Communications (CWC), University of Oulu, Oulu, Finland","2022 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit)","8 Jul 2022","2022","","","518","523","We study the practical phase shift design in a non-ideal reconfigurable intelligent surface (RIS)-aided ultra-reliable and low-latency communication (URLLC) system under finite blocklength (FBL) regime by leveraging a novel deep reinforcement learning (DRL) algorithm named as twin-delayed deep deterministic policy gradient (TD3). First, assuming industrial automation system with multiple actuators, the signal-to-interference-plus-noise ratio (SINR) and achievable rate in FBL regime are identified for each actuator in terms of the phase shift configuration matrix at the RIS. The channel state information (CSI) variations due to feedback delay are also considered that result in channel coefficients’ obsolescence. Then, the problem framework is proposed where the objective is to maximize the total achievable FBL rate in all ACs, subject to the practical phase shift constraint at the RIS elements. Since the problem is intractable to solve using conventional optimization methods, we resort to employing an actor-critic policy gradient DRL algorithm based on TD3, which relies on interacting RIS with FA environment by taking actions which are the phase shifts at the RIS elements, to maximize the expected observed reward, which is defined as the total FBL rate. The numerical results show that optimizing the practical phase shifts in the RIS via the proposed TD3 method is highly beneficial to improve the network total FBL rate in comparison with typical DRL methods.","2575-4912","978-1-6654-9871-5","10.1109/EuCNC/6GSummit54941.2022.9815804","Academy of Finland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815804","Block error probability;deep reinforcement learning (DRL);finite blocklength (FBL);factory automation;reconfigurable intelligent surface (RIS);twin delayed DDPG (TD3);ultra-reliable low-latency communications (URLLC)","Actuators;Uncertainty;Quantization (signal);Reinforcement learning;Ultra reliable low latency communication;Reconfigurable intelligent surfaces;Production facilities","6G mobile communication;deep learning (artificial intelligence);optimisation;reinforcement learning;telecommunication computing;telecommunication network reliability","RIS elements;DRL methods;practical phase shift optimization;RIS-assisted networks;short packet communications;practical phase shift design;finite blocklength regime;deep reinforcement learning algorithm;twin-delayed deep deterministic policy gradient;industrial automation system;FBL regime;phase shift configuration matrix;channel state information variations;channel coefficients;total achievable FBL rate;practical phase shift constraint;actor-critic policy gradient DRL algorithm;nonideal reconfigurable intelligent surface-aided ultra-reliable and low-latency communication system","","3","","20","IEEE","8 Jul 2022","","","IEEE","IEEE Conferences"
"Design of a Neural Controller Using Reinforcement Learning to Control a Rotational Inverted Pendulum","D. Brown; M. Strube","Institut für Kommunikationssysteme und Technologien, Ostfalia University of Applied Sciences, Wolfenbüttel, Germany; Institut für Kommunikationssysteme und Technologien, Ostfalia University of Applied Sciences, Wolfenbüttel, Germany","2020 21st International Conference on Research and Education in Mechatronics (REM)","12 Jan 2021","2020","","","1","5","Artificial intelligence (AI) has an increasing influence in the manufacturing and processing industries. An increasingly interesting area of application is the design of controllers for technical systems through reinforcement learning. This allows the design of a controller with reduced effort for human experts, which is normally involved in the design process, for example by eliminating the determination of suitable controller parameters. In this paper a neural controller for a rotational inverted pendulum is developed by using artificial neural networks in combination with reinforcement learning. The rotational inverted pendulum is a classic example of a nonlinear and unstable system and has been little covered in previous publications on this subject. Furthermore, the behaviour of the neural controller is compared with a conventional controller when swinging up and balancing a rotational inverted pendulum.","","978-1-7281-6224-9","10.1109/REM49740.2020.9313887","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9313887","machine learning;reinforcement learning;rotational inverted pendulum;artificial neural network","Artificial neural networks;Training;Reinforcement learning;Control engineering;Torque;Task analysis;Process control","control engineering computing;learning (artificial intelligence);neurocontrollers;nonlinear control systems;pendulums","neural controller;reinforcement learning;rotational inverted pendulum;artificial intelligence;processing industries;design process;suitable controller parameters;artificial neural networks;conventional controller","","1","","10","IEEE","12 Jan 2021","","","IEEE","IEEE Conferences"
"Idle Vehicle Rebalancing in Semiconductor Fabrication Using Factorized Graph Neural Network Reinforcement Learning","K. Ahn; J. Park","Department of Industrial and Systems Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; Department of Industrial and Systems Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea","2019 IEEE 58th Conference on Decision and Control (CDC)","12 Mar 2020","2019","","","132","138","With the advancement in the semiconductor industry, the size of fab becomes larger and thus more overhead hoist transportation (OHT) vehicles need to be operated, which necessitate efficient operation strategies for a large number of OHTs. In this study, we propose a cooperative rebalancing strategy of OHTs to increase the overall productivity of the material handling process in the fab. We discretize the fab into a number of zones and derives decentralized rebalancing strategies for each zone by applying a graph neural network (GNN) based multi-agent reinforcement learning (MARL). The proposed algorithm first represents the overall state of the fab into a directed graph and uses the graph representation to construct embedding values for each zone. The node embedding values are then used to determine the rebalancing action from each zone in a decentralized manner but to induce cooperation among zones. Simulation studies have shown that the proposed algorithm is effective in increasing various system-level key performance metrics compared to other heuristic and learning-based rebalancing strategies.","2576-2370","978-1-7281-1398-2","10.1109/CDC40024.2019.9030245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9030245","","Computational modeling;Neural networks;Load modeling;Productivity;Task analysis;Public transportation;Numerical models","directed graphs;hoists;learning (artificial intelligence);materials handling;multi-agent systems;neural nets;production engineering computing;semiconductor industry","idle vehicle rebalancing;semiconductor fabrication;semiconductor industry;overhead hoist transportation vehicles;OHT;material handling process;multiagent reinforcement learning;directed graph;graph representation;node embedding values;graph neural network","","2","","11","IEEE","12 Mar 2020","","","IEEE","IEEE Conferences"
"Data-Driven Grinding Control Using Reinforcement Learning","L. Guo; H. Wang; J. Zhang","School of Physical Sciences & Computing, The University of Central Lancashire, Preston, United Kingdom; Ansteel Mining Corporation, Anshan, Liaoning, P.R China; Ansteel Mining Corporation, Anshan, Liaoning, P.R China","2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)","3 Oct 2019","2019","","","2817","2824","In the mineral industry, the grinding circuit (GC) is the most critical unit for mineral processing operations. The goal for GC control optimisation is to ensure the outputs of the controlled processes best follow the control actions and to ensure that the grinding product quality and efficiency are well controlled within the optimal ranges. However, it is hard to achieve these goals at the level of basic feedback control where global operational indices are not considered. Therefore, the higher-level advanced control mechanism is required for grinding operations. In this paper, we present our work using a big data driven and reinforcement learning-based approach for optimising GC processes. With our approach, it is not necessary to manually construct a system process model as it can be learnt from the historical GC log data automatically. To evaluate our method, a series of experiments have been conducted, and the experiment results show evident enhancement with regards to both product quality and grinding process efficiency.","","978-1-7281-2058-4","10.1109/HPCC/SmartCity/DSS.2019.00395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8855498","Optimal Control;Reinforcement Learning;Actor-Critic;Proximal Policy Optimisation","Process control;Ores;Optimization;Economics;Mathematical model;Throughput","feedback;grinding;learning (artificial intelligence);mineral processing industry;product quality;production control","grinding process efficiency;data-driven grinding control;mineral industry;grinding circuit;critical unit;mineral processing operations;GC control optimisation;controlled processes;grinding product quality;optimal ranges;basic feedback control;global operational indices;higher-level;control mechanism;grinding operations;big data;reinforcement learning-based approach;optimising GC processes;system process model;historical GC","","2","","43","IEEE","3 Oct 2019","","","IEEE","IEEE Conferences"
"Multi-Objective Optimization of AGV Real-Time Scheduling Based on Deep Reinforcement Learning","P. Durst; X. Jia; L. Li","Department of Control Science & Engineering, Tongji University, Shanghai, China; Department of Control Science & Engineering, Tongji University, Shanghai, China; Department of Control Science & Engineering, Tongji University, Shanghai, China","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","5535","5540","Driven by the trend away from mass production towards more customization and individualism, manufacturing is under massive price pressure. Therefore, continuously improving production efficiency and reducing costs have always been the focus of manufacturing companies. With recent advances in Industry 4.0 and industrial Artificial Intelligence (AI), Automated Guided Vehicles (AGVs) have become a very promising technology to support this trend. Nowadays, they are widely used in job shop environments for material handling. However, this promising technology goes along with various new challenges, such as the high dynamics, complexity, and uncertainty of the job shop environment for AGV scheduling. To address these challenges, an adaptive Deep Reinforcement Learning (DRL) based AGV real-time scheduling approach is approached to optimize several efficiency parameters of the overall job shop system. Firstly, the DRL optimization problem is formulated as Markov Decision Process (MDP). The state and action representation, reward, and optimal policy function are described in detail. Then a novel DRL method is further developed to achieve the optimal mixed rule policy. For that, a novel RL algorithm, Proximal Policy Optimization (PPO), is applied to the Deep Neural Network (DNN) and implemented in TensorForce, a reinforcement learning package in Python based on TensorFlow. This algorithm is compared to conventional heuristic rules. Furthermore, SimPy, a relatively new discrete-event simulation package in Python, is used to implement the job shop environment. The job shop environment is based on a real-world scenario of a semiconductor factory, which is implemented in a simplified manner and applied to the DRL agent. The results are presented afterward, followed by a feasibility and effectiveness analysis of this approach.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240797","National Natural Science Foundation of China(grant numbers:62003245,72171172,62088101); Shanghai Municipal Science and Technology, China Major Project(grant numbers:2021SHZDZX0100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240797","automated guided vehicles;multi-objective optimization;real-time scheduling;deep reinforcement learning","Industries;Job shop scheduling;Uncertainty;Manufacturing processes;Heuristic algorithms;Reinforcement learning;Market research","","","","","","28","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Data-Driven Nonzero-Sum Game for Discrete-Time Systems Using Off-Policy Reinforcement Learning","Y. Yang; S. Zhang; J. Dong; Y. Yin","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China","IEEE Access","23 Jan 2020","2020","8","","14074","14088","In this paper, we develop a data-driven algorithm to learn the Nash equilibrium solution for a two-player non-zero-sum (NZS) game with completely unknown linear discrete-time dynamics based on off-policy reinforcement learning (RL). This algorithm solves the coupled algebraic Riccati equations (CARE) forward in time in a model-free manner by using the online measured data. We first derive the CARE for solving the two-player NZS game. Then, model-free off-policy RL is developed to obviate the requirement of complete knowledge of system dynamics. Besides, on- and off-policy RL algorithms are compared in terms of the robustness against the probing noise. Finally, a simulation example is presented to show the efficacy of the presented approach.","2169-3536","","10.1109/ACCESS.2019.2960064","National Natural Science Foundation of China(grant numbers:61903028,61673055,61773053,61873024); China Postdoctoral Science Foundation(grant numbers:2018M641197); Fundamental Research Funds for the Central Universities(grant numbers:FRF-TP-18-031A1,FRF-BD-19-002A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933509","Coupled algebraic Riccati equations;off-policy reinforcement learning;nonzero-sum game","Games;Nash equilibrium;Heuristic algorithms;Discrete-time systems;Mathematical model;System dynamics","continuous time systems;discrete time systems;game theory;learning (artificial intelligence);linear systems;Riccati equations","on-policy algorithms;discrete-time systems;data-driven nonzero-sum game;off-policy RL algorithms;system dynamics;model-free off-policy RL;two-player NZS game;coupled algebraic Riccati equations;off-policy reinforcement;completely unknown linear discrete-time dynamics;two-player nonzero-sum game;Nash equilibrium solution","","4","","56","CCBY","16 Dec 2019","","","IEEE","IEEE Journals"
"Reinforcement Learning Behavioral Control for Nonlinear Autonomous System","Z. Zhang; Z. Mo; Y. Chen; J. Huang","College of Electrical Engineering and Automation, Fuzhou University, Fuzhou; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou","IEEE/CAA Journal of Automatica Sinica","23 Aug 2022","2022","9","9","1561","1573","Behavior-based autonomous systems rely on human intelligence to resolve multi-mission conflicts by designing mission priority rules and nonlinear controllers. In this work, a novel two-layer reinforcement learning behavioral control (RLBC) method is proposed to reduce such dependence by trial-and-error learning. Specifically, in the upper layer, a reinforcement learning mission supervisor (RLMS) is designed to learn the optimal mission priority. Compared with existing mission supervisors, the RLMS improves the dynamic performance of mission priority adjustment by maximizing cumulative rewards and reducing hardware storage demand when using neural networks. In the lower layer, a reinforcement learning controller (RLC) is designed to learn the optimal control policy. Compared with existing behavioral controllers, the RLC reduces the control cost of mission priority adjustment by balancing control performance and consumption. All error signals are proved to be semi-globally uniformly ultimately bounded (SGUUB). Simulation results show that the number of mission priority adjustment and the control cost are significantly reduced compared to some existing mission supervisors and behavioral controllers, respectively.","2329-9274","","10.1109/JAS.2022.105797","National Natural Science Foundation of China(grant numbers:61603094); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9865036","Behavioral control;mission supervisor;nonlinear autonomous system;reinforcement learning","Costs;Autonomous systems;Heuristic algorithms;Simulation;Human intelligence;Optimization methods;Reinforcement learning","control system synthesis;neurocontrollers;nonlinear control systems;optimal control;reinforcement learning","nonlinear autonomous system;multimission conflicts;mission priority rules;nonlinear controllers;trial-and-error learning;reinforcement learning mission supervisor;RLMS;optimal mission priority;mission priority adjustment;cumulative rewards;reinforcement learning controller;optimal control policy;control cost;control performance;two-layer reinforcement learning behavioral control;hardware storage demand;semiglobally uniformly ultimately bounded;SGUUB;RLC;neural networks","","4","","46","","23 Aug 2022","","","IEEE","IEEE Journals"
"Three-Dimensional Area Coverage with UAV Swarm based on Deep Reinforcement Learning","Z. Mou; Y. Zhang; F. Gao; H. Wang; T. Zhang; Z. Han","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","ICC 2021 - IEEE International Conference on Communications","6 Aug 2021","2021","","","1","6","In this paper, we study the fast coverage problem of 3D irregular terrain surfaces with a hierarchical UAV swarm. We first build a 3D model of a random irregular terrain and project the 3D terrain surface into many weighted 2D patches. Then we develop a two-level hierarchical UAV swarm architecture, including the low-level follower UAVs (FUAVs) and the high-level leader UAVs (LUAVs). For FUAVs, we adopt the traditional coverage trajectory algorithm to carry out specific coverage tasks within patches based on the star communication topology. For LUAVs, we propose a swarm deep Q-learning (SDQN) reinforcement learning algorithm to select patches. The numerical results show that the total coverage time of the SDQN is less than that of existing methods, which demonstrates the effectiveness of the proposed algorithm.","1938-1883","978-1-7281-7122-7","10.1109/ICC42927.2021.9500895","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9500895","","Solid modeling;Three-dimensional displays;Conferences;Reinforcement learning;Trajectory;Topology;Task analysis","aerospace engineering;autonomous aerial vehicles;control engineering computing;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;motion control;multi-robot systems;path planning","three-dimensional area coverage;deep reinforcement learning;3D irregular terrain surfaces;two-level hierarchical UAV swarm architecture;LUAV;coverage trajectory algorithm;star communication topology;swarm deep Q-learning reinforcement learning;FUAV;high-level leader UAV","","2","","22","IEEE","6 Aug 2021","","","IEEE","IEEE Conferences"
"Car-Following Safe Headway Strategy with Battery-Health Conscious: A Reinforcement Learning Approach","X. Jia; J. Peng; Y. Liu; B. Liu; P. Wang; Y. Lu; M. Wen; Z. Huang","School of Automation, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; Changsha College for Preschool Education, Changsha, China; School of Automation, Central South University, Changsha, China","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","14 Dec 2020","2020","","","2432","2437","This paper proposes an optimal car-following strategy for pure electric vehicles (EVs) with the aim of keeping an expected headway of the leader and reducing vehicle battery loss. In particular, a car-following system model is established. The primary task of the automatic vehicle is to follow the trajectory of the preceding car and maintain an expected headway. Then, the paper analyzes the powertrain of the electric vehicle. The loss of battery life over a period of time is proportional to the acceleration, so it takes the battery life into consideration. The Q-learning algorithm is conducted for the optimal car-following strategy using system data instead of system dynamics information. It utilizes reward function and greedy strategy to select actions to train the following vehicle to achieve car-following safety. When there is no collision in these two cars, acceleration is considered into reward function to reduce battery loss. Finally, it is verified by simulation that the proposed car-following strategy can keep good tracking, maintain the expected headway from the preceding vehicle, and reduce battery loss.","2577-1655","978-1-7281-8526-2","10.1109/SMC42975.2020.9283452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283452","Car-following system;Q-learning algorithm;safe headway;battery loss","Tracking;Electric vehicles;Batteries;Safety;Trajectory;Acceleration;Automobiles","automobiles;battery powered vehicles;control engineering computing;learning (artificial intelligence);power transmission (mechanical);road safety;road traffic control;traffic engineering computing;trajectory control","car-following safe headway strategy;battery-health conscious;reinforcement learning;car-following system model;automatic vehicle;electric vehicle;battery life;Q-learning;system data;reward function;greedy strategy;car-following safety;vehicle battery loss;trajectory;powertrain","","2","","19","IEEE","14 Dec 2020","","","IEEE","IEEE Conferences"
"Optimized Formation Control for a Class of Second-order Multi-agent Systems based on Single Critic Reinforcement Learning Method","W. Shao; Y. Chen; J. Huang","College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China","2021 IEEE International Conference on Networking, Sensing and Control (ICNSC)","10 Feb 2022","2021","1","","1","6","In this paper, an optimized formation control based on single critic reinforcement learning is developed for a class of second-order multi-agent systems. Unlike first-order systems, both position and velocity variables need to be considered in second-order system control. Therefore, the control of second-order systems is more challenging. In the control design, single critic reinforcement learning method combined with fuzzy logic systems is used. Fuzzy logic systems approximator is used to compensate the nonlinearity of the systems. Compared with the actor-critic reinforcement learning method, single critic reinforcement learning requires only one network iterative training such that the training errors are smaller, and the calculation time caused by the iterative loop between actor and critic can be reduced. According to the analysis of Lyapunov stability theory, the proposed control design can achieve the control objective. Finally, the effectiveness of the proposed method is verified by simulation.","","978-1-6654-4048-6","10.1109/ICNSC52481.2021.9702159","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9702159","Single critic network;reinforcement learning(RL);optimized formation control;multi-agent systems(MASs);fuzzy logic systems(FLSs)","Training;Fuzzy logic;Control design;Simulation;Reinforcement learning;Stability analysis;Sensors","adaptive control;control system synthesis;fuzzy control;fuzzy logic;learning (artificial intelligence);Lyapunov methods;multi-agent systems;multi-robot systems;nonlinear control systems;position control;stability","optimized formation control;second-order multiagent systems;single critic reinforcement learning method;first-order systems;second-order system control;second-order systems;control design;fuzzy logic systems approximator;actor-critic reinforcement learning method;control objective","","1","","13","IEEE","10 Feb 2022","","","IEEE","IEEE Conferences"
"Cross-overlapping Hierarchical Reinforcement Learning in Humanoid Robots","K. Chen; Z. Liang; W. Liang; H. Zhou; L. Chen; S. Qin","College of Automation, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Automation, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Automation, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Automation, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Automation, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Automation, Nanjing University of Posts and Telecommunications, Nanjing, China","2021 33rd Chinese Control and Decision Conference (CCDC)","30 Nov 2021","2021","","","3340","3345","In the RoboCup3D project, how to make the humanoid robot with faster running speed and more accurate kicking action is a popular research direction. In this paper, we extend the Overlapping Layered Learning method by proposing a cross-overlapping hierarchical reinforcement learning method, which is based on overlapping layered learning to smooth the action articulation by cross-learning the articulated action parameters or cross-learning the higher-level action parameters to obtain better action execution. The article also introduces the baseline-based optimization technique and elaborates the specific optimization strategy and optimization task. Finally, the effectiveness of cross-overlapping hierarchical reinforcement learning and baseline-based optimization techniques is demonstrated experimentally.","1948-9447","978-1-6654-4089-9","10.1109/CCDC52312.2021.9602590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9602590","RoboCup;Soccer robots;Cross-overlapping Hierarchical Reinforcement Learning;baseline-based optimization techniques;optimization framework","Deep learning;Learning systems;Humanoid robots;Optimization methods;Reinforcement learning;Coherence;Task analysis","humanoid robots;learning (artificial intelligence);mobile robots;multi-robot systems;optimisation","overlapping layered learning method;cross-overlapping hierarchical reinforcement learning method;action articulation;cross-learning;articulated action parameters;higher-level action parameters;action execution;baseline-based optimization technique;RoboCup3D project;humanoid robot;accurate kicking action","","1","","8","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs","Q. Zhang; Y. Yang; J. Ruan; X. Xiong; D. Xing; B. Xu","Chinese Academy of Sciences, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China","2023 International Joint Conference on Neural Networks (IJCNN)","2 Aug 2023","2023","","","1","8","Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into sub goal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on sub goal representation functions and sub goal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent sub goal representations and lack an efficient sub goal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance. Our code is available at https://github.com/papercode2022/HILL.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10190993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190993","Hierarchical Reinforcement Learning;Exploration and Exploitation;Representation Learning;Landmark Graph;Contrastive Learning","Representation learning;Visualization;Codes;Neural networks;Buildings;Reinforcement learning;Coherence","graph theory;reinforcement learning","asymptotic performance;continuous control tasks;contrastive representation learning objective;exploration-exploitation dilemma;GCHRL;goal-conditioned hierarchical reinforcement learning;hierarchical reinforcement learning via dynamically building latent landmark graphs;HILL;latent subgoal representations;subgoal conditional subtasks;subgoal representation functions;subgoal selection strategy;subgoal space;temporal coherence","","","","35","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Multi-UAV Cooperative Short-Range Combat via Attention-Based Reinforcement Learning using Individual Reward Shaping","T. Zhang; T. Qiu; Z. Liu; Z. Pu; J. Yi; J. Zhu; R. Hu","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Key Laboratory of Science and Technology on Aerospace Intelligent Control, Beijing Aerospace Automatic Control Institute, China","2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","26 Dec 2022","2022","","","13737","13744","In this paper, we propose a novel distributed method based on attention-based deep reinforcement learning using individual reward shaping, for multiple unmanned aerial vehicles (UAVs) cooperative short-range combat mission. Specifically, a two-level attention distributed policy, composed of observation-level and communication-level attention networks, is designed to enable each UAV to selectively focus on important environmental features and messages, for enhancing the effectiveness of the cooperative policy. Moreover, due to the high complexity and stochasticity of the UAV combat mission, the learning of UAVs is tricky and low efficient. To embed knowledge to accelerate the policy learning, a potential-based individual reward function is constructed by implicitly translating the individual reward into the specific form of dynamic action potentials. In addition, an actor-critic training algorithm based on the centralized training and decentralized execution framework is adopted to train the policy network of UAV maneuver decision. We build a three-dimensional UAV simulation and training platform based on Unity for multi-UAV short-range combat missions. Simulation results demonstrate the effectiveness of the proposed method and the superiority of the attention policy and individual reward shaping.","2153-0866","978-1-6654-7927-1","10.1109/IROS47612.2022.9982096","National Key Research and Development Program of China(grant numbers:2018AAA0102405,2018AAA0102402); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982096","","Training;Solid modeling;Action potentials;Simulation;Heuristic algorithms;Reinforcement learning;Autonomous aerial vehicles","aircraft control;autonomous aerial vehicles;learning (artificial intelligence);multi-robot systems;remotely operated vehicles","attention policy;attention-based deep reinforcement learning;attention-based reinforcement learning;communication-level attention networks;important environmental features;individual reward shaping;multiple unmanned aerial vehicles cooperative short-range combat mission;multiUAV short-range combat missions;novel distributed method;observation-level;policy learning;policy network;potential-based individual reward function;training platform;two-level attention distributed policy;UAV combat mission;UAV maneuver decision","","","","26","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"A Goal-Conditioned Reinforcement Learning Algorithm with Environment Modeling","Z. Yu; K. Sun; C. Li; D. Zhong; Y. Yang; Q. Zhao","Department of Automation, Center for Intelligent and Networked Systems, BNRist, Tsinghua University, Beijing, China; Department of Automation, Center for Intelligent and Networked Systems, BNRist, Tsinghua University, Beijing, China; Department of Automation, Center for Intelligent and Networked Systems, BNRist, Tsinghua University, Beijing, China; Department of Automation, Center for Intelligent and Networked Systems, BNRist, Tsinghua University, Beijing, China; Department of Automation, Center for Intelligent and Networked Systems, BNRist, Tsinghua University, Beijing, China; Department of Automation, Center for Intelligent and Networked Systems, BNRist, Tsinghua University, Beijing, China","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","8763","8768","Goal-conditioned Reinforcement Learning (GcRL) has achieved remarkable success in navigating towards goals in recent years. However, learning efficiency and generalization ability remain challenging issues when dealing with uncertain motion patterns of dynamic objects in the environment. To address these issues, existing model-based GcRL algorithms with the environmental prediction have become the main stream. However, these methods are limited for capturing uncertain motion patterns. In fact, humans will consider explicit knowledge, including environmental prediction and policy priors, when making decisions. To improve navigation ability in uncertain environments, we present a novel approach that integrates the policy priors model, which is utilized to infer navigational direction based on the agent's successful historical trajectories, into the model-based GcRL algorithms. Our experimental results demonstrate that our approach is more reliable in navigating towards goals, and outperforms competitive prior works in challenging environments.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240963","Environmental Prediction;Policy Priors;Goal Navigation;Deep Reinforcement Learning","Navigation;Scalability;Cloning;Reinforcement learning;Self-supervised learning;Predictive models;Prediction algorithms","","","","","","34","","18 Sep 2023","","","IEEE","IEEE Conferences"
"LNOA: A Real-time Obstacle Avoidance Motion Planning Method for Redundant Manipulator Based on Reinforcement Learning","Z. Huang; G. Chen; Y. Shen; Y. Liu; H. You; T. Li","School of Automation, Beijing University of Posts and Telecommunications, Beijing, China; School of Automation, Beijing University of Posts and Telecommunications, Beijing, China; School of Automation, Beijing University of Posts and Telecommunications, Beijing, China; School of Automation, Beijing University of Posts and Telecommunications, Beijing, China; School of Automation, Beijing University of Posts and Telecommunications, Beijing, China; School of Automation, Beijing University of Posts and Telecommunications, Beijing, China","2022 International Conference on Service Robotics (ICoSR)","30 May 2023","2022","","","1","6","Aiming at the redundant manipulator operation task that needs to ensure the end-effector trajectory tracking as much as possible in the dynamic obstacle scene, a loose null-space obstacle avoidance (LNOA) method based on reinforcement learning (RL) is proposed. Firstly, the joint motion is decomposed into trajectory tracking motion and loose null-space obstacle avoidance motion, and the latter is further decomposed into joint null-space motion and end-effector slack motion; on this basis, LNOA framework for obstacle avoidance is designed. Secondly, the RL method is introduced to learn the loose null-space obstacle avoidance motion generation strategy, so as to generate the end-effector slack component and joint null-space component autonomously, which is then combined with the trajectory tracking component to realize obstacle avoidance and end-effector trajectory maintenance simultaneously. Finally, the simulation is conducted to verify the effectiveness of the proposed LNOA method.","","978-1-6654-5438-4","10.1109/ICoSR57188.2022.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136939","redundant manipulator;obstacle avoidance;null-space;loose constraint;reinforcement learning","Training;Trajectory tracking;Reinforcement learning;End effectors;Real-time systems;Trajectory;Planning","collision avoidance;end effectors;mobile robots;path planning;redundant manipulators;reinforcement learning","dynamic obstacle scene;end-effector slack component;end-effector slack motion;end-effector trajectory maintenance;end-effector trajectory tracking;joint motion;joint null-space component;joint null-space motion;LNOA framework;LNOA method;loose null-space obstacle avoidance method;loose null-space obstacle avoidance motion generation strategy;real-time obstacle avoidance motion planning method;redundant manipulator operation task;reinforcement learning;RL method;trajectory tracking component;trajectory tracking motion","","","","12","IEEE","30 May 2023","","","IEEE","IEEE Conferences"
"A Power System Corrective Control Method Based on Evolutionary Reinforcement Learning","H. Zhang; P. Xu; K. Zhang; H. Zhao; Y. Dai; T. Gao; J. Zhang","School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China","IEEE Journal of Radio Frequency Identification","12 Dec 2022","2022","6","","815","819","Corrective control becomes more and more important for power systems due to the increasing penetration of renewable energy. As an effective method in control problems, the Reinforcement learning method is considered to provide decisions for corrective control in power networks. However, the large size of action and state space, as well as the sparse reward problem in corrective control limits the application of the RL method. This paper proposed an evolutionary reinforcement learning method. Combining the evolutionary algorithm and Reinforcement learning methods, this method decreases the training difficulty of reinforcement learning. The experiments based on the Grid2op environment show that the proposed method has a longer operation time than other traditional methods in the corrective control of power networks.","2469-7281","","10.1109/JRFID.2022.3205359","National Key Research and Development Program of China(grant numbers:2018AAA0101504); Science and Technology Project of State Grid Corporation of China (SGCC): Fundamental Theory of Human-in-the-Loop Hybrid Augmented Intelligence for Power Grid Dispatch and Control; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9888772","Corrective control;evolutionary algorithm;reinforcement learning;power network","Power transmission lines;Reinforcement learning;Generators;Aerospace electronics;Evolutionary computation;Reinforcement learning;Social factors","control engineering computing;evolutionary computation;power engineering computing;power grids;reinforcement learning;state-space methods","control problems;evolutionary reinforcement learning method;Grid2op environment;power networks;power system corrective control method;RL method","","","","21","IEEE","13 Sep 2022","","","IEEE","IEEE Journals"
"Topology Derivation of Multiport DC–DC Converters Based on Reinforcement Learning","M. Dong; R. Liang; J. Yang; C. Xu; D. Song; J. Wan","School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China","IEEE Transactions on Power Electronics","16 Feb 2023","2023","38","4","5055","5064","Multiport dc–dc convertersare attracting wide attention in various applications. However, conventional topology derivation methods for multiport dc–dc converters are usually intricate and time-consuming. In this article, a reinforcement learning (RL)-based topology derivation method is proposed, which can derive topologies of complex converters quickly. To apply the RL framework, the topology derivation process is regarded as a Markov decision process. In each step, the agent selects and connects two components until a complete topology is made. To ensure that the derived topologies are feasible and match given voltage specifications, basic circuit constraints and duty cycle constraints are added as hard constraints. Soft constraints are also added to obtain optimal circuits with low voltage stresses or low current stresses. The testing on topology derivation of multiport dc–dc converters shows the great speed of the proposed method. Simulation and experimental results verify that the derived topologies cannot only satisfy given voltage specifications, but also achieve optimization targets of low voltage stresses or current stresses.","1941-0107","","10.1109/TPEL.2023.3235053","National Natural Science Foundation of China(grant numbers:52177204,51777217); Natural Science Foundation of Hunan Province(grant numbers:2020JJ4744); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10012061","Multiport dc–dc converters;reinforcement learning (RL);topology derivation","Topology;Voltage;Stress;Computer architecture;Optimization;Network topology;Voltage control","DC-DC power convertors;Markov processes;optimisation;power engineering computing;reinforcement learning","circuit constraints;complex converters;current stress;duty cycle constraints;Markov decision process;multiport dc-dc converters;optimal circuits;reinforcement learning-based topology derivation method;RL framework;voltage specifications;voltage stress","","","","27","IEEE","9 Jan 2023","","","IEEE","IEEE Journals"
"A Deep Reinforcement Learning Method with Action Switching for Autonomous Navigation","Z. Wang; X. Liao; F. Zhang; M. Xu; Y. Liu; X. Liu; X. Zhang; R. W. Dong; Z. Li","School of Automation and Key Laboratory for Intelligent Control & Decision on Complex Systems, Beijing Institute of Technology, Beijing, P. R. China; School of Automation and Key Laboratory for Intelligent Control & Decision on Complex Systems, Beijing Institute of Technology, Beijing, P. R. China; Beijing Aerospace Automatic Control Institute, P. R. China; Beijing Aerospace Automatic Control Institute, P. R. China; School of Automation and Key Laboratory for Intelligent Control & Decision on Complex Systems, Beijing Institute of Technology, Beijing, P. R. China; School of Automation and Key Laboratory for Intelligent Control & Decision on Complex Systems, Beijing Institute of Technology, Beijing, P. R. China; School of Automation and Key Laboratory for Intelligent Control & Decision on Complex Systems, Beijing Institute of Technology, Beijing, P. R. China; Capital Aerospace Machinery Co., Ltd, Beijing; School of Automation and Key Laboratory for Intelligent Control & Decision on Complex Systems, Beijing Institute of Technology, Beijing, P. R. China","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","3491","3496","Stochastic policy-based deep reinforcement learning (DRL) has successfully gained the widespread application but demands plenty of stochastic exploration to learn the environment at the initial training stage. When the agent is exposed to more complex environment, not only is the methodology inefficient, but its performance may also suffer from the issue of high variance. This paper develops a framework to accelerate the training procedure and reduce the variance by introducing a stochastic switching network, which specifically allows the agent to choose between heuristic actions and actions output by proximal policy optimization (PPO) algorithm. Instead of starting from the random actions, the agent can be effectively guided by the heuristic actions so that the navigation capability of the agent can be rapidly bootstrapped. The vanilla policy gradient (VPG) algorithm is further utilized to train the switching network, which can be jointly trained with the baseline PPO. By the experimental comparison with the baseline PPO in the customized maze environment with openAI Gym toolkit, our method greatly contributes to the more efficient execution of navigation task by means of the heuristic actions for guidance.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9549631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9549631","Robot navigation;proximal policy optimization (PPO);deep reinforcement learning (DRL);Vanilla policy gradient (VPG);action switching","Training;Navigation;Heuristic algorithms;Switches;Reinforcement learning;Control systems;Trajectory","control engineering computing;deep learning (artificial intelligence);gradient methods;metaheuristics;mobile robots;multi-agent systems;navigation;path planning;robot programming;stochastic processes","bootstrapping;openAI Gym toolkit;VPG algorithm;PPO;proximal policy optimization;stochastic switching network;stochastic exploration;stochastic policy-based deep reinforcement learning;autonomous navigation;action switching;heuristic actions;customized maze environment;vanilla policy gradient algorithm","","","","17","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Quadruped Reinforcement Learning without Explicit State Estimation","Q. Li; G. Dong; R. Qin; J. Chen; K. Xu; X. Ding","Robotics Institute, Beihang University, School of Mechanical Engineering and Automation, Beijing, China; Robotics Institute, Beihang University, School of Mechanical Engineering and Automation, Beijing, China; Robotics Institute, Beihang University, School of Mechanical Engineering and Automation, Beijing, China; Robotics Institute, Beihang University, School of Mechanical Engineering and Automation, Beijing, China; Robotics Institute, Beihang University, School of Mechanical Engineering and Automation, Beijing, China; Robotics Institute, Beihang University, School of Mechanical Engineering and Automation, Beijing, China","2022 IEEE International Conference on Robotics and Biomimetics (ROBIO)","18 Jan 2023","2022","","","1989","1994","Reinforcement learning is a promising approach to developing legged robot locomotion controllers. The gen-eral process of development is: large-scale training in the virtual simulation environment to obtain reliable control policy network, and then the policy network is deployed to real legged robot. In the training procedure, a complete robot state increases the speed of training and the stability of policy. The robot's states like body velocities are easily available in simulation training, but they are difficult to obtain in the real robot, hence specifically designed robot state estimators are needed. However, the development of state estimators requires expert knowledge related to control theory and robotics, limiting the direct application of reinforcement learning to robots. To take advantage of the end-to-end mapping of artificial neural networks, we simplified the existing reinforcement learning process for quadruped robots and propose a training method based on curriculum learning in this work. The proposed method can produce a reliable policy that does not require robot state estimator and only take raw sensors data. The feasibility of the proposed method is verified in simulations and real quadrupedal robot. Video of the quadrupedal robot is available at www.youtube.com/watch?v=-iho4KIlEPw.","","978-1-6654-8109-0","10.1109/ROBIO55434.2022.10011765","National Natural Science Foundation of China(grant numbers:51775011,91748201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011765","","Training;Legged locomotion;Process control;Reinforcement learning;Robot sensing systems;Sensors;Control theory","computer simulation;control engineering computing;control system synthesis;legged locomotion;motion control;neural nets;robot programming;stability;state estimation;virtual reality","artificial neural networks;curriculum learning;end-to-end mapping;expert knowledge;explicit state estimation;legged robot locomotion controllers;policy stability;quadruped reinforcement learning;quadrupedal robot;robot state estimator design;virtual simulation;virtual simulation environment","","","","20","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"Multi-USV System Antidisturbance Cooperative Searching Based on the Reinforcement Learning Method","Y. Liu; C. Chen; D. Qu; Y. Zhong; H. Pu; J. Luo; Y. Peng; J. Xie; R. Zhou","School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; State Key Laboratory of Mechanical Transmission, Chongqing University, Chongqing, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China","IEEE Journal of Oceanic Engineering","13 Oct 2023","2023","48","4","1019","1047","The remarkable advantages of multiple unmanned surface vehicle (USV) systems for operational efficiency, accuracy, and robustness give them great application prospects in future maritime search missions. However, at this stage, reasonable mission models, efficient control methods, and effective verification have become the key problems that must be solved in the practical application of multi-USV systems in maritime search missions. Aiming at these problems, we establish a dynamic searching and updating mission model in this article. On the basis of the model, a cooperative search control method and an antidisturbance navigation control method based on the reinforcement learning method are proposed. By using the two methods, the cooperative search control and antidisturbance navigation control of a multi-USV system are combined for the first time. We design corresponding simulations and field tests to verify the proposed methods. The test results show that compared with other methods, the proposed methods give the multi-USV system certain advantages in search efficiency and accuracy.","1558-1691","","10.1109/JOE.2023.3281630","National Natural Science Foundation of China(grant numbers:U2013202,62325302); Shanghai Education Development Foundation; Shanghai Municipal Education Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261771","Multiple unmanned surface vehicle (USV) system;reinforcement learning (RL) algorithm;target search;USV","Navigation;Search problems;Autonomous vehicles;Trajectory;Reinforcement learning;Marine vehicles","","","","","","52","IEEE","25 Sep 2023","","","IEEE","IEEE Journals"
"Symmetry-informed Reinforcement Learning and Its Application to the Attitude Control of Quadrotors","J. Huang; W. Zeng; H. Xiong; B. R. Noack; G. Hu; S. Liu; Y. Xu; H. Cao","School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China; School of Civil and Environmental Engineering, Harbin Institute of Technology Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China","IEEE Transactions on Artificial Intelligence","","2023","PP","99","1","14","Symmetry is ubiquitous in nature, physics, and mathematics. However, a classical symmetry-agnostic Reinforcement Learning (RL) approach cannot guarantee to respect symmetry. Researchers have shown that if the symmetry of a system cannot be respected, the performance of a symmetry-agnostic RL approach can be inhibited. To this end, this paper develops a generally applicable Neural Network (NN) module with symmetry that can enforce the symmetry of a system to be respected. Based on the NN module with symmetry, this paper proposes a symmetry-informed Model-Based RL (MBRL) approach that respects symmetry and improves data efficiency. The symmetry-informed MBRL approach is applied to the attitude control of a quadrotor in simulation to evaluate the effectiveness of the approach. The simulation results show that the data efficiency of the symmetry-informed MBRL approach is much superior to that of a symmetry-agnostic MBRL approach. An NN module with symmetry can respect the symmetry of a quadrotor while a naive NN cannot enforce the symmetry of a quadrotor to be respected.","2691-4581","","10.1109/TAI.2023.3249683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054413","Data efficiency;model-based reinforcement learning;neural network;quadrotor;symmetry","Quadrotors;Artificial neural networks;Data models;Attitude control;Torque;Rotors;Reinforcement learning","","","","","","","IEEE","27 Feb 2023","","","IEEE","IEEE Early Access Articles"
"Adapting a reinforcement learning method for the distributed blocking hybrid flow shop scheduling problem","H. Qin; Y. Wang; Y. Han; Q. Chen; J. Li","School of Computer Science, Liaocheng University, Liaocheng, China; School of Computer Science, Liaocheng University, Liaocheng, China; School of Computer Science, Liaocheng University, Liaocheng, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; School of information science and Engineering, Shandong Normal University, Jinan, China","2021 5th Asian Conference on Artificial Intelligence Technology (ACAIT)","14 Mar 2022","2021","","","751","757","With the continuous emission of energy in the past years, the environmental problems are becoming more and more serious. For example, in manufacturing, the energy efficient scheduling problem has become particularly prominent, and attracted much attention of the researchers. As a common scheduling problem in the real world, the research on distributed blocking hybrid flow shop (DBHFSP) is very few. In this paper, we will carry out a study of the problem. Because of its NP-hard character, therefore, we use the intelligent optimization algorithm to solve the problem. we firstly introduce the MILP model of the DBHFSP, then, the Q-learning method combined with the IG algorithm framework (IGQ) is proposed to solve this problem. In the experimental part, through the experiment results and comparison with other algorithms in the recent literatures, the proposed algorithm shows the excellent performance in the simulation experiment with the objective of minimizing the energy consumption.","","978-1-6654-2630-5","10.1109/ACAIT53529.2021.9731228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9731228","DBHFSP;Q-learning method;IG algorithm;energy-efficient","Energy consumption;Adaptation models;Q-learning;Job shop scheduling;Uncertainty;Heuristic algorithms;Dynamic scheduling","energy conservation;energy consumption;environmental factors;flow shop scheduling;integer programming;linear programming;production engineering computing;reinforcement learning","reinforcement learning method;distributed blocking hybrid flow shop scheduling problem;environmental problems;energy efficient scheduling problem;common scheduling problem;DBHFSP;intelligent optimization algorithm;Q-learning method;IG algorithm framework;energy consumption;NP-hard character;MILP model;IGQ","","","","17","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"A Data-Driven Solution for Energy Management Strategy of Hybrid Electric Vehicles Based on Uncertainty-Aware Model-Based Offline Reinforcement Learning","B. Hu; Y. Xiao; S. Zhang; B. Liu","Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing, China; Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing, China; Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing, China; Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing, China","IEEE Transactions on Industrial Informatics","24 May 2023","2023","19","6","7709","7719","Energy management strategy (EMS) is the key technology to improving the fuel efficiency of hybrid electric vehicles (HEV). In recent years, the development of artificial intelligence has enabled tremendous advances by utilizing reinforcement learning (RL) for training and deploying deep neural network-based EMS. However, in contrast to the fields of deep learning, such as computer vision and natural language processing, which mainly rely on large-scale offline datasets, most RL-based policies must be trained online by trial-and-error with the initial performance being almost arbitrary. Such a paradigm is considered inefficient and unsafe for industrial automation and can only be used to tackle the EMS problems in the simulation world. Considering that large historical interactive datasets are readily available in the EMS domain, if an RL algorithm can be used to extract a policy purely offline from the prior collected dataset and improve upon data logging policy, the current issues, including sample inefficiency, unsafe exploration, and simulation-to-real gap that prevent the widespread use of RL methods, could be mitigated to a great extent. To this end, this article presents a feasible algorithmic framework for model-based offline RL. Unlike vanilla RL approaches without any consideration against distributional shift, a data-driven dynamic model is built before the policy training using RL. After that, two techniques, namely, conservative MDP and state regularization, are augmented, which are proved to be effective against model overexploitation. By incorporating the guidance of uncertainty awareness, a near optimal policy can be obtained by using only the dataset from a suboptimal controller.","1941-0050","","10.1109/TII.2022.3213026","National Natural Science Foundation of China(grant numbers:51905061); China Postdoctoral Science Foundation(grant numbers:2020M671842); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9914662","Data-driven;energy management strategy (EMS);hybrid electric vehicle (HEV);model-based;offline reinforcement learning (RL)","Environmental management;Energy management;Hybrid electric vehicles;Informatics;Data models;Behavioral sciences;Batteries","artificial intelligence;deep learning (artificial intelligence);energy management systems;hybrid electric vehicles;learning (artificial intelligence);natural language processing;power engineering computing;reinforcement learning","artificial intelligence;computer vision;data-driven dynamic model;data-driven solution;deep learning;EMS domain;EMS problems;energy management strategy;fuel efficiency;historical interactive datasets;hybrid electric vehicles;large-scale offline datasets;model overexploitation;model-based offline RL;natural language processing;optimal policy;policy purely offline;policy training;prior collected dataset;RL algorithm;RL methods;RL-based policies;simulation world;tremendous advances;uncertainty awareness;uncertainty-aware model-based offline reinforcement learning;vanilla RL approaches","","","","30","CCBY","10 Oct 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Optimization Algorithm for Permutation Flow-Shop Scheduling","Z. Pan; L. Wang; J. Wang; J. Lu","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Huawei Noah's Ark Lab, Shenzhen, China","IEEE Transactions on Emerging Topics in Computational Intelligence","21 Jul 2023","2023","7","4","983","994","As a new analogy paradigm of human learning process, reinforcement learning (RL) has become an emerging topic in computational intelligence (CI). The synergy between the RL and CI is an emerging way to develop efficient solution algorithms for solving complex combinatorial optimization (CO) problems like machine scheduling problem. In this paper, we proposed an efficient optimization algorithm based on Deep RL for solving permutation flow-shop scheduling problem (PFSP) to minimize the maximum completion time. Firstly, a new deep neural network (PFSPNet) is designed for the PFSP to achieve the end-to-end output without limitation of problem sizes. Secondly, an actor-critic method of RL is used to train the PFSPNet without depending on the collection of high-quality labelled data. Thirdly, an improvement strategy is designed to refine the solution provided by the PFSPNet. Simulation results and statistical comparison show that the proposed optimization algorithm based on deep RL can obtain better results than the existing heuristics in similar computational time for solving the PFSP.","2471-285X","","10.1109/TETCI.2021.3098354","National Science Fund for Distinguished Young Scholars of China(grant numbers:61525304); National Natural Science Foundation of China(grant numbers:61873328); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9594768","Reinforcement learning;deep neural network;flow-shop scheduling;optimization algorithm;improvement strategy","Optimization;Job shop scheduling;Decoding;Reinforcement learning;Dynamic scheduling;Encoding;Computational intelligence","combinatorial mathematics;deep learning (artificial intelligence);flow shop scheduling;heuristic programming;minimisation;production engineering computing;reinforcement learning","actor-critic method;CI;combinatorial optimization problems;computational intelligence;deep neural network;deep reinforcement learning based optimization algorithm;deep RL;end-to-end output;human learning process;machine scheduling problem;maximum completion time;minimisation;permutation flow-shop scheduling problem;PFSPNet;statistical comparison","","13","","55","IEEE","1 Nov 2021","","","IEEE","IEEE Journals"
"Spontaneous Coordinated Behavior of Robots through Reinforcement Learning","T. Shibata; K. Ohkawa; K. Tanie","Artificial Intelligence Laboratory, MIT, USA; Tsukuba University, Japan; Artificial Intelligence Laboratory, MIT, USA","Proceedings of ICNN'95 - International Conference on Neural Networks","6 Aug 2002","1995","6","","2908","","","","0-7803-2768-3","10.1109/ICNN.1995.487242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=487242","","Robot kinematics;Learning;Humans;Intelligent robots;Laboratories;Biological system modeling;Mechanical engineering;Artificial intelligence;Production;Predictive models","","","","4","","6","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Estimating Player Completion Rate in Mobile Puzzle Games Using Reinforcement Learning","J. T. Kristensen; A. Valdivia; P. Burelli","IT University of Copenhagen/Tactile Games, Copenhagen, Denmark; Tactile Games, Copenhagen, Denmark; IT University of Copenhagen/Tactile Games, Copenhagen, Denmark","2020 IEEE Conference on Games (CoG)","20 Oct 2020","2020","","","636","639","In this work we investigate whether it is plausible to use the performance of a reinforcement learning (RL) agent to estimate the difficulty measured as the player completion rate of different levels in the mobile puzzle game Lily's Garden.For this purpose we train an RL agent and measure the number of moves required to complete a level. This is then compared to the level completion rate of a large sample of real players.We find that the strongest predictor of player completion rate for a level is the number of moves taken to complete a level of the ~5% best runs of the agent on a given level. A very interesting observation is that, while in absolute terms, the agent is unable to reach human-level performance across all levels, the differences in terms of behaviour between levels are highly correlated to the differences in human behaviour. Thus, despite performing sub-par, it is still possible to use the performance of the agent to estimate, and perhaps further model, player metrics.","2325-4289","978-1-7281-4533-4","10.1109/CoG47356.2020.9231581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9231581","reinforcement learning;ppo;player agent;player modelling;playtesting;autonomous agent","Games;Training;Correlation;Reinforcement learning;Production;Image color analysis;Data models","computer games;learning (artificial intelligence);mobile computing","player metrics;Lily's Garden;human-level performance;level completion rate;RL agent;reinforcement learning;mobile puzzle games;player completion rate","","3","","10","IEEE","20 Oct 2020","","","IEEE","IEEE Conferences"
"Object-Oriented State Abstraction in Reinforcement Learning for Video Games","Y. Chen; H. Yuan; Y. Li","Guanghua School of Management, Peking University, Beijing, China; School of Mathematical Sciences, Peking University, Beijing, China; Department of Computer Science and Engineering, Shanghai Jiao Tong Univeristy, Shanghai, China","2019 IEEE Conference on Games (CoG)","26 Sep 2019","2019","","","1","4","We present a novel method to obtain object-oriented state representations for video games. Inspired by the mechanism of attention to objects in human vision, we try to make the agents automatically detect the important objects during the learning process. The detection is directed by the Q value based on the abstract state representations. The process does not require human prior knowledge and provides a faster and lighter way for AI playing games. We present empirical results on the Battle City game to validate our method. In comparison with raw images input and other preprocessing methods, our approach achieves better final results and uses smaller state space.","2325-4289","978-1-7281-1884-0","10.1109/CIG.2019.8848099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8848099","reinforcement learning;video games;state abstraction;object detection","Games;Object detection;Clustering algorithms;Reinforcement learning;Task analysis;Trajectory;Optimized production technology","computer games;learning (artificial intelligence);object-oriented methods","object-oriented state abstraction;reinforcement learning;video games;object-oriented state representations;human vision;learning process;abstract state representations;human prior knowledge;AI playing games;preprocessing methods;state space;battle city game;Q value","","2","","13","IEEE","26 Sep 2019","","","IEEE","IEEE Conferences"
"Reinforcement Learning solution for economic scheduling with stochastic cost function","T. P. Imthias Ahmed; F. R. Pazheri; E. A. Jasmin","Saudi Aramco Chair in Electrical Power, King Saud University, Riyadh, Saudi Arabia; Saudi Aramco Chair in Electrical Power, King Saud University, Riyadh, Saudi Arabia; Dept.of Electrical & Electronics Engineering, Govt. Engineering College, Thrishur, Kerala, India","2011 IEEE Recent Advances in Intelligent Computational Systems","3 Nov 2011","2011","","","437","440","Reinforcement Learning (RL) is a machine learning paradigm in which learning system learns which action to take in different situations by using a scalar evaluation received from the environment on performing an action. One major feature of this learning method is that it can learn in a stochastic environment. RL has been successfully applied to many power system optimization problems. Economic Scheduling is an important optimization problem to decide the amount of generation to be allocated to each generating unit so that the total cost of generation is minimized without violating system constraints. One scheduling issue is to accommodate the stochastic cost behaviour of the different generating units. In this paper we demonstrate the capacity of RL algorithm to account the stochastic nature of fuel cost.","","978-1-4244-9477-4","10.1109/RAICS.2011.6069350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6069350","Reinforcement Learning;Q learning;Power system scheduling","Learning;Economics;Resource management;Power systems;Schedules;Production;Fuels","costing;learning (artificial intelligence);power engineering computing;power generation economics;stochastic processes","reinforcement learning solution;economic scheduling;stochastic cost function;machine learning paradigm;stochastic environment;power system optimization problems;stochastic cost behaviour;fuel cost","","1","","11","IEEE","3 Nov 2011","","","IEEE","IEEE Conferences"
"Multi-robot Cooperation Based on Continuous Reinforcement Learning with Two State Space Representations","T. Yasuda; K. Ohkura; K. Yamada","Institute of Engineering, Hiroshima University, Hiroshima, Japan; Institute of Engineering, Hiroshima University, Hiroshima, Japan; Department of Mechanical Engineering, Toyo University Kawagoe, Saitama, Japan","2013 IEEE International Conference on Systems, Man, and Cybernetics","27 Jan 2014","2013","","","4470","4475","The field of multi-robot systems (MRS) deals with groups of autonomous robots and is attracting much research interest from robotics researchers. MRSs are expected to achieve tasks that are difficult to perform by an individual robot. In MRS, reinforcement learning (RL) is a promising approach for the distributed control of each robot. RL allows robots to learn mappings from their states to their actions through rewards or payoffs obtained through interactions with their environment. Theoretically, the environment of an MRS is nonstationary, and therefore rewards or payoffs received by learning robots depend not only on their own actions but also on the actions of other robots. From this perspective, our research group has been developing a technique that segments state and action spaces simultaneously and autonomously to extend the adaptability of an MRS to dynamic environments. To improve learning performance, we introduce a mechanism of selecting either of the two state spaces: one is represented by a parametric model for exploration, while the other is represented by a nonparametric model for exploitation. The proposed technique is expected to show better learning performance and robustness against an unpredicted environmental change. We investigate our proposed technique by conducting computer simulations of a cooperative box-pushing task with six autonomous mobile robots.","1062-922X","978-1-4799-0652-9","10.1109/SMC.2013.760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6722515","Multi-robot systems;reinforcement learning;continuous space;parametric model;nonparametric model;robustness","Support vector machines;Robot sensing systems;Mobile robots;Standards;Robustness;Production","distributed control;learning (artificial intelligence);mobile robots;multi-robot systems;state-space methods","cooperative box-pushing task;exploitation nonparametric model;exploration parametric model;action spaces;mapping learning;distributed control;RL;autonomous mobile robots;MRS;state space representation;continuous reinforcement learning;multirobot cooperation","","1","","17","IEEE","27 Jan 2014","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning with Different Rewards for Scheduling in High-Performance Computing Systems","M. F. Reza; B. Zhao","School of Computer Science and Mathematics, University of Central Missouri, Warrensburg, MO, USA; School of Computer Science and Mathematics, University of Central Missouri, Warrensburg, MO, USA","2021 IEEE International Midwest Symposium on Circuits and Systems (MWSCAS)","13 Sep 2021","2021","","","183","186","Scheduling is a challenging task for high-performance computing systems since it involves complex allocations of various types of resources among jobs with different characteristics. Because incoming jobs often vary with resource requests and may interact with other jobs, heuristics based scheduling algorithms tend to be suboptimal and require substantial amount of time to design and test under diverse conditions. As a result, reinforcement learning (RL) based approaches have been proposed to tackle various job scheduling challenges. We have also used deep neural networks for approximating the decisions in RL agents as table-based RL agent is not scalable for large-scale problem sizes. The performance of RL agents, however, has proven to been notoriously instable and sensitive to training hyperparameters and the reward signal. In this work, we aim to study how different reward signals might affect the RL agents’ performance. We trained RL agents with four different reward signals and simulations results under Alibaba workloads showed that trained RL agents’ improve the performance for 60-65% of the jobset compared to two popular heuristics.","1558-3899","978-1-6654-2461-5","10.1109/MWSCAS47672.2021.9531852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9531852","","Training;Deep learning;Scheduling algorithms;Circuits and systems;Simulation;Reinforcement learning;Production","deep learning (artificial intelligence);parallel processing;processor scheduling;resource allocation","trained RL agents;high-performance computing systems;complex allocations;incoming jobs;resource requests;job scheduling challenges;deep neural networks;table-based RL agent;reward signal;Alibaba workloads;deep reinforcement learning","","","","12","IEEE","13 Sep 2021","","","IEEE","IEEE Conferences"
"Reinforcement learning in steady-state cellular genetic algorithms","Cin-Young Lee; E. K. Antonsson",NA; NA,"Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600)","7 Aug 2002","2002","2","","1793","1797 vol.2","A novel cellular genetic algorithm is developed to address the issues of good mate selection. This is accomplished through reinforcement learning where good mating individuals attract and poor mating individuals repel. Adaptation of good mate choice occurs, thus leading to more efficient search. Results are presented for various test cases.","","0-7803-7282-4","10.1109/CEC.2002.1004514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1004514","","Learning;Steady-state;Genetic algorithms;Testing;Topology;Computation theory;Robustness;Convergence;Computational modeling;Production","learning (artificial intelligence);genetic algorithms","steady-state cellular genetic algorithms;good mate selection;reinforcement learning;search","","","","11","IEEE","7 Aug 2002","","","IEEE","IEEE Conferences"
"Reinforcement Learning from Simulated Environments: An Encoder Decoder Framework","B. Choo; G. Crannel; S. Adams; F. Dadgostari; P. A. Beling; A. Bolcavage; R. McIntyre","Department of Engineering Systems and Environment, University of Virginia, Charlottesville, VA, UVA; Department of Engineering Systems and Environment, University of Virginia, Charlottesville, VA, UVA; Department of Engineering Systems and Environment, University of Virginia, Charlottesville, VA, UVA; Department of Engineering Systems and Environment, University of Virginia, Charlottesville, VA, UVA; Department of Engineering Systems and Environment, University of Virginia, Charlottesville, VA, UVA; Rolls-Royce Corporation, Indianapolis, IN, USA; Rolls-Royce PLC, Derby, UK","2020 Spring Simulation Conference (SpringSim)","3 Sep 2020","2020","","","1","12","Reinforcement learning (RL) is used for sequential decision making such as operating and maintaining manufacturing systems. In RL, the system is modeled as a Markov decision process with states, actions, rewards, and policies. A policy is learned through repeated interaction with the environment. When a RL agent cannot interact with the real system due to time and cost constraints, a simulation of the system may be used in its place. Unfortunately, most simulations are not built for the purpose of interacting with a RL agent. Simulations built to function as the environment are often structured only according to the defined state, action, and reward and can lack fidelity, detail, and accuracy. We propose a general framework for bridging the worlds of simulation and RL. This is accomplished by placing ""interpreters"" between the simulation and the RL agent that translate information into a form that is coherent to each entity.","","978-1-56555-370-5","10.22360/SpringSim.2020.AIS.001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9185415","reinforcement learning;Markov decision process;manufacturing","Learning (artificial intelligence);Decoding;Encoding;Decision making;Markov processes;Manufacturing;Machine learning","control engineering computing;decision making;learning (artificial intelligence);Markov processes","reinforcement learning;simulated environments;encoder decoder framework;sequential decision;manufacturing systems;Markov decision process;repeated interaction;RL agent;cost constraints;defined state","","","","19","","3 Sep 2020","","","IEEE","IEEE Conferences"
"Scheduling Algorithm for Raw Material Transportation Via Deep Reinforcement Learning","Y. Zhang; Y. -Y. Chen; F. Zhang","School of Automation, Southeast University, Nanjing; School of Automation, Southeast University, Nanjing; School of Automation, Southeast University, Nanjing","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","2218","2223","Note that the traditional scheduling control algorithms (e.g., Scheduling control algorithm based on mixed integer programming.) depend on the specific scheduling system model, which leads to get the optimal solution difficultly. This paper proposes a novel scheduling algorithm based on deep reinforcement learning, where three parts of raw material transfer scheduling control are included in the algorithm, that is, raw materials are transferred from the cargo ship to the wharf, from the wharf to the factory, and from the factory to the processing location. The algorithm takes the system state as the input of Deep Q-Network, calculates the action value of each part of the scheduling action through the deep neural network, and finally selects the optimal scheduling action based on the action value, which effectively reduces the storage cost and the number of pipeline changes in the process of raw material transportation. In addition, this paper also proposes to optimize the input of neural network through state reuse, which further reduces the switching times of transmission pipeline. Simulation results show that the proposed method does not depend on the specific scheduling model, reduces the solution time, and effectively reduces the storage cost and pipeline switching cost in the scheduling process.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902861","Scheduling;Deep Q-Network;Neural Network;State reuse","Costs;Neural networks;Pipelines;Process control;Transportation;Optimal scheduling;Switches","cost reduction;deep learning (artificial intelligence);integer programming;logistics;pipelines;production engineering computing;raw materials;scheduling","action value;cargo ship;deep neural network;deep Q-network;deep reinforcement learning;mixed integer programming;optimal scheduling action;pipeline switching cost reduction;raw material transfer scheduling control;raw material transportation;state reuse;storage cost reduction;wharf","","","","14","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Smooth Path Planning of 6-DOF Robot Based on Reinforcement Learning","J. Tian; D. Li","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China","2022 4th International Conference on Control and Robotics (ICCR)","6 Mar 2023","2022","","","89","93","The current path planning algorithms such as A-star(all stars) algorithm and RRT (Rapidly-exploring Random Trees) algorithm can meet the obstacle avoidance planning of the 6-DOF robot, but the smoothness of the path is not considered. Working in an unreasonable path for a long time will produce a great load on the joints of the 6-DOF robot and seriously affect its life. In this paper, we use reinforcement learning reconcile A-star algorithm and RRT algorithm for smooth path planning of the robot. Experimental results show that compared with A-star algorithm and RRT algorithm, the fusion algorithm has smoother path and more reasonable time.","","978-1-6654-8641-5","10.1109/ICCR55715.2022.10053875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10053875","path planning;A-star;RRT;RL;6-DOF robot","Stars;Reinforcement learning;Transforms;Production;Solids;6-DOF;Path planning","collision avoidance;mobile robots;path planning;trees (mathematics)","current path planning algorithms;fusion algorithm;obstacle avoidance planning;Rapidly-exploring Random Trees;reinforcement learning;RRT algorithm;smooth path planning;smoother path;unreasonable path","","","","15","IEEE","6 Mar 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Optimization for Cobot's Path Generation in Collaborative Tasks","S. el Zaatari; Y. Wang; W. Li","Faculty of Engineering, Environment and Computing, Coventry University, Coventry, United Kingdom; School of Logistics Engineering, Wuhan University of Technology, Wuhan, China; School of Logistics Engineering, Wuhan University of Technology, Wuhan, China","2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD)","28 May 2021","2021","","","975","980","Task-Parameterized Learning from Demonstrations (TP-LfD) is an effective approach for collaborative robot (cobot). It aims at generating a path of a cobot moving in a dynamic collaborative task (e.g., a pick-and-place task) adaptively with respect to knowledge learnt from demonstrated tasks. That is, the learnt knowledge from demonstrated tasks are considered task parameters, which are critical input for TP-LfD to generate a movement path of a cobot for a new dynamic task. To further enhance the adaptability of TP-LfD, in this paper, an improved TP-LfD ( i TP-LfD) approach over other developed TP-LfD approaches is presented. One of the major contributions in i TP-LfD is that a reinforcement learning based optimization algorithm is designed to eliminate irrelevant task parameters identified in demonstrations, which boosts the overall computational performance of cobot's path generation. In the end, case studies were used to validate and highlight the adaptability and robustness of the approach.","","978-1-7281-6597-4","10.1109/CSCWD49262.2021.9437874","Coventry University; National Natural Science Foundation of China(grant numbers:51975444); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9437874","Learning from demonstrations;reinforcement learning;collaborative robots (Cobots)","Service robots;Heuristic algorithms;Conferences;Collaboration;Reinforcement learning;Robustness;Production facilities","learning (artificial intelligence);manipulators;optimisation","dynamic collaborative task;pick-and-place task;demonstrated tasks;movement path;cobot;dynamic task;reinforcement learning based optimization algorithm;irrelevant task parameters;collaborative tasks;collaborative robot;task-parameterized learning from demonstrations;TP-LfD approaches","","","","9","IEEE","28 May 2021","","","IEEE","IEEE Conferences"
"Research on Intelligent Control Decision of Manipulator Based on Deep Reinforcement Learning","M. Li; D. Zhu; J. Shen; H. Yan; S. Li; K. Wang; H. Yuan","Suzhou Power Supply Company, State Grid Jiangsu Electric Power Co., Ltd., Suzhou, China; Suzhou Power Supply Company, State Grid Jiangsu Electric Power Co., Ltd., Suzhou, China; Suzhou Power Supply Company, State Grid Jiangsu Electric Power Co., Ltd., Suzhou, China; School of Mechanical Engineering, Jiangnan University, Wuxi, China; School of Mechanical Engineering, Jiangnan University, Wuxi, China; School of Mechanical Engineering, Jiangnan University, Wuxi, China; Nanjing Xingchen Intelligent Co., Ltd., Nanjing, China","2022 4th International Academic Exchange Conference on Science and Technology Innovation (IAECST)","17 Mar 2023","2022","","","1454","1457","For the general reinforcement learning algorithm, the training time will be too long due to the need for constant trial and error to provide sufficient data in the control of the robot arm. This paper uses unity3d engine to build a simulation environment, including UR robot and intelligent electricity meter. M-SAC algorithm, which combines multi-agent with SAC (Soft Actor Critical) algorithm, speeds up the training speed and shortens the training agent time. The efficiency of M-SAC algorithm is verified by analyzing the average reward value after training.","","979-8-3503-2000-8","10.1109/IAECST57965.2022.10061884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10061884","Strengthen learning;Unity3d engine;M-SAC algorithm;Average reward value;Multi-agent","Training;Motion planning;Technological innovation;Service robots;Reinforcement learning;Production;Manipulators","control engineering computing;deep learning (artificial intelligence);intelligent control;learning (artificial intelligence);manipulators;multi-agent systems;reinforcement learning","constant trial;deep reinforcement learning;general reinforcement learning algorithm;intelligent electricity meter;M-SAC algorithm;robot arm;simulation environment;Soft Actor Critical;training agent time;training speed;training time;unity3d engine","","","","10","IEEE","17 Mar 2023","","","IEEE","IEEE Conferences"
"Decomposing FANET to Counter Massive UAV Swarm Based on Reinforcement Learning","N. Zhang; C. Liu; J. Ba","College of Intelligence Science, National University of Defense Technology, Changsha, China; College of Intelligence Science, National University of Defense Technology, Changsha, China; School of Intelligence Science, National University of Defense Technology, Changsha, China","IEEE Communications Letters","11 Jul 2023","2023","27","7","1784","1788","Armed and autonomous unmanned aerial vehicle (UAV) swarms are a new type of aerial threat due to their numerical superiority and cooperative communication, and existing countermeasures cannot completely eliminate whole swarms. In this letter, we design an algorithm based on deep reinforcement learning called GCPDDQN to find the optimal attack sequence for large-scale UAV swarm, so as to achieve the purpose of decomposing the network into small pieces and destroying swarm communications. Numerical simulations show that GCPDDQN can speed up the collapse of the network using only the simplest features and network architectures which are changeable to adjust to different scenarios.","1558-2558","","10.1109/LCOMM.2023.3269221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10106270","UAV swarm;graph representation;deep Q-network;FANET","Drones;Autonomous aerial vehicles;Training;Reinforcement learning;Optimized production technology;Topology;Three-dimensional displays","autonomous aerial vehicles;cooperative communication;deep learning (artificial intelligence);mobile robots;reinforcement learning;remotely operated vehicles","aerial threat;autonomous unmanned aerial vehicle swarms;cooperative communication;counter massive UAV swarm;deep reinforcement learning;destroying swarm communications;existing countermeasures;FANET;GCPDDQN;numerical simulations;numerical superiority;optimal attack sequence;pieces","","","","16","IEEE","21 Apr 2023","","","IEEE","IEEE Journals"
"Traffic signal timing via deep reinforcement learning","L. Li; Y. Lv; F. -Y. Wang","Tsinghua University, Beijing, Beijing, CN; Institute of Automation Chinese Academy of Sciences, Beijing, Beijing, CN; Institute of Automation Chinese Academy of Sciences, Beijing, Beijing, CN","IEEE/CAA Journal of Automatica Sinica","12 Jul 2016","2016","3","3","247","254","In this paper, we propose a set of algorithms to design signal timing plans via deep reinforcement learning. The core idea of this approach is to set up a deep neural network (DNN) to learn the Q-function of reinforcement learning from the sampled traffic state/control inputs and the corresponding traffic system performance output. Based on the obtained DNN, we can find the appropriate signal timing policies by implicitly modeling the control actions and the change of system states. We explain the possible benefits and implementation tricks of this new approach. The relationships between this new approach and some existing approaches are also carefully discussed.","2329-9274","","10.1109/JAS.2016.7508798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508798","Traffic control;reinforcement learning;deep learning;deep reinforcement learning","Learning (artificial intelligence);Timing;Neural networks;Mathematical model;Optimization;Analytical models;Machine learning","learning (artificial intelligence);neural nets;road traffic control;traffic engineering computing","traffic signal timing;deep reinforcement learning;deep neural network;DNN;Q-function;traffic system performance output;signal timing policies","","333","","","","12 Jul 2016","","","IEEE","IEEE Journals"
"Reinforcement learning with average cost for adaptive control of traffic lights at intersections","L. A. Prashanth; S. Bhatnagar","Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India","2011 14th International IEEE Conference on Intelligent Transportation Systems (ITSC)","17 Nov 2011","2011","","","1640","1645","We propose for the first time two reinforcement learning algorithms with function approximation for average cost adaptive control of traffic lights. One of these algorithms is a version of Q-learning with function approximation while the other is a policy gradient actor-critic algorithm that incorporates multi-timescale stochastic approximation. We show performance comparisons on various network settings of these algorithms with a range of fixed timing algorithms, as well as a Q-learning algorithm with full state representation that we also implement. We observe that whereas (as expected) on a two-junction corridor, the full state representation algorithm shows the best results, this algorithm is not implementable on larger road networks. The algorithm PG-AC-TLC that we propose is seen to show the best overall performance.","2153-0017","978-1-4577-2197-7","10.1109/ITSC.2011.6082823","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6082823","Traffic signal control;reinforcement learning;Q-learning;policy gradient actor-critic","Approximation algorithms;Roads;Function approximation;Junctions;Algorithm design and analysis;Learning;Cost function","adaptive control;approximation theory;function approximation;learning (artificial intelligence);road traffic;stochastic processes;traffic control;traffic engineering computing","reinforcement learning;average cost;adaptive control;traffic lights;function approximation;policy gradient actor-critic algorithm;multitimescale stochastic approximation;Q-learning algorithm;two-junction corridor;full state representation algorithm","","39","","15","IEEE","17 Nov 2011","","","IEEE","IEEE Conferences"
"Robot path planning based on deep reinforcement learning","Y. Long; H. He","School of Automation, Wuhan University of Technology, Wuhan, China; School of Automation, Wuhan University of Technology, Wuhan, China","2020 IEEE Conference on Telecommunications, Optics and Computer Science (TOCS)","8 Feb 2021","2020","","","151","154","Q-learning algorithm based on Markov decision process as a reinforcement learning algorithm can achieve better path planning effect for mobile robot in continuous trial and error. However, Q-learning needs a huge Q-value table, which is easy to cause dimension disaster in decision-making, and it is difficult to get a good path in complex situations. By combining deep learning with reinforcement learning and using the perceptual advantages of deep learning to solve the decision-making problem of reinforcement learning, the deficiency of Q-learning algorithm can be improved. At the same time, the path planning of deep reinforcement learning is simulated by MATLAB, the simulation results show that the deep reinforcement learning can effectively realize the obstacle avoidance of the robot and plan a collision free optimal path for the robot from the starting point to the end point.","","978-1-7281-8117-2","10.1109/TOCS50858.2020.9339752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9339752","mobile robot;deep reinforcement learning;obstacle avoidance;optimal path","Deep learning;Simulation;Decision making;Reinforcement learning;Path planning;Telecommunications;Collision avoidance","collision avoidance;decision making;deep learning (artificial intelligence);Markov processes;mobile robots","deep reinforcement learning;Q-learning algorithm;Markov decision process;robot path planning effect","","10","","15","IEEE","8 Feb 2021","","","IEEE","IEEE Conferences"
"Reinforcement learning for multi-agent patrol policy","Z. Hu; D. Zhao","Lab of Complex Systems and Intelligence Sciences, Institute of Automation, Chinese Academy and Sciences, Beijing, China; Lab of Complex Systems and Intelligence Sciences, Institute of Automation, Chinese Academy and Sciences, Beijing, China","9th IEEE International Conference on Cognitive Informatics (ICCI'10)","11 Oct 2010","2010","","","530","535","This paper presents a reinforcement learning (RL) algorithm for multi-agent patrol tasks, which can be thought of as a dynamic programming problem with stochastic demands. We define the cover rate as the reward, the multi-agent physical positions including edges and nodes as the state, and the nodes adjacent to the agent as the action to model the patrol task. The modeling of this problem is totally different from other's work, which facilitates the communication and cooperation among these agents. Furthermore, we map the state from four dimensions to one dimension in order to improve the training efficiency and reduce the coding complexity. A deterministic Softmax algorithm is designed for comparison. We test both two algorithms in patrolling and rescuing scenarios. Results show the patrol cover rate with RL greatly outperforms Softmax about 15.38%, and the average rescue time for emergent pots is reduced by 20% with RL compared to Softmax.","","978-1-4244-8042-5","10.1109/COGINF.2010.5599681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5599681","","Learning;Indexes;Algorithm design and analysis;Training;Markov processes;Vehicles","dynamic programming;learning (artificial intelligence);military computing;multi-robot systems","reinforcement learning;multiagent patrol policy;dynamic programming;coding complexity;Softmax algorithm","","8","","12","IEEE","11 Oct 2010","","","IEEE","IEEE Conferences"
"Network load balancing strategy based on supervised reinforcement learning with shaping rewards","Z. Hu; H. Chen","Power Grid Automation Laboratory, Key Laboratory of China Southern Power Grid Company, Guangzhou, China; Power Grid Automation Laboratory, Key Laboratory of China Southern Power Grid Company, Guangzhou, China","2013 Fourth International Conference on Intelligent Control and Information Processing (ICICIP)","25 Jul 2013","2013","","","393","397","This paper proposes supervised reinforcement learning (SRL) algorithm for network load balancing strategy with shaping rewards. We define the index of router as state set; design additional distance improving reward and load balancing reward to construct the supervisor; adopt epsilon greedy algorithm as the action selecting strategy and prove that the state transmission is a deterministic matrix. Besides, we carry out the simulation work which demonstrates that by maximizing the sum of discounted rewards, SRL is an effective controller for network load balancing strategy; each router can apply this algorithm to calculate the optimal path to other routers with network load balancing requirement.","","978-1-4673-6249-8","10.1109/ICICIP.2013.6568104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6568104","","Load management;Learning (artificial intelligence);Routing;Training;Load modeling;Network topology;Indexes","greedy algorithms;learning (artificial intelligence);matrix algebra;resource allocation","network load balancing strategy;supervised reinforcement learning algorithm;epsilon greedy algorithm;state transmission;deterministic matrix;optimal path;network load balancing requirement","","5","","14","IEEE","25 Jul 2013","","","IEEE","IEEE Conferences"
"Unsupervised Reinforcement Learning for Transferable Manipulation Skill Discovery","D. Cho; J. Kim; H. J. Kim","Department of Mechanical and Aerospace Engineering, Seoul National University, Seoul, South Korea; Department of Mechanical and Aerospace Engineering, Seoul National University, Seoul, South Korea; Department of Mechanical and Aerospace Engineering, Seoul National University, Seoul, South Korea","IEEE Robotics and Automation Letters","24 Jun 2022","2022","7","3","7455","7462","Current reinforcement learning (RL) in robotics often experiences difficulty in generalizing to new downstream tasks due to the innate task-specific training paradigm. To alleviate it, unsupervised RL, a framework that pre-trains the agent in a task-agnostic manner without access to the task-specific reward, leverages active exploration for distilling diverse experience into essential skills or reusable knowledge. For exploiting such benefits also in robotic manipulation, we propose an unsupervised method for transferable manipulation skill discovery that ties structured exploration toward interacting behavior and transferable skill learning. It not only enables the agent to learn interaction behavior, the key aspect of the robotic manipulation learning, without access to the environment reward, but also to generalize to arbitrary downstream manipulation tasks with the learned task-agnostic skills. Through comparative experiments, we show that our approach achieves the most diverse interacting behavior and significantly improves sample efficiency in downstream tasks including the extension to multi-object, multitask problems.","2377-3766","","10.1109/LRA.2022.3171915","Agency for Defense Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770346","Deep learning methods;reinforcement learning;machine learning for robot control;transfer learning","Task analysis;Robots;Space exploration;Reinforcement learning;Mutual information;Uncertainty;Training","manipulators;reinforcement learning;task analysis;unsupervised learning","unsupervised reinforcement learning;transferable manipulation skill discovery;unsupervised RL;task-agnostic manner;task-specific reward;transferable skill learning;interaction behavior;robotic manipulation learning;downstream manipulation tasks;task-specific training paradigm","","4","","28","IEEE","6 May 2022","","","IEEE","IEEE Journals"
"An Overview of Robust Reinforcement Learning","S. Chen; Y. Li","School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, Shenzhen, China","2020 IEEE International Conference on Networking, Sensing and Control (ICNSC)","4 Nov 2020","2020","","","1","6","Reinforcement learning (RL) is one of the popular methods for intelligent control and decision making in the field of robotics recently. The goal of RL is to learn an optimal policy of the agent by interacting with the environment via trail and error. There are two main algorithms for RL problems, including model-free and model-based methods. Model-free RL is driven by historical trajectories and empirical data of the agent to optimize the policy, which needs to take actions in the environment to collect the trajectory data and may cause the damage of the robot during training in the real environment. The main different between model-based and model-free RL is that a model of the transition probability in the interaction environment is employed. Thus the agent can search the optimal policy through internal simulation. However, the model of the transition probability is usually estimated from historical data in a single environment with statistical errors. Therefore, an issue is faced by the agent is that the optimal policy is sensitive to perturbations in the model of the environment which can lead to serious degradation in performance. Robust RL aims to learn a robust optimal policy that accounts for model uncertainty of the transition probability to systematically mitigate the sensitivity of the optimal policy in perturbed environments. In this overview, we begin with an introduction to the algorithms in RL, then focus on the model uncertainty of the transition probability in robust RL. In parallel, we highlight the current research and challenges of robust RL for robot control. To conclude, we describe some research areas in robust RL and look ahead to the future work about robot control in complex environments.","","978-1-7281-6855-5","10.1109/ICNSC48988.2020.9238129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238129","robust;reinforcement learning;dynamic programming;transition probability;model uncertainty","Uncertainty;Perturbation methods;Heuristic algorithms;Robot control;Reinforcement learning;Robot sensing systems;Trajectory","learning (artificial intelligence);probability","robust reinforcement learning;model-based methods;trajectory data collection;transition probability;interaction environment;historical data;single environment;robust RL;robust optimal policy;model uncertainty;perturbed environments;robot control;complex environments;intelligent control;decision making;model-free RL method;historical trajectories;empirical data;optimal policy;statistical errors;performance degradation","","3","","47","IEEE","4 Nov 2020","","","IEEE","IEEE Conferences"
"Data-driven unmanned surface vessel path following control method based on reinforcement learning","W. Deng; H. Li; Y. Wen","School of Automation, Wuhan University of Technology, Wuhan Hubei; School of Automation, Wuhan University of Technology, Wuhan Hubei; Hubei Key Laboratory of Inland Shipping Technology, Wuhan Hubei","2019 Chinese Control And Decision Conference (CCDC)","12 Sep 2019","2019","","","3035","3040","Aiming at many difficult problems of path-following(PF) control of Unmanned surface vessel (USV), such as disturbance of wind, wave and current, nonlinearity of control effect, etc. This paper proposes a data-driven USV PF control method based on SARSA. Using the behavior-reward scoring mechanism of reinforcement learning to learn a set of behavioral rules. These behavioral rules are used for USV to make the best control behavior based on the state of the ternary array consisting of heading deviation, wind and current. The experiment uses the simulation environment to learn the algorithm. Through the analysis of the experimental results and the comparison with the traditional PID algorithm, the feasibility and advantages of the algorithm are verified.","1948-9447","978-1-7281-0106-4","10.1109/CCDC.2019.8832655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8832655","Unmanned surface vessel(USV);Path-following(PF);Data-driven;Reinforcement learning","","control engineering computing;learning (artificial intelligence);marine vehicles;mobile robots;position control;remotely operated vehicles;three-term control","data-driven USV PF control method;behavior-reward scoring mechanism;reinforcement learning;behavioral rules;data-driven unmanned surface vessel;path following control method;PID algorithm","","3","","14","IEEE","12 Sep 2019","","","IEEE","IEEE Conferences"
"Direct-reinforcement-adaptive-learning fuzzy logic control for a class of nonlinear systems","Y. H. Kim; F. L. Lewis","Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA; Automation and Robotics Research Institute, University of Texas, Arlington, Fort Worth, TX, USA","Proceedings of 12th IEEE International Symposium on Intelligent Control","6 Aug 2002","1997","","","281","286","The paper is concerned with the application of reinforcement learning techniques to feedback control of nonlinear systems using adaptive fuzzy logic systems (FLS). Even if a good model of the nonlinear system is known, it is often difficult to formulate a control law. The work in this paper addresses this problem by showing how an adaptive FLS can cope with nonlinearities through reinforcement learning with no preliminary off-line learning phase required. The reinforcement learning rules for finding proper fuzzy rules and tuning membership functions (MFs) do not assume that there is a supervisor to decide whether the current control action is correct. Instead, the FLS is indirectly told about the affect of its control action on the system performance. The learning is performed online based on a binary reinforcement signal from a critic without knowing the nonlinearity appearing in the system, and so is called direct reinforcement adaptive learning (DRAL). The learning algorithm is derived from Lyapunov stability analysis, so that both system tracking stability and error convergence can be guaranteed in the closed-loop system.","2158-9860","0-7803-4116-3","10.1109/ISIC.1997.626472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=626472","","Fuzzy logic;Learning;Nonlinear systems;Feedback control;Adaptive systems;Programmable control;Adaptive control;Nonlinear control systems;Fuzzy control;Current control","nonlinear control systems;fuzzy control;adaptive control;learning (artificial intelligence);feedback;Lyapunov methods;stability;closed loop systems","direct-reinforcement-adaptive-learning fuzzy logic control;nonlinear systems;feedback control;membership function tuning;binary reinforcement signal;DRAL;Lyapunov stability analysis;system tracking stability;error convergence;closed-loop system","","2","","14","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Data Fusion of Air Combat Based on Reinforcement Learning","T. Zhou; M. Chen; J. Zou","College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Science and Technology on Electron-Optic Control Laboratory, Luoyang Institute of Electro-Optical Equipment of Avic, Luoyang, China","2019 IEEE 4th International Conference on Advanced Robotics and Mechatronics (ICARM)","12 Sep 2019","2019","","","492","497","To improve the accuracy of data fusion system in modern air combat, an improved method based on reinforcement learning technique is developed in this paper. Firstly, the cubic spline interpolation is used for time alignment of multi-source data. Then, a reinforcement learning based data fusion method is proposed. The fusion accuracy reinforcement is realized by the error between observations and actual value. With an example, the simulation results indicate that the developed method is effective and feasible for the multi-sensor data fusion.","","978-1-7281-0064-7","10.1109/ICARM.2019.8834217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8834217","data fusion;air combat;cubic spline interpolation;reinforcement learning","Data integration;Reinforcement learning;Sensor systems;Sensor fusion;Splines (mathematics);Interpolation","interpolation;learning (artificial intelligence);military aircraft;military computing;sensor fusion;splines (mathematics)","multisource data;fusion accuracy reinforcement;multisensor data fusion;data fusion system;modern air combat;reinforcement learning technique;cubic spline interpolation;time alignment","","2","","12","IEEE","12 Sep 2019","","","IEEE","IEEE Conferences"
"Research of Control Strategy of Power System Stabilizer Based on Reinforcement Learning","X. Zhu; T. Jin","College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, P.R. China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, P.R. China","2020 IEEE 2nd International Conference on Circuits and Systems (ICCS)","3 Feb 2021","2020","","","81","85","Power system stabilizer (PSS) is used to generate excitation system auxiliary control signals which can suppress low frequency oscillation in power system. It has the ability of self-learning and parameter online tuning, which is a development trend of smart grid PSS controller in the future. This paper presents a design method of power system stabilizer based on reinforcement learning. Q-learning algorithm is one of reinforcement learning, and is used to PSS as the additional control. The simulation results show that the PSS based on Q-learning can effectively improve the ability of suppressing low frequency oscillation in power system, and the robustness of the system is significantly enhanced.","","978-1-7281-9122-5","10.1109/ICCS51219.2020.9336612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9336612","low frequency oscillation;power system stabilizer;reinforcement learning","Simulation;Reinforcement learning;Power system stability;Robustness;Smart grids;Oscillators;Tuning","frequency control;learning (artificial intelligence);oscillations;power system control;power system simulation;power system stability;smart power grids","reinforcement learning;Q-learning algorithm;low frequency oscillation;power system stabilizer;smart grid PSS controller;auxiliary control signals","","2","","18","IEEE","3 Feb 2021","","","IEEE","IEEE Conferences"
"Single Image Dehazing via Reinforcement Learning","Y. Zhang; Y. Dong","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)","14 Dec 2020","2020","1","","123","126","Single image dehazing aims at restoring the clean image from single hazy one. A novel Reinforcement Learning (RL) based image dehazing method is proposed in this paper to handle the dehazing task with interpretable ability and extensibility. We model the dehazing problem as a Markov Decision Process (MDP) with several existing simple traditional image processing operations and prior knowledge-based dehazing methods as actions. Furthermore, a deep Q-learning network is established to learn the value function for image dehazing. Finally the learned network can iteratively choose the correct action during the processing sequence to produce dehazing results. Extensive results on real hazy images have been conducted to verify the proposed method. Also the learned sequence of image dehazing can provide considerable guidance for human.","","978-1-7281-5224-0","10.1109/ICIBA50161.2020.9277382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9277382","Image Dehazing;Markov Decision Process;Deep Learning;Reinforcement Learning","Feature extraction;Reinforcement learning;Image color analysis;Image restoration;Computer vision;Task analysis;Transforms","image colour analysis;image enhancement;image restoration;learning (artificial intelligence);Markov processes","deep Q-learning network;learned network;dehazing results;hazy images;learned sequence;single image dehazing aims;clean image;dehazing task;interpretable ability;dehazing problem;Markov Decision Process;existing simple traditional image processing operations;prior knowledge-based dehazing methods;novel reinforcement learning based image dehazing method","","2","","25","IEEE","14 Dec 2020","","","IEEE","IEEE Conferences"
"A neural architecture to address Reinforcement Learning problems","R. L. S. de Arruda; F. J. Von Zuben","Laboratory of Bioinformatics and Bioinspired Computing (LBiC), Department of Computer Engineering and Industrial Automation (DCA), School of Electrical and Computer Engineering (FEEC), University of Campinas, Campinas, Sao Paulo, Brazil; Laboratory of Bioinformatics and Bioinspired Computing (LBiC), Department of Computer Engineering and Industrial Automation (DCA), School of Electrical and Computer Engineering (FEEC), University of Campinas, Campinas, Sao Paulo, Brazil","The 2011 International Joint Conference on Neural Networks","3 Oct 2011","2011","","","2930","2935","In this paper, the Reinforcement Learning problem is formulated equivalently to a Markov Decision Process. We address the solution of such problem using a novel Adaptive Dynamic Programming algorithm which is based on a Multilayer Perceptron Neural Network composed of a parameterized function approximator called Wire-Fitting. Extending such established model, this work makes use of concepts of eligibility to conceive faster learning algorithms. The advantage of the proposed approach is founded on the capability to handle continuous environments and to learn a better policy while following another. Simulation results involving the automatic control of an inverted pendulum are presented to indicate the effectiveness of the proposed algorithm.","2161-4407","978-1-4244-9637-2","10.1109/IJCNN.2011.6033606","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6033606","","Mathematical model;Equations;Monte Carlo methods;Heuristic algorithms;Markov processes;Approximation methods;Dynamic programming","dynamic programming;learning (artificial intelligence);Markov processes;multilayer perceptrons;neural net architecture","neural architecture;reinforcement learning;Markov decision process;adaptive dynamic programming;multilayer perceptron neural network;parameterized function approximator;wire-fitting;learning algorithm;automatic control;inverted pendulum","","1","","17","IEEE","3 Oct 2011","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning with Successive Over-Relaxation and its Application in Autoscaling Cloud Resources","I. John; S. Bhatnagar","Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India","2020 International Joint Conference on Neural Networks (IJCNN)","28 Sep 2020","2020","","","1","6","We present a new deep reinforcement learning algorithm using the technique of successive over-relaxation (SOR) in Deep Q-networks (DQNs). The new algorithm, named SOR-DQN, uses modified targets in the DQN framework with the aim of accelerating training. This work is motivated by the problem of auto-scaling resources for cloud applications, for which existing algorithms suffer from issues such as slow convergence, poor performance during the training phase and non-scalability. For the above problem, SOR-DQN achieves significant improvements over DQN on both synthetic and real datasets. We also study the generalization ability of the algorithm to multiple tasks by using it to train agents playing Atari video games.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9206598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206598","reinforcement learning;deep learning;cloud computing;resource allocation;atari games","Cloud computing;Training;Games;Machine learning;Resource management;Learning (artificial intelligence);Convergence","cloud computing;learning (artificial intelligence)","successive over-relaxation;deep Q-networks;accelerating training;auto-scaling resources;cloud applications;training phase;SOR-DQN;autoscaled cloud resources;deep reinforcement learning","","1","","21","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
"ICDVAE: An Effective Self-supervised Reinforcement Learning Method for behavior decisions of Non-player Characters in Metaverse Games","F. Lin; W. Ning; H. Zhai","School of Automation Guangdong University of Technology, Guangdong, China; School of Automation Guangdong University of Technology, Guangdong, China; School of Electromechanical Engineering Guangdong University of Technology, Guangdong, China","2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","3 Aug 2022","2022","10","","1727","1731","Reinforcement Learning (RL) is widely recognized as a powerful approach to constructing intelligent NPCs. Unlike general games, metaverse games tend to build up the larger, more complex, and more diverse virtual scenes with scores of various NPCs. In this case, it is challenging and hig-cost to design the reward functions for large-scale RL-based NPCs. To solve this challenge, we propose an effective distribution-aware curiosity mechanism termed Intrinsic Curiosity-Driven Variational Auto-Encoder (ICDVAE). It constructs the state distribution to rep-resent novelty by integrating the Intrinsic Curiosity Mechanism (ICM) and Generative Model (GM). In addition, we develop two effective techniques termed Inverse Dynamic Mechanism (IDM) and Variational Encoder Bottleneck (VEB) to extract the ego-motion feature and prevent overfitting. The empirical results show that our algorithm is more effective and robust than baselines on Atari games.","2693-2865","978-1-6654-2207-9","10.1109/ITAIC54216.2022.9836631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9836631","Reinforcement Learning;Mateverse Games;Non-Player Characters;Intrinsic Curiosity Mechanism;Variational Auto-Encoder","Metaverse;Heuristic algorithms;Conferences;Games;Reinforcement learning;Feature extraction;Behavioral sciences","computer games;feature extraction;image motion analysis;neural nets;supervised learning","ICDVAE;behavior decisions;nonplayer characters;metaverse games;intelligent NPC;general games;diverse virtual scenes;reward functions;state distribution;Atari games;self-supervised reinforcement learning method;large-scale RL-based NPC;distribution-aware curiosity mechanism;intrinsic curiosity-driven variational auto-encoder;ICM;generative model;inverse dynamic mechanism;IDM;variational encoder bottleneck;VEB;ego-motion feature extraction","","1","","27","IEEE","3 Aug 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning Control for Robot Arm Grasping Based on Improved DDPG","G. Qi; Y. Li","School of Automation, Beijing Institute of Technology, Beijing; School of Automation, Beijing Institute of Technology, Beijing","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","4132","4137","Although the traditional robot arm grasping control has high control accuracy, its price is based on high-precision hardware and lacks flexibility. In order to achieve high control accuracy and flexibility on a relatively inexpensive robot arm. This paper proposes an improved DDPG (Deep Deterministic Policy Gradient) reinforcement learning algorithm to control the gripping of a robot arm. First, build a simulation environment for a six-DOF (six-degree-of-freedom) manipulator with a gripper in ROS (Robot Operating System). Then, aiming at the shortcomings of traditional DDPG rewards, research and design a composite reward function. Aiming at the problem of low sampling efficiency in the free exploration of the robot arm, a batch of teaching data was added to the experience replay pool to improve learning efficiency. The simulation experiment results show that under the same number of episode of training. The improved DDPG grasping control algorithm has significantly improved the grasping success rate. The grasping success rate after comprehensive improvement reaches 70%, which is higher than the 36% level of unimproved DDPG.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9550413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9550413","DDPG;Reward Function;Demonstration;Six-DOF Arm Robot","Training;Operating systems;Grasping;Reinforcement learning;Manipulators;Hardware;Task analysis","deep learning (artificial intelligence);gradient methods;grippers;manipulators;motion control;robot programming","reinforcement learning control;DDPG reinforcement learning algorithm;deep deterministic policy gradient;DDPG grasping control algorithm;robot arm grasping control;robot operating system;gripping control;six-DOF manipulator","","1","","14","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Research on Fuzzy Reinforcement Learning Algorithm for Agents in Grids","F. Li; F. Luo; Y. Gao; D. Qi; J. Hu","School of Computer Science & Educational Software, Guangzhou University, Guangzhou, China; College of Automation Science & Engineering, South China University of Technology, Guangzhou, China; School of Computer Science & Educational Software, Guangzhou University, Guangzhou, China; School of Computer Science & Engineering, South China University of Technology, Guangzhou, China; School of Computer Science & Engineering, South China University of Technology, Guangzhou, China","2009 Third International Symposium on Intelligent Information Technology Application Workshops","25 Feb 2010","2009","","","336","339","How to improve the efficiency and performance of job scheduling in grid computing is one of the most important and challenging techniques. This paper tries to give out a novel grid job scheduling model based on agent technology. To make full use of intelligence and adaptability of the agents, dynamic fuzzy knowledge-base and corresponding fuzzy reinforcement learning algorithm are proposed for the job scheduling agents. The model and algorithm can largely meet the needs of intelligence, flexibility, scalability and optimization for grid job scheduling. Simulation experiments show that the proposed reinforcement learning algorithm for agents based on dynamic fuzzy knowledge-base works better compared with other similar learning algorithm.","","978-1-4244-6421-0","10.1109/IITAW.2009.119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5419425","Fuzzy Reinforcement Learning;Job Sscheduling Agents;Dynamic Fuzzy Knowledgebase;Rule","Learning;Scheduling algorithm;Grid computing;Processor scheduling;Dynamic scheduling;Distributed computing;Heuristic algorithms;Intelligent agent;Application software;Computer science","fuzzy set theory;grid computing;knowledge based systems;learning (artificial intelligence);multi-agent systems;scheduling","fuzzy reinforcement learning;grid computing;grid job scheduling;agent technology;dynamic fuzzy knowledge-base","","1","","15","IEEE","25 Feb 2010","","","IEEE","IEEE Conferences"
"Traffic Signal Control Based on Reinforcement Learning and Fuzzy Neural Network","H. Zhao; S. Chen; F. Zhu; H. Tang","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Fujian Provincial Key Laboratory of Intelligent Identification and Control of Complex Dynamic System, Fujian Institute of Research on the Structure of Matter, Chinese Academy of Sciences, Fuzhou, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China","2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)","1 Nov 2022","2022","","","4030","4035","For traffic signal control of intersections in cities, a new controller based on reinforcement learning and fuzzy neural network is proposed in this paper. The fuzzy neural network has the advantages of both fuzzy control and neural network, and overcome the former's lack of self-learning and generalization ability, and the latter's lack of understandability. Meanwhile, the reinforcement learning can make the controller improve itself on line continually by the simple feedback of environment. The result of computational experiments shows that the proposed traffic signal control algorithm can achieve a more effective optimization control.","","978-1-6654-6880-0","10.1109/ITSC55140.2022.9922570","National Natural Science Foundation of China(grant numbers:52071312); State Key Laboratory for Management and Control of Complex Systems(grant numbers:20220115); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9922570","","Fuzzy control;Adaptation models;Urban areas;Neural networks;Green products;Transportation;Reinforcement learning","feedback;fuzzy control;fuzzy neural nets;learning (artificial intelligence);neural nets;neurocontrollers;optimal control;road traffic;road traffic control;traffic control","reinforcement learning;fuzzy neural network;fuzzy control;traffic signal control algorithm;effective optimization control","","1","","31","IEEE","1 Nov 2022","","","IEEE","IEEE Conferences"
"Cooperative multiagent reinforcement learning using factor graphs","Z. Zhang; D. Zhao","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2013 Fourth International Conference on Intelligent Control and Information Processing (ICICIP)","25 Jul 2013","2013","","","797","802","In this paper, we propose a sparse reinforcement learning (RL) algorithm using factor graphs. The contribution is to make the original sparse RL algorithm applicable for tasks decomposed in a more general manner. For some problems, it is more reasonable to divide agents into cliques, each of which is responsible for its specific subtask. In this way, the global Q-value function is decomposed into the sum of simpler local Q-value functions, each of which may contain more than two action variables. Such decomposition can be expressed by a factor graph and exploited by the general max-plus algorithm to get the global greedy joint action. The experimental results show that our methodology is feasible and effective.","","978-1-4673-6249-8","10.1109/ICICIP.2013.6568181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6568181","","Joints;Learning (artificial intelligence);Games;Visualization;Approximation algorithms;Belief propagation","graph theory;learning (artificial intelligence);multi-agent systems","cooperative multiagent reinforcement learning;factor graphs;sparse reinforcement learning;cliques;global Q-value function;global greedy joint action","","1","","28","IEEE","25 Jul 2013","","","IEEE","IEEE Conferences"
"Homogeneous Transformation Matrix Based Neural Network for Model Based Reinforcement Learning on Robot Manipulator","M. R. Diprasetya; A. Schwung","Department of Automation Technology, South Westphalia University of Applied Science, Soest, Germany; Department of Automation Technology, South Westphalia University of Applied Science, Soest, Germany","2022 IEEE International Conference on Industrial Technology (ICIT)","5 Jan 2023","2022","","","1","6","This paper introduces a novel architecture of neural networks based on a homogeneous transformation matrix for model-based reinforcement learning in robotics. It shows how the homogeneous transformation neural network outperforms the standard feed-forward neural network in the robot manipulator area. The homogeneous transformation matrix can be an alternative for modeling robotic manipulators as a neural network. Since, there are a lot of differences in the robotic manipulator model, especially in geometry, a geometry transfer is introduced in this paper. It shows how to transfer the feature to the other homogeneous transformation matrix for a different type of geometry of the robot manipulator.","","978-1-7281-1948-9","10.1109/ICIT48603.2022.10002834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002834","Model-Based Reinforcement Learning;Homogeneous Transformation Matrix;Robotic","Geometry;Training;Service robots;Neural networks;Reinforcement learning;Predictive models;Manipulators","feedforward neural nets;manipulators;reinforcement learning","homogeneous transformation matrix;homogeneous transformation neural network;model based reinforcement learning;model-based reinforcement;modeling robotic manipulators;robot manipulator area;robotic manipulator model","","1","","13","IEEE","5 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement learning for mobile robot: From reaction to deliberation","C. Chunlin; C. Zonghai","Dept. of Automation, Univ. of Science and Technology of China, Hefei 230027, P. R. China; Dept. of Automation, Univ. of Science and Technology of China, Hefei 230027, P. R. China","Journal of Systems Engineering and Electronics","17 Jan 2012","2005","16","3","611","617","Reinforcement learning has been widely used for mobile robot learning and control. Some progress of this kind of approaches is surveyed and argued in a new way which emphasizes on different levels of algorithms according to different complexity of tasks. The central conjecture is that approaches which combine reactive and deliberative control to robotics scale better to complex real-world applications than purely reactive or deliberative ones. This paper describes basic reactive reinforcement learning algorithms and two classes of approaches to achieve deliberation, which are modular methods and hierarchical methods. By combining reactive and deliberative paradigms, the whole system gains advantages from different control levels. The paper gives results of experiments as a case study to verify the effectiveness of the proposed approaches.","1004-4132","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6071212","reinforcement learning;mobile robot;reactive control;deliberative control","Mobile robots;Collision avoidance;Markov processes;Learning;Function approximation;Humans","","","","1","","","","17 Jan 2012","","","BIAI","BIAI Journals"
"Reinforcement Learning Based Dynamic Inverse Attitude Control of Near-space Vehicle","Y. Shen; M. Chen","College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, P. R. China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, P. R. China","2020 39th Chinese Control Conference (CCC)","9 Sep 2020","2020","","","6972","6977","In this paper, a reinforcement learning (RL) based dynamic inverse attitude control scheme is proposed for near-space vehicle (NSV). Firstly, the conventional dynamic inverse control is employed to ensure the basic capability of NSV attitude tracking. Subsequently, RL is employed to tackle the system uncertainties. Actor-critic RL method is adopted to generate a compensation control signal in order to track attitude command better. Finally, simulation results illustrate that the proposed RL based dynamic inverse control scheme can obtain a better performance compared with the conventional dynamic inverse control scheme.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9189285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9189285","Near-space vehicle (NSV);Reinforcement learning (RL);Dynamic inverse;Attitude control","Aerodynamics;Attitude control;Vehicle dynamics;Mathematical model;Uncertainty;Manganese;Neural networks","attitude control;control engineering computing;learning (artificial intelligence);mechanical engineering computing;position control;space vehicles;vehicle dynamics","reinforcement learning;near-space vehicle;dynamic inverse attitude control scheme;basic capability;NSV attitude tracking;actor-critic RL method;compensation control signal;attitude command;conventional dynamic inverse control scheme","","1","","27","","9 Sep 2020","","","IEEE","IEEE Conferences"
"DC Motor Control based on Integral Reinforcement Learning","G. Bujgoi; D. Sendrescu","Department of Automation and Electronics, University of Craiova, Craiova, Romania; Department of Automation and Electronics, University of Craiova, Craiova, Romania","2022 23rd International Carpathian Control Conference (ICCC)","27 Jun 2022","2022","","","282","286","The paper presents the control of a DC motor using a machine learning technique known as integral reinforcement learning. The integral reinforcement learning control method belongs to the category of intelligent control systems. The main advantage of the integral reinforcement learning method is that it addresses continuous systems while most reinforcement learning methods are developed for discrete systems. The control system is based on a classic structure in reinforcement learning of critical – actor type. The critic is represented by a neural network that evaluates the efficiency of the actions generated by the actor (the correspondent of the controller in conventional control systems). Critic tuning (neural network training) is done online using the technique known as Temporal Difference Learning. The presented technique is tested and analysed both by simulation and implementation on an experimental platform.","","978-1-6654-6636-3","10.1109/ICCC54292.2022.9805935","European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9805935","reinforcement learning;dc motor control;artificial intelligence","Training;Terminology;Neural networks;Data acquisition;Process control;Reinforcement learning;DC motors","continuous systems;control engineering computing;DC motors;discrete systems;intelligent control;neural nets;power engineering computing;power system control;reinforcement learning","DC motor control;machine learning;integral reinforcement learning control;intelligent control systems;continuous systems;discrete systems;control system;critic tuning;temporal difference learning;neural network training","","1","","7","IEEE","27 Jun 2022","","","IEEE","IEEE Conferences"
"Research on Adaptive Proportion-Integral Controller Based on Deep Reinforcement Learning for DC-DC Boost Converter","Y. Mao; H. Zheng; L. Li; P. Song; X. Yu","Zhejiang Electric Transmission and Transformation Co., Ltd., Hangzhou, China; Zhejiang Electric Transmission and Transformation Co., Ltd., Hangzhou, China; Zhejiang Electric Transmission and Transformation Co., Ltd., Hangzhou, China; College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China","2022 7th International Conference on Power and Renewable Energy (ICPRE)","30 Nov 2022","2022","","","1","6","The control effect of boost converter with fixed parameters PI controller is not ideal when facing large and frequent load changes. A self-tuning PI control strategy based on the Double DQN algorithm is proposed to improve the performance. Dynamic optimization function of deep reinforcement learning is applied to adjust the parameters of PI controller. This strategy generates self-tuning PI-KP/PI-KI controller based on the Double DQN algorithm. Simulation results show that the output voltage of the boost converter is stable when the external environment interferes or the input voltage or load fluctuates. Compared with traditional PI control, this strategy can adjust parameters in real time, and both steady-state performance and dynamic performance are improved.","2768-0525","978-1-6654-5063-8","10.1109/ICPRE55555.2022.9960514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9960514","boost converter;reinforcement learning;PI control;parameter optimization;adaptive control","Deep learning;Renewable energy sources;PI control;Heuristic algorithms;Simulation;Reinforcement learning;Real-time systems","adaptive control;DC-DC power convertors;fuzzy control;PI control;reinforcement learning;self-adjusting systems","adaptive proportion-integral controller;control effect;DC-DC boost converter;deep reinforcement learning;double DQN algorithm;dynamic optimization function;dynamic performance;fixed parameters PI controller;frequent load changes;PI-KI controller;PI-KP controller;self-tuning PI control strategy","","","","7","IEEE","30 Nov 2022","","","IEEE","IEEE Conferences"
"Playing games with reinforcement learning via perceiving orientation and exploring diversity","D. Zhang; L. Yang; H. Shi; F. Mou; M. Hu","School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China; School of Electronics Engineering and Computer Science, Peking University, Peking, China","2017 International Conference on Progress in Informatics and Computing (PIC)","17 May 2018","2017","","","30","34","The reinforcement learning can guide the agents to perform optimally under various complex environments. Although reinforcement learning has brought breakthrough for many domains, they are constrained by two bottlenecks: extremely delayed reward signal and the trade-off between diversity and speed. In this paper, we propose a novel framework to alleviate those two bottlenecks. For the delayed reward, we introduce a new term, named the orientation perception term, to calculate the award for each state. For a series of actions successfully leading to the target state, this term takes a difference to each state and assigns award to all states on the pathway, rather than only offers award to the target state. This mechanism allows the learning algorithm to percept the orientation information by distinguishing different states. For the trade-off between diversity and speed, we integrate the curriculum learning into the exploration process and propose the diversity exploration scheme. In the beginning, this scheme is prone to exploring the unexecuted action so as to discover the optimal action series. With the learning process carrying on, the scheme gradually relays more on the acquired knowledge and reduces the random probability. Such randomicity to certainty diversity exploration scheme guides the learning scheme to achieve proper balance between strategy diversity and convergency speed. We name the complete framework OpDe Reinforcement Learning and prove the algorithm convergence. Experiments on a standard platform demonstrate the effectiveness of the complete framework.","","978-1-5386-1978-0","10.1109/PIC.2017.8359509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359509","reinforcement learning;delayed reward;orientation perception;diversity exploration;curriculum learning","Learning (artificial intelligence);Heuristic algorithms;Task analysis;Stochastic processes;Convergence;Standards;Computer science","learning (artificial intelligence);probability","strategy diversity;complete framework OpDe Reinforcement Learning;exploring diversity;bottlenecks;delayed reward signal;orientation perception term;target state;learning algorithm;orientation information;exploration process;optimal action series;learning process;certainty diversity exploration scheme;learning scheme","","","","11","IEEE","17 May 2018","","","IEEE","IEEE Conferences"
"AgentFuzz: Fuzzing for Deep Reinforcement Learning Systems","T. Li; X. Wan; M. M. Özbek","School of Automation Science and Electronic Engineering, Beihang University, Bejing, China; School of Automation Science and Electronic Engineering, Beihang University, Bejing, China; Department of Aeronautical and Astronautical Engineering, Istanbul Technical University, Istanbul, Turkey","2022 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","26 Dec 2022","2022","","","110","113","In recent years, deep reinforcement learning (DRL) technology has developed rapidly, and the application of DRL has been extended to many fields such as game gaming, au-tonomous driving, financial transactions, and robot control. As DRL applications expand and enrich, quality assurance of DRL software is increasingly important, especially in safety -critical areas. Therefore, it is necessary and urgent to adequately test DRL models to ensure the reliability and security of DRL systems. However, due to fundamental differences, traditional software testing methods cannot be directly applied to D RL systems. To bridge this gap, we introduce a new DRL system testing framework in this proposal, which aims to generate various test cases that can cause D RL systems to fail. The proposed testing framework is the first fuzzing framework for systematically testing DRL systems which we call AgentFuzz.","","978-1-6654-7679-9","10.1109/ISSREW55968.2022.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985158","Deep reinforcement learning system;deep rein-forcement learning testing;fuzzing","Deep learning;System testing;Robot control;Reinforcement learning;Fuzzing;Software;Software reliability","deep learning (artificial intelligence);program testing;reinforcement learning","AgentFuzz;deep reinforcement learning systems;DRL software;DRL system testing","","","","26","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Quadrotor Mapless Navigation in Static and Dynamic Environments based on Deep Reinforcement Learning","T. -H. Tsai; Q. Li","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","2021 3rd International Conference on Industrial Artificial Intelligence (IAI)","30 Nov 2021","2021","","","1","7","In this paper, we propose a mapless autonomous navigation planner which plans a collision-free trajectory for quadrotor without any manual operations. Deep Reinforcement Learning (DRL) can optimize the policy by trial and error without knowing the prior information of the environment. The designed reward function has better convergence which compares to the benchmark method. The learned policy makes a real time collision free trajectory which can cope with the dynamic obstacles under different scenarios. The evaluation result shows that the trained model can be applied directly to the unknown environment without retraining the agent.","","978-1-6654-3517-8","10.1109/IAI53119.2021.9619200","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619200","deep reinforcement learning;obstacle avoidance;motion planning;autonomous navigation","Navigation;Reinforcement learning;Benchmark testing;Laser modes;Robustness;Real-time systems;Trajectory","collision avoidance;distributed control;learning (artificial intelligence);mobile robots;navigation;path planning;robot vision","collision-free trajectory;deep Reinforcement Learning;Deep Reinforcement Learning;designed reward function;dynamic obstacles;learned policy;manual operations;mapless autonomous navigation planner;quadrotor mapless navigation;time collision free trajectory;unknown environment","","","","34","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Method for AUV Hover Control in Uncertain Environment","Y. Zhao; Y. Tian","School of Automation, Hangzhou Dianzi University, Hangzhou; School of Automation, Hangzhou Dianzi University, Hangzhou","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","4704","4709","Autonomous underwater vehicle (AUV) is required to have the hovering positioning ability to maintain hovering and counteract environmental disturbance when performing some tasks such as ocean exploration and fishing. The complex and changeable marine environment, as well as the strong coupling and nonlinearity of the underwater vehicle itself, pose a great challenge to its control system. In this paper, a hover positioning method of autonomous underwater vehicle based on deep reinforcement learning algorithm is proposed to resist the interference of unknown size and direction of ocean current. Two kinds of neural networks, Actor and Critical, are designed using the depth deterministic strategy gradient method. The Actor neural network gives the control strategy, and the Critical neural network is used to evaluate the strategy. Combined with the principle of environmental optimal heading, the simulation results show that the AUV can effectively achieve hover control and save energy consumption under the uncertain environment.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240614","AUV;Hover Control;Reinforcement Learning;Deep Deterministic Policy Gradient;Weather Optimal Heading Control","Couplings;Energy consumption;Autonomous underwater vehicles;Oceans;Simulation;Neural networks;Reinforcement learning","autonomous underwater vehicles;deep learning (artificial intelligence);gradient methods;mobile robots;neurocontrollers;position control;reinforcement learning;remotely operated vehicles;underwater vehicles","Actor neural network;autonomous underwater vehicle;AUV hover control;changeable marine environment;complex marine environment;control strategy;control system;Critical neural network;deep reinforcement learning algorithm;depth deterministic strategy gradient method;environmental disturbance;environmental optimal heading;fishing;hover positioning method;hovering positioning ability;neural networks;nonlinearity;ocean current;ocean exploration;uncertain environment;unknown size","","","","12","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Collaborative Search Planning of UAV Swarms Based on Deep Reinforcement Learning","L. Zou; Y. Tan","National key lab of Multispectral Information Intelligent processing technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; National key lab of Multispectral Information Intelligent processing technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","2023 5th International Conference on Intelligent Control, Measurement and Signal Processing (ICMSP)","12 Jul 2023","2023","","","1184","1187","In a multi-agent environment, efficient collaborative search among multiple unmanned aerial vehicles (UAVs) is crucial for area search and path planning. Therefore, it is important to consider the collaborative learning among UAVs when designing collaborative strategies. This paper analyzes the problems of existing value-decomposition-based multi-agent reinforcement learning algorithms in multi-UAV area search and path planning, namely the underestimation of optimal joint actions, which leads to suboptimal policy generation. Subsequently, a new non-monotonicity value-decomposition algorithm is proposed based on the addition of a masked highway connection strategy. This algorithm suppresses the optimization of certain action pairs and focuses more on the optimal joint actions, thus better optimizing the objective and recovering the value of optimal joint actions. Multiple simulation experiments demonstrate that the proposed algorithm can achieve improved performance in environments involving UAV swarms and enhance the collaborative effectiveness of UAV swarms in search tasks.","","979-8-3503-3603-0","10.1109/ICMSP58539.2023.10170972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10170972","multi-agent;collaborative search;highway connection;UAVs","Road transportation;Signal processing algorithms;Collaboration;Reinforcement learning;Signal processing;Search problems;Autonomous aerial vehicles","autonomous aerial vehicles;deep learning (artificial intelligence);learning (artificial intelligence);multi-agent systems;multi-robot systems;path planning;reinforcement learning;remotely operated vehicles","collaborative effectiveness;collaborative learning;collaborative search planning;collaborative strategies;deep reinforcement learning;efficient collaborative search;existing value-decomposition-based multiagent reinforcement;masked highway connection strategy;multiagent environment;multiple unmanned aerial vehicles;multiUAV area search;nonmonotonicity value-decomposition algorithm;optimal joint actions;path planning;search tasks;UAV swarms;UAVs","","","","11","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Energy Saving Algorithm of HVAC System Based on Deep Reinforcement Learning with Modelica Model","Y. Zhang; Q. Zhao","Department of Automation, Center for Intelligent and Networked Systems, BNRist, Tsinghua University, Beijing, China; Department of Automation, Center for Intelligent and Networked Systems, BNRist, Tsinghua University, Beijing, China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","5277","5282","Aiming at the energy-saving control problem of Heating, Ventilation and Air Conditioning (HVAC) system, a Deep Reinforcement Learning (DRL) control method based on Deep Deterministic Policy Gradient (DDPG) is proposed. The optimization capability of DDPG is improved by collecting sensor data inside the HVAC system and using Autoencoder (AE) for feature extraction. DDPG improves the system Coefficient of Performance (system COP) by controlling the mass flow of the chilled water pump of the HVAC system. This method is simulated, trained and verified on the refined HVAC system model built in this paper. The experimental results show that the method proposed can effectively improve the system COP and achieve the goal of saving energy in the HVAC system.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9901641","National Natural Science Foundation of China(grant numbers:62192751,61425027); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9901641","HVAC;Modelica;DDPG;Autoencoder","Heating systems;Air conditioning;HVAC;Reinforcement learning;Aerospace electronics;Feature extraction;Control systems","control engineering computing;deep learning (artificial intelligence);energy conservation;feature extraction;flow control;gradient methods;HVAC;power engineering computing;reinforcement learning;water pumps","autoencoder;chilled water pump;coefficient of performance;DDPG;deep deterministic policy gradient;deep reinforcement learning;DRL;energy saving control;feature extraction;heating ventilation and air conditioning system;HVAC system;mass flow control;Modelica model;optimization capability;sensor data;system COP","","","","17","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Minimize Pressure Difference Traffic Signal Control Based on Deep Reinforcement Learning","P. Yu; J. Luo","College of Automation & College of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Automation & College of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","5493","5498","In the traffic signal light control method based on deep reinforcement learning, there is a challenging problem to coordinate control traffic signal because of the high complexity of the road network. This paper proposes a minimize pressure difference traffic signal control based on deep reinforcement learning. It use traffic pressure difference at each phase of the intersection as the reward. The pressure value considers the queue length of vehicles and the capacity of the roads. The traffic state of the intersection is converted into a two-dimensional matrix composed of vehicle position and speed information, the control strategy is learned online through the deep double Q-learning network model to realize the adaptive control of the traffic signal. The algorithm proposed in this paper is simulated by the traffic simulation platform SUMO and compared with two existing traffic signal control methods based on deep reinforcement learning. The experimental results show that the algorithm has a faster convergence rate and better performance in regional traffic signal coordination control.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9901790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9901790","Adaptive Traffic Signal Control;Multi-agent System;Deep Reinforcement Learning;Pressure Difference","Adaptation models;Q-learning;Roads;Reinforcement learning;Traffic control;Data models;Complexity theory","adaptive control;control engineering computing;deep learning (artificial intelligence);matrix algebra;position control;reinforcement learning;road traffic control;road vehicles;traffic engineering computing;velocity control","adaptive control;deep double Q-learning network;deep reinforcement learning;SUMO;traffic pressure difference;traffic signal light control;traffic simulation platform;two-dimensional matrix;vehicle position;vehicle speed","","","","18","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning-based Active Disturbance Rejection Control for Nonlinear Systems with Disturbance","X. Kong; Y. Xia","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China","2023 2nd Conference on Fully Actuated System Theory and Applications (CFASTA)","12 Sep 2023","2023","","","799","804","This paper proposes a reinforcement learning-based active disturbance rejection controller (RL-ADRC) for trajectory tracking control of partially unknown nonlinear systems with external disturbances. It is also a complementary combination of RL and ADRC. In this method, an actor-critic-based RL algorithm is employed to explore an optimal and adaptive control strategy. Unlike traditional ADRC, which requires manual tuning of controller parameters, RL-ADRC utilizes an actor-critic network to approximate the optimal control strategy. By adopting the disturbance compensation philosophy from ADRC and integrating it with RL, RL-ADRC exhibits improved robustness against tracking errors, and the learning process is more efficient in terms of time.","","979-8-3503-3216-2","10.1109/CFASTA57821.2023.10243334","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10243334","Reinforcement learning;Active Disturbance Rejection Controller;Trajectory Tracking Control","Robust control;Philosophical considerations;Trajectory tracking;Optimal control;Manuals;Approximation algorithms;Robustness","active disturbance rejection control;adaptive control;control system synthesis;learning (artificial intelligence);neurocontrollers;nonlinear control systems;optimal control;reinforcement learning;robust control","actor-critic-based RL algorithm;adaptive control strategy;controller parameters;disturbance compensation philosophy;external disturbances;optimal control strategy;partially unknown nonlinear systems;reinforcement learning-based active disturbance rejection control;reinforcement learning-based active disturbance rejection controller;RL-ADRC exhibits;trajectory tracking control","","","","31","IEEE","12 Sep 2023","","","IEEE","IEEE Conferences"
"Output-feedback Quadratic Tracking Control of Continuous-time Systems by Using Off-policy Reinforcement Learning with Neural Networks Observer","Q. Meng; Y. Peng","School of Automation Science and Engineering, South China University of Technology, Guangzhou; School of Automation Science and Engineering, South China University of Technology, Guangzhou","2020 Chinese Control And Decision Conference (CCDC)","11 Aug 2020","2020","","","1504","1509","In this paper, an improved off-policy reinforcement learning (RL) algorithm with neural networks(NN) observer is proposed to solve the linear quadratic tracking (LQT) problem for continuous-time (CT) systems without any knowledge of the system dynamics. The offline algorithm solves a Lyapunov equation to find a optimal solution which requires complete knowledge of the system dynamics. Later the off-policy RL algorithm was used to solve the state-feedback control which does not require any knowledge of the system dynamics by using the same input and state information repeatedly in previous research. The proposed output-feedback (OPFB) control algorithm solves Bellman equation which demands the system state information by using an adaptive NN state observer to estimate the system state with the input and output information of CT systems. Simulation results provide the efficiency of the proposed approach.","1948-9447","978-1-7281-5855-6","10.1109/CCDC49329.2020.9164737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9164737","Off-policy;Reinforcement Learning (RL);Linear Quadratic Tracking (LQT);Output-feedback (OPFB)","Heuristic algorithms;System dynamics;Observers;Trajectory;Adaptive systems;Optimal control;Mathematical model","adaptive control;continuous time systems;control engineering computing;learning (artificial intelligence);linear quadratic control;linear systems;Lyapunov methods;neurocontrollers;observers;optimal control;state feedback;tracking","continuous-time systems;neural networks observer;off-policy reinforcement learning algorithm;linear quadratic tracking problem;system dynamics;offline algorithm;off-policy RL algorithm;state-feedback control;output-feedback control algorithm;system state information;adaptive NN state observer;output information;CT systems;output-feedback quadratic tracking control;Lyapunov equation;Bellman equation","","","","29","IEEE","11 Aug 2020","","","IEEE","IEEE Conferences"
"A FPPT Scheme Based on Reinforcement Learning in Photovoltaic Power Systems","Y. Liu; C. Cui; S. Zhang","College of Automation Engineering Shanghai University of Electric Power, Shanghai, China; College of Automation Engineering Shanghai University of Electric Power, Shanghai, China; Shanghai Electrical Apparatus Research Institute, Shanghai, China","2022 4th International Conference on Electrical Engineering and Control Technologies (CEECT)","7 Feb 2023","2022","","","159","164","For Photovoltaic (PV) power systems to capture the maximum power from solar energy, the maximum power point tracking (MPPT) is necessary. Since the intermittent nature of solar energy and the ever-changing environment, MPPT may cause power and voltage fluctuations which may cause problems for grid stability. Furthermore, the fluctuation may increase the PV converter's heat load and reduce its service life. To limit power fluctuations, a flexible power point tracking (FPPT) scheme is proposed. As a result, the system is more dependable. This article proposes a FPPT scheme based on reinforcement learning (RL) to increase the FPPT performance in terms of high speed and precision. RL makes the system hardware structure simpler and enables faster tracking speed and higher accuracy than traditional methods. The open source OpenAI Gym platform is applied to construct the PV environment. The constructed simulation environment is applied to conduct experiments to evaluate the proposed control scheme's robustness in various environmental scenarios. The simulations results show the feasibility of the scheme with fast response and stable performance.","","978-1-6654-9899-9","10.1109/CEECT55960.2022.10030689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10030689","power control;flexible power point tracking (FPPT);photovoltaic system;reinforcement learning (RL)","Maximum power point trackers;Photovoltaic systems;Fluctuations;Voltage fluctuations;Simulation;Solar energy;Reinforcement learning","maximum power point trackers;photovoltaic power systems;power generation control;power grids;reinforcement learning","constructed simulation environment;control scheme;flexible power point tracking scheme;FPPT performance;FPPT scheme;grid stability;intermittent nature;maximum power point tracking;MPPT;Photovoltaic power systems;power fluctuations;PV converter;PV environment;reinforcement learning;RL;solar energy;system hardware structure;voltage fluctuations","","","","26","IEEE","7 Feb 2023","","","IEEE","IEEE Conferences"
"Active Gamma-Ray Log Pattern Localization With Distributionally Robust Reinforcement Learning","Y. Zi; L. Fan; X. Wu; J. Chen; S. Wang; Z. Han","Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; Department of Engineering Technology, University of Houston, Houston, TX, USA; Department of Information and Logistics Technology, University of Houston, Houston, TX, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA","IEEE Transactions on Geoscience and Remote Sensing","8 Jun 2023","2023","61","","1","11","Accurately localizing 1-D signal patterns, such as Gamma-ray well-log depth matching, is crucial in the oilfield service industry as it directly affects the quality of oil and gas exploration. However, traditional methods such as well-log curve analysis and pattern hand-picking matching are labor-intensive and heavily rely on human expertise, leading to inconsistent results. Although attempts have been made to automate this process, challenges such as low computational performance, nonrobustness, and nongeneralization remain unsolved. To address these challenges, we have developed a data-driven AI system that learns an active signal pattern localization strategy inspired by human attention. Our artificial intelligence system uses an offline reinforcement learning (RL) framework as its central component, which solves a highly abstracted Markov decision process (MDP) problem via offline training on human-labeled historical data. The RL agent uses top-down reasoning to determine the location of target signal fragments by deforming a bounding window using simple transformation actions. To overcome distribution shifts between logged data and real and ensure generalization, we propose a discrete distributionally robust soft actor-critic (SAC) RL framework (DRSAC-Discrete) to solve the MDP problem under uncertainty. By exploring unfamiliar environments in a restrictive manner, the DRSAC-Discrete algorithm provides a safe solution that can be used when data is limited during the early stage of this industrial application. We evaluated the RL-based localization system on augmented field Gamma-ray well-log datasets, and the results showed promising localization capability. Furthermore, the DRSAC-Discrete algorithm demonstrated relatively robust performance guarantees when facing data shortage.","1558-0644","","10.1109/TGRS.2023.3278491","NSF(grant numbers:CNS-2107216,CNS-2128368,CMMI-2222810,NSF-EPCN-2045978); U.S. Department of Transportation;; Toyota Motor Corporation; Amazon Catalyst; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138725","Distributionally robustness;gamma-ray log;Markov decision processes (MDPs);signal localization","Uncertainty;Location awareness;Reinforcement learning;Gamma-rays;Rocks;Robustness;Markov processes","gas industry;Markov processes;petroleum industry;reinforcement learning","1-D signal patterns;active gamma-ray log pattern localization;active signal pattern localization strategy;artificial intelligence system;augmented field Gamma-ray well;data shortage;data-driven AI system;discrete distributionally robust soft actor-critic RL framework;distributionally robust reinforcement learning;DRSAC-Discrete algorithm;gamma-ray well-log depth matching;gas exploration;highly abstracted Markov decision process problem;human-labeled historical data;industrial application;logged data;low computational performance;MDP problem;offline reinforcement learning framework;offline training;oilfield service industry;pattern hand-picking matching;RL agent;RL-based localization system;SAC;target signal fragments;well-log curve analysis","","","","37","IEEE","30 May 2023","","","IEEE","IEEE Journals"
