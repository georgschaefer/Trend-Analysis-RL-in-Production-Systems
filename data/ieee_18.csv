"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Reinforcement Learning-Based Model Predictive Control for Discrete-Time Systems","M. Lin; Z. Sun; Y. Xia; J. Zhang","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","13","This article proposes a novel reinforcement learning-based model predictive control (RLMPC) scheme for discrete-time systems. The scheme integrates model predictive control (MPC) and reinforcement learning (RL) through policy iteration (PI), where MPC is a policy generator and the RL technique is employed to evaluate the policy. Then the obtained value function is taken as the terminal cost of MPC, thus improving the generated policy. The advantage of doing so is that it rules out the need for the offline design paradigm of the terminal cost, the auxiliary controller, and the terminal constraint in traditional MPC. Moreover, RLMPC proposed in this article enables a more flexible choice of prediction horizon due to the elimination of the terminal constraint, which has great potential in reducing the computational burden. We provide a rigorous analysis of the convergence, feasibility, and stability properties of RLMPC. Simulation results show that RLMPC achieves nearly the same performance as traditional MPC in the control of linear systems and exhibits superiority over traditional MPC for nonlinear ones.","2162-2388","","10.1109/TNNLS.2023.3273590","Beijing Municipal Science Foundation(grant numbers:4222052); National Natural Science Foundation of China(grant numbers:62003040,61836001,61720106010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129251","Discrete-time systems;model predictive control;policy iteration (PI);reinforcement learning (RL)","Costs;Predictive control;Discrete-time systems;Stability criteria;Convergence;Task analysis;Sun","","","","1","","","IEEE","19 May 2023","","","IEEE","IEEE Early Access Articles"
"Speed and heading control of an unmanned surface vehicle using deep reinforcement learning","T. Wu; H. Ye; Z. Xiang; X. Yang","School of Automation, Jiangsu University of Science and Technology, Zhenjiang, China; School of Automation, Jiangsu University of Science and Technology, Zhenjiang, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Jiangsu University of Science and Technology, Zhenjiang, China","2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)","7 Jul 2023","2023","","","573","578","In this paper, a deep reinforcement learning-based speed and heading control method is proposed for an unmanned surface vehicle (USV). A deep deterministic policy gradient (DDPG) algorithm which combines with an actor-critic reinforcement learning mechanism, is adopted to provide continuous control variables by interacting with the environment. Moreover, two types of reward functions are created for speed and heading control of the USV. The control policy is trained by trial and error so that the USV can be guided to achieve the desired speed and heading angle steadily and rapidly. Simulation results verify the feasibility and effectiveness of the proposed approach by comparisons with classical PID control and S plane control.","2767-9861","979-8-3503-2105-0","10.1109/DDCLS58216.2023.10166143","National Natural Science Foundation of China(grant numbers:62173341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10166143","Deep reinforcement learning;DDPG algorithm;unmanned surface vehicle","Deep learning;Learning systems;PI control;Simulation;Optimal control;Reinforcement learning;Prediction algorithms","deep learning (artificial intelligence);gradient methods;mobile robots;reinforcement learning;three-term control;unmanned surface vehicles","actor-critic reinforcement learning mechanism;continuous control variables;control policy;deep deterministic policy gradient algorithm;deep reinforcement learning-based speed;heading angle;heading control method;PID control;S plane control;unmanned surface vehicle;USV","","1","","13","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"A kernel-based reinforcement learning approach to stochastic pole balancing control systems","X. Xu; P. Chengzhang; B. Dai; H. -g. He","Institute of Automation, National University of Defense Technology, Changsha, China; Institute of Automation, National University of Defense Technology, Changsha, China; Institute of Automation, National University of Defense Technology, Changsha, China; Institute of Automation, National University of Defense Technology, Changsha, China","2010 IEEE/ASME International Conference on Advanced Intelligent Mechatronics","20 Jan 2011","2010","","","1329","1334","As a benchmark control problem with nonlinearity and instability, the controller design for inverted pendulums becomes more difficult when there are model uncertainties and unknown disturbances in the plant dynamics. In this paper, a kernel-based reinforcement learning controller is developed for inverted pendulums with unknown dynamics and stochastic disturbances. The learning controller makes use of approximate policy iteration with kernel-based least-squares temporal difference learning for policy evaluation. Due to the nonlinear approximation ability of kernel methods, good convergence property and learning efficiency can be realized in the approximate policy iteration process so that the controller performance can be optimized in a few iterations. Simulation results demonstrate that the proposed learning controller for stochastic inverted pendulums can achieve much better performance than previous learning control approaches such as Q-learning with function approximation and least-squares policy iteration (LSPI).","2159-6255","978-1-4244-8032-6","10.1109/AIM.2010.5695878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695878","","Markov processes;Approximation algorithms;Function approximation;Convergence;Kernel","approximation theory;control system synthesis;convergence;iterative methods;learning (artificial intelligence);learning systems;nonlinear control systems;pendulums;poles and zeros;stochastic systems","stochastic pole balancing control systems;controller design;kernel-based reinforcement learning controller;approximate policy iteration;kernel-based least-squares temporal difference learning;nonlinear approximation;convergence property;stochastic inverted pendulums","","1","","17","IEEE","20 Jan 2011","","","IEEE","IEEE Conferences"
"The driver and the engineer: Reinforcement learning and robust control","N. Bernat; J. Chen; N. Matni; J. Doyle","Department of Computing and Mathematical Sciences, California Institute of Technology, Pasadena, CA; Department of Computing and Mathematical Sciences, California Institute of Technology, Pasadena, CA; Department of Electrical and Systems Engineering, University of Pennsylvania; Department of Computing and Mathematical Sciences, California Institute of Technology, Pasadena, CA","2020 American Control Conference (ACC)","27 Jul 2020","2020","","","3932","3939","Reinforcement learning (RL) and other AI methods are exciting approaches to data-driven control design, but RL's emphasis on maximizing expected performance contrasts with robust control theory (RCT), which puts central emphasis on the impact of model uncertainty and worst case scenarios. This paper argues that these approaches are potentially complementary, roughly analogous to that of a driver and an engineer in, say, formula one racing. Each is indispensable but with radically different roles. If RL takes the driver seat in safety critical applications, RCT may still play a role in plant design, and also in diagnosing and mitigating the effects of performance degradation due to changes or failures in component or environments. While much RCT research emphasizes synthesis of controllers, as does RL, in practice RCT's impact has perhaps already been greater in using hard limits and tradeoffs on robust performance to provide insight into plant design, interpreted broadly as including sensor, actuator, communications, and computer selection and placement in addition to core plant dynamics. More automation may ultimately require more rigor and theory, not less, if our systems are going to be both more efficient and robust. Here we use the simplest possible toy model to illustrate how RCT can potentially augment RL in finding mechanistic explanations when control is not merely hard, but impossible, and issues in making them more compatibly data-driven. Despite the simplicity, questions abound. We also discuss the relevance of these ideas to more realistic challenges.","2378-5861","978-1-5386-8266-1","10.23919/ACC45564.2020.9147347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9147347","","Computational modeling;Robustness;Robust control;System dynamics;Trajectory;Toy manufacturing industry","control system synthesis;learning (artificial intelligence);robust control;uncertain systems","model uncertainty;safety critical applications;reinforcement learning;data-driven control design;robust control theory;learning based control;controller synthesis","","1","","21","","27 Jul 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning With Part-Aware Exploration Bonus in Video Games","P. Xu; Q. Yin; J. Zhang; K. Huang","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent System and Engineering, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent System and Engineering, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent System and Engineering and the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Games","15 Dec 2022","2022","14","4","644","653","Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to agents. However, environments with dense rewards are rare, motivating the need for developing reward functions that are intrinsic to agents. Curiosity is a type of successful intrinsic reward function, which uses the prediction error as an reward signal. In prior work, the prediction problem used to generate intrinsic rewards is optimized in the pixel space rather than a learnable feature space to avoid randomness caused by feature changes. However, these methods ignore small but important elements of the states that are often associated with locations of the character, which makes it impossible to generate accurate internal rewards for efficient exploration. In this article, we first demonstrate the effectiveness of introducing prior learned features for existing prediction-based exploration methods. Then, an attention map mechanism is designed to discretize learned features, thereby updating the learned feature and meanwhile reducing the impact of randomness on intrinsic rewards caused by the learning process of features. We verify our method on some video games from the standard reinforcement learning Atari benchmark, achieving clear improvements over random network distillation, which is one of the most advanced exploration methods, in almost all Atari games.","2475-1510","","10.1109/TG.2021.3134259","National Natural Science Foundation of China(grant numbers:61876181,61721004); Beijing Nova Program of Science and Technology(grant numbers:Z191100001119043); Youth Innovation Promotion Association; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647912","Deep learning;exploration;reinforcement learning;video game","Games;Reinforcement learning;Incentive schemes;Feature extraction;Task analysis;Standards;Visualization","computer games;deep learning (artificial intelligence);human factors;reinforcement learning","accurate internal rewards;advanced exploration methods;deep reinforcement learning;dense rewards;feature changes;intrinsic rewards;learnable feature space;part-aware exploration bonus;pixel space;prediction error;prediction problem;prediction-based exploration methods;prior learned features;reinforcement learning algorithms;reward functions;reward signal;standard reinforcement learning Atari benchmark;successful intrinsic reward function;video games","","1","","35","IEEE","13 Dec 2021","","","IEEE","IEEE Journals"
"Transient Voltage Control Based on Physics-Informed Reinforcement Learning","J. Gao; S. Chen; X. Li; J. Zhang","School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China","IEEE Journal of Radio Frequency Identification","12 Dec 2022","2022","6","","905","910","With the continuous expansion of power grid scale and the continuous implementation of Energy Internet construction, the long-distance and large-capacity electric energy exchange between regional power grids is increasingly frequent, which makes the stability problem to a wide range of attention. Therefore, this paper proposes a transient voltage control method based on physics information and reinforcement learning, which is called Physics-Informed Reinforcement Learning. This method combines the physical model and the data-driven model of power system, and takes the constraints in the physical model as the constraints of the data-driven model to accelerate the convergence rate of the model, so as to realize the rapid scheduling of transient voltage instability. Finally, an example of IEEE-9 bus system is given to verify the effectiveness and superiority of the proposed method.","2469-7281","","10.1109/JRFID.2022.3213895","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9916278","Physics-informed;deep reinforcement learning;physics-informed reinforcement learning (PIRL);transient voltage control;transient instability","Transient analysis;Neural networks;Power system stability;Reinforcement learning;Mathematical models;Generators;Voltage control","learning (artificial intelligence);power engineering computing;power grids;voltage control","continuous expansion;continuous implementation;data-driven model;Energy Internet construction;large-capacity electric energy exchange;long-distance;physical model;physics information;Physics-Informed Reinforcement Learning;power grid scale;power system;regional power grids;transient voltage control method;transient voltage instability","","","","14","IEEE","12 Oct 2022","","","IEEE","IEEE Journals"
"Research on Frequency Stability Emergency Control Strategy Based on Deep Reinforcement Learning","X. Fu; T. Lin; Q. L. i; H. Du; X. Xu","School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; Central China Branch, State Grid Corporation of China, Wuhan, China","2022 IEEE 6th Conference on Energy Internet and Energy System Integration (EI2)","10 May 2023","2022","","","1190","1195","In order to achieve the goal of peak carbon dioxide emissions and carbon neutrality, and build a clean, low-carbon, safe, controllable, flexible, efficient, intelligent, friendly, open and interactive new-type power system, a large number of conventional generation units have been replaced by renewable energy sources, resulting in the reduction of the inertia coefficient of the power system and the deterioration of the transient frequency stability. After the HVDC blocking of a power grid, taking the sending end power grid as an example, a large amount of active power surplus leads to the increase of frequency, which may lead to the transient frequency instability of the power grid. However, the operation modes of the power system with high proportion of renewable energy sources are complex and changeable, and the manual establishment of different emergency control strategies for massive operation modes leads to a tremendous increase in workload and low efficiency. Therefore, this paper proposes a method, which in turn establishes a fast prediction model of frequency emergency control strategies based on deep reinforcement learning (DRL). From the perspective of data-driven, aiming at the frequency instability issues of HVDC blocking, a fast prediction model of frequency emergency control strategies is established to improve the adaptability of emergency control strategy to complex and variable operation modes. Finally, based on a regional power grid model, the application of this method is explained, and its effectiveness is verified.","","979-8-3503-4715-9","10.1109/EI256261.2022.10116102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10116102","New-type power system;HVDC blocking;Transient frequency characteristics;Emergency control;Deep reinforcement learning","Deep learning;Adaptation models;Renewable energy sources;Reinforcement learning;Power system stability;Predictive models;Stability analysis","frequency stability;HVDC power convertors;HVDC power transmission;power grids;power system transient stability;reinforcement learning;renewable energy sources","active power surplus;carbon neutrality;clean carbon;complex operation modes;controllable type power system;conventional generation units;deep reinforcement learning;different emergency control strategies;efficient type power system;fast prediction model;flexible type power system;frequency emergency control strategies;frequency instability issues;frequency stability emergency control strategy;friendly type power system;intelligent type power system;interactive new-type power system;low-carbon;massive operation modes;open type power system;peak carbon dioxide emissions;regional power grid model;renewable energy sources;safe type power system;sending end power grid;transient frequency instability;transient frequency stability;variable operation modes","","","","13","IEEE","10 May 2023","","","IEEE","IEEE Conferences"
"Deep Multi-Task Multi-Agent Reinforcement Learning With Knowledge Transfer","Y. Mai; Y. Zang; Q. Yin; W. Ni; K. Huang","Center for Research on Intelligent System and Engineering, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent System and Engineering, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent System and Engineering, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent System and Engineering, Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Games","","2023","PP","99","1","11","Despite the potential of Multi-Agent Reinforcement Learning (MARL) in addressing numerous complex tasks, training a single team of MARL agents to handle multiple diverse team tasks remains a challenge. In this paper, we introduce a novel Multi-task method based on Knowledge Transfer in cooperative MARL (MKT-MARL). By learning from task-specific teachers, our approach empowers a single team of agents to attain expert-level performance in multiple tasks. MKT-MARL utilizes a knowledge distillation algorithm specifically designed for the multi-agent architecture, which rapidly learns a team control policy incorporating common coordinated knowledge from the experience of task-specific teachers. Additionally, we enhance this training with teacher annealing, gradually shifting the model's learning from distillation towards environmental rewards. This enhancement helps the multi-task model surpass its single-task teachers. We extensively evaluate our algorithm using two commonly-used benchmarks: StarCraft II micro-management and multi-agent particle environment. The experimental results demonstrate that our algorithm outperforms both the single-task teachers and a jointly-trained team of agents. Extensive ablation experiments illustrate the effectiveness of the supervised knowledge transfer and the teacher annealing strategy.","2475-1510","","10.1109/TG.2023.3316697","National Key R&D Program of China(grant numbers:2022ZD0116403); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255234","Computer game;cooperation pattern;multi-agent reinforcement learning;multi-task","Task analysis;Multitasking;Reinforcement learning;Training;Knowledge transfer;Games;Video games","","","","","","","IEEE","19 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Image Enhancement Using Adaptive Region- Guided Multi-Step Exposure Fusion Based on Reinforcement Learning","R. Xi; T. Ma; X. Chen; J. Lyu; J. Yang; K. Sun; Y. Zhang","CCTEG Changzhou Research Institute, Tiandi (Changzhou) Automation Company Ltd., Changzhou, China; College of Computer Science and Technology, Xi’an University of Science and Technology, Xi’an, China; CCTEG Changzhou Research Institute, Tiandi (Changzhou) Automation Company Ltd., Changzhou, China; College of Computer Science and Technology, Xi’an University of Science and Technology, Xi’an, China; College of Computer Science and Technology, Xi’an University of Science and Technology, Xi’an, China; CCTEG Changzhou Research Institute, Tiandi (Changzhou) Automation Company Ltd., Changzhou, China; CCTEG Changzhou Research Institute, Tiandi (Changzhou) Automation Company Ltd., Changzhou, China","IEEE Access","4 Apr 2023","2023","11","","31686","31698","In photography, accurate exposure is the key to taking high-quality photos, particularly images with uneven exposure levels. Global exposure operations are usually difficult to effectively strengthen the various regions of the image. To achieve a balanced exposure level in different regions of the image, we propose a novel algorithm inspired by the luminance masking frequently employed by professional photographers. We use reinforcement learning to adaptively generate guiding regions and adjusting parameters as a basis for multi-step exposure fusion, to enhance the overall quality of the image. Firstly, reinforcement learning is employed to automatically segment the single image to be enhanced into multiple sub-images, with corresponding appropriate adjusting parameters for each sub-image generated. Then, the input image is enhanced using the local adjusting parameters, yielding a set of images with varying enhancing degrees. Finally, these images are fused in an exposure process to obtain the final result. Experimental results show that our method not only generates intuitive and interpretable guiding regions, but also its performance is comparable to that of other contemporaneous methods.","2169-3536","","10.1109/ACCESS.2023.3262751","National Key Research and Development Program Topics(grant numbers:2021YFB4000905); National Natural Science Foundation of China(grant numbers:62101432,62102309); Shaanxi Natural Science Fundamental Research Program Project(grant numbers:2022JM-508); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10083088","Deep reinforcement learning;image enhancement;multi-step decision","Image enhancement;Reinforcement learning;Image color analysis;Deep learning;Decision making;Table lookup;Brightness","image enhancement;image fusion;image segmentation;photography;reinforcement learning","balanced exposure level;corresponding appropriate adjusting parameters;exposure process;global exposure operations;high-quality photos;image enhancement;interpretable guiding regions;intuitive guiding regions;local adjusting parameters;multiple sub-images;multistep exposure fusion;reinforcement learning;uneven exposure levels;varying enhancing degrees","","","","45","CCBYNCND","28 Mar 2023","","","IEEE","IEEE Journals"
"A Cooperation Graph Approach for Multiagent Sparse Reward Reinforcement Learning","Q. Fu; T. Qiu; Z. Pu; J. Yi; W. Yuan","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Electronics Technology Group Corporation Information Science Academy of China, Beijing, China","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","1","8","Multiagent reinforcement learning (MARL) can solve complex cooperative tasks. However, the efficiency of existing MARL methods relies heavily on well-defined reward functions. Multiagent tasks with sparse reward feedback are especially challenging not only because of the credit distribution problem, but also due to the low probability of obtaining positive reward feedback. In this paper, we design a graph network called Cooperation Graph (CG). The Cooperation Graph is the combination of two simple bipartite graphs, namely, the Agent Clustering subgraph (ACG) and the Cluster Designating subgraph (CDG). Next, based on this novel graph structure, we propose a Cooperation Graph Multiagent Reinforcement Learning (CG-MARL) algorithm, which can efficiently deal with the sparse reward problem in multiagent tasks. In CG-MARL, agents are directly controlled by the Cooperation Graph. And a policy neural network is trained to manipulate this Cooperation Graph, guiding agents to achieve cooperation in an implicit way. This hierarchical feature of CG-MARL provides space for customized cluster-actions, an extensible interface for introducing fundamental cooperation knowledge. In experiments, CG-MARL shows state-of-the-art performance in sparse reward multiagent benchmarks, including the anti-invasion interception task and the multi-cargo delivery task.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9891991","National Key Research and Development Program of China(grant numbers:2018AAA0102404); National Natural Science Foundation of China(grant numbers:62073323); Chinese Academy of Sciences(grant numbers:XDA27030204,173211KYSB20200002); Science and Technology Development Fund of Macau(grant numbers:0025/2019/AKP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9891991","multiagent system;reinforcement learning;sparse reward","Neural networks;Clustering algorithms;Reinforcement learning;Benchmark testing;Aerospace electronics;Bipartite graph;Behavioral sciences","graph theory;learning (artificial intelligence);multi-agent systems;radio networks","Cooperation Graph approach;Multiagent sparse reward Reinforcement Learning;complex cooperative tasks;MARL methods;well-defined reward functions;multiagent tasks;sparse reward feedback;positive reward feedback;graph network;simple bipartite graphs;Agent Clustering subgraph;Cluster Designating subgraph;novel graph structure;Cooperation Graph Multiagent Reinforcement Learning algorithm;sparse reward problem;fundamental cooperation knowledge;CG-MARL shows;sparse reward multiagent benchmarks","","","","21","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-Based Fast Prediction of Strategies for Security Control","Q. Li; Y. Yu; T. Lin; X. Fu; H. Du; X. Xu","School of Electrical Engineering and Automation Wuhan University, Wuhan, China; Department of Information Technology, Monash University, Melbourne, Australia; School of Electrical Engineering and Automation Wuhan University, Wuhan, China; School of Electrical Engineering and Automation Wuhan University, Wuhan, China; School of Electrical Engineering and Automation Wuhan University, Wuhan, China; Central China Branch of State Grid Corporation of China, Wuhan, China","2021 IEEE 5th Conference on Energy Internet and Energy System Integration (EI2)","25 Feb 2022","2021","","","2737","2742","The hybrid connection of regional power grid via UHV AC / DC and increasing penetration of renewable energy brings a rapid growth of uncertainties. The operation modes of power grid are more changeable and the transient stability problem becomes more serious. In consequence, ensuring the stability of power systems is gaining more attention today than ever before. So, in this paper, aiming at the angle stability problem of large-scale system after UHVDC bipolar block, from the perspective of data-driven, a deep reinforcement learning-based method is adopted to quickly predict strategies of the security control for different power grid operation modes corresponding to the above-mentioned uncertainties. In detail, a Deep Q Network (DQN)-based model is established to formulate the security control prediction problem with the system information before disturbance as input and security control strategies as output. The reasonable design and tuning for the reward function of the agent is the most important for the settings of DQN and by which the effectiveness of security control strategies under different operation modes is guaranteed. Detailed analysis and validation on a simplified regional power grid in China are presented to prove feasibility and performance of the security control strategy prediction method.","","978-1-6654-3425-6","10.1109/EI252483.2021.9713426","National Key R&D Program of China(grant numbers:2017YFB0902600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9713426","security control;transient stability;bipolar block of UHVDC;data-driven;deep reinforcement learning;Deep Q Network;reward function","Renewable energy sources;Uncertainty;System integration;Power system stability;Predictive models;Power grids;Stability analysis","HVDC power transmission;learning (artificial intelligence);power generation faults;power grids;power system stability;power system transient stability","simplified regional power grid;security control strategy prediction method;hybrid connection;renewable energy;transient stability problem;power systems;angle stability problem;large-scale system;UHVDC bipolar block;deep reinforcement learning-based method;different power grid operation modes;above-mentioned uncertainties;Deep Q Network-based model;security control prediction problem;system information;security control strategies","","","","12","IEEE","25 Feb 2022","","","IEEE","IEEE Conferences"
"Population-coded Spiking Neural Network with Reinforcement Learning for Mapless Navigation","R. Xu; Y. Wu; X. Qin; P. Zhao","School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China","2022 International Conference on Cyber-Physical Social Intelligence (ICCSI)","16 Dec 2022","2022","","","518","523","Most of the navigation methods currently applied to mobile robots cost too much in building and maintaining maps. Therefore, it is crucial to implement mapless navigation for mobile robots. Although the recent deep reinforcement learning (DRL) methods have been able to make full use of the on-board resources to explore unknown space, their high energy cost limits their application. The low energy consumption of the spiking neural network (SNN) can help the DRL to overcome this difficulty. In this paper, we combine the SNN with the deep deterministic policy gradient (DDPG) method. To address the problem of long intervals between two adjacent pulses in the integrate-and-fire (LIF) neuron model, we change the way the membrane voltage resets and the dynamics of the neuron during the refractory period. On this basis, the environmental information was encoded using neuron population coding method and the two networks were trained jointly using an extended spatial-temporal backpropagation (STBP) method. The simulation results show that the proposed method achieves a higher success rate in navigation compared to traditional deep learning algorithms.","","978-1-6654-9835-7","10.1109/ICCSI55536.2022.9970598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9970598","mobile robots;navigation;biological neural networks","Deep learning;Costs;Navigation;Neurons;Reinforcement learning;Space exploration;Social intelligence","backpropagation;biocomputing;deep learning (artificial intelligence);encoding;mobile robots;navigation;neural nets;path planning;reinforcement learning","DDPG;deep deterministic policy gradient method;DRL;extended spatial-temporal backpropagation method;high energy cost;integrate-and-fire neuron model;integrate-and-fire neuron model;LIF;low-energy consumption;mapless navigation;membrane voltage resets;mobile robots;navigation methods;neuron population coding method;on-board resources;population-coded spiking neural network;recent deep reinforcement learning methods;SNN;STBP;traditional deep learning algorithms","","","","18","IEEE","16 Dec 2022","","","IEEE","IEEE Conferences"
"Transfer Reinforcement Learning for Dynamic Spectrum Environment","H. Sheng; W. Zhou; J. Zheng; Y. Zhao; W. Ma","School of Automation Engineering, University of Science and Technology, Chengdu, China; School of Automation Engineering, University of Science and Technology, Chengdu, China; School of Automation Engineering, University of Science and Technology, Chengdu, China; Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China; School of Automation Engineering, University of Science and Technology, Chengdu, China","IEEE Transactions on Wireless Communications","","2023","PP","99","1","1","Reinforcement learning (RL) has proven to be an effective approach for achieving intelligence in Cognitive Radio (CR). Through interactions with the environment, RL enables a CR to optimize in an efficient and flexible manner. The vast majority of studies, however, are carried out in a spectrum environment with prefixed user access rules, typically with a constant transition probability and reward distribution. In fact, in a real-world spectrum environment, changes in access rules are common, which has a significant impact on the effectiveness of RL, while few studies have been conducted on this topic. This paper demonstrates how changes in primary user’s (PU) access rules affect RL strategies. To improve the secondary user’s (SU) performance for the dynamic spectrum environment, a transfer Deep Q-Network (DQN) is proposed, this method screens out knowledge from historical experience while avoiding interference from irrelevant information with an experience playback mechanism. Experiments show that this method outperforms traditional RL methods in terms of conflict rate, spectrum utilization rate, and convergence rate in the dynamic spectrum. Given the scarcity of studies on this topic, this study is expected to serve as a benchmark for the future research.","1558-2248","","10.1109/TWC.2023.3289502","The funding from Shenzhen Institute of Artificial Intelligence and Robotics for Society; National Natural Science Foundation of China(grant numbers:61903066); China Postdoctoral Science Foundation(grant numbers:2021M690560); Sichuan Science and Technology Project under Grants(grant numbers:2019ZDZX0045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173764","cognitive radio;dynamic spectrum environment;deep Q-network;transfer learning","Heuristic algorithms;Sensors;Task analysis;Resource management;Electromagnetics;Cognitive radio;White spaces","","","","","","","IEEE","5 Jul 2023","","","IEEE","IEEE Early Access Articles"
"A Bi-level Network-wide Cooperative Driving Approach Including Deep Reinforcement Learning-based Routing","J. Zhang; J. Ge; S. Li; S. Li; L. Li","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Civil Engineering, Tsinghua University, Beijing, China; Department of Automation, BNRist, Tsinghua University, Beijing, China","IEEE Transactions on Intelligent Vehicles","","2023","PP","99","1","17","Cooperative driving of connected and automated vehicles (CAVs) has attracted extensive attention and researchers have proposed various approaches. However, existing approaches are limited to small-scale isolated scenarios and gaps remain in network-wide cooperative driving, especially in routing. In this paper, we decompose the network-level cooperative driving problem into two dominant sub-problems and accordingly propose a bi-level network-wide cooperative driving approach. The dynamic routing problem is considered in the upper level and we propose a multi-agent deep reinforcement learning (DRL) based routing model. The model can promote the equilibrium of network-wide traffic through distributed self-organized routing collaboration among vehicles, thereby improving efficiency for both individual vehicles and global traffic systems. In the lower level, we focus on the right-of-way assignment problem at signal-free intersections and propose an adaptive cooperative driving algorithm. The algorithm can adaptively evaluate priorities of different lanes, and then uses the lane priorities to guide the Monte Carlo tree search (MCTS) for better right-of-way assignments. Essentially, the upper level determines which conflict areas the vehicles will pass through, and the lower level addresses how the vehicles use the limited road resources more efficiently in each conflict area. The experimental results show that the upper and lower levels complement each other and work together to significantly improve the network-wide traffic efficiency and reduce the travel time of individual vehicles. Moreover, the results demonstrate that microscopic and mesoscopic cooperative driving behaviors of vehicles can significantly benefit the macroscopic traffic system.","2379-8904","","10.1109/TIV.2023.3305818","National Key Research and Development Program of China(grant numbers:2020AAA0108104); National Natural Science Foundation of China(grant numbers:52272420); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10221733","Connected and automated vehicles (CAVs);Network-wide cooperative driving;Routing;Deep reinforcement learning (DRL);Intersection management","Routing;Vehicle dynamics;Heuristic algorithms;Roads;Microscopy;Intelligent vehicles;Real-time systems","","","","","","","IEEE","16 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Task-Driven Reinforcement Learning With Action Primitives for Long-Horizon Manipulation Skills","H. Wang; H. Zhang; L. Li; Z. Kan; Y. Song","Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; School of Automation, Chongqing University, Chongqing, China","IEEE Transactions on Cybernetics","","2023","PP","99","1","14","It is an interesting open problem to enable robots to efficiently and effectively learn long-horizon manipulation skills. Motivated to augment robot learning via more effective exploration, this work develops task-driven reinforcement learning with action primitives (TRAPs), a new manipulation skill learning framework that augments standard reinforcement learning algorithms with formal methods and parameterized action space (PAS). In particular, TRAPs uses linear temporal logic (LTL) to specify complex manipulation skills. LTL progression, a semantics-preserving rewriting operation, is then used to decompose the training task at an abstract level, informs the robot about their current task progress, and guides them via reward functions. The PAS, a predefined library of heterogeneous action primitives, further improves the efficiency of robot exploration. We highlight that TRAPs augments the learning of manipulation skills in both learning efficiency and effectiveness (i.e., task constraints). Extensive empirical studies demonstrate that TRAPs outperforms most existing methods.Sign","2168-2275","","10.1109/TCYB.2023.3298195","National Key Research and Development Program of China(grant numbers:2022YFB4701400/4701403); National Natural Science Foundation of China(grant numbers:62173314); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10215053","Action primitives;linear temporal logic (LTL);long-horizon manipulation skills;task-driven RL","Task analysis;Reinforcement learning;Robot learning;Planning;Transformers;Training;Libraries","","","","","","","IEEE","11 Aug 2023","","","IEEE","IEEE Early Access Articles"
"A Deep Reinforcement Learning Approach Combined With Model-Based Paradigms for Multiagent Formation Control With Collision Avoidance","Z. Pu; T. Zhang; X. Ai; T. Qiu; J. Yi","Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","15 Jun 2023","2023","53","7","4189","4204","Generating collision-free formation control strategy for multiagent systems faces huge challenges in collaborative navigation tasks, especially in a highly dynamic and uncertain environment. Two typical methodologies for solving this problem are the conventional model-based paradigm and the data-driven paradigm, particularly the widely used deep reinforcement learning (DRL) method. However, both the model-based and data-driven paradigms encounter inherent drawbacks. In this paper, we present two novel general schemes that combine these two paradigms together in an online mode. Specifically, the two paradigms are combined in a parallel and a serial structure in these two schemes, respectively. In the parallel scheme, the outputs of the model-based and DRL-based controllers are lumped together. In the serial scheme, the output of the model-based controller is fed as an input of the DRL-based controller. The interpretation of the two combined schemes is suggested from a control-oriented perspective, where the parallel DRL controller is viewed as a complementary uncertainty compensator and the serial DRL controller is taken as an inverse dynamics estimator. Finally, comprehensive simulations are conducted to demonstrate the superiority of the proposed schemes, and the effectiveness is further verified by deploying our schemes to a physical experiment platform based on a set of three-wheeled omnidirectional robots.","2168-2232","","10.1109/TSMC.2023.3241337","National Key Research Development Program of China(grant numbers:2020AAA0103404); National Natural Science Foundation of China(grant numbers:62073323); Beijing Nova Program(grant numbers:20220484077); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDA27030204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10047984","Collision avoidance;combined model-based and data-driven;deep reinforcement learning (DRL);formation control;multiagent system (MAS)","Collision avoidance;Task analysis;Navigation;Maintenance engineering;Adaptation models;Shape;Predictive models","collision avoidance;deep learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;path planning;reinforcement learning","approach combined;collaborative navigation tasks;collision avoidance;combined schemes;control-oriented perspective;conventional model-based paradigm;data-driven paradigm;deep reinforcement;DRL-based controller;general schemes;generating collision-free formation control strategy;highly dynamic environment;model-based controller;model-based paradigms;multiagent formation control;multiagent systems;parallel DRL controller;parallel scheme;serial DRL controller;serial scheme;uncertain environment","","","","39","IEEE","17 Feb 2023","","","IEEE","IEEE Journals"
"SIM: A Scenario IMagination Based Deep Reinforcement Learning Method for Outdoor Transportation Environment Exploration","H. Li; Q. Zhang; Y. Chen; D. Zhao","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, P. R. China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, P. R. China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, P. R. China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, P. R. China","2022 IEEE 11th Data Driven Control and Learning Systems Conference (DDCLS)","26 Aug 2022","2022","","","636","642","Autonomous exploration is very important for robotics, especially for mapping, navigation, and planning in an unknown environment. In recent years, automatic exploration methods in the indoor environment have been extensively studied, but there is little research on exploration in the outdoor transportation environment. Due to the limitations of outdoor traffic rules and the scale of the environment, the methods for the indoor environment are difficult to apply to the transportation environment. Aiming at exploration in the transportation environment, this paper proposes a deep reinforcement learning algorithm based on Scenario IMagination(SIM), which has two important components: 1) a mid-level action space, which combines the classical robot control algorithm, addressing the inefficient learning and unstable navigation of deep reinforcement learning algorithms in automatic exploration. With this action space, the deep reinforcement learning algorithm achieves excellent exploration performance in both normal scale environments and large-scale branchless environments; 2) a scenarios buffer, which relieves hard exploration problems of deep reinforcement learning due to serious imbalances of samples in large-scale multibranch scenarios. Compared to the map-less navigation approaches, SIM achieves excellent exploration performance in large-scale multi-branch environments.","2767-9861","978-1-6654-9675-9","10.1109/DDCLS55054.2022.9858473","National Natural Science Foundation of China(grant numbers:62103409); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858473","","Space vehicles;Training;Navigation;Robot control;Transportation;Reinforcement learning;Aerospace electronics","control engineering computing;deep learning (artificial intelligence);navigation;path planning;reinforcement learning;road traffic control;robots;traffic engineering computing;transportation","deep reinforcement learning algorithm;classical robot control algorithm;inefficient learning;excellent exploration performance;normal scale environments;hard exploration problems;deep reinforcement learning method;outdoor transportation environment exploration;autonomous exploration;automatic exploration methods;indoor environment;outdoor traffic rules;large scale multibranch environments;large scale multibranch scenarios;large scale branchless environments","","","","29","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"Data-Driven MPC for Nonlinear Systems with Reinforcement Learning","Y. Li; Q. Wang; Z. Sun; Y. Xia","School of Automation, Beijing Institute of Technology, Beijing, P. R. China; School of Automation, Beijing Institute of Technology, Beijing, P. R. China; School of Automation, Beijing Institute of Technology, Beijing, P. R. China; School of Automation, Beijing Institute of Technology, Beijing, P. R. China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","2404","2409","Inspired by Willems and the co-authors' idea that continuously excited system trajectories can be used to represent the input-output behavior of discrete-time linear time-invariant (DT LTI) systems. We extend this idea to nonlinear systems. In this paper, we propose a data-driven model predictive control (MPC) scheme with reinforcement learning (RL) for unknown nonlinear systems. We utilize the input-output data of the system to form Hankel matrices to represent the system model implicitly. The accuracy of the prediction is improved by updating the data online. Another core idea of this scheme is to combine the standard MPC with RL to approximate the terminal cost function by TD-learning to ensure the closed-loop stability of the system. Simulation experiments on the cart-damper-spring system were used to demonstrate the feasibility of the proposed algorithm.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902257","National Natural Science Foundation (NNSF) of China(grant numbers:62003040); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902257","Model predictive control (MPC);reinforcement learning (RL);data-driven method;nonlinear systems","Reinforcement learning;Prediction algorithms;Cost function;Data models;Behavioral sciences;Trajectory;Steady-state","closed loop systems;control system synthesis;discrete time systems;Hankel matrices;linear systems;nonlinear control systems;predictive control;reinforcement learning;springs (mechanical);stability","cart-damper-spring system;controller design;data-driven model predictive control scheme;data-driven MPC;discrete-time linear time-invariant systems;DT LTI;Hankel matrices;input-output behavior;input-output data;reinforcement learning;RL;standard MPC;unknown nonlinear systems","","","","20","","11 Oct 2022","","","IEEE","IEEE Conferences"
"A Hierarchical Deep Reinforcement Learning Framework for 6-DOF UCAV Air-to-Air Combat","J. Chai; W. Chen; Y. Zhu; Z. -X. Yao; D. Zhao","State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Unmanned Aerial Vehicle, Shenyang Aircraft Design and Research Institute, Shenyang, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","17 Aug 2023","2023","53","9","5417","5429","Unmanned combat air vehicle (UCAV) combat is a challenging scenario with high-dimensional continuous state and action space and highly nonlinear dynamics. In this article, we propose a general hierarchical framework to resolve the within-vision-range (WVR) air-to-air combat problem under six dimensions of degree (6-DOF) dynamics. The core idea is to divide the whole decision-making process into two loops and use reinforcement learning (RL) to solve them separately. The outer loop uses a combat policy to decide the macro command according to the current combat situation. Then the inner loop uses a control policy to answer the macro command by calculating the actual input signals for the aircraft. We design the Markov decision-making process for the control policy and the Markov game between two aircraft. We present a two-stage training mechanism. For the control policy, we design an effective reward function to accurately track various macro behaviors. For the combat policy, we present a fictitious self-play mechanism to improve the combat performance by combating against the historical combat policies. Experiment results show that the control policy can achieve better tracking performance than conventional methods. The fictitious self-play mechanism can learn competitive combat policy, which can achieve high winning rates against conventional methods.","2168-2232","","10.1109/TSMC.2023.3270444","National Key Research and Development Program of China(grant numbers:2018AAA0101005); National Natural Science Foundation of China(grant numbers:62293541); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDA27030400); Youth Innovation Promotion Association CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10120732","6-DOF unmanned combat air vehicle (UCAV);air combat;hierarchical structure;reinforcement learning (RL);self-play","Aircraft;Aerospace control;6-DOF;Task analysis;Nose;Missiles;Heuristic algorithms","autonomous aerial vehicles;decision making;deep learning (artificial intelligence);learning (artificial intelligence);Markov processes;military aircraft;reinforcement learning","6-DOF;action space;combat performance;competitive combat policy;control policy;current combat situation;degree dynamics;fictitious self-play mechanism;general hierarchical framework;hierarchical deep reinforcement learning framework;high-dimensional continuous state;highly nonlinear dynamics;historical combat policies;inner loop;macro command;Markov decision-making process;outer loop;UCAV air-to-air combat;unmanned combat air vehicle combat;use reinforcement learning;within-vision-range air-to-air combat problem","","","","48","IEEE","8 May 2023","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Security-Constrained Battery Scheduling in Home Energy System","B. Wang; Z. Zha; L. Zhang; L. Liu; H. Fan","School of Artificial Intelligence and Automation, Key Laboratory of Ministry of Education for Image Processing and Intelligent Control, Huazhong University of Science and Technology, China; School of Artificial Intelligence and Automation, Key Laboratory of Ministry of Education for Image Processing and Intelligent Control, Huazhong University of Science and Technology, China; School of Artificial Intelligence and Automation, Key Laboratory of Ministry of Education for Image Processing and Intelligent Control, Huazhong University of Science and Technology, China; School of Artificial Intelligence and Automation, Key Laboratory of Ministry of Education for Image Processing and Intelligent Control, Huazhong University of Science and Technology, China; School of Artificial Intelligence and Automation, Key Laboratory of Ministry of Education for Image Processing and Intelligent Control, Huazhong University of Science and Technology, China","IEEE Transactions on Consumer Electronics","","2023","PP","99","1","1","The home energy system today involves multiple renewable energy sources and battery energy storage systems, which can be considered as a microgrid. The battery energy storage system is a key component in the home energy system for the sake of filling the gap between the user demand and volatile energy supplies to maximize the techno and economic performances. However, the battery scheduling must suffer the stochastic nature of renewable energy resources and loads, which results in an intractable multi-period stochastic optimization problem with security constraints. An improved actor-critic-based reinforcement learning is proposed for this issue, where a distributional critic net is applied for faster and more accurate reward estimation under uncertainties, and a policy net incorporating protective secondary control is adopted to satisfy security constraints, preventing the unsafe state of batteries during the trial-and-error process. Numerical tests show that the proposed approach outperforms conventional reinforcement learning algorithms, as well as the rule-based battery scheduling approach while guaranteeing safe operation. The robustness and adaptability of the proposed method are also verified in case studies with different optimization tasks.","1558-4127","","10.1109/TCE.2023.3279869","National Key R and D Program of China(grant numbers:2022YFE0198700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10142173","Home energy system;energy storage;renewable energy resources;security-constrained battery scheduling;reinforcement learning","Batteries;State of charge;Security;Degradation;Trajectory;Q-learning;Monte Carlo methods","","","","","","","IEEE","1 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Distribution Network Topology Control Using Attention Mechanism-Based Deep Reinforcement Learning","Z. Qiu; Y. Zhao; W. Shi; F. Su; Z. Zhu","School of Automation Central South University, ChangSha, China; School of Automation Central South University, ChangSha, China; State Grid (Suzhou) City & Energy Research Institute Co.,Ltd, JiangSu, China; School of Automation Central South University, ChangSha, China; School of Automation Central South University, ChangSha, China","2022 4th International Conference on Electrical Engineering and Control Technologies (CEECT)","7 Feb 2023","2022","","","55","60","As the distributed energy mainly based on wind and solar energy continues to be incorporated into the power grid, its automatic control and management has become a very complicated task, and it needs to seek more intelligent control technology. In this paper, a deep reinforcement learning method SAC (Soft Actor-Critic) combined with attention mechanism is proposed to manage power grid. This method changes the line connection and bus distribution of the substation by adjusting the topology structure of the power grid, so that it can transmit power efficiently. And by assigning different feature weights, the attention mechanism enables the neural network to focus on the input that is more relevant to the current target task from a large number of grid input feature states, which enhances the robustness and computational efficiency of the model. And Experiments have proved that our algorithm can automatically manage three different size distribution networks IEEE-5, IEEE-14 and L2RPN WCCI 2020 for three days without experts' help and make sure them run properly and safely.","","978-1-6654-9899-9","10.1109/CEECT55960.2022.10030642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10030642","Deep reinforcement learning;Attention mechanism;Power grid topology control","Deep learning;Network topology;Heuristic algorithms;Power system dynamics;Reinforcement learning;Distribution networks;Power grids","deep learning (artificial intelligence);intelligent control;learning (artificial intelligence);neural nets;power engineering computing;power grids;reinforcement learning;telecommunication network topology","attention mechanism-based deep reinforcement;automatic control;bus distribution;complicated task;current target task;deep reinforcement learning method SAC;different feature weights;different size distribution networks IEEE-5;distributed energy;distribution network topology control;grid input feature states;intelligent control technology;line connection;neural network;power grid;Soft Actor-Critic;solar energy;topology structure","","","","22","IEEE","7 Feb 2023","","","IEEE","IEEE Conferences"
"Domain Knowledge-Assisted Deep Reinforcement Learning Power Allocation for MIMO Radar Detection","Y. Wang; Y. Liang; H. Zhang; Y. Gu","School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China","IEEE Sensors Journal","30 Nov 2022","2022","22","23","23117","23128","The power allocation of multiple-input multiple-output (MIMO) radars is a key point in target tracking and detection. The optimality of a multitarget and multiconstraint optimization problem strictly depends on a priori model, which is difficult to obtain in time-varying, complex, and noncooperative environments. Recently, deep reinforcement learning (DRL) has been applied for target tracking tasks, which provides a trial-and-error interactive learning mechanism to improve the policy. Unlike tracking tasks with complete target state transition models, it remains an open issue for DRL-based MIMO radar detection that requires efficiently adapting the control policy to the environment of randomly appearing targets and extensive power transmission actions, which leads to sparse final task rewards and hence slow policy learning for the agent. Through introducing both the analytic model (radar equation) and empirical rules (expert preferences) for domain knowledge, this article proposes a domain-knowledge-assisted DRL (DKADRL) framework in which a domain-knowledge-based timely reward generator is utilized to generate timely rewards that assist the agent’s policy learning. To adjust the role of the timely rewards and the final task rewards, a reward fusion module is designed, which gradually increases the role of the final task rewards as the training process progresses, thus allows agent’s policy to converge to the final optimization goal. The algorithm is validated under two target motion scenarios, showing the higher target detection probability and the faster training speed, compared to equal power allocation and proximal policy optimization (PPO)-based power allocation.","1558-1748","","10.1109/JSEN.2022.3211606","National Natural Science Foundation of China(grant numbers:61771399,61801386,61873205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9915319","Domain knowledge assistance;multiple radar system;reinforcement learning (RL);resource allocation","Manganese;Radar;Interference;Resource management;Task analysis;Signal to noise ratio;Radar detection","deep learning (artificial intelligence);knowledge representation;MIMO radar;object detection;optimisation;radar detection;reinforcement learning;target tracking","deep reinforcement learning;DKADRL framework;domain knowledge-assisted deep reinforcement learning power allocation;domain-knowledge-assisted DRL framework;DRL-based MIMO radar detection;MIMO radar detection;multiconstraint optimization problem;multiple-input multiple-output radar power allocation;multitarget optimization problem;policy learning;PPO-based power allocation;priori model;proximal policy optimization-based power allocation;target detection;target detection probability;target motion scenarios;target state transition models;target tracking;trial-and-error interactive learning mechanism","","","","31","IEEE","10 Oct 2022","","","IEEE","IEEE Journals"
"Autonomous Motion Decision-making based on Deep Reinforcement Learning for Autonomous Driving","J. Hu; H. Kong; T. Liu; Y. Meng","School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China","2022 6th CAA International Conference on Vehicular Control and Intelligence (CVCI)","8 Dec 2022","2022","","","1","6","Motion decision-making is an open-challenging issue for autonomous driving, especially in the complex and diverse environment. A motion decision-making model based on deep reinforcement learning (DRL) is proposed in this work. To optimize the driving policy, a multi-objective reward function is designed to guide the autonomous driving system to explore optimal decision policy with the goal of safety, efficiency, and smoothness. A convolutional neural network (CNN) is designed as the backbone network of the DRL model to make full use of the observation information of environment. To enhance safety, a safety check module is designed for avoiding potential unsafe actions. A series of contrast experiments are conducted to justify the model performance of the proposed method on the DeepTraffic simulation environment. The experimental results demonstrate the effectiveness of the proposed DRL-based model in terms of the safe, efficient and smooth motion decision-making, and the generalization ability in different traffic conditions.","","978-1-6654-5374-5","10.1109/CVCI56766.2022.9964721","Hefei University of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964721","Autonomous driving;Motion decision;Reinforcement learning;Safety enhancement","Deep learning;Decision making;Reinforcement learning;Safety;Convolutional neural networks;Autonomous vehicles","control engineering computing;convolutional neural nets;decision making;deep learning (artificial intelligence);mobile robots;motion control;reinforcement learning;road safety;road traffic control;traffic engineering computing","autonomous driving system;autonomous motion decision-making;backbone network;CNN;convolutional neural network;deep reinforcement learning;DeepTraffic simulation environment;driving policy;DRL;multiobjective reward function;optimal decision policy;safety check module","","","","18","IEEE","8 Dec 2022","","","IEEE","IEEE Conferences"
"Multiexperience-Assisted Efficient Multiagent Reinforcement Learning","T. Zhang; Z. Liu; J. Yi; S. Wu; Z. Pu; Y. Zhao","Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Information Science Academy, China Electronics Technology Group Corporation, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","15","Recently, multiagent reinforcement learning (MARL) has shown great potential for learning cooperative policies in multiagent systems (MASs). However, a noticeable drawback of current MARL is the low sample efficiency, which causes a huge amount of interactions with environment. Such amount of interactions greatly hinders the real-world application of MARL. Fortunately, effectively incorporating experience knowledge can assist MARL to quickly find effective solutions, which can significantly alleviate the drawback. In this article, a novel multiexperience-assisted reinforcement learning (MEARL) method is proposed to improve the learning efficiency of MASs. Specifically, monotonicity-constrained reward shaping is innovatively designed using expert experience to provide additional individual rewards to guide multiagent learning efficiently, with the invariance guarantee of the team optimization objective. Furthermore, a reward distribution estimator is specially developed to model an implicated reward distribution of environment by using transition experience from environment, containing collected samples (state–action pair, reward, and next state). This estimator can predict the expectation reward of each agent for the taken action to accurately estimate the state value function and accelerate its convergence. Besides, the performance of MEARL is evaluated on two multiagent environment platforms: our designed unmanned aerial vehicle combat (UAV-C) and StarCraft II Micromanagement (SCII-M). Simulation results demonstrate that the proposed MEARL can greatly improve the learning efficiency and performance of MASs and is superior to the state-of-the-art methods in multiagent tasks.","2162-2388","","10.1109/TNNLS.2023.3264275","National Key Research and Development Program of China(grant numbers:2018AAA0102402); National Natural Science Foundation of China(grant numbers:62073323); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDA27030204); Science and Technology Development Fund of Macau(grant numbers:0025/2019/AKP); Beijing Nova Program(grant numbers:20220484077); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10098712","Experience assist;multiagent reinforcement learning (MARL);reward distribution estimation;reward shaping","Optimization;Training;Reinforcement learning;Shape;Task analysis;Convergence;Multi-agent systems","","","","","","","IEEE","10 Apr 2023","","","IEEE","IEEE Early Access Articles"
"SEA: A Spatially Explicit Architecture for Multi-Agent Reinforcement Learning","D. Li; Z. Xu; B. Zhang; G. Fan","Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China","2023 International Joint Conference on Neural Networks (IJCNN)","2 Aug 2023","2023","","","1","8","Spatial information is essential in various fields. How to explicitly model according to the spatial location of agents is also very important for the multi-agent problem, especially when the number of agents is changing and the scale is enormous. Inspired by the point cloud task in computer vision, we propose a spatial information extraction structure for multi-agent reinforcement learning in this paper. Agents can effectively share the neighborhood and global information through a spatially encoder-decoder structure. Our method follows the centralized training with decentralized execution (CTDE) paradigm. In addition, our structure can be applied to various existing mainstream reinforcement learning algorithms with minor modifications and can deal with the problem with a variable number of agents. The experiments in several multi-agent scenarios show that the existing methods can get convincing results by adding our spatially explicit architecture.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10191819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191819","","Training;Point cloud compression;Computer vision;Computational modeling;Neural networks;Reinforcement learning;Computer architecture","cloud computing;computer vision;learning (artificial intelligence);multi-agent systems;reinforcement learning","existing mainstream reinforcement;global information;multiagent problem;multiagent reinforcement learning;multiagent scenarios;spatial information extraction structure;spatial location;spatially encoder-decoder structure;spatially explicit architecture","","","","28","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Understanding via Exploration: Discovery of Interpretable Features With Deep Reinforcement Learning","J. Wei; Z. Qiu; F. Wang; W. Lin; N. Gui; W. Gui","School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Information Science and Technology, Zhejiang Sci-Tech University, Hangzhou, China; School of Automation, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","12","Understanding the environments through interactions has been one of the most important human intellectual activities in mastering unknown systems. Deep reinforcement learning (DRL) has already been known to achieve effective control through human-like exploration and exploitation in many applications. However, the opaque nature of deep neural network (DNN) often hides critical information about feature relevance to control, which is essential for understanding the target systems. In this article, a novel online feature selection framework, namely, the dual-world-based attentive feature selection (D-AFS), is first proposed to identify the contribution of the inputs over the whole control process. Rather than the one world used in most DRL, D-AFS has both the real world and its virtual peer with twisted features. The newly introduced attention-based evaluation (AR) module performs the dynamic mapping from the real world to the virtual world. The existing DRL algorithms, with slight modification, can learn in the dual world. By analyzing the DRL’s response in the two worlds, D-AFS can quantitatively identify respective features’ importance toward control. A set of experiments is performed on four classical control systems in OpenAI Gym. Results show that D-AFS can generate the same or even better feature combinations than the solutions provided by human experts and seven recent feature selection baselines. In all cases, the selected feature representations are closely correlated with the ones used by underlying system dynamic models.","2162-2388","","10.1109/TNNLS.2022.3184956","National Natural Science Foundation of China(grant numbers:61772473,62073345,61988101,62011530148); Hangzhou High-Flyer AI Fundamental Research Company Ltd.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810174","Attention mechanism;deep reinforcement learning (DRL);dual-world mechanism;feature selection","Feature extraction;Reinforcement learning;Control systems;Process control;Temperature measurement;Neural networks;Deep learning","","","","","","","IEEE","28 Jun 2022","","","IEEE","IEEE Early Access Articles"
"Active Distribution Network Reconfiguration with Renewable Energy Based on Multi-agent Deep Reinforcement Learning","Z. Lin; C. Jiang; Y. Lu; C. Liu","College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China","2023 6th International Conference on Energy, Electrical and Power Engineering (CEEPE)","7 Jul 2023","2023","","","535","542","Distributed generation (DG) represented by wind turbines and photovoltaic systems has been extensively connected to the power distribution network (DN). However, the random fluctuations of DG pose new challenges to the safety, stability, and economic performance of DN, while distribution network reconfiguration (DNR) technology can alleviate this problem to some extent. Traditional heuristic algorithms are difficult to deal with uncertainties in the source-load and the increasing complexity of DN. Therefore, this paper proposes an active DNR method based on a model-free multi-agent deep deterministic policy gradient reinforcement learning framework (MADDPG). Firstly, the number of fundamental loops in the distribution network are determined and agent for each fundamental loop are deployed. Each agent has an actor and a critic network, which can control operations of the branch switches in the loop. Next, a mathematical model of DNR will be constructed. Then, a MADDPG training framework for distribution network reconfiguration is built, which adopts centralized training and distributed execution. Finally, the simulation cases are performed on an improved IEEE 33-bus power system to prove the effectiveness of MADDPG algorithm. The results illustrate that MADDPG algorithm can improve the economic and stability performance of the distribution network to some extent, demonstrating the effectiveness of the proposed approach.","","979-8-3503-4827-9","10.1109/CEEPE58418.2023.10166046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10166046","Deep deterministic policy gradient;distribution network reconfiguration;active distribution network;multi-agent deep reinforcement learning","Training;Economics;Uncertainty;Distribution networks;Reinforcement learning;Power system stability;Prediction algorithms","deep learning (artificial intelligence);distributed power generation;electric vehicle charging;gradient methods;multi-agent systems;power distribution economics;power engineering computing;reinforcement learning;wind turbines","active distribution network reconfiguration;active DNR method;critic network;deep deterministic policy gradient reinforcement learning framework;DG;distributed execution;distributed generation;distribution network reconfiguration technology;economic performance;economic stability;fundamental loop;improved IEEE 33-bus power system;MADDPG algorithm;MADDPG training framework;model-free multiagent;multiagent deep reinforcement learning;photovoltaic systems;power distribution network;renewable energy;traditional heuristic algorithms;wind turbines","","","","30","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"Informative Trajectory Planning Using Reinforcement Learning for Minimum-Time Exploration of Spatiotemporal Fields","Z. Li; K. You; J. Sun; G. Wang","School of Automation, National Key Laboratory of Autonomous Intelligent Unmanned Systems, Beijing Institute of Technology, Beijing, China; Department of Automation and BNRist, Tsinghua University, Beijing, China; School of Automation, National Key Laboratory of Autonomous Intelligent Unmanned Systems, Beijing Institute of Technology, Beijing, China; School of Automation, National Key Laboratory of Autonomous Intelligent Unmanned Systems, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","11","This article studies the informative trajectory planning problem of an autonomous vehicle for field exploration. In contrast to existing works concerned with maximizing the amount of information about spatial fields, this work considers efficient exploration of spatiotemporal fields with unknown distributions and seeks minimum-time trajectories of the vehicle while respecting a cumulative information constraint. In this work, upon adopting the observability constant as an information measure for expressing the cumulative information constraint, the existence of a minimum-time trajectory is proven under mild conditions. Given the spatiotemporal nature, the problem is modeled as a Markov decision process (MDP), for which a reinforcement learning (RL) algorithm is proposed to learn a continuous planning policy. To accelerate the policy learning, we design a new reward function by leveraging field approximations, which is demonstrated to yield dense rewards. Simulations show that the learned policy can steer the vehicle to achieve an efficient exploration, and it outperforms the commonly-used coverage planning method in terms of exploration time for sufficient cumulative information.","2162-2388","","10.1109/TNNLS.2023.3300926","National Natural Science Foundation of China(grant numbers:62033006,61925303,62173034,62088101,U20B2073); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10219144","Autonomous vehicles;optimal control;reinforcement learning (RL);trajectory optimization","Trajectory;Spatiotemporal phenomena;Trajectory planning;Planning;Observability;Autonomous vehicles;Vehicle dynamics","","","","","","","IEEE","15 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Volt/Var Control Method Based on Agents Group Using Reinforcement Learning","S. Fan; X. Liu; Y. Wang; S. Wang","Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology, China Electric Power Research Institute, Beijing, China; Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology, China Electric Power Research Institute, Beijing, China; School of Electrical Engineering and Automation, Harbin Institute of Technology, Harbin, China; School of Electrical Engineering and Automation, Harbin Institute of Technology, Harbin, China","2020 Asia Energy and Electrical Engineering Symposium (AEEES)","19 Jun 2020","2020","","","638","646","Voltage and Var control is an important control measure to ensure voltage stability in the grid area. The existing single agent design method based on reinforcement learning has many problems, such as high coupling between state and action, various combinations of reactive power compensation devices, and unreasonable reward design based on target deviation model. In order to solve these problems, a novel method for describing the integrated operation state of the grid considering the node voltage amplitude and capacitor switching condition is proposed. The reinforcement learning agent group architecture for voltage and Var control is designed. The group determines the members of the agent according to the current comprehensive operating state of the grid, and gives corresponding reactive power regulation actions. Each agent member uses the improvement degree of the grid state in the adjacent time period as a reward mechanism. The example shows that the method can be applied to the grid level voltage and Var control environment. Compared with the single agent design method, the number of action sets can be effectively reduced, and various voltage and Var control conditions can be better dealt with.","","978-1-7281-6782-4","10.1109/AEEES48850.2020.9121380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9121380","reactive power and voltage;reinforcement learning;artificial intelligence;agent group","Reactive power;Voltage measurement;Design methodology;Capacitors;Stability criteria;Reinforcement learning;Switches","capacitor switching;learning (artificial intelligence);multi-agent systems;power engineering computing;power grids;power system stability;reactive power control;voltage control;voltage regulators","capacitor switching condition;reactive power regulation;grid state;grid level voltage;Var control environment;voltage stability;grid area;reactive power compensation devices;target deviation model;integrated operation state;node voltage amplitude;agent group architecture;voltage control","","","","20","IEEE","19 Jun 2020","","","IEEE","IEEE Conferences"
"Heterogeneous-graph Attention Reinforcement Learning for Football Matches","S. Wang; Y. Pan; Z. Pu; J. Yi; Y. Liang; D. Zhang","School of Artificial Intelligence, UCAS Institute of Automation, CAS, Beijing, China; CAS, Institute of Automation, Beijing, China; School of Artificial Intelligence, UCAS Institute of Automation, CAS, Beijing, China; School of Artificial Intelligence, UCAS Institute of Automation, CAS, Beijing, China; Macau University of Science and Technology, Macau, China; Macau University of Science and Technology, Macau, China","2023 International Joint Conference on Neural Networks (IJCNN)","2 Aug 2023","2023","","","1","7","Football player's decision-making problem is quite challenging because of the essential feature of football matches: many players with different and complex cooperative or competitive relationships. To better leverage these relationships, this paper proposes a player-policy learning method with heterogeneous-graph attention reinforcement learning (PPL-HGARL) to enable an active player closest to the ball to learn effective policies for playing football matches. Specifically, a multi-head feature representation module is designed to reconstruct raw observations using prior expert knowledge. Furthermore, a heterogeneous-graph player-relation attention network is designed by use of graph among different roles, in order to model cooperative and competitive relations among players. The network learns proper state representation for the active player, making the player pay attention to other important players. Besides, an actor-critic algorithm is adopted to train the policies efficiently. Competing with rule-based opponents of different difficulty levels in the Google Research Football environment, the active player achieves excellent results, which validates the effectiveness and superiority of the proposed method.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10191648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191648","football;graph attention network;deep rein-forcement learning","Learning systems;Simulation;Neural networks;Decision making;Reinforcement learning;Feature extraction;Internet","decision making;graph theory;reinforcement learning;sport","active player;actor-critic algorithm;competitive relationships;complex cooperative relationships;decision-making problem;football matches;football player;Google Research Football environment;heterogeneous-graph attention reinforcement learning;heterogeneous-graph player-relation attention network;important players;multihead feature representation module;player pay attention;player-policy learning method;rule-based opponents","","","","27","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"A Date-Driven Voltage Control Strategy for Distribution Network Using Deep Reinforcement Learning","J. Shen; J. Han; Y. Wang; Y. Dong; H. Li","School of Automation, Nanjing University of Science&Technology, Nanjing, China; School of Automation, Nanjing University of Science&Technology, Nanjing, China; School of Automation, Nanjing University of Science&Technology, Nanjing, China; School of Automation, Nanjing University of Science&Technology, Nanjing, China; State Grid Nanjing Power Supply Company, Nanjing, China","2023 IEEE 6th International Electrical and Energy Conference (CIEEC)","10 Jul 2023","2023","","","77","82","A data-driven voltage control strategy is proposed for real-time optimized distributed voltage control of distribution networks connected to the energy storage system (ESS) and distributed photovoltaics (PVs) with high penetration. Deep reinforcement learning (DRL) methods are used in the control of voltage in the distribution grid. Firstly, based on the division of distributed networks, the decision-making process of combined distributed voltage control is given, which is mainly based on the reactive power compensation of photovoltaic inverters and supplemented by ESS active power regulation. A multi-agent deep reinforcement learning (MADRL) model is then applied to the distributed voltage control model. The Multi-Agent Twin Delay Deep Deterministic Policy Gradient (MATD3) algorithm is then applied. Finally, offline training and online testing are performed on a 33-bus test network with a branch structure to demonstrate the effectiveness of the proposed method.","","979-8-3503-4667-1","10.1109/CIEEC58067.2023.10167323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10167323","data-driven method;distribution network;distributed voltage control;MATD3","Deep learning;Photovoltaic systems;Training;Reinforcement learning;Distribution networks;Real-time systems;Mathematical models","battery storage plants;decision making;deep learning (artificial intelligence);gradient methods;multi-agent systems;optimisation;photovoltaic power systems;power distribution control;power grids;reactive power;reinforcement learning;voltage control","33-bus test network;data-driven voltage control strategy;date-driven voltage control strategy;distributed photovoltaics;distribution network;energy storage system;ESS active power regulation;multiagent deep reinforcement learning model;MultiAgent Twin Delay Deep Deterministic Policy Gradient algorithm;reactive power compensation;real-time optimized distributed voltage control","","","","15","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"Causal Mean Field Multi-Agent Reinforcement Learning","H. Ma; Z. Pu; Y. Pan; B. Liu; J. Gao; Z. Guo","School of Nanjing, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China","2023 International Joint Conference on Neural Networks (IJCNN)","2 Aug 2023","2023","","","1","8","Scalability remains a challenge in multi-agent reinforcement learning and is currently under active research. A framework named mean-field reinforcement learning (MFRL) could alleviate the scalability problem by employing the Mean Field Theory to turn a many-agent problem into a two-agent problem. However, this framework lacks the ability to identify essential interactions under nonstationary environments. Causality contains relatively invariant mechanisms behind interactions, though environments are nonstationary. Therefore, we propose an algorithm called causal mean-field Q-learning (CMFQ) to address the scalability problem. CMFQ is ever more robust toward the change of the number of agents though inheriting the compressed representation of MFRL's action-state space. Firstly, we model the causality behind the decision-making process of MFRL into a structural causal model (SCM). Then the essential degree of each interaction is quantified via intervening on the SCM. Furthermore, we design the causality-aware compact representation for behavioral information of agents as the weighted sum of all behavioral information according to their causal effects. We test CMFQ in a mixed cooperative-competitive game and a cooperative game. The result shows that our method has excellent scalability performance in both training in environments containing a large number of agents and testing in environments containing much more agents.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10191654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191654","","Training;Q-learning;Scalability;Neural networks;Decision making;Games;Behavioral sciences","decision making;game theory;multi-agent systems;reinforcement learning","behavioral information;causal mean field multiagent reinforcement learning;causality-aware compact representation;CMFQ;decision-making process;many-agent problem;mean field theory;mean-field Q-learning;mean-field reinforcement learning;MFRLs action-state space;mixed cooperative-competitive game;nonstationary environments;scalability problem;structural causal model;two-agent problem","","","","31","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Underexplored Subspace Mining for Sparse-Reward Cooperative Multi-Agent Reinforcement Learning","Y. Yu; Q. Yin; J. Zhang; H. Chen; K. Huang","School of Artificial Intelligence, University of Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; University of Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences","2023 International Joint Conference on Neural Networks (IJCNN)","2 Aug 2023","2023","","","1","8","Learning cooperation in sparse-reward multi-agent reinforcement learning is challenging, since agents need to explore in the large joint-state space with sparse feedback. However, in cooperative games, the cooperative target is often related to partial attributes, hence there is no need to treat the whole state space equally. Therefore, we propose Underexplored Subspace Mining (USM), a novel type of intrinsic reward that encourages agents to selectively explore partial attributes instead of wasting time on the whole state space to accelerate learning. Specially, considering that the target-related attributes are varying in different games and hard to predefine, we choose to focus on the underexplored subspace as an alternative, which is an automatic aggregation of the underexplored bottom-level dimensions without any human design or learning parameters. We evaluate our method in cooperative games with discrete and continuous state space separately. Results demonstrate that USM consistently outperforms existing state-of-the-art methods, and becomes the only method that has succeeded in sparse-reward games evaluated with larger state space or more complicated cooperation dynamics.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10191833","Youth Innovation Promotion Association CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191833","sparse-reward cooperation;multi-agent system;reinforcement learning;selective exploration","Accelerated aging;Neural networks;Games;Reinforcement learning;Task analysis","game theory;learning (artificial intelligence);multi-agent systems;reinforcement learning","continuous state space;different games;intrinsic reward;joint-state space;larger state space;learning cooperation;partial attributes;sparse feedback;sparse-reward games;sparse-reward multiagent reinforcement learning;target-related attributes;underexplored bottom-level dimensions;Underexplored Subspace Mining;USM","","","","32","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Application of reinforcement learning for energy consumption optimization of district heating system","J. Deng; M. Eklund; S. Sierla; J. Savolainen; H. Niemistö; T. Karhela; V. Vyatkin","Department of Electrical Engineering and Automation, Aalto University, Espoo, Finland; Department of Information Technology, Abo Akademi University, Turku, Finland; Department of Electrical Engineering and Automation, Aalto University, Espoo, Finland; Semantum Ltd, Espoo, Finland; Semantum Ltd, Espoo, Finland; Department of Electrical Engineering and Automation, Aalto University, Espoo, Finland; Department of Electrical Engineering and Automation, Aalto University, Espoo, Finland","2023 IEEE 32nd International Symposium on Industrial Electronics (ISIE)","31 Aug 2023","2023","","","1","6","Heating residential spaces consumed 64 percent of total household energy consumption in Finland. Considering the heat transfer and time delay in the district heating system, the calculation of setpoints of supply temperature requires a comprehensive understanding of the real system, and experienced operators need to manually determine the setpoints. To save energy, a more effective and accurate method is needed for setpoints calculation. In this paper, a reinforcement learning based method is proposed. Through interacting with an Apros-based simulation model, the agents learn to calculate supply temperature parallelly for lowering energy costs. Simulation results show that the proposed method outperforms the existing method and has the potential to address the problem in real factories.","2163-5145","979-8-3503-9971-4","10.1109/ISIE51358.2023.10228102","Academy of Finland; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10228102","district heating;energy consumption optimization;reinforcement learning","Industrial electronics;Energy consumption;Temperature;Costs;District heating;Simulation;Reinforcement learning","district heating;energy conservation;energy consumption;power engineering computing;reinforcement learning","Apros-based simulation model;district heating system;energy consumption optimization;energy costs;heat transfer;heating residential spaces;reinforcement learning based method;time delay;total household energy consumption","","","","24","IEEE","31 Aug 2023","","","IEEE","IEEE Conferences"
"Meta-Reinforcement Learning With Dynamic Adaptiveness Distillation","H. Hu; G. Huang; X. Li; S. Song","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","28 Feb 2023","2023","34","3","1454","1464","Deep reinforcement learning is confronted with problems of sampling inefficiency and poor task migration capability. Meta-reinforcement learning (meta-RL) enables meta-learners to utilize the task-solving skills trained on similar tasks and quickly adapt to new tasks. However, meta-RL methods lack enough queries toward the relationship between task-agnostic exploitation of data and task-related knowledge introduced by latent context, limiting their effectiveness and generalization ability. In this article, we develop an algorithm for off-policy meta-RL that can provide the meta-learners with self-oriented cognition toward how they adapt to the family of tasks. In our approach, we perform dynamic task-adaptiveness distillation to describe how the meta-learners adjust the exploration strategy in the meta-training process. Our approach also enables the meta-learners to balance the influence of task-agnostic self-oriented adaption and task-related information through latent context reorganization. In our experiments, our method achieves 10%–20% higher asymptotic reward than probabilistic embeddings for actor–critic RL (PEARL).","2162-2388","","10.1109/TNNLS.2021.3105407","National Science and Technology Major Project of the Ministry of Science and Technology of China(grant numbers:2018AAA0101604); National Natural Science Foundation of China(grant numbers:61906106,61803321,62022048); Beijing Academy of Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9525812","Meta-learning;reinforcement learning (RL);task adaptiveness","Task analysis;Training;Trajectory;Learning systems;Heuristic algorithms;Feature extraction;Benchmark testing","deep learning (artificial intelligence);generalisation (artificial intelligence);reinforcement learning","asymptotic reward;deep reinforcement learning;dynamic task-adaptiveness distillation;exploration strategy;generalization ability;latent context reorganization;meta-reinforcement learning;meta-training process;off-policy meta-RL;self-oriented cognition;task migration capability;task-agnostic exploitation;task-agnostic self-oriented adaption;task-related information;task-related knowledge;task-solving skills","","","","37","IEEE","31 Aug 2021","","","IEEE","IEEE Journals"
"Deep-Reinforcement-Learning-Aided Loss-Tolerant Congestion Control for 6LoWPAN Networks","Y. Hou; H. He; X. Jiang; S. Chen; J. Yang","Department of Automation, University of Science and Technology of China, Hefei, Anhui, China; Department of Automation, University of Science and Technology of China, Hefei, Anhui, China; Department of Automation, University of Science and Technology of China, Hefei, Anhui, China; Department of Automation, University of Science and Technology of China, Hefei, Anhui, China; Department of Automation, University of Science and Technology of China, Hefei, Anhui, China","IEEE Internet of Things Journal","19 Oct 2023","2023","10","21","19125","19140","The IPv6 over low-power wireless personal area network (6LoWPAN) protocol stack is a promising solution to connect wireless sensor networks (WSNs) with the Internet for realizing a ubiquitous network interconnection of all things. However, 6LoWPAN networks face a critical challenge to control congestion caused by the burst of data traffic from wireless sensors. Packet loss will occur when the buffer overflows. This article focuses on the loss-tolerant congestion control problem in 6LoWPAN networks, which has not been addressed in existing works. We formulate the congestion control problem as a noncooperative Markov game framework and conceive a novel congestion control method, namely, deep reinforcement learning-aided loss-tolerant congestion control (DLCC), to alleviate congestion while maintaining a tolerable packet loss imposed by the buffer overflow. The proposed DLCC employs deep reinforcement learning (DRL) to solve the curse of state dimensionality, while packet loss constraints are handled by utilizing Lagrange multipliers to integrate the reward with loss constraints. By dynamically updating Lagrange multipliers in an online learning procedure, DLCC finds the optimal congestion control policy. Our simulation results show that DLCC maintains the packet loss rate below the tolerable threshold in the presence of congestion. In contrast to existing hybrid congestion control algorithms, the proposed DLCC algorithm is more energy efficient and provides higher throughput, lower average delay, and better fairness.","2327-4662","","10.1109/JIOT.2023.3281482","National Key Research and Development Program of China(grant numbers:2022YFB3902800); National Natural Science Foundation of China(grant numbers:62341113,62173315,62101525,62201543,62021001); Youth Innovation Promotion Association CAS(grant numbers:2020450); Strategic Priority Research Program of CAS(grant numbers:XDC07020200); Fundamental Research Funds for the Central Universities; China Environment for Network Innovations (CENI); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138656","Congestion control;deep reinforcement learning (DRL);IPv6 over low-power wireless personal area network (6LoWPAN);Lagrange multiplier;loss-tolerant network;noncooperative Markov game","Wireless sensor networks;Internet of Things;Packet loss;Propagation losses;Games;Throughput;Quality of service","","","","","","49","IEEE","30 May 2023","","","IEEE","IEEE Journals"
"Position and Attitude Tracking Control of a Biomimetic Underwater Vehicle via Deep Reinforcement Learning","R. Ma; Y. Wang; C. Tang; S. Wang; R. Wang","University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Beijing Unisoc Technologies Co., Ltd., Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE/ASME Transactions on Mechatronics","18 Oct 2023","2023","28","5","2810","2819","This article addresses a deep reinforcement learning (DRL) control method of position and attitude tracking for a biomimetic underwater vehicle (BUV). The BUV is actuated by two biomimetic propulsors. Each propulsor has a thick and flexible fin, which is manipulated by 12 short fin rays and can undulate in multiple wave patterns for propulsion. To achieve position and attitude tracking control on the BUV, a periodic dynamics-reparameterized soft actor-critic (SAC) algorithm is proposed. In detail, the algorithm uses the DRL method of SAC to train the controller by interacting with a simulated BUV, which is based on the propulsion model of the undulatory fin. Considering that the simulated environment may be inaccurate when compared with the real environment, some specially designed tricks are proposed. Simulations and experiments are conducted to prove the effectiveness and robustness of the proposed controller.","1941-014X","","10.1109/TMECH.2023.3249194","Beijing Natural Science Foundation(grant numbers:4222055); National Natural Science Foundation of China(grant numbers:62122087,62073316,62033013); Youth Innovation Promotion Association of the Chinese Academy of Sciences; Scientific Research Program of Beijing Municipal Commission of Education -Natural Science Foundation of Beijing(grant numbers:KZ202210017024); Beijing Nova Program(grant numbers:Z211100002121152); Young Elite Scientist Sponsorship Program by CAST(grant numbers:YESS20210236); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10092192","Biomimetic underwater vehicle (BUV);deep reinforcement learning (DRL);soft actor-critic (SAC);undulatory fin","Propulsion;Attitude control;Task analysis;Force;Biomimetics;Force measurement;Shape","","","","","","32","IEEE","4 Apr 2023","","","IEEE","IEEE Journals"
"Baggage Routing with Scheduled Departures using Deep Reinforcement Learning","R. A. SØrensen; J. Rosenberg; H. Karstoft","Department of Electrical and Computer Engineering, Aarhus University, Aarhus N, DK; BEUMER Group A/S, Aarhus N, DK; Department of Electrical and Computer Engineering, Aarhus University, Aarhus N, DK","2021 International Symposium on Computer Science and Intelligent Controls (ISCSIC)","23 Dec 2021","2021","","","13","19","As the number of travellers in airports increase, the load on the Baggage Handling Systems naturally gets higher. To accommodate this, airports can either expand or optimize their Baggage Handling System. Therefore, capacity is a common parameter to evaluate Baggage Handling Systems, and methods that can increase the capacity are highly valued within the airport industry. Previous work has shown that Deep Reinforcement Learning methods can be applied to increase the system capacity when a high load is constantly applied. It is, however, still not clear, how well such Deep Reinforcement Learning agents perform when the load of the system can change according to distributed flight schedules and realistic distributions of incoming baggage. In this work, we apply Deep Reinforcement Learning to a simulated Baggage Handling System with flight schedules and a distribution of incoming baggage generalized from data from a real airport. As opposed to previous work, we allow empty baggage totes to be stored at the entry point until new baggage arrives. The centralized Deep Reinforcement Learning agent must learn to balance the number of baggage totes in the entry queue, while also learning optimal routing strategies, ensuring that all bags meet their scheduled departure times. The performance is measured by the average number of delivered bags and the average number of rush bags that occurred in the example environment. We find that by using Deep Reinforcement Learning in this type of congested system with scheduled departures, we can reduce the number of rush bags, compared to a dynamic shortest path method with deadlock avoidance, resulting in a higher number of delivered bags in the system.","","978-1-6654-1627-6","10.1109/ISCSIC54682.2021.00014","Innovation Fund Denmark (IFD)(grant numbers:8053-00040B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9644329","Baggage handling system;routing;scheduled departures;deep reinforcement learning","Industries;Computer science;Schedules;Reinforcement learning;System recovery;Airports;Routing","airports;deep learning (artificial intelligence);multi-agent systems;scheduling;transportation;travel industry","airport;Deep Reinforcement Learning methods;system capacity;Deep Reinforcement Learning agents;distributed flight schedules;incoming baggage;baggage totes;congested system;scheduled departures;Baggage routing;centralized deep reinforcement learning agent;simulated baggage handling system","","","","13","IEEE","23 Dec 2021","","","IEEE","IEEE Conferences"
"Deep reinforcement learning based resource allocation for cloud edge collaboration fault detection in smart grid","Q. Li; Y. Zhu; J. Ding; W. Li; W. Sun; L. Ding","Hefei University of Technology, Hefei, Anhui and Engineering Technology Research Center of Industrial Automation, Hefei, Anhui, China; Hefei University of Technology, Hefei, Anhui and Engineering Technology Research Center of Industrial Automation, Hefei, Anhui, China; Electric Power Research Institute of State Grid Anhui Electric Power Co., Ltd., Hefei, Anhui, China; Hefei University of Technology, Hefei, Anhui and Engineering Technology Research Center of Industrial Automation, Hefei, Anhui, China; Hefei University of Technology, Hefei, Anhui and Engineering Technology Research Center of Industrial Automation, Hefei, Anhui, China; Hefei University of Technology, Hefei","CSEE Journal of Power and Energy Systems","","2022","PP","99","1","10","Real-time fault detection is important for the operation of smart grid. It has become a trend of future development to design an anomaly detection system based on deep learning by using the powerful computing power of cloud. However, the delay of Internet transmission is large, which may make the delay time of detection and transmission go beyond the limit. However, the edge-based scheme may not be able to undertake all the data detection tasks due to the limited computing resources of edge devices. Therefore, we propose a cloud-edge collaborative smart grid fault detection system, next to which edge devices are placed, and equipped with the lightweight neural network with different precision for fault detection. In addition, a sub-optimal and real time communication and computing resources allocation method is proposed based on deep reinforcement learning. This method greatly speeds up the solution time, can meet the requirements of data transmission delay, maximizes the system throughput, and improves the communication efficiency. The simulation results show that the scheme is superior in transmission delay and improves the real-time performance of the smart grid detection system.","2096-0042","","10.17775/CSEEJPES.2021.02390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9862540","smart grid;fault detection;deep reinforcement learning;cloud-edge collaboration;communication delay","Delays;Fault detection;Cloud computing;Smart grids;Resource management;Real-time systems;Image edge detection","","","","","","","","18 Aug 2022","","","CSEE","CSEE Early Access Articles"
"Path Following Control for Unmanned Surface Vehicles: A Reinforcement Learning-Based Method With Experimental Validation","Y. Wang; J. Cao; J. Sun; X. Zou; C. Sun","School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; Zhuhai Yunzhou Intelligence Technology Company Ltd, Zhuhai, China; School of Automation, Southeast University, Nanjing, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","14","In this article, a reinforcement learning (RL)-based strategy for unmanned surface vehicle (USV) path following control is developed. The proposed method learns integrated guidance and heading control policy, which directly maps the USV’s navigation states to motor control commands. By introducing a twin-critic design and an integral compensator to the conventional deep deterministic policy gradient (DDPG) algorithm, the tracking accuracy and robustness of the controller can be significantly improved. Moreover, a pretrained neural network-based USV model is built to help the learning algorithm efficiently deal with unknown nonlinear dynamics. The self-learning and path following capabilities of the proposed method were validated in both simulations and real sea experiments. The results show that our control policy can achieve better performance than a traditional cascade control policy and a DDPG-based control policy.","2162-2388","","10.1109/TNNLS.2023.3313312","National Natural Science Foundation of China(grant numbers:62103104,62136008); Natural Science Foundation of Jiangsu Province(grant numbers:BK20210215); China Postdoctoral Science Foundation(grant numbers:2021M690615); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256669","Neural network;path following control;reinforcement learning (RL);unmanned surface vehicle (USV)","Heuristic algorithms;Training;Task analysis;Vehicle dynamics;Nonlinear dynamical systems;Neural networks;Sun","","","","","","","IEEE","20 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Neural Manifold Modulated Continual Reinforcement Learning for Musculoskeletal Robots","J. Chen; Z. Chen; C. Yao; H. Qiao","Institute of Automation, National Key Laboratory of Multimodal Artificial Intelligence System, Chinese Academy of Sciences, Beijing, China; Institute of Automation, National Key Laboratory of Multimodal Artificial Intelligence System, Chinese Academy of Sciences, Beijing, China; Institute of Automation, National Key Laboratory of Multimodal Artificial Intelligence System, Chinese Academy of Sciences, Beijing, China; Institute of Automation, National Key Laboratory of Multimodal Artificial Intelligence System, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Cognitive and Developmental Systems","","2022","PP","99","1","1","The continual learning and development are significant for robots to learn multiple tasks sequentially. The difficulty lies in balancing the efficient learning of new tasks and overcoming catastrophic forgetting of old tasks. Although many continual learning methods have been proposed for pattern recognition, continual reinforcement learning methods for redundant musculoskeletal and robotic systems are few and have limitations. Therefore, inspired by the developmental mechanisms in motor cortex, this article proposes a neural manifold modulated continual reinforcement learning method for musculoskeletal and robotic systems. First, a recurrent neural network (RNN) with expected neural manifold is designed and conditions of weights are derived. Second, the ability of projectors for characterizing the neural manifold within RNN is analyzed. Furthermore, a continual reinforcement learning method of RNN is proposed with the modulation of neural manifold. The method is validated by redundant musculoskeletal and robotic systems in simulation. The results suggest that it can realize continual reinforcement learning of multiple tasks in different movements and environments. Furthermore, compared with related works, the proposed method achieves better performance.","2379-8939","","10.1109/TCDS.2022.3231055","National Natural Science Foundation of China(grant numbers:62203439,91948303); State Key Laboratory of Management and Control for Complex Systems(grant numbers:E2S9020904); Strategic Priority Research Program of Chinese Academy of Science(grant numbers:XDB32050100); Major Project of Science and Technology Innovation 2030 C Brain Science and Brain-Inspired Intelligence(grant numbers:2021ZD0200408); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9995720","Musculoskeletal robots;Continual reinforcement learning;Recurrent neural network;Neural manifold","Task analysis;Robots;Manifolds;Musculoskeletal system;Neurons;Reinforcement learning;Recurrent neural networks","","","","","","","IEEE","21 Dec 2022","","","IEEE","IEEE Early Access Articles"
"Hierarchical Multi-Agent Deep Reinforcement Learning for Coordinated Voltage Regulation in Active Distribution Networks with Hybrid Devices","T. Zhang; L. Yu; D. Yue; D. Zhang","College of Automation & College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Automation & College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Automation & College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Automation & College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","7219","7224","Voltage regulation is indispensable to the secure operation of active distribution networks (ADNs) with high penetrated renewables. In this paper, we investigate an optimal two-timescale voltage regulation problem of ADNs with hybrid devices. Specifically, we intend to minimize the total power loss of the whole ADNs while maintaining all bus voltages within a safe range. Due to the existence of uncertain parameters, temporal couplings, multiple timescales, mixed decision variables, and unknown system models, it is challenging to solve the above optimization problem. To this end, we propose a coordinated voltage regulation algorithm based on hierarchical multi-agent attention-based deep reinforcement learning. The proposed algorithm can support flexible collaboration among hybrid devices. Simulation results based on real-world traces show the effectiveness of the proposed algorithm.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240379","National Natural Science Foundation of China(grant numbers:62192751,61972214); Qinglan Project of Jiangsu Province; Nanjing University of Posts and Telecommunications (NUPT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240379","Active distribution networks;two-timescale voltage regulation;hierarchical multi-agent deep reinforcement learning","Deep learning;Couplings;Renewable energy sources;Simulation;Reinforcement learning;Games;Markov processes","deep learning (artificial intelligence);distributed power generation;distribution networks;multi-agent systems;power distribution control;reinforcement learning;voltage control","active distribution networks;ADNs;bus voltages;coordinated voltage regulation algorithm;hierarchical multiagent attention-based deep reinforcement learning;hierarchical multiagent deep reinforcement learning;high penetrated renewables;hybrid devices;mixed decision variables;multiple timescales;optimization problem;safe range;secure operation;total power loss;two-timescale voltage regulation problem;unknown system models","","","","20","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Q-RAN: A Constructive Reinforcement Learning Approach for Robot Behavior Learning","L. Jun; A. Lilienthal; T. Martinez-Marin; T. Duckett","AASS, Department of Technology, Orebro University, Orebro, Sweden; AASS, Department of Technology, Orebro University, Orebro, Sweden; Department of Physics, System Engineering and Signal Theory, University of Alicante, Alicante, Spain; Department of Computing and Informatics, University of Lincoln, Lincoln, UK","2006 IEEE/RSJ International Conference on Intelligent Robots and Systems","15 Jan 2007","2006","","","2656","2662","This paper presents a learning system that uses Q-learning with a resource allocating network (RAN) for behavior learning in mobile robotics. The RAN is used as a function approximator, and Q-learning is used to learn the control policy in 'off-policy' fashion that enables learning to be bootstrapped by a prior knowledge controller, thus speeding up the reinforcement learning. Our approach is verified on a PeopleBot robot executing a visual servoing based docking behavior in which the robot is required to reach a goal pose. Further experiments show that the RAN network can also be used for supervised learning prior to reinforcement learning in a layered architecture, thus further improving the performance of the docking behavior","2153-0866","1-4244-0258-1","10.1109/IROS.2006.281986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4058792","","Learning systems;Radio access networks;Intelligent robots;State-space methods;Backpropagation;Neurons;Robotics and automation;Mobile robots;Resource management;Visual servoing","learning (artificial intelligence);learning systems;mobile robots;visual servoing","Q-RAN;constructive reinforcement learning approach;robot behavior learning;learning system;Q-learning;resource allocating network;mobile robotics;function approximator;a prior knowledge controller;PeopleBot robot;visual servoing;docking behavior;supervised learning","","6","","22","IEEE","15 Jan 2007","","","IEEE","IEEE Conferences"
"Autonomous GUI Testing using Deep Reinforcement Learning","S. Saber; F. Elbadry; H. Negm; R. A. El-Ershad; O. Magdy; M. Bahnassawi; R. El Adawi; A. Bayoumi","Computer Engineering Department, Cairo University, Giza, Egypt; Computer Engineering Department, Cairo University, Giza, Egypt; Computer Engineering Department, Cairo University, Giza, Egypt; Computer Engineering Department, Cairo University, Giza, Egypt; Siemens EDA, Siemens, Cairo, Egypt; Siemens EDA, Siemens, Cairo, Egypt; Siemens EDA, Siemens, Cairo, Egypt; Computer Engineering Department, Cairo University, Giza, Egypt","2021 17th International Computer Engineering Conference (ICENCO)","21 Feb 2022","2021","","","94","100","Automating software testing looks forward to speeding up testing processes and ensuring possible replication of discovered software bugs. However, Automating the GUI testing process is highly challenging due to the need for human intervention to determine actions and assess outcomes. We introduce a novel approach to fully automate GUI testing using deep reinforcement learning. Our deep reinforcement learning model discovers all system states and determines possible testing sequences. The automated testing agent starts with exploring the tested environment to learn the most efficient paths for reaching maximum coverage while discovering GUI bugs. In this case, testers could focus more on functionality testing to improve the overall software quality. We evaluated the developed model on a couple of industry products, and it showed a substantial increase in coverage than random testing.","2475-2320","978-1-7281-6448-9","10.1109/ICENCO49852.2021.9715282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9715282","reinforcement learning;GUI testing;testing;test automation;functional testing","Software testing;Industries;Computational modeling;Computer bugs;Reinforcement learning;Software quality;Testing","graphical user interfaces;learning (artificial intelligence);program testing;software quality","autonomous GUI;Automating software testing;testing processes;ensuring possible replication;discovered software bugs;GUI testing process;human intervention;fully automate GUI testing;deep reinforcement learning model discovers all system states;possible testing sequences;automated testing agent;tested environment;discovering GUI bugs;functionality testing;software quality;random testing","","","","30","IEEE","21 Feb 2022","","","IEEE","IEEE Conferences"
"Deep-Reinforcement-Learning-Based Cybertwin Architecture for 6G IIoT: An Integrated Design of Control, Communication, and Computing","H. Xu; J. Wu; J. Li; X. Lin","Shanghai Key Laboratory of Integrated Administration Technologies for Information Security and the School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Key Laboratory of Integrated Administration Technologies for Information Security, and the School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Integrated Administration Technologies for Information Security and the School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Integrated Administration Technologies for Information Security and the School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Internet of Things Journal","4 Nov 2021","2021","8","22","16337","16348","The cybertwin and 6G-enabled Industrial Internet of Things (6G-IIoT) are the critical technologies that create the digital counterparts for physical systems and enable the near-instant interconnectivity in the industrial domain. It is in demand but challenging to conduct the integrated design for 6G-IIoT, which intertwines the cyber subsystems, such as control, communication, computing (3C), and the physical industrial factories and plants. Therefore, the cybertwin, which synchronizes between the digital counterparts and its physical entities during the system runtime, is the ideal proving ground for conducting the integrated design on the highly intertwined 3C of 6G-IIoT. However, the cybertwin lacks artificial intelligence to capacitate the automated integrated design for the 6G-IIoT. In this article, we first demonstrate the architecture of the machine-learning-based cybertwin for 6G-IIoT. Then, we leverage deep reinforcement learning (DRL) to conduct the integrated design via systematic trial and error in the cybertwin model, which is otherwise costly and dangerous in real industrial systems. Moreover, we invent the adaptive observation window for deep  $Q$ -network (AOW-DQN), which generates system states adaptive to the control system’s physical dynamics. Finally, the experimental results demonstrate the effectiveness and efficiency of our approach. To the best of our knowledge, we are the first to present the machine-learning-based cybertwin for carrying out the integrated design on the 3C for 6G-IIoT.","2327-4662","","10.1109/JIOT.2021.3098441","National Natural Science Foundation of China(grant numbers:61972255,U20B2048); Shanghai Municipal Science and Technology Major Project(grant numbers:2021SHZDZX0102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490666","6G Industrial Internet of Things (6G-IIoT);cybertwin;deep reinforcement learning (DRL);integrated design","6G mobile communication;Integrated design;Industrial Internet of Things;Computational modeling;Sensors;Servers;Control systems","6G mobile communication;deep learning (artificial intelligence);industrial plants;Internet of Things;production engineering computing;production facilities;reinforcement learning","physical industrial plants;artificial intelligence;DQN;AOW;adaptive observation window;deep Q-network;DRL;3C design;control communication and computing design;deep reinforcement learning;6G-enabled Industrial Internet of Things;control system physical dynamics;cybertwin architecture;machine learning;automated integrated design;physical industrial factories;6G IIoT","","18","","28","IEEE","19 Jul 2021","","","IEEE","IEEE Journals"
"An Inverse Reinforcement Learning Approach for Customizing Automated Lane Change Systems","J. Liu; L. N. Boyle; A. G. Banerjee","Department of Industrial & Systems Engineering, University of Washington, Seattle, WA, USA; Department of Industrial & Systems Engineering Department of Civil & Environmental Engineering, University of Washington, Seattle, WA, USA; Department of Industrial & Systems Engineering Department of Mechanical Engineering, University of Washington, Seattle, WA, USA","IEEE Transactions on Vehicular Technology","16 Sep 2022","2022","71","9","9261","9271","Vehicle automation seeks to enhance road safety and improve the driving experience. However, a standard system does not account for variations in users and driving conditions. Customizing vehicle automation based on users’ preferences aims to improve the user experience and adoption of the technologies. This study introduces a systematic paradigm that starts with naturalistic driving data to identify the driving behaviors and styles for a customized automated lane change system. The driving behaviors are first extracted using Multivariate Functional Principal Component Analysis (MFPCA) with minimum prior expert knowledge. The driving styles are identified by clustering the extracted driving behaviors. An Inverse Reinforcement Learning (IRL) algorithm is then used to train the automated lane change system from grouped demonstrations of the identified driving styles to capture the preferences of a group of drivers with a similar driving style. The performance of the proposed customized automated lane change system is compared to (1) a non-customized system trained on all the sample trips, (2) customized systems built on expert-coded reward functions, and (3) customized systems trained using a Generative Adversarial Imitation Learning (GAIL) algorithm. The results show that our method outperforms all the other systems with respect to the prediction accuracy of the lane change actions. Additionally, our method gains insights on the representative behaviors of different driving styles to enable customization of automated lane change systems.","1939-9359","","10.1109/TVT.2022.3179332","National Science Foundation(grant numbers:CPS 1739085); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785911","Customized automated lane change systems;driving style identification;inverse reinforcement learning;multivariate functional principal component analysis","Behavioral sciences;Vehicles;Automation;Data models;Principal component analysis;Task analysis;Reinforcement learning","behavioural sciences;driver information systems;learning (artificial intelligence);principal component analysis;road safety;road traffic;road vehicles;traffic engineering computing","Inverse Reinforcement Learning approach;customizing automated lane change systems;vehicle automation;driving experience;driving conditions;naturalistic driving data;customized automated lane change system;extracted driving behaviors;identified driving styles;similar driving style;noncustomized system;lane change actions;different driving styles","","5","","49","IEEE","31 May 2022","","","IEEE","IEEE Journals"
"DeepDefrag: A deep reinforcement learning framework for spectrum defragmentation","E. Etezadi; C. Natalino; R. Diaz; A. Lindgren; S. Melin; L. Wosinska; P. Monti; M. Furdek","Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Telia AB, Solna, Sweden; Telia AB, Solna, Sweden; Telia AB, Solna, Sweden; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden","GLOBECOM 2022 - 2022 IEEE Global Communications Conference","11 Jan 2023","2022","","","3694","3699","Exponential growth of bandwidth demand, spurred by emerging network services with diverse characteristics and stringent performance requirements, drives the need for dynamic operation of optical networks, efficient use of spectral resources, and automation. One of the main challenges of dynamic, resource-efficient Elastic Optical Networks (EONs) is spectrum fragmentation. Fragmented, stranded spectrum slots lead to poor resource utilization and increase the blocking probability of in-coming service requests. Conventional approaches for Spectrum Defragmentation (SD) apply various criteria to decide when, and which portion of the spectrum to defragment. However, these polices often address only a subset of tasks related to defragmentation, are not adaptable, and have limited automation potential. To address these issues, we propose DeepDefrag, a novel framework based on reinforcement learning that addresses the main aspects of the SD process: determining when to perform de-fragmentation, which connections to reconfigure, and which part of the spectrum to reallocate them to. DeepDefrag outperforms the well-known Older-First First-Fit (OF-FF) defragmentation heuristic, achieving lower blocking probability under smaller defragmentation overhead.","2576-6813","978-1-6654-3540-6","10.1109/GLOBECOM48099.2022.10000736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10000736","Spectrum defragmentation;Reinforcement learning;Service blocking ratio","Deep learning;Automation;Law enforcement;Simulation;Reinforcement learning;Optical fiber networks;Dynamic scheduling","deep learning (artificial intelligence);optical fibre networks;probability;reinforcement learning;telecommunication computing","automation potential;bandwidth demand;deep reinforcement learning framework;DeepDefrag;diverse characteristics;dynamic operation;exponential growth;in-coming service requests;lower blocking probability;network services;poor resource utilization;resource-efficient elastic optical networks;service requests;smaller defragmentation overhead;spectral resources;spectrum defragmentation;spectrum fragmentation;stranded spectrum slots;stringent performance requirements","","2","","24","EU","11 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning for UAV control with Policy and Reward Shaping","C. Millán-Arias; R. Contreras; F. Cruz; B. Fernandes","Escola Politécnica de Pernambuco, Universidade de Pernambuco, Recife, Brazil; Escuela de Ingeniería, Universidad Central de Chile, Santiago, Chile; School of Computer Science and Engineering, University of New South Wales, Sydney, Australia; Escola Politécnica de Pernambuco, Universidade de Pernambuco, Recife, Brazil","2022 41st International Conference of the Chilean Computer Science Society (SCCC)","4 Jan 2023","2022","","","1","8","In recent years, unmanned aerial vehicle (UAV) related technology has expanded knowledge in the area, bringing to light new problems and challenges that require solutions. Furthermore, because the technology allows processes usually carried out by people to be automated, it is in great demand in industrial sectors. The automation of these vehicles has been addressed in the literature, applying different machine learning strategies. Reinforcement learning (RL) is an automation framework that is frequently used to train autonomous agents. RL is a machine learning paradigm wherein an agent interacts with an environment to solve a given task. However, learning autonomously can be time consuming, computationally expensive, and may not be practical in highly-complex scenarios. Interactive reinforcement learning allows an external trainer to provide advice to an agent while it is learning a task. In this study, we set out to teach an RL agent to control a drone using reward-shaping and policy-shaping techniques simultaneously. Two simulated scenarios were proposed for the training; one without obstacles and one with obstacles. We also studied the influence of each technique. The results show that an agent trained simultaneously with both techniques obtains a lower reward than an agent trained using only a policy-based approach. Nevertheless, the agent achieves lower execution times and less dispersion during training.","2691-0632","978-1-6654-5674-6","10.1109/SCCC57464.2022.10000286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10000286","Reinforcement learning;unmanned aerial vehicles;policy-shaping;reward-shaping","Training;Automation;Reinforcement learning;Autonomous aerial vehicles;Market research;Autonomous agents;Behavioral sciences","autonomous aerial vehicles;reinforcement learning","automation framework;autonomous agents;highly-complex scenarios;industrial sectors;interactive reinforcement learning;machine learning strategies;policy-based approach;reward-shaping;RL agent;UAV control;unmanned aerial vehicle related technology","","","","34","IEEE","4 Jan 2023","","","IEEE","IEEE Conferences"
"Control of robotic manipulators using a CMAC-based reinforcement learning system","Mei Han; Bo Zhang","Department of Computer Science, Tsinghua University, Beijing, China; Department of Computer Science, Tsinghua University, Beijing, China","Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS'94)","6 Aug 2002","1994","3","","2117","2122 vol.3","A practical learning control system is described in this paper, which is applicable to the control of complex robotic systems. In the controller, a stochastic reinforcement learning algorithm is used to learn functions with continuous outputs as control signals. The authors present a CMAC-based network incorporating stochastic real-valued units that learns to perform an underconstrained positioning task using a simulated 2-degree-of-freedom robot arm. The authors also investigate the effects of varying learning algorithm parameters.<>","","0-7803-1933-8","10.1109/IROS.1994.407573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=407573","","Robot control;Control systems;Manipulators;Learning;Stochastic processes;Orbital robotics;Neural networks;Computer science;Control system synthesis;Adaptive control","learning (artificial intelligence);cerebellar model arithmetic computers;learning systems;position control;manipulators","robotic manipulators;CMAC-based reinforcement learning system;learning control system;stochastic reinforcement learning algorithm;CMAC-based network;underconstrained positioning task;simulated 2-degree-of-freedom robot arm","","3","","6","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Efficient reinforcement learning of navigation strategies in an autonomous robot","J. del R. Millan; C. Torras","Institute for Systems Engineering and Informatics Ellropean Commission, Joint Research Centre, VA, USA; Institut de Cibernètica CSIC-UPC Diagonal, Barcelona, Spain","Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS'94)","6 Aug 2002","1994","1","","15","22 vol.1","Proposes a reinforcement learning architecture that allows an autonomous robot to acquire efficient navigation strategies in a few trials. Besides fast learning, the architecture has 3 further appealing features. (1) Since it learns from built-in reflexes, the robot is operational from the very beginning. (2) The robot improves its performance incrementally as it interacts with an initially unknown environment, and it ends up learning to avoid collisions even if its sensors cannot detect the obstacles. This is a definite advantage over non-learning reactive robots. (3) The robot exhibits high tolerance to noisy sensory data and good generalization abilities. All these features make this learning robot's architecture very well suited to real-world applications. The authors report experimental results obtained with a real mobile robot in an indoor environment that demonstrate the feasibility of this approach.<>","","0-7803-1933-8","10.1109/IROS.1994.407414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=407414","","Learning;Navigation;Robot sensing systems;Electronic mail;Sensor phenomena and characterization;Working environment noise;Mobile robots;Indoor environments","learning (artificial intelligence);generalisation (artificial intelligence);mobile robots;path planning;computerised navigation","reinforcement learning;navigation strategies;autonomous robot;fast learning;built-in reflexes;generalization abilities;real-world applications;indoor environment;collision avoidance","","1","","19","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Converging Game Theory and Reinforcement Learning For Industrial Internet of Things","T. M. Ho; K. K. Nguyen; M. Cheriet","Synchromedia Laboratory, Ècole de Technologie Supérieure, Université du Québec, Montreal, QC, Canada; Synchromedia Laboratory, Ècole de Technologie Supérieure, Université du Québec, Montreal, QC, Canada; Synchromedia Laboratory, Ècole de Technologie Supérieure, Université du Québec, Montreal, QC, Canada","IEEE Transactions on Network and Service Management","29 Jun 2023","2023","20","2","890","903","The fifth-generation (5G) wireless network provides high-rate, ultra-low latency, and high-reliability connections that can meet the Industrial Internet of Things (IIoT) requirements in factory automation, especially for robot motion control. In this paper, we address 5G service provisioning in an automated warehouse scenario, where swarm robotics is controlled by an industrial controller that provides routing and job instructions over the 5G network. Leveraging the coordinated multipoint (CoMP), we formulate a time-varying joint CoMP clustering and 5G ultra-reliable low-latency communication (URLLC) beamforming design problem to control the robots that move around the automated warehouse for goods storage with the planned reference tracks. Traditional iterative optimization approaches are impractical in such a dynamic wireless environment due to high computational time. We propose a game-theoretic CoMP clustering algorithm combined with the Proximal Policy Optimization method to obtain a stationary solution closed to that of the exhaustive search algorithm considered as the global optimal solution.","1932-4537","","10.1109/TNSM.2022.3202168","Mitacs, Ciena, and ENCQOR(grant numbers:IT13947); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9868824","5G network;industrial IoT;URLLC","Ultra reliable low latency communication;Gaussian processes;5G mobile communication;Clustering algorithms;Array signal processing;Heuristic algorithms;Optimization","5G mobile communication;array signal processing;factory automation;game theory;industrial control;Internet of Things;iterative methods;motion control;multi-robot systems;optimisation;pattern clustering;reinforcement learning;telecommunication network reliability;telecommunication scheduling","5G service provisioning;automated warehouse scenario;communication beamforming design problem;coordinated multipoint;dynamic wireless environment;factory automation;fifth-generation wireless network;game theory;game-theoretic CoMP clustering algorithm;high computational time;high-reliability connections;industrial controller;job instructions;planned reference tracks;reinforcement learning;robot motion control;swarm robotics;Things requirements;ultra-low latency","","","","30","IEEE","29 Aug 2022","","","IEEE","IEEE Journals"
"Genetic reinforcement learning approach to the heterogeneous machine scheduling problem","Gyoung Hwan Kim; C. S. G. Lee","School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Robotics and Automation","6 Aug 2002","1998","14","6","879","893","Focuses on the development of a learning-based heuristic for scheduling heterogeneous machines. Although list scheduling methods have been widely used for a large class of scheduling problems, including the heterogeneous machine scheduling problem, they involve designing priority rules, which usually require a fair amount of insights on the characteristics of the problem to be solved. Instead of elaborate design of priority rules in a single step, we propose an iterative list scheduling process, which refines priority rules while generating a number of schedules. The proposed iterative list scheduling is formulated as a reinforcement learning problem, with states and actions defined in list scheduling. Due to the large number of possible states, reinforcement learning algorithms which use value functions in constructing an optimal policy may not be suitable for scheduling problems. Thus, to directly work with policies rather than the values of states, we propose genetic reinforcement learning (GRL), in which the policies of reinforcement learning are encoded into the chromosomes of genetic algorithms and a near-optimal policy is searched for by genetic algorithms. A GRL-based scheduler, called evolutionary intracell scheduler (EVIS), has been developed and applied to various scheduling problems such as the heterogeneous machine scheduling, the processor scheduling, the job-shop scheduling, the flow-shop scheduling, and the open-shop scheduling problems. The proposed model of EVIS, which has a linear order of population-fitness convergence, is verified by computer experiments. Even without fine tuning EVIS, the quality of solutions achieved by EVIS is comparable to that of problem-tailored heuristics for most of the problem instances.","2374-958X","","10.1109/70.736772","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=736772","","Learning;Job shop scheduling;Processor scheduling;Scheduling algorithm;Approximation algorithms;Genetic algorithms;Iterative algorithms;Biological cells;Cost function","learning (artificial intelligence);genetic algorithms;scheduling;production control;iterative methods;minimisation","genetic reinforcement learning;heterogeneous machine scheduling problem;learning-based heuristic;iterative list scheduling process;near-optimal policy;evolutionary intracell scheduler;job-shop scheduling;flow-shop scheduling;open-shop scheduling problems","","10","","67","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Design of Nonlinear Iterative Learning Control Based on Deep Reinforcement Learning Algorithm","J. Shi; K. Wen; X. Xu; X. Hu; D. Luo","Department of Chemical and Biochemical Engineering, School of Chemistry and Chemical Engineering, Xiamen University, Xiamen, Fujian, China; Department of Chemical and Biochemical Engineering, School of Chemistry and Chemical Engineering, Xiamen University, Xiamen, Fujian, China; Department of Chemical and Biochemical Engineering, School of Chemistry and Chemical Engineering, Xiamen University, Xiamen, Fujian, China; Department of Chemical and Biochemical Engineering, School of Chemistry and Chemical Engineering, Xiamen University, Xiamen, Fujian, China; TencentAILab, Shenzhen, Guangdong, China","2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS)","25 Jun 2021","2021","","","722","727","Iterative learning control (ILC) is an advanced control method which has been studied and widely used in periodical, repetitive or batch processes. However, there is still a lack of an effective method for the design of nonlinear iterative learning control (NILC). In view of the excellent performance of deep reinforcement learning (DRL) in dealing with the decision-making problems for the complex dynamical processes, in this paper, we propose an intelligent design method for NILC system by using deep deterministic policy gradient (DDPG), a typical DRL algorithm. By properly designing the state information and the instant reward, the design algorithm can gradually realize the optimal NILC law through the interaction without any requirement of prior knowledge of the processes. The numerical simulation based on a nonlinear model illustrates the effectiveness and applicability of the proposed design method.","2767-9861","978-1-6654-2423-3","10.1109/DDCLS52934.2021.9455494","NationalNaturalScience Foundation (NNSF) of China(grant numbers:61573295); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9455494","iterative learning control;reinforcement learning;deep neural network;policy gradient","Learning systems;Design methodology;Heuristic algorithms;Industry applications;Process control;Reinforcement learning;Numerical simulation","batch processing (industrial);decision making;iterative methods;learning (artificial intelligence);learning systems;nonlinear control systems","typical DRL algorithm;design algorithm;nonlinear iterative learning control;deep reinforcement learning algorithm;advanced control method;periodical batch processes;repetitive batch processes;complex dynamical processes;intelligent design method;deep deterministic policy gradient","","1","","18","IEEE","25 Jun 2021","","","IEEE","IEEE Conferences"
"Acquiring robot skills via reinforcement learning","V. Gullapalli; J. A. Franklin; H. Benbrahim","Computer Science Department, University of Massachusetts, Amherst, MA, USA; GTE Laboratories, Inc., Waltham, MA, USA; GTE Laboratories, Inc., Waltham, MA, USA","IEEE Control Systems Magazine","6 Aug 2002","1994","14","1","13","24","Skill acquisition is a difficult , yet important problem in robot performance. The authors focus on two skills, namely robotic assembly and balancing and on two classic tasks to develop these skills via learning: the peg-in hole insertion task, and the ball balancing task. A stochastic real-valued (SRV) reinforcement learning algorithm is described and used for learning control and the authors show how it can be used with nonlinear multilayer ANNs. In the peg-in-hole insertion task the SRV network successfully learns to insert to insert a peg into a hole with extremely low clearance, in spite of high sensor noise. In the ball balancing task the SRV network successfully learns to balance the ball with minimal feedback.<>","1941-000X","","10.1109/37.257890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=257890","","Control systems;Uncertainty;Delay;Control design;Robot control;Robotic assembly;Supervised learning;Adaptive control;Robust control;Feedback","learning (artificial intelligence);robots;backpropagation","robot skills;skill acquisition;robotic assembly;peg-in hole insertion task;ball balancing;stochastic real-valued reinforcement learning algorithm;learning control;nonlinear multilayer neural networks","","123","3","37","IEEE","6 Aug 2002","","","IEEE","IEEE Magazines"
"Distributed Deep Reinforcement Learning-Based Energy and Emission Management Strategy for Hybrid Electric Vehicles","X. Tang; J. Chen; T. Liu; Y. Qin; D. Cao","College of Mechanical and Vehicle Engineering, Chongqing University, Chongqing, China; College of Mechanical and Vehicle Engineering, Chongqing University, Chongqing, China; Department of Mechanical and Mechatronics Engineering, University of Waterloo, Waterloo, Ontario, Canada; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; Department of Mechanical and Mechatronics Engineering, University of Waterloo, Waterloo, Ontario, Canada","IEEE Transactions on Vehicular Technology","15 Oct 2021","2021","70","10","9922","9934","Advanced algorithms can promote the development of energy management strategies (EMSs) as a key technology in hybrid electric vehicles (HEVs). Reinforcement learning (RL) with distributed structure can significantly improve training efficiency in complex environments, and multi-threaded parallel computing provides a reliable algorithm basis for promoting adaptability. Dedicated to trying more efficient deep reinforcement learning (DRL) algorithms, this paper proposed a deep q-network (DQN)-based energy and emission management strategy (E&EMS) at first. Then, two distributed DRL algorithms, namely asynchronous advantage actor-critic (A3C) and distributed proximal policy optimization (DPPO), were adopted to propose EMSs, respectively. Finally, emission optimization was taken into account and then distributed DRL-based E&EMSs were proposed. Regarding dynamic programming (DP) as the optimal benchmark, simulation results show that three DRL-based control strategies can achieve near-optimal fuel economy and outstanding computational efficiency, and compared with DQN, two distributed DRL algorithms have improved the learning efficiency by four times.","1939-9359","","10.1109/TVT.2021.3107734","National Natural Science Foundation of China(grant numbers:52072051); Natural Science Foundation of Chongqing(grant numbers:cstc2020jcyj-msxmX0956); Fundamental Research Funds for the Central Universities(grant numbers:2020CDJ-LHZZ-041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524518","Asynchronous advantage actor-critic;deep reinforcement learning;distributed proximal policy optimization;energy management strategy;hybrid electric vehicle","Energy management;Optimization;Hybrid electric vehicles;Heuristic algorithms;Real-time systems;Engines;Training","automobile industry;deep learning (artificial intelligence);dynamic programming;energy management systems;fuel economy;hybrid electric vehicles;multi-threading;production engineering computing","DRL-based E&EMSs;DRL-based control strategies;distributed DRL;distributed deep reinforcement learning;hybrid electric vehicles;multithreaded parallel computing;deep Q-network;asynchronous advantage actor-critic;distributed proximal policy optimization;emission optimization;energy and emission management strategy;dynamic programming","","57","","41","IEEE","27 Aug 2021","","","IEEE","IEEE Journals"
"A Reinforcement Learning approach for a Decision Support System for logistics networks","M. Rabe; F. Dross","Department of IT in Production and Logistics, Technical University Dortmund, Dortmund, GERMANY; Graduate School of Logistics, Technical University Dortmund, Dortmund, GERMANY","2015 Winter Simulation Conference (WSC)","18 Feb 2016","2015","","","2020","2032","This paper presents the architecture and working principles of a Decision Support System (DSS) for logistics networks. The system relies on a data-driven discrete-event simulation model. A brief introduction to Reinforcement Learning (RL) and an explanation of the adoption of RL to the concepts of the DSS is given. An illustration of the realization is presented using a specific aspect of a logistics network. The logistics network is described in a data model which is represented by database tables. The tables are used to dynamically instantiate the simulation model. The authors describe how SQL queries can be used to model actions of an RL agent. A Data Warehouse can be used to measure Key Performance Indicators on the simulation output data of the simulation model, which can be used as a reward criterion for the RL agent. The paper presents a basis for the ongoing development of an RL agent.","1558-4305","978-1-4673-9743-8","10.1109/WSC.2015.7408317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7408317","","Decision support systems;Data models;Load modeling;Companies;Supply chains;Learning (artificial intelligence)","data warehouses;decision support systems;discrete event simulation;learning (artificial intelligence);logistics;production engineering computing;query processing;SQL","reinforcement learning approach;decision support system;logistics networks;data-driven discrete-event simulation model;DSS;database tables;data model;simulation model;SQL queries;RL agent;data warehouse;key performance indicators","","23","","32","IEEE","18 Feb 2016","","","IEEE","IEEE Conferences"
"Towards a Deep Reinforcement Learning based approach for real time decision making and resource allocation for Prognostics and Health Management applications","R. Ludeke; P. S. Heyns","Department of Mechanical and Aeronautical Engineering, Centre for Asset Integrity Management, University of Pretoria, Pretoria, South Africa; Department of Mechanical and Aeronautical Engineering, Centre for Asset Integrity Management, University of Pretoria, Pretoria, South Africa","2023 IEEE International Conference on Prognostics and Health Management (ICPHM)","2 Aug 2023","2023","","","20","29","Industrial operational environments are stochastic and can have complex system dynamics which introduce multiple levels of uncertainty. This uncertainty can lead to sub-optimal decision making and resource allocation. Digitalization and automation of production equipment and the maintenance environment enable predictive maintenance, which means that equipment can be stopped for maintenance at the optimal time instant. Resource constraints in maintenance capacity could however result in further undesired downtime if maintenance cannot be performed when scheduled. In this paper the use of a multi-agent deep reinforcement learning based approach for decision making is investigated to determine the optimal maintenance scheduling policy for a fleet of assets where there are maintenance resource constraints. By considering the underlying system dynamics of maintenance capacity, as well as the health state of individual assets, a near-optimal decision making policy is found that increases equipment availability while also maximizing maintenance capacity. The proposed solution is compared to a run-to-failure corrective maintenance strategy, a constant interval preventive maintenance strategy and a condition based predictive maintenance strategy. The proposed approach outperformed traditional maintenance strategies across several asset and operational maintenance performance metrics. It is concluded that deep reinforcement learning based decision making for asset health management and resource allocation is more effective than human based decision making.","2166-5656","979-8-3503-4625-1","10.1109/ICPHM57936.2023.10194168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10194168","deep reinforcement learning;multi-agent reinforcement learning;maintenance policy","Deep learning;Measurement;Uncertainty;Decision making;Reinforcement learning;Maintenance engineering;Data models","condition monitoring;decision making;deep learning (artificial intelligence);maintenance engineering;multi-agent systems;optimisation;preventive maintenance;production equipment;reinforcement learning;resource allocation","asset health management;complex system dynamics;constant interval preventive maintenance strategy;deep reinforcement learning based decision;health management applications;health state;human based decision making;increases equipment availability while also maximizing maintenance capacity;industrial operational environments;maintenance environment;maintenance resource constraints;multiagent deep reinforcement learning based approach;near-optimal decision making policy;operational maintenance performance metrics;optimal maintenance scheduling policy;optimal time instant;predictive maintenance strategy;production equipment;resource allocation;run-to-failure corrective maintenance strategy;sub-optimal decision making;time decision making;traditional maintenance strategies;underlying system dynamics","","","","9","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach to Health Aware Control Strategy","M. S. Jha; P. Weber; D. Theilliol; J. -C. Ponsart; D. Maquin","UMR 7039, University of Lorraine, Vandoeuvre Cedex, France; UMR 7039, University of Lorraine, Vandoeuvre Cedex, France; UMR 7039, University of Lorraine, Vandoeuvre Cedex, France; UMR 7039, University of Lorraine, Vandoeuvre Cedex, France; UMR 7039, University of Lorraine, Vandoeuvre Cedex, France","2019 27th Mediterranean Conference on Control and Automation (MED)","15 Aug 2019","2019","","","171","176","Health-aware control (HAC) has emerged as one of the domains where control synthesis is sought based upon the failure prognostics of system/component or the Remaining Useful Life (RUL) predictions of critical components. The fact that mathematical dynamic (transition) models of RUL are rarely available, makes it difficult for RUL information to be incorporated into the control paradigm. A novel framework for health aware control is presented in this paper where reinforcement learning based approach is used to learn an optimal control policy in face of component degradation by integrating global system transition data (generated by an analytical model that mimics the real system) and RUL predictions. The RUL predictions generated at each step, is tracked to a desired value of RUL. The latter is integrated within a cost function which is maximized to learn the optimal control. The proposed method is studied using simulation of a DC motor and shaft wear.","2473-3504","978-1-7281-2803-0","10.1109/MED.2019.8798548","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798548","","Wireless communication;Sensors;Transmitters;Deep learning;Wireless sensor networks","condition monitoring;control system synthesis;learning (artificial intelligence);optimal control;production engineering computing;remaining life assessment;shafts","reinforcement learning approach;health aware control strategy;control synthesis;RUL information;control paradigm;reinforcement learning based approach;optimal control policy;component degradation;global system transition data;RUL predictions;HAC;remaining useful life predictions;shaft wear;DC motor wear","","5","","17","IEEE","15 Aug 2019","","","IEEE","IEEE Conferences"
"Landing A Mobile Robot Safely from Tall Walls Using Manipulator Motion Generated from Reinforcement Learning","C. F. Goh; K. Vazquez-Santiago; K. Shimada","Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA","2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)","8 Oct 2020","2020","","","8","15","A three-tracked-link robot was designed previously for autonomous welding inside double-hulled ship blocks with tight spaces and protruding stiffeners. Bilge blocks, a type of double-hulled blocks, have a tall wall at the entrance. Climbing down from this tall wall involves a risk of toppling as neither of the three links of the robot (front arm, body, and rear arm) is long enough to reach the ground from the wall top, and the robot carries a heavy manipulator for welding. Instead of being a burden, we explore the use of the manipulator motion to shift the center of gravity, helping the robot climbing down safely. In this paper, we proposed the use of reinforcement learning and physics-based computer simulation to determine suitable motion sequences for safe climbing down from a tall wall. We discovered two effective safe-landing modes that use both arms for major balancing acts and a manipulator for balance trimming during the controlled landing. The method also allowed us to explore the effect of other design factors such as the choice of manipulator size, manipulator motion type, and change in environment on the motion sequence.","2161-8089","978-1-7281-6904-0","10.1109/CASE48305.2020.9216977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9216977","","Manipulators;Welding;Robot kinematics;Mobile robots;Learning (artificial intelligence);Tracking","learning (artificial intelligence);manipulators;mechanical stability;mobile robots;motion control;shipbuilding industry;ships","bilge blocks;double-hulled blocks;tall wall;heavy manipulator;reinforcement learning;motion sequences;safe climbing;safe-landing modes;manipulator motion type;mobile robot;three-tracked-link robot;double-hulled ship blocks;tight spaces;protruding stiffeners","","1","","23","IEEE","8 Oct 2020","","","IEEE","IEEE Conferences"
"Accelerating Reinforcement Learning with Local Data Enhancement for Process Control","R. Lin; J. Chen; H. Su; L. Xie","State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; Department of Chemical Engineering, Chung-Yuan Christian University, Taoyuan, ROC; State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","5690","5695","Reinforcement learning (RL) directly achieves the control goal by interacting with the environment, and is of great significance under uncertainties. However, the extremely low sample efficiency of the RL training process has become a practical problem, therefore it is difficult to directly apply the existing RL schemes to industrial control. In this paper, a novel RL control algorithm based on local data enhancement (CBR-MA-DDPG) is proposed to accelerate its training speed, combining the concept of case-based reasoning and model-assisted imagination. When the operating mode changes, the proposed method can quickly adapt to the new mode and achieve the satisfactory control performance in fewer training episodes. A continuous stirred tank reactor (CSTR) process is used to illustrate the effectiveness of our method. The results show that the control performance of the proposed control scheme is much better than the traditional PID control, and that it has higher training efficiency over the state-of-the-art DDPG algorithm.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727937","National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727937","reinforcement learning;process control;data enhancement;case-based reasoning","Training;Uncertainty;Systematics;PI control;Transfer learning;Process control;Reinforcement learning","case-based reasoning;control engineering computing;industrial control;process control;production engineering computing;reinforcement learning;three-term control","industrial control;RL control algorithm;local data enhancement;CBR-MA-DDPG;training speed;case-based reasoning;model-assisted imagination;operating mode changes;satisfactory control performance;training episodes;continuous stirred tank reactor process;control scheme;traditional PID control;training efficiency;state-of-the-art DDPG algorithm;reinforcement learning;process control;sample efficiency;RL training process;RL schemes","","","","20","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Offline Model-Free Reinforcement Learning with a Recurrent Augmented Dataset for Highly Transient Industrial Processes","J. Ruan; U. Inyang-Udohl; B. Nooning; I. Parkes; W. Blejde; G. Chiu; N. Jain","School of Mechanical Engineering, Purdue University, West Lafayette, Indiana, USA; School of Mechanical Engineering, Purdue University, West Lafayette, Indiana, USA; Castrip LLC, Charlotte, North Carolina, USA; Castrip LLC, Charlotte, North Carolina, USA; Castrip LLC, Charlotte, North Carolina, USA; School of Mechanical Engineering, Purdue University, West Lafayette, Indiana, USA; School of Mechanical Engineering, Purdue University, West Lafayette, Indiana, USA","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","7","In industrial processes, human operators commonly supervise predominantly autonomous processes and adjust control setpoints accordingly. However, based on their individual understanding of the process, different human operators will typically apply a different set of rules or logic when choosing setpoints. Although differing control policies may drive the process to a suitable steady-state operating condition, some may be more efficient than others. Moreover, process startup in industrial processes can be highly transient and very difficult to model. Hence, using model-based methods to improve plant performance is not trivial. In this paper, an offline and model-free reinforcement learning algorithm is proposed to evaluate, select, and learn a combined control policy from data generated by multiple human operators. In addition, inspired by the type of recurrent neural network whose recurrence is the previous output, a policy function with previous output recurrence is constructed; based on that, augmented datasets are generated to improve the policy function robustness. A comparison shows that the policy function trained with augmented datasets outperforms the policy function trained without augmented datasets.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260473","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260473","","Training;Casting;Strips;Force;Process control;Reinforcement learning;Trajectory","data analysis;production engineering computing;recurrent neural nets;reinforcement learning","control policy;industrial processes;output recurrence;policy function;process startup;recurrent augmented dataset;recurrent neural network;reinforcement learning","","","","24","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"A Hybrid Cloud and Edge Control Strategy for Demand Responses Using Deep Reinforcement Learning and Transfer Learning","Y. Tao; J. Qiu; S. Lai","School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Cloud Computing","8 Mar 2022","2022","10","1","56","71","A number of electric devices in buildings can be considered as important demand response (DR) resources, for instance, the battery energy storage system (BESS) and the heat, ventilation, and air conditioning (HVAC) systems. The conventional model-based DR methods rely on efficient on-demand computing resources. However, the current buildings suffer from the high cost of computing resources and lack a cost-effective automation system, which becomes the main obstacle to the popularization and implementation of the DR program. Therefore, in this paper, we present a hybrid cloud and edge control strategy for BESS and HVAC based on deep reinforcement learning (DRL). On the cloud infrastructure, the agent learns the control strategy online based on the proposed continuous dueling deep Q-learning (C-DDQN) algorithm, and the learned strategy is distributed to the edge devices for execution. Under this framework, the data-intensive application of cloud computing in real-time DR shows advantages in high processing speed, unlimited data aggregation, fault-tolerant, cost-saving, security, and confidentiality. However, if every controller is trained from the beginning, the cloud resources are wasted to a large extent. Therefore, we propose a transfer deep reinforcement learning methodology to transfer the control strategies between BESS and HVAC units. The transfer learning is realized based on fine-tuning and the proposed Evolving Domain Adaptation Network (EDAN). In case studies, it is verified that the proposed transfer deep reinforcement learning algorithm shows better convergence and learning capability compared with not applying transfer learning technologies. Compared with the conventional model-based method, the proposed methodology speeds up the decision-making time by 105 times.","2168-7161","","10.1109/TCC.2021.3117580","ARC Research Hub(grant numbers:IH180100020); ARC Training Centre(grant numbers:IC200100023); ARC linkage project(grant numbers:LP200100056); Sir William Tyree Foundation-Distributed Power Generation Research Fund; Shenzhen Institute of Artificial Intelligence and Robotics for Society (AIRS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9557846","Demand response;battery energy storage system;heat;ventilation;and air conditioning;hybrid cloud and edge;deep reinforcement learning;transfer learning","Cloud computing;HVAC;Costs;Training;Transfer learning;Buildings;Reinforcement learning","cloud computing;data aggregation;decision making;deep learning (artificial intelligence);HVAC;reinforcement learning","battery energy storage system;BESS;air conditioning systems;HVAC systems;conventional model-based DR methods;on-demand computing resources;cost-effective automation system;hybrid cloud;edge control strategy;cloud infrastructure;continuous dueling deep Q-learning;cloud resources;transfer deep reinforcement learning algorithm;heat, ventilation, and air conditioning system;DRL;C-DDQN algorithm;data-intensive application;cloud computing;data aggregation;fault-tolerant;cost-saving;security;evolving domain adaptation network;EDAN;decision-making","","10","","35","IEEE","4 Oct 2021","","","IEEE","IEEE Journals"
"Asymmetric Self-Play-Enabled Intelligent Heterogeneous Multirobot Catching System Using Deep Multiagent Reinforcement Learning","Y. Gao; J. Chen; X. Chen; C. Wang; J. Hu; F. Deng; T. L. Lam","Shenzhen Institute of Artificial Intelligence and Robotics for Society, The Chinese University of Hong Kong, Shenzhen, China; Shenzhen Institute of Artificial Intelligence and Robotics for Society, The Chinese University of Hong Kong, Shenzhen, China; Department of Automation, Tsinghua University, Beijing, China; University College London, London, U.K.; Shenzhen Institute of Artificial Intelligence and Robotics for Society, The Chinese University of Hong Kong, Shenzhen, China; Shenzhen Institute of Artificial Intelligence and Robotics for Society, The Chinese University of Hong Kong, Shenzhen, China; Shenzhen Institute of Artificial Intelligence and Robotics for Society, The Chinese University of Hong Kong, Shenzhen, China","IEEE Transactions on Robotics","9 Aug 2023","2023","39","4","2603","2622","Aiming to develop a more robust and intelligent heterogeneous system for adversarial catching in security and rescue tasks, in this article, we discuss the specialities of applying asymmetric self-play and curriculum learning techniques to deal with the increasing heterogeneity and number of different robots in modern heterogeneous multirobot systems (HMRS). Our method, based on actor-critic multiagent reinforcement learning, provides a framework that can enable cooperative behaviors among heterogeneous multirobot teams. This leads to the development of an HMRS for complex catching scenarios that involve several robot teams and real-world constraints. We conduct simulated experiments to evaluate different mechanisms' influence on our method's performance, and real-world experiments to assess our system's performance in complex real-world catching problems. In addition, a bridging study is conducted to compare our method with a state-of-the-art method called S2M2 in heterogeneous catching problems, and our method performs better in adversarial settings. As a result, we show that the proposed framework, through fusing asymmetric self-play and curriculum learning during training, is able to successfully complete the HMRS catching task under realistic constraints in both simulation and the real world, thus providing a direction for future large-scale intelligent security & rescue HMRS.","1941-0468","","10.1109/TRO.2023.3257541","National Key R&D Program of China(grant numbers:2020YFB1313300); China Postdoctoral Science Foundation(grant numbers:2022M721838); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2022A1515110787); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10101687","Asymmetric self-play;catching systems;heterogeneous multirobot system (HMRS);reinforcement learning (RL)","Robots;Task analysis;Training;Planning;Multi-robot systems;Mathematical models;Heuristic algorithms","deep learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;reinforcement learning","actor-critic multiagent reinforcement learning;adversarial catching;asymmetric self-play-enabled intelligent heterogeneous multirobot catching system;complex catching scenarios;curriculum learning techniques;deep multiagent reinforcement learning;heterogeneous catching problems;heterogeneous multirobot teams;HMRS catching task;intelligent heterogeneous system;large-scale intelligent security & rescue HMRS;modern heterogeneous multirobot systems;real-world catching problems;real-world experiments;robot teams;robust system;S2M2","","","","80","CCBY","12 Apr 2023","","","IEEE","IEEE Journals"
"Intelligent Energy-Efficient Train Trajectory Optimization Approach Based on Supervised Reinforcement Learning for Urban Rail Transits","G. Li; S. W. Or; K. W. Chan","Department of Electrical Engineering, The Hong Kong Polytechnic University, Hong Kong, China; Department of Electrical Engineering, The Hong Kong Polytechnic University, Hong Kong, China; Department of Electrical Engineering, The Hong Kong Polytechnic University, Hong Kong, China","IEEE Access","3 Apr 2023","2023","11","","31508","31521","Artificial intelligence of things (AIoT)-enabled intelligent automatic train operation (iATO) is an urgently needed technology to expand the capability of ATO in addressing the real-time responsiveness and dynamic online challenges to energy-efficient train trajectory optimization (TTO) and its associated ride-comfort, punctuality, and safety issues in modern urban rail transit networks. This paper proposes a three-step supervised reinforcement learning-based intelligent energy-efficient train trajectory optimization (SRL-IETTO) approach for iATO by hybrid-integrating deep reinforcement learning (DRL) and supervised learning. First, multiple objectives are formulated based on real-time train operation and systematically integrated into the RL algorithm by a binary function-based goal-directed reward design method. Second, an IETTO model is established to handle uncertain disturbances in real-time train operation and generate optimal energy-efficient train trajectories online by optimizing energy efficiency and receiving supervisory information from trajectories of pre-trained TTO models. Finally, numerical simulations are implemented to validate the effectiveness of the SRL-IETTO using in-service subway line data. The results demonstrate the superiority and improved energy saving of the proposed approach and confirm its adaptability to online trip time adjustments within the practical running time range under uncertain disturbances with less trip time error compared to other intelligent TTO algorithms.","2169-3536","","10.1109/ACCESS.2023.3261900","Research Grants Council of the Hong Kong Special Administrative Region (HKSAR) Government(grant numbers:R5020-18); Innovation and Technology Commission of the HKSAR Government to the Hong Kong Branch of National Rail Transit Electrification and Automation Engineering Technology Research Center(grant numbers:K-BBY1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10081361","Deep reinforcement learning;energy-efficient train trajectory optimization;intelligent automatic train operation;supervised reinforcement learning;urban rail transits","Real-time systems;Energy efficiency;Trajectory optimization;Reinforcement learning;Deep learning;Delays;Safety;Railway transportation","deep learning (artificial intelligence);energy conservation;Internet of Things;optimisation;railway engineering;reinforcement learning;supervised learning","artificial intelligence;binary function-based goal-directed reward design method;energy efficiency;IETTO model;improved energy saving;in-service subway line data;intelligent energy-efficient train trajectory optimization approach;intelligent TTO algorithms;Internet of things-enabled intelligent automatic train operation;modern urban rail transit networks;online trip time adjustments;optimal energy-efficient train trajectories;pretrained TTO models;real-time train operation;receiving supervisory information;SRL-IETTO;supervised learning;three-step supervised reinforcement learning-based intelligent energy-efficient train trajectory optimization;urban rail transits","","","","44","CCBYNCND","27 Mar 2023","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Robot for Search and Rescue Applications: Exploration in Unknown Cluttered Environments","F. Niroui; K. Zhang; Z. Kashino; G. Nejat","Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada","IEEE Robotics and Automation Letters","28 Jan 2019","2019","4","2","610","617","Rescue robots can be used in urban search and rescue (USAR) applications to perform the important task of exploring unknown cluttered environments. Due to the unpredictable nature of these environments, deep learning techniques can be used to perform these tasks. In this letter, we present the first use of deep learning to address the robot exploration task in USAR applications. In particular, we uniquely combine the traditional approach of frontier-based exploration with deep reinforcement learning to allow a robot to autonomously explore unknown cluttered environments. Experiments conducted with a mobile robot in unknown cluttered environments of varying sizes and layouts showed that the proposed exploration approach can effectively determine appropriate frontier locations to navigate to, while being robust to different environment layouts and sizes. Furthermore, a comparison study with other frontier exploration approaches showed that our learning-based frontier exploration technique was able to explore more of an environment earlier on, allowing for potential identification of a larger number of victims at the beginning of the time-critical exploration task.","2377-3766","","10.1109/LRA.2019.2891991","Natural Sciences and Engineering Council of Canada; Canada Research Chairs Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606991","Autonomous agents;deep learning in robotics and automation;search and rescue robots","Computer architecture;Robot sensing systems;Microprocessors;Task analysis;Navigation;Layout","learning (artificial intelligence);path planning;rescue robots","urban search and rescue applications;deep reinforcement learning robot;mobile robot;USAR applications;robot exploration task;rescue robots;unknown cluttered environments;time-critical exploration task;learning-based frontier exploration technique","","169","","32","IEEE","10 Jan 2019","","","IEEE","IEEE Journals"
"DeepGait: Planning and Control of Quadrupedal Gaits Using Deep Reinforcement Learning","V. Tsounis; M. Alge; J. Lee; F. Farshidian; M. Hutter","Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland; Robotic Systems Lab, ETH Zürich, Zürich, Switzerland","IEEE Robotics and Automation Letters","1 Apr 2020","2020","5","2","3699","3706","This letter addresses the problem of legged locomotion in non-flat terrain. As legged robots such as quadrupeds are to be deployed in terrains with geometries which are difficult to model and predict, the need arises to equip them with the capability to generalize well to unforeseen situations. In this work, we propose a novel technique for training neural-network policies for terrain-aware locomotion, which combines state-of-the-art methods for model-based motion planning and reinforcement learning. Our approach is centered on formulating Markov decision processes using the evaluation of dynamic feasibility criteria in place of physical simulation. We thus employ policy-gradient methods to independently train policies which respectively plan and execute foothold and base motions in 3D environments using both proprioceptive and exteroceptive measurements. We apply our method within a challenging suite of simulated terrain scenarios which contain features such as narrow bridges, gaps and stepping-stones, and train policies which succeed in locomoting effectively in all cases.","2377-3766","","10.1109/LRA.2020.2979660","Intel Labs; Swiss National Science Foundation(grant numbers:166232,188596); National Centre of Competence in Research Robotics; European Union's Horizon 2020 research and innovation program(grant numbers:780883); ANYmal Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9028188","Legged robots;deep learning in robotics and automation;motion and path planning","Deep learning;Motion planning;Legged locomotion;Optimization;Path planning","control engineering computing;decision theory;gradient methods;learning (artificial intelligence);legged locomotion;Markov processes;neural nets;path planning","DeepGait;quadrupedal gaits;deep reinforcement learning;legged locomotion;nonflat terrain;legged robots;terrain-aware locomotion;model-based motion planning;Markov decision processes;dynamic feasibility criteria;policy-gradient methods;model-based motion planning;exteroceptive measurements;proprioceptive measurements","","89","","19","IEEE","9 Mar 2020","","","IEEE","IEEE Journals"
"High-Speed Autonomous Drifting With Deep Reinforcement Learning","P. Cai; X. Mei; L. Tai; Y. Sun; M. Liu","The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China; A. I. Lab, Alibaba Group, Hangzhou, China; The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China","IEEE Robotics and Automation Letters","3 Feb 2020","2020","5","2","1247","1254","Drifting is a complicated task for autonomous vehicle control. Most traditional methods in this area are based on motion equations derived by the understanding of vehicle dynamics, which is difficult to be modeled precisely. We propose a robust drift controller without explicit motion equations, which is based on the latest model-free deep reinforcement learning algorithm soft actor-critic. The drift control problem is formulated as a trajectory following task, where the error-based state and reward are designed. After being trained on tracks with different levels of difficulty, our controller is capable of making the vehicle drift through various sharp corners quickly and stably in the unseen map. The proposed controller is further shown to have excellent generalization ability, which can directly handle unseen vehicle types with different physical properties, such as mass, tire friction, etc.","2377-3766","","10.1109/LRA.2020.2967299","National Natural Science Foundation of China(grant numbers:U1713211); Research Grant Council of Hong Kong SAR Government, China(grant numbers:11210017,21202816); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8961997","Deep learning in robotics and automation;field robots;motion control;deep reinforcement learning;racing car","Trajectory;Mathematical model;Tires;Vehicle dynamics;Reinforcement learning;Automobiles;Task analysis","automobiles;generalisation (artificial intelligence);learning (artificial intelligence);motion control;robust control;vehicle dynamics","robust drift controller;explicit motion equations;actor-critic;drift control problem;trajectory following task;error-based state;vehicle drift;unseen vehicle types;vehicle dynamics;autonomous vehicle control;deep reinforcement learning;high-speed autonomous drifting","","61","","23","IEEE","17 Jan 2020","","","IEEE","IEEE Journals"
"A Two-Stage Reinforcement Learning Approach for Multi-UAV Collision Avoidance Under Imperfect Sensing","D. Wang; T. Fan; T. Han; J. Pan","Department of Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Science, The University of Hong Kong, Hong Kong, China; Department of Biomedical Engineering, City University of Hong Kong, Hong Kong, China; Department of Computer Science, The University of Hong Kong, Hong Kong, China","IEEE Robotics and Automation Letters","4 Mar 2020","2020","5","2","3098","3105","Unlike autonomous ground vehicles (AGVs), unmanned aerial vehicles (UAVs) have a higher dimensional configuration space, which makes the motion planning of multi-UAVs a challenging task. In addition, uncertainties and noises are more significant in UAV scenarios, which increases the difficulty of autonomous navigation for multi-UAV. In this letter, we proposed a two-stage reinforcement learning (RL) based multi-UAV collision avoidance approach without explicitly modeling the uncertainty and noise in the environment. Our goal is to train a policy to plan a collision-free trajectory by leveraging local noisy observations. However, the reinforcement learned collision avoidance policies usually suffer from high variance and low reproducibility, because unlike supervised learning, RL does not have a fixed training set with ground-truth labels. To address these issues, we introduced a two-stage training method for RL based collision avoidance. For the first stage, we optimize the policy using a supervised training method with a loss function that encourages the agent to follow the well-known reciprocal collision avoidance strategy. For the second stage, we use policy gradient to refine the policy. We validate our policy in a variety of simulated scenarios, and the extensive numerical simulations demonstrate that our policy can generate time-efficient and collision-free paths under imperfect sensing, and can well handle noisy local observations with unknown noise levels.","2377-3766","","10.1109/LRA.2020.2974648","HKSAR General Research Fund(grant numbers:HKU 11202119,11207818); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9001167","Collision avoidance;deep learning in robotics and automation","Collision avoidance;Robot sensing systems;Learning (artificial intelligence);Training;Navigation","aerospace navigation;autonomous aerial vehicles;collision avoidance;learning (artificial intelligence);multi-robot systems;numerical analysis;robot programming","loss function;noisy local observations;unknown noise levels;extensive numerical simulations;policy gradient;local noisy observations;autonomous navigation;motion planning;collision-free paths;reciprocal collision avoidance strategy;supervised training method;RL based collision avoidance;two-stage training method;supervised learning;collision-free trajectory;two-stage reinforcement learning based multiUAV collision avoidance approach;higher dimensional configuration space;unmanned aerial vehicles;imperfect sensing","","56","","33","IEEE","18 Feb 2020","","","IEEE","IEEE Journals"
"Visual Navigation for Biped Humanoid Robots Using Deep Reinforcement Learning","K. Lobos-Tsunekawa; F. Leiva; J. Ruiz-del-Solar","Department of Electrical Engineering and Advanced Mining Technology Center, Universidad de Chile, Santiago, Chile; Department of Electrical Engineering and Advanced Mining Technology Center, Universidad de Chile, Santiago, Chile; Department of Electrical Engineering and Advanced Mining Technology Center, Universidad de Chile, Santiago, Chile","IEEE Robotics and Automation Letters","19 Jul 2018","2018","3","4","3247","3254","In this letter, we propose a map-less visual navigation system for biped humanoid robots, which extracts information from color images to derive motion commands using deep reinforcement learning (DRL). The map-less visual navigation policy is trained using the Deep Deterministic Policy Gradients (DDPG) algorithm, which corresponds to an actor-critic DRL algorithm. The algorithm is implemented using two separate networks, one for the actor and one for the critic, but with similar structures. In addition to convolutional and fully connected layers, Long Short-Term Memory (LSTM) layers are included to address the limited observability present in the problem. As a proof of concept, we consider the case of robotic soccer using humanoid NAO V5 robots, which have reduced computational capabilities, and low-cost Red - Green - Blue (RGB) cameras as main sensors. The use of DRL allowed to obtain a complex and high performant policy from scratch, without any prior knowledge of the domain, or the dynamics involved. The visual navigation policy is trained in a robotic simulator and then successfully transferred to a physical robot, where it is able to run in 20 ms, allowing its use in real-time applications.","2377-3766","","10.1109/LRA.2018.2851148","Fondo Nacional de Desarrollo Científico y Tecnológico(grant numbers:1161500); Fondo Nacional de Desarrollo Científico y Tecnológico(grant numbers:1161500); CONICYT-PFCHA/Magíster Nacional/2018-22182130; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8398461","Visual-based navigation;deep learning in robotics and automation and humanoid robots","Navigation;Visualization;Task analysis;Robot kinematics;Robot sensing systems","humanoid robots;image colour analysis;learning systems;legged locomotion;multi-robot systems;path planning;robot vision","Long Short-Term Memory layers;robotic soccer;humanoid NAO V5 robots;high performant policy;visual navigation policy;robotic simulator;physical robot;biped humanoid robots;deep reinforcement learning;color images;Deep Deterministic Policy Gradients algorithm;actor-critic DRL algorithm;convolutional connected layers;fully connected layers;motion commands;low-cost Red-Green-Blue cameras;real-time applications;time 20.0 ms","","50","","28","IEEE","27 Jun 2018","","","IEEE","IEEE Journals"
"Robot-Assisted Training in Laparoscopy Using Deep Reinforcement Learning","X. Tan; C. -B. Chng; Y. Su; K. -B. Lim; C. -K. Chui","Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore","IEEE Robotics and Automation Letters","17 Jan 2019","2019","4","2","485","492","Minimally invasive surgery (MIS) is increasingly becoming a vital method of reducing surgical trauma and significantly improving postoperative recovery. However, skillful handling of surgical instruments used in MIS, especially for laparoscopy, requires a long period of training and depends highly on the experience of surgeons. This letter presents a new robot-assisted surgical training system which is designed to improve the practical skills of surgeons through intrapractice feedback and demonstration from both human experts and reinforcement learning (RL) agents. This system utilizes proximal policy optimization to learn the control policy in simulation. Subsequently, a generative adversarial imitation learning agent is trained based on both expert demonstrations and learned policies in simulation. This agent then generates demonstration policies on the robot-assisted device for trainees and produces feedback scores during practice. To further acquire surgical tools coordinates and encourage self-oriented practice, a mask region-based convolution neural network is trained to perform the semantic segmentation of surgical tools and targets. To the best of our knowledge, this system is the first robot-assisted laparoscopy training system which utilizes actual surgical tools and leverages deep reinforcement learning to provide demonstration training from both human expert perspectives and RL criterion.","2377-3766","","10.1109/LRA.2019.2891311","Ministry of Education Academic Research Fund Tier 1(grant numbers:R265-000-614-114); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8604070","Surgical robotics;laparoscopy;learning from demonstration;deep learning in robotics and automation;AI-based methods","Training;Laparoscopes;Tools;Trajectory;Robot kinematics;Task analysis","biomedical education;computer based training;feedforward neural nets;image segmentation;learning (artificial intelligence);medical robotics;surgery","human experts;proximal policy optimization;control policy;generative adversarial imitation learning agent;expert demonstrations;learned policies;demonstration policies;robot-assisted device;feedback scores;surgical tools coordinates;self-oriented practice;mask region-based convolution neural network;robot-assisted laparoscopy training system;actual surgical tools;leverages deep reinforcement learning;demonstration training;human expert perspectives;robot-assisted training;minimally invasive surgery;MIS;vital method;surgical trauma;postoperative recovery;skillful handling;surgical instruments;robot-assisted surgical training system;intrapractice feedback","","33","","29","IEEE","6 Jan 2019","","","IEEE","IEEE Journals"
"A Sim-to-Real Pipeline for Deep Reinforcement Learning for Autonomous Robot Navigation in Cluttered Rough Terrain","H. Hu; K. Zhang; A. H. Tan; M. Ruan; C. Agia; G. Nejat","Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada","IEEE Robotics and Automation Letters","21 Jul 2021","2021","6","4","6569","6576","Robots that autonomously navigate real-world 3D cluttered environments need to safely traverse terrain with abrupt changes in surface normals and elevations. In this letter, we present the development of a novel sim-to-real pipeline for a mobile robot to effectively learn how to navigate real-world 3D rough terrain environments. The pipeline uses a deep reinforcement learning architecture to learn a navigation policy from training data obtained from the simulated environment and a unique combination of strategies to directly address the reality gap for such environments. Experiments in the real-world 3D cluttered environment verified that the robot successfully performed point-to-point navigation from arbitrary start and goal locations while traversing rough terrain. A comparison study between our DRL method, classical, and deep learning-based approaches showed that our method performed better in terms of success rate, and cumulative travel distance and time in a 3D rough terrain environment.","2377-3766","","10.1109/LRA.2021.3093551","Natural Sciences and Engineering Research Council of Canada; Canada Research Chairs; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468918","Autonomous agents;deep learning for robotics and automation;search and rescue robots","Robots;Three-dimensional displays;Navigation;Pipelines;Mobile robots;Training;Surface reconstruction","control engineering computing;deep learning (artificial intelligence);mobile robots;path planning","deep reinforcement learning architecture;autonomous robot navigation;cluttered rough terrain;real-world 3D cluttered environment;mobile robot;navigation policy;simulated environment;point-to-point navigation;deep learning-based approaches;sim-to-real pipeline","","27","","38","IEEE","30 Jun 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning for POMDP: Partitioned Rollout and Policy Iteration With Application to Autonomous Sequential Repair Problems","S. Bhattacharya; S. Badyal; T. Wheeler; S. Gil; D. Bertsekas","REACT Lab, Arizona State University, Tempe, USA; REACT Lab, Arizona State University, Tempe, USA; REACT Lab, Arizona State University, Tempe, USA; REACT Lab, Arizona State University, Tempe, USA; McAfee Professor of Engineering, Massachusetts Institute of Technology, Cambridge, USA","IEEE Robotics and Automation Letters","22 Apr 2020","2020","5","3","3967","3974","In this letter we consider infinite horizon discounted dynamic programming problems with finite state and control spaces, and partial state observations. We discuss an algorithm that uses multistep lookahead, truncated rollout with a known base policy, and a terminal cost function approximation. This algorithm is also used for policy improvement in an approximate policy iteration scheme, where successive policies are approximated by using a neural network classifier. A novel feature of our approach is that it is well suited for distributed computation through an extended belief space formulation and the use of a partitioned architecture, which is trained with multiple neural networks. We apply our methods in simulation to a class of sequential repair problems where a robot inspects and repairs a pipeline with potentially several rupture sites under partial information about the state of the pipeline.","2377-3766","","10.1109/LRA.2020.2978451","National Science Foundation CAREER Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9024010","Optimization and optimal control;distributed robot systems;autonomous agents;search and rescue robots;deep learning in robotics and automation","Cost function;Robots;Approximation algorithms;Maintenance engineering;Aerospace electronics;Pipelines;Computer architecture","decision theory;dynamic programming;function approximation;infinite horizon;iterative methods;learning (artificial intelligence);Markov processes;neural nets","autonomous sequential repair problems;infinite horizon;dynamic programming problems;finite state;control spaces;partial state observations;multistep lookahead;truncated rollout;known base policy;terminal cost function approximation;policy improvement;neural network classifier;distributed computation;extended belief space formulation;partitioned architecture;multiple neural networks;partial information;reinforcement learning;POMDP;partitioned rollout","","22","","43","IEEE","4 Mar 2020","","","IEEE","IEEE Journals"
"Motor Synergy Development in High-Performing Deep Reinforcement Learning Algorithms","J. Chai; M. Hayashibe","Neuro-Robotics Lab, Department of Robotics, Graduate School of Engineering, Tohoku University, Sendai, Japan; Neuro-Robotics Lab, Department of Robotics, Graduate School of Engineering, Tohoku University, Sendai, Japan","IEEE Robotics and Automation Letters","30 Jan 2020","2020","5","2","1271","1278","As human motor learning is hypothesized to use the motor synergy concept, we investigate if this concept could also be observed in deep reinforcement learning for robotics. From this point of view, we carried out a joint-space synergy analysis on multi-joint running agents in simulated environments trained using two state-of-the-art deep reinforcement learning algorithms. Although a synergy constraint has never been encoded into the reward function, the synergy emergence phenomenon could be observed statistically in the learning agent. To our knowledge, this is the first attempt to quantify the synergy development in detail and evaluate its emergence process during deep learning motor control tasks. We then demonstrate that there is a correlation between our synergy-related metrics and the performance and energy efficiency of a trained agent. Interestingly, the proposed synergy-related metrics reflected a better learning capability of SAC over TD3. It suggests that these metrics could be additional new indices to evaluate deep reinforcement learning algorithms for motor learning. It also indicates that synergy is required for multi-joints robots to move energy-efficiently.","2377-3766","","10.1109/LRA.2020.2968067","JSPS Grant-in-Aid for Scientific Research (B)(grant numbers:18H01399); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8966298","Deep learning in robotics and automation;performance evaluation and benchmarking;motor synergy","Robot kinematics;Reinforcement learning;Measurement;Spatiotemporal phenomena;Mathematical model;Task analysis","learning (artificial intelligence)","synergy-related metrics;high-performing deep reinforcement learning algorithms;deep learning motor control tasks;emergence process;learning agent;synergy emergence phenomenon;synergy constraint;deep reinforcement learning algorithms;simulated environments;multijoint running agents;joint-space synergy analysis;motor synergy concept;human motor learning;motor synergy development;multijoints robots;learning capability;trained agent;energy efficiency","","19","","24","IEEE","22 Jan 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Instruction Following Visual Navigation in 3D Maze-Like Environments","A. Devo; G. Costante; P. Valigi","Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy","IEEE Robotics and Automation Letters","30 Jan 2020","2020","5","2","1175","1182","In this work, we address the problem of visual navigation by following instructions. In this task, the robot must interpret a natural language instruction in order to follow a predefined path in a possibly unknown environment. Despite different approaches have been proposed in the last years, they are all based on the assumption that the environment contains objects or other elements that can be used to formulate instructions, such as houses or offices. On the contrary, we focus on situations where the environment objects cannot be used to specify a navigation path. In particular, we consider 3D maze-like environments as our test bench because they can be very large and offer very intricate structures. We show that without reference points, visual navigation and instruction following can be rather challenging, and that standard approaches can not be applied successfully. For this reason, we propose a new architecture that explicitly learns both visual navigation and instruction understanding. We demonstrate with simulated experiments that our method can effectively follow instructions and navigate in previously unseen mazes of various sizes.","2377-3766","","10.1109/LRA.2020.2965857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8957297","Deep learning in robotics and automation;visual-based navigation;visual learning","Navigation;Visualization;Task analysis;Robots;Natural languages;Three-dimensional displays;Computer architecture","learning (artificial intelligence);mobile robots;path planning;robot vision","visual navigation;3D maze-like environments;natural language instruction;unknown environment;environment objects;navigation path;instruction understanding","","19","","34","IEEE","14 Jan 2020","","","IEEE","IEEE Journals"
"DeepIG: Multi-Robot Information Gathering With Deep Reinforcement Learning","A. Viseras; R. Garcia","Institute of Communications and Navigation of the German Aerospace Center (DLR), Oberpfaffenhofen, Germany; E.T.S.I. Telecomunicación, Universidad Politécnica de Madrid, Madrid, Spain","IEEE Robotics and Automation Letters","3 Jul 2019","2019","4","3","3059","3066","State-of-the-art multi-robot information gathering (MR-IG) algorithms often rely on a model that describes the structure of the information of interest to drive the robots motion. This causes MR-IG algorithms to fail when they are applied to new IG tasks, as existing models cannot describe the information of interest. Therefore, we propose in this letter a MR-IG algorithm that can be applied to new IG tasks with little algorithmic changes. To this end, we introduce DeepIG: a MR-IG algorithm that uses deep reinforcement learning to allow robots to learn how to gather information. Nevertheless, there are IG tasks for which accurate models have been derived. Therefore, we extend DeepIG to exploit existing models for such IG tasks. This algorithm we term it model-based DeepIG (MB-DeepIG). First, we evaluate DeepIG in simulations, and in an indoor experiment with three quadcopters that autonomously map an unknown terrain profile built in our lab. Results demonstrate that DeepIG can be applied to different IG tasks without algorithmic changes, and that it is robust to measurement noise. Then, we benchmark MB-DeepIG against state-of-theart information-driven Gaussian-processes-based IG algorithms. Results demonstrate that MB-DeepIG outperforms the considered benchmarks.","2377-3766","","10.1109/LRA.2019.2924839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744599","Multi-robot systems;deep learning in robotics and automation","Robot sensing systems;Task analysis;Robot kinematics;Collision avoidance;Reinforcement learning;Monitoring","Gaussian processes;helicopters;indoor environment;learning (artificial intelligence);mobile robots;motion control;multi-robot systems","deep reinforcement learning;robots motion;MR-IG algorithm;multi-robot information gathering algorithms;quadcopters;IG tasks;MB-DeepIG;information-driven Gaussian-processes-based IG algorithms","","17","","14","IEEE","24 Jun 2019","","","IEEE","IEEE Journals"
"Planning Approximate Exploration Trajectories for Model-Free Reinforcement Learning in Contact-Rich Manipulation","S. Hoppe; Z. Lou; D. Hennes; M. Toussaint","Bosch Corporate Research, Stuttgart, Germany; Bosch Corporate Research, Stuttgart, Germany; Machine Learning and Robotics Lab, Google Deepmind, Stuttgart, Germany; Machine Learning and Robotics Lab, University of Stuttgart, Stuttgart, Germany","IEEE Robotics and Automation Letters","8 Aug 2019","2019","4","4","4042","4047","Recent progress in deep reinforcement learning has enabled simulated agents to learn complex behavior policies from scratch, but their data complexity often prohibits real-world applications. The learning process can be sped up by expert demonstrations but those can be costly to acquire. We demonstrate that it is possible to employ model-free deep reinforcement learning combined with planning to quickly generate informative data for a manipulation task. In particular, we use an approximate trajectory optimization approach for global exploration based on an upper confidence bound of the advantage function. The advantage is approximated by a network for Q-learning with separately updated streams for state value and advantage that allows ensembles to approximate model uncertainty for one stream only. We evaluate our method on new extensions to the classical peg-in-hole task, one of which is only solvable by active usage of contacts between peg tips and holes. The experimental evaluation suggests that our method explores more relevant areas of the environment and finds exemplar solutions faster-both on a real robot and in simulation. Combining our exploration with learning from demonstration outperforms state-of-the-art model-free reinforcement learning in terms of convergence speed for contact-rich manipulation tasks.","2377-3766","","10.1109/LRA.2019.2928212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8760436","Deep learning in robotics and automation;dexterous manipulation;learning and adaptive systems","Reinforcement learning;Task analysis;Robots;Uncertainty;Data models;Trajectory optimization","learning (artificial intelligence);path planning","peg-in-hole task;peg tips;contact-rich manipulation tasks;planning approximate exploration trajectories;simulated agents;complex behavior policies;data complexity;real-world applications;learning process;model-free deep reinforcement learning;informative data;manipulation task;approximate trajectory optimization approach;global exploration;separately updated streams;approximate model uncertainty","","15","","28","IEEE","12 Jul 2019","","","IEEE","IEEE Journals"
"Control of Rough Terrain Vehicles Using Deep Reinforcement Learning","V. Wiberg; E. Wallin; T. Nordfjell; M. Servin","Department of Physics, Umeå University, Umeå, Sweden; Department of Physics, Umeå University, Umeå, Sweden; Swedish University of Agricultural Sciences, Uppsala, Sweden; Department of Physics, Umeå University, Umeå, Sweden","IEEE Robotics and Automation Letters","24 Nov 2021","2022","7","1","390","397","We explore the potential to control terrain vehicles using deep reinforcement in scenarios where human operators and traditional control methods are inadequate. This letter presents a controller that perceives, plans, and successfully controls a 16-tonne forestry vehicle with two frame articulation joints, six wheels, and their actively articulated suspensions to traverse rough terrain. The carefully shaped reward signal promotes safe, environmental, and efficient driving, which leads to the emergence of unprecedented driving skills. We test learned skills in a virtual environment, including terrains reconstructed from high-density laser scans of forest sites. The controller displays the ability to handle obstructing obstacles, slopes up to 27$^\circ$, and a variety of natural terrains, all with limited wheel slip, smooth, and upright traversal with intelligent use of the active suspensions. The results confirm that deep reinforcement learning has the potential to enhance control of vehicles with complex dynamics and high-dimensional observation data compared to human operators or traditional control methods, especially in rough terrain.","2377-3766","","10.1109/LRA.2021.3126904","Mistra Digital Forest(grant numbers:DIA 2017/14 #6); Algoryx Simulation AB; Swedish National Infrastructure for Computing(grant numbers:2021/5-234); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9611016","Deep learning methods;reinforcement learning;autonomous vehicle navigation;model learning for control;robotics and automation in agriculture and forestry","Wheels;Forestry;Joints;Reinforcement learning;Solid modeling;Suspensions (mechanical systems);Computational modeling","forestry;learning (artificial intelligence);mobile robots;remotely operated vehicles;suspensions (mechanical components);vehicles;wheels","unprecedented driving skills;efficient driving;carefully shaped reward signal;actively articulated suspensions;frame articulation joints;16-tonne forestry vehicle;control terrain vehicles;rough terrain vehicles;traditional control methods;human operators;deep reinforcement learning;active suspensions;natural terrains;high-density laser scans","","8","","23","CCBYNCND","10 Nov 2021","","","IEEE","IEEE Journals"
"Binarized P-Network: Deep Reinforcement Learning of Robot Control from Raw Images on FPGA","Y. Kadokawa; Y. Tsurumine; T. Matsubara","Division of Information Science, Graduate School of Science and Technology, Nara Institute of Science and Technology, Ikoma-shi, Japan; Division of Information Science, Graduate School of Science and Technology, Nara Institute of Science and Technology, Ikoma-shi, Japan; Division of Information Science, Graduate School of Science and Technology, Nara Institute of Science and Technology, Ikoma-shi, Japan","IEEE Robotics and Automation Letters","22 Sep 2021","2021","6","4","8545","8552","This letter explores a deep reinforcement learning (DRL) approach for designing image-based control for edge robots to be implemented on Field Programmable Gate Arrays (FPGAs). Although FPGAs are more power-efficient than CPUs and GPUs, a typical DRL method cannot be applied since they are composed of many Logic Blocks (LBs) for high-speed logical operations but low-speed real-number operations. To cope with this problem, we propose a novel DRL algorithm called Binarized P-Network (BPN), which learns image-input control policies using Binarized Convolutional Neural Networks (BCNNs). To alleviate the instability of reinforcement learning caused by a BCNN with low function approximation accuracy, our BPN adopts a robust value update scheme called Conservative Value Iteration, which is tolerant of function approximation errors. We confirmed the BPN's effectiveness through applications to a visual tracking task in simulation and real-robot experiments with FPGA.","2377-3766","","10.1109/LRA.2021.3111416","JSPS KAKENHI(grant numbers:JP21H03522); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9534708","Reinforcement learning;embedded systems for robotic and automation;hardware-software integration in robotics","Field programmable gate arrays;Robots;Function approximation;Servers;Reinforcement learning;Real-time systems;Task analysis","approximation theory;convolutional neural nets;deep learning (artificial intelligence);field programmable gate arrays;robot vision","logic blocks;high-speed logical operations;DRL algorithm;BPN;image-input control policies;binarized convolutional neural networks;low function approximation accuracy;robust value update scheme;FPGA;deep reinforcement learning approach;image-based control;edge robots;field programmable gate arrays;binarized P-network;BCNN","","1","","24","IEEE","9 Sep 2021","","","IEEE","IEEE Journals"
"Model-Free Control for Dynamic-Field Acoustic Manipulation Using Reinforcement Learning","K. Latifi; A. Kopitca; Q. Zhou","Department of Electrical Engineering and Automation, Aalto University, Espoo, Finland; Department of Electrical Engineering and Automation, Aalto University, Espoo, Finland; Department of Electrical Engineering and Automation, Aalto University, Espoo, Finland","IEEE Access","3 Feb 2020","2020","8","","20597","20606","Dynamic-field acoustic manipulation techniques benefit numerous applications in microsystem assembly, pattern formation, biological research, tissue engineering, and lab-on-a-chip. These techniques generally rely on a theoretical dynamic model of particle motion in the acoustic field. Accordingly, success of the manipulation task highly depends on the accuracy of the employed dynamic model. However, modelling such dynamic behavior is a great challenge in more advanced acoustic manipulation devices and requires significant simplifications. Here, we introduce a model-free control method based on reinforcement learning for highly-dynamic acoustic manipulation devices. In our method, the controller does not need a prior knowledge of the acoustic field and learns the optimal control policy for each manipulation task by merely interacting with the acoustic field. As a proof-of-concept, we apply our method to a classic acoustic manipulation device, a Chladni plate consisting of a centrally-actuated vibrating plate. By employing the controller, we demonstrate successful manipulation of single and multiple particles towards target locations on the plate surface. The model-free control method is not limited to the Chladni plate and can be potentially applied to a broad range of acoustic manipulation devices as well as other forms of field-based micromanipulation systems, where accurate theoretical modelling of the field is challenging.","2169-3536","","10.1109/ACCESS.2020.2969277","Academy of Finland(grant numbers:296250,328239); Sähkötekniikan Korkeakoulu, Aalto-yliopisto; FinELib consortium; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968432","Acoustic manipulation;Chladni plate;dynamic-filed acoustic device;model-free control;real-time control;reinforcement learning","Acoustics;Dynamics;Manipulator dynamics;Force;Resonant frequency;Task analysis;Reinforcement learning","acoustic field;learning (artificial intelligence);micromanipulators;optimal control;plates (structures);vibrations","highly-dynamic acoustic manipulation devices;acoustic field;optimal control policy;Chladni plate;model-free control method;field-based micromanipulation systems;reinforcement learning;dynamic-field acoustic manipulation techniques;dynamic behavior;acoustic manipulation devices","","8","","32","CCBY","24 Jan 2020","","","IEEE","IEEE Journals"
"An Approach to the Development of a Game Agent Based on SOM and Reinforcement Learning","K. Kamei; Y. Kakizoe","Department of Production Systems, Nishinippon Institute of Technology, Fukuoka, Japan; Department of Production Systems, Nishinippon Institute of Technology, Fukuoka, Japan","2016 5th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI)","1 Sep 2016","2016","","","669","674","Recently, the researches that create agents which play board games have been studied actively. According to those studies, those agents have abilities that are comparable to the strongest experts. However, it can be said that those agents depend on the computational capability because that abilities of those agents are realized by thousands of lookahead search. On theotherhand, humanbeingshavenoadvantagescomparedwith numerical capability of computers, however, experts sometimes defeat those agents. In contrast to other approaches, our purpose is to create the agent which requires only low computational capability but is strong, like human beings. To realize our aim, we have proposed to develop the agent based on Self-Organizing Maps and reinforcement learning. From the experimental results, the agent learned by MC-learning achieved a 58% winning rate against the adversary program, so that we have succeeded in improving the winning rate over 10%.","","978-1-4673-8985-3","10.1109/IIAI-AAI.2016.115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7557696","Game agent;Self-Organizing Maps;Reinforcement learning;Reversi (Othello) game","Neurons;Games;Proposals;Genetic algorithms;Learning (artificial intelligence);Gaussian distribution;Statistical analysis","computer games;learning (artificial intelligence);multi-agent systems;self-organising feature maps","game agent development;SOM;reinforcement learning;board game playing;computational capability;lookahead search;self-organizing maps;MC-learning;winning rate;adversary program","","1","","9","IEEE","1 Sep 2016","","","IEEE","IEEE Conferences"
"Developing Smart Lane-changing Strategies for CAVs on Freeways based on MOBIL and Reinforcement Learning","Y. Ma; L. Wang; Y. Wang; J. Guo; L. Zhang; S. Hu; I. Papamichail; M. Papageorgiou","Institute of Intelligent Transportation Systems, Zhejiang University, Hangzhou, China; Institute of Intelligent Transportation Systems, Zhejiang University, Hangzhou, China; Institute of Intelligent Transportation Systems, Zhejiang University, Hangzhou, China; Key Laboratory of Road and Traffic Engineering of the Ministry of Education (China), Tongji University, Shanghai, China; Institute of Intelligent Transportation Systems, Zhejiang University, Hangzhou, China; Institute of Intelligent Transportation Systems, Zhejiang University, Hangzhou, China; Dynamic Systems and Simulation Laboratory, School of Production Engineering and Management, Technical University of Crete, Chania, Greece; Dynamic Systems and Simulation Laboratory, School of Production Engineering and Management, Technical University of Crete, Chania, Greece","2021 IEEE International Intelligent Transportation Systems Conference (ITSC)","25 Oct 2021","2021","","","2027","2033","This paper is concerned with the impacts of smart lane changes of connected and automated vehicles (CAVs) on their own travel performance as well as the entire traffic flow. Based on MOBIL and reinforcement learning, two ego-efficient lane-changing strategies were developed in this work to enable lane-changing decisions for CAV s to improve their travel efficiency. The MOBIL approach intends to establish such a lane-changing strategy by optimizing MOBIL's two parameters, while the reinforcement learning approach tries to develop such a strategy from scratch using Q-learning with sufficient traffic environmental information. The lane-changing strategies were developed and compared on the basis of intensive microscopic traffic simulation. In addition, the information impact on the performance of the reinforcement learning approach was examined to determine the essential amount of environmental information required.","","978-1-7281-9142-3","10.1109/ITSC48978.2021.9564678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564678","","Microscopy;Conferences;Buildings;Reinforcement learning;Traffic control;Optimization;Intelligent transportation systems","mobile robots;reinforcement learning;road traffic;road vehicles","sufficient traffic environmental information;reinforcement learning approach;smart lane-changing strategies;connected automated vehicles;ego-efficient lane-changing strategies;lane-changing decisions;CAV;travel efficiency;MOBIL's two parameters;microscopic traffic simulation","","","","28","IEEE","25 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Grinding Circuit Control in Mineral Processing","M. Hallén; M. Åstrand; J. Sikström; M. Servin","ABB Corporate Research, Västerås, Sweden; ABB Corporate Research, Västerås, Sweden; Boliden Mines, Boliden, Sweden; Department of Physics, Umeå University, Umeå, Sweden","2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)","17 Oct 2019","2019","","","488","495","Grinding, i.e. reducing the particle size of mined ore, is often the bottleneck of the mining concentrating process. Thus, even small improvements may lead to large increases in profit. The goal of the grinding circuit is two-sided; to maximize the throughput of ore, and minimize the resulting particle size of the ground ore within some acceptable range. In this work we study the control of a two-stage grinding circuit using reinforcement learning. To this end, we present a solution for integrating industrial simulation models into the reinforcement learning framework OpenAI Gym.We compare an existing PID controller, based on vast domain knowledge and years of hand-tuning, with a black-box algorithm called Proximal Policy Optimization on a calibrated grinding circuit simulation model. The comparison show that it is possible to control the grinding circuit using reinforcement learning. In addition, contrasting reinforcement learning from the existing PID control, the algorithm is able to maximize an abstract control goal: maximizing profit as defined by a profit function given by our industrial collaborator. In some operating cases the algorithm is able to control the plant more efficiently compared to existing control.","1946-0759","978-1-7281-0303-7","10.1109/ETFA.2019.8869212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8869212","","Ores;Reinforcement learning;Rocks;Materials processing;Integrated circuit modeling;Optimization","grinding;learning (artificial intelligence);mineral processing industry;minerals;neural nets;optimisation;particle size;profitability;three-term control","reinforcement learning model;grinding circuit control;mineral processing;mining concentrating process;PID controller;mined ore particle size reduction;OpenAI Gym model;black-box algorithm;proximal policy optimization model;profit maximization;industrial collaborator;integrating industrial simulation model","","","","20","IEEE","17 Oct 2019","","","IEEE","IEEE Conferences"
"Imitation and Reinforcement Learning","J. Kober; J. Peters","Department of Empirical Inference and Machine Learning, Max Planck Institute for Biological Cybernetics, Tubingen, Germany; Robot Learning Laboratory, MPI","IEEE Robotics & Automation Magazine","7 Jun 2010","2010","17","2","55","62","In this article, we present both novel learning algorithms and experiments using the dynamical system MPs. As such, we describe this MP representation in a way that it is straightforward to reproduce. We review an appropriate imitation learning method, i.e., locally weighted regression, and show how this method can be used both for initializing RL tasks as well as for modifying the start-up phase in a rhythmic task. We also show our current best-suited RL algorithm for this framework, i.e., PoWER. We present two complex motor tasks, i.e., ball-in-a-cup and ball paddling, learned on a real, physical Barrett WAM, using the methods presented in this article. Of particular interest is the ball-paddling application, as it requires a combination of both rhythmic and discrete dynamical systems MPs during the start-up phase to achieve a particular task.","1558-223X","","10.1109/MRA.2010.936952","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480345","","Humans;Learning systems;Robot programming;Service robots;Intelligent robots;Manufacturing;Anthropomorphism;Stability;Planar motors;Legged locomotion","discrete systems;learning (artificial intelligence);regression analysis;robots","imitation learning;reinforcement learning;motor primitive;whole arm manipulator;industrial robots;discrete dynamical system;weighted regression;ball-in-a-cup;ball paddling","","91","1","27","IEEE","7 Jun 2010","","","IEEE","IEEE Magazines"
"Feedback Linearization for Uncertain Systems via Reinforcement Learning","T. Westenbroek; D. Fridovich-Keil; E. Mazumdar; S. Arora; V. Prabhu; S. S. Sastry; C. J. Tomlin","EECS, UC Berkeley; EECS, UC Berkeley; EECS, UC Berkeley; EECS, UC Berkeley; EECS, UC Berkeley; EECS, UC Berkeley; EECS, UC Berkeley","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","1364","1371","We present a novel approach to control design for nonlinear systems which leverages model-free policy optimization techniques to learn a linearizing controller for a physical plant with unknown dynamics. Feedback linearization is a technique from nonlinear control which renders the input-output dynamics of a nonlinear plant linear under application of an appropriate feedback controller. Once a linearizing controller has been constructed, desired output trajectories for the nonlinear plant can be tracked using a variety of linear control techniques. However, the calculation of a linearizing controller requires a precise dynamics model for the system. As a result, model-based approaches for learning exact linearizing controllers generally require a simple, highly structured model of the system with easily identifiable parameters. In contrast, the model-free approach presented in this paper is able to approximate the linearizing controller for the plant using general function approximation architectures. Specifically, we formulate a continuous-time optimization problem over the parameters of a learned linearizing controller whose optima are the set of parameters which best linearize the plant. We derive conditions under which the learning problem is (strongly) convex and provide guarantees which ensure the true linearizing controller for the plant is recovered. We then discuss how model-free policy optimization algorithms can be used to solve a discrete-time approximation to the problem using data collected from the real-world plant. The utility of the framework is demonstrated in simulation and on a real-world robotic platform.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197158","","Feedback linearization;Conferences;Automation;Uncertain systems;Learning (artificial intelligence);Control design;Nonlinear systems","approximation theory;continuous time systems;control system synthesis;feedback;function approximation;learning (artificial intelligence);linearisation techniques;nonlinear control systems;optimisation;uncertain systems","model-free policy optimization techniques;feedback linearization;nonlinear control;nonlinear plant;feedback controller;linear control techniques;exact linearizing controllers;learned linearizing controller;model-free policy optimization algorithms","","10","","41","USGov","15 Sep 2020","","","IEEE","IEEE Conferences"
"Multifidelity Reinforcement Learning With Gaussian Processes: Model-Based and Model-Free Algorithms","V. Suryan; N. Gondhalekar; P. Tokekar","Department of Computer Science, University of Maryland, College Park; Qualcomm, Inc., San Diego, California; Department of Computer Science, University of Maryland, College Park","IEEE Robotics & Automation Magazine","10 Jun 2020","2020","27","2","117","128","We study the problem of reinforcement learning (RL) using as few real-world samples as possible. A naive application of RL can be inefficient in large and continuous-state spaces. We present two versions of multifidelity RL (MFRL), model based and model free, that leverage Gaussian processes (GPs) to learn the optimal policy in a real-world environment. In the MFRL framework, an agent uses multiple simulators of the real environment to perform actions. With increasing fidelity in a simulator chain, the number of samples used in successively higher simulators can be reduced. By incorporating GPs in the MFRL framework, we empirically observe an up to 40% reduction in the number of samples for model-based RL and 60% reduction for the model-free version. We examine the performance of our algorithms through simulations and realworld experiments for navigation with a ground robot.","1558-223X","","10.1109/MRA.2020.2977971","Center for Unmanned Aircraft Systems C-UAS a National Science Foundation(grant numbers:IIP-1161036); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9069479","","Robots;Prediction algorithms;Approximation algorithms;Automation;Training;Planning","Gaussian processes;learning (artificial intelligence)","multifidelity reinforcement learning;model-free algorithms;naive application;continuous-state spaces;Gaussian processes;MFRL framework;simulator chain;model-based RL;model-free version","","2","","21","IEEE","16 Apr 2020","","","IEEE","IEEE Magazines"
"Learning to Solve 3-D Bin Packing Problem via Deep Reinforcement Learning and Constraint Programming","Y. Jiang; Z. Cao; J. Zhang","School of Computer Science and Engineering, Nanyang Technological University, Singapore; Singapore Institute of Manufacturing Technology, Agency for Science, Technology and Research (A*STAR), Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Cybernetics","21 Apr 2023","2023","53","5","2864","2875","Recently, there is a growing attention on applying deep reinforcement learning (DRL) to solve the 3-D bin packing problem (3-D BPP). However, due to the relatively less informative yet computationally heavy encoder, and considerably large action space inherent to the 3-D BPP, existing DRL methods are only able to handle up to 50 boxes. In this article, we propose to alleviate this issue via a DRL agent, which sequentially addresses three subtasks of sequence, orientation, and position, respectively. Specifically, we exploit a multimodal encoder, where a sparse attention subencoder embeds the box state to mitigate the computation while learning the packing policy, and a convolutional neural network subencoder embeds the view state to produce auxiliary spatial representation. We also leverage an action representation learning in the decoder to cope with the large action space of the position subtask. Besides, we integrate the proposed DRL agent into constraint programming (CP) to further improve the solution quality iteratively by exploiting the powerful search framework in CP. The experiments show that both the sole DRL and hybrid methods enable the agent to solve large-scale instances of 120 boxes or more. Moreover, they both could deliver superior performance against the baselines on instances of various scales.","2168-2275","","10.1109/TCYB.2021.3121542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606618","Bin packing problem (BPP);constraint programming (CP);deep reinforcement learning (DRL);multi-task learning","Optimization;Decoding;Reinforcement learning;Search problems;Routing;Task analysis;Standards","bin packing;convolutional neural nets;deep learning (artificial intelligence);learning (artificial intelligence);reinforcement learning;telecommunication computing","3-D bin packing problem;3-D BPP;action representation learning;action space;box state;computationally heavy encoder;constraint programming;convolutional neural network subencoder embeds;deep reinforcement learning;DRL agent;DRL methods;multimodal encoder;packing policy;sole DRL;sparse attention subencoder","","10","","57","IEEE","8 Nov 2021","","","IEEE","IEEE Journals"
"Acceleration of Actor-Critic Deep Reinforcement Learning for Visual Grasping by State Representation Learning Based on a Preprocessed Input Image","T. Kim; Y. Park; Y. Park; S. H. Lee; I. Hong Suh","Department of Electronics and Computer Engineering, Hanyang University, Korea; Department of Electronics and Computer Engineering, Hanyang University, Korea; Department of Electronics and Computer Engineering, Hanyang University, Korea; Innovative Smart Manufacturing R&D Department, Korea Institute of Industrial Technology; CogAplex Co., Ltd","2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","16 Dec 2021","2021","","","198","205","For robotic grasping tasks with diverse target objects, some deep learning-based methods have achieved state-of-the-art results using direct visual input. In contrast, actor-critic deep reinforcement learning (RL) methods typically perform very poorly when applied to grasp diverse objects, especially when learning from raw images and sparse rewards. To render these RL techniques feasible for vision-based grasping tasks, we used state representation learning (SRL), in which we encode essential information for subsequent use in RL. However, typical representation learning procedures are unsuitable for extracting pertinent information for learning grasping skills owing to the high complexity of visual inputs for representation learning, in which a robot attempts to grasp a target object. We found that the proposed preprocessed input image is the key to capturing effectively a compact representation. This enables deep RL to learn robotic grasping skills from highly varied and diverse visual inputs. Further, we demonstrate the effectiveness of the proposed approach with varying levels of preprocessing in a realistic simulated environment. We also describe how the resulting model can be transferred to a real-world robot and also demonstrate a 68% success rate on real-world grasp attempts.","2153-0866","978-1-6654-1714-3","10.1109/IROS51168.2021.9635931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635931","","Representation learning;Training;Learning systems;Visualization;Data preprocessing;Grasping;Reinforcement learning","deep learning (artificial intelligence);grippers;image representation;reinforcement learning;robot programming;robot vision","state representation learning;image preprocessing;visual grasping;robotic grasping tasks;direct visual input;actor-critic deep reinforcement learning;raw images;sparse rewards;vision based grasping tasks","","2","","38","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"IPAPRec: A Promising Tool for Learning High-Performance Mapless Navigation Skills With Deep Reinforcement Learning","W. Zhang; Y. Zhang; N. Liu; K. Ren; P. Wang","Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Smart Manufacturing Division, Advanced Remanufacturing and Technology Centre, Singapore, Singapore; School of Mechanical Engineering, Zhejiang University, Hangzhou, China; Temasek Laboratories, National University of Singapore, Singapore","IEEE/ASME Transactions on Mechatronics","14 Dec 2022","2022","27","6","5451","5461","This article studies how to improve the generalization performance and learning speed of the navigation agents trained with deep reinforcement learning (DRL). Although DRL exhibits huge potential in robot mapless navigation, DRL agents performing well in training scenarios are often found to perform poorly in unfamiliar scenarios. In this work, we propose that the representation of LiDAR readings is a key factor behind the degradation of agents’ performance and present a powerful input preprocessing (IP) approach to address this issue. As this approach uses adaptively parametric reciprocal functions to preprocess LiDAR readings, we refer to this approach as IPAPRec and its normalized version as IPAPRecN. IPAPRec/IPAPRecN can highlight important short-distance values and compress the range of less-important long-distance values in laser scans, which well address the issues induced by conventional representations of laser scans. Their high performance was validated by extensive simulation and real-world experiments. The results show that our methods can substantially improve navigation agents’ generalization performance and greatly reduce the training time compared to conventional methods.","1941-014X","","10.1109/TMECH.2022.3182427","China Scholarship Council; National University of Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9804689","Autonomous systems;deep reinforcement learning (DRL);mapless navigation;mobile robots","Mobile robots;Navigation;Laser radar;Autonomous systems;Control systems;Lasers;Robot kinematics","learning (artificial intelligence);mobile robots;navigation;path planning;robot vision","adaptively parametric reciprocal functions;deep reinforcement learning;DRL agents;DRL exhibits huge potential;generalization performance;high-performance mapless navigation skills;laser scans;learning speed;LiDAR readings;long-distance values;navigation agents;robot mapless navigation;short-distance values;training scenarios;training time;unfamiliar scenarios","","1","","35","IEEE","23 Jun 2022","","","IEEE","IEEE Journals"
"Web Service Classification Based on Reinforcement Learning and Structured Representation Learning","H. Sheng; Z. Li; J. Liu; X. Zhang","School of Computer Science and Engineering Hunan University of Science & Technology, Xiang Tan, China; School of Computer Science and Engineering Hunan University of Science & Technology, Xiang Tan, China; Key Laboratory of Knowledge Processing & Networked Manufacturing, Hunan University of Science & Technology, Xiang Tan, China; School of Computer Science and Engineering Hunan University of Science & Technology, Xiang Tan, China","2021 International Conference on Artificial Intelligence and Blockchain Technology (AIBT)","4 Feb 2022","2021","","","21","27","In recent years, the rapid growth of the number of web services on the Internet has made it difficult to discover services from a large number of web services and classify them efficiently, which poses a huge challenge for users to provide appropriate web services. The existing web service classification technology has problems such as sparse description text, insufficient consideration of attribute information and structural relationships, so the accuracy of web service classification can be further improved. In response to the above problems, this paper first proposes an information distillation model based on reinforcement learning. Reinforcement learning is introduced to delete redundant information and retain valid information. Then, the classification network classifies the distilled information and determines the reward based on the prediction result. Then, the reward is fed to reinforcement learning for gradient update and the classification network is updated inversely according to the classification result. It can effectively alleviate the inherent noise caused by the introduction of the attention mechanism. It can also learn classification tasks by identifying important words or task-related structures without displaying structural annotations (ie, artificially building models). Through comparative experiments on the real data set of Programmable Web, the results show that the proposed model achieves better results on the benchmark data set than other methods.","","978-1-6654-3267-2","10.1109/AIBT53261.2021.00011","National Natural Science Foundation of China(grant numbers:61873316); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9694006","Web service classification;Reinforcement learning;Information distillation;Structured representation learning","Representation learning;Web services;Buildings;Reinforcement learning;Benchmark testing;Data models;Blockchains","Internet;learning (artificial intelligence);pattern classification;text analysis;Web services","reinforcement learning;structured representation learning;appropriate web services;existing web service classification technology;attribute information;information distillation model;redundant information;valid information;classification network;distilled information;classification result;classification tasks;Programmable Web","","1","","23","IEEE","4 Feb 2022","","","IEEE","IEEE Conferences"
"GCN-RL Circuit Designer: Transferable Transistor Sizing with Graph Neural Networks and Reinforcement Learning","H. Wang; K. Wang; J. Yang; L. Shen; N. Sun; H. -S. Lee; S. Han",Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology; University of Texas at Austin; University of Texas at Austin; Massachusetts Institute of Technology; Massachusetts Institute of Technology,"2020 57th ACM/IEEE Design Automation Conference (DAC)","9 Oct 2020","2020","","","1","6","Automatic transistor sizing is a challenging problem in circuit design due to the large design space, complex performance tradeoffs, and fast technology advancements. Although there have been plenty of work on transistor sizing targeting on one circuit, limited research has been done on transferring the knowledge from one circuit to another to reduce the re-design overhead. In this paper, we present GCN-RL Circuit Designer, leveraging reinforcement learning (RL) to transfer the knowledge between different technology nodes and topologies. Moreover, inspired by the simple fact that circuit is a graph, we learn on the circuit topology representation with graph convolutional neural networks (GCN). The GCN-RL agent extracts features of the topology graph whose vertices are transistors, edges are wires. Our learning-based optimization consistently achieves the highest Figures of Merit (FoM) on four different circuits compared with conventional black box optimization methods (Bayesian Optimization, Evolutionary Algorithms), random search and human expert designs. Experiments on transfer learning between five technology nodes and two circuit topologies demonstrate that RL with transfer learning can achieve much higher FoMs than methods without knowledge transfer. Our transferable optimization method makes transistor sizing and design porting more effective and efficient.","0738-100X","978-1-7281-1085-1","10.1109/DAC18072.2020.9218757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9218757","Circuit Design Automation;Transistor Sizing;Reinforcement Learning;Graph Neural Network;Transfer Learning","Circuit topology;Network topology;Transfer learning;Optimization methods;Reinforcement learning;Feature extraction;Topology","circuit CAD;convolutional neural nets;graph theory;learning (artificial intelligence);network topology;optimisation","graph neural networks;automatic transistor sizing;circuit design;design space;GCN-RL circuit designer;reinforcement learning;technology nodes;circuit topology representation;graph convolutional neural networks;GCN-RL agent;topology graph;learning-based optimization;human expert designs;transfer learning;circuit topologies;knowledge transfer;transferable optimization method;transferable transistor sizing;figures of merit","","100","","25","IEEE","9 Oct 2020","","","IEEE","IEEE Conferences"
"AutoCkt: Deep Reinforcement Learning of Analog Circuit Designs","K. Settaluri; A. Haj-Ali; Q. Huang; K. Hakhamaneshi; B. Nikolic","University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley","2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)","15 Jun 2020","2020","","","490","495","Domain specialization under energy constraints in deeply-scaled CMOS has been driving the need for agile development of Systems on a Chip (SoCs). While digital subsystems have design flows that are conducive to rapid iterations from specification to layout, analog and mixed-signal modules face the challenge of a long human-in-the-middle iteration loop that requires expert intuition to verify that post-layout circuit parameters meet the original design specification. Existing automated solutions that optimize circuit parameters for a given target design specification have limitations of being schematic-only, inaccurate, sample-inefficient or not generalizable. This work presents AutoCkt, a machine learning optimization framework trained using deep reinforcement learning that not only finds post-layout circuit parameters for a given target specification, but also gains knowledge about the entire design space through a sparse subsampling technique. Our results show that for multiple circuit topologies, AutoCkt is able to converge and meet all target specifications on at least 96.3% of tested design goals in schematic simulation, on average 40× faster than a traditional genetic algorithm. Using the Berkeley Analog Generator, AutoCkt is able to design 40 LVS passed operational amplifiers in 68 hours, 9.6× faster than the state-of-the-art when considering layout parasitics.","1558-1101","978-3-9819263-4-7","10.23919/DATE48585.2020.9116200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9116200","analog sizing;reinforcement learning;transfer learning;automation of analog design","Trajectory;Reinforcement learning;Training;Layout;Genetic algorithms;Mathematical model","circuit CAD;circuit optimisation;CMOS analogue integrated circuits;integrated circuit layout;iterative methods;learning (artificial intelligence);network topology;operational amplifiers","AutoCkt;deep reinforcement learning;Analog circuit designs;domain specialization;energy constraints;deeply-scaled CMOS;digital subsystems;mixed-signal modules;human-in-the-middle iteration loop;post-layout circuit parameters;circuit topologies;Berkeley Analog Generator;layout parasitics;operational amplifiers;machine learning optimization;time 68.0 hour","","66","","11","","15 Jun 2020","","","IEEE","IEEE Conferences"
"Energy-Based Legged Robots Terrain Traversability Modeling via Deep Inverse Reinforcement Learning","L. Gan; J. W. Grizzle; R. M. Eustice; M. Ghaffari","Robotics Institute, University of Michigan, Ann Arbor, MI, USA; Robotics Institute, University of Michigan, Ann Arbor, MI, USA; Robotics Institute, University of Michigan, Ann Arbor, MI, USA; Robotics Institute, University of Michigan, Ann Arbor, MI, USA","IEEE Robotics and Automation Letters","18 Jul 2022","2022","7","4","8807","8814","This work reports ondeveloping a deep inverse reinforcement learning method for legged robots terrain traversability modeling that incorporates both exteroceptive and proprioceptive sensory data. Existing works use robot-agnostic exteroceptive environmental features or handcrafted kinematic features; instead, we propose to also learn robot-specific inertial features from proprioceptive sensory data for reward approximation in a single deep neural network. Incorporating the inertial features can improve the model fidelity and provide a reward that depends on the robot’s state during deployment. We train the reward network using the Maximum Entropy Deep Inverse Reinforcement Learning (MEDIRL) algorithm and propose simultaneously minimizing a trajectory ranking loss to deal with the suboptimality of legged robot demonstrations. The demonstrated trajectories are ranked by locomotion energy consumption, in order to learn an energy-aware reward function and a more energy-efficient policy than demonstration. We evaluate our method using a dataset collected by an MIT Mini-Cheetah robot and a Mini-Cheetah simulator. The code is publicly available.1","2377-3766","","10.1109/LRA.2022.3188100","Toyota Research Institute; MIT Biomimetic Robotics Lab; NAVER LABS; NSF(grant numbers:2118818); NVIDIA Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9813568","Energy and environment-aware automation;legged robots;learning from demonstration","Robots;Robot sensing systems;Legged locomotion;Costs;Trajectory;Behavioral sciences;Entropy","learning (artificial intelligence);legged locomotion;mobile robots;neural nets","energy-based legged robots terrain traversability modeling;deep inverse reinforcement learning method;proprioceptive sensory data;robot-agnostic exteroceptive environmental features;kinematic features;robot-specific inertial features;reward approximation;single deep neural network;model fidelity;reward network;Maximum Entropy Deep Inverse Reinforcement Learning algorithm;legged robot demonstrations;locomotion energy consumption;energy-aware reward function;energy-efficient policy;MIT Mini-Cheetah robot","","9","","40","IEEE","4 Jul 2022","","","IEEE","IEEE Journals"
"Transferable Deep Reinforcement Learning Framework for Autonomous Vehicles With Joint Radar-Data Communications","N. Q. Hieu; D. T. Hoang; D. Niyato; P. Wang; D. I. Kim; C. Yuen","School of Electrical and Data Engineering, University of Technology Sydney, Ultimo, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Ultimo, NSW, Australia; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Electrical Engineering and Computer Science, Lassonde School of Engineering, York University, Toronto, ON, Canada; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Engineering Product Development (EPD) Pillar, Singapore University of Technology and Design, Singapore","IEEE Transactions on Communications","16 Aug 2022","2022","70","8","5164","5180","Autonomous Vehicles (AVs) are required to operate safely and efficiently in dynamic environments. For this, the AVs equipped with Joint Radar-Communications (JRC) functions can enhance the driving safety by utilizing both radar detection and data communication functions. However, optimizing the performance of the AV system with two different functions under uncertainty and dynamic of surrounding environments is very challenging. In this work, we first propose an intelligent optimization framework based on the Markov Decision Process (MDP) to help the AV make optimal decisions in selecting JRC operation functions under the dynamic and uncertainty of the surrounding environment. We then develop an effective learning algorithm leveraging recent advances of deep reinforcement learning techniques to find the optimal policy for the AV without requiring any prior information about surrounding environment. Furthermore, to make our proposed framework more scalable, we develop a Transfer Learning (TL) mechanism that enables the AV to leverage valuable experiences for accelerating the training process when it moves to a new environment. Extensive simulations show that the proposed transferable deep reinforcement learning framework reduces the obstacle miss detection probability by the AV up to 67% compared to other conventional deep reinforcement learning approaches. With the deep reinforcement learning and transfer learning approaches, our proposed solution can find its applications in a wide range of autonomous driving scenarios from driver assistance to full automation transportation.","1558-0857","","10.1109/TCOMM.2022.3182034","Australian Research Council under the DECRA(grant numbers:DE210100651); DesCartes - the National Research Foundation, Prime Minister’s Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme and under its Emerging Areas Research Projects (EARP) Funding Initiative; Alibaba Group through Alibaba Innovative Research (AIR) Program and Alibaba-NTU Singapore Joint Research Institute (JRI); National Research Foundation, Singapore under the AI Singapore Programme (AISG)(grant numbers:AISG2- RP-2020-019); Singapore Ministry of Education (MOE) Tier 1 (RG16/20); National Research Foundation of Korea (NRF); Korean Government (MSIT)(grant numbers:2021R1A2C2007638); MSIT under the ICT Creative Consilience program(grant numbers:IITP-2020-0-01821); A*STAR under its RIE2020 Advanced Manufacturing and Engineering (AME) Industry Alignment Fund – Pre Positioning (IAF-PP)(grant numbers:A19D6a0053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793620","Joint radar-communications;autonomous vehicles;deep reinforcement learning;transfer learning","Radar detection;Radar;Data communication;Reinforcement learning;Vehicle dynamics;Safety;Meteorology","advanced driver assistance systems;data communication;deep learning (artificial intelligence);Markov processes;optimisation;probability;radar detection;reinforcement learning","AV system;intelligent optimization framework;Markov decision process;transferable deep reinforcement learning framework;conventional deep reinforcement learning approaches;autonomous driving scenarios;autonomous vehicles;radar detection;joint radar-data communication function;JRC operation functions;transfer learning mechanism;TL mechanism;probability detection;driver assistance;automation transportation","","8","","41","IEEE","10 Jun 2022","","","IEEE","IEEE Journals"
"Overcoming Exploration: Deep Reinforcement Learning for Continuous Control in Cluttered Environments From Temporal Logic Specifications","M. Cai; E. Aasi; C. Belta; C. -I. Vasile","Mechanical Engineering, Lehigh University, Bethlehem, PA, USA; Mechanical Engineering Department, Boston University, Boston, MA, USA; Mechanical Engineering Department, Boston University, Boston, MA, USA; Mechanical Engineering, Lehigh University, Bethlehem, PA, USA","IEEE Robotics and Automation Letters","2 Mar 2023","2023","8","4","2158","2165","Model-free continuous control for robot navigation tasks using Deep Reinforcement Learning (DRL) that relies on noisy policies for exploration is sensitive to the density of rewards. In practice, robots are usually deployed in cluttered environments, containing many obstacles and narrow passageways. Designing dense effective rewards is challenging, resulting in exploration issues during training. Such a problem becomes even more serious when tasks are described using temporal logic specifications. This work presents a deep policy gradient algorithm for controlling a robot with unknown dynamics operating in a cluttered environment when the task is specified as a Linear Temporal Logic (LTL) formula. To overcome the environmental challenge of exploration during training, we propose a novel path planning-guided reward scheme by integrating sampling-based methods to effectively complete goal-reaching missions. To facilitate LTL satisfaction, our approach decomposes the LTL mission into sub-goal-reaching tasks that are solved in a distributed manner. Our framework is shown to significantly improve performance (effectiveness, efficiency) and exploration of robots tasked with complex missions in large-scale cluttered environments.","2377-3766","","10.1109/LRA.2023.3246844","Under Secretary of Defense for Research and Engineering; Air Force Contract(grant numbers:FA8702-15-D-0001); Lehigh University; NSF(grant numbers:IIS-2024606); Boston University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049066","Formal methods in robotics and automation;deep reinforcement learning;sampling-based method","Robots;Task analysis;Navigation;Heuristic algorithms;Standards;Training;Deep learning","collision avoidance;control engineering computing;deep learning (artificial intelligence);gradient methods;learning (artificial intelligence);mobile robots;navigation;path planning;reinforcement learning;temporal logic","cluttered environment;deep policy gradient algorithm;Deep Reinforcement;dense effective rewards;environmental challenge;exploration issues;goal-reaching missions;large-scale cluttered environments;Linear Temporal Logic formula;model-free continuous control;narrow passageways;noisy policies;overcoming exploration;reward scheme;robot navigation tasks;sub-goal-reaching tasks;temporal logic specifications","","5","","39","IEEE","20 Feb 2023","","","IEEE","IEEE Journals"
"Dynamic scheduling in modern processing systems using expert-guided distributed reinforcement learning","S. Qu; J. Wang; J. Jasperneite","Civil&Environmental Engineering, Stanford Unverisity, Palo Alto, CA, 94305, USA; Center for Sustainable Development and Global Competiveness, Stanford University, Palo Alto, CA, 94305, USA; Fraunhofer IOSB-INA, Lemgo, Germany","2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)","17 Oct 2019","2019","","","459","466","In the environment of modern processing systems, one topic of great interest is how to optimally schedule (i.e., allocate) jobs with different requirements for the systems to meet various objectives. Methods using distributed reinforcement learning (DRL) have recently achieved great success with large-scale dynamic scheduling problems. However, most DRL methods require a huge amount of computational time and a large amount of data for the DRL agents for a control policy. Meanwhile, various scheduling experts have already developed several scheduling policies (i.e., dispatching rules) that can successfully fulfill different objectives with acceptable performance for a processing system based on their understandings of the processing systems characteristics. In this paper, we propose to learn from experts to reduce the learning and searching costs of a good policy in large-scale dynamic scheduling problems. In the learning process, our DRL agents select the experts who have better performance in the scheduling environment, observe the experts' actions, and learn a scheduling policy guided by these experts' demonstrations. Our realistic simulations results demonstrate that this expert-guided DRL (EGDRL) approach outperforms DRL methods without expert guidance, as well as some other reinforcement learning from demonstration (RLfD) methods in several systems. To the best of our knowledge, our research is one of the first works that incorporates existing expert policies to guide the learning of optimal policies for large-scale dynamic scheduling problems.","1946-0759","978-1-7281-0303-7","10.1109/ETFA.2019.8869023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8869023","Queueing system;reinforcement learning;expert knowledge;intelligent automation","Job shop scheduling;Task analysis;Dynamic scheduling;Reinforcement learning;Servers;Processor scheduling;Optimal scheduling","dynamic scheduling;learning (artificial intelligence)","modern processing systems;reinforcement learning;large-scale dynamic scheduling problems;DRL methods;DRL agents;scheduling experts;scheduling policy;processing systems characteristics;searching costs;learning process;scheduling environment;expert-guided DRL approach;expert guidance;expert policies;optimal policies","","3","","23","IEEE","17 Oct 2019","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Decentralized Multi-Robot Exploration With Macro Actions","A. H. Tan; F. P. Bejarano; Y. Zhu; R. Ren; G. Nejat","Autonomous Systems and Biomechatronics Laboratory (ASBLab), Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory (ASBLab), Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory (ASBLab), Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory (ASBLab), Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory (ASBLab), Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada","IEEE Robotics and Automation Letters","8 Dec 2022","2023","8","1","272","279","Cooperative multi-robot teams need to be able to explore cluttered and unstructured environments while dealing with communication dropouts that prevent them from exchanging local information to maintain team coordination. Therefore, robots need to consider high-level teammate intentions during action selection. In this letter, we present the first Macro Action Decentralized Exploration Network (MADE-Net) using multi-agent deep reinforcement learning (DRL) to address the challenges of communication dropouts during multi-robot exploration in unseen, unstructured, and cluttered environments. Simulated robot team exploration experiments were conducted and compared against classical and DRL methods where MADE-Net outperformed all benchmark methods in terms of computation time, total travel distance, number of local interactions between robots, and exploration rate across various degrees of communication dropouts. A scalability study in 3D environments showed a decrease in exploration time with MADE-Net with increasing team and environment sizes. The experiments presented highlight the effectiveness and robustness of our method.","2377-3766","","10.1109/LRA.2022.3224667","Natural Sciences and Engineering Research Council of Canada; NSERC Collaborative Research and Development; Canada Research Chairs; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9963690","Autonomous Agents;deep learning for robotics and automation;search and rescue robots","Robots;Robot kinematics;Robot sensing systems;Task analysis;Training;Sensors;Reinforcement learning","deep learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems","action selection;cluttered environments;communication dropouts;cooperative multirobot teams;decentralized multirobot exploration;DRL;high-level teammate intention;local information;local interactions;macroaction decentralized exploration network;MADE-Net;multiagent deep reinforcement learning;simulated robot team exploration experiments;team coordination;unstructured environments","","3","","40","IEEE","24 Nov 2022","","","IEEE","IEEE Journals"
"Natural Language Specification of Reinforcement Learning Policies Through Differentiable Decision Trees","P. Tambwekar; A. Silva; N. Gopalan; M. Gombolay","School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, USA; School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, USA; School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, USA; School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Robotics and Automation Letters","1 May 2023","2023","8","6","3621","3628","Human-AI policy specification is a novel procedure we define in which humans can collaboratively warm-start a robot's reinforcement learning policy. This procedure is comprised of two steps; (1) Policy Specification, i.e. humans specifying the behavior they would like their companion robot to accomplish, and (2) Policy Optimization, i.e. the robot applying reinforcement learning to improve the initial policy. Existing approaches to enabling collaborative policy specification are often unintelligible black-box methods, and are not catered towards making the autonomous system accessible to a novice end-user. In this letter, we develop a novel collaborative framework to allow humans to initialize and interpret an autonomous agent's behavior. Through our framework, we enable humans to specify an initial behavior model via unstructured, natural language (NL), which we convert to lexical decision trees. Next, we leverage these translated specifications, to warm-start reinforcement learning and allow the agent to further optimize these potentially suboptimal policies. Our approach warm-starts an RL agent by utilizing non-expert natural language specifications without incurring the additional domain exploration costs. We validate our approach by showing that our model is able to produce $>$80% translation accuracy, and that policies initialized by a human can match the performance of relevant RL baselines in two domains.","2377-3766","","10.1109/LRA.2023.3268593","Office of Naval Research(grant numbers:N00014-19-1-2076); National Science Foundation(grant numbers:IIS-2112633,FMRG-2229260); Georgia Tech Research Foundation by Konica Minolta; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10105980","Human-centered automation;human-robot collaboration","Decision trees;Natural languages;Behavioral sciences;Robots;Collaboration;Reinforcement learning;Optimization","artificial intelligence;decision trees;learning (artificial intelligence);natural language processing;reinforcement learning","approach warm-starts;autonomous agent;collaborative policy specification;companion robot;differentiable decision trees;human-AI policy specification;initial behavior model;initial policy;lexical decision trees;natural language Specification;nonexpert natural language specifications;novel collaborative framework;optimize these potentially suboptimal policies;reinforcement learning policy;translated specifications;unstructured language","","","","69","IEEE","20 Apr 2023","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning With a Look-Ahead Search for Robotic Cell Scheduling","H. -J. Kim; J. -H. Lee","Department of Industrial and Systems Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; School of Business, Chungnam National University, Daejeon, Republic of Korea","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2023","PP","99","1","12","Robotized manufacturing systems consisting of several processing machines and a robot for transporting jobs between the machines have been widely used in mechanical and electronic manufacturing industries. The sequence of robot tasks in such a robotized manufacturing system affects its productivity significantly, which also has the large impact on the overall production line consisting of multiple robotized manufacturing systems. This article addresses the scheduling problem in a single-gripper robotic cell, one of a robotized manufacturing systems. The objective is to minimize the makespan. To achieve this, a novel RL method is proposed, which combines a look-ahead search (LAS) to improve decision-making using more accurate estimated makespan. Experimental results demonstrate the superior performance of the proposed method compared to existing approaches. Moreover, the method is applicable in dynamic environments with uncertain processing times.","2168-2232","","10.1109/TSMC.2023.3317390","Korean Government (MSIT)(grant numbers:2022R1A2C4001978,2022R1F1A1059500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10274095","Planning;reinforcement learning (RL);scheduling and coordination;single-gripper robotic cell","Robots;Job shop scheduling;Robot kinematics;Task analysis;Service robots;Schedules;Manufacturing systems","","","","","","","IEEE","9 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Optimal Demand Response Using Device-Based Reinforcement Learning","Z. Wen; D. O’Neill; H. Maei","Yahoo Labs, CA, USA; Department of Electrical Engineering, Stanford University, Stanford, CA, USA; Samsung Research America, Mountain View, CA, USA","IEEE Transactions on Smart Grid","19 Aug 2015","2015","6","5","2312","2324","Demand response (DR) for residential and small commercial buildings is estimated to account for as much as 65% of the total energy savings potential of DR, and previous work shows that a fully automated energy management system (EMS) is a necessary prerequisite to DR in these areas. In this paper, we propose a novel EMS formulation for DR problems in these sectors. Specifically, we formulate a fully automated EMS’s rescheduling problem as a reinforcement learning (RL) problem, and argue that this RL problem can be approximately solved by decomposing it over device clusters. Compared with existing formulations, our new formulation does not require explicitly modeling the user’s dissatisfaction on job rescheduling, enables the EMS to self-initiate jobs, allows the user to initiate more flexible requests, and has a computational complexity linear in the number of device clusters. We also demonstrate the simulation results of applying Q-learning, one of the most popular and classical RL algorithms, to a representative example.","1949-3061","","10.1109/TSG.2015.2396993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042790","Building and home automation;demand response (DR);energy management system (EMS);Markov decision process (MDP);reinforcement learning (RL);Building and home automation;demand response (DR);energy management system (EMS);Markov decision process (MDP);reinforcement learning (RL)","Energy management;Buildings;Optimal scheduling;Clustering algorithms;Markov processes;Real-time systems;Energy consumption","demand side management;energy management systems;learning (artificial intelligence);power engineering computing","optimal demand response;device-based reinforcement learning;energy management system;EMS;RL problem;DR problems;device clusters;job rescheduling;computational complexity;Q-learning","","162","","27","IEEE","16 Feb 2015","","","IEEE","IEEE Journals"
"A Multiple-Goal Reinforcement Learning Method for Complex Vehicle Overtaking Maneuvers","D. C. K. Ngai; N. H. C. Yung","ASM Assembly Automation Hong Kong Limited, Kwai Chung, Hong Kong, China; Department of Electrical and Electronic Engineering, University of Hong Kong, Pokfulam, Hong Kong, China","IEEE Transactions on Intelligent Transportation Systems","31 May 2011","2011","12","2","509","522","In this paper, we present a learning method to solve the vehicle overtaking problem, which demands a multitude of abilities from the agent to tackle multiple criteria. To handle this problem, we propose to adopt a multiple-goal reinforcement learning (MGRL) framework as the basis of our solution. By considering seven different goals, either Q-learning (QL) or double-action QL is employed to determine action decisions based on whether the other vehicles interact with the agent for that particular goal. Furthermore, a fusion function is proposed according to the importance of each goal before arriving to an overall but consistent action decision. This offers a powerful approach for dealing with demanding situations such as overtaking, particularly when a number of other vehicles are within the proximity of the agent and are traveling at different and varying speeds. A large number of overtaking cases have been simulated to demonstrate its effectiveness. From the results, it can be concluded that the proposed method is capable of the following: 1) making correct action decisions for overtaking; 2) avoiding collisions with other vehicles; 3) reaching the target at reasonable time; 4) keeping almost steady speed; and 5) maintaining almost steady heading angle. In addition, it should also be noted that the proposed method performs lane keeping well when not overtaking and lane changing effectively when overtaking is in progress.","1558-0016","","10.1109/TITS.2011.2106158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5710424","Artificial intelligence;learning control systems","Vehicles;Quantization;Roads;Sensors;Navigation;Collision avoidance;Learning","collision avoidance;learning systems;road vehicles","multiple-goal reinforcement learning;complex vehicle overtaking maneuvers;Q-learning;double-action QL;fusion function;collision avoidance","","85","","38","IEEE","7 Feb 2011","","","IEEE","IEEE Journals"
