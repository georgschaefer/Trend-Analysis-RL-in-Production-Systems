"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Adaptive sleep-wake control using reinforcement learning in sensor networks","L. A. Prashanth; A. Chatterjee; S. Bhatnagar","INRIA Lille - Nord Europe, Team SequeL, FRANCE; System Sciences and Automation, Indian Institute of Science, Bangalore, INDIA; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, INDIA","2014 Sixth International Conference on Communication Systems and Networks (COMSNETS)","10 Feb 2014","2014","","","1","8","The aim in this paper is to allocate the ‘sleep time’ of the individual sensors in an intrusion detection application so that the energy consumption from the sensors is reduced, while keeping the tracking error to a minimum. We propose two novel reinforcement learning (RL) based algorithms that attempt to minimize a certain long-run average cost objective. Both our algorithms incorporate feature-based representations to handle the curse of dimensionality associated with the underlying partially-observable Markov decision process (POMDP). Further, the feature selection scheme used in our algorithms intelligently manages the energy cost and tracking cost factors, which in turn assists the search for the optimal sleeping policy. We also extend these algorithms to a setting where the intruder's mobility model is not known by incorporating a stochastic iterative scheme for estimating the mobility model. The simulation results on a synthetic 2-d network setting are encouraging.","2155-2509","978-1-4799-3635-9","10.1109/COMSNETS.2014.6734874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6734874","Sensor Networks;Sleep-Wake Scheduling;Reinforcement Learning;Q-learning;Function Approximation;SPSA","Tin;Computational modeling;Adaptation models","adaptive control;energy consumption;feature selection;learning (artificial intelligence);Markov processes;mobility management (mobile radio);security of data;wireless sensor networks","adaptive sleep-wake control;sensor networks;intrusion detection application;energy consumption;reinforcement learning;RL based algorithm;feature-based representation;partially-observable Markov decision process;POMDP;feature selection scheme;energy cost factor;tracking cost factor;optimal sleeping policy;intruder mobility model;stochastic iterative scheme;synthetic 2d network","","2","","19","IEEE","10 Feb 2014","","","IEEE","IEEE Conferences"
"A Policy-Based Reinforcement Learning Approach for High-Speed Railway Timetable Rescheduling","Y. Wang; Y. Lv; J. Zhou; Z. Yuan; Q. Zhang; M. Zhou","State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Information Management, Shanghai Lixin University of Accounting and Finance, Shanghai, China; Center of National Railway Intelligent Transportation System Engineering and Technology, China Academy of Railway Sciences Corporation Limited, China; Center of National Railway Intelligent Transportation System Engineering and Technology, China Academy of Railway Sciences Corporation Limited, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijig, China","2021 IEEE International Intelligent Transportation Systems Conference (ITSC)","25 Oct 2021","2021","","","2362","2367","In the daily management of high-speed railway systems, the train timetable rescheduling problem with unpredictable disturbances is a challenging task. The large number of stations and trains leads to a long-time consumption to solve the rescheduling problem, making it difficult to meet the realtime requirements in real-world railway networks. This paper proposes a policy-based reinforcement learning approach to address the high-speed railway timetable rescheduling problem, in which the agent minimizes the total delay by adjusting the departure sequence of all trains along the railway line. A two-stage Markov Decision Process model is established to model the environment where states, actions, and reward functions are designed. The proposed method contains an offline learning process and an online application process, which can give the optimal rescheduling schedule based on the current state immediately. Numerical experiments are performed over two different delay scenarios on the Beijing-Shanghai high-speed railway line. The simulation results show that our approach can find a high-quality rescheduling strategy within one second, which is superior to the First-Come-First-Served (FCFS) and First-Scheduled-First-Served (FSFS) methods.","","978-1-7281-9142-3","10.1109/ITSC48978.2021.9564980","National Natural Science Foundation of China(grant numbers:61790573,61790575); China Academy of Railway Sciences Corporation Limited(grant numbers:N2019G020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564980","","Schedules;Monte Carlo methods;Simulation;Microscopy;Reinforcement learning;Rail transportation;Numerical models","learning (artificial intelligence);Markov processes;numerical analysis;railways;scheduling","policy-based reinforcement learning;high-speed railway systems;train timetable rescheduling problem;real-world railway networks;high-speed railway timetable rescheduling problem;offline learning process;online application process;optimal rescheduling schedule;Beijing-Shanghai high-speed railway line;high-quality rescheduling strategy;two-stage Markov decision process model;first-come-first-served method;FCFS;first-scheduled-first-served method;FSFS","","2","","17","IEEE","25 Oct 2021","","","IEEE","IEEE Conferences"
"Log Analytics in HPC: A Data-driven Reinforcement Learning Framework","Z. Luo; T. Hou; T. T. Nguyen; H. Zeng; Z. Lu","University of South Florida, Tampa, FL, USA; University of South Florida, Tampa, FL, USA; Intelligent Automation Inc., Rockville, MD, USA; Intelligent Automation Inc., Rockville, MD, USA; University of South Florida, Tampa, FL, USA","IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)","10 Aug 2020","2020","","","550","555","High Performance Computing (HPC) has been employed in many fields such as aerospace, weather forecast, numerical simulation, scientific research etc. Security of HPC, especially anomaly/intrusion detection, has attracted many attentions in recent years. Given the heavily instrumented property of HPC systems, logs become an effective and direct data source that can be utilized to evaluate the system status, further, to detect anomalies or malicious users. In this paper, we offer a novel perspective, treating the anomaly detection in HPC as a sequential decision process, and further applying reinforcement learning techniques to learn the state transition process, based on which we build a framework named as ReLog to detect anomalies or malicious users. Besides, a common challenge of employing machine learning techniques is lacking sufficient data, we provide a generative adversarial network (GAN)-based solution to generate sufficient training data in HPC. The experimental validations are conducted based on real-world collected MPI logs, and our results demonstrate a 93% of detection accuracy on the collected dataset.","","978-1-7281-8695-5","10.1109/INFOCOMWKSHPS50562.2020.9162664","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162664","High performance computing;security;reinforcement learning;defenses and attacks;log analytics","Feature extraction;Learning (artificial intelligence);Security;Machine learning;Generative adversarial networks;Microsoft Windows;Data mining","data analysis;learning (artificial intelligence);neural nets;parallel processing;security of data","data source;malicious users;anomaly detection;machine learning techniques;log analytics;data-driven reinforcement learning framework;high performance computing;HPC systems;generative adversarial network;GAN;ReLog;sequential decision process","","1","","26","IEEE","10 Aug 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning Tracking Control for Unknown Continuous Dynamic Systems","L. Ye; J. Li; C. Wang; H. Liu; B. Liang","The Center of Intelligent Control and Telescience, Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, Shenzhen, China; Shanghai Academy of Spaceflight Technology, Shanghai, China; The Center of Intelligent Control and Telescience, Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Department of Automation, Navigation and Control Research Center, Tsinghua University, Beijing, China","2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS)","25 Jun 2021","2021","","","114","119","Reinforcement learning tracking control (ILC) is proposed to solve the traditional iterative learning control (ILC) problem. For unknown continuous dynamic systems, precision output tracking is achieved for a given trajectory after several times of trials. The controller is composed of a feedback part and a feedforward part, where the feedback part uses linear states feedback to stabilize the system and the feedforward part is a time-dependent signal to ensure precision tracking. The desired trajectory is defined in a finite time interval and the controller is applied repeatedly to the system with the feedforward signal updated by reinforcement learning in each trial. The tracking problem is treated as a black-box optimization problem where the integral quadratic tracking error is a cost to be minimized under input constraints. In order to apply reinforcement learning, the feedforward signal is approximated by spline interpolation of a few representative points distributed uniformly along the tracking time interval. A search strategy based on squeeze theorem is adopted to update the value of the representative points to achieve minimum tracking error cost. The cart pole example illustrates the effectiveness of the proposed method.","2767-9861","978-1-6654-2423-3","10.1109/DDCLS52934.2021.9455473","National Natural Science Foundation (NNSF) of China(grant numbers:62003188,61803221,11702176,U1813216); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9455473","Tracking control;iterative learning control;reinforcement learning;black-box optimization","Interpolation;Tracking loops;State feedback;Reinforcement learning;Trajectory;Feedforward systems;Task analysis","continuous systems;feedforward;interpolation;iterative learning control;learning (artificial intelligence);optimisation;search problems;splines (mathematics);stability;state feedback","reinforcement learning tracking control;unknown continuous dynamic systems;ILC;iterative learning control;precision output tracking;linear states feedback;time-dependent signal;precision tracking;finite time interval;feedforward signal;black-box optimization;integral quadratic tracking error;tracking time interval;minimum tracking error cost;input constraints;spline interpolation;search strategy;squeeze theorem","","1","","16","IEEE","25 Jun 2021","","","IEEE","IEEE Conferences"
"Dual Reinforcement Learning based Attack Path Prediction for 5G Industrial Cyber-Physical Systems","X. Li; X. Hu; T. Jiang","Autonomous Intelligent Unmanned System Engineering Research Center, Ministry of Education of China, Key Laboratory of Ministry of Education for Image Processing and Intelligent Control, Huazhong University of Science and Technology, Wuhan, China; Autonomous Intelligent Unmanned System Engineering Research Center, Ministry of Education of China, Key Laboratory of Ministry of Education for Image Processing and Intelligent Control, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics and School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China","IEEE Internet of Things Journal","","2023","PP","99","1","1","5G industrial cyber-physical systems have attracted substantial research interests due to their capability in the interconnection of everything. However, integrating the 5G network may expose systems to more potential risks. To reveal attack propagation, an attack path prediction approach based on dual reinforcement learning is proposed. First, a dual-network model is established, incorporating the security constraints for attacks against the 5G network into the attack graph. Second, employing reinforcement learning, Q-value updating functions and reward mechanisms based on topology and vulnerability are designed. Finally, an optimal attack path prediction algorithm is developed. Unlike traditional methods, the proposed approach does not rely on the monotonicity assumption that a system component has only one vulnerability, enabling it to accurately predict the optimal attack paths. Our simulation results demonstrate that the proposed approach can identify possible attack sources and paths from a 5G industrial cyber-physical system.","2327-4662","","10.1109/JIOT.2023.3285224","National Natural Science Foundation of China(grant numbers:62173153); the National Key Research and Development Project(grant numbers:2020YFB1708601); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10149069","5G;Industrial cyber-physical system;attack path prediction;dual reinforcement learning;dual-network","5G mobile communication;Security;Topology;Network topology;Behavioral sciences;Reinforcement learning;Cyber-physical systems","","","","1","","","IEEE","12 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Autonomous Highway Merging in Mixed Traffic Using Reinforcement Learning and Motion Predictive Safety Controller","Q. Liu; F. Dang; X. Wang; X. Ren","School of Computer Engineering and Science, Shanghai University, Shanghai, China; Department of Mechanical Engineering, Michigan State University, East Lansing, MI, USA; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China","2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)","1 Nov 2022","2022","","","1063","1069","Deep reinforcement learning (DRL) has a great potential for solving complex decision-making problems in autonomous driving, especially in mixed-traffic scenarios where autonomous vehicles and human-driven vehicles (HDVs) drive together. Safety is a key during both the learning and deploying reinforcement learning (RL) algorithms processes. In this paper, we formulate the on-ramp merging as a Markov Decision Process (MDP) problem and solve it with an off-policy RL algorithm, i.e., Soft Actor-Critic for Discrete Action Settings (SAC-Discrete). In addition, a motion predictive safety controller including a motion predictor and an action substitution module, is proposed to ensure driving safety during both training and testing. The motion predictor estimates the trajectories of the ego vehicle and surrounding vehicles from kinematic models, and predicts potential collisions. The action substitution module replaces risky actions based on safety distance, before sending them to the low-level controller. We train, evaluate and test our approach on a gym-like highway simulator with three different levels of traffic modes. The simulation results show that even in harder traffic densities, the proposed method still significantly reduces collision rate while maintaining high efficiency, outperforming several state-of-the-art baselines in the considered on-ramp merging scenarios. The video demo of the evaluation process can be found at: https://www.youtube.com/watch?v=7FvjbAM4oFw","","978-1-6654-6880-0","10.1109/ITSC55140.2022.9921741","National Key R&D Program of China(grant numbers:2018AAA0102804); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9921741","","Training;Road transportation;Merging;Reinforcement learning;Prediction algorithms;Safety;Behavioral sciences","collision avoidance;decision making;decision theory;learning (artificial intelligence);Markov processes;road safety;road traffic control;road vehicles;safety;telecommunication traffic;traffic engineering computing","Markov Decision Process problem;off-policy RL algorithm;Discrete Action Settings;motion predictive safety controller;motion predictor;action substitution module;driving safety;ego vehicle;risky actions;safety distance;low-level controller;gym-like highway simulator;traffic modes;harder traffic densities;merging scenarios;autonomous highway merging;mixed traffic;deep reinforcement learning;complex decision-making problems;autonomous driving;mixed-traffic scenarios;autonomous vehicles;human-driven vehicles;on-ramp merging","","1","","27","IEEE","1 Nov 2022","","","IEEE","IEEE Conferences"
"Energy Management of Autonomous Electric Vehicles by Reinforcement Learning Techniques","M. Alonso; H. Amaris; D. Martin; A. De La Escalera","Dept. of Electrical Engineering, University Carlos III, Madrid, Spain; Dept. of Electrical Engineering, University Carlos III, Madrid, Spain; Dept. of Systems Engineering and Automation, University Carlos III, Madrid, Spain; Dept. of Systems Engineering and Automation, University Carlos III, Madrid, Spain","2022 Second International Conference on Sustainable Mobility Applications, Renewables and Technology (SMART)","23 Dec 2022","2022","","","1","7","The increase in e-mobility poses new challenges to power grid operators who must cope with the variability and uncertainty of renewable energy sources and customer demand and the electric vehicle integration into the grid. In this paper a Reinforcement Learning algorithm based on Principal Policy optimization is proposed for energy management of electric vehicles and PV storage units. The RL algorithm considers the vehicle battery constraints, range anxiety and battery aging constraints. Moreover, the algorithm controls the charging of the photovoltaic storage unit to minimize the PV energy curtailment. Results show the improvement of the proposed algorithm compared to Business-as-usual and value-iteration solutions.","","978-1-6654-7146-6","10.1109/SMART55236.2022.9990292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9990292","Electric Vehicles;Reinforcement Learning;Proncipal Policy optimization;Energy Management","Photovoltaic systems;Renewable energy sources;Costs;Uncertainty;Anxiety disorders;Reinforcement learning;Aging","ageing;battery powered vehicles;energy management systems;iterative methods;optimisation;photovoltaic power systems;power engineering computing;power grids;reinforcement learning","autonomous electric vehicles;battery aging constraints;customer demand;electric vehicle integration;energy management;photovoltaic storage unit;power grid operators;principal policy optimization;PV energy curtailment;PV storage units;reinforcement learning techniques;renewable energy sources;value-iteration solutions;vehicle battery constraints","","1","","12","IEEE","23 Dec 2022","","","IEEE","IEEE Conferences"
"An Adaptive V2G Capacity-based Frequency Regulation Scheme with Integral Reinforcement Learning against DoS Attacks","J. Sun; G. Qi; Y. Chai; Z. Zhu; J. M. Guerrero","School of Electronic and Information Engineering, Southwest University, Chongqing, P.R.China; Computer Information Systems Department, The State University of New York at Buffalo State, Buffalo, NY, USA; School of Automation, Chongqing University, Chongqing, P.R.China; College of Automation, Chongqing University of posts and telecommunications, Chongqing, P.R.China; Institute of Energy Technology, Aalborg University, Aalborg, Denmark","IEEE Transactions on Smart Grid","","2023","PP","99","1","1","Aggregated electrical vehicles (EVs) can flexibly serve grid frequency regulation (FR) with adjustable FR capacity (FRC) via vehicle-to-grid (V2G) technology. Therefore, current research applies integral reinforcement learning (IRL) to FR as it can easily solve difficult modeling and optimization problems. However, communication between EVs is vulnerable to denial of service (DoS) attacks, which can significantly degrade FR performance and even destabilize V2G FR systems. This paper proposes an adaptive V2G FRC-based FR scheme with IRL to improve FR resilience and mitigate FR performance degradation against DoS attacks. The proposed scheme optimizes V2G control by IRL without analytical models, integrating an event-triggered mechanism to reduce communication burden. It adapts V2G FRC to attack intensity to minimize frequency deviation and mitigate the impact of DoS attacks. The analytical estimation of convergence rate establishes the quantitative relationships among control performance, V2G FRC, and attack intensity, thereby deriving an adaptive mechanism. The theoretical analysis of the proposed scheme yields stability conditions that guarantee the FR performance. The simulations were performed on the IEEE 39-bus test system to verify the effectiveness and advantages of the proposed scheme. The results indicate that V2G FRC should be scaled down to mitigate performance degradation when intensive attacks occur.","1949-3061","","10.1109/TSG.2023.3270564","Chengdu-Chongqing Economic Circle? innovation funding of Chongqing Municipal Education Commission(grant numbers:KJCXZD2020028); National Natural Science Foundation of China(grant numbers:61703347); Natural Science Foundation of Chongqing(grant numbers:stc2021jcyj-msxmX0416); Fundamental Research Funds for the Central Universities(grant numbers:XDJK2020B010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10109154","Frequency Regulation;Denial of Service Attacks;Integral Reinforcement learning;Vehicle to Grid","Vehicle-to-grid;Frequency control;Power system stability;Degradation;Reinforcement learning;Power system dynamics;Convergence","","","","1","","","IEEE","26 Apr 2023","","","IEEE","IEEE Early Access Articles"
"Reinforcement Learning Based Parameter Lookup Table Generating Method for Optimal Torque Control of Induction Motors","X. Qi; W. Cao; L. Aarniovuori","School of Electrical Engineering and Automation, Anhui University, Hefei, China; School of Electrical Engineering and Automation, Anhui University, Hefei, China; Electrical Drives Laboratory, LUT-University, Lappeenranta, Finland","IEEE Transactions on Industrial Electronics","4 Jan 2023","2023","70","5","4516","4525","In the optimal control of induction motors, it is a challenging task to maintain the optimal torque over the varying operation conditions. This paper proposes a parameter lookup table generating method, that can achieve an optimal torque over a wide range of currents and speeds, even though the commands of current are not set correctly. Based on the motor’s testing data, this method uses a reinforcement-learning algorithm to generate parameter lookup tables iteratively. Experimental results show that the proposed method can learn appropriate parameters from the running data to output an optimal torque. The comparative studies show that the proposed method can generate 5%–25% more torque than traditional model-based parameter estimation methods, over a wide range of currents and speeds. Furthermore, the proposed method has a faster convergence feature and a higher identification resolution than many conventional search-based methods.","1557-9948","","10.1109/TIE.2022.3189103","Science and Technology Key Special Project of Anhui Province(grant numbers:2022h11020023,202103a05020019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829029","Induction motor (IM);optimal torque;parameters lookup table;reinforcement learning","Torque;Table lookup;Rotors;Adaptation models;Core loss;Resistance;Mathematical models","iterative methods;learning (artificial intelligence);optimal control;parameter estimation;table lookup;torque control","conventional search-based methods;induction motors;motor;optimal control;optimal torque control;parameter lookup table generating method;parameter lookup tables;reinforcement-learning algorithm;traditional model-based parameter estimation methods","","1","","32","IEEE","13 Jul 2022","","","IEEE","IEEE Journals"
"Frustratingly Easy Regularization on Representation Can Boost Deep Reinforcement Learning","Q. He; H. Su; J. Zhang; X. Hou","Institute of Automation, Chinese Academy of Sciences, Beijing, China; Carnegie Mellon University, Pittsburgh, United States; University of Washington, Seattle, United States; Institute of Automation, Chinese Academy of Sciences, Beijing, China","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","20215","20225","Deep reinforcement learning (DRL) gives the promise that an agent learns good policy from high-dimensional information, whereas representation learning removes irrelevant and redundant information and retains pertinent information. In this work, we demonstrate that the learned representation of the Q-network and its target Q-network should, in theory, satisfy a favorable distinguishable representation property. Specifically, there exists an upper bound on the representation similarity of the value functions of two adjacent time steps in a typical DRL setting. However, through illustrative experiments, we show that the learned DRL agent may violate this property and lead to a sub-optimal policy. Therefore, we propose a simple yet effective regularizer called P_olicy E_valuation with E_asy Regularization on Representation (PEER), which aims to maintain the distinguishable representation property via explicit regularization on internal representations. And we provide the convergence rate guarantee of PEER. Implementing PEER requires only one line of code. Our experiments demonstrate that incorporating PEER into DRL can significantly improve performance and sample efficiency. Comprehensive experiments show that PEER achieves state-of-the-art performance on all 4 environments on PyBullet, 9 out of 12 tasks on DM-Control, and 19 out of 26 games on Atari. To the best of our knowledge, PEER is the first work to study the inherent representation property of Q-network and its target. Our code is available at https://sites.google.com/view/peer-cvpr2023/.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01936","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205264","Deep learning architectures and techniques","Deep learning;Training;Representation learning;Upper bound;Codes;Smoothing methods;Reinforcement learning","control engineering computing;deep learning (artificial intelligence);reinforcement learning","Atari;deep reinforcement learning;DM-Control;DRL setting;high-dimensional information;incorporating PEER;inherent representation property;learned DRL agent;PEER;pertinent information;PyBullet;Q-network;sub-optimal policy","","1","","63","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"A reinforcement learning based algorithm for Markov decision processes","S. Bhatnagar; S. Kumar","Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India","Proceedings of 2005 International Conference on Intelligent Sensing and Information Processing, 2005.","14 Nov 2005","2005","","","199","204","A variant of a recently proposed two-timescale reinforcement learning based actor-critic algorithm for infinite horizon discounted cost Markov decision processes with finite state and compact action spaces is proposed. On the faster timescale, the value function corresponding to a given stationary deterministic policy is updated and averaged while the policy itself is updated on the slower scale. The latter recursion uses the sign of the gradient estimate instead of the estimate itself. A potential advantage in the use of sign function lies in significantly reduced computation and communication overheads in applications such as congestion control in communication networks and distributed computation. Convergence analysis of the algorithm is briefly sketched and numerical experiments for a problem of congestion control are presented.","","0-7803-8840-2","10.1109/ICISIP.2005.1529448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1529448","","Learning;Recursive estimation;Computer networks;Distributed computing;Communication system control;Infinite horizon;Costs;Communication networks;Convergence of numerical methods;Algorithm design and analysis","learning (artificial intelligence);decision theory;Markov processes;gradient methods;convergence","reinforcement learning algorithm;actor-critic algorithm;finite state space;compact action space;stationary deterministic policy;gradient estimation;communication network congestion control;distributed computation;convergence analysis;infinite horizon discounted cost Mark-ov decision processes","","1","","18","IEEE","14 Nov 2005","","","IEEE","IEEE Conferences"
"Model-Based Transfer Reinforcement Learning Based on Graphical Model Representations","Y. Sun; K. Zhang; C. Sun","School of Automation, Southeast University, Nanjing, China; Department of Philosophy, Carnegie Mellon University, Pittsburgh, PA, USA; School of Automation, Southeast University, Nanjing, China","IEEE Transactions on Neural Networks and Learning Systems","3 Feb 2023","2023","34","2","1035","1048","Reinforcement learning (RL) plays an essential role in the field of artificial intelligence but suffers from data inefficiency and model-shift issues. One possible solution to deal with such issues is to exploit transfer learning. However, interpretability problems and negative transfer may occur without explainable models. In this article, we define Relation Transfer as explainable and transferable learning based on graphical model representations, inferring the skeleton and relations among variables in a causal view and generalizing to the target domain. The proposed algorithm consists of the following three steps. First, we leverage a suitable casual discovery method to identify the causal graph based on the augmented source domain data. After that, we make inferences on the target model based on the prior causal knowledge. Finally, offline RL training on the target model is utilized as prior knowledge to improve the policy training in the target domain. The proposed method can answer the question of what to transfer and realize zero-shot transfer across related domains in a principled way. To demonstrate the robustness of the proposed framework, we conduct experiments on four classical control problems as well as one simulation to the real-world application. Experimental results on both continuous and discrete cases demonstrate the efficacy of the proposed method.","2162-2388","","10.1109/TNNLS.2021.3107375","China Scholarship Council; National Key Research and Development Program of China(grant numbers:2018AAA0101400); National Natural Science Foundation of China(grant numbers:61921004); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20202006); U.S. Air Force(grant numbers:FA8650-17-C-7715); National Institutes of Health(grant numbers:R01HL159805); Apple; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9540989","Auxiliary variable;causal graph;reinforcement learning (RL);transfer learning","Task analysis;Data models;Transfer learning;Sun;Graphical models;Training;Reinforcement learning","graph theory;reinforcement learning","artificial intelligence;augmented source domain data;casual discovery method;causal graph;causal knowledge;causal view;explainable learning;explainable models;graphical model representations;interpretability problems;model-based transfer reinforcement;model-shift issues;negative transfer;offline RL training;reinforcement learning;relation transfer;skeleton;target domain;transfer learning;transferable learning;zero-shot transfer","","1","","77","IEEE","20 Sep 2021","","","IEEE","IEEE Journals"
"Optimal Priority Rule-Enhanced Deep Reinforcement Learning for Charging Scheduling in an Electric Vehicle Battery Swapping Station","J. Jin; S. Mao; Y. Xu","College of Information Science and Technology, Donghua University, Shanghai, China; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Central Ave, Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Central Ave, SAR, Hong Kong","IEEE Transactions on Smart Grid","20 Oct 2023","2023","14","6","4581","4593","For a battery swapping station (BSS) with solar generation,  $N$  charging bays, and an inventory of  $M$  batteries, we study the charging scheduling problem under random EV arrivals, renewable generation, and electricity prices. To minimize the expected weighted sum of charging cost (sum of electricity and battery degradation costs) and EV owners’ waiting cost, we formulate the problem as a Markov decision process with unknown state transition probability. Under a mild heavy-traffic assumption, we rigorously establish the optimality of the Less Demand First (LDF) priority rule under arbitrary system dynamics: batteries with less demand shall be charged first. The optimality result enables us to integrate the LDF rule into a state-of-the-art deep reinforcement learning (DRL) method, proximal policy optimization (PPO), reducing the dimensionality of its output from  $O(M+N)$  to  $O(1)$ , without loss of optimality in the heavy-traffic scenario. Numerical results (on real-world data) demonstrate that the proposed LDF enhanced PPO approach significantly outperforms classical DRL methods and FCFS (first come, first served) priority rule based DRL counterparts.","1949-3061","","10.1109/TSG.2023.3250505","General Research Fund (GRF) of the Hong Kong University Grants Committee(grant numbers:14200720); National Natural Science Foundation of China (NSFC)(grant numbers:62073273); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10056371","Electric vehicle;battery swapping station;Markov decision process;deep reinforcement learning;renewable generation","Batteries;Costs;Renewable energy sources;Optimization;Dynamic scheduling;Reinforcement learning;Processor scheduling","","","","1","","49","IEEE","28 Feb 2023","","","IEEE","IEEE Journals"
"Prescribed-Time Formation Control for a Class of Multi-agent Systems via Fuzzy Reinforcement Learning","Y. Zhang; M. Chadli; Z. Xiang","School of Automation, Nanjing University of Science and Technology, Nanjing, China; University Paris-Saclay, Univ Evry, IBISC, Evry, France; School of Automation, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Fuzzy Systems","","2023","PP","99","1","10","This paper concerns optimal prescribed-time forma- tion control for a class of nonlinear multi-agent systems (MASs). Optimal control depends on the solution of the Hamilton-Jacobi- Bellman equation, which is hard to be calculated directly due to its inherent nonlinearity. To overcome this difficulty, the rein- forcement learning strategy with fuzzy logic systems is proposed, in which identifier, actor, and critic are used to estimate unknown nonlinear dynamics, implement control behavior, and evaluate system performance, respectively. Different from the existing optimal control algorithms, a new performance index function considering formation error cost and control input energy cost is constructed to achieve optimal formation control of MASs within a prescribed time. The presented control strategy can ensure that the formation error converges to the desired accuracy within a prescribed time. Finally, the validity of the presented strategy is verified via a simulation example.","1941-0034","","10.1109/TFUZZ.2023.3277480","National Natural Science Foundation of China(grant numbers:61873128,62273210); Research and Practice Innovation Program of Jiangsu Province; Shandong Provincial Natural Science Foundation(grant numbers:ZR2021MF052,ZR2020MF062); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129082","prescribed-time control;formation control;non- linear multi-agent systems;reinforcement learning;fuzzy logic systems;Hamilton-Jacobi-Bellman","Formation control;Optimal control;Reinforcement learning;Performance analysis;Multi-agent systems;Protocols;Heuristic algorithms","","","","1","","","IEEE","18 May 2023","","","IEEE","IEEE Early Access Articles"
"Curiosity Driven Deep Reinforcement Learning for Motion Planning in Multi-agent Environment","G. Perovic; N. Li","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)","20 Jan 2020","2019","","","375","380","Motion planning in dynamic multi-agent environment tends to be a challenging feat by itself. If the agent is also required to complete an elaborate task the complexity of the problem substantially increases. Heuristically designing policies for unstructured environments can be unfeasible and time consuming for certain scenarios. We propose a Deep Reinforcement Learning approach for a continuous multi-agent setting that is robust enough to handle high-level task accomplishment and low-level collision avoidance control. To alleviate disadvantages of a sparse reward environment we introduce intrinsic reward inspired by the curious behavior of animals and humans. As for the low-level, collision avoidance segment of control, we introduce a general reward function. Furthermore, we formulate an agent-centric state space and implement robust Reinforcement Learning algorithm that is capable of handling reward signal from both sources. Suitable 3D physical environment is constructed to examine the feasibility of our approach. Subsequently, case study is performed and effectiveness of our method is validated when compared with the state-of-the-art Reinforcement Learning algorithm.","","978-1-7281-6321-5","10.1109/ROBIO49542.2019.8961660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8961660","autonomous agents;collision avoidance;motion planning;multi-agent systems;robot learning","","collision avoidance;control engineering computing;learning (artificial intelligence);multi-agent systems;robot programming","multiagent environment;motion planning;curiosity driven Deep Reinforcement Learning;3D physical environment;robust Reinforcement Learning algorithm;agent-centric state space;general reward function;intrinsic reward;sparse reward environment;low-level collision avoidance control;high-level task accomplishment;continuous multiagent;Deep Reinforcement Learning approach;unstructured environments","","1","","22","IEEE","20 Jan 2020","","","IEEE","IEEE Conferences"
"An Online Deep Reinforcement Learning-Based Order Recommendation Framework for Rider-Centered Food Delivery System","X. Wang; L. Wang; C. Dong; H. Ren; K. Xing","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; School of Mechanical and Automotive Engineering, Qingdao Hengxing University of Science and Technology, Qingdao, Shandong, China; Meituan, Beijing, China; Meituan, Beijing, China","IEEE Transactions on Intelligent Transportation Systems","8 May 2023","2023","24","5","5640","5654","As an important part of intelligent transportation systems, On-demand Food Delivery (OFD) becomes a prevalent logistics service in modern society. With the continuously increasing scale of transactions, rider-centered assignment manner is gaining more attraction than traditional platform-centered assignment among food delivery companies. However, problems such as dynamic arrivals of orders, uncertain rider behaviors and various false-negative feedbacks inhibit the platform to make a proper decision in the interaction process with riders. To address such issues, we propose an online Deep Reinforcement Learning-based Order Recommendation (DRLOR) framework to solve the decision-making problem in the scenario of OFD. The problem is modeled as a Markov Decision Process (MDP). The DRLOR framework mainly consists of three networks, i.e., the actor-critic network that learns an optimal order ranking policy at each interaction step, the rider behavior prediction network that predicts the grabbing behavior of riders and the feedback correlation network based on attention mechanism that identifies valid feedback information from false feedbacks and learns a high-dimensional state embedding to represent the states of riders. Extensive offline and online experiments are conducted on Meituan delivery platform and the results demonstrate that the proposed DRLOR framework can significantly shorten the length of interactions between riders and the platform, leading to a better experience of both riders and customers.","1558-0016","","10.1109/TITS.2023.3237580","National Natural Science Foundation of China(grant numbers:62273193); Tsinghua University–Meituan Joint Institute for Digital Life; Research and Development Project of China Railway Signal & Communication Corp. (CRSC) Research and Design Institute Group Company Ltd; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024429","On-demand food delivery;deep reinforcement learning;order recommendation;attention mechanism;feedback information","Behavioral sciences;Decision making;Uncertainty;Task analysis;Deep learning;Transportation;Reinforcement learning","decision making;deep learning (artificial intelligence);feedback;intelligent transportation systems;learning (artificial intelligence);logistics;Markov processes;recommender systems;reinforcement learning","actor-critic network;decision-making problem;DRLOR framework;false feedbacks;false-negative feedbacks;feedback correlation network;food delivery companies;intelligent transportation systems;interaction process;Markov Decision Process;Meituan delivery platform;OFD;On-demand Food Delivery;online deep reinforcement learning-based order recommendation framework;optimal order ranking policy;prevalent logistics service;proper decision;rider behavior prediction network;rider-centered assignment manner;rider-centered food delivery system;riders;traditional platform-centered assignment;uncertain rider behaviors","","1","","51","IEEE","23 Jan 2023","","","IEEE","IEEE Journals"
"On the Combination of PID control and Reinforcement Learning: A Case Study with Water Tank System","Y. Wu; L. Xing; F. Guo; X. Liu","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Automation, Zhejiang University of Technology, Hangzhou, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","2021 IEEE 16th Conference on Industrial Electronics and Applications (ICIEA)","30 Aug 2021","2021","","","1877","1882","Reinforcement learning (RL) has attracted great interest from researchers in recent years. RL performs as human or better in many fields such as games and robot control. Although this technology is booming in computer science, it has not been practically applied in industrial process control. Up to now, proportional–integral–derivative (PID) control is still the most dominating and popular control method in industrial control. In this paper, we propose a combination of deep reinforcement learning (DRL) and PID control for better process control performance. The idea is generated by the following observations: for PID controller, its transient performance is not usually well enough to meet a strict requirement or in complex signal tracking tasks; For RL technology, a perfectly designed reward function is required for training. However, in practice, the reward function needs to be tested through trial and error, which will lead to a waste of computational power and time. By combining these two strategies, PID controller can help to improve the steady-state performance of RL control by its integral term, while the trained RL agent is able to improve the transient performance of PID controller. Several case studies with the water tank system are presented to demonstrate the effectiveness of the combined PID + RL control strategy.","2158-2297","978-1-6654-2248-2","10.1109/ICIEA51954.2021.9516140","Nanyang Technological University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516140","PID;Reinforcement learning;Water tank system","Training;PI control;Robot control;Process control;Reinforcement learning;Storage tanks;Steady-state","control engineering computing;learning (artificial intelligence);process control;three-term control","industrial process control;proportional-integral-derivative control;deep reinforcement learning;process control performance;transient performance;water tank system;robot control;PID + RL control strategy;signal tracking;steady-state performance","","1","","12","IEEE","30 Aug 2021","","","IEEE","IEEE Conferences"
"Nearly Optimal Control for Mixed Zero-Sum Game Based on Off-Policy Integral Reinforcement Learning","R. Song; G. Yang; F. L. Lewis","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; UTA Research Institute, The University of Texas at Arlington, Arlington, TX, USA","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","12","In this article, we solve a class of mixed zero-sum game with unknown dynamic information of nonlinear system. A policy iterative algorithm that adopts integral reinforcement learning (IRL), which does not depend on system information, is proposed to obtain the optimal control of competitor and collaborators. An adaptive update law that combines critic-actor structure with experience replay is proposed. The actor function not only approximates optimal control of every player but also estimates auxiliary control, which does not participate in the actual control process and only exists in theory. The parameters of the actor-critic structure are simultaneously updated. Then, it is proven that the parameter errors of the polynomial approximation are uniformly ultimately bounded. Finally, the effectiveness of the proposed algorithm is verified by two given simulations.","2162-2388","","10.1109/TNNLS.2022.3191847","National Natural Science Foundation of China(grant numbers:61873300,61722312); Fundamental Research Funds for the Central Universities(grant numbers:FRF-MP-20-11); Interdisciplinary Research Project for Young Teachers of the University of Science and Technology Beijing (USTB) (Fundamental Research Funds for the Central Universities)(grant numbers:FRF-IDRY-20-030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9839458","Actor-critic structure;integral reinforcement learning (IRL);mixed zero-sum (MZS) game;polynomial approximation","Games;Optimal control;Mathematical models;Game theory;Artificial neural networks;System dynamics;Convergence","","","","1","","","IEEE","25 Jul 2022","","","IEEE","IEEE Early Access Articles"
"Collision-Aware UAV Trajectories for Data Collection via Reinforcement Learning","X. Wang; M. C. Gursoy; T. Erpek; Y. E. Sagduyu","Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY; Intelligent Automation, Inc., Rockville, MD; Intelligent Automation, Inc., Rockville, MD","2021 IEEE Global Communications Conference (GLOBECOM)","2 Feb 2022","2021","","","1","6","Unmanned aerial vehicles (UAVs) are expected to be an integral part of wireless networks, and determining collision-free trajectories in multi-UAV non-cooperative scenarios is a challenging task. In this paper, we consider a path planning optimization problem to maximize the collected data from multiple Internet of Things (IoT) nodes under realistic constraints. The considered multi-UAV non-cooperative scenarios involve random number of other UAVs in addition to the typical UAV, and UAVs do not communicate with each other. We translate the problem into an Markov decision process (MDP). Dueling double deep Q-network (D3QN) is proposed to learn the decision making policy for the typical UAV, without any prior knowledge of the environment (e.g., channel propagation model and locations of the obstacles) and other UAVs (e.g., their missions, movements, and policies). Numerical results demonstrate that real-time navigation can be efficiently performed with high success rate, high data collection rate, and low collision rate.","","978-1-7281-8104-2","10.1109/GLOBECOM46510.2021.9686015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9686015","Data collection;multi-UAV scenarios;path planning;collision avoidance;deep reinforcement learning","Training;Navigation;Reinforcement learning;Data collection;Autonomous aerial vehicles;Real-time systems;Trajectory","aerospace control;autonomous aerial vehicles;collision avoidance;decision making;learning (artificial intelligence);Markov processes;mobile robots;optimisation","collision-aware UAV trajectories;reinforcement learning;unmanned aerial vehicles;collision-free trajectories;multiUAV noncooperative scenarios;typical UAV;dueling double deep Q-network;high data collection rate;multiple Internet of Things nodes;IoT nodes;Markov decision process;D3QN;decision making policy","","1","","18","IEEE","2 Feb 2022","","","IEEE","IEEE Conferences"
"Disturbance rejection of multi-agent systems: A reinforcement learning differential game approach","Qiang Jiao; H. Modares; Shengyuan Xu; F. L. Lewis; K. G. Vamvoudakis","School of Automation, Nanjing University of Science and Technology, Nanjing, P.R. China; University of Texas at Arlington Research Institute, TX, USA; School of Automation, Nanjing University of Science and Technology, Nanjing, P.R. China; University of Texas at Arlington Research Institute, TX, USA; Center for Control, Dynamical-systems and Computation(CCDC), University of California, Santa Barbara, CA, USA","2015 American Control Conference (ACC)","30 Jul 2015","2015","","","737","742","Distributed tracking control of multi-agent linear systems in the presence of disturbances is considered in this paper. The given problem is first formulated into a multi-player zero-sum differential graphical game. It is shown that the solution to this problem requires solving the coupled Hamilton-Jacobi-Isaacs (HJI) equations. A multi-agent reinforcement learning algorithm is developed to find the solution to these coupled HJI equations. The convergence of this algorithm to the optimal solution is proven. It is also shown that the proposed method guarantees L2-bounded synchronization errors in the presence of dynamical disturbances.","2378-5861","978-1-4799-8684-2","10.1109/ACC.2015.7170822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7170822","","Games;Synchronization;Learning (artificial intelligence);Convergence;Nash equilibrium;Heuristic algorithms","convergence of numerical methods;differential games;directed graphs;learning (artificial intelligence);linear matrix inequalities;linear systems;multi-agent systems;synchronisation","disturbance rejection;reinforcement learning differential game approach;distributed tracking control;multiagent linear systems;multiplayer zero-sum differential graphical game;coupled Hamilton-Jacobi-Isaacs equations;coupled HJI equations;multiagent reinforcement learning algorithm;algorithm convergence;optimal solution;L2-bounded synchronization errors;dynamical disturbances","","1","","25","","30 Jul 2015","","","IEEE","IEEE Conferences"
"GE-DDRL: Graph Embedding and Deep Distributional Reinforcement Learning for Reliable Shortest Path: A Universal and Scale Free Solution","H. Guo; W. Sheng; Y. Zhou; Y. Chen","College of Computer Science, Sichuan University, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Intelligent Transportation Systems","","2023","PP","99","1","19","This paper studies the reliable shortest path (RSP) problem in stochastic transportation networks. State-of-the-art RSP solutions usually target one specific RSP problem; moreover, the corresponding algorithm’s computational complexity scales at least linearly with the size of the underlying transportation network. While in this paper, we propose a graph embedding and deep distributional reinforcement learning (GE-DDRL) method, which serves as a universal and scale-free solution to the RSP problem. GE-DDRL uses deep distributional reinforcement learning (DDRL) to estimate the full travel-time distribution of a given routing policy, and improves the given routing policy with the generalized policy iteration (GPI) scheme. Further, in order to achieve the generalization ability to new destination nodes, we employ one of the canonical graph embedding techniques (Skip-Gram) to compress the nodes’ representation into  $d$ -dimensional real-valued vectors. With the properly compressed node features, GE-DDRL is able to generalize its estimation of the routing policy’s travel-time distribution to untrained destination nodes, and hence achieve the ‘all-to-all’ navigation functionality. To the best of our knowledge, GE-DDRL serves as the first RSP planner, which applies simultaneously to almost all RSP objectives and in the meanwhile, is scale free with the size of the transportation network in terms of the online decision-making time and memory complexity. Experimental results and comparisons with state of the arts show the efficacy and efficiency of GE-DDRL in a range of transportation networks.","1558-0016","","10.1109/TITS.2023.3285770","Higher Education Discipline Innovation Project(grant numbers:B21044); Sichuan Science and Technology Program(grant numbers:2023NSFSC1965); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10159570","Reliable shortest path (RSP);distributional reinforcement learning (DRL);universal and scale free solution;graph embedding and deep distributional reinforcement learning (GE-DDRL);skip-gram","Reliability;Transportation;Navigation;Routing;Reinforcement learning;Planning;Bibliographies","","","","","","","IEEE","22 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Structural Parameter Space Exploration for Reinforcement Learning via a Matrix Variate Distribution","S. Wang; R. Yang; B. Li; Z. Kan","Department of Automation, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application Systems, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application Systems, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China","IEEE Transactions on Emerging Topics in Computational Intelligence","21 Jul 2023","2023","7","4","1025","1035","The trade-off between exploration and exploitation is essential for reinforcement learning, where an agent needs to be aware of when to explore for high reward policies and when to exploit the optimal policy known so far. Parameter space exploration provides an elegant solution. As one of the principal methods, injecting noise into the model parameters greatly improves exploration. However, directly stretching the parameters of the neural network into a vector and generating noise for this vector ignore the structural information of the model. In this paper, we aim to incorporate spatial information into weight matrices and propose matrix-variate noise exploration, which exploits the structural weight uncertainty brought by matrix variate noise to enhance the stochasticity of the agent. Indeed, we construct a bridge between the matrix noise exploration and probabilistic neural networks, which theoretically explains the improved performance of parameter space exploration. Extensive experiments have shown that matrix variate noise exploration outperforms fully factorized noisy exploration on most Atari tasks and Super Mario Bros tasks and is competitive to the state-of-the-art methods.","2471-285X","","10.1109/TETCI.2022.3140380","National Natural Science Foundation of China(grant numbers:U2013601,62173314,U19B2044,61836011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9679819","Reinforcement learning;parameter space exploration;weight uncertainty","Uncertainty;Neural networks;Covariance matrices;Space exploration;Noise measurement;Matrix decomposition;Symmetric matrices","matrix algebra;neural nets;reinforcement learning;vectors","high reward policies;matrix noise exploration;matrix variate distribution;matrix variate noise exploration;matrix-variate noise exploration;model parameters;noisy exploration;optimal policy;reinforcement learning;structural information;structural parameter space exploration;structural weight uncertainty;vector","","","","36","IEEE","13 Jan 2022","","","IEEE","IEEE Journals"
"Dynamics-Adaptive Continual Reinforcement Learning via Progressive Contextualization","T. Zhang; Z. Lin; Y. Wang; D. Ye; Q. Fu; W. Yang; X. Wang; B. Liang; B. Yuan; X. Li","Department of Automation, Tsinghua University, Beijing, China; Tencent AI Lab, Shenzhen, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Tencent AI Lab, Shenzhen, China; Tencent AI Lab, Shenzhen, China; Tencent AI Lab, Shenzhen, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Department of Automation, Tsinghua University, Beijing, China; Research Institute of Tsinghua University in Shenzhen, Shenzhen, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","15","A key challenge of continual reinforcement learning (CRL) in dynamic environments is to promptly adapt the reinforcement learning (RL) agent’s behavior as the environment changes over its lifetime while minimizing the catastrophic forgetting of the learned information. To address this challenge, in this article, we propose DaCoRL, that is, dynamics-adaptive continual RL. DaCoRL learns a context-conditioned policy using progressive contextualization, which incrementally clusters a stream of stationary tasks in the dynamic environment into a series of contexts and opts for an expandable multihead neural network to approximate the policy. Specifically, we define a set of tasks with similar dynamics as an environmental context and formalize context inference as a procedure of online Bayesian infinite Gaussian mixture clustering on environment features, resorting to online Bayesian inference to infer the posterior distribution over contexts. Under the assumption of a Chinese restaurant process (CRP) prior, this technique can accurately classify the current task as a previously seen context or instantiate a new context as needed without relying on any external indicator to signal environmental changes in advance. Furthermore, we employ an expandable multihead neural network whose output layer is synchronously expanded with the newly instantiated context and a knowledge distillation regularization term for retaining the performance on learned tasks. As a general framework that can be coupled with various deep RL algorithms, DaCoRL features consistent superiority over existing methods in terms of stability, overall performance, and generalization ability, as verified by extensive experiments on several robot navigation and MuJoCo locomotion tasks.","2162-2388","","10.1109/TNNLS.2023.3280085","Science and Technology Innovation 2030-Key Project(grant numbers:2021ZD0201404); Tencent Rhino-Bird Research Elite Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10145851","Adaptive network expansion;continual reinforcement learning (CRL);dynamic environment;incremental context detection","Task analysis;Context modeling;Adaptation models;Neural networks;Heuristic algorithms;Bayes methods;Vehicle dynamics","","","","","","","IEEE","7 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Data Efficient Safe Reinforcement Learning","S. Padakandla; P. K. J; S. Ganguly; S. Bhatnagar","Dept. of Computer Science and Automation, Indian Institute of Science; Dept. of Computer Science Engineering, IIT Dharwad; Dept. of Computer Science Engineering, IIT Dharwad; Dept. of Computer Science and Automation, Indian Institute of Science","2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","18 Nov 2022","2022","","","1167","1172","Applying reinforcement learning (RL) methods for real world applications pose multiple challenges - the foremost being safety of the system controlled by the learning agent and the learning efficiency. An RL agent learns to control a system by exploring the available actions in various operating states. In some states, when the RL agent exercises an exploratory action, the system may enter unsafe operation, which can lead to safety hazards both for the system as well as for humans supervising the system. RL algorithms thus must learn to control the system respecting safety. In this work, we formulate the safe RL problem in the constrained off-policy setting that facilitates safe exploration by the RL agent. We then develop a sample efficient algorithm utilizing the cross-entropy method. The proposed algorithm’s safety performance is evaluated numerically on benchmark RL problems.","2577-1655","978-1-6654-5258-8","10.1109/SMC53654.2022.9945313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9945313","Safe exploration;Constrained RL;Off-Policy","Reinforcement learning;Benchmark testing;Prediction algorithms;Control systems;Hazards;Entropy;Cybernetics","entropy;multi-agent systems;reinforcement learning","cross-entropy method;exploratory action;learning agent;learning efficiency;operating states;reinforcement learning;RL agent;safety hazards;sample efficient algorithm","","","","26","IEEE","18 Nov 2022","","","IEEE","IEEE Conferences"
"Connectivity Restoration of Urban Road Networks With Multivehicle Systems: A Reinforcement Learning Approach","Z. Luo; B. Yang","School of Automation, Wuhan University of Technology, Wuhan, China; School of Automation, Wuhan University of Technology, Wuhan, China","IEEE Systems Journal","25 Aug 2022","2022","16","3","4731","4734","The connectivity of urban road networks plays an important role in ensuring the functions of the infrastructure and the transportation of emergency supplies of a postdisaster city. When a disaster happens, clearing vehicles that are able to open blocked roads are precious resources in connectivity restoration and their work should be planned systematically and promptly. In this article, we proposed a novel reinforcement learning approach to swiftly providing restoration plans for clearing vehicles according to damage situations. This is a cluster-first, route-second type of method that includes an event-triggering algorithm to deal with the time-space model of vehicles and a specially designed cognitive module to decide the vehicle behaviors, which is represented by three kinds of preference in the behavior probability: the guidance, the memory, and the rules. Our approach has been tested on damaged Istanbul and synthetic road networks, confirming its superior capacity to generate near-optimal solutions in an extraordinarily short time, which is more practical and efficient in dealing with the case of multivehicle systems operating on complex large-scale urban road networks.","1937-9234","","10.1109/JSYST.2022.3181146","National Natural Science Foundation of China(grant numbers:61203032); Hubei Provincial Natural Science Foundation of China(grant numbers:2012FFB05007); China Scholarship Council(grant numbers:201406955049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9805664","Connectivity restoration;disaster response;multivehicle systems;reinforcement learning;road networks","Roads;Behavioral sciences;Urban areas;Synchronization;Reinforcement learning;Simulation;Search problems","disasters;emergency management;emergency services;learning (artificial intelligence);probability;road traffic;road vehicles;roads;traffic engineering computing;transportation","connectivity restoration;multivehicle systems;reinforcement learning approach;emergency supplies;postdisaster city;clearing vehicles;blocked roads;precious resources;novel reinforcement;restoration plans;time-space model;specially designed cognitive module;vehicle behaviors;synthetic road networks;large-scale urban road networks","","","","18","IEEE","23 Jun 2022","","","IEEE","IEEE Journals"
"Improving Speech Separation with Adversarial Network and Reinforcement Learning","G. Liu; J. Shi; X. Chen; J. Xu; B. Xu","Chinese Academy of Sciences (CASIA), Institute of Automation, Beijing, China; University of Chinese Academy of Sciences; University of Chinese Academy of Sciences; Chinese Academy of Sciences (CASIA), Institute of Automation, Beijing, China; Center for Excellence in Brain Science and Intelligence Technology, CAS, China","2018 International Joint Conference on Neural Networks (IJCNN)","14 Oct 2018","2018","","","1","7","In contrast to the conventional deep neural network for single-channel speech separation, we propose a separation framework based on adversarial network and reinforcement learning. The purpose of the adversarial network inspired by the generative adversarial network is to make the separated result and ground-truth with the same data distribution by evaluating the discrepancy between them. Meanwhile, in order to enable the model to bias the generation towards desirable metrics and reduce the discrepancy between training loss (such as mean squared error) and testing metric (such as SDR), we present the future success based on reinforcement learning. We directly optimize the performance metric to accomplish exactly that. With the combination of adversarial network and reinforcement learning, our model is able to improve the performance of single-channel speech separation.","2161-4407","978-1-5090-6014-6","10.1109/IJCNN.2018.8489444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489444","speech separation;generative adversarial network;reinforcement learning","Training;Learning (artificial intelligence);Machine learning;Time-frequency analysis;Generative adversarial networks;Testing","learning (artificial intelligence);neural nets;optimisation;source separation;speech processing","reinforcement learning;single-channel speech separation;conventional deep neural network;separation framework;generative adversarial network;training loss;testing metric;mean squared error;SDR;optimization;performance metric","","","","26","IEEE","14 Oct 2018","","","IEEE","IEEE Conferences"
"Optimal Containment Control of a Quadrotor Team With Active Leaders via Reinforcement Learning","M. Cheng; H. Liu; Q. Gao; J. Lü; X. Xia","School of Astronautics, Beihang University, Beijing, China; Institute of Artificial Intelligence, Beihang University, Beijing, China; Zhongguancun Laboratory, Beijing, China; Zhongguancun Laboratory, Beijing, China; Department of Electrical, Electronic and Computer Engineering, University of Pretoria, Pretoria, South Africa","IEEE Transactions on Cybernetics","","2023","PP","99","1","11","This article proposes an optimal controller for a team of underactuated quadrotors with multiple active leaders in containment control tasks. The quadrotor dynamics are underactuated, nonlinear, uncertain, and subject to external disturbances. The active team leaders have control inputs to enhance the maneuverability of the containment system. The proposed controller consists of a position control law to guarantee the achievement of position containment and an attitude control law to regulate the rotational motion, which are learned via off-policy reinforcement learning using historical data from quadrotor trajectories. The closed-loop system stability can be guaranteed by theoretical analysis. Simulation results of cooperative transportation missions with multiple active leaders demonstrate the effectiveness of the proposed controller.","2168-2275","","10.1109/TCYB.2023.3284648","Beijing Natural Science Foundation(grant numbers:4232045); National Natural Science Foundation of China(grant numbers:62273015,61873012); Beijing Nova Program; National Science Foundation(grant numbers:1730675,1714519); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10164021","Cooperative control;multiagent system;optimal control;quadrotor;reinforcement learning (RL)","Quadrotors;Vehicle dynamics;Optimal control;Reinforcement learning;Protocols;Position control;Multi-agent systems","","","","","","","IEEE","27 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Bounds for off-policy prediction in reinforcement learning","A. G. Joseph; S. Bhatnagar","Dept. of Computer Science and Automation, Indian Institute of Science, Bangalore; Dept. of Computer Science and Automation, Indian Institute of Science, Bangalore","2017 International Joint Conference on Neural Networks (IJCNN)","3 Jul 2017","2017","","","3991","3997","In this paper, we provide for the first time, error bounds for the off-policy prediction in reinforcement learning. The primary objective in off-policy prediction is to estimate the value function of a given target policy of interest using the linear function approximation architecture by utilizing a sample trajectory generated by a behaviour policy which is possibly different from the target policy. The stability of the off-policy prediction has been an open question for a long time. Only recently, could Yu provide a generalized proof, which makes our results more appealing to the reinforcement learning community. The off-policy prediction is useful in complex reinforcement learning settings, where the sample trajectory is hard to obtain and one has to rely on the sample behaviour of the system with respect to an arbitrary policy. We provide here error bound on the solution of the off-policy prediction with respect to a closeness measure between the target and the behaviour policy.","2161-4407","978-1-5090-6182-2","10.1109/IJCNN.2017.7966359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966359","","Approximation algorithms;Markov processes;Learning (artificial intelligence);Prediction algorithms;Trajectory;Function approximation;Random variables","function approximation;learning (artificial intelligence)","off-policy prediction;reinforcement learning;value function;linear function approximation architecture;behaviour policy","","","","12","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Data-driven Optimal Control Strategy for Systems with Disturbance","Z. -X. Fan; S. Li; R. Liu","School of Automation, Southeast University, Nanjing, P. R. China; School of Automation, Southeast University, Nanjing, P. R. China; Department of Statistics, Florida State University, Tallahassee, FL, USA","2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)","7 Jul 2023","2023","","","567","572","This paper proposes a partially model-free optimal control strategy for a class of continuous-time systems in a data-driven way. Although a series of optimal control have achieving superior performance, the following challenges still exist: (i) The controller designed based on the nominal system is difficult to cope with sudden disturbances. (ii) Feedback control is highly dependent on system dynamics and generally requires full state information. A novel composite control method combining output feedback reinforcement learning and input-output disturbance observer for these two challenges is concluded in this paper. Firstly, an output feedback policy iteration (PI) algorithm is given to acquire the feedback gain iteratively. Simultaneously, the observer continuously provides estimates of the disturbance. System dynamic information and states information are not needed to be known in advance in our approach, thus offering a higher degree of robustness and practical implementation prospects. Finally, an example is given to show the effectiveness of the proposed controller.","2767-9861","979-8-3503-2105-0","10.1109/DDCLS58216.2023.10167230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10167230","Reinforcement learning;data-driven;input-output data;disturbance observer;adaptive dynamic programming;output feedback","Learning systems;System dynamics;Riccati equations;Optimal control;Reinforcement learning;Robustness;Disturbance observers","continuous time systems;control system synthesis;feedback;iterative methods;observers;optimal control","composite control method;continuous-time systems;data-driven optimal control strategy;feedback gain;input-output disturbance observer;model-free optimal control strategy;nominal system;output feedback policy iteration algorithm;output feedback reinforcement learning;PI algorithm;state information;sudden disturbances;system dynamics","","","","35","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"Prior Knowledge-Augmented Broad Reinforcement Learning Framework for Fault Diagnosis of Heterogeneous Multiagent Systems","L. Guo; Y. Ren; R. Li; B. Jiang","School of Electrical Engineering, Anhui Polytechnic University, Wuhu, China; postgraduate with the School of Electrical Engineering, Anhui Polytechnic University, Wuhu, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Transactions on Cognitive and Developmental Systems","","2023","PP","99","1","1","A heterogeneous multiagent system (MAS) can easily experience unpredictable faults due to its complex structure and involvement of different individuals. However, existing approaches have several issues, including complicated network architecture, insufficient feature extraction, and poor generalization ability. This study proposes a novel framework called prior-knowledge-augmented broad reinforcement learning (PK-BRL) to effectively diagnose faults in a heterogeneous MAS. First, we construct a highly realistic visualized heterogeneous MAS and perfom fault injection. Second, we present a novel fault diagnosis framework based on broad reinforcement learning with prior knowledge that effectively integrates offline reinforcement learning and broad learning into the fault diagnosis process. The interaction between heterogeneous multiagents and the constructed environment enables us to learn a superior fault diagnosis strategy. Finally, experiments conducted on software-in-the-loop and hardware-in-the-loop platforms demonstrate that the proposed PK-BRL framework has a state-of-the-art diagnostic accuracy for the heterogeneous MAS, which offers valuable theoretical and practical significance for real-world application.","2379-8939","","10.1109/TCDS.2023.3266791","the National Natural Science Foundation of China(grant numbers:62263010, 62020106003); High Level Talent Research Start-up Fund of Anhui Polytechnic University(grant numbers:2022YQQ052); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10102288","Heterogeneous multiagent systems;distributed fault diagnosis;prior knowledge augmentation;broad reinforcement learning","Feature extraction;Reinforcement learning;Fault diagnosis;Training;Multi-agent systems;Data mining;Bridges","","","","","","","IEEE","13 Apr 2023","","","IEEE","IEEE Early Access Articles"
"Adaptive Optimal Consensus Control of Multiagent Systems With Unknown Dynamics and Disturbances via Reinforcement Learning","L. Chen; C. Dong; S. -L. Dai","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; South China Sea Marine Survey Center, Ministry of Natural Resources, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Artificial Intelligence","","2023","PP","99","1","11","An adaptive optimal consensus control design technique is presented for uncertain multiagent systems with prescribed performance guarantees using reinforcement learning (RL) algorithm. Firstly, an adaptive neural network identifier is employed to learn the knowledge of uncertain system dynamics and a disturbance observer is developed to compensate for time-varying disturbances. Secondly, a critic-network learning structure is established to obtain the approximate solution of Hamilton-Jacobi-Bellman (HJB) equations of multiagent systems. Then, an experience replay method is applied to update the critic network weights without requiring persistence of excitation condition. Thirdly, RL-based optimized consensus controllers are developed such that (i) the cost function is minimized, (ii) transient and steady-state performances of consensus error systems are guaranteed, and (iii) uniform ultimate boundedness of the closed-loop systems is achieved. Finally, application to consensus control of unmanned surface vehicles with uncertain hydrodynamic dampings is given to demonstrate the effectiveness of the optimal control design technique.","2691-4581","","10.1109/TAI.2023.3324895","National Natural Science Foundation of China(grant numbers:42227901,61973129); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2021A1515012004,2023A1515011423); Innovation Group Project of Southern Marine Science and Engineering Guangdong Laboratory (Zhuhai)(grant numbers:SML2022008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286389","Reinforcement learning (RL);neural networks (NNs);prescribed performance;distributed optimal consensus control;unmanned surface vehicles (USVs)","Multi-agent systems;Consensus control;Optimal control;Transient analysis;Heuristic algorithms;Autonomous vehicles;Vehicle dynamics","","","","","","","IEEE","16 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Autonomous tumor palpation and resection path planning using tactile array sensor and deep reinforcement learning for surgical robot","F. Ju; H. Ye; D. Bai; Y. Zhang; C. Zhu; Y. Cao; W. Yue","College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Jinhua Polytechnic, Key Laboratory of Crop Harvesting Equipment Technology of Zhejiang Province, Jinhua, China; School of Control Science and Engineering, Shandong University, Jinan, China; Department of Oncology, Jiangsu Province Hospital, Nanjing, China; Department of Mechanical and Automation Engineering and Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Mechanical and Automation Engineering and Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong","2022 IEEE International Conference on Robotics and Biomimetics (ROBIO)","18 Jan 2023","2022","","","871","876","Surgical robots have been widely used in tumor resection, but they still have shortcomings such as the lack of tactile perception, which may lead to inaccurate intraoperative tumor identification and resection. A piezoresistive tactile array sensor is proposed in this paper, which features small size (10 mm X 10 mm) as well as the high-efficiency array detection mode. The sensing principle is simply applying a constant voltage to each tactile element and measuring the current to generate a tactile image. Its effectiveness and performance are verified by finite element simulations. In addition, a deep reinforcement learning-based autonomous detection algorithm is developed to further improve the detection efficiency and facilitate the planning of the resection path, which provides an effective guarantee for accurate tumor resection in future autonomous robotic surgeries.","","978-1-6654-8109-0","10.1109/ROBIO55434.2022.10012022","National Natural Science Foundation of China(grant numbers:61973335,62111530151,52105103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10012022","Surgical robots;tactile perception;tumor resection;deep reinforcement learning;autonomous palpation","Deep learning;Medical robotics;Voltage measurement;Tactile sensors;Surgery;Reinforcement learning;Path planning","control engineering computing;deep learning (artificial intelligence);finite element analysis;medical robotics;mobile robots;path planning;piezoresistive devices;reinforcement learning;robot vision;surgery;tactile sensors;tumours","autonomous robotic surgeries;autonomous tumor palpation;deep reinforcement learning-based autonomous detection algorithm;finite element simulations;high-efficiency array detection mode;intraoperative tumor identification;piezoresistive tactile array sensor;resection path planning;surgical robot;tactile element;tactile image generation;tumor resection","","","","13","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"Optimization of the Ice Storage Air Conditioning System Operation Based on Deep Reinforcement Learning","M. Li; F. Xia; L. Xia","College of Automation Engineering, Shanghai University of Electric Power, Shanghai; College of Automation Engineering, Shanghai University of Electric Power, Shanghai; ArcTron Data & Innovation Technology Co., LTD, Shanghai","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","8554","8559","With the intention of obtaining the room temperature and economic cost control strategy of an ice storage air conditioning system in a small office building in Shanghai, the ice storage air conditioning system is established in this paper as a Markov decision process model and deep reinforcement learning algorithms are adopted to optimize its operation. In order to avoid the problem of over-dimension and over-estimation of value function caused by reinforcement learning, the DDQN (Double Deep Q-Network) algorithm with dual neural network structure is adopted to optimize the operation of ice storage air-conditioning system. Aiming at overcoming the shortcoming of slow convergence of DDQN algorithm, the action space of DDQN is taken into consideration in this paper. Firstly, the appropriate action set is selected according to the convergence speed of different actions. Secondly, the exponential function is addressed in the reward function. Based on the exponential function, the reward function can adjust the penalty value according to the difference between the expected room temperature and the actual room temperature, thus speeding up the convergence of the DDQN algorithm. Finally, Python is adopted to model and simulate buildings and ice storage air conditioning systems. The simulation results show that the operating cost and the proportion of uncomfortable time are both reduced by DDQN control method proposed in this paper with better control performance.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9549908","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9549908","DDQN;Ice storage air-conditioning;Optimization operation;reward function","Air conditioning;Costs;Temperature;Atmospheric modeling;Simulation;Buildings;Reinforcement learning","air conditioning;control engineering computing;learning (artificial intelligence);Markov processes;Python","Python;neural network structure;Markov decision process model;optimization;ice storage air-conditioning system;deep reinforcement learning algorithms;DDQN algorithm","","","","8","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Formation Tracking of Networked Autonomous Surface Vehicles with Bounded Inputs via Cloud-Supported Communication","T. -F. Ding; M. -F. Ge; Z. -W. Liu; L. Wang; J. Liu","School of Mechanical Engineering and Electronic Information, China University of Geosciences, Wuhan, China; School of Mechanical Engineering and Electronic Information, China University of Geosciences, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Civil and Hydraulic Engineering, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Intelligent Vehicles","","2023","PP","99","1","11","This paper investigates formation tracking (FT) problem of the networked autonomous surface vehicles (NASVs) with bounded inputs. In order to achieve distributed control, a prescribed-time observer is employed to reshape the leader's states for the follower ASVs, which can only receive the message from the neighbor ASVs. For reducing communication costs and the negative effect of bounded inputs and the unknown uncertainties, a hierarchical reinforcement learning control (HRLC) algorithm based on the cloud-supported communication is proposed, where the cloud-supported estimator is constructed such that the estimated states approach the leader's states with the less communication costs. The local reinforcement learning controller is designed according to the actor-critic strategy such that the actual states converge to the estimated states with the given formation offset. With the help of Lyapunov stability and Hurwitz stability theory, some sufficient conditions of the close-loop system have be obtained. Finally, simulation examples have be proposed to validate the theoretical analysis.","2379-8904","","10.1109/TIV.2023.3323767","National Natural Science Foundation of China(grant numbers:61703374); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10285408","Networked autonomous surface vehicles (NASVs);reinforcement learning;formation tracking;bounded inputs;cloud-supported communication","Actuators;Vehicle dynamics;Reinforcement learning;Costs;Monitoring;Target tracking;Stability criteria","","","","","","","IEEE","13 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Optimal Operable Power Flow: Sample-efficient Holomorphic Embedding-based Reinforcement Learning","A. R. Sayed; X. Zhang; G. Wang; C. Wang; J. Qiu","School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; College of Mechatronics and Control Engineering, Shenzhen University, Shenzhen, China; School of Electrical and Electronic Engineering, North China Electric Power University, Beijing, China; School of Electrical and Information Engineering, University of Sydney, NSW, Australia","IEEE Transactions on Power Systems","","2023","PP","99","1","13","The nonlinearity of physical power flow equations divides the decision-making space into operable and non-operable regions. Therefore, existing control techniques could be attracted to non-operable mathematically-feasible decisions. Moreover, the raising uncertainties of modern power systems need quick-optimal actions to maintain system security and stability. This paper proposes a holomorphic embedding-based soft actor-critic (HE-SAC) algorithm to find fast optimal operable power flow (OOPF) by leveraging deep reinforcement learning and advanced complex analysis techniques. First, a dynamic HE-based layer is developed to guarantee the solution operability and uses the previous operable germ instead of the no-load germ for high computational efficiency. Second, a model-based policy optimization is built based on a novel predictive model to generate more data and raise the sample efficiency of the SAC algorithm. Third, the reward function is augmented with the degree of constraint violations and policy entropy to enhance the solution's feasibility. Simulation results demonstrate the computational performance of the proposed dynamic HE layer, and the surpassing of the proposed HE-SAC over a number of state-of-the-art model-free RL algorithms and optimization methods. The proposed approach indicates its practicability and fast operable control for power system operation.","1558-0679","","10.1109/TPWRS.2023.3266773","National Natural Science Foundation of China(grant numbers:72001058,72171155); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2023A1515010724,2022A1515240051); General Program of Foundation of Shenzhen Science and Technology Committee(grant numbers:GXWD20201230155427003-20200822103658001,JCYJ20190808141019317); Major Science and Technology Special Projects in Xinjiang Autonomous Region(grant numbers:2022A01007); Xinjiang Autonomous Region Key Research and Development Task Special(grant numbers:2022B01016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10102328","Reinforcement learning;Holomorphic embedding;Operable power flow;Soft actor-critic algorithm;Model-based policy optimization","Power system stability;Load flow;Security;Mathematical models;Training;Heuristic algorithms;Costs","","","","","","","IEEE","13 Apr 2023","","","IEEE","IEEE Early Access Articles"
"Air-Ground Coordination Communication by Multi-Agent Deep Reinforcement Learning","R. Ding; F. Gao; G. Yang; X. S. Shen","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; School of Intelligent Systems Science and Engineering, Jinan University, Zhuhai, China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada","ICC 2021 - IEEE International Conference on Communications","6 Aug 2021","2021","","","1","6","In this paper, we investigate an air-ground coordination communication system where ground users (GUs) access suitable UAV base stations (UAV-BSs) to maximize their own throughput and UAV-BSs design their trajectories to maximize the total throughput and keep GU fairness. Note that the action space of GUs is discrete, and UAV-BSs’ action space is continuous. To deal with the hybrid action space, we propose a multi-agent deep reinforcement learning (MADRL) approach, named AG-PMADDPG (air-ground probabilistic multi-agent deep deterministic policy gradient), where GUs transform the discrete actions to continuous action probabilities, and then sample actions according to the probabilities. The proposed method enable the users make decisions based on their local information, which is beneficial for user privacy. Simulation results demonstrate that AG-PMADDPG can outperform the benchmark algorithms in terms of fairness and throughput.","1938-1883","978-1-7281-7122-7","10.1109/ICC42927.2021.9500477","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9500477","","Privacy;Simulation;System performance;Reinforcement learning;Transforms;Benchmark testing;Throughput","aerospace computing;autonomous aerial vehicles;data privacy;deep learning (artificial intelligence);multi-agent systems;probability;trajectory optimisation (aerospace);vehicular ad hoc networks","air-ground coordination communication system;UAV base stations;multiagent deep reinforcement learning;air-ground probabilistic multiagent deep deterministic policy gradient;user privacy;ground users;MADRL;AG-PMADDPG;UAV-BS trajectory design","","","","14","IEEE","6 Aug 2021","","","IEEE","IEEE Conferences"
"Model-Free Economic Dispatch for Virtual Power Plants: An Adversarial Safe Reinforcement Learning Approach","Z. Yi; Y. Xu; C. Wu","School of Electrical Engineering and Automation, Harbin Institute of Technology, Harbin, P. R. China; School of Electrical Engineering and Automation, Harbin Institute of Technology, Harbin, P. R. China; School of Electrical Engineering, Southeast University, Nanjing, P. R. China","IEEE Transactions on Power Systems","","2023","PP","99","1","15","To address the model inaccuracy and uncertainty of virtual power plants (VPPs), a model-free economic dispatch approach for multiple VPPs is studied in this article, which does not rely on an accurate environmental model. An adversarial safe reinforcement learning approach is proposed, which promotes the safety of the actions and makes the model robust to deviations between the training and testing environments. Moreover, a two-stage reinforcement learning framework is formulated based on the proposed algorithm. The dispatch policy is pretrained in the simulator and then fine-tuned in the real-world environment. The numerical simulations illustrate that the proposed approach is adaptive to the deviation between the training and testing environments, and it provides higher robustness to the noise of the network parameters and uncertainty of the VPPs' power outputs. The scalability and superiority of the proposed approach are verified by comparing it with existing methods.","1558-0679","","10.1109/TPWRS.2023.3289334","Science and Technology Project of State Grid Corporation of China(grant numbers:5100-202399366A-2-2-ZB,5108-202218280A-2-291-XG); National Natural Science Foundation of China(grant numbers:52107076); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10163055","Economic dispatch;virtual power plant;safe reinforcement learning;adversarial learning","Reinforcement learning;Power systems;Safety;Training;Security;Load flow;Uncertainty","","","","","","","IEEE","26 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Q-funcT: A Reinforcement Learning Approach for Automated Black Box Functionality Testing","Y. P. López; J. G. Colonna; E. De Araujo Silva; R. H. Degaki; J. M. Silva","SITA-AIG Automation Innovation Group SIDIA Development and Research Institute, Manaus, Brazil; ICOMP Computing Institute of Amazonas UFAM Federal University of Amazonas, Manaus, Brazil; ICOMP Computing Institute of Amazonas UFAM Federal University of Amazonas, Manaus, Brazil; ICOMP Computing Institute of Amazonas UFAM Federal University of Amazonas, Manaus, Brazil; SITA-AIG Automation Innovation Group SIDIA Development and Research Institute, Manaus, Brazil","2022 IEEE 2nd International Conference on Software Engineering and Artificial Intelligence (SEAI)","25 Jul 2022","2022","","","119","123","The steady growth of mobile applications users over the past years has resulted in increasing the workload of Software Quality Assurance teams. Regarding this, automated Android Testing has become a highlighted research subject. However, the state-of-art academic and industrial solutions available have mainly focused in exploratory or Automated Input Generation approaches, fewer works have addressed the challenge of automated functionality testing. Moreover, the proposed solutions exhibit several limitations standing out the vulnerability to app evolution and fragmentation. In this work we propose Q-funcT, a Reinforcement Learning based approach that aims to improve automated functionality testing by increasing portability. When compared with Scripted Test Cases our method takes a few minutes longer to complete the defined missions; however, regarding portability, Q-funcT shown a notably better performance.","","978-1-6654-8223-3","10.1109/SEAI55746.2022.9832177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832177","functionality testing;reinforcement learning;Q-learning;Android testing;fragmentation;portability","Q-learning;Conferences;Semantics;Software quality;Natural language processing;Mobile applications;Proposals","learning (artificial intelligence);mobile computing;program testing;software quality","Q-funcT;Automated black box functionality Testing;steady growth;mobile applications users;Software Quality Assurance teams;automated Android Testing;highlighted research subject;industrial solutions;Automated Input Generation;automated functionality testing;Scripted Test Cases our method","","","","29","IEEE","25 Jul 2022","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning With Policy Clipping and Average Evaluation for UAV-Assisted Communication Markov Game","Z. Feng; M. Huang; D. Wu; E. Q. Wu; C. Yuen","School of Information and Communication Engineering, Hainan University, Haikou, China; School of Information and Communication Engineering, Hainan University, Haikou, China; School of Information and Communication Engineering, Hainan University, Haikou, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore","IEEE Transactions on Intelligent Transportation Systems","","2023","PP","99","1","13","Unmanned aerial vehicle (UAV)-assisted communication is a significant technology in 6G communication. In order to cope with the dynamic trajectory optimization problem of the air-ground network, the interaction between entities is modeled as a Markov game firstly. Then, the model-free multi-agent reinforcement learning (MARL) is adopted to optimize individual decision-making. This enables agents to learn the mobile patterns of others, so as to optimize their own mobile strategy. However, there are some common issues when executing the benchmark MARL algorithms, such as biased estimation and local optimum. To solve these problems, an enhanced multi-agent proximal policy optimization algorithm is proposed with policy clipping and average evaluation to guarantee the fast convergence and accurate estimation. Simulations demonstrate that this method produces superior convergence than the benchmark algorithms. It allows the UAV base station, ground users and the aerial jammer to adopt the optimal mobile strategies to achieve their respective maximum cumulative rewards. In addition, the stable strategies of agents constitute the approximate Nash equilibrium for the UAV-assisted communication Markov Game.","1558-0016","","10.1109/TITS.2023.3296769","Scientific Research Fund Project of Hainan University(grant numbers:KYQD(ZR)-21007); Natural Science Foundation of Hainan Province(grant numbers:621QN212); Hainan Provincial Natural Science Foundation of China(grant numbers:622RC618); National Natural Science Foundation of China(grant numbers:62062030); State Scholarship Fund, China Scholarship Council(grant numbers:202207565036); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10197291","Multi-agent reinforcement learning;UAV-assisted communication;game theory","Autonomous aerial vehicles;Base stations;Jamming;Markov processes;Games;Convergence;Resource management","","","","","","","IEEE","28 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Towards Optimal Real-time Volumetric Video Streaming: A Rolling Optimization and Deep Reinforcement Learning Based Approach","J. Li; H. Wang; Z. Liu; P. Zhou; X. Chen; Q. Li; R. Hong","School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; The University of Electro-Communications, Tokyo, Japan; University of Science and Technology of China, Hefei, China; VTT Technical Research Centre of Finland, Finland; school of Electrical Engineering and Automation, Hefei University of Technology, and Engineering Technology Research Center of Industrial Automation Anhui Province, Hefei, Anhui, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China","IEEE Transactions on Circuits and Systems for Video Technology","","2023","PP","99","1","1","Volumetric video provides users with a good viewing experience of six degrees of freedom (DoF) and has wide applications in many fields such as teleconferencing and online games. However, the huge data volume and strict latency requirements of point cloud video, the most popular representative of volumetric video, pose a challenge to its transmission. Existing point cloud video transmission algorithms usually segment a long video by every one or several group of frames, predict network bandwidth and FoV information, then perform adaptive transmission by solving the quality of experience (QoE) optimization problem. However, such segmentation neglects the impact of current optimization decisions on the subsequent video streaming process, as well as the large prediction error for a long interval, severely degrading user’s QoE. Moreover, the complex constrained optimization problem makes the solution time too long to meet the real-time video streaming requirements. To this end, in this paper, we propose a rolling prediction-optimization-transmission (POT) framework, which makes predictions of network bandwidth and FoV in each short rolling window to reduce prediction error. And our framework takes into account the upper bounded QoE contribution of the subsequent point cloud video to improve the system performance. In addition, we design a deep reinforcement learning based real-time solver to make decisions of the fixed structure optimization problem in each roll, allowing our system to run in real-time. We have performed simulations and experiments, and the results show that our solution outperforms existing methods.","1558-2205","","10.1109/TCSVT.2023.3277893","National Natural Science Foundation of China(grant numbers:52077049); Natural Science Foundation of Anhui Province(grant numbers:2008085UD04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129903","Point cloud video;video streaming;real-time transmission;QoE model;deep reinforcement learning;rolling optimization","Streaming media;Optimization;Point cloud compression;Quality of experience;Bandwidth;Real-time systems;Prediction algorithms","","","","","","","IEEE","19 May 2023","","","IEEE","IEEE Early Access Articles"
"A Combat Decision Support Method Based on OODA and Dynamic Graph Reinforcement Learning","B. Xu; W. Zeng","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan","2022 34th Chinese Control and Decision Conference (CCDC)","14 Feb 2023","2022","","","4872","4878","Modern network centric joint combat greatly increases the complexity of war, and the demand for intelligent decision-making is becoming stronger. In this paper, a combat decision support method based on graph neural network and reinforcement learning is proposed. Based on OODA cycle theory, TSDI combat network is constructed, and the characteristics of nodes, OODA loops and network topology are extracted. A graph neural network GMQN which can adapt to dynamic network and dynamic action space is designed as the built-in neural network of reinforcement learning. The global joint action is obtained by combining GMQN and Kuhn-Munkras algorithm. Finally, the effectiveness and advantage of the proposed method are verified on the joint combat simulation platform.","1948-9447","978-1-6654-7896-0","10.1109/CCDC55256.2022.10033986","Nature; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10033986","Combat Network;OODA Loop;Reinforcement Learning;Graph Neural Network","Network topology;Atmospheric modeling;Weapons;Heuristic algorithms;Decision making;Reinforcement learning;Graph neural networks","decision making;graph neural networks;graph theory;military computing;reinforcement learning","combat decision support method;dynamic action space;dynamic graph reinforcement learning;dynamic network;global joint action;graph neural network;intelligent decision-making;joint combat simulation platform;modern network centric joint combat;network topology;OODA cycle theory;OODA loops;TSDI combat network","","","","28","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Optimal Control of Flotation Industrial Process Using Model-based Reinforcement Learning","R. Jia; X. Chen; J. Zheng; G. Yu","College of Information Science and Engineering, Northeastern University, Shenyang, Liaoning, China; College of Information Science and Engineering, Northeastern University, Shenyang, Liaoning, China; College of Information Science and Engineering, Northeastern University, Shenyang, Liaoning, China; State Key Laboratory of Process Automation in Mining & Metallurgy/Beijing Key Laboratory of Process Automation in Mining & Metallurgy, Beijing, China","2022 4th International Conference on Industrial Artificial Intelligence (IAI)","23 Dec 2022","2022","","","1","6","In this paper, the optimal control of the flotation industrial process (FIP) is studied. The flotation process uses differences in the physical and chemical properties of mineral surfaces to selectively attach minerals to air bubbles, and separate useful from useless minerals. To optimize control of the process, we use the model-based reinforcement learning (MBRL) method to design the optimal controller for the flotation process. A case study on the flotation mechanism model verifies the efficiency of the proposed method. The results show that the MBRL method can learn the optimal control policy with fewer episodes.","","978-1-6654-5120-8","10.1109/IAI55780.2022.9976694","National Key Research and Development Program of China(grant numbers:2021YFC2902703); National Natural Science Foundation of China(grant numbers:61873049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9976694","","Atmospheric modeling;Design methodology;Process control;Optimal control;Reinforcement learning;Minerals;Artificial intelligence","control engineering computing;flotation (process);mineral processing;minerals;optimal control;reinforcement learning;separation","chemical properties;FIP;flotation industrial process;flotation mechanism model;MBRL;mineral surfaces;model-based reinforcement learning method;optimal control policy;optimal controller;physical properties","","","","13","IEEE","23 Dec 2022","","","IEEE","IEEE Conferences"
"Distributed Input Design with Combination of System Identification and Reinforcement Learning","X. Mao; J. He","The Dept. of Automation, Shanghai Jiao Tong University, Shanghai, China; The Dept. of Automation, Shanghai Jiao Tong University, Shanghai, China","2022 IEEE International Conference on Unmanned Systems (ICUS)","29 Dec 2022","2022","","","896","901","The optimal input design of unknown large-scale network systems encounters difficulties when only local observation is available. Traditional system identification methods cannot guarantee to obtain an accurate system model because it cannot access the global information. Meanwhile, the direct use of model-free methods such as reinforcement learning lacks convergence guarantees and efficiency due to lack of prior knowledge of the large scale of the network. To tackle these problems, in this paper, we propose a distributed optimal input design method for large-scale networks by combining the system identification method with the reinforcement learning. First, we propose a distributed system identification method based on the least square method. Then, a distributed optimal input design algorithm is proposed based on deep reinforcement learning to update the results of system identification and minimize the cost function of optimal control at the same time. The identified system model is used as prior knowledge to determine the environment reward to contribute to the convergence of the reinforcement learning algorithm. It is proved that our method can obtain the identification results of system and the optimal input signal via local observation. Simulations demonstrate the effectiveness of the proposed method.","2771-7372","978-1-6654-8456-5","10.1109/ICUS55513.2022.9987073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9987073","distributed identification;input design;reinforcement learning;optimal control","Knowledge engineering;Design methodology;Optimal control;Reinforcement learning;Observers;Cost function;System identification","identification;learning systems;least squares approximations;optimal control;reinforcement learning","convergence guarantees;deep reinforcement learning;distributed input design;distributed optimal input design algorithm;distributed system identification method;identified system model;large-scale network systems;least square method;local observation;model-free methods;optimal control;optimal input signal","","","","19","IEEE","29 Dec 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Predictive Control for Autonomous Electrified Vehicles","T. Liu; C. Yang; C. Hu; H. Wang; L. Li; D. Cao; F. -Y. Wang","Vehicle Intelligence Pioneers Inc., Qingdao, Shandong, China; Jiangsu XCMG Construction Machinery Research Institute Ltd., Xuzhou, Jiangsu, China; Jiangsu XCMG Construction Machinery Research Institute Ltd., Xuzhou, Jiangsu, China; Mechanical and Mechatronics Engineering Department, Waterloo University, Canada; Faculty of Department of Automation, Tsinghua University, China; Mechanical and Mechatronics Engineering Department, Waterloo University, Canada; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2018 IEEE Intelligent Vehicles Symposium (IV)","21 Oct 2018","2018","","","185","190","This paper proposes a learning-based predictive control technique for self-driving hybrid electric vehicle (HEV). This approach is a hierarchical framework. The higher-level is a human-like driver model, which is applied to predict accelerations in the car following situation to replicate a human driver's demonstrations. The lower-level is a reinforcement learning (RL)-based controller, which enforces the battery and fuel consumption constraints to improve energy efficiency of HEV. In addition, we present induced matrix norm (IMN) to handle cases that the training data cannot provide sufficient information on how to operate in current driving situation. Simulation results illustrate that the proposed method can reproduce human driver's driving style and promote fuel economy.","1931-0587","978-1-5386-4452-2","10.1109/IVS.2018.8500719","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8500719","","Acceleration;Hybrid electric vehicles;Trajectory;Hidden Markov models;Predictive control;Energy efficiency","energy consumption;fuel economy;hybrid electric vehicles;learning (artificial intelligence);predictive control","reinforcement learning-based predictive control;learning-based predictive control technique;hybrid electric vehicle;HEV;hierarchical framework;driver model;human driver;reinforcement learning-based controller;fuel consumption constraints","","","","10","IEEE","21 Oct 2018","","","IEEE","IEEE Conferences"
"A Motion Planning and Control Method of Quadruped Robot Based on Deep Reinforcement Learning","W. Liu; B. Li; L. Hou; S. Yang; Y. Xu; L. Liu","School of Mathematics and Statistics, Qilu University of Technology (Shandong Academy of Sciences), Jinan, Shandong, China; School of Mathematics and Statistics, Qilu University of Technology (Shandong Academy of Sciences), Jinan, Shandong, China; School of Electrical Engineering and Automation, Qilu University of Technology (Shan-dong Academy of Sciences), Jinan, Shandong, China; School of Mathematics and Statistics, Qilu University of Technology (Shandong Academy of Sciences), Jinan, Shandong, China; School of Electrical Engineering and Automation, Qilu University of Technology (Shan-dong Academy of Sciences), Jinan, Shandong, China; School of Mathematics and Statistics, Qilu University of Technology (Shandong Academy of Sciences), Jinan, Shandong, China","2022 IEEE International Conference on Robotics and Biomimetics (ROBIO)","18 Jan 2023","2022","","","2003","2008","Motion planning and control issue of quadruped robots is a challenging topic because it requires a lot of professional knowledge and tedious manual design based on traditional control methods. This paper proposes a novel method of training quadruped robots to generate locomotion gaits by combining the deep reinforcement learning. The control problem of quadruped robots is modeled as a Markov Decision Process, and a universal quadruped robot motion control reward mechanism is developed. A variety of gaits for the quadruped robot is generated by adding a gait reference frame. Two deep reinforcement learning algorithms are used for training and comparison. The desired gait locomotion control strategy is generated through training so that the robot can realize stable walking with the desired gait. Finally, the proposed method is tested and evaluated in a simulation environment using the A1 quadruped robot model.","","978-1-6654-8109-0","10.1109/ROBIO55434.2022.10011798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011798","","Training;Deep learning;Legged locomotion;Process control;Reinforcement learning;Switches;Aerospace electronics","gait analysis;learning (artificial intelligence);legged locomotion;Markov processes;mobile robots;motion control;path planning;robot dynamics;robot kinematics","control issue;control method;control problem;deep reinforcement learning algorithms;desired gait locomotion control strategy;motion planning;robot model;traditional control methods;training quadruped robots;universal quadruped robot motion control reward mechanism","","","","21","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Method for Lion and Man Problem","J. Xing; X. Zeng","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","8366","8371","David Gale’s lion and man problem, which is a fundamental pursuit-evasion game, attracts the interest of many scholars. We propose a deep reinforcement learning method based on Deep Deterministic Policy Gradient (DDPG) to solve this problem. We first improve the exploration strategy by adding guided exploration and dynamic spaces exploration strategies to the greedy algorithm. Then we introduce a learning reset mechanism to help the agents escape the traps in the learning process. With these improvements, our method achieves better performance than the classic DDPG algorithm in the lion and man problem. The simulation result shows that deep reinforcement learning method may be promising to solve this problem.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9550113","Beijing Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9550113","Lion and Man Problem;Exploration Strategy;Deep Reinforcement Learning","Greedy algorithms;Heuristic algorithms;Simulation;Reinforcement learning;Games;Space exploration","game theory;gradient methods;greedy algorithms;learning (artificial intelligence)","lion and man problem;deep reinforcement learning method;deep deterministic policy gradient;dynamic spaces exploration strategies;learning reset mechanism;pursuit-evasion game;greedy algorithm;DDPG algorithm","","","","17","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Hierarchical Reinforcement Learning Based Multi-Agent Collaborative Control Approach","T. Qiao; P. Cui; Y. Zhang","School of Automation, Southeast University, Nanjing, P. R. China; Key Laboratory of Information System Engineering, Nanjing, China; School of Automation, Southeast University, Nanjing, P. R. China","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","8645","8650","To address the issues of slow convergence and poor interpretability, this paper proposes a novel hierarchical reinforcement learning framework consisting of an upper-level macro-decision model and a lower-level micro-execution model. To enable agents to explore in an orderly manner, expert knowledge is incorporated into the framework to design explainable subtasks. Furthermore, a hierarchical multi-agent reinforcement learning algorithm with explainable subtasks is developed and evaluated in the SC2LE scenario. Experimental results show that the proposed algorithm outperforms the traditional MARL approach in complex scenarios involving heterogeneous agents' cooperation, effectively solves the multi-agent behavior interpretability challenge, and significantly improves the training convergence speed.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240170","National Key R&D Program of China(grant numbers:2021ZD0112700); National Natural Science Foundation(grant numbers:61973082); Natural Science Foundation of Jiangsu Province(grant numbers:BK20202006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240170","Hierarchical reinforcement learning;Interpretability;Multi-agent systems;SC2LE","Training;Collaboration;Reinforcement learning;Behavioral sciences;Task analysis;Standards;Convergence","","","","","","21","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Graph Relational Reinforcement Learning for Mobile Robot Navigation in Large-Scale Crowded Environments","Z. Liu; Y. Zhai; J. Li; G. Wang; Y. Miao; H. Wang","MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China; School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Intelligent Transportation Systems","2 Aug 2023","2023","24","8","8776","8787","Mobile robot autonomous navigation in large-scale environments with crowded dynamic objects and static obstacles is still an essential yet challenging task. Recent works have demonstrated the potential of using deep reinforcement learning to enable autonomous navigation in crowds. However, only considering the human-robot interactions results in short-sighted and unsafe behaviors, and they typically use hand-crafted features and assume the global observation range, leading to large performance declines in large-scale crowded environments. Recent advances have shown the power of graph neural networks to learn local interactions among surrounding objects. In this paper, we consider autonomous navigation task in large-scale environments with crowded static and dynamic objects (such as humans). Particularly, local interactions among dynamic objects are learned for better-understanding their moving tendency and relational graph learning is introduced for aggregating both the object-object interactions and object-robot interactions. In addition, local observations are transformed into graphical inputs to achieve the scalability to various number of surrounding dynamic objects and various static obstacle patterns, and the globally guided reinforcement learning strategy is introduced to achieve the fixed-sized learning model even in large-scale complex environments. Simulation results validate our generalizability to various environments and advanced performance compared with existing works in large-scale crowded environments. In particular, our method with only local observations performs better than the benchmarks with global complete observability. Finally, physical robotic experiments demonstrate our effectiveness and practical applicability in real scenarios.","1558-0016","","10.1109/TITS.2023.3269533","Natural Science Foundation of China(grant numbers:62225309,62073222,U21A20480,U1913204); Shanghai Municipal Science and Technology Major Project(grant numbers:2021SHZDZX0102); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10115223","Crowded environment;reinforcement learning;relational graph learning;robot navigation","Navigation;Task analysis;Mobile robots;Autonomous robots;Reinforcement learning;Scalability;Training","collision avoidance;deep learning (artificial intelligence);graph neural networks;graph theory;human-robot interaction;learning (artificial intelligence);mobile robots;navigation;path planning;reinforcement learning","autonomous navigation task;crowded dynamic objects;deep reinforcement learning;fixed-sized learning model;globally guided reinforcement learning strategy;graph neural networks;graph relational reinforcement;human-robot interactions results;large-scale complex environments;large-scale crowded environments;large-scale environments;local interactions;local observations;mobile robot autonomous navigation;mobile robot navigation;object-object interactions;object-robot interactions;relational graph learning;surrounding objects","","","","36","IEEE","3 May 2023","","","IEEE","IEEE Journals"
"Real-time Optimal Allocation for Uncertain Time Coupled Resource Based on Reinforcement Learning","Q. Huang; L. Yang; C. Chi; X. Kong; C. Zhou","School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Information Engineering, Dalian University, Dalian, China; School of Information Engineering, Dalian University, Dalian, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","2022 34th Chinese Control and Decision Conference (CCDC)","14 Feb 2023","2022","","","1264","1269","The allocation of the time coupled resource plays an important role in many applications, such as the charging/discharging control of electric vehicles, utilization of electromagnetic spectrum, etc. The availability of time coupled resource is usually uncertain and has limited working time and budget which makes its allocation non-trivial to be solved. In this paper, we use the electromagnetic spectrum allocation as an illustrative example to make better use of time coupled resource among different frequency demand while reducing the electromagnetic interference. However, the frequency demand is usually uncertain and the allocation is usually required to be made in real-time which makes this allocation problem difficult to solve. Therefore, we consider this important problem in this paper and make the following contributions. First, this problem is formulated as a Markov decision process (MDP) model which considers the uncertainty of the frequency demand. Second, in order to meet the real-time decision making of the frequency demand, a reinforcement learning approach with Gaussian process regression is proposed to solve the model. The Gaussian process regression is used to learn the optimal Q-factor in reinforcement learning. Third, the numerical results demonstrate the proposed method can improve the occupancy efficiency of the electromagnetic spectrum.","1948-9447","978-1-6654-7896-0","10.1109/CCDC55256.2022.10033599","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10033599","Resource allocation;Optimization;Markov decision process;Reinforcement learning","Q-factor;Time-frequency analysis;Uncertainty;Reinforcement learning;Gaussian processes;Real-time systems;Numerical models","decision making;Gaussian processes;learning (artificial intelligence);Markov processes;regression analysis;reinforcement learning;resource allocation","allocation nontrivial;allocation problem;different frequency demand;electromagnetic spectrum allocation;real-time decision making;reinforcement learning;time coupled resource;time optimal allocation;uncertain time","","","","19","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Robot Control","Z. Guliyev; A. Parsayan","Process Automation Engineering Department, Baku Higher Oil School, Baku, Azerbaijan; Process Automation Engineering Department, Baku Higher Oil School, Baku, Azerbaijan","2022 IEEE 16th International Conference on Application of Information and Communication Technologies (AICT)","16 Jan 2023","2022","","","1","6","Reinforcement learning (RL) has been proven to be a feasible method for learning complicated actions autonomously from sensory observations. Even though many of the deep RL studies have been centered on modelled control and computer games, which has nothing to do with the limits of learning in actual surroundings, deep RL has also revealed its potential in allowing robots to acquire complicated abilities in the real-world situations. Real-world robotics, on the other hand, is an intriguing area for testing the algorithms of this kind, because it is directly related to the learning procedure of humans. Deep RL might enable developing movement abilities without a precise modelling of the robot dynamics and with minimum engineering. However, because of hyper-parameter sensitivity and low sampling capability, it is difficult to implement deep RL to robotic tasks involving real-world applications. It is comparable simple to tune hyper-parameters in simulations, while it can be a challenging task when it comes to physical world, for example, biped robots. Acquiring the ability to move and perceive in the actual world involves a variety of difficulties, some are simpler to handle than others that are frequently overlooked in RL studies which are limited to simulated environments. This paper provides approaches to deal with a variety of frequent difficulties in deep RL arising while training a biped robot to walk and follow a specific path.","2472-8586","978-1-6654-5162-8","10.1109/AICT55583.2022.10013595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10013595","Reinforcement Learning;Biped Locomotion;Humanoid Robot Walking;Path Tracking","Training;Legged locomotion;Video games;Sensitivity;Reinforcement learning;Robot sensing systems;Behavioral sciences","control engineering computing;deep learning (artificial intelligence);legged locomotion;reinforcement learning;robot dynamics","biped robot;computer games;deep RL studies;real-world robotics;reinforcement learning based robot control;robot dynamics","","","","16","IEEE","16 Jan 2023","","","IEEE","IEEE Conferences"
"Multi-Agent Cooperation Decision-Making by Reinforcement Learning with Encirclement Rewards","M. Rubing; W. Bo; J. Jingyuan; L. Changchun; D. Hao","School of Automation, Beijing Institute of Technology, Beijing; School of Automation, Beijing Institute of Technology, Beijing; NORINCO GROUP North Automatic Control Technology Institute, Taiyuan; NORINCO GROUP North Automatic Control Technology Institute, Taiyuan; NORINCO GROUP North Automatic Control Technology Institute, Taiyuan","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","8306","8311","Multi-agent decision-making is increasingly applied on many situations especially on military applications, but their autonomous decision-making ability needs to be improved. Multi-agent Deep Deterministic Policy Gradient (MADDPG) adopts the method of centralized evaluation and decentralized execution, while updates each agent's network parameters based on the global state information rather than only its own state, which can make the entire agent's policy network update in the direction of the global optimum, rather than the individual optimum. In the process of multi-agent cooperation decision-making, the encirclement rewards is introduced to guide agents to make cooperative actions and alleviate the problem of sparse rewards. Firstly, we define the encirclement. By using Graham's algorithm, we find out the effective encirclement of $N$ agents. We evaluate the encirclement quality from the area of the encirclement and the difficulty of breaking-through for the adversary, and then design the rewards function based on this. Simulation experiments show that the convergence speed and win rate of MADDPG algorithm based on encirclement rewards is significantly improved, and it also has strong adaptability to various task scenarios.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240463","Multi-agent Decision-making;Reinforcement Learning;Encirclement Rewards;Reward Shaping","Decision making;Reinforcement learning;Task analysis;Convergence","","","","","","15","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Dynamic Event-Triggered Reinforcement Learning-Based Consensus Tracking of Nonlinear Multi-Agent Systems","B. Xu; Y. -X. Li; Z. Hou; C. K. Ahn","School of Automation, Qingdao University, Qingdao, China; College of Science, Liaoning University of Technology, Jinzhou, China; School of Automation, Qingdao University, Qingdao, China; School of Electrical Engineering, Korea University, Seoul, South Korea","IEEE Transactions on Circuits and Systems I: Regular Papers","27 Apr 2023","2023","70","5","2120","2132","In this paper, we present a novel approach to address the event-triggered optimized consensus tracking control problem in a class of uncertain nonlinear multi-agent systems (MASs). To optimize control performance, we employ an adaptive reinforcement learning (RL) algorithm based on the actor-critic architecture and utilize the backstepping method. The proposed RL-based optimized controller employs a novel event-triggered strategy, dynamically adjusting sampling errors online to reduce communication resource usage and computational complexity through the intermittent transmission of state signals. We establish the boundedness of all signals in the closed-loop MAS through stability analysis using the Lyapunov method, and demonstrate the prevention of Zeno behavior. Numerical simulations of a practical multi-electromechanical system are provided to validate the effectiveness of the proposed scheme.","1558-0806","","10.1109/TCSI.2023.3246001","Funds of National Science of China(grant numbers:61973146,61833001); Distinguished Young Scientific Research Talents Plan in Liaoning Province(grant numbers:XLYC1907077); Applied Basic Research Program in Liaoning Province(grant numbers:2022JH2/101300276); Taishan Scholar Project of Shandong Province of China(grant numbers:tsqn201909097); National Research Foundation of Korea (NRF) funded by the Korean Government (Ministry of Science and ICT)(grant numbers:NRF-2020R1A2C1005449); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10050096","Nonlinear MASs;optimized consensus;event-triggered control (ETC);reinforcement learning (RL);Hamilton--Jacobi--Bellman (HJB) equation","Optimal control;Mathematical models;Reinforcement learning;Multi-agent systems;Artificial neural networks;Trajectory;Topology","adaptive control;closed loop systems;control nonlinearities;electromechanical actuators;Lyapunov methods;multi-agent systems;nonlinear control systems;optimal control;reinforcement learning;stability;uncertain systems","actor-critic architecture;adaptive reinforcement learning algorithm;backstepping method;closed-loop MAS;dynamic event-triggered reinforcement learning-based consensus tracking;event-triggered optimized consensus tracking control problem;Lyapunov method;practical multielectromechanical system;RL-based optimized controller;stability analysis;uncertain nonlinear multiagent systems","","","","52","IEEE","22 Feb 2023","","","IEEE","IEEE Journals"
"Fixed-Time Fault-Tolerant Optimal Attitude Control of Spacecraft With Performance Constraint Via Reinforcement Learning","B. Xiao; H. Zhang; Z. Chen; L. Cao","School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; Institute of Advanced Structure Technology, Beijing Institute of Technology, Beijing, China; National Innovation Institute of Defense Technology, Chinese Academy of Military Science, Beijing, China","IEEE Transactions on Aerospace and Electronic Systems","","2023","PP","99","1","10","The attitude stabilization control problem of spacecraft with actuator fault, external disturbance, and performance constraint is studied via reinforcement learning (RL). The attitude stabilization error constrained by prescribed performance is firstly transformed into an unconstrained variable. Unlike the existing optimal controllers ensuring uniformly ultimately bounded stability, an RL-based fixed-time optimal control framework is then proposed. In this control framework, a neural network (NN) weight updating law with the persistent excitation condition eliminated is designed. Moreover, a fixed-time estimator is developed and added into the classical RL-based optimal controller to synthesize a fixed-time fault-tolerant controller. The closed-loop system and the estimation errors of the NN weights are stabilized within fixed time. The control cost is also significantly reduced. The effectiveness of the control policy is finally examined through numerical simulation.","1557-9603","","10.1109/TAES.2023.3292809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10175617","Actuator fault;fixed-time stability;prescribed performance;reinforcement learning;spacecraft","Attitude control;Space vehicles;Optimal control;Actuators;Fault tolerant systems;Fault tolerance;Mathematical models","","","","","","","IEEE","7 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Joint Beamforming and Trajectory Optimization for UAV-Assisted Double IRS Secure Transmission System: A Deep Reinforcement Learning Approach","Y. Qi; Z. Su; Q. Xu; D. Fang","School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; Department of Computer Science, California Polytechnic State University, San Luis Obispo, USA","2023 IEEE International Conference on Metaverse Computing, Networking and Applications (MetaCom)","6 Oct 2023","2023","","","504","509","Unmanned aerial vehicle (UAV) integrated with intelligent reflecting surface (IRS) has excellent potential to improve air-to-ground communication performance. However, the openness of the air-to-ground channel makes secure information transmission a challenging issue. In this paper, we first propose a UAV-assisted double IRS secure transmission system, where one IRS is carried by UAV, and the other is deployed near the base station. The collaborative beamforming gain provided by the cascaded reflection link overcomes the performance bottleneck of a single IRS in the air. Secondly, with the aim of maximizing the sum secrecy rate, a problem is formulated for jointly optimizing the trajectory of UAV and the phase shift of each IRS. As the formulated problem is a non-convex optimization problem and varies dynamically with the environment, a deep reinforcement learning-based secure transmission approach is presented to continuously achieve the optimal phase shift and trajectory of UAV in dynamic environments. Furthermore, we reduce the dimension of the action space by dividing the IRS into several sub-surfaces, each of which shares the same phase shift. The simulation results demonstrate that the secrecy rate of the UAV-assisted double IRS secure transmission system can be significantly improved by the proposed approach.","","979-8-3503-3333-6","10.1109/MetaCom57706.2023.00092","National Natural Science Foundation of China; Natural Science Foundation of Shanghai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10271787","UAV-assisted secure transmission;intelligent reflecting surface;deep reinforcement learning;secrecy rate","Deep learning;Array signal processing;Metaverse;Simulation;Reinforcement learning;Information processing;Autonomous aerial vehicles","","","","","","16","IEEE","6 Oct 2023","","","IEEE","IEEE Conferences"
"Path Planning Technology of Unmanned Vehicle Based on Improved Deep Reinforcement Learning","K. Zhang; L. Wang; J. Hu; Z. Xu; C. Guo","CETC Key Laboratory of Data Link Technology, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; CETC Key Laboratory of Data Link Technology, Xi’an, China","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","8392","8397","As the basic problem of unmanned vehicle navigation control, path planning has been widely studied. Reinforcement learning (RL) has been found an effective way of path optimization for the highly nonlinear and unmodeled dynamics. However, the RL based methods suffer from the ""dimension disaster"" under the high-dimension state spaces. In this paper, the path planning of an unmanned vehicle with collision avoidance is considered, and an improved Deep Q-Network (DQN) algorithm is proposed to reduce the computation load in the high-dimension state space. First, the states, actions and rewards are determined based on the task requirement, and a smoothing function is defined as an additional penalty term to modify the basic reward function. Then, the two-dimension grid of the state space is mapped to a gray image, which is applied as the input of a neural network, i.e., the Q-Network. Finally, simulation results show that the modified DQN algorithm is more stable and the fluctuation frequency is significantly reduced.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9549620","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Research and Development; Natural Science Foundation of Shaanxi Province; China Postdoctoral Science Foundation; Aeronautical Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9549620","Reinforcement learning;path planning;DQN","Training;Smoothing methods;Simulation;Neural networks;Reinforcement learning;Unmanned vehicles;Path planning","collision avoidance;deep learning (artificial intelligence);learning systems;mobile robots;navigation;neurocontrollers;optimisation;path planning;remotely operated vehicles","path optimization;highly nonlinear dynamics;unmodeled dynamics;RL based methods;dimension disaster;high-dimension state space;deep Q-network algorithm;basic reward function;two-dimension grid;modified DQN algorithm;path planning;unmanned vehicle navigation control;deep reinforcement learning;collision avoidance","","","","20","","6 Oct 2021","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Based Incentive Scheme for Multi-Hop Communications in Vehicular Networks","A. Shan; X. Fan; X. Chen; Y. Ji; C. Wu","Department of Automation and Information Engineering, Xi’an University of Technology, Xi’an, China; Department of Automation and Information Engineering, Xi’and University of Technology, Xi’an, China; VTT Technical Research Centre of Finland, Oulu, Finland; Information Systems Architecture Research Division, National Institute of Informatics, Tokyo, Japan; Meta-Networking Research Center, The University of Electro-Communications, Tokyo, Japan","IEEE Transactions on Cognitive Communications and Networking","","2023","PP","99","1","1","Vehicular network applications heavily rely on multi-hop communication technologies to ensure efficient and reliable data transmissions because of the limited wireless transmission range of vehicles, road side units, and some other mobile devices in the network. There is a common assumption in most of the applications that all network nodes are cooperative during the entire network procedure. However, this is not practical in reality, because rational nodes are reluctant to assist others with consuming own precious resources. Hence, incentive mechanisms are inevitably required to encourage the intermediate nodes to forward data packets for multi-hop communications. Nevertheless, the high mobility of nodes and the intermittent connectivity among them make the estimation of the environment extremely challenging in vehicular networks. Moreover, in some traffic scenarios where there is a lack of fixed infrastructures (rural area, disaster areas, or during maintenance), the incentive schemes should be self-organized in a distributed manner. In this paper, we propose an incentive scheme for multi-hop communications in vehicular networks. The proposed scheme estimates the complicated dynamic environment based on a fuzzy logic approach, then further stimulates intermediate nodes to maximize the expected utility from a long-term perspective based on a reinforcement learning method in a fully distributed manner. To evaluate the performance of our proposed scheme, we conducted extensive computer simulations based on realistic traffic networks in Xi’an city of China. The results of the simulations demonstrate that the proposed scheme has the superiority of incentive capabilities over other baseline approaches in terms of packet delivery ratio, end-to-end delay within vehicular networks.","2332-7731","","10.1109/TCCN.2023.3316644","Inner Mongolia Autonomous Region Natural Science Foundation(grant numbers:2022MS06007); Research Program of Science and Technology at Universities of Inner Mongolia Autonomous Region(grant numbers:NJZY22035); National Natural Science Foundation of China(grant numbers:62062031); National Natural Science Foundation of China(grant numbers:61272509); Scientific Research Project of Baotou Teachers? College(grant numbers:BSYKJ2021-ZY01); JSPS KAKENHI(grant numbers:21H03424); Key Research and Development Plan of Shaanxi Province(grant numbers:2021GY-072); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254587","Incentive schemes;vehicular networks;selfish nodes;reinforcement learning;fuzzy logic","Incentive schemes;Game theory;Vehicle dynamics;Spread spectrum communication;Games;Delays;Reliability","","","","","","","IEEE","18 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Simplified Deep Reinforcement Learning Based Volt-var Control of Topologically Variable Power System","Q. Ma; C. Deng","School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China","Journal of Modern Power Systems and Clean Energy","20 Sep 2023","2023","11","5","1396","1404","The high penetration and uncertainty of distributed energies force the upgrade of volt-var control (VVC) to smooth the voltage and var fluctuations faster. Traditional mathematical or heuristic algorithms are increasingly incompetent for this task because of the slow online calculation speed. Deep reinforcement learning (DRL) has recently been recognized as an effective alternative as it transfers the computational pressure to the off-line training and the online calculation timescale reaches milliseconds. However, its slow offline training speed still limits its application to VVC. To overcome this issue, this paper proposes a simplified DRL method that simplifies and improves the training operations in DRL, avoiding invalid explorations and slow reward calculation speed. Given the problem that the DRL network parameters of original topology are not applicable to the other new topologies, side-tuning transfer learning (TL) is introduced to reduce the number of parameters needed to be updated in the TL process. Test results based on IEEE 30-bus and 118-bus systems prove the correctness and rapidity of the proposed method, as well as their strong applicability for large-scale control variables.","2196-5420","","10.35833/MPCE.2022.000468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10026204","Voltvar control (VVC);deep reinforcement learning (DRL);topologically variable power system;transfer learning","Training;Power systems;Reactive power;Topology;Network topology;Mathematical models;Optimization","control engineering computing;deep learning (artificial intelligence);power distribution control;power engineering computing;reactive power control;reinforcement learning;voltage control","118-bus systems;deep reinforcement learning;distributed energies;DRL network parameters;heuristic algorithms;large-scale control variables;off-line training;online calculation timescale;original topology;side-tuning transfer learning;simplified DRL method;slow offline training speed;slow online calculation speed;slow reward calculation speed;topologically variable power system;traditional mathematical algorithms;training operations;var fluctuations;volt-var control;VVC","","","","30","","25 Jan 2023","","","SGEPRI","SGEPRI Journals"
"A New Solution to the PID18 Challenge: Reinforcement-Learning-based PI Control","Y. Wu; L. Xing; X. -K. Liu; F. Guo","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Control Science and Engineering, Shandong Unvieristy, Jinan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; Department of Automation, Zhejiang University of Technology, Hangzhou, China","2022 34th Chinese Control and Decision Conference (CCDC)","14 Feb 2023","2022","","","5755","5760","The PID18 benchmark challenge, first presented at the 3rd IFAC Conference on Advances in 2018, is widely used to test the effectiveness of proportional-integral-derivative (PID) controllers. In this challenge, a 2-Input and 2Output vapor-compression refrigeration system with strong nonlinearities and couplings is used as the test control plant. In this paper, we address this challenge by proposing a new reinforcement-learning-based PI controller. Unlike current existing results, the PI parameters of each RL-based PI controller are determined by two adaptive signals, which are generated by a reinforcement learning (RL) agent, aiming at achieving better control performance. Extensive comparisons with existing methods have been conducted, and it is shown that our proposed strategy yields the best control performance thus far.","1948-9447","978-1-6654-7896-0","10.1109/CCDC55256.2022.10033480","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10033480","PI control;Reinforcement learning;PID18 benchmark;refrigeration system","Training;Couplings;PI control;Transfer functions;Reinforcement learning;Benchmark testing;Computational efficiency","control engineering computing;control nonlinearities;control system synthesis;PI control;power engineering computing;refrigeration;reinforcement learning;three-term control","3rd IFAC Conference on Advances;control performance;output vapor-compression refrigeration system;PI parameters;PID18 benchmark challenge;PID18 challenge;proportional-integral-derivative controllers;reinforcement learning agent;reinforcement-learning-based PI control;reinforcement-learning-based PI controller;RL-based PI controller;test control plant","","","","15","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Autonomous Boundary of Human-Machine Collaboration System Based on Reinforcement Learning","Q. Zhang; Y. -B. Zhao; Y. Kang","Department of Automation, University of Science and Technology of China, Hefei, China; College of Information Engineering, Zhejiang University of Technology, Hangzhou, China; Department of Automation, the State Key Laboratory of Fire Science, University of Science and Technology of China, and Institute of Advanced Technology, Hefei, China","2020 Australian and New Zealand Control Conference (ANZCC)","21 Jan 2021","2020","","","160","165","This paper provides a human-machine collaborative control framework, including artificial intelligence decision systems, human-level control, arbiter judgment, and learning of autonomous boundary, so that human suggestions are incorporated into the training process of decisions, assisting agents to learn quickly control decision tasks. Based on the model-free deep reinforcement learning algorithm HITL-AC, the human feedback (reward or punishment) is connected with the reward of the agent, so that the agent continuously tries to find a better boundary during the system's operation, avoiding defects of pre-fixed boundary. This formulation improves the data efficiency of reinforcement learning and plays a guiding role in seeking human intervention when the agent is in an uncertain environmental state during the test use phase. The fourth section of the paper gives a training demonstration of a realtime environment (bipedal walker). Compared with existing standard reinforcement learning methods that do not consider boundary concepts, the method with boundary information mentioned in this article can accelerate the process of agent reinforcement learning during the training phase, and seek human help when guiding the dangerous state of the agent during the test phase. And the real-time optimization algorithm (HITL-AC) for the boundary is better than the fixed value algorithm (HITL-FIX). This is beneficial for solving real-world problems, further proving the feasibility and effectiveness of the proposed framework and method.","","978-1-7281-9992-4","10.1109/ANZCC50923.2020.9318326","National Natural Science Foundation of China(grant numbers:61673361,61725304,61673350); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9318326","","Reinforcement learning;Man-machine systems;Markov processes;Training;Task analysis;Decision making;Control systems","decision making;human computer interaction;learning (artificial intelligence);multi-agent systems","human intervention;training demonstration;autonomous boundary;human-machine collaboration system;human-machine collaborative control framework;artificial intelligence decision systems;human-level control;arbiter judgment;human feedback;deep reinforcement learning;HITL-AC","","","","20","IEEE","21 Jan 2021","","","IEEE","IEEE Conferences"
"Human-in-the-Loop Behavior Modeling via an Integral Concurrent Adaptive Inverse Reinforcement Learning","H. -N. Wu; M. Wang","School of Automation Science and Electrical Engineering, Science and Technology on Aircraft Control Laboratory, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Science and Technology on Aircraft Control Laboratory, Beihang University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","12","One goal of artificial intelligence (AI) research is to teach machines how to learn from humans, such that they can perform a certain task in a natural human-like way. In this article, an online adaptive inverse reinforcement learning (IRL) approach to human behavior modeling is proposed to enhance machine intelligence for a class of linear human-in-the-loop (HiTL) systems using the state data only, where the human behavior is described by a linear quadratic optimal control model with an unknown weighting matrix for the quadratic cost function. First, an integral concurrent adaptive law is developed to learn the human feedback gain matrix online using the demonstrated state data only, which removes the persistent excitation (PE) conditions required by traditional adaptive estimation approaches and thus is more in line with real applications. Then, with the learned feedback gain matrix, the IRL problem is formulated as a linear matrix inequality (LMI) optimization problem, which can be efficiently solved to retrieve the weighting matrix of the human cost function. Finally, a simulation example is provided to illustrate the effectiveness of the proposed approach.","2162-2388","","10.1109/TNNLS.2023.3259581","National Key Research and Development Program of China(grant numbers:2018AAA0101400); National Natural Science Foundation of China(grant numbers:62073011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10083106","Adaptive estimation;concurrent learning (CL);human behavior modeling;human-in-the-loop (HiTL);inverse reinforcement learning (IRL);linear quadratic regulator (LQR)","Behavioral sciences;Adaptation models;Optimal control;Cost function;Adaptive systems;Linear matrix inequalities;Task analysis","","","","","","","IEEE","28 Mar 2023","","","IEEE","IEEE Early Access Articles"
"Time-Varying Formation of Heterogeneous Multiagent Systems via Reinforcement Learning Subject to Switching Topologies","D. Liu; H. Liu; J. Lü; F. L. Lewis","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Institute of Artificial Intelligence, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; University of Texas at Arlington Research Institute, The University of Texas at Arlington, Fort Worth, TX, USA","IEEE Transactions on Circuits and Systems I: Regular Papers","26 May 2023","2023","70","6","2550","2560","This paper investigates the optimal formation control of a heterogeneous multiagent system consisting of multiple quadrotors and ground vehicles via reinforcement learning to achieve the time-varying formation under switching topologies. A distributed observer is firstly constructed to generate references using local information for each vehicle to form time-varying formation and the convergence of the observer under switching topologies is proven. Then, reinforcement learning methods are provided for the heterogeneous vehicle group to realize the optimal tracking control without information of vehicle dynamical model. Simulation tests are given to confirm the effectiveness of the proposed method.","1558-0806","","10.1109/TCSI.2023.3250516","National Natural Science Foundation of China(grant numbers:62273015,61873012); Beijing Nova Program; China Postdoctoral Science Foundation(grant numbers:2021M700336); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10058696","Heterogeneous vehicles;time-varying formation control;reinforcement learning;air-ground coordination","Topology;Quadrotors;Switches;Vehicle dynamics;Optimal control;Time-varying systems;Reinforcement learning","autonomous aerial vehicles;helicopters;multi-agent systems;multi-robot systems;optimal control;reinforcement learning;time-varying systems;vehicle dynamics","ground vehicles;heterogeneous multiagent system;heterogeneous vehicle group;multiple quadrotors;optimal formation control;optimal tracking control;reinforcement learning methods;reinforcement learning subject;switching topologies;time-varying formation","","","","32","IEEE","3 Mar 2023","","","IEEE","IEEE Journals"
"Auxiliary Input Design for Active Fault Detection via Deep Reinforcement Learning","F. Jia; X. He","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","2021 CAA Symposium on Fault Detection, Supervision, and Safety for Technical Processes (SAFEPROCESS)","1 Feb 2022","2021","","","1","6","The paper addresses a novel algorithm of active fault detection and control for closed-loop system with stochastic noise, where an auxiliary input signal generator is trained by deep reinforcement learning algorithm. The input signal consists of optimal control input and auxiliary input. In order to obtain the optimal control policy, a linear quadratic tracker is designed by using the model-based policy iterative algorithm. The problem of auxiliary input signal design is transformed into an optimization problem with the aim of minimizing a discounted cost, which is a trade-off between fault detection and control performance. A simulation example is carried out to demonstrate the effectiveness of the proposed method.","","978-1-6654-0115-9","10.1109/SAFEPROCESS52771.2021.9693676","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693676","Active fault detection;tracking control;auxiliary input design;deep reinforcement learning;neural network","Costs;Fault detection;Simulation;Optimal control;Stochastic processes;Reinforcement learning;Approximation algorithms","closed loop systems;fault diagnosis;iterative methods;learning (artificial intelligence);linear quadratic control;Markov processes;optimal control;optimisation;stochastic processes;stochastic systems","auxiliary input design;active fault detection;closed-loop system;stochastic noise;auxiliary input signal generator;deep reinforcement learning algorithm;optimal control input;optimal control policy;linear quadratic tracker;model-based policy iterative algorithm;auxiliary input signal design;optimization problem;trade-off between fault detection;control performance","","","","27","IEEE","1 Feb 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Non-Affine Nonlinear Non-Minimum Phase System Tracking Under Additive-State-Decomposition-Based Control Framework","L. Chen; Q. Quan","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)","7 Jul 2023","2023","","","2043","2049","This paper proposes a reinforcement-learning additive-state-decomposition-based tracking controller for a class of non-affine nonlinear non-minimum phase systems. Because the tracking performance is not satisfied with the model-based additive-state-decomposition tracking control with an approximate ideal internal model, two reinforcement learning schemes are introduced to improve the performance under the proposed additive-state-decomposition-based control framework. One is used to generate control commands, and the other is used to generate tracking reference commands. Finally, numerical simulations show the effectiveness of the proposed controller.","2767-9861","979-8-3503-2105-0","10.1109/DDCLS58216.2023.10166298","National Natural Science Foundation of China(grant numbers:61973015,62103335); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10166298","Reinforcement Learning;Non-affine Systems;Non-minimum Phase Systems;Additive State Decomposition","Learning systems;Simulation;Reinforcement learning;Numerical simulation;Control systems;Trajectory;Numerical models","control system synthesis;nonlinear control systems;reinforcement learning","controller design;nonaffine nonlinear nonminimum phase system tracking;reinforcement-learning additive-state-decomposition-based tracking controller;tracking reference commands","","","","12","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Dynamic Order Recommendation for On-Demand Food Delivery","X. Wang; L. Wang; C. Dong; H. Ren; K. Xing","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; School of Mechanical and Automotive Engineering, Qingdao Hengxing University of Science and Technology, Qingdao, China; Meituan, Beijing, China; Meituan, Beijing, China","Tsinghua Science and Technology","22 Sep 2023","2024","29","2","356","367","On-demand food delivery (OFD) is gaining more and more popularity in modern society. As a kernel order assignment manner in OFD scenario, order recommendation directly influences the delivery efficiency of the platform and the delivery experience of riders. This paper addresses the dynamism of the order recommendation problem and proposes a reinforcement learning solution method. An actor-critic network based on long short term memory (LSTM) unit is designed to deal with the order-grabbing conflict between different riders. Besides, three rider sequencing rules are accordingly proposed to match different time steps of the LSTM unit with different riders. To test the performance of the proposed method, extensive experiments are conducted based on real data from Meituan delivery platform. The results demonstrate that the proposed reinforcement learning based order recommendation method can significantly increase the number of grabbed orders and reduce the number of order-grabbing conflicts, resulting in better delivery efficiency and experience for the platform and riders.","1007-0214","","10.26599/TST.2023.9010041","National Natural Science Foundation of China(grant numbers:62273193); Tsinghua University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10258252","on-demand food delivery;order recommendation;reinforcement learning;actor-critic network;long short term memory","Sequential analysis;Reinforcement learning;Kernel;Long short term memory","deep learning (artificial intelligence);learning (artificial intelligence);recommender systems;recurrent neural nets;reinforcement learning","actor-critic network;delivery efficiency;different riders;grabbed orders;kernel order assignment manner;long short term memory unit;LSTM unit;Meituan delivery platform;OFD scenario;On-Demand Food Delivery;On-demand food delivery;order recommendation problem;order-grabbing conflict;reinforcement learning based order recommendation method;reinforcement learning solution method;reinforcement learning-based dynamic order recommendation;rider sequencing rules","","","","32","","22 Sep 2023","","","TUP","TUP Journals"
"Accelerating Reinforcement Learning-Based CCSL Specification Synthesis Using Curiosity-Driven Exploration","M. Hu; M. Zhang; F. Mallet; X. Fu; M. Chen","MoE Research Engineering Center of Hardware/Software Co-design Technology and Application, East China Normal University, Shanghai, China; MoE Research Engineering Center of Hardware/Software Co-design Technology and Application, East China Normal University, Shanghai, China; CNRS, Inria, Université Cote d'Azur, Nice, France; ECE Department, University of Houston, Houston, TX, USA; MoE Research Engineering Center of Hardware/Software Co-design Technology and Application, East China Normal University, Shanghai, China","IEEE Transactions on Computers","6 Apr 2023","2023","72","5","1431","1446","The Clock Constraint Specification Language (CCSL) has been widely acknowledged as a promising system-level specification for the modeling and analysis of timing behaviors of real-time and embedded systems. However, along with the increasing complexity of modern systems coupled with strict time-to-market constraints, it becomes more and more difficult for requirement engineers to accurately figure out CCSL specifications from natural language-based requirement documents, since they lack both expertise in formal CCSL modeling and design automation tools to support quick and automatic generation of CCSL specifications. To solve the above problem, in this paper we introduce a novel and efficient Reinforcement Learning (RL)-based synthesis approach that can facilitate requirement engineers to quickly figure out their expected CCSL specifications. For a given incomplete CCSL specification, our approach adopts RL-based enumeration to explore all the feasible solutions to fill the holes within CCSL constraints, and leverages curiosity-driven exploration to accelerate the enumeration process. Based on the combination of our proposed curiosity-driven exploration heuristic and deductive reasoning techniques, our approach can not only prune unfruitful enumeration solutions effectively, but also optimize the enumeration process to search for the tightest solution quickly, thus the overall synthesis process can be accelerated dramatically. Comprehensive experimental results demonstrate that our approach significantly outperforms state-of-the-art methods in terms of both synthesis time and synthesis accuracy.","1557-9956","","10.1109/TC.2022.3197956","National Key Research and Development Program of China(grant numbers:2018YFB2101300); National Natural Science Foundation of China(grant numbers:61872147); Shanghai Trusted Industry Internet Software Collaborative Innovation Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858908","Clock constraint specification language;curiosity-driven exploration;reinforcement learning;specification synthesis","Unified modeling language;Timing;Real-time systems;Embedded systems;Semantics;Cognition;Clocks","embedded systems;formal specification;formal verification;reinforcement learning;specification languages;Unified Modeling Language","automatic generation;CCSL constraints;CCSL specifications;clock constraint specification language;curiosity-driven exploration;curiosity-driven exploration heuristic;deductive reasoning techniques;design automation tools;natural language-based requirement documents;reinforcement learning-based CCSL specification synthesis;reinforcement learning-based synthesis approach;requirement engineers;RL-based enumeration;system-level specification;time-to-market constraints","","","","45","IEEE","17 Aug 2022","","","IEEE","IEEE Journals"
"Meta-ATMoS+: A Meta-Reinforcement Learning Framework for Threat Mitigation in Software-Defined Networks","H. Tsang; M. A. Salahuddin; N. Limam; R. Boutaba","David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada","2023 IEEE 48th Conference on Local Computer Networks (LCN)","6 Sep 2023","2023","","","1","9","As cyber threats become increasingly common, automated threat mitigation solutions are more necessary than ever. Conventional threat mitigation frameworks are difficult to tune for different network environments, but frameworks utilizing deep reinforcement learning (RL) have been proven to be an effective approach that can adapt to different networks automatically. Existing RL-based frameworks have shown to be generalizable to different network sizes and threats, and robust to false positives. However, training RL agents for these frameworks can be challenging in a production environment as the training process is time-consuming and disruptive to the production network. Hence, a staging environment is required to effectively train them. In this paper, we propose Meta-ATMoS+, a meta-RL framework for threat mitigation in software-defined networks. We leverage Model-Agnostic Meta-Learning (MAML) to find an initialization for the RL agent that generalizes to a variety of different network configurations. We show that the RL agent with MAML-learned initialization can accomplish few-shot learning on a target network with comparable performance to training on a staging environment. Few-shot learning not only allows the model to be trainable directly in the production environment but also enables human-in-the-loop RL for the mitigation of threats that do not have an easily-definable reward function.","2832-1421","979-8-3503-0073-4","10.1109/LCN58197.2023.10223403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10223403","Meta-Reinforcement learning;human-in-the-loop;threat mitigation;software-defined networks","Training;Metalearning;Adaptation models;Computational modeling;Production;Reinforcement learning;Human in the loop","","","","","","18","IEEE","6 Sep 2023","","","IEEE","IEEE Conferences"
"AutoFS: Automated Feature Selection via Diversity-Aware Interactive Reinforcement Learning","W. Fan; K. Liu; H. Liu; P. Wang; Y. Ge; Y. Fu","Department of Computer Science, University of Central Florida, Orlando; Department of Computer Science, University of Central Florida, Orlando; Business Intelligence Lab, Baidu Research, Beijing; Department of Computer Science, University of Central Florida, Orlando; Eller College of Management, University of Arizona, Tucson; Department of Computer Science, University of Central Florida, Orlando","2020 IEEE International Conference on Data Mining (ICDM)","9 Feb 2021","2020","","","1008","1013","In this paper, we study the problem of balancing effectiveness and efficiency in automated feature selection. Feature selection is to find the optimal feature subset from large-scale feature space, and is a fundamental intelligence for machine learning and predictive analysis. After exploring many feature selection methods, we observe a computational dilemma: 1) traditional feature selection methods (e.g., K-Best, decision tree based ranking, mRMR) are mostly efficient, but difficult to identify the best subset; 2) the emerging reinforced feature selection methods automatically navigate feature space to explore the best subset, but are usually inefficient. Are automation and efficiency always apart from each other? Can we bridge the gap between effectiveness and efficiency under automation? Motivated by such a computational dilemma, this study is to develop a novel feature space navigation method. To that end, we propose an Interactive Reinforced Feature Selection (IRFS) framework that guides agents by not just self-exploration experience, but also diverse external skilled trainers to accelerate learning for feature exploration. Specifically, we formulate the feature selection problem into an interactive reinforcement learning framework. In this framework, we first model two trainers skilled at different searching strategies: (1) KBest based trainer; (2) Decision Tree based trainer. We then develop two strategies: (1) to identify assertive and hesitant agents to diversify agent training, and (2) to enable the two trainers to take the teaching role in different stages to fuse the experience of the trainers and diversify teaching process. Such a hybrid teaching strategy can help agents to learn broader knowledge, and thereafter be more effective. Finally, we present extensive experiments on real-world datasets to demonstrate the improved performances of our method: more efficient than reinforced selection and more effective than classic feature selection.","2374-8486","978-1-7281-8316-9","10.1109/ICDM50108.2020.00117","National Science Foundation(grant numbers:1755946,I2040950,2006889); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9338318","","Training;Automation;Navigation;Reinforcement learning;Feature extraction;Space exploration;Decision trees","decision trees;feature extraction;feature selection;learning (artificial intelligence);teaching","optimal feature subset;large-scale feature space;computational dilemma;diverse external skilled trainers;automated feature selection;AutoFS;diversity-aware interactive reinforcement learning;feature space navigation method;IRFS framework;KBest based trainer;decision tree based trainer;hybrid teaching strategy;machine learning;predictive analysis","","17","","25","IEEE","9 Feb 2021","","","IEEE","IEEE Conferences"
"Safe Reinforcement Learning using Formal Verification for Tissue Retraction in Autonomous Robotic-Assisted Surgery","A. Pore; D. Corsi; E. Marchesini; D. Dall’Alba; A. Casals; A. Farinelli; P. Fiorini","Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy; Center of Research in Biomedical Engineering, Universitat Politècnica de Catalunya, Barcelona, Spain; Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy","2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","16 Dec 2021","2021","","","4025","4031","Deep Reinforcement Learning (DRL) is a viable solution for automating repetitive surgical subtasks due to its ability to learn complex behaviours in a dynamic environment. This task automation could lead to reduced surgeon’s cognitive workload, increased precision in critical aspects of the surgery, and fewer patient-related complications. However, current DRL methods do not guarantee any safety criteria as they maximise cumulative rewards without considering the risks associated with the actions performed. Due to this limitation, the application of DRL in the safety-critical paradigm of robot-assisted Minimally Invasive Surgery (MIS) has been constrained. In this work, we introduce a Safe-DRL framework that incorporates safety constraints for the automation of surgical subtasks via DRL training. We validate our approach in a virtual scene that replicates a tissue retraction task commonly occurring in multiple phases of an MIS. Furthermore, to evaluate the safe behaviour of the robotic arms, we formulate a formal verification tool for DRL methods that provides the probability of unsafe configurations. Our results indicate that a formal analysis guarantees safety with high confidence such that the robotic instruments operate within the safe workspace and avoid hazardous interaction with other anatomical structures.","2153-0866","978-1-6654-1714-3","10.1109/IROS51168.2021.9636175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636175","","Training;Automation;Tissue damage;Virtual environments;Reinforcement learning;Safety;Trajectory","biological tissues;deep learning (artificial intelligence);formal verification;medical robotics;mobile robots;probability;reinforcement learning;safety;surgery","safe reinforcement learning;automating repetitive surgical subtasks;complex behaviours;dynamic environment;DRL methods;safety-critical paradigm;robot-assisted minimally invasive surgery;MIS;DRL training;virtual scene;tissue retraction task;robotic arms;formal verification tool;formal analysis guarantees safety;robotic instruments;autonomous robotic-assisted surgery;deep reinforcement learning;safe-DRL framework;surgeons cognitive;probability;patient-related complications","","9","","31","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Interactive Reinforcement Learning for Feature Selection With Decision Tree in the Loop","W. Fan; K. Liu; H. Liu; Y. Ge; H. Xiong; Y. Fu","Department of Computer Science, University of Central Florida, Orlando, FL, USA; Department of Computer Science, University of Central Florida, Orlando, FL, USA; Hong Kong University of Science and Technology, Hong Kong, China; Eller College of Management, University of Arizona, Tucson, AZ, USA; Rutgers University, New Brunswick, NJ, USA; Department of Computer Science, University of Central Florida, Orlando, FL, USA","IEEE Transactions on Knowledge and Data Engineering","10 Jan 2023","2023","35","2","1624","1636","We study the problem of balancing effectiveness and efficiency in automated feature selection. Feature selection is to find an optimal feature subset from large feature space. After exploring many feature selection methods, we observe a computational dilemma: 1) traditional feature selection (e.g., mRMR) is mostly efficient, but difficult to identify the best subset; 2) the emerging reinforced feature selection automatically navigates feature space to search the best subset, but is usually inefficient. Are automation and efficiency always apart from each other? Can we bridge the gap between effectiveness and efficiency under automation? Motivated by this dilemma, we aim to develop a novel feature space navigation method. In our preliminary work, we leveraged interactive reinforcement learning to accelerate feature selection by external trainer-agent interaction. Our preliminary work can be significantly improved by modeling the structured knowledge of its downstream task (e.g., decision tree) as learning feedback. In this journal version, we propose a novel interactive and closed-loop architecture to simultaneously model interactive reinforcement learning (IRL) and decision tree feedback (DTF). Specifically, IRL is to create an interactive feature selection loop and DTF is to feed structured feature knowledge back to the loop. The DTF improves IRL from two aspects. First, the tree-structured feature hierarchy generated by decision tree is leveraged to improve state representation. In particular, we represent the selected feature subset as an undirected graph of feature-feature correlations and a directed tree of decision features. We propose a new embedding method capable of empowering Graph Convolutional Network (GCN) to jointly learn state representation from both the graph and the tree. Second, the tree-structured feature hierarchy is exploited to develop a new reward scheme. In particular, we personalize reward assignment of agents based on decision tree feature importance. In addition, observing agents’ actions can also be a feedback, we devise another new reward scheme, to weigh and assign reward based on the selected frequency ratio of each agent in historical action records. Finally, we present extensive experiments with real-world datasets to demonstrate the improved performances of our method.","1558-2191","","10.1109/TKDE.2021.3102120","National Science Foundation(grant numbers:1755946,I2040950,2006889,2045567,IIS-2040799); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507351","Reinforcement learning;interaction mechanism;decision tree in the loop;feature selection","Feature extraction;Decision trees;Task analysis;Reinforcement learning;Correlation;Automation;Space exploration","decision trees;feature extraction;learning (artificial intelligence);pattern classification;trees (mathematics)","automated feature selection;decision features;decision tree feature importance;emerging reinforced feature selection;external trainer-agent interaction;feature selection methods;feature-feature correlations;interactive feature selection loop;interactive reinforcement learning;leveraged interactive reinforcement;novel feature space navigation method;optimal feature subset;selected feature subset;selected frequency ratio;structured feature knowledge;tree-structured feature hierarchy","","7","","41","IEEE","4 Aug 2021","","","IEEE","IEEE Journals"
"Energy Efficiency Optimization for SWIPT-Based D2D-Underlaid Cellular Networks Using Multiagent Deep Reinforcement Learning","S. Muy; D. Ron; J. -R. Lee","School of Electronics and Electrical Engineering, Chung-Ang University, Seoul, Republic of Korea; School of Electronics and Electrical Engineering, Chung-Ang University, Seoul, Republic of Korea; School of Electronics and Electrical Engineering, Chung-Ang University, Seoul, Republic of Korea","IEEE Systems Journal","13 Jun 2022","2022","16","2","3130","3138","In this article, we study the optimization of energy efficiency in wireless device-to-device (D2D-underlaid cellular networks where multiple D2D pairs adopt simultaneous wireless information and power transfer functionality. We formulate the optimization problem, which is a NP-hard combinatorial problem with nonlinear constraints. First, we use optimization-based-iterative techniques such as exhaustive search (ES) and gradient search (GS) with barrier, which are generally used to obtain the global optimum and local optimum of the nonconvex optimization problem, respectively. Considering that these techniques require a centralized unit to share information with each other, we propose multiagent deep reinforcement learning to solve this optimization problem in a distributed manner, which provides optimal decision making together with efficient deep network training under inequality constraints including transmit power, power splitting ratio, and minimum requirement data rate for D2D and cellular users. In this proposed method, we consider the virtual environment in which each agent can train their model according to shared information, and then, we deploy the trained model into the actual environment where each agent can only know their channel gain, interference power, and required minimum throughput. Simulation results show that the proposed algorithm can afford a near-global-optimum solution with much lower computation complexity than ES and outperforms the GS.","1937-9234","","10.1109/JSYST.2021.3098860","Ministry of Science and ICT, South Korea; Iran Telecommunication Research Center(grant numbers:IITP-2021-2018-0-01799); Institute for Information and communications Technology Planning and Evaluation; Korea Institute of Energy Technology Evaluation and Planning; Ministry of Trade, Industry and Energy(grant numbers:20214000000280); National Research Foundation of Korea; MEST(grant numbers:NRF-2020R1A2C1010929); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9509390","Device-to-device (D2D) communication;energy dissipation;energy efficiency;multiagent deep reinforcement learning (DRL);power control;simultaneous wireless information and power transfer (SWIPT)","Device-to-device communication;Receivers;Transmitters;Throughput;Interference;Optimization;Energy harvesting","cellular radio;decision making;optimisation;reinforcement learning;telecommunication computing;telecommunication equipment;wireless channels","energy efficiency optimization;multiagent deep reinforcement learning;multiple D2D pairs;power transfer functionality;NP-hard combinatorial problem;nonlinear constraints;gradient search;nonconvex optimization problem;deep network training;transmit power;power splitting ratio;minimum requirement data rate;cellular users;shared information;interference power;optimization based iterative techniques;swipt based D2D underlaid cellular networks","","5","","26","IEEE","9 Aug 2021","","","IEEE","IEEE Journals"
"Self-Configurable Cyber-Physical Intrusion Detection for Smart Homes Using Reinforcement Learning","R. Heartfield; G. Loukas; A. Bezemskij; E. Panaousis","School of Computing and Mathematical Sciences, University of Greenwich, London, U.K.; School of Computing and Mathematical Sciences, University of Greenwich, London, U.K.; School of Computing and Mathematical Sciences, University of Greenwich, London, U.K.; School of Computing and Mathematical Sciences, University of Greenwich, London, U.K.","IEEE Transactions on Information Forensics and Security","28 Dec 2020","2021","16","","1720","1735","The modern Internet of Things (IoT)-based smart home is a challenging environment to secure: devices change, new vulnerabilities are discovered and often remain unpatched, and different users interact with their devices differently and have different cyber risk attitudes. A security breach's impact is not limited to cyberspace, as it can also affect or be facilitated in physical space, for example, via voice. In this environment, intrusion detection cannot rely solely on static models that remain the same over time and are the same for all users. We present MAGPIE, the first smart home intrusion detection system that is able to autonomously adjust the decision function of its underlying anomaly classification models to a smart home's changing conditions (e.g., new devices, new automation rules and user interaction with them). The method achieves this goal by applying a novel probabilistic cluster-based reward mechanism to non-stationary multi-armed bandit reinforcement learning. MAGPIE rewards the sets of hyperparameters of its underlying isolation forest unsupervised anomaly classifiers based on the cluster silhouette scores of their output. Experimental evaluation in a real household shows that MAGPIE exhibits high accuracy because of two further innovations: it takes into account both cyber and physical sources of data; and it detects human presence to utilise models that exhibit the highest accuracy in each case. MAGPIE is available in open-source format, together with its evaluation datasets, so it can benefit from future advances in unsupervised and reinforcement learning and be able to be enriched with further sources of data as smart home environments and attacks evolve.","1556-6021","","10.1109/TIFS.2020.3042049","European Coordinated Research on Long-term Challenges in Information and Communication Sciences and Technologies ERA-NET (CHIST-ERA), under Project COCOON; EPSRC(grant numbers:EP/P016448/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9277640","Intrusion detection system;cyber-physical attacks;smart home;reinforcement learning","Smart homes;IP networks;Intrusion detection;Hidden Markov models;Reinforcement learning;Wireless fidelity;Monitoring","home automation;home computing;Internet of Things;learning (artificial intelligence);pattern classification;pattern clustering;probability;security of data","smart home environments;self-configurable cyber-physical intrusion detection;users interact;security breach;static models;MAGPIE;smart home intrusion detection system;decision function;user interaction;nonstationary multiarmed bandit reinforcement learning;isolation forest;anomaly classifiers;cluster silhouette scores;probabilistic cluster-based reward mechanism;anomaly classification models;cyber risk attitudes;Internet of Things-based smart home","","44","","45","IEEE","2 Dec 2020","","","IEEE","IEEE Journals"
"A Reinforcement Learning-Based Framework for Solving Physical Design Routing Problem in the Absence of Large Test Sets","U. Gandhi; I. Bustany; W. Swartz; L. Behjat","University of Calgary, Calgary, Canada; Xilinx, San Jose, United States; TimberWolf Systems Inc, University of Texas at Dallas, Dallas, United States; University of Calgary, Calgary, Canada","2019 ACM/IEEE 1st Workshop on Machine Learning for CAD (MLCAD)","16 Jul 2020","2019","","","1","6","Advances in Electronic Design Automation(EDA) methods have made the designers and programmers to search for new ways to solve the complex problems seen in today's Very Large Scale Integration circuits. Machine learning (ML), especially supervised learning, has been used to predict design rule violations. However, supervised learning requires large amount of labeled data. With the competitive nature of EDA based companies, there is limited access to benchmarks and labeled data. In this work, we propose a data-independent reinforcement learning (RL) based routing model called Alpha-PD-Router, which learns to route a circuit and correct short violations. The Alpha-PD-Router is based on a two-player collaborative game model that has been trained on a small circuit and successfully resolves 75 violations in 99 cases of 2 pins net arrangements in the testing phase. The proposed model has the potential to be used as a framework to develop RL based routing techniques untethered by the scarce availability of large routing data samples or designer expertise.","","978-1-7281-5758-0","10.1109/MLCAD48534.2019.9142109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9142109","Physical design;routing;machine learning;reinforcement learning;collaborative min-max game theory","Routing;Games;Collaboration;Physical design;Machine learning;Integrated circuit modeling;Pins","electronic design automation;learning (artificial intelligence);network routing;VLSI","two-player collaborative game model;testing phase;RL;routing techniques;routing data samples;designer expertise;reinforcement learning-based framework;physical design routing problem;complex problems;very large scale integration circuits;machine learning;supervised learning;design rule violations;competitive nature;EDA based companies;data-independent reinforcement learning;correct short violations;alpha-PD-router","","8","","39","IEEE","16 Jul 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Path Generation Using Sequential Pattern Reduction and Self-Directed Curriculum Learning","T. Kim; J. -H. Lee","Department of Computer Software, Korea University of Science and Technology, Daejeon, South Korea; Department of Computer Software, Korea University of Science and Technology, Daejeon, South Korea","IEEE Access","18 Aug 2020","2020","8","","147790","147807","Recent advancements in robots and deep learning have led to active research in human-robot interaction. However, non-physical interaction using visual devices such as laser pointers has gained less attention than physical interaction using complex robots such as humanoids. Such vision-based interaction has high potential for use in recent human-robot collaboration environments such as assembly guidance, even with a minimum amount of configuration. In this paper, we introduce a simple robotic laser pointer device that follows an arbitrary planar path and is designed to be a visual instructional aid. We also propose an image-based automatic path generation method using reinforcement learning and a sequential pattern reduction technique. However, such vision-based human-robot interaction is generally performed in a dynamic environment, and it can frequently be necessary to calibrate the devices more than once. In this paper, we avoid the need for this re-calibration process through episodic randomization learning and improved learning efficiency. In particular, contrary to previous approaches, the agent controls the curriculum difficulty in a self-directed manner to determine the optimal curriculum. To our knowledge, this is the first study of curriculum learning that incorporates an explicit learning environment control signal initiated by the agent itself. Through quantitative and qualitative analyses, we show that the proposed self-directed curriculum learning method outperforms ordinary episodic randomization and curriculum learning. We hope that the proposed method can be extended to a general reinforcement learning framework.","2169-3536","","10.1109/ACCESS.2020.3015245","Institute of Information & communications Technology Planning & Evaluation (IITP); Korea government (MSIT) (Development of Human-care Robot Technology for Aging Society)(grant numbers:2017-0-00162); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162635","Path generation;robotic laser pointer;deep reinforcement learning;curriculum learning","Visualization;Learning systems;Robot sensing systems;Three-dimensional displays;Human-robot interaction;Lasers","control engineering computing;humanoid robots;human-robot interaction;learning (artificial intelligence);path planning","optimal curriculum;explicit learning environment control signal;curriculum learning method;general reinforcement learning framework;reinforcement learning-based path generation;self-directed curriculum learning;deep learning;nonphysical interaction;visual devices;laser pointers;physical interaction;complex robots;vision-based interaction;human-robot collaboration environments;arbitrary planar path;visual instructional aid;image-based automatic path generation method;sequential pattern reduction technique;vision-based human-robot interaction;dynamic environment;episodic randomization learning;curriculum difficulty;robotic laser pointer device","","5","","68","CCBY","10 Aug 2020","","","IEEE","IEEE Journals"
"A Path-Integral-Based Reinforcement Learning Algorithm for Path Following of an Autoassembly Mobile Robot","W. Zhu; X. Guo; Y. Fang; X. Zhang","Institute of Robotics and Automatic Information System, Nankai University, Tianjin, China; Institute of Robotics and Automatic Information System, Nankai University, Tianjin, China; Institute of Robotics and Automatic Information System, Nankai University, Tianjin, China; Institute of Robotics and Automatic Information System, Nankai University, Tianjin, China","IEEE Transactions on Neural Networks and Learning Systems","29 Oct 2020","2020","31","11","4487","4499","Reinforcement learning (RL) combined with deep neural networks has led to a number of great achievements for robot control in virtual computer environments, where sufficient data can be obtained without any difficulty to train various models. However, thus far, only few and relatively simple tasks have been accomplished for practical robots, which is mainly caused by the following two reasons. First, training with real robots, especially with dynamic systems, is too complicated to be fully and accurately represented in simulations. Second, it is very costly to obtain training data from real systems. To address these two problems effectively, in this article, a path-integral-based RL algorithm is proposed for the task of path following of an autoassembly mobile robot, wherein three kernel techniques are introduced. First, a generalized path-integral-control approach is proposed to obtain the numerical solution of a stochastic dynamical system, wherein the calculation of the gradient and kinematics inverse is avoided to ensure fast and reliable training convergence. Second, a novel parameterization method using Lyapunov techniques is introduced into the RL algorithm to ensure good performance of the system when directly transferring simulation results into practical systems. Third, the optimal parameters for all discrete initial states are first learned offline and then tuned online to improve the generalization and real-time performance. In addition to the optimization control for the mobile robot, the proposed method also possesses general applicability for a class of nonlinear systems such as crane systems. Simulation and experimental results are included and analyzed to illustrate the superior performance of the proposed algorithm.","2162-2388","","10.1109/TNNLS.2019.2955699","National Natural Science Foundation of China(grant numbers:61603200,U1313210,61873132); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941307","Autoassembly mobile robot;Lyapunov techniques;path following;path integral;reinforcement learning (RL)","Optimal control;Mobile robots;Task analysis;Dynamical systems;Training;Real-time systems","learning (artificial intelligence);mobile robots;neurocontrollers;optimal control;robotic assembly","Lyapunov techniques;crane systems;nonlinear systems;optimization control;stochastic dynamical system;generalized path-integral-control approach;path-integral-based RL algorithm;virtual computer environments;robot control;deep neural networks;autoassembly mobile robot;path-integral-based reinforcement learning algorithm","","21","","35","IEEE","24 Dec 2019","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Optimal Decoupling Capacitor Design Method for Silicon Interposer-Based 2.5-D/3-D ICs","H. Park; J. Park; S. Kim; K. Cho; D. Lho; S. Jeong; S. Park; G. Park; B. Sim; S. Kim; Y. Kim; J. Kim","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Nara Institute of Science and Technology, Nara, Japan; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Components, Packaging and Manufacturing Technology","10 Mar 2020","2020","10","3","467","478","In this article, we first propose a deep reinforcement learning (RL)-based optimal decoupling capacitor (decap) design method for silicon interposer-based 2.5-D/3-D integrated circuits (ICs). The proposed method provides an optimal decap design that satisfies target impedance with a minimum area. Using deep RL algorithms based on reward feedback mechanisms, an optimal decap design guideline can be derived. For verification, the proposed method was applied to test power distribution networks (PDNs) and self-PDN impedance was compared with full search simulation results. We successfully verified by the full search simulation that the proposed method provides one of the solution sets. Conventional approaches are based on complex analytical models from power integrity (PI) domain expertise. However, the proposed method requires only specifications of the PDN structure and decap, along with a simple reward model, achieving fast and accurate data-driven results. Computing time of the proposed method was a few minutes, significantly reduced than that of the full search simulation, which took more than a month. Furthermore, the proposed deep RL method covered up to 1017-1018 cases, an approximately 1012-1013 order increase compared to the previous RL-based methods that did not utilize deep-learning techniques.","2156-3985","","10.1109/TCPMT.2020.2972019","ANSYS Korea; National Research Foundation of Korea; National Research Foundation of Korea(grant numbers:2019M3F3A1A03079612); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8985416","Decoupling capacitor;deep reinforcement learning (RL);power distribution network (PDN);silicon interposer","Impedance;Design methodology;Silicon;Capacitors;Through-silicon vias;Inductance","capacitors;circuit optimisation;electronic engineering computing;integrated circuit design;integrated circuit interconnections;integrated circuit modelling;learning (artificial intelligence);three-dimensional integrated circuits","simple reward model;PI domain expertise;self-PDN impedance;power distribution networks;reward feedback mechanism;decap design method;2.5D IC;3D IC;silicon interposer;deep reinforcement learning;optimal decoupling capacitor design method;deep RL method;power integrity domain expertise;deep RL algorithms","","42","","27","IEEE","6 Feb 2020","","","IEEE","IEEE Journals"
"IronMan-Pro: Multiobjective Design Space Exploration in HLS via Reinforcement Learning and Graph Neural Network-Based Modeling","N. Wu; Y. Xie; C. Hao","Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","17 Feb 2023","2023","42","3","900","913","Despite the great success of high-level synthesis (HLS) tools, we observe several unresolved challenges: 1) the high-level abstraction of HLS programming styles sometimes conceals optimization opportunities; 2) the actual quality of resulting RTL designs is hard to predict; and 3) existing HLS tools do not provide flexible tradeoff (Pareto) solutions among different objectives and constraints. To this end, we propose an end-to-end framework, namely, IronMan-Pro. The primary goal is to enable a flexible and automated design space exploration (DSE), to provide either optimized solutions under user-specified constraints or Pareto tradeoffs among different objectives (such as resource types, area, and latency). IronMan-Pro consists of three components: 1) GPP, a highly accurate graph-neural-network-based performance and resource predictor; 2) RLMD, a reinforcement-learning-based multiobjective design exploration engine for optimal resource allocation strategies, aiming to provide Pareto solutions among different objectives; and 3) CT, a code transformer to assist RLMD and GPP, which extracts the data flow graphs from original HLS C/C++ and automatically generates synthesizable code with optimized HLS directives. Experimental results show that, 1) GPP achieves high prediction accuracy, reducing the prediction errors of HLS tools by  $10.9\times $  in resource utilization and  $5.7\times $  in critical path (CP) timing; 2) compared with meta-heuristic-based techniques, IronMan-Pro generates superior solutions improving resource utilization by  $16.0\% \sim 29.5\%$  and CP timing by 7.6%–16.5%; and 3) under user-specified constraints, IronMan-Pro can find satisfying solutions over 96% of the cases, more than twice as many as that of meta-heuristic-based techniques and with a speedup of up to  $400\times $ . This work demonstrates the great potential of applying machine learning algorithms in the electronic design automation domain, especially for the hard-to-solve problems, such as timing estimation and optimization. IronMan-Pro is available at https://github.com/lydiawunan/IronMan.","1937-4151","","10.1109/TCAD.2022.3185540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9803218","Design space exploration (DSE);graph neural network (GNN);high-level synthesis (HLS);reinforcement learning (RL)","Optimization;Timing;Resource management;Codes;Table lookup;Space exploration;Field programmable gate arrays","graph theory;high level synthesis;neural nets;Pareto optimisation;reinforcement learning;resource allocation","automated design space exploration;DSE;electronic design automation domain;flexible design space exploration;GPP;graph neural network-based;graph-neural-network-based performance;high prediction accuracy;high-level abstraction;high-level synthesis;HLS programming styles;IronMan-Pro;machine learning algorithms;metaheuristic-based techniques;multiobjective design space exploration;optimal resource allocation strategies;optimized HLS tool directives;Pareto solutions;reinforcement-learning-based multiobjective design exploration engine;resource predictor;resource utilization;RLMD;RTL designs","","4","","56","IEEE","22 Jun 2022","","","IEEE","IEEE Journals"
"Optimizing Ranking Algorithm in Recommender System via Deep Reinforcement Learning","J. Han; Y. Yu; F. Liu; R. Tang; Y. Zhang","Data & Knowledge Management Lab, Shanghai Jiao Tong University, Shanghai, China; Data & Knowledge Management Lab, Shanghai Jiao Tong University, Shanghai, China; Harbin Institute of Technology, Shenzhen, China; Huawei Noah’s Ark Lab, Shenzhen, China; Huawei Noah’s Ark Lab, Shenzhen, China","2019 International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM)","9 Jan 2020","2019","","","22","26","Recommender system, which attempts to narrow down selections for users based on their preference, plays a crucial role in many E-commerce platforms like Amazon and Taobao. Sponsored search can be regarded as a major revenue contributor of recommender system. The platform sorts the items by a ranking function and charges the advertisers for the users' positive feedback (e.g., click). However, traditional ranking strategies usually apply greedy ranking strategies (e.g.,pCTR*Bid), which considers the recommendation processes to be static and only focuses on the immediate reward. In practice, recommendation processes may be highly correlated with each other and cumulative reward needs to be taken into account. To address these issues, this paper redefines the ranking function and proposes a Deep Reinforcement learning based Ranking Strategy (DRRS) to maximize the cumulative reward of the platform. The experiments conducted on real-world datasets demonstrate the effectiveness of the proposed framework.","","978-1-7281-4691-1","10.1109/AIAM48774.2019.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950897","Recommender system, Actor-Critic network, Deep reinforcement learning, Ranking","","electronic commerce;greedy algorithms;learning (artificial intelligence);neural nets;recommender systems;search engines","Ranking algorithm;recommender system;E-commerce platforms;greedy ranking strategies;recommendation processes;DRRS;deep reinforcement learning based ranking strategy","","3","","21","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Towards Safe Human-Robot Collaboration Using Deep Reinforcement Learning","M. El-Shamouty; X. Wu; S. Yang; M. Albus; M. F. Huber","Department of Robot and Assistive Systems, Fraunhofer IPA; Center for Cyber Cognitive Intelligence, Fraunhofer IPA; Department of Robot and Assistive Systems, Fraunhofer IPA; Department of Robot and Assistive Systems, Fraunhofer IPA; Center for Cyber Cognitive Intelligence, Fraunhofer IPA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","4899","4905","Safety in Human-Robot Collaboration (HRC) is a bottleneck to HRC-productivity in industry. With robots being the main source of hazards, safety engineers use over-emphasized safety measures, and carry out lengthy and expensive risk assessment processes on each HRC-layout reconfiguration. Recent advances in deep Reinforcement Learning (RL) offer solutions to add intelligence and comprehensibility of the environment to robots. In this paper, we propose a framework that uses deep RL as an enabling technology to enhance intelligence and safety of the robots in HRC scenarios and, thus, reduce hazards incurred by the robots. The framework offers a systematic methodology to encode the task and safety requirements and context of applicability into RL settings. The framework also considers core components, such as behavior explainer and verifier, which aim for transferring learned behaviors from research labs to industry. In the evaluations, the proposed framework shows the capability of deep RL agents learning collision-free point-to-point motion on different robots inside simulation, as shown in the supplementary video.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196924","","Task analysis;Training;Hazards;Robot sensing systems;Service robots","hazards;human-robot interaction;industrial robots;learning (artificial intelligence);mobile robots;neural nets;occupational safety;risk management","hazard source;safety engineers;risk assessment processes;deep RL agents;human-robot collaboration;HRC-productivity;deep reinforcement learning;systematic methodology;core components","","21","","38","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"A new approach for the design of reinforcement schemes for learning automata: stochastic estimator learning algorithms","G. I. Papadimitriou","Department of Computer Engineering, University of Patras, Patras, Greece","[Proceedings] Third International Conference on Tools for Artificial Intelligence - TAI 91","6 Aug 2002","1991","","","308","317","A new approach to the design of S-model ergodic learning automata is introduced. The new scheme uses a stochastic estimator and is able to operate in nonstationary environments with high accuracy and high adaptation rate. The estimator is always recently updated and, consequently, is able to be adapted to environmental changes. The performance of the stochastic estimator learning automation (SELA) is superior to that of the previous well-known S-model ergodic schemes. Furthermore, it is proved that SELA is absolutely expedient in every stationary S-model random environment.<>","","0-8186-2300-4","10.1109/TAI.1991.167109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=167109","","Learning automata;Stochastic processes;Application software;Convergence;Feedback;Computer networks","automata theory;learning systems;stochastic processes","reinforcement schemes;learning automata;stochastic estimator learning algorithms;S-model ergodic;nonstationary environments","","2","","19","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"DRAS: Deep Reinforcement Learning for Cluster Scheduling in High Performance Computing","Y. Fan; B. Li; D. Favorite; N. Singh; T. Childers; P. Rich; W. Allcock; M. E. Papka; Z. Lan","Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA","IEEE Transactions on Parallel and Distributed Systems","4 Oct 2022","2022","33","12","4903","4917","Cluster schedulers are crucial in high-performance computing (HPC). They determine when and which user jobs should be allocated to available system resources. Existing cluster scheduling heuristics are developed by human experts based on their experience with specific HPC systems and workloads. However, the increasing complexity of computing systems and the highly dynamic nature of application workloads have placed tremendous burden on manually designed and tuned scheduling heuristics. More aggressive optimization and automation are needed for cluster scheduling in HPC. In this work, we present an automated HPC scheduling agent named DRAS (Deep Reinforcement Agent for Scheduling) by leveraging deep reinforcement learning. DRAS is built on a hierarchical neural network incorporating special HPC scheduling features such as resource reservation and backfilling. An efficient training strategy is presented to enable DRAS to rapidly learn the target environment. Once being provided a specific scheduling objective given by the system manager, DRAS automatically learns to improve its policy through interaction with the scheduling environment and dynamically adjusts its policy as workload changes. We implement DRAS into a HPC scheduling platform called CQGym. CQGym provides a common platform allowing users to flexibly evaluate DRAS and other scheduling methods such as heuristic and optimization methods. The experiments using CQGym with different production workloads demonstrate that DRAS outperforms the existing heuristic and optimization approaches by up to 50%.","1558-2183","","10.1109/TPDS.2022.3205325","National Science Foundation(grant numbers:CNS-1717763,CCF-2109316,CCF-2119294); U.S. Department of Energy(grant numbers:DE-AC02-06CH11357,DE-AC02-05CH11231); National Energy Research Scientific Computing Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894371","High-performance computing;cluster scheduling;deep reinforcement learning;job starvation;backfilling;resource reservation;OpenAI Gym","Processor scheduling;Dynamic scheduling;Runtime;Neural networks;Training;Q-learning;Production","deep learning (artificial intelligence);optimisation;parallel processing;processor scheduling;reinforcement learning","DRAS;deep reinforcement learning;high performance computing;cluster scheduling heuristics;highly dynamic nature;automated HPC scheduling agent;system manager;heuristic optimization approaches;deep reinforcement agent for scheduling;CQGym","","2","","51","IEEE","16 Sep 2022","","","IEEE","IEEE Journals"
"Adaptive Incentivize for Cross-Silo Federated Learning in IIoT: A Multi-agent Reinforcement Learning Approach","S. Yuan; B. Dong; H. Lvy; H. Liu; H. Chen; C. Wu; S. Guo; Y. Ding; J. Li","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; NA; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Zhejiang Lab, Hangzhou, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, Hong Kong; School of Software, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Internet of Things Journal","","2023","PP","99","1","1","In the Industrial Internet of Things (IIoT), Cross-Silo Federated Learning (CSFL) enables entities such as manufacturers and suppliers to train global models for optimizing production processes while ensuring data privacy. A well-designed incentive mechanism is essential to persuade clients to contribute data resources. However, existing methodologies overlook the dynamic nature of the training process, where the accuracy of the globally trained model and the clients data ownership change over time. Furthermore, the majority of previous research assumes a defined functional relationship between the data contribution and the model accuracy, which is infeasible in realistic and dynamic training environments. To address these challenges, we design a novel adaptive mechanism for CSFL that inspires organizations to contribute data resources in a dynamic training environment with the aim of maximizing their long-term payoffs. This mechanism leverages multi-agent reinforcement learning (MARL) to ascertain near-optimal data contribution strategies from potential game histories without necessitating private organizational information or a precise accuracy function. Experimental results indicate that our mechanism achieves adaptive incentive in dynamic environments and effectively enhances the long-term payoffs of organizations.","2327-4662","","10.1109/JIOT.2023.3315770","Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101400003); National Key Research and Development Program of China(grant numbers:2020YFB1806700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10258027","IIoT;Cross-Silo FL;Incentivization;Multi-agent Reinforcement Learning;Potential Game","Organizations;Training;Data models;Games;Reinforcement learning;Industrial Internet of Things;Servers","","","","","","","IEEE","20 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Recognition of important Korean text structure based on Reinforcement Learning and Self-attention mechanism","F. Yang; Y. Zhao; X. Cui; R. Cui","Intelligent Information Processing Lab., Dept. of Computer Science & Technology, Yanbian University, Yanji, China; Intelligent Information Processing Lab., Dept. of Computer Science & Technology, Yanbian University, Yanji, China; Intelligent Information Processing Lab., Dept. of Computer Science & Technology, Yanbian University, Yanji, China; Intelligent Information Processing Lab., Dept. of Computer Science & Technology, Yanbian University, Yanji, China","2020 2nd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM)","11 May 2021","2020","","","1","7","Aiming at the problem that the artificial tagging of Korean corpus is too time-consuming and laborious, and it is difficult for minority languages to integrate with various resources. We intend to construct an effective Korean structural representation from the perspective of representation learning to improve the effectiveness of subsequent natural language processing tasks. Combining deep reinforcement learning with Self-attention mechanism, we propose a hierarchical self-attention model (Hierarchically Structured Korean, HS-K). By using the Actor-Critic idea in reinforcement learning, the model takes the text classification effect as the label feedback of reinforcement learning, and transforms the text structure division task into the sequence decision task. The experimental results show that the model can identify the important text structure of Korean which closer the manual tagging, and it has a friendly auxiliary effect on Korean informatization and intelligentialize.","","978-1-7281-9986-3","10.1109/AIAM50918.2020.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425986","Korean Natural Language Processing;Deep Reinforcement Learning;Self-Attention mechanism;Text structured","Training;Text recognition;Text categorization;Reinforcement learning;Manuals;Transforms;Tagging","learning (artificial intelligence);natural language processing;natural languages;text analysis","Korean corpus;minority languages;effective Korean structural representation;subsequent natural language processing tasks;deep reinforcement learning;Self-attention mechanism;hierarchical self-attention model;Hierarchically Structured Korean;text classification effect;text structure division task;important text structure;Korean which closer the manual tagging;friendly auxiliary effect;intelligentialize;important Korean text structure;artificial tagging","","","","26","IEEE","11 May 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-based Optimal and Fast Hybrid Equalizer Design Method for High Bandwidth Memory (HBM) Module","S. Choi; K. Son; H. Park; S. Kim; B. Sim; J. Kim; J. Park; M. Kim; H. Kim; J. Song; Y. Kim; J. Kim","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Electrical and Computer Engineering, Missouri University of Science and Technology (MST), Rolla, MO, USA; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Samsung electronics, Hwaseong, South Korea; Department of semiconductor systems engineering, Sejong University, Seoul, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Components, Packaging and Manufacturing Technology","","2023","PP","99","1","1","In this article, we propose a deep reinforcement learning (DRL)-based optimal and fast hybrid equalizer (HYEQ) design method for high bandwidth memory (HBM) module. The HYEQ is a promising key to improving the signal integrity (SI) performance along the broad frequency bands by combining a passive equalizer (PEQ) and an active equalizer (AEQ) for the HBM module. However, the equalizer design process is extremely complex because all of the parameters should be co-optimized by considering the SI characteristics of the target channel. To tackle the design complexity, we utilize the DRL method that trains the policy network to optimize the HYEQ design for maximizing an eye-opening value. The policy network is configured with a recurrent type of neural network, and sequentially determines the optimal values of the HYEQ parameters by considering the relevance between the parameters. Furthermore, the training process for the policy network is implemented in diverse channel dimensions of the silicon interposer. By learning the feature between channel dimensions and equalizer parameters, the policy network directly designs the optimal HYEQ for arbitrary channel dimensions. For verification, the proposed method is compared with the random search (RS) and genetic algorithm (GA) in terms of optimality performance and computational time. The result shows that the proposed method outperforms both RS and GA.","2156-3985","","10.1109/TCPMT.2023.3317295","National Research Foundation of Korea (NRF)(grant numbers:NRF-2022M3I7A4072293); Samsung Electronics Co., Ltd(grant numbers:IO201207-07813-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255709","Deep reinforcement learning (DRL);high bandwidth memory (HBM);hybrid equalizer;signal integrity (SI)","Equalizers;Silicon;Genetic algorithms;Design methodology;Decoding;Reinforcement learning;Packaging","","","","","","","IEEE","19 Sep 2023","","","IEEE","IEEE Early Access Articles"
"QMIX Multiple Intelligences Reinforcement Learning Damping Control for Cylindrical Shell","Y. Song; X. Kai; S. X. Hua; Z. Gang","Faculty of Maritime and Transportation, Ningbo University, Ningbo, China; Faculty of Maritime and Transportation, Ningbo University, Ningbo, China; Faculty of Maritime and Transportation, Ningbo University, Ningbo, China; Faculty of Maritime and Transportation, Ningbo University, Ningbo, China","2022 5th World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM)","27 Jan 2023","2022","","","980","989","One of the fundamental mechanical constructions of ships and navigators is the cylindrical shell structure. Their damping control is difficult to predict and frequently depends on precise control models. For that reason, this work provides a data-driven multi-intelligence reinforcement learning damping control approach that is significance for damping control of massive structures. Firstly, the dynamics equations of cylindrical shell structure are established based on the hypothetical modal method, and modal variables are introduced to derive the state-space equations for damping control of cylindrical shell structure, and an interactive environment for multi-intelligent reinforcement learning is established. Secondly, the damping control strategy of cylindrical shell structure with multiple intelligences is designed based on the value decomposition QMIX algorithm. For a single smart body design vibration displacement, velocity, piezoelectric actuator voltage, smart body operation steps as the state space, quadratic performance indicators with saturation characteristics as the damping effect reward function, greedy strategy as damping action selection method for multi-intelligent body cooperative damping. The QMIX algorithm hybrid network performs fusion evaluation of the joint action value of each intelligence and updates the action value function of a single intelligence. Finally, five sets of hyperparameters are set based on the Grid Search approach for comparative simulation experiments for deep learning network hyperparameter selection. The result of the simulation demonstrate that the current tactic effectively suppresses the vibration of the cylindrical shell construction. Furthermore, the optimal hyperparameter is determined by comparing simulation trials with different values, proving that the approach described in this article has better damping performance under this parameter.","","978-1-6654-7369-9","10.1109/WCMEIM56910.2022.10021369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021369","Cylindrical shell structure;Vibration Control;Intensive Learning;Multi-Intelligent Body","Damping;Vibrations;Navigation;Reinforcement learning;Piezoelectric actuators;Aerospace electronics;Predictive models","control engineering computing;damping;deep learning (artificial intelligence);design engineering;greedy algorithms;piezoelectric actuators;reinforcement learning;search problems;shells (structures);ships;state-space methods;structural engineering computing;vibration control;vibrations","action value function;cylindrical shell construction;cylindrical shell structure;damping action selection;damping effect reward function;damping performance;data-driven multiintelligence reinforcement learning damping control;deep learning network hyperparameter selection;dynamics equations;fusion evaluation;greedy strategy;grid search approach;hypothetical modal method;mechanical constructions;modal variables;multiintelligent body cooperative damping;multiintelligent reinforcement learning;piezoelectric actuator voltage;QMIX algorithm hybrid network;QMIX multiple intelligence reinforcement learning damping control;quadratic performance indicators;saturation characteristics;ships;single smart body design;smart body operation steps;state-space equations;value decomposition QMIX algorithm;velocity;vibration displacement","","","","18","IEEE","27 Jan 2023","","","IEEE","IEEE Conferences"
"Green Simulation Assisted Reinforcement Learning With Model Risk for Biomanufacturing Learning and Control","H. Zheng; W. Xie; M. B. Feng","Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA, USA; Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA, USA; Department of Statistics and Actuarial Science, University of Waterloo, Waterloo, CANADA","2020 Winter Simulation Conference (WSC)","29 Mar 2021","2020","","","337","348","Biopharmaceutical manufacturing faces critical challenges, including complexity, high variability, lengthy lead time, and limited historical data and knowledge of the underlying system stochastic process. To address these challenges, we propose a green simulation assisted model-based reinforcement learning to support process online learning and guide dynamic decision making. Basically, the process model risk is quantified by the posterior distribution. At any given policy, we predict the expected system response with prediction risk accounting for both inherent stochastic uncertainty and model risk. Then, we propose green simulation assisted reinforcement learning and derive the mixture proposal distribution of decision process and likelihood ratio based metamodel for the policy gradient, which can selectively reuse process trajectory outputs collected from previous experiments to increase the simulation data-efficiency, improve the policy gradient estimation accuracy, and speed up the search for the optimal policy. Our numerical study indicates that the proposed approach demonstrates the promising performance.","1558-4305","978-1-7281-9499-8","10.1109/WSC48552.2020.9384107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384107","","Biological system modeling;Decision making;Stochastic processes;Estimation;Reinforcement learning;Numerical models;Trajectory","decision making;gradient methods;learning (artificial intelligence);Markov processes;stochastic processes","lengthy lead time;biomanufacturing learning;simulation data-efficiency;process trajectory;mixture proposal distribution;green simulation assisted reinforcement learning;prediction risk accounting;process model risk;green simulation assisted model-based reinforcement;underlying system stochastic process;limited historical data","","","","11","IEEE","29 Mar 2021","","","IEEE","IEEE Conferences"
"Learning a Diagnostic Strategy on Medical Data With Deep Reinforcement Learning","M. Zhu; H. Zhu","State Key Laboratory of Software Development Environment, Beihang University, Beijing, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beijing, China","IEEE Access","14 Jun 2021","2021","9","","84122","84133","In recent years, Artificial Intelligence based disease diagnosis has drawn considerable attention both in academia and industry. In medical scenarios, a well-trained classifier can effectively detect a disease with sufficient features associating with medical tests. However, such features are not always readily available due to the high cost of time and money associating with medical tests. To address this, this study identifies the diagnostic strategy learning problem and proposes a novel framework consisting of three components to learn a diagnostic strategy with limited features. First, as we often encounter incomplete medical records of the patients, a sequence encoder is designed to encode any set of information in various sizes into fixed-length vectors. Second, taking the output of the encoder as the input, a feature selector based on reinforcement learning techniques is proposed to learn the best feature sequence for diagnosis. Finally, with the best feature sequence, an oracle classifier is used to give the final diagnosis. To evaluate the performance of the proposed method, experiments are conducted on nine real medical datasets. The results suggest that the proposed method is effective for providing personalized diagnostic strategies and makes better diagnoses with fewer features compared with existing methods.","2169-3536","","10.1109/ACCESS.2021.3087493","National Natural Science Foundation(grant numbers:61702027); Beijing Municipal Science and Technology Commission(grant numbers:Z171100000117022,KZ70001302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448279","Diagnostic strategy;feature acquisition;reinforcement learning;medical data","Medical diagnostic imaging;Medical tests;Reinforcement learning;Feature extraction;Diseases;Reliability","deep learning (artificial intelligence);diseases;feature extraction;medical diagnostic computing;patient diagnosis;pattern classification","medical data;deep reinforcement learning;medical scenarios;well-trained classifier;medical tests;diagnostic strategy learning problem;medical records;sequence encoder;fixed-length vectors;feature selector;reinforcement learning techniques;feature sequence;oracle classifier;medical datasets;personalized diagnostic strategies;artificial intelligence based disease diagnosis","","5","","24","CCBY","8 Jun 2021","","","IEEE","IEEE Journals"
"Comparison of Deep Reinforcement Learning Techniques with Gradient based approach in Cooperative Control of Wind Farm","K. N. Pujari; V. Srivastava; S. S. Miriyala; K. Mitra","Department of Chemical Engineering, Indian Institute of Technology, Hyderabad, Telangana, INDIA; Department of Electrical Engineering, Indian Institute of Technology, Hyderabad, Telangana, INDIA; Department of Chemical Engineering, Indian Institute of Technology, Hyderabad, Telangana, INDIA; Department of Chemical Engineering, Indian Institute of Technology, Hyderabad, Telangana, INDIA","2021 Seventh Indian Control Conference (ICC)","14 Feb 2022","2021","","","400","405","The control settings of a turbines play a major role in increasing the energy production from a wind farm. The nonlinear interactions of wake between the turbines make optimal control of wind farm a challenging task. Therefore, it's hard to find the proper model based method to optimize the control settings. In the recent years, Reinforcement Learning (RL) has been emerging as a promising method for wind farm control. However, its efficacy is not evaluated when compared with nonlinear control strategies. In this study, yaw misalignment is used as control parameter to deflect the wakes and increase the power production from a 4×4 wind farm. A model-free Deep Deterministic Policy Gradient (DDPG) method and model-based iterative Linear Quadratic Regulator (iLQR) based Reinforcement Learning Techniques are utilized to optimize the yaw misalignments. To prove the efficiency of RL techniques, the results of DDPG and iLQR are compared with a nonlinear cooperative control strategy, Maximum Power Point Tracking solved through gradient based optimization approach.","","978-1-6654-0978-0","10.1109/ICC54714.2021.9703186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9703186","","Maximum power point trackers;Regulators;Optimal control;Production;Reinforcement learning;Wind farms;Task analysis","control system synthesis;deep learning (artificial intelligence);gradient methods;iterative methods;linear quadratic control;nonlinear control systems;optimal control;power system control;reinforcement learning;wind power plants;wind turbines","deep reinforcement learning techniques;gradient based approach;optimal control;wind farm control;nonlinear control strategies;control parameter;gradient based optimization approach;model-free deep deterministic policy gradient method;model-based iterative linear quadratic regulator;DDPG;iLQR;nonlinear cooperative control strategy","","","","14","IEEE","14 Feb 2022","","","IEEE","IEEE Conferences"
"Noah: Reinforcement-Learning-Based Rate Limiter for Microservices in Large-Scale E-Commerce Services","Z. Li; H. Sun; Z. Xiong; Q. Huang; Z. Hu; D. Li; S. Ruan; H. Hong; J. Gui; J. He; Z. Xu; Y. Fang","Alibaba Group, Hangzhou, China; Department of Computer and Science, Peking University, Beijing, China; Alibaba Group, Hangzhou, China; Department of Computer and Science, Peking University, Beijing, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; Department of Computer and Science, Peking University, Beijing, China; Department of Computer and Science, Peking University, Beijing, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China","IEEE Transactions on Neural Networks and Learning Systems","1 Sep 2023","2023","34","9","5403","5417","Modern large-scale online service providers typically deploy microservices into containers to achieve flexible service management. One critical problem in such container-based microservice architectures is to control the arrival rate of requests in the containers to avoid containers from being overloaded. In this article, we present our experience of rate limit for the containers in Alibaba, one of the largest e-commerce services in the world. Given the highly diverse characteristics of containers in Alibaba, we point out that the existing rate limit mechanisms cannot meet our demand. Thus, we design Noah, a dynamic rate limiter that can automatically adapt to the specific characteristic of each container without human efforts. The key idea of Noah is to use deep reinforcement learning (DRL) that automatically infers the most suitable configuration for each container. To fully embrace the advantages of DRL in our context, Noah addresses two technical challenges. First, Noah uses a lightweight system monitoring mechanism to collect container status. In this way, it minimizes the monitoring overhead while ensuring a timely reaction to system load changes. Second, Noah injects synthetic extreme data when training its models. Thus, its model gains knowledge on unseen special events and hence remains highly available in extreme scenarios. To guarantee model convergence with the injected training data, Noah adopts task-specific curriculum learning to train the model from normal data to extreme data gradually. Noah has been deployed in the production of Alibaba for two years, serving more than 50000 containers and around 300 types of microservice applications. Experimental results show that Noah can well adapt to three common scenarios in the production environment. It effectively achieves better system availability and shorter request response time compared with four state-of-the-art rate limiters.","2162-2388","","10.1109/TNNLS.2023.3264038","National Key Research and Development Program of China(grant numbers:2019YFB1802600); Joint Funds of the National Natural Science Foundation of China(grant numbers:U20A20179); National Natural Science Foundation of China(grant numbers:62172007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10098822","Deep reinforcement learning (DRL);deployment experience;e-commerce;microservice;rate limit","Containers;Microservice architectures;Production;Electronic commerce;Monitoring;Measurement;Training","cloud computing;deep learning (artificial intelligence);electronic commerce;learning (artificial intelligence);reinforcement learning;service-oriented architecture;telecommunication computing","Alibaba;container status;container-based microservice architectures;dynamic rate limiter;existing rate limit mechanisms;flexible service management;large-scale e-commerce services;largest e-commerce services;microservices;modern large-scale online service providers;Noah;reinforcement-learning-based rate limiter;state-of-the-art rate limiters","","","","69","IEEE","11 Apr 2023","","","IEEE","IEEE Journals"
"A Generic Markov Decision Process Model and Reinforcement Learning Method for Scheduling Agile Earth Observation Satellites","Y. He; L. Xing; Y. Chen; W. Pedrycz; L. Wang; G. Wu","College of Systems Engineering, National University of Defense Technology, Changsha, China; College of Systems Engineering, National University of Defense Technology, Changsha, China; College of Systems Engineering, National University of Defense Technology, Changsha, China; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Canada; Department of Automation, Tsinghua University, Beijing, China; School of Traffic and Transportation Engineering, Central South University, Changsha, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","16 Feb 2022","2022","52","3","1463","1474","We investigate a general solution based on reinforcement learning for the agile satellite scheduling problem. The core idea of this method is to determine a value function for evaluating the long-term benefit under a certain state by training from experiences, and then apply this value function to guide decisions in unknown situations. First, the process of agile satellite scheduling is modeled as a finite Markov decision process with continuous state space and discrete action space. Two subproblems of the agile Earth observation satellite scheduling problem, i.e., the sequencing problem and the timing problem are solved by the part of the agent and the environment in the model, respectively. A satisfactory solution of the timing problem can be quickly produced by a constructive heuristic algorithm. The objective function of this problem is to maximize the total reward of the entire scheduling process. Based on the above design, we demonstrate that Q-network has advantages in fitting the long-term benefit of such problems. After that, we train the Q-network by Q-learning. The experimental results show that the trained Q-network performs efficiently to cope with unknown data, and can generate high total profit in a short time. The method has good scalability and can be applied to different types of satellite scheduling problems by customizing only the constraints checking process and reward signals.","2168-2232","","10.1109/TSMC.2020.3020732","National Natural Science Foundation of China(grant numbers:71701203,61873328); State Key Laboratory of Digital Manufacturing Equipment and Technology(grant numbers:DMETKF2020030); China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200466","Agile Earth observation satellite (AEOS);Markov decision process (MDP);Q-learning;reinforcement learning (RL);task scheduling","Satellites;Job shop scheduling;Heuristic algorithms;Task analysis;Linear programming;Decision making","artificial satellites;Markov processes;optimisation;reinforcement learning;scheduling","Q-learning;trained Q-network performs;satellite scheduling problems;generic Markov decision process model;reinforcement learning method;agile satellite scheduling problem;value function;finite Markov decision process;continuous state space;discrete action space;agile Earth observation satellite scheduling problem","","27","","47","IEEE","18 Sep 2020","","","IEEE","IEEE Journals"
"Analyzing Different Unstated Goal Constraints on Reinforcement Learning Algorithm for Reacher Task in the Robotic Scrub Nurse Application","C. E. Gandana; J. D. K. Disu; H. Xie; L. Gu","Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China; Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China; Department of Cardiology, Peking Union Medical College Hospital, Peking, China; Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China","2020 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)","20 Aug 2020","2020","","","42","47","The main objective paper is to make an empirical analysis of the effect of various unstated spatial goal constraints on reinforcement learning policy for the “reacher” task in the Robotic Scrub Nurse (RSN) application. This “reacher” task is an essential part of the RSN manipulation task, such as the task of picking, grasping, or placing the surgical instruments. This paper provides our experimental results and the evaluation of the “reacher” task under different spatial goal constraints. We researched the effect of this unstated assumption on a reinforcement learning (RL) algorithm: Soft-Actor Critic with Hindsight Experience Replay (SAC+HER). We used the 7-DoF robotic arm to evaluate this state-of-the-art deep RL algorithm. We performed our experiments in a virtual environment while training the robotic arm to reach the random target points. The implementation of this RL algorithm showed a robust performance, which is measured by reward values and success rates. We observed, these reinforcement learning assumptions, particularly the unstated spatial goal constraints, can affect the performance of the RL agent. The important aspect of the “reacher” task and the development of reinforcement learning applications in medical robotics is one of the main motivations behind this research objective.","","978-1-7281-9336-6","10.1109/IAICT50021.2020.9172009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172009","“reacher” task;spatial constraints;Robotic Scrub Nurse;Reinforcement Learning;Soft-Actor Critic;Hindsight Experiment Replay","","human-robot interaction;learning (artificial intelligence);manipulators;medical robotics;robots;surgery","reacher task;reinforcement learning applications;different unstated goal constraints;reinforcement learning algorithm;Robotic Scrub Nurse application;unstated spatial goal constraints;reinforcement learning policy;RSN manipulation task;reinforcement learning assumptions;state-of-the-art deep RL algorithm;robotic arm;different spatial goal constraints","","1","","30","IEEE","20 Aug 2020","","","IEEE","IEEE Conferences"
"Deep-Reinforcement-Learning-Based Collision Avoidance in UAV Environment","S. Ouahouah; M. Bagaa; J. Prados-Garzon; T. Taleb","Department of Communications and Networking, School of Electrical Engineering, Aalto University, Espoo, Finland; Department of Communications and Networking, School of Electrical Engineering, Aalto University, Espoo, Finland; Department of Signal Theory, Telematics and Communications, University of Granada, Granada, Spain; Department of Communications and Networking, School of Electrical Engineering, Aalto University, Espoo, Finland","IEEE Internet of Things Journal","8 Mar 2022","2022","9","6","4015","4030","Unmanned aerial vehicles (UAVs) have recently attracted both academia and industry representatives due to their utilization in tremendous emerging applications. Most UAV applications adopt visual line of sight (VLOS) due to ongoing regulations. There is a consensus between industry for extending UAVs’ commercial operations to cover the urban and populated area-controlled airspace beyond VLOS (BVLOS). There is ongoing regulation for enabling BVLOS UAV management. Regrettably, this comes with unavoidable challenges related to UAVs’ autonomy for detecting and avoiding static and mobile objects. An intelligent component should either be deployed onboard the UAV or at a multiaccess-edge computing (MEC) that can read the gathered data from different UAV’s sensors, process them, and then make the right decision to detect and avoid the physical collision. The sensing data should be collected using various sensors but not limited to Lidar, depth camera, video, or ultrasonic. This article proposes probabilistic and deep-reinforcement-learning (DRL)-based algorithms for avoiding collisions while saving energy consumption. The proposed algorithms can be either run on top of the UAV or at the MEC according to the UAV capacity and the task overhead. We have designed and developed our algorithms to work for any environment without a need for any prior knowledge. The proposed solutions have been evaluated in a harsh environment that consists of many UAVs moving randomly in a small area without any correlation. The obtained results demonstrated the efficiency of these solutions for avoiding the collision while saving energy consumption in familiar and unfamiliar environments.","2327-4662","","10.1109/JIOT.2021.3118949","Spanish National Project TRUE-5G(grant numbers:PID2019-108713RB-C53); European Union’s Horizon 2020 Research and Innovation Program through the 5G!Drones Project(grant numbers:857031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564258","Collision avoidance;deep reinforcement learning;machine learning;multiaccess-edge computing (MEC);unmanned aerial vehicles (UAVs)","Sensors;Unmanned aerial vehicles;Collision avoidance;Reinforcement learning;Vehicular ad hoc networks;Regulation;Industries","autonomous aerial vehicles;collision avoidance;deep learning (artificial intelligence);reinforcement learning","urban area-controlled airspace;populated area-controlled airspace;BVLOS UAV management;UAV autonomy;static objects;mobile objects;UAV sensors;UAV capacity;deep-reinforcement-learning-based collision avoidance;UAV environment;unmanned aerial vehicles;emerging applications;UAV commercial operations","","13","","35","IEEE","8 Oct 2021","","","IEEE","IEEE Journals"
"RLOps: Development Life-Cycle of Reinforcement Learning Aided Open RAN","P. Li; J. Thomas; X. Wang; A. Khalil; A. Ahmad; R. Inacio; S. Kapoor; A. Parekh; A. Doufexi; A. Shojaeifard; R. J. Piechocki","Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; Vilicom U.K. Ltd., Reading, U.K; Vilicom U.K. Ltd., Reading, U.K; Applied Research, Suffolk, U.K; Applied Research, Suffolk, U.K; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K; InterDigital Communications Inc., Wilmington, DE, USA; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K","IEEE Access","7 Nov 2022","2022","10","","113808","113826","Radio access network (RAN) technologies continue to evolve, with Open RAN gaining the most recent momentum. In the O-RAN specifications, the RAN intelligent controllers (RICs) are software-defined orchestration and automation functions for the intelligent management of RAN. This article introduces principles for machine learning (ML), in particular, reinforcement learning (RL) applications in the O-RAN stack. Furthermore, we review the state-of-the-art research in wireless networks and cast it onto the RAN framework and the hierarchy of the O-RAN architecture. We provide a taxonomy for the challenges faced by ML/RL models throughout the development life-cycle: from the system specification to production deployment (data acquisition, model design, testing and management, etc.). To address the challenges, we integrate a set of existing MLOps principles with unique characteristics when RL agents are considered. This paper discusses a systematic model development, testing and validation life-cycle, termed: RLOps. We discuss fundamental parts of RLOps, which include: model specification, development, production environment serving, operations monitoring and safety/security. Based on these principles, we propose the best practices for RLOps to achieve an automated and reproducible model development process. At last, a holistic data analytics platform rooted in the O-RAN deployment is designed and implemented, aiming to embrace and fulfil the aforementioned principles and best practices of RLOps.","2169-3536","","10.1109/ACCESS.2022.3217511","Innovate UK/CELTIC-NEXT European collaborative project on AI-enabled Massive MIMO (AIMM); Next-Generation Converged Digital Infrastructure (NG-CDI) Project; BT and Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/R004935/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931127","O-RAN;machine learning;reinforcement learning;MLOps;RLOps;digital twins;data engineering","Radio access networks;Computer architecture;Reinforcement learning;Task analysis;Adaptation models;3GPP;Biological system modeling","data acquisition;data analysis;intelligent control;radio access networks;reinforcement learning;software defined networking;software radio","RLOps;development life-cycle;Open RAN;radio access network technologies;O-RAN specifications;RAN intelligent controllers;software-defined orchestration;automation functions;machine learning;reinforcement learning applications;O-RAN stack;state-of-the-art research;wireless networks;RAN framework;O-RAN architecture;system specification;model design;existing MLOps principles;RL agents;systematic model development;validation life-cycle;model specification;automated model development process;reproducible model development process;O-RAN deployment","","2","","96","CCBY","26 Oct 2022","","","IEEE","IEEE Journals"
"A reinforcement learning algorithm based technique for thermal energy management of a PEM fuel cell power plant","S. Chowdhury; M. Y. El-sharkh","Dept. of Electrical and Computer Engineering, University of South Alabama, Mobile, USA; Dept. of Electrical and Computer Engineering, University of South Alabama, Mobile, USA","2015 IEEE Industry Applications Society Annual Meeting","17 Dec 2015","2015","","","1","8","In this paper, a reinforcement learning dynamic programming algorithm (RLDP) has been developed to manage the cogenerated thermal energy and the operation of a PEM fuel cell power plant. The solution methodology is based on selecting the optimal operational schedule of a fuel cell to minimize the total expected cost of generation incurred during a specified schedule period. The optimal operation schedule of the PEM fuel cell power plant is based on estimating the hourly generated electrical power, electrical energy trade with a local utility, and the recovered thermal energy utilization and storage based on electrical and thermal demand. The proposed technique is based on the Q-learning algorithm. At the training stage, the reinforcement learning (RL) is scheduled to explore and learn the Q values using the epsilon greedy policy, and after that, decision making is done using the learned Q values. The proposed algorithm is tested using a 24-hour based electrical and thermal load, the obtained results indicate the viability of the proposed approach.","","978-1-4799-8394-0","10.1109/IAS.2015.7356834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7356834","Reinforcement learning;proton exchange membrane fuel cell;Fuel cell cost-optimized approach;Dynamic Programming;Energy management","Dynamic programming;Protons;Learning systems;Energy management;Fuel cells","energy management systems;fuel cell power plants;learning (artificial intelligence);power engineering computing;power generation economics;power generation scheduling;power markets;proton exchange membrane fuel cells;thermal energy storage","training stage;Q-learning algorithm;electrical demand;thermal demand;recovered thermal energy storage;recovered thermal energy utilization;local utility;electrical energy trade;optimal operation scheduling;generation cost minimization;reinforcement learning dynamic programming algorithm;thermal energy management;PEM fuel cell power plant","","1","","12","IEEE","17 Dec 2015","","","IEEE","IEEE Conferences"
"Autonomous Load Carrier Approaching Based on Deep Reinforcement Learning with Compressed Visual Information","S. Hadwiger; T. Meisen","Siemens AG, University of Wuppertal, Nuremberg, Germany; Chair of Technologies and Management of Digital Transformation, University of Wuppertal, Wuppertal, Germany","2022 5th International Conference on Artificial Intelligence for Industries (AI4I)","15 May 2023","2022","","","48","53","In intralogistics, a large number of tasks are already fully automated. This holds true especially for tasks where strictly predefined positions and paths are specified and implementable. Challenges still exist when this is not the case and a more dynamic environment is present. One example of such a dynamic environment is the approaching and lifting of freely positioned pallet-like carriers with forklifts. In this work, we propose a method for approaching and picking up pallet-like carriers with a forklift based on data from an RGB camera. Unlike previous work, our method does not require an estimation of the pose of the load carrier. In order to control the forklift, we use a soft actor critical reinforcement learning agent. The required input consists of the bounding box of the load carrier in combination with the current speed and steering of the forklift. Our simulation experiments show that this compressed visual information is sufficient to successfully approach load carriers while reducing training time and network size. In a next step, we are going to apply the presented result on a real-world scenario and investigate its transferability.","2770-4718","978-1-6654-5961-7","10.1109/AI4I54798.2022.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10123543","Automated Guided Vehicles;Forklift;Pallets;Deep Reinforcement Learning","Training;Visualization;Pose estimation;Neural networks;Reinforcement learning;Detectors;Learning (artificial intelligence)","automatic guided vehicles;data compression;deep learning (artificial intelligence);fork lift trucks;image colour analysis;industrial robots;lifting;loading equipment;path planning;position control;reinforcement learning;robot vision","AGV;autonomous load carrier;bounding box;compressed visual information;deep reinforcement learning;dynamic environment;forklift control;forklift steering;freely positioned pallet-like carriers;fully automated tasks;intralogistics;RGB camera;soft actor critical reinforcement learning agent;strictly predefined positions","","1","","25","IEEE","15 May 2023","","","IEEE","IEEE Conferences"
"Signal Synchronization of Traffic Lights Using Reinforcement Learning","İ. Aydin; M. Sevi; G. Güngören; H. C. İrez","Department of Computer Engineering, Fırat University, Elazığ, Turkey; Information Technologies Department, Muş Alparslan University, Muş, Turkey; Department of Computer Engineering, Fırat University, Elazığ, Turkey; Department of Computer Engineering, Fırat University, Elazığ, Turkey","2022 International Conference on Data Analytics for Business and Industry (ICDABI)","14 Feb 2023","2022","","","103","108","Today, the traffic problem is a serious problem, especially in big cities. The increasing number of cars with the increasing population further increases the traffic problem. This traffic problem increases travel times, increases fuel consumption, causes many accidents, and negatively affects human psychology. One of the reasons that increase traffic on the roads the most is traffic lights. Since traffic lights are used at most intersections, large traffic occurs at intersections. The signal periods of most traffic lights are predetermined using data from the intersection. However, these traffic lights are not adaptive to different situations that may occur on the road. In the study, we tried to make non-adaptive traffic lights adaptive using deep reinforcement learning. In the study, the signal periods of traffic lights were managed by using a reinforcement learning agent trained on simulation. When the training performances of the Reinforcement learning agent suggested in the study were examined, it was seen that the training was successful. It has been observed that there is a decrease in queue length and average delays experienced by vehicles at the intersection. It has been observed that the reinforcement learning algorithm can work well at intersections.","","978-1-6654-9058-0","10.1109/ICDABI56818.2022.10041559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10041559","reinforcement learning;traffic lights;signal control of traffic lights;deep q learning","Training;Deep learning;Roads;Urban areas;Sociology;Reinforcement learning;Planning","automobiles;deep learning (artificial intelligence);multi-agent systems;psychology;queueing theory;reinforcement learning;road traffic;traffic engineering computing","average delays;cars;deep reinforcement learning;human psychology;nonadaptive traffic lights;queue length;reinforcement learning agent;signal synchronization;traffic problem","","","","26","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"High-Frequency Limitation Deep Reinforcement Learning Based Energy Management Strategy for Dual Fuel Cell Electric Aircraft","W. Shi; Y. Huangfu; S. Pang; L. Xu; C. Yuan; S. Zhuo","Department of electrical engineering, Northwestern Polytechnical University, Xi'an, China; Department of electrical engineering, Northwestern Polytechnical University, Xi'an, China; Unmanned System Research Institute, Northwestern Polytechnical University, Xi'an, China; Department of electrical engineering, Northwestern Polytechnical University, Xi'an, China; Department of electrical engineering, Northwestern Polytechnical University, Xi'an, China; Department of electrical engineering, Northwestern Polytechnical University, Xi'an, China","2022 IEEE Industry Applications Society Annual Meeting (IAS)","17 Nov 2022","2022","","","1","6","As an important part of hybrid power source system, energy management strategy (EMS) can be modeled by Markov decision processes and solved by reinforcement learning. A high-frequency limitation deep deterministic policy gradient (DDPG) EMS for electric aircraft is proposed, which consists of two parallel proton exchange membrane fuel cells (PEMFCs) and one battery. DDPG is a classic reinforcement learning algorithm with the advantage of making decisions in a continuous action space. However, its actions have been unsatisfactory due to volatility. By adding high-frequency limitation, high-frequency limitation DDPG can make decisions that limit the output power of the PEMFC when the high-frequency scale is large. To validate the proposed EMS, it is compared with the conventional DDPG EMS, and the simulation results show that the proposed EMS significantly reduces the PEMFC fluctuation compared to the conventional DDPG. Under the same environment, the equivalent hydrogen consumption is 8g lower than that of DDPG, and standard deviations of PEMFC1's stress and PEMFC2's stress are reduced by 68.03% and 55.17%, respectively. In addition, its generalization ability is also verified by adjusting the initial state of charge and extending the operating conditions.","2576-702X","978-1-6654-7815-1","10.1109/IAS54023.2022.9939937","Shaanxi Natural Science basic research plan enterprise-SHCCIG(grant numbers:S2021-JC-LHJJXM-LH-QY-SM-0119); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9939937","Energy management strategy;High-Frequency Limitation;Deep Deterministic Policy Gradient;Dual Fuel Cell","Fluctuations;Simulation;Hydrogen;Fuel cells;Reinforcement learning;Batteries;State of charge","energy management systems;fuel cell vehicles;learning (artificial intelligence);Markov processes;proton exchange membrane fuel cells","classic reinforcement learning algorithm;continuous action space;conventional DDPG EMS;dual fuel cell electric aircraft;energy management strategy;high-frequency limitation DDPG;high-frequency limitation deep deterministic policy gradient EMS;high-frequency limitation deep reinforcement;high-frequency scale;hybrid power source system;Markov decision processes;mass 8.0 g;parallel proton exchange membrane fuel cells;PEMFC fluctuation;PEMFC1's stress;PEMFC2's stress","","","","15","IEEE","17 Nov 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Guidewire Navigation in Coronary Artery Phantom","J. Kweon; K. Kim; C. Lee; H. Kwon; J. Park; K. Song; Y. I. Kim; J. Park; I. Back; J. -H. Roh; Y. Moon; J. Choi; Y. -H. Kim","Departments of Convergence Medicine, Asan Medical Center, University of Ulsan College of Medicine, Seoul, South Korea; Medipixel, Inc., Seoul, South Korea; Medipixel, Inc., Seoul, South Korea; Medipixel, Inc., Seoul, South Korea; Medipixel, Inc., Seoul, South Korea; Medipixel, Inc., Seoul, South Korea; Department of Medical Science, Asan Medical Institute of Convergence Science and Technology, Asan Medical Center, University of Ulsan College of Medicine, Seoul, South Korea; Department of Medical Science, Asan Medical Institute of Convergence Science and Technology, Asan Medical Center, University of Ulsan College of Medicine, Seoul, South Korea; Department of Medical Science, Asan Medical Institute of Convergence Science and Technology, Asan Medical Center, University of Ulsan College of Medicine, Seoul, South Korea; Department of Cardiology in Internal Medicine, Chungnam National University Sejong Hospital, Chungnam National University School of Medicine, Daejeon, South Korea; Departments of Convergence Medicine, Asan Medical Center, University of Ulsan College of Medicine, Seoul, South Korea; Department of Biomedical Engineering, Asan Medical Center, University of Ulsan College of Medicine, Seoul, South Korea; Department of Internal Medicine, Asan Medical Center, Division of Cardiology, University of Ulsan College of Medicine, Seoul, South Korea","IEEE Access","24 Dec 2021","2021","9","","166409","166422","In percutaneous intervention for treatment of coronary plaques, guidewire navigation is a primary procedure for stent delivery. Steering a flexible guidewire within coronary arteries requires considerable training, and the non-linearity between the control operation and the movement of the guidewire makes precise manipulation difficult. Here, we introduce a deep reinforcement learning (RL) framework for autonomous guidewire navigation in a robot-assisted coronary intervention. Using Rainbow, a segment-wise learning approach is applied to determine how best to accelerate training using human demonstrations, transfer learning, and weight initialization. ‘State’ for RL is customized as a focus window near the guidewire tip, and subgoals are placed to mitigate a sparse reward problem. The RL agent improves performance, eventually enabling the guidewire to reach all valid targets in ‘stable’ phase. For the last 300 out of 1000 episodes, the success rates of the guidewire navigation to the distal-main and side targets were 98% and 99% in 2D and 3D phantoms, respectively. Our framework opens a new direction in the automation of robot-assisted intervention, providing guidance on RL in physical spaces involving mechanical fatigue.","2169-3536","","10.1109/ACCESS.2021.3135277","Ministry of Trade, Industry & Energy (MOTIE, Korea), Ministry of Science & ICT (MSIT, Korea), and Ministry of Health & Welfare (MOHW, Korea) under Technology Development Program for AI-Bio-Robot-Medicine Convergence(grant numbers:20001638); Korea Medical Device Development Fund grant funded by Korea government (the Ministry of Science & ICT, the Ministry of Trade, Industry & Energy, the Ministry of Health & Welfare, the Ministry of Food and Drug Safety)(grant numbers:1711137894,KMDF_PR_20200901_0013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9648308","Coronary intervention;guidewire navigation;reinforcement learning","Training;Robots;Navigation;Phantoms;Arteries;Lesions;Reinforcement learning","blood vessels;deep learning (artificial intelligence);diseases;medical computing;medical robotics;patient treatment;phantoms;reinforcement learning","coronary artery phantom;percutaneous intervention;flexible guidewire;deep reinforcement learning framework;autonomous guidewire navigation;robot-assisted coronary intervention;segment-wise learning approach;transfer learning;guidewire tip;robot-assisted intervention;coronary plaque treatment;stent delivery;Rainbow;human demonstrations;3D phantoms;2D phantoms;mechanical fatigue","","10","","45","CCBYNCND","13 Dec 2021","","","IEEE","IEEE Journals"
"Autonomous Navigation of Swarms in 3D Environments Using Deep Reinforcement Learning","M. Shahbaz; A. Khan","Pattern Recognition Lab, DCIS, Pakistan Institute of Engineering and Applied Sciences, Islamabad, Pakistan; Pattern Recognition Lab, DCIS, Pakistan Institute of Engineering and Applied Sciences, Islamabad, Pakistan","2020 International Symposium on Recent Advances in Electrical Engineering & Computer Sciences (RAEE & CS)","26 Nov 2020","2020","5","","1","6","The presence of swarm intelligence in many natural systems has always been an inspiration to develop such distributed intelligence in artificial multi-agent systems. It finds its applications in high-level control of complex swarms, distributed sensing technologies, and telecom networks. In this paper, we present an end-to-end approach to train a group of cooperative agents to navigate through 3-dimensional (3D) environments. The problem is particularly hard because the agents can only observe the environment partially and the number of agents in the swarm (also known as the size of the swarm) may change over time. Our approach uses deep reinforcement learning, mapping raw sensory data to high-level commands, in order to optimize (1) navigation, and (2) distributed assembly of the swarm while keeping the swarm (3) unaffected from its size dynamics. Here, we use suitable reward shaping for navigation and distributed assembly and deal size dynamics by exploiting histograms as an observational input to the model. The simulations were performed in the Unity 3D engine. The results demonstrate that our approach effectively improves swarm navigation and assembly in rough 3D environments and can be generalized to real-world scenarios.","","978-0-7381-3109-2","10.1109/RAEECS50817.2020.9265845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9265845","swarm intelligence;distributed intelligence;high-level control of swarms;deep reinforcement learning;swarm navigation;swarm organization;swarm robotics,","Reinforcement learning;Sensors;Navigation;Three-dimensional displays;Optimization;Histograms;Task analysis","learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;navigation;neurocontrollers;path planning;swarm intelligence","distributed intelligence;artificial multiagent systems;high-level control;complex swarms;distributed sensing;3-dimensional environments;deep reinforcement learning;high-level commands;deal size dynamics;Unity 3D engine;swarm navigation;rough 3D environments;autonomous navigation;swarm intelligence;natural systems;telecom network;cooperative agents;distributed assembly;reward shaping","","2","","17","IEEE","26 Nov 2020","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Approach for Competitive Task Assignment in Enterprise Blockchain","G. Volpe; A. M. Mangini; M. P. Fanti","Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy; Department of Electrical and Information Engineering, Polytechnic University of Bari, Bari, Italy","IEEE Access","23 May 2023","2023","11","","48236","48247","With the advent of Industry 4.0, the demand of high computing power for tasks such as data mining, 3D rendering, file conversion and cryptography is continuously growing. To this extent, distributed and decentralized environments play a fundamental role by dramatically increasing the amount of available resources. However, there are still several issues in the existing resource sharing solutions, such as the uncertainty of task running time, the renting price and the security of transactions. In this work, we present a blockchain-enabled task assignment platform by performance prediction based on Hyperledger Fabric, an open-source solution for private and permissioned blockchains in enterprise contexts that outperforms other technologies in terms of modularity, security and performance. We propose a model-free deep reinforcement learning framework to predict task runtime in agents current load state while the agent is engaged in multiple concurrent tasks. In addition, we let clients choose between prediction accuracy and price saving on each request. This way, we implicitly give inaccurate agents a chance to get assignments by competing in price rather than in time, allowing them to collect new experiences and improve future predictions. We conduct extensive experiments to evaluate the performance of the proposed scheme.","2169-3536","","10.1109/ACCESS.2023.3276859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10126111","Blockchain;cloud;deep reinforcement learning (DRL);resource sharing","Task analysis;Blockchains;Runtime;Peer-to-peer computing;Distributed ledger;Fabrics;Prediction algorithms","blockchains;data mining;deep learning (artificial intelligence);learning (artificial intelligence);production engineering computing;reinforcement learning","agents current load state;blockchain-enabled task assignment platform;competitive task assignment;cryptography;data mining;enterprise blockchain;enterprise contexts;existing resource sharing solutions;file conversion;future predictions;high computing power;Hyperledger Fabric;inaccurate agents;model-free deep reinforcement learning framework;multiple concurrent tasks;open-source solution;performance prediction;permissioned blockchains;prediction accuracy;private blockchains;renting price;task running time;task runtime","","","","45","CCBYNCND","16 May 2023","","","IEEE","IEEE Journals"
"A Simulation-Aided Deep Reinforcement Learning Approach for Optimization of Automated Sorting Center Processes","D. Mohapatra; A. Pal; A. Ojha; S. Ghosh; M. Agarwal; C. Sarkar","Tata Consultancy Services Research, Noida, UP, INDIA; Tata Consultancy Services Research, Noida, UP, INDIA; Tata Consultancy Services Research, Noida, UP, INDIA; Tata Consultancy Services Research, Noida, UP, INDIA; Tata Consultancy Services Research, Noida, UP, INDIA; Tata Consultancy Services Research, Noida, UP, INDIA","2022 Winter Simulation Conference (WSC)","23 Jan 2023","2022","","","2795","2806","Operations in a parcel sorting center (SC) are multi-fold which lead to multiple NP-hard optimization problems, namely, parcel-chute assignment, online bin-packing, scheduling, and routing. The advent of multi-agent robotics has accelerated the process of automation in sorting centers which has led to the requirement of sophisticated algorithms to optimize these operations within an SC. To this end, we propose RL - SORT : a simulation-aided deep reinforcement learning based algorithm which jointly optimizes the parcel-chute assignment and online roller-cage (RC) packing problems. Through experimentation on our simulation framework, we show that RL- SORT not only outperforms baselines, but also has a low computational burden. Further, it is able to significantly reduce the number of RCs used, thereby, reducing the transportation costs.","1558-4305","978-1-6654-7661-4","10.1109/WSC57314.2022.10015430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015430","","Deep learning;Costs;Uncertainty;Computational modeling;Transportation;Reinforcement learning;Routing","bin packing;deep learning (artificial intelligence);multi-agent systems;optimisation;process planning;production engineering computing;reinforcement learning;scheduling;sorting","automated sorting center processes;multiagent robotics;multiple NP-hard optimization problems;online bin-packing;online roller-cage packing problems;parcel sorting center;parcel-chute assignment;RC packing problems;RL- SORT;simulation framework;simulation-aided deep reinforcement learning approach;sorting centers","","","","16","IEEE","23 Jan 2023","","","IEEE","IEEE Conferences"
