@inproceedings{10.1145/3460569.3460573,
author = {Xu, Pei and Chen, GuangDa and Ji, Jianmin},
title = {Applying Soft Attention in Deep Reinforcement Learning for Robot Collision Avoidance},
year = {2021},
isbn = {9781450389464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460569.3460573},
doi = {10.1145/3460569.3460573},
booktitle = {Proceedings of the 2021 6th International Conference on Mathematics and Artificial Intelligence},
pages = {34–38},
numpages = {5},
keywords = {Attention mechanism, Robot navigation, Deep reinforcement learning},
location = {Chengdu, China},
series = {ICMAI '21}
}

@inproceedings{10.1145/3544549.3585913,
author = {Gupta, Tanay and Gori, Julien},
title = {Modeling Reciprocal Adaptation in HCI: A Multi-Agent Reinforcement Learning Approach},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585913},
doi = {10.1145/3544549.3585913},
abstract = {Adaptation between users and computers is difficult because of the reciprocal long-term adaptation between the user and an adaptive tool. In this work in progress, we present a novel method for designing adaptive systems, by simulating reciprocal adaptation between a user and an intelligent tool via Multi-Agent Reinforcement Learning (MARL). We report on a simple target-selection simulation in which a simulated, rational user tries to reach a target and is aided by an intelligent tool that manipulates the user’s environment. The user and the tool’s behavior are jointly determined by an adaptive algorithm that maximizes their utility. Our approach shows that both agents are capable of learning realistic policies, with the user and tool combination being 30\% more effective than the user alone.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {209},
numpages = {6},
keywords = {co-evolution, human-computer interaction, deep reinforcement learning, multi-agent systems},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/2903220.2903254,
author = {Tziortziotis, Konstantinos and Tziortziotis, Nikolaos and Vlachos, Kostas and Blekas, Konstantinos},
title = {Autonomous Navigation of an Over-Actuated Marine Platform Using Reinforcement Learning},
year = {2016},
isbn = {9781450337342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2903220.2903254},
doi = {10.1145/2903220.2903254},
abstract = {This paper investigates the use of reinforcement learning for the navigation of an over-actuated marine platform in unknown environments. The proposed approach uses an online least-squared policy iteration scheme for value function approximation in order to estimate optimal policy. We evaluate our approach in a simulation platform and report some initial results concerning its performance on estimating optimal navigation policies to unknown environments under different environmental disturbances. The results are promising.},
booktitle = {Proceedings of the 9th Hellenic Conference on Artificial Intelligence},
articleno = {11},
numpages = {7},
keywords = {Autonomous navigation, over-actuated marine vehicle, Reinforcement Learning},
location = {Thessaloniki, Greece},
series = {SETN '16}
}

@inproceedings{10.1145/3543873.3587573,
author = {Chen, Junlong and Nie, Jiangtian and Xu, Minrui and Lyu, Lingjuan and Xiong, Zehui and Kang, Jiawen and Tong, Yongju and Jiang, Wenchao},
title = {Multiple-Agent Deep Reinforcement Learning for Avatar Migration in Vehicular Metaverses},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587573},
doi = {10.1145/3543873.3587573},
abstract = {Vehicular Metaverses are widely considered as the next Internet revolution to build a 3D virtual world with immersive virtual-real interaction for passengers and drivers. In vehicular Metaverse applications, avatars are digital representations of on-board users to obtain and manage immersive vehicular services (i.e., avatar tasks) in Metaverses and the data they generate. However, traditional Internet of Vehicles (IoV) data management solutions have serious data security risks and privacy protection. Fortunately, blockchain-based Web 3.0 enables avatars to have an ownership identity to securely manage the data owned by users in a decentralized and transparent manner. To ensure users’ immersive experiences and securely manage their data, avatar tasks often require significant computing resources. Therefore, it is impractical for the vehicles to process avatar tasks locally, massive computation resources are needed to support the avatar tasks. To this end, offloading avatar tasks to nearby RoadSide Units (RSUs) is a promising solution to avoid computation overload. To ensure real-time and continuous Metaverse services, the avatar tasks should be migrated among the RSUs when the vehicle navigation. It is challenging for the vehicles to independently decide whether migrate or not according to current and future avatar states. Therefore, in this paper, we propose a new avatar task migration framework for vehicular Metaverses. We then formulate the avatar task migration problem as a Partially Observable Markov Decision Process (POMDP), and apply a Multi-Agent Deep Reinforcement Learning (MADRL) algorithm to dynamically make migration decisions for avatar tasks. Numerous results show that our proposed algorithm outperforms existing baselines for avatar task migration and enables immersive vehicular Metaverse services.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {1258–1265},
numpages = {8},
keywords = {vehicular Metaverse, multi-agent deep reinforcement learning, avatar migration},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3477495.3531847,
author = {Liao, Guogang and Shi, Xiaowen and Wang, Ze and Wu, Xiaoxu and Zhang, Chuheng and Wang, Yongkang and Wang, Xingxing and Wang, Dong},
title = {Deep Page-Level Interest Network in Reinforcement Learning for Ads Allocation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531847},
doi = {10.1145/3477495.3531847},
abstract = {A mixed list of ads and organic items is usually displayed in feed and how to allocate the limited slots to maximize the overall revenue is a key problem. Meanwhile, user behavior modeling is essential in recommendation and advertising (e.g., CTR prediction and ads allocation). Most previous works only model point-level positive feedback (i.e., click), which neglect the page-level information of feedback and other types of feedback. To this end, we propose Deep Page-level Interest Network (DPIN) to model the page-level user preference and exploit multiple types of feedback. Specifically, we introduce four different types of page-level feedback, and capture user preference for item arrangement under different receptive fields through the multi-channel interaction module. Through extensive offline and online experiments on Meituan food delivery platform, we demonstrate that DPIN can effectively model the page-level user preference and increase the revenue.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2292–2296},
numpages = {5},
keywords = {reinforcement learning, ads allocation, user behavior modeling},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3511176.3511206,
author = {Zhou, Lijuan and Jiao, Xuri and Liu, Tao and Niu, Changyong},
title = {Sign Language Video Translation Based on BERT-SLSTM and Reinforcement Learning},
year = {2022},
isbn = {9781450385893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511176.3511206},
doi = {10.1145/3511176.3511206},
abstract = {This paper proposes a novel method based on BERT-Stacked LSTM and Reinforcement Learning (BSRL) model for joint sign language recognition and sign language translation. The proposed model applies the BERT model as the encoder to process the features of multi-channel communication of sign language videos. The stacked LSTM is applied as the decoder to effectively extract the text representation. A reinforcement learning algorithm based on self-critical sequence training is applied to model parameters based on evaluation indicators so that the generated texts have a high similarity to the ground truth texts. Experiment results on Phoenix2014T dataset demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 2021 5th International Conference on Video and Image Processing},
pages = {198–204},
numpages = {7},
keywords = {Sign language translation, Reinforcement learning, Video description generation},
location = {Hayward, CA, USA},
series = {ICVIP '21}
}

@inproceedings{10.1145/3321707.3321829,
author = {Stork, J\"{o}rg and Zaefferer, Martin and Bartz-Beielstein, Thomas and Eiben, A. E.},
title = {Surrogate Models for Enhancing the Efficiency of Neuroevolution in Reinforcement Learning},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321829},
doi = {10.1145/3321707.3321829},
abstract = {In the last years, reinforcement learning received a lot of attention. One method to solve reinforcement learning tasks is Neuroevolution, where neural networks are optimized by evolutionary algorithms. A disadvantage of Neuroevolution is that it can require numerous function evaluations, while not fully utilizing the available information from each fitness evaluation. This is especially problematic when fitness evaluations become expensive. To reduce the cost of fitness evaluations, surrogate models can be employed to partially replace the fitness function. The difficulty of surrogate modeling for Neuroevolution is the complex search space and how to compare different networks. To that end, recent studies showed that a kernel based approach, particular with phenotypic distance measures, works well. These kernels compare different networks via their behavior (phenotype) rather than their topology or encoding (genotype). In this work, we discuss the use of surrogate model-based Neuroevolution (SMB-NE) using a phenotypic distance for reinforcement learning. In detail, we investigate a) the potential of SMB-NE with respect to evaluation efficiency and b) how to select adequate input sets for the phenotypic distance measure in a reinforcement learning problem. The results indicate that we are able to considerably increase the evaluation efficiency using dynamic input sets.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {934–942},
numpages = {9},
keywords = {reinforcement learning, surrogate models, neuroevolution},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.1145/3503161.3548331,
author = {Kan, Nuowen and Jiang, Yuankun and Li, Chenglin and Dai, Wenrui and Zou, Junni and Xiong, Hongkai},
title = {Improving Generalization for Neural Adaptive Video Streaming via Meta Reinforcement Learning},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548331},
doi = {10.1145/3503161.3548331},
abstract = {In this paper, we present a meta reinforcement learning (Meta-RL)-based neural adaptive bitrate streaming (ABR) algorithm that is able to rapidly adapt its control policy to the changing network throughput dynamics. Specifically, to allow rapid adaptation, we discuss the necessity of detaching the inference of throughput dynamics with the universal control mechanism that is in essence shared by all potential throughput dynamics for neural ABR algorithms. To meta-learn the ABR policy, we then build up a model-free system framework, composed of a probabilistic latent encoder that infers the underlying dynamics from the recent throughput context, and a policy network that is conditioned on latent variable and learns to quickly adapt to new environments. Additionally, to address the difficulties caused by training the policy on mixed dynamics, on-policy RL (or imitation learning) algorithms are suggested for policy training, with a mutual information-based regularization to make the latent variable more informative about the policy. Finally, we implement our algorithm's meta-training and meta-adaptation procedures under a variety of throughput dynamics. Empirical evaluations on different QoE metrics and multiple datasets containing real-world network traces demonstrate that our algorithm outperforms state-of-the-art ABR algorithms, in terms of the performance on the average chunk QoE, consistency and fast adaptation across a wide range of throughput patterns.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {3006–3016},
numpages = {11},
keywords = {meta deep reinforcement learning, generalization, rate adaptation},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3365921.3365932,
author = {Sutton, Rowan and Fraser, Kieran and Conlan, Owen},
title = {A Reinforcement Learning and Synthetic Data Approach to Mobile Notification Management},
year = {2020},
isbn = {9781450371780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365921.3365932},
doi = {10.1145/3365921.3365932},
abstract = {Mobile push-notifications are the primary mechanism for communicating new information to smartphone users, however they can also have a negative impact on user emotions, reduce work effectiveness and decrease current task performance. Through analysing state-of-the-art research on mobile Notification Management Systems, it was identified that few open-source notification data sets and, corresponding benchmarks, have been created and the majority of NMSs apply supervised learning methods. This paper investigates the use of a, freely shareable, synthetic mobile notification data set for developing and evaluating NMS performance using Reinforcement Learning. A Q-learning and Deep Q-learning agent were trained using synthetic data and an OpenAI Gym environment was created for evaluation. Final results illustrated that the Q-learning and Deep Q-learning agents could predict a users action toward notifications with ≈80\% success when trained and evaluated upon real or synthetic data and ≈65\% success when trained on synthetic and evaluated upon real notification data.},
booktitle = {Proceedings of the 17th International Conference on Advances in Mobile Computing \&amp; Multimedia},
pages = {155–164},
numpages = {10},
keywords = {push-notifications, synthetic data, reinforcement learning},
location = {Munich, Germany},
series = {MoMM2019}
}

@inproceedings{10.1145/3447548.3467181,
author = {Hao, Qianyue and Xu, Fengli and Chen, Lin and Hui, Pan and Li, Yong},
title = {Hierarchical Reinforcement Learning for Scarce Medical Resource Allocation with Imperfect Information},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467181},
doi = {10.1145/3447548.3467181},
abstract = {Facing the outbreak of COVID-19, shortage in medical resources becomes increasingly outstanding. Therefore, efficient strategies for medical resource allocation are urgently called for. Reinforcement learning (RL) is powerful for decision making, but three key challenges exist in solving this problem via RL: (1) complex situation and countless choices for decision making in the real world; (2) only imperfect information are available due to the latency of pandemic spreading; (3) limitations on conducting experiments in real world since we cannot set pandemic outbreaks arbitrarily. In this paper, we propose a hierarchical reinforcement learning method with a corresponding training algorithm. We design a decomposed action space to deal with the countless choices to ensure efficient and real time strategies. We also design a recurrent neural network based framework to utilize the imperfect information obtained from the environment. We build a pandemic spreading simulator based on real world data, serving as the experimental platform. We conduct extensive experiments and the results show that our method outperforms all the baselines, which reduces infections and deaths by 14.25\% on average.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2955–2963},
numpages = {9},
keywords = {medical resource allocation, imperfect information, hierarchical reinforcement learning, COVID-19 pandemic},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3450267.3450537,
author = {Lee, Xian Yeow and Esfandiari, Yasaman and Tan, Kai Liang and Sarkar, Soumik},
title = {Query-Based Targeted Action-Space Adversarial Policies on Deep Reinforcement Learning Agents},
year = {2021},
isbn = {9781450383530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450267.3450537},
doi = {10.1145/3450267.3450537},
abstract = {Advances in computing resources have resulted in the increasing complexity of cyber-physical systems (CPS). As the complexity of CPS evolved, the focus has shifted to deep reinforcement learning-based (DRL) methods for control of these systems. This is in part due to: 1) difficulty of obtaining accurate models of complex CPS for traditional control 2) DRL algorithms' capability of learning control policies from data which can be adapted and scaled to real, complex CPS. To securely deploy DRL in production, it is essential to examine the weaknesses of DRL-based controllers (policies) towards malicious attacks from all angles. This work investigates targeted attacks in the action-space domain (actuation attacks), which perturbs the outputs of a controller. We show that a black-box attack model that generates perturbations with respect to an adversarial goal can be formulated as another reinforcement learning problem. Thus, an adversarial policy can be trained using conventional DRL methods. Experimental results showed that adversarial policies which only observe the nominal policy's output generate stronger attacks than adversarial policies that observe the nominal policy's input and output. Further analysis revealed that nominal policies whose outputs are frequently at the boundaries of the action space are naturally more robust towards adversarial policies. Lastly, we propose the use of adversarial training with transfer learning to induce robust behaviors into the nominal policy, which decreases the rate of successful targeted attacks by approximately 50\%.},
booktitle = {Proceedings of the ACM/IEEE 12th International Conference on Cyber-Physical Systems},
pages = {87–97},
numpages = {11},
keywords = {black-box attacks, adversarial attacks, adversarial policies, deep reinforcement learning, adversarial training},
location = {Nashville, Tennessee},
series = {ICCPS '21}
}

@inproceedings{10.1145/3535782.3535803,
author = {Yi, Xiaodi and Huang, Kuihua and Huang, Jincai and Qian, Jing and Kang, Jie},
title = {Research on Order Acceptance Strategy for Military Manufactures Based on Semi-Markov Average Reward Reinforcement Learning},
year = {2022},
isbn = {9781450395816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535782.3535803},
doi = {10.1145/3535782.3535803},
abstract = {Order acceptance is an important decision-making issue that military manufacturers need to consider, which is crucial for military manufacturers to complete their production tasks and improve their benefits. In this paper, we first analyze the customer orders pattern and production pattern of military manufacturers, and study the order acceptance decision algorithm for the military products orders with the characteristics such as small orders, multiple batches, strong dynamics, emphasis on customer priority, and complex inventory requirements. The order acceptance problem model of traditional manufacturing enterprises mainly considers factors such as delay penalty cost, rejection cost and production cost under static state conditions, On the basis of these factors, we further take the inventory cost of orders completed before the lead time and multiple customer priorities as influencing factors of order acceptance problem into consideration under dynamic demand. Combine with the idea of reward management, we solve the order acceptance problem with a semi-Markov decision process (SMDP) model using a model-free reinforcement learning method: Semi-Markov Average Reward Technique (SMART). Finally, We present a detailed study of this algorithm on a military order acceptance problem in the simulation environment. We compare the average reward, order acceptance rate and product acceptance rate of different algorithms, and verify the superiority of the order acceptance strategy model proposed in this paper over the traditional decision model.},
booktitle = {Proceedings of the 4th International Conference on Management Science and Industrial Engineering},
pages = {152–161},
numpages = {10},
location = {Chiang Mai, Thailand},
series = {MSIE '22}
}

@inproceedings{10.1145/3366194.3366251,
author = {Yang, Songyue and Meng, Zhijun and Chen, Xuzhi and Xie, Ronglei},
title = {Real-Time Obstacle Avoidance with Deep Reinforcement Learning Three-Dimensional Autonomous Obstacle Avoidance for UAV},
year = {2019},
isbn = {9781450372985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366194.3366251},
doi = {10.1145/3366194.3366251},
abstract = {At present, drones are rapidly developing in the aviation industry and are applied to all aspects of life. However, letting drones autonomously avoid obstacles is still the focus of research by aviation scholars at this stage. However, the current automation is mostly based on human experience to determine the obstacle avoidance strategy of UAV. And the method only rely on the machine to avoid obstacle is very few. In this paper, the UAV collect visual and distance sensor information to make autonomous obstacle avoidance decision through the deep reinforcement learning algorithm, and the algorithm is tested in the v-rep simulation environment.},
booktitle = {Proceedings of the 2019 International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {324–329},
numpages = {6},
keywords = {obstacle avoidance, aircraft, v-rep, DQN},
location = {Shanghai, China},
series = {RICAI '19}
}

@inproceedings{10.1145/3495018.3495052,
author = {Chen, Donghai and Jiang, Xuedong and Shi, Liqin and Cai, Zhenhua and Wu, Hangjie},
title = {Energy Storage Backup Power Control Strategy Based on Improved Deep Reinforcement Learning Algorithm},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495052},
doi = {10.1145/3495018.3495052},
abstract = {Base on the virtual power plant (VPP), this paper studies the regulation strategy of using user-side energy storage as a backup power source to provide power supply for the park when the external power grid fails in the industrial park. Aiming at the continuity of the output of wind power plants, photovoltaic power plants and energy storage power sources, this paper uses an improved depth deterministic strategy gradient algorithm (DDPG). Based on the temporal characteristics of the external environment and the agent, the article uses a cyclic neural network to replace the original convolutional neural network, and at the same time uses a multi-simulator parallel processing strategy to accelerate processing. The calculation results show that, compared with the traditional DDPG algorithm, the strategy adjustment strategy described in this article has obtained a better adjustment effect.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {192–198},
numpages = {7},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/3580305.3599818,
author = {Wan, Runzhe and Liu, Yu and McQueen, James and Hains, Doug and Song, Rui},
title = {Experimentation Platforms Meet Reinforcement Learning: Bayesian Sequential Decision-Making for Continuous Monitoring},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599818},
doi = {10.1145/3580305.3599818},
abstract = {With the growing needs of online A/B testing to support the innovation in industry, the opportunity cost of running an experiment becomes non-negligible. Therefore, there is an increasing demand for an efficient continuous monitoring service that allows early stopping when appropriate. Classic statistical methods focus on hypothesis testing and are mostly developed for traditional high-stake problems such as clinical trials, while experiments at online service companies typically have very different features and focuses. Motivated by the real needs, in this paper, we introduce a novel framework that we developed in Amazon to maximize customer experience and control opportunity cost. We formulate the problem as a Bayesian optimal sequential decision making problem that has a unified utility function. We discuss extensively practical design choices and considerations. We further introduce how to solve the optimal decision rule via Reinforcement Learning and scale the solution. We show the effectiveness of this novel approach compared with existing methods via a large-scale meta-analysis on experiments in Amazon.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5016–5027},
numpages = {12},
keywords = {reinforcement learning, a/b testing, sequential decision making},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3490354.3494398,
author = {Zhao, Muchen and Linetsky, Vadim},
title = {High Frequency Automated Market Making Algorithms with Adverse Selection Risk Control via Reinforcement Learning},
year = {2022},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494398},
doi = {10.1145/3490354.3494398},
abstract = {Market makers provide liquidity by placing limit orders on both sides of the market (bids and offers) while aiming to earn the bid-offer (bid-ask) spread. Their long-term performance is significantly determined by their ability to mitigate the risk of adverse selection when their limit orders are picked off by informed traders possessing relevant information that moves the market to a new level resulting in losses to market makers. This paper proposes a high-frequency feature Book Exhaustion Rate (BER) and shows theoretically and empirically that the BER can serve as a direct measurement of the adverse selection risk from an equilibrium point of view. We train a market making algorithm via Reinforcement Learning using three years of limit order book data on Chicago Mercantile Exchange (CME) S\&amp;P 500 and 10-year Treasury note futures and demonstrate that with utilizing the BER allows the algorithm to avoid large losses due to adverse selection and achieve stable performance.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {33},
numpages = {9},
location = {Virtual Event},
series = {ICAIF '21}
}

@inproceedings{10.5555/3201607.3201648,
author = {Zhao, Pu and Wang, Yanzhi and Chang, Naehyuck and Zhu, Qi and Lin, Xue},
title = {A Deep Reinforcement Learning Framework for Optimizing Fuel Economy of Hybrid Electric Vehicles},
year = {2018},
publisher = {IEEE Press},
abstract = {Hybrid electric vehicles employ a hybrid propulsion system to combine the energy efficiency of electric motor and a long driving range of internal combustion engine, thereby achieving a higher fuel economy as well as convenience compared with conventional ICE vehicles. However, the relatively complicated powertrain structures of HEVs necessitate an effective power management policy to determine the power split between ICE and EM. In this work, we propose a deep reinforcement learning framework of the HEV power management with the aim of improving fuel economy. The DRL technique is comprised of an offline deep neural network construction phase and an online deep Q-learning phase. Unlike traditional reinforcement learning, DRL presents the capability of handling the high dimensional state and action space in the actual decision-making process, making it suitable for the HEV power management problem. Enabled by the DRL technique, the derived HEV power management policy is close to optimal, fully model-free, and independent of a prior knowledge of driving cycles. Simulation results based on actual vehicle setup over real-world and testing driving cycles demonstrate the effectiveness of the proposed framework on optimizing HEV fuel economy.},
booktitle = {Proceedings of the 23rd Asia and South Pacific Design Automation Conference},
pages = {196–202},
numpages = {7},
location = {Jeju, Republic of Korea},
series = {ASPDAC '18}
}

@inproceedings{10.1109/WI-IAT.2010.210,
author = {Tan, Ah-Hwee and Ng, Gee-Wah},
title = {A Biologically-Inspired Cognitive Agent Model Integrating Declarative Knowledge and Reinforcement Learning},
year = {2010},
isbn = {9780769541914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2010.210},
doi = {10.1109/WI-IAT.2010.210},
abstract = {The paper proposes a biologically-inspired cognitive agent model, known as FALCON-X, based on an integration of the Adaptive Control of Thought (ACT-R) architecture and a class of self-organizing neural networks called fusion Adaptive Resonance Theory (fusion ART). By replacing the production system of ACT-R by a fusion ART model, FALCON-X integrates high-level deliberative cognitive behaviors and real-time learning abilities, based on biologically plausible neural pathways. We illustrate how FALCON-X, consisting of a core inference area interacting with the associated intentional, declarative, perceptual, motor and critic memory modules, can be used to build virtual robots for battles in a simulated RoboCode domain. The performance of FALCON-X demonstrates the efficacy of the hybrid approach.},
booktitle = {Proceedings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {248–251},
numpages = {4},
keywords = {Reinforcement Learning, Knowledge Representation, Cognitive Agents},
series = {WI-IAT '10}
}

@inproceedings{10.1145/3474085.3475635,
author = {Mo, Clinton and Hu, Kun and Mei, Shaohui and Chen, Zebin and Wang, Zhiyong},
title = {Keyframe Extraction from Motion Capture Sequences with Graph Based Deep Reinforcement Learning},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475635},
doi = {10.1145/3474085.3475635},
abstract = {Animation production workflows centred around motion capture techniques often require animators to edit the motion for various artistic and technical reasons. This process generally uses a set of keyframes. Unsupervised keyframe selection methods for motion capture sequences are highly demanded to reduce the laborious annotations. However, most existing methods are optimization-based, which cause the issues of flexibility and efficiency and eventually constrains the interactions and controls with animators. To address these limitations, we propose a novel graph based deep reinforcement learning method for efficient unsupervised keyframe selection. First, a reward function is devised in terms of reconstruction difference by comparing the original sequence and the interpolated sequence produced by the keyframes. The reward complies with the requirements of the animation pipeline satisfying: 1) incremental reward to evaluate the interpolated keyframes immediately; 2) order insensitivity for consistent evaluation; and 3) non-diminishing return for comparable rewards between optimal and sub-optimal solutions. Then by representing each skeleton frame as a graph, a graph-based deep agent is guided to heuristically select keyframes to maximize the reward. During the inference it is no longer necessary to estimate the reconstruction difference, and the evaluation time can be reduced significantly. The experimental results on the CMU Mocap dataset demonstrate that our proposed method is able to select keyframes at a high efficiency without clearly compromising the quality in comparison with the state-of-the-art methods.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {5194–5202},
numpages = {9},
keywords = {graph convolutional networks, motion capture, keyframe selection, keyframe extraction, keyframe animation, reinforcement learning},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3583781.3590209,
author = {Kundu, Debraj and Vamsi, Gadikoyila Satya and Veman, Karnati Vivek and Mahidhar, Gurram and Roy, Sudip},
title = {Reinforcement Learning Based Module Placement for Enhancing Reliability of MEDA Digital Microfluidic Biochips},
year = {2023},
isbn = {9798400701252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583781.3590209},
doi = {10.1145/3583781.3590209},
abstract = {promising new generation microfluidic biochips consisting of a sea-of-micro-electrodes with dedicated detection circuit for each microelectrode. Moreover, the ability to manipulate discrete droplets of different volumes and to route them in any direction presents MEDA biochips as an advanced microfluidic technology. Due to similarity in the working principles, the reliability issues of both MEDA biochips and digital microfluidic biochips are similar. In this paper, we propose a module placement technique for MEDA biochips to improve the reliability of biochips. Reinforcement learning based placement method (RLPM) is designed for obtaining the reliability-aware placement of rectilinear shaped microfluidic modules. RLPM aims to minimize the area of a biochip while increasing its reliability. Simulation results confirm that on average RLPM minimizes the chip utilization area by 28.6\% while enhancing the reliability of MEDA biochips compared to the state-of-the-art method.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2023},
pages = {509–514},
numpages = {6},
keywords = {biochips, meda, placement, dqn, microfluidics, reliability},
location = {Knoxville, TN, USA},
series = {GLSVLSI '23}
}

@inproceedings{10.1145/3295500.3356202,
author = {Balaprakash, Prasanna and Egele, Romain and Salim, Misha and Wild, Stefan and Vishwanath, Venkatram and Xia, Fangfang and Brettin, Tom and Stevens, Rick},
title = {Scalable Reinforcement-Learning-Based Neural Architecture Search for Cancer Deep Learning Research},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356202},
doi = {10.1145/3295500.3356202},
abstract = {Cancer is a complex disease, the understanding and treatment of which are being aided through increases in the volume of collected data and in the scale of deployed computing power. Consequently, there is a growing need for the development of data-driven and, in particular, deep learning methods for various tasks such as cancer diagnosis, detection, prognosis, and prediction. Despite recent successes, however, designing high-performing deep learning models for nonimage and nontext cancer data is a time-consuming, trial-and-error, manual task that requires both cancer domain and deep learning expertise. To that end, we develop a reinforcement-learning-based neural architecture search to automate deep-learning-based predictive model development for a class of representative cancer data. We develop custom building blocks that allow domain experts to incorporate the cancer-data-specific characteristics. We show that our approach discovers deep neural network architectures that have significantly fewer trainable parameters, shorter training time, and accuracy similar to or higher than those of manually designed architectures. We study and demonstrate the scalability of our approach on up to 1,024 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {37},
numpages = {33},
keywords = {cancer, deep learning, neural architecture search, reinforcement learning},
location = {Denver, Colorado},
series = {SC '19}
}

@inproceedings{10.1145/3597926.3598066,
author = {Zhang, Zhaoxu and Winn, Robert and Zhao, Yu and Yu, Tingting and Halfond, William G.J.},
title = {Automatically Reproducing Android Bug Reports Using Natural Language Processing and Reinforcement Learning},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598066},
doi = {10.1145/3597926.3598066},
abstract = {As part of the process of resolving issues submitted by users via bug reports, Android developers attempt to reproduce and observe the crashes described by the bug reports. Due to the low-quality of bug reports and the complexity of modern apps, the reproduction process is non-trivial and time-consuming. Therefore, automatic approaches that can help reproduce Android bug reports are in great need. However, current approaches to help developers automatically reproduce bug reports are only able to handle limited forms of natural language text and struggle to successfully reproduce crashes for which the initial bug report had missing or imprecise steps. In this paper, we introduce a new fully automated approach to reproduce crashes from Android bug reports that addresses these limitations. Our approach accomplishes this by leveraging natural language processing techniques to more holistically and accurately analyze the natural language in Android bug reports and designing new techniques, based on reinforcement learning, to guide the search for successful reproducing steps. We conducted an empirical evaluation of our approach on 77 real world bug reports. Our approach achieved 67\% precision and 77\% recall in accurately extracting reproduction steps from bug reports, reproduced 74\% of the total bug reports, and reproduced 64\% of the bug reports that contained missing steps, significantly outperforming state of the art techniques.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {411–422},
numpages = {12},
keywords = {Reinforcement Learning, Android, Natural Language Processing, Bug Reproduction},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@article{10.1109/TNET.2020.2979966,
author = {Wang, Fangxin and Zhang, Cong and Wang, Feng and Liu, Jiangchuan and Zhu, Yifei and Pang, Haitian and Sun, Lifeng},
title = {DeepCast: Towards Personalized QoE for Edge-Assisted Crowdcast With Deep Reinforcement Learning},
year = {2020},
issue_date = {June 2020},
publisher = {IEEE Press},
volume = {28},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2979966},
doi = {10.1109/TNET.2020.2979966},
abstract = {Today's anywhere and anytime broadband connection and audio/video capture have boosted the deployment of crowdsourced livecast services (or crowdcast). Bridging a massive amount of geo-distributed broadcasters and their fellow viewers, such representatives as Twitch.tv, Youtube Gaming, and Inke.tv, have greatly changed the generation and distribution landscape of streaming content. They also enable rich online interactions among the crowd, and strive to offer personalized Quality-of-Experience (QoE) for individual viewers. Given the ultra-large scale and the dynamics of the crowd, personalizing QoE however is much more challenging than in early generation streaming services. The rich interactions among the broadcasters, viewers, and the network system, on the other hand, also offer invaluable data that could be utilized towards informed management. This paper presents DeepCast, an edge-assisted crowdcast framework that explores the sheer amount of viewing data towards intelligent decisions for personalized QoE demands. DeepCast seamlessly integrates cloud, CDN, and edge servers for crowdcast content distribution, and advocates a data-driven design that extracts the hidden information from the complex interactions among the system components. Through deep reinforcement learning (DRL), it automatically identifies the most suitable strategies for viewer assignment and transcoding at edges. We collect multiple real-world datasets and evaluate the performance of DeepCast with trace-driven experiments. The results demonstrate its flexibility and effectiveness towards better personalized QoE and lower cost for crowdcast systems.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {1255–1268},
numpages = {14}
}

@inproceedings{10.1145/3564121.3564795,
author = {Borra, Kavya and Krishnan, Ashwin and Khadilkar, Harshad and Nambiar, Manoj and Basumatary, Ansuma and Singhal, Rekha and Mukherjee, Arijit},
title = {Performance Improvement of Reinforcement Learning Algorithms for Online 3D Bin Packing Using FPGA},
year = {2023},
isbn = {9781450398473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564121.3564795},
doi = {10.1145/3564121.3564795},
abstract = {Online 3D bin packing is a challenging real-time combinatorial optimisation problem that involves packing of parcels (typically rigid cuboids) arriving on a conveyor into a larger bin for further shipment. Recent automation methods have introduced manipulator robots for packing, which need a processing algorithm to specify the location and orientation in which each parcel must be loaded. Value-based Reinforcement learning (RL) algorithms such as DQN are capable of producing good solutions in the available computation times. However, their deployment on CPU based systems employs rule-based heuristics to reduce the search space which may lead to a sub-optimal solution. In this paper, we use FPGA as a hardware accelerator to reduce inference time of DQN as well as its pre-/post-processing steps. This allows the optimised algorithm to cover the entire search space within the given time constraints. We present various optimizations, such as accelerating DQN model inference and fast checking of constraints. Further, we show that our proposed architecture achieves almost 15x computational speed-ups compared to an equivalent CPU implementation. Additionally, we show that as a result of evaluating the entire search space, the DQN rewards generated for complex data sets has improved by 1\%, which can cause a significant reduction in enterprise operating costs.},
booktitle = {Proceedings of the Second International Conference on AI-ML Systems},
articleno = {18},
numpages = {7},
keywords = {hardware acceleration, FPGA, reinforcement learning},
location = {Bangalore, India},
series = {AIMLSystems '22}
}

@inproceedings{10.5555/3320516.3320910,
author = {Zhang, Tao and Xie, Shufang and Rose, Oliver},
title = {Real-Time Batching in Job Shops Based on Simulation and Reinforcement Learning},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {Real-time batching in job shops decides 1) whether to start processing a batch or to wait for more jobs joining the batch, and 2) which batch should be processed first. It is addressed as a sequential decision-making problem and formalized based on Markov decision processes. By adding a dummy batch, which means no batches are selected and all batches wait for additional jobs, the first decision-making is generalized to the second. A simulation-based neural fitted Q learning is introduced to solve the Markov decision processes and build a decision maker. The well-trained decision maker decides which batch in a batch list should be processed first at each decision epoch. The experiment results show that the proposed approach outperforms some other decision rules.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {3331–3339},
numpages = {9},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.5555/3466184.3466221,
author = {Zheng, Hua and Xie, Wei and Feng, M. Ben},
title = {Green Simulation Assisted Reinforcement Learning with Model Risk for Biomanufacturing Learning and Control},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Biopharmaceutical manufacturing faces critical challenges, including complexity, high variability, lengthy lead time, and limited historical data and knowledge of the underlying system stochastic process. To address these challenges, we propose a green simulation assisted model-based reinforcement learning to support process online learning and guide dynamic decision making. Basically, the process model risk is quantified by the posterior distribution. At any given policy, we predict the expected system response with prediction risk accounting for both inherent stochastic uncertainty and model risk. Then, we propose green simulation assisted reinforcement learning and derive the mixture proposal distribution of decision process and likelihood ratio based metamodel for the policy gradient, which can selectively reuse process trajectory outputs collected from previous experiments to increase the simulation data-efficiency, improve the policy gradient estimation accuracy, and speed up the search for the optimal policy. Our numerical study indicates that the proposed approach demonstrates the promising performance.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {337–348},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@article{10.1109/TASLP.2018.2851664,
author = {Weisz, Gellert and Budzianowski, Pawel and Su, Pei-Hao and Gasic, Milica},
title = {Sample Efficient Deep Reinforcement Learning for Dialogue Systems With Large Action Spaces},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2851664},
doi = {10.1109/TASLP.2018.2851664},
abstract = {In spoken dialogue systems, we aim to deploy artificial intelligence to build automated dialogue agents that can converse with humans. A part of this effort is the policy optimization task, which attempts to find a policy describing how to respond to humans, in the form of a function taking the current state of the dialogue and returning the response of the system. In this paper, we investigate deep reinforcement learning approaches to solve this problem. Particular attention is given to actor-critic methods, off-policy reinforcement learning with experience replay, and various methods aimed at reducing the bias and variance of estimators. When combined, these methods result in the previously proposed ACER algorithm that gave competitive results in gaming environments. These environments, however, are fully observable and have a relatively small action set so, in this paper, we examine the application of ACER to dialogue policy optimization. We show that this method beats the current state of the art in deep learning approaches for spoken dialogue systems. This not only leads to a more sample efficient algorithm that can train faster, but also allows us to apply the algorithm in more difficult environments than before. We thus experiment with learning in a very large action space, which has two orders of magnitude more actions than previously considered. We find that ACER trains significantly faster than the current state of the art.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {2083–2097},
numpages = {15}
}

@article{10.5555/3199517.3199521,
author = {Li, Teng and Xu, Zhiyuan and Tang, Jian and Wang, Yanzhi},
title = {Model-Free Control for Distributed Stream Data Processing Using Deep Reinforcement Learning},
year = {2018},
issue_date = {February 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {6},
issn = {2150-8097},
abstract = {In this paper, we focus on general-purpose Distributed Stream Data Processing Systems (DSDPSs), which deal with processing of unbounded streams of continuous data at scale distributedly in real or near-real time. A fundamental problem in a DSDPS is the scheduling problem (i.e., assigning workload to workers/machines) with the objective of minimizing average end-to-end tuple processing time. A widely-used solution is to distribute workload evenly over machines in the cluster in a round-robin manner, which is obviously not efficient due to lack of consideration for communication delay. Model-based approaches (such as queueing theory) do not work well either due to the high complexity of the system environment.We aim to develop a novel model-free approach that can learn to well control a DSDPS from its experience rather than accurate and mathematically solvable system models, just as a human learns a skill (such as cooking, driving, swimming, etc). Specifically, we, for the first time, propose to leverage emerging Deep Reinforcement Learning (DRL) for enabling model-free control in DSDPSs; and present design, implementation and evaluation of a novel and highly effective DRL-based control framework, which minimizes average end-to-end tuple processing time by jointly learning the system environment via collecting very limited runtime statistics data and making decisions under the guidance of powerful Deep Neural Networks (DNNs). To validate and evaluate the proposed framework, we implemented it based on a widely-used DSDPS, Apache Storm, and tested it with three representative applications: continuous queries, log stream processing and word count (stream version). Extensive experimental results show 1) Compared to Storm's default scheduler and the state-of-the-art model-based method, the proposed framework reduces average tuple processing by 33.5\% and 14.0\% respectively on average. 2) The proposed framework can quickly reach a good scheduling solution during online learning, which justifies its practicability for online control in DSDPSs.},
journal = {Proc. VLDB Endow.},
month = {feb},
pages = {705–718},
numpages = {14}
}

@inproceedings{10.5555/1516744.1516844,
author = {Gosavi, Abhijit},
title = {On Step Sizes, Stochastic Shortest Paths, and Survival Probabilities in Reinforcement Learning},
year = {2008},
isbn = {9781424427086},
publisher = {Winter Simulation Conference},
abstract = {Reinforcement Learning (RL) is a simulation-based technique useful in solving Markov decision processes if their transition probabilities are not easily obtainable or if the problems have a very large number of states. We present an empirical study of (i) the effect of step-sizes (learning rules) in the convergence of RL algorithms, (ii) stochastic shortest paths in solving average reward problems via RL, and (iii) the notion of survival probabilities (downside risk) in RL. We also study the impact of step sizes when function approximation is combined with RL. Our experiments yield some interesting insights that will be useful in practice when RL algorithms are implemented within simulators.},
booktitle = {Proceedings of the 40th Conference on Winter Simulation},
pages = {525–531},
numpages = {7},
location = {Miami, Florida},
series = {WSC '08}
}

@article{10.1145/3544015,
author = {Chen, Huili and Zhang, Xinqiao and Huang, Ke and Koushanfar, Farinaz},
title = {AdaTest: Reinforcement Learning and Adaptive Sampling for On-Chip Hardware Trojan Detection},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1539-9087},
url = {https://doi.org/10.1145/3544015},
doi = {10.1145/3544015},
abstract = {This paper proposes AdaTest, a novel adaptive test pattern generation framework for efficient and reliable Hardware Trojan (HT) detection. HT is a backdoor attack that tampers with the design of victim integrated circuits (ICs). AdaTest improves the existing HT detection techniques in terms of scalability and accuracy of detecting smaller Trojans in the presence of noise and variations. To achieve high trigger coverage, AdaTest leverages Reinforcement Learning (RL) to produce a diverse set of test inputs. Particularly, we progressively generate test vectors with high ‘reward’ values in an iterative manner. In each iteration, the test set is evaluated and adaptively expanded as needed. Furthermore, AdaTest integrates adaptive sampling to prioritize test samples that provide more information for HT detection, thus reducing the number of samples while improving the samples’ quality for faster exploration. We develop AdaTest with a Software/Hardware co-design principle and provide an optimized on-chip architecture solution. AdaTest’s architecture minimizes the hardware overhead in two ways: (i) Deploying circuit emulation on programmable hardware to accelerate reward evaluation of the test input; (ii) Pipelining each computation stage in AdaTest by automatically constructing auxiliary circuit for test input generation, reward evaluation, and adaptive sampling. We evaluate AdaTest’s performance on various HT benchmarks and compare it with two prior works that use logic testing for HT detection. Experimental results show that AdaTest engenders up to two orders of test generation speedup and two orders of test set size reduction compared to the prior works while achieving the same level or higher Trojan detection rate.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {jan},
articleno = {37},
numpages = {23},
keywords = {Hardware trojan detection, software/hardware co-design, logic testing}
}

@inproceedings{10.1145/3580305.3599900,
author = {Wei, Penghui and Chen, Yongqiang and Liu, ShaoGuo and Wang, Liang and Zheng, Bo},
title = {RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599900},
doi = {10.1145/3580305.3599900},
abstract = {To increase brand awareness, many advertisers conclude contracts with advertising platforms to purchase traffic and deliver advertisements to target audiences. In a whole delivery period, advertisers desire a certain impression count for the ads, and they expect that the delivery performance is as good as possible. Advertising platforms employ real-time pacing algorithms to satisfy the demands. However, the delivery procedure is also affected by publishers. Preloading is a widely used strategy for many types of ads (e.g., video ads) to make sure that the response time for displaying is legitimate, which results in delayed impression phenomenon. In this paper, we focus on a new research problem of impression pacing for preloaded ads, and propose a Reinforcement Learning To Pace framework RLTP. It learns a pacing agent that sequentially produces selection probabilities in the whole delivery period. To jointly optimize the objectives of impression count and delivery performance, RLTP employs tailored reward estimator to satisfy guaranteed impression count, penalize over-delivery and maximize traffic value. Experiments on large-scale datasets verify that RLTP outperforms baselines by a large margin. We have deployed it online to our advertising platform, and it achieves significant uplift for delivery completion rate and click-through rate.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5204–5214},
numpages = {11},
keywords = {reinforcement learning, pacing algorithms, online advertising},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3551901.3556486,
author = {Hsu, Hung-Yun and Lin, Mark Po-Hung},
title = {Automatic Analog Schematic Diagram Generation Based on Building Block Classification and Reinforcement Learning},
year = {2022},
isbn = {9781450394864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551901.3556486},
doi = {10.1145/3551901.3556486},
abstract = {Schematic visualization is important for analog circuit designers to quickly recognize the structures and functions of transistor-level circuit netlists. However, most of the original analog design or other automatically extracted analog circuits are stored in the form of transistor-level netlists in the SPICE format. It can be error-prone and time-consuming to manually create an elegant and readable schematic from a netlist. Different from the conventional graph-based methods, this paper introduces a novel analog schematic diagram generation flow based on comprehensive building block classification and reinforcement learning. The experimental results show that the proposed method can effectively generate aesthetic analog circuit schematics with a higher building block compliance rate, and fewer numbers of wire bends and net crossings, resulting in better readability, compared with existing methods and modern tools.},
booktitle = {Proceedings of the 2022 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {43–48},
numpages = {6},
keywords = {reinforcement learning, analog circuit, schematic generation, building block, schematic visualization},
location = {Virtual Event, China},
series = {MLCAD '22}
}

@inproceedings{10.1145/3543507.3583298,
author = {Sheng, Junjie and Wang, Lu and Yang, Fangkai and Qiao, Bo and Dong, Hang and Wang, Xiangfeng and Jin, Bo and Wang, Jun and Qin, Si and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
title = {Learning Cooperative Oversubscription for Cloud by Chance-Constrained Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583298},
doi = {10.1145/3543507.3583298},
abstract = {Oversubscription is a common practice for improving cloud resource utilization. It allows the cloud service provider to sell more resources than the physical limit, assuming not all users would fully utilize the resources simultaneously. However, how to design an oversubscription policy that improves utilization while satisfying some safety constraints remains an open problem. Existing methods and industrial practices are over-conservative, ignoring the coordination of diverse resource usage patterns and probabilistic constraints. To address these two limitations, this paper formulates the oversubscription for cloud as a chance-constrained optimization problem and proposes an effective Chance-Constrained Multi-Agent Reinforcement Learning (C2MARL) method to solve this problem. Specifically, C2MARL reduces the number of constraints by considering their upper bounds and leverages a multi-agent reinforcement learning paradigm to learn a safe and optimal coordination policy. We evaluate our C2MARL on an internal cloud platform and public cloud datasets. Experiments show that our C2MARL outperforms existing methods in improving utilization () under different levels of safety constraints.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {2927–2936},
numpages = {10},
keywords = {Reinforcement Learning, Cloud Computing, Over Subscription, Multi-Agent System},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3424636.3426907,
author = {Reda, Daniele and Tao, Tianxin and van de Panne, Michiel},
title = {Learning to Locomote: Understanding How Environment Design Matters for Deep Reinforcement Learning},
year = {2020},
isbn = {9781450381710},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424636.3426907},
doi = {10.1145/3424636.3426907},
abstract = {Learning to locomote is one of the most common tasks in physics-based animation and deep reinforcement learning (RL). A learned policy is the product of the problem to be solved, as embodied by the RL environment, and the RL algorithm. While enormous attention has been devoted to RL algorithms, much less is known about the impact of design choices for the RL environment. In this paper, we show that environment design matters in significant ways and document how it can contribute to the brittle nature of many RL results. Specifically, we examine choices related to state representations, initial state distributions, reward structure, control frequency, episode termination procedures, curriculum usage, the action space, and the torque limits. We aim to stimulate discussion around such choices, which in practice strongly impact the success of RL when applied to continuous-action control problems of interest to animation, such as learning to locomote.},
booktitle = {Proceedings of the 13th ACM SIGGRAPH Conference on Motion, Interaction and Games},
articleno = {16},
numpages = {10},
keywords = {reinforcement learning, simulation environments, locomotion},
location = {Virtual Event, SC, USA},
series = {MIG '20}
}

@inproceedings{10.5555/3545946.3599069,
author = {Curry, Michael and Trott, Alexander and Phade, Soham and Bai, Yu and Zheng, Stephan},
title = {Learning Solutions in Large Economic Networks Using Deep Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Real-world economies can be modeled as a network with many heterogeneous and strategic agents. In this setting, it is very challenging to find optimal mechanisms, e.g., taxes, 1) when taking strategic best responses into account and 2) even when using restrictive assumptions, e.g., that supply always meets demand. Deep multi-agent reinforcement learning (MARL) is a natural framework to learn mechanisms and model strategic best responses, but independent MARL often collapses to trivial solutions (e.g., where nobody works) as joint exploration severely distorts rewards and constraints. Here, we show how to use structured learning curricula and GPU-accelerated simulations to find non-trivial solutions in networks with many heterogeneous agents. We validate our approach in models with 100 worker-consumers, 10 firms, and a social planner who taxes and redistributes. We use empirical best-response analyses across agent types to show that it is difficult for agents to benefit by deviating from the learned solutions. In particular, we find income and corporate taxes that achieve 15\% higher social welfare compared to baselines.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2760–2762},
numpages = {3},
keywords = {multi-agent rl, economics, tax policy},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3488933.3489031,
author = {Kang, Junhyung and Woo, Simon S},
title = {DLPNet: Dynamic Loss Parameter Network Using Reinforcement Learning for Aerial Imagery Detection},
year = {2022},
isbn = {9781450384087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488933.3489031},
doi = {10.1145/3488933.3489031},
abstract = {Object Detection on aerial imagery is a challenging task due to the following unique characteristics of the aerial imagery data: the large image size and the massive volume of data. Since the original image size from remote sensing sources is generally more colossal than the typical natural images, training detection models with a small mini-batch size are expected for the many practical, real-world applications. Furthermore, using a small mini-batch size enable the model to have better generalization performance. However, it causes an unstable learning process due to high gradient noise. In this case, reducing the learning rate enable the model to train reliably. However, a small learning rate can cause slow learning and convergence to the local minimum point also. Therefore, in this work, we propose a novel method, DLPNet, to enable robust and stable training with a small mini-batch size and various learning rates. Our model composes of an object detection model and a reinforcement learning agent. Our reinforcement learning agent extracts features from training mini-batch data and determines optimal parameters to the loss function of the object detection model. This dynamic loss function with adaptive parameters can achieve a more robust and stable learning process than the original baseline model. We demonstrate the effectiveness of our approach with a challenging object detection dataset, DOTA-v2.0. In addition, we release our code for reproducibility and to promote further research in this area1.},
booktitle = {Proceedings of the 2021 4th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {191–198},
numpages = {8},
keywords = {object detection, parameter optimization, reinforcement learning, aerial imagery},
location = {Xiamen, China},
series = {AIPR '21}
}

@inproceedings{10.1145/3622896.3622927,
author = {Jia, Xinning and Zeng, Xianzhi and Xu, Jie and Liu, Yanhong and Lou, Yangsheng and Xu, Zehui},
title = {A Microgrid Power Trading Framework Based on Blockchain and Deep Reinforcement Learning},
year = {2023},
isbn = {9798400708190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622896.3622927},
doi = {10.1145/3622896.3622927},
abstract = {With the development of renewable energy technologies and the emergence of distributed power generation devices, traditional centralized power trading markets no longer meet people's transactional needs. Peer-to-peer (P2P) electricity trading within microgrids has become the future direction. However, in distributed trading scenarios, both parties in P2P transactions lack a foundation of trust and incentive mechanisms. To address these issues, we have designed an electricity trading framework based on blockchain and deep reinforcement learning. Users utilize deep reinforcement learning for load prediction and power planning to maximize their own interests. We propose a proof-of-work (POW) consensus algorithm based on reputation values to further improve consensus efficiency and block creation time. By introducing the InterPlanetary File System (IPFS), offloaded transactions are uploaded to IPFS for storage, enabling the unloading of unnecessary information and improving the resource utilization efficiency of blocks. Our framework is beneficial for reducing the costs for blockchain users, implementing credit management for P2P e-commerce transactions, and thereby enhancing the stability and efficiency of transactions.},
booktitle = {Proceedings of the 2023 4th International Conference on Control, Robotics and Intelligent System},
pages = {190–193},
numpages = {4},
location = {Guangzhou, China},
series = {CCRIS '23}
}

@inproceedings{10.1145/3580305.3599254,
author = {Wang, Haozhe and Du, Chao and Fang, Panyan and He, LI and Wang, Liang and Zheng, Bo},
title = {Adversarial Constrained Bidding via Minimax Regret Optimization with Causality-Aware Reinforcement Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599254},
doi = {10.1145/3580305.3599254},
abstract = {The proliferation of the Internet has led to the emergence of online advertising, driven by the mechanics of online auctions. In these repeated auctions, software agents participate on behalf of aggregated advertisers to optimize for their long-term utility. To fulfill the diverse demands, bidding strategies are employed to optimize advertising objectives subject to different spending constraints. Existing approaches on constrained bidding typically rely on i.i.d. train and test conditions, which contradicts the adversarial nature of online ad markets where different parties possess potentially conflicting objectives. In this regard, we explore the problem of constrained bidding in adversarial bidding environments, which assumes no knowledge about the adversarial factors. Instead of relying on the i.i.d. assumption, our insight is to align the train distribution of environments with the potential test distribution meanwhile minimizing policy regret. Based on this insight, we propose a practical Minimax Regret Optimization (MiRO) approach that interleaves between a teacher finding adversarial environments for tutoring and a learner meta-learning its policy over the given distribution of environments. In addition, we pioneer to incorporate expert demonstrations for learning bidding strategies. Through a causality-aware policy design, we improve upon MiRO by distilling knowledge from the experts. Extensive experiments on both industrial data and synthetic data show that our method, MiRO with Causality-aware reinforcement Learning (MiROCL), outperforms prior methods by over 30\%.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2314–2325},
numpages = {12},
keywords = {causality, reinforcement learning, constrained bidding},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@article{10.14778/3199517.3199521,
author = {Li, Teng and Xu, Zhiyuan and Tang, Jian and Wang, Yanzhi},
title = {Model-Free Control for Distributed Stream Data Processing Using Deep Reinforcement Learning},
year = {2018},
issue_date = {February 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3199517.3199521},
doi = {10.14778/3199517.3199521},
abstract = {In this paper, we focus on general-purpose Distributed Stream Data Processing Systems (DSDPSs), which deal with processing of unbounded streams of continuous data at scale distributedly in real or near-real time. A fundamental problem in a DSDPS is the scheduling problem (i.e., assigning workload to workers/machines) with the objective of minimizing average end-to-end tuple processing time. A widely-used solution is to distribute workload evenly over machines in the cluster in a round-robin manner, which is obviously not efficient due to lack of consideration for communication delay. Model-based approaches (such as queueing theory) do not work well either due to the high complexity of the system environment.We aim to develop a novel model-free approach that can learn to well control a DSDPS from its experience rather than accurate and mathematically solvable system models, just as a human learns a skill (such as cooking, driving, swimming, etc). Specifically, we, for the first time, propose to leverage emerging Deep Reinforcement Learning (DRL) for enabling model-free control in DSDPSs; and present design, implementation and evaluation of a novel and highly effective DRL-based control framework, which minimizes average end-to-end tuple processing time by jointly learning the system environment via collecting very limited runtime statistics data and making decisions under the guidance of powerful Deep Neural Networks (DNNs). To validate and evaluate the proposed framework, we implemented it based on a widely-used DSDPS, Apache Storm, and tested it with three representative applications: continuous queries, log stream processing and word count (stream version). Extensive experimental results show 1) Compared to Storm's default scheduler and the state-of-the-art model-based method, the proposed framework reduces average tuple processing by 33.5\% and 14.0\% respectively on average. 2) The proposed framework can quickly reach a good scheduling solution during online learning, which justifies its practicability for online control in DSDPSs.},
journal = {Proc. VLDB Endow.},
month = {feb},
pages = {705–718},
numpages = {14}
}

@inproceedings{10.5555/3545946.3598962,
author = {Li, Zun and Lanctot, Marc and McKee, Kevin R. and Marris, Luke and Gemp, Ian and Hennes, Daniel and Larson, Kate and Bachrach, Yoram and Wellman, Michael P. and Muller, Paul},
title = {Search-Improved Game-Theoretic Multiagent Reinforcement Learning in General and Negotiation Games},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multiagent reinforcement learning (MARL) has benefited significantly from population-based and game-theoretic training regimes. One approach, Policy-Space Response Oracles (PSRO), employs standard reinforcement learning to compute response policies via approximate best responses and combines them via meta-strategy selection. We augment PSRO by adding a novel search procedure with generative sampling of world states, and introduce two new meta-strategy solvers based on the Nash bargaining solution. We evaluate PSRO's ability to compute approximate Nash equilibrium, and its performance in negotiation games: Colored Trails and Deal-or-no-Deal. We conduct behavioral studies where human participants negotiate with our agents (N = 346). Search with generative modeling finds stronger policies during both training time and test time, enables online Bayesian co-player prediction, and can produce agents that achieve comparable social welfare negotiating with humans as humans trading among themselves.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2445–2447},
numpages = {3},
keywords = {multiagent, alphazero, negotiation games, nash bargaining solution, reinforcement learning, policy-space response oracles},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.14778/3184470.3184474,
author = {Li, Teng and Xu, Zhiyuan and Tang, Jian and Wang, Yanzhi},
title = {Model-Free Control for Distributed Stream Data Processing Using Deep Reinforcement Learning},
year = {2018},
issue_date = {February 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3184470.3184474},
doi = {10.14778/3184470.3184474},
abstract = {In this paper, we focus on general-purpose Distributed Stream Data Processing Systems (DSDPSs), which deal with processing of unbounded streams of continuous data at scale distributedly in real or near-real time. A fundamental problem in a DSDPS is the scheduling problem (i.e., assigning workload to workers/machines) with the objective of minimizing average end-to-end tuple processing time. A widely-used solution is to distribute workload evenly over machines in the cluster in a round-robin manner, which is obviously not efficient due to lack of consideration for communication delay. Model-based approaches (such as queueing theory) do not work well either due to the high complexity of the system environment.We aim to develop a novel model-free approach that can learn to well control a DSDPS from its experience rather than accurate and mathematically solvable system models, just as a human learns a skill (such as cooking, driving, swimming, etc). Specifically, we, for the first time, propose to leverage emerging Deep Reinforcement Learning (DRL) for enabling model-free control in DSDPSs; and present design, implementation and evaluation of a novel and highly effective DRL-based control framework, which minimizes average end-to-end tuple processing time by jointly learning the system environment via collecting very limited runtime statistics data and making decisions under the guidance of powerful Deep Neural Networks (DNNs). To validate and evaluate the proposed framework, we implemented it based on a widely-used DSDPS, Apache Storm, and tested it with three representative applications: continuous queries, log stream processing and word count (stream version). Extensive experimental results show 1) Compared to Storm's default scheduler and the state-of-the-art model-based method, the proposed framework reduces average tuple processing by 33.5\% and 14.0\% respectively on average. 2) The proposed framework can quickly reach a good scheduling solution during online learning, which justifies its practicability for online control in DSDPSs.},
journal = {Proc. VLDB Endow.},
month = {feb},
pages = {705–718},
numpages = {14}
}

@inproceedings{10.1145/3485447.3512234,
author = {Lee, Soyoung and Wi, Seongil and Son, Sooel},
title = {Link: Black-Box Detection of Cross-Site Scripting Vulnerabilities Using Reinforcement Learning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512234},
doi = {10.1145/3485447.3512234},
abstract = {Black-box web scanners have been a prevalent means of performing penetration testing to find reflected cross-site scripting (XSS) vulnerabilities. Unfortunately, off-the-shelf black-box web scanners suffer from unscalable testing as well as false negatives that stem from a testing strategy that employs fixed attack payloads, thus disregarding the exploitation of contexts to trigger vulnerabilities. To this end, we propose a novel method of adapting attack payloads to a target reflected XSS vulnerability using reinforcement learning (RL). We present Link, a general RL framework whose states, actions, and a reward function are designed to find reflected XSS vulnerabilities in a black-box and fully automatic manner. Link finds 45, 213, and 60 vulnerabilities with no false positives in Firing-Range, OWASP, and WAVSEP benchmarks, respectively, outperforming state-of-the-art web scanners in terms of finding vulnerabilities and ending testing campaigns earlier. Link also finds 43 vulnerabilities in 12 real-world applications, demonstrating the promising efficacy of using RL in finding reflected XSS vulnerabilities.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {743–754},
numpages = {12},
keywords = {cross-site scripting, penetration testing;, reinforcement learning},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3546000.3546002,
author = {Ling, Shuhao and Gao, Huaien and Chen, Jiasong and Liu, Dawei},
title = {Reinforcement Learning Enabled Throughput Optimization for Interconnection Networks of Interposer-Based System},
year = {2022},
isbn = {9781450396295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546000.3546002},
doi = {10.1145/3546000.3546002},
abstract = {Silicon interposer enables 2.5D stacking of memory chips and processor chips to pursue advanced memory access performance. In interposer-based system, different traffic transfers through network-on-interposer (NoI) lays on the silicon interposer which makes NoI throughput important to transmit the mass of data. However, the performance of the existing topology varies under different traffic patterns. In this paper, we use reinforcement learning (RL) is adapted to further optimize the throughput of NoI in various traffic. We design a dedicated RL framework for NoI enviroment to enable performance improvement. Three algorithms are used to maximize the throughput as well as reward in the RL Model. Simulation results demonstrate that the proposed RL approach provide higher throughput both in memory traffic and coherence traffic.},
booktitle = {Proceedings of the 6th International Conference on High Performance Compilation, Computing and Communications},
pages = {12–18},
numpages = {7},
location = {Jilin, China},
series = {HP3C '22}
}

@inproceedings{10.1145/3522749.3523075,
author = {Huo, Yanpeng and Liang, Yuning},
title = {Offline Reinforcement Learning Application in Robotic Manipulation with a COG Method Case},
year = {2022},
isbn = {9781450385916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522749.3523075},
doi = {10.1145/3522749.3523075},
abstract = {Artificial intelligence now has different applications in various industrial fields. Reinforcement learning (RL) is one of the hot topics in the artificial intelligence, also in robotics. It is an important learning method in the field of robotic manipulation. The training policies of reinforcement learning can be divided into online learning policy and offline learning policy. Besides, the reinforcement learning algorithm of offline policy has great potential in transforming large data sets into powerful decision engine. To solve the problem that most of robot applications involve collecting data from scratch for each new task, offline learning combined with online learning is to make the training more efficient and convenient. The aim of this paper is to clearly introduce the application of offline reinforcement learning in the field of robotic manipulation. The basic formulation of reinforcement learning includes two points: First, it introduces Markov Decision Process and one of method of solution – policy gradients. Then through analyzing an application of offline learning in the field of robotic manipulation - COG algorithm, this paper analyzes the process of offline learning combining the prior data to learn new robotic skills and uses this method to solve specific tasks of robotic, such as the problems of sample efficiency. The results show that the offline learning policy has important research value in the field of robotic manipulation by reducing training time and make process efficient, and it fully embodies its advantages in solving the problems of robotic sample efficiency.},
booktitle = {Proceedings of the 6th International Conference on Control Engineering and Artificial Intelligence},
pages = {62–66},
numpages = {5},
keywords = {Reinforcement learning, Prior experience, Robotic manipulation},
location = {Virtual Event, Japan},
series = {CCEAI '22}
}

@inproceedings{10.5555/3586210.3586243,
author = {Malhotra, Kanupriya and Lim, Zhi Jun and Alam, Sameer},
title = {A Multi-Agent Reinforcement Learning Approach for System-Level Flight Delay Absorption},
year = {2023},
publisher = {IEEE Press},
abstract = {With increasing air traffic, there is an ever-growing need for Air Traffic Controllers (ATCO) to efficiently manage traffic and congestion. Congestion often leads to increased delays in the Terminal Maneuvering Area (TMA), causing large amounts of fuel burn and detrimental environmental impacts. Approaches such as the Extended Arrival Manager (E-AMAN) propose solutions to absorb such delays, whereby flights are scheduled much before they enter the TMA. However, such an approach requires a speed management system where flights can coordinate to absorb system-level delays in their en-route phase. This paper proposes a Multi-Agent System (MAS) approach using Deep Reinforcement Learning to model and train flights as agents which can coordinate with each other to effectively absorb system-level delays. The simulations utilize Multi-Agent POsthumous Credit Assignment in Unity and test two reward approaches. Initial findings reveal an average of 3.3 minutes of system-level delay absorptions from a required delay of 4 minutes.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {406–417},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.5555/3539845.3540165,
author = {Chen, Lin and Li, Xiao and Xu, Jiang},
title = {Improve the Stability and Robustness of Power Management through Model-Free Deep Reinforcement Learning},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Achieving high performance with low energy consumption has become a primary design objective in multi-core systems. Recently, power management based on reinforcement learning has shown great potential in adapting to dynamic environments without much prior knowledge. However, conventional Q-learning (QL) algorithms adopted in most existing works encounter serious problems about scalability, instability, and overestimation. In this paper, we present a deep reinforcement learning-based approach to improve the stability and robustness of power management while reducing the energy-delay product (EDP) under user-specified performance requirements. The comprehensive status of the system is monitored periodically, making our controller sensitive to environmental change. To further improve the learning effectiveness, knowledge sharing among multiple devices is implemented in our approach. Experimental results on multiple realistic applications show that the proposed method can reduce the instability up to 68\% compared with QL. Through knowledge sharing among multiple devices, our federated approach achieves around 4.8\% EDP improvement over QL on average.},
booktitle = {Proceedings of the 2022 Conference \&amp; Exhibition on Design, Automation \&amp; Test in Europe},
pages = {1371–1376},
numpages = {6},
keywords = {multicore system, power management, deep reinforcement learning, experience replay, federated learning},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3092703.3092709,
author = {Spieker, Helge and Gotlieb, Arnaud and Marijan, Dusica and Mossige, Morten},
title = {Reinforcement Learning for Automatic Test Case Prioritization and Selection in Continuous Integration},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3092709},
doi = {10.1145/3092703.3092709},
abstract = {Testing in Continuous Integration (CI) involves test case prioritization, selection, and execution at each cycle. Selecting the most promising test cases to detect bugs is hard if there are uncertainties on the impact of committed code changes or, if traceability links between code and tests are not available. This paper introduces Retecs, a new method for automatically learning test case selection and prioritization in CI with the goal to minimize the round-trip time between code commits and developer feedback on failed test cases. The Retecs method uses reinforcement learning to select and prioritize test cases according to their duration, previous last execution and failure history. In a constantly changing environment, where new test cases are created and obsolete test cases are deleted, the Retecs method learns to prioritize error-prone test cases higher under guidance of a reward function and by observing previous CI cycles. By applying Retecs on data extracted from three industrial case studies, we show for the first time that reinforcement learning enables fruitful automatic adaptive test case selection and prioritization in CI and regression testing.},
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {12–22},
numpages = {11},
keywords = {Test case selection, Continuous Integration, Reinforcement Learning, Test case prioritization, Machine Learning, Regression testing},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1145/3426826.3426835,
author = {Rais Mart\'{\i}nez, Jasmina and Aznar Gregori, Fidel},
title = {Comparison of Evolutionary Strategies for Reinforcement Learning in a Swarm Aggregation Behaviour},
year = {2020},
isbn = {9781450388344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426826.3426835},
doi = {10.1145/3426826.3426835},
abstract = {This article studies the performance of different evolutionary strategies for deep reinforcement learning policy optimization. The policy will be centred in an important swarm robotic task: the aggregation of simple robots in the environment. The main inspiration for robotic swarm comes from the observation of social animals. Ants, bees, birds, and fish are some examples of how simple individuals can succeed when they gather in groups. In addition, is important to highlight that aggregation may be considered as a previous requirement to tackle another tasks.Due the design of a swarm behaviour is a comprehensive process of experimentation, one of the current solutions is learn a policy able to control a robot. Gradient descent techniques have demonstrated their learning power in the field of neural networks and reinforcement learning, among others. But some of these techniques are difficult to be applied in the field of robotics because the requirements needed to calculate the gradient to be informative are not met. For that reason, in this article we are going to use and compare the evolutionary strategies CMA-ES, PEPG, SES, GA y OpenAI-ES. A fast simulator, based in the differential robot Mbot Ranger, will be used. Once the aggregation task is learned we will compare each strategy for different swarm sizes to analyse the convergence time, the quality of the policies, its scalability and its capacity to generalize.},
booktitle = {Proceedings of the 2020 3rd International Conference on Machine Learning and Machine Intelligence},
pages = {40–45},
numpages = {6},
keywords = {Evolutionary Strategies, Evolutionary Robotics, Swarm Robotics, Deep Reinforcement Learning, Swarm Intelligence},
location = {Hangzhou, China},
series = {MLMI '20}
}

@inproceedings{10.1145/3301326.3301375,
author = {Wu, Qianlin and Zhu, Dandan and Liu, Yi and Du, Aimin and Chen, Dong and Ye, Zhihui},
title = {Comprehensive Control System for Gathering Pipe Network Operation Based on Reinforcement Learning},
year = {2018},
isbn = {9781450365536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301326.3301375},
doi = {10.1145/3301326.3301375},
abstract = {In the transmission process of crude oil gathering system, water-assisted heat transfer is often used to avoid wax formation, and pipeline temperature and pressure control are often controlled manually. In order to improve control efficiency and save labor cost. In this paper, we propose a DQN-based algorithm. The intensive learning model completes the temperature and pressure control in the pipeline. At the same time, because these two parameters have strong coupling, which affects the global control, this paper focuses on the joint optimization of valve opening and heating furnace and pressure pump. Finally, in order to verify the effectiveness of the system, the simulation control experiment is adopted. The test results show that the system control effect is excellent and the robustness is good.},
booktitle = {Proceedings of the 2018 VII International Conference on Network, Communication and Computing},
pages = {34–39},
numpages = {6},
keywords = {Reinforcement learning, Gathering Pipe Network, Deep Q Network, Intelligent Control System},
location = {Taipei City, Taiwan},
series = {ICNCC '18}
}

@inproceedings{10.5555/3586210.3586437,
author = {Craggs, Dustin and Lee, Kin Leong and Radenovic, Vanja and Campbell, Benjamin and Szabo, Claudia},
title = {Use of Reinforcement Learning for Prioritizing Communications in Contested and Dynamic Environments},
year = {2023},
publisher = {IEEE Press},
abstract = {Systems operating in military operations and crisis situations usually do so in contested and dynamic environments with poor and unreliable network conditions. Individual nodes within these systems usually have an incomplete, local and changing view of the system and its operating environment, and as such optimizing how nodes communicate in order to improve decision making is critical. In this paper, we propose the integration of reinforcement learning algorithms with the SMARTNet middleware, a middleware that prioritizes and controls messages sent by each node, allowing it to determine the best priority for each message type. We experiment with both direct and indirect prioritisation approaches, where the reinforcement learning dissemination system determines the specific priority of a message on its arrival or upon its sending respectively. Our experimental analysis show significant improvements over the baseline in some of the high congestion scenarios but also highlights several avenues for future work.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2699–2711},
numpages = {13},
location = {Singapore, Singapore},
series = {WSC '22}
}

