@article{10.1109/TNET.2021.3128836,
author = {Krishna Moorthy, Sabarish and Mcmanus, Maxwell and Guan, Zhangyu},
title = {ESN Reinforcement Learning for Spectrum and Flight Control in THz-Enabled Drone Networks},
year = {2021},
issue_date = {April 2022},
publisher = {IEEE Press},
volume = {30},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3128836},
doi = {10.1109/TNET.2021.3128836},
abstract = {Terahertz (THz)-band communications have been envisioned as a key technology to support ultra-high-data-rate applications in 5G-beyond (or 6G) wireless networks. Compared to the microwave and mmWave bands, the main challenges with the THz band are in its i) large path loss hence limited network coverage and ii) visible-light-like propagation characteristics hence poor support of mobility in blockage-rich environments. This paper studies quantitatively the applicability of THz-band communications in blockage-rich mobile environments, focusing on a new network scenario called <italic>FlyTera</italic>. In <italic>FlyTera</italic>, a set of hotspots mounted on flying drones collaboratively provide data streaming services to ground users, in the microwave, mmWave and THz bands. We first provide a mathematical formulation of the <italic>FlyTera</italic> control problem, where the objective is to maximize the network spectral efficiency by jointly controlling the flight of the drone hotspots, their association to the ground users, and the spectrum bands used by the users. To solve the resulting problem, which is shown to be a mixed integer nonlinear nonconvex programming (MINLP) problem, we design distributed solution algorithms based on a combination of echo state learning and reinforcement learning. An extensive simulation campaign is then conducted with SimBAG, a newly developed <underline>Sim</underline>ulator of <underline>B</underline>roadband <underline>A</underline>erial-<underline>G</underline>round wireless networks. It is shown that no single spectrum band can meet the requirements of high data rate and wide coverage simultaneously. Moreover, from the network-level point of view, THz-band communications can significantly benefit from the mobility of the flying drones, and on average <inline-formula> <tex-math notation="LaTeX">$4 - 6$ </tex-math></inline-formula> times <italic>higher (rather than lower)</italic> throughput can be achieved in mobile than in static environments.},
journal = {IEEE/ACM Trans. Netw.},
month = {nov},
pages = {782–795},
numpages = {14}
}

@inproceedings{10.1145/3551901.3556474,
author = {Uhlmann, Yannick and Essich, Michael and Bramlage, Lennart and Scheible, J\"{u}rgen and Curio, Crist\'{o}bal},
title = {Deep Reinforcement Learning for Analog Circuit Sizing with an Electrical Design Space and Sparse Rewards},
year = {2022},
isbn = {9781450394864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551901.3556474},
doi = {10.1145/3551901.3556474},
abstract = {There is still a great reliance on human expert knowledge during the analog integrated circuit sizing design phase due to its complexity and scale, with the result that there is a very low level of automation associated with it. Current research shows that reinforcement learning is a promising approach for addressing this issue. Similarly, it has been shown that the convergence of conventional optimization approaches can be improved by transforming the design space from the geometrical domain into the electrical domain. Here, this design space transformation is employed as an alternative action space for deep reinforcement learning agents. The presented approach is based entirely on reinforcement learning, whereby agents are trained in the craft of analog circuit sizing without explicit expert guidance. After training and evaluating agents on circuits of varying complexity, their behavior when confronted with a different technology, is examined, showing the applicability, feasibility as well as transferability of this approach.},
booktitle = {Proceedings of the 2022 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {21–26},
numpages = {6},
keywords = {reinforcement learning, neural networks, analog circuit sizing},
location = {Virtual Event, China},
series = {MLCAD '22}
}

@inproceedings{10.1145/3520304.3534039,
author = {Gabor, Thomas and Zorn, Maximilian and Linnhoff-Popien, Claudia},
title = {The Applicability of Reinforcement Learning for the Automatic Generation of State Preparation Circuits},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3534039},
doi = {10.1145/3520304.3534039},
abstract = {State preparation is currently the only means to provide input data for quantum algorithm, but finding the shortest possible sequence of gates to prepare a given state is not trivial. We approach this problem using reinforcement learning (RL), first on an agent that is trained to only prepare a single fixed quantum state. Despite the overhead of training a whole network to just produce one single data point, gradient-based backpropagation appears competitive to genetic algorithms in this scenario and single state preparation thus seems a worthwhile task. In a second case we then train a single network to prepare arbitrary quantum states to some degree of success, despite a complete lack of structure in the training data set. In both cases we find that training is severely improved by using QR decomposition to automatically map the agents' outputs to unitary operators to solve the problem of sparse rewards that usually makes this task challenging.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {2196–2204},
numpages = {9},
keywords = {neural network, quantum computing, reinforcement learning, actor/critic, state preparation, circuit design},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.5555/3408352.3408750,
author = {Dey, Somdip and Singh, Amit Kumar and Wang, Xiaohang and McDonald-Maier, Klaus},
title = {User Interaction Aware Reinforcement Learning for Power and Thermal Efficiency of CPU-GPU Mobile MPSoCs},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Mobile user's usage behaviour changes throughout the day and the desirable Quality of Service (QoS) could thus change for each session. In this paper, we propose a QoS aware agent to monitor mobile user's usage behaviour to find the target frame rate, which satisfies the desired user's QoS, and applies reinforcement learning based DVFS on a CPU-GPU MPSoC to satisfy the frame rate requirement. Experimental study on a real Exynos hardware platform shows that our proposed agent is able to achieve a maximum of 50\% power saving and 29\% reduction in peak temperature compared to stock Android's power saving scheme. It also outperforms the existing state-of-the-art power and thermal management scheme by 41\% and 19\%, respectively.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {1728–1733},
numpages = {6},
keywords = {agent system, reinforcement learning, user behaviour, GPU, mobile, smartphone, MPSoCs, user interaction, power optimization, machine learning, thermal optimization, CPU},
location = {Grenoble, France},
series = {DATE '20}
}

@inproceedings{10.5555/3535850.3536008,
author = {Xue, Wanqi and Qiu, Wei and An, Bo and Rabinovich, Zinovi and Obraztsova, Svetlana and Yeo, Chai Kiat},
title = {Mis-Spoke or Mis-Lead: Achieving Robustness in Multi-Agent Communicative Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent studies in multi-agent communicative reinforcement learning (MACRL) have demonstrated that multi-agent coordination can be greatly improved by allowing communication between agents. Meanwhile, adversarial machine learning (ML) have shown that ML models are vulnerable to attacks. Despite the increasing concern about the robustness of ML algorithms, how to achieve robust communication in multi-agent reinforcement learning has been largely neglected. In this paper, we systematically explore the problem of adversarial communication in MACRL. Our main contributions are threefold. First, we propose an effective method to perform attacks in MACRL, by learning a model to generate optimal malicious messages. Second, we develop a defence method based on message reconstruction, to maintain multi-agent coordination under message attacks. Third, we formulate the adversarial communication problem as a two-player zero-sum game and propose a game-theoretical method R-MACRL to improve the worst-case defending performance. Empirical results demonstrate that many state-of-the-art MACRL methods are vulnerable to message attacks, and our method can significantly improve their robustness.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1418–1426},
numpages = {9},
keywords = {adversarial reinforcement learning, multi-agent reinforcement learning, robust reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3534678.3539095,
author = {Sadeghi Eshkevari, Soheil and Tang, Xiaocheng and Qin, Zhiwei and Mei, Jinhan and Zhang, Cheng and Meng, Qianying and Xu, Jia},
title = {Reinforcement Learning in the Wild: Scalable RL Dispatching Algorithm Deployed in Ridehailing Marketplace},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539095},
doi = {10.1145/3534678.3539095},
abstract = {In this study, a scalable and real-time dispatching algorithm based on reinforcement learning is proposed and for the first time, is deployed in large scale. Current dispatching methods in ridehailing platforms are dominantly based on myopic or rule-based non-myopic approaches. Reinforcement learning enables dispatching policies that are informed of historical data and able to employ the learned information to optimize returns of expected future trajectories. Previous studies in this field yielded promising results, yet have left room for further improvements in terms of performance gain, self-dependency, transferability, and scalable deployment mechanisms. The present study proposes a standalone RL-based dispatching solution that is equipped with multiple novel mechanisms to ensure robust and efficient on-policy learning and inference while being adaptable for full-scale deployment. In particular, a new form of value updating based on temporal difference is proposed that is more adapted to the inherent uncertainty of the problem. For the driver-order assignment problem, a customized utility function is proposed that when tuned based on the statistics of the market, results in remarkable performance improvement and interpretability. In addition, for reducing the risk of cancellation after drivers' assignment, an adaptive graph pruning strategy based on the multi-arm bandit problem is introduced. The method is evaluated using offline simulation with real data and yields notable performance improvement. In addition, the algorithm is deployed online in multiple cities under DiDi's operation for A/B testing and more recently, is launched in one of the major international markets as the primary mode of dispatch. The deployed algorithm shows over 1.3\% improvement in total driver income from A/B testing. In addition, by causal inference analysis, as much as 5.3\% improvement in major performance metrics is detected after full-scale deployment.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3838–3848},
numpages = {11},
keywords = {dispatch, reinforcement learning, ridehailing, multi-agent},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3487075.3487137,
author = {Tong, Le and Chen, Yangyi and Zhou, Xin and Sun, Yifu},
title = {QoE-Fairness Tradeoff Scheme for Dynamic Spectrum Allocation Based on Deep Reinforcement Learning},
year = {2021},
isbn = {9781450389853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487075.3487137},
doi = {10.1145/3487075.3487137},
abstract = {In order to meet the tradeoff of QoE(quality of experience)-Fairness when spectrum resources are insufficient, it is necessary to study the dynamic spectrum allocation problem, especially in the scenario where a base station who acts as a single agent wishes to reliably communicate with the multiple users by centrally managing the spectrum resources. To overcome the fact that user behavior and environment are unknown and dynamic, this paper modeled the dynamic spectrum allocation as an optimization problem, and put forward a dynamic spectrum allocation strategy which based on adaptive deep Q-learning network (ADQN). On this basis, a new reward function is designed to drive the learning process which considering different types of user's communication needs, and a priority experience replay strategy is proposed to accelerate network training speed which based on reducing time error. Moreover, simulation results show that the proposed strategy can accelerate the convergence speed of ADQN and improve the rationality and effectiveness of dynamic spectrum allocation.},
booktitle = {Proceedings of the 5th International Conference on Computer Science and Application Engineering},
articleno = {62},
numpages = {7},
keywords = {fairness, quality of experience, deep reinforcement learning, dynamic spectrum allocation},
location = {Sanya, China},
series = {CSAE '21}
}

@inproceedings{10.1145/3543507.3583523,
author = {Wang, Xintong and Ma, Gary Qiurui and Eden, Alon and Li, Clara and Trott, Alexander and Zheng, Stephan and Parkes, David},
title = {Platform Behavior under Market Shocks: A Simulation Framework and Reinforcement-Learning Based Study},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583523},
doi = {10.1145/3543507.3583523},
abstract = {We study the behavior of an economic platform (e.g., Amazon, Uber Eats, Instacart) under shocks, such as COVID-19 lockdowns, and the effect of different regulation considerations. To this end, we develop a multi-agent simulation environment of a platform economy in a multi-period setting where shocks may occur and disrupt the economy. Buyers and sellers are heterogeneous and modeled as economically-motivated agents, choosing whether or not to pay fees to access the platform. We use deep reinforcement learning to model the fee-setting and matching behavior of the platform, and consider two major types of regulation frameworks: (1)&nbsp;taxation policies and (2)&nbsp;platform fee restrictions. We offer a number of simulated experiments that cover different market settings and shed light on regulatory tradeoffs. Our results show that while many interventions are ineffective with a sophisticated platform actor, we identify a particular kind of regulation—fixing fees to the optimal, no-shock fees while still allowing a platform to choose how to match buyers and sellers—as holding promise for promoting the efficiency and resilience of the economic system.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {3592–3602},
numpages = {11},
keywords = {Platform economy, multi-agent simulation, agent-based modeling, reinforcement learning, fee setting, market shock, matching},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3501409.3501570,
author = {wenwen, Xiao},
title = {Application Research of End to End Behavior Decision Based on Deep Reinforcement Learning},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501570},
doi = {10.1145/3501409.3501570},
abstract = {At present, the control method of driverless vehicle mainly adopts artificial rule making and behavior decision-making, which is difficult to adapt to the new scene. To solve this problem, this paper proposes an end-to-end behavior decision-making method, which uses deep reinforcement learning to interact with the environment and learn the decision model. The model has good generalization. The main reinforcement learning algorithms used in this paper are PPO, SAC, A2C, DDPG and TRPO algorithm, etc. This paper is based on the unity 3D virtual engine to build the scene of vehicle search target. The experimental results show that DDPG algorithm has better effect, the better generalization performance, the shortest time to find the target and the fastest training speed.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {889–894},
numpages = {6},
keywords = {Generalization, Reinforcement learning, End-to-end, Unity},
location = {Xiamen, China},
series = {EITCE '21}
}

@inproceedings{10.1145/3469877.3497700,
author = {Liu, Hao and Yan, Jinmeng and Zhou, Yuandong},
title = {A Reinforcement Learning-Based Reward Mechanism for Molecule Generation That Introduces Activity Information},
year = {2022},
isbn = {9781450386074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469877.3497700},
doi = {10.1145/3469877.3497700},
abstract = {In this paper, we propose an activity prediction method for molecule generation based on the framework of reinforcement learning. The method is used as a scoring module for the molecule generation process. By introducing information about known active molecules for specific set of target conformations, it overcomes the traditional molecular optimization strategy where the method only uses computable properties. Eventually, our prediction method improves the quality of the generated molecules. The prediction method utilized fusion features that consist of traditional countable properties of molecules such as atomic number and the binding property of the molecule to the target. Furthermore, this paper designs a ultra large-scale molecular docking parallel computing method, which greatly improves the performance of the molecular docking [1] scoring process. The computing method makes the high-quality docking computing to predict molecular activity possible. The final experimental result shows that the molecule generation model using the prediction method can produce nearly twenty percent active molecules, which shows that the method proposed in this paper can effectively improve the performance of molecule generation.},
booktitle = {ACM Multimedia Asia},
articleno = {82},
numpages = {5},
keywords = {deep reinforcement learning, reward mechanism, drug design, molecule generation},
location = {Gold Coast, Australia},
series = {MMAsia '21}
}

@inproceedings{10.1145/3580305.3599934,
author = {Wang, Lu and Zhang, Chaoyun and Ding, Ruomeng and Xu, Yong and Chen, Qihang and Zou, Wentao and Chen, Qingjun and Zhang, Meng and Gao, Xuedong and Fan, Hao and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
title = {Root Cause Analysis for Microservice Systems via Hierarchical Reinforcement Learning from Human Feedback},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599934},
doi = {10.1145/3580305.3599934},
abstract = {In microservice systems, the identification of root causes of anomalies is imperative for service reliability and business impact. This process is typically divided into two phases: (i)constructing a service dependency graph that outlines the sequence and structure of system components that are invoked, and (ii) localizing the root cause components using the graph, traces, logs, and Key Performance Indicators (KPIs) such as latency. However, both phases are not straightforward due to the highly dynamic and complex nature of the system, particularly in large-scale commercial architectures like Microsoft Exchange.In this paper, we propose a new framework that employs Hierarchical Reinforcement Learning from Human Feedback (HRLHF) to address these challenges. Our framework leverages the static topology of the microservice system and efficiently employs the feedback of engineers to reduce uncertainty in the discovery of the service dependency graph. The framework utilizes reinforcement learning to reduce the number of queries required from O(N2) to O(1), enabling the construction of the dependency graph with high accuracy and minimal human effort. Additionally, we extend the discovered dependency graphs to window causal graphs that capture the characteristics of time series over a specified time period, resulting in improved root cause analysis accuracy and robustness. Evaluations on both real datasets from Microsoft Exchange and synthetic datasets with injected anomalies demonstrate superior performance on various metrics compared to state-of-the-art methods. It is worth mentioning that, our framework has been integrated as a crucial component in Microsoft M365 Exchange service.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5116–5125},
numpages = {10},
keywords = {reinforcement learning from human feedback, root cause analysis, causal discovery},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3583131.3590441,
author = {Tran, Hai-Long and Doan, Long and Luong, Ngoc Hoang and Binh, Huynh Thi Thanh},
title = {A Two-Stage Multi-Objective Evolutionary Reinforcement Learning Framework for Continuous Robot Control},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590441},
doi = {10.1145/3583131.3590441},
abstract = {Real-world continuous control problems often require optimizing for multiple conflicting objectives. Various works in multi-objective reinforcement learning have been conducted to tackle such issues and obtained impressive performance. At the same time, evolutionary algorithms (EAs), which are extensively used in multi-objective optimization, have recently been demonstrated their competitiveness to RL algorithms, including multi-objective control for environments with discrete action spaces. However, using EAs for multi-objective continuous robot control is still an under-explored topic. For the single-objective setting, the Proximal Distilled Evolutionary Reinforcement Learning (PDERL) framework succeeds in combining the robustness of EA and the efficiency of RL methods. In this work, we bring the strengths of PDERL to the multiobjective realm to create the novel multi-objective PDERL framework called MOPDERL that consists of a warm-up stage and an evolution stage. In particular, MOPDERL collaboratively optimizes policies for each separate objective in the warm-up stage, and then exchanges that knowledge for further policy improvement during the multi-objective evolution stage. We benchmark MOPDERL on six MuJoCo robot locomotion environments, which have been modified for the multi-objective context. The results show that MOPDERL produces better-quality Pareto fronts and higher metric scores than the state-of-the-art PGMORL algorithm across five out of six environments.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {577–585},
numpages = {9},
keywords = {multi-objective optimization, reinforcement learning, genetic algorithms},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@article{10.5555/3586589.3586892,
author = {Gatti, Alice and Hu, Zhixiong and Smidt, Tess and Ng, Esmond G. and Ghysels, Pieter},
title = {Graph Partitioning and Sparse Matrix Ordering Using Reinforcement Learning and Graph Neural Networks},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {We present a novel method for graph partitioning, based on reinforcement learning and graph convolutional neural networks. Our approach is to recursively partition coarser representations of a given graph. The neural network is implemented using SAGE graph convolution layers, and trained using an advantage actor critic (A2C) agent. We present two variants, one for finding an edge separator that minimizes the normalized cut or quotient cut, and one that finds a small vertex separator. The vertex separators are then used to construct a nested dissection ordering to permute a sparse matrix so that its triangular factorization will incur less fill-in. The partitioning quality is compared with partitions obtained using METIS and SCOTCH, and the nested dissection ordering is evaluated in the sparse solver SuperLU. Our results show that the proposed method achieves similar partitioning quality as METIS, SCOTCH and spectral partitioning. Furthermore, the method generalizes across different classes of graphs, and works well on a variety of graphs from the SuiteSparse sparse matrix collection.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {303},
numpages = {28},
keywords = {machine learning for scientific computing, graph partitioning, reinforcement learning, sparse matrix ordering, graph neural networks}
}

@inproceedings{10.1145/3580305.3599811,
author = {Mao, Xiaowei and Wen, Haomin and Zhang, Hengrui and Wan, Huaiyu and Wu, Lixia and Zheng, Jianbin and Hu, Haoyuan and Lin, Youfang},
title = {DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599811},
doi = {10.1145/3580305.3599811},
abstract = {Pick-up and Delivery Route Prediction (PDRP), which aims to estimate the future service route of a worker given his current task pool, has received rising attention in recent years. Deep neural networks based on supervised learning have emerged as the dominant model for the task because of their powerful ability to capture workers' behavior patterns from massive historical data. Though promising, they fail to introduce the non-differentiable test criteria into the training process, leading to a mismatch in training and test criteria. Which considerably trims down their performance when applied in practical systems. To tackle the above issue, we present the first attempt to generalize Reinforcement Learning (RL) to the route prediction task, leading to a novel RL-based framework called DRL4Route. It combines the behavior-learning abilities of previous deep learning models with the non-differentiable objective optimization ability of reinforcement learning. DRL4Route can serve as a plug-and-play component to boost the existing deep learning models. Based on the framework, we further implement a model named DRL4Route-GAE for PDRP in logistic service. It follows the actor-critic architecture which is equipped with a Generalized Advantage Estimator that can balance the bias and variance of the policy gradient estimates, thus achieving a more optimal policy. Extensive offline experiments and the online deployment show that DRL4Route-GAE improves Location Square Deviation (LSD) by 0.9\%-2.7\%, and Accuracy@3 (ACC@3) by 2.4\%-3.2\% over existing methods on the real-world dataset.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4628–4637},
numpages = {10},
keywords = {deep reinforcement learning, route prediction, pick-up and delivery service},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3396851.3402365,
author = {Spangher, Lucas and Gokul, Akash and Khattar, Manan and Palakapilly, Joseph and Tawade, Akaash and Bouyamourn, Adam and Devonport, Alex and Spanos, Costas},
title = {Prospective Experiment for Reinforcement Learning on Demand Response in a Social Game Framework},
year = {2020},
isbn = {9781450380096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396851.3402365},
doi = {10.1145/3396851.3402365},
abstract = {Improving demand response can help optimize renewable energy use and might be possible using current tools in machine learning. We propose an experiment to test the development of Reinforcement Learning (RL) agents to learn to vary a daily grid price signal to optimize behavioral energy shift in office workers. We describe our application of Batch Constrained Q Learning and Soft Actor Critic (SAC) as RL agents and Social Cognitive Theory, LSTM networks, and linear regression as planning models. We report limited success within simulation with SAC and linear regression. Finally, we propose an experiment timeline for consideration.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Future Energy Systems},
pages = {438–444},
numpages = {7},
keywords = {Planning models for Reinforcement Learning, Social Cognitive Theory, Reinforcement Learning, Demand Response, Transactive Control Learning, Social Energy competitions},
location = {Virtual Event, Australia},
series = {e-Energy '20}
}

@inproceedings{10.5555/3451906.3451918,
author = {Park, Sung Woon and Boukerche, Azzedine and Guan, Shichao},
title = {A Novel Deep Reinforcement Learning Based Service Migration Model for Mobile Edge Computing},
year = {2021},
publisher = {IEEE Press},
abstract = {Cloud Computing has emerged as a foundation of smart environments by encapsulating and virtualizing the underlying design and implementation details. Concerning the inherent latency and deployment issues, Mobile Edge Computing seeks to migrate services in the vicinity of mobile users. However, the current migration-based studies lack the consideration of migration cost, transaction cost, and energy consumption on the system-level with discussion on the impact of personalized user mobility. In this paper, we implement an enhanced service migration model to address user proximity issues. We formalize the migration cost, transaction cost, energy consumption related to the migration process. We model the service migration issue as a complex optimization problem and adapt Deep Reinforcement Learning to approximate the optimal policy. We compare the performance of the proposed model with the recent Q-learning method and other baselines. The results demonstrate that the proposed model can estimate the optimal policy with complicated computation requirements.},
booktitle = {Proceedings of the IEEE/ACM 24th International Symposium on Distributed Simulation and Real Time Applications},
pages = {84–91},
numpages = {8},
keywords = {mobile edge computing, deep reinforcement learning, migration cost, service migration, energy consumption},
location = {Prague, Czech Republic},
series = {DS-RT '20}
}

@inproceedings{10.1145/3573942.3573987,
author = {Zeng, Yaoping and Hu, Yanwei and Yang, Ting},
title = {Research on Task Offloading Based on Deep Reinforcement Learning for Internet of Vehicles},
year = {2023},
isbn = {9781450396899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573942.3573987},
doi = {10.1145/3573942.3573987},
abstract = {Mobile Edge Computing (MEC) is a promising technology that facilitates the computational offloading and resource allocation in the Internet of Vehicles (IoV) environment. When the mobile device is not capable enough to meet its own demands for data processing, the task will be offloaded to the MEC server, which can effectively relieve the network pressure, meet the multi-task computing requirements, and ensure the quality of service (QoS). Via multi-user and multi-MEC servers, this paper proposes the Q-Learning task offloading strategy based on the improved deep reinforcement learning policy(IDRLP) to obtain an optimal strategy for task offloading and resource allocation. Simulation results suggest that the proposed algorithm compared with other benchmark schemes has better performance in terms of delay, energy consumption and system weighted cost, even with different tasks, users and data sizes.},
booktitle = {Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {1033–1038},
numpages = {6},
location = {Xiamen, China},
series = {AIPR '22}
}

@inproceedings{10.1145/3321707.3321831,
author = {Gabor, Thomas and Sedlmeier, Andreas and Kiermeier, Marie and Phan, Thomy and Henrich, Marcel and Pichlmair, Monika and Kempter, Bernhard and Klein, Cornel and Sauer, Horst and AG, Reiner SchmidSiemens and Wieghardt, Jan},
title = {Scenario Co-Evolution for Reinforcement Learning on a Grid World Smart Factory Domain},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321831},
doi = {10.1145/3321707.3321831},
abstract = {Adversarial learning has been established as a successful paradigm in reinforcement learning. We propose a hybrid adversarial learner where a reinforcement learning agent tries to solve a problem while an evolutionary algorithm tries to find problem instances that are hard to solve for the current expertise of the agent, causing the intelligent agent to co-evolve with a set of test instances or scenarios. We apply this setup, called scenario co-evolution, to a simulated smart factory problem that combines task scheduling with navigation of a grid world. We show that the so trained agent outperforms conventional reinforcement learning. We also show that the scenarios evolved this way can provide useful test cases for the evaluation of any (however trained) agent.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {898–906},
numpages = {9},
keywords = {coevolution, automatic test generation, evolutionary algorithms, adversarial learning, reinforcement learning},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.3115/1073336.1073364,
author = {Lecundefineduche, Renaud},
title = {Learning Optimal Dialogue Management Rules by Using Reinforcement Learning and Inductive Logic Programming},
year = {2001},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073336.1073364},
doi = {10.3115/1073336.1073364},
abstract = {Developing dialogue systems is a complex process. In particular, designing efficient dialogue management strategies is often difficult as there are no precise guidelines to develop them and no sure test to validate them. Several suggestions have been made recently to use reinforcement learning to search for the optimal management strategy for specific dialogue situations. These approaches have produced interesting results, including applications involving real world dialogue systems. However, reinforcement learning suffers from the fact that it is state based. In other words, the optimal strategy is expressed as a decision table specifying which action to take in each specific state. It is therefore difficult to see whether there is any generality across states. This limits the analysis of the optimal strategy and its potential for re-use in other dialogue situations. In this paper we tackle this problem by learning rules that generalize the state-based strategy. These rules are more readable than the underlying strategy and therefore easier to explain and re-use. We also investigate the capability of these rules in directing the search for the optimal strategy by looking for generalization whilst the search proceeds.},
booktitle = {Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies},
pages = {1–7},
numpages = {7},
location = {Pittsburgh, Pennsylvania},
series = {NAACL '01}
}

@inproceedings{10.1145/3437378.3437395,
author = {Holt-Quick, Chester and Warren, Jim},
title = {Establishing a Dialog Agent Policy Using Deep Reinforcement Learning in the Psychotherapy Domain},
year = {2021},
isbn = {9781450389563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437378.3437395},
doi = {10.1145/3437378.3437395},
abstract = {Recent years have seen a rise in the development and use of dialog agents (chatbots) including as personal friends, coaches and even counsellors (Virtual Counsellor, VC). Some of these [1] employ expert psychotherapy techniques, notably Cognitive Behavioural Therapy (CBT). The usually rule-based dialog policy of these chatbots requires significant authoring effort and can result in overly predictable dialog. This research examines the question of whether using deep reinforcement learning (DRL) to develop an agent dialog policy in the psychotherapy domain is feasible. A framework for DRL for this problem is established including the use of a user simulator that is configured through CBT manual transcript excerpts. It is found that a dialog policy conforming to transcript excerpts can be learned such that core CBT skills can be emulated. A hoped-for reduction in authoring effort is not seen, but a modest amount of local generalisation of the learned CBT skill can be observed which increases when the user simulator is configured with additional rule-based policies. Interesting use of a novel listening dialog move is observed by the trained VC.},
booktitle = {Proceedings of the 2021 Australasian Computer Science Week Multiconference},
articleno = {12},
numpages = {9},
keywords = {Cognitive Behaviour Therapy (CBT), Dialog Policy, Virtual Counsellor, Dialog Agent, Reinforcement Learning},
location = {Dunedin, New Zealand},
series = {ACSW '21}
}

@article{10.1145/3618107,
author = {Shi, Xiaoyu and Liu, Quanliang and Xie, Hong and Wu, Di and Peng, Bo and Shang, MingSheng and Lian, Defu},
title = {Relieving Popularity Bias in Interactive Recommendation: A Diversity-Novelty-Aware Reinforcement Learning Approach},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3618107},
doi = {10.1145/3618107},
abstract = {While personalization increases the utility of item recommendation, it also suffers from the issue of popularity bias. However, previous methods emphasize adopting supervised learning models to relieve popularity bias in the static recommendation, ignoring the dynamic transfer of user preference and amplification effects of the feedback loop in the recommender system (RS). In this paper, we focus on studying this issue in the interactive recommendation. We argue that diversification and novelty are both equally crucial for improving user satisfaction of IRS in the aforementioned setting. To achieve this goal, we propose a Diversity-Novelty-aware Interactive Recommendation framework (DNaIR) that augments offline reinforcement learning (RL) to increase the exposure rate of long-tail items with high quality. Its main idea is first to aggregate the item similarity, popularity, and quality into the reward model to help the planning of RL policy. It then designs a diversity-aware stochastic action generator to achieve an efficient and lightweight DNaIR algorithm. Extensive experiments are conducted on the three real-world datasets and an authentic RL environment (Virtual-Taobao). The experiments show that our model can better and full use of the long-tail items to improve recommendation satisfaction, especially those low popularity items with high-quality ones, thus achieving state-of-the-art performance.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = {sep},
keywords = {interactive recommendation, item fairness, reinforcement learning, popularity bias}
}

@inproceedings{10.1145/3587716.3587733,
author = {Yang, Dujia and Song, Changjian and Wang, Jian and Zhu, Rangang and Yang, Jun'An},
title = {QoE-Aware Adaptive Bitrate Algorithm Based on Subepisodic Deep Reinforcement Learning for DASH},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587716.3587733},
doi = {10.1145/3587716.3587733},
abstract = {Recently, mobile video service is booming and its traffic accounts for the vast majority of network traffic. The adaptive bitrate (ABR) algorithm in dynamic adaptive streaming over HTTP (DASH) is the key technology to improve user’s quality of experience (QoE). However, existing ABR algorithms still have some problems such as poor universality, high complexity, too much domain knowledge and lack of attention to initial buffering (IB). To this end, we propose a QoE-aware ABR algorithm based on subepisodic deep reinforcement learning (DRL) for DASH. Starting from the episodic attributes of video service, we introduce the efficient episodic DRL to model the ABR problem for the first time. Secondly, through analyzing problems of direct application of episodic DRL, a new algorithm with the idea of divide and conquer is designed creatively. The algorithm divides video session into two subepisodes (IB and formal playing), and models and executes the optimization strategy respectively. These two subepisodes are unified in the reward function with the same goal of maximizing users’ QoE. Finally, simulation results show that compared with the ABR algorithms based on traditional DRL and direct application of episodic DRL, our algorithms significantly improve not only the sample efficiency in training, but also both users’ subjective QoE and objective indicators of video playing quality in test, including the performance in the IB phase.},
booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
pages = {103–108},
numpages = {6},
keywords = {DASH, bitrate adaptation, deep reinforcement learning},
location = {Zhuhai, China},
series = {ICMLC '23}
}

@inproceedings{10.1145/3511808.3557373,
author = {Ayala-Romero, Jose A. and Mernyei, P\'{e}ter and Shi, Bichen and Maz\'{o}n, Diego},
title = {KRAF: A Flexible Advertising Framework Using Knowledge Graph-Enriched Multi-Agent Reinforcement Learning},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557373},
doi = {10.1145/3511808.3557373},
abstract = {Bidding optimization is one of the most important problems in online advertising. Auto-bidding tools are designed to address this problem and are offered by most advertising platforms for advertisers to allocate their budgets. In this work, we present a Knowledge Graph-enriched Multi-Agent Reinforcement Learning Advertising Framework (KRAF). It combines Knowledge Graph (KG) techniques with a Multi-Agent Reinforcement Learning (MARL) algorithm for bidding optimization with the goal of maximizing advertisers' return on ad spend (ROAS) and user-ad interactions, which correlates to the ad platform revenue. In addition, this proposal is flexible enough to support different levels of user privacy and the advent of new advertising markets with more heterogeneous data. In contrast to most of the current advertising platforms that are based on click-through rate models using a fixed input format and rely on user tracking, KRAF integrates the heterogeneous available data (e.g., contextual features, interest-based attributes, information about ads) as graph nodes to generate their dense representation (embeddings). Then, our MARL algorithm leverages the embeddings of the entities to learn efficient budget allocation strategies. To that end, we propose a novel coordination strategy based on a mean-field style to coordinate the learning agents and avoid the curse of dimensionality when the number of agents grows. Our proposal is evaluated on three real-world datasets to assess its performance and the contribution of each of its components, outperforming several baseline methods in terms of ROAS and number of ad clicks.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {47–56},
numpages = {10},
keywords = {online advertising, multi-agent reinforcement learning, knowledge graph, bid optimization},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3576842.3582366,
author = {Sandha, Sandeep Singh and Balaji, Bharathan and Garcia, Luis and Srivastava, Mani},
title = {Eagle: End-to-End Deep Reinforcement Learning Based Autonomous Control of PTZ Cameras},
year = {2023},
isbn = {9798400700378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576842.3582366},
doi = {10.1145/3576842.3582366},
abstract = {Existing approaches for autonomous control of pan-tilt-zoom (PTZ) cameras use multiple stages where object detection and localization are performed separately from the control of the PTZ mechanisms. These approaches require manual labels and suffer from performance bottlenecks due to error propagation across the multi-stage flow of information. The large size of object detection neural networks also makes prior solutions infeasible for real-time deployment in resource-constrained devices. We present an end-to-end deep reinforcement learning (RL) solution called Eagle1 to train a neural network policy that directly takes images as input to control the PTZ camera. Training reinforcement learning is cumbersome in the real world due to labeling effort, runtime environment stochasticity, and fragile experimental setups. We introduce a photo-realistic simulation framework for training and evaluation of PTZ camera control policies. Eagle achieves superior camera control performance by maintaining the object of interest close to the center of captured images at high resolution and has up to 17\% more tracking duration than the state-of-the-art. Eagle policies are lightweight (90x fewer parameters than Yolo5s) and can run on embedded camera platforms such as Raspberry PI (33 FPS) and Jetson Nano (38 FPS), facilitating real-time PTZ tracking for resource-constrained environments. With domain randomization, Eagle policies trained in our simulator can be transferred directly to real-world scenarios2.},
booktitle = {Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation},
pages = {144–157},
numpages = {14},
keywords = {simulation-to-reality transfer, pan-tilt-zoom cameras, edge AI, end-to-end control, deep reinforcement learning},
location = {San Antonio, TX, USA},
series = {IoTDI '23}
}

@article{10.1145/3447268,
author = {Zhu, Changxi and Leung, Ho-Fung and Hu, Shuyue and Cai, Yi},
title = {A Q-Values Sharing Framework for Multi-Agent Reinforcement Learning under Budget Constraint},
year = {2021},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3447268},
doi = {10.1145/3447268},
abstract = {In a teacher-student framework, a more experienced agent (teacher) helps accelerate the learning of another agent (student) by suggesting actions to take in certain states. In cooperative multi-agent reinforcement learning (MARL), where agents must cooperate with one another, a student could fail to cooperate effectively with others even by following a teacher’s suggested actions, as the policies of all agents can change before convergence. When the number of times that agents communicate with one another is limited (i.e., there are budget constraints), an advising strategy that uses actions as advice could be less effective. We propose a partaker-sharer advising framework (PSAF) for cooperative MARL agents learning with budget constraints. In PSAF, each Q-learner can decide when to ask for and share its Q-values. We perform experiments in three typical multi-agent learning problems. The evaluation results indicate that the proposed PSAF approach outperforms existing advising methods under both constrained and unconstrained budgets. Moreover, we analyse the influence of advising actions and sharing Q-values on agent learning.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {apr},
articleno = {4},
numpages = {28},
keywords = {Multi-agent reinforcement learning, cooperative learning, Q-learner, knowledge sharing}
}

@inproceedings{10.5555/1608974.1608986,
author = {Lemon, Oliver and Georgila, Kallirroi and Henderson, James and Stuttle, Matthew},
title = {An ISU Dialogue System Exhibiting Reinforcement Learning of Dialogue Policies: Generic Slot-Filling in the TALK in-Car System},
year = {2006},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We demonstrate a multimodal dialogue system using reinforcement learning for in-car scenarios, developed at Edinburgh University and Cambridge University for the TALK project. This prototype is the first "Information State Update" (ISU) dialogue system to exhibit reinforcement learning of dialogue strategies, and also has a fragmentary clarification feature. This paper describes the main components and functionality of the system, as well as the purposes and future use of the system, and surveys the research issues involved in its construction. Evaluation of this system (i.e. comparing the baseline system with handcoded vs. learnt dialogue policies) is ongoing, and the demonstration will show both.},
booktitle = {Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics: Posters \&amp; Demonstrations},
pages = {119–122},
numpages = {4},
location = {Trento, Italy},
series = {EACL '06}
}

@article{10.1145/3441139,
author = {Fragkos, Georgios and Minwalla, Cyrus and Tsiropoulou, Eirini Eleni and Plusquellic, Jim},
title = {Enhancing Privacy in PUF-Cash through Multiple Trusted Third Parties and Reinforcement Learning},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1550-4832},
url = {https://doi.org/10.1145/3441139},
doi = {10.1145/3441139},
abstract = {Electronic cash (e-Cash) is a digital alternative to physical currency such as coins and bank notes. Suitably constructed, e-Cash has the ability to offer an anonymous offline experience much akin to cash, and in direct contrast to traditional forms of payment such as credit and debit cards. Implementing security and privacy within e-Cash, i.e., preserving user anonymity while preventing counterfeiting, fraud, and double spending, is a non-trivial challenge. In this article, we propose major improvements to an e-Cash protocol, termed PUF-Cash, based on physical unclonable functions (PUFs). PUF-Cash was created as an offline-first, secure e-Cash scheme that preserved user anonymity in payments. In addition, PUF-Cash supports remote payments; an improvement over traditional currency. In this work, a novel multi-trusted-third-party exchange scheme is introduced, which is responsible for “blinding” Alice’s e-Cash tokens; a feature at the heart of preserving her anonymity. The exchange operations are governed by machine learning techniques which are uniquely applied to optimize user privacy, while remaining resistant to identity-revealing attacks by adversaries and trusted authorities. Federation of the single trusted third party into multiple entities distributes the workload, thereby improving performance and resiliency within the e-Cash system architecture. Experimental results indicate that improvements to PUF-Cash enhance user privacy and scalability.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = {sep},
articleno = {7},
numpages = {26},
keywords = {Internet of things, digital currency, electronic money, security, networks}
}

@inproceedings{10.1145/3511808.3557064,
author = {Yuan, Congde and Guo, Mengzhuo and Xiang, Chaoneng and Wang, Shuangyang and Song, Guoqing and Zhang, Qingpeng},
title = {An Actor-Critic Reinforcement Learning Model for Optimal Bidding in Online Display Advertising},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557064},
doi = {10.1145/3511808.3557064},
abstract = {The real-time bidding (RTB) paradigm allows the advertisers to submit a bid for each impression in online display advertising. A usual demand of the advertisers is to maximize the total value of winning impressions under constraints on some key performance indicators. Unfortunately, the existing RTB research in industrial applications can hardly achieve the optimum due to the stochastic decision scenarios and complex consumer behaviors. In this study, we address the application of RTB to mobile gaming where the in-app purchase action is of high uncertainty, making it challenging to evaluate individual impression opportunities. We first formulate the bidding process into a constrained optimization problem and then propose an actor-critic reinforcement learning (ACRL) model for obtaining the optimal policy under a dynamic decision environment. To avoid feeding too many samples with zero labels to the model, we provide a new way to quantify impression opportunities by integrating the in-app actions, such as conversion and purchase, and the characteristics of the candidate ad inventories. Moreover, the proposed ACRL learns a Gaussian distribution to simulate the audience's decision in a more real bidding scenario by taking additional contextual side information about both media and the audience. We also introduce how to deploy the learned model online to help adjust the final bid. At last, we conduct comprehensive offline experiments to demonstrate the effectiveness of ACRL and carefully set an online A/B testing experiment. The online experimental results verify the efficacy of the proposed ACRL in terms of multiple critical commercial indicators. ACRL has been deployed in the Tencent online display advertising platform and impacts billions of traffic every day. We believe proposed modifications for optimal bidding problems in RTB are practically innovative and can inspire the relative works in this field.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {3604–3613},
numpages = {10},
keywords = {real-time bidding, reinforcement learning, online display advertising},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3340531.3412682,
author = {Wu, Guojun and Li, Yanhua and Luo, Shikai and Song, Ge and Wang, Qichao and He, Jing and Ye, Jieping and Qie, Xiaohu and Zhu, Hongtu},
title = {A Joint Inverse Reinforcement Learning and Deep Learning Model for Drivers' Behavioral Prediction},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412682},
doi = {10.1145/3340531.3412682},
abstract = {Users' behavioral predictions are crucially important for many domains including major e-commerce companies, ride-hailing platforms, social networking, and education. The success of such prediction strongly depends on the development of representation learning that can effectively model the dynamic evolution of user's behavior. This paper aims to develop a joint framework of combining inverse reinforcement learning (IRL) with deep learning (DL) regression model, called IRL-DL, to predict drivers' future behavior in ride-hailing platforms. Specifically, we formulate the dynamic evolution of each driver as a sequential decision-making problem and then employ IRL as representation learning to learn the preference vector of each driver. Then, we integrate drivers' preference vector with their static features (e.g., age, gender) and other attributes to build a regression model (e.g., LTSM-neural network) to predict drivers' future behavior. We use an extensive driver data set obtained from a ride-sharing platform to verify the effectiveness and efficiency of our IRL-DL framework, and results show that our IRL-DL framework can achieve consistent and remarkable improvements over models without drivers' preference vectors.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
pages = {2805–2812},
numpages = {8},
keywords = {user modelling, drivers' behavioral prediction, inverse reinforcement learning},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.5555/3545946.3598752,
author = {Yu, Chao and Yang, Xinyi and Gao, Jiaxuan and Chen, Jiayu and Li, Yunfei and Liu, Jijia and Xiang, Yunfei and Huang, Ruixin and Yang, Huazhong and Wu, Yi and Wang, Yu},
title = {Asynchronous Multi-Agent Reinforcement Learning for Efficient Real-Time Multi-Robot Cooperative Exploration},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We consider the problem of cooperative exploration, where multiple robots need to cooperatively explore an unknown region as fast as possible. Multi-agent reinforcement learning (MARL) has recently become a trending paradigm for solving this challenge. However, existing MARL-based methods adopt action-making steps as the metric for exploration efficiency by assuming all the agents are acting in a fully synchronous manner: i.e., every single agent produces an action simultaneously and every single action is executed instantaneously at each time step. Despite its mathematical simplicity, such a synchronous MARL formulation can be problematic for real-world robotic applications. It can be typical that different robots may take slightly different wall-clock times to accomplish an atomic action or even periodically get lost due to hardware issues. Simply waiting for every robot being ready for the next action can be particularly time-inefficient. Therefore, we propose an asynchronous MARL solution, Asynchronous Coordination Explorer (ACE), to tackle this real-world challenge. We first extend a classical MARL algorithm, multi-agent PPO (MAPPO), to the asynchronous setting and additionally apply action-delay randomization to enforce the learned policy to generalize better to varying action delays in the real world. Moreover, each navigation agent is represented as a team-size-invariant CNN-based policy, which greatly benefits real-robot deployment by handling possible robot lost and allows bandwidth-efficient intra-agent communication through low-dimensional CNN features. We first validate our approach in a grid-based scenario. Both simulation and real-robot results show that ACE reduces over 10\% actual exploration time compared with classical approaches. We also apply our framework to a high-fidelity visual-based environment, Habitat, achieving 28\% improvement in exploration efficiency.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1107–1115},
numpages = {9},
keywords = {multi-agent reinforcement learning, cooperative exploration, asynchronous decision making},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3573900.3593631,
author = {Dantas, Joao P. A. and Maximo, Marcos R. O. A. and Yoneyama, Takashi},
title = {Autonomous Agent for Beyond Visual Range Air Combat: A Deep Reinforcement Learning Approach},
year = {2023},
isbn = {9798400700309},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573900.3593631},
doi = {10.1145/3573900.3593631},
abstract = {This work contributes to developing an agent based on deep reinforcement learning capable of acting in a beyond visual range (BVR) air combat simulation environment. The paper presents an overview of building an agent representing a high-performance fighter aircraft that can learn and improve its role in BVR combat over time based on rewards calculated using operational metrics. Also, through self-play experiments, it expects to generate new air combat tactics never seen before. Finally, we hope to examine a real pilot’s ability, using virtual simulation, to interact in the same environment with the trained agent and compare their performances. This research will contribute to the air combat training context by developing agents that can interact with real pilots to improve their performances in air defense missions.},
booktitle = {Proceedings of the 2023 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {48–49},
numpages = {2},
location = {Orlando, FL, USA},
series = {SIGSIM-PADS '23}
}

@inproceedings{10.1145/3586102.3586108,
author = {Yang, Yongjie and Tu, Shanshan and Yan, Haishuang and Zhang, Yihe and Wu, Aiming and Bai, Xuetao},
title = {Deep Reinforcement Learning-Based Computation Offloading for Anti-Jamming in Fog Computing Networks},
year = {2023},
isbn = {9781450397520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586102.3586108},
doi = {10.1145/3586102.3586108},
abstract = {With the rapid development of IoT and fog computing, mobile devices can offload computing tasks into the fog layer to reduce energy consumption and transmission delay. However, the communication between fog nodes and end users is vulnerable to malicious jamming attacks. In this paper, we propose a reinforcement learning-based computation offloading strategy to choose fog nodes, offloading rate, and transmit power to address jamming attacks and interference problems. We use dueling deep Q-network to accelerate the learning speed in the system, which can effectively determine the offloading strategy. The simulation results show that this scheme based on DDQN is better than DQN and Q-learning in terms of computational latency and energy consumption.},
booktitle = {Proceedings of the 2022 12th International Conference on Communication and Network Security},
pages = {36–42},
numpages = {7},
keywords = {Computation offloading, Dueling deep Q-network, Fog computing, Anti-jamming},
location = {Beijing, China},
series = {ICCNS '22}
}

@inproceedings{10.1145/3478586.3478600,
author = {Goel, Aakarsh and Chauhan, Shubham},
title = {Adaptive Look-Ahead Distance for Pure Pursuit Controller with Deep Reinforcement Learning Techniques},
year = {2022},
isbn = {9781450389716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478586.3478600},
doi = {10.1145/3478586.3478600},
abstract = {The trajectory generated by the motion planner depends on various factors such as road curvatures, static and dynamic obstacles, passenger comfort, and road conditions. Geometric path tracking is one of the most critical aspects of path tracking in Autonomous Vehicles. The lateral and longitudinal controllers aim to track the path as closely as possible and minimize along-track and cross-track errors considering the system’s dynamics and latencies present in it. Pure pursuit is one of the simplest geometric tracking controllers having only one parameter to tune, i.e., the look-ahead distance which handles steering aggressiveness. In this paper we propose a novel lateral controller which tunes the look-ahead distance of pure pursuit controller with the help of Deep Deterministic Policy Gradient(DDPG).},
booktitle = {Advances in Robotics - 5th International Conference of The Robotics Society},
articleno = {26},
numpages = {5},
keywords = {DDPG, Pure Pursuit Control, Autonomous Vehicle, Deep Reinforcement Learning},
location = {Kanpur, India},
series = {AIR2021}
}

@inproceedings{10.1145/3360322.3360845,
author = {Van Le, Duc and Liu, Yingbo and Wang, Rongrong and Tan, Rui and Wong, Yew-Wah and Wen, Yonggang},
title = {Control of Air Free-Cooled Data Centers in Tropics via Deep Reinforcement Learning},
year = {2019},
isbn = {9781450370059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360322.3360845},
doi = {10.1145/3360322.3360845},
abstract = {Air free-cooled data centers (DCs) have not existed in the tropical zone due to the unique challenges of year-round high ambient temperature and relative humidity (RH). The increasing availability of servers that can tolerate higher temperatures and RH due to the regulatory bodies' prompts to raise DC temperature setpoints sheds light upon the feasibility of air free-cooled DCs in tropics. This paper studies the problem of controlling the temperature and RH of the air supplied to the servers in a free-cooled tropical DC below certain thresholds to maintain servers' computing performance and reliability. To achieve the goal, a portion of the hot air generated by the servers is recirculated and mixed with the fresh outside air to adjust the RH of the supply air. To address the complex psychrometric dynamics, we apply deep reinforcement learning to learn the control policy that aims at minimizing the energy used for moving air and on-demand cooling. Extensive evaluation based on real data traces collected from an air free-cooled testbed and comparisons with hysteresis-based and model-predictive control approaches show the superior performance of our solution.},
booktitle = {Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {306–315},
numpages = {10},
keywords = {air free cooling, deep reinforcement learning, Data centers},
location = {New York, NY, USA},
series = {BuildSys '19}
}

@article{10.5555/3586589.3586601,
author = {Subramanian, Jayakumar and Sinha, Amit and Seraj, Raihan and Mahajan, Aditya},
title = {Approximate Information State for Approximate Planning and Reinforcement Learning in Partially Observed Systems},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {We propose a theoretical framework for approximate planning and learning in partially observed systems. Our framework is based on the fundamental notion of information state. We provide two definitions of information state--i) a function of history which is sufficient to compute the expected reward and predict its next value; ii) a function of the history which can be recursively updated and is sufficient to compute the expected reward and predict the next observation. An information state always leads to a dynamic programming decomposition. Our key result is to show that if a function of the history (called approximate information state (AIS)) approximately satisfies the properties of the information state, then there is a corresponding approximate dynamic program. We show that the policy computed using this is approximately optimal with bounded loss of optimality. We show that several approximations in state, observation and action spaces in literature can be viewed as instances of AIS. In some of these cases, we obtain tighter bounds. A salient feature of AIS is that it can be learnt from data. We present AIS based multi-time scale policy gradient algorithms and detailed numerical experiments with low, moderate and high dimensional environments.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {12},
numpages = {83},
keywords = {approximate information state, information state, partially observable Markov decision processes, partially observed reinforcement learning, approximate dynamic programming}
}

@inproceedings{10.5555/3306127.3331924,
author = {Barat, Souvik and Khadilkar, Harshad and Meisheri, Hardik and Kulkarni, Vinay and Baniwal, Vinita and Kumar, Prashant and Gajrani, Monika},
title = {Actor Based Simulation for Closed Loop Control of Supply Chain Using Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reinforcement Learning (RL) has achieved a degree of success in control applications such as online gameplay and robotics, but has rarely been used to manage operations of business-critical systems such as supply chains. A key aspect of using RL in the real world is to train the agent before deployment, so as to minimise experimentation in live operation. While this is feasible for online gameplay (where the rules of the game are known) and robotics (where the dynamics are predictable), it is much more difficult for complex systems due to associated complexities, such as uncertainty, adaptability and emergent behaviour. In this paper, we describe a framework for effective integration of a reinforcement learning controller with an actor-based simulation of the complex networked system, in order to enable deployment of the RL agent in the real system with minimal further tuning.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1802–1804},
numpages = {3},
keywords = {reinforcement learning, simulation of complex systems, model based simulation},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.5555/3400397.3400449,
author = {Li, Maojia P. and Sankaran, Prashant and Kuhl, Michael E. and Ptucha, Raymond and Ganguly, Amlan and Kwasinski, Andres},
title = {Task Selection by Autonomous Mobile Robots in a Warehouse Using Deep Reinforcement Learning},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {We introduce a deep Q-network (DQN) based model that addresses the dispatching and routing problems for autonomous mobile robots. The DQN model is trained to dispatch a small fleet of robots to perform material handling tasks in a virtual, as well as, in an actual warehouse environment. Specifically, the DQN model is trained to dispatch an available robot to the closest task that will avoid or minimize encounters with other robots. Based on a discrete event simulation experiment, the DQN model outperforms the shortest travel distance rule in terms of avoiding traffic conflicts, improving the makespan for completing a set of tasks, and reducing the mean time in system for tasks.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {680–688},
numpages = {9},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/3442381.3450097,
author = {Sharma, Ashish and Lin, Inna W. and Miner, Adam S. and Atkins, David C. and Althoff, Tim},
title = {Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450097},
doi = {10.1145/3442381.3450097},
abstract = {Online peer-to-peer support platforms enable conversations between millions of people who seek and provide mental health support. If successful, web-based mental health conversations could improve access to treatment and reduce the global disease burden. Psychologists have repeatedly demonstrated that empathy, the ability to understand and feel the emotions and experiences of others, is a key component leading to positive outcomes in supportive conversations. However, recent studies have shown that highly empathic conversations are rare in online mental health platforms. In this paper, we work towards improving empathy in online mental health support conversations. We introduce a new task of empathic rewriting which aims to transform low-empathy conversational posts to higher empathy. Learning such transformations is challenging and requires a deep understanding of empathy while maintaining conversation quality through text fluency and specificity to the conversational context. Here we propose Partner, a deep reinforcement learning (RL) agent that learns to make sentence-level edits to posts in order to increase the expressed level of empathy while maintaining conversation quality. Our RL agent leverages a policy network, based on a transformer language model adapted from GPT-2, which performs the dual task of generating candidate empathic sentences and adding those sentences at appropriate positions. During training, we reward transformations that increase empathy in posts while maintaining text fluency, context specificity, and diversity. Through a combination of automatic and human evaluation, we demonstrate that Partner&nbsp;successfully generates more empathic, specific, and diverse responses and outperforms NLP methods from related tasks such as style transfer and empathic dialogue generation. This work has direct implications for facilitating empathic conversations on web-based platforms.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {194–205},
numpages = {12},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3579654.3579736,
author = {Huang, Mingxuan and Sun, Kaixuan and Hou, Yunpeng and Ye, Zhicheng and Wan, Yuanlong and He, Huasen},
title = {Deep Reinforcement Learning Based Delay-Aware Task Offloading for UAV-Assisted Edge Computing},
year = {2023},
isbn = {9781450398336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579654.3579736},
doi = {10.1145/3579654.3579736},
abstract = {Multi-access edge computing has been widely used in various Internet of Things (IoT) devices because of its excellent computing and fast interaction abilities. How to improve the service extensibility of edge computing and optimize the computing offload strategy has become the key to improve the quality of service to edge computing users. However, the traditional offloading strategy based on mathematical programming has exposed its inherent limitations in dynamic scenarios, and cannot meet the requirements of multiple-mobile terminals distributed in a large area. Therefore, this paper use Unmanned Aerial Vehicles (UAVs) to establish a multi-UAV-assisted edge computing framework for extending the service range, and proposes an offloading strategy based on reinforcement learning to offload the growing computing requirements from mobile terminals to edge servers. By mapping the states of mobile terminals and UAVs to the corresponding action space, and then offloading computing tasks to UAVs, the energy consumption caused by computing and processing tasks of mobile terminals can be effectively reduced. Jointly considering the potential dimensional disaster of state space and the convergence failure imposed by the increase of device numbers, a novel computation offloading strategy based on deep reinforcement learning is proposed. Moreover, we design a load-balancing mechanism in the UAVs to improve the processing capacity. Experimental results prove that our proposed algorithm can effectively reduce the computing energy consumption of mobile terminals and avoid task timeout with a short convergence time.},
booktitle = {Proceedings of the 2022 5th International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {77},
numpages = {8},
keywords = {computing offloading, Multi-access edge computing, deep reinforcement learning},
location = {Sanya, China},
series = {ACAI '22}
}

@inproceedings{10.1145/3539618.3592018,
author = {Shi, Xiaowen and Wang, Ze and Cai, Yuanying and Wu, Xiaoxu and Yang, Fan and Liao, Guogang and Wang, Yongkang and Wang, Xingxing and Wang, Dong},
title = {MDDL: A Framework for Reinforcement Learning-Based Position Allocation in Multi-Channel Feed},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3592018},
doi = {10.1145/3539618.3592018},
abstract = {Nowadays, the mainstream approach in position allocation system is to utilize a reinforcement learning model to allocate appropriate locations for items in various channels and then mix them into the feed. There are two types of data employed to train reinforcement learning (RL) model for position allocation, named strategy data and random data. Strategy data is collected from the current online model, it suffers from an imbalanced distribution of stateaction pairs, resulting in severe overestimation problems during training. On the other hand, random data offers a more uniform distribution of state-action pairs, but is challenging to obtain in industrial scenarios as it could negatively impact platform revenue and user experience due to random exploration. As the two types of data have different distributions, designing an effective strategy to leverage both types of data to enhance the efficacy of the RL model training has become a highly challenging problem. In this study, we propose a framework namedMulti-Distribution Data Learning (MDDL) to address the challenge of effectively utilizing both strategy and random data for training RL models on mixed multi-distribution data. Specifically, MDDL incorporates a novel imitation learning signal to mitigate overestimation problems in strategy data and maximizes the RL signal for random data to facilitate effective learning. In our experiments, we evaluated the proposed MDDL framework in a real-world position allocation system and demonstrated its superior performance compared to the previous baseline. MDDL has been fully deployed on the Meituan food delivery platform and currently serves over 300 million users.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2159–2163},
numpages = {5},
keywords = {position allocation, reinforcement learning, multi-distribution data learning},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3394171.3413862,
author = {Wu, Jie and Li, Guanbin and Han, Xiaoguang and Lin, Liang},
title = {Reinforcement Learning for Weakly Supervised Temporal Grounding of Natural Language in Untrimmed Videos},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413862},
doi = {10.1145/3394171.3413862},
abstract = {Temporal grounding of natural language in untrimmed videos is a fundamental yet challenging multimedia task facilitating cross-media visual content retrieval. We focus on the weakly supervised setting of this task that merely accesses to coarse video-level language description annotation without temporal boundary, which is more consistent with reality as such weak labels are more readily available in practice. In this paper, we propose a Boundary Adaptive Refinement (BAR) framework that resorts to reinforcement learning (RL) to guide the process of progressively refining the temporal boundary. To the best of our knowledge, we offer the first attempt to extend RL to temporal localization task with weak supervision. As it is non-trivial to obtain a straightforward reward function in the absence of pairwise granular boundary-query annotations, a cross-modal alignment evaluator is crafted to measure the alignment degree of segment-query pair to provide tailor-designed rewards. This refinement scheme completely abandons traditional sliding window based solution pattern and contributes to acquiring more efficient, boundary-flexible and content-aware grounding results. Extensive experiments on two public benchmarks Charades-STA and ActivityNet demonstrate that BAR outperforms the state-of-the-art weakly-supervised method and even beats some competitive fully-supervised ones.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {1283–1291},
numpages = {9},
keywords = {reinforcement learning, boundary adaptive refinement, temporal grounding of natural language in untrimmed videos},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.5555/3463952.3464107,
author = {Van Havermaet, Stef and Khaluf, Yara and Simoens, Pieter},
title = {No More Hand-Tuning Rewards: Masked Constrained Policy Optimization for Safe Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In safe Reinforcement Learning (RL), the agent attempts to find policies which maximize the expectation of accumulated rewards and guarantee its safety to remain above a given threshold. Hence, it is straightforward to formalize safe RL problems by both a reward function and a safety constraint. We define safety as the probability of survival in environments where taking risky actions could lead to early termination of the task. Although the optimization problem is already constrained by a safety threshold, reward signals related to unsafe terminal states influence the original maximization objective of the task. Selecting the appropriate value of these signals is often a time consuming and challenging reward engineering task, which requires expert knowledge of the domain. This paper presents a safe RL algorithm, called Masked Constrained Policy Optimization (MCPO), in which the learning process is constrained by safety and excludes the risk reward signals. We develop MCPO as an extension of gradient-based policy search methods, in which the updates of the policy and the expected reward models are masked. Our method benefits from having a high probability of satisfying the given constraints for every policy in the learning process. We validate the proposed algorithm in two continuous tasks. Our findings prove the proposed algorithm is able to neglect risk reward signals, and thereby resolving the desired safety-performance trade-off without having the need for hand-tuning rewards.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1344–1352},
numpages = {9},
keywords = {reward engineering, safe reinforcement learning, constrained policy optimization},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@article{10.5555/2591248.2591268,
author = {Guez, Arthur and Silver, David and Dayan, Peter},
title = {Scalable and Efficient Bayes-Adaptive Reinforcement Learning Based on Monte-Carlo Tree Search},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {Bayesian planning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, planning optimally in the face of uncertainty is notoriously taxing, since the search space is enormous. In this paper we introduce a tractable, sample-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach avoids expensive applications of Bayes rule within the search tree by sampling models from current beliefs, and furthermore performs this sampling in a lazy manner. This enables it to outperform previous Bayesian model-based reinforcement learning algorithms by a significant margin on several well-known benchmark problems. As we show, our approach can even work in problems with an in finite state space that lie qualitatively out of reach of almost all previous work in Bayesian exploration.},
journal = {J. Artif. Int. Res.},
month = {oct},
pages = {841–883},
numpages = {43}
}

@inproceedings{10.1145/3486611.3486668,
author = {Jang, Doseok and Spangher, Lucas and Srivistava, Tarang and Khattar, Manan and Agwan, Utkarsha and Nadarajah, Selvaprabu and Spanos, Costas},
title = {Offline-Online Reinforcement Learning for Energy Pricing in Office Demand Response: Lowering Energy and Data Costs},
year = {2021},
isbn = {9781450391146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486611.3486668},
doi = {10.1145/3486611.3486668},
abstract = {Our team is proposing to run a full-scale energy demand response experiment in an office building. Although this is an exciting endeavor which will provide value to the community, collecting training data for the reinforcement learning agent is costly and will be limited. In this work, we examine how offline training can be leveraged to minimize data costs (accelerate convergence) and program implementation costs. We present two approaches to doing so: pretraining our model to warm start the experiment with simulated tasks, and using a planning model trained to simulate the real world's rewards to the agent. We present results that demonstrate the utility of offline reinforcement learning to efficient price-setting in the energy demand response problem.},
booktitle = {Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {131–139},
numpages = {9},
keywords = {aggregation, reinforcement learning, prosumer, transactive energy, microgrid},
location = {Coimbra, Portugal},
series = {BuildSys '21}
}

@inproceedings{10.5555/3535850.3536123,
author = {Delcourt, Kevin},
title = {Towards Multi-Agent Interactive Reinforcement Learning for Opportunistic Software Composition in Ambient Environments},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In order to manage the ever-growing number of devices present in modern and future ambient environments, as well as their dynamics and openness, we aim to propose a distributed multi-agent system that learns, in interaction with a human user, what would be their preferred applications given the services available.The goal of this Ph.D. thesis is to focus on the interaction between a reinforcement learning system and the human user, to improve the system's learning capabilities as well as the user's ease with the system, and ultimately build a working prototype, usable by end-users.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1839–1840},
numpages = {2},
keywords = {multi-agent system, emergence, human-in-the-loop, human-AI interaction, machine learning, ambient intelligence},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3511616.3513104,
author = {Rajapakshe, Thejan and Rana, Rajib and Khalifa, Sara and Liu, Jiajun and Schuller, Bjorn},
title = {A Novel Policy for Pre-Trained Deep Reinforcement Learning for Speech Emotion Recognition},
year = {2022},
isbn = {9781450396066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511616.3513104},
doi = {10.1145/3511616.3513104},
abstract = {Deep Reinforcement Learning (deep RL) has gained tremendous success in gaming but it has rarely been explored for Speech Emotion Recognition (SER). In the RL literature, policy used by the RL agent plays a major role in action selection, however, there is no RL policy tailored for SER. Also, an extended learning period is a general challenge for deep RL, which can impact the speed of learning for SER. In this paper, we introduce a novel policy, the “Zeta policy” tailored for SER and introduce pre-training in deep RL to achieve a faster learning rate. Pre-training with a cross dataset was also studied to discover the feasibility of pre-training the RL agent with a similar dataset in a scenario where real environmental data is not available. We use “IEMOCAP” and “SAVEE” datasets for the evaluation with the problem of recognising four emotions, namely happy, sad, angry, and neutral. The experimental results show that the proposed policy performs better than existing policies. Results also support that pre-training can reduce training time and is robust to a cross-corpus scenario.},
booktitle = {Proceedings of the 2022 Australasian Computer Science Week},
pages = {96–105},
numpages = {10},
location = {Brisbane, Australia},
series = {ACSW '22}
}

@inproceedings{10.5555/1639809.1639860,
author = {Wang, Jun and Tropper, Carl},
title = {Selecting GVT Interval for Time-Warp-Based Distributed Simulation Using Reinforcement Learning Technique},
year = {2009},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {In a Time-Warp-based distributed simulation system, a simulation process must save its states and events to handle rollbacks. Periodically, the global minimum of the timestamps of events and messages in the entire system is calculated. This value is known as the global virtual time (GVT), and it plays an important role in a Time Warp system. GVT is only computed periodically because of the computation overhead. An important problem is to determine the optimal interval between two GVT computations. In this paper we present a new approach that uses a simple Reinforcement Learning technique to select the optimal GVT interval. Used in a Time-Warp-based distributed VLSI simulation system, our method was successful in selecting good GVT interval and improving the system's performance.},
booktitle = {Proceedings of the 2009 Spring Simulation Multiconference},
articleno = {49},
numpages = {7},
keywords = {parallel and distributed simulation, reinforcement learning, distributed VLSI simulation, n-armed bandit, time warp, GVT},
location = {San Diego, California},
series = {SpringSim '09}
}

@inproceedings{10.1145/3490486.3538373,
author = {Green, Etan A. and Plunkett, E. Barry},
title = {The Science of the Deal: Optimal Bargaining on EBay Using Deep Reinforcement Learning},
year = {2022},
isbn = {9781450391504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490486.3538373},
doi = {10.1145/3490486.3538373},
abstract = {Bargaining is ubiquitous. How can people bargain better? We train a reinforcement learning agent to bargain optimally in "Best Offer" listings on eBay, and we characterize its behavior in a manner that humans can use. As a buyer, the agent starts lower than human buyers and bargains longer. As the seller, the agent interprets offers as signals---of the buyer's willingness to pay and of the item's desirability---that human sellers ignore. Simple strategies derived from these agents purchase more items for lower prices than human buyers and sell more items for higher prices than human sellers.},
booktitle = {Proceedings of the 23rd ACM Conference on Economics and Computation},
pages = {1–27},
numpages = {27},
keywords = {eBay, deep reinforcement learning, bargaining},
location = {Boulder, CO, USA},
series = {EC '22}
}

@inproceedings{10.1145/3340531.3411913,
author = {Shen, Wei and He, Xiaonan and Zhang, Chuheng and Ni, Qiang and Dou, Wanchun and Wang, Yan},
title = {Auxiliary-Task Based Deep Reinforcement Learning for Participant Selection Problem in Mobile Crowdsourcing},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411913},
doi = {10.1145/3340531.3411913},
abstract = {In mobile crowdsourcing (MCS), the platform selects participants to complete location-aware tasks from the recruiters aiming to achieve multiple goals (e.g., profit maximization, energy efficiency, and fairness). However, different MCS systems have different goals and there are possibly conflicting goals even in one MCS system. Therefore, it is crucial to design a participant selection algorithm that applies to different MCS systems to achieve multiple goals. To deal with this issue, we formulate the participant selection problem as a reinforcement learning problem and propose to solve it with a novel method, which we call auxiliary-task based deep reinforcement learning (ADRL). We use transformers to extract representations from the context of the MCS system and a pointer network to deal with the combinatorial optimization problem. To improve the sample efficiency, we adopt an auxiliary-task training process that trains the network to predict the imminent tasks from the recruiters, which facilitates the embedding learning of the deep learning model. Additionally, we release a simulated environment on a specific MCS task, the ride-sharing task, and conduct extensive performance evaluations in this environment. The experimental results demonstrate that ADRL outperforms and improves sample efficiency over other well-recognized baselines in various settings.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
pages = {1355–1364},
numpages = {10},
keywords = {reinforcement learning, mobile crowdsourcing, participant selection problem},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.5555/3306127.3332034,
author = {Narayan, Akshay and Leong, Tze Yun},
title = {Effects of Task Similarity on Policy Transfer with Selective Exploration in Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The SEAPoT algorithm [9] is a knowledge transfer mechanism in model-based reinforcement learning. By constructing subspaces around the changed regions, and selectively and efficiently exploring the target task, the transfer is most effective when the source and target tasks share similar objectives but differ in the transition dynamics. In this work, we identify the similarity between tasks using a new light-weight metric, based on the Jensen-Shannon distance, and show how the degree of similarity affects the transfer efficacy. We also empirically show that SEAPoT performs better in terms of jump starts and average rewards, as compared to the state-of-the-art policy reuse methods.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2132–2134},
numpages = {3},
keywords = {policy transfer, similarity metric, reinforcement learning, transfer in RL},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

