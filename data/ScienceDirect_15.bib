@article{SUN2019401,
title = {TIDE: Time-relevant deep reinforcement learning for routing optimization},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {401-409},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19305424},
author = {Penghao Sun and Yuxiang Hu and Julong Lan and Le Tian and Min Chen},
keywords = {Deep reinforcement learning, Recurrent neural network, Routing optimization, Software-defined networking},
abstract = {Routing optimization has been researched in network design for a long time, and plenty of optimization schemes have been proposed from both academia and industry. However, such schemes are either too complicated in applications or far from the optimal performance. In recent years, with the development of Software-defined Networking (SDN) and Artificial Intelligence (AI), AI-based methods of routing strategy are being considered. In this paper, we propose TIDE, an intelligent network control architecture based on deep reinforcement learning that can dynamically optimize routing strategies in an SDN network without human experience. TIDE is implemented and validated on a real network environment. Experiment result shows that TIDE can adjust the routing strategy dynamically according to the network condition and can improve the overall network transmitting delay by about 9% compared with traditional algorithms.}
}
@article{QIU2022109362,
title = {Trajectory planning and vibration control of translation flexible hinged plate based on optimization and reinforcement learning algorithm},
journal = {Mechanical Systems and Signal Processing},
volume = {179},
pages = {109362},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.109362},
url = {https://www.sciencedirect.com/science/article/pii/S0888327022004940},
author = {Zhi-cheng Qiu and Guo-hao Chen and Xian-min Zhang},
keywords = {Flexible hinged plate, Vibration suppression, System identification, Immune optimization, Auto-SAC algorithm},
abstract = {Aiming at the flexible hinged plate with translational motion,a hybrid control strategy combining motion trajectory optimization and piezoelectric active control is developed. Combining finite element modeling (FEM) and experimental system identification, the piezoelectric driving model and motor acceleration driving model of the flexible hinged plate are obtained. On this basis, the immune optimization algorithm is adopted to obtain the optimal trajectory of vibration suppression. An Auto-Soft-Actor-Critic (Auto-SAC) reinforcement learning (RL) algorithm is designed to train modal active controllers. The experimental setup is constructed. Compared with the classical trajectory, the optimized trajectory shows a good effect in avoiding excitation vibration. For piezoelectric active control, the Auto-SAC RL modal controller is superior to the large-gain PD controller in suppressing vibration, and its nonlinearity effectively overcomes the problem that the traditional linear controller has limited ability to suppress small amplitude vibration. In addition, the hybrid control strategy of trajectory optimization and Auto-SAC RL modal controllers can effectively reduce and suppress the vibration of the flexible hinged plate in translational motion.}
}
@article{CHEN2020118931,
title = {Stochastic model predictive control for energy management of power-split plug-in hybrid electric vehicles based on reinforcement learning},
journal = {Energy},
volume = {211},
pages = {118931},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.118931},
url = {https://www.sciencedirect.com/science/article/pii/S0360544220320387},
author = {Zheng Chen and Hengjie Hu and Yitao Wu and Yuanjian Zhang and Guang Li and Yonggang Liu},
keywords = {Energy management strategy, Reinforcement learning, Markov chain, Velocity prediction, Stochastic model prediction control},
abstract = {In this paper, a stochastic model predictive control (MPC) method based on reinforcement learning is proposed for energy management of plug-in hybrid electric vehicles (PHEVs). Firstly, the power transfer of each component in a power-split PHEV is described in detail. Then an effective and convergent reinforcement learning controller is trained by the Q-learning algorithm according to the driving power distribution under multiple driving cycles. By constructing a multi-step Markov velocity prediction model, the reinforcement learning controller is embedded into the stochastic MPC controller to determine the optimal battery power in predicted time domain. Numerical simulation results verify that the proposed method achieves superior fuel economy that is close to that by stochastic dynamic programming method. In addition, the effective state of charge tracking in terms of different reference trajectories highlight that the proposed method is effective for online application requiring a fast calculation speed.}
}
@article{BHOLA2023112018,
title = {Multi-fidelity reinforcement learning framework for shape optimization},
journal = {Journal of Computational Physics},
volume = {482},
pages = {112018},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112018},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123001134},
author = {Sahil Bhola and Suraj Pawar and Prasanna Balaprakash and Romit Maulik},
keywords = {Deep reinforcement learning, Multi-fidelity modeling, Transfer control},
abstract = {Deep reinforcement learning (DRL) is a promising outer-loop intelligence paradigm which can deploy problem solving strategies for complex tasks. Consequently, DRL has been utilized for several scientific applications, specifically in cases where classical optimization or control methods are limited. One key limitation of conventional DRL methods is their episode-hungry nature which proves to be a bottleneck for tasks which involve costly evaluations of a numerical forward model. In this article, we address this limitation of DRL by introducing a controlled transfer learning framework that leverages a multi-fidelity simulation setting. Our strategy is deployed for an airfoil shape optimization problem at high Reynolds numbers, where our framework can learn an optimal policy for generating efficient airfoil shapes by gathering knowledge from multi-fidelity environments and reduces computational costs by over 30%. Furthermore, our formulation promotes policy exploration and generalization to new environments, thereby preventing over-fitting to data from solely one fidelity. Our results demonstrate this framework's applicability to other scientific DRL scenarios where multi-fidelity environments can be used for policy learning.}
}
@article{XU2023106774,
title = {Operational optimization for the grid-connected residential photovoltaic-battery system using model-based reinforcement learning},
journal = {Journal of Building Engineering},
volume = {73},
pages = {106774},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.106774},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223009531},
author = {Yang Xu and Weijun Gao and Yanxue Li and Fu Xiao},
keywords = {Deep reinforcement learning, Operational optimization, Photovoltaic battery systems, Actor-critic algorithms},
abstract = {The development of distributed photovoltaic and energy storage devices has created challenges for energy management systems due to uncertainty and mismatch between local generation and residents' energy demand. Reinforcement learning is gaining attention as a control algorithm, but traditional model-free RL has data quality and quantity limitations for energy management applications. Therefore, this study proposed a model-based deep RL method to optimize the operation control of the energy storage system by taking the measured dataset of an actual existing building in Japan as the research object. With an optimization goal of reducing the microgrid's energy cost and ensuring the PV self-consumption ratio, we designed a new reward function for these goals. We took the benchmark strategy currently used by the target building's energy management system as the baseline model in the experiment. We applied four advanced RL algorithms (PPO, DQN, DDPG, and TD3) to optimize the baseline model. The results show that the proposed RL design can better achieve the two optimization objectives of minimizing energy cost and maximizing the PV self-consumption ratio. Among them, the TD3 algorithm presented the best performance. Compared with the baseline model, its annual energy cost can be reduced by 17.82%, and the photovoltaic self-consumption ratio can be increased by 0.86%. In addition, the model-based RL method proposed in this paper can provide a better energy management strategy with the training set of only one and a half years of measured data, which proves that it has a high potential for practical application.}
}
@article{EMAMJOMEHZADEH2023117046,
title = {Combining urban metabolism and reinforcement learning concepts for sustainable water resources management: A nexus approach},
journal = {Journal of Environmental Management},
volume = {329},
pages = {117046},
year = {2023},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.117046},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722026196},
author = {Omid Emamjomehzadeh and Reza Kerachian and Mohammad Javad Emami-Skardi and Marzieh Momeni},
keywords = {Water-energy-food-GHG nexus, Physical-behavioral simulation, Social choice procedures, Wastewater reuse, WaterMet, Tehran city},
abstract = {Modeling Water-Energy-Food (WEF) nexus is necessary for integrated water resources management (IWRM), especially in urban areas. This paper presents a new urban water metabolism-based methodology for WEF nexus modeling and management. A behavioral simulation model is used to incorporate the characteristics of stakeholders in an urban area. Modified versions of the Borda count, Copeland rule, and fallback bargaining procedures are implemented to choose the socially acceptable management scenarios. Finally, the selected scenarios’ effectiveness is evaluated using the fairness and total utility indices. The applicability of the proposed methodology is evaluated by applying it to the Kan River basin, Tehran, Iran, which is suffering from some water and environmental issues. The considered management scenarios include adding new water sources, leakage control plans, using rubber dams for enhancing groundwater recharge, revising water allocation priorities, and developing semi-centralized or decentralized reuse strategies for reclaimed wastewater. Results illustrate that considering different fluxes (i.e., water quantity, pollutants, energy, greenhouse gases (GHG), and materials) is as important as incorporating the social characteristics of stakeholders. Simulating the socially acceptable scenario shows that the aquifer's average water level improves by 3 (m), and its average nitrate concentration reduces by 16 (mg/L) in comparison with the business as usual (BAU) scenario. In addition, by implementing different water reuse strategies, which are energy-intensive, total energy consumption is reduced by 5% due to less groundwater pumping. Also, the selected scenario decreases GHG emissions by 18% and increases the sequestrated carbon dioxide by 20%. In conclusion, the proposed decision support tool can provide policies for sustainable water resources management considering water quality and quantity issues, energy usage, and GHG emission.}
}
@article{ERDEN2005151,
title = {DYNAMIC GAIT PATTERN GENERATION WITH REINFORCEMENT LEARNING},
journal = {IFAC Proceedings Volumes},
volume = {38},
number = {1},
pages = {151-156},
year = {2005},
note = {16th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20050703-6-CZ-1902.01295},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016373074},
author = {Mustafa Suphi Erden and Kemal Leblebicioĝlu},
keywords = {Six-legged robot, walking, gait pattern, reinforcement learning, radial basis function neural network},
abstract = {This paper presents the gait pattern generation work performed for the six-legged robot EA308 developed in our laboratory. The aim is to achieve a dynamically developing gait pattern generation structure using reinforcement learning. For the six legged robot a simplified simulative model is constructed. The algorithm constructs a radial basis function neural network (RBFNN) to command proper leg configurations to the simulative robot. The weights of the RBFNN are learned using reinforcement learning. The developed structure succeeded in learning gait patterns compatible with different speeds of the robot.}
}
@article{P2020106762,
title = {New interactive agent based reinforcement learning approach towards smart generator bidding in electricity market with micro grid integration},
journal = {Applied Soft Computing},
volume = {97},
pages = {106762},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106762},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620307006},
author = {Kiran P. and K.R.M. {Vijaya Chandrakala}},
keywords = {Deregulated electricity market, Locational Marginal Price (LMP), Reinforcement learning, Generation Company (GenCo), Load Serving Entity (LSE)},
abstract = {In order to suit the needs of the dynamically changing electricity market, software developers have developed various tools taking in to account the need of artificial intelligence for the electricity market entities. Algorithms in artificial intelligence are often divided into either supervised, unsupervised and reinforcement learning approach. A reinforcement learning when compared to supervised and unsupervised learning makes use of agent to learn from interaction with an environment and receives rewards based on the action it takes. It either exploits or explores in finding a solution. In the deregulated power market, the GenCos are modeled as agent by which the GenCo learns the market environment as agent and explores to get profited The Multi-agent based simulation is an effective method to incorporate this sort of intelligence and for providing efficient communication among the market entities. Using Multi-agent system, the problem existing in electricity market can be reduced since each entity problem can be solved by an individual agent. Multi-agent based reinforcement learning algorithm is used to handle the electricity market data. Here an agent based computational framework named Agent Based Modeling of Electricity Systems (AMES) under Java platform is developed for the design of electricity market. Market Agents balances the supply and demand through Market Clearing Price (no congestion) and Locational Marginal Price (congestion management) by performing optimal power flow. The agent also maximizes the profit of the Generator Companies (GenCo’s) through new learning strategy proposed using Variant Roth–Erev (VRE) interactive reinforcement learning method towards smart bidding among GenCo’s. The congestion relieving action in the transmission line and its effects on GenCo learning is discussed in this paper. The analysis is carried out on the electricity whole sale market functioning on a day-ahead basis developed by means of location and timing of injection of power. IEEE-3 bus system and IEEE-30 bus system with microgrid considered as non-dispatchable load is considered using agent based analysis. This technique helps the GenCo’s to attain possible high net earnings even with microgrid integration thus helps to relieve the congestion in the transmission lines.}
}
@article{SHANG2022103419,
title = {A new ensemble deep graph reinforcement learning network for spatio-temporal traffic volume forecasting in a freeway network},
journal = {Digital Signal Processing},
volume = {123},
pages = {103419},
year = {2022},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2022.103419},
url = {https://www.sciencedirect.com/science/article/pii/S1051200422000367},
author = {Pan Shang and Xinwei Liu and Chengqing Yu and Guangxi Yan and Qingqing Xiang and Xiwei Mi},
keywords = {Traffic volume forecasting, Graph convolutional network, Graph attention network, Deep reinforcement learning, Spatio-temporal traffic volume series},
abstract = {Spatio-temporal traffic volume forecasting technologies can effectively improve freeway traffic efficiency and the travel comfort of humans. To construct a high-precision traffic volume forecasting model, this study proposed a new ensemble deep graph reinforcement learning network. The modeling process of the spatio-temporal prediction model mainly included three steps. In step I, raw spatiotemporal traffic network datasets (traffic volumes, traffic speeds, weather, and holidays) were preprocessed and the adjacency matrix was constructed. In step II, a graph attention network (GAT) and graph convolution network (GCN) were used as the main predictors to build the spatio-temporal traffic volume forecasting model and obtain the forecasting results, respectively. In step III, deep reinforcement learning was used to effectively analyze the correlations between the forecasting results from these two neural networks and the final results, so as to optimize the weight coefficient. The final result of the proposed model was obtained by combining the forecasting results from the GAT and GCN with the weight coefficient. Based on summarizing and analyzing the experimental results, it can be concluded that: (1) deep reinforcement learning can effectively integrate the two different graph neural networks and achieve better results than traditional ensemble methods; and (2) the presented ensemble model performs better than twenty-one models proposed by other researchers for all studied cases.}
}
@article{ZHANG2023219,
title = {Deep reinforcement learning based scheduling strategy for federated learning in sensor-cloud systems},
journal = {Future Generation Computer Systems},
volume = {144},
pages = {219-229},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23000870},
author = {Tinghao Zhang and Kwok-Yan Lam and Jun Zhao},
keywords = {Federated learning, Sensor-cloud systems, Scheduling strategy, Deep reinforcement learning, Wireless networks, Communication},
abstract = {Sensor-cloud systems (SCSs) aim to provide flexible configurable platforms for monitoring and controlling the IoT-enabled applications. By integrating sensors, wireless networks and cloud for managing sensors, collecting data, and automating decision-making, the collected sensing data are typically used for machine learning purposes. With increasing emphasis in privacy protection, Federated Learning (FL) is widely adopted for enhancing privacy preservation. FL enables sharing of data for machine learning while preserving the privacy of the data owners. In SCSs, FL involves a large number of edge nodes in order to ensure a sufficient amount of data for model training. However, FL inevitably incurs prohibitive overheads if it simply gathers data from all the nodes, hence making it desirable to adopt some scheduling strategy so that data are collected only from a selected subset of nodes. This paper proposes a scheduling strategy based on deep reinforcement learning (DRL) for improving the performance and efficiency of FL in SCSs. The DRL environment, such as state space, action space, and reward function, is carefully designed. Proximal policy optimization is employed to train the DRL agent. Experimental results demonstrated that the proposed method outperforms other baselines on both independent and identically distributed (IID) and non-IID datasets.}
}
@article{KOKOLAKIS2023110732,
title = {Bounded rational Dubins vehicle coordination for target tracking using reinforcement learning},
journal = {Automatica},
volume = {149},
pages = {110732},
year = {2023},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2022.110732},
url = {https://www.sciencedirect.com/science/article/pii/S0005109822005982},
author = {Nick-Marios T. Kokolakis and Kyriakos G. Vamvoudakis},
keywords = {Game theory, Target tracking, Bounded rationality, Reinforcement learning, Switched systems, Target allocation},
abstract = {In this paper, we address the problem of cooperative tracking of multiple heterogeneous targets by deploying multiple and heterogeneous pursuers exhibiting different decision-making capabilities. Initially, under infinite resources, we formulate a game between the evader and the pursuing team, with an evader being the maximizing player and the pursuing team being the minimizing one. Subsequently, we relax the perfect rationality assumption via the use of a level-k thinking framework that allows the evaders to not exhibit the same levels of rationality. Such rationality policies are computed by using a reinforcement learning-based architecture and are proven to form Nash policies as the thinking levels increase. Finally, in the case of multiple pursuers against multiple targets, we develop a switched learning scheme with multiple convergence sets by assigning the most intelligent pursuers to the most intelligent evaders.}
}
@article{YANG2015577,
title = {Reinforcement learning for optimal control of low exergy buildings},
journal = {Applied Energy},
volume = {156},
pages = {577-586},
year = {2015},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2015.07.050},
url = {https://www.sciencedirect.com/science/article/pii/S030626191500879X},
author = {Lei Yang and Zoltan Nagy and Philippe Goffin and Arno Schlueter},
keywords = {Low exergy building systems, Zero net energy buildings, Reinforcement learning control, Energy efficient buildings, Sustainable building systems},
abstract = {Over a third of the anthropogenic greenhouse gas (GHG) emissions stem from cooling and heating buildings, due to their fossil fuel based operation. Low exergy building systems are a promising approach to reduce energy consumption as well as GHG emissions. They consists of renewable energy technologies, such as PV, PV/T and heat pumps. Since careful tuning of parameters is required, a manual setup may result in sub-optimal operation. A model predictive control approach is unnecessarily complex due to the required model identification. Therefore, in this work we present a reinforcement learning control (RLC) approach. The studied building consists of a PV/T array for solar heat and electricity generation, as well as geothermal heat pumps. We present RLC for the PV/T array, and the full building model. Two methods, Tabular Q-learning and Batch Q-learning with Memory Replay, are implemented with real building settings and actual weather conditions in a Matlab/Simulink framework. The performance is evaluated against standard rule-based control (RBC). We investigated different neural network structures and find that some outperformed RBC already during the learning phase. Overall, every RLC strategy for PV/T outperformed RBC by over 10% after the third year. Likewise, for the full building, RLC outperforms RBC in terms of meeting the heating demand, maintaining the optimal operation temperature and compensating more effectively for ground heat. This allows to reduce engineering costs associated with the setup of these systems, as well as decrease the return-of-invest period, both of which are necessary to create a sustainable, zero-emission building stock.}
}
@article{SYKIOTIS2023101124,
title = {A self-sustained EV charging framework with N-step deep reinforcement learning},
journal = {Sustainable Energy, Grids and Networks},
volume = {35},
pages = {101124},
year = {2023},
issn = {2352-4677},
doi = {https://doi.org/10.1016/j.segan.2023.101124},
url = {https://www.sciencedirect.com/science/article/pii/S2352467723001327},
author = {Stavros Sykiotis and Christoforos Menos-Aikateriniadis and Anastasios Doulamis and Nikolaos Doulamis and Pavlos S. Georgilakis},
keywords = {Smart grid, Smart charging, Demand response, Electric vehicle, Solar power, Self-consumption},
abstract = {Decarbonization of the transport sector is a major challenge in the transition towards net-zero emissions. Even though the penetration of electric vehicles (EV) in the passenger vehicle fleet is increasing, the energy mix is not yet dominated by renewables. This leads to the utilization of fossil-based power generation for EV charging, especially during peak hours. In this work, we introduce a residential smart EV charging framework that prioritizes solar photovoltaic (PV) power self-consumption to accelerate the transition to a carbon neutral passenger vehicle fleet. Our approach employs N-Step Deep Reinforcement Learning to charge the EV with clean energy from a PV, without neglecting other major factors that influence end users’ behavior, such as electricity cost or EV charging tendencies. Historical smart-meter data from the Pecan Street dataset on total consumption, EV demand and solar generation has been utilized as input features to train the Deep RL method so that it decides on a real-time basis whether to charge or not the EV, without the need for foresight of future observations. Experimental results on six residential houses validate that, compared to uncontrolled EV charging, the proposed method can increase the average self-consumption of solar energy for EV charging by 19.66%, as well as reduce network stress by 7% and electricity bill by 10.3%.}
}
@article{TRIKI201510,
title = {Hierarchical power management of a system with autonomously power-managed components using reinforcement learning},
journal = {Integration},
volume = {48},
pages = {10-20},
year = {2015},
issn = {0167-9260},
doi = {https://doi.org/10.1016/j.vlsi.2014.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167926014000364},
author = {M. Triki and Y. Wang and A.C. Ammari and M. Pedram},
keywords = {Power management, Reinforcement learning, Temporal difference learning, Semi-Markov decision process},
abstract = {This paper presents a hierarchical dynamic power management (DPM) framework based on reinforcement learning (RL) technique, which aims at power savings in a computer system with multiple I/O devices running a number of heterogeneous applications. The proposed framework interacts with the CPU scheduler to perform effective application-level scheduling, thereby enabling further power savings. Moreover, it considers non-stationary workloads and differentiates between the service request generation rates of various software application. The online adaptive DPM technique consists of two layers: component-level local power manager and system-level global power manager. The component-level PM policy is pre-specified and fixed whereas the system-level PM employs temporal difference learning on semi-Markov decision process as the model-free RL technique, and it is specifically optimized for a heterogeneous application pool. Experiments show that the proposed approach considerably enhances power savings while maintaining good performance levels. In comparison with other reference systems, the proposed RL-based DPM approach, further enhances power savings, performs well under various workloads, can simultaneously consider power and performance, and achieves wide and deep power-performance tradeoff curves. Experiments conducted with multiple service providers confirm that up to 63% maximum energy saving per service provider can be achieved.}
}
@article{XIE2020375,
title = {A composite learning method for multi-ship collision avoidance based on reinforcement learning and inverse control},
journal = {Neurocomputing},
volume = {411},
pages = {375-392},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.05.089},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220309401},
author = {Shuo Xie and Xiumin Chu and Mao Zheng and Chenguang Liu},
keywords = {Ship collision avoidance, Asynchronous advantage actor-critic, Long short-term memory neural network, Inverse control},
abstract = {Model-free reinforcement learning methods have potentials in ship collision avoidance under unknown environments. To defect the low efficiency problem of the model-free reinforcement learning, a composite learning method is proposed based on an asynchronous advantage actor-critic (A3C) algorithm, a long short-term memory neural network (LSTM) and Q-learning. The proposed method uses Q-learning for adaptive decisions between a LSTM inverse model-based controller and the model-free A3C policy. Multi-ship collision avoidance simulations are conducted to verify the effectiveness of the model-free A3C method, the proposed inverse model-based method and the composite learning method. The simulation results indicate that the proposed composite learning based ship collision avoidance method outperforms the A3C learning method and a traditional optimization-based method.}
}
@incollection{MORALES2022111,
title = {Chapter 6 - A brief introduction to supervised, unsupervised, and reinforcement learning},
editor = {Alejandro A. Torres-García and Carlos A. Reyes-García and Luis Villaseñor-Pineda and Omar Mendoza-Montoya},
booktitle = {Biosignal Processing and Classification Using Computational Learning and Intelligence},
publisher = {Academic Press},
pages = {111-129},
year = {2022},
isbn = {978-0-12-820125-1},
doi = {https://doi.org/10.1016/B978-0-12-820125-1.00017-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128201251000178},
author = {Eduardo F. Morales and Hugo Jair Escalante},
keywords = {Supervised learning, Unsupervised learning, Reinforcement learning},
abstract = {Inducing models is a critical aspect of biosignal information processing. Models can be used to learn a mapping from a characterization of signals to categories associated with the problem at hand. Models can also be used to group biosignal information into coherent clusters for further analysis. Models can be used to characterize sequential decision processes to decide the best action to make at a given state, to model certain human behavior. There is a bulk of machine learning techniques that can be used for inducing different models from data. This chapter provides a general overview of the most relevant machine learning techniques that can be used for modeling different aspects of biosignals. The aim of this chapter is thus to serve as a reference of machine learning techniques for biosignal processing students and practitioners.}
}
@article{YAN2023110601,
title = {Immune deep reinforcement learning-based path planning for mobile robot in unknown environment},
journal = {Applied Soft Computing},
volume = {145},
pages = {110601},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110601},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623006191},
author = {Chengliang Yan and Guangzhu Chen and Yang Li and Fuchun Sun and Yuanyuan Wu},
keywords = {Mobile robot, Path planning, DDPG, Reward function, Immune algorithm},
abstract = {A new deep deterministic policy gradient (DDPG) integrating kinematics analysis and immune optimization (KAI-DDPG) is proposed to address the drawbacks of DDPG in path planning. An orientation angle reward component, linear velocity reward factor, and safety performance reward factor are added to the DDPG reward function based on kinematic modeling and analysis of mobile robots. A multi-objective performance index turns the path planning problem into one of multi-objective optimization. We propose KA-DDPG, which uses the orientation angle, linear speed, and safety degree as evaluation indices, and information entropy to alter the influence coefficient of the multi-objective function in the reward function. KAI-DDPG is proposed to address the low learning and training efficiency of KA-DDPG, using immune optimization to optimize the experience samples in the experience buffer pool. Performance indices of traditional path planning and the proposed techniques are compared on a gazebo simulation platform, and the results suggest that KAI-DDPG can mitigate the drawbacks of DDPG, such as a protracted training cycle and poor path planning technique, and can broaden the range of application.}
}
@article{ZHAO2022108541,
title = {Reinforcement learning for adaptive maintenance policy optimization under imperfect knowledge of the system degradation model and partial observability of system states},
journal = {Reliability Engineering & System Safety},
volume = {224},
pages = {108541},
year = {2022},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2022.108541},
url = {https://www.sciencedirect.com/science/article/pii/S0951832022001922},
author = {Yunfei Zhao and Carol Smidts},
keywords = {Condition-based maintenance, Maintenance policy optimization, Imperfect knowledge, Partial observability, Reinforcement learning, Sequential Bayesian inference, Partially observable Markov decision process, Multi-state system, Hidden Markov model, Nuclear power plant},
abstract = {Maintenance policy optimization usually is faced with challenges that arise from an imperfect knowledge of system degradation models and from the partial observability of system degradation states. This paper proposes a reinforcement learning method to address these two challenges for a class of maintenance problems with Markov degradation processes. The reinforcement learning approach consists of a learning component and a planning component. Using sequentially collected observations, at each step of decision-making the learning component improves the knowledge of system degradation in terms of the probability distributions of the transition rates based on sequential Bayesian inference. Using the updated transition rates, at each step of decision-making the maintenance policy optimization problem is then formulated as a partially observable Markov decision problem, and the planning component computes the optimal maintenance policy that maximizes the expected cumulative reward. The proposed method is illustrated using a numerical example with repair and inspection maintenance actions. The result shows that as more observations are collected, the learning component progressively learns the true system degradation process, and the planning component adjusts the optimal maintenance policy accordingly as well, which leads to increased reward.}
}
@article{FALLAHI2022118018,
title = {A constrained multi-item EOQ inventory model for reusable items: Reinforcement learning-based differential evolution and particle swarm optimization},
journal = {Expert Systems with Applications},
volume = {207},
pages = {118018},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118018},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422012350},
author = {Ali Fallahi and Erfan {Amani Bani} and Seyed Taghi Akhavan Niaki},
keywords = {Economic order quantity, Recovery process, Differential evolution, Particle swarm optimization, Q-Learning},
abstract = {The growing environmental concerns, governmental regulations, and significant cost savings are the primary motivations for companies to consider the reuse and recovery of products in their inventory system. The previous research ignored several realistic features of reusable items inventory systems, such as the presence of multiple products and operational constraints. For the first time, this paper presents a new multiproduct economic order quantity inventory model for an inventory system of reusable products. The goal of the model is to determine the optimal replenishment quantity and reuse quantity of each item so that the system's total cost is minimized. Several operational constraints are considered to provide a more realistic framework, such as the total available budget, warehouse space, and holding cost. Due to the nonlinearity of the presented model, differential evolution (DE) and particle swarm optimization (PSO) algorithms are utilized as two solution approaches. However, these algorithms' performance is highly dependent on their control parameters. Therefore, for the first time, two new variants of these algorithms, called DEQL and PSOQL, are presented in this study, where the control parameters of algorithms are not pre-determined. A powerful reinforcement learning algorithm, Q-learning, adapts these values intelligently. In other words, as a research contribution, this research aims at introducing a new variant of hybrid the DE and PSO algorithms in which a machine learning algorithm controls the value of metaheuristic parameters. The other parameters of the proposed algorithms are tunned employing the Taguchi method. Extensive numerical examples are established in different sizes, and the outputs are discussed in terms of several criteria. Statistical analysis of the results is performed, demonstrating that the proposed reinforcement learning-based parameter adaption has significantly improved algorithms' performance.}
}
@article{SONG2023109117,
title = {Proximal policy optimization through a deep reinforcement learning framework for remedial action schemes of VSC-HVDC},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {150},
pages = {109117},
year = {2023},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2023.109117},
url = {https://www.sciencedirect.com/science/article/pii/S0142061523001746},
author = {Sungyoon Song and Yungun Jung and Gilsoo Jang and Seungmin Jung},
keywords = {Artificial Intelligence, Proximal Policy Optimization, VSC-HVDC, Remedial Action Schemes, Energy Management System},
abstract = {A proximal policy optimization (PPO)-based back-to-back VSC-HVDC emergency control strategy based on multi-agent deep reinforcement learning (DRL) approach is proposed for use in an energy management system (EMS). In this scheme, an advanced DRL algorithm is proposed by implementing both PPO and a communication neural network for large power systems. The PPO modeled as intelligent agents with objective functions have shown a higher convergence performance than have existing DRL algorithms. Further, the model was demonstrated to effectively address voltage variances caused by the high penetration of renewable energy sources. By implementing PPO, the learning procedure is stabilized and made robust to continuous changes in network topology. To escalate the effectiveness of the proposed algorithm, a comprehensive case studies were conducted on an standard test systems and Korean power system considering variations in load and PV generation and a weak centralized communication environment. The results indicate that outstanding control performance and autonomously regulated bus voltage and line flows, thereby validating the effectiveness of the method.}
}
@article{RENAUDO2015178,
title = {Respective Advantages and Disadvantages of Model-based and Model-free Reinforcement Learning in a Robotics Neuro-inspired Cognitive Architecture},
journal = {Procedia Computer Science},
volume = {71},
pages = {178-184},
year = {2015},
note = {6th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2015, 6-8 November Lyon, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.194},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036558},
author = {Erwan Renaudo and Benoît Girard and Raja Chatila and Mehdi Khamassi},
keywords = {Robotic Cognitive Architecture, Reinforcement Learning, Biological Inspiration},
abstract = {Combining model-based and model-free reinforcement learning systems in robotic cognitive architectures appears as a promising direction to endow artificial agents with flexibility and decisional autonomy close to mammals. In particular, it could enable robots to build an internal model of the environment, plan within it in response to detected environmental changes, and avoid the cost and time of planning when the stability of the environment is recognized as enabling habit learning. However, previously proposed criteria for the coordination of these two learning systems do not scale up to the large, partial and uncertain models autonomously learned by robots. Here we precisely analyze the performances of these two systems in an asynchronous robotic simulation of a cube-pushing task requiring a permanent trade-off between speed and accuracy. We propose solutions to make learning successful in these conditions. We finally discuss possible criteria for their efficient coordination within robotic cognitive architectures.}
}
@article{MARAVALL2015180,
title = {Alignment in vision-based syntactic language games for teams of robots using stochastic regular grammars and reinforcement learning: The fully autonomous case and the human supervised case},
journal = {Robotics and Autonomous Systems},
volume = {63},
pages = {180-186},
year = {2015},
note = {Cognition-oriented Advanced Robotic Systems},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2014.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0921889014001985},
author = {Darío Maravall and Jack Mario Mingo and Javier {De Lope}},
keywords = {Stochastic grammars, Reinforcement learning, Dynamics of artificial languages, Language games, Multi-robot systems, Human–robots interaction},
abstract = {This paper approaches the syntactic alignment of a robot team by means of dialogic language games and by applying online probabilistic reinforcement learning algorithms. The syntactic alignment is studied under two different configurations of the robot team: (a) when the team is formed exclusively by robots and (b) when a human is included in the team, in which case the human is endowed with a natural language to communicate with the other members of the team. For the two above mentioned cases, we are interested in the analysis of the convergence of the team to an optimal common language. The main contribution of the paper is the application of stochastic regular grammars, with learning capability, to generate the robots team’s language. Apart from the analysis of the convergence to a common language in the case of a fully autonomous robot team without human intervention we are also particularly interested in analyzing how the syntactic alignment of the robot team can be influenced or mediated by humans. The paper is organized as follows: first, we describe the syntactic language games, in particular the type of grammar and syntactic rules of the robots team’s language and the dynamic process of the language games which are based on dialogic communicative acts and a reinforcement learning policy that allows the robot team to converge to a common language. Afterwards, the experimental results are presented and discussed. The experimental work has been organized around the linguistic description of visual scenes of the blocks world type. The general conclusion of our experiments can be briefly stated in this way: “for the fully autonomous case (only robots) the final emergent grammar is arbitrary, while in the second case of including a human in the team the final emergent grammar is the one used by the human”.}
}
@article{SONG2023254,
title = {Data conversion control of virtual network devices in cloud computing: A deep reinforcement learning approach},
journal = {Computer Communications},
volume = {211},
pages = {254-262},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2023.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0140366423003213},
author = {Jian Song},
keywords = {Cloud computing, Virtual appliance deployment, Data conversion optimization, Deep neural networks, Gradient policy algorithm},
abstract = {As the response time of virtual space task requests is affected by different data conversion processes, it becomes more serious to solve the load imbalance problem between the underlying nodes and the underlying data links. The traditional least square method needs to distinguish each node for the remapping of the original virtual space. After the trajectories in different spaces and times are mapped again, it is easier to cause new resource bottlenecks. Aiming at the necessary virtual space remapping cycle of cloud service nodes, this paper selects the nodes with the highest upper and lower limits of load rate according to the dynamic critical value of each node's load rate. It migrates part of the virtual network space on the overloaded service nodes to the non-important nodes with a lower load rate, which is to avoid the gradual formation of new resource bottlenecks after remapping. The data transformation model based on reinforcement learning can further improve the acceptance rate of virtual space requests and reduce the computational overhead caused by space remapping. The experiments compare the performance evaluation of different algorithm models on the simulation platform data conversion tasks, including complex instructions test, virtual network performance test, average remapping link overhead, and root mean square error of the underlying node load rate test. The test results indicate that the calculation accuracy of the proposed method is 7.25% higher than other comparison methods, and the computational overhead is reduced by 15.22%. The overall performance is improved by 10.74%. It is verified that the technology in this paper improves the data conversion control method of virtual network devices in the cloud computing environment, and has practical significance for virtual network remapping and data services in cloud computing.}
}
@article{ALHAJJHASSAN2020101926,
title = {Reinforcement learning framework for freight demand forecasting to support operational planning decisions},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {137},
pages = {101926},
year = {2020},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2020.101926},
url = {https://www.sciencedirect.com/science/article/pii/S1366554519315169},
author = {Lama {Al Hajj Hassan} and Hani S. Mahmassani and Ying Chen},
keywords = {Freight demand forecasting, Time series, Reinforcement learning, Rolling horizon},
abstract = {Freight forecasting is essential for managing, planning operating and optimizing the use of resources. Multiple market factors contribute to the highly variable nature of freight flows, which calls for adaptive and responsive forecasting models. This paper presents a demand forecasting methodology that supports freight operation planning over short to long term horizons. The method combines time series models and machine learning algorithms in a Reinforcement Learning framework applied over a rolling horizon. The objective is to develop an efficient method that reduces the prediction error by taking full advantage of the traditional time series models and machine learning models. In a case study applied to container shipment data for a US intermodal company, the approach succeeded in reducing the forecast error margin. It also allowed predictions to closely follow recent trends and fluctuations in the market while minimizing the need for user intervention. The results indicate that the proposed approach is an effective method to predict freight demand. In addition to clustering and Reinforcement Learning, a method for converting monthly forecasts to long-term weekly forecasts was developed and tested. The results suggest that these monthly-to-weekly long-term forecasts outperform the direct long term forecasts generated through typical time series approaches.}
}
@article{HU2021106967,
title = {RL-VAEGAN: Adversarial defense for reinforcement learning agents via style transfer},
journal = {Knowledge-Based Systems},
volume = {221},
pages = {106967},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106967},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121002306},
author = {Yueyue Hu and Shiliang Sun},
keywords = {Reinforcement learning, Trusted artificial intelligence, Robust agents, Adversarial attack, Adversarial defense},
abstract = {Reinforcement learning (RL) agents parameterized by deep neural networks have achieved great success in many domains. However, deep RL policies have been shown to be vulnerable to adversarial attacks, i.e., inputs with slight perturbations should result in a substantial agent failure. Inspired by recent advances in deep generative networks that have greatly facilitated the development of adversarial attacks, in this paper, we investigate the adversarial robustness of RL agents and propose a novel defense framework for RL based on the idea of style transfer. More precisely, our defense framework containing variational autoencoders (VAEs) and generative adversarial networks (GANs), called RL-VAEGAN, learns the distribution of the styles of the original and adversarial states, respectively, and naturally eliminates the threat of adversarial attacks for RL agents by transferring adversarial states to unperturbed legitimate one under the shared-content latent space assumption. We empirically show that our methods are effective against the state-of-the-art methods in white-box and black-box scenarios with diverse magnitudes of perturbations.}
}
@article{FAN2022108613,
title = {A systematic method for the optimization of gas supply reliability in natural gas pipeline network based on Bayesian networks and deep reinforcement learning},
journal = {Reliability Engineering & System Safety},
volume = {225},
pages = {108613},
year = {2022},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2022.108613},
url = {https://www.sciencedirect.com/science/article/pii/S0951832022002563},
author = {Lin Fan and Huai Su and Wei Wang and Enrico Zio and Li Zhang and Zhaoming Yang and Shiliang Peng and Weichao Yu and Lili Zuo and Jinjun Zhang},
keywords = {Natural gas pipeline network, Gas supply reliability, Preventive maintenance, Bayesian network, Reinforcement learning},
abstract = {This study proposes a method based on Bayesian networks (BNs) to optimize the reliability of gas supply in natural gas pipeline networks. The method integrates probabilistic safety analysis with preventive maintenance to achieve the targets of minimizing gas shortage risk and reducing maintenance costs. For this, the tasks of unit failure probability calculation, system maximum supply capacity analysis, gas supply reliability assessment and system maintenance planning are performed. A stochastic capacity network model is coupled with a Markov model and graph theory to generate the state space of the pipeline network system. BN, is then, proposed as the modeling framework to describe the stochastic behavior of unit failures and customer gas shortage. The system maintenance problem is converted into a Markov decision process (MDP), and solved by using deep reinforcement learning (DRL). The effectiveness of the proposed method is validated on a case study of a European gas pipeline network. The results show that the proposed method outperforms others in identifying optimal maintenance strategies. The DRL-optimized maintenance strategy is capable of responding to a dynamic environment through continuous online learning, considering the randomness of the unit failures and the uncertainty in gas demand profiles.}
}
@article{HSU2023103811,
title = {Sim-to-Lab-to-Real: Safe reinforcement learning with shielding and generalization guarantees},
journal = {Artificial Intelligence},
volume = {314},
pages = {103811},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103811},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001515},
author = {Kai-Chieh Hsu and Allen Z. Ren and Duy P. Nguyen and Anirudha Majumdar and Jaime F. Fisac},
keywords = {Reinforcement learning, Sim-to-Real transfer, Safety analysis, Generalization},
abstract = {Safety is a critical component of autonomous systems and remains a challenge for learning-based policies to be utilized in the real world. In particular, policies learned using reinforcement learning often fail to generalize to novel environments due to unsafe behavior. In this paper, we propose Sim-to-Lab-to-Real to bridge the reality gap with a probabilistically guaranteed safety-aware policy distribution. To improve safety, we apply a dual policy setup where a performance policy is trained using the cumulative task reward and a backup (safety) policy is trained by solving the Safety Bellman Equation based on Hamilton-Jacobi (HJ) reachability analysis. In Sim-to-Lab transfer, we apply a supervisory control scheme to shield unsafe actions during exploration; in Lab-to-Real transfer, we leverage the Probably Approximately Correct (PAC)-Bayes framework to provide lower bounds on the expected performance and safety of policies in unseen environments. Additionally, inheriting from the HJ reachability analysis, the bound accounts for the expectation over the worst-case safety in each environment. We empirically study the proposed framework for ego-vision navigation in two types of indoor environments with varying degrees of photorealism. We also demonstrate strong generalization performance through hardware experiments in real indoor spaces with a quadrupedal robot. See https://sites.google.com/princeton.edu/sim-to-lab-to-real for supplementary material.}
}
@article{MCCLEMENT2022139,
title = {Meta-reinforcement learning for the tuning of PI controllers: An offline approach},
journal = {Journal of Process Control},
volume = {118},
pages = {139-152},
year = {2022},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2022.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0959152422001445},
author = {Daniel G. McClement and Nathan P. Lawrence and Johan U. Backström and Philip D. Loewen and Michael G. Forbes and R. Bhushan Gopaluni},
keywords = {Meta-learning, Deep learning, Reinforcement learning, Adaptive control, Process control, PID control},
abstract = {Meta-learning is a branch of machine learning which trains neural network models to synthesize a wide variety of data in order to rapidly solve new problems. In process control, many systems have similar and well-understood dynamics, which suggests it is feasible to create a generalizable controller through meta-learning. In this work, we formulate a meta reinforcement learning (meta-RL) control strategy that can be used to tune proportional–integral controllers. Our meta-RL agent has a recurrent structure that accumulates “context” to learn a system’s dynamics through a hidden state variable in closed-loop. This architecture enables the agent to automatically adapt to changes in the process dynamics. In tests reported here, the meta-RL agent was trained entirely offline on first order plus time delay systems, and produced excellent results on novel systems drawn from the same distribution of process dynamics used for training. A key design element is the ability to leverage model-based information offline during training in simulated environments while maintaining a model-free policy structure for interacting with novel processes where there is uncertainty regarding the true process dynamics. Meta-learning is a promising approach for constructing sample-efficient intelligent controllers.}
}
@incollection{SITTER2022337,
title = {Convex Q-learning: Reinforcement learning through convex programming},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {337-342},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50056-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596500567},
author = {Sophie Sitter and Damien {van de Berg} and Max Mowbray and Antonio {del Rio Chanona} and Panagiotis Petsagkourakis},
keywords = {machine learning, convex Q-learning, semi-definite programming, data-driven batch optimization, dynamic process control},
abstract = {Over the last decade, Reinforcement Learning (RL) has received significant attention as it promises novel and efficient solutions to complex control problems. This work builds on model-free RL, namely Q-learning, to determine optimal control policies for nonlinear, complex biochemical processes. We propose convex functions instead of deep neural networks as state-action value function approximators to reduce computational complexity. A convex Q-function surrogate is trained using semidefinite programming. The surrogate is then minimized to determine the optimal control action. This results in 75.3% lower computational time compared with deep Q-networks. By alleviating the computational burden of traditional RL approximation functions, this work addresses one of the major obstacles for the successful implementation of RL to real-world engineering applications.}
}
@article{DENG2022115030,
title = {Deep reinforcement learning based energy management strategy of fuel cell hybrid railway vehicles considering fuel cell aging},
journal = {Energy Conversion and Management},
volume = {251},
pages = {115030},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2021.115030},
url = {https://www.sciencedirect.com/science/article/pii/S0196890421012061},
author = {Kai Deng and Yingxu Liu and Di Hai and Hujun Peng and Lars Löwenstein and Stefan Pischinger and Kay Hameyer},
keywords = {Energy management, Fuel cell hybrid railway vehicle, Deep Reinforcement learning (DRL), TD3, Fuel cell aging},
abstract = {In the rail transportation industry, growing energy and environmental awareness requires the use of alternatives to combustion engines. These include hybrid electrically driven railway vehicles powered by fuel cells and batteries. The cost of hydrogen consumption and the lifetime of fuel cells are currently the main challenges that need to be addressed before widespread deployment of fuel cell railway vehicles can be realized. With this in mind, this work focuses on the energy management system with emphasis on optimizing the energy distribution to reduce the overall operational cost. The presented energy management strategy (EMS) aims at minimizing hydrogen consumption and fuel cell aging costs while achieving a favorable balance between battery charging and discharging. In order to take fuel cell aging into account in energy management and mitigate fuel cell aging trough power distribution, an online fuel cell aging estimation model based on four operation modes is introduced and applied. Moreover, the advanced deep reinforcement learning method Twin Delayed Deep Deterministic Policy Gradient (TD3) is used to obtain a promising EMS. To improve the adaptability of the strategy, a stochastic training environment, which is based on real measured speed profiles considering passenger numbers is used for training. Assuming different environmental and passenger transport volumes, the results confirm that the proposed TD3-EMS achieves battery charge-sustaining at low hydrogen consumption while slowing down fuel cell degradation.}
}
@article{DELAVARI2023106356,
title = {Adaptive reinforcement learning interval type II fuzzy fractional nonlinear observer and controller for a fuzzy model of a wind turbine},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106356},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106356},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623005407},
author = {Hadi Delavari and Ali Sharifi},
keywords = {Interval type II fuzzy, Reinforcement learning, Fault-tolerant control (FTC), Fractional calculus, Doubly-fed induction generator (DFIG), Adaptive},
abstract = {This article presents an innovative control technique for doubly fed induction generator (DFIG) based wind turbine. A DFIG equation is initially presented using fractional order Takagi and Sugeno interval type II fuzzy modeling. Moreover, fault and disturbance are considered in the DFIG model. Then, to design an observer that estimates both fault and disturbance, the fuzzy model of DFIG is decoupled into three subsystems using a different linear coordinate transformation: the state subsystem without disturbance and fault, the subsystem of fault without disturbance, and the subsystem of disturbance without fault. In the following, using the state subsystem, an adaptive reinforcement learning TS interval type II fuzzy fractional sliding mode observer is introduced. The observer gains are adjusted by applying proximal policy optimization, which is a new application of reinforcement learning. Robustness against uncertainties, fast convergence speed of estimation, high degree of freedom and precise estimation are among features of the suggested observer. In addition, only by utilizing the estimated state subsystem and obtained mathematical relationships, the actuator faults and disturbance are estimated. A fractional order sliding mode controller is proposed for the DFIG speed controller. Low chattering characteristics and accurate response are achieved owing to the fractional speed controller. Also, to compensate the influence of disturbances and faults, an adaptive interval type II fuzzy fractional sliding mode controller is used for rotor current regulations, which employs the estimated fault and disturbance. High robustness, fast response time, low tracking error and chattering are some of the advantages of the proposed fault tolerant controller (FTC). The closed-loop stability of controllers is proved by using the Lyapunov theorem. Simulations is accomplished to evaluate the effectiveness of the proposed method. The proposed controller is compared with other controllers. Robustness against parameter uncertainty is also investigated.}
}
@article{MABU201212349,
title = {Adaptability analysis of genetic network programming with reinforcement learning in dynamically changing environments},
journal = {Expert Systems with Applications},
volume = {39},
number = {16},
pages = {12349-12357},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.04.038},
url = {https://www.sciencedirect.com/science/article/pii/S0957417412006355},
author = {Shingo Mabu and Andre Tjahjadi and Kotaro Hirasawa},
keywords = {Evolutionary computation, Genetic network programming, Reinforcement learning, Adaptability, Khepera robot},
abstract = {Genetic network programming (GNP) has been proposed as one of the evolutionary algorithms and extended with reinforcement learning (GNP-RL). The combination of evolution and learning can efficiently evolve programs and the fitness improvement has been confirmed in the simulations of tileworld problems, elevator group supervisory control systems, stock trading models and wall following behavior of Khepera robot. However, its adaptability in testing environments, where the situations dynamically change, has not been analyzed in detail yet. In this paper, the adaptation mechanism in the testing environment is introduced and it is confirmed that GNP-RL can adapt to the environmental changes using a robot simulator WEBOTS, especially when unexperienced sensor troubles suddenly occur. The simulation results show that GNP-RL works well in the testing even if wrong sensor information is given because GNP-RL has a function to automatically change programs using alternative actions. In addition, the analysis on the effects of the parameters of GNP-RL is carried out in both training and testing simulations.}
}
@article{LU2022172,
title = {Deep reinforcement learning-based multi-objective edge server placement in Internet of Vehicles},
journal = {Computer Communications},
volume = {187},
pages = {172-180},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422000548},
author = {Jiawei Lu and Jielin Jiang and Venki Balasubramanian and Mohammad R. Khosravi and Xiaolong Xu},
keywords = {Edge server placement, IoV, Deep reinforcement learning, DQN},
abstract = {In the typical scenario of the Internet of Vehicles (IoV), the edge servers (ESs) are laid out near the road side units (RSUs) to process the collected data for a variety of IoV services in real time. Generally, because ESs are lightweight compared with cloud servers, if the ESs are not appropriately distributed, it will cause the unbalanced workload of the ESs. Thus, developing an ES plan to avoid the risk of overload and improve the quality of service (QoS) remains a challenge. To tackle it, a deep reinforcement learning-based multi-objective edge server placement strategy, named DESP, is fully explored, to promote the coverage rate, the workload balancing and reduce the average delay of finishing tasks in the IoV. In particular, the Markov Decision Process (MDP) of the ES placement problem is formulated and the deep reinforcement learning, i.e., Deep Q-Network (DQN) is applied to obtain the optimal placement scheme achieving the multiple objectives above. At last, a real vehicular data set is used for assessing the validity of DESP.}
}
@article{KOULOURIOTIS2008913,
title = {Reinforcement learning and evolutionary algorithms for non-stationary multi-armed bandit problems},
journal = {Applied Mathematics and Computation},
volume = {196},
number = {2},
pages = {913-922},
year = {2008},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2007.07.043},
url = {https://www.sciencedirect.com/science/article/pii/S0096300307007448},
author = {D.E. Koulouriotis and A. Xanthopoulos},
keywords = {Decision-making agents, Action selection, Exploration–exploitation, Multi-armed bandit, Genetic algorithms, Reinforcement learning},
abstract = {Multi-armed bandit tasks have been extensively used to model the problem of balancing exploitation and exploration. A most challenging variant of the MABP is the non-stationary bandit problem where the agent is faced with the increased complexity of detecting changes in its environment. In this paper we examine a non-stationary, discrete-time, finite horizon bandit problem with a finite number of arms and Gaussian rewards. A family of important ad hoc methods exists that are suitable for non-stationary bandit tasks. These learning algorithms that offer intuition-based solutions to the exploitation–exploration trade-off have the advantage of not relying on strong theoretical assumptions while in the same time can be fine-tuned in order to produce near-optimal results. An entirely different approach to the non-stationary multi-armed bandit problem presents itself in the face of evolutionary algorithms. We present an evolutionary algorithm that was implemented to solve the non-stationary bandit problem along with ad hoc solution algorithms, namely action-value methods with e-greedy and softmax action selection rules, the probability matching method and finally the adaptive pursuit method. A number of simulation-based experiments was conducted and based on the numerical results that we obtained we discuss the methods’ performances.}
}
@article{HUANG2023129177,
title = {Deep reinforcement learning based energymanagement strategy considering running costs and energy source aging for fuel cell hybrid electric vehicle},
journal = {Energy},
volume = {283},
pages = {129177},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.129177},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223025719},
author = {Yin Huang and Zehao Kang and Xuping Mao and Haoqin Hu and Jiaqi Tan and Dongji Xuan},
keywords = {Fuel cell hybrid electric vehicle, Energy management strategy, Deep reinforcement learning, Continuous action space, Energy source aging, Running costs},
abstract = {The main contribution of this study is to integrate energy source aging and running costs into the deep reinforcement learning (DRL) based EMS of fuel cell hybrid electric vehicles (FCHEV). For the FCHEV, a multi-objective energy management strategy (EMS) based on twin delayed deep deterministic policy gradient (TD3) is proposed, which aims to simultaneously reduce energy source degradation and lower running costs. To achieve this, the paper innovatively designs the reward function and it's comparative approach. Additionally, it verifies the superiority of the proposed EMS over other EMS based on continuous action space algorithm, including previous action guided deep deterministic policy gradient (PA-DDPG) and soft actor-critic (SAC). Lastly, the agent's action output is changed from fuel cell (FC) current to FC power ratio, and a comparative analysis on results generated by different action outputs is conducted. Simulation results show that the proposed EMS can reduce the running costs while extending the lifespan of battery and FC efficiently. This work holds significant practical significance in the energy distribution of automobiles.}
}
@article{YANG2022108643,
title = {Condition-based maintenance strategy for redundant systems with arbitrary structures using improved reinforcement learning},
journal = {Reliability Engineering & System Safety},
volume = {225},
pages = {108643},
year = {2022},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2022.108643},
url = {https://www.sciencedirect.com/science/article/pii/S0951832022002794},
author = {Ao Yang and Qingan Qiu and Mingren Zhu and Lirong Cui and Weilin Chen and Jianhui Chen},
keywords = {Redundant systems, Condition based maintenance, Markov decision process, Dynamic maintenance strategy, Q learning, Deep Q learning},
abstract = {The condition-based maintenance (CBM) decision-making for redundant systems has attracted increasing attention. Most existing studies are dedicated to k-out-of-n redundant systems and the search of the optimal maintenance policy is efficient for low-dimensional CBM. In practical applications, complex system structures and failure criteria are commonly observed, posing challenges for searching the optimal CBM policy. This paper studies the optimal CBM strategy for redundant systems with arbitrary system structures using improved reinforcement learning, considering failure and economic dependences. The decisions of imperfect repair and replacement of failed components are considered dynamically, and an efficient solution method of dynamic maintenance strategy is investigated via improved reinforcement learning incorporating re-learning and pre-learning processes. Numerical studies are conducted and the results indicate that the proposed method is effective in reducing the maintenance cost and efficient in searching the optimal CBM strategy for redundant systems.}
}
@article{MUNOZ2015112,
title = {Load balancing and handover joint optimization in LTE networks using Fuzzy Logic and Reinforcement Learning},
journal = {Computer Networks},
volume = {76},
pages = {112-125},
year = {2015},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2014.10.027},
url = {https://www.sciencedirect.com/science/article/pii/S1389128614003892},
author = {P. Muñoz and R. Barco and I. {de la Bandera}},
keywords = {Load balancing, Handover, Self-organizing networks, Long-term evolution, Fuzzy logic, Reinforcement learning},
abstract = {With the growing deployment of cellular networks, operators have to devote significant manual effort to network management. As a result, Self-Organizing Networks (SONs) have become increasingly important in order to raise the level of automated operation in cellular technologies. In this context, Load Balancing (LB) and Handover Optimization (HOO) have been identified by industry as key self-organizing mechanisms for the Radio Access Networks (RANs). However, most efforts have been focused on developing a stand-alone entity for each self-organizing mechanism, which will run in parallel with other entities, as well as designing coordination mechanisms in charge of stabilizing the network as a whole. Due to the importance of LB and HOO, in this paper, a unified self-management mechanism based on Fuzzy Logic and Reinforcement Learning is proposed. In particular, the proposed algorithm modifies handover parameters to optimize the main Key Performance Indicators related to LB and HOO. Results show that the proposed scheme effectively provides better performance than independent entities running simultaneously in the network.}
}
@article{TSEREMOGLOU2024109582,
title = {Condition-Based Maintenance scheduling of an aircraft fleet under partial observability: A Deep Reinforcement Learning approach},
journal = {Reliability Engineering & System Safety},
volume = {241},
pages = {109582},
year = {2024},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109582},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023004969},
author = {Iordanis Tseremoglou and Bruno F. Santos},
keywords = {Condition-Based Maintenance (CBM), Partially Observable Markov Decision Process, (POMDP), Partially Observable Monte–Carlo Planning (POMCP), Prognostics, Planning under uncertainty, Deep Reinforcement Learning},
abstract = {In the Condition-Based Maintenance (CBM) context, the definition of optimal maintenance plans for an aircraft fleet depends on an efficient integration of : (i) the probabilistic predictions of the health condition of the components and (ii) the stochastic arrival of the corrective maintenance tasks, together with consideration of the preventive maintenance tasks as defined in the Maintenance Planning Document (MPD) . To this end, in this paper, we present a two-stage dynamic scheduling framework to solve the aircraft fleet maintenance scheduling problem under a CBM strategy in a disruptive environment. In the first stage of the framework, we address the uncertainty in the predicted health state of the monitored components by planning the optimal maintenance policy based upon the belief state-space of the health of the components. The decision-making process is formulated as a Partially Observable Markov Decision Process (POMDP) and is solved using the Partially Observable Monte Carlo Planning (POMCP) algorithm, considering the aircraft maintenance scheduling problem requirements. In the second stage, a Deep Q-Network (DQN) is developed, that integrates the defined maintenance policy of the monitored components within the scheduling of the aircraft fleet’s preventive and corrective maintenance tasks. Our model, through a rolling horizon approach, continuously creates and adjusts the maintenance schedule, reacting to new updated task information, where the availability of maintenance resources constraints the execution of each task. The proposed framework was tested on a case study from a large airline and the performance was evaluated against the current state practice of the airline. The results show that our model can schedule 96.4% of monitored components on-time. As a consequence of this, a 46.2% maintenance cost reduction is achieved for the considered monitored components relative to a corrective maintenance approach.}
}
@article{HEIDARI2023106310,
title = {DeepValve: Development and experimental testing of a Reinforcement Learning control framework for occupant-centric heating in offices},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106310},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106310},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623004943},
author = {Amirreza Heidari and Dolaana Khovalyg},
keywords = {Reinforcement Learning, Occupant-centric, Thermal comfort, Energy efficiency, HVAC, Experimental test},
abstract = {Space heating controls in offices usually follow static schedules detached from actual occupancy, which results in energy waste by unnecessarily heating vacant offices. The uniqueness of stochastic occupancy profile and thermal response time of each office are two main challenges in hard-programming a transferrable control logic that can adapt space heating schedule to the occupancy profile. This study proposes a Reinforcement Learning-based control framework (called DeepValve) that learns by itself how to adapt the space heating schedule to the occupancy profile in each office to save energy while maintaining comfort. All the aspects of the proposed framework (design, training, hardware setup, etc.) are centered on ensuring that it can be implemented on many offices in practice. The methodology includes three main steps: training on a wide variety of simulated offices with real-world occupancy data, month-long tests on three simulated offices, and day-long experimental tests in an environmental chamber. Results indicate that the agent can quickly adapt to new offices and save energy (40% reduction in total temperature increment) while maintaining occupant comfort. The results highlight the importance of occupant-centric control in offices.}
}
@article{WANG2019549,
title = {UAV first view landmark localization with active reinforcement learning},
journal = {Pattern Recognition Letters},
volume = {125},
pages = {549-555},
year = {2019},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2019.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167865519300972},
author = {Xinran Wang and Chao Li and Leijian Yu and Lirong Han and Xiaogang Deng and Erfu Yang and Peng Ren},
keywords = {Reinforcement learning, First view landmark localization, Unmanned aerial vehicle},
abstract = {We present an active reinforcement learning framework for unmanned aerial vehicle (UAV) first view landmark localization. We formulate the problem of landmark localization as that of a Markov decision process and introduce an active landmark-localization network (ALLNet) to address it. The aim of the ALLNet is to locate a bounding box that surrounds the landmark in a first view image sequence. To this end, it is trained in a reinforcement learning fashion. Specifically, it employs support vector machine (SVM) scores on the bounding box patches as rewards and learns the bounding box transformations as actions. Furthermore, each SVM score indicates whether or not the landmark is detected by the bounding box such that it enables the ALLNet to have the capability of judging whether the landmark leaves or re-enters a first view image. Therefore, the operation of the ALLNet is not only dominated by the reinforcement learning process but also supplemented by an active learning motivated manner. Once the landmark is considered to leave the first view image, the ALLNet stops operating until the SVM detects its re-entry to the view. The active reinforcement learning model enables training a robust ALLNet for landmark localization. The experimental results validate the effectiveness of the proposed model for UAV first view landmark localization.}
}
@article{PINTO2021117642,
title = {Data-driven district energy management with surrogate models and deep reinforcement learning},
journal = {Applied Energy},
volume = {304},
pages = {117642},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117642},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921010096},
author = {Giuseppe Pinto and Davide Deltetto and Alfonso Capozzoli},
keywords = {Coordinated energy management, Deep reinforcement learning, Long short-term memory neural network, Data-driven modelling, Building energy flexibility},
abstract = {Demand side management at district scale plays a crucial role in the energy transition process, being an ideal candidate to balance the needs of both users and grid, by managing the volatility of renewable sources and increasing energy flexibility. The presented study aims to explore the benefits of a coordinated approach for the energy management of a cluster of buildings to optimise the electrical demand profiles and provide services to the grid without penalising indoor comfort conditions. The proposed methodology makes use of a fully data-driven control scheme which exploits Long Short-Term Memory (LSTM) Neural Networks, and Deep Reinforcement Learning (DRL). A simulation environment is introduced to train a DRL controller to manage the operation of heat pumps and chilled and domestic hot water storage for a cluster of four buildings. LSTM models are trained with synthetic data set created in EnergyPlus and are integrated into simulation environment to evaluate the indoor temperature dynamics in each building. The developed DRL controller is tested against a manually optimised Rule Based Controller (RBC). Results show that the DRL algorithm is able to reduce the overall cluster electricity costs, while decreasing the peak energy demand by 23% and the Peak to Average Ratio (PAR) by 20%, without penalizing indoor temperature control.}
}
@article{KHAWAJA2023249,
title = {Design of cost-based sizing and energy management framework for standalone microgrid using reinforcement learning},
journal = {Solar Energy},
volume = {251},
pages = {249-260},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23000336},
author = {Yara Khawaja and Issa Qiqieh and Jafar Alzubi and Omar Alzubi and Adib Allahham and Damian Giaouris},
keywords = {Sizing, Energy management, Energy storage, Levelized cost of energy, Reinforcement learning, Q-learning},
abstract = {The standalone photovoltaic-battery energy storage (PV-BES) microgrid has gained substantial interest recently due to its ability to provide uninterrupted power to consumers in remote areas. In such microgrids, components must be precisely sized and energy must be supplied most cost-effectively at all times. This paper presents a cost-based framework for determining the optimal size and energy management of standalone microgrids using reinforcement learning. Fundamental to this framework is two essential phases; the first is finding the best size of PV-BES using an analytical and economic sizing (AES) model based on minimum levelized cost of energy (LCOE). The AES phase is then followed by optimizing the energy management strategy (EMS) of the microgrid using reinforcement learning to provide optimum cost savings. The novelty in this work can be outlined as optimizing both the size and EMS of a standalone PV-BES microgrid using the AES model and Q-learning in an integrated framework. This can lead to improved performance demonstrated in reducing the LCOE, decreasing diesel generator working hours, and enhancing PV utilization and system efficiency. The results show an advantageous reduction in total cost while meeting load requirements. Additionally, the proposed framework is evaluated using several metrics to measure the impact of employing Q-learning against the AES-finite automata model. For instance, a decrease of 22% in diesel generator working hours and an increase of 6% in PV utilization while a reduction of 11% in the LCOE is accomplished. On the other hand, the proposed framework is examined against two rule-based EMSs, load following strategy (LFS) and cycle charging strategy (CCS), and outperforms these two EMSs in terms of LCOE, PV utilization, and system efficiency.}
}
@article{SOLEYMANI2020113456,
title = {Financial portfolio optimization with online deep reinforcement learning and restricted stacked autoencoder—DeepBreath},
journal = {Expert Systems with Applications},
volume = {156},
pages = {113456},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113456},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420302803},
author = {Farzan Soleymani and Eric Paquet},
keywords = {Portfolio management, Deep reinforcement learning, Restricted stacked autoencoder, Online leaning, Settlement risk, Blockchain},
abstract = {The process of continuously reallocating funds into financial assets, aiming to increase the expected return of investment and minimizing the risk, is known as portfolio management. In this paper, a portfolio management framework is developed based on a deep reinforcement learning framework called DeepBreath. The DeepBreath methodology combines a restricted stacked autoencoder and a convolutional neural network (CNN) into an integrated framework. The restricted stacked autoencoder is employed in order to conduct dimensionality reduction and features selection, thus ensuring that only the most informative abstract features are retained. The CNN is used to learn and enforce the investment policy which consists of reallocating the various assets in order to increase the expected return on investment. The framework consists of both offline and online learning strategies: the former is required to train the CNN while the latter handles concept drifts i.e. a change in the data distribution resulting from unforeseen circumstances. These are based on passive concept drift detection and online stochastic batching. Settlement risk may occur as a result of a delay in between the acquisition of an asset and its payment failing to deliver the terms of a contract. In order to tackle this challenging issue, a blockchain is employed. Finally, the performance of the DeepBreath framework is tested with four test sets over three distinct investment periods. The results show that the return of investment achieved by our approach outperforms current expert investment strategies while minimizing the market risk.}
}
@article{HENRY2021100092,
title = {Gym-ANM: Reinforcement learning environments for active network management tasks in electricity distribution systems},
journal = {Energy and AI},
volume = {5},
pages = {100092},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100092},
url = {https://www.sciencedirect.com/science/article/pii/S266654682100046X},
author = {Robin Henry and Damien Ernst},
keywords = {Gym-ANM, Reinforcement learning, Active network management, Distribution networks, Renewable energy},
abstract = {Active network management (ANM) of electricity distribution networks include many complex stochastic sequential optimization problems. These problems need to be solved for integrating renewable energies and distributed storage into future electrical grids. In this work, we introduce Gym-ANM, a framework for designing reinforcement learning (RL) environments that model ANM tasks in electricity distribution networks. These environments provide new playgrounds for RL research in the management of electricity networks that do not require an extensive knowledge of the underlying dynamics of such systems. Along with this work, we are releasing an implementation of an introductory toy-environment, ANM6-Easy, designed to emphasize common challenges in ANM. We also show that state-of-the-art RL algorithms can already achieve good performance on ANM6-Easy when compared against a model predictive control (MPC) approach. Finally, we provide guidelines to create new Gym-ANM environments differing in terms of (a) the distribution network topology and parameters, (b) the observation space, (c) the modeling of the stochastic processes present in the system, and (d) a set of hyperparameters influencing the reward signal. Gym-ANM can be downloaded at https://github.com/robinhenry/gym-anm.}
}
@article{KAZEMEINI2023136124,
title = {Identifying environmentally sustainable pavement management strategies via deep reinforcement learning},
journal = {Journal of Cleaner Production},
volume = {390},
pages = {136124},
year = {2023},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2023.136124},
url = {https://www.sciencedirect.com/science/article/pii/S0959652623002822},
author = {Ali Kazemeini and Omar Swei},
keywords = {Deep reinforcement learning, Global warming potential, Life cycle assessment, Pavement infrastructure, Proximal policy optimization},
abstract = {Pavement life cycle assessments (LCAs) enable decision-makers to evaluate the environmental impact of alternative maintenance, rehabilitation, and reconstruction strategies. This paper explores the viability of deep reinforcement learning (DRL), a framework that enables agents to learn optimal actions within a given situation, to identify environmentally benign pavement management strategies. More specifically, this study utilizes proximal-policy optimization (PPO), a subtype of DRL algorithms, to identify a management strategy that minimizes the expected global warming impact of a pavement facility over its lifecycle. Through an urban Interstate case study, this paper shows that the proposed PPO algorithm identifies management strategies that are anticipated to reduce the expected global warming impact of a pavement facility over its planning horizon by 16 percent relative to traditional practice. Furthermore, the PPO algorithm is able to identify this management strategy in only 25 learning iterations, which is in stark comparison to Q-learning, a common reinforcement learning algorithm, that requires 70,000 learning iterations. The results of this work highlight the viability of DRL to integrate within complex LCA models to determine environmentally sustainable pavement management strategies.}
}
@article{B2021100610,
title = {(ITMP) – Intelligent Traffic Management Prototype using Reinforcement Learning approach for Software Defined Data Center (SDDC)},
journal = {Sustainable Computing: Informatics and Systems},
volume = {32},
pages = {100610},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2021.100610},
url = {https://www.sciencedirect.com/science/article/pii/S2210537921000986},
author = {Balakiruthiga B. and Deepalakshmi P.},
keywords = {Software defined data center (SDDC), Data center network (DCN), Reinforcement learning (RL), Intelligent traffic management prototype (ITMP)},
abstract = {Software defined network architecture offers scalability and resilience as the significant advantages to data center networks. This increases the fault tolerance ability of traditional data center network architectures. Massive amounts of mobile network data as well as e-commerce application data requests are the key sources for data centers which recurrently desire attention. Researchers are yet to design a suitable prototype with functional intelligence to support traffic optimization techniques in SDDC. In this research work, we are proposing an intelligent traffic management prototype for software defined data center by means of reinforcement learning approach through the integration of the functionalities such as controller positioning, traffic load balancing, routing and energy efficiency. These are the key areas where traffic optimization becomes essential to improve network performance. The proposed prototype provides a complete framework for enterprises to deploy applications in an efficient manner. We model the prototype to handle dynamic network data applications such as information retrieval, communication and banking applications. We focus in this article on how communication happens among the data center nodes as an inter-data center communication process upon receiving requests from the applications considered. To further enhance the novelty and efficiency of our research work, we adopt multiple reinforcement learning agents to lever load balancing and routing functionalities. Moreover, to assess and ensure the optimized network performance, we evaluate the energy consumption of the network achieved through our proposed prototype.}
}
@article{ZHOU2021102949,
title = {Model-free perimeter metering control for two-region urban networks using deep reinforcement learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {124},
pages = {102949},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2020.102949},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X20308469},
author = {Dongqin Zhou and Vikash V. Gayah},
keywords = {Macroscopic Fundamental Diagram (MFD), Model free deep reinforcement learning (Deep-RL), Perimeter control},
abstract = {Various perimeter metering control strategies have been proposed for urban traffic networks that rely on the existence of well-defined relationships between network productivity and accumulation, known more commonly as network Macroscopic Fundamental Diagrams (MFD). Most existing perimeter metering control strategies require accurate modeling of traffic dynamics with full knowledge of the network MFD and dynamic equations to describe how vehicles move across regions of the network. However, such information is generally difficult to obtain and subject to error. Some model free perimeter metering control schemes have been recently proposed in the literature. However, these existing approaches require estimates of network properties (e.g., the critical accumulation associated with maximum network productivity) in the controller designs. In this paper, a model free deep reinforcement learning perimeter control (MFDRLPC) scheme is proposed for two-region urban networks that features agents with either continuous or discrete action spaces. The proposed agents learn to select control actions through a reinforcement learning process without assuming any information about environment dynamics. Results from extensive numerical experiments demonstrate that the proposed agents: (a) can consistently learn perimeter control strategies under various environment configurations; (b) are comparable in performance to the state-of-the-art, model predictive control (MPC); and, (c) are highly transferable to a wide range of traffic conditions and dynamics in the environment.}
}
@article{LIN20062078,
title = {A novel genetic reinforcement learning for nonlinear fuzzy control problems},
journal = {Neurocomputing},
volume = {69},
number = {16},
pages = {2078-2089},
year = {2006},
note = {Brain Inspired Cognitive Systems},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2005.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0925231205004534},
author = {Cheng-Jian Lin and Yong-Ji Xu},
keywords = {Reinforcement learning, Genetic algorithm, Fuzzy system, Nonlinear control, Efficient mutation, Sequential-search},
abstract = {Unlike a supervise learning, a reinforcement learning problem has only very simple “evaluative” or “critic” information available for learning, rather than “instructive” information. A novel genetic reinforcement learning, called reinforcement sequential-search-based genetic algorithm (R-SSGA), is proposed for solving the nonlinear fuzzy control problems in this paper. Unlike the traditional reinforcement genetic algorithm, the proposed R-SSGA method adopts the sequential-search-based genetic algorithms (SSGA) to tune the fuzzy controller. Therefore, the better chromosomes will be initially generated while the better mutation points will be determined for performing efficient mutation. The adjustable parameters of fuzzy controller are coded as real number components. We formulate a number of time steps before failure occurs as a fitness function. Simulation results have shown that the proposed R-SSGA method converges quickly and minimizes the population size.}
}
@article{SHAMS2023101446,
title = {Gym-preCICE: Reinforcement learning environments for active flow control},
journal = {SoftwareX},
volume = {23},
pages = {101446},
year = {2023},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2023.101446},
url = {https://www.sciencedirect.com/science/article/pii/S2352711023001425},
author = {Mosayeb Shams and Ahmed H. Elsheikh},
keywords = {Reinforcement learning, Active flow control, Gymnasium, OpenAI Gym, preCICE},
abstract = {Active flow control (AFC) involves manipulating fluid flow over time to achieve a desired performance or efficiency. AFC, as a sequential optimisation task, can benefit from utilising Reinforcement Learning (RL) for dynamic optimisation. In this work, we introduce Gym-preCICE, a Python adapter fully compliant with Gymnasium API to facilitate designing and developing RL environments for single- and multi-physics AFC applications. In an actor–environment setting, Gym-preCICE takes advantage of preCICE, an open-source coupling library for partitioned multi-physics simulations, to handle information exchange between a controller (actor) and an AFC simulation environment. Gym-preCICE provides a framework for seamless non-invasive integration of RL and AFC, as well as a playground for applying RL algorithms in various AFC-related engineering applications.}
}
@article{NAKABI2021100413,
title = {Deep reinforcement learning for energy management in a microgrid with flexible demand},
journal = {Sustainable Energy, Grids and Networks},
volume = {25},
pages = {100413},
year = {2021},
issn = {2352-4677},
doi = {https://doi.org/10.1016/j.segan.2020.100413},
url = {https://www.sciencedirect.com/science/article/pii/S2352467720303441},
author = {Taha Abdelhalim Nakabi and Pekka Toivanen},
keywords = {Artificial intelligence, Deep reinforcement learning, Demand Response, Dynamic pricing, Energy management system, Microgrid, Neural networks, Price-responsive loads, Smart grid, Thermostatically controlled loads},
abstract = {In this paper, we study the performance of various deep reinforcement learning algorithms to enhance the energy management system of a microgrid. We propose a novel microgrid model that consists of a wind turbine generator, an energy storage system, a set of thermostatically controlled loads, a set of price-responsive loads, and a connection to the main grid. The proposed energy management system is designed to coordinate among the different flexible sources by defining the priority resources, direct demand control signals, and electricity prices. Seven deep reinforcement learning algorithms were implemented and are empirically compared in this paper. The numerical results show that the deep reinforcement learning algorithms differ widely in their ability to converge to optimal policies. By adding an experience replay and a semi-deterministic training phase to the well-known asynchronous advantage actor–critic​ algorithm, we achieved the highest model performance as well as convergence to near-optimal policies.}
}
@article{JEON20232513,
title = {Solar irradiance prediction using reinforcement learning pre-trained with limited historical data},
journal = {Energy Reports},
volume = {10},
pages = {2513-2524},
year = {2023},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2023.09.042},
url = {https://www.sciencedirect.com/science/article/pii/S2352484723012866},
author = {Byung-Ki Jeon and Eui-Jong Kim},
keywords = {Reinforcement learning, Deep Q learning, Long short term memory, Solar irradiance},
abstract = {Accurate day-ahead forecasting of solar irradiance is crucial for maintaining a steady power supply and minimizing energy losses. To date, various solar irradiance prediction models have been established, but these typically require extensive weather data collected over long periods within the area of prediction or consistent updates using field measurements. This research introduces a reinforcement learning-based model capable of long-term solar irradiance prediction, even in areas with limited accumulated data. Our proposed model can forecast solar radiation for more than a year using just two weeks of solar radiation learning and readily available weather forecasts. It demonstrated a promising performance, with an annual average CVRMSE error of 7.0%, which is a more optimized predictive performance than the 12.8% CVRMSE yielded by the existing LSTM-based Reference model constructed by adding out-of-atmosphere solar radiation input values.}
}
@article{OU2022111732,
title = {Reinforcement learning-based calibration method for cameras with large FOV},
journal = {Measurement},
volume = {202},
pages = {111732},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.111732},
url = {https://www.sciencedirect.com/science/article/pii/S026322412200937X},
author = {Qiaofeng Ou and Qunqun Xie and Fuhan Chen and Jianhao Peng and Bangshu Xiong},
keywords = {Camera calibration, Large FOV, Reinforcement learning, MDP, Q-learning},
abstract = {The accuracy of camera calibration is of great importance to vision measurements. Target-based calibration methods should cover the whole field of view (FOV), which leads to complex operation in large FOVs and rely on personal experiences. To overcome the difficulty in accurate calibration, a calibration method based on reinforcement learning is proposed. First, a Markov decision process (MDP) model of the calibration procedure is established. Then, the reward function is designed, combining the requirement of calibration accuracy and the state-space constraint. Finally, the optimized target locations and poses are obtained by continuous interaction with the calibration environment based on Q-learning, which plays a key guiding role in camera calibration. Simulation experiments and real experiments are performed, which indicate that the proposed method effectively improves the success rate of large FOV camera calibration, and solves the problems of relying on personal experiences, low calibration accuracy, and poor stability caused by target placement in the camera calibration process.}
}
@article{AYDIN2022153,
title = {Using chains of bottleneck transitions to decompose and solve reinforcement learning tasks with hidden states},
journal = {Future Generation Computer Systems},
volume = {133},
pages = {153-168},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000917},
author = {Hüseyin Aydın and Erkin Çilden and Faruk Polat},
keywords = {Reinforcement learning, Task decomposition, Chains of bottleneck transitions},
abstract = {Reinforcement learning is known to underperform in large and ambiguous problem domains under partial observability. In such cases, a proper decomposition of the task can improve and accelerate the learning process. Even ambiguous and complex problems that are not solvable by conventional methods turn out to be easier to handle by using a convenient problem decomposition, followed by the incorporation of machine learning methods for the sub-problems. Like in most real-life problems, the decomposition of a task usually stems from the sequence of sub-tasks that must be achieved in order to get the main task done. In this study, assuming that unambiguous states are provided in advance, a decomposition of the problem is constructed by the agent based on a set of chains of bottleneck transitions, which are sequences of unambiguous and critical transitions leading to the goal state. At the higher level, an agent trains its sub-agents to extract sub-policies corresponding to the sub-tasks, namely two successive transitions in any chain, and learns the value of each sub-policy at the abstract level. Experimental study demonstrates that an early decomposition based on useful bottleneck transitions eliminates the necessity for excessive memory and improves the learning performance of the agent. It is also shown that knowing the correct order of bottleneck transitions in the decomposition results in faster construction of the solution.}
}
@article{XU2021100525,
title = {Adaptive workload adjustment for cyber-physical systems using deep reinforcement learning},
journal = {Sustainable Computing: Informatics and Systems},
volume = {30},
pages = {100525},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2021.100525},
url = {https://www.sciencedirect.com/science/article/pii/S2210537921000184},
author = {Shikang Xu and Israel Koren and C. Mani Krishna},
keywords = {Cyber-physical systems, Energy consumption, Adaptive fault-tolerance, Deep reinforcement learning},
abstract = {Reducing computational energy consumption in cyber-physical systems (CPSs) has attracted considerable attention in recent years. Associated with energy consumption is a heating of the devices. Device failure rate increases exponentially with increase temperature, so that high energy consumption leads to a significant shortening of processor lifetime. Reducing thermal stress without harming application safety and performance is the goal of this work. Our approach is to abort control tasks dispatch when this is judged, by a neural network, to not contribute to either safety or performance. This technique is orthogonal to others that have been used to reduce energy consumption such as dynamic voltage/frequency scaling and adaptive use of redundancy. Simulation experiments show that this approach leads to a further reduction in device aging when used in conjunction with these prior techniques.}
}
@article{QIU2020110055,
title = {Model-free control method based on reinforcement learning for building cooling water systems: Validation by measured data-based simulation},
journal = {Energy and Buildings},
volume = {218},
pages = {110055},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.110055},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819339945},
author = {Shunian Qiu and Zhenhai Li and Zhengwei Li and Jiajie Li and Shengping Long and Xiaoping Li},
keywords = {Cooling water system, Cooling tower, Cooling water pump, Optimal control, Reinforcement learning, Model-free control},
abstract = {In the domain of optimal control for building HVAC systems, the performance of model-based control has been widely investigated and validated. However, the performance of model-based control highly depends on an accurate system performance model and sufficient sensors, which are difficult to obtain for certain buildings. To tackle this problem, a model-free optimal control method based on reinforcement learning is proposed to control the building cooling water system. In the proposed method, the wet bulb temperature and system cooling load are taken as the states, the frequencies of fans and pumps are the actions, and the reward is the system COP (i.e., the comprehensive COP of chillers, cooling water pumps, and cooling towers). The proposed method is based on Q-learning. Validated with the measured data from a real central chilled water system, a three-month measured data-based simulation is conducted under the supervision of four types of controllers: basic controller, local feedback controller, model-based controller, and the proposed model-free controller. Compared with the basic controller, the model-free controller can conserve 11% of the system energy in the first applied cooling season, which is greater than that of the local feedback controller (7%) but less than that of the model-based controller (14%). Moreover, the energy saving rate of the model-free controller could reach 12% in the second applied cooling season, after which the energy saving rate gets stabilized. Although the energy conservation performance of the model-free controller is inferior to that of the model-based controller, the model-free controller requires less a priori knowledge and sensors, which makes it promising for application in buildings for which the lack of accurate system performance models or sensors is an obstacle. Moreover, the results suggest that for a central chilled water system with a designed peak cooling load close to 2000 kW, three months of learning during the cooling season is sufficient to develop a good model-free controller with an acceptable performance.}
}
@article{ALABDULLAH20229069,
title = {Microgrid energy management using deep Q-network reinforcement learning},
journal = {Alexandria Engineering Journal},
volume = {61},
number = {11},
pages = {9069-9078},
year = {2022},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2022.02.042},
url = {https://www.sciencedirect.com/science/article/pii/S1110016822001284},
author = {Mohammed H. Alabdullah and Mohammad A. Abido},
keywords = {Deep reinforcement learning, Deep Q-networks, Energy management, Microgrid},
abstract = {This paper proposes a deep reinforcement learning-based approach to optimally manage the different energy resources within a microgrid. The proposed methodology considers the stochastic behavior of the main elements, which include load profile, generation profile, and pricing signals. The energy management problem is formulated as a finite horizon Markov Decision Process (MDP) by defining the state, action, reward, and objective functions, without prior knowledge of the transition probabilities. Such formulation does not require explicit model of the microgrid, making use of the accumulated data and interaction with the microgrid to derive the optimal policy. An efficient reinforcement learning algorithm based on deep Q-networks is implemented to solve the developed formulation. To confirm the effectiveness of such methodology, a case study based on a real microgrid is implemented. The results of the proposed methodology demonstrate its capability to obtain online scheduling of various energy resources within a microgrid with optimal cost-effective actions under stochastic conditions. The achieved costs of operation are within 2% of those obtained in the optimal schedule.}
}
@article{PORTALPORRAS2023115775,
title = {Active flow control on airfoils by reinforcement learning},
journal = {Ocean Engineering},
volume = {287},
pages = {115775},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.115775},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823021595},
author = {Koldo Portal-Porras and Unai Fernandez-Gamiz and Ekaitz Zulueta and Roberto Garcia-Fernandez and Saioa {Etxebarria Berrizbeitia}},
keywords = {Artificial neural networks, Deep learning, Reinforcement learning, Computational fluid dynamics, Moving flap},
abstract = {Active flow control is a widespread practice for airfoil aerodynamic performance enhancement. Within active flow control, reactive strategies are very effective, but the adequate design of these strategies is often complex. This study proposes a reactive control strategy based on a Reinforcement Learning (RL) agent to effectively govern the motion of a rotating flap implemented on a NACA0012 airfoil. With this objective, first different Computational Fluid Dynamics (CFD) simulations are conducted to gather data about the tested case. Then, a numerical model based on Artificial Neural Networks (ANN) is developed to model the discussed case. Finally, the RL agent is trained and tested under different conditions. The results show that the trained RL agent is able to provide a fast and reliable response for every tested condition, setting the adequate position of the flap and obtaining an appropriate aerodynamic performance of the airfoil for all the tested conditions. In comparison with the optimum conditions, the absolute error in the position of the flap set by the agent is below 2.2 ° for all the angles of attack, resulting in an aerodynamic performance very close to the optimum, being only 0.39%–3.05% lower, depending on the case.}
}
@article{XU2022102973,
title = {Resource allocation for UAV-aided energy harvesting-powered D2D communications: A reinforcement learning-based scheme},
journal = {Ad Hoc Networks},
volume = {136},
pages = {102973},
year = {2022},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2022.102973},
url = {https://www.sciencedirect.com/science/article/pii/S1570870522001482},
author = {Yi-Han Xu and Qi-Ming Sun and Wen Zhou and Gang Yu},
keywords = {Unmanned aerial vehicle, D2D communications, Resource allocation, Deep deterministic policy gradient (DDPG), Long short-term memory (LSTM)},
abstract = {Unmanned Aerial Vehicle (UAV) has become one of the most significant component in future wireless networks since its on-demand and cost-effective deployment. Meanwhile, Device-to-Device (D2D) communications with the nature of providing proximity-based service to improve network performance has emerged as an important feature in 5th generation cellular networks. However, the explosive growth of smart devices and bandwidth-hungry applications cause enormous energy consumption. Energy Harvesting (EH) as a potential solution of improving energy efficiency has shown great importance. Consequently, in this paper, we investigate the resource allocation problem in UAV-aided EH-powered D2D Cellular Networks (UAV-EH-DCNs). Our objective is to maximize the energy efficiency while guaranteeing the satisfaction of ground users (GUs). Owning to the non-convexity of the problem, we formulate the problem as a Markov decision process. Afterward, Deep Deterministic Policy Gradient (DDPG) is proposed to find the optimal strategy. Additionally, Long Short-Term Memory (LSTM) network is employed to facilitate the convergence speed by extracting the previous information of GUs satisfaction to determine the current resource allocation strategy. Numerical results adduce the validity of the proposed DDPG+LSTM algorithm as compared to the DDPG and deep Q-learning algorithms.}
}
@article{PINTO2021120725,
title = {Coordinated energy management for a cluster of buildings through deep reinforcement learning},
journal = {Energy},
volume = {229},
pages = {120725},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.120725},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221009737},
author = {Giuseppe Pinto and Marco Savino Piscitelli and José Ramón Vázquez-Canteli and Zoltán Nagy and Alfonso Capozzoli},
keywords = {Coordinated energy management, Deep reinforcement learning, Building energy flexibility, Peak demand reduction, Grid interaction},
abstract = {Advanced control strategies can enable energy flexibility in buildings by enhancing on-site renewable energy exploitation and storage operation, significantly reducing both energy costs and emissions. However, when the energy management is faced shifting from a single building to a cluster of buildings, uncoordinated strategies may have negative effects on the grid reliability, causing undesirable new peaks. To overcome these limitations, the paper explores the opportunity to enhance energy flexibility of a cluster of buildings, taking advantage from the mutual collaboration between single buildings by pursuing a coordinated approach in energy management. This is achieved using Deep Reinforcement Learning (DRL), an adaptive model-free control algorithm, employed to manage the thermal storages of a cluster of four buildings equipped with different energy systems. The controller was designed to flatten the cluster load profile while optimizing energy consumption of each building. The coordinated energy management controller is tested and compared against a manually optimised rule-based one. Results shows a reduction of operational costs of about 4%, together with a decrease of peak demand up to 12%. Furthermore, the control strategy allows to reduce the average daily peak and average peak-to-average ratio by 10 and 6% respectively, highlighting the benefits of a coordinated approach.}
}
@article{CHEN2021130498,
title = {Optimal control towards sustainable wastewater treatment plants based on multi-agent reinforcement learning},
journal = {Chemosphere},
volume = {279},
pages = {130498},
year = {2021},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2021.130498},
url = {https://www.sciencedirect.com/science/article/pii/S0045653521009681},
author = {Kehua Chen and Hongcheng Wang and Borja Valverde-Pérez and Siyuan Zhai and Luca Vezzaro and Aijie Wang},
keywords = {Wastewater treatment, Reinforcement learning, Multi-objective optimization, Sustainability},
abstract = {Wastewater treatment plants (WWTPs) are designed to eliminate pollutants and alleviate environmental pollution resulting from human activities. However, the construction and operation of WWTPs consume resources, emit greenhouse gases (GHGs) and produce residual sludge, thus require further optimization. WWTPs are complex to control and optimize because of high non-linearity and variation. This study used a novel technique, multi-agent deep reinforcement learning (MADRL), to simultaneously optimize dissolved oxygen (DO) and chemical dosage in a WWTP. The reward function was specially designed from life cycle perspective to achieve sustainable optimization. Five scenarios were considered: baseline, three different effluent quality and cost-oriented scenarios. The result shows that optimization based on LCA has lower environmental impacts compared to baseline scenario, as cost, energy consumption and greenhouse gas emissions reduce to 0.890 CNY/m3-ww, 0.530 kWh/m3-ww, 2.491 kg CO2-eq/m3-ww respectively. The cost-oriented control strategy exhibits comparable overall performance to the LCA-driven strategy since it sacrifices environmental benefits but has lower cost as 0.873 CNY/m3-ww. It is worth mentioning that the retrofitting of WWTPs based on resources should be implemented with the consideration of impact transfer. Specifically, LCA-SW scenario decreases 10 kg PO4-eq in eutrophication potential compared to the baseline within 10 days, while significantly increases other indicators. The major contributors of each indicator are identified for future study and improvement. Last, the authors discussed that novel dynamic control strategies required advanced sensors or a large amount of data, so the selection of control strategies should also consider economic and ecological conditions. In a nutshell, there are still limitations of this work and future studies are required.}
}
@article{YANG2022122128,
title = {Short-term wind speed forecasting using deep reinforcement learning with improved multiple error correction approach},
journal = {Energy},
volume = {239},
pages = {122128},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.122128},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221023768},
author = {Rui Yang and Hui Liu and Nikolaos Nikitas and Zhu Duan and Yanfei Li and Ye Li},
keywords = {Short-term wind speed prediction, Adaptive data decomposition, Q-learning ensemble strategy, Improved multiple error correction technique},
abstract = {The safe and stable operation of wind power systems requires the support of wind speed prediction. To ensure the controllability and stability of smart grid dispatching, a novel hybrid model consisting of data-adaptive decomposition, reinforcement learning ensemble, and improved error correction is established for short-term wind speed forecasting. In decomposition module, empirical wavelet transform algorithm is used to adaptively disassemble and reconstruct the wind speed series. In ensemble module, Q-learning is utilized to integrate gated recurrent unit, bidirectional long short-term memory, and deep belief network. In error correction module, wavelet packet decomposition and outlier-robust extreme learning machine are combined to developing predictable components. An appropriate correction shrinkage rate is used to obtain the best correction effect. Ljung-Box Q-Test is utilized to judge the termination of the error correction iteration. Four real data are utilized to validate model performance in the case study. Experimental results show that: (a) The proposed hybrid model can accurately capture the changes of wind data. Taking 1-step prediction results as an example, the mean absolute errors for site #1, #2, #3, and #4 are 0.0829 m/s, 0.0661 m/s, 0.0906 m/s, and 0.0803 m/s, respectively; (b) Compared with several state-of-the-art models, the proposed model has the best prediction performance.}
}
@article{SHANTIA2021103731,
title = {Two-stage visual navigation by deep neural networks and multi-goal reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {138},
pages = {103731},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103731},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000166},
author = {Amirhossein Shantia and Rik Timmers and Yiebo Chong and Cornel Kuiper and Francesco Bidoia and Lambert Schomaker and Marco Wiering},
keywords = {Robotic navigation, Reinforcement learning, Deep neural networks, Localization and mapping, Robot simulation},
abstract = {In this paper, we propose a two-stage learning framework for visual navigation in which the experience of the agent during exploration of one goal is shared to learn to navigate to other goals. We train a deep neural network for estimating the robot’s position in the environment using ground truth information provided by a classical localization and mapping approach. The second simpler multi-goal Q-function learns to traverse the environment by using the provided discretized map. Transfer learning is applied to the multi-goal Q-function from a maze structure to a 2D simulator and is finally deployed in a 3D simulator where the robot uses the estimated locations from the position estimator deep network. In the experiments, we first compare different architectures to select the best deep network for location estimation, and then compare the effects of the multi-goal reinforcement learning method to traditional reinforcement learning. The results show a significant improvement when multi-goal reinforcement learning is used. Furthermore, the results of the location estimator show that a deep network can learn and generalize in different environments using camera images with high accuracy in both position and orientation.}
}
@article{ALMAHDI2019145,
title = {A constrained portfolio trading system using particle swarm algorithm and recurrent reinforcement learning},
journal = {Expert Systems with Applications},
volume = {130},
pages = {145-156},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419302441},
author = {Saud Almahdi and Steve Y. Yang},
keywords = {Recurrent reinforcement learning, Particle swarm optimization, Optimal portfolio rebalancing, Portfolio constraint},
abstract = {This study extends a recurrent reinforcement portfolio allocation and rebalancing management system with complex portfolio constraints using particle swarm algorithms. In particular, we propose to use a combination of recurrent reinforcement learning (RRL) and particle swarm algorithm (PSO) with Calmar ratio for both asset allocation and constraint optimization. Using S&P100 index stocks, we show such a system with a Calmar ratio based objective function yields a better efficient frontier than the Sharpe ratio and mean-variance based portfolios. By comparing with multiple PSO based long only constrained portfolios, we propose an optimal portfolio trading system that is capable of generating both long and short signals and handling the common portfolio constraints. We further develop an adaptive RRL-PSO portfolio rebalancing decision system with a market condition stop-loss retraining mechanism, and we show that the proposed portfolio trading system outperforms the benchmarks consistently especially under high transaction cost conditions.}
}
@article{ZHANG2022433,
title = {Physical-model-free intelligent energy management for a grid-connected hybrid wind-microturbine-PV-EV energy system via deep reinforcement learning approach},
journal = {Renewable Energy},
volume = {200},
pages = {433-448},
year = {2022},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2022.09.125},
url = {https://www.sciencedirect.com/science/article/pii/S0960148122014847},
author = {Bin Zhang and Weihao Hu and Xiao Xu and Tao Li and Zhenyuan Zhang and Zhe Chen},
keywords = {Neural network, Deep reinforcement learning, Energy management, Microgrid, Renewable energy},
abstract = {Renewable -based microgrid (MG) is recognized as an eco-friendly solution in the development of renewable energy (RE). Moreover, the MG energy management with high RE penetration faces complicate uncertainties due to the inaccuracy of predictions. Besides, the growing participation of electric vehicles (EVs) makes the traditional model-based methods even more infeasible. Considering uncertainties associated with RE, EVs, and electricity price, a model-free deep reinforcement learning (DRL), namely twin delayed deep deterministic policy gradient (TD3), is employed to develop an optimized control strategy to minimize operating costs and satisfy charging expectations. The energy management problem is first formulated as a Markov decision process. Then, TD3 solely relies on the limited observation to find the optimal continuous control strategy. The proposed method flexibly adjusts the operating and charging strategies of components according to RE output and electricity price. Its real-time optimized performance on three consecutive days along with electricity price is evaluated, indicating its practical potential for future application. Additionally, comparison results demonstrate that the proposed method reduces the total costs up to 15.27% and 4.24%, respectively, compared to traditional optimization and other DRL methods, which illustrates the superiority of the TD3 method on optimizing total costs of the considered MG.}
}
@article{GEURSEN2023102397,
title = {Fleet planning under demand and fuel price uncertainty using actor–critic reinforcement learning},
journal = {Journal of Air Transport Management},
volume = {109},
pages = {102397},
year = {2023},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2023.102397},
url = {https://www.sciencedirect.com/science/article/pii/S0969699723000406},
author = {Izaak L. Geursen and Bruno F. Santos and Neil Yorke-Smith},
keywords = {Airline fleet planning, Stochastic optimisation, Reinforcement learning, Advantage Actor–Critic, Fuel price uncertainty},
abstract = {Current state-of-the-art airline planning models face computational limitations, restricting the operational applicability to problems of representative sizes. This is particularly the case when considering the uncertainty necessarily associated with the long-term plan of an aircraft fleet. Considering the growing interest in the application of machine learning techniques to operations research problems, this article investigates the applicability of these techniques for airline planning. Specifically, an Advantage Actor–Critic (A2C) reinforcement learning algorithm is developed for the airline fleet planning problem. The increased computational efficiency of using an A2C agent allows us to consider real-world-sized problems and account for highly-volatile uncertainty in demand and fuel price. The result is a multi-stage probabilistic fleet plan describing the evolution of the fleet according to a large set of future scenarios. The A2C algorithm is found to outperform a deterministic model and a deep Q-network algorithm. The relative performance of the A2C increases as more complexity is added to the problem. Further, the A2C algorithm can compute a multi-stage fleet planning solution within a few seconds.}
}
@article{ZHOU2022122548,
title = {Self-learning energy management strategy for hybrid electric vehicle via curiosity-inspired asynchronous deep reinforcement learning},
journal = {Energy},
volume = {242},
pages = {122548},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.122548},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221027973},
author = {Jianhao Zhou and Yuan Xue and Da Xu and Chaoxiong Li and Wanzhong Zhao},
keywords = {Asynchronous deep reinforcement learning, Curiosity-inspired exploration, Energy management strategy, Model predictive control, Random network distillation},
abstract = {In this study, the formulation of a curiosity-inspired asynchronous advantage actor-critic (A3C+) based model-free, self-learning energy management strategy (EMS) is elaborated for hybrid powertrain. The fuel optimality of standard A3C based EMS is strongly influenced by the target strategy including charge sustenance (CS) and charge depletion (CD), being around 92% and 83% in comparison to dynamic programming (DP) based EMS under training cycle. The corresponding performance of A3C derived CD policy further declined to 75% under testing cycles. Similar tendencies are verified by model predictive control (MPC) and deep deterministic policy gradient (DDPG) based EMSs. To this end, random network distillation (RND) and inverse dynamics model (IDM) techniques are incorporated to form novelty-seeking module (NSM) which is leveraged as intrinsic rewards to facilitate the exploration performance. As a result, the proposed A3C + based EMS obtains significant improvement of global optimality, generalization ability and adaptivity. The fuel optimality of A3C + derived CS and CD policies can be guaranteed by at least 92% and 88% under training and testing cycles, comparing with DP based EMS. Besides, the training and running efficacy of A3C + based EMS are apparently superior to MPC based EMS, demonstrating its realtime implementation potential.}
}
@article{CAO2022103656,
title = {Trustworthy safety improvement for autonomous driving using reinforcement learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {138},
pages = {103656},
year = {2022},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103656},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22000997},
author = {Zhong Cao and Shaobing Xu and Xinyu Jiao and Huei Peng and Diange Yang},
keywords = {Autonomous Vehicle, Driving Safety, Reinforcement Learning},
abstract = {Reinforcement learning (RL) can learn from past failures and has the potential to provide self-improvement ability and higher-level intelligence. However, the current RL algorithms still suffer from challenges in reliability, especially compared to the rule/model-based algorithms that are pre-engineered, human-input intensive, but widely used in autonomous vehicles. To take advantages of both the RL and rule-based algorithms, this work aims to design a decision-making framework that leverages RL and use an existing rule-based policy as its performance lower bound. In this way, the final policy remains the potential of self-learning, while guaranteeing a better system performance compared with the integrated rule-based policy. Such a decision-making framework is called trustworthy improvement RL (TiRL). The basic idea is to make the RL policy iteration process synchronously estimate the given rule-based policy’s value function. AV will then use the RL policy to drive only in the cases where the RL has learned a better policy, i.e., a higher policy value. This work takes highway safe driving as the case study. The results are obtained through more than 42,000 km driving in stochastic simulated traffic, and calibrated by naturalistic driving data. The TiRL planner is given two typical rule-based highway-driving policies for comparison. The results show that the TiRL can outperform the given arbitrary rule-based driving policy. In summary, the proposed TiRL can leverage the learning-based method in stochastic and emergent scenarios, while having a trustworthy safety improvement from the existing rule-based policies.}
}
@article{YU2023129300,
title = {Reinforcement learning-based multi-objective differential evolution for wind farm layout optimization},
journal = {Energy},
volume = {284},
pages = {129300},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.129300},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223026944},
author = {Xiaobing Yu and Yangchen Lu},
keywords = {Wind energy, Multi-objective, Reinforcement learning, Differential evolution},
abstract = {Wind farm layout optimization is a challenging issue which demands to discover some trade-off solutions considering various criteria, such as the power generated and the cost of the farm. Due to the complexity of the problem, we developed a reinforcement learning-based multi-objective differential evolution (RLMODE) algorithm to address the issue. In the developed algorithm, RL technique is applied to coordinate the parameter of DE algorithm, which can balance the local and global search. A tournament-based mutation operator is used to accelerate the convergence of the RLMODE algorithm. We tested the performance of the proposed RLMODE in two wind scenarios. The spread and spacing indicators of the algorithm are the best; the power generated by the solution from the RLMODE algorithm is the most when compared with some representative optimization algorithms and existing methods.}
}
@incollection{AJAGEKAR2022463,
title = {Scheduling of Electrical Power Systems under Uncertainty using Deep Reinforcement Learning},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {463-468},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50077-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596500774},
author = {Akshay Ajagekar and Fengqi You},
keywords = {Unit Commitment, Deep Reinforcement Learning, Machine Learning},
abstract = {In this work, a deep reinforcement learning-based solution approach for the unit commitment of power generation resources in energy systems with intermittent renewable energy resources and uncertain loads is presented. Real-world unit commitment problems are plagued with uncertain parameters introduced by the possibility of forecast errors or equipment failure that may negatively impact the power supply. It is imperative to develop a robust and computationally tractable framework to provide cost-effective commitment decisions. In the proposed solution technique, temporal and spatial correlational structures of uncertainties present in the system are captured with a neural network function approximator. The proposed solution technique is able to capture the temporal and spatial correlational structure of uncertainties present in the system. A causal policy is obtained which relies only on previously observed wind power and demand forecasts along with forecast errors. We conduct computational experiments on the IEEE 39-bus test case to demonstrate the effectiveness of the proposed solution strategy and improvement over existing unit commitment solution techniques. The proposed deep reinforcement learning-based solution strategy demonstrates effective computational performance and a reduction in operating costs over deterministic and stochastic approaches.}
}
@article{SHU2023107114,
title = {Boosting input data sequences generation for testing EFSM-specified systems using deep reinforcement learning},
journal = {Information and Software Technology},
volume = {155},
pages = {107114},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107114},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922002233},
author = {Ting Shu and Cuiping Wu and Zuohua Ding},
keywords = {EFSM, Input data, Feasible test sequences, Deep reinforcement learning},
abstract = {Context:
Input data sequence (IDS) is an important component of test sequences for testing from the Extended Finite State Machine (EFSM) model. During test generation, frequent IDS derivation is time-consuming. Therefore, it has become one of the key factors that restrict the efficiency of test sequences generation.
Objective:
To address this issue, this paper introduces deep reinforcement learning (DRL) to propose a novel approach named IDSG-DRL to accelerate IDS derivation.
Method:
Our method first formalizes the problem of generating IDS for EFSM-based testing as a Markov decision problem. It then incorporates the DRL algorithm to learn experience from previous input data generation and train a decision-making model to significantly enhance the efficiency of subsequent data derivation for the newly generated test sequences. To improve the convergence of DRL algorithm and ensure the success rate of data generation, a state representation based on variable deviation and action formulation using adaptive exploration are elaborately designed. Finally, a DRL-based algorithm for efficiently yielding IDS is presented for any subject test sequence.
Results:
We evaluate the proposed approach against the random method, GA-based method as well as a particle swarm optimization (PSO) based method. Experimental statistics show that IDSG-DRL significantly outperforms the baselines in terms of iteration steps, runtime cost, and the success rate of input data derivation. Specifically, compared to random, GA-based and PSO-based methods, IDSG-DRL can reduce the average number of iteration steps by up to 87.09%, 78.57%, and 56.35%, respectively. Regarding the average runtime, our approach is about 3.52 and 1.58 times faster than the GA-based and PSO-based methods. Additionally, given a larger input range, we observed that the performance of IDSG-DRL is more stable and its advantages are more significant.
Conclusion:
The experimental results suggest that our method is very promising to speed up IDS generation for EFSM-based testing.}
}
@article{DONG2021136,
title = {Subject sensitive EEG discrimination with fast reconstructable CNN driven by reinforcement learning: A case study of ASD evaluation},
journal = {Neurocomputing},
volume = {449},
pages = {136-145},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221005270},
author = {Heyou Dong and Dan Chen and Lei Zhang and Hengjin Ke and Xiaoli Li},
keywords = {Electroencephalogram, Q-learning, Neural architecture search, Autism spectrum disorder evaluation},
abstract = {Recent Electroencephalogram (EEG) analysis in connection with brain disorders has been tremendously benefiting from the (Deep) Neural Network technology in neuroscience research and neuro-engineering practices. However, the performance of existing hand-crafted models, such as the stability, has largely been refrained. This is the case especially in the paradigms that sensitive to the individuality of subjects and the non-stationarity of cognitive dynamics, such as Autism Spectrum Disorder (ASD) evaluation. Aiming at this problem, this study develops a Q-Learning method to enable fast reconstruction of Convolutional Neural Network (CNN) thus to support EEG discrimination adapting to the individuality of subjects under examination. The proposed method first generates a CNN model with the structure and hyper-parameters determined (i.e., Neural Architecture Search) by the customized Q-Learning algorithm, where the CNN model is treated as a discrete system to be optimized. With the sharp shift of subjects, the Q-Learning algorithm reconstructs the CNN model to reach optimization reusing the tacit knowledge learned from the previous trials. A case study has been performed to check the proposed method versus state-of-the-art counterparts based on resting-state EEG collected from 175 ASD-suspicious children with a diverse geological distribution. The observations in the case study indicate that: 1) the method outperforms the counterparts with an individual/sample accuracy of 92.63%/83.23% achieved; 2) the method can quickly reconstruct the CNN model with the group of subjects shifting from one region to another to maintain an encouraging performance while the counterparts without reconstruction may drop by about 12%.}
}
@article{WASALA2020103799,
title = {Trajectory based lateral control: A Reinforcement Learning case study},
journal = {Engineering Applications of Artificial Intelligence},
volume = {94},
pages = {103799},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.103799},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620301858},
author = {Asanka Wasala and Donal Byrne and Philip Miesbauer and Joseph O’Hanlon and Paul Heraty and Peter Barry},
keywords = {Motion control, Sim-to-real, Autonomous vehicles, Reinforcement Learning},
abstract = {Reinforcement Learning (RL) has been employed in many applications of robotics and has steadily been gaining traction in the field of Autonomous Driving (AD). This paper proposes a Deep Reinforcement Learning based approach for lateral Vehicle Motion Control (VMC), and explores the generalization capabilities of the approach. The proposed methodology uses a sequence of waypoints generated from a planning module of an AD stack as the input. The network has been trained to predict accurate steering commands to follow the given trajectory. In this paper we detail our implementation and share our learning experience on real-vehicle deployment of the RL based controller. Our experiments yield promising results with an agent trained on less than 4 h of simulated driving experience without any real-world data. The trained agent is able to successfully complete unseen and more complex tracks using different unseen vehicle models. The agent safely reached up to 150km/h in simulation and up to 60km/h in a real-life Sport Utility Vehicle (SUV) weighing more than 2000kg.}
}
@article{KAMALAPURKAR201694,
title = {Model-based reinforcement learning for approximate optimal regulation},
journal = {Automatica},
volume = {64},
pages = {94-104},
year = {2016},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2015.10.039},
url = {https://www.sciencedirect.com/science/article/pii/S0005109815004392},
author = {Rushikesh Kamalapurkar and Patrick Walters and Warren E. Dixon},
keywords = {Model-based reinforcement learning, Concurrent learning, Simulated experience, Data-based control, Adaptive control, System identification},
abstract = {Reinforcement learning (RL)-based online approximate optimal control methods applied to deterministic systems typically require a restrictive persistence of excitation (PE) condition for convergence. This paper develops a concurrent learning (CL)-based implementation of model-based RL to solve approximate optimal regulation problems online under a PE-like rank condition. The development is based on the observation that, given a model of the system, RL can be implemented by evaluating the Bellman error at any number of desired points in the state space. In this result, a parametric system model is considered, and a CL-based parameter identifier is developed to compensate for uncertainty in the parameters. Uniformly ultimately bounded regulation of the system states to a neighborhood of the origin, and convergence of the developed policy to a neighborhood of the optimal policy are established using a Lyapunov-based analysis, and simulation results are presented to demonstrate the performance of the developed controller.}
}
@article{SUN2022105031,
title = {A hybrid deep reinforcement learning ensemble optimization model for heat load energy-saving prediction},
journal = {Journal of Building Engineering},
volume = {58},
pages = {105031},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2022.105031},
url = {https://www.sciencedirect.com/science/article/pii/S2352710222010415},
author = {Jiawang Sun and Mingju Gong and Yin Zhao and Cuitian Han and Lei Jing and Peng Yang},
keywords = {District heating system, Heat load energy-saving prediction, Similar sample selection approach, Weighted Euclidean norm, Deep deterministic policy gradient},
abstract = {Accurate heat load forecasting is crucial to achieving feed-forward control and on-demand heat supply in the district heating system (DHS). However, accurate forecasting is difficult to achieve effective energy-saving because experimental data are often not optimal or the most energy-efficient. In this study, a hybrid heat load forecasting model with energy-saving consideration is proposed, which consists of similar sample selection approach, short-term forecasting model pool and deep reinforcement learning ensemble strategy. In similar sample selection module, a novel weighted Euclidean norm (EN) is used to select suitable similar samples set for the training samples set. In model pool module, six popular deterministic prediction models are constructed. For energy-saving consideration, the average of minimum and actual heat load is selected as the training target in the training samples set and similar samples set. In ensemble module, deep deterministic policy gradient (DDPG) is used to integrate the prediction results of the base predictors. In the case study, real data from a heat exchange station in Tianjin are used to perform the prediction of heat load 24 h, 72 h and 168 h, respectively. Experimental results show that: (a) The proposed hybrid model is able to capture the change of heat load. The average root mean square error for different prediction periods are 0.0825 MW, 0.1007 MW and 0.1152 MW, respectively; (b) The proposed hybrid model can reduce energy consumption. The energy-saving rate for different periods are 5.33%, 5.31% and 5.07%, respectively.}
}
@article{WEI2023125752,
title = {Reinforcement learning based power management integrating economic rotational speed of turboshaft engine and safety constraints of battery for hybrid electric power system},
journal = {Energy},
volume = {263},
pages = {125752},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.125752},
url = {https://www.sciencedirect.com/science/article/pii/S036054422202638X},
author = {Zhengchao Wei and Yue Ma and Ningkang Yang and Shumin Ruan and Changle Xiang},
keywords = {Hybrid electric power system, Turboshaft engine, Economic rotational speed, Variable action space based on safety constraints, Power management, Reinforcement learning},
abstract = {Hybrid electric power system (HEPS) with turboshaft engine is a promising solution for the land and air vehicle, and the power management strategy (PMS) is the key to obtaining better performance of HEPS. In this paper, a reinforcement learning (RL)-based PMS integrating economic rotational speed (ERS) of turboshaft engine and safety constraints-based variable action space (SC-VAS) approach is proposed. First, an efficient algorithm based on the turbine performance map for calculating ERS is proposed, with low complexity which is 5.5% of the conventional algorithm. Second, based on the ERS feature, the SC-VAS approach is presented to further optimize the action space to prevent the discharging/charging power and state of charge of the battery from violating the safety constraints. Comparison results show that with no violation of constraints of battery, the convergence speed of RL agent incorporating the SC-VAS approach increases by 5 times, and the size of the optimized Q table decreases to 21.9% of that of the basic Q table. The proposed PMS with the ERS feature and SC-VAS approach can bring a 4.29% reduction in the fuel consumption under an air-land driving condition. Moreover, the results of the hardware-in-the-loop experiment demonstrate the real-time performance of the proposed strategy.}
}
@article{TANG2020105173,
title = {Reinforcement learning approach for optimal control of multiple electric locomotives in a heavy-haul freight train:A Double-Switch-Q-network architecture},
journal = {Knowledge-Based Systems},
volume = {190},
pages = {105173},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.105173},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119305180},
author = {Huiyue Tang and Yuan Wang and Xiang Liu and Xiaoyun Feng},
keywords = {Reinforcement learning, Double-Switch Q-network, Optimal control, Electric locomotive, Heavy-haul freight train},
abstract = {Electric locomotives provide high tractive power for fast acceleration of heavy-haul freight trains, and significantly reduce the energy consumption with regenerative braking. This paper proposes a reinforcement learning (RL) approach for the optimal control of multiple electric locomotives in a heavy-haul freight train, without using the prior knowledge of train dynamics and the pre-designed velocity profile. The optimization takes the velocity, energy consumption and coupler force as objectives, considering the constraints on locomotive notches and their change rates, speed restrictions, traction and regenerative braking. Besides, since the problem in this paper has continuous state space and large action space, and the adjacent actions’ influences on states share similarities, we propose a Double-Switch Q-network (DSQ-network) architecture to achieve fast approximation of the action-value function, which enhances the parameter sharing of states and actions, and denoises the action-value function. In the numerical experiments, we test DSQ-network in 28 cases using the data of China Railways HXD3B electric locomotive. The results indicate that compared with table-lookup Q-learning, DSQ-network converges much faster and uses less storage space in the optimal control of electric locomotives. Besides, we analyze 1)the influences of ramps and speed restrictions on the optimal policy, and 2)the inter-dependent and inter-conditioned relationships between multiple optimization objectives. Finally, the factors that influence the convergence rate and solution accuracy of DSQ-network are discussed based on the visualization of the high-dimensional value functions.}
}
@article{MAY2023120705,
title = {A multi-agent reinforcement learning approach for investigating and optimising peer-to-peer prosumer energy markets},
journal = {Applied Energy},
volume = {334},
pages = {120705},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.120705},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923000697},
author = {Ross May and Pei Huang},
keywords = {Peer-to-peer market, Community-based market, Dynamic pricing, Multi-agent systems, Multi-agent reinforcement learning, Proximal Policy Optimisation},
abstract = {Current power grid infrastructure was not designed with climate change in mind, and, therefore, its stability, especially at peak demand periods, has been compromised. Furthermore, in light of the current UN’s Intergovernmental Panel on Climate Change reports concerning global warming and the goal of the 2015 Paris climate agreement to constrain global temperature increase to within 1.5–2 °C above pre-industrial levels, urgent sociotechnical measures need to be taken. Together, Smart Microgrid and renewable energy technology have been proposed as a possible solution to help mitigate global warming and grid instability. Within this context, well-managed demand-side flexibility is crucial for efficiently utilising on-site solar energy. To this end, a well-designed dynamic pricing mechanism can organise the actors within such a system to enable the efficient trade of on-site energy, therefore contributing to the decarbonisation and grid security goals alluded to above. However, designing such a mechanism in an economic setting as complex and dynamic as the one above often leads to computationally intractable solutions. To overcome this problem, in this work, we use multi-agent reinforcement learning (MARL) alongside Foundation – an open-source economic simulation framework built by Salesforce Research – to design a dynamic price policy. By incorporating a peer-to-peer (P2P) community of prosumers with heterogeneous demand/supply profiles and battery storage into Foundation, our results from data-driven simulations show that MARL, when compared with a baseline fixed price signal, can learn a dynamic price signal that achieves both a lower community electricity cost, and a higher community self-sufficiency. Furthermore, emergent social–economic behaviours, such as price elasticity, and community coordination leading to high grid feed-in during periods of overall excess photovoltaic (PV) supply and, conversely, high community trading during overall low PV supply, have also been identified. Our proposed approach can be used by practitioners to aid them in designing P2P energy trading markets.}
}
@article{TRENTSIOS2023969,
title = {A Method for Reinforcement Learning-based Development of a System's Information Processing in the Product Development},
journal = {Procedia CIRP},
volume = {119},
pages = {969-974},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.03.141},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123006054},
author = {Pascalis Trentsios and Mario Wolf and Detlef Gerhard},
keywords = {machine learning, AI in product development, sim-to-real, behavior simulation, reinforcement learning},
abstract = {In current product development methods, the development of a system's information processing control unit is done by defining the exact algorithm of "how" to solve a problem. This is a major restriction because it relies on human creativity and problem-solving skills. Additionally and particularly in complex technical systems, problems exist that are intuitive for a human to solve, but the definition of a problem-solving algorithm is not trivial. In this paper, a method is presented and described that addresses the current challenges in the product development process by leveraging the advantages and properties of reinforcement learning. The presented method aims to avoid the complex programming process for the information processing of a system by not providing a specific definition of "how" a problem should be solved, but rather a specific definition of "what" should be solved. Through reinforcement learning-based development, the solution for a defined problem can be generated. In addition, this method is intended to respond to changing conditions, and the ability to make rapid adjustments when already deployed. This reduces the overall complexity during the product development process. The presented method is successfully applied to an exemplary use-case.}
}
@article{LI2019191,
title = {A reinforcement learning unit matching recurrent neural network for the state trend prediction of rolling bearings},
journal = {Measurement},
volume = {145},
pages = {191-203},
year = {2019},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2019.05.093},
url = {https://www.sciencedirect.com/science/article/pii/S0263224119305433},
author = {Feng Li and Yong Chen and Jiaxu Wang and Xueming Zhou and Baoping Tang},
keywords = {Reinforcement learning unit matching recurrent neural network, Reinforcement learning, Singular spectral entropy, State trend prediction, Rolling bearings},
abstract = {This paper proposes a novel neural network, called a reinforcement learning unit matching recurrent neural network (RLUMRNN), with the aim of resolving the problem that the generalization performance and nonlinear approximation ability of typical neural networks are not controllable, which is caused by the experience-based selection of the hidden layer number and hidden layer node number. In the proposed RLUMRNN, the monotone trend discriminator is constructed by using the least squares linear regression method for dividing the whole state degradation trend of rolling bearings into the following three kinds of monotonic trend units: ascending unit, descending unit and stationary unit. Moreover, by virtue of reinforcement learning, the recurrent neural network (RNN) with the hidden layer number and hidden layer node number fitted to a corresponding monotone trend unit is selected to enhance the generalization performance and nonlinear approximation ability of RLUMRNN. Additionally, three monotonic trend units and different hidden layer and node numbers are respectively used to represent the status and action of the Q value table, and a new reward function associated with the RNN’s output errors is constructed to clarify the purpose of reinforcement learning. This makes the RNN’s output errors smaller, which avoids the blind search of Agent (i.e., decision function) in the update of the Q value table and improves the convergence speed of RLUMRNN. By taking advantage of RLUMRNN in the generalization performance, nonlinear approximation ability and convergence speed, a new state trend prediction method for rolling bearings is proposed. In this prediction method, the moving average singular spectral entropy is first used as the state degradation feature of rolling bearings, and then the feature is input into RLUMRNN to accomplish the state trend prediction of rolling bearings. The examples of the state trend prediction for double-row roller bearings demonstrate the higher prediction accuracy and higher calculation efficiency of the proposed method.}
}
@article{WU2020105657,
title = {Reinforcement learning in dual-arm trajectory planning for a free-floating space robot},
journal = {Aerospace Science and Technology},
volume = {98},
pages = {105657},
year = {2020},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2019.105657},
url = {https://www.sciencedirect.com/science/article/pii/S1270963819325660},
author = {Yun-Hua Wu and Zhi-Cheng Yu and Chao-Yong Li and Meng-Jie He and Bing Hua and Zhi-Ming Chen},
keywords = {On-orbit servicing, Free-floating space robot, Dual-arm trajectory planning, Reinforcement learning, Fixed and moving targets},
abstract = {A free-floating space robot exhibits strong dynamic coupling between the arm and the base, and the resulting position of the end of the arm depends not only on the joint angles but also on the state of the base. Dynamic modeling is complicated for multiple degree of freedom (DOF) manipulators, especially for a space robot with two arms. Therefore, the trajectories are typically planned offline and tracked online. However, this approach is not suitable if the target has relative motion with respect to the servicing space robot. To handle this issue, a model-free reinforcement learning strategy is proposed for training a policy for online trajectory planning without establishing the dynamic and kinematic models of the space robot. The model-free learning algorithm learns a policy that maps states to actions via trial and error in a simulation environment. With the learned policy, which is represented by a feedforward neural network with 2 hidden layers, the space robot can schedule and perform actions quickly and can be implemented for real-time applications. The feasibility of the trained policy is demonstrated for both fixed and moving targets.}
}
@article{DING202312076,
title = {Quantized-data resource allocation for reinforcement learning cooperative control of networked Euler-Lagrange agents with input saturation},
journal = {Journal of the Franklin Institute},
volume = {360},
number = {16},
pages = {12076-12100},
year = {2023},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2023.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0016003223005872},
author = {Teng-Fei Ding and Li-Ping Song and Ming-Feng Ge},
abstract = {This paper investigates the resource allocation problem of networked Euler-Lagrange agents (NELAs) with quantized-data interactions and input saturation in the framework of reinforcement learning (RL). We propose a hierarchical control strategy that includes a distributed resource allocation estimator (DRAE) and a local RL linear sliding mode controller (RL-LSMC). Specifically, the DRAE based on the gradient descent and state feedback is proposed, which aims to achieve optimal resource allocation by estimated states. The local RL-LSMC is designed through utilizing the feedback of critic neural network and the approximation capacity of actor neural network, which prompts the states of the NELAs to track the optimal estimated states. Several sufficient conditions are established with the help of Lyapunov stability argument. Finally, the effectiveness of the proposed hierarchical control algorithm is verified by the two simulation examples.}
}
@incollection{PADMANABHAN2020251,
title = {9 - Reinforcement learning-based control of drug dosing with applications to anesthesia and cancer therapy},
editor = {Ahmad Taher Azar},
booktitle = {Control Applications for Biomedical Engineering Systems},
publisher = {Academic Press},
pages = {251-297},
year = {2020},
isbn = {978-0-12-817461-6},
doi = {https://doi.org/10.1016/B978-0-12-817461-6.00009-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128174616000093},
author = {Regina Padmanabhan and Nader Meskin and Wassim M. Haddad},
keywords = {Anesthesia control, Drug dosing, Hemodynamic regulation, Chemotherapy control, Reinforcement learning},
abstract = {Several recent studies in the area of clinical pharmacology highlight the necessity to guarantee patient safety by optimizing the desired effect of the drug and reducing its harmful side effects. Such drug-dosing strategies are particularly useful to develop control paradigms for applications that involve potent drugs as in the case of anesthesia administration and cancer chemotherapy treatments. Reinforcement learning-based algorithms can be used to derive the best control schedule to achieve a defined goal. Implementation of such methods does not require complete knowledge of the system dynamics, instead some information on the behavioral strategy of the system is sufficient. Hence, the concept of Q-learning is used to develop a general framework for designing controllers for drug-dosing applications. This method uses the measurable tracking error to regulate drug infusion to each patient and it can address important challenges in the area of drug dosing such as the absence of an accurate model, nonlinearity in the pharmacokinetics and pharmacodynamics, drug interaction, and side effects due to drug overdosing or underdosing while simultaneously accounting for multiple clinical objectives.}
}
@article{ZHANG2022426,
title = {RKD-VNE: Virtual network embedding algorithm assisted by resource knowledge description and deep reinforcement learning in IIoT scenario},
journal = {Future Generation Computer Systems},
volume = {135},
pages = {426-437},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2200173X},
author = {Peiying Zhang and Peng Gan and Neeraj Kumar and Ching-Hsien Hsu and Shigen Shen and Shibao Li},
keywords = {Industrial Internet of Things, Virtual network embedding, Social attribute perception, Virtual network security, Resource knowledge description, Deep reinforcement learning},
abstract = {In the era of Industry 4.0, the Industrial Internet of Things (IIoT) is developing rapidly, various IIoT applications pose new challenges to the existing network architecture. On the one hand, these applications put forward higher requirements for the efficient use of network resources. On the other hand, these applications generate massive amounts of information, and they pursue a more secure network environment. Therefore, in order to ensure security while effectively allocating network resources, this paper puts forward a virtual network embedding (VNE) algorithm assisted by resource knowledge description (RKD) and deep reinforcement learning (DRL). First, we use social attribute perception to measure the security of each physical node and regard it as one of the attributes of the physical node. Then, RKD is used to standardize resource constraints before the virtual network is embedded. Finally, the DRL agent derives the probability of the physical node being embedded according to the physical network attributes, and embeds the virtual node according to the probability. Simulation experiments show that compared with the BaseLine algorithm, the RKD-VNE algorithm proposed in this paper has obvious advantages in the general performance of VNE, especially in terms of long-term revenue consumption rate increased by 24.3%.}
}
@article{YIN2023108973,
title = {Towards complementary operations of offshore wind farm and photovoltaic array: A centralized reinforcement learning enabled control approach},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {153},
pages = {108973},
year = {2023},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2023.108973},
url = {https://www.sciencedirect.com/science/article/pii/S0142061523000303},
author = {Xiuxing Yin and Meizhen Lei},
keywords = {Integrated offshore wind and PV power system, Complementary operation, Wake effects, Twin-delayed deep deterministic policy gradient, Convergence performance},
abstract = {Integrated offshore wind and photovoltaic (PV) power generation has high potential in significantly improving renewable power utilization, but the complementary operation of the integrated power system is fundamentally challenging, especially under high uncertainties of meteorological conditions. In this paper, the joint regulation of the offshore wind farm and the PV array is considered where the generator torque of each wind turbine and the tilt angles of the PV array are regarded as the control actions to simultaneously maximize the overall power generation and improve power quality. The model of the offshore wind farm including wake models and the turbine drive-train dynamic model is established while model of the PV array using the tilt angle as the regulation variable is also constructed. By considering the control of the integrated power system of the wind farm and the PV array as a typical partially-observable Markov decision process (MDP), a twin-delayed deep deterministic policy gradient (TD3) algorithm is developed to jointly regulate the wind farm and PV array. Design experiments have been conducted to evaluate the potential and effectiveness of the proposed TD3 control method for the integrated offshore wind and PV power system. The test results demonstrate that the TD3 method has good convergence performances in the overall power output and power variation rate regulations. The control objectives of improving the overall power output and smoothing the power variations can be simultaneously achieved by using appropriate parameter settings in the TD3 algorithm.}
}
@article{XIAO2023102440,
title = {Multimodal fusion for autonomous navigation via deep reinforcement learning with sparse rewards and hindsight experience replay},
journal = {Displays},
volume = {78},
pages = {102440},
year = {2023},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2023.102440},
url = {https://www.sciencedirect.com/science/article/pii/S0141938223000732},
author = {Wendong Xiao and Liang Yuan and Teng Ran and Li He and Jianbo Zhang and Jianping Cui},
keywords = {Hindsight experience replay, Obstacle avoidance, Deep reinforcement learning, Sparse rewards, Multimodal navigation},
abstract = {The multimodal perception of intelligent robots is essential for achieving collision-free and efficient navigation. Autonomous navigation is enormously challenging when perception is acquired using only vision or LiDAR sensor data due to the lack of complementary information from different sensors. This paper proposes a simple yet efficient deep reinforcement learning (DRL) with sparse rewards and hindsight experience replay (HER) to achieve multimodal navigation. By adopting the depth images and pseudo-LiDAR data generated by an RGB-D camera as input, a multimodal fusion scheme is used to enhance the perception of the surrounding environment compared to using a single sensor. To alleviate the misleading way for the agent to navigate with dense rewards, the sparse rewards are intended to identify its tasks. Additionally, the HER technique is introduced to address the sparse reward navigation issue for accelerating optimal policy learning. The results show that the proposed model achieves state-of-the-art performance in terms of success, crash, and timeout rates, as well as generalization capability.}
}
@article{WANG2022111495,
title = {A novel path following approach for autonomous ships based on fast marching method and deep reinforcement learning},
journal = {Ocean Engineering},
volume = {257},
pages = {111495},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.111495},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822008666},
author = {Shuwu Wang and Xinping Yan and Feng Ma and Peng Wu and Yuanchang Liu},
keywords = {Path following, Autonomous ships, Fast marching method, Deep reinforcement learning, Navigation brain system},
abstract = {Path following is one of the indispensable tools for autonomous ships, which ensures that autonomous ships are sufficiently capable of navigating in specified collision-free waters. This study proposes a novel path following approach for autonomous ships based on the fast marching (FM) method and deep reinforcement learning (DRL). The proposed approach is capable of controlling a ship to follow different paths and ensuring that the path tracking errors are always within a set range. With the help of the FM method, a grid-based path deviation map is specially produced to indicate the minimum distance between grid points and the path. Besides, a path deviation perceptron is specifically designed to simulate a range sensor for sensing the set path deviation boundaries based on the path deviation map. Afterwards, an agent is trained to control a ship following a circular path based on the DRL. Particularly, the approach is validated and evaluated through simulations. The obtained results show that the proposed method is always capable of maintaining high overall efficiency with the same strategy to follow different paths. Moreover, the ability of this approach exhibits a significant contribution to the development of autonomous ships.}
}
@article{XU2023,
title = {Dynamic adversarial jamming-based reinforcement learning for designing constellations},
journal = {Digital Communications and Networks},
year = {2023},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2023.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S2352864823000937},
author = {Yizhou Xu and Haidong Xie and Nan Ji and Yuanqing Chen and Naijin Liu and Xueshuang Xiang},
keywords = {Wireless communication, Constellation design, Reinforcement learning, Adversarial jamming},
abstract = {To resist various types of jamming in wireless channels, appropriate constellation modulation is used in wireless communication to ensure a low bit error rate. Due to the complexity and variability of the channel environment, a simple preset constellation is difficult to adapt to all scenarios, so the online constellation optimization method based on Reinforcement Learning (RL) shows its potential. However, the existing RL technology is difficult to ensure the optimal convergence efficiency. Therefore, in this paper, Dynamic Adversarial Interference (DAJ) waveforms are introduced and the DAJ-RL method is proposed by referring to adversarial training in Deep Learning (DL). The algorithm can converge to the optimal state quickly by self-adaptive power and probability direction of dynamic strong adversary of DAJ. In this paper, a rigorous theoretical proof of the symbol error rate is given and it is shown that the method approaches the mathematical limit. Also, numerical and hardware experiments show that the constellations generated by DAJ-RL have the best error rate at all noise levels. In the end, the proposed DAJ-RL method effectively improves the RL-based anti-jamming modulation for cognitive electronic warfare.}
}
@article{ZHANG2020112875,
title = {A reinforcement learning based approach for on-line adaptive parameter extraction of photovoltaic array models},
journal = {Energy Conversion and Management},
volume = {214},
pages = {112875},
year = {2020},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2020.112875},
url = {https://www.sciencedirect.com/science/article/pii/S0196890420304131},
author = {Jingwei Zhang and Yongjie Liu and Yuanliang Li and Kun Ding and Li Feng and Xihui Chen and Xiang Chen and Jiabing Wu},
keywords = {Reinforcement learning, On-line adaptive extraction, PV array, Parameter extraction, Mathematical model},
abstract = {At present, most methods for the fault detection and diagnosis (FDD) of the photovoltaic (PV) array strongly rely on comparing the on-line measured electrical parameters with the modeled reference ones, which are challenging the on-line accuracy and time cost of the parameter extraction for modeling the current-voltage (I-V) curves of the PV array. In this paper, a reinforcement learning (RL) based approach for on-line adaptive parameter extraction of PV array models is proposed. The model parameters, including the ideality factor, series and shunt resistance, and the compensated irradiance for the uncalibrated pyranometer, are extracted. Corresponding environmental states, actions, rewards, and the entire framework for the on-line adaptive parameter extraction are reasonably designed and investigated. The annual experimental results verify that the proposed RL-based approach can obtain higher on-line accuracy for modeling the I-V curve of PV array with fast extraction speed, compared with the conventional meta-heuristic-based approach and the analytical approach for parameter extraction. The annual experimental results reveal that the proposed approach can guarantee the 50% probability for obtaining the root mean square error (RMSE) less than 0.1, and 90% probability for obtaining the RMSE less than 0.25. The average computational time cost of the proposed approach is approximate 38.12 ms. In addition, the annual trend of extracted model parameters is analyzed. The annual results also show that the series and shunt resistance have the inverse seasonal trend. Besides, the measurement error of the pyranometer can be identified statistically. The proposed RL-based approach can also be integrated with the presented on-line FDD method, which realizes the on-line training of RL agents and the FDD of PV array simultaneously.}
}
@incollection{CHOO2022127,
title = {Chapter 9 - Interactive reinforcement learning and error-related potential classification for implicit feedback},
editor = {Chang S. Nam and Jae-Yoon Jung and Sangwon Lee},
booktitle = {Human-Centered Artificial Intelligence},
publisher = {Academic Press},
pages = {127-143},
year = {2022},
isbn = {978-0-323-85648-5},
doi = {https://doi.org/10.1016/B978-0-323-85648-5.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323856485000050},
author = {Sanghyun Choo and Chang S. Nam},
keywords = {Deep Learning, Electroencephalogram (EEG), Error-related Potential (ErrP), Interactive Reinforcement Learning (IRL), Machine Learning},
abstract = {Implicit feedback-based interactive reinforcement learning (IRL) using error-related potentials (ErrP) is an emerging research topic in artificial intelligence (AI) society. This IRL both efficiently and effectively improves RL algorithm performance by intervening implicitly through ErrP. This approach is closely related to human-centered AI (HCAI) in that human feedback is directly involved in the RL model. However, an understanding of how to classify ErrP and develop the IRL based on human feedback is still needed for people in the HCAI field. Therefore, in this book chapter, we introduce the state-of-the-art machine learning and deep learning methods for ErrP classification and then show their performance using a public brain–computer interface dataset. Also, we introduce the mistake correcting technique, which is one of the IRL methods, and then show the IRL's effectiveness compared to the original RL method based on an RL problem provided in OpenAI Gym. These introductions to the ErrP classification and the IRL will help lower barriers to entry into implicit feedback-based IRL for researchers in the HCAI field.}
}
@incollection{WANJERKHEDE2007193,
title = {Modeling the sub-cellular signaling pathways involved in reinforcement learning at the striatum},
editor = {Rahul Banerjee and Bikas K. Chakrabarti},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {168},
pages = {193-206},
year = {2007},
booktitle = {Models of Brain and Mind},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(07)68016-9},
url = {https://www.sciencedirect.com/science/article/pii/S0079612307680169},
author = {Shesharao M. Wanjerkhede and Raju S. Bapi},
keywords = {NMDA, AMPA, mGluR, RL, TDL, CaMKII, LTP, GENESIS, Kinetikit, signaling pathways, reinforcement learning, striatum},
abstract = {A general discussion of various levels of models in computational neuroscience is presented. A detailed case study of modeling at the sub-cellular level is undertaken. The process of learning actions by reward or punishment is called ‘Instrumental Conditioning’ or ‘Reinforcement Learning’ (RL). Temporal difference learning (TDL) is a mathematical framework for RL. Houk et al. (1995) proposed a cellular signaling model for interaction of dopamine (DA) and glutamate activities at the striatum that forms the basis for TDL. In the model, glutamatergic input generates a membrane depolarization through N-methyl-d-aspartate (NMDA), α-amino-5-hydroxy-3-methyl-4-isoxazole propionic acid (AMPA), metabotropic glutamate receptors (mGluR), and opens calcium two plus ion (Ca2+) channels resulting in the influx of Ca2+ into the dendritic spine. This raises the postsynaptic calcium concentration in the dendritic spine leading to the autophosphorylation of calcium/calmodulin-dependent protein kinase II (CaMKII). The timely arrival of the DA input at the neck of the spine head generates a cascade of reactions which then leads to the prolongation of long-term potentiation (LTP) generated by the autophosphorylation of CaMKII. Since no simulations were done so far to support this proposal, we undertook the task of computational verification of the model. During the simulations it was found that there was enhancement and prolongation of autophosphorylation of CaMKII. This result verifies Houk's proposal for LTP in the striatum. Our simulation results are generally in line with the known biological experimental data and also suggest predictions for future experimental verification.}
}
@article{WU2023103126,
title = {DroidRL: Feature selection for android malware detection with reinforcement learning},
journal = {Computers & Security},
volume = {128},
pages = {103126},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103126},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823000366},
author = {Yinwei Wu and Meijin Li and Qi Zeng and Tao Yang and Junfeng Wang and Zhiyang Fang and Luyu Cheng},
keywords = {Reinforcement learning, Android malware detection, Feature selection, RNN, Sequence processing},
abstract = {Due to the completely open-source nature of Android, the exploitable vulnerability of malware attacks is increasing. Machine learning, leading to a great evolution in Android malware detection in recent years, is typically applied in the classification phase. Since the correlation between features is ignored in some traditional ranking-based feature selection algorithms, applying wrapper-based feature selection models is a topic worth investigating. Though considering the correlation between features,  wrapper-based approaches are time-consuming for exploring all possible valid feature subsets when processing a large number of Android features. To reduce the computational expense of wrapper-based feature selection, a framework named DroidRL is proposed. The framework deploys DDQN algorithm to obtain a subset of features which can be used for effective malware classification. To select a valid subset of features over a larger range, the exploration-exploitation policy is applied in the model training phase. The recurrent neural network (RNN) is used as the decision network of DDQN to give the framework the ability to sequentially select features. Word embedding is applied for feature representation to enhance the framework’s ability to find the semantic relevance of features. The framework’s feature selection exhibits high performance without any human intervention and can be ported to other feature selection tasks with minor changes. The experiment results show a significant effect when using the Random Forest as DroidRL’s classifier, which reaches 95.6% accuracy with only 24 features selected.}
}
@article{ZHU2022472,
title = {Swarm Deep Reinforcement Learning for Robotic Manipulation},
journal = {Procedia Computer Science},
volume = {198},
pages = {472-479},
year = {2022},
note = {12th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 11th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.272},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921025114},
author = {Xudong Zhu and Fan Zhang and Hui Li},
keywords = {Robotic Manipulation, Deep Reinforcement Learning, Blockchain, Swarm Learning},
abstract = {Deep reinforcement learning scheme, which combines both deep learning and reinforcement learning, enables robots to learn from exploration and flexibly performance in a range of different operational tasks under highly dynamic and complex environments encountered in daily life. However, robotic manipulation still face many serious threats due to inadequate data sharing between robots and concerns about data privacy and security. To privacy-protect the data of all owners, we propose a swarm reinforcement learning method, a decentralized deep reinforcement learning technology based on block chain. Specifically, each robotic agent controls the robot using actor-critic strategy optimization algorithm, and shares their learning experience (i.e. loss function gradient) through the blockchain network, and passes on a mature strategy model parameters to other agents. Experimental results indicate that our swarm reinforcement learning method can improve the learning process of several agents, and the more agents there are, the faster the learning speed will be.}
}
@article{BACCOUR2020982,
title = {RL-OPRA: Reinforcement Learning for Online and Proactive Resource Allocation of crowdsourced live videos},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {982-995},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.06.038},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20306269},
author = {Emna Baccour and Aiman Erbad and Amr Mohamed and Fatima Haouari and Mohsen Guizani and Mounir Hamdi},
keywords = {Live streaming, QoE, Geo-distributed clouds, Machine and reinforcement learning},
abstract = {With the advancement of rich media generating devices, the proliferation of live Content Providers (CP), and the availability of convenient internet access, crowdsourced live streaming services have witnessed unexpected growth. To ensure a better Quality of Experience (QoE), higher availability, and lower costs, large live streaming CPs are migrating their services to geo-distributed cloud infrastructure. However, because of the dynamics of live broadcasting and the wide geo-distribution of viewers and broadcasters, it is still challenging to satisfy all requests with reasonable resources. To overcome this challenge, we introduce in this paper a prediction driven approach that estimates the potential number of viewers near different cloud sites at the instant of broadcasting. This online and instant prediction of distributed popularity distinguishes our work from previous efforts that provision constant resources or alter their allocation as the popularity of the content changes. Based on the derived predictions, we formulate an Integer-Linear Program (ILP) to proactively and dynamically choose the right data center to allocate exact resources and serve potential viewers, while minimizing the perceived delays. As the optimization is not adequate for online serving, we propose a real-time approach based on Reinforcement Learning (RL), namely RL-OPRA, which adaptively learns to optimize the allocation and serving decisions by interacting with the network environment. Extensive simulation and comparison with the ILP have shown that our RL-based approach is able to present optimal results compared to heuristic-based approaches.}
}
@article{MIZUNO2023162,
title = {Application of Reinforcement Learning to Generate Non-linear Optimal Feedback Controller for Ship's Automatic Berthing System},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {1},
pages = {162-168},
year = {2023},
note = {12th IFAC Symposium on Nonlinear Control Systems NOLCOS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.02.028},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323002161},
author = {Naoki Mizuno and Tatsuya Koide},
keywords = {reinforcement learning, actor-critic method, non-linear optimal control, feedback control, ship control, automatic berthing},
abstract = {In this paper, we present an application of reinforcement learning for automatic ship's berthing system. In the proposed method, the model-based reinforcement learning is used to generate non-linear feedback controller. The proposed system is composed of an Actor-Critic algorithm suitable for continuous state and a function approximator by Radial Basis Function networks. To evaluate the performance of the proposed system, extensive computer simulations and actual sea tests are carried out using small training ship Shioji-Maru III under various conditions. As a result, we can see that the proposed non-linear optimal feedback controller by reinforcement learning is useful for designing automatic berthing system for the ship.}
}
@article{HEIN201787,
title = {Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies},
journal = {Engineering Applications of Artificial Intelligence},
volume = {65},
pages = {87-98},
year = {2017},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0952197617301537},
author = {Daniel Hein and Alexander Hentschel and Thomas Runkler and Steffen Udluft},
keywords = {Interpretable, Reinforcement learning, Fuzzy policy, Fuzzy controller, Particle swarm optimization},
abstract = {Fuzzy controllers are efficient and interpretable system controllers for continuous state and action spaces. To date, such controllers have been constructed manually or trained automatically either using expert-generated problem-specific cost functions or incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not found in most real-world reinforcement learning (RL) problems. In such applications, online learning is often prohibited for safety reasons because it requires exploration of the problem’s dynamics during policy training. We introduce a fuzzy particle swarm reinforcement learning (FPSRL) approach that can construct fuzzy RL policies solely by training parameters on world models that simulate real system dynamics. These world models are created by employing an autonomous machine learning technique that uses previously generated transition samples of a real system. To the best of our knowledge, this approach is the first to relate self-organizing fuzzy controllers to model-based batch RL. FPSRL is intended to solve problems in domains where online learning is prohibited, system dynamics are relatively easy to model from previously generated default policy transition samples, and it is expected that a relatively easily interpretable control policy exists. The efficiency of the proposed approach with problems from such domains is demonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole balancing, and cart-pole swing-up. Our experimental results demonstrate high-performing, interpretable fuzzy policies.}
}
@article{CUI2016220,
title = {Reinforcement learning-based asymptotic cooperative tracking of a class multi-agent dynamic systems using neural networks},
journal = {Neurocomputing},
volume = {171},
pages = {220-229},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.06.066},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215009418},
author = {Lili Cui and Xiaowei Wang and Yong Zhang},
keywords = {Reinforcement learning, Neural networks, Multi-agent dynamic systems, Cooperative tracking, RISE technique},
abstract = {In this paper, a novel reinforcement learning-based cooperative tracking control scheme is proposed for a class of multi-agent dynamic systems with disturbances and un-modeled dynamics on undirected graphs by using neural networks (NNs). For each agent, two NNs are employed, i.e., an actor NN which approximates the unknown nonlinearity and generates the control input, and a critic NN which evaluates the performance of the actor and updates the weights of actor NN. Further, a RISE technique is utilized in the design of the actor NN and the critic NN to compensate for the external disturbances and the NN approximation errors. Based on the Lyapunov theory, it is proved that the proposed control scheme can guarantee the tracking error of each agent to converge to zero asymptotically. Additionally, the proposed control scheme is distributed in the sense that the controller for each agent only uses the local neighbor information. Finally, two simulation examples are given to verify the effectiveness of the proposed control scheme.}
}
@article{LI2023103256,
title = {Multi-agent deep reinforcement learning based resource management in SWIPT enabled cellular networks with H2H/M2M co-existence},
journal = {Ad Hoc Networks},
volume = {149},
pages = {103256},
year = {2023},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2023.103256},
url = {https://www.sciencedirect.com/science/article/pii/S1570870523001762},
author = {Xuehua Li and Xing Wei and Shuo Chen and Lixin Sun},
keywords = {Machine-to-Machine communication, Human-to-Human H2H, Resource management, Simultaneous wireless information and power transfer, Quality-of-Service, Multi-agent deep reinforcement learning},
abstract = {Machine-to-Machine (M2M) communication is crucial in developing Internet of Things (IoT). As it is well known that cellular networks have been considered as the primary infrastructure for M2M communications, there are several key issues to be addressed in order to deploy M2M communications over cellular networks. Notably, the rapid growth of M2M traffic dramatically increases energy consumption, as well as degrades the performance of existing Human-to-Human (H2H) traffic. Sustainable operation technology and resource management are efficacious ways for solving these issues. In this paper, we investigate a resource management problem in cellular networks with H2H/M2M coexistence. First, considering the energy-constrained nature of machine type communication devices (MTCDs), we propose a novel network model enabled by simultaneous wireless information and power transfer (SWIPT), which empowers MTCDs with the ability to simultaneously perform energy harvesting (EH) and information decoding. Given the diverse characteristics of IoT devices, we subdivide MTCDs into critical and tolerable types, further formulating the resource management problem as an energy efficiency (EE) maximization problem under divers Quality-of-Service (QoS) constraints. Then, we develop a multi-agent deep reinforcement learning (DRL) based scheme to solve this problem. It provides optimal spectrum, transmit power and power splitting (PS) ratio allocation policies, along with efficient model training under designed behavior-tracking based state space and common reward function. Finally, we verify that with a reasonable training mechanism, multiple M2M agents successfully work cooperatively in a distributed way, resulting in network performance that outperforms other intelligence approaches in terms of convergence speed and meeting the EE and QoS requirements.}
}
@article{ZHANG2023120599,
title = {Hierarchical reinforcement learning based energy management strategy of plug-in hybrid electric vehicle for ecological car-following process},
journal = {Applied Energy},
volume = {333},
pages = {120599},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120599},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922018566},
author = {Hailong Zhang and Jiankun Peng and Hanxuan Dong and Huachun Tan and Fan Ding},
keywords = {Hybrid electric vehicle, Reinforcement learning, Adaptive cruise control, Energy management},
abstract = {The economy-oriented automated hybrid eclectic vehicles (HEV) provide great potential to save energy by optimizing both driving behaviors and power distribution. Recent advances in the ecological car following issue of HEV focus on fusing adaptive cruise control (ACC) and energy management system (EMS) by collaborative optimization. However, series control frameworks ACC+EMS breaks the internal coupling relation between motion control and energy distribution, leading to the natural limitation of its optimization. On the opposite, integrated ACC-EMS promises energy-saving improvement but brings complex optimization problems with multi-scale objectives and large exploration space. The huge computation load restricts the online application of ACC-EMS. To address these problems, a hierarchical reinforcement learning based ACC-EMS strategy is proposed with a hierarchical policy and non-hierarchical execution. The upper layer learns to plan state-of-charge and time-headway trajectories, while the low layer policy learns to achieve the expected goals by outputting control variables executed by the host vehicle. The proposed ACC-EMS strategy were self-learning by interaction in car-following scenario constructed with GPS data on I-880 highway. Comprehensive simulations show the proposed strategy has significantly improved the training speed and stability, compared to the offline global optimum, achieving the energy consumption difference of less than 3% and computational load of less than 600 times.}
}
@article{MALIK2021100920,
title = {A novel hybrid approach based on relief algorithm and fuzzy reinforcement learning approach for predicting wind speed},
journal = {Sustainable Energy Technologies and Assessments},
volume = {43},
pages = {100920},
year = {2021},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2020.100920},
url = {https://www.sciencedirect.com/science/article/pii/S2213138820313473},
author = {Hasmat Malik and Amit Kumar Yadav},
keywords = {Wind speed, Forecasting, Fuzzy logic, Q learning, Fuzzy system, FQL, Reinforcement},
abstract = {Wind speed (WS) prediction has become popular nowadays due to increasing demand for wind power generation and competitive development in wind energy. Many prediction models are used to predict WS for which wind is non-stationary, nonlinear and irregular. However, they neglect the effectiveness of feature selection methods in WS prediction, thereby creating very challenging for precise prediction of WS and safe operation of the wind industry. To overpower these challenges and further improve WS prediction accuracy, a prediction model is developed based on feature selection technique and prediction models. Therefore this study proposes an adaptive self-learning wind speed (WS) predicting model using fuzzy reinforcement learning (FRL) that is Fuzzy Q Learning (FQL). Proposed FQL based WS predictor model can predict with great accuracy. This is a first effort at developing a forecasting model using FRL for WS prediction. The presented model has no prior knowledge of the system or plant or target speed information. Measured WS is processed through Info Gain attribute evaluator with Ranker search method feature selection purpose which serves as input to the FQL based WS prediction model. The comparison of proposed prediction method and existing machine learning based is carried out using simulations. The performance analysis indicates that the proposed method serves as an important tool for wind potential assessment.}
}
@article{ZHANG2023102173,
title = {Deep reinforcement learning enabled UAV-IRS-assisted secure mobile edge computing network},
journal = {Physical Communication},
volume = {61},
pages = {102173},
year = {2023},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2023.102173},
url = {https://www.sciencedirect.com/science/article/pii/S1874490723001763},
author = {Yingzheng Zhang and Jufang Li and Guangchen Mu and Xiaoyu Chen},
keywords = {Mobile edge computing, Unmanned aerial vehicle, Intelligent reflecting surfaces, Deep reinforcement learning, Physical layer security},
abstract = {The deployment of intelligent reflecting surfaces (IRS) on dynamically moving unmanned aerial vehicles (UAVs) can enhance the communication performance of mobile edge computing (MEC), improve the system flexibility, and alleviate eavesdropping on air–ground channels. In this paper, an IRS-equipped unmanned aerial vehicle (UAV)-assisted secure MEC network is proposed. By jointly optimizing the Relay-UAV stopping point, IRS-UAV stopping point, IRS reflection coefficients and the task offloading ratio, the objective of our proposed optimization scheme is to minimize the transmission delay and computing delay while considering the secure transmission performance. To solve this non-convex optimization problem with coupled variables, we propose an intelligent optimization algorithm based on dueling double deep Q networks (D3QN)-deep deterministic policy gradient (DDPG) that can efficiently explore the trajectories and a great number of the IRS reflection elements. Simulation results demonstrate that the intelligent algorithm exhibits good convergence and our proposed scheme can achieve a good balance between system consumption and secrecy rate.}
}