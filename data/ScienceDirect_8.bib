@article{KARMAKAR2021104899,
title = {On tight bounds for function approximation error in risk-sensitive reinforcement learning},
journal = {Systems & Control Letters},
volume = {150},
pages = {104899},
year = {2021},
issn = {0167-6911},
doi = {https://doi.org/10.1016/j.sysconle.2021.104899},
url = {https://www.sciencedirect.com/science/article/pii/S0167691121000293},
author = {Prasenjit Karmakar and Shalabh Bhatnagar},
keywords = {Risk-sensitive reinforcement learning, Perron–Frobenius eigenvalue, Stochastic systems, Stochastic optimal control, Eigenvalue perturbation, Function approximation},
abstract = {In this letter we provide several informative tight error bounds when using value function approximators for the risk-sensitive cost setting for a given policy represented using exponential utility. The novelty of our approach is that we make use of the irreducibility of the underlying Markov chain (resulting in better bounds using Perron–Frobenius eigenvectors) to derive new bounds whereas the earlier work used primarily the spectral variation bound which holds for any matrix, hence did not make use of the irreducibility. All our bounds have a perturbation term for large state spaces. We also present examples where we show that the new bounds perform 90-100% better than the earlier proposed spectral variation bound.}
}
@article{OZCELIK2022103451,
title = {ALVS: Adaptive Live Video Streaming using deep reinforcement learning},
journal = {Journal of Network and Computer Applications},
volume = {205},
pages = {103451},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103451},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522001035},
author = {Ihsan Mert Ozcelik and Cem Ersoy},
keywords = {Adaptive playback speed, Deep reinforcement learning, Live streaming media and video quality},
abstract = {Achieving a high Quality of Experience (QoE) in live event streaming is a challenging problem given a low-latency requirement and time-varying network conditions. Adaptive video bitrate and adaptive playback speed techniques are two separate control knobs to address this challenge. In this paper, we consider these two control parameters in a joint optimization problem and present a deep reinforcement learning (DRL) framework to maximize QoE for live streaming without any assumption about the environment or fixed rule-based heuristics. With the proposed DRL framework, our approach (ALVS) constructs the inference model to make a joint decision of adaptive playback speed and video quality level for the next video segment. Simulation results through real network traces show that ALVS outperforms both state-of-the-art DRL-based and rule-based algorithms in terms of QoE without sacrificing live latency and skipping any content.}
}
@article{MUGHEES2023102206,
title = {Energy-efficient joint resource allocation in 5G HetNet using Multi-Agent Parameterized Deep Reinforcement learning},
journal = {Physical Communication},
pages = {102206},
year = {2023},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2023.102206},
url = {https://www.sciencedirect.com/science/article/pii/S1874490723002094},
author = {Amna Mughees and Mohammad Tahir and Muhammad Aman Sheikh and Angela Amphawan and Yap Kian Meng and Abdul Ahad and Kazem Chamran},
keywords = {5G, Energy efficiency, Ultra-dense network, DRL, Resource allocation machine learning, HetNet, User association, Power allocation, Reinforcement learning},
abstract = {Small cells are a promising technique to improve the capacity and throughput of future wireless networks. However, user association and power allocation in heterogeneous networks is complicated by the dense deployment of small cells, resulting in non-convex and combinatorial problems. Conventionally, machine learning techniques are applied to the joint optimization problem, which has different action spaces. Gauging the continuous spaces to discrete spaces results in the loss of granularity due to discretization (e.g. potential power values in power allocation). Due to its hybrid action space, it is sub-optimal to solve joint user association (discrete spaces) and power allocation (continuous spaces) problems by applying traditional machine learning approaches. This work proposes a Multi-Agent Parameterized Deep Reinforcement Learning (MA-PDRL) approach to address the joint user association and power allocation problem efficiently. According to simulation results, the proposed multi-agent PDRL performs better in energy efficiency and QoS satisfaction than WMMSE, game theory, Q-learning, and DRL techniques.}
}
@article{GUO2021104758,
title = {An integrated MPC and deep reinforcement learning approach to trams-priority active signal control},
journal = {Control Engineering Practice},
volume = {110},
pages = {104758},
year = {2021},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2021.104758},
url = {https://www.sciencedirect.com/science/article/pii/S0967066121000356},
author = {Ge Guo and Yunpeng Wang},
keywords = {Trams, Transit signal priority, Model predictive control, Deep reinforcement learning, Artificial intelligence},
abstract = {The problem of active signal priority control for trams is investigated. A combined model predictive control (MPC) and deep reinforcement learning solution is proposed, to minimize stopping of trams at intersections while reducing delay of general vehicles. An efficient new deep reinforcement learning (DRL) framework is introduced to improve the proximal policy optimization with model-based acceleration (PPOMA). The DRL module is strengthened by a model predictive controller, which provides low-precision prediction of the real-time traffic dynamics to improve the learning performance. The problem is modeled as a high-dimension Markov decision process. Dynamic phase sequence is used to improve the flexibility of signal priority control, instead of only optimizing a signal cycle in a fixed phase sequence as in other methods. The optimal traffic signal sequence is obtained by using real-time traffic information collected from vehicular networks. Experiments with SUMO have shown the advantage of our method in comparison with the existing methods.}
}
@article{LI20114488,
title = {Reinforcement learning control with adaptive gain for a Saccharomyces cerevisiae fermentation process},
journal = {Applied Soft Computing},
volume = {11},
number = {8},
pages = {4488-4495},
year = {2011},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2011.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S156849461100305X},
author = {Dazi Li and Li Qian and Qibing Jin and Tianwei Tan},
keywords = {-Learning algorithm, Multi-step action, Fermentation process, Adaptive gain},
abstract = {A tight and robust yeast fermentation controller is usually difficult to achieve because of the inherent uncertainty, nonlinear, and time-varying characteristics of the yeast fermentation dynamic process. This paper presented an alternative method for yeast fermentation process control by hybrid reinforcement learning algorithm and fuzzy logic. The fuzzy logic was used to adjust the weighting gain of control action adaptively from reinforcement learning. It led to faster tracking and helped to alleviate the overshoot of the controller. The improved multi-step action Q-learning control algorithm was developed and demonstrated through studies on ethanol concentration control of the yeast fermentation process. Experimental results show that the improved multi-step action Q-learning controller has much lower overshoot, faster tracking, shorter transition, and smoother control signal than the advanced PID controller.}
}
@article{YEN2004217,
title = {Reinforcement learning algorithms for robotic navigation in dynamic environments},
journal = {ISA Transactions},
volume = {43},
number = {2},
pages = {217-230},
year = {2004},
issn = {0019-0578},
doi = {https://doi.org/10.1016/S0019-0578(07)60032-9},
url = {https://www.sciencedirect.com/science/article/pii/S0019057807600329},
author = {Gary G. Yen and Travis W. Hickey},
keywords = {Reinforcement learning, Dynamic environment, Navigation, Obstacle avoidance},
abstract = {The purpose of this study was to examine improvements to reinforcement learning (RL) algorithms in order to successfully interact within dynamic environments. The scope of the research was that of RL algorithms as applied to robotic navigation. Proposed improvements include: addition of a forgetting mechanism, use of feature based state inputs, and hierarchical structuring of an RL agent. Simulations were performed to evaluate the individual merits and flaws of each proposal, to compare proposed methods to prior established methods, and to compare proposed methods to theoretically optimal solutions. Incorporation of a forgetting mechanism did considerably improve the learning times of RL agents in a dynamic environment. However, direct implementation of a feature-based RL agent did not result in any performance enhancements, as pure feature-based navigation results in a lack of positional awareness, and the inability of the agent to determine the location of the goal state. Inclusion of a hierarchical structure in an RL agent resulted in significantly improved performance, specifically when one layer of the hierarchy included a feature-based agent for obstacle avoidance, and a standard RL agent for global navigation. In summary, the inclusion of a forgetting mechanism, and the use of a hierarchically structured RL agent offer substantially increased performance when compared to traditional RL agents navigating in a dynamic environment.}
}
@article{TANG2022121593,
title = {Longevity-conscious energy management strategy of fuel cell hybrid electric Vehicle Based on deep reinforcement learning},
journal = {Energy},
volume = {238},
pages = {121593},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121593},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221018417},
author = {Xiaolin Tang and Haitao Zhou and Feng Wang and Weida Wang and Xianke Lin},
keywords = {Energy management strategy, Deep reinforcement learning, Fuel cell hybrid electric vehicles, DQN algorithm, Prioritized experience replay, Degradation},
abstract = {Deep reinforcement learning-based energy management strategy play an essential role in improving fuel economy and extending fuel cell lifetime for fuel cell hybrid electric vehicles. In this work, the traditional Deep Q-Network is compared with the Deep Q-Network with prioritized experience replay. Furthermore, the Deep Q-Network with prioritized experience replay is designed for energy management strategy to minimize hydrogen consumption and compared with the dynamic programming. Moreover, the fuel cell system degradation is incorporated into the objective function, and a balance between fuel economy and fuel cell system degradation is achieved by adjusting the degradation weight and the hydrogen consumption weight. Finally, the combined driving cycle is selected to further verify the effectiveness of the proposed strategy in unfamiliar driving environments and untrained situations. The training results under UDDS show that the fuel economy of the EMS decreases by 0.53 % when fuel cell system degradation is considered, reaching 88.73 % of the DP-based EMS in the UDDS, and the degradation of fuel cell system is effectively suppressed. At the same time, the computational efficiency is improved by more than 70 % compared to the DP-based strategy.}
}
@article{LI2023110701,
title = {Graph neural network architecture search for rotating machinery fault diagnosis based on reinforcement learning},
journal = {Mechanical Systems and Signal Processing},
volume = {202},
pages = {110701},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2023.110701},
url = {https://www.sciencedirect.com/science/article/pii/S088832702300609X},
author = {Jialin Li and Xuan Cao and Renxiang Chen and Xia Zhang and Xianzhen Huang and Yongzhi Qu},
keywords = {Rotating machinery, Fault diagnosis, Graph neural network, Neural architecture search, Reinforcement learning},
abstract = {In order to improve the accuracy of fault diagnosis, researchers are constantly trying to develop new diagnostic models. However, limited by the inherent thinking of human beings, it has always been difficult to build a pioneering architecture for rotating machinery fault diagnosis. In order to solve this problem, this paper uses reinforcement learning algorithm based on adjacency matrix to carry out network architecture search (NAS) of rotating machinery fault diagnosis model. A reinforcement learning agent for deep deterministic policy gradient (DDPG) is developed based on actor–critic neural networks. The observation state of reinforcement learning is used to develop the graph neural network (GNN) diagnosis model, and the diagnosis accuracy is fed back to the agent as a reward for updating the reinforcement learning parameters. The MFPT bearing fault datasets and the developed gear pitting fault experimental data are used to validate the proposed network architecture search method based on reinforcement learning (RL-NAS). The proposed method is proved to be practical and effective in various aspects such as fault diagnosis ability, search space, search efficiency and multi-working condition performance.}
}
@article{SHAH201689,
title = {Model-Free Predictive Control of Nonlinear Processes Based on Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {1},
pages = {89-94},
year = {2016},
note = {4th IFAC Conference on Advances in Control and Optimization of Dynamical Systems ACODS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.03.034},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316300349},
author = {Hitesh Shah and M. Gopal},
keywords = {Model predictive control, Reinforcement learning, Q-learning},
abstract = {Model predictive control (MPC) is a model-based control philosophy in which the current control action is obtained by on-line optimization of objective function. MPC is, by now, considered to be a mature technology owing to the plethora of research and industrial process control applications. The model under consideration is either linear or piece-wise linear. However, turning to the nonlinear processes, the difficulties are in obtaining a good nonlinear model, and the excessive computational burden associated with the control optimization. Proposed framework, named as model-free predictive control (MFPC), takes care of both the issues of conventional MPC. Model-free reinforcement learning formulates predictive control problem with a control horizon of only length one, but takes a decision based on infinite horizon information. In order to facilitate generalization in continuous state and action spaces, fuzzy inference system is used as a function approximator in conjunction with Q-learning. Empirical study on a continuous stirred tank reactor shows that the MFPC reinforcement learning framework is efficient, and strongly robust.}
}
@article{PAN2021107462,
title = {Constrained model-free reinforcement learning for process optimization},
journal = {Computers & Chemical Engineering},
volume = {154},
pages = {107462},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107462},
url = {https://www.sciencedirect.com/science/article/pii/S0098135421002404},
author = {Elton Pan and Panagiotis Petsagkourakis and Max Mowbray and Dongda Zhang and Ehecatl Antonio del Rio-Chanona},
keywords = {Machine learning, Batch optimization, Process control, Q-Learning, Dynamic systems, data-Driven optimization},
abstract = {Reinforcement learning (RL) is a control approach that can handle nonlinear stochastic optimal control problems. However, despite the promise exhibited, RL has yet to see marked translation to industrial practice primarily due to its inability to satisfy state constraints. In this work we aim to address this challenge. We propose an “oracle”-assisted constrained Q-learning algorithm that guarantees the satisfaction of joint chance constraints with a high probability, which is crucial for safety critical tasks. To achieve this, constraint tightening (backoffs) are introduced and adjusted using Broyden’s method, hence making the backoffs self-tuned. This results in a methodology that can be imbued into RL algorithms to ensure constraint satisfaction. We analyze the performance of the proposed approach and compare against nonlinear model predictive control (NMPC). The favorable performance of this algorithm signifies a step towards the incorporation of RL into real world optimization and control of engineering systems, where constraints are essential.}
}
@article{HAILEMICHAEL2022615,
title = {Safe Reinforcement Learning for an Energy-Efficient Driver Assistance System},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {37},
pages = {615-620},
year = {2022},
note = {2nd Modeling, Estimation and Control Conference MECC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.11.250},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322028932},
author = {Habtamu Hailemichael and Beshah Ayalew and Lindsey Kerbel and Andrej Ivanco and Keith Loiselle},
keywords = {RL driver-assist, Safe reinforcement learning, Safety filtering, Control barrier functions},
abstract = {Reinforcement learning (RL)-based driver assistance systems seek to improve fuel consumption via continual improvement of powertrain control actions considering experiential data from the field. However, the need to explore diverse experiences in order to learn optimal policies often limits the application of RL techniques in safety-critical systems like vehicle control. In this paper, an exponential control barrier function (ECBF) is derived and utilized to filter unsafe actions proposed by an RL-based driver assistance system. The RL agent freely explores and optimizes the performance objectives while unsafe actions are projected to the closest actions in the safe domain. The reward is structured so that driver's acceleration requests are met in a manner that boosts fuel economy and doesn't compromise comfort. The optimal gear and traction torque control actions that maximize the cumulative reward are computed via the Maximum a Posteriori Policy Optimization (MPO) algorithm configured for a hybrid action space. The proposed safe-RL scheme is trained and evaluated in car following scenarios where it is shown that it effectively avoids collision both during training and evaluation while delivering on the expected fuel economy improvements for the driver assistance system.}
}
@article{ZOU2021815,
title = {A reinforcement learning approach for dynamic multi-objective optimization},
journal = {Information Sciences},
volume = {546},
pages = {815-834},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.08.101},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520308677},
author = {Fei Zou and Gary G. Yen and Lixin Tang and Chunfeng Wang},
keywords = {Dynamic multi-objective optimization, Severity degree, Reinforcement learning, Prediction},
abstract = {Dynamic Multi-objective Optimization Problem (DMOP) is emerging in recent years as a major real-world optimization problem receiving considerable attention. Tracking the movement of Pareto front efficiently and effectively over time has been a central issue in solving DMOPs. In this paper, a reinforcement learning-based dynamic multi-objective evolutionary algorithm, called RL-DMOEA, which seamlessly integrates reinforcement learning framework and three change response mechanisms, is proposed for solving DMOPs. The proposed algorithm relocates the individuals based on the severity degree of environmental changes, which is estimated through the corresponding changes in the objective space of their decision variables. When identifying different severity degree of environmental changes, the proposed RL-DMOEA approach can learn better evolutionary behaviors from environment information, based on which apply the appropriate response mechanisms. Specifically, these change response mechanisms including the knee-based prediction, center-based prediction and indicator-based local search, are devised to promote both convergence and diversity of the algorithm under different severity of environmental changes. To verify this idea, the proposed RL-DMOEA is evaluated on CEC 2015 test problems involving various problem characteristics. Empirical studies on chosen state-of-the-art designs validate that the proposed RL-DMOEA is effective in addressing the DMOPs.}
}
@article{LI2023120291,
title = {Wind power forecasting considering data privacy protection: A federated deep reinforcement learning approach},
journal = {Applied Energy},
volume = {329},
pages = {120291},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120291},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922015483},
author = {Yang Li and Ruinong Wang and Yuanzheng Li and Meng Zhang and Chao Long},
keywords = {Wind power forecasting, Data openness and sharing, Privacy protection, Deep reinforcement learning, Federated learning, Uncertainty modeling},
abstract = {In a modern power system with an increasing proportion of renewable energy, wind power prediction is crucial to the arrangement of power grid dispatching plans due to the volatility of wind power. However, traditional centralized forecasting methods raise concerns regarding data privacy-preserving and data islands problem. To handle the data privacy and openness, we propose a forecasting scheme that combines federated learning and deep reinforcement learning (DRL) for ultra-short-term wind power forecasting, called federated deep reinforcement learning (FedDRL). Firstly, this paper uses the deep deterministic policy gradient (DDPG) algorithm as the basic forecasting model to improve prediction accuracy. Secondly, we integrate the DDPG forecasting model into the framework of federated learning. The designed FedDRL can obtain an accurate prediction model in a decentralized way by sharing model parameters instead of sharing private data which can avoid sensitive privacy issues. The simulation results show that the proposed FedDRL outperforms the traditional prediction methods in terms of forecasting accuracy. More importantly, while ensuring the forecasting performance, FedDRL can effectively protect the data privacy and relieve the communication pressure compared with the traditional centralized forecasting method. In addition, a simulation with different federated learning parameters is conducted to confirm the robustness of the proposed scheme.}
}
@article{QUEIROZDOSSANTOS20144939,
title = {Reactive Search strategies using Reinforcement Learning, local search algorithms and Variable Neighborhood Search},
journal = {Expert Systems with Applications},
volume = {41},
number = {10},
pages = {4939-4949},
year = {2014},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2014.01.040},
url = {https://www.sciencedirect.com/science/article/pii/S0957417414000645},
author = {João Paulo {Queiroz dos Santos} and Jorge Dantas {de Melo} and Adrião Dória {Duarte Neto} and Daniel Aloise},
keywords = {Reactive Search, Reinforcement Learning, Local search, Variable Neighborhood Search, Combinatorial optimization},
abstract = {Optimization techniques known as metaheuristics have been applied successfully to solve different problems, in which their development is characterized by the appropriate selection of parameters (values) for its execution. Where the adjustment of a parameter is required, this parameter will be tested until viable results are obtained. Normally, such adjustments are made by the developer deploying the metaheuristic. The quality of the results of a test instance [The term instance is used to refer to the assignment of values to the input variables of a problem.] will not be transferred to the instances that were not tested yet and its feedback may require a slow process of “trial and error” where the algorithm has to be adjusted for a specific application. Within this context of metaheuristics the Reactive Search emerged defending the integration of machine learning within heuristic searches for solving complex optimization problems. Based in the integration that the Reactive Search proposes between machine learning and metaheuristics, emerged the idea of putting Reinforcement Learning, more specifically the Q-learning algorithm with a reactive behavior, to select which local search is the most appropriate in a given time of a search, to succeed another local search that can not improve the current solution in the VNS metaheuristic. In this work we propose a reactive implementation using Reinforcement Learning for the self-tuning of the implemented algorithm, applied to the Symmetric Travelling Salesman Problem.}
}
@article{LI2023677,
title = {Predictive hierarchical reinforcement learning for path-efficient mapless navigation with moving target},
journal = {Neural Networks},
volume = {165},
pages = {677-688},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S089360802300309X},
author = {Hanxiao Li and Biao Luo and Wei Song and Chunhua Yang},
keywords = {Reinforcement learning, Deep learning, Navigation, Moving target},
abstract = {Deep reinforcement learning (DRL) has been proven as a powerful approach for robot navigation over the past few years. DRL-based navigation does not require the pre-construction of a map, instead, high-performance navigation skills can be learned from trial-and-error experiences. However, recent DRL-based approaches mostly focus on a fixed navigation target. It is noted that when navigating to a moving target without maps, the performance of the standard RL structure drops dramatically on both the success rate and path efficiency. To address the mapless navigation problem with moving target, the predictive hierarchical DRL (pH-DRL) framework is proposed by integrating the long-term trajectory prediction to provide a cost-effective solution. In the proposed framework, the lower-level policy of the RL agent learns robot control actions to a specified goal, and the higher-level policy learns to make long-range planning of shorter navigation routes by sufficiently exploiting the predicted trajectories. By means of making decisions over two level of policies, the pH-DRL framework is robust to the unavoidable errors in long-term predictions. With the application of deep deterministic policy gradient (DDPG) for policy optimization, the pH-DDPG algorithm is developed based on the pH-DRL structure. Finally, through comparative experiments on the Gazebo simulator with several variants of the DDPG algorithm, the results demonstrate that the pH-DDPG outperforms other algorithms and achieves a high success rate and efficiency even though the target moves fast and randomly.}
}
@article{ALIBEKOV2019224,
title = {Proxy Functions for Approximate Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {11},
pages = {224-229},
year = {2019},
note = {5th IFAC Conference on Intelligent Control and Automation Sciences ICONS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.09.145},
url = {https://www.sciencedirect.com/science/article/pii/S240589631930775X},
author = {Eduard Alibekov and Jiří Kubalík and Robert Babuška},
keywords = {reinforcement learning, continuous state space, optimal control, policy derivation, V-function},
abstract = {Approximate Reinforcement Learning (RL) is a method to solve sequential decisionmaking and dynamic control problems in an optimal way. This paper addresses RL for continuous state spaces which derive the control policy by using an approximate value function (V-function). The standard approach to derive a policy through the V-function is analogous to hill climbing: at each state the RL agent chooses the control input that maximizes the right-hand side of the Bellman equation. Although theoretically optimal, the actual control performance of this method is heavily influenced by the local smoothness of the V-function; a lack of smoothness results in undesired closed-loop behavior with input chattering or limit-cycles. To circumvent these problems, this paper provides a method based on Symbolic Regression to generate a locally smooth proxy to the V-function. The proposed method has been evaluated on two nonlinear control benchmarks: pendulum swing-up and magnetic manipulation. The new method has been compared with the standard policy derivation technique using the approximate V-function and the results show that the proposed approach outperforms the standard one with respect to the cumulative return.}
}
@article{ZHOU2022125187,
title = {Data-driven stochastic energy management of multi energy system using deep reinforcement learning},
journal = {Energy},
volume = {261},
pages = {125187},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.125187},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222020771},
author = {Yanting Zhou and Zhongjing Ma and Jinhui Zhang and Suli Zou},
keywords = {Carbon neutrality, Multi energy system, Renewable energy, Stochastic optimization, Deep reinforcement learning, Soft actor critic},
abstract = {The multi energy system (MES) is promising in the process of carbon neutrality, such that multi energy resources are utilized comprehensively to reduce the operation cost. Another way is to promote carbon neutrality by increasing the penetration of renewable energy. Hence, in this paper, we study the energy management of a typical MES under the challenges of stochastic renewable supplies and energy demands. To address the challenges, a stochastic optimization problem is established as a Markov decision process (MDP). An improved deep reinforcement learning (DRL) method is then developed to achieve the dynamic optimal energy dispatch. In particular, the comfort experience of users and complex coupling are both considered in the MES. In this framework, we propose an improved soft actor critic (SAC) algorithm based on maximum entropy to improve exploration ability, together with a long short-term memory (LSTM) network to extract temporal features efficiently. Meanwhile, we add the prioritized experience replay (PER) to increase the training efficiency to speed up the convergence of the algorithm. Finally, the case study demonstrates that the proposed algorithm can converge rapidly and greatly reduce the operation cost. In addition, the effectiveness and robustness of the improved method are verified.}
}
@article{BOMMISETTY2022100522,
title = {Resource Allocation in Time Slotted Channel Hopping (TSCH) Networks Based on Phasic Policy Gradient Reinforcement Learning},
journal = {Internet of Things},
volume = {19},
pages = {100522},
year = {2022},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2022.100522},
url = {https://www.sciencedirect.com/science/article/pii/S2542660522000257},
author = {Lokesh Bommisetty and T.G. Venkatesh},
keywords = {Industrial internet of things, IEEE 802.15.4e, Time slotted channel hopping, Deep reinforcement learning, Actor–critic policy gradient methods, Phasic policy gradient},
abstract = {The concept of the Industrial Internet of Things (IIoT) is gaining prominence due to its low-cost solutions and improved productivity of manufacturing processes. To address the ultra-high reliability and ultra-low power communication requirements of IIoT networks, Time Slotted Channel Hopping (TSCH) behavioral mode has been introduced in IEEE 802.15.4e standard. Scheduling the packet transmissions in IIoT networks is a difficult task owing to the limited resources and dynamic topology. In IEEE 802.15.4e TSCH, the design of the schedule is open to implementation. In this paper, we propose a phasic policy gradient (PPG) based TSCH schedule learning algorithm. We construct the utility function that accounts for the throughput, and energy efficiency of the TSCH network. The proposed PPG based scheduling algorithm overcomes the drawbacks of totally distributed and totally centralized deep reinforcement learning-based scheduling algorithms by employing the actor–critic policy gradient method that learns the scheduling algorithm in two phases, namely policy phase and auxiliary phase. In this method, we show that the schedule converges quickly compared to any other actor–critic method and also improves the system throughput performance by 58% compared to the minimal scheduling function, a default TSCH schedule.}
}
@article{HEIDARI2022119206,
title = {Reinforcement Learning for proactive operation of residential energy systems by learning stochastic occupant behavior and fluctuating solar energy: Balancing comfort, hygiene and energy use},
journal = {Applied Energy},
volume = {318},
pages = {119206},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119206},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922005712},
author = {Amirreza Heidari and François Maréchal and Dolaana Khovalyg},
keywords = {Reinforcement Learning, Space heating, Solar, Building, Control, Occupant behavior},
abstract = {When it comes to residential buildings, there are several stochastic parameters, such as renewable energy production, outdoor air conditions, and occupants’ behavior, that are hard to model and predict accurately, with some being unique in each specific building. This increases the complexity of developing a generalizable optimal control method that can be transferred to different buildings. Rather than hard-programming human knowledge into the controller (in terms of rules or models), a learning ability can be provided to the controller such that over the time it can learn by itself how to maintain an optimal operation in each specific building. This research proposes a model-free control framework based on Reinforcement Learning that takes into account the stochastic hot water use behavior of occupants, solar power generation, and weather conditions, and learns how to make a balance between the energy use, occupant comfort and water hygiene in a solar-assisted space heating and hot water production system. A stochastic-based offline training procedure is proposed to give a prior experience to the agent in a safe simulation environment, and further ensure occupants comfort and health when the algorithm starts online learning on the real house. To make a realistic assessment without interrupting the occupants, weather conditions and hot water use behavior are experimentally monitored in three case studies in different regions of Switzerland, and the collected data are used in simulations to evaluate the proposed control framework against two rule-based methods. Results indicate that the proposed framework could achieve an energy saving from 7% to 60%, mainly by adapting to solar power generation, without violating comfort or compromising the health of occupants.}
}
@article{LIU2022105060,
title = {Performance-based data-driven optimal tracking control of shape memory alloy actuated manipulator through reinforcement learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {114},
pages = {105060},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105060},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622002238},
author = {Hongshuai Liu and Qiang Cheng and Jichun Xiao and Lina Hao},
keywords = {SMA manipulator, Reinforcement learning, Prescribed constraints, Data-driven control},
abstract = {This article focuses on the continuous-time optimal tracking control problem of a shape memory alloy (SMA) actuated manipulator subject to prescribed error constraints and completely unknown nonlinear dynamics. Firstly, prespecified error constraints imposed by the prescribed performance function (PPF) are transformed into an equivalent unconstrained task by adopting a transformation function, where the newly designed PPF is irrelated to the initial condition. An unconstrained augmented system and a long-term discounted performance considering tracking errors and input cost are constructed for the convenience of designing the optimal controller. Then, a model-based optimal control algorithm is derived to solve the Hamilton–Jacobi–Bellman equation (HJBE). Next, data-driven reinforcement learning (RL) is employed to approximate the solution of the HJBE iteratively to obviate requiring an SMA manipulator model. Moreover, critic neural networks (NNs) and actor NNs are introduced to the RL-based optimal controller, and the least-squares method is used to find the parameters for the NNs-based RL algorithm. Rigorous theoretical analyses demonstrate that the proposed controller can stable the SMA manipulator system, and the tracking errors are always constrained within the prescribed region. Finally, experiments are conducted on the established SMA actuated manipulator platform, and the results illustrate that the proposed controller is feasible and effective.}
}
@article{YE2023106546,
title = {Application of a new type of lithium‑sulfur battery and reinforcement learning in plug-in hybrid electric vehicle energy management},
journal = {Journal of Energy Storage},
volume = {59},
pages = {106546},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2022.106546},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X2202535X},
author = {Yiming Ye and Jiangfeng Zhang and Srikanth Pilla and Apparao M. Rao and Bin Xu},
keywords = {Battery degradation, Li-S battery, Energy management strategy, Plug-in hybrid vehicle},
abstract = {The continuous increase in vehicle ownership has caused overall energy consumption to increase rapidly. Developing new energy vehicle technologies and improving energy utilization efficiency are significant in saving energy. Plug-in hybrid electric vehicles (PHEVs) present a practical solution to the arising energy shortage concerns. However, existing battery technologies restrict PHEV application as the most popular lithium-ion battery has a relatively high capital cost and degradation during service time. This paper studies the application of a new type of lithium‑sulfur (LiS) battery with bilateral solid electrolyte interphases in the PHEV. Compared with metals such as cobalt and nickel used in conventional lithium-ion batteries, sulfur utilized in LiS is cheaper and easier to manufacture. The high energy density of the new LiS battery also provides a longer range for PHEVs. In this paper, a PHEV propulsion system model is introduced, which includes vehicle dynamics, engine, electric motor, and LiS battery models. Dynamic programming is formulated as a benchmark energy management strategy to reduce energy consumption. Besides the offline global optimal benchmark from dynamic programming, the real-time performance of the LiS battery is evaluated by Q-learning and rule-based strategies. For a more comprehensive validation, both light-duty vehicles and heavy-duty vehicles are considered. Compared with lithium-ion batteries, the new LiS battery reduces the fuel consumption by up to 14.63 % and battery degradation by up to 82.37 %.}
}
@article{ZAMFIRACHE202299,
title = {Reinforcement Learning-based control using Q-learning and gravitational search algorithm with experimental validation on a nonlinear servo system},
journal = {Information Sciences},
volume = {583},
pages = {99-120},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.10.070},
url = {https://www.sciencedirect.com/science/article/pii/S002002552101094X},
author = {Iuliu Alexandru Zamfirache and Radu-Emil Precup and Raul-Cristian Roman and Emil M. Petriu},
keywords = {Gravitational search algorithm, NN training, Optimal reference tracking control, Q-learning, Reinforcement learning, Servo systems},
abstract = {This paper presents a novel Reinforcement Learning (RL)-based control approach that uses a combination of a Deep Q-Learning (DQL) algorithm and a metaheuristic Gravitational Search Algorithm (GSA). The GSA is employed to initialize the weights and the biases of the Neural Network (NN) involved in DQL in order to avoid the instability, which is the main drawback of the traditional randomly initialized NNs. The quality of a particular set of weights and biases is measured at each iteration of the GSA-based initialization using a fitness function aiming to achieve the predefined optimal control or learning objective. The data generated during the RL process is used in training a NN-based controller that will be able to autonomously achieve the optimal reference tracking control objective. The proposed approach is compared with other similar techniques which use different algorithms in the initialization step, namely the traditional random algorithm, the Grey Wolf Optimizer algorithm, and the Particle Swarm Optimization algorithm. The NN-based controllers based on each of these techniques are compared using performance indices specific to optimal control as settling time, rise time, peak time, overshoot, and minimum cost function value. Real-time experiments are conducted in order to validate and test the proposed new approach in the framework of the optimal reference tracking control of a nonlinear position servo system. The experimental results show the superiority of this approach versus the other three competing approaches.}
}
@article{JIANG20231,
title = {Deep reinforcement learning based multi-level dynamic reconfiguration for urban distribution network: A cloud-edge collaboration architecture},
journal = {Global Energy Interconnection},
volume = {6},
number = {1},
pages = {1-14},
year = {2023},
issn = {2096-5117},
doi = {https://doi.org/10.1016/j.gloei.2023.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096511723000105},
author = {Siyuan Jiang and Hongjun Gao and Xiaohui Wang and Junyong Liu and Kunyu Zuo},
keywords = {Cloud-edge collaboration architecture, Multi-agent deep reinforcement learning, Multi-level dynamic reconfiguration, Offline learning, Online learning},
abstract = {With the construction of the power Internet of Things (IoT), communication between smart devices in urban distribution networks has been gradually moving towards high speed, high compatibility, and low latency, which provides reliable support for reconfiguration optimization in urban distribution networks. Thus, this study proposed a deep reinforcement learning based multi-level dynamic reconfiguration method for urban distribution networks in a cloud-edge collaboration architecture to obtain a real-time optimal multi-level dynamic reconfiguration solution. First, the multi-level dynamic reconfiguration method was discussed, which included feeder-, transformer-, and substation-levels. Subsequently, the multi-agent system was combined with the cloud-edge collaboration architecture to build a deep reinforcement learning model for multi-level dynamic reconfiguration in an urban distribution network. The cloud-edge collaboration architecture can effectively support the multi-agent system to conduct “centralized training and decentralized execution” operation modes and improve the learning efficiency of the model. Thereafter, for a multi-agent system, this study adopted a combination of offline and online learning to endow the model with the ability to realize automatic optimization and updation of the strategy. In the offline learning phase, a Q-learning-based multi-agent conservative Q-learning (MACQL) algorithm was proposed to stabilize the learning results and reduce the risk of the next online learning phase. In the online learning phase, a multi- agent deep deterministic policy gradient (MADDPG) algorithm based on policy gradients was proposed to explore the action space and update the experience pool. Finally, the effectiveness of the proposed method was verified through a simulation analysis of a real-world 445-node system.}
}
@article{YANG2016731,
title = {Data-based robust adaptive control for a class of unknown nonlinear constrained-input systems via integral reinforcement learning},
journal = {Information Sciences},
volume = {369},
pages = {731-747},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.07.051},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516305382},
author = {Xiong Yang and Derong Liu and Biao Luo and Chao Li},
keywords = {Adaptive dynamic programming, Input constraint, Neural networks, Optimal control, Reinforcement learning, Robust control},
abstract = {This paper presents a data-based robust adaptive control methodology for a class of nonlinear constrained-input systems with completely unknown dynamics. By introducing a value function for the nominal system, the robust control problem is transformed into a constrained optimal control problem. Due to the unavailability of system dynamics, a data-based integral reinforcement learning (RL) algorithm is developed to solve the constrained optimal control problem. Based on the present algorithm, the value function and the control policy can be updated simultaneously using only system data. The convergence of the developed algorithm is proved via an established equivalence relationship. To implement the integral RL algorithm, an actor neural network (NN) and a critic NN are separately utilized to approximate the control policy and the value function, and the least squares method is employed to estimate the unknown parameters. By using Lyapunov’s direct method, the obtained approximate optimal control is verified to guarantee the unknown nonlinear system to be stable in the sense of uniform ultimate boundedness. Two examples are provided to demonstrate the effectiveness and applicability of the theoretical results.}
}
@article{ABOELENEEN2023107325,
title = {The role of Reinforcement Learning in software testing},
journal = {Information and Software Technology},
volume = {164},
pages = {107325},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107325},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923001805},
author = {Amr Abo-eleneen and Ahammed Palliyali and Cagatay Catal},
keywords = {Software testing, Machine learning, Reinforcement Learning, Artificial intelligence},
abstract = {Context:
Software testing is applied to validate the behavior of the software system and identify flaws and bugs. Different machine learning technique types such as supervised and unsupervised learning were utilized in software testing. However, for some complex software testing scenarios, neither supervised nor unsupervised machine learning techniques were adequate. As such, researchers applied Reinforcement Learning (RL) techniques in some cases. However, a systematic overview of the state-of-the-art on the role of reinforcement learning in software testing is lacking.
Objective:
The objective of this study is to determine how and to what extent RL was used in software testing.
Methods:
In this study, a Systematic Literature Review (SLR) was conducted on the use of RL in software testing, and 40 primary studies were investigated.
Results:
This study highlights different software testing types to which RL has been applied, commonly used RL algorithms and architecture for learning, challenges faced, advantages and disadvantages of using RL, and the performance comparison of RL-based models against other techniques.
Conclusions:
RL has been widely used in software testing but has almost narrowed to two applications. There is a shortage of papers using advanced RL techniques in addition to multi-agent RL. Several challenges were presented in this study.}
}
@article{YANG2023132,
title = {Reinforcement learning for robust stabilization of nonlinear systems with asymmetric saturating actuators},
journal = {Neural Networks},
volume = {158},
pages = {132-141},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022004531},
author = {Xiong Yang and Yingjiang Zhou and Zhongke Gao},
keywords = {Adaptive dynamic programming, Neural network control, Robust stabilization, Reinforcement learning, Saturating actuator},
abstract = {We study the robust stabilization problem of a class of nonlinear systems with asymmetric saturating actuators and mismatched disturbances. Initially, we convert such a robust stabilization problem into a nonlinear-constrained optimal control problem by constructing a discounted cost function for the auxiliary system. Then, for the purpose of solving the nonlinear-constrained optimal control problem, we develop a simultaneous policy iteration (PI) in the reinforcement learning framework. The implementation of the simultaneous PI relies on an actor–critic architecture, which employs actor and critic neural networks (NNs) to separately approximate the control policy and the value function. To determine the actor and critic NNs’ weights, we use the approach of weighted residuals together with the typical Monte-Carlo integration technique. Finally, we perform simulations of two nonlinear plants to validate the established theoretical claims.}
}
@article{MAZOUCHI20208070,
title = {Data-driven dynamic multi-objective optimal control: A Hamiltonian-inequality driven satisficing reinforcement learning approach},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {8070-8075},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2275},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320329359},
author = {Majid Mazouchi and Yongliang Yang and Hamidreza Modares},
keywords = {Multi-objective optimization, Pareto optimality, Reinforcement learning, Sum-of-Square theory},
abstract = {This paper presents an iterative data-driven algorithm for solving dynamic multi-objective (MO) optimal control problems arising in control of nonlinear continuous-time systems with multiple objectives. It is first shown that the Hamiltonian function corresponding to each objective can serve as a comparison function to compare the performance of admissible policies. Relaxed Hamilton-Jacobi-bellman (HJB) equations in terms of HJB inequalities are then solved in a dynamic constrained MO framework to find Pareto-optimal solutions. Relation to satisficing (good enough) decision-making framework is shown. A Sum-of-Square (SOS)-based iterative algorithm is developed to solve the formulated MO optimization with HJB inequalities. To obviate the requirement of complete knowledge of the system dynamics, a data-driven satisficing reinforcement learning approach is proposed to solve the SOS optimization problem in real-time using only the information of the system trajectories measured during a time interval without having full knowledge of the system dynamics. Finally, a simulation example is provided to show the effectiveness of the proposed algorithm.}
}
@article{DUTTA2023108386,
title = {A reinforcement learning-based transformed inverse model strategy for nonlinear process control},
journal = {Computers & Chemical Engineering},
volume = {178},
pages = {108386},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108386},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423002569},
author = {Debaprasad Dutta and Simant R. Upreti},
keywords = {Reinforcement learning, Process control, Inverse model control},
abstract = {Process control is evolving with a rapid inclusion of artificial intelligence (AI) based methods, which have the potential to significantly boost controller performances. This development is valuable to control engineers, especially for the control of nonlinear processes plagued by higher offsets and larger delays. With this motivation, we develop in this work a new strategy utilizing AI for process control by assimilating reinforcement learning (RL) with a novel approach for inverse model control. This approach involves expressing a dynamic process model in the controlled variable domain to obtain a computationally efficient, transformed inverse model (TIM). In the developed control strategy, the TIM provides baseline control, which is subsequently improved by RL’s deep deterministic policy gradient method in the resultant TIM-RL controller. This controller is examined in three different in-silico case studies of varying nonlinearity and complexity. Relative to nonlinear model predictive control, the TIM-RL controller is observed to have superior performance with more than 30% reduction in integral absolute error, 45% lower settling time, and about an order of magnitude smaller latency.}
}
@article{DENG2021110860,
title = {Reinforcement learning of occupant behavior model for cross-building transfer learning to various HVAC control systems},
journal = {Energy and Buildings},
volume = {238},
pages = {110860},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.110860},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821001444},
author = {Zhipeng Deng and Qingyan Chen},
keywords = {Thermal comfort, Machine learning, Artificial neural network, Air temperature, Thermostat set point, Q-learning, Building performance simulation},
abstract = {Occupant behavior plays an important role in the evaluation of building performance. However, many contextual factors, such as occupancy, mechanical system and interior design, have a significant impact on occupant behavior. Most previous studies have built data-driven behavior models, which have limited scalability and generalization capability. Our investigation built a policy-based reinforcement learning (RL) model for the behavior of adjusting the thermostat and clothing level. Occupant behavior was modelled as a Markov decision process (MDP). The action and state space in the MDP contained occupant behavior and various impact parameters. The goal of the occupant behavior was a more comfortable environment, and we modelled the reward for the adjustment action as the absolute difference in the thermal sensation vote (TSV) before and after the action. We used Q-learning to train the RL model in MATLAB and validated the model with collected data. After training, the model predicted the behavior of adjusting the thermostat set point with R2 from 0.75 to 0.8, and the mean absolute error (MAE) was less than 1.1 °C (2 °F) in an office building. This study also transferred the behavior knowledge of the RL model to other office buildings with different HVAC control systems. The transfer learning model predicted the occupant behavior with R2 from 0.73 to 0.8, and the MAE was less than 1.1 °C (2 °F) most of the time. Going from office buildings to residential buildings, the transfer learning model also had an R2 over 0.6. Therefore, the RL model combined with transfer learning was able to predict the building occupant behavior accurately with good scalability, and without the need for data collection.}
}
@article{CAO2023109894,
title = {Reinforcement learning based tasks offloading in vehicular edge computing networks},
journal = {Computer Networks},
volume = {234},
pages = {109894},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109894},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623003390},
author = {Shaohua Cao and Di Liu and Congcong Dai and Chengqi Wang and Yansheng Yang and Weishan Zhang and Danyang Zheng},
keywords = {Mobile edge computing, Reinforcement learning, Task offloading, Fuzzy inference, Internet of vehicles},
abstract = {With the rapid development of autonomous and intelligent techniques, vehicles are currently equipped with computation and communication modules for satisfying clients’ on-vehicle computing requests. To meet the client’s on-vehicle computation requests such as on-vehicle games and self-driving mechanisms, vehicles have to continuously generate computational tasks. However, due to the limited on-vehicle computation capacities, it is barely possible to handle the above requests by the vehicle itself. These requests are then offloaded to special devices such as roadside units or intelligent vehicles. With the fluid feature of the traffic, more requests are generated during the peak hours than at the low hours. Based on the above facts, two significant challenges arise in vehicular edge computing networks: (i) how to accurately determine whether the vehicular networks are in peak or low hours, and (ii) how to effectively offload the generated requests? In this paper, to tackle the above challenges, we investigate the problem of computational requests offloading under different vehicular networking scenarios. To handle the first challenge, we propose the fuzzy inference-based algorithm to identify the situation of the vehicular network (i.e., whether it is in peak hours or low hours). We employ the reinforcement learning-based algorithm for the second challenge to offload the computational requests effectively. Experiments show that our schemes outperform the benchmark by an average of 24.8% regarding resource utilization when satisfying the interests of both service providers and clients.}
}
@article{LIU2021207,
title = {Self-play reinforcement learning with comprehensive critic in computer games},
journal = {Neurocomputing},
volume = {449},
pages = {207-213},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221005245},
author = {Shanqi Liu and Junjie Cao and Yujie Wang and Wenzhou Chen and Yong Liu},
keywords = {Reinforcement learning, Self-play, Computer game},
abstract = {Self-play reinforcement learning, where agents learn by playing with themselves, has been successfully applied in many game scenarios. However, the training procedure for self-play reinforcement learning is unstable and more sample-inefficient than (general) reinforcement learning, especially in imperfect information games. To improve the self-play training process, we incorporate a comprehensive critic into the policy gradient method to form a self-play actor-critic (SPAC) method for training agents to play computer games. We evaluate our method in four different environments in both competitive and cooperative tasks. The results show that the agent trained with our SPAC method outperforms those trained with deep deterministic policy gradient (DDPG) and proximal policy optimization (PPO) algorithms in many different evaluation approaches, which vindicate the effect of our comprehensive critic in the self-play training procedure.}
}
@article{KAUR2022105498,
title = {CADxReport: Chest x-ray report generation using co-attention mechanism and reinforcement learning},
journal = {Computers in Biology and Medicine},
volume = {145},
pages = {105498},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105498},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522002906},
author = {Navdeep Kaur and Ajay Mittal},
keywords = {Chest x-ray reports, Convolution neural network, Co-attention, Deep neural network, Hierarchical LSTM, Reinforcement learning, Textual description},
abstract = {Background
Automated generation of radiological reports for different imaging modalities is essentially required to smoothen the clinical workflow and alleviate radiologists’ workload. It involves the careful amalgamation of image processing techniques for medical image interpretation and language generation techniques for report generation. This paper presents CADxReport, a coattention and reinforcement learning based technique for generating clinically accurate reports from chest x-ray (CXR) images.
Method
CADxReport, uses VGG19 network pre-trained over ImageNet dataset and a multi-label classifier for extracting visual and semantic features from CXR images, respectively. The co-attention mechanism with both the features is used to generate a context vector, which is then passed to HLSTM for radiological report generation. The model is trained using reinforcement learning to maximize CIDEr rewards. OpenI dataset, having 7, 470 CXRs along with 3, 955 associated structured radiological reports, is used for training and testing.
Results
Our proposed model is able to generate clinically accurate reports from CXR images. The quantitative evaluations confirm satisfactory results by achieving the following performance scores: BLEU-1 = 0.577, BLEU-2 = 0.478, BLEU-3 = 0.403, BLEU-4 = 0.346, ROUGE = 0.618 and CIDEr = 0.380.
Conclusions
The evaluation using BLEU, ROUGE, and CIDEr score metrics indicates that the proposed model generates sufficiently accurate CXR reports and outperforms most of the state-of-the-art methods for the given task.}
}
@article{LI2023377,
title = {Reinforcement learning based proportional–integral–derivative controllers design for consensus of multi-agent systems},
journal = {ISA Transactions},
volume = {132},
pages = {377-386},
year = {2023},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2022.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S0019057822003354},
author = {Jinna Li and Jiaqi Wang},
keywords = {Reinforcement learning, PID control, Consensus control, Multi-agent systems, Nonzero-sum game, Neural networks},
abstract = {This paper develops a novel Proportional–Integral–Derivative (PID) tuning method for multi-agent systems with a reinforced self-learning capability for achieving the optimal consensus of all agents. Unlike the traditional model-based and data-driven PID tuning methods, the developed PID self-learning method updates the controller parameters by actively interacting with unknown environment, with the outcomes of guaranteed consensus and performance optimization of agents. Firstly, the PID control-based consensus problem of multi-agent systems is formulated. Then, finding the PID gains is converted into solving a nonzero-sum game problem, thus an off-policy Q-learning algorithm with the critic-only structure is proposed to update the PID gains using only data, without the knowledge of dynamics of agents. Finally, simulations are given to verify the effectiveness of the proposed method.}
}
@article{ZHAO2020167,
title = {Neural networks-based optimal tracking control for nonzero-sum games of multi-player continuous-time nonlinear systems via reinforcement learning},
journal = {Neurocomputing},
volume = {412},
pages = {167-176},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.06.083},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220310717},
author = {Jingang Zhao},
keywords = {Neural networks, Multi-player nonzero-sum game, Optimal tracking control, Continuous-time nonlinear systems, Coupled Hamilton–Jacobi equations, Reinforcement learning},
abstract = {In this paper, optimal tracking control for nonzero-sum games of multi-player continuous-time nonlinear systems is investigated by using a novel reinforcement learning scheme. Based on the multi-player nonlinear systems and reference signal, we firstly formulate the tracking problem by constructing an augmented multi-player nonlinear systems. The optimal tracking control problem for nonzero-sum games of original multi-player nonlinear systems is thus transformed into solving the coupled Hamilton–Jacobi equations of the augmented multi-player nonlinear systems. The novel neural networks (NNs) – based online reinforcement learning (RL) method can learn the solution to coupled Hamilton–Jacobi equations in a forward-in-time manner without requiring any value, policy iterations. In order to relax the dependence of the traditional reinforcement learning method on Persistence of Excitation (PE) conditions, historical data from a period of time has been collected to design NNs tuning laws. The drift dynamic of the augmented system is not required in our scheme. The Uniformly Ultimately Boundedness (UUB) of NNs weight errors and closed-loop augmented system states are rigorous proved. Numerical simulation examples are given to demonstrate the effectiveness of our proposed scheme.}
}
@article{TIAN2021110643,
title = {Reinforcement learning approach for robustness analysis of complex networks with incomplete information},
journal = {Chaos, Solitons & Fractals},
volume = {144},
pages = {110643},
year = {2021},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2020.110643},
url = {https://www.sciencedirect.com/science/article/pii/S0960077920310341},
author = {Meng Tian and Zhengcheng Dong and Xianpei Wang},
keywords = {Robustness, Complex networks, Sequential attacks, Incomplete information, Reinforcement learning, Deep learning},
abstract = {Network robustness against sequential attacks is significant for complex networks. However, it is generally assumed that complete information of complex networks is obtained and arbitrary nodes can be removed in previous researches. In this paper, a sequential attack in complex networks is modeled as a partial observable Markov decision process (POMDP). Then a reinforcement learning (RL) approach for POMDP is proposed to analyze dynamical robustness of complex networks under sequential attacks, when information of networks is incomplete. According to this approach, an agent can learn to take action by exploiting experiences. To solve the problem of large state space in complex networks, deep Q-network algorithm is used to identify most damaging sequential attacks, as deep neural networks can build up progressively abstract representations of state space of complex networks. The performances of proposed approach are analyzed on scale-free networks and small-world networks. According to the numerical simulations, it is found that the RL-based sequential attacks perform better when load distributions are more heterogeneous and local connections are more significant. Furthermore, it is shown that increasing the proportions of observed and attacked nodes improves the performance of RL-based sequential attacks. Finally, the results are verified on the IEEE 300-bus system and the simulation results highlight the damages caused by RL-based sequential attacks.}
}
@article{SHIN2019556,
title = {Multi-timescale, multi-period decision-making model development by combining reinforcement learning and mathematical programming},
journal = {Computers & Chemical Engineering},
volume = {121},
pages = {556-573},
year = {2019},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2018.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S0098135418308913},
author = {Joohyun Shin and Jay H. Lee},
keywords = {Multi-timescale decision making, Decision under uncertainty, Markov decision process, Mathematical programming, Reinforcement learning},
abstract = {This study focuses on the linkage between decision layers that have different time scales. The resulting expansion of the boundary of decision-making process can provide more robust and flexible management and operation strategies by resolving inconsistencies between different levels. For this, we develop a multi-timescale decision-making model that combines Markov decision process (MDP) and mathematical programming (MP) in a complementary way and introduce a computationally tractable solution algorithm based on reinforcement learning (RL) to solve the MP-embedded MDP problem. To support the integration of the decision hierarchy, a data-driven uncertainty prediction model is suggested which is valid across all time scales considered. A practical example of refinery procurement and production planning is presented to illustrate the proposed method, along with numerical results of a benchmark case study.}
}
@article{QIAO2024121252,
title = {Distributed dynamic pricing of multiple perishable products using multi-agent reinforcement learning},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121252},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121252},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423017542},
author = {Wenchuan Qiao and Min Huang and Zheming Gao and Xingwei Wang},
keywords = {Curse of dimensionality, Fully cooperative Markov game, Credit assignment, Reward shaping},
abstract = {Revenue management (RM) is essential for a wide range of industries such as airlines, hotels, cruise lines, fashion, and seasonal retail. This paper focuses on the multi-perishable-product dynamic pricing (MPPDP) problem, a significant research field in RM, where a company sells multiple interactive and perishable products over a limited selling window without replenishment. Most studies in this field assume customer behavior, which is modeled by demand function, is known in advance. Even when considering uncertainty in customer behavior, most studies still assume the mathematical form or structural properties of the underlying demand function are known in advance. However, these assumptions are usually inconsistent with the actual market situation. Recently, Reinforcement Learning (RL), a potent technique for handling sequential decision-making problems, has been increasingly applied to solve complex dynamic pricing problems without relying on any assumption about demand functions. However, the curse of dimensionality poses a challenge for currently used centralized RL algorithms when solving the MPPDP problem due to the exponential expansion of the joint price space with the number of products. To address this issue, our paper proposes a distributed dynamic pricing framework and innovatively models the MPPDP problem as a Fully Cooperative Markov Game solved by Multi-Agent Reinforcement Learning (MARL). Additionally, we use counterfactual baselines to design appropriate agent-specific reward signals that facilitate faster learning for the agents in our established multi-agent cooperative system. Finally, two MARL-based distributed dynamic pricing algorithms, Counterfactual Q-learning, and Counterfactual DQN, are proposed for the MPPDP problem. Through the case studies on four computer-simulated markets, we show that our algorithms can alleviate the curse of dimensionality faced by centralized RL algorithms, expedite the learning process, and demonstrate satisfactory performance without relying on any assumption about demand functions. In conclusion, our work provides an effective MARL-based distributed dynamic pricing framework and algorithms for companies to efficiently price their multiple perishable products in modern highly uncertain markets.}
}
@article{BARTO1994888,
title = {Reinforcement learning control},
journal = {Current Opinion in Neurobiology},
volume = {4},
number = {6},
pages = {888-893},
year = {1994},
issn = {0959-4388},
doi = {https://doi.org/10.1016/0959-4388(94)90138-4},
url = {https://www.sciencedirect.com/science/article/pii/0959438894901384},
author = {Andrew G. Barto},
abstract = {Reinforcement learning refers to improving performance through trial-and-error. Despite recent progress in developing artificial learning systems, including new learning methods for artificial neural networks, most of these systems learn under the tutelage of a knowledgeable ‘teacher’ able to tell them how to respond to a set of training stimuli. Learning under these conditions is not adequate, however, when it is costly, or even impossible, to obtain this kind of training information. Reinforcement learning is attracting increasing attention in computer science and engineering because it can be used by autonomous systems to learn from their experiences instead of from knowledgeable teachers, and it is attracting attention in computational neuroscience because it is consonant with biological principles. Recent research has improved the efficiency of reinforcement learning and has provided some striking examples of its capabilities.}
}
@article{ZOU2022110363,
title = {Optimization of the electricity generation of a wave energy converter using deep reinforcement learning},
journal = {Ocean Engineering},
volume = {244},
pages = {110363},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.110363},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821016590},
author = {Shangyan Zou and Xiang Zhou and Irfan Khan and Wayne W. Weaver and Syed Rahman},
keywords = {Wave energy conversion, Direct-drive power take-off, Deep reinforcement learning, Deep Q-Network, PacWave ocean conditions},
abstract = {Ocean wave energy is one of the sustainable energy sources which continues to attract increasing research interests. Traditionally, the model-based controls of Wave Energy Converters (WECs) are derived from the linear hydrodynamics. Although showing promising performance, in practice, the actual electricity produced is significantly lower than predicted. On the other hand, it is extremely cumbersome to derive a model-based control from very complex dynamics. In this paper, a Deep Reinforcement Learning (DRL) control is proposed which is model-free and therefore makes it possible to be designed from a global point of view. To validate the performance of the control, a point absorber WEC with a direct-drive power take-off (PTO) unit is simulated. The results show the proposed DRL control outperforms the model-based controls in terms of wave power production (improved from 24% up to 152%). Furthermore, the results show the DRL control achieves the best power quality in terms of the operation efficiency and power variation (improved from 23% up to 84%). Finally, the performance of different controls is further validated with real ocean conditions (at PacWave) and the DRL can consistently shows the best performance in terms of both power production and power quality.}
}
@article{CHEN2021107788,
title = {Sentiment-influenced trading system based on multimodal deep reinforcement learning},
journal = {Applied Soft Computing},
volume = {112},
pages = {107788},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107788},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621007092},
author = {Yu-Fu Chen and Szu-Hao Huang},
keywords = {Finance, Multimodal learning, Reinforcement learning, Sentiment analysis, Stock trading},
abstract = {Owing to advancements in deep learning, studies involving the use of deep learning techniques to solve investment decision-making problems are increasing. However, although numerous aspects of the stock market may affect trends in financial data, previous studies have only considered price fluctuations. Therefore, investors may lose out on profits because of the complicated financial market condition. In this study, a multimodal reinforcement trading system is developed, which makes use of three techniques: reinforcement learning, sentiment analysis, and multimodal learning. The agent considers not only the price fluctuations but also news information when making a trading decision. Multimodal learning which can merge different modalities of data to enhance the performance of the model, and sentiment analysis for understanding the sentiment of news are introduced. In addition, an influence model is proposed to enable our agents to gain special insights on the impact that news has on the market. The influence model considers the relationship between sentiment of news and time. The experimental results show that multimodal agents outperform price-concerned agents by at least 13.26%. Our experimental results also indicate that the proposed influence model has the ability to shape the impact of news on the stock market. The model can aid the multimodal agents in evaluating the status of the market. The proposed multimodal reinforcement trading system is demonstrated to be robust in an experiment involving different sectors and evaluations by using various measures. In addition, because the data used are public, investors seeking to profit can easily implement the results of this paper. Therefore, it can be used in advanced research and financial applications.}
}
@article{LEI2022117796,
title = {A multi-action deep reinforcement learning framework for flexible Job-shop scheduling problem},
journal = {Expert Systems with Applications},
volume = {205},
pages = {117796},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117796},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422010624},
author = {Kun Lei and Peng Guo and Wenchao Zhao and Yi Wang and Linmao Qian and Xiangyin Meng and Liansheng Tang},
keywords = {Flexible job-shop scheduling problem, Multi-action deep reinforcement learning, Graph neural network, Markov decision process, Multi-proximal policy optimization},
abstract = {This paper presents an end-to-end deep reinforcement framework to automatically learn a policy for solving a flexible Job-shop scheduling problem (FJSP) using a graph neural network. In the FJSP environment, the reinforcement agent needs to schedule an operation belonging to a job on an eligible machine among a set of compatible machines at each timestep. This means that an agent needs to control multiple actions simultaneously. Such a problem with multi-actions is formulated as a multiple Markov decision process (MMDP). For solving the MMDPs, we propose a multi-pointer graph networks (MPGN) architecture and a training algorithm called multi-Proximal Policy Optimization (multi-PPO) to learn two sub-policies, including a job operation action policy and a machine action policy to assign a job operation to a machine. The MPGN architecture consists of two encoder-decoder components, which define the job operation action policy and the machine action policy for predicting probability distributions over different operations and machines, respectively. We introduce a disjunctive graph representation of FJSP and use a graph neural network to embed the local state encountered during scheduling. The computational experiment results show that the agent can learn a high-quality dispatching policy and outperforms handcrafted heuristic dispatching rules in solution quality and meta-heuristic algorithm in running time. Moreover, the results achieved on random and benchmark instances demonstrate that the learned policies have a good generalization performance on real-world instances and significantly larger scale instances with up to 2000 operations.}
}
@article{HIMANSHU2022281,
title = {Waypoint Navigation of Quadrotor using Deep Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {22},
pages = {281-286},
year = {2022},
note = {22nd IFAC Symposium on Automatic Control in Aerospace ACA 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.03.047},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323003051},
author = {K. Himanshu and Hari kumar and Jinraj V Pushpangathan},
keywords = {Reinforcement learning control, Tracking, Navigation, UAVs, Intelligent robotics},
abstract = {This paper proposes a Reinforcement Learning (RL) based technique to develop a simple neural network controller for the task of waypoint navigation in quadrotors. In this paper, the application of Twin Delayed Deep Deterministic (TD3) Policy Gradient algorithm for high and low-level control implementation for quadrotors is discussed. The proposed methods are tested on high fidelity Gym-Pybullet-Drones simulator. The effectiveness of the methods developed is validated through numerical simulations. The simulation results indicate that both control policies are successful in navigating through the assigned waypoint, with the low-level controller being accurate in the nominal flight conditions. In the presence of disturbance inputs, the high-level controller performs better when compared to the low-level controller.}
}
@article{SOARES2020109608,
title = {Using reinforcement learning for maximizing residential self-consumption – Results from a field test},
journal = {Energy and Buildings},
volume = {207},
pages = {109608},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2019.109608},
url = {https://www.sciencedirect.com/science/article/pii/S037877881930934X},
author = {Ana Soares and Davy Geysen and Fred Spiessens and Dominic Ectors and Oscar {De Somer} and Koen Vanthournout},
keywords = {Reinforcement learning, Q-Learning, Field test, Solar PV generation, Thermal storage, Thermostatically controlled loads, Electrical storage, Battery, Residential loads},
abstract = {This paper presents the results from a real residential field test in which one of the objectives was to maximize the instantaneous self-consumption of the local photovoltaic production. The field test was part of the REnnovates project and was conducted in different phases on houses in several residential districts located in Soesterberg, Heerhugowaard, Woerden and Soest, the Netherlands. To maximize self-consumption, buffered heat pump installations for domestic hot water and stationary residential battery systems were chosen due to their respective thermal and electrical storage capacities. The algorithm used to tackle the associated sequential decision-making problem was model-based reinforcement learning. The proposed algorithm learns the stochastic occupant behavior, uses predictions of local photovoltaic production and considers the dynamics of the system. The results show that this algorithm increased the average self-consumption percentage of the local PV generation (used instantaneously in situ) on average by 14%, even if only buffered heat pump installations for domestic hot water were used. This increase was achieved without causing any perceived discomfort to the residential end users. The average energy shifted per day from the solar production period to the night by the 2 kW/3.6 kWh batteries was 1.5 kWh. The main contribution of this work was therefore the real field implementation of the proposed algorithm. The results demonstrate that it is possible to improve even further the integration of local production using flexible loads.}
}
@article{CAARLS20208063,
title = {Deep Reinforcement Learning with embedded LQR Controllers⁎⁎This research was partially supported by the Brazilian National Council for Scientific and Technological Development (CNPq), grant number 304980/2018-8.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {8063-8069},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2261},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320329219},
author = {Wouter Caarls},
keywords = {Reinforcement learning, actor-critic methods, learning control, linear quadratic regulator},
abstract = {Reinforcement learning is a model-free optimal control method that optimizes a control policy through direct interaction with the environment. For reaching tasks that end in regulation, popular discrete-action methods are not well suited due to chattering in the goal state. We compare three different ways to solve this problem through combining reinforcement learning with classical LQR control. In particular, we introduce a method that integrates LQR control into the action set, allowing generalization and avoiding fixing the computed control in the replay memory if it is based on learned dynamics. We also embed LQR control into a continuous-action method. In all cases, we show that adding LQR control can improve performance, although the effect is more profound if it can be used to augment a discrete action set.}
}
@article{CUNHA202299,
title = {Reducing fuel consumption in platooning systems through reinforcement learning},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {15},
pages = {99-104},
year = {2022},
note = {6th IFAC Conference on Intelligent Control and Automation Sciences ICONS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.07.615},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322010266},
author = {Rafael F. Cunha and Tiago R. Gonçalves and Vineeth S. Varma and Salah E. Elayoubi and Ming Cao},
keywords = {Vehicle platoons, reinforcement learning, adaptive cruise control (ACC)},
abstract = {Fuel efficiency in platooning systems is a central topic of interest because of its significant economic and environmental impact on the transportation industry. In platoon systems, Adaptive Cruise Control (ACC) is widely adopted because it can guarantee string stability while requiring only radar or lidar measurements. A key parameter in ACC is the desired time gap between the platoon's neighboring vehicles. A small time gap results in a short inter-vehicular distance, which is fuel efficient when the vehicles are moving at constant speeds due to air drag reductions. On the other hand, when the vehicles accelerate and brake a lot, a bigger time gap is more fuel efficient. This motivates us to find a policy that minimizes fuel consumption by conveniently switching between two desired time gap parameters. Thus, one can interpret this formulation as a dynamic system controlled by a switching ACC, and the learning problem reduces to finding a switching rule that is fuel efficient. We apply a Reinforcement Learning (RL) algorithm to find a time switching policy between two desired time gap parameters of an ACC controller to reach our goal. We adopt the proximal policy optimization (PPO) algorithm to learn the appropriate transient shift times that minimize the platoon's fuel consumption when it faces stochastic traffic conditions. Numerical simulations show that the PPO algorithm outperforms both static time gap ACC and a threshold-based switching control in terms of the average fuel efficiency}
}
@article{CHEN2023110335,
title = {Deep reinforcement learning in recommender systems: A survey and new perspectives},
journal = {Knowledge-Based Systems},
volume = {264},
pages = {110335},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110335},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123000850},
author = {Xiaocong Chen and Lina Yao and Julian McAuley and Guanglin Zhou and Xianzhi Wang},
keywords = {Deep reinforcement learning, Deep learning, Recommender systems},
abstract = {In light of the emergence of deep reinforcement learning (DRL) in recommender systems research and several fruitful results in recent years, this survey aims to provide a timely and comprehensive overview of recent trends of deep reinforcement learning in recommender systems. We start by motivating the application of DRL in recommender systems, followed by a taxonomy of current DRL-based recommender systems and a summary of existing methods. We discuss emerging topics, open issues, and provide our perspective on advancing the domain. The survey serves as introductory material for readers from academia and industry to the topic and identifies notable opportunities for further research.}
}
@article{STAFYLOPATIS1998306,
title = {Autonomous vehicle navigation using evolutionary reinforcement learning},
journal = {European Journal of Operational Research},
volume = {108},
number = {2},
pages = {306-318},
year = {1998},
issn = {0377-2217},
doi = {https://doi.org/10.1016/S0377-2217(97)00372-X},
url = {https://www.sciencedirect.com/science/article/pii/S037722179700372X},
author = {A. Stafylopatis and K. Blekas},
keywords = {Learning classifier systems, Genetic algorithms, Reinforcement learning, Autonomous navigation},
abstract = {Reinforcement learning schemes perform direct on-line search in control space. This makes them appropriate for modifying control rules to obtain improvements in the performance of a system. The effectiveness of a reinforcement learning strategy is studied here through the training of a learning classifier system (LCS) that controls the movement of an autonomous vehicle in simulated paths including left and right turns. The LCS comprises a set of condition-action rules (classifiers) that compete to control the system and evolve by means of a genetic algorithm (GA). Evolution and operation of classifiers depend upon an appropriate credit assignment mechanism based on reinforcement learning. Different design options and the role of various parameters have been investigated experimentally. The performance of vehicle movement under the proposed evolutionary approach is superior compared with that of other (neural) approaches based on reinforcement learning that have been applied previously to the same benchmark problem.}
}
@article{QU2020114030,
title = {Jointly dampening traffic oscillations and improving energy consumption with electric, connected and automated vehicles: A reinforcement learning based approach},
journal = {Applied Energy},
volume = {257},
pages = {114030},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.114030},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919317179},
author = {Xiaobo Qu and Yang Yu and Mofan Zhou and Chin-Teng Lin and Xiangyu Wang},
keywords = {Electric vehicles, Connected and automated vehicles, Car following, Machine learning, Reinforcement learning, Deep Deterministic Policy Gradient, Traffic oscillations, Energy consumption},
abstract = {It has been well recognized that human driver’s limits, heterogeneity, and selfishness substantially compromise the performance of our urban transport systems. In recent years, in order to deal with these deficiencies, our urban transport systems have been transforming with the blossom of key vehicle technology innovations, most notably, connected and automated vehicles. In this paper, we develop a car following model for electric, connected and automated vehicles based on reinforcement learning with the aim to dampen traffic oscillations (stop-and-go traffic waves) caused by human drivers and improve electric energy consumption. Compared to classical modelling approaches, the proposed reinforcement learning based model significantly reduces the modelling constraints and has the capability of self-learning and self-correction. Experiment results demonstrate that the proposed model is able to improve travel efficiency by reducing the negative impact of traffic oscillations, and it can also reduce the average electric energy consumption.}
}
@article{FANG2021103163,
title = {Multi-agent Deep Reinforcement Learning for Distributed Energy Management and Strategy Optimization of Microgrid Market},
journal = {Sustainable Cities and Society},
volume = {74},
pages = {103163},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103163},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721004443},
author = {Xiaohan Fang and Qiang Zhao and Jinkuan Wang and Yinghua Han and Yuchun Li},
keywords = {Microgrid, Double auction, Strategy optimization, Multi-agent reinforcement learning, Deep Q-network, Optimal equilibrium},
abstract = {With the increasing demands for autonomous decision, market initiative, and privacy protection, a distributed energy management and strategy-making framework is more appropriate for the future microgrid. Besides, the integration of various distributed generations (DGs) and load facilities with high randomness poses challenges to traditional model-free microgrid market management approaches. Therefore, this paper concentrates on the distributed energy management and strategy optimization for a regional microgrid from the following aspects. First, a multi-agent reinforcement learning (MARL) framework is applied to generate independent real-time market decisions and to keep the benefits balance during agents’ interactive learning. Moreover, to solve the defects of traditional RL in the microgrid case of continuous state inputs, a multi-agent deep Q-network (MADQN) is adopted as the approximation of value function. Finally, an optimal equilibrium selection (OES) mechanism is proposed to calculate the collective update objective for promoting learning efficiency and to ensure benefits equilibrium. Combining theoretical analysis and simulation results, the proposed MARL framework can improve the microgrid operation performance from economy, independence, and benefit balance among microgrid participants.}
}
@article{WANG202372,
title = {Constructing autonomous, personalized, and private working management of smart home products based on deep reinforcement learning},
journal = {Procedia CIRP},
volume = {119},
pages = {72-77},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004389},
author = {Yuchen Wang and Ruxin Xiao and Xinheng Wang and Ang Liu},
keywords = {Smart product design, Deep reinforcement learning, Smart personalization, Artificial intelligence-enhanced design, Data privacy},
abstract = {Smart home products have been increasingly popular in markets and design research. Although modern smart home products are interactive and aware, they are not competently autonomous, personalized, and private to satisfy users’ experience in convenience and trustworthiness. As a branch of machine learning, deep reinforcement learning is characterized by its learning through interaction with environments without data exchanges and thus has the potential to resolve this problem. This paper proposes a method based on deep reinforcement learning to construct autonomous, personalized, and private working management of smart home products. This method generally splits into users’ manual management, training in deep reinforcement learning, and autonomous working management. Besides, this paper illustrates this method with a case study of robot vacuum cleaners. Overall, the contribution of this paper lies in the innovative application of deep reinforcement learning, which dynamically interacts with users’ contexts and working conditions, to realize autonomy, personalization, and data privacy of smart home products.}
}
@article{TSENG20233502,
title = {Reinforcement learning design framework for nacre-like structures optimized for pre-existing crack resistance},
journal = {Journal of Materials Research and Technology},
volume = {24},
pages = {3502-3512},
year = {2023},
issn = {2238-7854},
doi = {https://doi.org/10.1016/j.jmrt.2023.03.230},
url = {https://www.sciencedirect.com/science/article/pii/S2238785423006932},
author = {Bor-Yann Tseng and You-Cheng Cai and Chen-Wei {Conan Guo} and Elena Zhao and Chi-Hua Yu},
keywords = {Nacre, Bioinspired structural materials, Artificial intelligence, Reinforcement learning, Structural material design},
abstract = {Nacre is known for its uniquely high toughness and lightweight capabilities. Its unique structure is composed of soft nacre proteins and stiff calcium carbonates, allowing it to deflect cracks that expand in straight lines to increase energy dissipation. However, nacre microstructures are challenging to mimic due to the intractable number of combinations in the design space. We thus propose a reinforcement learning (RL) framework to efficiently design a high-toughness nacre-like structure. By designing local structures at the crack tip, we incorporated reinforcement learning with finite element to optimize the structure by replacing the soft and stiff materials in the design space. Starting from the initial unit cell, where the majority of the unit cell consists of soft materials, our method gradually improves the cell by arranging stiff and soft materials on the unit cell to achieve higher toughness. The optimized designs exhibit crack-insensitive behavior and excellent crack resistance when subjected to finite element simulation and experimental testing. This design framework can be used in synthetic instruments that require rapid construction rearrangements such as biomaterials and unexposed substructures, increasing their mechanical performance.}
}
@article{JABINI2022178,
title = {A Deep Reinforcement Learning Approach to Sensor Placement under Uncertainty},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {27},
pages = {178-183},
year = {2022},
note = {9th IFAC Symposium on Mechatronic Systems MECHATRONICS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.508},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322025617},
author = {Amin Jabini and Erik A Johnson},
keywords = {Sensor placement, Reinforcement learning, Information theory, Decision making in complex systems, Q-learning},
abstract = {Optimal sensor placement is critical for enhancing the effectiveness of monitoring dynamical systems. Deterministic solutions do not reflect the effects of input and parameter uncertainty on the sensor placement. Using a Markov decision process (MDP) and a sensor placement agent, this study proposes a stochastic approach to maximize the gain from placing a fixed number of sensors within the system. Utilizing Deep Reinforcement Learning (DRL), the agent is trained by collecting interactive samples within the environment, which uses an information-theoretic reward function that is a measure, based on Shannon entropy, of the identifiability of the model parameters. The goal of the agent is to maximize its expected future reward by selecting, at each step, the action (placing a sensor) that provides the most information. This framework is validated using a synthetic model of a base-isolated structure. To consider the existing uncertainty in the parameters, a prior probability distribution is chosen (e.g., based on expert judgement or preliminary study) for each model parameter. Further, a probabilistic model for the input is used to reflect input variability. In a Deep Q-network, a type of DRL algorithm, the agent learns a mapping from states (i.e., sensor configurations) to the ”quality” of each action at that state, called ”Q-values”. This network is trained using samples of state, action, and reward by interacting with the environment. The modular property of the framework and the function approximation used in this study makes it scalable to complex real-world applications of sensor placement problems in the presence of uncertainties.}
}
@article{PATEL2023108232,
title = {A practical Reinforcement Learning implementation approach for continuous process control},
journal = {Computers & Chemical Engineering},
volume = {174},
pages = {108232},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108232},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423001023},
author = {Kalpesh M Patel},
keywords = {Machine learning, Reinforcement learning, Autonomous operation, Intelligent process control, Safe RL, Explainable RL},
abstract = {Industrial process control using model-based technologies is well established. These technologies are typically non-adaptive and so have limitations. Reinforcement Learning (RL) provides a model-free adaptive alternative. RL is a type of machine learning (ML) where models or data sets of the environment are not necessary before learning can start. It generates data, by exploring the environment and then learn the behavior from it. Though RL has been successfully applied for learning and playing various games such as Go, Chess, Atari; its application to continuous process control problems is not trivial. There is a need for online RL implementation to be safe, fast learning and explainable when applied to industrial control problems. Rather than adding to the extensive research on augmenting existing RL algorithms, the paper presents a unique systematic method of formulating the RL problem incorporating domain-specific knowledge about process constraints and objectives, resulting in reduced dimensionality, along with modifications to the exploration process, applicable to any model free RL algorithm supporting continuous states and actions, to enhance safety, speed and explainability of online RL implementation without requiring a simulation model. The approach is successfully implemented on two multivariable processes: a simulated distillation column and a temperature control lab setup using the Deep Deterministic Policy Gradient (DDPG) algorithm. The work demonstrates that the presented method is applicable to multivariable, noisy, non-linear processes with disturbances. It will further the potential of introducing the advances in Artificial Intelligence and Machine Learning algorithms for intelligent process control capable of enabling autonomous operation in the process industry.}
}
@article{JI2022102382,
title = {Synthesizing the optimal gait of a quadruped robot with soft actuators using deep reinforcement learning},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {78},
pages = {102382},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102382},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000692},
author = {Qinglei Ji and Shuo Fu and Kaige Tan and Seshagopalan {Thorapalli Muralidharan} and Karin Lagrelius and David Danelia and Georgios Andrikopoulos and Xi Vincent Wang and Lihui Wang and Lei Feng},
keywords = {Quadruped robot, Soft actuators, Tendon-driven motion, Reinforcement learning, Robot gait, Motion control},
abstract = {Quadruped robots have the advantages of traversing complex terrains that are difficult for wheeled robots. Most of the reported quadruped robots are built by rigid parts. This paper proposes a new design of quadruped robots using soft actuators driven by tendons as the four legs. Compared to the rigid robots, the proposed soft quadruped robot has inherent safety, less weight and simpler mechanism for fabrication and control, but the corresponding challenge is that the accurate mathematical model applicable to model-based control design of the soft robot is difficult to derive by dynamics. To synthesize the optimal gait controller of the soft-legged robot, the paper makes the following contributions. First, the flexible components of the quadruped robot are modeled with different finite element and lumped parameter methods. The model accuracy and computation efficiency are analyzed. Second, soft actor–critic methods and curriculum learning are applied to learn the optimal gaits for different walking tasks. Third, The learned gaits are implemented in an in-house robot to transport hand tools. Preliminary results show that the robot can walk forward and correct the walking directions.}
}
@article{YUAN2023106487,
title = {Painless and accurate medical image analysis using deep reinforcement learning with task-oriented homogenized automatic pre-processing},
journal = {Computers in Biology and Medicine},
volume = {153},
pages = {106487},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.106487},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522011957},
author = {Di Yuan and Yunxin Liu and Zhenghua Xu and Yuefu Zhan and Junyang Chen and Thomas Lukasiewicz},
keywords = {Homogenized automatic pre-processing, Deep reinforcement learning, Medical image analysis, Policy gradient, Pediatric pneumonia classification},
abstract = {Pre-processing is widely applied in medical image analysis to remove the interference information. However, the existing pre-processing solutions mainly encounter two problems: (i) it is heavily relied on the assistance of clinical experts, making it hard for intelligent CAD systems to deploy quickly; (ii) due to the personnel and information barriers, it is difficult for medical institutions to conduct the same pre-processing operations, making a deep model that performs well on a specific medical institution difficult to achieve similar performances on the same task in other medical institutions. To overcome these problems, we propose a deep-reinforcement-learning-based task-oriented homogenized automatic pre-processing (DRL-HAPre) framework to overcome these two problems. This framework utilizes deep reinforcement learning techniques to learn a policy network to automatically and adaptively select the optimal pre-processing operations for the input medical images according to different analysis tasks, thus helping the intelligent CAD system to achieve a rapid deployment (i.e., painless) and maintain a satisfactory performance (i.e., accurate) among different medical institutes. To verify the effectiveness and advantages of the proposed DRL-HAPre framework, we further develop a homogenized automatic pre-processing model based on the DRL-HAPre framework to realize the automatic pre-processing of key region selection (called HAPre-KRS) in the pneumonia image classification task. Extensive experimental studies are conducted on three pediatric pneumonia classification datasets with different image qualities, and the results show that: (i) There does exist a hard-to-reproduce problem in clinical practices and the fact that having different medical image qualities in different medical institutes is an important reason for the existing of hard-to-reproduce problem, so it is compelling to propose homogenized automatic pre-processing method. (ii) The proposed HAPre-KRS model and DRL-HAPre framework greatly outperform three kinds of state-of-the-art baselines (i.e., pre-processing, attention and pneumonia baseline), and the lower the medical image quality, the greater the improvements of using our HAPre-KRS model and DRL-HAPre framework. (iii) With the help of homogenized pre-processing, HAPre-KRS (and DRL-HAPre framework) can greatly avoid performance degradation in real-world cross-source applications (i.e., thus overcoming the hard-to-reproduce problem).}
}
@article{SHI20209688,
title = {Deep Reinforcement Learning for Snake Robot Locomotion},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {9688-9695},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2619},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320333772},
author = {Junyao Shi and Tony Dear and Scott David Kelly},
keywords = {Mobile robots, Autonomous robotic systems, Intelligent robotics},
abstract = {The design of gaits for underactuated robots is often unintuitive, with many results derived from either trial and error or simplification of system structure. Recent advances in deep reinforcement learning have yielded results for systems continuous in either states or actions, which may extend to a variety of locomoting robots. In this work we employ reinforcement learning to derive efficient and novel gaits for both terrestrial and aquatic multi-link snake robots. Although such systems operate in different environments, we show that their shared geometric structure allows us to utilize the same learning techniques in both cases to find gaits without any human input. We present results learned and rolled out in simulation, and we describe preliminary efforts to implement the entire learning process on a physical system.}
}
@article{LONG202310564,
title = {Model-free algorithm for consensus of discrete-time multi-agent systems using reinforcement learning method},
journal = {Journal of the Franklin Institute},
volume = {360},
number = {14},
pages = {10564-10581},
year = {2023},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2023.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S001600322300501X},
author = {Mingkang Long and Qing An and Housheng Su and Hui Luo and Jin Zhao},
abstract = {In this work, we investigate consensus issues of discrete-time (DT) multi-agent systems (MASs) with completely unknown dynamic by using reinforcement learning (RL) technique. Different from policy iteration (PI) based algorithms that require admissible initial control policies, this work proposes a value iteration (VI) based model-free algorithm for consensus of DTMASs with optimal performance and no requirement of admissible initial control policy. Firstly, in order to utilize RL method, the consensus problem is modeled as an optimal control problem of tracking error system for each agent. Then, we introduce a VI algorithm for consensus of DTMASs and give a novel convergence analysis for this algorithm, which does not require admissible initial control input. To implement the proposed VI algorithm to achieve consensus of DTMASs without information of dynamics, we construct actor-critic networks to online estimate the value functions and optimal control inputs in real time. At last, we give some simulation results to show the validity of the proposed algorithm.}
}
@article{AN2022109583,
title = {Smart control of window and air cleaner for mitigating indoor PM2.5 with reduced energy consumption based on deep reinforcement learning},
journal = {Building and Environment},
volume = {224},
pages = {109583},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.109583},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322008137},
author = {Yuting An and Zhuolun Niu and Chun Chen},
keywords = {Deep reinforcement learning, Indoor PM, Energy consumption, Air cleaner, Window actuator},
abstract = {For naturally ventilated apartments equipped with air cleaners, it is essential to develop a controller that simultaneously controls the operation of a window and the air cleaner in order to mitigate indoor PM2.5 (particulate matter with aerodynamic diameter less than 2.5 μm) pollution with less energy consumption by the air cleaner. This investigation first employed the deep reinforcement learning approach to train a smart controller that minimizes the total economic loss due to PM2.5-related health risks and air cleaner energy consumption. The controller was trained offline in a virtual apartment constructed on the basis of a particle dynamics model with typical building parameters. The inputs required for the controller were the real-time indoor and outdoor PM2.5 concentrations, which could be measured by low-cost sensors. To test the trained deep Q-network (DQN) controller, a series of experiments were conducted in two laboratory chambers. Both the indoor PM2.5 concentrations and the operating time of the air cleaner were compared between the trained DQN controller and different benchmark controllers with various outdoor PM2.5 levels under different chamber conditions, in order to assess the controller performance. The trained DQN controller outperformed the benchmark controllers in reducing the total economic loss due to indoor PM2.5-related health risks and air cleaner energy consumption by 2.4%–43.7% for all 18 cases. Although the DQN controller was trained offline in a virtual apartment with typical building parameters, its performance was robust in the chamber experiments even when the parameters were very different from the typical values.}
}
@article{BOUZIDI2022108852,
title = {Dynamic clustering of software defined network switches and controller placement using deep reinforcement learning},
journal = {Computer Networks},
volume = {207},
pages = {108852},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.108852},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622000640},
author = {EL Hocine Bouzidi and Abdelkader Outtagarts and Rami Langar and Raouf Boutaba},
keywords = {SDN, Controller placement, DQN, Clustering, ONOS},
abstract = {Software defined networking (SDN) has emerged as a promising alternative to the traditional networks, offering many advantages, including flexibility in network management, network programmability and guaranteeing application Quality-of-Service (QoS) requirements. In SDN, the control plane is separated from the data plane, and deployed as a logically centralized controller. However, due to the large scale of networks as well as latency and reliability requirements, it is necessary to deploy multiple controllers to satisfy these requirements. The distributed deployment of SDN controllers unveiled new challenges in terms of determining the number of controllers needed, their locations and the assignment of switches to controllers that minimizes flow set delay. In this context, we propose, in this paper, a new method that dynamically computes the optimal number of controllers, determines their optimal locations, and at the same time partitions the set of data plane switches into clusters and assigns them to these controllers. First, we mathematically formulate the controller placement as an optimization problem, whose objectives are to minimize the controller response time, that is the delay between the SDN controller and assigned switches, the Control Load (CL), the Intra-Cluster Delay (ICD) and the Intra-Cluster Throughput (ICT). Second, we propose a simple yet computationally efficient heuristic, called Deep Q-Network based Dynamic Clustering and Placement (DDCP), that leverages the potential of reinforcement and deep learning techniques to solve the aforementioned optimization problem. Experimental results using ONOS controller show that the proposed approach can significantly improve the network performances in terms of response time and resource utilization.}
}
@article{LIN2022108127,
title = {Real-time power system generator tripping control based on deep reinforcement learning},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {141},
pages = {108127},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2022.108127},
url = {https://www.sciencedirect.com/science/article/pii/S0142061522001685},
author = {Bilin Lin and Huaiyuan Wang and Yang Zhang and Buying Wen},
keywords = {Emergency Control, Generator Tripping Control, Deep Reinforcement Learning, Convolutional Neural Network},
abstract = {In case of faults or severe disturbances, the power system will enter an emergency operation state. After the system instability is detected, oscillation and blackout will occur in the system if effective control measures are not taken in time. Generator tripping control (GTC) is the most effective emergency control measure. In view of the mismatch between the traditional GTC algorithm and the transient stability assessment method based on machine learning, a new real-time GTC method is needed. In this paper, a three-part control framework is designed for the GTC problem. The control agent is endowed with decision-making ability by interacting with the simulation environment in the offline pre-learning part. Then the trained agent is transplanted to the online application which can help system operators make decisions. Meanwhile, the agent is updated with real data to be better adapted to the actual system in the online learning part. A deep reinforcement learning algorithm, deep deterministic policy gradient (DDPG) is employed to train the control agent in this framework. A modified DDPG algorithm and the corresponding reward function are designed for the GTC problem. Convolution neural network (CNN) is added to the DDPG network, by which the training time of the agent is shortened and the generalization ability of the algorithm is improved. Trained with simulation data and real system experience, the control agent can determine control strategies timely according to the system operating conditions. Simulation results on the IEEE-39 bus system and the realistic regional power system of Eastern China show the effectiveness, generalizability, and timeliness of the decision algorithm.}
}
@article{MOUSA2023101434,
title = {Extended-deep Q-network: A functional reinforcement learning-based energy management strategy for plug-in hybrid electric vehicles},
journal = {Engineering Science and Technology, an International Journal},
volume = {43},
pages = {101434},
year = {2023},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2023.101434},
url = {https://www.sciencedirect.com/science/article/pii/S221509862300112X},
author = {Amr Mousa},
keywords = {Action masking, Deep Q-networks, Functional architecture, Reinforcement learning, Energy management strategy, Plug-in hybrid electric vehicles},
abstract = {Plug-in Hybrid Electric Vehicles offer a promising solution for the increasing CO2 emission problem. However, the improved economy strongly depends on the energy management strategy. Traditional rule-based strategies are no more practical considering the increasing complexity in control objectives. In this study, an adaptive online Reinforcement Learning (RL) agent is developed, which learned an energy management strategy with a near-optimal performance. A novel hybrid approach is proposed to integrate the agent into the existing rule-based hybrid control unit architecture with a limited operation domain for more practicality and suitability to series-production control systems. Dynamic Programming (DP) and rule-based strategy are used to benchmark the developed RL agent performance. The objective is to minimize the vehicle’s total fuel consumption and the frequent engine on/off switching to improve driver comfort and vehicle drivability. Several RL-based algorithms have been experimented and as a result, an Extended-Deep Q-Network (E-DQN) agent is proposed by this paper, trained on one cycle, and deployed on two other cycles with different onboard energy levels to evaluate the performance. The paper findings showed that E-DQN outperformed the rule-based strategy achieving up to 10.46% improvement in fuel economy closer to the DP performance alongside providing adequate compliance with the vehicle drivability and driver comfort objectives.}
}
@article{ALBABA20191,
title = {Modeling cyber-physical human systems via an interplay between reinforcement learning and game theory},
journal = {Annual Reviews in Control},
volume = {48},
pages = {1-21},
year = {2019},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2019.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1367578819301026},
author = {Berat Mert Albaba and Yildiray Yildiz},
keywords = {Cyber-physical human systems, Game theory, Reinforcement learning, Model validation},
abstract = {Predicting the outcomes of cyber-physical systems with multiple human interactions is a challenging problem. This article reviews a game theoretical approach to address this issue, where reinforcement learning is employed to predict the time-extended interaction dynamics. We explain that the most attractive feature of the method is proposing a computationally feasible approach to simultaneously model multiple humans as decision makers, instead of determining the decision dynamics of the intelligent agent of interest and forcing the others to obey certain kinematic and dynamic constraints imposed by the environment. We present two recent exploitations of the method to model (1) unmanned aircraft integration into the National Airspace System and (2) highway traffic. We conclude the article by providing ongoing and future work about employing, improving and validating the method. We also provide related open problems and research opportunities.}
}
@article{BAI2022115047,
title = {Wind farm layout optimization using adaptive evolutionary algorithm with Monte Carlo Tree Search reinforcement learning},
journal = {Energy Conversion and Management},
volume = {252},
pages = {115047},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2021.115047},
url = {https://www.sciencedirect.com/science/article/pii/S0196890421012231},
author = {Fangyun Bai and Xinglong Ju and Shouyi Wang and Wenyong Zhou and Feng Liu},
keywords = {Evolutionary computations, Adaptive genetic algorithm, Reinforcement learning, Monte-Carlo Tree Search, Wind farm layout optimization},
abstract = {Recent years have witnessed an enormous growth of wind farm capacity worldwide. Due to the wake effect, the velocity of incoming wind is reduced for the wind turbines in the downwind directions, thus causing discounted power generation in a wind farm. Previously, a self-informed adaptivity mechanism in evolutionary algorithms was introduced by the authors, which is inspired by the individuals’ self-adaptive capability to fit the environment in the natural world, where relocating the worst wind turbine with a surrogate model informed mechanism was found to be effective in improving the power conversion efficiency. In this paper, the exploitation capability in the adaptive genetic algorithm is further improved by casting the relocation of multiple wind turbines into a single-player reinforcement learning problem, which is further addressed by Monte-Carlo Tree Search embedded within the evolutionary algorithm. In contrast to the moderate improvements of the authors’ previous algorithms, significant improvement is achieved due to the enhanced algorithmic exploitation. The new algorithm is also applied to solve the optimal layout problem for a recently approved wind farm in New Jersey, and showed better performance against the benchmark algorithms.}
}
@article{AN2023113340,
title = {Energy-efficient control of indoor PM2.5 and thermal comfort in a real room using deep reinforcement learning},
journal = {Energy and Buildings},
volume = {295},
pages = {113340},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2023.113340},
url = {https://www.sciencedirect.com/science/article/pii/S0378778823005704},
author = {Yuting An and Chun Chen},
keywords = {Smart home, Deep reinforcement learning, Indoor PM, Thermal comfort, Energy consumption},
abstract = {To reduce indoor PM2.5 (particulate matter with aerodynamic diameter less than 2.5 μm) pollution and maintain thermal comfort with relatively low energy consumption, this study employed deep reinforcement learning (DRL) to develop a controller that could simultaneously control the window, air cleaner, and air conditioner in a real room. First, a room model was constructed on the basis of 3-week monitoring data in the real room. The controller was then trained in a virtual room utilizing the deep Q-network (DQN) algorithm. To evaluate the effectiveness of the DQN controller in the real world, a smart indoor environmental control system was established. Field testing was conducted in the real room for 4 days. The performance of the DQN controller was compared with that of an occupant-based baseline controller. During the testing period, the trained DQN controller could smartly control the window, air cleaner, and air conditioner in the real room. The PM2.5 healthy period and thermal comfort period was increased by around 21% and 16%, respectively, while the energy consumption was reduced by 23%, when compared with the baseline controller. Furthermore, simulations showed that the DQN controller still worked effectively when applied to other rooms with different characteristics.}
}
@article{RANA2015426,
title = {Dynamic pricing policies for interdependent perishable products or services using reinforcement learning},
journal = {Expert Systems with Applications},
volume = {42},
number = {1},
pages = {426-436},
year = {2015},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2014.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S095741741400400X},
author = {Rupal Rana and Fernando S. Oliveira},
keywords = {Dynamic pricing, Reinforcement learning, Revenue management, Service management, Simulation},
abstract = {Many businesses offer multiple products or services that are interdependent, in which the demand for one is often affected by the prices of others. This article considers a revenue management problem of multiple interdependent products, in which dynamically adjusted over a finite sales horizon to maximize expected revenue, given an initial inventory for each product. The main contribution of this article is to use reinforcement learning to model the optimal pricing of perishable interdependent products when demand is stochastic and its functional form unknown. We show that reinforcement learning can be used to price interdependent products. Moreover, we analyze the performance of the Q-learning with eligibility traces algorithm under different conditions. We illustrate our analysis with the pricing of services.}
}
@article{CAO2023108098,
title = {Reinforcement learning with prior policy guidance for motion planning of dual-arm free-floating space robot},
journal = {Aerospace Science and Technology},
volume = {136},
pages = {108098},
year = {2023},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2022.108098},
url = {https://www.sciencedirect.com/science/article/pii/S1270963822007726},
author = {Yuxue Cao and Shengjie Wang and Xiang Zheng and Wenke Ma and Xinru Xie and Lei Liu},
abstract = {Reinforcement learning methods as a promising technique have achieved superior results in the motion planning of free-floating space robots. However, due to the increase in planning dimension and the intensification of system dynamics coupling, the motion planning of dual-arm free-floating space robots remains an open challenge. In particular, the current study cannot handle the task of capturing a non-cooperative object due to the lack of the pose constraint of the end-effectors. To address the problem, we propose a novel algorithm, EfficientLPT, to facilitate RL-based methods to improve planning accuracy efficiently. Our core contributions are constructing a mixed policy with prior knowledge guidance and introducing ‖⋅‖∞ to build a more reasonable reward function. Furthermore, our method successfully captures a rotating object with different spinning speeds.}
}
@article{ARAI199921,
title = {Collision avoidance in multi-robot systems based on multi-layered reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {29},
number = {1},
pages = {21-32},
year = {1999},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(99)00035-4},
url = {https://www.sciencedirect.com/science/article/pii/S0921889099000354},
author = {Yoshikazu Arai and Teruo Fujii and Hajime Asama and Hayato Kaetsu and Isao Endo},
keywords = {Collision avoidance, Reinforcement learning, Multi-layered learning, Local communication, Mobile robot},
abstract = {It is important for a robot to acquire adaptive behaviors for avoiding surrounding robots and obstacles in complicated environments. Although the introduction of a learning scheme is expected to be one of the solutions for this purpose, a large size of memory and a large calculation cost are required to handle useful information such as motions of robots. In this paper, we introduce the multi-layered reinforcement learning method. By dividing a learning curriculum into multiple layers, the number of expected situations can be reduced. It is shown that real robots can adaptively avoid collision with each other and to obstacles in a complicated situation.}
}
@article{QIN2023126209,
title = {Energy-efficient heating control for nearly zero energy residential buildings with deep reinforcement learning},
journal = {Energy},
volume = {264},
pages = {126209},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.126209},
url = {https://www.sciencedirect.com/science/article/pii/S036054422203095X},
author = {Haosen Qin and Zhen Yu and Tailu Li and Xueliang Liu and Li Li},
keywords = {HVAC, Optimal control, Reinforcement learning, Deep Q learning, Prioritized replay, Model-free control},
abstract = {Controlling Heating, Ventilation and Air Conditioning (HVAC) systems is critical to improving energy efficiency of demand-side. In this paper, a model-free optimal control method based on deep reinforcement learning is proposed to control the heat pump start/stop and room temperature setting in residential buildings. The optimization goal of this method is to obtain the highest comprehensive reward which considering thermal comfort and energy cost. Firstly, the randomness, learning process, thermal comfort and energy consumption of the model-free controller are systematically investigated by a simulation system based on measured data. The results show that randomness has a significant impact on the initial performance and convergence speed of the model-free controller; The model-free controller has a linear accumulation of comprehensive rewards during the learning process, and the slope of the accumulated comprehensive rewards can be used to determine whether the controller converges; The model-free controller coordinates monitoring data, weather forecasts and building thermal inertia to achieve the highest comprehensive reward. Afterwards, the model-free controller was verified in a nearly zero energy residential building in Beijing, China. The results show that model-free controller improves the comprehensive reward by 15.3% compared to rule-based method.}
}
@article{LI2020139,
title = {Random curiosity-driven exploration in deep reinforcement learning},
journal = {Neurocomputing},
volume = {418},
pages = {139-147},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.08.024},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220312844},
author = {Jing Li and Xinxin Shi and Jiehao Li and Xin Zhang and Junzheng Wang},
keywords = {Deep reinforcement learning, Curiosity-driven exploration, Intrinsic rewards},
abstract = {Reinforcement learning (RL) depends on carefully engineering environment rewards. However, rewards from environments are extremely sparse for many RL tasks, challenging for the agent to learn skills and interact with the environment. One solution to this problem is to create intrinsic rewards for agents and to make rewards dense and more suitable for learning. Recent algorithms, such as curiosity-driven exploration, usually estimate the novelty of the next state through the prediction error of dynamics models. However, these methods are typically limited by the capacity of their dynamics models. In this paper, a random curiosity-driven model using deep reinforcement learning is proposed, which uses a target network with fixed weights to maintain the stability of dynamics models and create more suitable intrinsic rewards. We integrate the parametric exploration method for further promoting sufficient exploration. Besides, a deeper and more closely connected network is utilized for encoding the pixel images for policy-gradient. By comparing our method against the previous approaches in several environments, the experiments show that our method achieves state-of-the-art performance on most but not all of the Atari games.}
}
@article{LIU2021106781,
title = {Formula-E race strategy development using distributed policy gradient reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {216},
pages = {106781},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106781},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121000447},
author = {Xuze Liu and Abbas Fotouhi and Daniel J. Auger},
keywords = {Energy management, Formula-E race strategy, Deep deterministic policy gradient, Reinforcement leaning},
abstract = {Energy and thermal management is a crucial element in Formula-E race strategy development. In this study, the race-level strategy development is formulated into a Markov decision process (MDP) problem featuring a hybrid-type action space. Deep Deterministic Policy Gradient (DDPG) reinforcement learning is implemented under distributed architecture Ape-X and integrated with the prioritized experience replay and reward shaping techniques to optimize a hybrid-type set of actions of both continuous and discrete components. Soft boundary violation penalties in reward shaping, significantly improves the performance of DDPG and makes it capable of generating faster race finishing solutions. The new proposed method has shown superior performance in comparison to the Monte Carlo Tree Search (MCTS) with policy gradient reinforcement learning, which solves this problem in a fully discrete action space as presented in the literature. The advantages are faster race finishing time and better handling of ambient temperature rise.}
}
@article{HE2021176,
title = {Real-time Energy Optimization of Hybrid Electric Vehicle in Connected Environment Based on Deep Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {10},
pages = {176-181},
year = {2021},
note = {6th IFAC Conference on Engine Powertrain Control, Simulation and Modeling E-COSM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.160},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321015627},
author = {Weiliang He and Ying Huang},
keywords = {Powertrain control, Connected, automated vehicles, Hybrid electric vehicles, Vehicle-to-everything, Deep deterministic policy gradient},
abstract = {In this paper, a real-time control method of hybrid electric vehicle is proposed based on rule-based speed planning and deep deterministic policy gradient (DDPG) energy management algorithm. This method can optimize fuel economy in real time based on all traffic information in a connected environment, and satisfy the constraints of driving safety and driving time. The results show that the proposed deep reinforcement learning algorithm DDPG can achieve lower fuel consumption. In addition, the proposed speed planning algorithm will not violate traffic rules and has good results.}
}
@article{ZHANG2022494,
title = {Parameter Optimal Iterative Learning Control Design: from Model-based, Data-driven to Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {12},
pages = {494-499},
year = {2022},
note = {14th IFAC Workshop on Adaptive and Learning Control Systems ALCOS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.07.360},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322007613},
author = {Yueqing Zhang and Bing Chu and Zhan Shu},
keywords = {Iterative learning control, reinforcement learning control, data-based control},
abstract = {Iterative learning control (ILC) is a high-performance control design method for systems operating in a repetitive fashion by learning from past experience. Our recent work shows that reinforcement learning (RL) shares many features with ILC and thus opens the door to new ILC algorithm designs. This paper continues the research by considering a parameter optimal iterative learning control (POILC) algorithm. It has a very simple structure and appealing convergence properties, but requires a model of the system. We first develop a data-driven POILC algorithm without using model information by performing an extra experiment on the plant. We then use a policy gradient RL algorithm to design a new model-free POILC algorithm. Both algorithms achieve the high-performance control target without using model information, but the convergence properties do differ. In particular, by increasing the number of function approximators in the latter, the RL-based model-free ILC can approach the performance of the model-based POILC. A numerical study is presented to compare the performance of different approaches and demonstrate the effectiveness of the proposed designs.}
}
@article{WANG2023110761,
title = {Study on deep reinforcement learning-based multi-objective path planning algorithm for inter-well connected-channels},
journal = {Applied Soft Computing},
volume = {147},
pages = {110761},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110761},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623007792},
author = {Ruiqi Wang and Dongmei Zhang and Zhijiang Kang and Rucheng Zhou and Gang Hui},
keywords = {Fracture-cavity reservoir, Inter-well connected channels, Deep reinforcement learning, Multi-objective optimization},
abstract = {Defining inter-well connectivity is very important for the water injection development of carbonate fractured-vuggy reservoirs. However, most conventional methods based on logging or seismic data are subjective. In this research, multi-attribute seismic data were used to characterize the 3D search space, and an improved reward function was designed based on the law of fluid flow and appropriate heuristics. A deep reinforcement learning model based on an improved reward function was designed to search for inter-well connected channels. Selecting the shortest channel length and the largest flow volume were the optimization objectives. We propose the multi-objective evolutionary optimization algorithm with decomposition and differential evolution (MOEA/D-DE) based on an improved mutation operator to automatically obtain the optimal inter-well connected channels. The searched channels can readily show the spatial distribution of multi-scale fractures and caves. The experimental results of the Tahe oilfield show that the improved algorithm effectively enhanced the local convergence performance of the multi-objective algorithm and that the automatic search paths were consistent with the characteristics of seismic static data and tracer test results. Our model can better reflect the spatial distribution of a fracture cavity at different scales and offers important guidance for on-site development adjustment work in the water injection development stage.}
}
@article{SHEN2023284,
title = {A dynamic task assignment model for aviation emergency rescue based on multi-agent reinforcement learning},
journal = {Journal of Safety Science and Resilience},
volume = {4},
number = {3},
pages = {284-293},
year = {2023},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2023.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666449623000270},
author = {Yang Shen and Xianbing Wang and Huajun Wang and Yongchen Guo and Xiang Chen and Jiaqi Han},
keywords = {Aviation emergency rescue, Task assignment, Multi-agent reinforcement learning, Benefit evaluation},
abstract = {China's natural disaster situation presents a complex and severe scenario, resulting in substantial human and material losses as a result of large-scale emergencies. Recognizing the significance of aviation emergency rescue, the state provides strong support for its development. However, China's current aviation emergency rescue system is still under construction and encounters various challenges; one such challenge is to match the dynamically changing multi-point rescue demands with the limited availability of aircraft dispatch. We propose a dynamic task assignment model and a trainable model framework for aviation emergency rescue based on multi-agent reinforcement learning. Combined with a targeted design, the scheduling matching problem is transformed into a stochastic game process from the rescue location perspective. Subsequently, an optimized strategy model with high robustness can be obtained by solving the training framework. Comparative experiments demonstrate that the proposed model is able to achieve higher assignment benefits by considering the dynamic nature of rescue demands and the limited availability of rescue helicopter crews. Additionally, the model is able to achieve higher task assignment rates and average time satisfaction by assigning tasks in a more efficient and timely manner. The results suggest that the proposed dynamic task assignment model is a promising approach for improving the efficiency of aviation emergency rescue.}
}
@article{LEONG2020108759,
title = {Deep reinforcement learning for wireless sensor scheduling in cyber–physical systems},
journal = {Automatica},
volume = {113},
pages = {108759},
year = {2020},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2019.108759},
url = {https://www.sciencedirect.com/science/article/pii/S0005109819306223},
author = {Alex S. Leong and Arunselvan Ramaswamy and Daniel E. Quevedo and Holger Karl and Ling Shi},
abstract = {In many cyber–physical systems, we encounter the problem of remote state estimation of geographically distributed and remote physical processes. This paper studies the scheduling of sensor transmissions to estimate the states of multiple remote, dynamic processes. Information from the different sensors has to be transmitted to a central gateway over a wireless network for monitoring purposes, where typically fewer wireless channels are available than there are processes to be monitored. For effective estimation at the gateway, the sensors need to be scheduled appropriately, i.e., at each time instant one needs to decide which sensors have network access and which ones do not. To address this scheduling problem, we formulate an associated Markov decision process (MDP). This MDP is then solved using a Deep Q-Network, a recent deep reinforcement learning algorithm that is at once scalable and model-free. We compare our scheduling algorithm to popular scheduling algorithms such as round-robin and reduced-waiting-time, among others. Our algorithm is shown to significantly outperform these algorithms for many example scenarios.}
}
@article{TU2023120568,
title = {A deep reinforcement learning hyper-heuristic with feature fusion for online packing problems},
journal = {Expert Systems with Applications},
volume = {230},
pages = {120568},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120568},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423010709},
author = {Chaofan Tu and Ruibin Bai and Uwe Aickelin and Yuchang Zhang and Heshan Du},
keywords = {Hyper-heuristic, Deep reinforcement learning, Feature fusion, Knapsack problem, Strip packing problem},
abstract = {In recent years, deep reinforcement learning has shown great potential in solving computer games with sequential decision-making scenarios. Hyper-heuristic is a generic search framework, capable of intelligently selecting or generating algorithms to solve a class of optimisation problems with stochastic or dynamic settings. This paper proposes a new general framework for solving online packing problems using deep reinforcement learning hyper-heuristics. Although analytical approaches can address most offline packing problems successfully, their online versions have proved much more challenging and the performance of the existing methods is often not satisfactory. In this paper, we extend a recent deep reinforcement learning hyper-heuristic framework by fusing the visual information of real-time packing with distributional information of random parameters of the problem. Computational experiments show that our method outperforms the state of the art online methods with reductions in optimality gap between 2%–19% for knapsack problem and 0.7% for the online strip packing problem. In addition, a new visual analysis presentation is also devised to better interpret the learned packing strategies, which can reveal more information than the widely used landscape analysis. As online packing problems are widely available in production environments, the proposed approach can serve as an important reference to solve other similar combinatorial optimisation problems for which visual layout inputs would aid learning.}
}
@article{AZUATALAM2020100020,
title = {Reinforcement learning for whole-building HVAC control and demand response},
journal = {Energy and AI},
volume = {2},
pages = {100020},
year = {2020},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2020.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2666546820300203},
author = {Donald Azuatalam and Wee-Lih Lee and Frits {de Nijs} and Ariel Liebman},
keywords = {Demand response, Reinforcement learning, Whole-building HVAC control, Distributed energy resources, Optimal HVAC energy scheduling},
abstract = {This paper proposes a novel reinforcement learning (RL) architecture for the efficient scheduling and control of the heating, ventilation and air conditioning (HVAC) system in a commercial building while harnessing its demand response (DR) potentials. With advances in automated building management systems, this can be achieved seamlessly by a smart autonomous RL agent which takes the best action, for example, a change in HVAC temperature set point, necessary to change the electricity usage pattern of a building in response to demand response signals, and with minimal thermal comfort impact to customers. Previous research in this area has tackled only individual aspects of the problem using RL. Specifically, due to the challenges in implementing demand response with whole-building models, simpler analytical models which poorly capture reality have been used instead. And where whole-building models are applied, RL is used for HVAC control mainly to achieve energy efficiency goals while demand response is neglected. Thus, in this research, we implement a holistic framework by designing an efficient RL controller for a whole-building model which learns to optimise and control the HVAC system for improved energy efficiency and thermal comfort levels in addition to achieving demand response goals. Our simulation results show that by applying reinforcement learning for normal HVAC operation, a maximum weekly energy reduction of up to 22% can be achieved compared to a handcrafted baseline controller. Furthermore, by employing a DR-aware RL controller during demand response periods, average power reductions or increases of up to 50% can be achieved on a weekly basis compared to the default RL controller, while keeping occupant thermal comfort levels within acceptable bounds.}
}
@article{TUVU2022277,
title = {Disturbance observer-based adaptive reinforcement learning for perturbed uncertain surface vessels},
journal = {ISA Transactions},
volume = {130},
pages = {277-292},
year = {2022},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2022.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S0019057822001495},
author = {Van {Tu Vu} and Thanh Loc Pham and Phuong Nam Dao},
keywords = {Adaptive/approximate reinforcement learning (ARL), Surface vessels (SVs), Disturbance observer (DO), Optimal control, Lyapunov stability theory},
abstract = {This article considers a problem of tracking, convergence of disturbance observer (DO) based optimal control design for uncertain surface vessels (SVs) with external disturbance. The advantage of proposed optimal control using adaptive/approximate reinforcement learning (ARL) is that consideration for whole SVs with only one dynamic equation and without conventional separation technique. Additionally, thanks to appropriate disturbance observer, the attraction region of tracking error is remarkably reduced. On the other hand, the particular case of optimal control problem is presented by directly solving for the purpose of choosing the suitable activation functions of ARL. Furthermore, the proposed ARL based optimal control also deals with non-autonomous property of closed tracking error SV model by considering the equivalent system. Based on the Lyapunov function candidate using optimal function and quadratic form of estimated error of actor/critic weight, the stability and convergence of the closed system are proven. Some examples are given to verify and demonstrate the effectiveness of the new control strategy.}
}
@article{GRASSO202269,
title = {Multi-Agent Deep Reinforcement Learning in Flying Ad-Hoc Networks for Delay-Constrained Applications},
journal = {Procedia Computer Science},
volume = {203},
pages = {69-78},
year = {2022},
note = {17th International Conference on Future Networks and Communications / 19th International Conference on Mobile Systems and Pervasive Computing / 12th International Conference on Sustainable Energy Information Technology (FNC/MobiSPC/SEIT 2022), August 9-11, 2022, Niagara Falls, Ontario, Canada},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922006160},
author = {Christian Grasso and Raoul Raftopoulos and Giovanni Schembra},
keywords = {6G, Zero-Touch Network Management, UAVs, Multi-Agent Deep Reinforcement Learning, Latency Minimization},
abstract = {As a benefit of their compelling features, unmanned aerial vehicles (UAVs) are expected to play a vital role in 6G networks. UAVs are capable of providing wireless connectivity where network infrastructure is not available, or complement the conventional base stations. UAVs organized as a Flying Ad-hoc Network (FANET) can process jobs received by ground devices through vertical offload. Moreover, horizontal offload inside the FANET can applied to balance the load among UAVs to reduce the processing delay to offloaded jobs. However, offloading decisions inside a FANET have to be dynamically adapted to the current processing load distribution based on the state of the FANET and the number of job requests coming from the served geographical area. As the number of UAVs in the FANET increases, single agent reinforcement learning approaches fall short to achieve reasonable performance due to the exponential increase in the action space. For this reason, in this paper we propose a Multi-AgeNt Intra-FANET (MANIA-F) Framework based on Multi Agent PPO (MAPPO), in which each agent chooses the best offloading probabilities to forward incoming jobs to neighboring UAVs. The horizontal offload decision problem is defined as a Markov Decision Process (MDP) that is solved via Multi-Agent Deep Reinforcement Learning (MARL).}
}
@article{KHAN2012359,
title = {Corrigendum to ‘Reinforcement Learning and Optimal Adaptive Control: An Overview and Implementation Examples’ [Annual Reviews in Control 36 (1) (2012) 42–59]},
journal = {Annual Reviews in Control},
volume = {36},
number = {2},
pages = {359},
year = {2012},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2012.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1367578812000351},
author = {Said G. Khan and Guido Herrmann and Frank L. Lewis and Tony Pipe and Chris Melhuish}
}
@article{MORADI2022111882,
title = {Marine route optimization using reinforcement learning approach to reduce fuel consumption and consequently minimize CO2 emissions},
journal = {Ocean Engineering},
volume = {259},
pages = {111882},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.111882},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822012239},
author = {Mohammad Hossein Moradi and Martin Brutsche and Markus Wenig and Uwe Wagner and Thomas Koch},
keywords = {Marine, Route optimization, CO reduction, Reinforcement learning, Artificial intelligence},
abstract = {To meet the 2050 CO2 targets, the shipping industry which is responsible for about 3% of global CO2 emissions needs to be optimized in several aspects. Obviously, alternative fuels constitute the main measure in this respect. However, relatively high fuel prices in combination with increasing political and economic pressure may raise the need for more efficient ship operation. Ship route optimization can make an indispensable contribution to achieving this goal. In this sense, this paper applies an innovative approach for route optimization using Reinforcement Learning (RL). For this purpose, a generic ship model is first developed using Artificial Neural Networks (ANNs) to predict the fuel consumption of the ship. Moreover, various RL methods, namely Deep Q-Network (DQN), Deep Deterministic Policy Gradient (DDPG), and Proximal Policy Optimization (PPO) are applied. The application of RL enables continuous action space and simultaneous optimization of ship speed and heading. DDPG demonstrates the best results as an off-policy and policy gradient method which allows a continuous action space. For example, in the fuel consumption minimization scenario without time limitation, this method can achieve savings of 6.64%. For DQN as a method with discrete action space, this value is 1.07%.}
}
@article{REN2024121111,
title = {Two-layer coordinated reinforcement learning for traffic signal control in traffic network},
journal = {Expert Systems with Applications},
volume = {235},
pages = {121111},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121111},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423016135},
author = {Fuyue Ren and Wei Dong and Xiaodong Zhao and Fan Zhang and Yaguang Kong and Qiang Yang},
keywords = {Smart transportation, Deep reinforcement learning, Multi-agent, Traffic light control},
abstract = {Intersection traffic signal control considering vehicle emissions has become an important topic, however, the decision complexity of traffic signal control increases dramatically in a dynamic traffic environment with multi-intersections. It is a severe challenge to coordinate traffic signals at multi-intersections based on Internet of Things information to improve the traffic condition of the road network. This paper proposes a two-layer coordination algorithm based on multi-agent reinforcement learning—Multi-agent Coordinated Policy Optimization (MACoPO), for solving traffic signal control at multi-intersections. MACoPO consists of local cooperation, which adjusts the weights of individual rewards and neighborhood agents' rewards by using local cooperation factors (LCF), and global coordination, which updates the LCF to maximize global rewards. The state and reward functions are designed in terms of the current state of the signal, waiting queue length, vehicle density and emission concentration in the lane, vehicle delay, and vehicle emissions, thus making full use of the intersection state information. The proposed method is extensively assessed through simulation experiments using artificial and real road networks and the numerical results confirm its effectiveness in complex and dynamic real-time traffic environments with multi-intersections.}
}
@article{PAN2021102987,
title = {Integrated optimal control strategies for freeway traffic mixed with connected automated vehicles: A model-based reinforcement learning approach},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {123},
pages = {102987},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.102987},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X2100022X},
author = {Tianlu Pan and Renzhong Guo and William H.K. Lam and Renxin Zhong and Weixi Wang and Biao He},
keywords = {Integrated traffic control, Vehicle automation and communication system, Multiclass multilane cell transmission model, Connected automated vehicle, Penetration rate},
abstract = {This paper proposes an integrated freeway traffic flow control framework that aims to minimize the total travel cost, improve greenness and safety for freeway traffic mixed with connected automated vehicles (CAVs) and regular human-piloted vehicles (RHVs). The proposed framework devises an integrated action of several control strategies such as ramp metering, lane changing control (LCC) for CAVs and lane changing recommendation (LCR) for RHVs, variable speed limit control (VSLC) for CAVs and variable speed limit recommendation (VSLR) for RHVs with minimum safety gap control measures for lane changing and merging maneuvers. The CAVs are assumed to follow the system control instructions fully and immediately. In contrast, the RHVs would make decisions in response to the recommendations disseminated and also the behaviors of CAVs. The compliance rate of drivers to the LCR is captured by the underlying traffic flow model. A set of constraints is imposed to restrict VSLC/VSLR and LCC/LCR measures from changing too frequently or too sharply on both temporal and spatial dimensions to avoid excessive nuisance to passengers and traffic flow instability. A reinforcement learning based solution algorithm is proposed. First, a control parameterization technique is adopted to reduce the dimension of the original optimal control problem to increase computational efficiency. Then, a gradient-free Cross-Entropy-Method based algorithm is used to search the optimal parameters to circumvent the non-differentiability of the traffic flow model. The feasibility and effectiveness of the proposed framework are illustrated via numerical examples for a variety of penetration rates of CAVs under various traffic conditions. A sensitivity analysis is conducted to demonstrate the impacts of several important parameters such as the reaction time of the CAVs. It is found that the integrated control strategy can reduce the total travel cost by reducing the lane changing maneuvers and vehicles queuing at the bottleneck meanwhile smooth the traffic flow and suppress the adverse impact of shockwaves. The effect of ramp metering is not significant when the penetration rate of CAVs is high enough. Speed harmonization (with minimum gap control) in conjunction with LCC/LCR would be a better integrated control strategy under high penetration rate of CAVs.}
}
@article{KUMAR201884,
title = {Linguistic Lyapunov reinforcement learning control for robotic manipulators},
journal = {Neurocomputing},
volume = {272},
pages = {84-95},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.06.064},
url = {https://www.sciencedirect.com/science/article/pii/S092523121731192X},
author = {Abhishek Kumar and Rajneesh Sharma},
keywords = {Reinforcement learning, Fuzzy Q learning, Linguistic Lyapunov RL, Two link robotic manipulator, SCARA},
abstract = {We propose a Lyapunov theory based linguistic reinforcement learning (RL) framework for stable tracking control of robotic manipulators. In particular, we employ Lyapunov theory to constrain fuzzy rule consequents for ensuring stability of the designed controller. Proposed fuzzy RL controller employs Lyapunov theory dictated rules for discovering an optimal yet stable control strategy for robotic manipulators. Furthermore, our proposed linguistic RL controller handles payload variations and external disturbances quite effectively. We validate linguistic Lyapunov RL controller on two benchmark control problems: (i) a standard two-link robotic arm manipulator, and (ii) a two link selective compliance assembly robotic arm (SCARA). Simulation results and comparison against (a) baseline fuzzy Q learning (FQL) controller, and (b) a recently proposed Lyapunov theory based Markov game controller showcases our controller's superior tracking performance and lower computational complexity. Furthermore, our controller exhibits high stability with disturbances and payload variations.}
}
@article{KANG2023104538,
title = {A bi-level reinforcement learning model for optimal scheduling and planning of battery energy storage considering uncertainty in the energy-sharing community},
journal = {Sustainable Cities and Society},
volume = {94},
pages = {104538},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2023.104538},
url = {https://www.sciencedirect.com/science/article/pii/S221067072300149X},
author = {Hyuna Kang and Seunghoon Jung and Jaewon Jeoung and Juwon Hong and Taehoon Hong},
keywords = {Battery energy storage system, Reused battery, Optimal scheduling, Optimal planning, Reinforcement learning, Geographic information system},
abstract = {Sharing of battery energy storage systems (BESS) in the energy community by reflecting the real world can play a significant role in achieving carbon neutrality. Therefore, this study aimed to develop a bi-level reinforcement learning (RL) model of BESS considering uncertainty in the energy-sharing community for the following optimization strategies: (i) short-term scheduling model for optimal electricity flows considering operational objectives (i.e., self-sufficiency rate (SSR), peak load, and economic profit); and (ii) long-term planning model for optimal BESS plan (i.e., install, replace, and disuse) along with battery types (new or reused batteries). A case study in the South Korea Nonhyeon neighborhood was conducted to evaluate the developed bi-level RL model feasibility based on future scenarios considering the time-dependent variables. The developed model increased economic profit by up to 18,830 USD compared to the rule-based model. Compared to the case where BESS was not installed, SSR increased by up to 7.79% and peak demand decreased by up to 1.31 kWh. These results show that the developed model could maximize the economic feasibility of community-shared BESS by reflecting the uncertainty in the real world, ultimately benefiting participants in the energy-sharing community.}
}
@article{NAQVI2023109874,
title = {Implementability improvement of deep reinforcement learning based congestion control in cellular network},
journal = {Computer Networks},
volume = {233},
pages = {109874},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109874},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623003195},
author = {Haidlir Achmad Naqvi and Muhammad Hafizhuddin Hilman and Bayu Anggorojati},
keywords = {Congestion control, Deep reinforcement learning, Cellular network, Implementability},
abstract = {The application of deep reinforcement learning to improve the adaptability of congestion control is promising. However, the state-of-the-art method indicates a high packet loss and requires a high CPU to handle a flow. Those hinder the implementation of deep reinforcement learning-based congestion control in the production network. Therefore, we propose modifications in the agent’s deployment design, specifically in the monitoring component, interval, and transport protocol to reduce packet loss and CPU usage. Unfortunately, those agent modifications yield a tradeoff in throughput performance. In order to compensate for the tradeoff, we re-train the policy model using ns-3 (network-simulator-3) as a gym environment and custom reward function to improve the throughput. Our work shows that the proposed method evaluated in cellular networks is able to reduce packet loss by up to 50.7×, suppress CPU (central processing unit) usage by up to 4.13×, and increase the throughput by up to 6.94%. We hope our contribution can drive the adoption of deep reinforcement learning-based congestion control to the production network.}
}
@article{LI202210,
title = {Dynamic power allocation in IIoT based on multi-agent deep reinforcement learning},
journal = {Neurocomputing},
volume = {505},
pages = {10-18},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222008657},
author = {Fenglei Li and Zhixin Liu and Xinzhe Zhang and Yi Yang},
keywords = {IIOT, Power allocation, Deep reinforcement learning, Multi-agent, Time delay, Mobile devices},
abstract = {With the rapidly growing fifth generation (5G) wireless data traffic, the cellular network has gradually become an important mode for the Industrial Internet of Things (IIoT). To give full play to the advantages of cellular network, it is desirable to design the optimal allocation strategy under the condition of limited network resources. In this paper, we consider the problem of allocating downlink power to mobile IIoT devices based on multi-agent deep reinforcement learning (MADRL). To maximize the total system capacity, each base station (BS)-user device (UE) is modeled as a RL agent to learn optimal allocation policy. A centralized training and distributed execution learning framework is proposed, the influence of parameter transmission delay on allocation policy is considered. The designed state space and reward function can adapt to large-scale networks on account of their expandability. Simulation results show the effectiveness and superiority of the proposed method in terms of the system sum rate.}
}
@article{LIANG2021107535,
title = {Gated multi-attention representation in reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {233},
pages = {107535},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107535},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007978},
author = {Dayang Liang and Qihang Chen and Yunlong Liu},
keywords = {Deep reinforcement learning, Gated multi-attention module, Deep Q-learning network, Atari 2600 games},
abstract = {Deep reinforcement learning (DRL) has achieved great success in recent years by combining the feature extraction power of deep learning and the decision power of reinforcement learning techniques. In the literature, Convolutional Neural Networks (CNN) is usually used as the feature extraction method and recent studies have shown that the performances of the DRL algorithms can be greatly improved with the utilization of the attention mechanism, where the raw attentions are directly used for the decision-making. However, as is well-known, reinforcement learning is a trial-and-error process and it is almost impossible to learn an optimal policy in the beginning of the learning, especially in environments with sparse rewards, which in turn will cause the raw attention-based models can only remember and utilize the attention information indiscriminately for different areas and may focus on some task-irrelevant regions, but the focusing on such task-irrelevant regions is usually helpless and ineffective for the agent to find the optimal policy. To address this issue, we propose a gated multi-attention mechanism, which is then combined with the Deep Q-learning network (GMAQN). The gated multi-attention representation module (GMA) in GMAQN can effectively eliminate task-irrelevant attention information in the early phase of the trial-and-error process and improve the stability of the model. The proposed method has been demonstrated on the challenging domain of classic Atari 2600 games and experimental results show that compared with the baselines, our method can achieve better performance in terms of both the scores and the effect of focusing in the key regions.}
}
@article{HEIN20208082,
title = {Interpretable Control by Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {8082-8089},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2277},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320329372},
author = {Daniel Hein and Steffen Limmer and Thomas A. Runkler},
keywords = {Human supervised control, learning control, LQR, PID, fuzzy control},
abstract = {In this paper, three recently introduced reinforcement learning (RL) methods are used to generate human-interpretable policies for the cart-pole balancing benchmark. The novel RL methods learn human-interpretable policies in the form of compact fuzzy controllers and simple algebraic equations. The representations as well as the achieved control performances are compared with two classical controller design methods and three non-interpretable RL methods. All eight methods utilize the same previously generated data batch and produce their controller offline - without interaction with the real benchmark dynamics. The experiments show that the novel RL methods are able to automatically generate well-performing policies which are at the same time human-interpretable. Furthermore, one of the methods is applied to automatically learn an equation-based policy for a hardware cart-pole demonstrator by using only human-player-generated batch data. The solution generated in the first attempt already represents a successful balancing policy, which demonstrates the methods applicability to real-world problems.}
}
@article{ZHANG2019314,
title = {A Preliminary Study on the Relationship Between Iterative Learning Control and Reinforcement Learning⁎⁎This project is sponsored by the China Scholarship Council (CSC).},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {29},
pages = {314-319},
year = {2019},
note = {13th IFAC Workshop on Adaptive and Learning Control Systems ALCOS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.669},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319326187},
author = {Yueqing Zhang and Bing Chu and Zhan Shu},
keywords = {Iterative learning control, reinforcement learning},
abstract = {Iterative learning control is a control system design method that is able to achieve high tracking performance by repeatedly executing a task and learning the best input from previous attempts of performing the task. Reinforcement learning is a machine learning method that determines the best action such that some utility function (reward) is maximised by repeatedly interacting with the environment (system) and learning the best action policy based on the reward received from such interactions. These two methods belong to different subject disciplines but share a number of similarities. The relationship between these two design approaches, however, has not been investigated in detail. This paper presents a preliminary study on the relationship between iterative learning control and reinforcement learning, hopefully shedding some light on how these two areas can benefit each other in future research.}
}
@article{YANG2023115040,
title = {Improved reinforcement learning for collision-free local path planning of dynamic obstacle},
journal = {Ocean Engineering},
volume = {283},
pages = {115040},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.115040},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823014245},
author = {Xiao Yang and Qilong Han},
keywords = {Artificial intelligence, Intelligent ship, Dynamic obstacle avoidance and path planning, Exploration strategies, Actor–critic},
abstract = {The rapid development of artificial intelligence has driven the transformation of traditional industry. The proposal for an intelligent ship reflects the development of the shipping industry in the direction of intelligence. Safe maritime transportation is an essential branch of the intelligent ship industry. In this context, applying various intelligent algorithms in ship dynamic obstacle avoidance has attracted the attention and discussion of scholars. Due to the complexity and diversity of ship navigation, environments cannot be described by a definite mathematical model. Reinforcement learning has certain advantages in solving path planning in a complex environment. Traditional mathematical algorithms and swarm intelligence algorithms need mathematical models to constrain boundary conditions when solving complex problems. However, some complex dynamic environments in the real world cannot define mathematical models. This paper optimizes the parameter update and exploration strategies of the actor–critic algorithm in reinforcement learning to improve the algorithm’s performance. Under complex meteorological conditions, improved reinforcement learning is applied to the dynamic ship offshore channel simulation environment to verify the algorithm’s performance.}
}
@article{ZHANG2021117131,
title = {Testbed implementation of reinforcement learning-based demand response energy management system},
journal = {Applied Energy},
volume = {297},
pages = {117131},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117131},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921005705},
author = {Xiongfeng Zhang and Renzhi Lu and Junhui Jiang and Seung Ho Hong and Won Seok Song},
keywords = {Reinforcement learning (RL), Demand response (DR), Energy management, Practical implementation},
abstract = {Demand response (DR) has been acknowledged as an effective method to improve the stability, and financial efficiency of power grids. During operation of a DR program, there are usually multiple interactions among different grid entities, which complicates decision-making processes with respect to grid operations. Recently, reinforcement learning (RL) has attracted increasing attention for managing complex decision-making problems, owing to its self-learning capacity. Several theoretical RL-based approaches have been proposed for addressing various DR issues, but the practical feasibility of these theoretical approaches remains to be proven. In this paper, a conceptual architecture is firstly proposed to support DR management of a diversified facility in the context of a price-based DR environment. Secondly, exhaustive guidelines are provided to illustrate how to implement a multi-agent RL-based algorithm in the constructed DR management system. Afterwards, a laboratory-level testbed was set up to evaluate the effectiveness of the deployed DR algorithm. The experimental evaluation results show that the RL-based DR algorithm takes about 20s and 50 episodes to achieve optimal load control policy. By executing the optimal operation policy, the overall energy consumption during the highest price period (i.e., 15:00–18:00) is significantly reduced by 133.6% compared with the lowest price period (i.e., 02:00–05:00).}
}
@article{JIA2021119148,
title = {A reinforcement learning based blade twist angle distribution searching method for optimizing wind turbine energy power},
journal = {Energy},
volume = {215},
pages = {119148},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.119148},
url = {https://www.sciencedirect.com/science/article/pii/S0360544220322556},
author = {Liangyue Jia and Jia Hao and John Hall and Hamid Khakpour Nejadkhaki and Guoxin Wang and Yan Yan and Mengyuan Sun},
keywords = {Wind turbine blade, Blade design, Twist angle distribution, Optimization method, Reinforcement learning, Rapid optimization},
abstract = {The twist angle distribution (TAD) of a wind turbine blade determines its efficiency in terms of electricity production. As the blade is normally deployed in dynamic wind environments where wind speed varies widely, it is crucial to search the optimal TAD for different wind speeds in the design process. Due to the need to call a large number of complex simulators, traditional methods for optimal TAD searching are inefficient and time-consuming, which hinders the blade design process. Hence, this work presents a reinforcement learning-based method for searching optimal TAD efficiently, which is named RL-TAD. The fundamental idea of RL-TAD is to learn the TAD searching policy by an agent and reuse this agent to search optimal TAD under different wind speeds. This idea divided RL-TAD from the commonly used genetic algorithm-based methods, which only output the TAD and ignore reusing the searching policy. The RL-TAD includes the offline stage and online stage. In the offline stage, the RL-TAD learns the policy by reinforcement learning, in which the environment is constructed by a surrogate model, and a new reward policy is developed by integrating design experiences. Then in the online stage, the trained agent can be deployed to search TAD for different wind speeds. To verify the proposed RL-TAD, a case study is detailed. The empirical results show that the RL-TAD can converge to the optimal TAD at a speed of 3–5 times faster than the genetic algorithm-based method in the offline stage and also a better wind energy power coefficient is obtained. Besides, the response time is less than 0.1 s when using the trained agent to search the TAD, which proofs its potential to be used in rapid optimal TAD searching. Further, the rapid optimal TAD searching ability can support the real-time control of the wind turbine blade.}
}
@article{ZHANG2022115340,
title = {A multi-agent deep reinforcement learning approach enabled distributed energy management schedule for the coordinate control of multi-energy hub with gas, electricity, and freshwater},
journal = {Energy Conversion and Management},
volume = {255},
pages = {115340},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.115340},
url = {https://www.sciencedirect.com/science/article/pii/S0196890422001364},
author = {Guozhou Zhang and Weihao Hu and Di Cao and Zhenyuan Zhang and Qi Huang and Zhe Chen and Frede Blaabjerg},
keywords = {Multi-energy hub, Cost reduction, Distributed energy scheduling policy, Attention mechanism, Multi-agent deep reinforcement learning},
abstract = {In recent years, due to the deeply concerns on environment protection, the production, transformation and utilization of the energy sources with a more efficient and various way has become an important research topic. Under this background, this paper designs a renewable energy powered multi-energy hub system with gas, electricity, and freshwater sub-system. To enhance the flexibility and reduce the total cost of such system, the energy management of the multi-energy hub is formed as multi-agent cooperative control, and several targets, including operational costs, environment cost, and maintenance cost are considered along with the constraints. Subsequently, a novel attention mechanism-based multi-agent deep reinforcement learning algorithm is applied, where multi-agents are centrally trained to obtain the coordinate energy management strategy while being executed in a decentralized manner to provide the dispatch instruction for each energy hub with only local states. Finally, the effectiveness of the proposed method is investigated in the study system and the simulation results show that it can reduce the total cost by up to 7.28% compared with the other benchmark methods.}
}
@article{WANG2022108668,
title = {Erlang planning network: An iterative model-based reinforcement learning with multi-perspective},
journal = {Pattern Recognition},
volume = {128},
pages = {108668},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108668},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322001492},
author = {Jiao Wang and Lemin Zhang and Zhiqiang He and Can Zhu and Zihui Zhao},
keywords = {Model-based reinforcement learning, Multi-perspective, Bi-level, Planning, Trajectory imagination},
abstract = {For model-based reinforcement learning (MBRL), one of the key challenges is modeling error, which cripples the effectiveness of model planning and causes poor robustness during training. In this paper, we propose a bi-level Erlang Planning Network (EPN) architecture, which is composed of an upper-level agent and several multi-scale parallel sub-agents, trained in an iterative way. The proposed method focuses upon the expansion of representation by environment: a multi-perspective over the world model, which presents a varied way to represent an agent’s knowledge about the world that alleviates the problem of falling into local optimal points and enhances robustness during the progress of model planning. Moreover, our experiments evaluate EPN on a range of continuous-control tasks in MuJoCo, the evaluation results show that the proposed framework finds exemplar solutions faster and consistently reaches the state-of-the-art performance.}
}
@article{CONRADIE20011277,
title = {Neurocontrol of a ball mill grinding circuit using evolutionary reinforcement learning},
journal = {Minerals Engineering},
volume = {14},
number = {10},
pages = {1277-1294},
year = {2001},
issn = {0892-6875},
doi = {https://doi.org/10.1016/S0892-6875(01)00144-3},
url = {https://www.sciencedirect.com/science/article/pii/S0892687501001443},
author = {A.V.E. Conradie and C. Aldrich},
keywords = {Grinding, process control, artificial intelligence},
abstract = {A ball mill grinding circuit is a nonlinear system characterised by significant controller interaction between the manipulated variables. A rigorous ball mill grinding circuit is simulated and used in its entirety for the development of a neurocontroller through the use of evolutionary reinforcement learning. Reinforcement learning entails learning to achieve a desired control objective from direct cause—effect interactions with a simulated process plant. The SANE (symbiotic adaptive neuro-evolution) algorithm is able to learn implicitly to eliminate controller interactions in the grinding circuit by taking a plant wide approach to controller design. The ability of the neurocontroller to maintain high performance in the presence of large disturbances in feed particle size distribution and ore hardness variations is demonstrated. The generalisation afforded by the SANE algorithm in dealing with considerable uncertainty in its operating environment attests to a large degree of controller autonomy.}
}
@article{QIU2022118403,
title = {Safe reinforcement learning for real-time automatic control in a smart energy-hub},
journal = {Applied Energy},
volume = {309},
pages = {118403},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.118403},
url = {https://www.sciencedirect.com/science/article/pii/S030626192101638X},
author = {Dawei Qiu and Zihang Dong and Xi Zhang and Yi Wang and Goran Strbac},
keywords = {Multi-energy system, Energy hub, Safe reinforcement learning, Carbon emission, Renewable energy},
abstract = {Nowadays, multi-energy systems are receiving special attention from smart grid community owing to their high flexibility potentials integrating with multiple energy carriers. In this regard, energy hub is known as a flexible and efficient platform to supply energy demands with an acceptable range of affordability and reliability by relying on various energy production, storage and conversion facilities. Given the increasing penetration of renewable energy sources to promote a low-carbon energy transition, accurate economic and environmental assessment of energy hub, along with the real-time automatic energy management scheme has become a challenging task due to the high variability of renewable energy sources. Furthermore, the conventional model-based optimization approach requiring full knowledge of the employed mathematical operating models and accurate uncertainty distributions may become impractical for real-world applications. In this context, this paper proposes a model-free safe deep reinforcement learning method for the optimal control of a renewable-based energy hub operating in multiple energy carries while satisfying the physical constraints within the energy hub operation model. The main objective of this work is to minimize the system energy cost and carbon emission by considering various energy components. The proposed deep reinforcement learning method is trained and tested on a real-world dataset to validate its superior performance in reducing energy cost, carbon emission, and computational time with respect to the state-of-the-art deep reinforcement learning and optimized-based approaches. Moreover, the effectiveness of the proposed method in dealing with model operation constraints is evaluated on both training and test environments. Finally, the generalization performance for the learnt energy management scheme as well as the sensitivity analysis on storage flexibility and carbon price are also examined in the case studies.}
}
@article{WU2022124657,
title = {Strategic bidding in a competitive electricity market: An intelligent method using Multi-Agent Transfer Learning based on reinforcement learning},
journal = {Energy},
volume = {256},
pages = {124657},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124657},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222015602},
author = {Jiahui Wu and Jidong Wang and Xiangyu Kong},
keywords = {Intelligent bidding strategy, Electricity market, Multi-agent simulation, Transfer learning, Reinforcement learning},
abstract = {The electricity market will tend to be diverse and competitive to realize Carbon Neutrality goals under Energy Internet. Moreover, bidding strategies and methods are essential for the stable and benign operation of the electricity market. With the development of artificial intelligence and computer simulation technology, multi-agent simulation has gradually become a significant method for electricity market bidding. Among them, Multi-Agent Reinforcement Learning (MARL) can help agents adapt to changing environments. In contrast, Multi-Agent Transfer Learning (MATL) can help agents learn from not only the target task but also other similar tasks. This paper proposes an intelligent strategic bidding theoretical framework in a competitive electricity market using MATL based on MARL and studies four MATL algorithms, including RNN, LSTM, GRU and BGRU. An intelligent bidding simulation model based on the four MATL algorithms is established, and the performance of the intelligent bidding simulation model in the electricity market using the four MATL algorithms based on the MARL Q-learning algorithm is compared and analyzed from the perspective of accuracy and convergence speed. And based on the multi-agent simulation model, examples of bidding strategies are carried out to verify the rationality and effectiveness of the intelligent bidding method using MATL based on MARL.}
}
@article{WU2024103313,
title = {Collaborative caching relay algorithm based on recursive deep reinforcement learning in mobile vehicle edge network},
journal = {Ad Hoc Networks},
volume = {152},
pages = {103313},
year = {2024},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2023.103313},
url = {https://www.sciencedirect.com/science/article/pii/S1570870523002330},
author = {Honghai Wu and Baibing Wang and Huahong Ma and Ling Xing},
keywords = {Mobile vehicle edge network, Collaborative Cache Relay, Recursive Deep Reinforcement Learning},
abstract = {With the rapid development of Internet of vehicles (IoV) and the continuous emergence of vehicle information applications, the demand for content in vehicle networking is growing at an alarming speed. Mobile vehicular edge caching is regarded as a promising technology in improving Quality of Service (QoS) and reducing latency. Many caching algorithms have been proposed, which usually place contents in the Road Side Units (RSUs) to provide services to users near them. However, due to the high-speed movement of vehicles and limitations of RSU coverage, caching interrupts often occur frequently, leading to a deterioration in service quality. To deal with this problem, we make full use of Vehicle-to-Vehicle (V2V) collaboration to construct a caching system which does not require RSU support, and propose a Recursive Deep Reinforcement Learning based Collaborative Caching Relay strategy (RDRL-CR). On purpose to minimize the service delay under capacity constraints, the caching problem is formulated as an integer linear programming problem, and caching decisions are achieved through partially observable Markov Decision Process (MDP). Specifically, this strategy utilizes a Graph Neural Network (GNN) to predict vehicle trajectories, and then selects vehicles that can serve as caching nodes by calculating link stability metrics between vehicles. The Long Short Term Memory (LSTM) network is embedded into a deep deterministic strategy gradient algorithm to achieve the final caching decision. Compared with existing caching strategies, the proposed caching strategy in this paper improves the caching hit rate by about 25% and reduces content access latency by about 20%.}
}
@article{WEGENER2021102967,
title = {Automated eco-driving in urban scenarios using deep reinforcement learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {126},
pages = {102967},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.102967},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X2100005X},
author = {Marius Wegener and Lucas Koch and Markus Eisenbarth and Jakob Andert},
keywords = {Eco-driving, Deep reinforcement learning, Connected vehicles, Automated driving, Electric vehicles},
abstract = {Urban settings are challenging environments to implement eco-driving strategies for automated vehicles. It is often assumed that sufficient information on the preceding vehicle pulk is available to accurately predict the traffic situation. Because vehicle-to-vehicle communication was introduced only recently, this assumption will not be valid until a sufficiently high penetration of the vehicle fleet has been reached. Thus, in the present study, we employed Reinforcement Learning (RL) to develop eco-driving strategies for cases where little data on the traffic situation are available. An A-segment electric vehicle was simulated using detailed efficiency models to accurately determine its energy-saving potential. A probabilistic traffic environment featuring signalized urban roads and multiple preceding vehicles was integrated into the simulation model. Only information on the traffic light timing and minimal sensor data were provided to the control algorithm. A twin-delayed deep deterministic policy gradient (TD3) agent was implemented and trained to control the vehicle efficiently and safely in this environment. Energy savings of up to 19% compared with a simulated human driver and up to 11% compared with a fine-tuned Green Light Optimal Speed Advice (GLOSA) algorithm were determined in a probabilistic traffic scenario reflecting real-world conditions. Overall, the RL agents showed a better travel time and energy consumption trade-off than the GLOSA reference.}
}