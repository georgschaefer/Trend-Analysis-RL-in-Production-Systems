@article{ORHEAN2018292,
title = {New scheduling approach using reinforcement learning for heterogeneous distributed systems},
journal = {Journal of Parallel and Distributed Computing},
volume = {117},
pages = {292-302},
year = {2018},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517301521},
author = {Alexandru Iulian Orhean and Florin Pop and Ioan Raicu},
keywords = {Scheduling, Distributed systems, Machine learning, SARSA},
abstract = {Computer clusters, cloud computing and the exploitation of parallel architectures and algorithms have become the norm when dealing with scientific applications that work with large quantities of data and perform complex and time-consuming calculations. With the rise of social media applications and smart devices, the amount of digital data and the velocity at which it is produced have increased exponentially, determining the development of distributed system frameworks and platforms that increase productivity, consistency, fault-tolerance and security of parallel applications. The performance of such systems is mainly influenced by the architectural disposition and composition of the physical machines, the resource allocation and the scheduling of jobs and tasks. This paper proposes a reinforcement learning algorithm to solve the scheduling problem in distributed systems. The machine learning technique takes into consideration the heterogeneity of the nodes and their disposition within the grid, and the arrangement of tasks in a directed acyclic graph of dependencies, ultimately determining a scheduling policy for a better execution time. This paper also proposes a platform, in which the algorithm is implemented, that offers scheduling as a service to distributed systems.}
}
@article{DUONG202240,
title = {Stochastic intervention for causal inference via reinforcement learning},
journal = {Neurocomputing},
volume = {482},
pages = {40-49},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.01.086},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222001072},
author = {Tri Dung Duong and Qian Li and Guandong Xu},
keywords = {Stochastic intervention effect, Treatment effect estimation, Causal inference},
abstract = {Causal inference methods are widely applied in various decision-making domains such as precision medicine, optimal policy and economics. The main focus of causal inference is the treatment effect estimation of intervention strategies, such as changes in drug dosing and increases in financial aid. Existing methods are mostly restricted to the deterministic treatment and compare outcomes under different treatments. However, they are unable to address the substantial recent interests of treatment effect estimation under stochastic intervention, e.g., “how all units health status change if they adopt 50% dose reduction”. In other words, they lack the capability of addressing fine-grained treatment effect estimation to empower the decision-making applications. In this paper, we advance the causal inference research by proposing a new effective framework to estimate the treatment effect under the stochastic intervention. Particularly, we develop a stochastic intervention effect estimator (SIE) based on nonparametric influence function, with the theoretical guarantees of robustness and fast convergence rates. Additionally, we construct a customised reinforcement learning algorithm based on the random search solver which can effectively find the optimal policy to produce the greatest expected outcomes for the decision-making process. Finally, we conduct extensive empirical experiments to validate that our framework can achieve superior performance in comparison with state-of-the-art baselines. For reproducing experimental results, all the source codes and data are available at https://github.com/tridungduong16/Interpretable-Machine-Learning/tree/master/Source%20Code.}
}
@article{BEKIROS20101153,
title = {Heterogeneous trading strategies with adaptive fuzzy Actor–Critic reinforcement learning: A behavioral approach},
journal = {Journal of Economic Dynamics and Control},
volume = {34},
number = {6},
pages = {1153-1170},
year = {2010},
issn = {0165-1889},
doi = {https://doi.org/10.1016/j.jedc.2010.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0165188910000163},
author = {Stelios D. Bekiros},
keywords = {Agent-based modeling, Technical trading, Reinforcement learning, Fuzzy inference, Bounded rationality},
abstract = {The present study addresses the learning mechanism of boundedly rational agents in the dynamic and noisy environment of financial markets. The main objective is the development of a system that “decodes” the knowledge-acquisition strategy and the decision-making process of technical analysts called “chartists”. It advances the literature on heterogeneous learning in speculative markets by introducing a trading system wherein market environment and agent beliefs are represented by fuzzy inference rules. The resulting functionality leads to the derivation of the parameters of the fuzzy rules by means of adaptive training. In technical terms, it expands the literature that has utilized Actor–Critic reinforcement learning and fuzzy systems in agent-based applications, by presenting an adaptive fuzzy reinforcement learning approach that provides with accurate and prompt identification of market turning points and thus higher predictability. The purpose of this paper is to illustrate this concretely through a comparative investigation against other well-established models. The results indicate that with the inclusion of transaction costs, the profitability of the novel system in case of NASDAQ Composite, FTSE100 and NIKKEI255 indices is consistently superior to that of a Recurrent Neural Network, a Markov-switching model and a Buy and Hold strategy. Overall, the proposed system via the reinforcement learning mechanism, the fuzzy rule-based state space modeling and the adaptive action selection policy, leads to superior predictions upon the direction-of-change of the market.}
}
@article{RS2022104485,
title = {DeepNR: An adaptive deep reinforcement learning based NoC routing algorithm},
journal = {Microprocessors and Microsystems},
volume = {90},
pages = {104485},
year = {2022},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2022.104485},
url = {https://www.sciencedirect.com/science/article/pii/S0141933122000497},
author = {Reshma Raj R.S. and Rohit R. and Mushrif Shaikh Shahreyar and Akash Raut and Pournami P.N. and Saidalavi Kalady and Jayaraj P.B.},
keywords = {Network-on-Chip, Routing algorithm, Deep reinforcement learning, Packet latency, Throughput},
abstract = {Network-on-Chip (NoC) has become a cost-effective communication interconnect for Tiled Chip Multicore Processor systems. The communication between cores is done through packet exchange. As the computational intensity of applications increases, the amount of packet exchange between cores will also increase. The improper routing of these packets will result in high congestion thereby degrading the system performance. This marks the need for congestion-aware routing in NoC. In the real world, the applications running in NoC create diverse traffic, which in turn creates challenges in routing. Such challenges have resulted in more researchers relying on machine learning algorithms to tackle them. However, the issues pertaining to storage overhead and packet latency prevail in such methodologies. This paper presents an adaptive routing algorithm DeepNR, which uses a deep reinforcement learning approach. The proposed approach considers network information for state representation, routing directions for actions, and queuing delay for reward function. Experiments carried out on synthetic as well as real-time traffics to demonstrate the effectiveness and efficiency of DeepNR using the Gem5 simulator. The results obtained for DeepNR indicate a reduction of up to 21.25% and 44% in overall packet latency under high traffic conditions on real and synthetic traffic respectively, as compared to the existing approaches. Also, DeepNR achieves a throughput of above 90% in both the traffic scenarios.}
}
@article{ALHAZMI2022130993,
title = {A reinforcement learning-based economic model predictive control framework for autonomous operation of chemical reactors},
journal = {Chemical Engineering Journal},
volume = {428},
pages = {130993},
year = {2022},
issn = {1385-8947},
doi = {https://doi.org/10.1016/j.cej.2021.130993},
url = {https://www.sciencedirect.com/science/article/pii/S1385894721025766},
author = {Khalid Alhazmi and Fahad Albalawi and S. Mani Sarathy},
keywords = {Reinforcement learning, Parameter estimation, Model predictive control, Process optimization},
abstract = {Economic model predictive control (EMPC) is a promising methodology for optimal operation of dynamical processes that has been shown to improve process economics considerably. However, EMPC performance relies heavily on the accuracy of the process model used. As an alternative to model-based control strategies, reinforcement learning (RL) has been investigated as a model-free control methodology, but issues regarding its safety and stability remain an open research challenge. This work presents a novel framework for integrating EMPC and RL for online model parameter estimation of a class of nonlinear systems. In this framework, EMPC optimally operates the closed loop system while maintaining closed loop stability and recursive feasibility. At the same time, to optimize the process, the RL agent continuously compares the measured state of the process with the model’s predictions (nominal states), and modifies model parameters accordingly. The major advantage of this framework is its simplicity; state-of-the-art RL algorithms and EMPC schemes can be employed with minimal modifications. The performance of the proposed framework is illustrated on a network of reactions with challenging dynamics and practical significance. This framework allows control, optimization, and model correction to be performed online and continuously, making autonomous reactor operation more attainable.}
}
@article{YERUDKAR2023374,
title = {Sampled-data Control of Probabilistic Boolean Control Networks: A Deep Reinforcement Learning Approach},
journal = {Information Sciences},
volume = {619},
pages = {374-389},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522013196},
author = {Amol Yerudkar and Evangelos Chatzaroulas and Carmen {Del Vecchio} and Sotiris Moschoyiannis},
keywords = {Sampled-data control (SDC), Probabilistic Boolean control networks (PBCNs), Markov decision processes (MDPs), Deep reinforcement learning, Gene regulatory networks (GRNs)},
abstract = {The rise of reinforcement learning (RL) has guided a new paradigm: unraveling the intervention strategies to control systems with unknown dynamics. Model-free RL provides an exhaustive framework to devise therapeutic methods to alter the regulatory dynamics of gene regulatory networks (GRNs). This paper presents an RL-based technique to control GRNs modeled as probabilistic Boolean control networks (PBCNs). In particular, a double deep-Q network (DDQN) approach is proposed to address the sampled-data control (SDC) problem of PBCNs, and optimal state feedback controllers are obtained, rendering the PBCNs stabilized at a given equilibrium point. Our approach is based on options, i.e., the temporal abstractions of control actions in the Markov decision processes (MDPs) framework. First, we define options and hierarchical options and give their properties. Then, we introduce multi-time models to compute the optimal policies leveraging the options framework. Furthermore, we present a DDQN algorithm: i) to concurrently design the feedback controller and the sampling period; ii) wherein the controller intelligently decides the sampled period to update the control actions under the SDC scheme. The presented method is model-free and offers scalability, thereby providing an efficient way to control large-scale PBCNs. Finally, we compare our control policy with state-of-the-art control techniques and validate the presented results.}
}
@article{LIU202063,
title = {Heterogeneous formation control of multiple UAVs with limited-input leader via reinforcement learning},
journal = {Neurocomputing},
volume = {412},
pages = {63-71},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.06.040},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220310110},
author = {Hao Liu and Qingyao Meng and Fachun Peng and Frank L. Lewis},
keywords = {Formation control, Multi-agent systems, Heterogeneous systems, Reinforcement learning, Unmanned aerial vehicles},
abstract = {In this brief, a distributed optimal control method via reinforcement learning is proposed to address the heterogeneous unmanned aerial vehicle (UAV) formation trajectory tracking problem. The UAV formation is composed of a virtual leader with limited nonzero input and several follower vehicles with different unknown dynamics. The proposed control law contains a distributed observer and a model-free off-policy reinforcement learning (RL) protocol. The distributed optimal trajectory tracking problem is formulated for the heterogeneous formation system. A RL algorithm is designed to obtain the optimal control input online without any knowledge of the followers’ dynamics. Simulation example illustrates the effectiveness of the proposed method.}
}
@article{CANNON2019130a,
title = {Prediction of Metabolite Concentrations, Rate Constants and Post-Translational Regulation of Neurospora Crassa using Maximum Entropy Optimizations and Reinforcement Learning},
journal = {Biophysical Journal},
volume = {116},
number = {3, Supplement 1},
pages = {130a-131a},
year = {2019},
issn = {0006-3495},
doi = {https://doi.org/10.1016/j.bpj.2018.11.724},
url = {https://www.sciencedirect.com/science/article/pii/S0006349518319891},
author = {William R. Cannon and Samuel R. Britton and Mikahl Banwarth-Kuhn and Mark Alber and Jennifer M. Hurley and Meaghan S. Jankowski and Jeremy D. Zucker and Douglas J. Baxter and Neeraj Kumar and Scott E. Baker and Jay C. Dunlap}
}
@article{ZHANG2023128451,
title = {Graph attention reinforcement learning with flexible matching policies for multi-depot vehicle routing problems},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {611},
pages = {128451},
year = {2023},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2023.128451},
url = {https://www.sciencedirect.com/science/article/pii/S0378437123000067},
author = {Ke Zhang and Xi Lin and Meng Li},
keywords = {Multi-depot, Reinforcement learning, Graph attention, Soft time window, Multi-agent},
abstract = {Multi-depot vehicle routing problem with soft time windows (MD-VRPSTW) is a valuable practical issue in urban logistics. However, heuristic methods may fail to generate high-quality solutions for massive problems instantly. Thus, this paper presents a novel reinforcement learning algorithm integrated with graph attention network (GAT-RL) to efficiently solve the problem. This method utilizes the encoder–decoder architecture to produce routes for vehicles starting from different depots iteratively. The encoder architecture employs graph attention network to mine the complex spatial–temporal correlations within time windows. Then, the decoder architecture designs fixed-order and full-pair matching policies to generate solutions. After off-line training, experiments show that this approach consistently outperforms Google OR-Tools with negligible computational time. Particularly, the robustness of the pre-trained model is validated under multiple sources of variations and uncertainties, including customer/depot numbers, vehicle capacities, and en-route traffic conditions.}
}
@incollection{LIN1993182,
title = {Scaling Up Reinforcement Learning for Robot Control},
booktitle = {Machine Learning Proceedings 1993},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {182-189},
year = {1993},
isbn = {978-1-55860-307-3},
doi = {https://doi.org/10.1016/B978-1-55860-307-3.50030-7},
url = {https://www.sciencedirect.com/science/article/pii/B9781558603073500307},
author = {Long-Ji Lin},
abstract = {The aim of this research is to extend the state of the art of reinforcement learning and enable its applications to complex robot- learning problems. This paper presents a series of scaling-up extensions to reinforcement learning, including: generalization by neural networks, using action models, teaching, hierarchical learning, and having a short-term memory. These extensions have been tested in a physically-realistic robot simulator, and combined to solve a complex robot-learning problem. Simulation results indicate that each of the extensions could result in either significant learning speedup or new capabilities. This research concludes that it is possible to build artificial agents that can acquire complex control policies effectively by reinforcement learning.}
}
@article{ZHOU2023104033,
title = {Scalable multi-region perimeter metering control for urban networks: A multi-agent deep reinforcement learning approach},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {148},
pages = {104033},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2023.104033},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X23000220},
author = {Dongqin Zhou and Vikash V. Gayah},
keywords = {Macroscopic Fundamental Diagram (MFD), Multi-region perimeter metering control, Model-free multi-agent reinforcement learning (MARL)},
abstract = {Perimeter metering control based on macroscopic fundamental diagrams has attracted increasing research interests over the past decade. This strategy provides a convenient way to mitigate urban congestion by manipulating vehicular movements across homogeneous regions without modeling the detailed behaviors and interactions involved with individual vehicle presence. In particular, multi-region perimeter metering control holds promise for efficient traffic management in large-scale urban networks. However, most existing methods for multi-region control require knowledge of either the environment traffic dynamics or network properties (i.e., the critical accumulations), whereas such information is generally difficult to obtain and subject to significant estimation errors. The recently developed model-free techniques, on the other hand, have not yet been shown scalable or applicable to large urban networks. To fill this gap, this paper proposes a scalable model-free scheme based on multi-agent deep reinforcement learning. The proposed scheme features value function decomposition in the paradigm of centralized training with decentralized execution, coupled with critical advances of single-agent deep reinforcement learning and problem reformulation guided by domain expertise. Comprehensive experiment results on a seven-region urban network suggest the scheme is: (a) effective, with consistent convergence to final control outcomes that are comparable to the model predictive control method; (b) resilient, with superior learning and control efficacy in the presence of inaccurate input information from the environment; and (c) transferable, with sufficient implementation prospect as well as real time applicability to unencountered environments featuring increased uncertainty.}
}
@article{CHEN2021106838,
title = {A reinforcement learning approach to irrigation decision-making for rice using weather forecasts},
journal = {Agricultural Water Management},
volume = {250},
pages = {106838},
year = {2021},
issn = {0378-3774},
doi = {https://doi.org/10.1016/j.agwat.2021.106838},
url = {https://www.sciencedirect.com/science/article/pii/S0378377421001037},
author = {Mengting Chen and Yuanlai Cui and Xiaonan Wang and Hengwang Xie and Fangping Liu and Tongyuan Luo and Shizong Zheng and Yufeng Luo},
keywords = {Reinforcement learning, Irrigation decision-making, Paddy rice irrigation, Weather forecasts},
abstract = {Improving efficiency with the use of rainfall is one of the effective ways to conserve water in agriculture. At present, weather forecasting can be used to potentially conserve irrigation water, but the risks of unnecessary irrigation and the yield loss due to the uncertainty of weather forecasts should be avoided. Thus, a deep Q-learning (DQN) irrigation decision-making strategy based on short-term weather forecasts was proposed to determine the optimal irrigation decision. The utility of the method is demonstrated for paddy rice grown in Nanchang, China. The short-term weather forecasts and observed meteorological data of the paddy rice growth period from 2012 to 2019 were collected from stations near Nanchang. Irrigation was decided for two irrigation decision-making strategies, namely, conventional irrigation (i.e., flooded irrigation commonly used by local farmers) and DQN irrigation, and their performance in water conservation was evaluated. The results showed that the daily rainfall forecasting performance was acceptable, with potential space for learning and exploitation. The DQN irrigation strategy had strong generalization ability after training and can be used to make irrigation decisions using weather forecasts. In our case, simulation results indicated that compared with conventional irrigation decisions, DQN irrigation took advantage of water conservation from unnecessary irrigation, resulting in irrigation water savings of 23 mm and reducing drainage by 21 mm and irrigation timing by 1.0 times on average, without significant yield reduction. The DQN irrigation strategy of learning from past irrigation experiences and the uncertainties in weather forecasts avoided the risks of imperfect weather forecasting.}
}
@article{SWAIN2023100605,
title = {A reinforcement learning-based cluster routing scheme with dynamic path planning for mutli-UAV network},
journal = {Vehicular Communications},
volume = {41},
pages = {100605},
year = {2023},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2023.100605},
url = {https://www.sciencedirect.com/science/article/pii/S2214209623000359},
author = {Sipra Swain and Pabitra Mohan Khilar and Biswa Ranjan Senapati},
keywords = {Area coverage, Clustering, Path planning, Reinforcement learning, Routing, UAV},
abstract = {Unmanned Aerial Vehicles (UAVs) with visual sensors are widely used for area mapping, management of crops and traffic, rescuing lives, and many other applications that need to cover a large area. The process of coverage can be improved with the use of efficient path planning and data transfer algorithms. Numerous research studies have been performed by concentrating on each of the aforementioned elements separately. However, to tackle the rapidly changing environmental situations, this paper proposes a cluster-based routing approach by incorporating a dynamic planning algorithm. The proposed model is composed of four modules, such as an online path planning algorithm, clustering-based network topology construction, reinforcement learning-based cluster management, and a data routing mechanism. Firstly, to maximise the coverage output, an optimal set of waypoints has been generated for all UAVs. For each UAV to complete the coverage task, it needs to completely cover its own set of waypoints. Since the environment is changing, a static path planning approach might fail to achieve complete coverage. Therefore, to drive the mission without getting stuck, a dynamic path planning approach is proposed that decides the next waypoint for a UAV based on the current waypoint. The main purpose of the algorithm is to cover all the waypoints in a polynomial amount of time. Secondly, the topology construction module consists of the initialization process, cluster head election, and cluster formation. Based on five parameters such as degree of centrality, surplus energy, link stability time, connectivity with the backbone UAV, and velocity, a set of cluster heads are selected. Then the cluster management process is performed by the optimal re-clustering policy determined using an approach of reinforcement learning called State Action Reward State Action (SARSA) in ground control station. Finally, the introduction of inter-cluster forwarders and selective flooding of route requests makes the routing scheme enhance the packet delivery ratio and reduce the delay. The result shows that the proposed work performs better than the existing result in terms of different generic performance metrics used for path planning and routing.}
}
@article{ZHANG2022109281,
title = {Multimodal feature fusion and exploitation with dual learning and reinforcement learning for recipe generation},
journal = {Applied Soft Computing},
volume = {126},
pages = {109281},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109281},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622004781},
author = {Mengyang Zhang and Guohui Tian and Huanbing Gao and Shaopeng Liu and Ying Zhang},
keywords = {Visual question answering, Dual learning, Recipe generation, Reinforcement learning, Logic},
abstract = {Recipes belong to long paragraphs with a cooking logic. To recipes from images and food names is more challenging in VQA (Visual Question Answering) due to the gap between images and texts. Although multimodal feature fusion, as a typical solver in VQA, is adopted in most situations for enhancing the accuracy, fused features obtained in this way can hardly provide guidance for keeping logic in produced texts. In this paper, ingredients are introduced to enhancing the relationship between food images and recipes, since they can reflect the cooking logic to a great extent, and dual learning is adopted to provide a complementary view by reconstructing ingredients from produced recipes. In order to make a full exploitation of ingredients for producing effective recipes, ingredients are fused into images and food names with an attention mechanism in the forward flow, and in the backward flow, a reconstructor is designed to reproduce ingredients from recipes. In addition, reinforcement learning is employed to guide ingredient reconstruction for preserving effective features in fused information explicitly. Extensive experiments demonstrate that more attention is allocated for producing effective recipes, and ablative study shows the reasonability of different components in the proposed method.}
}
@article{MONFAREDI2023109292,
title = {Multi-agent deep reinforcement learning-based optimal energy management for grid-connected multiple energy carrier microgrids},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {153},
pages = {109292},
year = {2023},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2023.109292},
url = {https://www.sciencedirect.com/science/article/pii/S0142061523003496},
author = {Farzam Monfaredi and Hossein Shayeghi and Pierluigi Siano},
keywords = {Multi-agent deep reinforcement learning, Smart grid, Multiple energy carrier microgrids, System optimization},
abstract = {Multi-agent deep reinforcement learning (MA-DRL) method provides a groundbreaking approach to tackling computational problems in power systems, particularly for distributed energy resources that have been widely adopted to advance energy sustainability. This paper presents a novel optimal energy management based on proposed MA-DRL method. This method employs deep neural network to learn strategy based on stacked-denoising auto-encoders and multi-agent deep deterministic policy gradient learning capability. The MA-DRL method is adopted to find the optimal strategy of the optimal energy management problem under the Markov decision process framework. This method aims to coordinate multiple energies and achieve optimal operation over a variety of hourly dispatches while taking into account the distinct properties of electric and thermal energies. The primary challenge of the planning and operation of multiple energy carrier microgrids (MECMs) is determining the optimal interaction between renewable energy resources, energy storage systems, power-to-thermal conversion systems, and upstream power grid in order to improve overall energy utilization efficiency. The presented robust method can adaptively derive the optimal operation for MECMs through centralized learning and decentralized implementation. The optimization problem is employed in this study to concurrently reduce the total emissions and the operating costs while considering engineering design constraints. Finally, to demonstrate the efficiency of the proposed method, it is verified on an integrated modified IEEE 33-bus and 8-node gas systems.}
}
@article{XU2022109218,
title = {UAV-assisted task offloading for IoT in smart buildings and environment via deep reinforcement learning},
journal = {Building and Environment},
volume = {222},
pages = {109218},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.109218},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322004541},
author = {Jiajie Xu and Dejuan Li and Wei Gu and Ying Chen},
keywords = {Smart buildings and environment, Internet of things (IoT), Task offloading, Unmanned aerial vehicle (UAV), Deep reinforcement learning (DRL)},
abstract = {With the rapid development of Internet of Things (IoT) techniques, IoT devices with sensors have been widely deployed and used in smart buildings and environment, and the application scenarios of IoT in smart buildings and environment have been further extended. However, due to the limitations of computation, storage and battery capacity, IoT devices cannot process all the tasks locally by themselves and need to offload some tasks to edge servers typically deployed in base stations (BSs). Besides, unmanned aerial vehicles (UAVs) with controllable mobility and flexibility have been recognized as a promising solution to assist communication in emergency scenarios. In this paper, we investigate UAV-assisted offloading for IoT in smart buildings and environment. We formulate the offloading problem with the goal of minimizing the long-term energy consumption and minimizing the queue length at the same time. As the solution space size is extremely large and the offloading problem focuses on the long-term optimization goal, solving this problem faces several challenges. To address these challenges, we reformulate it as a Markov decision process (MDP)-based offloading problem and propose the UAV-assisted task offloading (UTO) approach based on deep reinforcement learning (DRL) techniques. Our UTO approach can cope well with the challenges brought by high-dimensional and consecutive state and action space. We carry out a series of comparison experiments with both DRL and non-DRL algorithms, and the results validate the performance of our proposed UTO approach.}
}
@article{LIU2023102079,
title = {Spatial–temporal graph neural network traffic prediction based load balancing with reinforcement learning in cellular networks},
journal = {Information Fusion},
pages = {102079},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102079},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523003950},
author = {Shang Liu and Miao He and Zhiqiang Wu and Peng Lu and Weixi Gu},
keywords = {Cellular networks, Load balancing, Traffic prediction, Graph neural network, Reinforcement learning},
abstract = {Balancing network traffic among base stations poses a primary challenge for mobile operators because of the escalating demand for enhanced data speeds in large-scale 5G radio applications. Within cellular networks, traffic flow prediction constitutes a pivotal issue in numerous applications, such as resource allocation, load balancing, and network slicing. In this paper, traffic prediction based load balancing framework with reinforcement learning is proposed to optimize neighbor cell relational parameters that can better balance traffic within a defined geographical cluster. Spatial–temporal-event cross attention graph convolution neural network (STECA-GCN) is put forward to predict the precise traffic flow. The model takes event dimension features into account, while also incorporating direct cross-fusion among diverse features. Concurrently, we have developed a strategy based on deep reinforcement learning to facilitate dynamic load balancing decisions. Simulation results show that our proposed load balancing framework can improve overall system performance. In particular, the combination of loading rate and energy efficiency can achieve a 12% improvement. The load balancing of the base station can better deal with social emergencies.}
}
@article{STEPHAN2022100284,
title = {Scene-adaptive radar tracking with deep reinforcement learning},
journal = {Machine Learning with Applications},
volume = {8},
pages = {100284},
year = {2022},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2022.100284},
url = {https://www.sciencedirect.com/science/article/pii/S2666827022000196},
author = {Michael Stephan and Lorenzo Servadei and José Arjona-Medina and Avik Santra and Robert Wille and Georg Fischer},
keywords = {Reinforcement learning, Radar tracking, Scene adaptation},
abstract = {Multi-target tracking with radars is a highly challenging problem due to detection artifacts, sensor noise, and interference sources. The traditional signal processing chain is, therefore, a complex combination of various algorithms with several tunable tracking-parameters. Usually, these are initially set by engineers and are independent of the scene tracked. For this reason, they are often non-optimal and generate poorly performing tracking. In this context, scene-adaptive radar processing refers to algorithms that can sense, understand and learn information related to detected targets as well as the environment and adapt its tracking-parameters to optimize the desired goal. In this paper, we propose a Deep Reinforcement Learning framework that guides the scene-adaptive choice of radar tracking-parameters towards an improved performance on multi-target tracking.}
}
@article{CHEN2023434,
title = {Emergency load shedding strategy for high renewable energy penetrated power systems based on deep reinforcement learning},
journal = {Energy Reports},
volume = {9},
pages = {434-443},
year = {2023},
note = {2022 2nd International Joint Conference on Energy and Environmental Engineering},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2023.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S2352484723002640},
author = {Hongwei Chen and Junzhi Zhuang and Gang Zhou and Yuwei Wang and Zhenglong Sun and Yoash Levron},
keywords = {Emergency load shedding, Mismatch scenario, Deep reinforcement learning, Design of decision space},
abstract = {Traditional event-driven emergency load shedding determines the quantitative strategy by simulation of a specific set of expected faults, which requires high model accuracy and operation mode matching. However, due to the model complexity of renewable power generators and fluctuating power generation, traditional event-driven load shedding strategy faces the risk of mismatching in high renewable energy penetrated power systems. To address these challenges, this paper proposes an emergency load shedding method based on data-driven strategies and deep reinforcement learning (RL). Firstly, the reason for the possible mismatch of the event-driven load shedding strategy in the renewable power system is analyzed, and a typical mismatch scenario is constructed. Then, the emergency load shedding strategy is transformed into a Markov Decision Process (MDP), and the decision process’s action space, state space, and reward function are designed. On this basis, an emergency control strategy platform based on the Gym framework is established for application of deep reinforcement learning in the power system emergency control strategy. In order to enhance the adaptability and efficiency of the RL intelligence agent to multi-fault scenarios, the Proximal Policy Optimization (PPO) is adopted to optimize the constructed MDP. Finally, the proposed reinforcement learning-based emergency load shedding strategy is trained and verified through a modified IEEE 39-bus system. The results show that the proposed strategy can effectively make correct strategies to restore system frequency in the event-driven load shedding mismatch scenario, and have good adaptability for different faults and operation scenarios.}
}
@article{YU2023121396,
title = {District cooling system control for providing regulation services based on safe reinforcement learning with barrier functions},
journal = {Applied Energy},
volume = {347},
pages = {121396},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121396},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923007602},
author = {Peipei Yu and Hongcai Zhang and Yonghua Song},
keywords = {Safe deep reinforcement learning, District cooling system, Frequency regulation service, Control barrier function},
abstract = {Thermostatically controlled loads (TCLs) in buildings are ideal resources to provide regulation services for power systems. As large-scale and centralized TCLs with high efficiency and large regulation capacity, district cooling systems (DCSs) have attracted great research attention for minimizing energy costs, but little on providing regulation services. However, controlling a DCS to provide high-quality regulation services is challenging due to its complex thermal dynamic model and uncertainties from regulation signals and cooling demands. To fill this research gap, we propose a novel safe deep reinforcement learning (DRL) control method for a DCS to provide regulation services. The objective is to adjust the DCS’s power consumption to follow real-time regulation signals subject to buildings’ temperature comfort constraints. The proposed method is model-free and adaptive to uncertainties from regulation signals and cooling demands. Furthermore, the barrier function is combined with traditional DRL to construct a safe DRL controller, which can not only avoid unsafe explorations during training (this may result in catastrophic control results) but also improve training efficiency. We conducted case studies based on a realistic DCS to evaluate the performance of the proposed control method compared to traditional methods, and the results demonstrate the increased effectiveness and superiority of the proposed control method.}
}
@article{LINOT2023109139,
title = {Turbulence control in plane Couette flow using low-dimensional neural ODE-based models and deep reinforcement learning},
journal = {International Journal of Heat and Fluid Flow},
volume = {101},
pages = {109139},
year = {2023},
issn = {0142-727X},
doi = {https://doi.org/10.1016/j.ijheatfluidflow.2023.109139},
url = {https://www.sciencedirect.com/science/article/pii/S0142727X23000383},
author = {Alec J. Linot and Kevin Zeng and Michael D. Graham},
keywords = {Turbulent shear flow, Flow control, Coherent structures, Machine learning, Reduced- order modeling},
abstract = {The high dimensionality and complex dynamics of turbulent flows remain an obstacle to the discovery and implementation of control strategies. Deep reinforcement learning (RL) is a promising avenue for overcoming these obstacles, but requires a training phase in which the RL agent iteratively interacts with the flow environment to learn a control policy, which can be prohibitively expensive when the environment involves slow experiments or large-scale simulations. We overcome this challenge using a framework we call “DManD-RL” (data-driven manifold dynamics-RL), which generates a data-driven low-dimensional model of our system that we use for RL training. With this approach, we seek to minimize drag in a direct numerical simulation (DNS) of a turbulent minimal flow unit of plane Couette flow at Re=400 using two slot jets on one wall. We obtain, from DNS data with O(105) degrees of freedom, a 25-dimensional DManD model of the dynamics by combining an autoencoder and neural ordinary differential equation. Using this model as the environment, we train an RL control agent, yielding a 440-fold speedup over training on the DNS, with equivalent control performance. The agent learns a policy that laminarizes 84% of unseen DNS test trajectories within 900 time units, significantly outperforming classical opposition control (58%), despite the actuation authority being much more restricted. The agent often achieves laminarization through a counterintuitive strategy that drives the formation of two low-speed streaks, with a spanwise wavelength that is too small to be self-sustaining. The agent demonstrates the same performance when we limit observations to wall shear rate.}
}
@article{BIJL201410391,
title = {Applying Gaussian Processes to Reinforcement Learning for Fixed-Structure Controller Synthesis},
journal = {IFAC Proceedings Volumes},
volume = {47},
number = {3},
pages = {10391-10396},
year = {2014},
note = {19th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20140824-6-ZA-1003.01623},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016432635},
author = {Hildo Bijl and Jan-Willem {van Wingerden} and Michel Verhaegen},
abstract = {In industrial applications, fixed-structure controllers are often desired. But for systems with large uncertainties, or for systems with mostly unknown system dynamics, it is often unclear as to how to choose the controller parameters. In this paper we propose an algorithm that chooses the parameters of such a controller using only a limited amount of system interaction data. The novel algorithm applies Gaussian process tools to a reinforcement learning problem set-up to derive an approximation of the value function. This approximation is expressed in the system state and in the controller parameters. Then, by assuming a distribution of the initial state of the system, the value function approximation is expressed only as a function of the controller parameters. By subsequently optimizing this value function approximation, the optimal controller parameters with respect to the value function approximation can be found. The effectiveness of the proposed methodology has been shown in a simulation study.}
}
@article{MORATO2023109144,
title = {Inference and dynamic decision-making for deteriorating systems with probabilistic dependencies through Bayesian networks and deep reinforcement learning},
journal = {Reliability Engineering & System Safety},
volume = {235},
pages = {109144},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109144},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023000595},
author = {P.G. Morato and C.P. Andriotis and K.G. Papakonstantinou and P. Rigo},
keywords = {Infrastructure management, Decision analysis, Deep reinforcement learning, Partially observable Markov decision processes, System reliability analysis, Dynamic Bayesian networks},
abstract = {In the context of modern engineering, environmental, and societal concerns, there is an increasing demand for methods able to identify rational management strategies for civil engineering systems, minimizing structural failure risks while optimally planning inspection and maintenance (I&M) processes. Most available methods simplify the I&M decision problem to the component level, often assuming statistical, structural, or cost independence among components, due to the computational complexity associated with global optimization methodologies under joint system-level state descriptions. In this paper, we propose an efficient algorithmic framework for inference and decision-making under uncertainty for engineering systems exposed to deteriorating environments, providing optimal management strategies directly at the system level. In our approach, the decision problem is formulated as a factored partially observable Markov decision process, whose dynamics are encoded in Bayesian network conditional structures. The methodology can handle environments under equal or general, unequal deterioration correlations among components, through Gaussian hierarchical structures and dynamic Bayesian networks, decoupling the originally joint system state space to component networks conditional on shared random variables. In terms of policy optimization, we adopt a deep decentralized multi-agent actor-critic (DDMAC) reinforcement learning approach, in which the policies are approximated by actor neural networks guided by a critic network. By including deterioration dependence in the simulated environment, and by formulating the cost model at the system level, DDMAC policies intrinsically consider the underlying system-effects. This is demonstrated through numerical experiments conducted for both a 9-out-of-10 system and a steel frame under fatigue deterioration. Results demonstrate that DDMAC policies offer substantial benefits when compared to state-of-the-art heuristic approaches. The inherent consideration of system-effects by DDMAC strategies is also interpreted based on the learned policies.}
}
@article{RENARD2021105240,
title = {Minimizing the global warming impact of pavement infrastructure through reinforcement learning},
journal = {Resources, Conservation and Recycling},
volume = {167},
pages = {105240},
year = {2021},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2020.105240},
url = {https://www.sciencedirect.com/science/article/pii/S0921344920305553},
author = {Sophie Renard and Benjamin Corbett and Omar Swei},
keywords = {Global warming potential, Life cycle assessment, Probabilistic methods, Reinforcement learning},
abstract = {Life cycle assessment (LCA) studies are frequently used to evaluate the environmental burdens of pavement facilities. This information can be used by decision-makers to advise their construction and maintenance policies. Within the pavement life cycle, there are a variety of uncertainties, such as future traffic growth and pavement deterioration. Currently, there is a lack of research examining the use of LCA models that can simultaneously optimize construction and maintenance plans while accounting for several sources of uncertainty. This study presents an approach to LCA modeling that implements a sub-type of reinforcement learning (RL) algorithms called Q-learning. Q-learning offers a model-free approach that can efficiently manage stochastic problems of parametric and non-parametric form. The algorithm iteratively learns a set of near-optimal decision rules to proactively manage pavement assets for a diverse range of possible future scenarios. These decision-rules are stored in a convenient look-up table, which will appeal to practitioners for its ease of use in probabilistic LCA studies. This paper subsequently tests the performance of the Q-learning approach across three representative case studies with varying traffic volumes: a local street-highway, a state highway, and an interstate. The case study results show that, on average, the proposed algorithm reduces the expected global warming impact of pavement infrastructure between 13% and 18% over a 50-year analysis period. Based on our results, Q-learning is a promising approach that can help decision-makers account for several sources of uncertainty and implement improved management strategies to mitigate the environmental impacts of their products and systems.}
}
@article{BONNEAU201430,
title = {Reinforcement learning-based design of sampling policies under cost constraints in Markov random fields: Application to weed map reconstruction},
journal = {Computational Statistics & Data Analysis},
volume = {72},
pages = {30-44},
year = {2014},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2013.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167947313003551},
author = {Mathieu Bonneau and Sabrina Gaba and Nathalie Peyrard and Régis Sabbadin},
keywords = {Sampling design, Markov decision process, Dynamic programming, Gibbs sampling, Least-squares linear regression, Weed mapping},
abstract = {Weeds are responsible for yield losses in arable fields, whereas the role of weeds in agro-ecosystem food webs and in providing ecological services has been well established. Innovative weed management policies have to be designed to handle this trade-off between production and regulation services. As a consequence, there has been a growing interest in the study of the spatial distribution of weeds in crops, as a prerequisite to management. Such studies are usually based on maps of weed species. The issues involved in building probabilistic models of spatial processes as well as plausible maps of the process on the basis of models and observed data are frequently encountered and important. As important is the question of designing optimal sampling policies that make it possible to build maps of high probability when the model is known. This optimization problem is more complex to solve than the pure reconstruction problem and cannot generally be solved exactly. A generic approach to spatial sampling for optimizing map construction, based on Markov Random Fields (MRF), is provided and applied to the problem of weed sampling for mapping. MRF offer a powerful representation for reasoning on large sets of random variables in interaction. In the field of spatial statistics, the design of sampling policies has been largely studied in the case of continuous variables, using tools from the geostatistics domain. In the MRF case with finite state space variables, some heuristics have been proposed for the design problem but no universally accepted solution exists, particularly when considering adaptive policies as opposed to static ones. The problem of designing an adaptive sampling policy in an MRF can be formalized as an optimization problem. By combining tools from the fields of Artificial Intelligence (AI) and Computational Statistics, an original algorithm is then proposed for approximate resolution. This generic procedure, referred to as Least-Squares Dynamic Programming (LSDP), combines an approximation of the value of a sampling policy based on a linear regression, the construction of a batch of MRF realizations and a backwards induction algorithm. Based on an empirical comparison of the performance of LSDP with existing one-step-look-ahead sampling heuristics and solutions provided by classical AI algorithms, the following conclusions can be derived: (i) a naïve heuristic consisting of sampling sites where marginals are the most uncertain is already an efficient sampling approach; (ii) LSDP outperforms all the classical approaches we have tested; and (iii) LSDP outperforms the naïve heuristic approach in cases where sampling costs are not uniform over the set of variables or where sampling actions are constrained.}
}
@incollection{KROHLING20191675,
title = {Contract Settlements for Exchanging Utilities through Automated Negotiations between Prosumers in Eco-Industrial Parks using Reinforcement Learning},
editor = {Anton A. Kiss and Edwin Zondervan and Richard Lakerveld and Leyla Özkan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {46},
pages = {1675-1680},
year = {2019},
booktitle = {29th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818634-3.50280-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186343502800},
author = {Dan E. Kröhling and Ernesto C. Martínez},
keywords = {automated negotiation, blockchain, reinforcement learning, smart contracts},
abstract = {Peer-to-peer trading of utilities (heating, cooling and electric power) in an eco-industrial park (EIP) is key to realize significant economic and environmental benefits by exploiting synergistic co-generation and surplus trading. A crucial aspect of this symbiotic scheme is that each selfish company (prosumer) will participate in exchanging utilities to increase its own profits depending on its internal load. In this paper, an automated negotiation approach based on utility tokens is proposed to incentivize participation in a market of prosumer peers for trading surpluses in an EIP. During each negotiation episode, a pair of prosumers engage in a bilateral negotiation and resort to a learned policy to bid, concede and accept/reject using both private information and environmental variables. Contract negotiation revolves around agreeing (or not) on the price expressed in tokens of a utility profile. The time-varying value of the utility token accounts for contextual variables beyond the control of each prosumer. Simulation results demonstrate that reinforcement learning allows finding Nash equilibrium policies for smart contract negotiation in a blockchain environment.}
}
@article{ALYAZIDI2023127810,
title = {An Online Adaptive Policy Iteration-Based Reinforcement Learning for a Class of a Nonlinear 3D Overhead Crane},
journal = {Applied Mathematics and Computation},
volume = {447},
pages = {127810},
year = {2023},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2022.127810},
url = {https://www.sciencedirect.com/science/article/pii/S0096300322008785},
author = {Nezar M. Alyazidi and Abdalrahman M. Hassanine and Magdi S. Mahmoud},
keywords = {Online policy iteration, Reinforcement learning, Nnonlinear three-dimensional overhead crane, Ounded uncertainties},
abstract = {We consider an online policy iteration-based reinforcement learning for a class of a nonlinear three-dimensional overhead crane with bounded uncertainties. Under the assumption that the rope length is fixed with small swing angles, a linearized model is derived. The system has four states; two actuated states: position x and y, and two un-actuated states, which are the rope angles θx and θy. The adaptive reinforcement learning controller is designed to handle the effects of measurement noises and outliers. We propose a model-free; hence it does not require precise knowledge of the system dynamics. When the state information is not available, a Kalman filter estimator is equipped with a dynamical saturation function to attenuate the effects of measurement noises and to remove outliers. A simulation study is established to illustrate the influence and robustness of the developed controller, and it can enhance the tracking trajectory under different scenarios to test the scheme.}
}
@article{LU2023109235,
title = {Battery-degradation-involved energy management strategy based on deep reinforcement learning for fuel cell/battery/ultracapacitor hybrid electric vehicle},
journal = {Electric Power Systems Research},
volume = {220},
pages = {109235},
year = {2023},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2023.109235},
url = {https://www.sciencedirect.com/science/article/pii/S0378779623001244},
author = {Hongxin Lu and Fazhan Tao and Zhumu Fu and Haochen Sun},
keywords = {Fuel cell hybrid electric vehicle, Energy management strategy, Deep reinforcement learning, Performance degradation, Heuristic experience replay},
abstract = {To acquire an optimal way to solve the energy management strategy (EMS) of fuel cell hybrid electric vehicles (FCHEVs), most of existing research focuses too much on the protection of fuel cell, while the degree of degradation of battery as an internal influence factor also plays an important role in EMS. In this paper, battery-degradation-involved hierarchical energy management framework utilizing an improved deep deterministic policy gradient (DDPG) algorithm is proposed for gaining the optimal EMS of FCHEV with three power sources. Firstly, to protect fuel cell and battery against the peak power, an adaptive fuzzy filter is employed to complete frequency-based decoupling of power demand for achieving the stratification of power. Then, the degradation model of battery is adopted according to the available data types of our test bench, and an adaptive multi-objective equivalent consumption minimization strategy model is constructed and solved by an improved DDPG-based algorithm. Finally, the simulation results show that, compared with the traditional DDPG, the proposed EMS can enhance the efficiency of fuel cell by 2.02% on average, and reduce the performance degradation of battery by 14.4% on average.}
}
@article{BARNAWI2022441,
title = {Deep reinforcement learning based trajectory optimization for magnetometer-mounted UAV to landmine detection},
journal = {Computer Communications},
volume = {195},
pages = {441-450},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422003413},
author = {Ahmed Barnawi and Neeraj Kumar and Ishan Budhiraja and Krishan Kumar and Amal Almansour and Bander Alzahrani},
keywords = {Landmine, UAV, DRL, Energy, Magnetometer},
abstract = {Unmanned aerial vehicles (UAVs) have emerged as a viable choice for data collection and landmine (LM) detection. The LM buried under the dirt or sand is detected using a UAV-mounted magnetometer in this paper. A UAV is deployed to gather data along the intended route when the magnetometer receives a signal from the LMs. During a whole round of data collection, we want to reduce the total energy consumption of the UAV-Magnetometer-LM system. To do this, we turn the energy consumption reduction issue into a limited combinatorial optimization problem by concurrently picking time slots and arranging the UAV’s visitation sequence to identify the LM. The problem of minimizing energy usage is NP-hard, making it difficult to solve optimally. In order to tackle this challenge, we used the deep reinforcement learning (DRL) based deep deterministic policy gradient (DDPG) scheme. DDPG is used to enhance the convergence speed and eliminate redundant computations. Furthermore, to improve the detection in real-time, we proposed the proximal online policy technique (POPT). Numerical results demonstrate that the proposed scheme consumes 37.14%, 31.25%, and 21.42% better results than synthetic aperture radar (SAR), convolution neural network (CNN), and double deep recurrent Q-network (DDRQN).}
}
@article{YANG2022116453,
title = {A reinforcement learning-based energy management strategy for fuel cell hybrid vehicle considering real-time velocity prediction},
journal = {Energy Conversion and Management},
volume = {274},
pages = {116453},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.116453},
url = {https://www.sciencedirect.com/science/article/pii/S0196890422012316},
author = {Duo Yang and Li Wang and Kunjie Yu and Jing Liang},
keywords = {Fuel cell, Hybrid dynamic system, Energy management strategy, Reinforcement learning, Velocity prediction},
abstract = {The fuel cell vehicle is an ideal new energy vehicle development direction, and its energy management strategy is one of the core technologies to ensure the safe and efficient operation of the vehicle. We proposed a novel reinforcement learning-based energy management method for the fuel cell/lithium battery hybrid system in this paper. In order to improve the reliability of the EMS, the real-time driving profile classification and velocity prediction method based on data driven and statistical analysis is proposed to forecast vehicle velocity in the near future. Then a reinforcement learning method is designed to realize the real-time power allocation. The reward value function which comprehensively considers the system safety, economics and fuel cell durability is creatively put forward. The double Q-learning strategy is applied to update the Q value function. In addition, the real-time reference path of power allocation is designed by taking battery state-of-charge as an indicator. A new dynamic test profile is conducted to verify the proposed method. The multiple groups of comparative simulation experiments show that the proposed EMS can effectively reduce the life decay rate of fuel cell, but also improves fuel economics by up to 6% compared with other commonly used methods.}
}
@incollection{BERMUDEZ20231649,
title = {Distributional Constrained Reinforcement Learning for Supply Chain Optimization},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {1649-1654},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50262-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152740502626},
author = {Jaime Sabal Bermúdez and Antonio {del Rio Chanona} and Calvin Tsay},
keywords = {Safe reinforcement learning, Process operations, Inventory management},
abstract = {This work studies reinforcement learning (RL) in the context of multi-period supply chains subject to constraints, e.g., on inventory. We introduce Distributional Constrained Policy Optimization (DCPO), a novel approach for reliable constraint satisfaction in RL. Our approach is based on Constrained Policy Optimization (CPO), which is subject to approximation errors that in practice lead it to converge to infeasible policies. We address this issue by incorporating aspects of distributional RL. Using a supply chain case study, we show that DCPO improves the rate at which the RL policy converges and ensures reliable constraint satisfaction by the end of training. The proposed method also greatly reduces the variance of returns between runs; this result is significant in the context of policy gradient methods, which intrinsically introduce high variance during training.}
}
@article{SONG199837,
title = {Reinforcement learning and its application to force control of an industrial robot},
journal = {Control Engineering Practice},
volume = {6},
number = {1},
pages = {37-44},
year = {1998},
issn = {0967-0661},
doi = {https://doi.org/10.1016/S0967-0661(97)10058-2},
url = {https://www.sciencedirect.com/science/article/pii/S0967066197100582},
author = {Kai-Tai Song and Te-Shan Chu},
keywords = {Learning control, stochastic reinforcement learning, industrial robots, force tracking control, performance optimization},
abstract = {This paper presents a learning control design, together with an experimental study for implementing it on an industrial robot working in constrained environments. A new reinforcement learning scheme is proposed, to enable performance optimization in industrial robots. Using this scheme, the learning process is split into generalized and specialized learning phases, increasing the convergence speed and aiding practical implementation. Initial computer simulations were carried out for force tracking control of a two-link robot arm. The results confirmed that even without calculating the inverse kinematics or possessing the relevant environmental information, operating rules for simultaneously controlling the force and velocity of the robot arm can be achieved via repetitive exploration. Furthermore, practical experiments were carried out on an ABB IRB-2000 industrial robot to demonstrate the developed reinforcement-learning scheme for real-world applications. Experimental results verify that the proposed learning algorithm can cope with variations in the contact environment, and achieve performance improvements.}
}
@article{LI2021104951,
title = {Optimal adaptive control for solid oxide fuel cell with operating constraints via large-scale deep reinforcement learning},
journal = {Control Engineering Practice},
volume = {117},
pages = {104951},
year = {2021},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2021.104951},
url = {https://www.sciencedirect.com/science/article/pii/S0967066121002288},
author = {Jiawen Li and Tao Yu},
keywords = {Large-scale agent deep reinforcement learning, Fittest survival strategy large-scale twin delayed deep deterministic policy gradient (FSSL-TD3), Solid oxide fuel cell, Fuel flow, Fuel utilization},
abstract = {Since a solid oxide fuel cell (SOFC) is a complicated nonlinear, time-varying and constrained system, it is difficult to control the fuel flow to stabilize the output voltage while considering fuel utilization operating constraints. To overcome this problem, an adaptive fractional-order proportional integral derivative (FOPID) controller, taking advantage of the adaptability and model-free features of large-scale deep reinforcement learning, is proposed in this paper. Furthermore, a fittest survival strategy large-scale twin delayed deep deterministic policy gradient (FSSL-TD3) algorithm is designed as the tuner of this controller. In this algorithm, the exploration efficacy is improved by way of the fittest survival strategy and imitation learning. Other techniques are also applied to this algorithm in order to improve the robustness of FOPID controller. In addition, by formulating the reward function of the FSSL-TD3 algorithm, the fuel utilization of the SOFC can always be kept in a safe range, which is not possible for conventional control algorithms. The simulation results in this paper show that the output voltage of SOFCs can be controlled effectively by this controller while fuel utilization is retained within a reasonable range.}
}
@article{ZHU2022108843,
title = {Step by step: A hierarchical framework for multi-hop knowledge graph reasoning with reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {248},
pages = {108843},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108843},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122004026},
author = {Anjie Zhu and Deqiang Ouyang and Shuang Liang and Jie Shao},
keywords = {Knowledge graph, Multi-hop reasoning, Hierarchical reinforcement learning, Dynamic prospect},
abstract = {Recently, knowledge graph reasoning has sparked great interest in research community, which aims at inferring missing information in triples and provides critical support to various tasks (e.g., question answering and recommendation). To date, multi-hop reasoning is a dominant approach which infers the target answer by walking along the path connecting entities and relations, ensuring both accuracy and interpretability. However, in most knowledge graphs, there are multiple relations related to an identical entity, and multiple tail entities for an identical pair of head entity and relation. Due to this one-to-many dilemma, enlarged action space and ignoring logical relationship between entity and relation increase the difficulty of learning. In order to deal with such an issue, this work presents a novel paradigm for knowledge graph reasoning by decomposing it to a two-level hierarchical decision process. We apply the hierarchical reinforcement learning framework which dismantles the task into a high-level process for relation detector and a low-level process for entity reasoning, respectively. In this way, the action space is effectively controlled where the policies can be optimized. The interactions between entity and relation decision enhance the rationality of reasoning. Moreover, we introduce a dynamic prospect mechanism for low-level policy where the information can guide us to a refined and improved action space, assisted by embedding based method. Our proposed model is evaluated on four benchmark datasets and the results validate its superiority over state-of-the-art baselines, showing the interpretability of reasoning process simultaneously.}
}
@article{NUGAMANOV2020123,
title = {Hierarchical Temporal Memory with Reinforcement Learning},
journal = {Procedia Computer Science},
volume = {169},
pages = {123-131},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.123},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920302465},
author = {Eduard Nugamanov and Aleksandr I. Panov},
keywords = {Hierarchical Temporal Memory, Reinforcement Learning, Image Recognition, Monte Carlo Control},
abstract = {Nowadays our knowledge of the brain is actively getting wider. Hierarchical Temporal Memory is the technology that arose due to new discoveries in neurobiology, such as research on the structure of the neocortex. One of the most popular applications of this technology is image recognition and anomaly detection. Nevertheless, both in the neocortex and in hierarchical temporal memory an image is recognized by its parts. Therefore, there is a problem of choosing the most meaningful parts of an image in order to perform fast and effective recognition. In this work we propose the architecture that unites Hierarchical Temporal Memory and Reinforcement Learning in order to find the optimal way of image exploration. Besides, we prove by experiments that this architecture is effective, and the quality of the resulting movement pattern is high.}
}
@article{CHANG2023128536,
title = {An energy management strategy of deep reinforcement learning based on multi-agent architecture under self-generating conditions},
journal = {Energy},
volume = {283},
pages = {128536},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.128536},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223019308},
author = {Chengcheng Chang and Wanzhong Zhao and Chunyan Wang and Zhongkai Luan},
keywords = {Hybrid electric vehicle, Energy management, Generative adversarial network, Multi-agent architecture, Deep reinforcement learning},
abstract = {To improve the driving efficiency of hybrid power vehicle, an energy management strategy of deep reinforcement learning based on multi-agent architecture under self-generating vehicle driving conditions is proposed. Firstly, the kinematics segments are self-generated based on the Wasserstein generative adversarial network. The generator network G is used to generate kinematics segments. The discriminator network D is used to judge the credibility of the generated kinematics segments with the Wasserstein distance. The speed distribution characteristics of the training conditions and verification conditions established based on the self-generated segments are verified. Afterward, a multi-agent algorithm based on twin delayed deep deterministic policy gradient algorithm for hybrid systems is proposed by introducing centralized training with decentralized execution framework. The engine and a motor are used as two independent agents respectively. Different reward functions are designed based on training objectives to establish a mutually beneficial relationship of cooperation-restraint between the two agents. A driving mode constraint is designed in the environment to improve sample utilization. Finally, the simulation results demonstrate that our method can achieve better performance compared with other existing works.}
}
@article{WANG2021101262,
title = {Joint resource allocation and power control for D2D communication with deep reinforcement learning in MCC},
journal = {Physical Communication},
volume = {45},
pages = {101262},
year = {2021},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2020.101262},
url = {https://www.sciencedirect.com/science/article/pii/S1874490720303396},
author = {Dan Wang and Hao Qin and Bin Song and Ke Xu and Xiaojiang Du and Mohsen Guizani},
keywords = {D2D, DRL, Resource allocation, Power control},
abstract = {Mission-critical communication (MCC) is one of the main goals in 5G, which can leverage multiple device-to-device (D2D) connections to enhance reliability for mission-critical communication. In MCC, D2D users can reuses the non-orthogonal wireless resources of cellular users without a base station (BS). Meanwhile, the D2D users will generate co-channel interference to cellular users and hence affect their quality-of-service (QoS). To comprehensively improve the user experience, we proposed a novel approach, which embraces resource allocation and power control along with Deep Reinforcement Learning (DRL). In this paper, multiple procedures are carefully designed to assist in developing our proposal. As a starter, a scenario with multiple D2D pairs and cellular users in a cell will be modeled; followed by the analysis of issues pertaining to resource allocation and power control as well as the formulation of our optimization goal; and finally, a DRL method based on spectrum allocation strategy will be created, which can ensure D2D users to obtain the sufficient resource for their QoS improvement. With the resource data provided, which D2D users capture by interacting with surroundings, the DRL method can help the D2D users autonomously selecting an available channel and power to maximize system capacity and spectrum efficiency while minimizing interference to cellular users. Experimental results show that our learning method performs well to improve resource allocation and power control significantly.}
}
@article{CHEN20201,
title = {Actor-critic reinforcement learning in the songbird},
journal = {Current Opinion in Neurobiology},
volume = {65},
pages = {1-9},
year = {2020},
note = {Whole-brain interactions between neural circuits},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2020.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0959438820301173},
author = {Ruidong Chen and Jesse H Goldberg},
abstract = {It feels rewarding to ace your opponent on match point. Here, we propose common mechanisms underlie reward and performance learning. First, when a singing bird unexpectedly hits the right note, its dopamine (DA) neurons are activated as when a thirsty monkey receives an unexpected juice reward. Second, these DA signals reinforce vocal variations much as they reinforce stimulus-response associations. Third, limbic inputs to DA neurons signal the predicted quality of song syllables much like they signal the predicted reward value of a place or a stimulus during foraging. Finally, songbirds may solve difficult problems in reinforcement learning – such as credit assignment and catastrophic forgetting – with node perturbation and consolidation of reinforced vocal patterns in motor cortical circuits. Consolidation occurs downstream of a canonical ‘actor-critic’ circuit motif that learns to maximize performance quality in essentially the same way it learns to maximize reward: by computing and learning from prediction errors.}
}
@article{LI2021101284,
title = {Distributed deep reinforcement learning-based multi-objective integrated heat management method for water-cooling proton exchange membrane fuel cell},
journal = {Case Studies in Thermal Engineering},
volume = {27},
pages = {101284},
year = {2021},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2021.101284},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X21004470},
author = {Jiawen Li and Yaping Li and Tao Yu},
keywords = {Large-scale deep reinforcement learning, Efficient curriculum exploration distributed double-delay deep determinate policy gradient (ECE-5DPG), Proton exchange membrane fuel cell (PEMFC), Integrated heat management method},
abstract = {In order to improve the operational efficiency and stability of proton exchange membrane fuel cell (PEMFC), a distributed deep reinforcement learning (DDRL)-based integrated control strategy is proposed to solve the coordinated control problem of water pump and radiator in stack heat management system. This strategy substitutes the independent controllers of the water pump and radiator in the traditional control framework, and employs multi-input multi-output (MIMO) agents which simultaneously control the cooling water velocity of the water pump and the air velocity of the radiator, whilst monitoring the optimal global stack temperature control performance. To this end, an efficient curriculum exploration distributed double-delay deep determinate policy gradient (ECE-5DPG) algorithm is proposed for the strategy, the design of which is based on the concepts of curriculum learning, imitation learning, and distributed exploration, thus improving the robustness of the proposed strategy. The experimental results show that the proposed integrated control strategy can effectively control the cooling water velocity and air velocity simultaneously, thereby improving the operating efficiency of the PEMFC.}
}
@article{ZHANG2023104063,
title = {Predictive trajectory planning for autonomous vehicles at intersections using reinforcement learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {149},
pages = {104063},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2023.104063},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X23000529},
author = {Ethan Zhang and Ruixuan Zhang and Neda Masoud},
keywords = {Trajectory planning, Trajectory prediction, Autonomous vehicles, Reinforcement learning},
abstract = {In this work we put forward a predictive trajectory planning framework to help autonomous vehicles plan future trajectories. We develop a partially observable Markov decision process (POMDP) to model this sequential decision making problem, and a deep reinforcement learning solution methodology to learn high-quality policies. The POMDP model utilizes driving scenarios, condensed into graphs, as inputs. More specifically, an input graph contains information on the history trajectory of the subject vehicle, predicted trajectories of other agents in the scene (e.g., other vehicles, pedestrians, and cyclists), as well as predicted risk levels posed by surrounding vehicles to devise safe, comfortable, and energy-efficient trajectories for the subject vehicle to follow. In order to obtain sufficient driving scenarios to use as training data, we propose a simulation framework to generate socially acceptable driving scenarios using a real world autonomous vehicle dataset. The simulation framework utilizes Bayesian Gaussian mixture models to learn trajectory patterns of different agent types, and Gibbs sampling to ensure that the distribution of simulated scenarios matches that of the real-world dataset collected by an autonomous fleet. We evaluate the proposed work in two complex urban driving environments: a non-signalized T-junction and a non-signalized lane merge intersection. Both environments provide vastly more complex driving scenarios compared to a highway driving environment, which has been mostly the focus of previous studies. The framework demonstrates promising performance for planning horizons as long as five seconds. We compare safety, comfort, and energy efficiency of the planned trajectories against human-driven trajectories in both experimental driving environments, and demonstrate that it outperforms human-driven trajectories in a statistically significant fashion in all aspects.}
}
@article{CEUSTERS2023100227,
title = {Safe reinforcement learning for multi-energy management systems with known constraint functions},
journal = {Energy and AI},
volume = {12},
pages = {100227},
year = {2023},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2022.100227},
url = {https://www.sciencedirect.com/science/article/pii/S2666546822000738},
author = {Glenn Ceusters and Luis Ramirez Camargo and Rüdiger Franke and Ann Nowé and Maarten Messagie},
keywords = {Reinforcement learning, Constraints, Multi-energy systems, Energy management system},
abstract = {Reinforcement learning (RL) is a promising optimal control technique for multi-energy management systems. It does not require a model a priori - reducing the upfront and ongoing project-specific engineering effort and is capable of learning better representations of the underlying system dynamics. However, vanilla RL does not provide constraint satisfaction guarantees — resulting in various potentially unsafe interactions within its environment. In this paper, we present two novel online model-free safe RL methods, namely SafeFallback and GiveSafe, where the safety constraint formulation is decoupled from the RL formulation. These provide hard-constraint satisfaction guarantees both during training and deployment of the (near) optimal policy. This is without the need of solving a mathematical program, resulting in less computational power requirements and more flexible constraint function formulations. In a simulated multi-energy systems case study we have shown that both methods start with a significantly higher utility compared to a vanilla RL benchmark and Optlayer benchmark (94,6% and 82,8% compared to 35,5% and 77,8%) and that the proposed SafeFallback method even can outperform the vanilla RL benchmark (102,9% to 100%). We conclude that both methods are viably safety constraint handling techniques applicable beyond RL, as demonstrated with random policies while still providing hard-constraint guarantees.}
}
@article{ZHONG2023109685,
title = {Deep reinforcement learning for class imbalance fault diagnosis of equipment in nuclear power plants},
journal = {Annals of Nuclear Energy},
volume = {184},
pages = {109685},
year = {2023},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2023.109685},
url = {https://www.sciencedirect.com/science/article/pii/S030645492300004X},
author = {Xianping Zhong and Lin Zhang and Heng Ban},
keywords = {Fault diagnosis, Nuclear power plant, Imbalanced dataset, Deep reinforcement learning, Agent, Reward},
abstract = {In equipment fault diagnosis in nuclear power plants, there may be far more samples in one class (e.g., a health state) than in another class (e.g., a fault state). The distribution of data in each class is highly skewed. Most machine learning algorithms are suitable for balanced training datasets. When faced with imbalanced samples, these algorithms tend to provide good identification for the majority classes and bias for the minority classes. However, the misclassification of minority classes can lead to high costs. To address the above problem, this paper develops a deep reinforcement learning-based diagnosis method that models fault diagnosis as a sequential decision-making process. At each time step, the agent receives the state of the environment represented by the training samples and then takes a diagnosis action guided by a policy. If the action is correct/incorrect, the agent receives a positive/negative reward. The reward for minority classes is higher than that for majority classes. The agent’s goal is to obtain as many cumulative rewards as possible in the process, i.e., to identify the sample as correctly as possible. Six demonstration scenarios are constructed, depending on the selected fault datasets and the designed model structures. Experiments show that the proposed method achieves a higher weighted-averaged F1 score than the classical supervised learning method in most cases of class imbalance. The proposed method has potential applications in the field of class imbalance fault diagnosis of equipment in nuclear power plants.}
}
@incollection{PETSAGKOURAKIS2019919,
title = {Reinforcement Learning for Batch-to-Batch Bioprocess Optimisation},
editor = {Anton A. Kiss and Edwin Zondervan and Richard Lakerveld and Leyla Özkan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {46},
pages = {919-924},
year = {2019},
booktitle = {29th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818634-3.50154-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186343501545},
author = {P. Petsagkourakis and I. Orson Sandoval and E. Bradford and D. Zhang and E.A. {del Rio-Chanona}},
keywords = {Reinforcement Learning, Batch Process, Recurrent Neural Networks, Bio-processes},
abstract = {Bioprocesses have received great attention from the scientific community as an alternative to fossil-based products by microorganisms-synthesised counterparts. However, bioprocesses are generally operated at unsteady-state conditions and are stochastic from a macro-scale perspective, making their optimisation a challenging task. Furthermore, as biological systems are highly complex, plant-model mismatch is usually present. To address the aforementioned challenges, in this work, we propose a reinforcement learning based online optimisation strategy. We first use reinforcement learning to learn an optimal policy given a preliminary process model. This means that we compute diverse trajectories and feed them into a recurrent neural network, resulting in a policy network which takes the states as input and gives the next optimal control action as output. Through this procedure, we are able to capture the previously believed behaviour of the biosystem. Subsequently, we adopted this network as an initial policy for the “real” system (the plant) and apply a batch-to-batch reinforcement learning strategy to update the network’s accuracy. This is computed by using a more complex process model (representing the real plant) embedded with adequate stochasticity to account for the perturbations in a real dynamic bioprocess. We demonstrate the effectiveness and advantages of the proposed approach in a case study by computing the optimal policy in a realistic number of batch runs.}
}
@article{ALMAHDI2017267,
title = {An adaptive portfolio trading system: A risk-return portfolio optimization using recurrent reinforcement learning with expected maximum drawdown},
journal = {Expert Systems with Applications},
volume = {87},
pages = {267-279},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.06.023},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417304402},
author = {Saud Almahdi and Steve Y. Yang},
keywords = {Recurrent reinforcement learning, Expected maximum drawdown, Optimal portfolio rebalancing, Downside risk},
abstract = {Dynamic control theory has long been used in solving optimal asset allocation problems, and a number of trading decision systems based on reinforcement learning methods have been applied in asset allocation and portfolio rebalancing. In this paper, we extend the existing work in recurrent reinforcement learning (RRL) and build an optimal variable weight portfolio allocation under a coherent downside risk measure, the expected maximum drawdown, E(MDD). In particular, we propose a recurrent reinforcement learning method, with a coherent risk adjusted performance objective function, the Calmar ratio, to obtain both buy and sell signals and asset allocation weights. Using a portfolio consisting of the most frequently traded exchange-traded funds, we show that the expected maximum drawdown risk based objective function yields superior return performance compared to previously proposed RRL objective functions (i.e. the Sharpe ratio and the Sterling ratio), and that variable weight RRL long/short portfolios outperform equal weight RRL long/short portfolios under different transaction cost scenarios. We further propose an adaptive E(MDD) risk based RRL portfolio rebalancing decision system with a transaction cost and market condition stop-loss retraining mechanism, and we show that the proposed portfolio trading system responds to transaction cost effects better and outperforms hedge fund benchmarks consistently.}
}
@article{ELFAKDI2013271,
title = {Two-step gradient-based reinforcement learning for underwater robotics behavior learning},
journal = {Robotics and Autonomous Systems},
volume = {61},
number = {3},
pages = {271-282},
year = {2013},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2012.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0921889012002205},
author = {Andres El-Fakdi and Marc Carreras},
keywords = {Reinforcement learning, Underwater robotics, Gradient descent algorithms, Actor–Critic algorithms, Model identification},
abstract = {This article proposes a field application of a Reinforcement Learning (RL) control system for solving the action selection problem of an autonomous robot in a cable tracking task. The Ictineu Autonomous Underwater Vehicle (AUV) learns to perform a visual based cable tracking task in a two step learning process. First, a policy is computed by means of simulation where a hydrodynamic model of the vehicle simulates the cable following task. The identification procedure follows a specially designed Least Squares (LS) technique. Once the simulated results are accurate enough, in a second step, the learnt-in-simulation policy is transferred to the vehicle where the learning procedure continues in a real environment, improving the initial policy. The Natural Actor–Critic (NAC) algorithm has been selected to solve the problem. This Actor–Critic (AC) algorithm aims to take advantage of Policy Gradient (PG) and Value Function (VF) techniques for fast convergence. The work presented contains extensive real experimentation. The main objective of this work is to demonstrate the feasibility of RL techniques to learn autonomous underwater tasks, the selection of a cable tracking task is motivated by an increasing industrial demand in a technology to survey and maintain underwater structures.}
}
@article{YAMAKAWA1995363,
title = {A neural network-like critic for reinforcement learning},
journal = {Neural Networks},
volume = {8},
number = {3},
pages = {363-373},
year = {1995},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(94)00086-2},
url = {https://www.sciencedirect.com/science/article/pii/0893608094000862},
author = {Hiroshi Yamakawa and Yoichi Okabe},
keywords = {Reactive system, Neural network, Agent, Maze-like environment, Recursive structure, Amygdala},
abstract = {An adaptive agent that contains a reactive network and a critic that supervises that reactive network have been studied. Agent actions are generated in response to stimuli through the reactive network and they influence the ambient environment. The critic has a new learning algorithm that recursively enhances reinforcement signals from fixed reinforcement signals by interacting with the environment. The reactive network learns appropriate stimulus-action relations by reinforcement learning. Computer simulation demonstrates that this neural critic is effective in environments where the concepts are embedded in a maze structure. We also suggest similarities between this critic model and the neural circuit in the human brain.}
}
@article{CHEN2022109931,
title = {Spectral graph theory-based virtual network embedding for vehicular fog computing: A deep reinforcement learning architecture},
journal = {Knowledge-Based Systems},
volume = {257},
pages = {109931},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109931},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122010243},
author = {Ning Chen and Peiying Zhang and Neeraj Kumar and Ching-Hsien Hsu and Laith Abualigah and Hailong Zhu},
keywords = {Intelligent transportation system, Virtual network embedding, Vehicular fog computing, Deep reinforcement learning, Graph Convolutional Networks},
abstract = {In the intelligent transportation system (ITS), the vehicular fog computing network (VFCN) can effectively alleviate the bottleneck existing in the cloud computing framework, such as high latency-sensitive applications, through edge computing offloading. It uses vehicles as the infrastructure, and fog nodes can communicate, perceive and share resources, so resource orchestration has become an essential issue of VFCN. To reduce the communication transportation cost and improve the resource utilization of VFCN, we propose a spectral graph theory-based resource orchestration algorithm by combining Virtual Network Embedding (VNE) and Deep Reinforcement Learning (DRL). Specifically, we propose a four-layer strategy network based on Graph Convolutional Networks (GCNs) for computing node embedding probability, where fog nodes fully mine spatial structure information by fusing themselves with neighborhood information to compensate for the lack of traditional heuristic VNE. Moreover, fog link embedding is performed by breadth-first search (BFS). Finally, the effectiveness of the proposed strategy is scientifically and rigorously proved in terms of long-term average revenue, long-term average revenue-cost ratio, and VNR acceptance rate through simulation cases, which can reasonably arrange the resources of VFCN.}
}
@article{SONG2023110350,
title = {Multi-objective acoustic sensor placement optimization for crack detection of compressor blade based on reinforcement learning},
journal = {Mechanical Systems and Signal Processing},
volume = {197},
pages = {110350},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2023.110350},
url = {https://www.sciencedirect.com/science/article/pii/S0888327023002571},
author = {Di Song and Junxian Shen and Tianchi Ma and Feiyun Xu},
keywords = {Compressor blades, Optimal sensor placement, Crack detection, Reinforcement learning, Acoustic sensor},
abstract = {Nowadays, acoustic sensors have been widely applied for structural health monitoring and crack detection of compressor blades. As the detection accuracy is mainly affected by signal quality, the optimal sensor placement (OSP) is significant for crack detection. To search the OSP for reliable signals, the multi-objective acoustic sensor placement optimization method is proposed based on information field fitting and optimization with reinforcement learning. First, the improved ridge and least squares support vector regression (R-LS-SVR) method is presented to fit information field of acoustic signal. Besides, the multi-objective function is constructed based on the compressor operation conditions, sound quality and crack damage observation. In addition, the Pareto front is obtained to realize multi-objective optimization (MOO) based on non-dominated sorting genetic algorithms (NSGA-II). Furtherly, the reinforcement learning-based pattern search method (RL-PSM) is proposed to quickly search the OSP of acoustic sensor. The compressor experiments are implemented to test the proposed method, and it can detect crack with the accuracy of 99.17%, which is superior to other locations. Comparing with other fitting and optimization methods, the advantage of the proposed method is validated for OSP optimization and crack detection.}
}
@article{BULLOCK2009757,
title = {Computational perspectives on forebrain microcircuits implicated in reinforcement learning, action selection, and cognitive control},
journal = {Neural Networks},
volume = {22},
number = {5},
pages = {757-765},
year = {2009},
note = {Advances in Neural Networks Research: IJCNN2009},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2009.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608009001117},
author = {Daniel Bullock and Can Ozan Tan and Yohan J. John},
keywords = {Basal ganglia, Acetylcholine, Dopamine, Striatum, Decision making},
abstract = {Abundant new information about signaling pathways in forebrain microcircuits presents many challenges, and opportunities for discovery, to computational neuroscientists who strive to bridge from microcircuits to flexible cognition and action. Accurate treatment of microcircuit pathways is especially critical for creating models that correctly predict the outcomes of candidate neurological therapies. Recent models are trying to specify how cortical circuits that enable planning and voluntary actions interact with adaptive subcortical microcircuits in the basal ganglia. The basal ganglia are strongly implicated in reinforcement learning, and in all behavior and cognition over which the frontal lobes exert flexible control. The persisting role of the basal ganglia shows that ancient vertebrate designs for motivated action selection proved adaptable enough to support many “modern” behavioral innovations, including fluent generation of language and speech. This paper summarizes how recent models have incorporated realistic representations of microcircuit features, and have begun to trace their computational implications. Also summarized are recent empirical discoveries that provide guidance regarding how to formulate the rules for synaptic modification that govern learning in cortico-striatal pathways. Such efforts are contributing to an emerging synthesis based on an interlocking set of computational hypotheses regarding cortical interactions with basal ganglia and thalamic nuclei. These hypotheses specify how specialized microcircuits solve learning and control problems inherent to the brain’s parallel design.}
}
@incollection{BURTEA20231643,
title = {Safe deployment of reinforcement learning using deterministic optimization over neural networks},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {1643-1648},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50261-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152740502614},
author = {Radu-Alexandru Burtea and Calvin Tsay},
keywords = {Constrained reinforcement learning, Optimization of neural network surrogates, Supply chain optimization, Optimization and machine learning toolkit},
abstract = {Enabling reinforcement learning (RL) to explicitly consider constraints is important for safe deployment in real-world process systems. This work exploits recent developments in deep RL and optimization over trained neural networks to introduce algorithms for safe training and deployment of RL agents. We show how optimization over trained neural-network state-action value functions (i.e., a critic function) can explicitly incorporate constraints and describe two corresponding RL algorithms: the first uses constrained optimization of the critic to give optimal actions for train- ing an actor, while the second guarantees constraint satisfaction by directly implementing actions from optimizing a trained critic model. The two algorithms are tested on a supply chain case study from OR-Gym and are compared against state-of-the-art algorithms TRPO, CPO, and RCPO.}
}
@article{AUNG2022182,
title = {Planning sequential interventions to tackle depression in large uncertain social networks using deep reinforcement learning},
journal = {Neurocomputing},
volume = {481},
pages = {182-192},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.01.030},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222000467},
author = {Aye Phyu Phyu Aung and Senthilnath Jayavelu and Xiaoli Li and Bo An},
keywords = {Mental state propagation, Dynamic influence maximization, Deep reinforcement learning, Discrete particle swarm optimization, Deep Q-learning},
abstract = {Studies, with the increasing concern for mental health, have shown that interventions along with social support can reduce stress and depression. However, counselling centers do not have enough resources to provide counselling and social support to all the participants in their interest. This paper helps social support organizations (e.g., university counselling centers) sequentially select the participants for interventions. Meanwhile, Deep Reinforcement Learning (DRL) has shown significant success in learning an efficient policy for sequential decision-making problems in both fully observable environments and partially observable environments with small action space. In this paper, we consider emotion propagation from other neighbours of the influencees, initial uncertainties of mental states and influence in the student network. We propose a new architecture called DRLPSO (Deep Reinforcement Learning with Particle Swarm Optimization) to enhance learning performance in a partially observable environment with a large state and action space. DRLPSO consists of two stages: the Discrete Particle Swarm Optimization (DPSO) and Deep Q-learning integrated with Long Short-Term Memory (DQ-LSTM). In the first stage, we apply DPSO by initializing n particles that converge to multiple optimal actions for each belief state. In the second stage, the action with the best Q-value from the DPSO action set is executed to obtain belief and observation (history of action). We evaluated the proposed method empirically with the simulated student networks with mental state propagation compared to the state-of-the-art algorithms. The experimental results demonstrate that DRLPSO outperforms the state-of-the-art DRL methods by an average of 32%.}
}
@article{WU2023115208,
title = {Deep reinforcement learning with dynamic window approach based collision avoidance path planning for maritime autonomous surface ships},
journal = {Ocean Engineering},
volume = {284},
pages = {115208},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.115208},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823015925},
author = {Chuanbo Wu and Wangneng Yu and Guangze Li and Weiqiang Liao},
keywords = {Ship collision avoidance, Dynamic window approach, Deep reinforcement learning, Maritime autonomous surface ships},
abstract = {Automatic obstacle avoidance technology is one of the key technologies for ship intelligence. The purpose of this paper is to investigate the obstacle avoidance problem of maritime autonomous surface ships(MASS) in a complex offshore environment, and an obstacle avoidance strategy based on deep reinforcement learning and a dynamic window algorithm was proposed. To solve the collision avoidance problems that may occur during intelligent ship navigation, the action space of the proximal policy optimization (PPO) algorithm is defined according to the description of ship motion by linear and angular velocity in the dynamic window approach (DWA). The maximum detection distance of the MASS is utilized to construct the ship safety domain, which determines the state space containing the information of this ship and the nearest obstacle. To solve the problem of sparse reward, the reward function of the PPO is improved by combining the evaluation functions for distance, velocity and heading in the DWA. To verify the effectiveness of the algorithm, simulation experiments are performed in various situations. It is also shown that the improved algorithm can make the optimal collision avoidance decision from the complex environment and can effectively realize autonomous collision avoidance path planning for the MASS.}
}
@article{KHOUZAIMI201893,
title = {A methodology for turn-taking capabilities enhancement in Spoken Dialogue Systems using Reinforcement Learning},
journal = {Computer Speech & Language},
volume = {47},
pages = {93-111},
year = {2018},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2017.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0885230816302376},
author = {Hatim Khouzaimi and Romain Laroche and Fabrice Lefèvre},
keywords = {Spoken Dialogue Systems, Turn-taking, Incremental dialogue, Reinforcement Learning},
abstract = {This article introduces a new methodology to enhance an existing traditional Spoken Dialogue System (SDS) with optimal turn-taking capabilities in order to increase dialogue efficiency. A new approach for transforming the traditional dialogue architecture into an incremental one at a low cost is presented: a new turn-taking decision module called the Scheduler is inserted between the Client and the Service. It is responsible for handling turn-taking decisions. Then, a User Simulator which is able to interact with the system using this new architecture has been implemented and used to train a new Reinforcement Learning turn-taking strategy. Compared to a non-incremental and a handcrafted incremental baselines, it is shown to perform better in simulation and in a real live experiment.}
}
@article{TONG2023536,
title = {Multi-type task offloading for wireless Internet of Things by federated deep reinforcement learning},
journal = {Future Generation Computer Systems},
volume = {145},
pages = {536-549},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001383},
author = {Zhao Tong and Jiake Wang and Jing Mei and Kenli Li and Wenbin Li and Keqin Li},
keywords = {Deep reinforcement learning, Federated learning, Internet of Things (IoT), Mobile edge computing (MEC), Multi-type task offloading, Service experience guarantee},
abstract = {With the popularity of Internet of Things (IoT) smart devices, the amount of data generated by these devices has grown rapidly. In these mobile edge computing (MEC) environments, it is not only important to save time and energy in offloading tasks, but also to protect user data. In this paper, due to the dynamics and complexity of the system, a multi-type task offloading based on a multi-capability federated deep Q-network (M2FD) algorithm is proposed to optimize the bi-objective performance. The algorithm consists of two parts, federated learning protects user privacy by transmitting model for training instead of data, and deep reinforcement learning trains model accuracy and identifies suitable offloading nodes with heterogeneous capabilities for multi-type tasks. In addition, under the constraints of the service experience guarantee (SEG) model, the tasks are offloaded with the goal of improving system utility while reducing system cost. Experiments show that the M2FD increases system utility, guarantees privacy, and reduces task response time and energy consumption.}
}
@article{JOHNSON20051163,
title = {Hippocampal replay contributes to within session learning in a temporal difference reinforcement learning model},
journal = {Neural Networks},
volume = {18},
number = {9},
pages = {1163-1171},
year = {2005},
note = {Computational Theories of the Functions of the Hippocampus},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2005.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005001991},
author = {Adam Johnson and A. David Redish},
abstract = {Temporal difference reinforcement learning (TDRL) algorithms, hypothesized to partially explain basal ganglia functionality, learn more slowly than real animals. Modified TDRL algorithms (e.g. the Dyna-Q family) learn faster than standard TDRL by practicing experienced sequences offline. We suggest that the replay phenomenon, in which ensembles of hippocampal neurons replay previously experienced firing sequences during subsequent rest and sleep, may provide practice sequences to improve the speed of TDRL learning, even within a single session. We test the plausibility of this hypothesis in a computational model of a multiple-T choice-task. Rats show two learning rates on this task: a fast decrease in errors and a slow development of a stereotyped path. Adding developing replay to the model accelerates learning the correct path, but slows down the stereotyping of that path. These models provide testable predictions relating the effects of hippocampal inactivation as well as hippocampal replay on this task.}
}
@article{DAI2023108771,
title = {Optimization method of power grid material warehousing and allocation based on multi-level storage system and reinforcement learning},
journal = {Computers and Electrical Engineering},
volume = {109},
pages = {108771},
year = {2023},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2023.108771},
url = {https://www.sciencedirect.com/science/article/pii/S0045790623001957},
author = {Zhou Dai and Peng Xie and Yingmin Huang and Guixian Cheng and Wuqin Tang and Kemin Zou and Ningjun Duan and Lianming Zou and Jun Ning and Jin Lian and Nan Yang},
keywords = {Power grid materials, Storage control, Reinforcement learning, Optimization},
abstract = {Businesses may save money and make better use of their resources by exercising reasonable control over the storage of power materials. Poor timeliness and ineffectiveness are the result of the old model's inability to fully use a significant volume of acquired data and information. This research presents a reinforcement learning-based material storage control model for the power grid to solve these issues. In order to achieve zero storage, real-time tracking of consumables is implemented. Material for large disasters need manufacturer coordination for storage. The reinforcement learning approach is used to dynamically stock and distribute emergency supplies. The three-tiered dynamic storage approach based on reinforcement learning that is suggested in this study is shown to be very efficient and significantly cuts operational expenses for power grid firms by verifying particular data.}
}
@article{LIU2022101496,
title = {A greedy-model-based reinforcement learning algorithm for Beyond-5G cooperative data collection},
journal = {Physical Communication},
volume = {50},
pages = {101496},
year = {2022},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2021.101496},
url = {https://www.sciencedirect.com/science/article/pii/S1874490721002305},
author = {Xinyu Liu and Qingfeng Zhou and Chi-Tsun Cheng and Chanzi Liu},
keywords = {Data collection, Cooperative, Mobile sink, UAVs, Beyond-5G, Intelligent communication, Reinforcement learning},
abstract = {Data collection is an essential part of Beyond-5G and Internet of Things applications. In urban area, heterogeneous access points such as Wi-Fi routers and base stations can meet the required communication coverage and bandwidth in data collection processes. However, in remote area, without communication infrastructures, it is hard to guarantee the communication quality of a large-scale data aggregation network. An existing approach is to use an unmanned aerial vehicle (UAV) to act as a mobile sink to perform data collection and increase the coverage of intelligent wireless sensing and communications. The efficiency and the reliability of such a UAV-assisted data collection system can be significantly enhanced with an intelligent cooperative strategy for the sensors deployed in the field to communicate with the UAV. Furthermore, an energy-efficient trajectory planning algorithm is crucial to address the physical limitations of the UAV in this application. In this paper, a data collection process is modeled as a Markov decision process (MDP). The paper begins with proposing two heuristic greedy algorithms, namely distance-greedy (DG) algorithm and rate-greedy (RG) algorithm, which are designed based on prior knowledge of the system and can guarantee the completion of the data collection process in a remote area without the help of fixed communication infrastructures. Based on the outcomes, a multi-agent greedy-model-based reinforcement learning (MG-RL) algorithm is proposed, which specifically designs the environmental state and the reward scheme, and introduces multiple UAVs with different parameters to explore environments in parallel to accelerate the training. In conclusion, the two proposed greedy algorithms have lower complexity of implementation while the proposed MG-RL algorithm yields practical UAVs’ flight trajectories and shortens the time for completing a data collection task.}
}
@article{KADRI202220,
title = {Multi-objective biogeography-based optimization and reinforcement learning hybridization for network-on chip reliability improvement},
journal = {Journal of Parallel and Distributed Computing},
volume = {161},
pages = {20-36},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521002124},
author = {Nassima Kadri and Mouloud Koudil},
keywords = {Network-on chip, Reliability, Task migration, Spare placement, Multi-objective optimization, Reinforcement learning},
abstract = {Reliability is increasingly a major concern in network-on-a-chip (NoC) design, alongside increased performance demands from new applications and the need for continued miniaturization of silicon technology. In this article, we look at the task migration mechanism, used to recover from permanent processing element (PE) failures in NoCs, by remapping tasks performed on faulty cores to spare ones. An innovative reliability-aware task mapping technique is presented, based on a hybridization between Multi-Objective Optimization (MOO) and Reinforcement Learning (RL). It takes place in two steps. In the first, a set of optimal remapping solutions for different failure scenarios is generated at design-time, using a Biogeography-Based Multi-Objective Optimization algorithm, while considering communication energy and migration costs. In the second step, an artificial neural network agent is trained to select the best remapping solution, from those generated at design-time, to recover from execution failures at run-time. Experiments were carried out to evaluate our technique for different sizes of networks and on different benchmarks. The results obtained show that the technique based on the hybridization MOO_RL brings a great improvement in the reliability of the NoC and achieves a good compromise between reliability and performance. It also guarantees a reduction of the overhead caused by the storage space of the remapping solutions, compared to the existing solutions.}
}
@article{QIN2021107252,
title = {A novel reinforcement learning-based hyper-heuristic for heterogeneous vehicle routing problem},
journal = {Computers & Industrial Engineering},
volume = {156},
pages = {107252},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107252},
url = {https://www.sciencedirect.com/science/article/pii/S036083522100156X},
author = {Wei Qin and Zilong Zhuang and Zizhao Huang and Haozhe Huang},
keywords = {Vehicle routing problem, Heterogeneous fleet, Hyper-heuristic, Reinforcement learning},
abstract = {This study investigates a practical heterogeneous vehicle routing problem that involves routing a predefined fleet with different vehicle capacities to serve a series of customers to minimize the maximum routing time of vehicles. The comprehensive utilization of different types of vehicles brings great challenges for problem modeling and solving. In this study, a mixed-integer linear programming (MILP) model is formulated to obtain optimal solutions for small-scale problems. To further improve the quality of solutions for large-scale problems, this study develops a reinforcement learning-based hyper-heuristic, which introduces several meta-heuristics with different characteristics as low-level heuristics and policy-based reinforcement learning as a high-level selection strategy. Moreover, deep learning is used to extract hidden patterns within the collected data to combine the advantages of low-level heuristics better. Numerical experiments have been conducted and results indicate that the proposed algorithm exceeds the MILP solution on large-scale problems and outperforms the existing meta-heuristic algorithms.}
}
@article{TAKASE2022100409,
title = {Stability-certified reinforcement learning control via spectral normalization},
journal = {Machine Learning with Applications},
volume = {10},
pages = {100409},
year = {2022},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2022.100409},
url = {https://www.sciencedirect.com/science/article/pii/S2666827022000846},
author = {Ryoichi Takase and Nobuyuki Yoshikawa and Toshisada Mariyama and Takeshi Tsuchiya},
keywords = {Reinforcement learning, Stability, Spectral normalization, Linear matrix inequality},
abstract = {In this study, two types of methods from different perspectives based on spectral normalization (SN) are described for ensuring the stability of a feedback system controlled by a neural network (NN). The first one is that the L2 gain of the feedback system is bounded less than 1 to satisfy a stability condition derived from the small-gain theorem. When explicitly including the stability condition, the first type of method may provide an insufficient performance on the NN controller due to its strict stability condition. To overcome this difficulty, the second type of method is proposed, ensuring local stability with a larger region of attraction. In this second type, the stability is ensured by solving linear matrix inequalities after training the NN controller. SN improves the feasibility of the a posteriori stability test by constructing tighter local sectors. Numerical experiments show that the second type of method provides sufficient performance compared with the first one and ensures sufficient stability compared with existing reinforcement learning algorithms.11Project page: https://sites.google.com/g.ecc.u-tokyo.ac.jp/stability-certified-rl-via-sn.}
}
@article{COBO2014103,
title = {Abstraction from demonstration for efficient reinforcement learning in high-dimensional domains},
journal = {Artificial Intelligence},
volume = {216},
pages = {103-128},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000861},
author = {Luis C. Cobo and Kaushik Subramanian and Charles L. Isbell and Aaron D. Lanterman and Andrea L. Thomaz},
keywords = {Reinforcement learning, Learning from demonstration, Dimensionality reduction, Function approximation},
abstract = {Reinforcement learning (RL) and learning from demonstration (LfD) are two popular families of algorithms for learning policies for sequential decision problems, but they are often ineffective in high-dimensional domains unless provided with either a great deal of problem-specific domain information or a carefully crafted representation of the state and dynamics of the world. We introduce new approaches inspired by these two techniques, which we broadly call abstraction from demonstration. Our first algorithm, state abstraction from demonstration (AfD), uses a small set of human demonstrations of the task the agent must learn to determine a state-space abstraction. Our second algorithm, abstraction and decomposition from demonstration (ADA), is additionally able to determine a task decomposition from the demonstrations. These abstractions allow RL to scale up to higher-complexity domains, and offer much better performance than LfD with orders of magnitude fewer demonstrations. Using a set of videogame-like domains, we demonstrate that using abstraction from demonstration can obtain up to exponential speed-ups in table-based representations, and polynomial speed-ups when compared with function approximation-based RL algorithms such as fitted Q-learning and LSPI.}
}
@article{HE2023109985,
title = {A reinforcement learning method for scheduling service function chains with multi-resource constraints},
journal = {Computer Networks},
volume = {235},
pages = {109985},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109985},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623004309},
author = {Rui He and Bangbang Ren and Junjie Xie and Deke Guo and Yuwen Zhou and Laiping Zhao and Yong Li},
keywords = {Service function chain, Deep reinforcement learning, Scheduling},
abstract = {Traditional networks are usually equipped with many dedicated middleboxes to provide various network services. Though these hardware-based devices certainly improve network performance, they are usually expensive and difficult to upgrade. To overcome this shortcoming, network function virtualization (NFV), which accomplishes network services in the form of virtual network functions (VNF) has been presented. Compared to middleboxes, the VNFs are easy to deploy and migrate. Usually, multiple VNFs are chained in a specified order as a service function chain (SFC) to serve a given flow. There are many works to schedule SFCs to minimize the average flow completion time. However, they only consider single resource limitation. In this paper, we are committed to addressing the problem of multi-resource SFC scheduling (MR-SFCS) and minimizing the average flow completion time. We formulate this problem with an Integer Linear Programming (ILP) model and prove its NP-hardness. To well tackle this problem, we propose an approach based on deep reinforcement learning (DRL), which has specific reward design and state representations. Besides, we extend the offline approach to online SFC scheduling. The experiment results demonstrate that our DRL method can significantly reduce the average flow completion time and achieves a cost saving of 69.07% against the benchmark method.}
}
@article{PAN2023113166,
title = {Data-driven distributed formation control of under-actuated unmanned surface vehicles with collision avoidance via model-based deep reinforcement learning},
journal = {Ocean Engineering},
volume = {267},
pages = {113166},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.113166},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822024490},
author = {Chao Pan and Zhouhua Peng and Lu Liu and Dan Wang},
keywords = {Unmanned surface vehicles, Model-based deep reinforcement learning, Distributed formation control, Collision avoidance},
abstract = {This paper addresses the distributed formation control with collision avoidance for multiple under-actuated unmanned surface vehicles (USVs) subject to fully unknown models. A fully data-driven distributed control approach is proposed for multiple USVs to achieve a desired formation based on model-based deep reinforcement learning. Specifically, a deep neural network is firstly trained to approximate the dynamic model of each USV by utilizing recorded input and output data. Then, by taking collision avoidance requirements into account, the model predictive formation controllers are proposed for USVs to achieve the safe formation control task based on the learned vehicle dynamics. It is shown that after learning with offline and online data, the proposed fully data-driven distributed controllers are able to achieve a safe formation. Simulations results are given to substantiate the feasibility and efficacy of the proposed model-based deep reinforcement learning method for distributed formation control of under-actuated USVs with fully unknown models.}
}
@article{CHEN2023120145,
title = {Multi-objective reinforcement learning approach for trip recommendation},
journal = {Expert Systems with Applications},
volume = {226},
pages = {120145},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120145},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423006474},
author = {Lei Chen and Guixiang Zhu and Weichao Liang and Youquan Wang},
keywords = {Recommender system, Deep neural network, Reinforcement learning, Attention mechanism},
abstract = {Trip recommendation is an intelligent service that provides personalized itinerary plans for tourists in unfamiliar cities. It aims to construct a series of ordered POIs that maximizes user travel experiences with temporal and spatial constraints. When appending a candidate POI to the recommended trip, it is critical to capture users’ dynamic preferences according to real-time context. Meanwhile, the diversity and popularity of the POIs in the personalized trip play an important role in users’ selections. To address these challenges, in this article, we propose a MORL-Trip (short for Multi-Objective Reinforcement Learning for Trip Recommendation) approach. MORL-Trip models the personalized trip recommendation as a Markov Decision Process (MDP), and implements it upon the Actor-Critic framework. MORL-Trip enhances the state representation with sequential information, geographic information and order information to learn user’s context from real-time location. In addition, MORL-Trip augments the standard Critic component by designing a composite reward function to enforce three principal objectives: accuracy, popularity and diversity. We conduct extensive experiments on the public datasets and compare the performance of MORL-Trip with the most advanced methods to verify its superiority, and show the importance of reinforcing popularity and diversity as complementary objectives in the personalized trip recommendation.}
}
@article{CARVALHO2019205,
title = {Autonomous power management in mobile devices using dynamic frequency scaling and reinforcement learning for energy minimization},
journal = {Microprocessors and Microsystems},
volume = {64},
pages = {205-220},
year = {2019},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2018.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0141933118301273},
author = {Sidartha A.L. Carvalho and Daniel C. Cunha and Abel G. Silva-Filho},
keywords = {Power management, Machine learning, Low power, Mobile power optimization, Android},
abstract = {Embedded systems execute applications that execute hardware differently depending on the computation task, generating time-varying workloads. Energy minimization can be reached by using the low-power central processing unit (CPU) frequency for each workload. We propose an autonomous and online approach, capable of reducing energy consumption from adaptation to workload variations even in an unknown environment. In this approach, we improved the AEWMA algorithm into a new algorithm called AEWMA-MSE, adding new functionality to detect workload changes and demonstrating why it is better to use statistical analysis for real user cases in a mobile environment. Also, a new power model for mobile devices based on k-NN algorithm for regression was proposed and validated proving to have a better trade-off between execution time and precision than neural networks and linear regression-based models. AEWMA-MSE and the proposed power model are integrated into a novel algorithm for energy management based on reinforcement learning that suitably selects the appropriate CPU frequency based on workload predictions to minimize energy consumption. The proposed approach is validated through simulation by using real smartphone data from an ARM Cortex A7 processor used in a commercial smartphone. Our proposal proved to have an improvement in the Q-learning cost function and can effectively minimize the average energy consumption by 21% and up to 29% when compared to the already existing approaches.}
}
@article{LAFARGE20211,
title = {Autonomous closed-loop guidance using reinforcement learning in a low-thrust, multi-body dynamical environment},
journal = {Acta Astronautica},
volume = {186},
pages = {1-23},
year = {2021},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2021.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0094576521002460},
author = {Nicholas B. LaFarge and Daniel Miller and Kathleen C. Howell and Richard Linares},
keywords = {Spacecraft autonomy, Closed-loop guidance, Reinforcement learning, Low-thrust, Cislunar space, Neural network control},
abstract = {Onboard autonomy is an essential component in enabling increasingly complex missions into deep space. In nonlinear dynamical environments, computationally efficient guidance strategies are challenging. Many traditional approaches rely on either simplifying assumptions in the dynamical model or on abundant computational resources. This research effort employs reinforcement learning, a subset of machine learning, to produce a ‘lightweight’ closed-loop controller that is potentially suitable for onboard low-thrust guidance in challenging dynamical regions of space. The results demonstrate the controller’s ability to directly guide a spacecraft despite large initial deviations and to augment a traditional targeting guidance approach. The proposed controller functions without direct knowledge of the dynamical model; direct interaction with the nonlinear equations of motion creates a flexible learning scheme that is not limited to a single force model, mission scenario, or spacecraft. The learning process leverages high-performance computing to train a closed-loop neural network controller. This controller may be employed onboard to autonomously generate low-thrust control profiles in real-time without imposing a heavy workload on a flight computer. Control feasibility is demonstrated through sample transfers between Lyapunov orbits in the Earth–Moon system. The sample low-thrust controller exhibits remarkable robustness to perturbations and generalizes effectively to nearby motion. Finally, the flexibility of the learning framework is demonstrated across a range of mission scenarios and low-thrust engine types.}
}
@article{OH2022117932,
title = {Reinforcement learning-based expanded personalized diabetes treatment recommendation using South Korean electronic health records},
journal = {Expert Systems with Applications},
volume = {206},
pages = {117932},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117932},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422011733},
author = {Sang Ho Oh and Jongyoul Park and Su Jin Lee and Seungyeon Kang and Jeonghoon Mo},
keywords = {Reinforcement learning, Precision medicine, Decision making, Data-driven optimization, Electronic health records},
abstract = {Currently, electronic medical records are becoming more accessible to a growing number of researchers seeking to develop personalized healthcare recommendations to aid physicians in making better clinical decisions and treating patients. As a result, clinical decision research has become more focused on data-driven optimization. In this study, we analyze Korean patients' electronic health records—including medical history, medications, laboratory tests, and more information—shared by the national health insurance system. We aim to develop a reinforcement learning-based expanded treatment recommendation model using the health records of South Korean citizens to assist physicians. This study is significant in that expert and intelligent systems harmoniously solve the problem that directly addresses many clinical challenges in prescribing proper diabetes medication when assessing the physical state of diabetes patients. Reinforcement learning is a mechanism for determining how agents should behave in a given environment to maximize a cumulative reward. The basic model for a reinforcement learning design environment is the Markov decision process (MDP) model. Although it is effective and easy to use, the MDP model is limited by dimensionality, i.e., many details about the patients cannot be considered when building the model. To address this issue, we applied a contextual bandits approach to create a more practical model that can expand states and actions by considering several details that are crucial for patients with diabetes. Finally, we validated the performance of the proposed contextual bandits model by comparing it with existing reinforcement-learning algorithms.}
}
@article{BEGHI201713754,
title = {Reinforcement Learning Control of Transcritical Carbon Dioxide Supermarket Refrigeration Systems},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {13754-13759},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.2565},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317334924},
author = {Alessandro Beghi and Mirco Rampazzo and Stefano Zorzi},
keywords = {Commercial Refrigeration Systems, CO, Control, Reinforcement Learning},
abstract = {Commercial refrigeration systems consume a substantial amount of electrical energy, resulting in high indirect global warming impact due to greenhouse gases emissions. The multitude of different system configurations, system complexity, component wear, and changing operating conditions make efficient operation of this kind of refrigeration systems a difficult task. This paper presents an investigation of machine learning for supervisory control of a supermarket refrigeration system. In particular, a reinforcement learning algorithm for a CO2 booster refrigeration system is designed by exploiting the Matlab-based “SRSim” simulation tool. The reinforcement learning controller learns to operate the refrigeration system based on the interaction with the environment. The analysis shows that learning control is a feasible model-free technique to find a suitable control strategy for demand-side management in a smart grid scenario.}
}
@article{HOSSAIN2020291,
title = {Edge computational task offloading scheme using reinforcement learning for IIoT scenario},
journal = {ICT Express},
volume = {6},
number = {4},
pages = {291-299},
year = {2020},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2020.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405959520301752},
author = {Md. Sajjad Hossain and Cosmas Ifeanyi Nwakanma and Jae Min Lee and Dong-Seong Kim},
keywords = {Edge computing, Industrial IoT, Offloading, Reinforcement learning},
abstract = {In this paper, end devices are considered here as agent, which makes its decisions on whether the network will offload the computation tasks to the edge devices or not. To tackle the resource allocation and task offloading, paper formulated the computation resource allocation problems as a sum cost delay of this framework. An optimal binary computational offloading decision is proposed and then reinforcement learning is introduced to solve the problem. Simulation results demonstrate the effectiveness of this reinforcement learning based scheme to minimize the offloading cost derived as computation cost and delay cost in industrial internet of things scenarios.}
}
@article{WANG2023123655,
title = {Closed-loop forced heat convection control using deep reinforcement learning},
journal = {International Journal of Heat and Mass Transfer},
volume = {202},
pages = {123655},
year = {2023},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2022.123655},
url = {https://www.sciencedirect.com/science/article/pii/S0017931022011243},
author = {Yi-Zhe Wang and Xian-Jun He and Yue Hua and Zhi-Hua Chen and Wei-Tao Wu and Zhi-Fu Zhou},
keywords = {Deep reinforcement learning, Active thermal control, Forced heat convection, Heat transfer enhancement, Machine learning},
abstract = {In this paper, deep reinforcement learning (DRL) is applied on forced convection control of conjugate heat transfer systems governed by the coupled Navier-Stokes and heat transport equations. A novel value-based deep Q-network (DQN) integrating with three advanced techniques is utilized for identifying underlying heat and mass transfer mechanism in the interaction of close-loop active control. The effectiveness and feasibility of DRL based forced convection control is firstly demonstrated by studying a two-dimensional cooling problem, where a single heat source immersed in an open cavity. A more complex testbed with multiple immersed heat source and multiple degree-of-freedom control is investigated further, which shows outstanding capability of DRL algorithm to learn strong nonlinear control strategy, and compared to conventional control method, the novel DRL approach has ability to obtain better cooling effect of about 8 degrees lower. Then the robustness of the method is verified by applying trained agent on several situations with unknown physical conditions and adding additional customized control requirements/aims. Moreover, sensitivity analysis confirms that the trained agent has a certain generalization ability on geometric configuration, which strengthens the confidence of applying the DRL-based active heat transfer control method on practical applications. Current research demonstrates the efficiency and applicability of the DRL based active thermal control on strong-nonlinearity system, and also encourages further investigations on more complex and practical problems.}
}
@article{MAHBOD2022119392,
title = {Energy saving evaluation of an energy efficient data center using a model-free reinforcement learning approach},
journal = {Applied Energy},
volume = {322},
pages = {119392},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119392},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922007309},
author = {Muhammad Haiqal Bin Mahbod and Chin Boon Chng and Poh Seng Lee and Chee Kong Chui},
keywords = {Data center, Data center cooling strategies, Cooling control, Deep reinforcement learning},
abstract = {To reduce cooling energy consumption, data centers are recommended to raise temperature setpoints of server intake. However, in tropical climates, Data Center operators are still found to be operating at lower temperatures. In this paper, we demonstrate that using a floating setpoint with a lowered temperature value for tropical climates reduces the overall energy consumption of Data Centers as opposed to raising the temperature in a static manner. We achieve this by applying a deep reinforcement learning algorithm to a hybrid data center model that was built from data collected off a highly efficient data center. This generates an optimal control strategy which minimizes the costs of energy consumption while operating within the required set of operational constraints. Following which, we evaluate the behavior of the control strategy to account for the exact sources of energy savings. The deep reinforcement learning algorithm learns by continually interacting with the built Data Center model without any prior knowledge of the Data Center. The algorithm is trained under the full-load and the part-load configuration of the Data Center. Testing results show that further energy savings of up to 3% and 5.5% (under full load and part load respectively) can be achieved with targeted cooling provisioning while operating within constraints in an already cooling-efficient Data Center. We find that while building level optimization studies of Data Centers generally improve energy efficiency, the source of energy savings is not well accounted for. Consequently, our studies show that the reduction of server fan usage and not the reduction of cooling energy consumption is the main contributor of energy savings in a deep reinforcement learning-driven data center operating in the tropics.}
}
@article{CHOWDHURY2023108393,
title = {Entropy-maximizing TD3-based reinforcement learning for adaptive PID control of dynamical systems},
journal = {Computers & Chemical Engineering},
volume = {178},
pages = {108393},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108393},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423002636},
author = {Myisha A. Chowdhury and Saif S.S. Al-Wahaibi and Qiugang Lu},
keywords = {PID control, Parameter tuning, Deep reinforcement learning, TD3, EMTD3},
abstract = {The proper tuning of proportional–integral–derivative (PID) control is critical for satisfactory control performance. However, existing tuning methods are often time-consuming and require system models that are difficult to obtain for complex processes. To this end, automatic PID tuning, particularly that based on deep reinforcement learning, eliminates the necessity of a system model by treating the PID tuning as a black-box optimization. However, these methods suffer from low sample efficiency. In this paper, we present an entropy-maximizing twin-delayed deep deterministic policy gradient (EMTD3) method for automatic PID tuning. In our method, an entropy-maximizing stochastic actor is deployed at the beginning to ensure sufficient explorations, followed by a deterministic actor to focus on local exploitation. Such a hybrid approach can enhance the sample efficiency to facilitate the PID tuning. Extensive simulation studies are provided to show the superior performance of the proposed method relative to other methods on data efficiency, adaptivity, and robustness.}
}
@article{XU202373,
title = {A novel deep reinforcement learning architecture for dynamic power and bandwidth allocation in multibeam satellites},
journal = {Acta Astronautica},
volume = {204},
pages = {73-82},
year = {2023},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2022.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0094576522006907},
author = {Jing Xu and Zhongtian Zhao and Lei Wang and Yizhai Zhang},
keywords = {Deep reinforcement learning, Dynamic resource allocation, Genetic algorithm, Multibeam satellite system, Proximal policy optimization},
abstract = {Due to the explosive growth and dynamic change of user demand, an efficient power and bandwidth allocation algorithm is quite essential for multibeam satellites with flexible digital payloads. To suit the real-time use of the multibeam satellites communication, we build a novel deep reinforcement learning (DRL) architecture for dynamic power and bandwidth allocation. For minimizing unmet system capacity (USC), the proposed DRL architecture adopts proximal policy optimization algorithm to directly allocate the resource in continuous space. Under the proposed DRL architecture, the DRL strategy for joint power and bandwidth allocation (named as pbDRL) reaps the best USC performance in comparison with the separate power or bandwidth allocation method of pDRL and bDRL. Besides, by implementing the allocation decision determined by pbDRL into the initial population of the existing genetic algorithm (GA), we also develop another improved GA, namely drlGA. Numerical results verify that (i) pbDRL outperforms the existing GA and PSO; (ii) pbDRL achieves comparable USC performance within a significantly reduced computation time compared with the existing optimized GA method; (iii) in comparison with the other three related heuristic algorithms, drlGA derives improved USC performance within the same computation time. These results draw a conclusion that the developed pbDRL and drlGA approaches are capable to meet the requirements of high timeliness and desirable demand satisfaction, respectively.}
}
@article{DUTTA2023103,
title = {A multiple neural network and reinforcement learning-based strategy for process control},
journal = {Journal of Process Control},
volume = {121},
pages = {103-118},
year = {2023},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2022.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S095915242200227X},
author = {Debaprasad Dutta and Simant R. Upreti},
keywords = {Artificial neural network, Multiple neural network, Reinforcement learning, Deep deterministic policy gradient, Process control},
abstract = {In this study, we propose a novel process control strategy by assimilating multiple neural network (MNN) and reinforcement learning (RL) to generate an MNNRL controller. The artificial neural networks (ANNs) constituting the MNN are trained using repositories of optimal control and state data to predict control actions to attain or stay at a set-point. This ability is significantly improved by developing an RL component for the MNNRL controller to provide enhanced set-point tracking, robustness against disturbances, and real-time learning to successfully adapt in the presence of persistent parameter upsets. The controller performance is examined in three different case studies utilizing the simulation of continuous processes with varying degrees of nonlinearity. Relative to both MNN and nonlinear model predictive control (NMPC), the MNNRL controller is found to provide better performance with about 32% less lower integral absolute error (IAE), and 50% reduced settling time with suppressed over-/under-shoots in controlled variables. Further, it is observed that the controller is fairly robust against disturbances, and can adapt via real-time learning to altered process dynamics caused by any persistent process upsets. The controller computation time is observed to be about an order of magnitude less in comparison to NMPC.}
}
@article{AYAS2023106050,
title = {A reinforcement learning approach to Automatic Voltage Regulator system},
journal = {Engineering Applications of Artificial Intelligence},
volume = {121},
pages = {106050},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106050},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623002348},
author = {Mustafa Sinasi Ayas and Ali Kivanc Sahin},
keywords = {Automatic voltage regulator (AVR), Reinforcement learning (RL) control, Deep deterministic policy gradient (DDPG) agent, Actor–critic, Frequency response},
abstract = {An Automatic Voltage Regulator (AVR) system utilized to keep the terminal voltage of a synchronous generator at the desired level has received much attention among researchers. Designing an efficient and robust control scheme for the AVR system to maintain a specified voltage level is an important research area. From the control area perspective, reinforcement learning, an adaptive optimal control method, has received increasing attention in reference tracking problems. This article discusses a reinforcement learning approach to an AVR system and its experimental validation. A deep deterministic policy gradient (DDPG) agent working in continuous-time is designed offline to improve dynamic system characteristics of the AVR system besides its robustness against load disturbance, parameter uncertainties, and reference change. In the DDPG agent design process, the limits of the produced control signal are taken into account to perform a feasible simulation similar to a real-time application. The performance of the proposed learning-based controller is analyzed in three categories: transient and steady-state responses, stability analysis, and robustness analysis against parameter uncertainties, reference change, and load disturbance. A comparison with recently published papers employing Fuzzy-PID, PID-F, PIλDND2N2, PIDD2 ,and PID controllers in which various heuristic optimization algorithms were employed to optimally tune the controller parameters is made. Furthermore, to demonstrate that the behavior of the learning-based approach provides a stable and satisfactory performance, it is analyzed for a real synchronous generator connected to a 230 kV network using Matlab/Simulink environment. The results presented in this paper indicate that the proposed learning-based controller ensures the stability of the AVR system, significantly improves the regulating performance, and most impressively, is robust against parameter uncertainties, reference change, and load disturbance.}
}
@article{LEI2022109426,
title = {QoS-oriented media access control using reinforcement learning for next-generation WLANs},
journal = {Computer Networks},
volume = {219},
pages = {109426},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109426},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622004601},
author = {Jianjun Lei and Lu Li and Ying Wang},
keywords = {802.11ax, MAC, Reinforcement learning, Orthogonal frequency division multiple access, Quality of service},
abstract = {Orthogonal frequency division multiple access (OFDMA) is introduced in IEEE 802.11ax to satisfy massive transmission demands. However, the uplink OFDMA-based random access (UORA) mechanism provides poor quality of service (QoS), and restricts channel utilization efficiency, especially in the dynamic network environment. To solve these issues, in this study, the transmission period is decoupled into a contention stage and a transmission stage, and an intelligent media access control (MAC) algorithm with QoS-guaranteed for next-generation wireless local area networks (WLANs) is presented. Specifically, we consider stochastic and various traffic in time-varying wireless communication conditions and propose two contention window (CW) optimization mechanisms based on Q-learning (QL) and a deep Q-network (DQN) (referred to as QL-MAC and DQN-MAC) for static and dynamic network scenarios, respectively. Meanwhile, the access point (AP) acts as the reinforcement learning (RL) agent and centrally optimizes the CW for all stations to eventually maximize the system throughput. Furthermore, we provide the QoS-guaranteed channel access mechanism for different priority data traffic, in which high-priority traffic can obtain more channel access opportunities in the uplink contention stage. Simulation results demonstrate that the proposed algorithms can significantly improve the network performance in terms of convergence, throughput, delay, and fairness compared to the adaptive grouping-based two-stage mechanism (BTM) and double random access QoS-oriented OFDMA MAC (DRA-OFDMA) algorithm in various scenarios.}
}
@article{ZHANG2023126485,
title = {Offline reinforcement learning control for electricity and heat coordination in a supercritical CHP unit},
journal = {Energy},
volume = {266},
pages = {126485},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.126485},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222033710},
author = {Guangming Zhang and Chao Zhang and Wei Wang and Huan Cao and Zhenyu Chen and Yuguang Niu},
keywords = {Combined heat and power, Data-driven modeling, electricity–heat coordinated control, Variable load conditions, Offline reinforcement learning},
abstract = {With a high proportion of renewable energy generation connected to the power grid, the combined heat and power (CHP) units need to have flexible operation and control capabilities over a wide range of variable load conditions. In China, the proportion of supercritical combined heat and power (S-CHP) units gradually increases due to their high thermal efficiency. In this paper, we propose a data-driven environment modeling method and an offline reinforcement learning-based electricity–heat coordinated control approach for the wide and flexible load adjustment capabilities of the S-CHP unit. First of all, a modeling method based on the multiple multilayer perceptron (MLP) ensemble is proposed to address the possible over-fitting produced by a single MLP. Then, the state and reward are set considering the dynamic characteristics of the S-CHP unit. Moreover, policy training is implemented through a soft actor-critic algorithm with a maximum entropy model to ensure more robust search capabilities under variable load conditions and targets. The simulation results show that the generalization ability of the environment in the multiple MLP ensemble mode is more substantial than that in the single MLP mode. In addition, when the electric load command is between 267 MW and 325 MW, the offline reinforcement learning can significantly reduce the integral of absolute error matrices of the output parameters, demonstrating that the proposed strategy can achieve electricity–heat coordinated control under variable load conditions.}
}
@article{SANTIAGOJUNIOR2020106760,
title = {Hyper-Heuristics based on Reinforcement Learning, Balanced Heuristic Selection and Group Decision Acceptance},
journal = {Applied Soft Computing},
volume = {97},
pages = {106760},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106760},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620306980},
author = {Valdivino Alexandre de {Santiago Júnior} and Ender Özcan and Vinicius Renan de Carvalho},
keywords = {Hyper-heuristic, Reinforcement learning, Balanced heuristic selection, Group decision-making, Multi-objective evolutionary algorithms, Multi-objective optimisation},
abstract = {In this paper, we introduce a multi-objective selection hyper-heuristic approach combining Reinforcement Learning, (meta)heuristic selection, and group decision-making as acceptance methods, referred to as Hyper-Heuristic based on Reinforcement LearnIng, Balanced Heuristic Selection and Group Decision AccEptance (HRISE), controlling a set of Multi-Objective Evolutionary Algorithms (MOEAs) as Low-Level (meta)Heuristics (LLHs). Along with the use of multiple MOEAs, we believe that having a robust LLH selection method as well as several move acceptance methods at our disposal would lead to an improved general-purpose method producing most adequate solutions to the problem instances across multiple domains. We present two learning hyper-heuristics based on the HRISE framework for multi-objective optimisation, each embedding a group decision-making acceptance method under a different rule: majority rule (HRISE_M) and responsibility rule (HRISE_R). A third hyper-heuristic is also defined where both a random LLH selection and a random move acceptance strategy are used. We also propose two variants of the late acceptance method and a new quality indicator supporting the initialisation of selection hyper-heuristics using low computational budget. An extensive set of experiments were performed using 39 multi-objective problem instances from various domains where 24 are from four different benchmark function classes, and the remaining 15 instances are from four different real-world problems. The cross-domain search performance of the proposed learning hyper-heuristics indeed turned out to be the best, particularly HRISE_R, when compared to three other selection hyper-heuristics, including a recently proposed one, and all low-level MOEAs each run in isolation.}
}
@article{LIN2021106350,
title = {Collision-free path planning for a guava-harvesting robot based on recurrent deep reinforcement learning},
journal = {Computers and Electronics in Agriculture},
volume = {188},
pages = {106350},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106350},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921003677},
author = {Guichao Lin and Lixue Zhu and Jinhui Li and Xiangjun Zou and Yunchao Tang},
keywords = {Collision-free path planning, Reinforcement learning, Deep deterministic policy gradient, Obstacle detection, Harvesting robot},
abstract = {In unstructured orchard environments, picking a target fruit without colliding with neighboring branches is a significant challenge for guava-harvesting robots. This paper introduces a fast and robust collision-free path-planning method based on deep reinforcement learning. A recurrent neural network is first adopted to remember and exploit the past states observed by the robot, then a deep deterministic policy gradient algorithm (DDPG) predicts a collision-free path from the states. A simulation environment is developed and its parameters are randomized during the training phase to enable recurrent DDPG to generalize to real-world scenarios. We also introduce an image processing method that uses a deep neural network to detect obstacles and uses many three-dimensional line segments to approximate the obstacles. Simulations show that recurrent DDPG only needs 29 ms to plan a collision-free path with a success rate of 90.90%. Field tests show that recurrent DDPG can increase grasp, detachment, and harvest success rates by 19.43%, 9.11%, and 10.97%, respectively, compared to cases where no collision-free path-planning algorithm is implemented. Recurrent DDPG strikes a strong balance between efficiency and robustness and may be suitable for other fruits.}
}
@article{HORTELANO2023103669,
title = {A comprehensive survey on reinforcement-learning-based computation offloading techniques in Edge Computing Systems},
journal = {Journal of Network and Computer Applications},
volume = {216},
pages = {103669},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103669},
url = {https://www.sciencedirect.com/science/article/pii/S1084804523000887},
author = {Diego Hortelano and Ignacio {de Miguel} and Ramón J. Durán Barroso and Juan Carlos Aguado and Noemí Merayo and Lidia Ruiz and Adrian Asensio and Xavi Masip-Bruin and Patricia Fernández and Rubén M. Lorenzo and Evaristo J. Abril},
keywords = {Computation offloading, Edge computing, MEC, Multi-Access Edge Computing, Reinforcement Learning, Deep Reinforcement Learning},
abstract = {In recent years, the number of embedded computing devices connected to the Internet has exponentially increased. At the same time, new applications are becoming more complex and computationally demanding, which can be a problem for devices, especially when they are battery powered. In this context, the concepts of computation offloading and edge computing, which allow applications to be fully or partially offloaded and executed on servers close to the devices in the network, have arisen and received increasing attention. Then, the design of algorithms to make the decision of which applications or tasks should be offloaded, and where to execute them, is crucial. One of the options that has been gaining momentum lately is the use of Reinforcement Learning (RL) and, in particular, Deep Reinforcement Learning (DRL), which enables learning optimal or near-optimal offloading policies adapted to each particular scenario. Although the use of RL techniques to solve the computation offloading problem in edge systems has been covered by some surveys, it has been done in a limited way. For example, some surveys have analysed the use of RL to solve various networking problems, with computation offloading being one of them, but not the primary focus. Other surveys, on the other hand, have reviewed techniques to solve the computation offloading problem, being RL just one of the approaches considered. To the best of our knowledge, this is the first survey that specifically focuses on the use of RL and DRL techniques for computation offloading in edge computing system. We present a comprehensive and detailed survey, where we analyse and classify the research papers in terms of use cases, network and edge computing architectures, objectives, RL algorithms, decision-making approaches, and time-varying characteristics considered in the analysed scenarios. In particular, we include a series of tables to help researchers identify relevant papers based on specific features, and analyse which scenarios and techniques are most frequently considered in the literature. Finally, this survey identifies a number of research challenges, future directions and areas for further study.}
}
@article{WANG2023121186,
title = {A transfer learning method for electric vehicles charging strategy based on deep reinforcement learning},
journal = {Applied Energy},
volume = {343},
pages = {121186},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121186},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923005500},
author = {Kang Wang and Haixin Wang and Zihao Yang and Jiawei Feng and Yanzhen Li and Junyou Yang and Zhe Chen},
keywords = {Electric vehicle, Transfer learning, Deep reinforcement learning, Charging strategy},
abstract = {Reinforcement learning (RL) is popularly used for the development of an orderly charging strategy for electric vehicles (EVs). However, a new environment (e.g., charging areas and times) will cause EV users' driving behaviors and electricity prices to change, which leads to the trained RL-based charging strategy is not suitable. Besides, developing a new RL-based charging strategy for the new environment will cost too much time and data samples. In this paper, a deep transfer reinforcement learning (DTRL)-based charging method for EVs is proposed to realize the transfer of trained RL-based charging strategy to the new environment. Firstly, we formulate the uncertainty problem of EV charging behaviors as a Markov Decision Process (MDP) with an unknown state transfer function. Furthermore, an RL-based charging strategy based on deep deterministic policy gradient (DDPG) is well-trained by using massive driving and environmental data samples. Finally, an EV charging method based on transfer learning (TL) and DDPG is proposed to perform the knowledge transfer on the trained RL-based charging strategy to the new environment. The proposed method is verified by numerous simulations. The results show that the proposed approach can reduce the outliers to meet the user charging demands and shorten the EV charging strategy development time in the new environment.}
}
@article{SALTOUROS20021415,
title = {Network resource brokerage by means of distributed agent-based systems encompassing reinforcement learning schemes},
journal = {Computer Communications},
volume = {25},
number = {16},
pages = {1415-1428},
year = {2002},
issn = {0140-3664},
doi = {https://doi.org/10.1016/S0140-3664(02)00043-9},
url = {https://www.sciencedirect.com/science/article/pii/S0140366402000439},
author = {M. Saltouros and A. Taskaris and P. Demestichas and M. Theologou and A. Vasilakos},
keywords = {Service provider, Routing, Learning algorithms, Foundation for intelligent physical agents},
abstract = {The success of a network provider in a competitive communications market depends on the cost-effective provision of the appropriate QoS levels, and the ability to promote the network infrastructure, using dynamic attracting service providers. Advanced management tools encompassing the appropriate intelligence are enabling concepts in this direction. The aim of this paper is to present a part of a Network and Service Management System that acts as a distributed Bandwidth Brokerage System (BBS) over an Internet segment. Aspects addressed are the overall role of the BBS, its functional decoupling, the component deployment pattern in complex (hierarchical) network structures, and the computational intelligence encompassed in the components. The BBS components' logic, which is based on the Stochastic Estimator Learning Algorithm (SELA) concept, solves a version of the hierarchical routing and bandwidth management problem. Mathematical descriptions of the SELA concept and the corresponding routing and bandwidth management schemes are provided. Finally, the paper provides results on the efficiency of the BBS in managing network segments of commercial size and connectivity degree.}
}
@article{ALQAHTANI2022122626,
title = {Dynamic energy scheduling and routing of multiple electric vehicles using deep reinforcement learning},
journal = {Energy},
volume = {244},
pages = {122626},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.122626},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221028759},
author = {Mohammed Alqahtani and Mengqi Hu},
keywords = {Mobile energy network, Electric vehicle, Vehicle routing, Energy scheduling, Deep reinforcement learning},
abstract = {The demand on energy is uncertain and subject to change with time due to several factors including the emergence of new technology, entertainment, divergence of people's consumption habits, changing weather conditions, etc. Moreover, increases in energy demand are growing every day due to increases in world's population and growth of global economy, which substantially increase the chances of disruptions in power supply. This makes the security of power supply a more challenging task especially during seasons (e.g. summer and winter). This paper proposes a reinforcement learning model to address the uncertainties in power supply and demand by dispatching a set of electric vehicles to supply energy to different consumers at different locations. An electric vehicle is mounted with various energy resources (e.g., PV panel, energy storage) that share power generation units and storages among different consumers to power their premises to reduce energy costs. The performance of the reinforcement learning model is assessed under different configurations of consumers and electric vehicles, and compared to the results from CPLEX and three heuristic algorithms. The simulation results demonstrate that the reinforcement learning algorithm can reduce energy costs up to 22.05%, 22.57%, and 19.33% compared to the genetic algorithm, particle swarm optimization, and artificial fish swarm algorithm results, respectively.}
}
@article{PENG2022109385,
title = {SmartTRO: Optimizing topology robustness for Internet of Things via deep reinforcement learning with graph convolutional networks},
journal = {Computer Networks},
volume = {218},
pages = {109385},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109385},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622004194},
author = {Yabin Peng and Caixia Liu and Shuxin Liu and Yuchen Liu and Yiteng Wu},
keywords = {Internet of Things, Robustness optimization, Graph convolutional network, Deep reinforcement learning, Rewiring operation},
abstract = {The reliability problems caused by random failure or malicious attacks in the Internet of Things (IoT) are becoming increasingly severe, while a highly robust network topology is the basis for highly reliable Quality of Service (QoS). Therefore, improving the robustness of the IoT against cyber-attacks by optimizing the network topology becomes a vital issue. Heuristic algorithms as the mainstream idea to solve the network robustness optimization, but their high computational cost cannot meet the timeliness requirements of real IoT scenarios. This paper proposes a Smart Topology Robustness Optimization (SmartTRO) algorithm based on Deep Reinforcement Learning (DRL). First, we design a rewiring operation as an evolutionary behavior in IoT network topology robustness optimization, which achieves topology optimization at a low cost without changing the degree of all nodes. Then, SmartTRO learns the evolutionary behavior characteristics of IoT network topology by combining Graph Convolutional Network (GCN) and policy network, where the training of neural network parameters is completed by DRL. Experimental results demonstrate that SmartTRO improves the ability of IoT topology to resist cyber-attacks effectively and outperforms the state-of-the-art heuristic algorithm in terms of both topology robustness optimization performance and computational cost.}
}
@article{YOU20191,
title = {Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {1-18},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018302021},
author = {Changxi You and Jianbo Lu and Dimitar Filev and Panagiotis Tsiotras},
keywords = {Reinforcement learning, Inverse reinforcement learning, Deep neural-network, Maximum entropy, Path planning, Autonomous vehicle},
abstract = {Autonomous vehicles promise to improve traffic safety while, at the same time, increase fuel efficiency and reduce congestion. They represent the main trend in future intelligent transportation systems. This paper concentrates on the planning problem of autonomous vehicles in traffic. We model the interaction between the autonomous vehicle and the environment as a stochastic Markov decision process (MDP) and consider the driving style of an expert driver as the target to be learned. The road geometry is taken into consideration in the MDP model in order to incorporate more diverse driving styles. The desired, expert-like driving behavior of the autonomous vehicle is obtained as follows: First, we design the reward function of the corresponding MDP and determine the optimal driving strategy for the autonomous vehicle using reinforcement learning techniques. Second, we collect a number of demonstrations from an expert driver and learn the optimal driving strategy based on data using inverse reinforcement learning. The unknown reward function of the expert driver is approximated using a deep neural-network (DNN). We clarify and validate the application of the maximum entropy principle (MEP) to learn the DNN reward function, and provide the necessary derivations for using the maximum entropy principle to learn a parameterized feature (reward) function. Simulated results demonstrate the desired driving behaviors of an autonomous vehicle using both the reinforcement learning and inverse reinforcement learning techniques.}
}
@incollection{GAO20232005,
title = {Transfer learning for process design with reinforcement learning},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {2005-2010},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50319-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044315274050319X},
author = {Qinghe Gao and Haoyu Yang and Shachi M. Shanbhag and Artur M. Schweidtmann},
keywords = {Reinforcement learning, process design, transfer learning},
abstract = {Process design is a creative task that is currently performed manually by engineers. Artificial intelligence provides new potential to facilitate process design. Specifically, reinforcement learning (RL) has shown some success in automating process design by integrating data-driven models that learn to build process flowsheets with process simulation in an iterative design process. However, one major challenge in the learning process is that the RL agent demands numerous process simulations in rigorous process simulators, thereby requiring long simulation times and expensive computational power. Therefore, typically short-cut simulation methods are employed to accelerate the learning process. Short-cut methods can, however, lead to inaccurate results. We thus propose to utilize transfer learning for process design with RL in combination with rigorous simulation methods. Transfer learning is an established approach from machine learning that stores knowledge gained while solving one problem and reuses this information on a different target domain. We integrate transfer learning in our RL framework for process design and apply it to an illustrative case study comprising equilibrium reactions, azeotropic separation, and recycles, our method can design economically feasible flowsheets with stable interaction with DWSIM. Our results show that transfer learning enables RL to economically design feasible flowsheets with DWSIM, resulting in a flowsheet with an 8% higher revenue. And the learning time can be reduced by a factor of 2.}
}
@article{OH2023108519,
title = {A multi-use framework of energy storage systems using reinforcement learning for both price-based and incentive-based demand response programs},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {144},
pages = {108519},
year = {2023},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2022.108519},
url = {https://www.sciencedirect.com/science/article/pii/S014206152200521X},
author = {Seongmun Oh and Junhyuk Kong and Yejin Yang and Jaesung Jung and Chul-Ho Lee},
keywords = {Reinforcement learning, Demand response, Energy storage system, Electricity market},
abstract = {This study proposes a multi-use energy storage system (ESS) framework to participate in both price-based and incentive-based demand response programs with reinforcement learning (RL) on the demand side. We focused on industrial customers, to provide them the opportunity to obtain additional profits through market participation in addition to managing their load. Since industrial customers pay their electricity bills according to the time of use tariff structure, they can benefit if they can shift their electricity usage from high-price hours to low-price hours. Furthermore, they are able obtain additional incentives by fulfilling a dispatch signal from the system operator by using the ESS. To model the multi-use ESS by industrial users, we used the RL framework to make customer decisions. The RL approach uses a control action policy by interacting with an environment with no prior knowledge. For this, we formulated the ESS operation as a Markov decision process so that the environmental information obtained by the customers provides RL, which takes optimal actions for the current environment considering customer benefits. We developed several RL agents to identify an acceptable control agent. We utilized the actual industrial load profile in South Korea to train the RL agents. The experimental results demonstrated that the proposed framework can make near-optimal decisions for using ESS in multiple ways.}
}
@article{SHI2022429,
title = {A behavior fusion method based on inverse reinforcement learning},
journal = {Information Sciences},
volume = {609},
pages = {429-444},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.07.100},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522007897},
author = {Haobin Shi and Jingchen Li and Shicong Chen and Kao-Shing Hwang},
keywords = {Inverse reinforcement learning, Generative adversarial network, Behavior fusion},
abstract = {Inverse reinforcement learning (IRL) is usually used in deep reinforcement learning systems for tasks that are difficult to design with manual reward functions. If the task is too complicated, the expert sample trajectories obtained artificially often have different preferences, resulting in a relatively large variance in the learned reward function. For this purpose, this study proposes a behavior fusion method based on adversarial IRL. We decompose complex tasks into several simple subtasks according to different preferences. After decoupling the tasks, we use the inherent relationship between IRL and generative adversarial network (GAN): the discriminator network fits the reward function and the generator network fits strategy, and the reward function and policy are learned respectively. Moreover, we improve the adversarial IRL model by using multiple discriminators to correspond to each subtask, and provide a more efficient update for the whole structure. The behavior fusion in this work acts a weighted network on the reward functions in different subtasks. The proposed method is evaluated on Atari enduro racing game with baseline methods, and we implement a wafer inspection experiment for further discussions. The experimental results show our method can learn more advanced policies in complicated tasks, and the training process is more stable.}
}
@article{NIU2023,
title = {A pipelining task offloading strategy via delay-aware multi-agent reinforcement learning in Cybertwin-enabled 6G network},
journal = {Digital Communications and Networks},
year = {2023},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2023.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2352864823000810},
author = {Haiwen Niu and Luhan Wang and Keliang Du and Zhaoming Lu and Xiangming Wen and Yu Liu},
keywords = {Cybertwin, Multi-Agent Deep Reinforcement Learning (MADRL), Task offloading, Pipelining, Delay-aware},
abstract = {Cybertwin-enabled 6th Generation (6G) network is envisioned to support artificial intelligence-native management to meet changing demands of 6G applications. Multi-Agent Deep Reinforcement Learning (MADRL) technologies driven by Cybertwins have been proposed for adaptive task offloading strategies. However, the existence of random transmission delay between Cybertwin-driven agents and underlying networks is not considered in related works, which destroys the standard Markov property and increases the decision reaction time to reduce the task offloading strategy performance. In order to address this problem, we propose a pipelining task offloading method to lower the decision reaction time and model it as a delay-aware Markov Decision Process (MDP). Then, we design a delay-aware MADRL algorithm to minimize the weighted sum of task execution latency and energy consumption. Firstly, the state space is augmented using the lastly-received state and historical actions to rebuild the Markov property. Secondly, Gate Transformer-XL is introduced to capture historical actions' importance and maintain the consistent input dimension dynamically changed due to random transmission delays. Thirdly, a sampling method and a new loss function with the difference between the current and target state value and the difference between real state-action value and augmented state-action value are designed to obtain state transition trajectories close to the real ones. Numerical results demonstrate that the proposed methods is effective in reducing reaction time and improving the task offloading performance in the random-delay Cybertwin-enabled 6G networks.}
}
@article{KOBAYASHI2022104019,
title = {Adaptive and multiple time-scale eligibility traces for online deep reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {151},
pages = {104019},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.104019},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021002670},
author = {Taisuke Kobayashi},
keywords = {Deep reinforcement learning, Online learning, Eligibility traces},
abstract = {Deep reinforcement learning (DRL) is one promising approach to teaching robots to perform complex tasks. Because methods that directly reuse the stored experience data cannot follow the change of the environment in robotic problems with a time-varying environment, online DRL is required. The eligibility traces method is well known as an online learning technique for improving sample efficiency in traditional reinforcement learning with linear regressors rather than DRL. The dependency between parameters of deep neural networks would destroy the eligibility traces, which is why they are not integrated with DRL. Although replacing the gradient with the most influential one rather than accumulating the gradients as the eligibility traces can alleviate this problem, the replacing operation reduces the number of reuses of previous experiences. To address these issues, this study proposes a new eligibility traces method that can be used even in DRL while maintaining high sample efficiency. When the accumulated gradients differ from those computed using the latest parameters, the proposed method takes into account the divergence between the past and latest parameters to adaptively decay the eligibility traces. Bregman divergences between outputs computed by the past and latest parameters are exploited due to the infeasible computational cost of the divergence between the past and latest parameters. In addition, a generalized method with multiple time-scale traces is designed for the first time. This design allows for the replacement of the most influential adaptively accumulated (decayed) eligibility traces. The proposed method outperformed conventional methods in terms of learning speed and task quality by the learned policy on benchmark tasks on a dynamic robotic simulator. A real-robot demonstration confirmed the significance of online DRL as well as the adaptability of the proposed method to a changing environment.}
}
@article{PANDA2024107183,
title = {An iterative gradient descent-based reinforcement learning policy for active control of structural vibrations},
journal = {Computers & Structures},
volume = {290},
pages = {107183},
year = {2024},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2023.107183},
url = {https://www.sciencedirect.com/science/article/pii/S0045794923002134},
author = {Jagajyoti Panda and Mudit Chopra and Vasant Matsagar and Souvik Chakraborty},
keywords = {Reinforcement learning, Gradient descent optimization, Active vibration control, Disturbance uncertainties, Robust performance and stability},
abstract = {A contemporary policy gradient-based optimization scheme is presented for active structural control by exerting the concept of reinforcement learning (RL). The RL-based control algorithm is demonstrated both in proportional (P) state, and proportional-integral (PI) state-output feedback approaches, wherein, the latter strategy is deliberated based on the theory of servo-mechanism. The search for optimal P and PI controller parameters in a training sequence is attained by engaging an efficient gradient descent-based optimization strategy. The utilization of gradient-based sequence within the RL framework accelerates the learning protocol, ensuring effective dissipation of structural energy, and achieving suboptimal control. The proposed algorithms are validated through numerical experiments on two different structural systems: (i) a quarter car model subjected to periodic road excitation, having a single actuator and presented in continuous time, and (ii) an 8-story building model subjected to random seismic excitation, having multiple actuators and presented in discrete time. Practical implementation concerns of control strategies are thoroughly investigated by considering perturbations in model parameters and input forces. The findings from this study indicate that the RL-based P and PI controllers exhibit high stabilizing performance, and are applicable in both analog and digital domains. Finally, it has affirmed that the proposed RL algorithms exhibit relatively low computational complexity in real-time, and thus, open up a wide range of perspectives for its application in large-scale complex structures.}
}
@article{VLACHOGIANNIS2020299,
title = {A reinforcement learning model for personalized driving policies identification},
journal = {International Journal of Transportation Science and Technology},
volume = {9},
number = {4},
pages = {299-308},
year = {2020},
issn = {2046-0430},
doi = {https://doi.org/10.1016/j.ijtst.2020.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2046043020300198},
author = {Dimitris M. Vlachogiannis and Eleni I. Vlahogianni and John Golias},
keywords = {Reinforcement learning, Q-learning, Machine learning, Intelligent transportation systems, Traffic data},
abstract = {Optimizing driving performance by addressing personalized aspects of driving behavior and without posing unrealistic restrictions on personal mobility may have far reaching implications to traffic safety, flow operations and the environment, as well as significant benefits for users. The present work addresses the problem of delivering personalized driving policies based on Reinforcement Learning for enhancing existing Intelligent Transportation Systems (ITS) to the benefit of traffic management and road safety. The proposed framework is implemented on appropriate driving behavior metrics derived from smartphone sensors’ data streams. Aggressiveness, speeding and mobile usage are considered to describe the driving profile per trip and are presented as inputs to the Q-learning algorithm. The implementation of the proposed methodological approach produces personalized quantified driving policies to be exploited for self-improvement. Finally, this paper establishes validation measures of the quality and effectiveness of the produced policies and methodological tools for comparing and classifying the examined drivers.}
}
@article{YANG2021121337,
title = {An indirect reinforcement learning based real-time energy management strategy via high-order Markov Chain model for a hybrid electric vehicle},
journal = {Energy},
volume = {236},
pages = {121337},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121337},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221015851},
author = {Ningkang Yang and Lijin Han and Changle Xiang and Hui Liu and Xunmin Li},
keywords = {Hybrid electric vehicle, Real-time energy management, Indirect reinforcement learning, High-order Markov chain},
abstract = {This paper proposes a real-time indirect reinforcement learning based strategy to reduce the fuel consumption. In order to improve the real-time performance and achieve learning online, the simulated experience from environment model is adopted for the learning process, which is called indirect reinforcement learning. To establish an accurate environment model, a high-order Markov Chain is introduced and detailed, which is more precise than a widely used first-order Markov Chain. Corresponding with the model, how the reinforcement learning algorithm learns from the simulated experience is illustrated. Furthermore, an online recursive form of the transition probability matrix is derived, through which the statistical characteristics from the practical driving conditions can be collected. The induced matrix norm is chosen as a criterion to quantify the differences between the transition probability matrices and to determine the time for updating the environment model and triggering the recalculation of the reinforcement learning algorithm. Simulation results demonstrate that, compared with the direct RL, the proposed strategy can effectively reduce the learning time while maintains satisfied fuel economy. Furthermore, a hardware-in-the-loop experiment verifies its real-time capability and actual applicability.}
}
@article{DEHAYBE2023,
title = {Deep Reinforcement Learning for inventory optimization with non-stationary uncertain demand},
journal = {European Journal of Operational Research},
year = {2023},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2023.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0377221723007646},
author = {Henri Dehaybe and Daniele Catanzaro and Philippe Chevalier},
keywords = {Inventory, Lot sizing, Forecast evolution, Deep Reinforcement Learning, Non-stationary demand},
abstract = {We consider here a single-item lot sizing problem with fixed costs, lead time, and both backorders and lost sales, and we show that, after an appropriate training in randomly generated environments, Deep Reinforcement Learning (DRL) agents can interpolate in real-time near-optimal dynamic policies on instances with a rolling-horizon, provided a previously unseen demand forecast and without the need to periodically resolve the problem. Extensive computational experiments show that the policies provided by these agents compete, and in some circumstances even outperform by several percentage points of gap, those provided by heuristics based on dynamic programming. These results confirm the importance of DRL in the context of inventory control problems and support its use in solving practical instances featuring realistic assumptions.}
}
@article{DEMARS2022100179,
title = {Reinforcement learning and A* search for the unit commitment problem},
journal = {Energy and AI},
volume = {9},
pages = {100179},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2022.100179},
url = {https://www.sciencedirect.com/science/article/pii/S2666546822000301},
author = {Patrick {de Mars} and Aidan O’Sullivan},
keywords = {Unit commitment, Reinforcement learning, Tree search, Power systems},
abstract = {Previous research has combined model-free reinforcement learning with model-based tree search methods to solve the unit commitment problem with stochastic demand and renewables generation. This approach was limited to shallow search depths and suffered from significant variability in run time across problem instances with varying complexity. To mitigate these issues, we extend this methodology to more advanced search algorithms based on A* search. First, we develop a problem-specific heuristic based on priority list unit commitment methods and apply this in Guided A* search, reducing run time by up to 94% with negligible impact on operating costs. In addition, we address the run time variability issue by employing a novel anytime algorithm, Guided IDA*, replacing the fixed search depth parameter with a time budget constraint. We show that Guided IDA* mitigates the run time variability of previous guided tree search algorithms and enables further operating cost reductions of up to 1%.}
}
@article{LIU2024109655,
title = {Integration of functional resonance analysis method and reinforcement learning for updating and optimizing emergency procedures in variable environments},
journal = {Reliability Engineering & System Safety},
volume = {241},
pages = {109655},
year = {2024},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109655},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023005690},
author = {Xuan Liu and Huixing Meng and Xu An and Jinduo Xing},
keywords = {Emergency scheme, Functional resonance analysis model, Reinforcement learning, Multi-objective decision making},
abstract = {Blowout accidents are prone to generate personal casualties, property losses, and even environmental disasters. To alleviate the consequences of accidents, it is essential to conduct effective emergency operations and update emergency schemes when necessary. In the update of the emergency plan, how to effectively optimize the allocation of resources is an open question. To deal with above difficulties, we propose a hybrid methodology by integrating the functional resonance analysis method (FRAM) and reinforcement learning (RL) for updating and optimizing emergency schemes. In the proposed methodology, FRAM is utilized to model the emergency response process based on function, variability, and coupling. Since the environment of emergency operations usually changes, RL is introduced to update emergency schemes that are constructed by FRAM. The selection of reward value by the agent reflects the variability of functional nodes in the FRAM model. To optimize emergency schemes, the interval analytic hierarchy process is integrated with multi-objective decision-making to analyze the duration, cost, and exposure risk of emergency operations. The installation of a capping stack, an emergency technique for deepwater blowout accidents, is employed to illustrate the applicability of the methodology. The results show that the proposed model is beneficial to determine emergency actions adapted to condition or scenario change in accidents.}
}
@article{SHAHRABI201775,
title = {A reinforcement learning approach to parameter estimation in dynamic job shop scheduling},
journal = {Computers & Industrial Engineering},
volume = {110},
pages = {75-82},
year = {2017},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217302309},
author = {Jamal Shahrabi and Mohammad Amin Adibi and Masoud Mahootchi},
keywords = {Reinforcement learning, Q–factor, Dynamic job shop scheduling, Variable neighborhood search},
abstract = {In this paper, reinforcement learning (RL) with a Q-factor algorithm is used to enhance performance of the scheduling method proposed for dynamic job shop scheduling (DJSS) problem which considers random job arrivals and machine breakdowns. In fact, parameters of an optimization process at any rescheduling point are selected by continually improving policy which comes from RL. The scheduling method is based on variable neighborhood search (VNS) which is introduced to address the DJSS problem. A new approach is also introduced to calculate reward values in learning processes based on quality of selected parameters. The proposed method is compared with general variable neighborhood search and some common dispatching rules that have been widely used in the literature for the DJSS problem. Results illustrate the high performance of the proposed method in a simulated environment.}
}
@article{JIANG2022112378,
title = {A human-like collision avoidance method for autonomous ship with attention-based deep reinforcement learning},
journal = {Ocean Engineering},
volume = {264},
pages = {112378},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.112378},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822016705},
author = {Lingling Jiang and Lanxuan An and Xinyu Zhang and Chengbo Wang and Xinjian Wang},
keywords = {Maritime safety, Ship collision avoidance, Self-attention mechanism, Deep reinforcement learning, Autonomous ship},
abstract = {Reinforcement learning has the characteristics of simple structure and strong adaptability, which has been widely used in the field of ship autonomous collision avoidance. In order to solve the problem of collision avoidance in multi-ship encounter situation, a novel collision avoidance method for autonomous ship with attention-based deep reinforcement learning (ADRL) is proposed, it consists of two parts, risk assessment module and motion planning module, the difference between the former and the existing collision risk calculation method is that from the officer's attention distribution, it encode the ship's information through the local map, and calculate each ship's collision avoidance decision in the form of attention score in real time under the constraints of the COLREGS. In addition, a composite learning method is designed, which integrates supervised learning into the common direct environmental exploration model, which accelerates the exploration efficiency of the model and shows excellent learning performance. Finally, based on the Open AI Gym platform, static obstacle situation, dynamic multi-ship encounter situation, dynamic and static obstacle coexistence situation are designed, and the rationality and effectiveness of collision avoidance decision are analyzed from the perspectives of collision risk and the closest safety distance respectively.}
}
@article{XIE2022476,
title = {Resource allocation for network slicing in dynamic multi-tenant networks: A deep reinforcement learning approach},
journal = {Computer Communications},
volume = {195},
pages = {476-487},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422003607},
author = {Yanghao Xie and Yuyang Kong and Lin Huang and Sheng Wang and Shizhong Xu and Xiong Wang and Jing Ren},
keywords = {Network slicing, Multi-tenant networks, Dynamic networks, Resource allocation, Deep reinforcement learning},
abstract = {To support the wide range of 5G use cases in a cost-efficient way, network slicing has been considered a promising solution, which makes it possible to serve multiple customized and isolated network services on a common physical infrastructure. In this paper, we investigate the resource allocation problem of network slicing in multi-tenant networks where network resources can be used by low-priority tenants change dynamically due to the preemption of high-priority tenants. We formulate the problem as an energy-minimizing mathematical optimization problem considering practical constraints. Due to the dynamic characteristics of the problem, the complexity of the optimization problem is exceptionally high, making it impossible to solve the problem in real-time using traditional optimization approaches. With discovering the special structure of the problem, we propose a Dueling-Deep Q Network (DQN)-based algorithm to solve the problem efficiently. The experimental results show that the proposed algorithm outperforms compared algorithms in terms of total energy cost, runtime, and robustness.}
}
@article{XU202290,
title = {An improved communication resource allocation strategy for wireless networks based on deep reinforcement learning},
journal = {Computer Communications},
volume = {188},
pages = {90-98},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.02.018},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422000615},
author = {Ting Xu and Ming Zhao and Xin Yao and Yusen Zhu},
keywords = {Teacon, Lora, Allocating resources, Deep reinforcement learning, Low-power consumption, 5G network},
abstract = {With the advent of 5G networks, user demand for high-speed, low-latency, and high-reliability services continues to grow. When traditional communication technologies cannot meet the needs, wireless network LoRa technology has emerged. Although LoRa has low power consumption, Long-distance, and other advantages, terminal nodes still face frequent data collection and energy consumption issues how to more efficiently combine the deep reinforcement learning method for LoRa wireless network communication and allocate resources reasonably and effectively. This paper proposes a communication channel resource allocation strategy based on deep reinforcement learning, the CL-LoRa strategy. It uses extended preamble and low-power interception technologies to achieve on-demand synchronization and low-power communication. The basic idea of this strategy is to detect channel quality based on CAD, coordinate node scheduling, and wireless channel allocation. The node will choose different ways to acquire the channel according to the current network load, namely CSMA-CA competition and dynamic duty cycle communication. In this way, the channel utilization rate is improved, and the energy consumption problem of the long-distance communication data volume is perfectly solved. The duty cycle access method is based on the imbalance of energy consumption in the Internet of Things. It uses the remaining energy of the remote central node to dynamically adjust the duty cycle of the node, wake up the working time of the node, and send more beacons to the sleeping node. Reduce the sleep delay of the node. Through theoretical analysis of CL-LoRa protocol performance, compared with DDC-LoRa protocol and ADC-LoRa protocol, CL-LoRa protocol can increase channel utilization by 9%, reduce terminal energy consumption by 1.6%, and increase throughput by 1.5%.}
}