"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Learning to Herd Agents Amongst Obstacles: Training Robust Shepherding Behaviors Using Deep Reinforcement Learning","J. Zhi; J. -M. Lien","Department of Computer Science, George Mason University, Fairfax, VA, USA; Department of Computer Science, George Mason University, Fairfax, VA, USA","IEEE Robotics and Automation Letters","8 Apr 2021","2021","6","2","4163","4168","Robotic shepherding problem considers the control and navigation of a group of coherent agents (e.g., a flock of bird or a fleet of drones) through the motion of an external robot, called shepherd. Machine learning based methods have successfully solved this problem in an environment with no obstacles. Rule-based methods, on the other hand, can handle more complex scenarios in which environments are cluttered with obstacles and allow multiple shepherds to work collaboratively. However, these rule-based methods are fragile due to the difficulty in defining a comprehensive set of behavioral rules. To overcome these limitations, we propose the first known learning-based method that can herd agents amongst obstacles. By using deep reinforcement learning techniques combined with the probabilistic roadmaps, we train a shepherding model using noisy but controlled environmental and behavioral parameters. Our experimental results show that the trained shepherding controller is robust, namely, it is insensitive to the uncertainties originated from either the group behavioral models or from environments with a small of path homotopy classes. Consequently, the proposed method has a higher success rate, shorter completion time and path length than the rule-based behavioral methods have. These advantages are particularly prominent in more challenging scenarios involving more difficult groups and strenuous passages.","2377-3766","","10.1109/LRA.2021.3068955","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387150","Motion and path planning;reinforcement learning;task and motion planning","Reinforcement learning;Path planning;Motion planning;Learning systems","","","","14","","23","IEEE","25 Mar 2021","","","IEEE","IEEE Journals"
"Human-Guided Robot Behavior Learning: A GAN-Assisted Preference-Based Reinforcement Learning Approach","H. Zhan; F. Tao; Y. Cao","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Electrical and Computer Engineering, University of Texas at San Antonio, San Antonio, Texas, USA; Department of Electrical and Computer Engineering, University of Texas at San Antonio, San Antonio, Texas, USA","IEEE Robotics and Automation Letters","24 Mar 2021","2021","6","2","3545","3552","Human demonstrations can provide trustful samples to train reinforcement learning algorithms for robots to learn complex behaviors in real-world environments. However, obtaining sufficient demonstrations may be impractical because many behaviors are difficult for humans to demonstrate. A more practical approach is to replace human demonstrations by human queries, i.e., preference-based reinforcement learning. One key limitation of the existing algorithms is the need for a significant amount of human queries because a large number of labeled data is needed to train neural networks for the approximation of a continuous, high-dimensional reward function. To reduce and minimize the need for human queries, we propose a new GAN-assisted human preference-based reinforcement learning approach that uses a generative adversarial network (GAN) to learn human preferences and then replace the role of human in assigning preferences. The adversarial neural network is simple and only has a binary output, hence requiring much less human queries to train. Moreover, a maximum entropy based reinforcement learning algorithm is designed to shape the loss towards the desired regions or away from the undesired regions. To show the effectiveness of the proposed approach, we present some studies on complex robotic tasks without access to the environment reward in a typical MuJoCo robot locomotion environment. The obtained results show our method can achieve a reduction of about 99.8% human time without performance sacrifice.","2377-3766","","10.1109/LRA.2021.3063927","Army Research Office(grant numbers:W911NF-21-1-0103); Office of Naval Research(grant numbers:N000141912278,N000141712613); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369902","Generative adversarial network (GAN);human preferences;reinforcement learning","Trajectory;Reinforcement learning;Neural networks;Robots;Training;Entropy;Task analysis","control engineering computing;human-robot interaction;intelligent robots;learning (artificial intelligence);neural nets","high-dimensional reward function;continuous reward function;MuJoCo robot locomotion environment;adversarial neural network;generative adversarial network;human-guided robot behavior learning;GAN-assisted human preference-based reinforcement learning;maximum entropy based reinforcement learning;human time;human preferences;human queries;human demonstrations;robot behavior learning","","8","","36","IEEE","4 Mar 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning Based, Staircase Negotiation Learning: Simulation and Transfer to Reality for Articulated Tracked Robots","A. Mitriakov; P. Papadakis; J. Kerdreux; S. Garlatti","Research Laboratory in Information and Communication Science and Technology, IMT Atlantique, Lab-STICC, UMR 6285, team RAMBO, Brest, France; Research Laboratory in Information and Communication Science and Technology, IMT Atlantique, Lab-STICC, UMR 6285, team RAMBO, Brest, France; Research Laboratory in Information and Communication Science and Technology, IMT Atlantique, Lab-STICC, UMR 6285, team RAMBO, Brest, France; Research Laboratory in Information and Communication Science and Technology, IMT Atlantique, Lab-STICC, UMR 6285, team RAMBO, Brest, France","IEEE Robotics & Automation Magazine","10 Dec 2021","2021","28","4","10","20","Autonomous control of reconfigurable robots is crucial for their deployment in diverse environments. However, its development is hampered by the diversity of hardware and task constraints. We advocate the use of artificial intelligence-based approaches to improve scalability to different conditions and portability to platforms of comparable traversability skills. In particular, we succeed in tackling the problem of staircase traversal via a reinforcement learning (RL)-based control framework applicable to different articulated, tracked robots and powerful enough to generalize to varying conditions learned in simulation and transferred to reality in a zero-shot setting. Our extensive experiments demonstrate the robustness of the framework in learning tasks with increased risk and difficulty induced by platform diversification and increased control dimensionality.","1558-223X","","10.1109/MRA.2021.3114105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9583665","","Autonomous systems;Robot kinematics;Reinforcement learning;Safety;Kinematics;Collision avoidance;Sensors","mobile robots;reinforcement learning","staircase negotiation learning;autonomous control;reconfigurable robots;diverse environments;task constraints;artificial intelligence;portability;staircase traversal;reinforcement learning-based control;control dimensionality;traversability skills;articulated tracked robots","","5","","20","IEEE","21 Oct 2021","","","IEEE","IEEE Magazines"
"Stability-Guaranteed Reinforcement Learning for Contact-Rich Manipulation","S. A. Khader; H. Yin; P. Falco; D. Kragic","RPL, EECS, KTH, Stockholm, Sweden; RPL, EECS, KTH, Stockholm, Sweden; ABB Corporate Research, Vasteras, Sweden; RPL, EECS, KTH, Stockholm, Sweden","IEEE Robotics and Automation Letters","14 Oct 2020","2021","6","1","1","8","Reinforcement learning (RL) has had its fair share of success in contact-rich manipulation tasks but it still lags behind in benefiting from advances in robot control theory such as impedance control and stability guarantees. Recently, the concept of variable impedance control (VIC) was adopted into RL with encouraging results. However, the more important issue of stability remains unaddressed. To clarify the challenge in stable RL, we introduce the term all-the-time-stability that unambiguously means that every possible rollout should be stability certified. Our contribution is a model-free RL method that not only adopts VIC but also achieves all-the-time-stability. Building on a recently proposed stable VIC controller as the policy parameterization, we introduce a novel policy search algorithm that is inspired by Cross-Entropy Method and inherently guarantees stability. Our experimental studies confirm the feasibility and usefulness of stability guarantee and also features, to the best of our knowledge, the first successful application of RL with all-the-time-stability on the benchmark problem of peg-in-hole.","2377-3766","","10.1109/LRA.2020.3028529","Wallenberg AI, Autonomous Systems and Software Program; Knut och Alice Wallenbergs Stiftelse; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211756","Reinforcement learning;compliance and impedance control;compliant assembly","Trajectory;Impedance;Stability criteria;Task analysis;Manipulator dynamics","entropy;learning (artificial intelligence);learning systems;manipulators;search problems;stability","cross entropy method;policy search algorithm;stability guaranteed reinforcement learning;contact rich manipulation;all-the-time-stability;model free RL method;VIC controller stability;variable impedance control;robot control theory","","15","","33","IEEE","2 Oct 2020","","","IEEE","IEEE Journals"
"Two Dimensional (2D) Feedback Control Scheme Based on Deep Reinforcement Learning Algorithm for Nonlinear Non-repetitive Batch Processes","J. Liu; W. Hong; J. Shi","Institute of Artificial Intelligence, Xiamen University, Xiamen, China; Institute of Artificial Intelligence, Xiamen University, Xiamen, China; Institute of Artificial Intelligence, Xiamen University, Xiamen, China","2022 IEEE 11th Data Driven Control and Learning Systems Conference (DDCLS)","26 Aug 2022","2022","","","262","267","The repetitive/periodic/batch process is widely used in modern industrial production. However, in the context of complex batch processes with nonlinear and non-repetitive nature, designing an effective control scheme is still a critical problem in theoretical research and practical application. In terms of the excellent performance of deep reinforcement learning (DRL) in dealing with the decision-making problems for complex dynamical systems and interacting without any requirement of prior knowledge of the processes, in this paper, we propose a model-free controller design scheme by using soft actor-critic (SAC), an advanced off-policy DRL algorithm. By properly designing the state information, the neural network structure of the policy, and the reward function, the SAC agent is trained as a nonlinear two-dimensional (2D) state feedback control to achieve high tracking performance and strong robustness for the nonlinear non-repetitive batch processes. Our simulation results demonstrate the proposed control method's effectiveness and applicability, and its significant performance is superior to the conventional iterative learning control (ILC) schemes.","2767-9861","978-1-6654-9675-9","10.1109/DDCLS55054.2022.9858421","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858421","deep reinforcement learning;nonlinear non-repetitive batch processes;soft-actor-critic;iterative learning control","Training;State feedback;Simulation;Two dimensional displays;Neural networks;Batch production systems;Optimal control","batch processing (industrial);control engineering computing;control system synthesis;deep learning (artificial intelligence);nonlinear control systems;production engineering computing;reinforcement learning;state feedback","nonlinear two-dimensional state feedback control;two-dimensional feedback control scheme;model-free controller design scheme;complex dynamical systems;deep reinforcement learning algorithm;complex batch processes;control method;nonlinear nonrepetitive batch processes;off-policy DRL algorithm","","1","","28","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"Federated Deep Reinforcement Learning for Task Scheduling in Heterogeneous Autonomous Robotic System","T. M. Ho; K. -K. Nguyen; M. Cheriet","Synchromedia Lab, Ecole de Technologie Supérieure, Univorsité du Québec, QC, Canada; Synchromedia Lab, Ecole de Technologie Supérieure, Univorsité du Québec, QC, Canada; Synchromedia Lab, Ecole de Technologie Supérieure, Univorsité du Québec, QC, Canada","GLOBECOM 2022 - 2022 IEEE Global Communications Conference","11 Jan 2023","2022","","","1134","1139","In this paper, we investigate the problem of task scheduling in automated warehouses with hetero-geneous autonomous robotic systems. We formulate the task scheduling for a heterogeneous autonomous robots (HAR) system in each warehouse as a queueing control optimization problem in which we aim to minimize the queue length of tasks that are waiting to be processed. We propose a deep reinforcement learning (DRL) based approach that employs the proximal policy optimization (PPO) to achieve an optimal task scheduling policy. We then propose a federated learning based algorithm to improve the performance of the PPO agents. The simulation results fully demonstrate the performance improvement of our proposed algorithm in terms of average queue length compared to the distributed learning algorithm.","","978-1-6654-3540-6","10.1109/GLOBECOM48099.2022.10000980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10000980","industrial automation;federated deep reinforcement learning","Deep learning;Computer aided instruction;Job shop scheduling;Upper bound;Distance learning;Simulation;Reinforcement learning","industrial robots;mobile robots;optimisation;queueing theory;reinforcement learning;scheduling;warehouse automation","automated warehouses;average queue length;distributed learning algorithm;DRL;federated deep reinforcement learning;HAR;heterogeneous autonomous robotic system;optimal task scheduling policy;PPO agents;proximal policy optimization;queueing control optimization problem","","1","","9","IEEE","11 Jan 2023","","","IEEE","IEEE Conferences"
"On the Performance of Data-Driven Reinforcement Learning for Commercial HVAC Control","S. Faddel; G. Tian; Q. Zhou; H. Aburub","Departement of Electrical and Computer Engineering, University of Central Florida, Orlando, Fl, USA; Departement of Electrical and Computer Engineering, University of Central Florida, Orlando, Fl, USA; Departement of Electrical and Computer Engineering, University of Central Florida, Orlando, Fl, USA; Departement of Electrical and Computer Engineering, Florida International University, Miami, Fl, USA","2020 IEEE Industry Applications Society Annual Meeting","1 Feb 2021","2020","","","1","7","Commercial heating, ventilation and air conditioning (HVAC ) system consumes large portion of the building energy use. With the abundance of the available data in the building automation systems (BAS) of commercial buildings, ample opportunities have emerged to help develop adequate data-driven control of HVAC systems. This paper proposes the use of data-driven reinforcement learning (RL) that can evaluate control policies and develop new ones. A Q- learning algorithm is used as a type of reinforcement learning to minimize the building energy consumption cost while maintaining the comfort level. The proposed Q-learning algorithm is trained using actual data where the data is first used to develop temperature and energy models. Four different machine learning methodologies are used to obtain these models which are linear regression, deep neural network, support vector machines and random forests. The performance of the Q- learning algorithm under each methodology is tested and compared with the others. The algorithm is validated using a decent physics-based model of a 3-floor office/classroom building. The results showed that though all the four methodologies yield satisfactory results, the Q-learning algorithm performed the best under support vector machine and random forest.","2576-702X","978-1-7281-7192-0","10.1109/IAS44978.2020.9334865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9334865","HVAC;Demand Response;Comfort level;Machine Learning;Reinforcement Learning","Support vector machines;Machine learning algorithms;HVAC;Buildings;Neural networks;Reinforcement learning;Random forests","air conditioning;building management systems;control engineering computing;deep learning (artificial intelligence);energy consumption;HVAC;power engineering computing;regression analysis;support vector machines","data-driven reinforcement learning;commercial HVAC control;building energy use;building automation systems;building energy consumption cost;Q-learning algorithm;different machine learning methodologies;support vector machine;cmmercial heating ventilation and air conditioning control;linear regression;deep neural network;random forests;support vector machines","","5","","28","IEEE","1 Feb 2021","","","IEEE","IEEE Conferences"
"Aggressive Quadrotor Flight Using Curiosity-Driven Reinforcement Learning","Q. Sun; J. Fang; W. X. Zheng; Y. Tang","Key Laboratory of Smart Manufacturing in Energy Chemical Process Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process Ministry of Education, East China University of Science and Technology, Shanghai, China; School of Computer, Data and Mathematical Sciences, Western Sydney University, Sydney, NSW, Australia; Key Laboratory of Smart Manufacturing in Energy Chemical Process Ministry of Education, East China University of Science and Technology, Shanghai, China","IEEE Transactions on Industrial Electronics","12 Jul 2022","2022","69","12","13838","13848","The ability to perform aggressive movements,which are called aggressive flights, is important for quadrotors during navigation. However, aggressive quadrotor flights are still a great challenge to practical applications. The existing solutions to aggressive flights heavily rely on a predefined trajectory, which is a time-consuming preprocessing step. To avoid such path planning, we propose a curiosity-driven reinforcement learning method for aggressive flight missions and a similarity-based curiosity module is introduced to speed up the training procedure. A branch structure exploration strategy is also applied to guarantee the robustness of the policy and to ensure the policy trained in simulations can be performed in real-world experiments directly. The experimental results in simulations demonstrate that our reinforcement learning algorithm performs well in aggressive flight tasks, speeds up the convergence process and improves the robustness of the policy. Besides, our algorithm shows a satisfactory simulated to real transferability and performs well in real-world experiments.","1557-9948","","10.1109/TIE.2022.3144586","Ministry of Science and Technology of the People's Republic of China(grant numbers:2018AAA0101602); National Natural Science Foundation of China(grant numbers:62136003); Introducing Talents of Discipline to Universities(grant numbers:B17017); Program of Shanghai Academic Research Leader(grant numbers:20XD1401300); Chinesisch-Deutsche Zentrum für Wissenschaftsförderung(grant numbers:M-0066); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693334","Aggressive flight;reinforcement learning;unmanned aerial vehicles (UAVs)","Reinforcement learning;Training;Navigation;Trajectory;Trajectory planning;Task analysis;Planning","aircraft control;control engineering computing;helicopters;path planning;reinforcement learning;robust control","policy robustness improvement;training procedure;aggressive flights;aggressive quadrotor flight;aggressive flight tasks;similarity-based curiosity module;aggressive flight missions;curiosity-driven reinforcement learning method;time-consuming preprocessing step","","4","","40","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"EFCam: Configuration-Adaptive Fog-Assisted Wireless Cameras with Reinforcement Learning","S. Zhou; D. V. Le; J. Qiping Yang; R. Tan; D. Ho","HP-NTU Digital Manufacturing Corporate Lab, Nanyang Technological University; HP-NTU Digital Manufacturing Corporate Lab, Nanyang Technological University; HP-NTU Digital Manufacturing Corporate Lab, Nanyang Technological University; School of Computer Science and Engineering, Nanyang Technological University; HP Inc.","2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)","26 Jul 2021","2021","","","1","9","Visual sensing has been increasingly employed in industrial processes. This paper presents the design and implementation of an industrial wireless camera system, namely, EFCam, which uses low-power wireless communications and edge-fog computing to achieve cordless and energy-efficient visual sensing. The camera performs image pre-processing (i.e., compression or feature extraction) and transmits the data to a resourceful fog node for advanced processing using deep models. EFCam admits dynamic configurations of several parameters that form a configuration space. It aims to adapt the configuration to maintain desired visual sensing performance of the deep model at the fog node with minimum energy consumption of the camera in image capture, pre-processing, and data communications, under dynamic variations of application requirement and wireless channel conditions. However, the adaptation is challenging due primarily to the complex relationships among the involved factors. To address the complexity, we apply deep reinforcement learning to learn the optimal adaptation policy. Extensive evaluation based on trace-driven simulations and experiments show that EFCam complies with the accuracy and latency requirements with lower energy consumption for a real industrial product object tracking application, compared with four baseline approaches incorporating hysteresis-based adaptation.","2155-5494","978-1-6654-4108-7","10.1109/SECON52354.2021.9491609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9491609","","Wireless communication;Wireless sensor networks;Visualization;Energy consumption;Adaptation models;Reinforcement learning;Cameras","cameras;deep learning (artificial intelligence);distributed processing;energy conservation;feature extraction;image capture;object tracking;radiocommunication;telecommunication power management;wireless channels","image pre-processing;low-power wireless communications;EFCam;cordless visual sensing;energy-efficient visual sensing;edge-fog computing;industrial wireless camera system;configuration-adaptive fog-assisted wireless cameras;hysteresis-based adaptation;industrial product object tracking application;energy consumption;optimal adaptation policy;deep reinforcement learning;wireless channel conditions;data communications;image capture","","1","","17","IEEE","26 Jul 2021","","","IEEE","IEEE Conferences"
"Securing IoT Communication Using Physical Sensor Data — Graph Layer Security with Federated Multi-agent Deep Reinforcement Learning","L. Wang; Z. Wei; W. Guo","School of Aerospace, Transport and Manufacturing, Cranfield University, Milton Keynes, UK; School of Aerospace, Transport and Manufacturing, Cranfield University, Milton Keynes, UK; School of Aerospace, Transport and Manufacturing, Cranfield University, Milton Keynes, UK","2023 8th International Conference on Signal and Image Processing (ICSIP)","9 Oct 2023","2023","","","860","865","Internet-of-Things (IoT) devices are often used to transmit physical sensor data over digital wireless channels. Traditional Physical Layer Security (PLS)-based cryptography approaches rely on accurate channel estimation and information exchange for key generation, which irrevocably ties key quality with digital channel estimation quality. Recently, we proposed a new concept called Graph Layer Security (GLS), where digital keys are derived from physical sensor readings. The sensor readings between legitimate users are correlated through a common background infrastructure environment (e.g., a common water distribution network or electric grid). The challenge for GLS has been how to achieve distributed key generation. This paper presents a Federated multi-agent Deep reinforcement learning-assisted Distributed Key generation scheme (FD2K), which fully exploits the common features of physical dynamics to establish secret key between legitimate users. We present for the first time initial experimental results of GLS with federated learning, achieving considerable security performance in terms of key agreement rate (KAR), and key randomness.","","979-8-3503-9793-2","10.1109/ICSIP57908.2023.10271026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10271026","graph layer security;multi-agent deep reinforcement learning;distributed key generation;federated learning;feature Extraction","Deep learning;Wireless communication;Wireless sensor networks;Federated learning;Channel estimation;Training data;Reinforcement learning","","","","","","21","IEEE","9 Oct 2023","","","IEEE","IEEE Conferences"
"A hybrid algorithm combining data-driven and simulation-based reinforcement learning approaches to energy management of hybrid electric vehicles","B. Hu; S. Zhang; B. Liu","Ministry of Education, Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Chongqing University of Technology, Chongqing, China; Ministry of Education, Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Chongqing University of Technology, Chongqing, China; Ministry of Education, Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Chongqing University of Technology, Chongqing, China","IEEE Transactions on Transportation Electrification","","2023","PP","99","1","1","Energy management strategy (EMS) is one of the key technologies that improves the fuel efficiency of hybrid electric vehicles (HEVs) by governing the energy flow between the fuel tank and the electric energy storage. With the rapid development of artificial intelligence especially after the great success of AlphaGo, reinforcement learning (RL) has opened up a new window for EMS. Although many RL-based solutions have been successfully applied to EMS tasks, most current RL-based approaches only consider RL as an offline optimization tool, i.e., RL is used to solve optimization problems given a simulation model. This method may be utilized to realize optimal control in simulation, but there is no guarantee that a well-behaved performance can also be achieved in the real-physical world due to the simulation-to-real gap. Meanwhile, for many industrial EMS tasks especially when optimizing an existing controller, a high-fidelity simulator may be unavailable, but some logging data generated by some suboptimal existing controller and simple models of the principal powertrain components neglecting detailed component dynamics could exist. In this context, a hybrid algorithm combining data-driven and simulation-based RL approach is proposed that tries to learn experience from both the real logging data and simulated simple model. Based on the hardware-in-the-loop (HIL) results, a near optimal policy can be obtained (fuel consumption is decreased by approximately 6.10%) by this hybrid algorithm from a small batch logging data and a simple simulated model when compared with the dynamic programming (DP) method using a high-fidelity simulation model mimicking the real-physical world.","2332-7782","","10.1109/TTE.2023.3266734","National Natural Science Foundation of China(grant numbers:51905061); Science and Technology Research Program of Chongqing Municipal Education Commission(grant numbers:KJQN202201129); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10103547","Energy Management Strategy;Reinforcement Learning;Data-driven;Simulation-based","Energy management;Data models;Heuristic algorithms;Hybrid electric vehicles;Resistance;Vehicle dynamics;Atmospheric modeling","","","","","","","IEEE","17 Apr 2023","","","IEEE","IEEE Early Access Articles"
"Hierarchical Gait Generation for Modular Robots Using Deep Reinforcement Learning","J. Wang; C. Hu; Y. Zhu","State Key Laboratory of Tribology, Department of Mechanical Engineering, Tsinghua University, Beijing, China; State Key Laboratory of Tribology, Department of Mechanical Engineering, Tsinghua University, Beijing, China; State Key Laboratory of Tribology, Department of Mechanical Engineering, Tsinghua University, Beijing, China","2021 IEEE International Conference on Mechatronics (ICM)","30 Mar 2021","2021","","","1","6","Modular robots have the ability to perform versatile locomotion with a high diversity of morphologies. However, designing robust locomotion gaits for arbitrary robot morphologies remains exceptionally challenging. In this paper, a two-level hierarchical locomotion framework is presented for addressing modular robot locomotion tasks. The framework combines a central pattern generator controller (CPG) with a neural network trained by deep reinforcement learning. First, the low-level CPG controllers are learned by offline optimization and generate robust straight walking gaits. Second, a high-level neural network is then learned using deep reinforcement learning via trial-and-errors. The high-level learned controller can modulate the low-level CPG parameters based on online inputs including robot states and user commands. Simulation experiments are employed on a 3D modular robot. The results show that the proposed method achieves better overall performance than the baseline methods on different locomotion skills including straight walking, velocity tracking, and circular turning. Simulation results confirm the effectiveness and robustness of the proposed method.","","978-1-7281-4442-9","10.1109/ICM46511.2021.9385659","Beijing Natural Science Foundation(grant numbers:JQ19010); State Key Laboratory of Tribology(grant numbers:SKLT2020D22); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9385659","Reinforcement learning;central pattern generator;locomotion control;modular robot","Legged locomotion;Neural networks;Morphology;Reinforcement learning;Robustness;Task analysis;Robots","deep learning (artificial intelligence);mobile robots;motion control;neurocontrollers;robot dynamics;robot kinematics","high-level learned controller;low-level CPG parameters;robot states;3D modular robot;locomotion skills;hierarchical gait generation;modular robots;deep reinforcement learning;versatile locomotion;robust locomotion gaits;arbitrary robot morphologies;two-level hierarchical locomotion framework;modular robot locomotion tasks;central pattern generator controller;low-level CPG controllers;robust straight walking gaits;high-level neural network","","","","19","IEEE","30 Mar 2021","","","IEEE","IEEE Conferences"
"Multiobjective Combinatorial Optimization Using a Single Deep Reinforcement Learning Model","Z. Wang; S. Yao; G. Li; Q. Zhang","School of System Design and Intelligent Manufacturing and the Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, Shenzhen, China; School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, Shenzhen, China; Department of Computer Science, City University of Hong Kong, Hong Kong, Hong Kong","IEEE Transactions on Cybernetics","","2023","PP","99","1","13","This article proposes utilizing a single deep reinforcement learning model to solve combinatorial multiobjective optimization problems. We use the well-known multiobjective traveling salesman problem (MOTSP) as an example. Our proposed method employs an encoder-decoder framework to learn the mapping from the MOTSP instance to its Pareto-optimal set. Specifically, it leverages a novel routing encoder to extract information for both the entire multiobjective aspect and every individual objective from the MOTSP instance. The global embeddings and each objective’s embeddings are adaptively aggregated via a routing network to form the subproblems’ embedding that can well represent the MOTSP features. Using a modified context embedding, the subproblems’ embeddings are fed into a decoder to produce a set of approximate Pareto-optimal solutions in parallel. Additionally, we develop a Top-k baseline to enable more efficient data utilization and lightweight training for our proposed method. We compare our method with heuristic-based and learning-based ones on various types of MOTSP instances, and the experimental results show that our method can solve MOTSP instances in real-time and outperform the other algorithms, especially on large-scale problem instances.","2168-2275","","10.1109/TCYB.2023.3312476","National Natural Science Foundation of China(grant numbers:62106096,62206120,62276223); Characteristic Innovation Project of Colleges and Universities in Guangdong Province(grant numbers:2022KTSCX110); Shenzhen Technology Plan(grant numbers:JCYJ202205301130130311); Research Grants Council of the Hong Kong Special Administrative Region(grant numbers:CityU,11215622); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10266708","Attention mechanism;combinatorial optimization (CO);deep reinforcement learning;multiobjective optimization;routing network;traveling salesman problem","Heuristic algorithms;Optimization;Approximation algorithms;Urban areas;Training;Decoding;Routing","","","","","","","IEEE","28 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Design of Observer-Based Control With Residual Generator Using Actor–Critic Reinforcement Learning","L. Qian; X. Zhao; P. Liu; Z. Zhang; Y. Lv","School of Transportation and Logistics Engineering, Wuhan University of Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; School of Transportation and Logistics Engineering, Wuhan University of Technology, Wuhan, China","IEEE Transactions on Artificial Intelligence","21 Jul 2023","2023","4","4","734","743","Observer-based control has been widely used in mechatronic systems. In this article, an observer-based control integrated with a residual generator is designed in the framework of actor–critic reinforcement learning, which has been applied to robot systems. In the learning process, a critic function is constructed by the state of the original system and its twin system. Thus, the system parameters and control gain can be obtained simultaneously through trial-and-error learning. To achieve system stability and reliability, the observer-based control with the residual generator is designed based on the learned results. The performance and effectiveness of the proposed scheme are demonstrated through a robot test rig. After a short period of learning, the robot is controlled only with the measured joint angle, and meanwhile, the residual generator can be used for fault detection to improve the system reliability.","2691-4581","","10.1109/TAI.2022.3215671","National Natural Science Foundation of China(grant numbers:52275020,72101194); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9924571","Observer-based control;residual generator;reinforcement learning (RL);robot","Observers;Generators;Robots;Fault detection;Reinforcement learning;Mathematical models;Process control","fault diagnosis;learning (artificial intelligence);mechatronics;observers;reinforcement learning","-error learning;actor-critic reinforcement learning;control gain;mechatronic systems;observer-based control;reliability;residual generator;robot systems;system parameters;system stability;twin system","","","","42","IEEE","19 Oct 2022","","","IEEE","IEEE Journals"
"Incremental Reinforcement Learning Via Performance Evaluation and Policy Perturbation","G. Deng; H. Fu; X. Wang; C. Liu; K. Tang; C. Chen","Mechanical Engineering, School of Manufacturing Science and Engineering, Southwest University of Science and Technology, Mianyang, China; Department of Control and Systems Engineering, School of Management and Engineering, Nanjing University, Nanjing, China; Mechanical Engineering, School of Manufacturing Science and Engineering, Southwest University of Science and Technology, Mianyang, China; Mechanical Engineering, School of Manufacturing Science and Engineering, Southwest University of Science and Technology, Mianyang, China; Department of Control and Systems Engineering, School of Management and Engineering, Nanjing University, Nanjing, China; Department of Control and Systems Engineering, School of Management and Engineering, Nanjing University, Nanjing, China","IEEE Transactions on Artificial Intelligence","","2023","PP","99","1","11","Rapid adaptation to the environment is the long-term task of reinforcement learning. However, reinforcement learning faces great challenges in dynamic environments, especially with continuous state-action spaces. In this paper, we propose a systematic Incremental Reinforcement Learning method via Performance Evaluation and Policy Perturbation (IRL-PEPP) to improve the adaptability of reinforcement learning algorithms in dynamic environments with continuous state-action spaces, which mainly includes three parts, i.e., performance evaluation, policy perturbation and importance weighting. Firstly, in performance evaluation, we apply the learned optimal policy to sample a few episodes in the original environment and use these samples to evaluate the policy applicability in the new environment. Then, in policy perturbation, the policy is perturbed according to the policy applicability to balance the trade-off between exploration and exploitation in the new environment. Finally, importance weighting is applied to weight the information to speed up the adjustment process of the policy. Experimental results demonstrate the feasibility and efficiency of the proposed IRL-PEPP method for continuous control tasks in comparison to the existing state-of-the-art methods.","2691-4581","","10.1109/TAI.2023.3316637","National Key Research and Development Program of China(grant numbers:2018AAA0101100); National Natural Science Foundation of China(grant numbers:62073160); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255268","Continuous state-action spaces;dynamic environments;incremental reinforcement learning;performance evaluation;policy perturbation","Task analysis;Reinforcement learning;Perturbation methods;Performance evaluation;Training;Deep learning;Monte Carlo methods","","","","","","","IEEE","19 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Hierarchical Free Gait Motion Planning for Hexapod Robots Using Deep Reinforcement Learning","X. Wang; H. Fu; G. Deng; C. Liu; K. Tang; C. Chen","Department of Process Equipment and Control Engineering, School of Manufacturing Science and Engineering, Southwest University of Science and Technology, Mianyang, China; Department of Control and Systems Engineering, School of Management and Engineering, Nanjing University, Nanjing, China; Department of Process Equipment and Control Engineering, School of Manufacturing Science and Engineering, Southwest University of Science and Technology, Mianyang, China; Department of Process Equipment and Control Engineering, School of Manufacturing Science and Engineering, Southwest University of Science and Technology, Mianyang, China; Department of Control and Systems Engineering, School of Management and Engineering, Nanjing University, Nanjing, China; Department of Control and Systems Engineering, School of Management and Engineering, Nanjing University, Nanjing, China","IEEE Transactions on Industrial Informatics","19 Sep 2023","2023","19","11","10901","10912","This article addresses the problem of legged locomotion in unstructured environments, and a novel hierarchical multicontact motion planning method for hexapod robots is proposed by combining free gait motion planning and deep reinforcement learning. We structurally decompose the complex free gait multicontact motion planning task into path planning in discrete state space and gait planning in a continuous state space. First, the soft deep Q-network is used to obtain the global prior path information in the path planner (PP). Second, a free gait planner (FGP) is proposed to obtain the gait sequence. Finally, based on the PP and the FGP, the center-of-mass sequence is generated by the trained optimal policy using the designed deep reinforcement learning algorithm. Experimental results in different environments demonstrate the feasibility, effectiveness, and advancement of the proposed method.","1941-0050","","10.1109/TII.2023.3240758","National Key Research and Development Program of China(grant numbers:2018AAA0101100); National Natural Science Foundation of China(grant numbers:62073160); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10036102","Deep reinforcement learning (DRL);free gait;hexapod robots;hierarchical motion planning","Robots;Planning;Legged locomotion;Path planning;Heuristic algorithms;Dynamics;Deep learning","legged locomotion;path planning;reinforcement learning","complex free gait multicontact motion planning task;continuous state space;designed deep reinforcement learning algorithm;discrete state space;free gait planner;gait planning;gait sequence;global prior path information;hexapod robots;hierarchical free gait motion planning;legged locomotion;novel hierarchical multicontact motion planning method;path planner;path planning;soft deep Q-network;unstructured environments","","","","36","IEEE","3 Feb 2023","","","IEEE","IEEE Journals"
"Strategic Conflict Management for Performance-based Urban Air Mobility Operations with Multi-agent Reinforcement Learning","C. Huang; I. Petrunin; A. Tsourdos","School of Aerospace, Transport and Manufacturing, Cranfield University, Cranfield, UK; School of Aerospace, Transport and Manufacturing, Cranfield University, Cranfield, UK; School of Aerospace, Transport and Manufacturing, Cranfield University, Cranfield, UK","2022 International Conference on Unmanned Aircraft Systems (ICUAS)","26 Jul 2022","2022","","","442","451","With the urban air mobility (UAM) quickly evolving, the great demand for public airborne transit and deliveries, besides creating a big market, will result in a series of technical, operational, and safety problems. This paper addresses the strategic conflict issue in low-altitude UAM operations with multi-agent reinforcement learning (MARL). Considering the difference in flight characteristics, the aircraft performance is fully integrated into the design process of strategic deconfliction components. With this concept, the multi-resolution structure for the low-altitude airspace organization, Gaussian Mixture Model (GMM) for the speed profile generation, and dynamic separation minima enable efficient UAM operations. To resolve the demand and capacity balancing (DCB) issue and the separation conflict at the strategic stage, the multi-agent asynchronous advantage actor-critic (MAA3C) framework is built with mask recurrent neural networks (RNNs). Meanwhile, variable agent number, dynamic environments, heterogeneous aircraft performance, and action selection between speed adjustment and ground delay can be well handled. Experiments conducted on a developed prototype and various scenarios indicate the obvious advantages of the constructed MAA3C in minimizing the delay cost and refining speed profiles. And the effectiveness, scalability, and stabilization of the MARL solution are ultimately demonstrated.","2575-7296","978-1-6654-0593-5","10.1109/ICUAS54217.2022.9836139","China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9836139","","Recurrent neural networks;Scalability;Refining;Weather forecasting;Reinforcement learning;Organizations;Delays","aerospace computing;air traffic control;aircraft control;control engineering computing;Gaussian processes;mixture models;recurrent neural nets;reinforcement learning","delay cost;deliveries;dynamic separation minima;efficient UAM operations;heterogeneous aircraft performance;low-altitude airspace organization;low-altitude UAM operations;multiagent asynchronous advantage actor-critic framework;multiagent reinforcement learning;multiresolution structure;operational problem;performance-based urban air mobility operations;public airborne transit;refining speed profiles;safety problem;separation conflict;speed profile generation;strategic conflict issue;strategic conflict management;strategic deconfliction components;strategic stage;technical problem;variable agent number","","","","25","IEEE","26 Jul 2022","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Scheme for Active Multi-Debris Removal Mission Planning With Modified Upper Confidence Bound Tree Search","J. Yang; X. Hou; Y. H. Hu; Y. Liu; Q. Pan","School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; Department of Electrical and Computer Engineering, University of Wisconsin–Madison, Madison, USA; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China","IEEE Access","17 Jun 2020","2020","8","","108461","108473","The increasing number of space debris is a critical impact on space environment. Active multi-debris removal (ADR) mission planning technique with maximal reward objective is getting more attention. As the goal of Reinforcement Learning (RL) is in accordance with maximal-reward optimization model of ADR, the planning will be more efficient with the advanced RL scheme and RL algorithm. In this paper, first, an RL formulation is presented for the ADR mission planning problem. All the basic components of maximal-reward optimization model are recast in RL scheme. Second, a modified Upper Confidence bound Tree (UCT) search algorithm for the ADR planning task is developed, which both leverages the neural-network-assisted selection and expansion procedures to facilitate exploration and incorporates roll-out simulation in the backup procedure to achieve robust value estimation. This algorithm fits the RL scheme of ADR mission planning and better balances the exploration and exploitation. Experimental comparison using three subsets of Iridium 33 debris cloud data reveals a better performance of this modified UCT over previously reported results and close UCT variants.","2169-3536","","10.1109/ACCESS.2020.3001311","National Natural Science Foundation of China(grant numbers:61772059,61421003); Beijing Advanced Innovation Center for Big Data and Brain Computing (BDBC); State Key Laboratory of Software Development Environment; Fundamental Research Funds for the Central Universities and by the Beijing S&T Committee; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9113461","Monte Carlo tree search;multi-debris active removal;reinforcement learning;space mission planning","Planning;Perturbation methods;Optimization;Reinforcement learning;Space vehicles;Planetary orbits","aerospace computing;learning (artificial intelligence);neural nets;optimisation;space debris;tree searching","reinforcement learning scheme;space debris;space environment;active multidebris removal mission planning;maximal reward objective;maximal-reward optimization model;advanced RL scheme;RL algorithm;neural-network-assisted selection;Iridium 33 debris cloud data;modified UCT;modified upper confidence bound tree search algorithm;neural-network-assisted expansion procedure;ADR mission planning;ADR planning","","4","","45","CCBY","10 Jun 2020","","","IEEE","IEEE Journals"
"Lessons Learned from Data-Driven Building Control Experiments: Contrasting Gaussian Process-based MPC, Bilevel DeePC, and Deep Reinforcement Learning","L. Di Natale; Y. Lian; E. T. Maddalena; J. Shi; C. N. Jones","Automatic Control Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland; Automatic Control Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland; Automatic Control Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland; Automatic Control Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland; Automatic Control Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland","2022 IEEE 61st Conference on Decision and Control (CDC)","10 Jan 2023","2022","","","1111","1117","This manuscript offers the perspective of experimentalists on a number of modern data-driven techniques: model predictive control relying on Gaussian processes, adaptive data-driven control based on behavioral theory, and deep reinforcement learning. These techniques are compared in terms of data requirements, ease of use, computational burden, and robustness in the context of real-world applications. Our remarks and observations stem from a number of experimental investigations carried out in the field of building control in diverse environments, from lecture halls and apartment spaces to a hospital surgery center. The final goal is to support others in identifying what technique is best suited to tackle their own problems.","2576-2370","978-1-6654-6761-2","10.1109/CDC51059.2022.9992445","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9992445","","Deep learning;Hospitals;Surgery;Reinforcement learning;Gaussian processes;Aerospace electronics;Robustness","adaptive control;construction industry;Gaussian processes;predictive control;reinforcement learning;surgery","adaptive data-driven control;behavioral theory;bilevel DeePC;data-driven building control experiments;deep reinforcement learning;Gaussian process-based MPC;model predictive control","","3","","35","IEEE","10 Jan 2023","","","IEEE","IEEE Conferences"
"SNR-based Reinforcement Learning Rate Adaptation for Time Critical Wi-Fi Networks: Assessment through a Calibrated Simulator","G. Peserico; T. Fedullo; A. Morato; F. Tramarin; L. Rovati; S. Vitturi","Dept. of Information Engineering, Autec s.r.l., University of Padova, Italy; Dept. of Management and Engineering, University of Padova, Italy; Dept. of Information Engineering, CMZ Sistemi Elettronici s.r.l., University of Padova, Italy; Dept. of Engineering “Enzo Ferrari”, University of Modena and Reggio Emilia, Italy; Dept. of Engineering “Enzo Ferrari”, University of Modena and Reggio Emilia, Italy; National Research Council of Italy, CNR-IEIIT","2021 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)","28 Jun 2021","2021","","","1","6","Nowadays, the Internet of Things is spreading in several different research fields, such as factory automation, instrumentation and measurement, and process control, where it is referred to as Industrial Internet of Things. In these scenarios, wireless communication represents a key aspect to guarantee the required pervasive connectivity required. In particular, Wi-Fi networks are revealing ever more attractive also in time- and mission-critical applications, such as distributed measurement systems. Also, the multi-rate support feature of Wi-Fi, which is implemented by rate adaptation (RA) algorithms, demonstrated its effectiveness to improve reliability and timeliness. In this paper, we propose an enhancement of RSIN, which is a RA algorithm specifically conceived for industrial real-time applications. The new algorithm starts from the assumption that an SNR measure has been demonstrated to be effective to perform RA, and bases on Reinforcement Learning techniques. In detail, we start from the design of the algorithm and its implementation on the OmNet++ simulator. Then, the simulation model is adequately calibrated exploiting the results of a measurement campaign, to reflect the channel behavior typical of industrial environments. Finally, we present the results of an extensive performance assessment that demonstrate the effectiveness of the proposed technique.","2642-2077","978-1-7281-9539-1","10.1109/I2MTC50364.2021.9460075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460075","Factory Automation;Wi-Fi;Rate Adaptation;Reinforcement Learning","Wireless communication;Adaptation models;Reinforcement learning;Time measurement;Real-time systems;Production facilities;Instrumentation and measurement","calibration;learning (artificial intelligence);measurement systems;wireless LAN","distributed measurement systems;multirate support feature;rate adaptation algorithms;reliability;RA algorithm;real-time applications;SNR measure;Reinforcement Learning techniques;OmNet++ simulator;simulation model;measurement campaign;industrial environments;extensive performance assessment;SNR-based Reinforcement Learning rate adaptation;Time Critical Wi-Fi Networks;calibrated simulator;research fields;instrumentation;process control;wireless communication;pervasive connectivity;mission-critical applications","","1","","16","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Learning socially normative robot navigation behaviors with Bayesian inverse reinforcement learning","B. Okal; K. O. Arras","Social Robotics Lab, Univ. of Freiburg; Bosch Corporate Research, Germany","2016 IEEE International Conference on Robotics and Automation (ICRA)","9 Jun 2016","2016","","","2889","2895","Mobile robots that navigate in populated environments require the capacity to move efficiently, safely and in human-friendly ways. In this paper, we address this task using a learning approach that enables a mobile robot to acquire navigation behaviors from demonstrations of socially normative human behavior. In the past, such approaches have been typically used to learn only simple behaviors under relatively controlled conditions using rigid representations or with methods that scale poorly to large domains. We thus develop a flexible graph-based representation able to capture relevant task structure and extend Bayesian inverse reinforcement learning to use sampled trajectories from this representation. In experiments with a real robot and a large-scale pedestrian simulator, we are able to show that the approach enables a robot to learn complex navigation behaviors of varying degrees of social normativeness using the same set of simple features.","","978-1-4673-8026-3","10.1109/ICRA.2016.7487452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487452","","Navigation;Trajectory;Mobile robots;Bayes methods;Learning (artificial intelligence);Measurement","Bayes methods;graph theory;learning systems;mobile robots;path planning","socially normative robot navigation behaviors;Bayesian inverse reinforcement learning;mobile robots;socially normative human behavior;graph-based representation;large-scale pedestrian simulator","","62","","25","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"Learning with Training Wheels: Speeding up Training with a Simple Controller for Deep Reinforcement Learning","L. Xie; S. Wang; S. Rosa; A. Markham; N. Trigoni","Department of Computer Science, University of Oxford, Oxford, United Kingdom; School of Engineering and Physical Sciences, Heriot-Watt University, Edinburgh, United Kingdom; Department of Computer Science, University of Oxford, Oxford, United Kingdom; Department of Computer Science, University of Oxford, Oxford, United Kingdom; Department of Computer Science, University of Oxford, Oxford, United Kingdom","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","6276","6283","Deep Reinforcement Learning (DRL) has been applied successfully to many robotic applications. However, the large number of trials needed for training is a key issue. Most of existing techniques developed to improve training efficiency (e.g. imitation) target on general tasks rather than being tailored for robot applications, which have their specific context to benefit from. We propose a novel framework, Assisted Reinforcement Learning, where a classical controller (e.g. a PID controller) is used as an alternative, switchable policy to speed up training of DRL for local planning and navigation problems. The core idea is that the simple control law allows the robot to rapidly learn sensible primitives, like driving in a straight line, instead of random exploration. As the actor network becomes more advanced, it can then take over to perform more complex actions, like obstacle avoidance. Eventually, the simple controller can be discarded entirely. We show that not only does this technique train faster, it also is less sensitive to the structure of the DRL network and consistently outperforms a standard Deep Deterministic Policy Gradient network. We demonstrate the results in both simulation and real-world experiments.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8461203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461203","","Training;Navigation;Acceleration;Robot kinematics;Machine learning;Task analysis","collision avoidance;learning (artificial intelligence);mobile robots;three-term control","DRL network;standard Deep Deterministic Policy Gradient network;training wheels;deep reinforcement learning;robotic applications;robot applications;Assisted Reinforcement Learning;PID controller;local planning;navigation problems;simple control law","","43","","26","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Using Deep Reinforcement Learning to Learn High-Level Policies on the ATRIAS Biped","T. Li; H. Geyer; C. G. Atkeson; A. Rai","Mechanical Engineenng, Carnegie Mellon University, USA; Robotics Institute, School of Computer Science, Carnegie Mellon University, USA; Robotics Institute, School of Computer Science, Carnegie Mellon University, USA; Facebook AI Research","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","263","269","Learning controllers for bipedal robots is a challenging problem, often requiring expert knowledge and extensive tuning of parameters that vary in different situations. Recently, deep reinforcement learning has shown promise at automatically learning controllers for complex systems in simulation. This has been followed by a push towards learning controllers that can be transferred between simulation and hardware, primarily with the use of domain randomization. However, domain randomization can make the problem of finding stable controllers even more challenging, especially for under actuated bipedal robots. In this work, we explore whether policies learned in simulation can be transferred to hardware with the use of high-fidelity simulators and structured controllers. We learn a neural network policy which is a part of a more structured controller. While the neural network is learned in simulation, the rest of the controller stays fixed, and can be tuned by the expert as needed. We show that using this approach can greatly speed up the rate of learning in simulation, as well as enable transfer of policies between simulation and hardware. We present our results on an ATRIAS robot and explore the effect of action spaces and cost functions on the rate of transfer between simulation and hardware. Our results show that structured policies can indeed be learned in simulation and implemented on hardware successfully. This has several advantages, as the structure preserves the intuitive nature of the policy, and the neural network improves the performance of the hand-designed policy. In this way, we propose a way of using neural networks to improve expert designed controllers, while maintaining ease of understanding.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793864","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793864","","Neural networks;Hardware;Legged locomotion;Reinforcement learning;Torso;Foot","control system synthesis;learning (artificial intelligence);legged locomotion;neurocontrollers","deep reinforcement learning;expert knowledge;domain randomization;stable controllers;high-fidelity simulators;neural network policy;ATRIAS robot;bipedal robots","","28","","24","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning","C. Schaff; D. Yunis; A. Chakrabarti; M. R. Walter","Toyota Technological Institute at Chicago, Chicago, IL, USA; Department of Mathematics, University of Chicago, Chicago, IL, USA; Department of Computer Science and Engineering, Washington University in St. Louis, St. Louis, MO, USA; Toyota Technological Institute at Chicago, Chicago, IL, USA","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","9798","9805","The physical design of a robot and the policy that controls its motion are inherently coupled, and should be determined according to the task and environment. In an increasing number of applications, data-driven and learning-based approaches, such as deep reinforcement learning, have proven effective at designing control policies. For most tasks, the only way to evaluate a physical design with respect to such control policies is empirical-i.e., by picking a design and training a control policy for it. Since training these policies is time-consuming, it is computationally infeasible to train separate policies for all possible designs as a means to identify the best one. In this work, we address this limitation by introducing a method that jointly optimizes over the physical design and control network. Our approach maintains a distribution over designs and uses reinforcement learning to optimize a control policy to maximize expected reward over the design distribution. We give the controller access to design parameters to allow it to tailor its policy to each design in the distribution. Throughout training, we shift the distribution towards higher-performing designs, eventually converging to a design and control policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel designs and walking gaits, outperforming baselines across different settings.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793537","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793537","","Physical design;Optimization;Reinforcement learning;Training;Legged locomotion;Task analysis","learning (artificial intelligence);legged locomotion;optimisation","control policy;walking gaits;deep reinforcement learning;learning-based approaches;control network;design distribution;controller access;design parameters;legged locomotion;physical design","","23","","49","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Learning assistive strategies from a few user-robot interactions: Model-based reinforcement learning approach","M. Hamaya; T. Matsubara; T. Noda; T. Teramae; J. Morimoto","Graduate School of Frontier Bioscience, Osaka University, Osaka, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Department of Brain Robot Interface, ATR-CNS, Kyoto, Japan; Department of Brain Robot Interface, ATR-CNS, Kyoto, Japan; Department of Brain Robot Interface, ATR-CNS, Kyoto, Japan","2016 IEEE International Conference on Robotics and Automation (ICRA)","9 Jun 2016","2016","","","3346","3351","Designing an assistive strategy for exoskeletons is a key ingredient in movement assistance and rehabilitation. While several approaches have been explored, most studies are based on mechanical models of the human user, i.e., rigid-body dynamics or Center of Mass (CoM)-Zero Moment Point (ZMP) inverted pendulum moECenter of Massdel, or only focus on periodic movements with using oscillator models. On the other hand, the interactions between the user and the robot are often not considered explicitly because of its difficulty in modeling. In this paper, we propose to learn the assistive strategies directly from interactions between the user and the robot. We formulate the learning problem of assistive strategies as a policy search problem. To alleviate heavy burdens to the user for data acquisition, we exploit a data-efficient model-based reinforcement learning framework. To validate the effectiveness of our approach, an experimental platform composed of a real subject, an electromyography (EMG)-measurement system, and a simulated robot arm is developed. Then, a learning experiment with the assistive control task of the robot arm is conducted. As a result, proper assistive strategies that can achieve the robot control task and reduce EMG signals of the user are acquired only by 30 seconds interactions.","","978-1-4673-8026-3","10.1109/ICRA.2016.7487509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487509","","Robot kinematics;Learning (artificial intelligence);Electromyography;Data models;Robot control;Uncertainty","human-robot interaction;learning (artificial intelligence);service robots","learning assistive strategies;user robot interactions;reinforcement learning approach;movement assistance;movement rehabilitation;mechanical models;human user;rigid-body dynamics;Center of Mass;ZMP;inverted pendulum;policy search problem;electromyography;EMG measurement system","","9","","28","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"Scalable Scheduling of Semiconductor Packaging Facilities Using Deep Reinforcement Learning","I. -B. Park; J. Park","Department of Industrial and Management Engineering, Myongji University, Yongin, South Korea; Department of Industrial Engineering, Institute for Industrial Systems Innovation, Seoul National University, Seoul, South Korea","IEEE Transactions on Cybernetics","17 May 2023","2023","53","6","3518","3531","Reinforcement learning (RL) has emerged as a promising approach for scheduling semiconductor operations. Yet, it is still challenging to solve large-scale scheduling problems based on an RL method since learning complexity grows fast as the size of shop floor increases. This challenge becomes more apparent when solving the scheduling problems with a diverse number of job types, which leads to the difficulties in exploration and function approximation in RL. This article presents a scheduling method for semiconductor packaging facilities using deep RL in which an agent allocates a job to one of machines in a centralized manner. Specifically, a novel state representation is introduced to effectively accommodate the variations in the number of available machines and the production requirements. Furthermore, we propose a continuous representation of an action to maintain the size of the action space even when the numbers of jobs, machines, and operation types are subject to change. Extensive experiments on large-scale datasets demonstrate that the proposed method mostly outperforms the metaheuristics and rule-based methods, as well as the other RL approaches considered in terms of makespan while requiring much less computation time than the metaheuristics.","2168-2275","","10.1109/TCYB.2021.3128075","National Research Foundation of Korea (NRF); Korea Government (MSIP)(grant numbers:NRF-2015R1D1A1A01057496); Institute of Engineering Research at Seoul National University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635709","Deep deterministic policy gradient (DDPG);deep reinforcement learning (DRL);flexible job shop scheduling;semiconductor manufacturing system;semiconductor packaging","Job shop scheduling;Processor scheduling;Packaging;Production;Artificial neural networks;Metaheuristics;Aerospace electronics","deep learning (artificial intelligence);function approximation;learning (artificial intelligence);reinforcement learning;scheduling","available machines;deep reinforcement learning;deep RL;diverse number;function approximation;job types;large-scale datasets;large-scale scheduling problems;operation types;RL approaches;RL method;scalable scheduling;scheduling method;scheduling semiconductor operations;semiconductor packaging facilities;shop floor increases;state representation","","8","","53","IEEE","3 Dec 2021","","","IEEE","IEEE Journals"
"Learning task-parametrized assistive strategies for exoskeleton robots by multi-task reinforcement learning","M. Hamaya; T. Matsubara; T. Noda; T. Teramae; J. Morimoto","Graduate School of Frontier Bioscience, Osaka University, Osaka, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Department of Brain Robot Interface, ATR-CNS, Kyoto, Japan; Department of Brain Robot Interface, ATR-CNS, Kyoto, Japan; Department of Brain Robot Interface, ATR-CNS, Kyoto, Japan","2017 IEEE International Conference on Robotics and Automation (ICRA)","24 Jul 2017","2017","","","5907","5912","Recent studies suggest that reinforcement learning has great potential for generating assistive strategies in exoskeletons through physical interactions between a user and a robot. Previous methods focused on a task-specific assistive strategy, where for every single task (situation/context), the user needs to interact with a robot to learn an appropriate assistive strategy. Therefore, the learned strategies cannot be generalized for a new task. Since the sampling cost is expensive for such human-in-the-loop systems as exoskeletons, generalization must be enabled. In this paper, we propose to learn task-parametrized assistive strategies for exoskeleton robots. Our method employs an assistive strategy, which depends on the task parameter and the state variable, that can be learned from multiple sets of human-robot interaction data across different tasks and generalized even for an unseen task, given the task parameter without additional learning. To alleviate the user's burden in the learning process across multiple tasks, we exploit a data-efficient multi-task reinforcement learning framework. To verify the effectiveness of our method, we developed an experimental platform with an exoskeleton robot. We conducted a series of experiments whose experimental results show that our method can learn such a task-parametrized assistive strategy and be generalized for unseen tasks to reduce the user's electromyography signals (EMGs) during tasks.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989695","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989695","","Exoskeletons;Learning (artificial intelligence);Electromyography;Robot kinematics;Training;Robot sensing systems","electromyography;human-robot interaction;learning (artificial intelligence);medical robotics;prosthetics","learning task-parametrized assistive strategies;exoskeleton robots;multi-task reinforcement learning;human-robot interaction;electromyography signals;EMGs","","8","","31","IEEE","24 Jul 2017","","","IEEE","IEEE Conferences"
"LASER: Learning a Latent Action Space for Efficient Reinforcement Learning","A. Allshire; R. Martín-Martín; C. Lin; S. Manuel; S. Savarese; A. Garg","Vector Institute, University of Toronto; Stanford University; Stanford University; Stanford University; Stanford University; Vector Institute, University of Toronto","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","6650","6656","The process of learning a manipulation task depends strongly on the action space used for exploration: posed in the incorrect action space, solving a task with reinforcement learning can be drastically inefficient. Additionally, similar tasks or instances of the same task family impose latent manifold constraints on the most effective action space: the task family can be best solved with actions in a manifold of the entire action space of the robot. Combining these insights we present LASER, a method to learn latent action spaces for efficient reinforcement learning. LASER factorizes the learning problem into two sub-problems, namely action space learning and policy learning in the new action space. It leverages data from similar manipulation task instances, either from an offline expert or online during policy learning, and learns from these trajectories a mapping from the original to a latent action space. LASER is trained as a variational encoder-decoder model to map raw actions into a disentangled latent action space while maintaining action reconstruction and latent space dynamic consistency. We evaluate LASER on two contact-rich robotic tasks in simulation, and analyze the benefit of policy learning in the generated latent action space. We show improved sample efficiency compared to the original action space from better alignment of the action space to the task space, as we observe with visualizations of the learned action space manifold. Additional details: pair.toronto.edu/laser","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561232","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561232","","Manifolds;Training;Reinforcement learning;Transforms;Laser modes;Laser ablation;Space exploration","image motion analysis;learning (artificial intelligence)","task space;learned action space manifold;efficient reinforcement learning;incorrect action space;task family;latent manifold constraints;effective action space;entire action space;learning problem;action space learning;policy learning;similar manipulation task instances;map raw actions;disentangled latent action space;action reconstruction;latent space dynamic consistency;contact-rich robotic tasks;generated latent action space;original action space","","7","","36","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Multi-Agent Exploration for Faster and Reliable Deep Q-Learning Convergence in Reinforcement Learning","A. Majumdar; P. Benavidez; M. Jamshidi","Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX; Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX; Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX","2018 World Automation Congress (WAC)","9 Aug 2018","2018","","","1","6","Function approximation based Q-learning, using deep q-learning has had recent extraordinary developments applicable to generalized applications. Many techniques have been introduced to counter the inherent caveats in using a deep neural network in reinforcement learning. We demonstrate the use of multi-agent virtual exploration integrated into existing algorithms to show better convergence property, and show how they can be applied as extensions to provide faster and better converged values.","","978-1-5323-7791-4","10.23919/WAC.2018.8430409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8430409","Reinforcement Learning;Q-learning;DQN;Parallel processing;Unmanned autonomous vehicles;Simulation;Multi-agent;Exploration","Neural networks;Learning (artificial intelligence);Convergence;Mathematical model;Training;Optimization;Approximation algorithms","convergence;function approximation;learning (artificial intelligence);multi-agent systems;neural nets","reinforcement learning;function approximation;deep neural network;multiagent virtual exploration;deep Q-learning convergence","","5","","25","","9 Aug 2018","","","IEEE","IEEE Conferences"
"Visuotactile-RL: Learning Multimodal Manipulation Policies with Deep Reinforcement Learning","J. Hansen; F. Hogan; D. Rivkin; D. Meger; M. Jenkin; G. Dudek","Samsung AI Research Center Montreal, Montreal, QC, Canada; Samsung AI Research Center Montreal, Montreal, QC, Canada; Samsung AI Research Center Montreal, Montreal, QC, Canada; Samsung AI Research Center Montreal, Montreal, QC, Canada; Samsung AI Research Center Montreal, Montreal, QC, Canada; Samsung AI Research Center Montreal, Montreal, QC, Canada","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","8298","8304","Manipulating objects with dexterity requires timely feedback that simultaneously leverages the senses of vision and touch. In this paper, we focus on the problem setting where both visual and tactile sensors provide pixel-level feedback for Visuotactile reinforcement learning agents. We investigate the challenges associated with multimodal learning and propose several improvements to existing RL methods; including tactile gating, tactile data augmentation, and visual degradation. When compared with visual-only and tactile-only baselines, our Visuotactile-RL agents showcase (1) significant improvements in contact-rich tasks; (2) improved robustness to visual changes (lighting/camera view) in the workspace; and (3) resilience to physical changes in the task environment (weight/friction of objects).","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812019","","Visualization;Perturbation methods;Optical feedback;Tactile sensors;Reinforcement learning;Robustness;Optical sensors","deep learning (artificial intelligence);image sensors;manipulators;reinforcement learning;robot programming;robot vision;tactile sensors","robotic manipulation;visuotactile-RL agents;visuotactile reinforcement learning agents;dexterous object manipulation;physical changes;contact-rich tasks;deep reinforcement learning;multimodal manipulation policies;visual changes;visual degradation;tactile data augmentation;tactile gating;multimodal learning;pixel-level feedback;tactile sensors;visual sensors","","5","","36","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Approximate Inverse Reinforcement Learning from Vision-based Imitation Learning","K. Lee; B. Vlahov; J. Gibson; J. M. Rehg; E. A. Theodorou","Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","10793","10799","In this work, we present a method for obtaining an implicit objective function for vision-based navigation. The proposed methodology relies on Imitation Learning, Model Predictive Control (MPC), and an interpretation technique used in Deep Neural Networks. We use Imitation Learning as a means to do Inverse Reinforcement Learning in order to create an approximate cost function generator for a visual navigation challenge. The resulting cost function, the costmap, is used in conjunction with MPC for real-time control and outperforms other state-of-the-art costmap generators in novel environments. The proposed process allows for simple training and robustness to out-of-sample data. We apply our method to the task of vision-based autonomous driving in multiple real and simulated environments and show its generalizability. Supplementary video: https://youtu.be/WyJfT5lc0aQ","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9560916","Langley Research Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560916","","Training;Visualization;Navigation;Reinforcement learning;Streaming media;Cost function;Feature extraction","deep learning (artificial intelligence);mobile robots;path planning;predictive control;reinforcement learning;robot vision","real-time control;costmap generators;vision-based autonomous driving;approximate inverse reinforcement learning;vision-based imitation learning;implicit objective function;vision-based navigation;model predictive control;MPC;deep neural networks;approximate cost function generator;visual navigation challenge;cost function","","4","","29","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Continuous Value Iteration (CVI) Reinforcement Learning and Imaginary Experience Replay (IER) For Learning Multi-Goal, Continuous Action and State Space Controllers","A. Gerken; M. Spranger","Sony Computer Science Laboratories Inc., Tokyo, Japan; Sony Computer Science Laboratories Inc., Tokyo, Japan","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","7173","7179","This paper presents a novel model-free Reinforcement Learning algorithm for learning behavior in continuous action, state, and goal spaces. The algorithm approximates optimal value functions using non-parametric estimators. It is able to efficiently learn to reach multiple arbitrary goals in deterministic and nondeterministic environments. To improve generalization in the goal space, we propose a novel sample augmentation technique. Using these methods, robots learn faster and overall better controllers. We benchmark the proposed algorithms using simulation and a real-world voltage controlled robot that learns to maneuver in a non-observable Cartesian task space.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794347","","Aerospace electronics;Robot kinematics;Task analysis;Trajectory;Mathematical model;Voltage control","iterative methods;learning (artificial intelligence);optimal control;state-space methods","continuous action;state space controllers;goal space;optimal value functions;nonparametric estimators;multiple arbitrary goals;real-world voltage controlled robot;nonobservable Cartesian task space;multigoal;model-free reinforcement learning algorithm","","4","","26","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Safe-Control-Gym: A Unified Benchmark Suite for Safe Learning-Based Control and Reinforcement Learning in Robotics","Z. Yuan; A. W. Hall; S. Zhou; L. Brunke; M. Greeff; J. Panerati; A. P. Schoellig","Dynamic Systems Lab and Institute for Aerospace Studies, University of Toronto, Toronto, ON, Canada; Dynamic Systems Lab and Institute for Aerospace Studies, University of Toronto, Toronto, ON, Canada; Dynamic Systems Lab and Institute for Aerospace Studies, University of Toronto, Toronto, ON, Canada; Dynamic Systems Lab and Institute for Aerospace Studies, University of Toronto, Toronto, ON, Canada; Dynamic Systems Lab and Institute for Aerospace Studies, University of Toronto, Toronto, ON, Canada; Dynamic Systems Lab and Institute for Aerospace Studies, University of Toronto, Toronto, ON, Canada; Dynamic Systems Lab and Institute for Aerospace Studies, University of Toronto, Toronto, ON, Canada","IEEE Robotics and Automation Letters","26 Aug 2022","2022","7","4","11142","11149","In recent years, both reinforcement learning and learning-based control—as well as the study of their safety, which is crucial for deployment in real-world robots—have gained significant traction. However, to adequately gauge the progress and applicability of new results, we need the tools to equitably compare the approaches proposed by the controls and reinforcement learning communities. Here, we propose a new open-source benchmark suite, called safe-control-gym, supporting both model-based and data-based control techniques. We provide implementations for three dynamic systems—the cart-pole, the 1D, and 2D quadrotor—and two control tasks—stabilization and trajectory tracking. We propose to extend OpenAI's Gym API—the de facto standard in reinforcement learning research—with (i) the ability to specify (and query) symbolic dynamics and (ii) constraints, and (iii) (repeatably) inject simulated disturbances in the control inputs, state measurements, and inertial properties. To demonstrate our proposal and in an attempt to bring research communities closer together, we show how to use safe-control-gym to quantitatively compare the control performance, data efficiency, and safety of multiple approaches from the fields of traditional control, learning-based control, and reinforcement learning.","2377-3766","","10.1109/LRA.2022.3196132","Natural Sciences and Engineering Research Council of Canada; Mitacs Elevate Fellowship Program; Canada Research Chairs; Canada CIFAR AI Chairs Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849119","Machine learning for robot control;reinforcement learning;robot safety;software tools for benchmarking and reproducibility","Robots;Reinforcement learning;Open source software;Safety;Task analysis;Benchmark testing;Robot control","benchmark testing;control engineering computing;learning (artificial intelligence);public domain software;reinforcement learning;robots","reinforcement learning;unified benchmark suite;safe learning-based control;open-source benchmark suite;data-based control techniques;control inputs;control performance;traditional control;safe-control-gym;real-world robots;significant traction;reinforcement learning communities;dynamic systems;cart-pole;1D;2D quadrotor;control tasks stabilization;trajectory tracking;Gym API;de facto standard;symbolic dynamics;state measurements;inertial properties;data efficiency;learning-based control","","3","","36","IEEE","3 Aug 2022","","","IEEE","IEEE Journals"
"Myocarditis Diagnosis: A Method using Mutual Learning-Based ABC and Reinforcement Learning","S. Danaei; A. Bostani; S. V. Moravvej; F. Mohammadi; R. Alizadehsani; A. Shoeibi; H. Alinejad-Rokny; S. Nahavandi","Adiban Institute of Higher Education, Semnan, Iran; Department of mechanical engineering of biosystems, Urmia university; Internship in UNSW BioMedical Machine Learning Lab, Sydney, NSW, Australia; Internship in UNSW BioMedical Machine Learning Lab, Sydney, NSW, Australia; Institute for Intelligent Systems Research and Innovation (IISRI) Deakin University, Waurn Ponds, Victoria, Australia; UNSW Data Science Hub, The University of New South Wales (UNSW Sydney), Sydney, New South Wales, Australia; BioMedical Machine Learning Lab, The Graduate School of Biomedical Engineering, UNSW Sydney, Sydney, NSW, Australia; Institute for Intelligent Systems Research and Innovation (IISRI) Deakin University, Waurn Ponds, Victoria, Australia","2022 IEEE 22nd International Symposium on Computational Intelligence and Informatics and 8th IEEE International Conference on Recent Achievements in Mechatronics, Automation, Computer Science and Robotics (CINTI-MACRo)","3 Feb 2023","2022","","","000265","000270","Myocarditis occurs when the heart muscle becomes inflamed and inflammation occurs when your body’s immune system responds to infections. It can be diagnosed using cardiac magnetic resonance image (MRI), a non-invasive imaging technique with the possibility of operator bias. This paper proposes a hybrid method of deep reinforcement learning-based algorithms and meta-heuristics algorithms. A mutual learning-based artificial bee colony (ML-ABC) is employed for initial weight, which adjusts the candidate food source generated with the higher fitness between two individuals determined by a mutual learning factor. Moreover, a sequential decision-making process investigates the imbalanced classification issue, in which a convolutional neural network (CNN) is used as the foundation for policy architecture. At first, initial weights are produced using the ML-ABC algorithm. After that, the agent receives a sample at each phase and classifies it, obtaining environmental rewards. The minority class receives more rewards than the majority class. Eventually, the agent discovers an ideal strategy with the aid of a specific reward function and a beneficial learning environment. We evaluate our proposed approach on the Z-Alizadeh Sani myocarditis dataset based on standard criteria and demonstrate that the proposed method gives superior myocarditis diagnosis performance.","2471-9269","979-8-3503-9882-3","10.1109/CINTI-MACRo57952.2022.10029403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10029403","Myocarditis diagnosis;MRI;Deep reinforcement learning;Classification;ABC algorithm.","Mechatronics;Computational modeling;Metaheuristics;Reinforcement learning;Myocardium;Markov processes;Classification algorithms","biomedical MRI;cardiology;convolutional neural nets;decision making;deep learning (artificial intelligence);diseases;medical image processing;muscle;pattern classification;reinforcement learning","beneficial learning environment;candidate food source;cardiac magnetic resonance image;CNN;convolutional neural network;deep reinforcement learning-based algorithms;heart muscle;initial weight;meta-heuristics algorithms;ML-ABC algorithm;MRI;mutual learning factor;mutual learning-based artificial bee colony;myocarditis diagnosis performance;noninvasive imaging technique;sequential decision-making process;Z-Alizadeh Sani myocarditis dataset","","3","","20","IEEE","3 Feb 2023","","","IEEE","IEEE Conferences"
"Learning Multi-Task Transferable Rewards via Variational Inverse Reinforcement Learning","S. -W. Yoo; S. -W. Seo","Department of Electrical and Computer Engineering, Seoul National University, Republic of Korea; Department of Electrical and Computer Engineering, Seoul National University, Republic of Korea","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","434","440","Many robotic tasks are composed of a lot of temporally correlated sub-tasks in a highly complex environment. It is important to discover situational intentions and proper actions by deliberating on temporal abstractions to solve problems effectively. To understand the intention separated from changing task dynamics, we extend an empowerment-based regularization technique to situations with multiple tasks based on the framework of a generative adversarial network. Under the multitask environments with unknown dynamics, we focus on learning a reward and policy from the unlabeled expert examples. In this study, we define situational empowerment as the maximum of mutual information representing how an action conditioned on both a certain state and sub-task affects the future. Our proposed method derives the variational lower bound of the situational mutual information to optimize it. We simultaneously learn the transferable multi-task reward function and policy by adding an induced term to the objective function. By doing so, the multi-task reward function helps to learn a robust policy for environmental change. We validate the advantages of our approach on multi-task learning and multi-task transfer learning. We demonstrate our proposed method has the robustness of both randomness and changing task dynamics. Finally, we prove that our method has significantly better performance and data efficiency than existing imitation learning methods on various benchmarks.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811697","National Research Foundation of Korea (NRF); Seoul National University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811697","","Learning systems;Transfer learning;Reinforcement learning;Multitasking;Linear programming;Robustness;Behavioral sciences","neural nets;reinforcement learning;robot programming","variational inverse reinforcement learning;robotic tasks;temporally correlated sub-tasks;situational intentions;temporal abstractions;empowerment-based regularization technique;generative adversarial network;multitask environments;unknown dynamics;unlabeled expert examples;situational empowerment;situational mutual information;transferable multitask reward function;multitask learning;multitask transfer learning;task dynamics;learning multitask transferable rewards","","2","","21","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Task-specific pre-learning to improve the convergence of reinforcement learning based on a deep neural network","Y. Yang; X. Li; L. Zhang","School of Computer Science and Technology, Northwestern Polytechnical University, Xi'an, Shaanxi, CHINA; School of Computer Science and Technology, Northwestern Polytechnical University, Xi'an, Shaanxi, CHINA; School of Computer Science and Technology, Northwestern Polytechnical University, Xi'an, Shaanxi, CHINA","2016 12th World Congress on Intelligent Control and Automation (WCICA)","29 Sep 2016","2016","","","2209","2214","The poor convergence of reinforcement learning based on neural networks is of great challenging for autonomous robots. Inspiring from human reinforcement learning, we propose a two-phase reinforcement learning model based on deep neural networks to improve the convergence problem. In phase 1, task-specific pre-learning based on supervised learning is employed to train a nonlinear neural network in order to approximate a strong reward function. In phase 2, the learned neural network is modified and expanded to a deep neural network to realize reinforcement learning. We test this two-phase approach on the famous mountain-car problem and an autonomous movement controller of a wheeled robot by simulation. The experimental results show that task-specific pre-learning can significantly improve the convergence of traditional reinforcement learning based on neural networks.","","978-1-4673-8414-8","10.1109/WCICA.2016.7578787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7578787","","Learning (artificial intelligence);Neural networks;Robots;Convergence;Automobiles;Mathematical model;Approximation algorithms","approximation theory;convergence;learning (artificial intelligence);mobile robots;neural nets","wheeled robot;autonomous movement controller;mountain-car problem;strong-reward function approximation;nonlinear neural network training;supervised learning;convergence problem improvement;two-phase reinforcement learning model;autonomous robots;deep neural network;reinforcement learning convergence improvement;task-specific prelearning","","1","","20","IEEE","29 Sep 2016","","","IEEE","IEEE Conferences"
"Torque-Based Deep Reinforcement Learning for Task-and-Robot Agnostic Learning on Bipedal Robots Using Sim-to-Real Transfer","D. Kim; G. Berseth; M. Schwartz; J. Park","Department of Intelligence and Information, Seoul National University, Seoul, Republic of Korea; Université de Montréal, Quebec, Canada; College of Architecture and Design, New Jersey Institute of Technology, Newark, NJ, USA; Department of Intelligence and Information, Seoul National University, Seoul, Republic of Korea","IEEE Robotics and Automation Letters","21 Aug 2023","2023","8","10","6251","6258","In this letter, we review the question of which action space is best suited for controlling a real biped robot in combination with Sim2Real training. Position control has been popular as it has been shown to be more sample efficient and intuitive to combine with other planning algorithms. However, for position control, gain tuning is required to achieve the best possible policy performance. We show that, instead, using a torque-based action space enables task-and-robot agnostic learning with less parameter tuning and mitigates the sim-to-reality gap by taking advantage of torque control's inherent compliance. Also, we accelerate the torque-based-policy training process by pre-training the policy to remain upright by compensating for gravity. The letter showcases the first successful sim-to-real transfer of a torque-based deep reinforcement learning policy on a real human-sized biped robot.","2377-3766","","10.1109/LRA.2023.3304561","National Research Foundation of Korea; Korea Government(grant numbers:2021R1A2C3005914); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214627","Reinforcement learning;humanoid and bipedal locomotion;torque-based control","Robots;Legged locomotion;Task analysis;Aerospace electronics;Tuning;Torque;PD control","deep learning (artificial intelligence);humanoid robots;legged locomotion;position control;reinforcement learning;torque control","bipedal robots;human-sized biped robot;planning algorithms;position control;sim-to-real transfer;Sim2Real training;task-and-robot agnostic learning;torque control;torque-based deep reinforcement learning policy;torque-based-policy training process","","","","24","IEEE","11 Aug 2023","","","IEEE","IEEE Journals"
"DreamWaQ: Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning","I. M. Aswin Nahrendra; B. Yu; H. Myung","School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5078","5084","Quadrupedal robots resemble the physical ability of legged animals to walk through unstructured terrains. However, designing a controller for quadrupedal robots poses a significant challenge due to their functional complexity and requires adaptation to various terrains. Recently, deep reinforcement learning, inspired by how legged animals learn to walk from their experiences, has been utilized to synthesize natural quadrupedal locomotion. However, state-of-the-art methods strongly depend on a complex and reliable sensing framework. Furthermore, prior works that rely only on proprioception have shown a limited demonstration for overcoming challenging terrains, especially for a long distance. This work proposes a novel quadrupedal locomotion learning framework that allows quadrupedal robots to walk through challenging terrains, even with limited sensing modalities. The proposed framework was validated in real-world outdoor environments with varying conditions within a single run for a long distance.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161144","","Legged locomotion;Deep learning;Animals;Reinforcement learning;Stairs;Robot sensing systems;Robustness","deep learning (artificial intelligence);learning (artificial intelligence);legged locomotion;mobile robots;reinforcement learning","challenging terrains;complex sensing framework;deep reinforcement learning;functional complexity;implicit terrain imagination;learning robust quadrupedal locomotion;legged animals;natural quadrupedal locomotion;novel quadrupedal;physical ability;quadrupedal robots;reliable sensing framework;unstructured terrains","","","","39","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Learning Risk-Aware Costmaps via Inverse Reinforcement Learning for Off-Road Navigation","S. Triest; M. G. Castro; P. Maheshwari; M. Sivaprakasam; W. Wang; S. Scherer","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Mathematics, Indian Institute of Technology Kharagpur; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","924","930","The process of designing costmaps for off-road driving tasks is often a challenging and engineering-intensive task. Recent work in costmap design for off-road driving focuses on training deep neural networks to predict costmaps from sensory observations using corpora of expert driving data. However, such approaches are generally subject to over-confident mis-predictions and are rarely evaluated in-the-loop on physical hardware. We present an inverse reinforcement learning-based method of efficiently training deep cost functions that are uncertainty-aware. We do so by leveraging recent advances in highly parallel model-predictive control and robotic risk estimation. In addition to demonstrating improvement at reproducing expert trajectories, we also evaluate the efficacy of these methods in challenging off-road navigation scenarios. We observe that our method significantly outperforms a geometric baseline, resulting in 44% improvement in expert path reconstruction and 57% fewer interventions in practice. We also observe that varying the risk tolerance of the vehicle results in qualitatively different navigation behaviors, especially with respect to higher-risk scenarios such as slopes and tall grass.33More detailed algorithms and additional visualizations are provided in the appendix Appendix (appendix link: tinyurl.com/mtkj63e8)","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161268","ARL(grant numbers:W911NF1820218,W911NF20S0005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161268","","Training;Learning systems;Navigation;Neural networks;Reinforcement learning;Robot sensing systems;Hardware","control engineering computing;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;path planning;predictive control;reinforcement learning","challenging engineering-intensive task;costmap design;deep cost functions;deep neural networks;demonstrating improvement;expert driving data;expert path reconstruction;expert trajectories;higher-risk scenarios;highly parallel model-predictive control;inverse reinforcement learning-based method;learning risk-aware costmaps;leveraging recent advances;mis-predictions;off-road driving tasks;off-road navigation scenarios;physical hardware;qualitatively different navigation behaviors;risk tolerance;robotic risk estimation;sensory observations;uncertainty-aware","","","","45","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Investigation of Reinforcement Learning-based Attack on Logic Locking","J. Mellor; A. Shelton; S. Tehranipoor","Electrical and Computer Engineering Department, Santa Clara University, Santa Clara, California, USA; Electrical and Computer Engineering Department, Santa Clara University, Santa Clara, California, USA; Electrical and Computer Engineering Department, Santa Clara University, Santa Clara, California, USA","2021 IEEE International Symposium on Technologies for Homeland Security (HST)","30 Nov 2021","2021","","","1","7","Logic Locking is an emerging form of hardware obfuscation that is intended to be a solution to many of the trust issues associated with the modern globalized IC supply chain. By inserting extra key-gates into a circuit, the functionality of the circuit can be locked until the correct order of bits or “key” is applied to the key gates. To assess the strength of logic locking techniques, we propose a new attack that uses “deep reinforcement learning This research aims to test the robustness of logic locking as well as evaluate reinforcement learning as a possible attack against logic locking. We decided to use a Deep Q-Learning Neural Network to implement logic locking. If traditional Q-Learning were to be used, a massive Q-value table would be generated for all the possible states the key could be in, which is an exponential function of the key’s size. By creating other supporting modules to enable the reinforcement learning model to run simulations with any desired key and receive a reward back based on the correctness of these outputs when compared against the unlocked circuit, or an oracle, the model can be reinforced to learn. The unpredictable nature of the outputs of a logic locked circuit creates a weak correlation between the accuracy of any key the model tries, and the correctness of the outputs. This lack of correlation makes it difficult to properly reinforce the model, making it unable to converge on a correct key, even after over 100,000 timesteps of training.","","978-1-6654-4152-0","10.1109/HST53381.2021.9619800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619800","","Training;Correlation;Supply chains;Neural networks;Reinforcement learning;Benchmark testing;Hardware","deep learning (artificial intelligence);integrated circuit manufacture;production engineering computing;reinforcement learning;supply chain management","reinforcement learning-based attack;key gates;logic locking techniques;deep Q-learning neural network;globalized IC supply chain;Q-value table;exponential function","","","","14","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Learning Jumping Skills From Human with a Fast Reinforcement Learning Framework","Y. Kuang; S. Wang; B. Sun; J. Hao; H. Cheng","Center For Robotics, University of Electronic Science and Technology of China, Chengdu, China; Center For Robotics, University of Electronic Science and Technology of China, Chengdu, China; Center For Robotics, University of Electronic Science and Technology of China, Chengdu, China; Center For Robotics, University of Electronic Science and Technology of China, Chengdu, China; Center For Robotics, University of Electronic Science and Technology of China, Chengdu, China","2018 IEEE 8th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","11 Apr 2019","2018","","","510","515","Dynamic policies of locomotion control, which is one of the key problems for legged robots, remains great challenges, especially for newly arising bionic robots. In this paper, we propose a framework based on reinforcement learning to learn jumping skills from human demonstrations for a single legged robot. This approach adopts guided policy search (GPS)algorithms to apply trajectory optimization to direct strategy learning avoiding falling into the local optimum and uses differential dynamic programming to generate suitable bootstrap samples with the single legged hopping robot. The parameterized joint trajectories are learned from human demonstrations, and optimal control policies are established efficiently in an up to 8-dimensional parameter space, therefore the legged robot is controlled to jump as graceful as human beings do. The proposed algorithm is tested in our physical simulation based single leg platform which includes a completely rigid robot without any compliance mechanisms. Experiments demonstrate this approach works efficiently with relative small samples and the robot can learn to jump pretty well within only a few iterations.","2379-7711","978-1-5386-7057-6","10.1109/CYBER.2018.8688117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8688117","RL;GPS;robot;physical simulation","","control engineering computing;dynamic programming;learning (artificial intelligence);legged locomotion;motion control;optimal control;trajectory control","trajectory optimization;differential dynamic programming;single legged hopping robot;human demonstrations;optimal control policies;jumping skills;fast reinforcement learning framework;dynamic policies;locomotion control;bionic robots;guided policy search algorithms","","","","26","IEEE","11 Apr 2019","","","IEEE","IEEE Conferences"
"Learning to Break Rocks With Deep Reinforcement Learning","P. Samtani; F. Leiva; J. Ruiz-del-Solar","Department of Electrical Engineering and the Advanced Mining Technology Center, Universidad de Chile, Santiago, Chile; Department of Electrical Engineering and the Advanced Mining Technology Center, Universidad de Chile, Santiago, Chile; Department of Electrical Engineering and the Advanced Mining Technology Center, Universidad de Chile, Santiago, Chile","IEEE Robotics and Automation Letters","19 Jan 2023","2023","8","2","1077","1084","This work proposes a scheme for learning how to break rocks with an impact hammer. The problem is formulated as a Partially Observable Markov's Decision Process, and then solved through deep reinforcement learning. We propose a simple formulation, requiring only a basic sensorization of the hammer's manipulator, and involving just two discrete actions. We use Dueling Double Deep-Q Networks to parameterize the policy, and wield it with an auxiliary output. The proposed auxiliary task is also trained in simulation, and allows deciding when to stop the operation by detecting the absence of a rock from the observed joints' movement. The resulting policy is tested in a real world experimental environment, using a Bobcat E10 mini-excavator, and various rock types. The results show that a good performance can be obtained in a safe, and robust manner. A video showing some of the obtained results is available in https://youtu.be/tMqDJjK6zPo.","2377-3766","","10.1109/LRA.2023.3236562","FONDECYT Project(grant numbers:1201170); ANID-PIA Projects(grant numbers:AFB180004,AFB220002); FONDEF IDeA Project(grant numbers:ID19I10142); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015854","Reinforcement learning;mining robotics;machine learning for robot control","Rocks;End effectors;Task analysis;Ores;Hydraulic systems;Reinforcement learning;Excavation","deep learning (artificial intelligence);excavators;industrial manipulators;Markov processes;reinforcement learning","Bobcat E10 mini-excavator;break rocks;deep reinforcement learning;Dueling Double Deep-Q Networks;hammer manipulator;impact hammer;observed joints;Partially Observable Markov's Decision Process;rock types;sensorization","","","","11","IEEE","12 Jan 2023","","","IEEE","IEEE Journals"
"Learning to Navigate in Turbulent Flows With Aerial Robot Swarms: A Cooperative Deep Reinforcement Learning Approach","D. Patiño; S. Mayya; J. Calderon; K. Daniilidis; D. Saldaña","GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; Amazon Robotics, North Reading, Cambridge, MA, USA; Universidad Santo Tomas, Bogotá, Colombia; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; Autonomous and Intelligent Robotics Laboratory (AIRLab), Lehigh University, Bethlehem, PA, USA","IEEE Robotics and Automation Letters","8 Jun 2023","2023","8","7","4219","4226","Aerial operation in turbulent environments is a challenging problem due to the chaotic behavior of the flow. This problem is made even more complex when a team of aerial robots is trying to achieve coordinated motion in turbulent wind conditions. In this letter, we present a novel multi-robot controller to navigate in turbulent flows, decoupling the trajectory-tracking control from the turbulence compensation via a nested control architecture. Unlike previous works, our method does not learn to compensate for the air-flow at a specific time and space. Instead, our method learns to compensate for the flow based on its effect on the team. This is made possible via a deep reinforcement learning approach, implemented via a Graph Convolutional Neural Network (GCNN)-based architecture, which enables robots to achieve better wind compensation by processing the spatial-temporal correlation of wind flows across the team. Our approach scales well to large robot teams —as each robot only uses information from its nearest neighbors—, and generalizes well to robot teams larger than seen in training. Simulated experiments demonstrate how information sharing improves turbulence compensation in a team of aerial robots and demonstrate the flexibility of our method over different team configurations.","2377-3766","","10.1109/LRA.2023.3280806","ARO MURI(grant numbers:W911NF-20-1-0080); ONR(grant numbers:N00014-17-1-2093); ONR(grant numbers:N00014-22-1-2677); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10137747","Swarm robotics;reinforcement learning;wind turbulence;machine learning for robot control;graph neural networks","Robots;Robot kinematics;Robot sensing systems;Wind;Navigation;Force;Drag","aerospace robotics;autonomous aerial vehicles;control engineering computing;convolutional neural nets;deep learning (artificial intelligence);graph theory;learning (artificial intelligence);mobile robots;multi-robot systems;reinforcement learning;trajectory control","aerial operation;aerial robot swarms;aerial robots;air-flow;approach scales;chaotic behavior;deep reinforcement learning approach;different team configurations;Graph Convolutional Neural Network-based architecture;nested control architecture;novel multirobot controller;robot teams;trajectory-tracking control;turbulence compensation;turbulent environments;turbulent flows;turbulent wind conditions;wind compensation","","","","31","IEEE","29 May 2023","","","IEEE","IEEE Journals"
"Learning Collision-freed Trajectory of welding manipulator based on Safe Reinforcement Learning","Y. Xu; T. Wang; C. Chen; B. Hu","Guangdong Provincial Key Laboratory of Cyber-Physical System, Guangdong University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Cyber-Physical System, Guangdong University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Cyber-Physical System, Guangdong University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Cyber-Physical System, Guangdong University of Technology, Guangzhou, China","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","836","841","To obtain a reliable collision-free path, relevant constraints can be added to the robot. In this paper, safety reinforcement learning with kinematic constraints and torque-limited is studied to ensure safety in planning, with the designing of reinforcement learning action space to ensure the feasibility of action. For evaluation, path planning was carried out in an industrial welding scenario to allow the robot to reach the welding point in the narrow space. The experimental results show that the proposed method not only ensures convergence but also ensures the safety and reliability of the task.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926592","Natural Science Foundation of Guangdong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926592","","Service robots;Trajectory planning;Welding;Reinforcement learning;Kinematics;Safety;Planning","collision avoidance;industrial manipulators;learning (artificial intelligence);manipulators;mobile robots;path planning;robotic welding;safety","action space;path planning;industrial welding scenario;welding point;narrow space;collision-freed trajectory;manipulator;safe reinforcement learning;reliable collision-free path;relevant constraints;safety reinforcement;kinematic constraints;torque","","","","31","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"Learn to climb: teaching a reinforcement learning agent the single rope ascending technique","B. Varga","Department of Control for Transportation and Vehicle Systems, Faculty of Transportation Engineering and Vehicle Engineering, Budapest University of Technology and Economics, Budapest, Hungary","2022 IEEE 22nd International Symposium on Computational Intelligence and Informatics and 8th IEEE International Conference on Recent Achievements in Mechatronics, Automation, Computer Science and Robotics (CINTI-MACRo)","3 Feb 2023","2022","","","000209","000214","Single rope ascending technique is used in industrial alpinism, forestry, or various leisure activities. This paper presents a multi-body model of this technique involving an actuated 3D model of a humanoid, the climbing gear, and the rope, modeled as a finite-element object. This model serves as a training ground for reinforcement learning agents trying to mimic humans in rope climbing. To demonstrate the environment, an agent with a state-of-the-art reinforcement learning algorithm (Soft Actor-Critic) was trained. Results suggest that the agent can learn how to ascend the rope with speed comparable to real humans. However, the learned technique is not human-like: the artificial agent uses its arms excessively to climb, which would be too tiring for a human. That is because the environment only rewards ascension and does not penalize the energy used. The presented learning environment is developed for humanoid robots in mind that can perform complex tasks while on the rope and can carry much heavier payloads compared to climbing robots in the literature.","2471-9269","979-8-3503-9882-3","10.1109/CINTI-MACRo57952.2022.10029600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10029600","multi-body simulation;reinforcement learning;rope climbing","Training;Solid modeling;Three-dimensional displays;Mechatronics;Gears;Humanoid robots;Reinforcement learning","control engineering computing;finite element analysis;humanoid robots;reinforcement learning;ropes","artificial agent;climbing gear;climbing robots;finite element object;humanoid robots;industrial alpinism;learned technique;leisure activities;multibody model;presented learning environment;reinforcement learning agents;rope climbing;single rope ascending technique;soft actor critic;state-of-the-art reinforcement learning algorithm;training ground","","","","26","IEEE","3 Feb 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based UAV for Indoor Navigation and Exploration in Unknown Environments","A. Seel; F. Kreutzjans; B. Küster; M. Stonis; L. Overmeyer","IPH – Institut für Integrierte Produktion Hannover gGmbH University of Hanover, Hanover, Germany; IPH – Institut für Integrierte Produktion Hannover gGmbH University of Hanover, Hanover, Germany; IPH – Institut für Integrierte Produktion Hannover gGmbH University of Hanover, Hanover, Germany; IPH – Institut für Integrierte Produktion Hannover gGmbH University of Hanover, Hanover, Germany; IPH – Institut für Integrierte Produktion Hannover gGmbH University of Hanover, Hanover, Germany","2022 8th International Conference on Control, Automation and Robotics (ICCAR)","31 May 2022","2022","","","388","393","Factory planning can increase the productivity of manufacturing significantly, though the process is expensive when it comes to cost and time. In this paper, we propose an Unmanned Aerial Vehicle (UAV) framework that accelerates this process and decreases the costs. The framework consists of a UAV that is equipped with an IMU, a camera and a LiDAR sensor in order to navigate and explore unknown indoor environments. Thus, it is independent of GNSS and solely uses on-board sensors. The acquired data should enable a DRL agent to perform autonomous decision making, applying a reinforcement learning approach. We propose a simulation of this framework including several training and testing environments, that should be used for developing a DRL agent.","2251-2454","978-1-6654-8116-8","10.1109/ICCAR55106.2022.9782602","German Federation of Industrial Research Associations (AiF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782602","UAV;Deep Reinforcement Learning;factory planning;autonomous exploration;autonomous navigation;GNSS-denied environment","Training;Laser radar;Costs;Reinforcement learning;Autonomous aerial vehicles;Robot sensing systems;Production facilities","autonomous aerial vehicles;decision making;learning (artificial intelligence);mobile robots;navigation;path planning;remotely operated vehicles","autonomous decision making;reinforcement learning approach;testing environments;DRL agent;deep reinforcement;UAV;indoor navigation;factory planning;productivity;Unmanned Aerial Vehicle framework;LiDAR sensor;unknown indoor environments;on-board sensors","","1","","35","IEEE","31 May 2022","","","IEEE","IEEE Conferences"
"Learning a Low-Dimensional Representation of a Safe Region for Safe Reinforcement Learning on Dynamical Systems","Z. Zhou; O. S. Oguz; M. Leibold; M. Buss","Chair of Automatic Control Engineering, Technical University of Munich, Munich, Germany; Max Planck Institute for Intelligent Systems, University of Stuttgart, Stuttgart, Germany; Chair of Automatic Control Engineering, Technical University of Munich, Munich, Germany; Chair of Automatic Control Engineering, Technical University of Munich, Munich, Germany","IEEE Transactions on Neural Networks and Learning Systems","2 May 2023","2023","34","5","2513","2527","For the safe application of reinforcement learning algorithms to high-dimensional nonlinear dynamical systems, a simplified system model is used to formulate a safe reinforcement learning (SRL) framework. Based on the simplified system model, a low-dimensional representation of the safe region is identified and used to provide safety estimates for learning algorithms. However, finding a satisfying simplified system model for complex dynamical systems usually requires a considerable amount of effort. To overcome this limitation, we propose a general data-driven approach that is able to efficiently learn a low-dimensional representation of the safe region. By employing an online adaptation method, the low-dimensional representation is updated using the feedback data to obtain more accurate safety estimates. The performance of the proposed approach for identifying the low-dimensional representation of the safe region is illustrated using the example of a quadcopter. The results demonstrate a more reliable and representative low-dimensional representation of the safe region compared with previous works, which extends the applicability of the SRL framework.","2162-2388","","10.1109/TNNLS.2021.3106818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9528905","Data-driven model order reduction;deep learning in robotics and automation;learning and adaptive systems;safe reinforcement learning (SRL)","Safety;Computational modeling;Heuristic algorithms;Reinforcement learning;Control systems;Probabilistic logic;Nonlinear dynamical systems","helicopters;nonlinear dynamical systems;optimal control;reinforcement learning","complex dynamical systems;data-driven approach;feedback data;high-dimensional nonlinear dynamical systems;low-dimensional representation;online adaptation method;optimal control;quadcopter;safe region;safe reinforcement learning framework;safety estimates;simplified system model;SRL framework","","2","","36","CCBY","3 Sep 2021","","","IEEE","IEEE Journals"
"Deep reinforcement learning for the control of a desiccant wheel-based air-conditioning system","J. Liu; C. Jiang; X. Li; W. Zhang","School of Electrical Engineering and Automation, Hefei University of Technology, Hefei; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei; Guangdong Deer Smart Factory Technology Co., Ltd.","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","7130","7135","Moisture removal is an important concern in many industries, e.g. producing food and pharmacy, and manufacturing lithium-ion batteries. However, humidity control has been paid less attention in research compared to temperature control. In this paper, the control of a desiccant wheel-based air conditioning (AC) system under the influence of changing climate is investigated. A one-dimensional gas-side resistance model is employed to mimic a silica gel desiccant wheel, and one of the state-of-the-art policy gradient algorithms, proximal policy optimization (PPO), is utilized to train a feedback controller for this AC system. Compared with model predictive control, the advantage of this strategy is that the policy network obtained by PPO can be used without involving online optimization. Numerical simulations based on real environmental data show that the controller is effective to keep indoor humidity and temperature to their set points. Compared with deep deterministic policy gradient algorithm, the obtained controller has higher reward and less regulation variance.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9901521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9901521","Desiccant wheel;Humidity and temperature control;Deep reinforcement learning;Proximal policy optimization","Silicon compounds;Resistance;Temperature distribution;Wheels;Humidity;Prediction algorithms;Numerical simulation","air conditioning;gradient methods;humidity;humidity control;learning (artificial intelligence);moisture;optimisation;predictive control;secondary cells;temperature control;wheels","AC system;deep deterministic policy gradient algorithm;deep reinforcement learning;desiccant wheel-based air conditioning system;desiccant wheel-based air-conditioning system;feedback controller;humidity control;indoor humidity;lithium-ion batteries;model predictive control;moisture removal;one-dimensional gas-side resistance model;pharmacy;policy network;PPO;proximal policy optimization;silica gel desiccant wheel;state-of-the-art policy gradient algorithms;temperature control","","","","19","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Adaptive Deep Reinforcement Learning for Dynamic Parallel Machines Scheduling with Maintenance Activities","M. Wang; P. Zhang; J. Zhang; G. Zhang; J. Bi; M. Jin","College of Mechanical Engineering, Donghua University, Shanghai, China; Institute of Artificial Intelligence Donghua University, Shanghai, China; Institute of Artificial Intelligence Donghua University, Shanghai, China; College of Mechanical Engineering, Donghua University, Shanghai, China; College of Mechanical Engineering, Donghua University, Shanghai, China; College of Information Science and Technology, Donghua University, Shanghai, China","2023 28th International Conference on Automation and Computing (ICAC)","16 Oct 2023","2023","","","1","6","In this paper, the parallel machines scheduling with jobs arrive over time and maintenance activities is investigated. The difficulty of the problem is how to achieve the trade-off between the scheduling and maintenance on parallel machines to optimize the cost and reliability of machines under the dynamic environment. For this challenge, this paper proposes an adaptive deep reinforcement learning method to achieve rapid response and optimization. Under the framework, a deep reinforcement learning is applied to learn the optimal scheduling and maintenance policy. However, it is difficult to adapt to dynamic environment by observing static state features. Therefore, a feature selection method based on deep reinforcement learning was raised to adaptively select the key state features of the production process so as to improve the adaptive ability and generalization of the proposed method. The experimental results of the instances with different scales indicate the proposed method can select state features adaptively and effectively solve the dynamical scheduling on parallel machines with maintenance activities.","","979-8-3503-3585-9","10.1109/ICAC57885.2023.10275156","National Natural Science Foundation of China(grant numbers:52005099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275156","parallel machines;jobs arrive over time;maintenance;adaptive deep reinforcement learning;state feature selection","Deep learning;Heuristic algorithms;Reinforcement learning;Optimal scheduling;Production;Maintenance engineering;Parallel machines","","","","","","27","IEEE","16 Oct 2023","","","IEEE","IEEE Conferences"
"Deep Direct Reinforcement Learning for Financial Signal Representation and Trading","Y. Deng; F. Bao; Y. Kong; Z. Ren; Q. Dai","Automation Department, Tsinghua University, Beijing, China; Automation Department, Tsinghua University, Beijing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; Automation Department, Tsinghua University, Beijing, China; Automation Department, Tsinghua University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","20 May 2017","2017","28","3","653","664","Can we train the computer to beat experienced traders for financial assert trading? In this paper, we try to address this challenge by introducing a recurrent deep neural network (NN) for real-time financial signal representation and trading. Our model is inspired by two biological-related learning concepts of deep learning (DL) and reinforcement learning (RL). In the framework, the DL part automatically senses the dynamic market condition for informative feature learning. Then, the RL module interacts with deep representations and makes trading decisions to accumulate the ultimate rewards in an unknown environment. The learning system is implemented in a complex NN that exhibits both the deep and recurrent structures. Hence, we propose a task-aware backpropagation through time method to cope with the gradient vanishing issue in deep training. The robustness of the neural system is verified on both the stock and the commodity future markets under broad testing conditions.","2162-2388","","10.1109/TNNLS.2016.2522401","Project of the National Natural Science Foundation of China(grant numbers:61327902,61120106003); National Science Foundation of Jiangsu Province, China(grant numbers:BK20150650); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407387","Deep learning (DL);financial signal processing;neural network (NN) for finance;reinforcement learning (RL)","Training;Robustness;Learning (artificial intelligence);Artificial neural networks;Feature extraction;Optimization;Signal representation","backpropagation;decision making;feedforward neural nets;financial data processing;recurrent neural nets;stock markets","deep direct reinforcement learning;financial assert trading;recurrent deep neural network;real-time financial signal representation;biological-related learning concepts;dynamic market condition;informative feature learning;deep representations;trading decision making;complex NN;task-aware backpropagation;deep training;neural system robustness","","377","","44","IEEE","15 Feb 2016","","","IEEE","IEEE Journals"
"Reinforcement-Learning-Based Robust Controller Design for Continuous-Time Uncertain Nonlinear Systems Subject to Input Constraints","D. Liu; X. Yang; D. Wang; Q. Wei","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Cybernetics","20 May 2017","2015","45","7","1372","1385","The design of stabilizing controller for uncertain nonlinear systems with control constraints is a challenging problem. The constrained-input coupled with the inability to identify accurately the uncertainties motivates the design of stabilizing controller based on reinforcement-learning (RL) methods. In this paper, a novel RL-based robust adaptive control algorithm is developed for a class of continuous-time uncertain nonlinear systems subject to input constraints. The robust control problem is converted to the constrained optimal control problem with appropriately selecting value functions for the nominal system. Distinct from typical action-critic dual networks employed in RL, only one critic neural network (NN) is constructed to derive the approximate optimal control. Meanwhile, unlike initial stabilizing control often indispensable in RL, there is no special requirement imposed on the initial control. By utilizing Lyapunov's direct method, the closed-loop optimal control system and the estimated weights of the critic NN are proved to be uniformly ultimately bounded. In addition, the derived approximate optimal control is verified to guarantee the uncertain nonlinear system to be stable in the sense of uniform ultimate boundedness. Two simulation examples are provided to illustrate the effectiveness and applicability of the present approach.","2168-2275","","10.1109/TCYB.2015.2417170","National Natural Science Foundation of China(grant numbers:61034002,61233001,61273140,61304086,61374105); Beijing Natural Science Foundation(grant numbers:4132078); Early Career Development Award of the State Key Laboratory of Management and Control for Complex Systems; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7083712","Approximate dynamic programming (ADP);neural networks (NNs);neuro-dynamic programming;nonlinear systems;optimal control;reinforcement learning (RL);robust control;Approximate dynamic programming (ADP);neural networks (NNs);neuro-dynamic programming;nonlinear systems;optimal control;reinforcement learning (RL);robust control","Optimal control;Nonlinear systems;Robust control;Artificial neural networks;Algorithm design and analysis;Approximation algorithms;Robustness","adaptive control;closed loop systems;continuous time systems;control system synthesis;learning (artificial intelligence);Lyapunov methods;neurocontrollers;nonlinear systems;optimal control;robust control;uncertain systems","reinforcement-learning-based robust controller design;continuous-time uncertain nonlinear systems;input constraints;stabilizing controller design;control constraints;RL-based robust adaptive control algorithm;constrained optimal control problem;value function;nominal system;action-critic dual networks;critic neural network;NN;Lyapunov direct method;closed-loop optimal control system;uniform ultimate boundedness","","248","","56","IEEE","9 Apr 2015","","","IEEE","IEEE Journals"
"Kernel-Based Least Squares Policy Iteration for Reinforcement Learning","X. Xu; D. Hu; X. Lu","Institute of Automation, College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; Department of Automation Control, College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; School of Computer, National University of Defense Technology, Changsha, China","IEEE Transactions on Neural Networks","9 Jul 2007","2007","18","4","973","992","In this paper, we present a kernel-based least squares policy iteration (KLSPI) algorithm for reinforcement learning (RL) in large or continuous state spaces, which can be used to realize adaptive feedback control of uncertain dynamic systems. By using KLSPI, near-optimal control policies can be obtained without much a priori knowledge on dynamic models of control plants. In KLSPI, Mercer kernels are used in the policy evaluation of a policy iteration process, where a new kernel-based least squares temporal-difference algorithm called KLSTD-Q is proposed for efficient policy evaluation. To keep the sparsity and improve the generalization ability of KLSTD-Q solutions, a kernel sparsification procedure based on approximate linear dependency (ALD) is performed. Compared to the previous works on approximate RL methods, KLSPI makes two progresses to eliminate the main difficulties of existing results. One is the better convergence and (near) optimality guarantee by using the KLSTD-Q algorithm for policy evaluation with high precision. The other is the automatic feature selection using the ALD-based kernel sparsification. Therefore, the KLSPI algorithm provides a general RL method with generalization performance and convergence guarantee for large-scale Markov decision problems (MDPs). Experimental results on a typical RL task for a stochastic chain problem demonstrate that KLSPI can consistently achieve better learning efficiency and policy quality than the previous least squares policy iteration (LSPI) algorithm. Furthermore, the KLSPI method was also evaluated on two nonlinear feedback control problems, including a ship heading control problem and the swing up control of a double-link underactuated pendulum called acrobot. Simulation results illustrate that the proposed method can optimize controller performance using little a priori information of uncertain dynamic systems. It is also demonstrated that KLSPI can be applied to online learning control by incorporating an initial controller to ensure online performance.","1941-0093","","10.1109/TNN.2007.899161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4267723","Approximate dynamic programming;kernel methods;least squares;Markov decision problems (MDPs);reinforcement learning (RL)","Least squares methods;Learning;Kernel;Feedback control;Least squares approximation;State-space methods;Programmable control;Adaptive control;Linear approximation;Convergence","adaptive control;feedback;iterative methods;learning (artificial intelligence);least squares approximations;Markov processes;nonlinear control systems;optimal control;state-space methods;uncertain systems","kernel-based least squares policy iteration;reinforcement learning;continuous state spaces;adaptive feedback control;uncertain dynamic systems;near-optimal control policy;control plants;Mercer kernels;policy evaluation;kernel-based least squares temporal-difference algorithm;generalization ability;kernel sparsification procedure;approximate linear dependency;large-scale Markov decision problems;stochastic chain problem;least squares policy iteration algorithm;nonlinear feedback control;ship heading control problem;swing up control;double-link underactuated pendulum;acrobot;online learning control","Algorithms;Artificial Intelligence;Biomimetics;Computer Simulation;Decision Support Techniques;Feedback;Least-Squares Analysis;Markov Chains;Models, Theoretical;Reinforcement (Psychology)","192","","47","IEEE","9 Jul 2007","","","IEEE","IEEE Journals"
"UAV-Enabled Secure Communications by Multi-Agent Deep Reinforcement Learning","Y. Zhang; Z. Mou; F. Gao; J. Jiang; R. Ding; Z. Han","Department of Automation, State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Institute for Artificial Intelligence, Tsinghua University, Beijing, China; Department of Automation, State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Institute for Artificial Intelligence, Tsinghua University, Beijing, China; Department of Automation, State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Institute for Artificial Intelligence, Tsinghua University, Beijing, China; Shaanxi Key Laboratory of Information Communication Network and Security, Xi’an University of Posts and Telecommunications, Xi’an, China; Department of Automation, State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Institute for Artificial Intelligence, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA","IEEE Transactions on Vehicular Technology","26 Oct 2020","2020","69","10","11599","11611","Unmanned aerial vehicles (UAVs) can be employed as aerial base stations to support communication for the ground users (GUs). However, the aerial-to-ground (A2G) channel link is dominated by line-of-sight (LoS) due to the high flying altitude, which is easily wiretapped by the ground eavesdroppers (GEs). In this case, a single UAV has limited maneuvering capacity to obtain the desired secure rate in the presence of multiple eavesdroppers. In this paper, we propose a cooperative jamming approach by letting UAV jammers help the UAV transmitter defend against GEs. To be specific, the UAV transmitter sends the confidential information to GUs, and the UAV jammers send the artificial noise signals to the GEs by 3D beamforming. We propose a multi-agent deep reinforcement learning (MADRL) approach, i.e., multi-agent deep deterministic policy gradient (MADDPG) to maximize the secure capacity by jointly optimizing the trajectory of UAVs, the transmit power from UAV transmitter and the jamming power from the UAV jammers. The MADDPG algorithm adopts centralized training and distributed execution. The simulation results show the MADRL method can realize the joint trajectory design of UAVs and achieve good performance. To improve the learning efficiency and convergence, we further propose a continuous action attention MADDPG (CAA-MADDPG) method, where the agent learns to pay attention to the actions and observations of other agents that are more relevant with it. From the simulation results, the rewards performance of CAA-MADDPG is better than the MADDPG without attention.","1939-9359","","10.1109/TVT.2020.3014788","National Key Research and Development Program of China(grant numbers:2018AAA0102401); National Natural Science Foundation of China(grant numbers:61831013,61771274,61531011,61871321); Natural Science Foundation of Beijing Municipality(grant numbers:4182030,L182042); National Science Foundation(grant numbers:EARS-1839818,CNS1717454,CNS-1731424,CNS-1702850); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9161257","UAV;multi-agent deep reinforcement learning;trajectory design;policy gradient;physical layer security","Jamming;Trajectory;Training;Radio transmitters;Machine learning;Three-dimensional displays","aerospace communication;array signal processing;autonomous aerial vehicles;cooperative communication;jamming;learning (artificial intelligence);optimisation;telecommunication computing;telecommunication security","GU;multiagent deep deterministic policy gradient;multiagent deep reinforcement learning approach;desired secure rate;GEs;ground eavesdroppers;high flying altitude;aerial-to-ground channel link;ground users;aerial base stations;unmanned aerial vehicles;UAV-enabled secure communications;CAA-MADDPG;continuous action attention MADDPG method;UAV jammers;UAV transmitter;secure capacity","","99","","39","IEEE","6 Aug 2020","","","IEEE","IEEE Journals"
"Depth Control of Model-Free AUVs via Reinforcement Learning","H. Wu; S. Song; K. You; C. Wu","Department of Automation and the TNList, Tsinghua University, Beijing, China; Department of Automation and the TNList, Tsinghua University, Beijing, China; Department of Automation and the TNList, Tsinghua University, Beijing, China; Department of Automation and the TNList, Tsinghua University, Beijing, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","19 Nov 2019","2019","49","12","2499","2510","In this paper, we consider depth control problems of an autonomous underwater vehicle (AUV) for tracking the desired depth trajectories. Due to the unknown dynamical model and the coupling between surge and yaw motions of the AUV, the problems cannot be effectively solved by most of the model-based or proportional-integral-derivative like controllers. To this purpose, we formulate the depth control problems of the AUV as continuous-state, continuous-action Markov decision processes under unknown transition probabilities. Based on the deterministic policy gradient theorem and neural network approximation, we propose a model-free reinforcement learning (RL) algorithm that learns a state-feedback controller from sampled trajectories of the AUV. To improve the performance of the RL algorithm, we further propose a batch-learning scheme through replaying previous prioritized trajectories. We illustrate with simulations that our model-free method is even comparable to the model-based controllers. Moreover, we validate the effectiveness of the proposed RL algorithm on a seafloor data set sampled from the South China Sea.","2168-2232","","10.1109/TSMC.2017.2785794","National Natural Science Foundation of China(grant numbers:41427806,41576101); National Basic Research Program of China (973 Program)(grant numbers:2016YFC0300801); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8263166","Autonomous underwater vehicle (AUV);depth control;deterministic policy gradient (DPG);neural network;prioritized experience replay;reinforcement learning (RL)","Trajectory;Cost function;Zirconium;Markov processes;Neural networks;Underwater vehicles;Learning (artificial intelligence)","autonomous underwater vehicles;decision theory;feedback;function approximation;gradient methods;learning (artificial intelligence);Markov processes;neurocontrollers;robot dynamics;trajectory control","AUV;desired depth trajectories;unknown dynamical model;yaw motions;proportional-integral-derivative;depth control problems;continuous-state;continuous-action Markov decision;unknown transition probabilities;deterministic policy gradient theorem;model-free reinforcement learning algorithm;state-feedback controller;RL algorithm;batch-learning scheme;model-free method;model-based controllers;model-free AUVs;autonomous underwater vehicle","","74","","29","IEEE","18 Jan 2018","","","IEEE","IEEE Journals"
"Parameterized Batch Reinforcement Learning for Longitudinal Control of Autonomous Land Vehicles","Z. Huang; X. Xu; H. He; J. Tan; Z. Sun","College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; Department of Electrical, Computer, and Biomedical Engineering, University of Rhode Island, Kingston, RI, USA; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","15 Mar 2019","2019","49","4","730","741","This paper presents a parameterized batch reinforcement learning algorithm for near-optimal longitudinal control of autonomous land vehicles (ALVs). The proposed approach uses an actor-critic architecture, where parameterized feature vectors based on kernels are learned from collected samples for approximating the value functions and policies. One difference between the parameterized batch actor-critic (PBAC) algorithm and previous actor-critic learning approaches is that the critic and actor in PBAC share the same linear features, which has been theoretically proved to be a beneficial property for the convergence of actor-critic learning approaches. In order to obtain better learning efficiency, least-squares-based batch updating rules are designed for the critic and actor, respectively. Based on the PBAC learning algorithm, a data-driven longitudinal control method is presented for ALVs to obtain near-optimal control policies which adaptively tune the fuel/brake control signals to track different speeds. A multiobjective reward function is designed so that both tracking precision and driving smoothness are considered. Extensive experiments were conducted on a real ALV platform while driving on flat, slippery, sloping, and bumpy roads. The experimental results illustrate the superiority of the PBAC-based self-learning controller over conventional longitudinal control methods such as proportional-integral (PI) control and learning-based PI control.","2168-2232","","10.1109/TSMC.2017.2712561","National Natural Science Foundation of China(grant numbers:U1564214,61611540348); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8002677","Autonomous land vehicles (ALVs);feature learning;longitudinal control;parameterized batch actor-critic (PBAC);reinforcement learning (RL)","Algorithm design and analysis;Approximation algorithms;Heuristic algorithms;Land vehicles;Learning (artificial intelligence);Control systems;Autonomous vehicles","brakes;learning systems;mobile robots;optimal control;road vehicles","parameterized batch reinforcement learning;autonomous land vehicles;near-optimal longitudinal control;ALVs;actor-critic architecture;parameterized feature vectors;parameterized batch actor-critic algorithm;PBAC share;least-squares-based batch updating rules;PBAC learning algorithm;data-driven longitudinal control method;near-optimal control policies;fuel/brake control signals;PBAC-based self-learning;actor-critic learning approaches","","65","","44","IEEE","4 Aug 2017","","","IEEE","IEEE Journals"
"FMRQ—A Multiagent Reinforcement Learning Algorithm for Fully Cooperative Tasks","Z. Zhang; D. Zhao; J. Gao; D. Wang; Y. Dai","College of Automation Engineering, Qingdao University, Qingdao, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; College of Automation Engineering, Qingdao University, Qingdao, China; College of Automation Engineering, Qingdao University, Qingdao, China; China Academy of Railway Sciences, Transportation and Economics Institute, Beijing, China","IEEE Transactions on Cybernetics","20 May 2017","2017","47","6","1367","1379","In this paper, we propose a multiagent reinforcement learning algorithm dealing with fully cooperative tasks. The algorithm is called frequency of the maximum reward Q-learning (FMRQ). FMRQ aims to achieve one of the optimal Nash equilibria so as to optimize the performance index in multiagent systems. The frequency of obtaining the highest global immediate reward instead of immediate reward is used as the reinforcement signal. With FMRQ each agent does not need the observation of the other agents' actions and only shares its state and reward at each step. We validate FMRQ through case studies of repeated games: four cases of two-player two-action and one case of three-player two-action. It is demonstrated that FMRQ can converge to one of the optimal Nash equilibria in these cases. Moreover, comparison experiments on tasks with multiple states and finite steps are conducted. One is box-pushing and the other one is distributed sensor network problem. Experimental results show that the proposed algorithm outperforms others with higher performance.","2168-2275","","10.1109/TCYB.2016.2544866","National Natural Science Foundation of China(grant numbers:61273136,61573353,61533017,61573205); Foundation of Shandong Province(grant numbers:ZR2015FM015,ZR2015FM017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7452571","Multiagent reinforcement learning (MARL);Nash equilibrium;Q-learning;repeated game","Games;Nash equilibrium;Algorithm design and analysis;Stability criteria;Stochastic processes;Learning (artificial intelligence);Heuristic algorithms","game theory;learning (artificial intelligence);multi-agent systems","multiagent reinforcement learning algorithm;fully cooperative tasks;frequency of the maximum reward Q-learning;FMRQ;optimal Nash equilibria;global immediate reward;reinforcement signal;two-player two-action game;three-player two-action game;box-pushing;distributed sensor network problem","","55","","42","IEEE","14 Apr 2016","","","IEEE","IEEE Journals"
"Safe Intermittent Reinforcement Learning With Static and Dynamic Event Generators","Y. Yang; K. G. Vamvoudakis; H. Modares; Y. Yin; D. C. Wunsch","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Daniel Guggenheim School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Mechanical Engineering, Michigan State University, East Lansing, MI, USA; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Department of Electrical and Computer Engineering, Missouri University of Science and Technology, Rolla, MO, USA","IEEE Transactions on Neural Networks and Learning Systems","30 Nov 2020","2020","31","12","5441","5455","In this article, we present an intermittent framework for safe reinforcement learning (RL) algorithms. First, we develop a barrier function-based system transformation to impose state constraints while converting the original problem to an unconstrained optimization problem. Second, based on optimal derived policies, two types of intermittent feedback RL algorithms are presented, namely, a static and a dynamic one. We finally leverage an actor/critic structure to solve the problem online while guaranteeing optimality, stability, and safety. Simulation results show the efficacy of the proposed approach.","2162-2388","","10.1109/TNNLS.2020.2967871","National Natural Science Foundation of China(grant numbers:61903028); China Post-Doctoral Science Foundation(grant numbers:2018M641197); Fundamental Research Funds for the Central Universities(grant numbers:FRF-TP-18-031A1,FRF-BD-17-002A); National Science Foundation(grant numbers:S&AS-1849264,CPS-1851588); ONR Minverva(grant numbers:N00014-18-1-2874); DARPA/Microsystems Technology Office; Army Research Laboratory(grant numbers:W911NF-18-2-0260); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8989956","Actor/critic structures;asymptotic stability;barrier functions;reinforcement learning (RL);safety-critical systems","Reinforcement learning;Asymptotic stability;Safety;Mathematical model;Learning systems;Lyapunov methods","learning (artificial intelligence);optimisation","safe reinforcement learning;barrier function-based system transformation;state constraints;unconstrained optimization;optimal derived policies;intermittent feedback RL algorithms;safe intermittent reinforcement learning;dynamic event generators;static event generators;actor/critic structure","","52","","49","IEEE","10 Feb 2020","","","IEEE","IEEE Journals"
"Distributed Fault-Tolerant Containment Control Protocols for the Discrete-Time Multiagent Systems via Reinforcement Learning Method","T. Li; W. Bai; Q. Liu; Y. Long; C. L. P. Chen","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; Navigation College, Dalian Maritime University, Dalian, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Neural Networks and Learning Systems","4 Aug 2023","2023","34","8","3979","3991","This article investigates the model-free fault-tolerant containment control problem for multiagent systems (MASs) with time-varying actuator faults. Depending on the relative state information of neighbors, a distributed containment control method based on reinforcement learning (RL) is adopted to achieve containment control objective without prior knowledge on the system dynamics. First, based on the information of agent itself and its neighbors, a containment error system is established. Then, the optimal containment control problem is transformed into an optimal regulation problem for the containment error system. Furthermore, the RL-based policy iteration method is employed to deal with the corresponding optimal regulation problem, and the nominal controller is proposed for the original fault-free system. Based on the nominal controller, a fault-tolerant controller is further developed to compensate for the influence of actuator faults on MAS. Meanwhile, the uniform boundedness of the containment errors can be guaranteed by using the presented control scheme. Finally, numerical simulations are given to show the effectiveness and advantages of the proposed method.","2162-2388","","10.1109/TNNLS.2021.3121403","National Natural Science Foundation of China(grant numbers:51939001,61976033,61903092,61773187); Liaoning Revitalization Talents Program(grant numbers:XLYC1908018); Science and Technology Innovation Funds of Dalian(grant numbers:2018J11CY022); Natural Science Foundation of Liaoning Province(grant numbers:2019-ZD-0151); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9597477","Containment control;fault-tolerant control (FTC);multiagent systems (MASs);reinforcement learning (RL)","Actuators;Fault tolerant systems;Fault tolerance;Control systems;Protocols;Artificial neural networks;Reinforcement learning","actuators;control system synthesis;discrete time systems;distributed control;fault tolerance;fault tolerant control;iterative methods;multi-agent systems;nonlinear control systems;reinforcement learning;time-varying systems","containment control objective;containment error system;containment errors;corresponding optimal regulation problem;discrete-time multiagent systems;distributed containment control method;distributed fault-tolerant containment control protocols;fault-tolerant controller;model-free fault-tolerant containment control problem;nominal controller;optimal containment control problem;original fault-free system;presented control scheme;reinforcement learning method;relative state information;RL-based policy iteration method;system dynamics;time-varying actuator faults","","45","","41","IEEE","1 Nov 2021","","","IEEE","IEEE Journals"
"Event-Triggered Multigradient Recursive Reinforcement Learning Tracking Control for Multiagent Systems","W. Bai; T. Li; Y. Long; C. L. P. Chen","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Neural Networks and Learning Systems","5 Jan 2023","2023","34","1","366","379","In this article, the tracking control problem of event-triggered multigradient recursive reinforcement learning is investigated for nonlinear multiagent systems (MASs). Attention is focused on the distributed reinforcement learning approach for MASs. The critic neural network (NN) is applied to estimate the long-term strategic utility function, and the actor NN is designed to approximate the uncertain dynamics in MASs. The multigradient recursive (MGR) strategy is tailored to learn the weight vector in NN, which eliminates the local optimal problem inherent in gradient descent method and decreases the dependence of initial value. Furthermore, reinforcement learning and event-triggered mechanism can improve the energy conservation of MASs by decreasing the amplitude of the controller signal and the controller update frequency, respectively. It is proved that all signals in MASs are semiglobal uniformly ultimately bounded (SGUUB) according to the Lyapunov theory. Simulation results are given to demonstrate the effectiveness of the proposed strategy.","2162-2388","","10.1109/TNNLS.2021.3094901","National Natural Science Foundation of China(grant numbers:51939001,61903092,61976033,61773187,61751202,U1813203); Liaoning Revitalization Talents Program(grant numbers:XLYC1908018); Dalian Science and Technology Innovation Fund(grant numbers:2018J11CY022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9488578","Event-triggered control;multiagent systems (MASs);multigradient recursive (MGR);reinforcement learning","Reinforcement learning;Artificial neural networks;Multi-agent systems;Graph theory;Stability criteria;Regulation;Performance analysis","control engineering computing;control system synthesis;gradient methods;Lyapunov methods;multi-agent systems;neural nets;nonlinear control systems;reinforcement learning;uncertain systems;vectors","actor NN;controller signal;controller update frequency;critic neural network;distributed reinforcement learning;event-triggered multigradient recursive reinforcement learning tracking control;long-term strategic utility function;Lyapunov theory;MAS;nonlinear multiagent systems;semiglobal uniformly ultimately bounded;SGUUB","","38","","48","IEEE","16 Jul 2021","","","IEEE","IEEE Journals"
"DMP-Based Motion Generation for a Walking Exoskeleton Robot Using Reinforcement Learning","Y. Yuan; Z. Li; T. Zhao; D. Gan","College of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Department of Automation, University of Science and Technology of China, Hefei, China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Industrial Electronics","4 Feb 2020","2020","67","5","3830","3839","For the purpose of the assistance for human walking, this paper describes a novel coupled movement sequences planning and motion adaption based on dynamic movement primitives (DMPs) for a walking exoskeleton robot. The developed exoskeleton robot has eight degrees of freedom (DOFs). The hip and knee of each artificial leg can provide two electric-powered DOFs to flexion or extension, two passive-installed DOFs of the ankle are to achieve the motion of inversion/eversion and plantarflexion/dorsiflexion, and two passive DOFs of the hip are to achieve the motion of roll or yaw. A novel trajectory-learning scheme based on reinforcement learning (RL) combined with DMPs is presented for a lower limb exoskeleton robot, aiming to give assistance to human walking. In the proposed strategy, a two-level planning is designed. In the first level, the inverted pendulum approximation under the consideration of the locomotion parameters is utilized to guarantee the zero-moment point within the ankle joint of the support leg in the phase of single support. In the second level, the joint trajectories are modeled and learned by DMPs. Meanwhile, the RL is adopted to learn the trajectories for eliminating the effects of uncertainties in joint space. The experiment involving four subjects based on a lower limb exoskeleton robot demonstrates that the proposed scheme can effectively suppress the disturbances and uncertainties.","1557-9948","","10.1109/TIE.2019.2916396","National Natural Science Foundation of China(grant numbers:61573147,61625303,61751310); National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2017YFB-1302302); Guangdong Science and Technology Research Collaborative Innovation(grant numbers:2014B090901056,2015B020214003,2016A020220003); Application Technology Research Foundation(grant numbers:2015B020233006); Anhui Science and Technology Major Program(grant numbers:17030901029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718029","Dynamic movement primitives (DMP);exoskeleton robot;locomotion trajectory;reinforcement learning (RL)","Exoskeletons;Legged locomotion;Trajectory;Hip;Knee;Robot sensing systems","control engineering computing;gait analysis;learning (artificial intelligence);legged locomotion;medical robotics;motion control;robot dynamics;robot programming;wearable robots","human walking;coupled movement sequences planning;dynamic movement primitives;walking exoskeleton robot;reinforcement learning;lower limb exoskeleton robot;DMP-based motion generation;trajectory-learning scheme;passive-installed DOF;electric-powered DOF","","35","","34","IEEE","17 May 2019","","","IEEE","IEEE Journals"
"From Discriminant to Complete: Reinforcement Searching-Agent Learning for Weakly Supervised Object Detection","D. Zhang; J. Han; L. Zhao; T. Zhao","School of Machine-Electronical Engineering, Xidian University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","30 Nov 2020","2020","31","12","5549","5560","Weakly supervised object detection (WSOD) is an interesting yet challenging task in the computer vision community. The core is to discover the image regions that contain the complete object instances under the image-level supervision. Existing works usually solve this problem via a proposal selection strategy, which selects the most discriminative box regions from the weakly labeled training images. However, these regions usually only contain the discriminative object parts rather than the complete object instances. To address this problem, this article proposes to learn a searching-agent to gradually mine desirable object regions under a region searching paradigm, where we formulate the searching process as a Markov decision process and learn the searching-agent under a deep reinforcement learning framework. To learn such a searching-agent under the weak supervision, we extract the pseudo-complete object regions and the corresponding local discriminative object parts and introduce the obtained pseudo-target-part training pairs into the reinforcement learning process of the search-agent. This learning strategy has twofold advantages: 1) it can mimic the searching process to reveal complete object regions from a certain discriminative part of the object under the weak supervision and 2) it will not suffer from the learning difficulty arise from the long-action sequence that happens when searching from the entire image range. Comprehensive experiments on benchmark data sets demonstrate that by integrating the learned searching-agent with the existing WSOD method, we can achieve better performance than the other state-of-the-art and baseline methods.","2162-2388","","10.1109/TNNLS.2020.2969483","National Key Research and Development Program of China(grant numbers:2017YFB1002201); National Science Foundation of China(grant numbers:61876140,61773301); China Postdoctoral Support Scheme for Innovative Talents(grant numbers:BX20180236); Northwestern Polytechnical University (NWPU) Funds for Interdisciplinary Subject; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9007022","Computer vision;machine learning;object detection","Training;Machine learning;Feature extraction;Object detection;Computer vision","computer vision;image segmentation;learning (artificial intelligence);Markov processes;object detection;search problems","searching process;reinforcement searching-agent learning;weakly supervised object detection;image-level supervision;proposal selection strategy;discriminative box regions;region searching;pseudotarget-part training pairs;search-agent;learning strategy;local discriminative object parts;deep reinforcement learning;computer vision;image region discovery;pseudocomplete object region extraction;Markov decision process","","34","","61","IEEE","21 Feb 2020","","","IEEE","IEEE Journals"
"Many-Objective Distribution Network Reconfiguration Via Deep Reinforcement Learning Assisted Optimization Algorithm","Y. Li; G. Hao; Y. Liu; Y. Yu; Z. Ni; Y. Zhao","School of Artificial Intelligence and Automation, and Ministry of Education Key Laboratory of Image Processing and Intelligence Control, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, and Ministry of Education Key Laboratory of Image Processing and Intelligence Control, Huazhong University of Science and Technology, Wuhan, China; School of Electric Power, South China University of Technology, Guangzhou, China; School of Artificial Intelligence and Automation, and Ministry of Education Key Laboratory of Image Processing and Intelligence Control, Huazhong University of Science and Technology, Wuhan, China; China-EU Institute for Clean and Renewable Energy, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, and Ministry of Education Key Laboratory of Image Processing and Intelligence Control, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Power Delivery","23 May 2022","2022","37","3","2230","2244","With the increasing penetration of renewable energy (RE), the operations of distribution network are threatened and some issues may appear, i.e., large voltage deviation, deterioration of statistic voltage stability, high power loss, etc. In turn, RE accommodation would be significantly impacted. Therefore, we propose a many-objective distribution network reconfiguration (MDNR) model, with the consideration of RE curtailment, voltage deviation, power loss, statistic voltage stability, and generation cost. This aims to assess the trade-off among these objectives for better operations of distribution networks. As the proposed model is a non-convex, non-linear, many-objective optimization problem, it is difficult to be solved. We further propose a deep reinforcement learning (DRL) assisted multi-objective bacterial foraging optimization (DRL-MBFO) algorithm. This algorithm combines the advantages of DRL and MBFO, and is targeted to find the Pareto front of proposed MDNR model with better searching efficiency. Finally, we conduct case study on the modified IEEE 33-bus, 69-bus, and 118-bus power distribution systems, and results verify the effectiveness of the MDNR model and outperformance of the proposed DRL-MBFO.","1937-4208","","10.1109/TPWRD.2021.3107534","National Natural Science Foundation of China(grant numbers:62073148); Tencent Rhinoceros Foundation of China(grant numbers:CCF-Tencent RAGR20210102); National Natural Science Foundation of China(grant numbers:51807120); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524511","Distribution network reconfiguration;renewable energy;many-objective optimization;deep reinforcement learning","Distribution networks;Optimization;Reinforcement learning;Generators;Renewable energy sources;Power system stability;Microorganisms","deep learning (artificial intelligence);demand side management;distribution networks;Pareto optimisation;power engineering computing;power system stability;reinforcement learning;renewable energy sources","voltage deviation;statistic voltage stability;many-objective distribution network reconfiguration model;MDNR model;118-bus power distribution systems;deep reinforcement learning assisted optimization algorithm;renewable energy;RE accommodation;RE curtailment;power loss;generation cost;distribution networks;multi-objective bacterial foraging optimization;DRL-MBFO algorithm;Pareto front;modified IEEE 33-bus power distribution system","","29","","55","IEEE","27 Aug 2021","","","IEEE","IEEE Journals"
"Input–Output Data-Based Output Antisynchronization Control of Multiagent Systems Using Reinforcement Learning Approach","Z. Peng; Y. Zhao; J. Hu; R. Luo; B. K. Ghosh; S. K. Nguang","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Business Administration, Southwestern University of Finance and Economics, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical, Computer, and Software Engineering, The University of Auckland, Auckland, New Zealand","IEEE Transactions on Industrial Informatics","30 Jul 2021","2021","17","11","7359","7367","This article investigates an output antisynchronization problem of multiagent systems by using an input-output data-based reinforcement learning approach. Till now, most of the existing results on antisynchronization problems required full-state information and exact system dynamics in the controller design, which is always invalid in practical scenarios. To address this issue, a new system representation is constructed by using just the available input/output data from the multiagent system. Then, a novel value iteration algorithm is proposed to compute the optimal control laws for the agents; moreover, a convergence analysis is presented for the proposed algorithm. In the implementation of the data-based controllers, an actor-critic network structure is established to learn the optimal control laws without the requirement of information of the agent dynamics. An incremental weight updating rule is proposed to improve the learning performance. Finally, simulation results are presented to demonstrate the effectiveness of the proposed antisynchronization control strategy.","1941-0050","","10.1109/TII.2021.3050768","National Natural Science Foundation of China(grant numbers:61473061,61104104,71503206); Program for New Century Excellent Talents in University(grant numbers:NCET-13-0091); Sichuan Science and Technology Program(grant numbers:2020YFSY0012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321152","Incremental actor–critic (AC) network;input–output data;optimal antisynchronization;partially observable multiagent systems;reinforcement learning (RL)","Multi-agent systems;Informatics;Synchronization;Artificial neural networks;Optimal control;Network topology;Heuristic algorithms","iterative methods;learning (artificial intelligence);multi-agent systems;optimal control","input-output data-based output;multiagent system;output antisynchronization problem;input-output data-based reinforcement learning approach;antisynchronization problems;full-state information;exact system dynamics;controller design;system representation;optimal control laws;data-based controllers;learning performance;antisynchronization control strategy","","28","","30","IEEE","12 Jan 2021","","","IEEE","IEEE Journals"
"Harmonious Lane Changing via Deep Reinforcement Learning","G. Wang; J. Hu; Z. Li; L. Li","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, BNRist, Tsinghua University, Beijing, China","IEEE Transactions on Intelligent Transportation Systems","3 May 2022","2022","23","5","4642","4650","In this paper, we study how to learn a harmonious deep reinforcement learning (DRL) based lane-changing strategy for autonomous vehicles without Vehicle-to-Everything (V2X) communication support. The basic framework of this paper can be viewed as a multi-agent reinforcement learning in which different agents will exchange their strategies after each round of learning to reach a zero-sum game state. Unlike cooperation driving, harmonious driving only relies on individual vehicles’ limited sensing results to balance overall and individual efficiency. Specifically, we propose a well-designed reward that combines individual efficiency with overall efficiency for harmony, instead of only emphasizing individual interests like competitive strategy. Testing results show that competitive strategy often leads to selfish lane change behaviors, anarchy of crowd, and thus the degeneration of traffic efficiency. In contrast, the proposed harmonious strategy can promote traffic efficiency in both free flow and traffic jam than the competitive strategy. This interesting finding indicates that we should take care of the reward setting for reinforcement learning-based AI robots (e.g., automated vehicles) design, when the utilities of these robots are not strictly in alignment.","1558-0016","","10.1109/TITS.2020.3047129","National Key Research and Development Program of China(grant numbers:2018AAA0101402); National Natural Science Foundation of China(grant numbers:61790565); Shenzhen Municipal Science and Technology Innovation Committee(grant numbers:JCYJ20170412172030008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9325948","Lane changing;reinforcement learning;deep learning","Reinforcement learning;Vehicle-to-everything;Space vehicles;Sensors;Roads;Mathematical model;Delays","game theory;learning (artificial intelligence);multi-agent systems;road traffic control","harmonious lane changing;harmonious deep reinforcement learning based lane-changing strategy;autonomous vehicles;Vehicle-to-Everything communication support;multiagent reinforcement learning;zero-sum game state;individual efficiency;harmony;individual interests;competitive strategy;selfish lane change behaviors;traffic efficiency;harmonious strategy;reinforcement learning-based AI robots;automated vehicles","","26","","46","IEEE","15 Jan 2021","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Three-Dimensional Area Coverage With UAV Swarm","Z. Mou; Y. Zhang; F. Gao; H. Wang; T. Zhang; Z. Han","Department of Automation, Tsinghua University, Institute for Artificial Intelligence, Tsinghua University (THUAI), the State Key Laboratory of Intelligent Technologies and Systems, and the Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; Department of Automation, Tsinghua University, Institute for Artificial Intelligence, Tsinghua University (THUAI), the State Key Laboratory of Intelligent Technologies and Systems, and the Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; Department of Automation, Tsinghua University, Institute for Artificial Intelligence, Tsinghua University (THUAI), the State Key Laboratory of Intelligent Technologies and Systems, and the Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; Department of Automation, School of Information Science and Technology, Tsinghua University, Beijing, China; Department of Automation, School of Information Science and Technology, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA","IEEE Journal on Selected Areas in Communications","15 Sep 2021","2021","39","10","3160","3176","Unmanned aerial vehicle (UAV) technology is recognized as a promising solution to area coverage problems (ACPs) and has been extensively studied recently. In this paper, we study the 3D irregular terrain surface coverage problem with a hierarchical UAV swarm. We first build the 3D model of a random irregular terrain and propose a geometric way to project the 3D terrain surface into many weighted 2D patches. Then we develop a two-level hierarchical UAV swarm architecture, including the low-level follower UAVs (FUAVs) and the high-level leader UAVs (LUAVs). For FUAVs, we design a coverage trajectory algorithm to carry out specific coverage tasks within patches based on the star communication topology. For LUAVs, we propose a swarm deep Q-learning (SDQN) reinforcement learning algorithm to select patches. Moreover, an observation history model based on convolutional neural networks (CNNs) and the mean embedding method is integrated into SDQN to address the communication limitation problems of LUAVs. The numerical results show that FUAVs can cover the entire area of each patch with little redundancies, and the total coverage time of the SDQN is less than that of existing methods, which demonstrates the effectiveness of the proposed algorithms.","1558-0008","","10.1109/JSAC.2021.3088718","National Key Research and Development Program of China(grant numbers:2018AAA0102401); National Natural Science Foundation of China(grant numbers:61831013,61771274); Beijing Municipal Natural Science Foundation(grant numbers:L182042,4212002); U.S. Multidisciplinary University Research Initiative(grant numbers:18RT0073); NSF(grant numbers:EARS-1839818,CNS1717454,CNS-1731424,CNS-1702850); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9453825","UAV swarm system;3D area coverage;deep reinforcement learning;trajectory design","Three-dimensional displays;Unmanned aerial vehicles;Topology;Solid modeling;Wireless sensor networks;Trajectory;Task analysis","aircraft control;autonomous aerial vehicles;control engineering computing;convolutional neural nets;deep learning (artificial intelligence);geometry;mobile robots;multi-robot systems;path planning;solid modelling;swarm intelligence;topology;trajectory control","FUAVs;coverage trajectory algorithm;star communication topology;LUAVs;SDQN;observation history model;deep reinforcement learning;three-dimensional area coverage;unmanned aerial vehicle technology;3D irregular terrain surface coverage problem;random irregular terrain;3D terrain surface;two-level hierarchical UAV swarm architecture;low-level follower UAVs;high-level leader UAVs;swarm deep Q-learning reinforcement learning;convolutional neural networks","","24","","43","IEEE","14 Jun 2021","","","IEEE","IEEE Journals"
"Explainable AI in Deep Reinforcement Learning Models for Power System Emergency Control","K. Zhang; J. Zhang; P. -D. Xu; T. Gao; D. W. Gao","School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; Department of Electrical and Computer Engineering, University of Denver, Denver, CO, USA","IEEE Transactions on Computational Social Systems","1 Apr 2022","2022","9","2","419","427","Artificial intelligence (AI) technology has become an important trend to support the analysis and control of complex and time-varying power systems. Although deep reinforcement learning (DRL) has been utilized in the power system field, most of these DRL models are regarded as black boxes, which are difficult to explain and cannot be used on occasions when human operators need to participate. Using the explainable AI (XAI) technology to explain why power system models make certain decisions is as important as the accuracy of the decisions themselves because it ensures trust and transparency in the model decision-making process. The interpretability issue in DRL models in power system emergency control is discussed in this article. The proposed interpretable method is a backpropagation deep explainer based on Shapley additive explanations (SHAPs), which is named the Deep-SHAP method. The Deep-SHAP method is adopted to provide a reasonable interpretable model for a DRL-based emergency control application. For the DRL model, the importance of input features has been quantified to obtain contributions for the outcome of the model. Further, feature classification of the inputs and probabilistic analysis of the outputs in the XAI model is added to interpretability results for better clarity.","2329-924X","","10.1109/TCSS.2021.3096824","National Key Research and Development Program of China(grant numbers:2018AAA0101504); Science and Technology Project of the State Grid Corporation of China (SGCC): Fundamental Theory of Human-in-the-loop Hybrid-Augmented Intelligence for Power Grid Dispatch and Control; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506997","Deep reinforcement learning (DRL);emergency control;explainable artificial intelligence (XAI);power system","Load modeling;Power systems;Artificial intelligence;Load shedding;Training;Neurons;Control systems","artificial intelligence;backpropagation;decision making;learning (artificial intelligence)","Deep-SHAP method;reasonable interpretable model;DRL-based emergency control application;DRL model;XAI model;Deep reinforcement learning models;power system emergency control;artificial intelligence technology;important trend;complex time-varying power systems;power system field;explainable AI technology;power system models;model decision-making process;backpropagation deep explainer","","20","","32","IEEE","4 Aug 2021","","","IEEE","IEEE Journals"
"Data-Driven Human-Robot Interaction Without Velocity Measurement Using Off-Policy Reinforcement Learning","Y. Yang; Z. Ding; R. Wang; H. Modares; D. C. Wunsch","Key Laboratory of Knowledge Automation for Industrial Processes Ministry of Education, School of Automation &Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation, Beijng Institute Technology, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Mechanical Engineering Department, Michigan State University, East Lansing, MI, USA; Department of Electrical &Computer Engineering, Missouri University of Science &Technology, Rolla, MO, USA","IEEE/CAA Journal of Automatica Sinica","20 Oct 2021","2022","9","1","47","63","In this paper, we present a novel data-driven design method for the human-robot interaction (HRI) system, where a given task is achieved by cooperation between the human and the robot. The presented HRI controller design is a two-level control design approach consisting of a task-oriented performance optimization design and a plant-oriented impedance controller design. The task-oriented design minimizes the human effort and guarantees the perfect task tracking in the outer-loop, while the plant-oriented achieves the desired impedance from the human to the robot manipulator end-effector in the inner-loop. Data-driven reinforcement learning techniques are used for performance optimization in the outer-loop to assign the optimal impedance parameters. In the inner-loop, a velocity-free filter is designed to avoid the requirement of end-effector velocity measurement. On this basis, an adaptive controller is designed to achieve the desired impedance of the robot manipulator in the task space. The simulation and experiment of a robot manipulator are conducted to verify the efficacy of the presented HRI design framework.","2329-9274","","10.1109/JAS.2021.1004258","National Natural Science Foundation of China(grant numbers:61903028); Youth Innovation Promotion Association, Chinese Academy of Sciences(grant numbers:2020137); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9536670","Adaptive impedance control;data-driven method;human-robot interaction (HRI);reinforcement learning;velocity-free","Adaptation models;Human-robot interaction;Reinforcement learning;Aerospace electronics;Numerical simulation;End effectors;Impedance","adaptive control;control system synthesis;end effectors;human-robot interaction;manipulator dynamics;optimisation;position control","end-effector velocity measurement;adaptive controller;desired impedance;task space;data-driven human-robot interaction;off-policy reinforcement;data-driven design method;human-robot interaction system;HRI controller design;two-level control design approach;task-oriented performance optimization design;plant-oriented impedance controller design;task-oriented design minimizes;perfect task tracking;outer-loop;robot manipulator end-effector;inner-loop;data-driven reinforcement learning techniques;optimal impedance parameters;velocity-free filter;HRI design framework","","15","","45","","13 Sep 2021","","","IEEE","IEEE Journals"
"Leader–Follower Bipartite Output Synchronization on Signed Digraphs Under Adversarial Factors via Data-Based Reinforcement Learning","Q. Li; L. Xia; R. Song; J. Liu","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","5 Oct 2020","2020","31","10","4185","4195","The optimal solution to the leader-follower bipartite output synchronization problem is proposed for heterogeneous multiagent systems (MASs) over signed digraphs in the presence of adversarial inputs in this article. For the MASs, the dynamics and dimensions of the followers are different. Distributed observers are first designed to estimate the leader's two-way state and output over signed digraphs. Then, the leader-follower bipartite output synchronization problem on signed graphs is translated into a conventional output distributed leader-follower problem over nonnegative graphs after the state transformation by using the information of followers and observers. The effect of adversarial inputs in sensors or actuators of agents is mitigated by designing the resilient H∞ controller. A data-based reinforcement learning (RL) algorithm is proposed to obtain the optimal control law, which implies that the dynamics of the followers is not required. Finally, a simulation example is given to verify the effectiveness of the proposed algorithm.","2162-2388","","10.1109/TNNLS.2019.2952611","National Natural Science Foundation of China(grant numbers:61873300,61722312); Fundamental Research Funds for the Central Universities(grant numbers:FRF-GF-17-B45); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931002","Adversarial inputs;bipartite output synchronization;heterogeneous multiagent systems (MASs);reinforcement learning (RL);resilient H∞ controller;signed digraphs","Synchronization;Heuristic algorithms;Vehicle dynamics;Observers;Output feedback;Laplace equations;Learning systems","directed graphs;distributed control;graph theory;H∞ control;learning systems;multi-agent systems;optimal control;synchronisation","signed graphs;conventional output distributed leader-follower problem;adversarial inputs;data-based reinforcement learning algorithm;leader-follower bipartite output synchronization;signed digraphs;leader-follower bipartite output synchronization problem;adversarial factors;H∞ controller resilience;agent actuators","","15","","39","IEEE","11 Dec 2019","","","IEEE","IEEE Journals"
"Integrated Localization and Tracking for AUV With Model Uncertainties via Scalable Sampling-Based Reinforcement Learning Approach","J. Yan; X. Li; X. Yang; X. Luo; C. Hua; X. Guan","Department of Automation, Yanshan University, Qinhuangdao, China; Department of Automation, Yanshan University, Qinhuangdao, China; Institute of Information Science and Engineering, Yanshan University, Qinhuangdao, China; Department of Automation, Yanshan University, Qinhuangdao, China; Department of Automation, Yanshan University, Qinhuangdao, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","14 Oct 2022","2022","52","11","6952","6967","This article studies the joint localization and tracking issue for the autonomous underwater vehicle (AUV), with the constraints of asynchronous time clock in cyberchannels and model uncertainty in physical channels. More specifically, we develop a reinforcement learning (RL)-based asynchronous localization algorithm to localize the position of AUV, where the time clock of AUV is not required to be well synchronized with the real time. Based on the estimated position, a scalable sampling strategy called multivariate probabilistic collocation method with orthogonal fractional factorial design (M-PCM-OFFD) is employed to evaluate the time-varying uncertain model parameters of AUV. After that, an RL-based tracking controller is designed to drive AUV to the desired target point. Besides that, the performance analyses for the integration solution are also presented. Of note, the advantages of our solution are highlighted as: 1) the RL-based localization algorithm can avoid local optimal in traditional least-square methods; 2) the M-PCM-OFFD-based sampling strategy can address the model uncertainty and reduce the computational cost; and 3) the integration design of localization and tracking can reduce the communication energy consumption. Finally, simulation and experiment demonstrate that the proposed localization algorithm can effectively eliminate the impact of asynchronous clock, and more importantly, the integration of M-PCM-OFFD in the RL-based tracking controller can find accurate optimization solutions with limited computational costs.","2168-2232","","10.1109/TSMC.2021.3129534","NSFC(grant numbers:61873345,61973263,62033011); Youth Talent Program of Hebei(grant numbers:BJ2020031); NSFH(grant numbers:2020203002,2021203056); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9633120","Autonomous underwater vehicle (AUV);localization;reinforcement learning (RL);tracking;uncertain system","Location awareness;Uncertainty;Target tracking;Clocks;Computational modeling;Synchronization;Global Positioning System","autonomous underwater vehicles;large-scale systems;learning (artificial intelligence);least squares approximations;mobile robots;optimisation;position control;remotely operated vehicles;sampling methods;statistical analysis;time-varying systems;tracking;uncertain systems;underwater vehicles;wireless sensor networks","integrated localization;AUV;model uncertainty;scalable sampling-based reinforcement learning approach;joint localization;autonomous underwater vehicle;asynchronous time clock;reinforcement learning-based asynchronous localization algorithm;scalable sampling strategy;multivariate probabilistic collocation method;time-varying uncertain model parameters;integration solution;RL-based localization algorithm;local optimal;M-PCM-OFFD-based sampling strategy;integration design;asynchronous clock;RL-based tracking controller","","13","","45","CCBY","2 Dec 2021","","","IEEE","IEEE Journals"
"Hierarchical Optimal Synchronization for Linear Systems via Reinforcement Learning: A Stackelberg–Nash Game Perspective","M. Li; J. Qin; Q. Ma; W. X. Zheng; Y. Kang","Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; School of Computer, Data and Mathematical Sciences, Western Sydney University, Sydney, NSW, Australia; Department of Automation, University of Science and Technology of China, Hefei, China","IEEE Transactions on Neural Networks and Learning Systems","2 Apr 2021","2021","32","4","1600","1611","Considering the fact that in the real world, a certain agent may have some sort of advantage to act before others, a novel hierarchical optimal synchronization problem for linear systems, composed of one major agent and multiple minor agents, is formulated and studied in this article from a Stackelberg-Nash game perspective. The major agent herein makes its decision prior to others, and then, all the minor agents determine their actions simultaneously. To seek the optimal controllers, the Hamilton-Jacobi-Bellman (HJB) equations in coupled forms are established, whose solutions are further proven to be stable and constitute the Stackelberg-Nash equilibrium. Due to the introduction of the asymmetric roles for agents, the established HJB equations are more strongly coupled and more difficult to solve than that given in most existing works. Therefore, we propose a new reinforcement learning (RL) algorithm, i.e., a two-level value iteration (VI) algorithm, which does not rely on complete system matrices. Furthermore, the proposed algorithm is shown to be convergent, and the converged values are exactly the optimal ones. To implement this VI algorithm, neural networks (NNs) are employed to approximate the value functions, and the gradient descent method is used to update the weights of NNs. Finally, an illustrative example is provided to verify the effectiveness of the proposed algorithm.","2162-2388","","10.1109/TNNLS.2020.2985738","National Natural Science Foundation of China(grant numbers:61922076,61873252,61725304,61673361); Fok Ying-Tong Education Foundation for Young Teachers in Higher Education Institutions of China(grant numbers:161059); NSW Cyber Security Network(grant numbers:P00025091); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079191","Hierarchical optimal synchronization;linear systems;neural networks (NNs);stackelberg–Nash game (SNG);value iteration (VI)","Synchronization;Games;Decision making;Mathematical model;Linear systems;Approximation algorithms;Artificial neural networks","game theory;gradient methods;iterative methods;learning (artificial intelligence);optimal control;optimisation","VI algorithm;optimal ones;complete system matrices;reinforcement learning algorithm;established HJB equations;Stackelberg-Nash equilibrium;coupled forms;Hamilton-Jacobi-Bellman equations;optimal controllers;novel hierarchical optimal synchronization problem;Stackelberg-Nash game perspective;linear systems","","13","","33","IEEE","27 Apr 2020","","","IEEE","IEEE Journals"
"Sparse Proximal Reinforcement Learning via Nested Optimization","T. Song; D. Li; Q. Jin; K. Hirasawa","Institute of Automation, Beijing University of Chemical Technology, Beijing, China; Institute of Automation, Beijing University of Chemical Technology, Beijing, China; Institute of Automation, Beijing University of Chemical Technology, Beijing, China; Institute of Automation, Beijing University of Chemical Technology, Beijing, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","14 Oct 2020","2020","50","11","4020","4032","We consider the tasks of feature selection and policy evaluation based on linear value function approximation in reinforcement learning problems. High-dimension feature vectors and limited number of samples can easily cause over-fitting and computation expensive. To prevent this problem, ℓ1-regularized method obtains sparse solutions and thus improves generalization performance. We propose an efficient ℓ1-regularized recursive least squares-based online algorithm with O(n2) complexity per time-step, termed ℓ1-RC. With the help of nested optimization decomposition, ℓ1-RC solves a series of standard optimization problems and avoids minimizing mean squares projected Bellman error with ℓ1-regularization directly. In ℓ1-RC, we propose RC with iterative refinement to minimize the operator error, and we propose an alternating direction method of multipliers with proximal operator to minimize the fixed-point error. The convergence of ℓ1-RC is established based on ordinary differential equation method and some extensions are also given. In empirical computations, some state-of-the-art ℓ1-regularized methods are chosen as the baselines, and ℓ1-RC are tested in both policy evaluation and learning control benchmarks. The empirical results show the effectiveness and advantages of ℓ1-RC.","2168-2232","","10.1109/TSMC.2018.2865505","National Natural Science Foundation of China(grant numbers:61873022,61573052); Beijing Natural Science Foundation(grant numbers:4182045); Fundamental Research Funds for the Central Universities(grant numbers:ZY1839); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457478","Alternating direction method of multipliers (ADMM);l₁-regularization;nested optimization;proximal algorithm;temporal difference (TD);value function approximation (VFA)","Optimization;Convex functions;Complexity theory;Task analysis;Function approximation;Approximation algorithms;Standards","computational complexity;convergence of numerical methods;differential equations;feature selection;function approximation;iterative methods;learning (artificial intelligence);least squares approximations;optimisation","convergence;alternating direction method of multipliers;iterative refinement;ℓ1-RC;ℓ1-regularized recursive least squares-based online algorithm;ℓ1-regularized method;sparse solutions;high-dimension feature vectors;linear value function approximation;feature selection;sparse proximal reinforcement learning;learning control;policy evaluation;ordinary differential equation;fixed-point error;proximal operator;nested optimization decomposition","","11","","43","IEEE","11 Sep 2018","","","IEEE","IEEE Journals"
"Data-Driven Integral Reinforcement Learning for Continuous-Time Non-Zero-Sum Games","Y. Yang; L. Wang; H. Modares; D. Ding; Y. Yin; D. Wunsch","Key Laboratory of Knowledge Automation for Industrial Processes, Ministry of Education, University of Science and Technology Beijing, Beijing, China; Key Laboratory of Knowledge Automation for Industrial Processes, Ministry of Education, University of Science and Technology Beijing, Beijing, China; Department of Mechanical Engineering, Michigan State University, East Lansing, MI, USA; Key Laboratory of Knowledge Automation for Industrial Processes, Ministry of Education, University of Science and Technology Beijing, Beijing, China; Key Laboratory of Knowledge Automation for Industrial Processes, Ministry of Education, University of Science and Technology Beijing, Beijing, China; Department of Electrical and Computer Engineering, Missouri University of Science and Technology, Rolla, MO, USA","IEEE Access","3 Jul 2019","2019","7","","82901","82912","This paper develops an integral value iteration (VI) method to efficiently find online the Nash equilibrium solution of two-player non-zero-sum (NZS) differential games for linear systems with partially unknown dynamics. To guarantee the closed-loop stability about the Nash equilibrium, the explicit upper bound for the discounted factor is given. To show the efficacy of the presented online model-free solution, the integral VI method is compared with the model-based off-line policy iteration method. Moreover, the theoretical analysis of the integral VI algorithm in terms of three aspects, i.e., positive definiteness properties of the updated cost functions, the stability of the closed-loop systems, and the conditions that guarantee the monotone convergence, is provided in detail. Finally, the simulation results demonstrate the efficacy of the presented algorithms.","2169-3536","","10.1109/ACCESS.2019.2923845","National Natural Science Foundation of China(grant numbers:61873028,61333002); China Postdoctoral Science Foundation(grant numbers:2018M641197); Fundamental Research Funds for the China Central Universities of USTB(grant numbers:FRF-TP-18-031A1,FRF-BD-17-002A,FRF-GF-17-B48); Mary K. Finley Endowment; Missouri S&T Intelligent Systems Center; Lifelong Learning Machines Program from the DARPA/Microsystems Technology Office; Army Research Laboratory(grant numbers:W911NF-18-2-0260); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8740904","Coupled Riccati equations;integral reinforcement learning;non-zero-sum games;optimal control","Games;Heuristic algorithms;Nash equilibrium;Approximation algorithms;Stability analysis;Asymptotic stability;Convergence","closed loop systems;continuous time systems;differential games;iterative methods;learning (artificial intelligence);linear systems;stability","linear systems;discounted factor;model-based off-line policy iteration method;integral VI algorithm;data-driven integral reinforcement learning;continuous-time nonzero-sum games;Nash equilibrium solution;differential games;closed loop stability;closed loop systems;online model-free solution;monotone convergence;integral value iteration method","","10","","38","CCBY","19 Jun 2019","","","IEEE","IEEE Journals"
"Enhanced Bayesian Compression via Deep Reinforcement Learning","X. Yuan; L. Ren; J. Lu; J. Zhou","Department of Automation, Tsinghua University; Department of Automation, Tsinghua University; Department of Automation, Tsinghua University; Department of Automation, Tsinghua University","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","6939","6948","In this paper, we propose an Enhanced Bayesian Compression method to flexibly compress the deep networks via reinforcement learning. Unlike the existing Bayesian compression method which cannot explicitly enforce quantization weights during training, our method learns flexible codebooks in each layer for an optimal network quantization. To dynamically adjust the state of codebooks, we employ an Actor-Critic network to collaborate with the original deep network. Different from most existing network quantization methods, our EBC does not require re-training procedures after the quantization. Experimental results show that our method obtains low-bit precision with acceptable accuracy drop on MNIST, CIFAR and ImageNet.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954232","Deep Learning;Recognition: Detection;Categorization;Retrieval","","Bayes methods;data compression;learning (artificial intelligence)","deep reinforcement learning;deep networks;quantization weights;flexible codebooks;optimal network quantization;Actor-Critic network;re-training procedures;deep network;enhanced Bayesian compression method;low-bit precision;MNIST;ImageNet;CIFAR","","9","","61","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Reinforcement Q-Learning Incorporated With Internal Model Method for Output Feedback Tracking Control of Unknown Linear Systems","C. Chen; W. Sun; G. Zhao; Y. Peng","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China","IEEE Access","28 Jul 2020","2020","8","","134456","134467","This paper investigates the output feedback (OPFB) tracking control problem for discrete-time linear (DTL) systems with unknown dynamics. With the approach of augmented system, the tracking control problem is first turned into a regulation problem with a discounted performance function, the solution of which relies on the Q-function based Bellman equation. Then, a novel value iteration (VI) scheme based on reinforcement Q-learning mechanism is proposed for solving the Q-function Bellman equation without knowing the system dynamics. Moreover, the convergence of the VI based Q-learning is proved by indicating that it converges to the Q-function Bellman equation and it brings out no bias of solution even under the probing noise satisfying the persistent excitation (PE) condition. As a result, the OPFB tracking controller can be learned online by using the past input, output, and reference trajectory data of the augmented system. The proposed scheme removes the requirement of initial admissible policy in the policy iteration (PI) method. Finally, effectiveness of the proposed scheme is demonstrated through a simulation example.","2169-3536","","10.1109/ACCESS.2020.3011194","National Natural Science Foundation (NNSF) of China(grant numbers:61573154); Science and Technology Planning Project of Guangdong Province(grant numbers:2015A010106003,2017A010101009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146286","Adaptive dynamic programming (ADP);optimal control;Bellman equation;on-policy;internal model","Mathematical model;Heuristic algorithms;Optimal control;System dynamics;Trajectory;Linear systems;Convergence","continuous time systems;control system synthesis;convergence of numerical methods;discrete time systems;feedback;iterative methods;learning systems;linear systems;trajectory control","internal model method;unknown linear systems;output feedback tracking control problem;discrete-time linear systems;unknown dynamics;augmented system;regulation problem;discounted performance function;reinforcement Q-learning mechanism;Q-function Bellman equation;system dynamics;VI based Q-learning;OPFB tracking controller;policy iteration method;probing noise;persistent excitation condition;convergence","","8","","42","CCBY","22 Jul 2020","","","IEEE","IEEE Journals"
"UNMAS: Multiagent Reinforcement Learning for Unshaped Cooperative Scenarios","J. Chai; W. Li; Y. Zhu; D. Zhao; Z. Ma; K. Sun; J. Ding","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; X-Lab, The Second Academy of CASIC, Beijing, China; X-Lab, The Second Academy of CASIC, Beijing, China; X-Lab, The Second Academy of CASIC, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","4 Apr 2023","2023","34","4","2093","2104","Multiagent reinforcement learning methods, such as VDN, QMIX, and QTRAN, that adopt centralized training with decentralized execution (CTDE) framework have shown promising results in cooperation and competition. However, in some multiagent scenarios, the number of agents and the size of the action set actually vary over time. We call these unshaped scenarios, and the methods mentioned above fail in performing satisfyingly. In this article, we propose a new method, called Unshaped Networks for Multiagent Systems (UNMAS), which adapts to the number and size changes in multiagent systems. We propose the self-weighting mixing network to factorize the joint action-value. Its adaption to the change in agent number is attributed to the nonlinear mapping from each-agent Q value to the joint action-value with individual weights. Besides, in order to address the change in an action set, each agent constructs an individual action-value network that is composed of two streams to evaluate the constant environment-oriented subset and the varying unit-oriented subset. We evaluate UNMAS on various StarCraft II micromanagement scenarios and compare the results with several state-of-the-art MARL algorithms. The superiority of UNMAS is demonstrated by its highest winning rates especially on the most difficult scenario 3s5z_vs_3s6z. The agents learn to perform effectively cooperative behaviors, while other MARL algorithms fail. Animated demonstrations and source code are provided in https://sites.google.com/view/unmas.","2162-2388","","10.1109/TNNLS.2021.3105869","National Key Research and Development Program of China(grant numbers:2018AAA0102404); Strategic Priority Research Program of Chinese Academy of Sciences (CAS)(grant numbers:XDA27030400); Youth Innovation Promotion Association of CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9525046","Centralized training with decentralized execution (CTDE);multiagent;reinforcement learning;StarCraft II","Multi-agent systems;Training;Task analysis;Reinforcement learning;Sun;Learning systems;Semantics","computer games;multi-agent systems;reinforcement learning","agent number;agent Q value;centralized training with decentralized execution;constant environment-oriented subset;CTDE;individual action-value network;joint action-value;multiagent reinforcement learning;multiagent systems;QMIX;QTRAN;self-weighting mixing network;StarCraft II micromanagement scenarios;unit-oriented subset;UNMAS;unshaped cooperative scenarios;unshaped networks;VDN","","8","","43","IEEE","30 Aug 2021","","","IEEE","IEEE Journals"
"Omnidirectional-Wheel Conveyor Path Planning and Sorting Using Reinforcement Learning Algorithms","W. Zaher; A. W. Youssef; L. A. Shihata; E. Azab; M. Mashaly","Electronics Department, Faculty of Information Engineering and Technology, German University in Cairo, New Cairo City, Egypt; Mechatronics Department, Faculty of Engineering and Materials Science, German University in Cairo, New Cairo City, Egypt; Design and Production Department, Faculty of Engineering and Materials Science, German University in Cairo, New Cairo City, Egypt; Electronics Department, Faculty of Information Engineering and Technology, German University in Cairo, New Cairo City, Egypt; Networks Department, Faculty of Information Engineering and Technology, German University in Cairo, New Cairo City, Egypt","IEEE Access","17 Mar 2022","2022","10","","27945","27959","In this paper, path planning and sorting of packages for Omnidirectional-Wheel conveyor are presented using Reinforcement Learning (RL). Q-learning, Double Q-learning, Deep Q-learning, and the Double Deep Q-learning algorithms are investigated. The RL algorithms enable the conveyor to self-learn the packages path and sort them without using conventional control or path planning theories. The RL algorithms are used for two different case studies on conveyors structures with different numbers of cells to compare and evaluate their performances in large- and small-scale sized structures. To explore the proposed methods response to external environment effects, two types of collisions between multiple packages were considered, the proposed RL algorithms showed their ability to resolve both types successfully. Comparative study between multiple RL algorithms for path planning showed that the Q-learning and Double Q-learning algorithms had outperformed their Deep learning versions for path planning in the two case studies. Furthermore, the proposed RL methods are compared experimentally to classic control and path planning theories using a hardware prototype for one of the presented case studies. The hardware experimental results showed that the proposed RL methods were as successful as the conventional methods in path planning and sorting in much less processing time. Two types of sorting scenarios (Type I and II) were tested for same package type and for multiple ones. For Type I sorting the Q-learning algorithm performed better than the Q-learning with weights approach, achieving better mean and minimum rewards while maximum rewards remain the same for both techniques. As for Type II sorting, only the Q-learning with weights approach was able to achieve it and converge in a reasonable time.","2169-3536","","10.1109/ACCESS.2022.3156924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9729190","Deep Q-learning (DQN);double deep Q-learning (DDQN);double Q-learning;omnidirectional-wheel conveyor;path planning;Q-learning;reinforcement learning;sorting","Path planning;Q-learning;Sorting;Task analysis;Heuristic algorithms;Routing;Prediction algorithms","conveyors;deep learning (artificial intelligence);path planning;production engineering computing;sorting;wheels","reinforcement learning algorithms;RL algorithms;omnidirectional-wheel conveyor path planning;double deep Q-learning algorithm;type II sorting;type I sorting;weights approach;packages path self-learning","","7","","32","CCBYNCND","4 Mar 2022","","","IEEE","IEEE Journals"
"A Gradient-Based Reinforcement Learning Algorithm for Multiple Cooperative Agents","Z. Zhang; D. Wang; D. Zhao; Q. Han; T. Song","School of Automation, Qingdao University, Qingdao, China; School of Automation, Qingdao University, Qingdao, China; University of Chinese Academy of Sciences, Beijing, China; School of Automation, Qingdao University, Qingdao, China; School of Automation, Qingdao University, Qingdao, China","IEEE Access","9 Dec 2018","2018","6","","70223","70235","Multi-agent reinforcement learning (MARL) can be used to design intelligent agents for solving cooperative tasks. Within the MARL category, this paper proposes the probability of maximal reward based on the infinitesimal gradient ascent (PMR-IGA) algorithm to reach the maximal total reward in repeated games. Theoretical analyses show that in a finite-player-finite-action repeated game with two pure optimal joint actions where no common component action exists, both the optimal joint actions are stable critical points of the PMR-IGA model. Furthermore, we apply the Q-value function to estimate the gradient and derive the probability of maximal reward based on estimated gradient ascent (PMR-EGA) algorithm. Theoretical analyses and simulations of case studies of repeated games show that the maximal total reward can be achieved under any initial conditions. The PMR-EGA can be naturally extended to optimize cooperative stochastic games. Two stochastic games, i.e., box pushing and a distributed sensor network, are used as test beds. The simulations show that the PMR-EGA displays consistently an excellent performance for both stochastic games.","2169-3536","","10.1109/ACCESS.2018.2878853","Natural Science Foundation of Shandong Province(grant numbers:ZR2017PF005); National Natural Science Foundation of China(grant numbers:61873138,61803218,61573353,61533017,61573205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517104","Multi-agent reinforcement learning;gradient ascent;Q-learning;cooperative tasks","Games;Stochastic processes;Mathematical model;Heuristic algorithms;Convergence;Task analysis","game theory;gradient methods;learning (artificial intelligence);multi-agent systems;optimisation;stochastic games","maximal reward;estimated gradient ascent;PMR-EGA;PMR-IGA model;stable critical points;common component action;pure optimal joint actions;finite-player-finite-action repeated game;infinitesimal gradient ascent algorithm;MARL category;intelligent agents;multiagent reinforcement learning;multiple cooperative agents;gradient-based reinforcement learning algorithm;stochastic games;maximal total reward;repeated games;theoretical analyses","","7","","36","OAPA","31 Oct 2018","","","IEEE","IEEE Journals"
"Study of reinforcement learning based shared control of walking-aid robot","W. Xu; J. Huang; Y. Wang; H. Cai","School of Automation, Huazhong University of Science and Technology, Wuhan, China; School of Automation, Huazhong University of Science and Technology, Wuhan, China; School of Automation, Huazhong University of Science and Technology, Wuhan, China; School of Automation, Huazhong University of Science and Technology, Wuhan, China","Proceedings of the 2013 IEEE/SICE International Symposium on System Integration","24 Mar 2014","2013","","","282","287","In this paper, we experimentally investigated a new reinforcement learning based robot shared control algorithm for walking-aid robot. To autonomously adapt to different user operation habits and motor ability, robot dynamically adjusted user control weight by proposed algorithm. The weight adjustment is performed online based on user control efficiency, current robot walking state and environment information by reinforcement learning algorithm. The shared control synthetizes the final robot velocity according to the control weight. The effectiveness of proposed reinforcement learning based shared control algorithm is verified by experiments in a specified environment.","","978-1-4799-2625-1","10.1109/SII.2013.6776656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6776656","","Legged locomotion;Robot sensing systems;Learning (artificial intelligence);Heuristic algorithms;Estimation","assisted living;geriatrics;learning (artificial intelligence);medical robotics;mobile robots;service robots;velocity control","walking-aid robot;reinforcement learning based robot shared control algorithm;operation habits;motor ability;robot dynamically adjusted user control weight;user control efficiency;robot walking state;environment information;final robot velocity","","5","","11","IEEE","24 Mar 2014","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning on Autonomous Driving Policy With Auxiliary Critic Network","Y. Wu; S. Liao; X. Liu; Z. Li; R. Lu","School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; Dongguan University of Technology, Dongguan, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Neural Networks and Learning Systems","6 Jul 2023","2023","34","7","3680","3690","Deep reinforcement learning (DRL) is a machine learning method based on rewards, which can be extended to solve some complex and realistic decision-making problems. Autonomous driving needs to deal with a variety of complex and changeable traffic scenarios, so the application of DRL in autonomous driving presents a broad application prospect. In this article, an end-to-end autonomous driving policy learning method based on DRL is proposed. On the basis of proximal policy optimization (PPO), we combine a curiosity-driven method called recurrent neural network (RNN) to generate an intrinsic reward signal to encounter the agent to explore its environment, which improves the efficiency of exploration. We introduce an auxiliary critic network on the original actor–critic framework and choose the lower estimate which is predicted by the dual critic network when the network update to avoid the overestimation bias. We test our method on the lane- keeping task and overtaking task in the open racing car simulator (TORCS) driving simulator and compare with other DRL methods, experimental results show that our proposed method can improve the training efficiency and control performance in driving tasks.","2162-2388","","10.1109/TNNLS.2021.3116063","National Natural Science Foundation of China(grant numbers:61922026,62033003); Guangdong Province Higher Vocational Colleges and Schools Pearl River Scholar approved in 2018; Guangdong Natural Science Funds for Distinguished Young Scholar(grant numbers:2018B030306013); Local Innovative and Research Teams Project of Guangdong Special Support Program(grant numbers:2019BT02X353); Science and Technology Planning Project of Guangdong Province(grant numbers:2017B010116006); Basic and Applied Basic Research Foundation of Guangdong Province of China(grant numbers:2019A1515111075); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582785","Autonomous driving;deep reinforcement learning (DRL);driving policy;TORCS","Autonomous vehicles;Sensors;Task analysis;Optimization;Automobiles;Linear programming;Learning systems","decision making;deep learning (artificial intelligence);learning (artificial intelligence);recurrent neural nets;reinforcement learning","auxiliary critic network;broad application prospect;changeable traffic scenarios;complex decision-making problems;complex traffic scenarios;curiosity-driven method;deep reinforcement learning;driving tasks;DRL methods;dual critic network;end-to-end autonomous driving policy learning method;intrinsic reward signal;network update;open racing car simulator driving simulator;original actor-critic framework;proximal policy optimization;realistic decision-making problems;recurrent neural network","Neural Networks, Computer;Reinforcement, Psychology;Reward;Machine Learning;Policy","4","","26","IEEE","20 Oct 2021","","","IEEE","IEEE Journals"
"Hierarchical Reinforcement Learning for Waypoint-based Exploration in Robotic Devices","J. Zinn; B. Vogel-Heuser; F. Schuhmann; L. A. Cruz Salazar","Institute of Automation and Information Systems, Technical University of Munich, Munich, Germany; Institute of Automation and Information Systems, Technical University of Munich, Munich, Germany; Institute of Automation and Information Systems, Technical University of Munich, Munich, Germany; Institute of Automation and Information Systems, Technical University of Munich, Munich, Germany","2021 IEEE 19th International Conference on Industrial Informatics (INDIN)","11 Oct 2021","2021","","","1","7","The training of Deep Reinforcement Learning algorithms on robotic devices is challenging due to their large number of actuators and limited number of feasible action sequences. This paper addresses this challenge by extending and transferring existing approaches for waypoint-based exploration with Hierarchical Reinforcement Learning to the domain of robotic devices. The resulting algorithm utilizes a top-level policy, which suggests waypoints to a bottom-level policy that controls the system actuators. The waypoints can either be provided to the top-level policy as domain knowledge or be learned from scratch. The algorithm explicitly accounts for the low number of feasible waypoints and waypoint transitions that are characteristic of robotic devices. The effectiveness of the approach is evaluated on the simulation of a research demonstrator, and a separate ablation study proves the importance of its components.","","978-1-7281-4395-8","10.1109/INDIN45523.2021.9557406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9557406","Intelligent Systems;Mechatronics and Robotics;Fault tolerant design","Training;Actuators;Service robots;Conferences;Reinforcement learning;Control systems;Intelligent systems","actuators;control engineering computing;deep learning (artificial intelligence);path planning;robots","deep reinforcement learning algorithms;robotic devices;limited number;feasible action sequences;waypoint-based exploration;hierarchical reinforcement learning;top-level policy;bottom-level policy;feasible waypoints;waypoint transitions;ablation study","","4","","28","IEEE","11 Oct 2021","","","IEEE","IEEE Conferences"
"Overview of Reinforcement Learning Based on Value and Policy","Y. -t. Liu; J. -m. Yang; L. Chen; T. Guo; Y. Jiang","School Of Automation And Electrical Engineering, Shenyang Ligong University, Shenyang; School Of Automation And Electrical Engineering, Shenyang Ligong University, Shenyang; School Of Automation And Electrical Engineering, Shenyang Ligong University, Shenyang; School Of Automation And Electrical Engineering, Shenyang Ligong University, Shenyang; College of Art and Design college, Shenyang Normal University, Shenyang","2020 Chinese Control And Decision Conference (CCDC)","11 Aug 2020","2020","","","598","603","Reinforcement learning methods are mainly divided into two categories based on value functions and policies. This article systematically introduces and summarizes reinforcement learning methods from these two categories. First, it summarizes the reinforcement learning methods based on value functions, including classic Q-learning, DQN, and effective improvement methods based on DQN. Then it introduces policy-based reinforcement learning methods, including policy gradient, policy optimization, actor critic, and their improvements. Finally, the frontier research and applications of reinforcement learning is summarized.","1948-9447","978-1-7281-5855-6","10.1109/CCDC49329.2020.9164615","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9164615","Reinforcement Learning;Value function;Policy Search","Learning (artificial intelligence);Optimization;Instruction sets;Neural networks;Markov processes;Approximation algorithms;Training","learning (artificial intelligence)","value functions;classic Q-learning;policy-based reinforcement learning methods;DQN","","4","","36","IEEE","11 Aug 2020","","","IEEE","IEEE Conferences"
"A parallel hybrid implementation using genetic algorithm, GRASP and reinforcement learning","J. P. Q. dos Santos; F. C. de Lima; R. Marrocos Magalhaes; J. D. de Melo; A. D. D. Neto","Department of Automation and Control, Federal University of Rio Grande do Norte, Brazil; Department of Computing, College of Science and Technology Mater Christi, State University of Rio Grande do Norte, Brazil; Department of Automation and Control, Federal University of Rio Grande do Norte, Brazil; Department of Automation and Control, Federal University of Rio Grande do Norte, Brazil; Department of Automation and Control, Federal University of Rio Grande do Norte, Brazil","2009 International Joint Conference on Neural Networks","31 Jul 2009","2009","","","2798","2803","In the process of searching for better solutions, a metaheuristic can be guided to regions of promising solutions using the acquisition of information on the problem under study. In this work this is done through the use of reinforcement learning. The performance of a metaheuristic can also be improved using multiple search trajectories, which act competitively and/or cooperatively. This can be accomplished using parallel processing. Thus, in this paper we propose a hybrid parallel implementation for the GRASP metaheuristics and the genetic algorithm, using reinforcement learning, applied to the symmetric traveling salesman problem.","2161-4407","978-1-4244-3548-7","10.1109/IJCNN.2009.5178938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5178938","","Genetic algorithms;Learning;Parallel processing;Traveling salesman problems;Neural networks;Large-scale systems;Context modeling;Stochastic processes;Power generation","genetic algorithms;learning (artificial intelligence);parallel processing;travelling salesman problems","parallel hybrid implementation;genetic algorithm;reinforcement learning;GRASP;metaheuristic;multiple search trajectories;parallel processing;traveling salesman problem","","4","","14","IEEE","31 Jul 2009","","","IEEE","IEEE Conferences"
"Position Control of an Underwater Biomimetic Vehicle-Manipulator System via Reinforcement Learning","R. Ma; Y. Wang; Z. Gao; T. Zhao; R. Wang; S. Wang; C. Zhou","University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Beijing Institute of Petrochemical Technology, Beijing, China; Beijing Institute of Petrochemical Technology, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Naval Research Academy, Beijing, China","2020 IEEE 9th Data Driven Control and Learning Systems Conference (DDCLS)","7 Dec 2020","2020","","","573","578","This paper addresses a position control method of an underwater biomimetic vehicle-manipulator system (UBVMS) through reinforcement learning. The system description of the UBVMS with undulating fins is given. Considering the force/torqu generated by undulating fins and hydrodynamic force/torqu, the dynamic model of this UBVMS is established. The position control problem is modeled into a continuous-state, continuous-action Markov decision process (MDP) with a deterministic state transition algorithm based on the dynamic model. To solve this MDP, a reinforcement learning method is presented, which is based on the deep deterministic policy gradient (DDPG) theorem. The simulations of the position control in 5 cases are shown in the end.","","978-1-7281-5922-5","10.1109/DDCLS49620.2020.9275206","National Natural Science Foundation of China; Foundation for Innovative Research Groups of the National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9275206","Position Control;Underwater Biomimetic Vehicle-manipulator System (UBVMS);Deep Deterministic Policy Gradient (DDPG);Reinforcement Learning","Robots;Force;Robot kinematics;Position control;Propellers;Reinforcement learning;Heuristic algorithms","autonomous underwater vehicles;biomimetics;decision theory;learning (artificial intelligence);manipulator dynamics;Markov processes;mobile robots;position control","underwater biomimetic vehicle-manipulator system;MDP;deep deterministic state transition algorithm;reinforcement learning method;continuous-action Markov decision process;position control problem;dynamic model;undulating fins;UBVMS;system description","","4","","16","IEEE","7 Dec 2020","","","IEEE","IEEE Conferences"
"Action Mapping: A Reinforcement Learning Method for Constrained-Input Systems","X. Yuan; Y. Wang; J. Liu; C. Sun","School of Automation and the Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, China; School of Automation and the Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, China; School of Automation and the Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, China; School of Automation and the Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, China","IEEE Transactions on Neural Networks and Learning Systems","5 Oct 2023","2023","34","10","7145","7157","Existing approaches to constrained-input optimal control problems mainly focus on systems with input saturation, whereas other constraints, such as combined inequality constraints and state-dependent constraints, are seldom discussed. In this article, a reinforcement learning (RL)-based algorithm is developed for constrained-input optimal control of discrete-time (DT) systems. The deterministic policy gradient (DPG) is introduced to iteratively search the optimal solution to the Hamilton–Jacobi–Bellman (HJB) equation. To deal with input constraints, an action mapping (AM) mechanism is proposed. The objective of this mechanism is to transform the exploration space from the subspace generated by the given inequality constraints to the standard Cartesian product space, which can be searched effectively by existing algorithms. By using the proposed architecture, the learned policy can output control signals satisfying the given constraints, and the original reward function can be kept unchanged. In our study, the convergence analysis is given. It is shown that the iterative algorithm is convergent to the optimal solution of the HJB equation. In addition, the continuity of the iterative estimated  $Q$ -function is investigated. Two numerical examples are provided to demonstrate the effectiveness of our approach.","2162-2388","","10.1109/TNNLS.2021.3138924","National Key Research and Development Program of China(grant numbers:2018AAA0101400); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20202006); National Natural Science Foundation of China(grant numbers:61921004,62103104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681079","Constrained-input systems;neural network;optimal control;reinforcement learning (RL)","Optimal control;Cost function;Reinforcement learning;Convergence;Aerospace electronics;TV;System dynamics","","","","4","","43","IEEE","13 Jan 2022","","","IEEE","IEEE Journals"
"Locomotion Control of a Hybrid Propulsion Biomimetic Underwater Vehicle via Deep Reinforcement Learning","T. Zhang; R. Wang; Y. Wang; S. Wang","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2021 IEEE International Conference on Real-time Computing and Robotics (RCAR)","31 Aug 2021","2021","","","211","216","This paper presents a novel deep reinforcement learning (DRL) method to solve the locomotion control problem of the biomimetic underwater vehicle (BUV) with hybrid propulsion, in order to meet the challenge of intractable multi-fins coordination and the complex hydrodynamic model. The system overview of the BUV, named RoboDact, with two flexible long fins and a double-joint fishtail as hybrid propulsion, is introduced. After that, the locomotion control problem is modeled as a Markov decision process (MDP) to be solved. Therefore, the locomotion control method based on soft actor-critic (SAC, a novel DRL algorithm) is proposed. The simulation environment is established based on the kinetic model for interaction. Finally, the feasibility and effectiveness of the proposed control method is demonstrated after extensive simulations. It will provide rich insights into the coordination control of biomimetic underwater vehicles.","","978-1-6654-3678-6","10.1109/RCAR52367.2021.9517392","National Natural Science Foundation of China(grant numbers:U1713222,62073316,U1806204); Youth Innovation Promotion Association CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9517392","","Trajectory tracking;Biological system modeling;Robot kinematics;Velocity control;Process control;Reinforcement learning;Propulsion","biomimetics;control engineering computing;deep learning (artificial intelligence);hydrodynamics;marine robots;Markov processes;mobile robots;robot kinematics;underwater vehicles;vehicle dynamics","locomotion control;DRL;kinetic model;coordination control;hybrid propulsion biomimetic underwater vehicle;complex hydrodynamic model;flexible long fins;double-joint fishtail;deep reinforcement learning;hybrid propulsion BUV;Markov decision process;SAC;MDP;RoboDact","","4","","11","IEEE","31 Aug 2021","","","IEEE","IEEE Conferences"
"Adaptive Natural Policy Gradient in Reinforcement Learning","D. Li; Z. Qiao; T. Song; Q. Jin","Institute of Automation, Beijing University of Chemical Technology, Beijing; Institute of Automation, Beijing University of Chemical Technology, Beijing; Institute of Automation, Beijing University of Chemical Technology, Beijing; Institute of Automation, Beijing University of Chemical Technology, Beijing","2018 IEEE 7th Data Driven Control and Learning Systems Conference (DDCLS)","1 Nov 2018","2018","","","605","610","In recent years, the policy gradient method in intensive learning has attracted wide attention with its good convergence performance. At the same time, regulation of hyper parameters is also a matter of concern. Based on the advantages of Actor-Critic structure (AC), the Natural-Gradient Actor-Critic algorithm (NAC) in the discount model is studied in this article. Then the Natural-Gradient Actor-Critic with ADADELTA (A-NAC) algorithm is proposed .The use of ADADELTA is adapted to adjust the learning rate in the actor network, and further improves the convergence speed of the NAC algorithm. Simulation results show that NAC/A-NAC have better learning efficiency and faster convergence rate than regular gradient AC methods.","","978-1-5386-2618-4","10.1109/DDCLS.2018.8515994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8515994","Actor-Critic;natural gradient;adadelta;adaptive;value function approximation;reinforcement learning.","Convergence;Approximation algorithms;Gradient methods;Function approximation;Learning systems;Machine learning algorithms","gradient methods;learning (artificial intelligence)","reinforcement learning;ADADELTA algorithm;convergence speed;learning efficiency;natural policy gradient;natural-gradient actor-critic algorithm;natural-gradient actor-critic with ADADELTA;NAC-A-NAC;gradient AC methods","","4","","26","IEEE","1 Nov 2018","","","IEEE","IEEE Conferences"
"Temporal-Spatial Causal Interpretations for Vision-Based Reinforcement Learning","W. Shi; G. Huang; S. Song; C. Wu","Department of Automation / Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation / Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation / Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation / Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","7 Nov 2022","2022","44","12","10222","10235","Deep reinforcement learning (RL) agents are becoming increasingly proficient in a range of complex control tasks. However, the agent’s behavior is usually difficult to interpret due to the introduction of black-box function, making it difficult to acquire the trust of users. Although there have been some interesting interpretation methods for vision-based RL, most of them cannot uncover temporal causal information, raising questions about their reliability. To address this problem, we present a temporal-spatial causal interpretation (TSCI) model to understand the agent’s long-term behavior, which is essential for sequential decision-making. TSCI model builds on the formulation of temporal causality, which reflects the temporal causal relations between sequential observations and decisions of RL agent. Then a separate causal discovery network is employed to identify temporal-spatial causal features, which are constrained to satisfy the temporal causality. TSCI model is applicable to recurrent agents and can be used to discover causal features with high efficiency once trained. The empirical results show that TSCI model can produce high-resolution and sharp attention masks to highlight task-relevant temporal-spatial information that constitutes most evidence about how vision-based RL agents make sequential decisions. In addition, we further demonstrate that our method is able to provide valuable causal interpretations for vision-based RL agents from the temporal perspective.","1939-3539","","10.1109/TPAMI.2021.3133717","National Science and Technology Major Project(grant numbers:2018AAA0100701); Major Research and Development Project of Guangdong Province(grant numbers:2020B1111500002); National Natural Science Foundation of China(grant numbers:61906106,62022048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645253","Reinforcement learning;markov decision process;interpretability;attention map;temporal causality","Adaptation models;Reliability;Decision making;Perturbation methods;Visualization;Task analysis;Feature extraction","causality;computer vision;decision making;deep learning (artificial intelligence);multi-agent systems;reinforcement learning","agent long-term behavior;attention masks;black-box function;causal discovery network;deep reinforcement learning agents;recurrent agents;sequential decision-making;sequential observations;task-relevant temporal-spatial information;temporal causal relations;temporal causality;temporal-spatial causal interpretations;TSCI;vision-based reinforcement learning;vision-based RL agents","Reproducibility of Results;Algorithms;Reinforcement, Psychology;Attention;Models, Theoretical","3","","69","IEEE","9 Dec 2021","","","IEEE","IEEE Journals"
"Health Indicator Construction Based on Multisensors for Intelligent Remaining Useful Life Prediction: A Reinforcement Learning Approach","Z. Peng; X. Huang; D. Tang; Q. Quan","School of Automation and Electrical Engineering, Beihang University, Beijing, China; School of Automation and Electrical Engineering, Beihang University, Beijing, China; School of Automation and Electrical Engineering, Beihang University, Beijing, China; School of Automation and Electrical Engineering, Beihang University, Beijing, China","IEEE Transactions on Instrumentation and Measurement","28 Feb 2023","2023","72","","1","13","Health indicator (HI) representing the latent degradation pattern of engineering systems plays an irreplaceable role in system remaining useful life (RUL) prediction tasks. The HI is often constructed by fusing multiple sensors of the analyzed system and further applied to RUL prediction tasks. However, most existing HI construction methods combine signals without directly considering the following RUL prediction performance, resulting in a limited prediction accuracy based on the constructed HI. Therefore, this article proposes a reinforcement learning (RL)-based approach to construct HI based on multisensors, to directly link HI construction and the RUL prediction task. The HI construction problem is then transformed into leading an RL agent to automatically learn to find a combination rule of sensors with the most accurate predicted RUL result. Moreover, by setting different rewards for the RL agent, unique requirements for intelligent RUL prediction, such as HI being sensitive to a specific life stage, can also be fulfilled, which cannot be achieved by any other HI construction counterparts. Comparison with benchmark HI construction methods is conducted using two different datasets, and the advantages of our proposed approach are revealed.","1557-9662","","10.1109/TIM.2023.3244221","National Natural Science Foundation of China(grant numbers:61973015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10043772","Health indicator (HI);intelligent remaining useful life (RUL) prediction;reinforcement learning (RL)","Degradation;Sensors;Task analysis;Optimization;Predictive models;Sensor systems;Physics","condition monitoring;mechanical engineering computing;multi-agent systems;reinforcement learning;remaining life assessment;sensor fusion","engineering systems;health indicator construction;HI construction;intelligent remaining useful life prediction;intelligent RUL prediction;latent degradation pattern;multisensors;reinforcement learning;RL agent;RUL prediction performance","","3","","31","IEEE","13 Feb 2023","","","IEEE","IEEE Journals"
"MMD-MIX: Value Function Factorisation with Maximum Mean Discrepancy for Cooperative Multi-Agent Reinforcement Learning","Z. Xu; D. Li; Y. Bai; G. Fan","Fusion Innovation Center, Institute of Automation, Chinese Academy of Sciences School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Fusion Innovation Center, Institute of Automation, Chinese Academy of Sciences School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Fusion Innovation Center, Institute of Automation, Chinese Academy of Sciences School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Fusion Innovation Center, Institute of Automation, Chinese Academy of Sciences School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China","2021 International Joint Conference on Neural Networks (IJCNN)","20 Sep 2021","2021","","","1","7","In the real world, many tasks require multiple agents to cooperate with each other under the condition of local observations. To solve such problems, many multi-agent reinforcement learning methods based on Centralized Training with Decentralized Execution have been proposed. One representative class of work is value decomposition, which decomposes the global joint Q-value Qjtinto individual Q-values Qa to guide individuals' behaviors, e.g. VDN (Value-Decomposition Networks) and QMIX. However, these baselines often ignore the randomness in the situation. We propose MMD-MIX, a method that combines distributional reinforcement learning and value decomposition to alleviate the above weaknesses. Besides, to improve data sampling efficiency, we were inspired by REM (Random Ensemble Mixture) which is a robust RL algorithm to explicitly introduce randomness into the MMD-MIX. The experiments demonstrate that MMD-MIX outperforms prior baselines in the StarCraft Multi-Agent Challenge (SMAC) environment.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9533636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533636","Multi-Agent System;Distributional Reinforcement Learning;Coordination and Collaboration","Training;Neural networks;Collaboration;Reinforcement learning;Task analysis","learning (artificial intelligence);multi-agent systems","global joint Q-value;value-decomposition networks;distributional reinforcement learning;maximum mean discrepancy;cooperative multiagent reinforcement learning;local observations;centralized training;decentralized execution;value function factorisation;random ensemble mixture;RL algorithm;starcraft multiagent challenge environment;MMD-MIX;Q-values Qa","","3","","30","IEEE","20 Sep 2021","","","IEEE","IEEE Conferences"
"Hierarchical Reinforcement Learning-Based Policy Switching Towards Multi-Scenarios Autonomous Driving","Y. Guo; Q. Zhang; J. Wang; S. Liu","The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2021 International Joint Conference on Neural Networks (IJCNN)","23 Sep 2021","2021","","","1","8","Reinforcement learning has gained a lot of attention and applications in the field of autonomous driving in recent years. However, the actual scenarios of automatic driving applications are often diverse, so the reinforcement learning algorithm using only a single driving strategy is difficult to meet the multiple requirements of efficiency and safety in the multi-scenarios autonomous driving task. To solve this challenge, we propose a hierarchical reinforcement learning structure to learn a unified top-level switching master policy between different driving styles policies. The whole framework uses a bottom-up training manner with diverse reward function designing. Through experimental comparison, our method exceeds the performance of single policy and rule-based switching strategy. Based on this framework, we won the first place in the DAI 2020 Autonomous Driving Workshop single-agent track competition.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9534349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9534349","Autonomous Driving;Hierarchical Reinforcement Learning;Semi-Markov Decision Process","Training;Conferences;Neural networks;Switches;Reinforcement learning;Safety;Task analysis","learning (artificial intelligence);multi-agent systems;traffic engineering computing","DAI 2020 Autonomous Driving Workshop single-agent track competition;automatic driving applications;single driving strategy;multiscenarios autonomous driving task;hierarchical reinforcement learning structure;hierarchical reinforcement learning-based policy;driving styles policy;unified top-level switching master policy;reward function","","3","","34","IEEE","23 Sep 2021","","","IEEE","IEEE Conferences"
"State-Temporal Compression in Reinforcement Learning With the Reward-Restricted Geodesic Metric","S. Guo; Q. Yan; X. Su; X. Hu; F. Chen","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Institute for Artificial Intelligence, Beijing National Research Center for Information Science and Technology, State Key Laboratory of Intelligent Technology and Systems, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","4 Aug 2022","2022","44","9","5572","5589","It is difficult to solve complex tasks that involve large state spaces and long-term decision processes by reinforcement learning (RL) algorithms. A common and promising method to address this challenge is to compress a large RL problem into a small one. Towards this goal, the compression should be state-temporal and optimality-preserving (i.e., the optimal policy of the compressed problem should correspond to that of the uncompressed problem). In this paper, we propose a reward-restricted geodesic (RRG) metric, which can be learned by a neural network, to perform state-temporal compression in RL. We prove that compression based on the RRG metric is approximately optimality-preserving for the raw RL problem endowed with temporally abstract actions. With this compression, we design an RRG metric-based reinforcement learning (RRG-RL) algorithm to solve complex tasks. Experiments in both discrete (2D Minecraft) and continuous (Doom) environments demonstrated the superiority of our method over existing RL approaches.","1939-3539","","10.1109/TPAMI.2021.3069005","National Natural Science Foundation of China(grant numbers:61671266,61836004,U19B2034,61836014); Tsinghua-Guoqiang research program(grant numbers:2019GQG0006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387144","Semi-Markov decision process (SMDP);reward-restricted geodesic (RRG) metric;option;state compression;state-temporal compression;reinforcement learning (RL)","Measurement;Task analysis;Reinforcement learning;Neural networks;Time-domain analysis;Semiconductor device measurement;Mathematical model","reinforcement learning","compressed problem;long-term decision processes;optimality-preserving;raw RL problem;reinforcement learning algorithms;reward-restricted geodesic metric;RRG metric-based reinforcement learning algorithm;state spaces processes;state-temporal compression;uncompressed problem","","2","","63","IEEE","25 Mar 2021","","","IEEE","IEEE Journals"
"Cooperative Multi-Agent Deep Reinforcement Learning with Counterfactual Reward","K. Shao; Y. Zhu; Z. Tang; D. Zhao","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2020 International Joint Conference on Neural Networks (IJCNN)","28 Sep 2020","2020","","","1","8","In partially observable fully cooperative games, agents generally tend to maximize global rewards with joint actions, so it is difficult for each agent to deduce their own contribution. To address this credit assignment problem, we propose a multi-agent reinforcement learning algorithm with counterfactual reward mechanism, which is termed as CoRe algorithm. CoRe computes the global reward difference in condition that the agent does not take its actual action but takes other actions, while other agents fix their actual actions. This approach can determine each agent's contribution for the global reward. We evaluate CoRe in a simplified Pig Chase game with a decentralised Deep Q Network (DQN) framework. The proposed method helps agents learn end-to-end collaborative behaviors. Compared with other DQN variants with global reward, CoRe significantly improves learning efficiency and achieves better results. In addition, CoRe shows excellent performances in various size game environments.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9207169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207169","reinforcement learning;deep reinforcement learning;cooperative games;counterfactual reward","Games;Learning (artificial intelligence);Machine learning;Collaboration;Training;Task analysis;Multi-agent systems","computer games;game theory;learning (artificial intelligence);multi-agent systems","actual action;cooperative multiagent deep reinforcement learning;partially observable fully cooperative games;joint actions;credit assignment problem;multiagent reinforcement;counterfactual reward mechanism;CoRe algorithm;global reward difference;simplified pig chase game;decentralised deep Q network framework;DQN variants;learning efficiency;game environments","","2","","35","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
"Deep-Reinforcement-Learning-Based Multitarget Coverage With Connectivity Guaranteed","S. Wu; Z. Pu; T. Qiu; J. Yi; T. Zhang","Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Industrial Informatics","9 Nov 2022","2023","19","1","121","132","Deriving a distributed, time-efficient, and connectivity-guaranteed coverage policy in multitarget environment poses huge challenges for a multirobot team with limited coverage and limited communication. In particular, the robot team needs to cover multiple targets while preserving connectivity. In this article, a novel deep-reinforcement-learning-based approach is proposed to take both multitarget coverage and connectivity preservation into account simultaneously, which consists of four parts: a hierarchical observation attention representation, an interaction attention representation, a two-stage policy learning, and a connectivity-guaranteed policy filtering. The hierarchical observation attention representation is designed for each robot to extract the latent features of the relations from its neighboring robots and the targets. To promote the cooperation behavior among the robots, the interaction attention representation is designed for each robot to aggregate information from its neighboring robots. Moreover, to speed up the training process and improve the performance of the learned policy, the two-stage policy learning is presented using two reward functions based on algebraic connectivity and coverage rate. Furthermore, the learned policy is filtered to strictly guarantee the connectivity based on a model of connectivity maintenance. Finally, the effectiveness of the proposed method is validated by numerous simulations. Besides, our method is further deployed to an experimental platform based on quadrotor unmanned aerial vehicles and omnidirectional vehicles. The experiments illustrate the practicability of the proposed method.","1941-0050","","10.1109/TII.2022.3160629","National Key Research and Development Program of China(grant numbers:2018AAA0102404); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDA27030000); National Natural Science Foundation of China(grant numbers:62073323); External Cooperation Key Project of the Chinese Academy of Sciences(grant numbers:173211KYSB20200002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9738445","Connectivity maintenance;deep reinforcement learning (DRL);multirobot system;multitarget coverage","Robots;Optimization;Maintenance engineering;Task analysis;Informatics;Topology;Reinforcement learning","autonomous aerial vehicles;learning (artificial intelligence);mobile robots;multi-robot systems;object detection;path planning;remotely operated vehicles;target tracking","algebraic connectivity;connectivity guaranteed;connectivity maintenance;connectivity preservation;connectivity-guaranteed coverage policy;connectivity-guaranteed policy filtering;coverage rate;deep-reinforcement-learning-based approach;deep-reinforcement-learning-based multitarget coverage;distributed time-efficient;hierarchical observation attention representation;interaction attention representation;learned policy;limited communication;multiple targets;multirobot team;multitarget environment;neighboring robots;robot team;two-stage policy learning","","2","","28","IEEE","18 Mar 2022","","","IEEE","IEEE Journals"
"A Knowledge Driven Dialogue Model With Reinforcement Learning","Y. Jia; G. Min; C. Xu; X. Li; D. Zhang","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Beijing Key Laboratory of Knowledge Engineering for Materials Science, Beijing, China","IEEE Access","24 Jul 2020","2020","8","","131741","131749","In recent decades, many researchers pay a lot of attention on generating informative responses in end-to-end neural dialogue systems. In order to output the responses with knowledge and fact, many works leverage external knowledge to guide the process of response generation. However, human dialogue is not a simple sequence to sequence task but a process heavily relying on their background knowledge about the topic. Thus, the key of generating informative responses is leveraging the appropriate knowledge associated with current topic. This paper focus on addressing incorporating the appropriate knowledge in response generation. We adopt the reinforcement learning to select the most proper knowledge as the input information of the response generation part. Then we design an end-to-end dialogue model consisting of the knowledge decision part and the response generation part. The proposed model is able to effectively complete the knowledge driven dialogue task with specific topic. Our experiments clearly demonstrate the superior performance of our model over other baselines.","2169-3536","","10.1109/ACCESS.2020.2993924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091129","Dialogue model;policy gradient;knowledge graph;transformer network","Learning (artificial intelligence);Task analysis;Knowledge engineering;Decision making;Information retrieval;Computational modeling;Cognition","interactive systems;learning (artificial intelligence);neural nets","knowledge driven dialogue task;response generation;end-to-end neural dialogue systems;informative responses;reinforcement learning","","1","","15","CCBY","11 May 2020","","","IEEE","IEEE Journals"
"Implementation of Transferring Reinforcement Learning for DC–DC Buck Converter Control via Duty Ratio Mapping","C. Cui; T. Yang; Y. Dai; C. Zhang; Q. Xu","Intelligent Autonomous Systems Laboratory, College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; Intelligent Autonomous Systems Laboratory, College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; Intelligent Autonomous Systems Laboratory, College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; Intelligent Autonomous Systems Laboratory, College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; Electric Power and Energy Systems Division, KTH Royal Institute of Technology, Stockholm, Sweden","IEEE Transactions on Industrial Electronics","26 Jan 2023","2023","70","6","6141","6150","The reinforcement learning (RL) control approach with application to power electronics systems has become an emerging topic, while the sim-to-real issue remains a challenging problem as very few results can be referred to in the literature. Indeed, due to the inevitable mismatch between simulation models and real-life systems, offline-trained RL control strategies may sustain unexpected hurdles in practical implementation during the transfer procedure. In this article, a transfer methodology via a delicately designed duty ratio mapping is proposed for a dc–dc buck converter. Then, a detailed sim-to-real process is presented to enable the implementation of a model-free deep reinforcement learning controller. As the main contribution of this article, the proposed methodology is able to endow the control system to achieve: 1) voltage regulation and 2) adaptability and optimization abilities in the presence of uncertain circuit parameters and various working conditions. The feasibility and efficacy of the proposed methodology are demonstrated by comparative experimental studies.","1557-9948","","10.1109/TIE.2022.3192676","National Natural Science Foundation of China(grant numbers:51607111,62173221); Shanghai Rising-Star Program(grant numbers:20QA1404000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9841427","DC–DC buck converter;deep reinforcement learning (DRL);duty ratio mapping (DRM);practical implementation","Computational modeling;Buck converters;Microgrids;Voltage control;Adaptation models;Control systems;Load modeling","DC-DC power convertors;power electronics;reinforcement learning;voltage control","DC-DC buck converter control;duty ratio mapping;model-free deep reinforcement learning controller;offline-trained RL control strategies;power electronics systems;real-life systems;sim-to-real process;simulation models;transfer procedure;voltage regulation","","1","","35","IEEE","26 Jul 2022","","","IEEE","IEEE Journals"
"Human-in-the-Loop Reinforcement Learning in Continuous-Action Space","B. Luo; Z. Wu; F. Zhou; B. -C. Wang","School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","10","Human-in-the-loop for reinforcement learning (RL) is usually employed to overcome the challenge of sample inefficiency, in which the human expert provides advice for the agent when necessary. The current human-in-the-loop RL (HRL) results mainly focus on discrete action space. In this article, we propose a  $Q$  value-dependent policy (QDP)-based HRL (QDP-HRL) algorithm for continuous action space. Considering the cognitive costs of human monitoring, the human expert only selectively gives advice in the early stage of agent learning, where the agent implements human-advised action instead. The QDP framework is adapted to the twin delayed deep deterministic policy gradient algorithm (TD3) in this article for the convenience of comparison with the state-of-the-art TD3. Specifically, the human expert in the QDP-HRL considers giving advice in the case that the difference between the twin  $Q$ -networks’ output exceeds the maximum difference in the current queue. Moreover, to guide the update of the critic network, the advantage loss function is developed using expert experience and agent policy, which provides the learning direction for the QDP-HRL algorithm to some extent. To verify the effectiveness of QDP-HRL, the experiments are conducted on several continuous action space tasks in the OpenAI gym environment, and the results demonstrate that QDP-HRL greatly improves learning speed and performance.","2162-2388","","10.1109/TNNLS.2023.3289315","National Natural Science Foundation of China(grant numbers:62022094,61873350); Zhejiang Laboratory(grant numbers:2021NB0AB01); Hunan Provincial Natural Science Foundation of China(grant numbers:2020JJ2049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10175618","Continuous action space;human-in-the-loop;reinforcement learning (RL)","Training;Human in the loop;Reinforcement learning;Costs;Uncertainty;Training data;Task analysis","","","","1","","","IEEE","7 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Deep Reinforcement Learning for Real-Time Energy Management in Smart Home","G. Wei; M. Chi; Z. -W. Liu; M. Ge; C. Li; X. Liu","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Mechanical Engineering and Electronic Information, China University of Geosciences, Wuhan, China; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, NSW, Australia; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","IEEE Systems Journal","8 Jun 2023","2023","17","2","2489","2499","Energy management in the smart home can help reduce residential energy costs by scheduling various energy consumption activities. However, accurately modeling factors, such as user behavior, renewable power generation, weather conditions, and real-time electricity prices can be challenging, making the design of an efficient energy management strategy difficult. This article proposes a real-time energy management algorithm based on deep reinforcement learning (DRL) for smart homes equipped with rooftop photovoltaics, energy storage systems, and smart appliances. The algorithm aims to minimize the energy cost while ensuring user comfort. A policy network that can output both discrete and continuous actions is designed to generate actions for different types of devices in a smart home. The proposed DRL-agent is trained using a proximal policy optimization approach with historical data and is used for real-time scheduling. Finally, simulations based on real-world data demonstrate the effectiveness and robustness of the proposed algorithm.","1937-9234","","10.1109/JSYST.2023.3247592","National Natural Science Foundation of China(grant numbers:62222205,61973133,61972170,62073301); Natural Science Foundation of Hubei Province of China(grant numbers:2022CFA052,2021CFB343); Australian Research Council(grant numbers:DE210100274); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10066193","Deep reinforcement learning (DRL);energy storage system (ESS);home energy management;proximal policy optimization (PPO)","Smart homes;HVAC;Costs;Home appliances;Real-time systems;Energy management;Task analysis","building management systems;deep learning (artificial intelligence);domestic appliances;energy consumption;energy management systems;energy storage;learning (artificial intelligence);optimisation;power engineering computing;pricing;reinforcement learning","accurately modeling factors;deep reinforcement learning;efficient energy management strategy;energy consumption activities;energy cost;energy storage systems;real-time electricity prices;real-time energy management algorithm;real-time scheduling;renewable power generation;residential energy costs;smart appliances;smart home","","1","","36","IEEE","10 Mar 2023","","","IEEE","IEEE Journals"
"Peer Incentive Reinforcement Learning for Cooperative Multi-Agent Games","T. Zhang; Z. Liu; Z. Pu; J. Yi","Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Games","","2022","PP","99","1","14","Social learning, especially social incentives, is extremely important for humans to achieve a high level of coordination. Inspired by this, we introduce this concept into cooperative multi-agent reinforcement learning (MARL), to implicitly address the credit assignment problem and promote the inter-agent direct interactions for cooperations among agents in cooperative multi-agent games. In this paper, we propose a novel Intrinsic Reward method with Peer Incentives (IRPI) based on actor-critic policy gradient. This method can enable agents to incentivize each other for their cooperations through using causal influence among them. Specifically, a novel intrinsic reward mechanism is innovatively designed to empower each agent the ability to give positive or negative rewards to other peer agents' actions through considering the causal influence of the other agents on it. The mechanism is realized by a feed-forward neural network through utilizing causal influence between the agents. The causal influence of one agent on another is inferred via counterfactual reasoning using the joint action-value function in MARL. The quality of the influence is assessed via counterfactual reasoning using the individual value function in MARL. Simulations are carried out on two popular multi-agent game testbeds: Starcraft II Micromanagement and Multi-Agent Particle Environments. Simulational results demonstrate that the proposed IRPI can enhance cooperations among the agents to achieve better performance compared with a number of state-of-the-art MARL methods in a variety of cooperative multi-agent games.","2475-1510","","10.1109/TG.2022.3196925","National Key Research and Development Program of China(grant numbers:2018AAA0102402); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDA27030204); External Cooperation Key Project of Chinese Academy Sciences(grant numbers:173211KYSB20200002); Science and Technology Development Fund of Macau(grant numbers:0025/2019/AKP); National Natural Science Foundation of China(grant numbers:62073323); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851854","Cooperative multi-agent games;multi-agent reinforcement learning;intrinsic reward;Starcraft II Micromanagement","Games;Cognition;Training;Neural networks;Testing;Task analysis;Sports","","","","1","","","IEEE","8 Aug 2022","","","IEEE","IEEE Early Access Articles"
"A Deep Reinforcement Learning Voltage Control Method for Distribution Network","P. Li; J. Shen; M. Yin; Y. Zhang; Z. Wu","School of Automation, Nanjing University of Science & Technology, Nanjing, China; School of Automation, Nanjing University of Science & Technology, Nanjing, China; School of Automation, Nanjing University of Science & Technology, Nanjing, China; School of Automation, Nanjing University of Science & Technology, Nanjing, China; School of Electrical Engineering, Southeast University, Nanjing, China","2022 IEEE 5th International Electrical and Energy Conference (CIEEC)","11 Aug 2022","2022","","","2283","2288","Aiming at the distributed voltage control optimally in real-time for distribution network with high penetration of renewable energy sources, a deep reinforcement learning voltage control (DRLVC) method is proposed in this paper. Based on the network partition, the voltage control method with coordinated optimizing the reactive power output of PV inverter and active power output of energy storage system (ESS) is formulated as the Markov decision process for each sub-network. Then, with information communication, the distributed voltage control model is formulated as the multi-agent deep reinforcement learning model. And the improved multi-agent depth determination strategy gradient (MADDPG) algorithm is employed to solve the constructed model efficiently. Finally, numerical simulations are conducted on the 322-bus test network to show the effectiveness of the proposed DRLVC method.","","978-1-6654-1104-2","10.1109/CIEEC54735.2022.9846033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9846033","voltage control;distribution network;mult-agents;MADDPG","Renewable energy sources;Voltage fluctuations;Reinforcement learning;Distribution networks;Markov processes;Numerical simulation;Real-time systems","deep learning (artificial intelligence);distribution networks;invertors;Markov processes;multi-agent systems;optimisation;photovoltaic power systems;power engineering computing;reactive power;reinforcement learning;renewable energy sources;voltage control","distribution network;renewable energy sources;deep reinforcement learning voltage control method;network partition;coordinated optimizing;reactive power output;active power output;energy storage system;distributed voltage control model;multiagent deep reinforcement learning model;multiagent depth determination strategy gradient algorithm;322-bus test network;DRLVC method;PV inverter;Markov decision process;information communication;multiagent depth determination strategy gradient;MADDPG algorithm","","1","","13","IEEE","11 Aug 2022","","","IEEE","IEEE Conferences"
"Over-the-Horizon Air Combat Environment Modeling and Deep Reinforcement Learning Application","A. Wang; S. Zhao; Z. Shi; J. Wang","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2022 4th International Conference on Data-driven Optimization of Complex Systems (DOCS)","5 Dec 2022","2022","","","1","6","As we all know, over-the-horizon air combat has become one of the important fight forms that determine the trend of modern warfare. The biggest challenge in the confrontation process is how to make aircrafts cooperative-decision to lock, launch and avoid operations. To this end, this paper investigates the deep reinforcement learning application on the over-the-horizon air combat environment to enhance the ability of multi-aircraft cooperative decision-making and intelligent optimization. First, a novel over-the-horizon air combat environment is constructed as a training environment for deep reinforcement learning, which could provide an easy-to-calculate simulation environment with higher precision. Then, we propose the proximal policy optimization combined with the long short-term memory network to deal with incomplete information and realize intelligent decision optimization at the same time. Finally, the effectiveness of the proposed algorithm is verified by simulation experiments.","","978-1-6654-5982-2","10.1109/DOCS55193.2022.9967482","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9967482","Air combat;Reinforcement learning;Proximal policy optimization;Long short term memory;Self-play","Deep learning;Training;Missiles;Atmospheric modeling;Decision making;Reinforcement learning;Market research","aerospace computing;decision making;deep learning (artificial intelligence);knowledge based systems;military aircraft;military computing;multi-agent systems;optimisation;recurrent neural nets;reinforcement learning","confrontation process;deep reinforcement learning application;intelligent decision optimization;long short-term memory network;modern warfare;multiaircraft cooperative decision-making;over-the-horizon air combat environment modeling;proximal policy optimization","","1","","13","IEEE","5 Dec 2022","","","IEEE","IEEE Conferences"
