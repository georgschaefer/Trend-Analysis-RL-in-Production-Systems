@article{WANJERKHEDE201427,
title = {Reinforcement learning and dopamine in the striatum: A modeling perspective},
journal = {Neurocomputing},
volume = {138},
pages = {27-40},
year = {2014},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2013.02.061},
url = {https://www.sciencedirect.com/science/article/pii/S0925231214003439},
author = {Shesharao M. Wanjerkhede and Raju S. Bapi and Vithal D. Mytri},
keywords = {Actor–critic, Basal ganglia, Dopamine, LTP, LTD},
abstract = {The recent research evidences show that the dopamine (DA) system in the brain is involved in various functions like reward-related learning, exploration, preparation, and execution in goal directed behavior. It is suggested that dopaminergic neurons provide a prediction error akin to the error computed in the temporal difference learning (TDL) models of reinforcement learning (RL). Houk et al. (1995) [26] proposed a biochemical model in the spine head of neurons at the striatum in the basal ganglia which generates and uses neural signals to predict reinforcement. The model explains how the DA neurons are able to predict reinforcement and how the output from these neurons might then be used to reinforce the behaviors that lead to primary reinforcement. They proposed a scheme drawing that parallels between actor–critic architecture and dopamine activity in the basal ganglia. Houk et al. (1995) [26] also proposed a biochemical model of interactions between protein molecules which supports learning earlier predictions of reinforcement in the spine head of medium spiny neurons at the striatum. However, Houk׳s proposed cellular model fails to account for the time delay between the dopaminergic and glutamatergic activity required for reward-related learning and also fails to explain the ‘eligibility trace’ condition needed in delayed tasks of associative conditioning in which a memory trace of the antecedent signal is needed at the time of a succeeding reward. In this article, we review various models of RL with an emphasis on the cellular models of RL. In particular, we emphasize biochemical models of RL, and point out the future directions.}
}
@article{LI2022111903,
title = {Reinforcement learning of room temperature set-point of thermal storage air-conditioning system with demand response},
journal = {Energy and Buildings},
volume = {259},
pages = {111903},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.111903},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822000743},
author = {Zeyang Li and Zhe Sun and Qinglong Meng and Yuxiang Wang and Yang Li},
keywords = {Demand response, Air-conditioning, Reinforcement learning, Active thermal energy storage, Temperature set-point},
abstract = {Demand response (DR) is an effective means to reduce peak loads and enhance grid stability. Heating, ventilation, and air-conditioning (HVAC) systems have potential energy transfer characteristics and can be used as a typical flexible load for building DR. The HVAC thermostat settings are the key parameters that directly affect the elasticity of building DR and reflect the willingness of users to participate in DR. For air-conditioning DR control, the conventional method to determine thermostat settings is model-dependent, while reinforcement learning (RL) is a model-free, adaptive continuous control algorithm. Taking the proximal policy optimization RL algorithms, a neural network is used to construct a strategic framework to obtain discrete control actions, that is, thermostat settings, and a new objective function truncation method is adopted to limit the update step size and enhance the robustness of the algorithm. Thus, a TRNSYS and MATLAB joint simulation platform for the thermal storage air-conditioning system was built. This study formulated a DR strategy based on time-of-use electricity prices, which considers factors, such as environment, thermal comfort, and energy consumption; and the proposed RL algorithm is used to learn the thermostat settings in DR time. The results show that the proposed RL algorithm could realize the temperature set-point control, which saved 9.17% of the operating cost compared with a non-thermal storage air-conditioning system with a constant set-point.}
}
@article{SAMMA2016276,
title = {A new Reinforcement Learning-based Memetic Particle Swarm Optimizer},
journal = {Applied Soft Computing},
volume = {43},
pages = {276-297},
year = {2016},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2016.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1568494616000132},
author = {Hussein Samma and Chee Peng Lim and Junita {Mohamad Saleh}},
keywords = {Memetic algorithm, Particle Swarm Optimization, Reinforcement learning, Local search},
abstract = {Developing an effective memetic algorithm that integrates the Particle Swarm Optimization (PSO) algorithm and a local search method is a difficult task. The challenging issues include when the local search method should be called, the frequency of calling the local search method, as well as which particle should undergo the local search operations. Motivated by this challenge, we introduce a new Reinforcement Learning-based Memetic Particle Swarm Optimization (RLMPSO) model. Each particle is subject to five operations under the control of the Reinforcement Learning (RL) algorithm, i.e. exploration, convergence, high-jump, low-jump, and fine-tuning. These operations are executed by the particle according to the action generated by the RL algorithm. The proposed RLMPSO model is evaluated using four uni-modal and multi-modal benchmark problems, six composite benchmark problems, five shifted and rotated benchmark problems, as well as two benchmark application problems. The experimental results show that RLMPSO is useful, and it outperforms a number of state-of-the-art PSO-based algorithms.}
}
@article{ERDEN2008199,
title = {Free gait generation with reinforcement learning for a six-legged robot},
journal = {Robotics and Autonomous Systems},
volume = {56},
number = {3},
pages = {199-212},
year = {2008},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2007.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0921889007000991},
author = {Mustafa Suphi Erden and Kemal Leblebicioğlu},
keywords = {Locomotion, Walking, Six-legged robot, Reinforcement learning, Free gait},
abstract = {In this paper the problem of free gait generation and adaptability with reinforcement learning are addressed for a six-legged robot. Using the developed free gait generation algorithm the robot maintains to generate stable gaits according to the commanded velocity. The reinforcement learning scheme incorporated into the free gait generation makes the robot choose more stable states and develop a continuous walking pattern with a larger average stability margin. While walking in normal conditions with no external effects causing unstability, the robot is guaranteed to have stable walk, and the reinforcement learning only improves the stability. The adaptability of the learning scheme is tested also for the abnormal case of deficiency in one of the rear-legs. The robot gets a negative reinforcement when it falls, and a positive reinforcement when a stable transition is achieved. In this way the robot learns to achieve a continuous pattern of stable walk with five legs. The developed free gait generation with reinforcement learning is applied in real-time on the actual robot both for normal walking with different speeds and learning of five-legged walking in the abnormal case.}
}
@article{WANG2021116722,
title = {Surrogate model enabled deep reinforcement learning for hybrid energy community operation},
journal = {Applied Energy},
volume = {289},
pages = {116722},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116722},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921002403},
author = {Xiaodi Wang and Youbo Liu and Junbo Zhao and Chang Liu and Junyong Liu and Jinyue Yan},
keywords = {P2P transaction, Community market, Deep reinforcement learning, Optimization, Transactive control},
abstract = {Local peer-to-peer (P2P) transactions in a community are becoming a trend for energy integration and management. The introduction of P2P trading scheme requires comprehensive consideration on various aspects, such as peer privacy, computational efficiency, network security and operational economics. This paper provides a novel hybrid community P2P market framework for multi-energy systems, where a data-driven market surrogate model-enabled deep reinforcement learning (DRL) method is proposed to facilitate P2P transaction within technical constraints of the community delivery networks. Specifically, to achieve privacy protection, a market surrogate model based on deep belief network (DBN) is developed to characterize P2P transaction behaviors of peers in the community without disclosing their private data. Since the energy inputs and outputs of peers are highly correlated with real time signals of retail energy prices, the data-driven market surrogate model is further integrated into the DRL-enabled optimization model of a community agent (CA) for on-line retail energy price generation. Particularly, by integrating network constraints into DRL reward function, the P2P transaction scheme among community peers under specific retail energy price is guaranteed to proceed within a feasible region of community networks. Numerical results indicate that the proposed market framework can achieve 7.6% energy cost saving for community peers over none P2P transaction scheme while increase 284.4$ economic benefits for CA in one day over other comparison algorithms. This study provides an effective prototype to supplement existing P2P markets.}
}
@article{CEVALLOSM2023110016,
title = {Deep Reinforcement Learning for intrusion detection in Internet of Things: Best practices, lessons learnt, and open challenges},
journal = {Computer Networks},
volume = {236},
pages = {110016},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110016},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623004619},
author = {Jesús F. {Cevallos M.} and Alessandra Rizzardi and Sabrina Sicari and Alberto {Coen Porisini}},
keywords = {Deep Reinforcement Learning, Intrusion Detection Systems, Internet of Things},
abstract = {The Internet of Things (IoT) scenario places important challenges even for deep learning-based intrusion detection systems. IoTs are highly heterogeneous networks in which multiple types of nodes and connections between them proliferate at a fast pace. From a deep learning perspective, such complexity translates into dynamic feature spaces where the extraction of semantic patterns and correlations among features may require sophisticated inductive biases to be learnt by gradient-based techniques. The research community has recently suggested using Deep Reinforcement Learning (DRL) as a potent approach to effectively identify cyber-threat attempts in IoTs. DRL consists of a Markov Decision Process-based meta-model that permits solving high-dimensional combinatorial optimization problems where differentiable supervisory signals may be absent. For this reason, multiple intelligent intrusion detection systems have been proposed for the IoT environment where high-level requirements are been pursued alongside the detection accuracy. These goals are related to optimizing the computational overhead, reducing power consumption at the edge, and preserving the privacy of sensitive information, among others. This survey offers a clear bird’s eye view of the most recent design choices for DRL-based intrusion detection systems with a focus on the specific context of IoT. Our aim is not only to offer an exhaustive taxonomy of design alternatives made by DRL practitioners in the field of Intrusion detection, but also to discuss the advantages and the effective deployment of each setting concerning real IoT environments. We hope this work would guide the researchers interested in Intrusion Detection for IoTs to establish solid criteria for the most effective usage of Deep Reinforcement Learning in their future work.}
}
@article{BORA2019688,
title = {Multi-objective optimization of the environmental-economic dispatch with reinforcement learning based on non-dominated sorting genetic algorithm},
journal = {Applied Thermal Engineering},
volume = {146},
pages = {688-700},
year = {2019},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2018.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S1359431118349317},
author = {Teodoro Cardoso Bora and Viviana Cocco Mariani and Leandro dos Santos Coelho},
keywords = {Environmental-economic dispatch, Power systems, Multi-objective optimization, Reinforcement learning, Genetic algorithm},
abstract = {This paper presents an improved non-dominated sorting genetic algorithm II (NSGA-II) approach incorporating a parameter-free self-tuning by reinforcement learning technique called learner non-dominated sorting genetic algorithm (NSGA-RL) for the multi-objective optimization of the environmental/economic dispatch (EED) problem. To evaluate the performance features, the proposed NSGA-RL approach is investigated on ten multi-objective benchmark functions. Besides, to evaluate the effectiveness of the proposed approach, the standard IEEE (Institute of Electrical and Electronics Engineers) of 30-bus network with six generating units (with/without considering losses) is adopted, with operating cost (fuel cost) and pollutant emission as two conflicting objectives to be optimized at the same time. In comparison to literature, it was observed that the proposed approach provides a better satisfaction level in conflicting objectives with well distributed Pareto front, in comparison with the classical NSGA-II method, and to other existing methods reported in the literature. The NSGA-RL was found to be comparable to them considering the quality of the solutions obtained, with the advantage of non-time spent for parameters tuning.}
}
@article{BERSINI1992467,
title = {Reinforcement Learning and Recruitment Mechanism for Adaptive Distributed Control},
journal = {IFAC Proceedings Volumes},
volume = {25},
number = {10},
pages = {467-473},
year = {1992},
note = {IFAC Symposium on Artificial Intelligence in Real Time Control 1992, Delft, The Netherlands, 16-18 June 1992},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)50864-2},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017508642},
author = {H. Bersini},
keywords = {Process Control, Reinforcement Learning, Dynamic Programming, Distributed Control, Immune System, Recruitment Mechanism},
abstract = {The work presented in this paper is an attempt to spread further the inspiration gained from the knowledge of biological systems into the field of adaptive control. After the neural controllers and the evolutionary based mechanisms, new hints for the control of complex processes might be derived from other biological domains such as immunology or the study of conditioning learning. The conception of a system equipped with a complex controller, interacting with an uncertain and varying environment, and basing its learning on its own experiences entails quite naturally the integration of a reinforcement learning mechanism. Two learning processes characterized by two different time scales will be introduced, will be connected to their respective biological origins and will be illustrated on the classical cart-pole control problem. These two learning processes are the rapid reinforcement learning and the slower recruitment mechanism.}
}
@article{WENG2020171,
title = {Portfolio trading system of digital currencies: A deep reinforcement learning with multidimensional attention gating mechanism},
journal = {Neurocomputing},
volume = {402},
pages = {171-182},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220305427},
author = {Liguo Weng and Xudong Sun and Min Xia and Jia Liu and Yiqing Xu},
keywords = {Portfolio, Deep-reinforcement learning, Reinforcement learning, Attention gating mechanism},
abstract = {As a hot topic in the financial engineering, the portfolio optimization aims to increase investors’ wealth. In this paper, a portfolio management system based on deep-reinforcement learning is proposed. In contrast to inflexible traditional methods, the proposed system achieves a better trading strategy through Reinforcement learning. The reward signal of Reinforcement learning is updated by action weights from Deep learning networks. Low price, high price and close price constitute the inputs, but the importance of these three features is quite different. Traditional methods and the classical CNN can’t deal with these three features separately, but in our method, a designed depth convolution is proposed to deal with these three features separately. In a virtual currency market, the price rise only occurs in a flash. Traditional methods and CNN networks can’t accurately judge the critical time. In order to solve this problem, a three-dimensional attention gating network is proposed and it gives higher weights on rising moments and assets. Under different market conditions, the proposed system achieves more substantial returns and greatly improves the Sharpe ratios. The short-term risk index of the proposed system is lower than those of the traditional algorithms. Simulation results show that the traditional algorithms (including Best, CRP, PAMR, CWMR and CNN) are unable to perform as well as our approach.}
}
@article{GEDIKLI2022109202,
title = {Deep reinforcement learning based flexible preamble allocation for RAN slicing in 5G networks},
journal = {Computer Networks},
volume = {215},
pages = {109202},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109202},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622002912},
author = {Ahmet Melih Gedikli and Mehmet Koseoglu and Sevil Sen},
keywords = {Deep reinforcement learning, Preamble allocation, Network slicing, 5G, RAN, M2M},
abstract = {One of the most difficult challenges in radio access network slicing occurs in the connection establishment phase where multiple devices use a common random access channel in order to gain access to the network. It is now very well known that random access channel congestion is a serious issue in case of sporadic arrival of machine-to-machine nodes and may result in a significant delay for all nodes. Hence, random access channel resources are also needed to be allocated to different services to enable random access network slicing. In the random access channel procedure, the nodes transmit a selected preamble from a predefined set of preambles. If multiple nodes transmit the same preamble at the same random access channel opportunity, a collision occurs at the eNodeB. To isolate the one service class from others during this phase, one approach is to allocate different preamble subsets to different service classes. This research proposes an adaptive preamble subset allocation method using deep reinforcement learning. The proposed method can distribute preambles to different service classes according to their priority providing virtual isolation for service classes. The results indicate that the proposed mechanism can quickly adapt the preamble allocation according to the changing traffic demands of service classes.}
}
@article{HUANG2020106693,
title = {A fitness landscape ruggedness multiobjective differential evolution algorithm with a reinforcement learning strategy},
journal = {Applied Soft Computing},
volume = {96},
pages = {106693},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106693},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620306311},
author = {Ying Huang and Wei Li and Furong Tian and Xiang Meng},
keywords = {Multiobjective, Differential evolution, Reinforcement learning, Fitness landscape, Search strategy},
abstract = {Optimization is the process of finding and comparing feasible solutions and adopting the best one until no better solution can be found. Because solving real-world problems often involves simulations and multiobjective optimization, the results and solutions of these problems are conceptually different from those of single-objective problems. In single-objective optimization problems, the global optimal solution is the solution that yields the optimal value of the objective function. However, for multiobjective optimization problems, the optimal solutions are Pareto-optimal solutions produced by balancing multiple objective functions. The strategic variables calculated in multiobjective problems produce different effects on the mapping imbalance and the search redundancy in the search space. Therefore, this paper proposes a fitness landscape ruggedness multiobjective differential evolution (LRMODE) algorithm with a reinforcement learning strategy. The proposed algorithm analyses the ruggedness of landscapes using information entropy to estimate whether the local landscape has a unimodal or multimodal topology and then combines the outcome with a reinforcement learning strategy to determine the optimal probability distribution of the algorithm’s search strategy set. The experimental results show that this novel algorithm can ameliorate the problem of search redundancy and search-space mapping imbalances, effectively improving the convergence of the search algorithm during the optimization process.}
}
@incollection{BERSINI1993467,
title = {REINFORCEMENT LEARNING AND RECRUITMENT MECHANISM FOR ADAPTIVE DISTRIBUTED CONTROL},
editor = {H.B. VERBRUGGEN and M.G. RODD},
booktitle = {Artificial Intelligence in Real-Time Control 1992},
publisher = {Pergamon},
address = {Oxford},
pages = {467-473},
year = {1993},
series = {IFAC Symposia Series},
isbn = {978-0-08-041898-8},
doi = {https://doi.org/10.1016/B978-0-08-041898-8.50080-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780080418988500803},
author = {H. Bersini},
abstract = {The work presented in this paper is an attempt to spread further the inspiration gained from the knowledge of biological systems into the field of adaptive control. After the neural controllers and the evolutionary based mechanisms, new hints for the control of complex processes might be derived from other biological domains such as immunology or the study of conditioning learning. The conception of a system equipped with a complex controller, interacting with an uncertain and varying environment, and basing its learning on its own experiences entails quite naturally the integration of a reinforcement learning mechanism. Two learning processes characterized by two different time scales will be introduced, will be connected to their respective biological origins and will be illustrated on the classical cart-pole control problem. These two learning processes are the rapid reinforcement learning and the slower recruitment mechanism.}
}
@article{PADMANABHAN201711,
title = {Reinforcement learning-based control of drug dosing for cancer chemotherapy treatment},
journal = {Mathematical Biosciences},
volume = {293},
pages = {11-20},
year = {2017},
issn = {0025-5564},
doi = {https://doi.org/10.1016/j.mbs.2017.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0025556417304327},
author = {Regina Padmanabhan and Nader Meskin and Wassim M. Haddad},
keywords = {Active drug dosing, Chemotherapy control, Reinforcement learning},
abstract = {The increasing threat of cancer to human life and the improvement in survival rate of this disease due to effective treatment has promoted research in various related fields. This research has shaped clinical trials and emphasized the necessity to properly schedule cancer chemotherapy to ensure effective and safe treatment. Most of the control methodologies proposed for cancer chemotherapy scheduling treatment are model-based. In this paper, a reinforcement learning (RL)-based, model-free method is proposed for the closed-loop control of cancer chemotherapy drug dosing. Specifically, the Q-learning algorithm is used to develop an optimal controller for cancer chemotherapy drug dosing. Numerical examples are presented using simulated patients to illustrate the performance of the proposed RL-based controller.}
}
@article{XU2020114200,
title = {Parametric study on reinforcement learning optimized energy management strategy for a hybrid electric vehicle},
journal = {Applied Energy},
volume = {259},
pages = {114200},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.114200},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919318872},
author = {Bin Xu and Dhruvang Rathod and Darui Zhang and Adamu Yebi and Xueyu Zhang and Xiaoya Li and Zoran Filipi},
keywords = {Reinforcement learning, -learning, Energy management strategy, Hybrid electric vehicle},
abstract = {An efficient energy split among different source of energy has been a challenge for existing hybrid electric vehicle (HEV) supervisory control system. It requires an optimized energy use of internal combustion engine and electric source such as battery, fuel cell, ultracapacitor, etc. In recent years,Reinforcement Learning (RL) based energy management strategy (EMS) has emerged as one of theefficient control strategies. The effectivenessReinforcement Learningmethod largely depends on optimized parameter selections.However, a thorough parametric study still lacks in this field. It is a fundamental step for efficient implementation of the RL-based EMS. Different from existing RL-based EMS literature, this study conducts a parametric study on several key factors during the RL-based EMS development, including: (1) state types and number of states, (2) states and action discretization, (3) exploration and exploitation, and (4) learning experience selection. The main results show that learning experience selection can effectively reduce the vehicle fuel consumption. The study of the states and action discretization show that the vehicle fuel consumption reduces as action discretization increases while increasing the states discretization is detrimental to the fuel consumption. Moreover, the increasing number of states improves fuel economy. With the help of the proposed parametric analysis, the RL-based EMS can be easily adapted to other power split problems in a HEV application.}
}
@article{CHALI2015252,
title = {A reinforcement learning formulation to the complex question answering problem},
journal = {Information Processing & Management},
volume = {51},
number = {3},
pages = {252-272},
year = {2015},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2015.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306457315000035},
author = {Yllias Chali and Sadid A. Hasan and Mustapha Mojahid},
keywords = {Complex question answering, Multi-document summarization, Reinforcement learning, Reward function, User interaction modeling},
abstract = {We use extractive multi-document summarization techniques to perform complex question answering and formulate it as a reinforcement learning problem. Given a set of complex questions, a list of relevant documents per question, and the corresponding human generated summaries (i.e. answers to the questions) as training data, the reinforcement learning module iteratively learns a number of feature weights in order to facilitate the automatic generation of summaries i.e. answers to previously unseen complex questions. A reward function is used to measure the similarities between the candidate (machine generated) summary sentences and the abstract summaries. In the training stage, the learner iteratively selects the important document sentences to be included in the candidate summary, analyzes the reward function and updates the related feature weights accordingly. The final weights are used to generate summaries as answers to unseen complex questions in the testing stage. Evaluation results show the effectiveness of our system. We also incorporate user interaction into the reinforcement learner to guide the candidate summary sentence selection process. Experiments reveal the positive impact of the user interaction component on the reinforcement learning framework.}
}
@article{CHEN2022103759,
title = {Data efficient reinforcement learning and adaptive optimal perimeter control of network traffic dynamics},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {142},
pages = {103759},
year = {2022},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103759},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22001929},
author = {C. Chen and Y.P. Huang and W.H.K. Lam and T.L. Pan and S.C. Hsu and A. Sumalee and R.X. Zhong},
keywords = {Macroscopic fundamental diagram, Adaptive optimal perimeter control, Heterogeneous data resolution, Integral reinforcement learning, Experience replay, Closed-loop stability},
abstract = {Existing data-driven and feedback traffic control strategies do not consider the heterogeneity of real-time data measurements. Besides, traditional reinforcement learning (RL) methods for traffic control usually converge slowly for lacking data efficiency. Moreover, conventional optimal perimeter control schemes require exact knowledge of the system dynamics and thus they would be fragile to endogenous uncertainties. To handle these challenges, this work proposes an integral reinforcement learning (IRL) based approach to learning the macroscopic traffic dynamics for adaptive optimal perimeter control. This work makes the following primary contributions to the transportation literature: (a) A continuous-time control is developed with discrete gain updates to adapt to the discrete-time sensor data. Different from the conventional RL approaches, the reinforcement interval of the proposed IRL method can be varying with respect to the real-time resolution of data measurements. Approximate optimization methods are carried out to address the curse of dimensionality of the optimal control problem with consideration on the resolution of data measurement. (b) To reduce the sampling complexity and use the available data more efficiently, the experience replay (ER) technique is introduced to the IRL algorithm. (c) The proposed method relaxes the requirement on model calibration in a “model-free” manner that enables robustness against modeling uncertainty and enhances the real-time performance via a data-driven RL algorithm. (d) The convergence of the IRL based algorithms and the stability of the controlled traffic dynamics are proven via the Lyapunov theory. The optimal control law is parameterized and then approximated by neural networks (NN), which moderates the computational complexity. Both state and input constraints are considered while no model linearization is required. Numerical examples and simulation experiments are presented to verify the effectiveness and efficiency of the proposed method.}
}
@article{LEE2020108053,
title = {Reinforcement learning-based adaptive PID controller for DPS},
journal = {Ocean Engineering},
volume = {216},
pages = {108053},
year = {2020},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2020.108053},
url = {https://www.sciencedirect.com/science/article/pii/S0029801820309951},
author = {Daesoo Lee and Seung Jae Lee and Solomon C. Yim},
keywords = {Dynamic positioning system (DPS), Proportional-integral-derivative (PID), Fine-tuning, Deep reinforcement learning (DRL), Deep deterministic policy gradient (DDPG)},
abstract = {A conventional PID controller for the DPS has limitations due to fixed gains and dependence on manual adjustment for its gains. Therefore, several previous studies developed a fuzzy-based adaptive PID controller for the DPS which tunes the gains based on the fuzzy logic. However, the fuzzy logic has its disadvantages due to a manual definition of fuzzy rules and fuzzy variables. To overcome those limitations, a deep reinforcement learning algorithm is adopted to learn the efficient adaptive gain-tuning strategy without human intuition behind since it does not require any prior knowledge about the dynamics of a ship or DPS. Finally, it is shown that the proposed system can result in better station-keeping performance without deterioration in its control efficiency.}
}
@incollection{PENNARTZ2000231,
title = {The glutamate hypothesis of reinforcement learning},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {126},
pages = {231-253},
year = {2000},
booktitle = {Cognition, emotion and autonomic responses: The integrative role of the prefrontal cortex and limbic structures},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(00)26017-2},
url = {https://www.sciencedirect.com/science/article/pii/S0079612300260172},
author = {C.M.A. Pennartz and B.L. McNaughton and A.B. Mulder},
abstract = {Publisher Summary
In this chapter, the brain circuitry processing reward information is discussed and subsequently a computational model is presented describing how reward-related signals in the brain may guide learning in multistructural neuronal networks. This computational model specifies the functional components and inters connections of a reward-controlled sensorimotor system, as well as the learning rules governing changes in synaptic efficacy. In evaluating the neurobiological plausibility of the model, two anatomically specified networks were outlined as putative analogues of the model's architecture. In both networks, the amygdala, prefrontal and cingulate cortex are centrally positioned as reinforcement-processing module. In the first network, the premotor, supplementary and primary motor areas function to convert sensory input to motor output and receive reinforcement signals guiding synaptic weight changes. In the second network, this function is fulfilled by the nucleus accumbens and caudateputamen, receiving extensive input from sensory, motor and associational cortical areas; a third analogue would by a hybrid of these two networks consisting of premotor/motor cortico-basal ganglia loops. While much of the theory and experimental evidence reviewed in this chapter pertains to positive reinforcers (reward), the theoretical concepts can also be applied to negative reinforcement.}
}
@article{JALALIMANESH2017235,
title = {Simulation-based optimization of radiotherapy: Agent-based modeling and reinforcement learning},
journal = {Mathematics and Computers in Simulation},
volume = {133},
pages = {235-248},
year = {2017},
note = {Biomath 2014 and Biomath 2015},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2016.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0378475416300878},
author = {Ammar Jalalimanesh and Hamidreza {Shahabi Haghighi} and Abbas Ahmadi and Madjid Soltani},
keywords = {Agent-based modeling, Reinforcement learning, Radiotherapy, Q-learning, Cancer treatment},
abstract = {Along with surgery and chemotherapy, radiotherapy is an effective way to treat cancer. Many cancer patients take delivery of radiation. The goal of radiotherapy is to destroy the tumor without damaging healthy tissue. Due to the complexity of the procedure, modeling and simulation can be useful for radiotherapy. In this research we propose a new approach to optimize dose calculation in radiotherapy. We consider fix schedule of irradiation and varying the fraction size during the treatment. The proposed approach contains two steps. At the first step, we develop an agent-based simulation of vascular tumor growth based on biological evidences. We consider a multi-scale model in which cellular and subcellular scales are observed. We consider heterogeneity of tumor oxygen diffusion and also the effects of cancer cells hypoxia on radiotherapy. Besides, different radiosensitivity of cells related to their cell-cycle phase is modeled. The agent-based model was implemented in NetLogo package. Based on this model, we simulate different scenarios of radiotherapy. At the second step, we propose an algorithm for the optimization of radiotherapy. Radiation dose and fractionation scheme are considered as two key elements of radiation therapy. To optimize the therapy we apply Q-learning algorithm. Finally, we combine the simulation and optimization compartments together using R-NetLogo package. By tuning the parameters of learning algorithm optimal treatment plans are achieved to cure tumor together with minimum side effects. Our research presents the power of agent-based approach combined with reinforcement learning for simulating and optimizing complex biological problems such as radiotherapy. The proposed modeling approach lets us to study different scenarios of tumor growth and radiotherapy. Furthermore, our optimization algorithm works fast and finds the best treatment plan.}
}
@article{PALETTA200071,
title = {Active object recognition by view integration and reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {31},
number = {1},
pages = {71-86},
year = {2000},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(99)00079-2},
url = {https://www.sciencedirect.com/science/article/pii/S0921889099000792},
author = {Lucas Paletta and Axel Pinz},
keywords = {Active recognition, Reinforcement learning, Information fusion, Viewpoint planning},
abstract = {A mobile agent with the task to classify its sensor pattern has to cope with ambiguous information. Active recognition of three-dimensional objects involves the observer in a search for discriminative evidence, e.g., by change of its viewpoint. This paper defines the recognition process as a sequential decision problem with the objective to disambiguate initial object hypotheses. Reinforcement learning provides then an efficient method to autonomously develop near-optimal decision strategies in terms of sensorimotor mappings. The proposed system learns object models from visual appearance and uses a radial basis function (RBF) network for a probabilistic interpretation of the two-dimensional views. The information gain in fusing successive object hypotheses provides a utility measure to reinforce actions leading to discriminative viewpoints. The system is verified in experiments with 16 objects and two degrees of freedom in sensor motion. Crucial improvements in performance are gained using the learned in contrast to random camera placements.}
}
@article{SON2014322,
title = {Bio-insect and artificial robot interaction using cooperative reinforcement learning},
journal = {Applied Soft Computing},
volume = {25},
pages = {322-335},
year = {2014},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2014.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1568494614004414},
author = {Ji-Hwan Son and Young-Cheol Choi and Hyo-Sung Ahn},
keywords = {Reinforcement learning, Fuzzy control, Intelligent interaction},
abstract = {In this paper, we propose fuzzy logic-based cooperative reinforcement learning for sharing knowledge among autonomous robots. The ultimate goal of this paper is to entice bio-insects towards desired goal areas using artificial robots without any human aid. To achieve this goal, we found an interaction mechanism using a specific odor source and performed simulations and experiments [1]. For efficient learning without human aid, we employ cooperative reinforcement learning in multi-agent domain. Additionally, we design a fuzzy logic-based expertise measurement system to enhance the learning ability. This structure enables the artificial robots to share knowledge while evaluating and measuring the performance of each robot. Through numerous experiments, the performance of the proposed learning algorithms is evaluated.}
}
@article{ZHANG2022116285,
title = {A distributed real-time pricing strategy based on reinforcement learning approach for smart grid},
journal = {Expert Systems with Applications},
volume = {191},
pages = {116285},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116285},
url = {https://www.sciencedirect.com/science/article/pii/S095741742101592X},
author = {Li Zhang and Yan Gao and Hongbo Zhu and Li Tao},
keywords = {Smart grid, Demand side management, Real-time pricing, Reinforcement learning, Markov decision process},
abstract = {The real-time pricing (RTP) plays an important role in demand side management of smart grid. In this paper, a distributed RTP strategy which takes the interests of both supply and demand sides into consideration is studied. A holistic model focusing on the interactions between users and the power supplier is proposed in the framework of Markov decision process (MDP). The MDP presentation of smart appliances’ operational processes well embodies their characteristics and the energy correlation of adjacent time slots. Different from existing ones, a novel distributed online algorithm based on a reinforcement learning approach is proposed to solve the MDP model without acquisition of the transition probabilities. Through information exchange between users and the power supplier, the real-time electricity price is decided adaptively, meanwhile, the optimal strategy of power supply and consumption is obtained. The specific information of the utility function for each user does not need to be disclosed to the supplier and other users. Simulation results show that the proposed model and algorithm balance energy supply and demand well, and have a good performance in peak shaving and valley filling.}
}
@article{GIANNELLI2022128054,
title = {A tutorial on optimal control and reinforcement learning methods for quantum technologies},
journal = {Physics Letters A},
volume = {434},
pages = {128054},
year = {2022},
issn = {0375-9601},
doi = {https://doi.org/10.1016/j.physleta.2022.128054},
url = {https://www.sciencedirect.com/science/article/pii/S0375960122001360},
author = {Luigi Giannelli and Pierpaolo Sgroi and Jonathon Brown and Gheorghe Sorin Paraoanu and Mauro Paternostro and Elisabetta Paladino and Giuseppe Falci},
keywords = {Quantum technologies, Quantum control, Optimal control, Machine learning, Reinforcement learning, STIRAP},
abstract = {Quantum Optimal Control is an established field of research which is necessary for the development of Quantum Technologies. In recent years, Machine Learning techniques have been proved useful to tackle a variety of quantum problems. In particular, Reinforcement Learning has been employed to address typical problems of control of quantum systems. In this tutorial we introduce the methods of Quantum Optimal Control and Reinforcement Learning by applying them to the problem of three-level population transfer. The jupyter notebooks to reproduce some of our results are open-sourced and available on github1.}
}
@article{PANDEY2020102715,
title = {Deep reinforcement learning algorithm for dynamic pricing of express lanes with multiple access locations},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {119},
pages = {102715},
year = {2020},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2020.102715},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X20306306},
author = {Venktesh Pandey and Evana Wang and Stephen D. Boyles},
keywords = {Managed lanes, Express lanes, High occupancy/toll (HOT) lanes, Dynamic pricing, Deep reinforcement learning, Traffic control, Feedback control heuristic},
abstract = {This article develops a deep reinforcement learning (Deep-RL) framework for dynamic pricing on managed lanes with multiple access locations and heterogeneity in travelers’ value of time, origin, and destination. This framework relaxes assumptions in the literature by considering multiple origins and destinations, multiple access locations to the managed lane, en route diversion of travelers, partial observability of the sensor readings, and stochastic demand and observations. The problem is formulated as a partially observable Markov decision process (POMDP) and policy gradient methods are used to determine tolls as a function of real-time observations. Tolls are modeled as continuous and stochastic variables and are determined using a feedforward neural network. The method is compared against a feedback control method used for dynamic pricing. We show that Deep-RL is effective in learning toll policies for maximizing revenue, minimizing total system travel time, and other joint weighted objectives, when tested on real-world transportation networks. The Deep-RL toll policies outperform the feedback control heuristic for the revenue maximization objective by generating revenues up to 8.5% higher than the heuristic and for the objective minimizing total system travel time (TSTT) by generating TSTT up to 8.4% lower than the heuristic. We also propose reward shaping methods for the POMDP to overcome the undesired behavior of toll policies, like the jam-and-harvest behavior of revenue-maximizing policies. Additionally, we test transferability of the algorithm trained on one set of inputs for new input distributions and offer recommendations on real-time implementations of Deep-RL algorithms. The source code for our experiments is available online at https://github.com/venktesh22/ExpressLanes_Deep-RL.}
}
@article{REZZOUG20091229,
title = {A reinforcement learning based neural network architecture for obstacle avoidance in multi-fingered grasp synthesis},
journal = {Neurocomputing},
volume = {72},
number = {4},
pages = {1229-1241},
year = {2009},
note = {Brain Inspired Cognitive Systems (BICS 2006) / Interplay Between Natural and Artificial Computation (IWINAC 2007)},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2008.01.026},
url = {https://www.sciencedirect.com/science/article/pii/S0925231208001483},
author = {Nasser Rezzoug and Philippe Gorce},
keywords = {Reinforcement learning, Shaping, Neural network, Grasping, Obstacle avoidance},
abstract = {The ability to learn from interaction with the exterior world as well as variability are two main features of living organisms. The aim of this study is to present and discuss the property of a stochastic reinforcement learning based model of upper limb posture generation that exhibits both properties. The capacity of the model to discover suitable postures satisfying task and obstacle avoidance constraints is demonstrated by simulation. Also, task equivalent configurations that can be linked to recent findings in the motor control literature are generated by the proposed formalism due to its stochastic nature.}
}
@article{XING2022102918,
title = {Robot path planner based on deep reinforcement learning and the seeker optimization algorithm},
journal = {Mechatronics},
volume = {88},
pages = {102918},
year = {2022},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2022.102918},
url = {https://www.sciencedirect.com/science/article/pii/S0957415822001362},
author = {Xiangrui Xing and Hongwei Ding and Zhuguan Liang and Bo Li and Zhijun Yang},
keywords = {Seeker optimization algorithm, Advantage actor-critic, Path planning, Path de-redundancy, Deep reinforcement learning},
abstract = {Path planning is one of the key technologies for mobile robot applications. However, the traditional robot path planner has a slow planning response, which leads to a long navigation completion time. In this paper, we propose a novel robot path planner (SOA+A2C) that produces global and local path planners with the seeker optimization algorithm (SOA) and the advantage actor-critic (A2C) algorithm, respectively. In addition, to solve the problems of poor convergence performance when training deep reinforcement learning (DRL) agents in complex path planning tasks and path redundancy when metaheuristic algorithms, such as SOA, are used for path planning, we propose the incremental map training method and path de-redundancy method. Simulation results show that first, the incremental map training method can improve the convergence performance of the DRL agent in complex path planning tasks. Second, the path de-redundancy method can effectively alleviate path redundancy without sacrificing the search capability of the metaheuristic algorithm. Third, the SOA+A2C path planner is superior to the Dijkstra & dynamic window approach (Dijkstra+DWA) and the Dijkstra & timed elastic band (Dijkstra+TEB) path planners provided by the robot operating system (ROS) in terms of path length, path planning response time, and navigation completion time. Therefore, the developed SOA+A2C path planner can serve as an effective tool for mobile robot path planning.}
}
@article{TADEPALLI1998177,
title = {Model-based average reward reinforcement learning},
journal = {Artificial Intelligence},
volume = {100},
number = {1},
pages = {177-224},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00002-2},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000022},
author = {Prasad Tadepalli and DoKyeong Ok},
keywords = {Machine learning, Reinforcement learning, Average reward, Model-based, Exploration, Bayesian networks, Linear regression, AGV scheduling},
abstract = {Reinforcement Learning (RL) is the study of programs that improve their performance by receiving rewards and punishments from the environment. Most RL methods optimize the discounted total reward received by an agent, while, in many domains, the natural criterion is to optimize the average reward per time step. In this paper, we introduce a model-based Averagereward Reinforcement Learning method called H-learning and show that it converges more quickly and robustly than its discounted counterpart in the domain of scheduling a simulated Automatic Guided Vehicle (AGV). We also introduce a version of H-learning that automatically explores the unexplored parts of the state space, while always choosing greedy actions with respect to the current value function. We show that this “Auto-exploratory H-Learning” performs better than the previously studied exploration strategies. To scale H-learning to larger state spaces, we extend it to learn action models and reward functions in the form of dynamic Bayesian networks, and approximate its value function using local linear regression. We show that both of these extensions are effective in significantly reducing the space requirement of H-learning and making it converge faster in some AGV scheduling tasks.}
}
@article{CHARVILLAT20071034,
title = {Reinforcement learning for dynamic multimedia adaptation},
journal = {Journal of Network and Computer Applications},
volume = {30},
number = {3},
pages = {1034-1058},
year = {2007},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2005.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S108480450600004X},
author = {Vincent Charvillat and Romulus Grigoraş},
keywords = {Multimedia adaptation, Reinforcement learning, Markov decision process, Ubiquitous streaming, Prefetching policies},
abstract = {In this paper we present an integration of several user and resource-related factors for the design of dynamic adaptation techniques. Our first contribution is an original reinforcement-learning approach to develop better adaptation agents. Integrated with the content, these agents improve gradually, by taking into account both user's behaviour and the usage context. Our second contribution is to apply this generic approach to solve an ubiquitous streaming problem. Mobile users experience large latencies while accessing streaming media. We propose to adapt the streaming by prefetching and to model this decision problem by using a Markov decision process. We discuss this formal framework and make explicit reference to its relationship with reinforcement learning. We support the benefits of our approach by presenting results from simulations and experiments.}
}
@article{GOLDSHTEIN20204096,
title = {Reinforcement Learning Enables Resource Partitioning in Foraging Bats},
journal = {Current Biology},
volume = {30},
number = {20},
pages = {4096-4102.e6},
year = {2020},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2020.07.079},
url = {https://www.sciencedirect.com/science/article/pii/S0960982220311386},
author = {Aya Goldshtein and Michal Handel and Ofri Eitan and Afrine Bonstein and Talia Shaler and Simon Collet and Stefan Greif and Rodrigo A. Medellín and Yuval Emek and Amos Korman and Yossi Yovel},
keywords = {nectar feeding bats, reinforcement learning, resource partitioning, trapline, behavioral ecology, movement ecology, territories},
abstract = {Summary
Every evening, from late spring to mid-summer, tens of thousands of hungry lactating female lesser long-nosed bats (Leptonycteris yerbabuenae) emerge from their roost and navigate over the Sonoran Desert, seeking for nectar and pollen [1, 2]. The bats roost in a huge maternal colony that is far from the foraging grounds but allows their pups to thermoregulate [3] while the mothers are foraging. Thus, the mothers have to fly tens of kilometers to the foraging sites—fields with thousands of Saguaro cacti [4, 5]. Once at the field, they must compete with many other bats over the same flowering cacti. Several solutions have been suggested for this classical foraging task of exploiting a resource composed of many renewable food sources whose locations are fixed. Some animals randomly visit the food sources [6], and some actively defend a restricted foraging territory [7, 8, 9, 10, 11] or use simple forms of learning, such as “win-stay lose-switch” strategy [12]. Many species have been suggested to follow a trapline, that is, to revisit the food sources in a repeating ordered manner [13, 14, 15, 16, 17, 18, 19, 20, 21, 22]. We thus hypothesized that lesser long-nosed bats would visit cacti in a sequenced manner. Using miniature GPS devices, aerial imaging, and video recordings, we tracked the full movement of the bats and all of their visits to their natural food sources. Based on real data and evolutionary simulations, we argue that the bats use a reinforcement learning strategy that requires minimal memory to create small, non-overlapping cacti-cores and exploit nectar efficiently, without social communication.}
}
@incollection{KAMALAPURKAR2016247,
title = {Chapter Eight - Model-Based Reinforcement Learning for Approximate Optimal Regulation},
editor = {Kyriakos G. Vamvoudakis and Sarangapani Jagannathan},
booktitle = {Control of Complex Systems},
publisher = {Butterworth-Heinemann},
pages = {247-273},
year = {2016},
isbn = {978-0-12-805246-4},
doi = {https://doi.org/10.1016/B978-0-12-805246-4.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128052464000082},
author = {R. Kamalapurkar and P. Walters and W.E. Dixon},
keywords = {Model-based reinforcement learning, Concurrent learning, Simulated experience, Data-based control, Adaptive control, System identification},
abstract = {Reinforcement learning (RL)-based online approximate optimal control methods applied to deterministic systems typically require a restrictive persistence of excitation (PE) condition for convergence. We develop a model-based RL algorithm to solve approximate optimal regulation problems online under a PE-like rank condition. The development is based on the observation that, given a model of the system, model-based RL can be implemented by evaluation of the Bellman error at any number of desired points in the state space. Uniformly ultimately bounded regulation of the system states to a neighborhood of the origin and convergence of the developed policy to a neighborhood of the optimal policy are established using a Lyapunov-based analysis, and simulations are presented to demonstrate the performance of the developed controller.}
}
@article{SHARMA2022109113,
title = {DeepEvap: Deep reinforcement learning based ensemble approach for estimating reference evapotranspiration},
journal = {Applied Soft Computing},
volume = {125},
pages = {109113},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109113},
url = {https://www.sciencedirect.com/science/article/pii/S156849462200388X},
author = {Gitika Sharma and Ashima Singh and Sushma Jain},
keywords = {Reference evapotranspiration, Limited climate data, Deep reinforcement learning, Deep neural network, Ensemble learning},
abstract = {Precision agriculture aims to increase crop yield by employing an efficient resource management scheme, such as estimating irrigation requirements. Reference evapotranspiration (ET0), defined as the process of water loss from the soil and reference plant, is one of the indispensable components on which crop irrigation requirement depends. It is mainly calculated by using empirical models. However, these models require a large climate dataset that is sometimes unavailable in data-scarce regions. The present study focuses on the estimation of ET0 values by using three climate parameters as input variables i.e., minimum temperature (Tmin), maximum temperature (Tmax), and solar radiation (Rs). Moreover, to consider the effect of time-varying characteristics of the ET0 process, deep reinforcement learning (DRL) based ensemble approach, DeepEvap, is introduced to estimate ET0 values. The whole modeling procedure of the proposed ensemble model incorporates three phases. In phase I, the data preprocessing technique is performed on the meteorological data to clean the existing impurities as it affects the performance of any machine learning (ML) based approach. In phase II, four different deep neural network-based models are used to build the estimation model of ET0 and calculate the prediction results. In phase III, the DRL approach is used to ensemble the prediction results of these four models. The meteorological dataset of two stations of India: Ludhiana and Patiala, is selected to validate the proposed approach. The results of the conducted study depict that: (a) The proposed DeepEvap approach is competitive for ET0 prediction by achieving a coefficient of determination (R2) = 0.96. It significantly outperforms four baseline models; (b) The proposed technique also integrates four deep neural network models and works better than existing ensemble approaches.}
}
@article{FERNANDEZGAUNA201525,
title = {Reinforcement Learning endowed with safe veto policies to learn the control of Linked-Multicomponent Robotic Systems},
journal = {Information Sciences},
volume = {317},
pages = {25-47},
year = {2015},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2015.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S002002551500273X},
author = {Borja Fernandez-Gauna and Manuel Graña and Jose Manuel Lopez-Guede and Ismael Etxeberria-Agiriano and Igor Ansoategui},
keywords = {Reinforcement Learning, Linked Multicomponent Robotic Systems, Safe exploration policies, Speeding convergence of RL},
abstract = {Performing reinforcement learning-based control of systems whose state space has many Undesired Terminal States (UTS) experiences severe convergence problems. We define UTS as terminal states without associated positive reward information. They appear in the training of over-constrained systems, when breaking a constraint implies that all the effort invested during a learning episode is lost without gathering any constructive information about how to achieve the target task. The random exploration performed by RL algorithms is unfruitful until the system reaches any final state bearing some reward that may be used to update the state-action value functions, hence UTS seriously impede the convergence of the learning process. The most efficient learning strategies avoid reaching any UTS, ensuring that each learning process episode provides useful reward information. Safe Modular State Action Veto (Safe-MSAV) policies learn specifically how to avoid state transitions leading to an UTS. The application of MSAV makes state space exploration much more efficient. Bigger ratio of UTS to the total number of states provide greater improvements. Safe-MSAV uses independent concurrent modules, each dealing with a separate kind of UTS. We report experiments on the control of Linked Multicomponent Robotic Systems (L-MCRS) showing a dramatic decrease on the computational resources required, ensuring faster as well as more accurate results than conventional exploration strategies that do not implement explicit mechanisms to avoid falling in UTS.}
}
@article{LIU2023104915,
title = {Supporting virtual power plants decision-making in complex urban environments using reinforcement learning},
journal = {Sustainable Cities and Society},
volume = {99},
pages = {104915},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2023.104915},
url = {https://www.sciencedirect.com/science/article/pii/S2210670723005267},
author = {Chengyang Liu and Rebecca Jing Yang and Xinghuo Yu and Chayn Sun and Gary Rosengarten and Ariel Liebman and Ron Wakefield and Peter SP Wong and Kaige Wang},
keywords = {Virtual power plant, Urban environment, Distributed energy resources, Reinforcement learning, Scenario analysis, Decision-making},
abstract = {Virtual Power Plants (VPPs) are becoming popular for managing energy supply in urban environments with Distributed Energy Resources (DERs). However, decision-making for VPPs in such complex environments is challenging due to multiple uncertainties and complexities. This paper proposes an approach that optimizes decision-making for VPPs using Reinforcement Learning (RL) in urban environments with diverse supply-demand profiles and DERs. The approach addresses challenges related to integrating renewable energy sources and achieving energy efficiency. An RL-based VPP system is trained and tested under different scenarios, and a case study is conducted in a real-world urban environment. The proposed approach achieves multi-objective optimization by performing actions such as load-shifting, demand offsetting, and providing ancillary services in response to demand, renewable generators, and market signals. The study validates the effectiveness and robustness of the proposed approach under complex environmental conditions. Results demonstrate that the approach provides optimized decisions in various urban environments with different available resources and supply-demand profiles. This paper contributes to understanding the use of RL in optimizing VPP decision-making and provides valuable insights for policymakers and practitioners in sustainable and resilient cities.}
}
@article{GROSSBERG19923,
title = {A neural network model of adaptively timed reinforcement learning and hippocampal dynamics},
journal = {Cognitive Brain Research},
volume = {1},
number = {1},
pages = {3-38},
year = {1992},
issn = {0926-6410},
doi = {https://doi.org/10.1016/0926-6410(92)90003-A},
url = {https://www.sciencedirect.com/science/article/pii/092664109290003A},
author = {Stephen Grossberg and John W.L. Merrill},
keywords = {Learning, Timing, Neural network, Reinforcement, Emotion, Recognition, Attention, Motor control, Hippocampus, Thalamus, Cortex, Cerebellum, }
}
@article{ZENDEHROUH20131,
title = {Modeling error detection in human brain: A preliminary unification of reinforcement learning and conflict monitoring theories},
journal = {Neurocomputing},
volume = {103},
pages = {1-13},
year = {2013},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2012.04.026},
url = {https://www.sciencedirect.com/science/article/pii/S0925231212005048},
author = {Sareh Zendehrouh and Shahriar Gharibzadeh and Farzad Towhidkhah},
keywords = {Performance monitoring, Error detection, Reinforcement learning, Conflict monitoring},
abstract = {The error detection concept plays a critical role in theories of performance monitoring. In this study, we have proposed a model that somehow unifies two main theories of performance monitoring: reinforcement learning and conflict monitoring. The proposed model, which is a modified and extended version of computational model of reinforcement learning theory, is used to simulate behavioral and event-related brain potential data in a modified version of Eriksen flanker task. This model captures the idea of conflict monitoring theory. Therefore, it can generate a component of event-related brain potential (N200) that reinforcement learning theory is not capable of producing it.}
}
@article{PETTER2018911,
title = {Integrating Models of Interval Timing and Reinforcement Learning},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {10},
pages = {911-922},
year = {2018},
note = {Special Issue: Time in the Brain},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2018.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661318301931},
author = {Elijah A. Petter and Samuel J. Gershman and Warren H. Meck},
abstract = {We present an integrated view of interval timing and reinforcement learning (RL) in the brain. The computational goal of RL is to maximize future rewards, and this depends crucially on a representation of time. Different RL systems in the brain process time in distinct ways. A model-based system learns ‘what happens when’, employing this internal model to generate action plans, while a model-free system learns to predict reward directly from a set of temporal basis functions. We describe how these systems are subserved by a computational division of labor between several brain regions, with a focus on the basal ganglia and the hippocampus, as well as how these regions are influenced by the neuromodulator dopamine.}
}
@article{SIERRAGARCIA2022104769,
title = {Wind turbine pitch reinforcement learning control improved by PID regulator and learning observer},
journal = {Engineering Applications of Artificial Intelligence},
volume = {111},
pages = {104769},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.104769},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622000598},
author = {J. Enrique Sierra-Garcia and Matilde Santos and Ravi Pandit},
keywords = {Intelligent control, Reinforcement learning, Learning observer, Pitch control, Wind turbines},
abstract = {Wind turbine (WT) pitch control is a challenging issue due to the non-linearities of the wind device and its complex dynamics, the coupling of the variables and the uncertainty of the environment. Reinforcement learning (RL) based control arises as a promising technique to address these problems. However, its applicability is still limited due to the slowness of the learning process. To help alleviate this drawback, in this work we present a hybrid RL-based control that combines a RL-based controller with a proportional–integral–derivative (PID) regulator, and a learning observer. The PID is beneficial during the first training episodes as the RL based control does not have any experience to learn from. The learning observer oversees the learning process by adjusting the exploration rate and the exploration window in order to reduce the oscillations during the training and improve convergence. Simulation experiments on a small real WT show how the learning significantly improves with this control architecture, speeding up the learning convergence up to 37%, and increasing the efficiency of the intelligent control strategy. The best hybrid controller reduces the error of the output power by around 41% regarding a PID regulator. Moreover, the proposed intelligent hybrid control configuration has proved more efficient than a fuzzy controller and a neuro-control strategy.}
}
@article{WANG2019105557,
title = {Deep reinforcement learning-based cooperative interactions among heterogeneous vehicular networks},
journal = {Applied Soft Computing},
volume = {82},
pages = {105557},
year = {2019},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2019.105557},
url = {https://www.sciencedirect.com/science/article/pii/S1568494619303370},
author = {Jingyu Wang and Zirui Zhuang and Qi Qi and Tonghong Li and Jianxin Liao},
keywords = {Internet of vehicle, Co-existing virtual networks, Deep reinforcement learning, Asymmetric nash bargaining solution},
abstract = {Most real-world vehicle nodes can be structured into an interconnected network of vehicles. Through structuring these services and vehicle device interactions into multiple types, such internet of vehicles becomes multidimensional heterogeneous overlay networks. The heterogeneousness of the overlays makes it difficult for the overlay networks to coordinate with each other to improve their performance. Therefore, it poses an interesting but critical challenge to the effective analysis of heterogeneous virtual vehicular networks. A variety of virtual vehicular networks can be easily deployed onto the native network by applying the concept of SDN (Software Defined Networking). These virtual networks reflect their heterogeneousness due to their different performance goals, and they compete for the same physical resources of the underlying network, so that a sub-optimal performance of the virtual networks may be achieved. Therefore, we propose a Deep Reinforcement Learning (DRL) approach to make the virtual networks cooperate with each other through the SDN controller. A cooperative solution based on the asymmetric Nash bargaining is proposed for co-existing virtual networks to improve their performance. Moreover, the Markov Chain model and DRL resolution are introduced to leverage the heterogeneous performance goals of virtual networks. The implementation of the approach is introduced, and simulation results confirm the performance improvement of the latency sensitive, loss-rate sensitive and throughput sensitive heterogeneous vehicular networks using our cooperative solution.}
}
@article{SHANG2022107885,
title = {Energy optimal dispatching of ship's integrated power system based on deep reinforcement learning},
journal = {Electric Power Systems Research},
volume = {208},
pages = {107885},
year = {2022},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2022.107885},
url = {https://www.sciencedirect.com/science/article/pii/S0378779622001158},
author = {Chengya Shang and Lijun Fu and Xianqiang Bao and Xinghua Xu and Yan Zhang and Haipeng Xiao},
keywords = {Integrated power system, Energy management strategy, Reinforcement Learning, Deep Q network},
abstract = {With the pressure mounting on environmental protection, all-electric ships(AES)are becoming increasingly popular for maritime transportation. The optimal dispatch of energy is of great significance to achieving AES' safe and economic operation. This paper proposes a deep reinforcement learning (DRL) based energy optimization scheduling method for the ship power system, and the generator and the energy storage system (ESS) are directly driven by the original measurement data of the ship's power system. This paper first describes optimal energy scheduling in the ship power system mathematically and then expresses the scheduling decision problem as a reinforcement-learning framework. Next, it introduces a deep Q network algorithm to optimize the end-to-end control strategy between the measurement data of the ship power system and the action instructions of the generator and ESS. The method proposed in this paper does not need to model the complex system and can realize the dynamic optimization scheduling decision of energy with the goal of economy. Finally, two case studies are analyzed based on the historical data of the ship power system, and the simulation verifies the effectiveness and superiority of the proposed energy optimization scheduling method based on DRL.}
}
@article{JIN2022109382,
title = {Controlling mixed-mode fatigue crack growth using deep reinforcement learning},
journal = {Applied Soft Computing},
volume = {127},
pages = {109382},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109382},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622005294},
author = {Yuteng Jin and Siddharth Misra},
keywords = {Reinforcement learning, DDPG, Fatigue crack growth, Control, Reward function},
abstract = {Mechanical discontinuity embedded in a material determines the bulk mechanical, physical, and chemical properties. Under external forces, mechanical discontinuity undergo spatiotemporal propagation; thereby altering various properties of the material. This paper is a proof-of-concept development and deployment of a reinforcement learning framework, based on deep deterministic policy gradient, to precisely control both the direction and rate of the fatigue crack growth. The ability to control mechanical discontinuity in essence determines the key material properties. The desired control is relatively hard to achieve considering the large, continuous state and action spaces along with the exponential relationship between crack growth and stress cycle. The reinforcement-learning scheme is capable of learning an optimal and computational tractable control strategy. In the proposed approach, the reinforcement learning framework is integrated into an OpenAI-Gym-based environment that implements the mechanistic equations governing the fatigue crack growth. The learning agent does not explicitly know about the underlying physics, nonetheless, the learning agent can infer the control strategy by continuously interacting the numerical environment. The paper formulates an adaptive reward function involving reward shaping that can be generalized to similar control problems to improve the training efficiency. The reinforcement learning framework can successfully control the fatigue crack growth in a material despite the complexity of the propagation/growth pathway determined by multiple goal points. The paper provides the mathematical/physical basis of the reward function and the effect of neural network size and architecture and the state and action space that boosts the training speed while preserving the stability of the RL agents for the desired control problem.}
}
@article{TAN2022333,
title = {Multi-agent reinforcement learning for long-term network resource allocation through auction: A V2X application},
journal = {Computer Communications},
volume = {194},
pages = {333-347},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.07.047},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422003000},
author = {Jing Tan and Ramin Khalili and Holger Karl and Artur Hecker},
keywords = {Offloading, Distributed systems, Reinforcement learning, Decentralized decision-making},
abstract = {We formulate offloading of computational tasks from a dynamic group of mobile agents (e.g., cars) as decentralized decision making among autonomous agents. We design an interaction mechanism that incentivizes such agents to align private and system goals by balancing between competition and cooperation. In the static case, the mechanism provably has Nash equilibria with optimal resource allocation. In a dynamic environment, this mechanism’s requirement of complete information is impossible to achieve. For such environments, we propose a novel multi-agent online learning algorithm that learns with partial, delayed and noisy state information, thus greatly reducing information need. Our algorithm is also capable of learning from long-term and sparse reward signals with varying delay. Empirical results from the simulation of a V2X application confirm that through learning, agents with the learning algorithm significantly improve both system and individual performance, reducing up to 30% of offloading failure rate, communication overhead and load variation, increasing computation resource utilization and fairness. Results also confirm the algorithm’s good convergence and generalization property in different environments.}
}
@article{LI2019113762,
title = {Energy management for a power-split hybrid electric bus via deep reinforcement learning with terrain information},
journal = {Applied Energy},
volume = {255},
pages = {113762},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.113762},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919314497},
author = {Yuecheng Li and Hongwen He and Amir Khajepour and Hong Wang and Jiankun Peng},
keywords = {Energy management strategy, Deep reinforcement learning, Discrete-continuous hybrid action space, Power-split hybrid electric bus, Terrain information},
abstract = {Due to the high mileage and heavy load capabilities of hybrid commercial vehicles, energy management becomes crucial in improving their fuel economy. In this paper, terrain information is systematically integrated into the energy management strategy for a power-split hybrid electric bus based on a deep reinforcement learning approach: the deep deterministic policy gradient algorithm. Specially, this energy management method is improved and capable of searching optimal energy management strategies in a discrete-continuous hybrid action space, which, in this work, consists of two continuous actions for the engine and four discrete actions for powertrain mode selections. Additionally, a Critic network with dueling architecture and a pre-training stage ahead of the reinforcement learning process are combined for efficient strategy learning with the adopted algorithm. Assuming the current terrain information was available to the controller, the deep reinforcement learning based energy management strategy is trained and tested on different driving cycles and simulated terrains. Simulation results of the trained strategy show that reasonable energy allocation schemes and mode switching rules are learned simultaneously. Its fuel economy gap with the baseline strategy using dynamic programming is narrowed down to nearly 6.4% while reducing the times of engine starts by around 76%. Further comparisons also indicate approximately 2% promotion in fuel economy is contributed by the incorporation of terrain information in this learning-based energy management. The main contribution of this study is to explore the inclusion of terrain information in a learning-based energy management method that can deal with large hybrid action spaces.}
}
@article{LEE2021109421,
title = {Policy iterations for reinforcement learning problems in continuous time and space — Fundamental theory and methods},
journal = {Automatica},
volume = {126},
pages = {109421},
year = {2021},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2020.109421},
url = {https://www.sciencedirect.com/science/article/pii/S0005109820306233},
author = {Jaeyoung Lee and Richard S. Sutton},
keywords = {Policy iteration, Reinforcement learning, Optimization under uncertainties, Continuous time and space, Iterative schemes, Adaptive systems},
abstract = {Policy iteration (PI) is a recursive process of policy evaluation and improvement for solving an optimal decision-making/control problem, or in other words, a reinforcement learning (RL) problem. PI has also served as the fundamental for developing RL methods. In this paper, we propose two PI methods, called differential PI (DPI) and integral PI (IPI), and their variants, for a general RL framework in continuous time and space (CTS), where the environment is modeled by a system of ordinary differential equations (ODEs). The proposed methods inherit the current ideas of PI in classical RL and optimal control and theoretically support the existing RL algorithms in CTS: TD-learning and value-gradient-based (VGB) greedy policy update. We also provide case studies including (1) discounted RL and (2) optimal control tasks. Fundamental mathematical properties – admissibility, uniqueness of the solution to the Bellman equation (BE), monotone improvement, convergence, and optimality of the solution to the Hamilton–Jacobi–Bellman equation (HJBE) – are all investigated in-depth and improved from the existing theory, along with the general and case studies. Finally, the proposed ones are simulated with an inverted-pendulum model and their model-based and partially model-free implementations to support the theory and further investigate them beyond.}
}
@article{ZHANG2022120215,
title = {Multi-agent deep reinforcement learning-based coordination control for grid-aware multi-buildings},
journal = {Applied Energy},
volume = {328},
pages = {120215},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120215},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922014726},
author = {Bin Zhang and Weihao Hu and Amer M.Y.M. Ghias and Xiao Xu and Zhe Chen},
keywords = {Commercial buildings, HVAC systems, Multi-building coordination, Multi-agent deep reinforcement learning, Voltage regulation},
abstract = {In commercial buildings, Heat, Ventilation, and Air Conditioning (HVAC) systems account for about 40–50 % of total electricity usage, contributing to an economic burden on building operators. Furthermore, increasing amounts of distributed generation can bring challenges and opportunities for voltage regulation in low voltage distribution networks. In this paper, we tend to develop intelligent management to save the electricity bills of HVAC systems in multiple multi-zone buildings while relieving the stress of voltage regulation across the network. However, it is challenging to achieve the above aims due to the existence of parameter uncertainties (e.g., electricity load, outdoor temperature, photovoltaic generation, etc.), a sizeable continuous decision space, unknown thermal dynamics model, and distribution network topology, and a non-convex multi-objective function. In this context, a novel model-free multi-agent deep reinforcement learning (MADRL)-based multi-building control algorithm is proposed to achieve building-side and grid-level objectives. The proposed method adopts a centralized training and decentralized execution framework while integrating an attention mechanism to ease training and preserve privacy. This also enables the agent to achieve control purposes based only on local measurements, reducing communication cost. Simulation results based on real-world data verify that the proposed method can achieve real-time physical-model-free control of multi-buildings to tackle fast fluctuations of voltage and temperature caused by the uncertain external factors while being advantageous over other two MADRL methods. Additionally, comparison analysis on untouched datasets illustrates that the proposed method achieves similar results and better computation performance with the perfect physical-model-based approach.}
}
@article{LU2020847,
title = {Optimization of lightweight task offloading strategy for mobile edge computing based on deep reinforcement learning},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {847-861},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19308209},
author = {Haifeng Lu and Chunhua Gu and Fei Luo and Weichao Ding and Xinping Liu},
keywords = {Mobile edge computing, Task offloading, Deep reinforcement learning, LSTM network, Candidate network},
abstract = {With the maturity of 5G technology and the popularity of intelligent terminal devices, the traditional cloud computing service model cannot deal with the explosive growth of business data quickly. Therefore, the purpose of mobile edge computing (MEC) is to effectively solve problems such as latency and network load. In this paper, deep reinforcement learning (DRL) is first proposed to solve the offloading problem of multiple service nodes for the cluster and multiple dependencies for mobile tasks in large-scale heterogeneous MEC. Then the paper uses the LSTM network layer and the candidate network set to improve the DQN algorithm in combination with the actual environment of the MEC. Finally, the task offloading problem is simulated by using iFogSim and Google Cluster Trace. The simulation results show that the offloading strategy based on the improved IDRQN algorithm has better performance in energy consumption, load balancing, latency and average execution time than other algorithms.}
}
@article{ABBASI2021104234,
title = {Deep Reinforcement Learning for QoS provisioning at the MAC layer: A Survey},
journal = {Engineering Applications of Artificial Intelligence},
volume = {102},
pages = {104234},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104234},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621000816},
author = {Mahmoud Abbasi and Amin Shahraki and Md. {Jalil Piran} and Amir Taherkordi},
keywords = {Quality of Service, Medium Access Control, Rate control, Resource sharing and scheduling, Deep Reinforcement Learning, Survey},
abstract = {Quality of Service (QoS) provisioning is based on various network management techniques including resource management and medium access control (MAC). Various techniques have been introduced to automate networking decisions, particularly at the MAC layer. Deep reinforcement learning (DRL), as a solution to sequential decision making problems, is a combination of the power of deep learning (DL), to represent and comprehend the world, with reinforcement learning (RL), to understand the environment and act rationally. In this paper, we present a survey on the applications of DRL in QoS provisioning at the MAC layer. First, we present the basic concepts of QoS and DRL. Second, we classify the main challenges in the context of QoS provisioning at the MAC layer, including medium access and data rate control, and resource sharing and scheduling. Third, we review various DRL algorithms employed to support QoS at the MAC layer, by analyzing, comparing, and identifying their pros and cons. Furthermore, we outline a number of important open research problems and suggest some avenues for future research.}
}
@article{ZOU2021106988,
title = {Stochastic multi-carrier energy management in the smart islands using reinforcement learning and unscented transform},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {130},
pages = {106988},
year = {2021},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.106988},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521002283},
author = {Hongbo Zou and Juan Tao and Salah K. Elsayed and Ehab E. Elattar and Abdulaziz Almalaq and Mohamed A. Mohamed},
keywords = {Smart Island, Multi-carrier water and energy systems (MCWES), Stochastic optimization, Reinforcement learning (RL), Uncertainty, Renewable energy},
abstract = {This article investigates the optimal management of multi-carrier water and energy system(MCWES) considering the high penetration of renewable energy sources as non-dispatchable units and the seawater desalinization mechanism for serving water demand in the target area. The proposed model encompasses several demand layers including power energy, natural gas and water layer which supplies the electricity, thermal and drinkable water demands in the smart island. In order to capture the uncertainty effects in the technical decisions of optimal scheduling, a stochastic approach based on unscented transform (UT) is developed to handle the forecast error in the electrical and thermal energy demands, market energy prices related to the different energy layers and the output power forecast error in the renewable energy sources. Solving the proposed coordinated scheduling problem in an hourly timescale requires heavy calculations that make it impractical. Therefore, a novel reinforcement learning (RL) based approach is devised for finding a near optimal solution and facilitates the searching process with a trivial computational burden. The simulation results indicate that the proposed cooperation approach minimizes both the operation and investment costs substantially with an efficient computational burden based on the advanced features coming out of the proposed RL approach. Last but not least, the simulation results on a practical smart island advocate the effectiveness and high efficacy of the proposed model. Also it was seen that the RL approach could properly solve the optimization model and the selection of the sizes of the components was highly linked to the hourly values of demands and prices of the energy.}
}
@article{QIU2023113052,
title = {Reinforcement learning for electric vehicle applications in power systems:A critical review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {173},
pages = {113052},
year = {2023},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.113052},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122009339},
author = {Dawei Qiu and Yi Wang and Weiqi Hua and Goran Strbac},
keywords = {Electric vehicles, Vehicle-to-grid, Reinforcement learning, Power systems},
abstract = {Electric vehicles (EVs) are playing an important role in power systems due to their significant mobility and flexibility features. Nowadays, the increasing penetration of renewable energy resources has been observed in modern power systems, which brings many benefits for improving climate change and accelerating the low-carbon transition. However, the intermittent and unstable nature of renewable energy sources introduces new challenges to both the planning and operation of power systems. To address these issues, vehicle-to-grid (V2G) technology has been gradually recognized as a valid solution to provide various ancillary service provisions for power systems. Many studies have developed model-based optimization methods for EV dispatch problems. Nevertheless, this type of method cannot effectively handle the highly dynamic and stochastic environment due to the complexity of power systems. Reinforcement learning (RL), a model-free and online learning method, can capture various uncertainties through numerous interactions with the environment and adapt to various state conditions in real-time. As a result, using advanced RL algorithms to solve various EV dispatch problems has attracted a surge of attention in recent years, leading to many outstanding research papers and important findings. This paper provides a comprehensive review of popular RL algorithms categorized by single-agent RL and multi-agent RL, and summarizes how these advanced algorithms can be applied to various EV dispatch problems, including grid-to-vehicle (G2V), vehicle-to-home (V2H), and V2G. Finally, key challenges and important future research directions are discussed, which involve five aspects: (a) data quality and availability; (b) environment setup; (c) safety and robustness; (d) training performance; and (e) real-world deployment.}
}
@article{WANG202222,
title = {Enriching query semantics for code search with reinforcement learning},
journal = {Neural Networks},
volume = {145},
pages = {22-32},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003877},
author = {Chaozheng Wang and Zhenhao Nong and Cuiyun Gao and Zongjie Li and Jichuan Zeng and Zhenchang Xing and Yang Liu},
keywords = {Code search, Query semantics, Semantic enrichment, Reinforcement learning},
abstract = {Code search is a common practice for developers during software implementation. The challenges of accurate code search mainly lie in the knowledge gap between source code and natural language (i.e., queries). Due to the limited code-query pairs and large code-description pairs available, the prior studies based on deep learning techniques focus on learning the semantic matching relation between source code and corresponding description texts for the task, and hypothesize that the semantic gap between descriptions and user queries is marginal. In this work, we found that the code search models trained on code-description pairs may not perform well on user queries, which indicates the semantic distance between queries and code descriptions. To mitigate the semantic distance for more effective code search, we propose QueCos, a Query-enriched Code search model. QueCos learns to generate semantic enriched queries to capture the key semantics of given queries with reinforcement learning (RL). With RL, the code search performance is considered as a reward for producing accurate semantic enriched queries. The enriched queries are finally employed for code search. Experiments on the benchmark datasets show that QueCos can significantly outperform the state-of-the-art code search models.}
}
@article{LIU2021109347,
title = {A new hybrid model based on secondary decomposition, reinforcement learning and SRU network for wind turbine gearbox oil temperature forecasting},
journal = {Measurement},
volume = {178},
pages = {109347},
year = {2021},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2021.109347},
url = {https://www.sciencedirect.com/science/article/pii/S0263224121003432},
author = {Hui Liu and Chengqing Yu and Chengming Yu},
keywords = {Oil temperature forecasting, Simple Recurrent Unit, Reinforcement Learning, Secondary decomposition method},
abstract = {Oil temperature forecasting technology can realize real-time detection of the gearbox status of wind turbines. To make the oil temperature forecasting more accurate, a new hybrid model is presented in this study. The main modeling process of the presented method consists of three main steps. In step I, the proposed secondary decomposition method is utilized to preprocess the raw oil temperature data. In step II, the feature selection algorithm based on reinforcement learning selects the features of each sub-series. In step III, the simple recurrent unit network establishes forecasting models for each sub-series after feature selection and obtains the final forecasting results. By analyzing the forecasting results of multiple experiments, it can be concluded that: (1) the presented hybrid model can obtain satisfying forecasting results. Its RMSE values are 0.1101 °C, 0.1683 °C, and 0.1784 °C in three cases. (2) The presented hybrid model can get higher forecasting accuracy than the seventeen alternative models and six existing models in all cases. It improves the performance of traditional neural networks by over 90 percent.}
}
@article{LIU2019102,
title = {Reinforcement learning-based cell selection in sparse mobile crowdsensing},
journal = {Computer Networks},
volume = {161},
pages = {102-114},
year = {2019},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619303706},
author = {Wenbin Liu and Leye Wang and En Wang and Yongjian Yang and Djamal Zeghlache and Daqing Zhang},
keywords = {Mobile crowdsensing, Cell selection, Reinforcement learning, Compressive sensing},
abstract = {Sparse Mobile Crowdsensing (MCS) is a novel MCS paradigm which allows us to use the mobile devices to collect sensing data from only a small subset of cells (sub-areas) in the target sensing area while intelligently inferring the data of other cells with quality guarantee. Since selecting sensed data from different cell sets will probably lead to diverse levels of inference data quality, cell selection (i.e., choosing which cells in the target area to collect sensed data from participants) is a critical issue that will impact the total amount of data that requires to be collected (i.e., data collection costs) for ensuring a certain level of data quality. To address this issue, this paper proposes the reinforcement learning-based cell selection algorithm for Sparse MCS. First, we model the key concepts in reinforcement learning including state, action, and reward, and then propose a Q-learning based cell selection algorithm. To deal with the large state space, we employ the deep Q-network to learn the Q-function that can help decide which cell is a better choice under a certain state during cell selection. Then, we modify the Q-network to a deep recurrent Q-network with LSTM to catch the temporal patterns and handle partial observability. Furthermore, we leverage the transfer learning techniques to relieve the dependency on a large amount of training data. Experiments on various real-life sensing datasets verify the effectiveness of our proposed algorithms over the state-of-the-art mechanisms in Sparse MCS by reducing up to 20% of sensed cells with the same data inference quality guarantee.}
}
@article{SHIN2019282,
title = {Reinforcement Learning – Overview of recent progress and implications for process control},
journal = {Computers & Chemical Engineering},
volume = {127},
pages = {282-294},
year = {2019},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419300754},
author = {Joohyun Shin and Thomas A. Badgwell and Kuang-Hung Liu and Jay H. Lee},
keywords = {Reinforcement Learning, Mathematical programming, Model predictive control, Process control, Strategic/operational decision-making},
abstract = {This paper provides an introduction to Reinforcement Learning (RL) technology, summarizes recent developments in this area, and discusses their potential implications for the field of process control, and more generally, of operational decision-making. The paper begins with an introduction to RL that allows an agent to learn, through trial and error, the best way to accomplish a task. We then highlight new developments in RL that have led to the recent wave of applications and media interest. A comparison of the key features of RL and mathematical programming based methods (e.g., model predictive control) is then presented to clarify their similarities and differences. This is followed by an assessment of several ways that RL technology can potentially be used in process control and operational decision applications. A final section summarizes our conclusions and lists directions for future RL research that may improve its relevance for the process systems engineering field.}
}
@article{CARLUCHO201871,
title = {Adaptive low-level control of autonomous underwater vehicles using deep reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {107},
pages = {71-86},
year = {2018},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018301519},
author = {Ignacio Carlucho and Mariano {De Paula} and Sen Wang and Yvan Petillot and Gerardo G. Acosta},
keywords = {Autonomous robot, Deep reinforcement learning, AUV, Adaptive low-level control},
abstract = {Low-level control of autonomous underwater vehicles (AUVs) has been extensively addressed by classical control techniques. However, the variable operating conditions and hostile environments faced by AUVs have driven researchers towards the formulation of adaptive control approaches. The reinforcement learning (RL) paradigm is a powerful framework which has been applied in different formulations of adaptive control strategies for AUVs. However, the limitations of RL approaches have lead towards the emergence of deep reinforcement learning which has become an attractive and promising framework for developing real adaptive control strategies to solve complex control problems for autonomous systems. However, most of the existing applications of deep RL use video images to train the decision making artificial agent but obtaining camera images only for an AUV control purpose could be costly in terms of energy consumption. Moreover, the rewards are not easily obtained directly from the video frames. In this work we develop a deep RL framework for adaptive control applications of AUVs based on an actor-critic goal-oriented deep RL architecture, which takes the available raw sensory information as input and as output the continuous control actions which are the low-level commands for the AUV’s thrusters. Experiments on a real AUV demonstrate the applicability of the stated deep RL approach for an autonomous robot control problem.}
}
@article{BOGACZ2007111,
title = {Short-term memory traces for action bias in human reinforcement learning},
journal = {Brain Research},
volume = {1153},
pages = {111-121},
year = {2007},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2007.03.057},
url = {https://www.sciencedirect.com/science/article/pii/S000689930700666X},
author = {Rafal Bogacz and Samuel M. McClure and Jian Li and Jonathan D. Cohen and P. Read Montague},
keywords = {Reinforcement learning, Eligibility traces, Dopamine},
abstract = {Recent experimental and theoretical work on reinforcement learning has shed light on the neural bases of learning from rewards and punishments. One fundamental problem in reinforcement learning is the credit assignment problem, or how to properly assign credit to actions that lead to reward or punishment following a delay. Temporal difference learning solves this problem, but its efficiency can be significantly improved by the addition of eligibility traces (ET). In essence, ETs function as decaying memories of previous choices that are used to scale synaptic weight changes. It has been shown in theoretical studies that ETs spanning a number of actions may improve the performance of reinforcement learning. However, it remains an open question whether including ETs that persist over sequences of actions allows reinforcement learning models to better fit empirical data regarding the behaviors of humans and other animals. Here, we report an experiment in which human subjects performed a sequential economic decision game in which the long-term optimal strategy differed from the strategy that leads to the greatest short-term return. We demonstrate that human subjects' performance in the task is significantly affected by the time between choices in a surprising and seemingly counterintuitive way. However, this behavior is naturally explained by a temporal difference learning model which includes ETs persisting across actions. Furthermore, we review recent findings that suggest that short-term synaptic plasticity in dopamine neurons may provide a realistic biophysical mechanism for producing ETs that persist on a timescale consistent with behavioral observations.}
}
@article{CHEMBE2019100161,
title = {Infrastructure based spectrum sensing scheme in VANET using reinforcement learning},
journal = {Vehicular Communications},
volume = {18},
pages = {100161},
year = {2019},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2019.100161},
url = {https://www.sciencedirect.com/science/article/pii/S2214209617302243},
author = {Christopher Chembe and Douglas Kunda and Ismail Ahmedy and Rafidah {Md Noor} and Aznul Qalid {Md Sabri} and Md Asri Ngadi},
keywords = {Reinforcement learning, Spectrum sensing, Vehicle-to-infrastructure, Cognitive radio, Adaptive sensing},
abstract = {Spectrum sensing is one of the fundamental functionality performed by a cognitive radio to identify vacant radio spectrum for dynamic spectrum access (DSA). However, there are many challenges still existing before the benefits of DSA can be realized. The challenges include multipath fading, shadowing and hidden primary user (PU) problem. The challenges are more severe in vehicular communication due to unique characteristics such as dynamic topology caused by vehicle mobility. Furthermore, spectrum sensing is dependent on the activities of the PU traffic pattern which are not known in advance. In a typical cognitive radio network, the PU plays a passive role. Therefore, a sensing technique should account for traffic pattern of the PU autonomously. However, most of the proposed spectrum sensing schemes in vehicular communication assumes a static ON/OFF PU model which does not realistically model the PU traffic pattern. In this paper, we propose reinforcement learning (RL) to model the traffic pattern of the PU and use the model to predict channels likely to be free in future. The RL is implemented on road side unit (RSU) which send predicted vacant PU channels to vehicles on the road. Before the channels can be used, vehicles perform spectrum sensing. To account for multipath fading and shadowing, adaptive spectrum sensing is proposed. The results from spectrum sensing, sensing time and PU channel capacity are calculated into a scalar value and used as reward for RL at RSU. The RSU continuously update the reward for channels of interest using sensing history from passing vehicles as reward. Compared to history based schemes from literature, the RL technique proposed in this paper performs better.}
}
@article{QIU2022118790,
title = {Multi-service provision for electric vehicles in power-transportation networks towards a low-carbon transition: A hierarchical and hybrid multi-agent reinforcement learning approach},
journal = {Applied Energy},
volume = {313},
pages = {118790},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.118790},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922002379},
author = {Dawei Qiu and Yi Wang and Mingyang Sun and Goran Strbac},
keywords = {Electric vehicles, Ancillary services, Carbon intensity, Power and transportation networks, Hierarchical multi-agent reinforcement learning, Hybrid discrete–continuous action space},
abstract = {In order to achieve the target of carbon peak and carbon neutrality, electric vehicles (EVs) have increasingly received a prominent interest to electrify the transportation sector due to their advantages of mobility and flexibility on handling complicated transportation and power networks. However, it is still challenging to realize the significant potential of EVs towards an emerging low-carbon transition. Previous works have focused on vehicle-to-grid (V2G) technology that allows for an increased utilization of EVs to make arbitrage by the temporal differentials of electricity prices. Nevertheless, the economic potential of EVs flexibility may not be fully exploited lacking an appropriate business model. This paper addresses this challenge by developing a coupled power-transportation network for cooperative EVs to optimize the provision of multiple inter-dependent services, including charging service, demand management service, carbon intensity service, and balancing service. In order to unlock this value, the EVs operation problem has already been tackled using model-based optimization approaches, which may raise privacy issues since the requirement for global information and also can be time consuming due to the high variability of transportation and power networks. In this paper, we propose a model-free hierarchical and hybrid multi-agent reinforcement learning method to learn the routing and scheduling decisions of EVs in a coupled power-transportation network with the objective of optimizing multi-service provisions. To this end, EVs do not reply on any knowledge of the simulated environment and are capable of handling system uncertainties via the learning process. Extensive case studies based on a 15-bus radial power distribution network and a 9-node 12-edge transportation network are developed to show that the proposed method outperforms the conventional learning algorithms in terms of policy quality and convergence speed. Finally, the generalizability and scalability are also investigated for different environment circumstances and EV numbers.}
}
@article{SON2022104898,
title = {Value-based reinforcement learning approaches for task offloading in Delay Constrained Vehicular Edge Computing},
journal = {Engineering Applications of Artificial Intelligence},
volume = {113},
pages = {104898},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.104898},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622001336},
author = {Do Bao Son and Ta Huu Binh and Hiep Khac Vo and Binh Minh Nguyen and Huynh Thi Thanh Binh and Shui Yu},
keywords = {Vehicular Edge Computing, Deep Q-Learning, Fuzzy Logic, Quality of Experience, Offloading, Delay constraint},
abstract = {In the age of booming information technology, human-being has witnessed the need for new paradigms with both high computational capability and low latency. A potential solution is Vehicular Edge Computing (VEC). Previous work proposed a Fuzzy Deep Q-Network in Offloading scheme (FDQO) that combines Fuzzy rules and Deep Q-Network (DQN) to improve DQN’s early performance by using Fuzzy Controller (FC). However, we notice that frequent usage of FC can hinder the future growth performance of model. One way to overcome this issue is to remove Fuzzy Controller entirely. We introduced an algorithm called baseline DQN (b-DQN), represented by its two variants Static baseline DQN (Sb-DQN) and Dynamic baseline DQN (Db-DQN), to modify the exploration rate base on the average rewards of closest observations. Our findings confirm that these baseline DQN algorithms surpass traditional DQN models in terms of average Quality of Experience (QoE) in 100 time slots by about 6%, but still suffer from poor early performance (such as in the first 5 time slots). Here, we introduce baseline FDQO (b-FDQO). This algorithm has a strategy to modify the Fuzzy Logic usage instead of removing it entirely while still observing the rewards to modify the exploration rate. It brings a higher average QoE in the first 5 time slots compared to other non-fuzzy-logic algorithms by at least 55.12%, prevent the model from getting too bad result over all time slots, while having the late performance as good as that of b-DQN.}
}
@article{BRUZZONE2020107346,
title = {Reinforcement Learning control of an onshore oscillating arm Wave Energy Converter},
journal = {Ocean Engineering},
volume = {206},
pages = {107346},
year = {2020},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2020.107346},
url = {https://www.sciencedirect.com/science/article/pii/S0029801820303784},
author = {Luca Bruzzone and Pietro Fanghella and Giovanni Berselli},
keywords = {WEC, Oscillating arm, Linear Wave Theory, Reinforcement Learning, Q-learning},
abstract = {The paper discusses the application of Reinforcement Learning to the control of an onshore Wave Energy Converter (WEC). The proposed WEC features a simple and low-cost architecture. It is characterized by an oscillating floating rocker arm which moves a four bar linkage in the vertical plane. A mechanical rectifier, based on two one-way clutches and a multiplier gearbox, transforms the low speed oscillating motion of the four bar into a higher-speed unidirectional rotation of the electrical generator. The dynamic model of the WEC, based on multibody approach and Linear Wave Theory, is presented. Then a Reinforcement Learning (RL) algorithm, a Q-learning method, is applied to dynamically adjust the generator speed-torque ratio as a function of the sea state. Simulation results show the effectiveness of this model-free adaptive control in tuning the system in order to maximize the generated power. Starting from a simple monochromatic model of the sea, the presented approach is verified according to sea conditions of increasing complexity, and finally to long term time series, obtained from measurements of real sea states in the considered geographical region. The tuning of the hyper-parameters of RL algorithm with respect to the speed of convergence and optimality of generated power is also discussed.}
}
@article{BELZNER2021102620,
title = {Synthesizing safe policies under probabilistic constraints with reinforcement learning and Bayesian model checking},
journal = {Science of Computer Programming},
volume = {206},
pages = {102620},
year = {2021},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2021.102620},
url = {https://www.sciencedirect.com/science/article/pii/S0167642321000137},
author = {Lenz Belzner and Martin Wirsing},
keywords = {Policy synthesis, Constrained Markov decision process, Safe reinforcement learning, Bayesian model checking, Probabilistic constraints},
abstract = {We propose to leverage epistemic uncertainty about constraint satisfaction of a reinforcement learner in safety critical domains. We introduce a framework for specification of requirements for reinforcement learners in constrained settings, including confidence about results. We show that an agent's confidence in constraint satisfaction provides a useful signal for balancing optimization and safety in the learning process.}
}
@article{HAYAMANISHIDA2020101853,
title = {A framework to shift basins of attraction of gene regulatory networks through batch reinforcement learning},
journal = {Artificial Intelligence in Medicine},
volume = {107},
pages = {101853},
year = {2020},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2020.101853},
url = {https://www.sciencedirect.com/science/article/pii/S0933365719310887},
author = {Cyntia Eico {Hayama Nishida} and Reinaldo A. {Costa Bianchi} and Anna Helena {Reali Costa}},
keywords = {Reinforcement learning, Gene regulatory network, Basin of attraction},
abstract = {A major challenge in gene regulatory networks (GRN) of biological systems is to discover when and what interventions should be applied to shift them to healthy phenotypes. A set of gene activity profiles, called basin of attraction (BOA), takes this network to a specific phenotype; therefore, a healthy BOA leads the GRN to a healthy phenotype. However, without the complete observability of the genes, it is not possible to identify whether the current BOA is healthy. In this article we investigate external interventions in GRN with partial observability aiming to bring it to healthy BOAs. We propose a new batch reinforcement learning method (BRL), called mSFQI, to define intervention strategies based on the probabilities of the gene activity profiles being in healthy BOAs, which are calculated from a set of previous observed experiences. BRL uses approximation functions and repeated applications of previous experiences to accelerate learning. Results demonstrate that our proposal can quickly shift a partially observable GRN to healthy BOAs, while reducing the number of interventions. In addition, when observability is poor, mSFQI produces better results when the probabilities for a greater amount of previous observations are available.}
}
@article{HASAN2019107,
title = {Dynamic multi-objective optimisation using deep reinforcement learning: benchmark, algorithm and an application to identify vulnerable zones based on water quality},
journal = {Engineering Applications of Artificial Intelligence},
volume = {86},
pages = {107-135},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619302003},
author = {Md Mahmudul Hasan and Khin Lwin and Maryam Imani and Antesar Shabut and Luiz Fernando Bittencourt and M.A. Hossain},
keywords = {Dynamic environment, Reinforcement learning, Deep Q network, Water quality resilience, Meta-policy selection, Artificial intelligence},
abstract = {Dynamic multi-objective optimisation problem (DMOP) has brought a great challenge to the reinforcement learning (RL) research area due to its dynamic nature such as objective functions, constraints and problem parameters that may change over time. This study aims to identify the lacking in the existing benchmarks for multi-objective optimisation for the dynamic environment in the RL settings. Hence, a dynamic multi-objective testbed has been created which is a modified version of the conventional deep-sea treasure (DST) hunt testbed. This modified testbed fulfils the changing aspects of the dynamic environment in terms of the characteristics where the changes occur based on time. To the authors’ knowledge, this is the first dynamic multi-objective testbed for RL research, especially for deep reinforcement learning. In addition to that, a generic algorithm is proposed to solve the multi-objective optimisation problem in a dynamic constrained environment that maintains equilibrium by mapping different objectives simultaneously to provide the most compromised solution that closed to the true Pareto front (PF). As a proof of concept, the developed algorithm has been implemented to build an expert system for a real-world scenario using Markov decision process to identify the vulnerable zones based on water quality resilience in São Paulo, Brazil. The outcome of the implementation reveals that the proposed parity-Q deep Q network (PQDQN) algorithm is an efficient way to optimise the decision in a dynamic environment. Moreover, the result shows PQDQN algorithm performs better compared to the other state-of-the-art solutions both in the simulated and the real-world scenario.}
}
@article{SHAHAB2022107787,
title = {Reinforcement-Learning designs droplet microfluidic networks},
journal = {Computers & Chemical Engineering},
volume = {161},
pages = {107787},
year = {2022},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.107787},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422001284},
author = {Mohammad Shahab and Raghunathan Rengaswamy},
keywords = {Reinforcement-learning, Microfluidic network, Functionality, Solution quality, Droplet sorting},
abstract = {Droplet microfluidics provides a platform for realizing programmable and flexible devices for performing diverse process operations on a single chip. Although recent developments have incorporated basic functionalities in droplet-based devices such as droplet sorting, sequencing, synchronization, etc., the design of microfluidic networks for specific functionalities remains to be of research interest. This requires the development of integrated devices that can perform complex operations and incorporate multiple functionalities on a single microfluidic chip. In this work, a state-of-the-art Reinforcement-learning (RL) algorithm based on Temporal-difference Q-learning is used to identify discrete microfluidic networks for a given functionality. This is the first time that a machine learning-based approach is deployed for discovering integrated network designs. The implementation of the algorithm has been illustrated through a combinatorial sorting problem, where the objective is to sort all the incoming droplet sequences at the inlet for two distinct droplet types. The algorithm identifies unapparent network designs using RL-based decisions in a microfluidic network space. The superiority of the RL-based algorithm is established against random search based on execution time, and against Genetic Algorithms (GA) based on solution quality. This work could be a first step towards achieving the ultimate goal of developing an unified droplet microfluidic design framework that can cater to a large number of drops and varying functionalities.}
}
@article{SOLINAS2021104235,
title = {Peak shaving in district heating exploiting reinforcement learning and agent-based modelling},
journal = {Engineering Applications of Artificial Intelligence},
volume = {102},
pages = {104235},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104235},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621000828},
author = {Francesco M. Solinas and Lorenzo Bottaccioli and Elisa Guelpa and Vittorio Verda and Edoardo Patti},
keywords = {Agent-based model, Demand-side management, District heating, Peak shaving, Reinforcement learning},
abstract = {District Heating (DH) technology is considered to be a sustainable and quasi-renewable way of producing and distributing hot water along the city to heat buildings. However, the main obstacle to wider adoption of DH technology is represented by the thermal request peak in the morning hours of winter days, especially in Mediterranean countries. In this paper, this peak-shaving problem is tackled by combining three different approaches. A thermodynamic model is used to monitor the buildings’ thermal response to energy profile modifications. An agent-based model is adopted in order to represent the end-users and their adaptability to variations of temperatures in buildings. Finally, a Reinforcement Learning algorithm is used to optimally mediate between two needs: on the one hand, a set of anticipations and delays is applied to the energy profiles in order to reduce the thermal request peak. On the other hand, the algorithm learns by trial and error the individual agents’ sensitivity to thermal comfort, avoiding drastic modifications for the most sensitive users. The experiments carried out in the DH network in Torino (north-west of Italy) demonstrate that the proposed approach, compared with a literature solution chosen as a baseline, allows to achieve better results in terms of overall performances and speed of convergence.}
}
@article{DUTECH201136,
title = {A reinforcement learning approach to instrumental contingency degradation in rats},
journal = {Journal of Physiology-Paris},
volume = {105},
number = {1},
pages = {36-44},
year = {2011},
note = {Computational Neuroscience: Neurocomp 2010},
issn = {0928-4257},
doi = {https://doi.org/10.1016/j.jphysparis.2011.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0928425711000209},
author = {Alain Dutech and Etienne Coutureau and Alain R. Marchand},
keywords = {Rats, Instrumental, Prefrontal cortex, Contingency degradation, Simulation, Model-free learning, SARSA},
abstract = {Goal-directed action involves a representation of action consequences. Adapting to changes in action-outcome contingency requires the prefrontal region. Indeed, rats with lesions of the medial prefrontal cortex do not adapt their free operant response when food delivery becomes unrelated to lever-pressing. The present study explores the bases of this deficit through a combined behavioural and computational approach. We show that lesioned rats retain some behavioural flexibility and stop pressing if this action prevents food delivery. We attempt to model this phenomenon in a reinforcement learning framework. The model assumes that distinct action values are learned in an incremental manner in distinct states. The model represents states as n-uplets of events, emphasizing sequences rather than the continuous passage of time. Probabilities of lever-pressing and visits to the food magazine observed in the behavioural experiments are first analyzed as a function of these states, to identify sequences of events that influence action choice. Observed action probabilities appear to be essentially function of the last event that occurred, with reward delivery and waiting significantly facilitating magazine visits and lever-pressing respectively. Behavioural sequences of normal and lesioned rats are then fed into the model, action values are updated at each event transition according to the SARSA algorithm, and predicted action probabilities are derived through a softmax policy. The model captures the time course of learning, as well as the differential adaptation of normal and prefrontal lesioned rats to contingency degradation with the same parameters for both groups. The results suggest that simple temporal difference algorithms with low learning rates can largely account for instrumental learning and performance. Prefrontal lesioned rats appear to mainly differ from control rats in their low rates of visits to the magazine after a lever press, and their inability to initially detect weak contingency changes.}
}
@incollection{KHAMASSI2013441,
title = {Chapter 22 - Medial prefrontal cortex and the adaptive regulation of reinforcement learning parameters},
editor = {V.S. Chandrasekhar Pammi and Narayanan Srinivasan},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {202},
pages = {441-464},
year = {2013},
booktitle = {Decision Making},
issn = {0079-6123},
doi = {https://doi.org/10.1016/B978-0-444-62604-2.00022-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780444626042000228},
author = {Mehdi Khamassi and Pierre Enel and Peter Ford Dominey and Emmanuel Procyk},
keywords = {reinforcement learning, metalearning, decision making, medial prefrontal cortex, computational modeling, neurorobotics},
abstract = {Converging evidence suggest that the medial prefrontal cortex (MPFC) is involved in feedback categorization, performance monitoring, and task monitoring, and may contribute to the online regulation of reinforcement learning (RL) parameters that would affect decision-making processes in the lateral prefrontal cortex (LPFC). Previous neurophysiological experiments have shown MPFC activities encoding error likelihood, uncertainty, reward volatility, as well as neural responses categorizing different types of feedback, for instance, distinguishing between choice errors and execution errors. Rushworth and colleagues have proposed that the involvement of MPFC in tracking the volatility of the task could contribute to the regulation of one of RL parameters called the learning rate. We extend this hypothesis by proposing that MPFC could contribute to the regulation of other RL parameters such as the exploration rate and default action values in case of task shifts. Here, we analyze the sensitivity to RL parameters of behavioral performance in two monkey decision-making tasks, one with a deterministic reward schedule and the other with a stochastic one. We show that there exist optimal parameter values specific to each of these tasks, that need to be found for optimal performance and that are usually hand-tuned in computational models. In contrast, automatic online regulation of these parameters using some heuristics can help producing a good, although non-optimal, behavioral performance in each task. We finally describe our computational model of MPFC–LPFC interaction used for online regulation of the exploration rate and its application to a human–robot interaction scenario. There, unexpected uncertainties are produced by the human introducing cued task changes or by cheating. The model enables the robot to autonomously learn to reset exploration in response to such uncertain cues and events. The combined results provide concrete evidence specifying how prefrontal cortical subregions may cooperate to regulate RL parameters. It also shows how such neurophysiologically inspired mechanisms can control advanced robots in the real world. Finally, the model's learning mechanisms that were challenged in the last robotic scenario provide testable predictions on the way monkeys may learn the structure of the task during the pretraining phase of the previous laboratory experiments.}
}
@incollection{DAW2014283,
title = {Chapter 15 - Value Learning through Reinforcement: The Basics of Dopamine and Reinforcement Learning},
editor = {Paul W. Glimcher and Ernst Fehr},
booktitle = {Neuroeconomics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {283-298},
year = {2014},
isbn = {978-0-12-416008-8},
doi = {https://doi.org/10.1016/B978-0-12-416008-8.00015-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160088000152},
author = {Nathaniel D. Daw and Philippe N. Tobler},
keywords = {Dopamine, Reinforcement learning},
abstract = {In order to choose advantageously in many circumstances, the values of choice alternatives have to be learned from experience. We provide an introduction to theoretical and experimental work on reinforcement learning, that is, trial-and-error learning to obtain rewards or avoid punishments. We introduce one version, the temporal-difference learning model, and review evidence that its predictions relate to the firing properties of midbrain dopamine neurons and to activity recorded with functional neuroimaging in humans. We also present evidence that this computational and neurophysiological mechanism affects human and animal behavior in decision and conditioning tasks.}
}
@article{PENG2021106442,
title = {Online integral reinforcement learning control for an uncertain highly flexible aircraft using state and output feedback},
journal = {Aerospace Science and Technology},
volume = {109},
pages = {106442},
year = {2021},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2020.106442},
url = {https://www.sciencedirect.com/science/article/pii/S127096382031124X},
author = {Chi Peng and Jianjun Ma},
keywords = {Integral reinforcement learning, Highly flexible aircraft, Uncertain, Full state feedback, Output feedback},
abstract = {This paper discussed the data-driven control design of a highly flexible aircraft (HFA) with uncertainties. By introducing an integral reinforcement learning (IRL) technique, a novel online model-free control strategy is developed to stabilize the uncertain HFA. Full state feedback with all states measurable and output feedback using an online reinforcement learning scheme to estimate unmeasurable states are considered. With the help of Lyapunov's direct method and under some system assumptions, it is rigorously proved that the proposed IRL based controller can guarantee the asymptotic stability of the closed-loop system. Simulation results show the effectiveness of the proposed scheme.}
}
@article{LI2023121359,
title = {Physics-model-free heat-electricity energy management of multiple microgrids based on surrogate model-enabled multi-agent deep reinforcement learning},
journal = {Applied Energy},
volume = {346},
pages = {121359},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121359},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923007237},
author = {Sichen Li and Weihao Hu and Di Cao and Zhe Chen and Qi Huang and Frede Blaabjerg and Kaiji Liao},
keywords = {Energy management of multiple microgrids, Combined heat and power, Sparse variational Gaussian processes, Multi-agent deep reinforcement learning},
abstract = {Reliability and cost-effectiveness in the operation of the multiple microgrid (MMG) system depend on the skillful management of its energy resources. Traditional energy management approaches are physics-model-based, which rely on the precise system parameters (e.g., line parameters) of the electricity and heat network. It is difficult to establish the precise system parameters in practice because they depend on a variety of factors. In this context, this paper proposes a physics-model-free control framework for the energy management of MMGs with heat-electricity energy, consisting of proposed surrogate model and multi-agent deep reinforcement learning (MADRL) approach. An important step is to use historical data to train a surrogate model in supervised manner that can imitate the realistic power and thermal flow calculations. Meanwhile, the energy management problem is reformulated as a Markov game. It is solved by the proposed MADRL-based approach by modeling each MG controller as an agent with a specific objective. The historical trajectories representation, parameter space technology, and deep dense architecture in reinforcement learning are introduced in MADRL to overcome the negative impact brought by the time series data from the input state on the decision-making process and construct an efficient exploration mechanism to overcome inefficient optimization of the MMG system in a multi-agent setting. During training period of MADRL, the trained surrogate models integrate into the environment of the MADRL, which can develop optimal energy management strategy based on the continuous interaction with the surrogate models. The proposed surrogate model enabled MADRL approach can reduce the reliance on precise physical systems and prevent having an impact on the real system while being trained involving trial and error process. Simulation results demonstrate the effectiveness of the proposed control framework.}
}
@article{MONTESERIN20132182,
title = {A reinforcement learning approach to improve the argument selection effectiveness in argumentation-based negotiation},
journal = {Expert Systems with Applications},
volume = {40},
number = {6},
pages = {2182-2188},
year = {2013},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0957417412011694},
author = {Ariel Monteserin and Analía Amandi},
keywords = {Reinforcement learning, Argument selection, Argumentation-based negotiation, Autonomous agents},
abstract = {Deciding what argument to utter during a negotiation is a key part of the strategy to reach an expected agreement. An agent, which is arguing during a negotiation, must decide what arguments are the best to persuade the opponent. In fact, in each negotiation step, the agent must select an argument from a set of candidate arguments by applying some selection policy. By following this policy, the agent observes some factors of the negotiation context (for instance, trust in the opponent and expected utility of the negotiated agreement). Usually, argument selection policies are defined statically. However, as the negotiation context varies from a negotiation to another, defining a static selection policy is not useful. Therefore, the agent should modify its selection policy in order to adapt it to the different negotiation contexts as the agent gains experience. In this paper, we present a reinforcement learning approach that allows the agent to improve the argument selection effectiveness by updating the argument selection policy. To carry out this goal, the argument selection mechanism is represented as a reinforcement learning model. We tested this approach in a multiagent system, in a stationary as well as in a dynamic environment. We obtained promising results in both.}
}
@article{TEJEDOR2020101836,
title = {Reinforcement learning application in diabetes blood glucose control: A systematic review},
journal = {Artificial Intelligence in Medicine},
volume = {104},
pages = {101836},
year = {2020},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2020.101836},
url = {https://www.sciencedirect.com/science/article/pii/S0933365718304548},
author = {Miguel Tejedor and Ashenafi Zebene Woldaregay and Fred Godtliebsen},
keywords = {Reinforcement learning, Blood glucose control, Artificial pancreas, Closed-loop, Insulin infusion},
abstract = {Background
Reinforcement learning (RL) is a computational approach to understanding and automating goal-directed learning and decision-making. It is designed for problems which include a learning agent interacting with its environment to achieve a goal. For example, blood glucose (BG) control in diabetes mellitus (DM), where the learning agent and its environment are the controller and the body of the patient respectively. RL algorithms could be used to design a fully closed-loop controller, providing a truly personalized insulin dosage regimen based exclusively on the patient’s own data.
Objective
In this review we aim to evaluate state-of-the-art RL approaches to designing BG control algorithms in DM patients, reporting successfully implemented RL algorithms in closed-loop, insulin infusion, decision support and personalized feedback in the context of DM.
Methods
An exhaustive literature search was performed using different online databases, analyzing the literature from 1990 to 2019. In a first stage, a set of selection criteria were established in order to select the most relevant papers according to the title, keywords and abstract. Research questions were established and answered in a second stage, using the information extracted from the articles selected during the preliminary selection.
Results
The initial search using title, keywords, and abstracts resulted in a total of 404 articles. After removal of duplicates from the record, 347 articles remained. An independent analysis and screening of the records against our inclusion and exclusion criteria defined in Methods section resulted in removal of 296 articles, leaving 51 relevant articles. A full-text assessment was conducted on the remaining relevant articles, which resulted in 29 relevant articles that were critically analyzed. The inter-rater agreement was measured using Cohen Kappa test, and disagreements were resolved through discussion.
Conclusions
The advances in health technologies and mobile devices have facilitated the implementation of RL algorithms for optimal glycemic regulation in diabetes. However, there exists few articles in the literature focused on the application of these algorithms to the BG regulation problem. Moreover, such algorithms are designed for control tasks as BG adjustment and their use have increased recently in the diabetes research area, therefore we foresee RL algorithms will be used more frequently for BG control in the coming years. Furthermore, in the literature there is a lack of focus on aspects that influence BG level such as meal intakes and physical activity (PA), which should be included in the control problem. Finally, there exists a need to perform clinical validation of the algorithms.}
}
@article{YANG2018388,
title = {An investor sentiment reward-based trading system using Gaussian inverse reinforcement learning algorithm},
journal = {Expert Systems with Applications},
volume = {114},
pages = {388-401},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.07.056},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418304810},
author = {Steve Y. Yang and Yangyang Yu and Saud Almahdi},
keywords = {Investor sentiment, Inverse reinforcement learning, Support vector machine learning, Sentiment reward},
abstract = {Investor sentiment has been shown as an important factor that influences market returns, and a number of profitable trading systems have been proposed by taking advantage of investor sentiment signals. In this paper, we aim to design an investor sentiment reward-based trading system using Gaussian inverse reinforcement learning method. Our hypothesis is that while markets interact with investor’s sentiment, there exists an intrinsic mapping between investor’s sentiment and market conditions revealing future market directions. We propose an investor sentiment reward based trading system aimed at extracting only signals that generate either negative or positive market responses. Such a reward extraction mechanism is based not only on market returns but also market volatility representing a succinct and robust feature space. The back-test results show that the proposed sentiment reward-based trading system is superior to various benchmark strategies on S&P 500 index and market-based ETFs as well as few other existing news sentiment-based trading signals. Moreover, we find that sentiment reward trading system is much more effective in a volatile market, but it is sensitive to transaction costs.}
}
@article{SACHIO2021510,
title = {Simultaneous Process Design and Control Optimization using Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {3},
pages = {510-515},
year = {2021},
note = {16th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.293},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321010661},
author = {Steven Sachio and Antonio E. del-Rio Chanona and Panagiotis Petsagkourakis},
keywords = {process design, process control, reinforcement learning, policy gradient, optimization},
abstract = {The performance of a chemical plant is highly affected by its design and control. A design cannot be accurately evaluated without its controls and vice versa. To optimally address design and control simultaneously, one must formulate a bi-level mixed-integer nonlinear program with a dynamic optimization problem as the inner problem; this is intractable. However, by computing an optimal policy using reinforcement learning, a controller with a closed-form expression can be computed and embedded into the mathematical program. In this work, an approach that uses a policy gradient method to compute the optimal policy, which is then embedded into the mathematical program is proposed. The approach is tested in a tank design case study and the performance of the controller is evaluated. It is shown that the proposed approach outperforms current state-of-the-art control strategies. This opens a whole new range of possibilities to address the simultaneous design and control of engineering systems.}
}
@article{VARGA2020247,
title = {Reducing human efforts in video segmentation annotation with reinforcement learning},
journal = {Neurocomputing},
volume = {405},
pages = {247-258},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.02.127},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220308110},
author = {Viktor Varga and András Lőrincz},
keywords = {Video segmentation, Interactive annotation, Reinforcement learning},
abstract = {Manual annotation of video segmentation datasets requires an immense amount of human effort, thus, reduction of human annotation costs is an active topic of research. While many papers deal with the propagation of masks through frames of a video, only a few results attempt to optimize annotation task selection. In this paper we present a deep learning based solution to the latter problem and train it using Reinforcement Learning. Our approach utilizes a modified version of the Dueling Deep Q-Network sharing weight parameters across the temporal axis of the video. This technique enables the trained agent to select annotation tasks from the whole video. We evaluate our annotation task selection method by means of a hierarchical supervoxel segmentation based mask propagation algorithm.}
}
@article{TAN20101465,
title = {A self-organizing neural architecture integrating desire, intention and reinforcement learning},
journal = {Neurocomputing},
volume = {73},
number = {7},
pages = {1465-1477},
year = {2010},
note = {Advances in Computational Intelligence and Learning},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2009.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925231209004196},
author = {Ah-Hwee Tan and Yu-Hong Feng and Yew-Soon Ong},
keywords = {Reinforcement learning, Plan learning, Self-organizing neural networks, BDI, Minefield navigation},
abstract = {This paper presents a self-organizing neural architecture that integrates the features of belief, desire, and intention (BDI) systems with reinforcement learning. Based on fusion Adaptive Resonance Theory (fusion ART), the proposed architecture provides a unified treatment for both intentional and reactive cognitive functionalities. Operating with a sense-act-learn paradigm, the low level reactive module is a fusion ART network that learns action and value policies across the sensory, motor, and feedback channels. During performance, the actions executed by the reactive module are tracked by a high level intention module (also a fusion ART network) that learns to associate sequences of actions with context and goals. The intention module equips the architecture with deliberative planning capabilities, enabling it to purposefully maintain an agenda of actions to perform and to reduce the need of constantly sensing the environment. Through reinforcement learning, plans can also be evaluated and refined without the rigidity of user-defined plans. We examine two strategies for combining the intention and reactive modules for decision making in a real time environment. Our experiments based on a minefield navigation domain show that the integrated architecture is able to learn plans efficiently, achieve good plan utilization, and combine both intentional and reactive action execution to yield a robust performance.}
}
@article{XIE2022110357,
title = {An active-controlled heaving plate breakwater trained by an intelligent framework based on deep reinforcement learning},
journal = {Ocean Engineering},
volume = {244},
pages = {110357},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.110357},
url = {https://www.sciencedirect.com/science/article/pii/S002980182101653X},
author = {Yulin Xie and Xizeng Zhao and Min Luo},
keywords = {Active-controlled heaving plate, Control strategy, Deep reinforcement learning, Numerical flume, Wave dissipation},
abstract = {This paper discusses the application of Deep Reinforcement Learning (DRL) to the control of a heaving plate breakwater. It is the first time that the DRL framework is utilized to find the optimal strategy for wave dissipation. The dynamic model of the wave-plate interaction, based on an in-house Computational Fluid Dynamics (CFD) solver, is presented. After training, exciting results show that the wave dissipation performance of the active-controlled heaving plate is more outstanding than that of the passive heaving plate, especially under comparatively long period waves. Meanwhile, the availability of the control strategy to different wave conditions is proved. The Fast Fourier Transform (FFT) analyses indicate that the heaving motion of the active-controlled plate can reduce the amplitude of the fundamental component of the transmitted wave, compared to the fixed plate. Finally, the influence of the hyper-parameters on the DRL convergence rate is discussed. This study proves the potential of DRL+CFD in actively dissipating ocean waves.}
}
@article{DANESHVARGARMROODI2023106147,
title = {Optimal dispatch of an energy hub with compressed air energy storage: A safe reinforcement learning approach},
journal = {Journal of Energy Storage},
volume = {57},
pages = {106147},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2022.106147},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X22021363},
author = {Alireza {Daneshvar Garmroodi} and Fuzhan Nasiri and Fariborz Haghighat},
keywords = {Compressed air energy storage, Energy management, Energy hub, Safe reinforcement learning},
abstract = {With the advancements in renewable energy and energy storage technologies, the energy hubs (EH) have been emerging in recent years. The scheduling of EH is a challenging task due to the need to incorporate uncertainties at energy supply and load side. The existing model-based optimization approaches proposed for the above purpose have limitations in terms of solution accuracy and computational efficiency, which makes hinders their applications. This paper proposes a model-free, safe deep reinforcement learning (DRL) approach, using primal-dual optimization and imitation learning, for optimal scheduling of an EH that includes a tri-generative advanced adiabatic compressed air energy storage (AA-CAES). First, the operation of an AA-CAES under off-design conditions is modeled and linearized using a mixed-integer linear programming (MILP). Then, a safe DRL approach is proposed with training and testing steps considering a case study. The performance of the proposed approach in reducing operational cost and satisfying constraints is compared to the state-of-the-art DRL algorithms as well as a deterministic MILP approach. In addition, the generalization of the proposed approach is examined on a test-set. Finally, the effect of off-design conditions of a tri-generative AA-CAES on the optimal dispatch strategy is investigated. The results indicate that the proposed approach can effectively reduce the operational cost and satisfy the operational constraints.}
}
@article{FRANKENHUIS201994,
title = {Enriching behavioral ecology with reinforcement learning methods},
journal = {Behavioural Processes},
volume = {161},
pages = {94-100},
year = {2019},
note = {Behavioral Evolution},
issn = {0376-6357},
doi = {https://doi.org/10.1016/j.beproc.2018.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0376635717303637},
author = {Willem E. Frankenhuis and Karthik Panchanathan and Andrew G. Barto},
keywords = {Adaptation, Evolution, Development, Learning, Dynamic programming, Reinforcement learning},
abstract = {This article focuses on the division of labor between evolution and development in solving sequential, state-dependent decision problems. Currently, behavioral ecologists tend to use dynamic programming methods to study such problems. These methods are successful at predicting animal behavior in a variety of contexts. However, they depend on a distinct set of assumptions. Here, we argue that behavioral ecology will benefit from drawing more than it currently does on a complementary collection of tools, called reinforcement learning methods. These methods allow for the study of behavior in highly complex environments, which conventional dynamic programming methods do not feasibly address. In addition, reinforcement learning methods are well-suited to studying how biological mechanisms solve developmental and learning problems. For instance, we can use them to study simple rules that perform well in complex environments. Or to investigate under what conditions natural selection favors fixed, non-plastic traits (which do not vary across individuals), cue-driven-switch plasticity (innate instructions for adaptive behavioral development based on experience), or developmental selection (the incremental acquisition of adaptive behavior based on experience). If natural selection favors developmental selection, which includes learning from environmental feedback, we can also make predictions about the design of reward systems. Our paper is written in an accessible manner and for a broad audience, though we believe some novel insights can be drawn from our discussion. We hope our paper will help advance the emerging bridge connecting the fields of behavioral ecology and reinforcement learning.}
}
@article{SHARMA2010675,
title = {Synergizing reinforcement learning and game theory—A new direction for control},
journal = {Applied Soft Computing},
volume = {10},
number = {3},
pages = {675-688},
year = {2010},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2009.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S1568494609002117},
author = {Rajneesh Sharma and M. Gopal},
keywords = {Reinforcement learning, Game theory, Markov games, Markov game based RL control},
abstract = {Reinforcement learning (RL) has now evolved as a major technique for adaptive optimal control of nonlinear systems. However, majority of the RL algorithms proposed so far impose a strong constraint on the structure of environment dynamics by assuming that it operates as a Markov decision process (MDP). An MDP framework envisages a single agent operating in a stationary environment thereby limiting the scope of application of RL to control problems. Recently, a new direction of research has focused on proposing Markov games as an alternative system model to enhance the generality and robustness of the RL based approaches. This paper aims to present this new direction that seeks to synergize broad areas of RL and Game theory, as an interesting and challenging avenue for designing intelligent and reliable controllers. First, we briefly review some representative RL algorithms for the sake of completeness and then describe the recent direction that seeks to integrate RL and game theory. Finally, open issues are identified and future research directions outlined.}
}
@article{BOUTE2022401,
title = {Deep reinforcement learning for inventory control: A roadmap},
journal = {European Journal of Operational Research},
volume = {298},
number = {2},
pages = {401-412},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2021.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0377221721006111},
author = {Robert N. Boute and Joren Gijsbrechts and Willem {van Jaarsveld} and Nathalie Vanvuchelen},
keywords = {Inventory management, Machine learning, Reinforcement learning, Neural networks},
abstract = {Deep reinforcement learning (DRL) has shown great potential for sequential decision-making, including early developments in inventory control. Yet, the abundance of choices that come with designing a DRL algorithm, combined with the intense computational effort to tune and evaluate each choice, may hamper their application in practice. This paper describes the key design choices of DRL algorithms to facilitate their implementation in inventory control. We also shed light on possible future research avenues that may elevate the current state-of-the-art of DRL applications for inventory control and broaden their scope by leveraging and improving on the structural policy insights within inventory research. Our discussion and roadmap may also spur future research in other domains within operations management.}
}
@article{LOPEZMARTIN2020112963,
title = {Application of deep reinforcement learning to intrusion detection for supervised problems},
journal = {Expert Systems with Applications},
volume = {141},
pages = {112963},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.112963},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419306815},
author = {Manuel Lopez-Martin and Belen Carro and Antonio Sanchez-Esguevillas},
keywords = {Intrusion detection, Data networks, Deep reinforcement learning},
abstract = {The application of new techniques to increase the performance of intrusion detection systems is crucial in modern data networks with a growing threat of cyber-attacks. These attacks impose a greater risk on network services that are increasingly important from a social end economical point of view. In this work we present a novel application of several deep reinforcement learning (DRL) algorithms to intrusion detection using a labeled dataset. We present how to perform supervised learning based on a DRL framework. The implementation of a reward function aligned with the detection of intrusions is extremely difficult for Intrusion Detection Systems (IDS) since there is no automatic way to identify intrusions. Usually the identification is performed manually and stored in datasets of network features associated with intrusion events. These datasets are used to train supervised machine learning algorithms for classifying intrusion events. In this paper we apply DRL using two of these datasets: NSL-KDD and AWID datasets. As a novel approach, we have made a conceptual modification of the classic DRL paradigm (based on interaction with a live environment), replacing the environment with a sampling function of recorded training intrusions. This new pseudo-environment, in addition to sampling the training dataset, generates rewards based on detection errors found during training. We present the results of applying our technique to four of the most relevant DRL models: Deep Q-Network (DQN), Double Deep Q-Network (DDQN), Policy Gradient (PG) and Actor-Critic (AC). The best results are obtained for the DDQN algorithm. We show that DRL, with our model and some parameter adjustments, can improve the results of intrusion detection in comparison with current machine learning techniques. Besides, the classifier obtained with DRL is faster than alternative models. A comprehensive comparison of the results obtained with other machine learning models is provided for the AWID and NSL-KDD datasets, together with the lessons learned from the application of several design alternatives to the four DRL models.}
}
@article{CHEN2014149,
title = {Cost reduction of CO2 capture processes using reinforcement learning based iterative design: A pilot-scale absorption–stripping system},
journal = {Separation and Purification Technology},
volume = {122},
pages = {149-158},
year = {2014},
issn = {1383-5866},
doi = {https://doi.org/10.1016/j.seppur.2013.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S1383586613006163},
author = {Junghui Chen and Fan Wang},
keywords = {Absorption–stripping process, Optimal design, Reinforcement learning},
abstract = {An economical and practical way of designing the optimal condition for CO2 capture processes is proposed. This learning strategy, called reinforcement learning based iterative design, is developed to learn the optimal condition from hybrid information. One is from simulation data, the other, from real plant data. The simulation data is easily accessible, but the optimal condition is restricted to a simulated model being selected. To make up the mismatch, new info from the real process is explored. Only fewer operating data supplemented from the real process is used to update the learning scheme, so time, costs, and efforts can be saved. The fused info from the two kinds of data is also proposed. To demonstrate the effectiveness of the proposed method, design of a pilot-scale CO2 absorption–stripping experiment is conducted for recovery of CO2 from flue gases.}
}
@article{O20062121,
title = {Adaptive stock trading with dynamic asset allocation using reinforcement learning},
journal = {Information Sciences},
volume = {176},
number = {15},
pages = {2121-2147},
year = {2006},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2005.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0020025505003166},
author = {Jangmin O and Jongwoo Lee and Jae Won Lee and Byoung-Tak Zhang},
keywords = {Stock trading, Reinforcement learning, Multiple-predictors approach, Asset allocation},
abstract = {Stock trading is an important decision-making problem that involves both stock selection and asset management. Though many promising results have been reported for predicting prices, selecting stocks, and managing assets using machine-learning techniques, considering all of them is challenging because of their complexity. In this paper, we present a new stock trading method that incorporates dynamic asset allocation in a reinforcement-learning framework. The proposed asset allocation strategy, called meta policy (MP), is designed to utilize the temporal information from both stock recommendations and the ratio of the stock fund over the asset. Local traders are constructed with pattern-based multiple predictors, and used to decide the purchase money per recommendation. Formulating the MP in the reinforcement learning framework is achieved by a compact design of the environment and the learning agent. Experimental results using the Korean stock market show that the proposed MP method outperforms other fixed asset-allocation strategies, and reduces the risks inherent in local traders.}
}
@article{BERSINI1992467,
title = {Reinforcement learning and recruitment mechanism for adaptive distributed control},
journal = {Annual Review in Automatic Programming},
volume = {17},
pages = {467-473},
year = {1992},
note = {Artificial Intelligence in Real-time Control 1992},
issn = {0066-4138},
doi = {https://doi.org/10.1016/S0066-4138(09)91076-X},
url = {https://www.sciencedirect.com/science/article/pii/S006641380991076X},
author = {H. Bersini},
keywords = {Process Control, Reinforcement Learning, Dynamic Programming, Distributed Control, Immune System, Recruitment Mechanism},
abstract = {The work presented in this paper is an attempt to spread further the inspiration gained from the knowledge of biological systems into the field of adaptive control. After the neural controllers and the evolutionary based mechanisms, new hints for the control of complex processes might be derived from other biological domains such as immunology or the study of conditioning learning. The conception of a system equipped with a complex controller, interacting with an uncertain and varying environment, and basing its learning on its own experiences entails quite naturally the integration of a reinforcement learning mechanism. Two learning processes characterized by two different time scales will be introduced, will be connected to their respective biological origins and will be illustrated on the classical cart-pole control problem. These two learning processes are the rapid reinforcement learning and the slower recruitment mechanism.}
}
@article{19931095,
title = {169 Learning to avoid collisions: A reinforcement learning paradigm for mobile robot navigation: B.J.A. Kröse, J.W.M. van Dam, pp 317–322},
journal = {Control Engineering Practice},
volume = {1},
number = {6},
pages = {1095},
year = {1993},
issn = {0967-0661},
doi = {https://doi.org/10.1016/0967-0661(93)90188-W},
url = {https://www.sciencedirect.com/science/article/pii/096706619390188W}
}
@article{PLATE2015105,
title = {Utilizing kinematics and selective sweeping in reinforcement learning-based routing algorithms for underwater networks},
journal = {Ad Hoc Networks},
volume = {34},
pages = {105-120},
year = {2015},
note = {ADVANCES IN UNDERWATER COMMUNICATIONS AND NETWORKS},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2014.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S1570870514002017},
author = {R. Plate and C. Wakayama},
keywords = {Reinforcement learning, Routing protocol, Acoustic communication, Underwater sensor network},
abstract = {Effective utilization of mobile ad hoc underwater distributed networks is challenging due to high system costs and the harsh environment characterized by low bandwidth, large latency, high energy consumption, and node mobility. This work addresses the routing issue, which is critical in successfully establishing and utilizing an underwater network. In particular, it focuses on reinforcement learning (RL)-based routing algorithms, which possess the ability to explore the network environment and adapt routing decisions to the constantly changing topology of the network due to node mobility and energy usage. This paper presents a routing algorithm based on Q-learning, one of the RL approaches, with additional Kinematic and Sweeping features, therefore referred to as QKS. These two additional features are introduced to address the potential slow convergence associated with pure RL algorithms. The results of a detailed packet-level simulation have been obtained using the NS-2 open-source network simulator with underwater modeling additions. The energy efficiency, convergence, and delivery performance of QKS are compared with two other routing protocols for underwater networks, a basic flooding approach (ICRP (Liang, 2007)) and a basic Q-learning implementation (QELAR (Hu, 2010)), using simulations of networks with both fixed and mobile nodes.}
}
@article{CORONATO2020101964,
title = {Reinforcement learning for intelligent healthcare applications: A survey},
journal = {Artificial Intelligence in Medicine},
volume = {109},
pages = {101964},
year = {2020},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2020.101964},
url = {https://www.sciencedirect.com/science/article/pii/S093336572031229X},
author = {Antonio Coronato and Muddasar Naeem and Giuseppe {De Pietro} and Giovanni Paragliola},
keywords = {Artificial intelligence, Reinforcement learning, Healthcare, Personalized medicine},
abstract = {Discovering new treatments and personalizing existing ones is one of the major goals of modern clinical research. In the last decade, Artificial Intelligence (AI) has enabled the realization of advanced intelligent systems able to learn about clinical treatments and discover new medical knowledge from the huge amount of data collected. Reinforcement Learning (RL), which is a branch of Machine Learning (ML), has received significant attention in the medical community since it has the potentiality to support the development of personalized treatments in accordance with the more general precision medicine vision. This report presents a review of the role of RL in healthcare by investigating past work, and highlighting any limitations and possible future contributions.}
}
@article{HEDRICK2022107727,
title = {Reinforcement learning for online adaptation of model predictive controllers: Application to a selective catalytic reduction unit},
journal = {Computers & Chemical Engineering},
volume = {160},
pages = {107727},
year = {2022},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.107727},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422000692},
author = {Elijah Hedrick and Katherine Hedrick and Debangsu Bhattacharyya and Stephen E. Zitney and Benjamin Omell},
keywords = {Process systems engineering, Reinforcement learning, Energy, Modeling, Process control},
abstract = {This work presents a novel application of reinforcement learning (RL) for online dynamic tuning of model predictive controllers (MPC). Applying a state-action-reward-state-action (SARSA) algorithm for temporal difference learning with a control-specific reward function improves the error tracking performance of a standard MPC formulation. The proposed RL approach is also readily adaptable to other MPCs, or entirely different control approaches. Practical details for the implementation of the RL-MPC algorithm are also presented. The proposed algorithm is applied to a case study of controlling nitrogen oxide (NOx) emissions in an industrial selective catalytic reduction (SCR) unit, a control problem characterized by significant nonlinearity and time delay. Along with an RL-MPC formulation for NOx control, another MPC is proposed to mitigate ammonia slip and decrease ammonia consumption in the SCR. Results showing the efficacy of the RL-MPC for NOx control through learning and implementation on the nonlinear SCR dynamic model are presented.}
}
@article{KIM2020166,
title = {A model-based deep reinforcement learning method applied to finite-horizon optimal control of nonlinear control-affine system},
journal = {Journal of Process Control},
volume = {87},
pages = {166-178},
year = {2020},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2020.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959152419306596},
author = {Jong Woo Kim and Byung Jun Park and Haeun Yoo and Tae Hoon Oh and Jay H. Lee and Jong Min Lee},
keywords = {Reinforcement learning, Approximate dynamic programming, Deep neural networks, Globalized dual heuristic programming, Finite horizon optimal control problem, Hamilton–Jacobi–Bellman equation},
abstract = {The Hamilton–Jacobi–Bellman (HJB) equation can be solved to obtain optimal closed-loop control policies for general nonlinear systems. As it is seldom possible to solve the HJB equation exactly for nonlinear systems, either analytically or numerically, methods to build approximate solutions through simulation based learning have been studied in various names like neurodynamic programming (NDP) and approximate dynamic programming (ADP). The aspect of learning connects these methods to reinforcement learning (RL), which also tries to learn optimal decision policies through trial-and-error based learning. This study develops a model-based RL method, which iteratively learns the solution to the HJB and its associated equations. We focus particularly on the control-affine system with a quadratic objective function and the finite horizon optimal control (FHOC) problem with time-varying reference trajectories. The HJB solutions for such systems involve time-varying value, costate, and policy functions subject to boundary conditions. To represent the time-varying HJB solution in high-dimensional state space in a general and efficient way, deep neural networks (DNNs) are employed. It is shown that the use of DNNs, compared to shallow neural networks (SNNs), can significantly improve the performance of a learned policy in the presence of uncertain initial state and state noise. Examples involving a batch chemical reactor and a one-dimensional diffusion-convection-reaction system are used to demonstrate this and other key aspects of the method.}
}
@article{DESHARNAIS201321,
title = {Testing probabilistic equivalence through Reinforcement Learning},
journal = {Information and Computation},
volume = {227},
pages = {21-57},
year = {2013},
issn = {0890-5401},
doi = {https://doi.org/10.1016/j.ic.2013.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0890540113000163},
author = {Josée Desharnais and François Laviolette and Sami Zhioua},
keywords = {Verification, Stochastic systems, Markov processes, Distance, Divergence, Reinforcement Learning, Testing, Equivalence relations},
abstract = {Checking if a given system implementation respects its specification is often done by proving that the two are “equivalent”. The equivalence is chosen, in particular, for its computability and of course for its meaning, that is, for its adequacy with what is observable from the two systems (implementation and specification). Trace equivalence is easily testable (decidable from interaction), but often considered too weak; in contrast, bisimulation is accepted as the canonical equivalence for interaction, but it is not testable. Richer than an equivalence is a form of distance: it is zero between equivalent systems, and it provides an estimation of their difference if the systems are not equivalent. Our main contribution is to define such a distance in a context where (1) the two systems to be compared have a stochastic behavior; (2) the model of one of them (e.g., the implementation) is unknown, hence our only knowledge is obtained by interacting with it; (3) consequently the target equivalence (observed when distance is zero) must be testable. To overcome the problem that the model is unknown, we use a Reinforcement Learning approach that provides powerful stochastic algorithms that only need to interact with the model. Our second main contribution is a new family of testable equivalences, called K-moment. The weakest of them, 1-moment equivalence, is trace equivalence; as K grows, K-moment equivalences become finer, all remaining, as well as their limit, weaker than bisimulation. We propose a framework to define (and test) a bigger class of testable equivalences: Test-Observation-Equivalences (TOEs), and we show how they can be made coarser or not, by tuning some parameters.}
}
@article{LOPESSILVA2019148,
title = {A reinforcement learning-based multi-agent framework applied for solving routing and scheduling problems},
journal = {Expert Systems with Applications},
volume = {131},
pages = {148-171},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.04.056},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419302866},
author = {Maria Amélia {Lopes Silva} and Sérgio Ricardo {de Souza} and Marcone Jamilson {Freitas Souza} and Ana Lúcia C. Bazzan},
keywords = {Multi-agent framework for optimization, Reinforcement learning, Metaheuristics, Multi-agent systems, Vehicle routing problem with time window, Unrelated parallel machine scheduling problem},
abstract = {This article presents a multi-agent framework for optimization using metaheuristics, called AMAM. In this proposal, each agent acts independently in the search space of a combinatorial optimization problem. Agents share information and collaborate with each other through the environment. The goal is to enable the agent to modify their actions based on experiences gained in interacting with the other agents and the environment using the concepts of Reinforcement Learning. For better introduction and validation of the AMAM framework, this article uses the instantiation of the Vehicle Routing Problem with Time Windows (VRPTW) and the Unrelated Parallel Machine Scheduling Problem with Sequence-Dependent Setup Times (UPMSP-ST), i.e., two classic combinatorial optimization problems. The main objective of the experiments is to evaluate the performance of the proposed adaptive agents. The experiments confirm that the ability to learn attributed to the agent directly influences the quality of solutions, both from the individual point of view and from the point of view of teamwork. In this way, the framework presented here is a step forward in relation to the other frameworks of the literature regarding to the adaptation to the particular aspects of the problems. Additionally, the cooperation between agents and their ability to influence the quality of the solutions of the agents involved in the search of the solution is confirmed. The results also strengthen the issue of the scalability of the framework, since, with the addition of new agents, there is an improvement of the solutions obtained.}
}
@article{KUZNETSOVA2013133,
title = {Reinforcement learning for microgrid energy management},
journal = {Energy},
volume = {59},
pages = {133-146},
year = {2013},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2013.05.060},
url = {https://www.sciencedirect.com/science/article/pii/S0360544213004817},
author = {Elizaveta Kuznetsova and Yan-Fu Li and Carlos Ruiz and Enrico Zio and Graham Ault and Keith Bell},
keywords = {Smartgrids, Microgrids, Markov chain model, Reinforcement learning, Sensitivity analysis},
abstract = {We consider a microgrid for energy distribution, with a local consumer, a renewable generator (wind turbine) and a storage facility (battery), connected to the external grid via a transformer. We propose a 2 steps-ahead reinforcement learning algorithm to plan the battery scheduling, which plays a key role in the achievement of the consumer goals. The underlying framework is one of multi-criteria decision-making by an individual consumer who has the goals of increasing the utilization rate of the battery during high electricity demand (so as to decrease the electricity purchase from the external grid) and increasing the utilization rate of the wind turbine for local use (so as to increase the consumer independence from the external grid). Predictions of available wind power feed the reinforcement learning algorithm for selecting the optimal battery scheduling actions. The embedded learning mechanism allows to enhance the consumer knowledge about the optimal actions for battery scheduling under different time-dependent environmental conditions. The developed framework gives the capability to intelligent consumers to learn the stochastic environment and make use of the experience to select optimal energy management actions.}
}
@article{GHAZANFARI201661,
title = {Extracting bottlenecks for reinforcement learning agent by holonic concept clustering and attentional functions},
journal = {Expert Systems with Applications},
volume = {54},
pages = {61-77},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.01.030},
url = {https://www.sciencedirect.com/science/article/pii/S0957417416000403},
author = {Behzad Ghazanfari and Nasser Mozayani},
keywords = {Reinforcement learning, Abstraction, Concept, Holonic/hierarchical clustering, Attention},
abstract = {Reinforcement learning is not well scalable in state spaces with high-dimensions. The hierarchical reinforcement learning resolves this problem by task decomposition. Task decomposition is done by extracting bottlenecks, which is in turn another challenging issue, especially in terms of time and memory complexity and the need to the prior knowledge of the environment. To alleviate these issues, a new approach is proposed toward the problem of extracting bottlenecks. Holonic concept clustering and attentional functions are proposed to extract bottleneck states. To this end, states are organized based on the effects of actions by means of a holonic clustering to extract high-level concepts. High-level concepts are used as cues for controlling attention. The proposed mechanism has a better time complexity and fewer requirements to the designer's help. The experimental results showed a considerable improvement in the precision of bottleneck detection and agent's performance for traditional benchmarks comparing to other similar methods.}
}
@article{MARAVALL2009887,
title = {Hybridizing evolutionary computation and reinforcement learning for the design of almost universal controllers for autonomous robots},
journal = {Neurocomputing},
volume = {72},
number = {4},
pages = {887-894},
year = {2009},
note = {Brain Inspired Cognitive Systems (BICS 2006) / Interplay Between Natural and Artificial Computation (IWINAC 2007)},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2008.04.058},
url = {https://www.sciencedirect.com/science/article/pii/S0925231208004785},
author = {Darío Maravall and Javier {de Lope} and José Antonio {Martín H.}},
keywords = {Evolutionary computation, Reinforcement learning, State-space granulation, Autonomous robot motion control, Sensory-motor coordination},
abstract = {In this paper a hybrid approach to the autonomous motion control of robots in cluttered environments with unknown obstacles is introduced. It is shown the efficiency of a hybrid solution by combining the optimization power of evolutionary algorithms and at the same time the efficiency of reinforcement learning in real-time and on-line situations. Experimental results concerning the navigation of a L-shaped robot in a cluttered environment with unknown obstacles are also presented. In such environments there appear real-time and on-line constraints well-suited to RL algorithms and, at the same time, there exists an extremely high dimension of the state space usually unpractical for RL algorithms but well-suited to evolutionary algorithms. The experimental results confirm the validity of the hybrid approach to solve hard real-time, on-line and high dimensional robot motion planning and control problems, where the RL approach shows some difficulties.}
}
@article{ZHONG2021116623,
title = {Deep reinforcement learning framework for dynamic pricing demand response of regenerative electric heating},
journal = {Applied Energy},
volume = {288},
pages = {116623},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116623},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921001586},
author = {Shengyuan Zhong and Xiaoyuan Wang and Jun Zhao and Wenjia Li and Hao Li and Yongzhen Wang and Shuai Deng and Jiebei Zhu},
keywords = {Regenerative electric heating, Demand response, Load aggregators, deep Q network, Weber–Fechner law},
abstract = {Applications of electric heating, which can improve carbon emission reduction and renewable energy utilization, have brought new challenges to the safe operation of energy systems around the world. Regenerative electric heating with load aggregators and demand response is an effective means to mitigate the wind curtailment and grid operational risks caused by electric heating. However, there is still a lack of models related to demand response, which results in participants not being able to obtain maximum benefits through dynamic subsidy prices. This study uses the Weber–Fechner law and a clustering algorithm to construct quantitative response characteristics models. The deep Q network was used to build a dynamic subsidy price generation framework for load aggregators. Through simulation analysis based on the evolutionary game model of a project in a rural area in Tianjin, China, the following conclusions were drawn: compared with the benchmark model, regenerative electric heating users can save up to 8.7% of costs, power grid companies can save 56.6% of their investment, and wind power plants can increase wind power consumption by 17.6%. The framework proposed in this study considers user behavior quantification of demand response participants and the differences among users. Therefore, the framework can provide a more reasonable, applicable, and intelligent system for regenerative electric heating.}
}
@article{SALGADO2018266,
title = {Measuring the emotional state among interacting agents: A game theory approach using reinforcement learning},
journal = {Expert Systems with Applications},
volume = {97},
pages = {266-275},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.12.036},
url = {https://www.sciencedirect.com/science/article/pii/S095741741730859X},
author = {Mireya Salgado and Julio B. Clempner},
keywords = {Adaptive autonomous agents, Emotional model, Kullback–Leibler distance, Game theory, Reinforcement learning},
abstract = {Studies on emotion perception often require stimuli that convey different emotions. These stimuli can serve as a tool to understand how agents react to different circumstances. Although different stimuli have been commonly used to change the emotions of an agent, it is not clear how to measure the emotional state of an agent. This paper suggests a new method for measuring the emotional state among interacting agents in a given environment. We present the modeling of an adaptive emotional framework that takes into account agent emotion, interaction and learning process. For solving the problem, we employ a non-cooperative game theory approach for representing the interaction between agents and a Reinforcement Learning (RL) process for introducing the stimuli to the environment. We restrict our problem to a class of finite and homogeneous Markov games. The emotional problem is ergodic: each emotion can be represented by a state in a Markov chain which has a probability to be reached. Each emotional strategy of the Markov model is represented as a probability distribution. Then, for measuring the emotional state among agents, we employ the Kullback-Leibler distance between the resulting emotional strategies of the interacting agents. It is a distribution-wise asymmetric measure, then the feelings of one player for another are relative (different). We propose an algorithm for the RL process and for solving the game is proposed a two-step approach. We present an application example related to the selection process of a candidate for a specific position using assessment centers to show the effectiveness of the proposed method by a) measuring the emotional distance among the interacting agents and b) measuring the “emotional closeness degree” of the interacting agents to an ideal proposed candidate agent.}
}
@article{NAUG2022112584,
title = {Deep reinforcement learning control for non-stationary building energy management},
journal = {Energy and Buildings},
volume = {277},
pages = {112584},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112584},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822007551},
author = {Avisek Naug and Marcos Quinones-Grueiro and Gautam Biswas},
keywords = {Smart building management, Deep reinforcement learning, Non-stationary systems, Continuous adaptation},
abstract = {Developing an optimal supervisory control policy for building energy management is a complex problem because the system exhibits non-stationary behaviors, and the target policy needs to evolve with changes in the state transition and reward functions. Non-stationary real-world problems often present a set of challenges: non-stationary changes are difficult to detect; and thermodynamic systems with larger time-constants can create sample-inefficiency problems for learning algorithms. In addition, the system may have to satisfy safety–critical constraints, and, therefore, the policy must be learned offline unless actuation rules are correctly designed. To address these challenges, we propose a data-driven deep reinforcement learning framework. A reinforcement learning supervisory controller is firstly developed and deployed on the building for the heating, ventilation and air conditioning (HVAC) system and monitored for performance degradation by tracking an aggregate metric. When degradation is detected, a relearning loop is triggered. Then, a set of data-driven models of the building behavior is updated with the latest real data. Subsequently, the deployed controller is re-tuned by letting it interact with the model and is then redeployed on the system. Our proposed approach is demonstrated extensively on the standard ASHRAE 5-zone testbed and a real building. It is benchmarked against state-of-the-art algorithms in building supervisory control: Guideline-36, Proximal Policy Optimization, Deep Deterministic Policy Gradient, and Model Predictive Path Integral control. Our approach performs significantly better than the previously mentioned supervisory control strategies and highlights the need for a condition-based offline relearning framework in dynamic systems.}
}
@article{JANSSENS2007466,
title = {Allocating time and location information to activity–travel patterns through reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {20},
number = {5},
pages = {466-477},
year = {2007},
note = {Intelligent Knowledge Engineering Systems},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2007.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S095070510700010X},
author = {Davy Janssens and Yu Lan and Geert Wets and Guoqing Chen},
keywords = {Reinforcement Learning, -learning, Agent-based micro-simulation systems, Activity-based modelling},
abstract = {The Reinforcement Machine Learning technique presented in this paper simulates time and location information for a given sequence of activities and transport modes. The main contributions to the current state-of-the art are the allocation of location information in the simulation of activity–travel patterns, the non-restriction to a given number of activities and the incorporation of realistic travel times. Furthermore, the time and location allocation problem were treated and integrated simultaneously, which means that the respondents’ reward is not only maximized in terms of minimum travel duration, but also simultaneously in terms of optimal time allocation.}
}
@article{SALLAM2021114479,
title = {A reinforcement learning based multi-method approach for stochastic resource constrained project scheduling problems},
journal = {Expert Systems with Applications},
volume = {169},
pages = {114479},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114479},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420311271},
author = {Karam M. Sallam and Ripon K. Chakrabortty and Michael J. Ryan},
keywords = {Resource constrained project scheduling problems, Multi-operator differential evolution, Cuckoo search, Multi-method approach, Reinforcement learning},
abstract = {The Resource-Constrained Project Scheduling Problem (RCPSP) has been widely accepted as a challenging research topic due to its NP-hard nature. Because of the dynamic nature of real-world problems, stochastic-RCPSPs (SRCPSPs) are also receiving greater attention among researchers. To solve these extended RCPSPs (i.e., SRCPSPs), this paper proposes an reinforcement learning based meta-heuristic switching approach that utilizes the powers of both multi-operator differential evolution (MODE) and discrete cuckoo search (DCS) algorithms in single algorithmic framework. Reinforcement learning (RL) is introduced as a technique to select either MODE or DCS based on the diversity of population and quality of solutions. To deal with uncertain durations, a chance-constrained based approach with some belief degrees is also considered and solved by this proposed RL based multi-method approach (i.e., DECSwRL-CC). Extensive experimentation with benchmark data from the project scheduling library (PSPLIB) demonstrates the efficacy of this proposed multi-method approach. Numerous state of the art chance constrained approaches are taken from the literature to compare the proposed approach and to validate the efficacy of this multi-method approach. This particular strategy is applicable to the risk-averse decision-makers who want to realize the project schedule with a high degree of certainty.}
}
@article{ZHANG201110436,
title = {Elucidating the Design Principles of Signal Transduction Networks – application to Dopamine and Calcium signaling in reinforcement learning},
journal = {IFAC Proceedings Volumes},
volume = {44},
number = {1},
pages = {10436-10441},
year = {2011},
note = {18th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20110828-6-IT-1002.03693},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016452894},
author = {Shuoqi Zhang and Elling W. Jacobsen},
keywords = {Systems biology, modeling, model reduction, sensitivity analysis, signal transduction},
abstract = {Abstract
The biochemical networks underlying biological functions are in general highly complex. An important aim of systems biology is to provide mechanistic insight into how the different interactions within the network give rise to specific behaviors and properties. In this paper we consider the use of structured dynamic perturbations applied to the network nodes and edges for elucidating the most important interactions in signal transduction networks. Signal transduction networks mediate extracellular and intracellular signals to the nucleus, resulting in an appropriate response by the gene regulatory network. The most important characteristic of signal transduction networks is usually the specific temporal amplification of stimuli signals. As a case study we consider the intracellular signaling pathways that underlie reinforcement learning in striatum neuronal cells of the brain. It has recently been found that these networks respond to Dopamine and Calcium signals in a fashion which is strongly dependent on the signal shape, and the hypothesis is that this is related to the existence of a resonant feedback loop within the network. By systematically perturbing the nodes and edges of the network using general dynamic perturbations, affecting both the strength and phase lag of the direct interactions within the network, we are able to identify the most important components and interactions underlying the “resonant” signal amplification. Based on this we derive a reduced order model of the network, with retained physical states, from which we can show that the apparent resonance is caused by two parallel pathways with opposing effects and widely different time-constants. We postulate that this is a sound architecture for signal amplification of mid-frequency signals based on the fact that the robustness can be made almost arbitrarily large, as compared to resonant feedback loops that are inherently unrobust.}
}
@article{RUIZMONTIEL2013230,
title = {Design with shape grammars and reinforcement learning},
journal = {Advanced Engineering Informatics},
volume = {27},
number = {2},
pages = {230-245},
year = {2013},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2012.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1474034612001139},
author = {Manuela Ruiz-Montiel and Javier Boned and Juan Gavilanes and Eduardo Jiménez and Lawrence Mandow and José-Luis Pérez-de-la-Cruz},
keywords = {Computational design, Shape grammars, Reinforcement learning, Architecture},
abstract = {Shape grammars are a powerful and appealing formalism for automatic shape generation in computer-based design systems. This paper presents a proposal complementing the generative power of shape grammars with reinforcement learning techniques. We use simple (naive) shape grammars capable of generating a large variety of different designs. In order to generate those designs that comply with given design requirements, the grammar is subject to a process of machine learning using reinforcement learning techniques. Based on this method, we have developed a system for architectural design, aimed at generating two-dimensional layout schemes of single-family housing units. Using relatively simple grammar rules, we learn to generate schemes that satisfy a set of requirements stated in a design guideline. Obtained results are presented and discussed.}
}