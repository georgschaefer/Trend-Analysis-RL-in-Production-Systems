"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Modelling and controlling uncertainty in optimal disassembly planning through reinforcement learning","S. A. Reveliotis","School of Industrial & Systems Engineering, Georgia Institute of Technology, USA","IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004","6 Jul 2004","2004","3","","2625","2632 Vol.3","Currently there is increasing consensus that one of the main issues differentiating the remanufacturing from the more traditional manufacturing processes is the need to effectively model and manage the high levels of uncertainty inherent in these new processes. The work presented in this paper formally establishes that the theory of reinforcement learning, one of the most actively researched areas in computational learning theory, constitutes a rigorous, effectively implementable modelling framework for providing (near) optimal solutions to the optimal disassembly planning (ODP) problem, one of the key problems to be addressed by remanufacturing processes, in the face of the aforementioned uncertainties. The developed results are exemplified and validated by application on a case study borrowed from the relevant literature.","1050-4729","0-7803-8232-3","10.1109/ROBOT.2004.1307457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307457","","Optimal control;Uncertainty;Learning;Process planning;Reverse logistics;Technology planning;Electrical equipment industry;Manufacturing industries;Systems engineering and theory;Manufacturing processes","assembly planning;learning (artificial intelligence);manufacturing processes;uncertain systems;optimisation","optimal disassembly planning;reinforcement learning;remanufacturing process;computational learning theory;aforementioned uncertainties","","6","","13","IEEE","6 Jul 2004","","","IEEE","IEEE Conferences"
"Expert Initialized Reinforcement Learning with Application to Robotic Assembly","J. Langaa; C. Sloth","Maersk McKinney Moller Institute, University of Southern Denmark, Odense, Denmark; Maersk McKinney Moller Institute, University of Southern Denmark, Odense, Denmark","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","1405","1410","This paper investigates the advantages and boundaries of actor-critic reinforcement learning algorithms in an industrial setting. We compare and discuss Cycle of Learning, Deep Deterministic Policy Gradient and Twin Delayed Deep Deterministic Policy Gradient with respect to performance in simulation as well as on a real robot setup. Furthermore, it emphasizes the importance and potential of combining demonstrated expert behavior with the actor-critic reinforcement learning setting while using it with an admittance controller to solve an industrial assembly task. Cycle of Learning and Twin Delayed Deep Deterministic Policy Gradient showed to be equally usable in simulation, while Cycle of Learning proved to be best on a real world application due to the behavior cloning loss that enables the agent to learn rapidly. The results also demonstrated that it is a necessity to incorporate an admittance controller in order to transfer the learned behavior to a real robot.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926540","Innovation Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926540","","Training;Robotic assembly;Computer aided software engineering;Automation;Service robots;Cloning;Reinforcement learning","gradient methods;reinforcement learning;robotic assembly","expert behavior;actor-critic reinforcement learning setting;admittance controller;industrial assembly task;twin delayed deep deterministic policy gradient;learned behavior;robotic assembly;expert initialized reinforcement learning","","2","","20","EU","28 Oct 2022","","","IEEE","IEEE Conferences"
"Model-Based Reinforcement Learning for Robotic Arm Control with Limited Environment Interaction","Y. Chen; X. Sun; S. Zheng; W. Huang; S. Zeng; D. Chen; Z. Xu","Education Department of Fujian Province, Key Laboratory of Industrial Automation Control Technology and Information Processing, College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Evomotion Co., Ltd., Guangdong, Shenzhen, China; Education Department of Fujian Province, Key Laboratory of Industrial Automation Control Technology and Information Processing, College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Evomotion Co., Ltd., Guangdong, Shenzhen, China; Education Department of Fujian Province, Key Laboratory of Industrial Automation Control Technology and Information Processing, College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Education Department of Fujian Province, Key Laboratory of Industrial Automation Control Technology and Information Processing, College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Education Department of Fujian Province, Key Laboratory of Industrial Automation Control Technology and Information Processing, College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China","2023 IEEE 13th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","28 Sep 2023","2023","","","154","158","Reinforcement Learning (RL) has been applied to robotic arm control, which enables the agent to learn an effective policy to solve complex tasks. However, it requires constant interaction with the environment leading to low sample efficiency. In this paper, we propose a robotic arm control approach based on planning via lookahead search, which is a model-based RL algorithm to improve the sample efficiency. The approach builds an environment model in order to obtain the dynamics of the environment. Thus the model can be used to plan future actions by a tree-based search. The experiments show that our approach can solve the task of robotic arm control with less environmental samples.","2642-6633","979-8-3503-1519-6","10.1109/CYBER59472.2023.10256476","NSF of China(grant numbers:61973085); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256476","","Training;Automation;Heuristic algorithms;Process control;Reinforcement learning;Manipulators;Control systems","control engineering computing;end effectors;planning (artificial intelligence);reinforcement learning;tree searching","end effector;environment model;limited environment interaction;lookahead search;model-based reinforcement learning;model-based RL algorithm;planning;robotic arm control approach;sample efficiency;tree-based search","","","","20","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"High-level Reward Deep Reinforcement Learning Approach for a Novel Physical-Logical Hybrid Factory Line Robot Vehicle Simulation","R. Higa; S. Nakadai","Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology, Koto-ku, Japan; Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology, Koto-ku, Japan","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","1399","1404","We propose a novel factory automation method that simultaneously optimizes a logical factory production line, such as inventory and production amount, and the physical path planning of robot vehicles. Traditionally, path planning for robot vehicles and overall factory line optimization have been studied independently. However, actual factory production lines require a use case for path planning that considers the balance between inventory control, production maximization, and coordination with assembly workers. Therefore, we developed a novel approach for the mobile simulation of a logistic-physical factory line and devised a deep reinforcement learning method based on high-level rewards. This method is capable of sequential path planning when considering the balance between the inventory and product number as well as the coordination of agents among the production lines. Consequently, our mobile agent successfully learns to plan the shortest route without a bottleneck in the factory production line during any given episode. Moreover, the mobile agent appropriately adjusts the route when a bottleneck occurs and inventory is excessive. This suggests that path planning for robot vehicle agents can be achieved based on indicators that optimize the entire factory, which we expect to be a novel application of robot vehicle coordination that operates along the entire production line.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926449","","Computer aided software engineering;Robot kinematics;Mobile agents;Reinforcement learning;Inventory control;Production facilities;Path planning","assembling;factory automation;learning (artificial intelligence);mobile agents;mobile robots;multi-agent systems;path planning;production engineering computing","mobile agent;robot vehicle agents;entire factory;robot vehicle coordination;entire production line;high-level reward deep reinforcement learning approach;novel physical-logical hybrid factory line robot vehicle simulation;novel factory automation method;logical factory production line;physical path;robot vehicles;factory line optimization;actual factory production lines;inventory control;production maximization;mobile simulation;logistic-physical factory line;deep reinforcement learning method;high-level rewards;sequential path planning;product number","","","","16","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Multiobjective Evolutionary Algorithm for Mixed-Model Multimanned Assembly Line Balancing Under Uncertain Demand","Z. Zhang; Q. Tang; M. Chica; Z. Li","the Hubei Key Laboratory of Mechanical Transmission and Manufacturing Engineering, and the Precision Manufacturing Institute, Key Laboratory of Metallurgical Equipment and Control Technology, Ministry of Education, Wuhan University of Science and Technology, Wuhan, China; the Hubei Key Laboratory of Mechanical Transmission and Manufacturing Engineering, and the Precision Manufacturing Institute, Key Laboratory of Metallurgical Equipment and Control Technology, Ministry of Education, Wuhan University of Science and Technology, Wuhan, China; Department of AI and Computer Science, Andalusian Research Institute in Data Science and Computational Intelligence, DaSCI, University of Granada, Granada, Spain; the Hubei Key Laboratory of Mechanical Transmission and Manufacturing Engineering, and the Precision Manufacturing Institute, Key Laboratory of Metallurgical Equipment and Control Technology, Ministry of Education, Wuhan University of Science and Technology, Wuhan, China","IEEE Transactions on Cybernetics","","2023","PP","99","1","14","In practical assembly enterprises, customization and rush orders lead to an uncertain demand environment. This situation requires managers and researchers to configure an assembly line that increases production efficiency and robustness. Hence, this work addresses cost-oriented mixed-model multimanned assembly line balancing under uncertain demand, and presents a new robust mixed-integer linear programming model to minimize the production and penalty costs simultaneously. In addition, a reinforcement learning-based multiobjective evolutionary algorithm (MOEA) is designed to tackle the problem. The algorithm includes a priority-based solution representation and a new task-worker-sequence decoding that considers robustness processing and idle time reductions. Five crossover and three mutation operators are proposed. The  $Q$ -learning-based strategy determines the crossover and mutation operator at each iteration to effectively obtain Pareto sets of solutions. Finally, a time-based probability-adaptive strategy is designed to effectively coordinate the crossover and mutation operators. The experimental study, based on 269 benchmark instances, demonstrates that the proposal outperforms 11 competitive MOEAs and a previous single-objective approach to the problem. The managerial insights from the results as well as the limitations of the algorithm are also highlighted.","2168-2275","","10.1109/TCYB.2022.3229666","China Postdoctoral Science Foundation(grant numbers:2021M702536); National Natural Science Foundation of China(grant numbers:52275504,62173260); Andalusian Government and the European Regional Development Fund(grant numbers:SIMARK PY18-4475); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10008106","Assembly line balancing (ALB);evolutionary algorithms;multimanned;multiobjective optimization;reinforcement learning (RL);uncertain demand","Workstations;Costs;Production;Task analysis;Optimization;Robustness;Job shop scheduling","","","","1","","","IEEE","6 Jan 2023","","","IEEE","IEEE Early Access Articles"
"An application of reinforcement learning to manufacturing scheduling problems","Y. Tanaka; T. Yoshida","School of Information Science, Japan Advanced Institute of Science and Technology, Tatsunokuchi, Ishikawa, Japan; Japan Advanced Institute of Science and Technology, Tatsunokuchi, Ishikawa, Japan","IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)","6 Aug 2002","1999","4","","534","539 vol.4","The feasibility of applying reinforcement learning to a flow shop scheduling problem, the objective of which is to minimize the maximum completion time, is studied for two and three machines. It is generally hard to obtain any optimal solution in this problem domain with more than two machines, whereas with exactly two machines, the optimal is given by Johnson's algorithm. The impressive points revealed by the implementation of various instances of the reinforcement learning formulations are as follows. First, a good formulation may sometimes lead an agent to acquire the optimal rule that minimizes an objective function. Secondly, an agent can learn and obtain the improved schedules even when the formulation is not perfect. Thirdly, the same formulation is sound not only for the two-machine problem, but for the three-machine problem where Johnson's algorithm does not necessarily give any optimal solution. Consequently, the utilization of the reinforcement learning has potential to help us find an approximate solution or sometimes the optimal solution in a relatively simple way. The capability of a reinforcement learning agent, however, mostly depends upon the problem formulation. It is devised by utilizing theoretical solution methods and heuristics. At the same time, the agent has great flexibility to obtain improved schedules under the formulation with less prior knowledge.","1062-922X","0-7803-5731-0","10.1109/ICSMC.1999.812460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=812460","","Learning;Manufacturing;Job shop scheduling;Information science;Testing","learning (artificial intelligence);production control;software agents","reinforcement learning;manufacturing scheduling problems;flow shop scheduling problem;maximum completion time;Johnson's algorithm;optimal rule;two-machine problem;three-machine problem","","1","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Efficient Scheduling in Complex Semiconductor Equipment","D. Suerich; T. Young","PEER Group Inc., Kitchener, Canada; PEER Group Inc., Kitchener, Canada","2020 31st Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC)","3 Sep 2020","2020","","","1","3","Semiconductor cluster tools add an integral component to the modern semiconductor manufacturing process. These complex tools provide a flexible deployment option to group multiple processing steps into a single piece of equipment, allowing for more efficient processing. They also contribute to a reduction in the number of times a wafer must go through the atmospheric-vacuum-atmospheric cycle. These highly automated tools present a complex scheduling challenge where process-specific requirements are balanced against a need to achieve maximum wafer throughput in a fault tolerant manner. Previous work demonstrated that a reinforcement learning algorithm would be suitable for automated generation of efficient planners for simple tools. This investigation looked at how these same techniques could be extended to operate on more complex equipment.","2376-6697","978-1-7281-5876-1","10.1109/ASMC49169.2020.9185293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9185293","scheduling;reinforcement learning;throughput;optimization","Tools;Learning (artificial intelligence);Job shop scheduling;Optimization;Semiconductor device modeling;Throughput","learning (artificial intelligence);scheduling;semiconductor device manufacture;semiconductor industry","complex semiconductor equipment;semiconductor cluster tools;integral component;modern semiconductor manufacturing process;complex tools;flexible deployment option;group multiple processing steps;atmospheric-vacuum-atmospheric cycle;highly automated tools present a complex scheduling challenge;process-specific requirements;maximum wafer throughput;fault tolerant manner;reinforcement learning algorithm;automated generation;efficient planners;simple tools;complex equipment","","1","","3","IEEE","3 Sep 2020","","","IEEE","IEEE Conferences"
"Smart Scheduling for Flexible and Hybrid Production with Multi-Agent Deep Reinforcement Learning","S. Wang; J. Li; Y. Luo","School of Mechanical and Automotive Engineering, South China University of Technology, Guangzhou, China; School of Mechanical and Automotive Engineering, South China University of Technology, Guangzhou, China; School of Electrical Engineering, Guangzhou City University of Technology, Guangzhou, China","2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)","3 Feb 2022","2021","2","","288","294","As a consequence of growing personalized consumption, the demand for customized production processes is steadily increasing. Therefore, production systems should have a flexible structure, redundant resources, and dynamic scheduling mechanism to incorporate hybrid production of small-lot or even one-item orders. A greater flexibility will result in a higher uncertainty of production performance which can be mitigated by the introduction of smart scheduling mechanisms. In this paper, the scheduling problem of flexible and hybrid production is modelled as a workpiece-machine and workpiece-workpiece interaction problem. As an approach to problem solving, the Multi-Agent Deep Reinforcement Learning algorithm is applied. Simulation results confirm that the scheduling algorithm converges in the training process resulting in a clear performance improvement with respect to processing time.","","978-1-6654-2877-4","10.1109/ICIBA52610.2021.9688235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688235","Industry 4.0;smart factory;job shop scheduling;deep reinforcement learning","Training;Industries;Production systems;Job shop scheduling;Uncertainty;Scheduling algorithms;Simulation","deep learning (artificial intelligence);dynamic scheduling;multi-agent systems;production engineering computing;reinforcement learning","flexible structure;dynamic scheduling mechanism;smart scheduling mechanisms;scheduling problem;flexible production;workpiece-workpiece interaction problem;multiagent deep reinforcement learning algorithm;scheduling algorithm;customized production processes;production systems","","","","13","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"Learning compliance control laws for robotic assembly: an application of adaptive reinforcement learning control","Boo-Ho Yang; H. Asada","Center for Information-Driven Mechanical Systems Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA; Center for Information-Driven Mechanical Systems Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA","Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)","6 Aug 2002","1993","2","","1821","1824 vol.2","The primary objective of this paper is to demonstrate that the adaptive reinforcement learning method proposed by Asada-Izumi (1989) is efficient and useful in learning a compliance control law in a class of robotic assembly tasks. A simple ball-aligning task is used as an example, where a robot is required to move a ball to the corner of a box. In the simulation, the robot is initially provided with only position feedback gains to follow the nominal trajectory. However, at each attempt, the location of the box is randomly deviated from the nominal position as uncertainty of the environment and, therefore, compliant motion control is required to guide the ball to the corner of the box. After repeating the collision with the walls of the box, the robot can successfully generate force feedback gains to modify its nominal motion. Our results show that the new learning method can be used to learn a compliance control law effectively.","","0-7803-1421-2","10.1109/IJCNN.1993.717008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=717008","","Robot control;Robotic assembly;Programmable control;Adaptive control;Force feedback;Force control;Force measurement;Learning;Education;Motion control","assembling;robots;motion control;force control;feedback;compliance control;intelligent control;learning (artificial intelligence);adaptive control","compliance control;robotic assembly;adaptive reinforcement learning control;ball-aligning task;position feedback;compliant motion control;force feedback gains","","1","","11","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Parallel Approach Control of Micro-Assembly Manipulators","J. Zhang; L. Bi; W. Wu; K. Du","Research Center of Laser Fusion, China Academy of Engineering Physics, Mianyang, China; Research Center of Laser Fusion, China Academy of Engineering Physics, Mianyang, China; Research Center of Laser Fusion, China Academy of Engineering Physics, Mianyang, China; Research Center of Laser Fusion, China Academy of Engineering Physics, Mianyang, China","2022 14th International Conference on Computer and Automation Engineering (ICCAE)","26 Apr 2022","2022","","","25","30","Micro-devices are usually assembled by micro-assembly robot operating multi-manipulators in a narrow assembly space. To ensure assembly accuracy, manipulators are required to assemble multiple parts in parallel. However, in the traditional assembly, in order to prevent the parts from interfering, the movement trajectory of each manipulator must be manually input, which leads to low planning efficiency. In this paper, a multi-body spatial approach algorithm is established based on reinforcement learning methods, and a multi-body collision avoidance control method based on grid method and reinforcement learning is proposed, which realizes the purpose of efficiently generating the running trajectory and improving the planning efficiency on the premise that multi-parts achieve the target pose without interference. In addition, the calibration method of the simulation space coordinate systems and the Cartesian space coordinate systems is proposed, the motion trajectory in simulation space is transformed into the Cartesian space motion trajectory to control manipulators movement. Experimental results verify the effectiveness of the proposed method, and realize intelligent and safe parallel approaching of multi-manipulators.","","978-1-6654-8380-3","10.1109/ICCAE55086.2022.9762422","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762422","collision avoidance control;approach planning;reinforcement learning;micro-assembly","Robot kinematics;Reinforcement learning;Interference;Human factors;Aerospace electronics;Manipulators;Trajectory","assembling;calibration;collision avoidance;control engineering computing;learning (artificial intelligence);manipulators;mobile robots;motion control;path planning;position control;robot vision;robotic assembly","movement trajectory;manipulator;manually input;low planning efficiency;multibody spatial approach algorithm;reinforcement learning methods;multibody collision avoidance control method;grid method;running trajectory;calibration method;simulation space;Cartesian space motion trajectory;manipulators movement;intelligent parallel;safe parallel;multimanipulators;reinforcement learning-based parallel approach control;microassembly manipulators;microdevices;microassembly robot;narrow assembly space;assembly accuracy;multiple parts;traditional assembly","","1","","13","IEEE","26 Apr 2022","","","IEEE","IEEE Conferences"
"Real-Time Decision Making for a Car Manufacturing Process Using Deep Reinforcement Learning","T. P. Gros; J. Groß; V. Wolf","Saarland Informatics Campus Saarland University, Saarbrücken, GERMANY; Saarland Informatics Campus Saarland University, Saarbrücken, GERMANY; Saarland Informatics Campus Saarland University, Saarbrücken, GERMANY","2020 Winter Simulation Conference (WSC)","29 Mar 2021","2020","","","3032","3044","Computer simulations of manufacturing processes are in widespread use for optimizing production planning and order processing. If unforeseeable events are common, real-time decisions are necessary to maximize the performance of the manufacturing process. Pre-trained AI-based decision support offers promising opportunities for such time-critical production processes. Here, we explore the effectiveness of deep reinforcement learning for real-time decision making in a car manufacturing process. We combine a simulation model of a central production part, the line buffer, with deep reinforcement learning algorithms, in particular with deep Q-Learning and Monte Carlo tree search. We simulate two different versions of the buffer, a single-agent and a multi-agent one, to generate large amounts of data and train neural networks to represent near-optimal strategies. Our results show that deep reinforcement learning performs extremely well and the resulting strategies provide near-optimal decisions in real-time, while alternative approaches are either slow or give strategies of poor quality.","1558-4305","978-1-7281-9499-8","10.1109/WSC48552.2020.9383884","European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383884","","Manufacturing processes;Decision making;Reinforcement learning;Production;Real-time systems;Automobiles;Time factors","decision making;decision support systems;learning (artificial intelligence);Monte Carlo methods;neural nets;optimisation;production planning;tree searching","order processing;real-time decisions;decision support;time-critical production processes;real-time decision making;car manufacturing process;simulation model;central production part;deep reinforcement learning algorithms;deep Q-Learning;near-optimal decisions;computer simulations;production planning","","4","","23","IEEE","29 Mar 2021","","","IEEE","IEEE Conferences"
"A Memetic Algorithm With Reinforcement Learning for Sociotechnical Production Scheduling","F. Grumbach; N. E. A. Badr; P. Reusch; S. Trojahn","Center for Applied Data Science (CfADS), Bielefeld University of Applied Sciences and Arts, Gütersloh, Germany; Center for Applied Data Science (CfADS), Bielefeld University of Applied Sciences and Arts, Gütersloh, Germany; Center for Applied Data Science (CfADS), Bielefeld University of Applied Sciences and Arts, Gütersloh, Germany; Department of Economics, Anhalt University of Applied Sciences, Bernburg, Germany","IEEE Access","13 Jul 2023","2023","11","","68760","68775","The following interdisciplinary article presents a memetic algorithm with deep reinforcement learning (DRL) for solving practically oriented dual resource constrained flexible job shop scheduling problems (DRC-FJSSP). From research projects in industry, we recognize the need to consider flexible machines, flexible human workers, worker capabilities, setup and processing operations, material arrival times, complex job paths with parallel tasks for bill of material (BOM) manufacturing, sequence-dependent setup times and (partially) automated tasks in human-machine-collaboration. In recent years, there has been extensive research on metaheuristics and DRL techniques but focused on simple scheduling environments. However, there are few approaches combining metaheuristics and DRL to generate schedules more reliably and efficiently. In this paper, we first formulate a DRC-FJSSP to map complex industry requirements beyond traditional job shop models. Then, we propose a scheduling framework integrating a discrete event simulation (DES) for schedule evaluation, considering parallel computing and multicriteria optimization. Here, a memetic algorithm is enriched with DRL to improve sequencing and assignment decisions. Through numerical experiments with real-world production data, we confirm that the framework generates feasible schedules efficiently and reliably for a balanced optimization of makespan (MS) and total tardiness (TT). Utilizing DRL instead of random metaheuristic operations leads to better results in fewer algorithm iterations and outperforms traditional approaches in such complex environments.","2169-3536","","10.1109/ACCESS.2023.3292548","Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)(grant numbers:490988677); Hochschule Bielefeld —University of Applied Sciences and Arts; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173523","Discrete event simulation;genetic algorithm;job scheduling;production planning;reinforcement learning;simheuristics","Job shop scheduling;Metaheuristics;Optimization;Minimization;Production;Sequential analysis;Schedules;Discrete event simulation;Genetic algorithms;Production planning;Reinforcement learning;Heuristic algorithms","bills of materials;deep learning (artificial intelligence);discrete event simulation;job shop scheduling;metaheuristics;parallel processing;production engineering computing;reinforcement learning","assignment decisions;automated tasks;bill of material manufacturing;complex job paths;deep reinforcement learning;DES;discrete event simulation;DRC-FJSSP;dual resource constrained flexible job shop scheduling problems;flexible human workers;flexible machines;human-machine-collaboration;interdisciplinary article;makespan optimization;material arrival times;memetic algorithm;metaheuristics;multicriteria optimization;parallel computing;parallel tasks;processing operations;sequence-dependent setup times;sociotechnical production scheduling;total tardiness optimization;worker capabilities","","","","50","CCBY","5 Jul 2023","","","IEEE","IEEE Journals"
"Multi-agent Reinforcement Learning Approach for Scheduling Cluster Tools with Condition Based Chamber Cleaning Operations","C. Hong; T. -E. Lee","Dept. Industrial and Systems Engineering, KAIST, Daejeon, South Korea; Dept. Industrial and Systems Engineering, KAIST, Daejeon, South Korea","2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)","17 Jan 2019","2018","","","885","890","To improve the performance of semiconductors, manufacturers shrink the wafer circuit width dramatically. This increases the importance of quality control during wafer fabrication process. Thus, fabs recently tend to clean each chamber for every predetermined period to remove chemical residues and heat in the chamber. Such a chamber cleaning process can improve the quality of wafers, but the productivity is lowered. Therefore, the quality and the productivity of wafers have trade-off relations according to the cleaning period. In this paper, we propose a new class of cleaning process, condition based cleaning, which aims to maximize productivity while maintaining wafers quality. We then propose a way to find scheduling cluster tools based on multi-agent reinforcement learning. Finally, we experimentally verify that our algorithm can archive higher performance than existing sequences, under condition-based cleaning.","","978-1-5386-6805-4","10.1109/ICMLA.2018.00143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614168","Scheduling;Chamber cleaning;Cluster tool;Multi-agent reinforcement learning;semiconductor manufacturing","Cleaning;Robots;Tools;Contamination;Productivity;Reinforcement learning;Job shop scheduling","cluster tools;learning (artificial intelligence);multi-agent systems;process control;production engineering computing;quality control;scheduling;semiconductor device manufacture;semiconductor industry;semiconductor technology;surface cleaning","cluster tools scheduling;cleaning period;chamber cleaning process;chemical residues;predetermined period;wafer fabrication process;quality control;wafer circuit width;condition based chamber cleaning operations;multiagent reinforcement learning approach;condition-based cleaning;wafers quality;productivity;condition based cleaning","","9","","16","IEEE","17 Jan 2019","","","IEEE","IEEE Conferences"
"Quality of Service-Adaptive Industrial Internet of Things leveraging Edge Computing and Deep Reinforcement Learning : The Deep QoS-Adaptive Learning Environment (DeQALE) Architecture","J. L. Herrera; J. Berrocal; J. Galán-Jiménez; J. García-Alonso; J. M. Murillo","Department of Computer Systems and Telematics Engineering, University of Extremadura, Cáceres, Spain; Department of Computer Systems and Telematics Engineering, University of Extremadura, Cáceres, Spain; Department of Computer Systems and Telematics Engineering, University of Extremadura, Cáceres, Spain; Department of Computer Systems and Telematics Engineering, University of Extremadura, Cáceres, Spain; Department of Computer Systems and Telematics Engineering, University of Extremadura, Cáceres, Spain","2022 17th Iberian Conference on Information Systems and Technologies (CISTI)","14 Jul 2022","2022","","","1","4","The advent of the Internet of Things (IoT) paradigm allows for real-world tasks to be monitorized and managed using computing applications. The application of IoT to the industrial environment leads to the Industrial Internet of Things (IIoT), in which industrial processes are managed through IoT, thus allowing industry workers to better control their facilities and processes. However, IIoT applications have very strict Quality of Service (QoS) requirements, such as short response times, that require for the deployment of their services in edge nodes, close to the facilities. In IIoT scenarios, deploying each of the services so that the QoS requirements are met is not an easy task. Moreover, the dynamicity of the environment requires for a fast, adaptive solution. In this position paper, we propose DeQALE, an approach to train a deep reinforcement learning agent to solve this problem in short cycles.","2166-0727","978-9-8933-3436-2","10.23919/CISTI54924.2022.9820397","European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820397","Quality of Service;Industry 4.0;Internet of Things;Deep Reinforcement Learning;Edge computing","Industries;Process control;Quality of service;Reinforcement learning;Time factors;Task analysis;Monitoring","cloud computing;deep learning (artificial intelligence);Internet of Things;multi-agent systems;production engineering computing;quality of service;reinforcement learning","adaptive industrial Internet of Things;DeQALE architecture;edge nodes;quality of service requirements;IIoT;industry workers;industrial environment;deep QoS-adaptive learning environment;edge computing;deep reinforcement learning agent;QoS requirements","","","","12","","14 Jul 2022","","","IEEE","IEEE Conferences"
"Exploration of Unknown Environment using Deep Reinforcement Learning","A. Ali; S. Gul; T. Mahmood; A. Ullah","Control, Automotive and Robotics Lab, National Centre of Robotics and Automation (NCRA), Rawalpindi, Pakistan; Control, Automotive and Robotics Lab, National Centre of Robotics and Automation (NCRA), Rawalpindi, Pakistan; Department of Electrical Engineering, Balochistan University of IT, Engineering and Management Sciences, Quetta, Pakistan; Department of Electrical Engineering, Balochistan University of IT, Engineering and Management Sciences, Quetta, Pakistan","2023 International Conference on Robotics and Automation in Industry (ICRAI)","6 Apr 2023","2023","","","1","6","Exploring the unknown environment is a very crucial task where human life is at risks like search and rescue operations, abandoned nuclear plants, covert operations and more. Autonomous robots could serve this task efficiently. The existing methods use uncertainty models for localization and map building to explore the unknown areas requiring high onboard computation and time. We propose to use Deep Reinforcement Learning (DRL) for the autonomous exploration of unknown environments. In DRL, the agent interacts with the environment and learns based on experiences (feedback/reward). We propose extrinsic and curiosity-driven reward functions to explore the environment. The curiosity-based reward function motivates the agent to explore unseen areas by predicting future states, while the extrinsic reward function avoids collisions. We train the differential drive robot in one environment and evaluate its performance in another unknown environment. We observe curiosity-driven reward function outperformed the extrinsic reward by exploring more areas in the unknown environment. The test results show the generalization capability to explore unknown environments with the proposed methods.","2831-3313","978-1-6654-6472-7","10.1109/ICRAI57502.2023.10089589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10089589","Double Deep Q-Network;Exploration;Mobile Robots;Navigation;Reinforcement Learning","Deep learning;Location awareness;Industries;Uncertainty;Service robots;Computational modeling;Reinforcement learning","deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;reinforcement learning","autonomous exploration;curiosity-based reward function;curiosity-driven reward function;Deep Reinforcement Learning;unknown areas;unknown environment","","","","14","IEEE","6 Apr 2023","","","IEEE","IEEE Conferences"
"Off-Policy Reinforcement Learning: Optimal Operational Control for Two-Time-Scale Industrial Processes","J. Li; B. Kiumarsi; T. Chai; F. L. Lewis; J. Fan","State Key Laboratory of Synthetical Automation for Process Industries and the International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; UTA Research Institute, University of Texas at Arlington, Arlington, TX, USA; State Key Laboratory of Synthetical Automation for Process Industries and the International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries and the International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries and the International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China","IEEE Transactions on Cybernetics","9 Nov 2017","2017","47","12","4547","4558","Industrial flow lines are composed of unit processes operating on a fast time scale and performance measurements known as operational indices measured at a slower time scale. This paper presents a model-free optimal solution to a class of two time-scale industrial processes using off-policy reinforcement learning (RL). First, the lower-layer unit process control loop with a fast sampling period and the upper-layer operational index dynamics at a slow time scale are modeled. Second, a general optimal operational control problem is formulated to optimally prescribe the set-points for the unit industrial process. Then, a zero-sum game off-policy RL algorithm is developed to find the optimal set-points by using data measured in real-time. Finally, a simulation experiment is employed for an industrial flotation process to show the effectiveness of the proposed method.","2168-2275","","10.1109/TCYB.2017.2761841","NSFC(grant numbers:61673280,61104093,61525302,61333012,61590922,61533015); Open Project of State Key Laboratory of Synthetical Automation for Process Industries(grant numbers:PAL-N201603); 111 Project(grant numbers:B08015); Fundamental Research Funds for the Central Universities(grant numbers:N160804001); Project of Liaoning Province(grant numbers:LJQ2015088,2015020164); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8100717","H∞ tracking control;operational index;operational optimization;reinforcement learning (RL);zero-sum game","Process control;Optimization;Automation;Time measurement;Velocity measurement","game theory;learning (artificial intelligence);optimisation;process control","performance measurements;operational indices;model-free optimal solution;off-policy reinforcement learning;lower-layer unit process control loop;fast sampling period;upper-layer operational index dynamics;general optimal operational control problem;zero-sum game off-policy RL algorithm;industrial flotation process;two-time-scale industrial processes;industrial flow lines","","52","","30","IEEE","9 Nov 2017","","","IEEE","IEEE Journals"
"Efficiency Optimization Design of Three-Level Active Neutral Point Clamped Inverter Based on Deep Reinforcement Learning","J. Wang; R. Yang; Z. Yao","School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China","2022 IEEE 6th Conference on Energy Internet and Energy System Integration (EI2)","10 May 2023","2022","","","605","610","The traditional power electronic converter design mostly adopts the sequential design method, which relies on manual experience. In recent years, power electronics automation design has attracted much attention by rapidly optimizing the parameters of power electronic systems by computers, but most of them cannot provide design solutions to adapt to changes in design requirements. Taking the efficiency optimization design of the three-level active neutral point clamped (ANPC) inverter as an example, this paper proposes a power electronics automation design method based on deep reinforcement learning (DRL), which can realize that quickly obtain the optimal design parameters according to the design objectives when the design requirements of converter change. Firstly, the overall efficiency optimization framework for the inverter based on DRL is introduced; then the efficiency model of the inverter is established; after that, the agent is continuously trained through the self-learning of the deep deterministic policy gradient (DDPG) algorithm, and an optimization strategy that minimizes power losses is obtained; the strategy can quickly respond to design specification changes and provide design parameters that maximize efficiency; finally, a 140 kW experimental prototype is built, and the experimental results verify the effectiveness of the proposed method.","","979-8-3503-4715-9","10.1109/EI256261.2022.10117037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10117037","power electronics automation design;three-level ANPC inverter;efficiency;DRL;DDPG algorithm","Deep learning;Computers;Automation;Prototypes;Reinforcement learning;System integration;Manuals","deep learning (artificial intelligence);gradient methods;invertors;optimisation;power engineering computing;reinforcement learning;switching convertors","deep deterministic policy gradient algorithm;deep reinforcement learning;design objectives;design requirements;efficiency model;efficiency optimization design;efficiency optimization framework;minimizes power losses;optimal design parameters;optimization strategy;power 140.0 kW;power electronic systems;power electronics automation design method;sequential design method;three-level active neutral point clamped inverter;traditional power electronic converter design","","1","","19","IEEE","10 May 2023","","","IEEE","IEEE Conferences"
"Hand-in-Hand Guidance: An Explore-Exploit Based Reinforcement Learning Method for Performance Driven Assembly-Adjustment","G. Duan; Y. Xu; Z. Liu; J. Tan","State Key Laboratory of CAD&CG, Zhejiang University, Hangzhou, China; State Key Laboratory of CAD&CG, Zhejiang University, Hangzhou, China; State Key Laboratory of CAD&CG, Zhejiang University, Hangzhou, China; State Key Laboratory of CAD&CG, Zhejiang University, Hangzhou, China","IEEE Transactions on Industrial Informatics","11 Aug 2023","2023","19","10","10045","10055","Nowadays, most high-precision products are still assembled manually, which leads to a low one-time pass rate of products. Workers need to adjust unqualified products repeatedly based on experience, resulting in inefficiency and poor quality consistency. In this work, we propose an explore-exploit reinforcement learning (EERL) framework to suggest the assembly parameters and quantity that workers need to adjust at each step. Jointed with the pretrained product performance prediction model, EERL can output a sequential decision to guide workers hand in hand to adjust unqualified products. EERL includes a two-phase learning process: 1) exploration; and 2) exploitation. In exploration phase, agents are encouraged by the curiosity to fully explore the qualified assembly states in the assembly-adjustment feature space. The regulated difference of random network distillation is used as a measure of curiosity. During exploitation, the agent is trained to learn an assembly-adjustment guidance policy of moving from any unqualified initial assembly state to the corresponding qualified state while satisfying the assembly-adjustment constraints. The curriculum learning mechanism is introduced to learn effectively in the complex environment with sparse reward and adjustment constraints. The proposed approach is validated on an benchmark optimization function and a case study of the gyroscope. The experimental results demonstrate that the proposed approach outperforms other existing approaches.","1941-0050","","10.1109/TII.2022.3232774","National Natural Science Foundation of China(grant numbers:52075480); Key Research and Development Program of Zhejiang Province(grant numbers:2021C01008); High-level Talent Special Support Plan of Zhejiang Province(grant numbers:2020R52004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018503","Assembly and adjustment;curiosity driven;performance driven;reinforcement learning","Task analysis;Predictive models;Informatics;Training;Gyroscopes;Trajectory;Quality assessment","assembling;optimisation;production engineering computing;reinforcement learning","assembly parameters;assembly-adjustment constraints;assembly-adjustment feature space;assembly-adjustment guidance policy;benchmark optimization function;curiosity;curriculum learning mechanism;EERL;exploration phase;explore-exploit based reinforcement learning method;gyroscope;hand-in-hand guidance;high-precision products;performance driven assembly-adjustment;pretrained product performance prediction model;qualified assembly states;quality consistency;reinforcement learning framework;sparse reward;two-phase learning process;unqualified initial assembly state;unqualified products","","","","26","IEEE","17 Jan 2023","","","IEEE","IEEE Journals"
"A Novel Method Combining Leader-Following Control and Reinforcement Learning for Pursuit Evasion Games of Multi-Agent Systems","Z. -Y. Zhu; C. -L. Liu","Key Laboratory of Advanced Process Control for Light Industry, (Ministry of Education), Institute of Automation, Jiangnan University, Wuxi, China; Key Laboratory of Advanced Process Control for Light Industry, (Ministry of Education), Institute of Automation, Jiangnan University, Wuxi, China","2020 16th International Conference on Control, Automation, Robotics and Vision (ICARCV)","8 Jan 2021","2020","","","166","171","In the existing literature, many methods have been utilized in solving the pursuit evasion game. However, most of these methods have one thing in common: all pursuers have to know the evader's position. This paper presents a method combining the multi-agent leader-following control and reinforcement learning, which aims at addressing a pursuit evasion game only when partial pursuers have the knowledge of the position of the evader who moves randomly in two-dimensional grid space. Through decomposing the team task, pursuers realize the encirclement of the evader. Simulation experiment validates the effectiveness of our method.","","978-1-7281-7709-0","10.1109/ICARCV50220.2020.9305441","National Natural Science Foundation of China(grant numbers:61973139,61473138); Fundamental Research Funds for the Central Universities(grant numbers:JUSRP22014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305441","","Task analysis;Games;Reinforcement learning;Multi-agent systems;Robot sensing systems;Markov processes;Automation","computer games;game theory;learning (artificial intelligence);multi-agent systems","reinforcement learning;pursuit evasion game;multiagent systems;evader position;multiagent leader-following control;partial pursuers;two-dimensional grid space","","1","","20","IEEE","8 Jan 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Approach to Vibration Compensation for Dynamic Feed Drive Systems","R. Gulde; M. Tuscher; A. Csiszar; O. Riedel; A. Verl","Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, Stuttgart, Germany; Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, Stuttgart, Germany; Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, Stuttgart, Germany; Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, Stuttgart, Germany; Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, Stuttgart, Germany","2019 Second International Conference on Artificial Intelligence for Industries (AI4I)","9 Mar 2020","2019","","","26","29","Vibration compensation is important for many domains. For the machine tool industry it translates to higher machining precision and longer component lifetime. Current methods for vibration damping have their shortcomings (e.g. need for accurate dynamic models). In this paper we present a reinforcement learning based approach to vibration compensation applied to a machine tool axis. The work describes the problem formulation, the solution, the implementation and experiments using industrial machine tool hardware and control system.","","978-1-7281-4087-2","10.1109/AI4I46381.2019.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9027784","Vibration Compensation;Reinforcement Learning;Dynamic Systems;Dynamic Feed Drive Systems;Vibration Damping;Machine Tool Axis","Vibrations;Machine tools;Learning (artificial intelligence);Hardware;Optimization;Feeds;Entropy","damping;drives;learning (artificial intelligence);machine tools;machining;vibrations","reinforcement learning approach;dynamic feed drive systems;machine tool industry;machining precision;vibration damping;industrial machine tool hardware;control system","","2","","22","IEEE","9 Mar 2020","","","IEEE","IEEE Conferences"
"Path Planning for Automation of Surgery Robot based on Probabilistic Roadmap and Reinforcement Learning","D. Baek; M. Hwang; H. Kim; D. -S. Kwon","Department of Robotics, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Robotics, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea","2018 15th International Conference on Ubiquitous Robots (UR)","23 Aug 2018","2018","","","342","347","Laparoscopic robotic surgery is a new surgical method performed by inserting several surgical tools and a laparoscope through an umbilical incision [12]. Compared with conventional laparoscopic surgery minimizes patient pain with minimally invasive surgery and has many advantages in terms of beauty. However, medical doctor's fatigue due to repetitive operations such as tissue resection and suturing still remains a problem to be improved. To solve this problem, there are a lot of automation researches on surgical robots [1], [7]-[10]. Especially in cutting automaton, for high accuracy, optimal path planning is essential factor. Probabilistic Roadmap (PRM) is a popular method for path planning. It creates path from static environment to desired point without collision. However, this does not show great performance in a dynamic environment. Reinforcement Learning (RL) shows strong performance in an unspecified probabilistic environment and it is widely applied to robot motion learning because learning data is not needed before [4]. In this paper, we suggest a collision avoidance path planning for automation of surgery robot by using PRM and RL in dynamic situation. We found the collision avoidance path through PRM and RL, and used mapping algorithm of coordination system from pixel to world coordination and transformed the coordination system from cartesian space to joint space using inverse kinematics. Finally, we apply it to the APOLLON laparoscopic surgery robotic system developed by KAIST in V-rep simulator. As a result, we confirmed a possible of collision avoidance path planning for automation of resection task for surgery robot.","","978-1-5386-6334-9","10.1109/URAI.2018.8441801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8441801","Laparoscopic surgery robot;Automation of resection;Probabilistic Roadmap;Reinforcement Learning;APOLLON system","Collision avoidance;Robot kinematics;Automation;Surgery;Tools;Probabilistic logic","collision avoidance;control engineering computing;learning (artificial intelligence);medical computing;medical robotics;optimal control;optimisation;probability;surgery","RL;APOLLON laparoscopic surgery robotic system;laparoscopic robotic surgery;surgical method;surgical tools;minimally invasive surgery;optimal path planning;PRM;unspecified probabilistic environment;robot motion learning;collision avoidance path planning;probabilistic roadmap;reinforcement learning","","15","","15","IEEE","23 Aug 2018","","","IEEE","IEEE Conferences"
"Digital Twin Enabled Dual-System Reinforcement Learning Method","H. Xie; S. Tan; F. Ling; J. Wu; L. He; X. Zhang","Haier Digital Technology (Shanghai) Co., Ltd., Shanghai, China; R&D dept., Haier Digital Technology (Shanghai) Co., Ltd., Shanghai, China; R&D dept., Haier Digital Technology (Shanghai) Co., Ltd., Shanghai, China; R&D dept., Haier Digital Technology (Shanghai) Co., Ltd., Shanghai, China; R&D dept., Haier Digital Technology (Shanghai) Co., Ltd., Shanghai, China; R&D dept., Haier Digital Technology (Shanghai) Co., Ltd., Shanghai, China","2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta)","27 Jul 2023","2022","","","2218","2223","The emergence of digital twins provides a new technical means for enabling cyber-physical manufacturing systems. When digital twins are applied in workshops, a major problem is how to use virtual-real mapping to connect the cyber and the physical world to improve decision-making and management in the workshop, to enable the self-adaptive scheduling automation of flexible workshops in multi-disturbance and uncertain production environments. This paper studies the adaptive scheduling of flexible workshops and combines the digital twin technology to establish a digitaltwin-based prototype system of self-adaptive scheduling of flexible workshop. Targeting the real-time and adaptability requirements of workshop scheduling in dynamic production environments, and based on the digital twin environment of workshops, this paper selects the average delivery time of workpieces as the optimization target, and proposes a Deep Q-network (DQN) method driven by the perception-cognitive dual system. The perception system generates workshop states using digital twins and knowledge graphs; the cognitive system abstracts the scheduling process into a process sequencing agent to optimize the completion time, and uses a workshop state matrix to describe the constraints of the problem, introducing order selection and workpiece selection actions step by step into the scheduling decision-making process; then a reward function is designed to evaluate the decision. The superiority of the proposed method is verified by the example verification and algorithm comparison analysis in the sewing workshop of a garment factory.","","979-8-3503-4655-8","10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00359","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10189650","Smart factory;flexible workshop;deep reinforcement learning;digital twin;DQN network","Job shop scheduling;Conferences;Decision making;Production equipment;Production;Optimal scheduling;Search problems","cyber-physical systems;decision making;deep learning (artificial intelligence);digital twins;manufacturing systems;production engineering computing;reinforcement learning;scheduling","adaptability requirements;cognitive system;cyber-physical manufacturing systems;Deep Q-network method;digital twin enabled dual-system reinforcement learning method;digital twin environment;digital twin technology;digital twins;digitaltwin-based prototype system;dynamic production environments;flexible workshop;order selection;perception system;perception-cognitive dual system;scheduling decision-making process;scheduling process;self-adaptive scheduling automation;sewing workshop;uncertain production environments;workpiece selection actions;workshop scheduling;workshop state matrix;workshop states","","","","14","IEEE","27 Jul 2023","","","IEEE","IEEE Conferences"
"Nonzero-Sum Game Reinforcement Learning for Performance Optimization in Large-Scale Industrial Processes","J. Li; J. Ding; T. Chai; F. L. Lewis","School of Information and Control Engineering, Liaoning Shihua University, Liaoning, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China","IEEE Transactions on Cybernetics","18 Aug 2020","2020","50","9","4132","4145","This article presents a novel technique to achieve plant-wide performance optimization for large-scale unknown industrial processes by integrating the reinforcement learning method with the multiagent game theory. A main advantage of this technique is that plant-wide optimal performance is achieved by a distributed approach where multiple agents solve simplified local nonzero-sum optimization problems so that a global Nash equilibrium is reached. To this end, first, the plant-wide performance optimization problem is reformulated by decomposition into local optimization subproblems for each production index in a multiagent framework. Then, the nonzero-sum graphical game theory is utilized to compute the operational indices for each unit process with the purpose of reaching the global Nash equilibrium, resulting in production indices following their prescribed target values. The stability and the global Nash equilibrium of this multiagent graphical game solution are rigorously proved. The reinforcement learning methods are then developed for each agent to solve the nonzero-sum graphical game problem using data measurements available in the system in real time. The plant dynamics do not have to be known. Finally, the emulation results are given to show the effectiveness of the proposed automated decision algorithm by using measured data from a large mineral processing plant in Gansu Province, China.","2168-2275","","10.1109/TCYB.2019.2950262","National Natural Science Foundation of China(grant numbers:61988101,61673280,61525302,61590922); National Basic Research Program of China (973 Program)(grant numbers:2018 YFB1701104); Open Project of Key Field Alliance of Liaoning Province(grant numbers:2019-KF-03-06); Project of Liaoning Province(grant numbers:LR2017006); Project of Liaoning Shihua University(grant numbers:2018XJJ-005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8906005","Game theory;Nash equilibrium;plant-wide performance optimization;reinforcement learning","Optimization;Production;Games;Heuristic algorithms;Nash equilibrium;Reinforcement learning","decision theory;game theory;industrial plants;learning (artificial intelligence);multi-agent systems;optimisation","mineral processing plant;sum game reinforcement learning;large-scale industrial processes;large-scale unknown industrial processes;reinforcement learning method;multiagent game theory;plant-wide optimal performance;distributed approach;simplified local nonzero-sum optimization problems;global Nash equilibrium;plant-wide performance optimization problem;local optimization subproblems;production index;multiagent framework;nonzero-sum graphical game theory;unit process;production indices;multiagent graphical game solution;nonzero-sum graphical game problem;plant dynamics","","45","","43","IEEE","19 Nov 2019","","","IEEE","IEEE Journals"
"Pattern driven dynamic scheduling approach using reinforcement learning","W. Yingzi; J. Xinli; H. Pingbo; G. Kanfeng","Shenyang Ligong University, Shenyang, China; Shenyang Ligong University, Shenyang, China; Shenyang Ligong University, Shenyang, China; Shenyang Institute of Automation, Chinese Academy and Sciences, Shenyang, China","2009 IEEE International Conference on Automation and Logistics","25 Sep 2009","2009","","","514","519","Production scheduling is critical for manufacturing system. Dispatching rules are usually applied dynamically to schedule the job in the dynamic job-shop. The paper presents an adaptive iterative scheduling algorithm that operates dynamically to schedule the job in the dynamic job-shop. In order to get adaptive behavior, the reinforcement learning system is done with the phased Q-learning by defining the intermediate state pattern. We convert the scheduling problem into reinforcement learning problems by constructing a multi-phase dynamic programming process, including the definition of state representation, actions and the reward function. We use five heuristic rules, CNP-CR, CNP-FCFS, CNP-EFT, CNP-EDD and CNP-SPT, as actions and the scheduling objective: minimization of maximum completion time. So a complex dynamic scheduling problem can be divided into a sequential sub-problem easier to solve. We also analyze the time and the solution and present some experimental results.","2161-816X","978-1-4244-4794-7","10.1109/ICAL.2009.5262867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5262867","Reinforcement Learning;Contract Net Protocol (CNP);State Pattern;Dynamic Scheduling","Dynamic scheduling;Job shop scheduling;Optimal scheduling;Dispatching;Learning systems;Single machine scheduling;Manufacturing automation;Production systems;Scheduling algorithm;Machine learning","adaptive scheduling;dispatching;dynamic programming;dynamic scheduling;iterative methods;job shop scheduling;learning (artificial intelligence);manufacturing systems;minimisation","pattern driven dynamic scheduling approach;reinforcement learning;production scheduling;manufacturing system;dispatching rule;dynamic job-shop scheduling;adaptive iterative scheduling algorithm;Q-learning;multiphase dynamic programming process;state representation;maximum completion time minimization","","5","","12","IEEE","25 Sep 2009","","","IEEE","IEEE Conferences"
"An improved reinforcement learning control strategy for batch processes","P. Zhang; J. Zhang; Y. Long; B. Hu","School of Engineering, Newcastle University, Newcastle upon Tyne, UK; School of Engineering, Newcastle University, Newcastle upon Tyne, UK; School of Computing, Durham University, Durham, UK; School of Computing, Newcastle University, Newcastle upon Tyne, UK","2019 24th International Conference on Methods and Models in Automation and Robotics (MMAR)","14 Oct 2019","2019","","","360","365","Batch processes are significant and essential manufacturing route for the agile manufacturing of high value added products and they are typically difficult to control because of unknown disturbances, model plant mismatches, and highly nonlinear characteristic. Traditional one-step reinforcement learning and neural network have been applied to optimize and control batch processes. However, traditional one-step reinforcement learning and the neural network lack accuracy and robustness leading to unsatisfactory performance. To overcome these issues and difficulties, a modified multi-step action Q-learning algorithm (MMSA) based on multiple step action Q-learning (MSA) is proposed in this paper. For MSA, the action space is divided into some periods of same time steps and the same action is explored with fixed greedy policy being applied continuously during a period. Compared with MSA, the modification of MMSA is that the exploration and selection of action will follow an improved and various greedy policy in the whole system time which can improve the flexibility and speed of the learning algorithm. The proposed algorithm is applied to a highly nonlinear batch process and it is shown giving better control performance than the traditional one-step reinforcement learning and MSA.","","978-1-7281-0933-6","10.1109/MMAR.2019.8864632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8864632","Batch process;optimal control;reinforcement learning","Batch production systems;Reinforcement learning;Mathematical model;Neural networks;Process control;Inductors;Task analysis","agile manufacturing;batch processing (industrial);greedy algorithms;learning (artificial intelligence);learning systems;neural nets;process control","multiple step action Q-learning;fixed greedy policy;modified multi-step action Q-learning algorithm;highly nonlinear batch process control;plant mismatches;neural network;high value added products;agile manufacturing;reinforcement learning control strategy","","2","","20","IEEE","14 Oct 2019","","","IEEE","IEEE Conferences"
"Motion Planning for Human-Robot Collaboration based on Reinforcement Learning","T. Yu; Q. Chang","Department of Mechanical and Aerospace Engineering, University of Virginia, Charlottesville, VA, USA; Department of Mechanical and Aerospace Engineering and Department of Engineering Systems and Environment, University of Virginia, Charlottesville, VA, USA","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","1866","1871","Robots are good at performing repetitive tasks in modern manufacturing industries. However, robot motions are mostly planned and preprogramed with a notable lack of adaptivity to task changes. Even for slightly changed tasks, the whole system must be reprogrammed by robotics experts. Therefore, it is highly desirable to have a flexible motion planning method, with which robots can adapt to certain task changes in unstructured environments, such as production systems or warehouses, with little or no intervention needed from non-expert personnel. In this paper, we propose a user-guided motion planning algorithm in combination with reinforcement learning (RL) method to enable robots to automatically generate their motion plans for new tasks by learning from a few common tasks saved as primitive actions by kinesthetic human demonstrations. Features of these primitive actions are captured through screw transformation of the end-effector during the task. A mapping method is developed to convert features of primitive actions to new task segments and further used to construct the reward function in RL. A Q-learning algorithm is applied to train the motion planning policy, following which an adaptive motion plan for the new task can be generated or a request for additional primitive actions will be returned if current primitive actions are insufficient for satisfying new task constraints. Multiple experiments conducted on common tasks and scenarios demonstrate that the proposed RL-based motion planning method is effective.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926471","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926471","","Robot motion;Manufacturing industries;Production systems;Q-learning;Service robots;Motion segmentation;Fasteners","human-robot interaction;learning (artificial intelligence);mobile robots;motion control;path planning;production engineering computing;robots","human-robot collaboration;repetitive tasks;modern manufacturing industries;robot motions;task changes;slightly changed tasks;robotics experts;flexible motion planning method;production systems;warehouses;nonexpert personnel;user-guided motion;reinforcement learning method;motion plans;common tasks;kinesthetic human demonstrations;mapping method;task segments;Q-learning algorithm;motion planning policy;adaptive motion plan;additional primitive actions;current primitive actions;task constraints;RL-based motion planning method","","","","15","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-Based Optimal Control of Variable Cycle Engine Performance","B. Tao; L. -Y. Yang; D. -S. Wu; S. -L. Li; Z. -X. Huang; X. -S. Sun","College of Automation and Electrical Engineering, Shenyang Ligong University, Shenyang, China; State Key Lab of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; College of Automation and Electrical Engineering, Shenyang Ligong University, Shenyang, China; State Key Lab of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; State Key Lab of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; State Key Lab of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China","2022 International Conference on Advanced Robotics and Mechatronics (ICARM)","29 Nov 2022","2022","","","1002","1005","In order to find the lowest fuel consumption rate in steady-state operation of a variable-cycle engine, and thus increase the flight range as well as the flight radius of the aircraft. In this paper, we design a multivariable control law for variable-cycle engines and adopt a deep reinforcement learning algorithm in machine learning to optimal the control law for variable-cycle engines in real time online, which results in a lower fuel consumption rate compared to the engine fuel consumption rate under conventional optimal algorithms.","","978-1-6654-8306-3","10.1109/ICARM54641.2022.9959322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9959322","","Deep learning;Machine learning algorithms;Optimal control;Reinforcement learning;Real-time systems;Steady-state;Fuels","engines;fuel economy;learning (artificial intelligence);multivariable control systems;optimal control;optimisation","deep reinforcement learning algorithm;engine fuel consumption rate;multivariable control law;optimal control;steady-state operation;variable cycle engine performance;variable-cycle engine","","1","","8","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach for Dynamic Supplier Selection","T. I. Kim; R. U. Bilsel; S. R. T. Kumara","Industrial & Manufacturing Engineering, Pennsylvania State University, USA; Industrial & Manufacturing Engineering, Pennsylvania State University, USA; Industrial & Manufacturing Engineering, Pennsylvania State University, USA","2007 IEEE International Conference on Service Operations and Logistics, and Informatics","19 Nov 2007","2007","","","1","6","Supplier selection is one of the most critical decisions in a supply chain. While good suppliers can contribute to the supply chain's overall performance, incorrect selection can drive the whole supply chain into disarray. In this paper, we focus on the problem of supplier selection in a manufacturing firm. We allow each supplier to compete with each other to be selected by the buyer for procurement. The competition is modeled in an auction framework as a bidding process where a supplier cannot observe immediate actions of other suppliers but has complete knowledge of their previous actions. We allow a supplier to use this knowledge in guessing other suppliers future actions and bid accordingly. Our model enables repeated games, which can be assumed to be more flexible compared to most game theory applications in the supplier selection literature. Reinforcement learning and fictitious play are used in the auction framework to implement repeated games.","","978-1-4244-1117-7","10.1109/SOLI.2007.4383959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383959","Fictitious play;reinforcement learning;repeated games;supplier selection","Learning;Supply chains;Game theory;Manufacturing industries;Acoustical engineering;Data envelopment analysis;Costs;Drives;Pulp manufacturing;Procurement","","","","3","","28","IEEE","19 Nov 2007","","","IEEE","IEEE Conferences"
"Two-Stage Strategy to Achieve a Reinforcement Learning-Based Upset Recovery Policy for Aircraft","H. Cao; W. Zeng; H. Jiang; H. Hu; C. Li; W. Lu; H. Xiong","School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","2080","2085","Aircraft upset situations are the highest risk to civil aviation. Thus, a reliable upset recovery policy is necessary for aircraft. In this paper, a two-stage strategy to achieve a reinforcement learning (RL)-based upset recovery policy that takes time of recovery and loss of altitude into account is proposed for aircraft to recover from an arbitration upset situation to level flight. Based on the proposed two-stage strategy and Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm, an algorithm to achieve a TD3-based upset recovery policy for aircraft is developed. Experiments are conducted based on X-Plane 11 to evaluate the effectiveness of the proposed two-stage strategy and the performance of the achieved upset recovery policy in stall recovery and spin recovery.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727381","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727381","aircraft upset recovery;fine-tuning;pre-train;reinforcement learning;twin delayed deep deterministic policy gradient","Training;Automation;Reinforcement learning;Reliability;Aircraft","aircraft control;deep learning (artificial intelligence);gradient methods;motion control;reinforcement learning","stall recovery;two-stage strategy;reinforcement learning;aircraft upset situations;arbitration upset situation;twin delayed deep deterministic policy gradient algorithm;altitude recovery;TD3-based upset recovery policy;spin recovery;loss-of-control conditions;civil aviation risk","","","","19","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"A case-based reinforcement learning for probe robot path planning","Yang Li; Chen Zonghai; Chen Feng","Department of Automation, USTC, Hefei, China; Department of Automation, USTC, Hefei, China; Department of Automation, USTC, Hefei, China","Proceedings of the 4th World Congress on Intelligent Control and Automation (Cat. No.02EX527)","7 Nov 2002","2002","2","","1161","1165 vol.2","This paper discusses the application of case-based learning for probe robot path planning in unknown environments. Case-based learning which makes use of past experience, is an incremental learning process. This paper proposes an algorithm of introducing reinforcement learning to case-based-reasoning, which makes full use of knowledge acquired by reinforcement learning to construct and extend the case-library. This method can enhance the adaptability of robot to unknown environments and solve the problem of case acquiring as well as poor real-time performance, high learning risk of reinforcement learning. Also, with the forget-rule, case-library can be updated in time so that efficiency of case searching and learning is increased. As the learning progressing and the case-library dynamically updated, robot's intelligence has been greatly improved. A simulation shows the validity and feasibility of this method.","","0-7803-7268-9","10.1109/WCICA.2002.1020762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1020762","","Learning;Probes;Path planning;Intelligent robots;Robotics and automation","mobile robots;path planning;case-based reasoning;learning (artificial intelligence)","case-based reinforcement learning;probe robot path planning;incremental learning process;case acquisition;robot adaptability;unknown environments;poor real-time performance;forget-rule;case library;case searching;case learning","","","","","IEEE","7 Nov 2002","","","IEEE","IEEE Conferences"
"Research on Intelligent Maintenance Decision for Flexible Electronic Manufacturing Equipment Based on Deep Reinforcement Learning","Y. Deng; Z. Zhang; H. Huang; X. Liu; J. Tang","School of Electro-mechanical Engeneering, Guangdong University of Technology, Guangzhou, China; School of Electro-mechanical Engeneering, Guangdong University of Technology, Guangzhou, China; School of Electro-mechanical Engeneering, Guangdong University of Technology, Guangzhou, China; School of Electro-mechanical Engeneering, Guangdong University of Technology, Guangzhou, China; School of Electro-mechanical Engeneering, Guangdong University of Technology, Guangzhou, China","2023 IEEE 16th International Conference on Electronic Measurement & Instruments (ICEMI)","10 Oct 2023","2023","","","1","4","In order to achieve the intelligent maintenance of flexible electronic manufacturing equipment, this paper constructs an intelligent maintenance decision model based on deep reinforcement learning by combining a lifetime prediction network and Markov decision process. First, the life prediction network extracts a feature map characterizing the operating state of the equipment based on the raw data input from the equipment; the intelligent decision network then takes the feature map as an environmental input and uses the agent in the network to output maintenance actions for the equipment; finally, through the continuous iterative update of the life prediction network and the intelligent decision network, the decision model can continuously optimize the maintenance decision of the equipment, thus assisting the equipment to complete the maintenance decision task. Experiments on the bearing dataset show that the proposed model can effectively implement intelligent maintenance decision for the equipment.","","979-8-3503-2714-4","10.1109/ICEMI59194.2023.10270494","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10270494","deep reinforcement learning;flexible electronic manufacturing equipment;intelligent maintenance decision","Deep learning;Reinforcement learning;Maintenance engineering;Predictive models;Markov processes;Feature extraction;Manufacturing","","","","","","10","IEEE","10 Oct 2023","","","IEEE","IEEE Conferences"
"Dynamic scheduling in large-scale stochastic processing networks for demand-driven manufacturing using distributed reinforcement learning","S. Qu; J. Wang; J. Jasperneite","Civil&Environmental Engineering, Stanford Unverisity, Palo Alto, CA, USA; Center for Sustainable Development and Global Competiveness, Stanford University, Palo Alto, CA, USA; Fraunhofer IOSB-INA, Lemgo, Germany","2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA)","25 Oct 2018","2018","1","","433","440","In the area of demand-driven manufacturing systems, one critical problem currently is how to dynamically and optimally schedule, i.e., allocate, actual jobs with different customer requirements in large-scale manufacturing systems in order to meet the various objectives. Scheduling experts have proposed several scheduling methods with acceptable performance for manufacturing systems based on their understanding of the systems' characteristics. However, the problem remains challenging due to the complicated composition of the multiple objectives of the system, the complex system dynamics, constraints, and the extremely high computational cost for large-scale manufacturing systems. In this paper, we apply a stochastic processing network that can capture the stochasticity and dynamics of discrete manufacturing systems. We then propose a data-driven, distributed reinforcement learning (DRL) method so that little information about the system dynamics is required, and the learning and search costs of a scheduling policy with high production performance in a processing system can be reduced, thus this method is capable of scaling to large-scale processing systems. In particular, we first use a stochastic processing network, i.e., a queueing model, to represent the production processes in a typical discrete manufacturing system so that it can be simulated. We then decompose the reinforcement learning into local processes. Each local processs agent can make decisions locally by assigning indices to jobs based on each job's real-time information (index policy). Because of this distributed learning characteristic and index policy, our approach is much more scalable and efficient than either centralized methods or traditional decentralized reinforcement learning methods. Based on our simulation, we find our approach can achieve higher production performance than other heuristics, past decentralized reinforcement learning methods, or centralized methods in the stochastic processing networks with different scales.","1946-0759","978-1-5386-7108-5","10.1109/ETFA.2018.8502508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502508","demand-driven;scheduling;stochastic;dynamic;large scale;distributed reinforcement learning;queueing network","Task analysis;Job shop scheduling;Stochastic processes;Manufacturing systems;Dynamic scheduling","dynamic scheduling;manufacturing systems;stochastic processes","dynamic scheduling;large-scale stochastic processing networks;demand-driven manufacturing systems;discrete manufacturing systems;distributed reinforcement learning method;decentralized reinforcement learning methods;job real-time information","","6","","37","IEEE","25 Oct 2018","","","IEEE","IEEE Conferences"
"Study on inertia based MPPT control using reinforcement learning algorithm for extra power production","N. Jargalsaikhan; S. Byambaa; F. Masahiro; S. Tomonobu","Grad. School of Engineering and Science, University of the Ryukyus, Okinawa, Japan; Department of Electrical Engineering, Mongolian University of Science and Technology, Ulaanbaatar, Mongolia; National Institute of Technology, Sasebo College, Nagasaki, Japan; Faculty of Engineering, University of the Ryukyus, Okinawa, Japan","2023 3rd International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)","22 Sep 2023","2023","","","1","5","Permanent magnet synchronous generator (PMSG) based wind energy conversion system (WECS) also known as Type 4-WECS has many advantages over other types of WECS. One of the advantages is full variable speed operation for low wind speed. With the MPPT control, most wind turbines try to follow optimal rotational speed for maximum possible power capture. However, due to rotor inertia, tracking loss is inevitable, which causes decreased power production. This study investigates the use of inertia in the rotational masses of the WT for increased power production. The idea is to use the rotor inertia of the large-scale wind turbine (VSWT) to temporarily retain the rotational speed at a higher value for a short time rather than following optimal rotational speed in a variable speed region. As a result, the wind turbine can capture more power. The optimal tip speed ratio (TSR) method is used as the main MPPT control method, and additional Reinforcement Learning (RL) algorithm-based reference rotational readjustment was used for the purpose mentioned above. The simulation studies have been conducted in Matlab/Simulink® software and it showed that power production of wind turbines has increased slightly than the standard method.","","979-8-3503-2297-2","10.1109/ICECCME57830.2023.10252813","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10252813","Variable speed wind generator;PMSG;use of rotor inertia;MPPT;reinforcement learning","Wind speed;Velocity control;Software algorithms;Rotors;Production;Reinforcement learning;Synchronous generators","maximum power point trackers;permanent magnet generators;power generation control;rotors;synchronous generators;wind power;wind power plants;wind turbines","extra power production;increased power production;large-scale wind turbine;low wind speed;main MPPT control method;maximum possible power capture;optimal rotational speed;optimal tip speed ratio method;permanent magnet synchronous generator based wind energy conversion system;reinforcement learning algorithm;rotational masses;rotor inertia;Type 4-WECS;variable speed operation;variable speed region","","","","13","IEEE","22 Sep 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-Driven Scheduling in Multijob Serial Lines: A Case Study in Automotive Parts Assembly","S. Lee; J. Kim; G. Wi; Y. Won; Y. Eun; K. -J. Park","Department of Electrical, Engineering, and Computer Science, Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea; Department of Electrical, Engineering, and Computer Science, Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea; Department of Electrical, Engineering, and Computer Science, Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea; Department of Electrical, Engineering, and Computer Science, Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea; Department of Electrical, Engineering, and Computer Science, Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea; Department of Electrical, Engineering, and Computer Science, Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea","IEEE Transactions on Industrial Informatics","","2023","PP","99","1","12","Multijob production (MJP) is a class of flexible manufacturing systems, which produces different products within the same production system. MJP is widely used in product assembly, and efficient MJP scheduling is crucial for productivity. Most of the existing MJP scheduling methods are inefficient for multijob serial lines with practical constraints. We propose a deep reinforcement learning (DRL)-driven scheduling framework for multijob serial lines by properly considering the practical constraints of identical machines, finite buffers, machine breakdown, and delayed reward. We analyze the starvation and the blockage time, and derive a DRL-driven scheduling strategy to reduce the blockage time and balance the loads. We validate the proposed framework by using real-world factory data collected over six months from a tier-one vendor of a world top-three automobile company. Our case study shows that the proposed scheduling framework improves the average throughput by 24.2% compared with the conventional approach.","1941-0050","","10.1109/TII.2023.3292538","DGIST R&D Program of the Ministry of Science and ICT(grant numbers:23-DPIC-16); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10210628","Multijob serial lines;production scheduling;reinforcement learning (RL);smart manufacturing","Job shop scheduling;Electric breakdown;Production;Production systems;Maintenance engineering;Data models;Production facilities","","","","","","","CCBYNCND","8 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Optimising discrete event simulation models using a reinforcement learning agent","D. C. Creighton; S. Nahavandi","School of Engineering and Technology, Deakin University, Geelong, VIC, Australia; School of Engineering and Technology, Deakin University, Geelong, VIC, Australia","Proceedings of the Winter Simulation Conference","22 Jan 2003","2002","2","","1945","1950 vol.2","A reinforcement learning agent has been developed to determine optimal operating policies in a multi-part serial line. The agent interacts with a discrete event simulation model of a stochastic production facility. This study identifies issues important to the simulation developer who wishes to optimise a complex simulation or develop a robust operating policy. Critical parameters pertinent to 'tuning' an agent quickly and enabling it to rapidly learn the system were investigated.","","0-7803-7614-5","10.1109/WSC.2002.1166494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166494","","Discrete event simulation;Learning;Mathematical model;Intelligent agent;Production systems;Power system modeling;Job shop scheduling;Manufacturing;Optimization methods;Australia","learning (artificial intelligence);software agents;discrete event simulation;production engineering computing","reinforcement learning agent;optimal operating policies;multi-part serial line;discrete event simulation model;stochastic production facility;optimisation","","8","","15","","22 Jan 2003","","","IEEE","IEEE Conferences"
"Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot","T. Huang; K. Chen; B. Li; Y. -H. Liu; Q. Dou","Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","4640","4647","Task automation of surgical robot has the potentials to improve surgical efficiency. Recent reinforcement learning (RL) based approaches provide scalable solutions to surgical automation, but typically require extensive data collection to solve a task if no prior knowledge is given. This issue is known as the exploration challenge, which can be alleviated by providing expert demonstrations to an RL agent. Yet, how to make effective use of demonstration data to improve exploration efficiency still remains an open challenge. In this work, we introduce Demonstration-guided EXploration (DEX), an efficient reinforcement learning algorithm that aims to overcome the exploration problem with expert demonstrations for surgical automation. To effectively exploit demonstrations, our method estimates expert-like behaviors with higher values to facilitate productive interactions, and adopts non-parametric regression to enable such guidance at states unobserved in demonstration data. Extensive experiments on 10 surgical manipulation tasks from SurRoL, a comprehensive surgical simulation platform, demonstrate significant improvements in the exploration efficiency and task success rates of our method. Moreover, we also deploy the learned policies to the da Vinci Research Kit (dVRK) platform to show the effectiveness on the real robot. Code is available at https://github.com/med-air/DEX.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160327","CUHK T Stone Robotics Institute, Hong Kong Innovation and Technology Commission(grant numbers:ITS/237/21FP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160327","","Automation;Medical robotics;Codes;Reinforcement learning;Data collection;Space exploration;Behavioral sciences","control engineering computing;dexterous manipulators;learning (artificial intelligence);manipulators;medical robotics;public domain software;regression analysis;reinforcement learning;surgery;surgical robots;telerobotics","10 surgical manipulation tasks;comprehensive surgical simulation platform;demonstration data;Demonstration-guided EXploration;Demonstration-guided reinforcement learning;efficient reinforcement learning algorithm;expert demonstrations;exploration challenge;exploration efficiency;exploration problem;extensive data collection;learned policies;recent reinforcement learning based approaches;RL agent;scalable solutions;surgical automation;surgical efficiency;surgical robot;task automation;task success rates","","","","70","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Real-Time Charging Scheduling of Automated Guided Vehicles in Cyber-Physical Smart Factories Using Feature-Based Reinforcement Learning","C. -C. Lin; K. -Y. Chen; L. -T. Hsieh","Department of Industrial Engineering and Management, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Industrial Engineering and Management, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Industrial Engineering and Management, National Yang Ming Chiao Tung University, Hsinchu, Taiwan","IEEE Transactions on Intelligent Transportation Systems","29 Mar 2023","2023","24","4","4016","4026","In smart factories, a variety of automated guided vehicles (AGVs) communicate with cyber-physical systems (CPSs) to autonomously deliver raw materials and workpieces among smart production facilities. In practice, instead of acquiring more costly AGVs to cause congestion in existing working space, most factories develop rule-based and model-based approaches to improve the AGV utilization rate and further the production efficiency. However, these charging strategies require predefined rules or models for estimating the internal information of batteries, so that they lead to huge computational costs and estimation errors. As a consequence, this work creates a Markov decision process problem for real-time charging scheduling of AGVs to fulfill uncertain AGV dispatching requests from the CPS for production lines, in which four bounds for charging heterogeneous AGVs are considered from practical experiences for increasing the AGV utilization rate. This work further improves a feature-based reinforcement learning approach, in which the state and action space can be effectively reduced through approximating the state-value function by five feature functions, including the estimated revenue for improving the utilization time, the total AGV charging cost, the cost of penalizing unfulfilled dispatching requests, the priority of charging newer batteries, and the priority of charging the batteries close to be fully charged, respectively. Experimental results show that the proposed algorithm obtains better benefits than the current practical approach, and improves the AGV utilization rate.","1558-0016","","10.1109/TITS.2023.3234010","MOST(grant numbers:MOST 111-2221-E-A49-081,MOST 111-2622-E-A49-011,MOST 109-2221-E-009-068-MY3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10014532","Automated guided vehicle (AGV);charging scheduling;smart manufacturing;reinforcement learning;state-action-reward-state-action (SARSA);cyber physical system","Batteries;Dispatching;Smart manufacturing;Job shop scheduling;Heuristic algorithms;Real-time systems;Production facilities","automatic guided vehicles;cyber-physical systems;electric vehicle charging;learning (artificial intelligence);Markov processes;production engineering computing;production facilities;reinforcement learning;scheduling","AGV utilization rate;automated guided vehicles;charging strategies;costly AGVs;current practical approach;cyber-physical smart factories;cyber-physical systems;existing working space;feature functions;feature-based reinforcement learning approach;heterogeneous AGVs;huge computational costs;Markov decision process problem;model-based approaches;production efficiency;production lines;raw materials;real-time charging scheduling;smart production facilities;total AGV charging cost;uncertain AGV dispatching requests;utilization time;workpieces","","2","","36","IEEE","10 Jan 2023","","","IEEE","IEEE Journals"
"Dynamic Scheduling of Maintenance by a Reinforcement Learning Approach - A Semiconductor Simulation Study","M. Geurtsen; I. Adan; Z. Atan","Industrial Technology and Engineering Centre, Nexperia, Nijmegen, THE NETHERLANDS; Department of Industrial Engineering, Eindhoven University of Technology, Eindhoven, THE NETHERLANDS; Department of Industrial Engineering, Eindhoven University of Technology, Eindhoven, THE NETHERLANDS","2022 Winter Simulation Conference (WSC)","23 Jan 2023","2022","","","3110","3121","Scheduling in a semiconductor back-end factory is an extremely sophisticated and complex task. In semiconductor industry, more often than not, the scheduling of maintenance is underexposed to production scheduling. This is a missed opportunity as maintenance and production activities are deeply intertwined. This study considers the dynamic scheduling of maintenance activities on an assembly line. A policy is constructed to schedule a cleaning activity on the last machine of an assembly line such that the average production rate is maximized. The policy takes into account the given flexibility and the buffer content of the buffers in-between the machines in the assembly line. A Markov Decision Process is formulated for the problem and solved using Value Iteration and Reinforcement Learning Algorithms. In addition, for a real world case study, a simulation analysis is performed to evaluate the potential practical benefits.","1558-4305","978-1-6654-7661-4","10.1109/WSC57314.2022.10015402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015402","","Schedules;Job shop scheduling;Heuristic algorithms;Electronics industry;Reinforcement learning;Maintenance engineering;Markov processes","Markov processes;production engineering computing;reinforcement learning;scheduling;semiconductor industry","assembly line;average production rate;cleaning activity;dynamic scheduling;maintenance;Markov decision process;production activities;production scheduling;reinforcement learning algorithms;semiconductor back-end factory;semiconductor industry;semiconductor simulation study","","","","10","IEEE","23 Jan 2023","","","IEEE","IEEE Conferences"
"Performance optimization of function localization neural network by using reinforcement learning","T. Sasakawa; Jinglu Hu; K. Hirasawa","Graduate School of Information, Production and Systems, Waseda University, Japan; Graduate School of Information, Production and Systems, Waseda University, Japan; Graduate School of Information, Production and Systems, Waseda University, Japan","Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.","27 Dec 2005","2005","2","","1314","1319 vol. 2","According to Hebb's cell assembly theory, the brain has the capability of function localization. On the other hand, it is suggested that the brain has three different learning paradigms: supervised, unsupervised and reinforcement learning. Inspired by the above knowledge of brain, we present a self-organizing function localization neural network (FLNN), that contains supervised, unsupervised and reinforcement learning paradigms. In this paper, we concentrate our discussion mainly on applying a simplified reinforcement learning called evaluative feedback to optimization of the self-organizing FLNN. Numerical simulations show that the self-organizing FLNN has superior performance to an ordinary artificial neural network (ANN).","2161-4407","0-7803-9048-2","10.1109/IJCNN.2005.1556044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1556044","","Optimization;Neural networks;Artificial neural networks;Neurons;Biological neural networks;Brain modeling;Assembly;Hebbian theory;Supervised learning;Unsupervised learning","learning (artificial intelligence);neural nets;brain;neurophysiology;cellular biophysics","reinforcement learning;function localization neural network;Hebb cell assembly theory;supervised learning;unsupervised learning;evaluative feedback","","1","","11","IEEE","27 Dec 2005","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Dynamic Scheduling of Random Arrival Tasks in Cloud Manufacturing","L. Chen; L. Zhou; M. Zhou; X. Yu; Y. Zhu; W. Song; Z. Lu; J. Li","School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; School of Medicine, Duke University, Durham, USA; School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; Software Institute Nanjing University, Nanjing, China; School of Internet of Things, Jiangnan University, Wuxi, China; School of Computer Science and Technology, Nanjing University of Information Science and Technology, Nanjing, China; School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; School of Mathematics and Statistics, Nanjing University of Information Science and Technology, Nanjing, China","2022 6th International Conference on Universal Village (UV)","26 Jul 2023","2022","","","1","6","Compared with the stable orders of traditional manufacturing, cloud manufacturing (CMfg) fulfilled with masses of random orders, so the CMfg server needs an algorithm with low time and space complexity to prevent the server from crashing due to excessive instantaneous data. Besides, the random changes of manufacturing resources and service must be considered when establishing a scheduling model for CMfg. To solve this problem, we propose an adaptive Deep Q-Networks (ADQN) method with a resizable network that converts cloud manufacturing scheduling problems with multiple objectives into specific reinforcement learning goal and can adapt to changing environments. Our experimental results show that ADQN is comparable to other real-time scheduling methods, the average subtask completion time and the standard deviation of occupation obtained by ADQN keep at a low level.","","978-1-6654-7477-1","10.1109/UV56588.2022.10185485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10185485","cloud manufacturing;reinforcement learning;deep Q-learning;dynamic scheduling;random arrival tasks","Adaptation models;Job shop scheduling;Adaptive systems;Reinforcement learning;Dynamic scheduling;Real-time systems;Manufacturing","cloud computing;computational complexity;deep learning (artificial intelligence);dynamic scheduling;manufacturing processes;production engineering computing;reinforcement learning;resource allocation","adaptive deep Q-networks method;ADQN;cloud manufacturing resources;deep reinforcement learning;dynamic scheduling;instantaneous data;random arrival tasks;resizable network;standard deviation;time-space complexity","","","","27","IEEE","26 Jul 2023","","","IEEE","IEEE Conferences"
"The application of actor-critic reinforcement learning for fab dispatching scheduling","N. Kim; H. Shin","Department of Industrial and Systems Engineering, KAIST, Daejeon, KOREA; Department of Industrial and Systems Engineering, KAIST, Daejeon, KOREA","2017 Winter Simulation Conference (WSC)","8 Jan 2018","2017","","","4570","4571","This paper applies Actor-Critic reinforcement learning to control lot dispatching scheduling in reentrant line manufacture model. To minimize the Work-In-Process(WIP) and Cycle Time(CT), the lot dispatching policy is directly optimized through Actor-Critic algorithm. The results show that the optimized dispatching policy yields smaller average WIP and CT than traditional dispatching policy such as Shortest Processing Time, Latest-Step-First-Served, and Least-Work-Next-Queue.","1558-4305","978-1-5386-3428-8","10.1109/WSC.2017.8248209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8248209","","Economic indicators","dispatching;integrated circuit manufacture;learning (artificial intelligence);lot sizing;manufacturing processes;optimisation;production control;scheduling;work in progress","fab dispatching scheduling control;lot dispatching policy optimization;cycle time minimization;work-in-process minimization;actor-critic reinforcement learning;reentrant line manufacture model","","1","","3","IEEE","8 Jan 2018","","","IEEE","IEEE Conferences"
"Automation of Human Decision Making by Using Reinforcement-learning for Office Work with PC","M. Fukai; M. Tadokoro; H. Oishi; R. Uchida; K. Tsuchikawa","NTT Access Network Service Systems Laboratories, Yokosuka City, Japan; NTT Access Network Service Systems Laboratories, Yokosuka City, Japan; NTT Access Network Service Systems Laboratories, Yokosuka City, Japan; NTT Access Network Service Systems Laboratories, Yokosuka City, Japan; NTT Access Network Service Systems Laboratories, Yokosuka City, Japan","2023 24st Asia-Pacific Network Operations and Management Symposium (APNOMS)","25 Sep 2023","2023","","","275","278","AI technology is a key which improve diversified and complex business with saving personnel. However, the technologies considered so far have limitations in supporting human decision-making. In this paper, we aim to expand decision support by automating human decision making without expertise or analytical techniques by machine learning human actions for office operations. Reinforcement learning acquires optimal actions through interaction between the agent and the environment, and has a high affinity for learning operations on a computer. However, in actual workflow, it is essential to design rewards for reinforcement learning and to avoid interfering with workers and business systems. Therefore, we propose a new method for machine learning without the need to design rewards and without invading actual work environments, by using the operation logs collected from the personnel's computers and assigning rewards to human interactions with computers as correct answers.","2576-8565","978-89-950043-9-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10258142","Automation;Reinforcement learning;Operational logs","Automation;Decision making;Reinforcement learning;Personnel;Business","","","","","","5","","25 Sep 2023","","","IEEE","IEEE Conferences"
"Development of Parametric Reinforcement Learning for different operation preferences","R. Qiu; G. Yang; Z. Xu; Z. Shao","Department of Control Science and Engineering, Zhejiang University, Hangzhou, China; Department of Control Science and Engineering, Zhejiang University, Hangzhou, China; Department of Control Science and Engineering, Zhejiang University, Hangzhou, China; Department of Control Science and Engineering, Zhejiang University, Hangzhou, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","4859","4864","With the industry process becoming more and more complex, there are more and more different working conditions and operation policy requirements. Accordingly, operators with different operation preferences are needed in different conditions. Some are conservative and some are aggressive. At the same time, Deep Reinforcement Learning (DRL) is becoming notable for its excellent performance in exploring controlling policies in various kinds of problems in the past few years. However, although DRL can help solve the problem in some condition, it can be difficult for one agent to adapt to all situations, and it is also a waste of time to train a new agent when new operation requirements come. In this paper, a method called Parametric Reinforcement Learning is proposed to solve the problem, using a parameter to represent the policy characteristic of a single agent, so that the target operation can be fitted with some base agents trained ahead of time. Also, Shell benchmark is used to simulate the effectiveness of this method.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055517","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055517","DRL;Parametric Reinforcement Learning;Shell;operation preference;operation policy","Industries;Employee welfare;Deep learning;Automation;Process control;Reinforcement learning;Benchmark testing","deep learning (artificial intelligence);multi-agent systems;personnel;reinforcement learning","base agents;controlling policies;deep reinforcement learning;different operation preferences;DRL;industry process;operation policy requirements;operation requirements;parametric reinforcement learning;Shell benchmark;single agent;target operation","","","","15","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Electronic Design Automation: Case Studies and Perspectives: (Invited Paper)","A. F. Budak; Z. Jiang; K. Zhu; A. Mirhoseini; A. Goldie; D. Z. Pan",The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; Google; Google; The University of Texas at Austin,"2022 27th Asia and South Pacific Design Automation Conference (ASP-DAC)","21 Feb 2022","2022","","","500","505","Reinforcement learning (RL) algorithms have recently seen rapid advancement and adoption in the field of electronic design automation (EDA) in both academia and industry. In this paper, we first give an overview of RL and its applications in EDA. In particular, we discuss three case studies: chip macro placement, analog transistor sizing, and logic synthesis. In collaboration with Google Brain, we develop a hybrid RL and analytical mixed -size placer and achieve better results with less training time on public and proprietary benchmarks. Working with Intel, we develop an RL-inspired optimizer for analog circuit sizing, combining the strengths of deep neural networks and reinforcement learning to achieve state-of-the-art black-box optimization results. We also apply RL to the popular logic synthesis framework ABC and obtain promising results. Through these case studies, we discuss the advantages, disadvantages, opportunities, and challenges of RL in EDA.","2153-697X","978-1-6654-2135-5","10.1109/ASP-DAC52403.2022.9712578","NSF(grant numbers:1704758,1718570,2112665); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712578","","Training;Industries;Deep learning;Electric potential;Design automation;Neural networks;Reinforcement learning","analogue circuits;circuit optimisation;deep learning (artificial intelligence);electronic design automation;reinforcement learning","analog transistor sizing;Google Brain;hybrid RL;public benchmarks;proprietary benchmarks;RL-inspired optimizer;analog circuit sizing;optimization;EDA;electronic design automation;reinforcement learning;rapid advancement;chip macro placement;logic synthesis framework ABC","","1","","25","IEEE","21 Feb 2022","","","IEEE","IEEE Conferences"
"Research on Preventive Maintenance of Industrial Internet Based on Reinforcement Learning","M. Ma; Z. Wang; S. Wang; P. Lin","Ningxia Electric Power, Dispatching Control Center, Yinchuan, China; Beijing University of Posts and Telecommunications, Beijing, China; State Grid Ningxia Electric Power Co., Ltd. Information Communication Company, Yinchuan, China; Beijing Vectinfo Technologies Co., LTD., Beijing, China","2022 International Conference on Information Processing and Network Provisioning (ICIPNP)","20 Mar 2023","2022","","","1","5","In the Industrial Internet, the failure prediction and health management of industrial equipment can help managers to further predict and determine the damage and danger of the current equipment, ensure the safe and stable operation of industrial equipment. A reasonable forecast and management plan can greatly save the cost in the industrial production process and improve the industrial production efficiency. In order to adapt to more complex application scenarios, this paper models multiple devices with different decay rates and their upstream production buffers in a pipeline system. Considering the semi-Markov decision process corresponding to different decay rates, a preventive maintenance strategy based on DDQN is proposed. This strategy can help managers determine the optimal maintenance methods for different types of equipment under the condition of limited resources, achieve the purpose of increasing production and reducing maintenance costs, and has a certain guiding role in solving equipment maintenance problems in the actual production process.","","978-1-6654-6405-5","10.1109/ICIPNP57450.2022.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10062512","Industrial Internet;Preventive Maintenance;DDQN;Semi-Markov Decision","Adaptation models;Costs;Pipelines;Production;Reinforcement learning;Information processing;Internet","Internet;Markov processes;preventive maintenance;production engineering computing;reinforcement learning","complex application scenarios;decay rates;equipment maintenance problems;failure prediction;health management;industrial equipment;industrial production efficiency;industrial production process;maintenance cost reduction;management plan;optimal maintenance methods;preventive maintenance strategy;production process;reinforcement learning;safe operation;semiMarkov decision process;stable operation;upstream production buffers","","","","7","IEEE","20 Mar 2023","","","IEEE","IEEE Conferences"
"Manipulation Skill Acquisition for Robotic Assembly using Deep Reinforcement Learning","F. Li; Q. Jiang; W. Quan; R. Song; Y. Li","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China","2019 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)","17 Oct 2019","2019","","","13","18","Nowadays mobile manipulators are commonly used in assembly tasks, which can reach greater workspace but also cause more uncertainties. Uncertainty is one of the main factors affecting the quality and efficiency of assembly tasks. The framework of skill acquisition based on machine learning can make a good job of solving the uncertainties in mobile manipulation. In this paper, a method based on deep reinforcement learning is proposed to learn assembly skills. The Deep Deterministic Policy Gradient algorithm based on off-policy can scale to complex manipulation tasks. The safety constraint was set to the torque of each joint. The positive and negative reward function is designed to improve learning efficiency, which could be used in different assembly tasks. The deep neural network policies could be trained out efficiently on a real robot platform. This approach was performed on the KUKA iiwa robot arm equipped with 7-dof force/torque sensors. The results show that the robot could learn assembly skills properly with no prior knowledge.","2159-6255","978-1-7281-2493-3","10.1109/AIM.2019.8868579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8868579","","Task analysis;Reinforcement learning;Robotic assembly;Service robots;Training;Circuit breakers","force sensors;gradient methods;industrial manipulators;learning (artificial intelligence);mobile robots;neurocontrollers;robotic assembly;torque control","manipulation skill acquisition;robotic assembly;deep reinforcement learning;mobile manipulators;machine learning;mobile manipulation;assembly skills;deep deterministic policy gradient algorithm;complex manipulation tasks;positive reward function;negative reward function;learning efficiency;assembly tasks;deep neural network policies;robot platform;KUKA iiwa robot arm;7-dof force/torque sensors","","8","","26","IEEE","17 Oct 2019","","","IEEE","IEEE Conferences"
"An adaptive AQM algorithm based on neuron reinforcement learning","C. Zhou; D. Di; Q. Chen; J. Guo","School of Automation, Nanjing University of Science and Technology of PLA, Nanjing, China; School of Automation, Nanjing University of Science and Technology of PLA, Nanjing, China; School of Automation, Nanjing University of Science and Technology of PLA, Nanjing, China; School of Automation, Nanjing University of Science and Technology of PLA, Nanjing, China","2009 IEEE International Conference on Control and Automation","8 Feb 2010","2009","","","1342","1346","In recent years, it has become an active research direction to develop adaptive and robust active queue management (AQM) scheme for congestion control of complex time-varying network. A novel adaptive AQM scheme based on neuron reinforcement learning (NRL) is presented in this paper. This scheme uses queue length and link rate as congestion notification to determine an appropriate drop/mark probability, and the parameters of neuron can be adjusted online according to the time-varying network environment so that the stability of queue dynamics and robustness for fluctuation of TCP loads are guaranteed. This scheme is easy to implement with simple structure, and it is independent of the model of plant to be controlled. Simulation results show that this proposed algorithm is especially suitable for solving the complex network congestion control problem, and also has better stability and robustness.","1948-3457","978-1-4244-4706-0","10.1109/ICCA.2009.5410198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5410198","","Neurons;Learning;Robust stability;Automation;Programmable control;Adaptive control;Robust control;Automatic control;Control systems;Control theory","adaptive control;learning (artificial intelligence);probability;queueing theory;stability;telecommunication congestion control;telecommunication network management;time-varying systems","adaptive AQM algorithm;neuron reinforcement learning;active queue management;network congestion control;time-varying network;drop-mark probability;queue dynamics stability;TCP load","","6","","11","IEEE","8 Feb 2010","","","IEEE","IEEE Conferences"
"Selective Data Collection Method for Deep Reinforcement Learning","T. Wang; H. Yang; Z. Tan; Y. Yu","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Key Laboratory of Knowledge Automation for Industrial Processes of Ministry of Education School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","886","889","In deep reinforcement learning, reinforcement learning is responsible for interacting with the environment to produce data, and artificial neural networks are responsible for value function fitting. It is observed that artificial neural networks converged differently to different inputs, which, in our analysis, is due to imbalanced data. Therefore, we propose selective data collection to boost the quality of the data by then discarding the excess data. It has been proved experimentally that our method can significantly contribute to the convergence rate of the reinforcement learning algorithm.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023607","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023607","Reinforcement Learning;Experience Replay;imbalanced data","Deep learning;Automation;Fitting;Reinforcement learning;Artificial neural networks;Data collection;Filtering algorithms","convergence;data acquisition;data integrity;deep learning (artificial intelligence);reinforcement learning","artificial neural networks;convergence rate;data quality;deep reinforcement learning;imbalanced data;selective data collection;value function fitting","","","","12","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Sample-Efficient Deep Reinforcement Learning via Balance Sample","H. Yang; T. Wang; Z. Tan; Y. Yu","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Key Laboratory of Knowledge Automation for Industrial Processes of Ministry of Education School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","890","895","In this paper, we propose two algorithms to improve sample efficiency by focusing on late stage samples in episodes. The first algorithm is Balanced Sample Experience Replay (BSER). Unlike the traditional random sampling approach, this algorithm improves the final score and stability in environment by learning more late stage experience in the corresponding episode. The second algorithm is weight-corrected DQN (WCDQN). This algorithm differs from the traditional undifferentiated update approach by differentially updating the samples used for training to improve the final score and stability in environment. We tested both algorithms on a classic Atari game environment and demonstrated the effectiveness of the algorithms.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023918","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023918","sample efficiency;Experience Replay;Balanced Sample;DQN","Training;Deep learning;Automation;Focusing;Games;Reinforcement learning;Convergence","computer games;deep learning (artificial intelligence);reinforcement learning;sampling methods","balanced sample experience replay;BSER;classic Atari game environment;episodes;late stage experience;late stage samples;sample-efficient deep reinforcement learning;traditional random sampling approach;traditional undifferentiated update approach;WCDQN;weight-corrected DQN","","","","11","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning-based Hierarchical Obstacle Avoidance Strategy for Fixed-wing Aircraft","Y. Ou; H. Jiang; Z. Xu; W. Lu; H. Xiong","School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","1815","1819","Obstacle avoidance is a crucial issue to enhance the safety of aircraft. Aircraft usually need to avoid a drop in altitude and keep a set course while avoiding an obstacle. In this paper, a hierarchical obstacle avoidance strategy is proposed to address obstacle avoidance, the drop in altitude, and course keeping simultaneously. The hierarchical obstacle avoidance strategy integrates a high-level reinforcement learning-based navigator and a low-level attitude controller. Experiments are conducted in X-Plane, a flight simulator, to evaluate the proposed hierarchical obstacle avoidance strategy.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055344","Harbin Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055344","aircraft;course keeping;loss of altitude;reinforcement learning;obstacle avoidance","Automation;Attitude control;Aircraft navigation;Safety;Collision avoidance;Aircraft","aerospace simulation;aircraft;attitude control;collision avoidance;mobile robots;reinforcement learning","fixed-wing aircraft;high-level reinforcement learning-based navigator;reinforcement learning-based hierarchical obstacle avoidance strategy","","","","21","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"New Methods for Optimal Operational Control of Industrial Processes Using Reinforcement Learning on Two Time Scales","W. Xue; J. Fan; V. G. Lopez; J. Li; Y. Jiang; T. Chai; F. L. Lewis","State Key Laboratory of Synthetical Automation for Process Industries and the International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries and the International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; UTA Research Institute, The University of Texas at Arlington, Arlington, USA; School of Information and Control Engineering, Liaoning Shihua University, Fushun, China; State Key Laboratory of Synthetical Automation for Process Industries and the International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries and the International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; UTA Research Institute, The University of Texas at Arlington, Arlington, USA","IEEE Transactions on Industrial Informatics","14 Feb 2020","2020","16","5","3085","3099","Current challenges in industrial processes control include achieving optimum operation for systems with two-time-scale dynamics and unknown models. This paper presents, for the first time, the integration of singular perturbation theory and reinforcement learning to solve this problem. To this end, an optimal operational control (OOC) problem with two time scales is formulated to reach the desired operational indices. Then, a singularly perturbed dynamics for two-time-scale industrial operational processes is developed by introducing a perturbed scale, resulting in the separation of the original system dynamics. Thus, the original optimization problem is decomposed into a reduced slow subproblem and a boundary fast subproblem. The fact that the sum of the separate solutions of these subproblems is approximately equal to the solution of the OOC problem is proven. Then, two Q-learning algorithms are proposed to obtain a composite feedback control. Finally, an industrial thickener example is employed to show the effectiveness of the proposed method.","1941-0050","","10.1109/TII.2019.2912018","National Natural Science Foundation of China(grant numbers:61533015,61304028,61673280); Higher Education Discipline Innovation Project(grant numbers:B08015); Fundamental Research Funds for the Central Universities(grant numbers:N180804001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693769","Optimal operational control (OOC),  $Q$ -learning;reinforcement learning (RL);singular perturbation;two-time-scale industrial processes","Perturbation methods;Process control;Optimization;Reinforcement learning;Heuristic algorithms;Automation;Economics","approximation theory;feedback;optimal control;optimisation;process control;singularly perturbed systems","original optimization problem;OOC problem;Q-learning algorithms;composite feedback control;reinforcement learning;industrial processes control;two-time-scale dynamics;singular perturbation theory;operational control problem;singularly perturbed dynamics;two-time-scale industrial operational processes","","29","","56","IEEE","18 Apr 2019","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Impedance Learning for Robot Admittance Control in Industrial Assembly","X. Feng; T. Shi; W. Li; P. Lu; Y. Pan","School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Department of Mechanical Engineering, University of Hong Kong, Hong Kong; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China","2022 International Conference on Advanced Robotics and Mechatronics (ICARM)","29 Nov 2022","2022","","","1092","1097","Industrial assembly is essential for manufacturing, and robots have been broadly applied to assembly tasks. The conventional impedance control with fixed impedance parameters may restrict the compliance and interactivity of robots, posing a threat to the safety of robots and environments during assembly tasks. This paper presents an impedance learning method using reinforcement learning (RL) for robot admittance control in industrial assembly. A model-free RL method termed twin delayed deep deterministic policy gradient is utilized to tune stiffness parameters, guaranteeing the safety of robots and environments during the completion of assembly tasks. The proposed method is applied to a collaborative robot to complete the peg-in-hole task, a benchmark problem in industrial assembly. Simulation results show that the proposed method achieves a trade-off between force response and tracking accuracy, and performs better than the fixed admittance control in terms of safety and efficiency.","","978-1-6654-8306-3","10.1109/ICARM54641.2022.9959152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9959152","","Mechatronics;Service robots;Simulation;Force;Reinforcement learning;Safety;Numerical models","assembling;learning (artificial intelligence);robotic assembly","assembly tasks;collaborative robot;conventional impedance control;fixed admittance control;fixed impedance parameters;industrial assembly;model-free RL method;reinforcement learning-based impedance learning;robot admittance control","","","","25","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"NFV Closed-loop Automation Experiments using Deep Reinforcement Learning","Z. Zhou; T. Zhang; A. Kwatra","Intel Corporation, Hillsboro, Oregon, USA; Intel Corporation, Santa Clara, California, USA; Intel Corporation, Chandler, Arizona, USA","IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)","23 Sep 2019","2019","","","696","701","Closed-loop automation is a critical component in NFV based network operation. Machine learning techniques including reinforcement learning can increase network efficiency through intelligent resource management and operation. This paper details a closed-loop optimization flow that consists of feature selection, traffic forecasting and deep reinforcement learning for dynamic frequency scaling in the context of vCMTS test system. The results demonstrate power saving by running workload at lower frequency while maintaining required service level agreement.","","978-1-7281-1878-9","10.1109/INFCOMW.2019.8845222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8845222","NFV;VNF;telemetry data;machine learning;feature selection;RNN;Deep Reinforcement Learning;vCMTS","Reinforcement learning;Telemetry;Bandwidth;Packet loss;Automation","closed loop systems;computer network management;contracts;feature selection;learning (artificial intelligence);neural nets;optimisation;power aware computing;telecommunication traffic;virtualisation","closed-loop optimization flow;deep reinforcement learning;NFV closed-loop automation experiments;NFV based network operation;intelligent resource management;machine learning;feature selection;traffic forecasting;dynamic frequency scaling;vCMTS test system;power saving;Virtual Network Function;service level agreement","","4","","12","IEEE","23 Sep 2019","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Joint User Association and Resource Allocation in Factory Automation","M. Farzanullah; H. V. Vu; T. Le-Ngoc","Department of Electrical & Computer Engineering, McGill University, Montreal, QC, Canada; Huawei Technologies Canada, Ottawa, ON, Canada; Department of Electrical & Computer Engineering, McGill University, Montreal, QC, Canada","2022 IEEE Wireless Communications and Networking Conference (WCNC)","16 May 2022","2022","","","2059","2064","We consider the problem of joint user association, channel assignment, and power allocation for mobile robot application in factory automation system that require ultrareliable and low latency communications (URLLC). The aim is to deliver control commands from the controller to mobile robots with stringent requirements of latency and reliability. To achieve URLLC, we develop a two-phase communication scheme. The robots work close to each other in a factory environment and can form clusters for reliable device-to-device (D2D) communications. Within the latency requirements, the combined payload of a cluster is transmitted to the leader in Phase I. In Phase II, the leader broadcasts the payload to its members. Under this strategy, we use multi-agent reinforcement learning (MARL) for resource allocation. The cluster leaders in Phase I act as the agents and interact with the environment to optimally select the Access Point (AP) for connection along with the sub-band and power level. The objective is to maximize the successful payload delivery probability to all the robots. Illustrative simulation results indicate that the proposed scheme can offer average successful payload delivery probability close to that of centralized exhaustive search algorithm.","1558-2612","978-1-6654-4266-4","10.1109/WCNC51071.2022.9771770","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9771770","Factory Automation;Reinforcement Learning;Resource Allocation","Clustering algorithms;Reinforcement learning;Ultra reliable low latency communication;Production facilities;Device-to-device communication;Resource management;Mobile robots","channel allocation;control engineering computing;deep learning (artificial intelligence);factory automation;mobile radio;mobile robots;multi-agent systems;probability;production engineering computing;radio networks;resource allocation;telecommunication network reliability","D2D communications;MARL;access point;centralized exhaustive search algorithm;reliable device-to-device communications;deep reinforcement learning;average successful payload delivery probability;control commands;ultrareliable and low latency communications;factory automation system;mobile robot application;power allocation;channel assignment;joint user association;power level;cluster leaders;resource allocation;multiagent reinforcement learning;combined payload;latency requirements;factory environment;two-phase communication scheme;URLLC","","1","","21","IEEE","16 May 2022","","","IEEE","IEEE Conferences"
"Influence of the space segmentation and its adaptive automation for reinforcement learning","A. Notsu; Y. Komori; K. Honda; H. Ichihashi","Osaka Prefecture University, Sakai, Osaka, Japan; Osaka Prefecture University, Sakai, Osaka, Japan; Osaka Prefecture University, Sakai, Osaka, Japan; Osaka Prefecture University, Sakai, Osaka, Japan","2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)","1 Sep 2011","2011","","","1079","1083","We performed a single pendulum simulation and observed the influence of the situation space segmentation pattern in reinforcement learning processes in order to propose a new adaptive automation for situation space segmentation. Usually, in real-world reinforcement learning processes, infinite states and actions and the uncertainty of the optimum solution make the learning process more difficult than the finite Markov decision process. In a numerical experiment, a single pendulum simulation is performed in order to demonstrate the influence and adaptability of the proposed method.","1098-7584","978-1-4244-7317-5","10.1109/FUZZY.2011.6007504","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007504","reinforcement learning;space segmentation","Learning;Markov processes;Automation;Machine learning;Adaptive systems;Adaptation models;Presses","learning (artificial intelligence);pendulums","adaptive automation;reinforcement learning;single pendulum simulation;situation space segmentation pattern;infinite states","","","","6","IEEE","1 Sep 2011","","","IEEE","IEEE Conferences"
"Reinforcement Learning in Anylogic Simulation Models: A Guiding Example Using Pathmind","M. Farhan; B. Göhre; E. Junprung","Industrial, Manufacturing, & Systems Engineering, University of Texas at Arlington, Arlington, TX, USA; Pathmind. Inc, San Francisco, CA, USA; Pathmind. Inc, San Francisco, CA, USA","2020 Winter Simulation Conference (WSC)","29 Mar 2021","2020","","","3212","3223","Reinforcement Learning has recently gained a lot of exposure in the simulation industry. In this paper, we demonstrate the use of reinforcement learning in AnyLogic software models using Pathmind. A coffee shop simulation is built to train a barista to make correct operational decisions and improve efficiency that directly affects customer service time. The trained policy outperforms rule-based functions in terms of customer service time and throughput.","1558-4305","978-1-7281-9499-8","10.1109/WSC48552.2020.9383916","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383916","","Industries;Customer services;Reinforcement learning;Throughput;Software","catering industry;computer simulation;customer services;learning (artificial intelligence)","operational decisions;AnyLogic simulation models;customer service time;coffee shop simulation;AnyLogic software models;simulation industry;Pathmind;reinforcement learning","","6","","16","IEEE","29 Mar 2021","","","IEEE","IEEE Conferences"
"Smart Disaster Management and Prevention using Reinforcement Learning in IoT Environment","Y. S. Lonkar; A. S. Bhagat; S. A. S. Manjur","G. S. Moze College of Engineering, Balewadi, Pune; G. S. Moze College of Engineering, Balewadi, Pune; G. S. Moze College of Engineering, Balewadi, Pune","2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI)","11 Oct 2019","2019","","","35","38","At starting of the Internet of Things (IoT), it is passing around a world, in which diverse kinds of different objects are there connected to the Internet. It contains the use of smart phones, sensors, cameras, and other devices to make over the actions of people and things into data and link it to the Internet. With its capability to model the real world in digital form and accomplish scrutiny and replication in cyberspace, the IoT is able to reveal new value at an unparalleled rate and deliver it as response to the real world. This is set to convey main changes that will lengthen to the structure of industry in addition to the infrastructure of society itself. Therefore although the occurrence of the IoT contributes rise to new value, it besides means the occurrence of new threats. The proposed work covenant with disaster management as well as prevention to manufacturing industry using IoT. System first investigates the threat scenario during general execution of work, and finds the critical situations. The system processes learning approach for identifying such critical situations and execute the output appliances. System utilized multiple input along with output sensor for experiment. The Q-Learning approach has used for updating the policy which can provide the best result with high accuracy.","","978-1-5386-9439-8","10.1109/ICOEI.2019.8862602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8862602","Q-learning;Reinforcement Learning;IoT;ML","Security;Sensors;Industries;Monitoring;Reinforcement learning;Smart manufacturing","emergency management;Internet of Things;learning (artificial intelligence)","Q-Learning approach;manufacturing industry;Internet of Things;smart phones;IoT environment;reinforcement Learning;smart disaster management","","4","","14","IEEE","11 Oct 2019","","","IEEE","IEEE Conferences"
"Uprising E-sports Industry: machine learning/AI improve in-game performance using deep reinforcement learning","X. Du; X. Fuqian; J. Hu; Z. Wang; D. Yang","School of Jilin University, Jilin, China; University of Toronto- St. George Campus, Toronto M5S, Canada; Shanghai Jiao Tong University, Shanghai, China; Harbin Institute of Technology, Weihai, China; School of Jilin University, Jilin, China","2021 International Conference on Machine Learning and Intelligent Systems Engineering (MLISE)","25 Nov 2021","2021","","","547","552","With the quick development of machine learning, deep reinforcement learning will have a big influence on E-sports. It can be considered machine learning will help us train E-sports players easily and effectively and give coaches and players some new ideas to train and win the games. Flappy Bird is a game where the players try to keep the bird alive as long as possible and get a high score. Flappy Bird is a good example to prove that the thought is feasible. In this project, a flappy bird training AI is developed based on Q-learning and DQN. The game is played by the models obtained from deep reinforcement learning, and the game is also played by humans. Then get experimental data from these two ways and compare them. For the two ways of playing the game (by AI or manually), there are many similarities in the increased rate of scores as training sessions increase, which means AI can “teach” players how to train to get a higher score. It can be applied to skills and experience-based games and help us to train top players. Maybe it can also be applied to other fields, such as helping engineers escape from potential errors and accidents.","","978-1-6654-1736-5","10.1109/MLISE54096.2021.00112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9611712","Deep reinforcement learning;game players;e-sports","Training;Industries;Machine learning algorithms;Costs;Games;Reinforcement learning;Spot welding","computer games;deep learning (artificial intelligence);reinforcement learning;sport","flappy bird training AI;Q-learning;deep reinforcement learning;training sessions;experience-based games;e-sports industry;in-game performance;e-sports players;bird alive;machine learning;DQN","","2","","8","IEEE","25 Nov 2021","","","IEEE","IEEE Conferences"
"Fuzzy Reinforcement Learning Based Trajectory-tracking Control of an Autonomous Mobile Robot","M. Q. Zaman; H. -M. Wu","Graduate Institute of Manufacturing Technology, National Taipei University of Technology, Taipei, Taiwan; Department of Intelligent Automation Engineering, National Taipei University of Technology, Taipei, Taiwan","2022 22nd International Conference on Control, Automation and Systems (ICCAS)","9 Jan 2023","2022","","","840","845","The aim of this study is to achieve trajectory-tracking of an Autonomous Mobile Robot (AMR). The proposed Fuzzy Reinforcement Learning Control (FRLC) takes fuzzified sliding surfaces consisting of tracking errors and local velocities as inputs and generates actuator voltages as outputs. The relationship between inputs and outputs is reasoned by using the trained knowledge of a reinforcement learning agent. Simulations are conducted to investigate the performance of the proposed FRLC. It is shown that the proposed FRLC has excellent control performance for trajectory-tracking without reasoning human designed.","2642-3901","978-89-93215-24-3","10.23919/ICCAS55662.2022.10003839","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003839","Autonomous mobile robot;Trajectory-tracking;Fuzzy membership function;Reinforcement learning agent","Actuators;Automation;Simulation;Reinforcement learning;Control systems;Cognition;Automatic voltage control","control system synthesis;fuzzy control;mobile robots;reinforcement learning;trajectory control;variable structure systems","actuator voltages;autonomous mobile robot;FRLC;fuzzy reinforcement learning based trajectory-tracking control;local velocities","","","","27","","9 Jan 2023","","","IEEE","IEEE Conferences"
"Using Goal-Conditioned Reinforcement Learning With Deep Imitation to Control Robot Arm in Flexible Flat Cable Assembly Task","J. Li; H. Shi; K. -S. Hwang","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","12","Leveraging reinforcement learning on high-precision decision-making in Robot Arm assembly scenes is a desired goal in the industrial community. However, tasks like Flexible Flat Cable (FFC) assembly, which require highly trained workers, pose significant challenges due to sparse rewards and limited learning conditions. In this work, we propose a goal-conditioned self-imitation reinforcement learning method for FFC assembly without relying on a specific end-effector, where both perception and behavior plannings are learned through reinforcement learning. We analyze the challenges faced by Robot Arm in high-precision assembly scenarios and balance the breadth and depth of exploration during training. Our end-to-end model consists of hindsight and self-imitation modules, allowing the Robot Arm to leverage futile exploration and optimize successful trajectories. Our method does not require rule-based or manual rewards, and it enables the Robot Arm to quickly find feasible solutions through experience relabeling, while unnecessary explorations are avoided. We train the FFC assembly policy in a simulation environment and transfer it to the real scenario by using domain adaptation. We explore various combinations of hindsight and self-imitation learning, and discuss the results comprehensively. Experimental findings demonstrate that our model achieves fast and advanced flexible flat cable assembly, surpassing other reinforcement learning-based methods. Note to Practitioners—The motivation of this article stems from the need to develop an efficient and accurate FFC assembly policy for 3C (Computer, Communication, and Consumer Electronic) industry, promoting the development of intelligent manufacturing. Traditional control methods are incompetent to complete such a high-precision task with Robot Arm due to the difficult-to-model connectors, and existing reinforcement learning methods cannot converge with restricted epochs because of the difficult goals or trajectories. To quickly learn a high-quality assembly for Robot Arm and accelerate the convergence speed, we combine the goal-conditioned reinforcement learning and self-imitation mechanism, balancing the depth and breadth of exploration. The proposal takes visual information and six-dimensions force as state, obtaining satisfactory assembly policies. We build a simulation scene by the Pybullet platform and pre-train the Robot Arm on it, and then the pre-trained policies can be reused in real scenarios with finetuning.","1558-3783","","10.1109/TASE.2023.3323307","Major Research Project of National Natural Science Foundation of China(grant numbers:92267110); National Natural Science Foundation of China(grant numbers:61976178,62076202); Open Research Projects of Zhejiang Laboratory(grant numbers:2022NB0AB07); Shaanxi Province Key Research and Development Program of China(grant numbers:2022GY-090,2023-YBGY-354); Doctor’s Scientific Research and Innovation Foundation of Northwestern Polytechnical University(grant numbers:CX2022016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286522","Deep reinforcement learning;robot arm;intelligent assembly","Robots;Manipulators;Reinforcement learning;Task analysis;Connectors;Service robots;Production","","","","","","","IEEE","16 Oct 2023","","","IEEE","IEEE Early Access Articles"
"PRIMAL: Pathfinding via Reinforcement and Imitation Multi-Agent Learning","G. Sartoretti; J. Kerr; Y. Shi; G. Wagner; T. K. S. Kumar; S. Koenig; H. Choset","Robotics Institute at Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute at Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute at Carnegie Mellon University, Pittsburgh, PA, USA; Commonwealth Scientific and Industrial Research Organisation, Pullenvale, QLD, Australia; Computer Science Department, University of Southern California, Los Angeles, CA, USA; Computer Science Department, University of Southern California, Los Angeles, CA, USA; Robotics Institute at Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Robotics and Automation Letters","21 Mar 2019","2019","4","3","2378","2385","Multi-agent path finding (MAPF) is an essential component of many large-scale, real-world robot deployments, from aerial swarms to warehouse automation. However, despite the community's continued efforts, most state-of-the-art MAPF planners still rely on centralized planning and scale poorly past a few hundred agents. Such planning approaches are maladapted to realworld deployments, where noise and uncertainty often require paths be recomputed online, which is impossible when planning times are in seconds to minutes. We present PRIMAL, a novel framework for MAPF that combines reinforcement and imitation learning to teach fully decentralized policies, where agents reactively plan paths online in a partially observable world while exhibiting implicit coordination. This framework extends our previous work on distributed learning of collaborative policies by introducing demonstrations of an expert MAPF planner during training, as well as careful reward shaping and environment sampling. Once learned, the resulting policy can be copied onto any number of agents and naturally scales to different team sizes and world dimensions. We present results on randomized worlds with up to 1024 agents and compare success rates against state-of-the-art MAPF planners. Finally, we experimentally validate the learned policies in a hybrid simulation of a factory mockup, involving both real world and simulated robots.","2377-3766","","10.1109/LRA.2019.2903261","National Science Foundation(grant numbers:ACI-1445606); CMU Manufacturing Futures Initiative; Richard King Mellon Foundation; National Science Foundation(grant numbers:1409987,1724392,1817189,1837779,ACI-1445606); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661608","Path planning for multiple mobile robots or agents;deep learning in robotics and automation;distributed robot systems;AI-based methods;factory automation","Planning;Robot kinematics;Training;Robot sensing systems;Production facilities;Multi-agent systems","learning (artificial intelligence);multi-agent systems;multi-robot systems;path planning;warehouse automation","PRIMAL;multiagent path finding;essential component;real-world robot deployments;aerial swarms;warehouse automation;community;centralized planning;planning approaches;noise;uncertainty;planning times;imitation learning;fully decentralized policies;partially observable world;distributed learning;collaborative policies;expert MAPF planner;environment sampling;world dimensions;randomized worlds;learned policies;simulated robots;team sizes;reward shaping;reinforcement learning","","128","","43","IEEE","6 Mar 2019","","","IEEE","IEEE Journals"
"Reinforcement neural learning with application to gas sensors","H. S. Abdel-Aty-Zohdy","Microelectronics System Design Laboratory, Department of Electrical and Systems Engineering, Oakland University, Rochester, MI, USA","Proceedings of 40th Midwest Symposium on Circuits and Systems. Dedicated to the Memory of Professor Mac Van Valkenburg","6 Aug 2002","1997","2","","1269","1273 vol.2","Neural net reinforcement learning algorithm and its application to gas (chemicals) classification and quantification are considered in this paper. The approach does not require memory and has simplified reward/punishment computations. The advantages and disadvantages of the reinforcement algorithm are contrasted against other competing neural algorithms. The reinforcement learning has been found practical, when each gas (chemical) is distinguished by several features and characteristics with possible overlap. The paper also gives an account of integrated circuit digital chip implementation for typical four gases with temperature, pressure, and flow quantity features. The number of required iterations has been found to depend on reward and penalty parameters, as well as the threshold governing the learning. It is believed that our implementation approach has potential uses in auto industry safety and emission control, as well in automated semiconductor manufacturing process.","","0-7803-3694-1","10.1109/MWSCAS.1997.662312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=662312","","Gas detectors;Learning;Chemicals;Neural networks;Digital integrated circuits;Gases;Temperature;Industrial control;Manufacturing industries;Safety","learning (artificial intelligence);neural chips;digital integrated circuits;gas sensors;intelligent sensors;computerised instrumentation","reinforcement learning algorithm;neural learning;gas sensors;reward/punishment computations;digital chip implementation;temperature features;pressure features;flow quantity features;emission control;auto industry safety;automated semiconductor manufacturing process","","8","","18","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Adaptive Fault-Tolerant Tracking Control for MIMO Discrete-Time Systems via Reinforcement Learning Algorithm With Less Learning Parameters","L. Liu; Z. Wang; H. Zhang","State Key Laboratory of Synthetical Automation for Process Industries, Shenyang, Liaoning, China; State Key Laboratory of Synthetical Automation for Process Industries, Shenyang, Liaoning, China; State Key Laboratory of Synthetical Automation for Process Industries, Shenyang, Liaoning, China","IEEE Transactions on Automation Science and Engineering","4 Jan 2017","2017","14","1","299","313","This paper is concerned with a reinforcement learning-based adaptive tracking control technique to tolerate faults for a class of unknown multiple-input multiple-output nonlinear discrete-time systems with less learning parameters. Not only abrupt faults are considered, but also incipient faults are taken into account. Based on the approximation ability of neural networks, action network and critic network are proposed to approximate the optimal signal and to generate the novel cost function, respectively. The remarkable feature of the proposed method is that it can reduce the cost in the procedure of tolerating fault and can decrease the number of learning parameters and thus reduce the computational burden. Stability analysis is given to ensure the uniform boundedness of adaptive control signals and tracking errors. Finally, three simulations are used to show the effectiveness of the present strategy.","1558-3783","","10.1109/TASE.2016.2517155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7389437","Adaptive critic design;fault tolerant control;multiple-input multiple-output discrete-time systems;neural networks;reinforcement learning algorithm","MIMO;Adaptive systems;Discrete-time systems;Cost function;Control systems;Neural networks;Fault tolerance","adaptive control;approximation theory;discrete time systems;fault tolerant control;learning systems;MIMO systems;neurocontrollers;nonlinear control systems;stability","adaptive fault-tolerant tracking control;MIMO discrete-time systems;reinforcement learning-based adaptive tracking control technique;unknown multiple-input multiple-output nonlinear discrete-time systems;abrupt faults;incipient faults;neural network approximation ability;action network;critic network;cost function;stability analysis","","176","","58","IEEE","21 Jan 2016","","","IEEE","IEEE Journals"
"The SLS-Generated Soft Robotic Hand - An Integrated Approach Using Additive Manufacturing and Reinforcement Learning","A. Rost; S. Schädle","Corporate Research, Festo AG & Co. KG, Esslingen, Germany; Institute for Artificial Intelligence, University of Applied Sciences Ravensburg-Weingarten, Weingarten, Germany","2013 12th International Conference on Machine Learning and Applications","10 Apr 2014","2013","1","","215","220","To develop a robotic system for a complex task is a time-consuming process. Merging methods available today, a new approach for a faster realization of a multi-finger soft robotic hand is presented here. This paper introduces a robotic hand with four fingers and 12 Degrees of Freedom (DoFs) using bellow actuators. The hand is generated via Selective Laser Sintering (SLS), an Additive Manufacturing method. The complex task execution of a specific action, i.e. the lifting, rotating and precise positioning of a handling-object with this robotic hand, is used to structure the whole development process. To validate reliable functionality of the hand from the beginning, each development stage is SLS-generated and the targeted task execution is trained via Reinforcement Learning, a machine learning approach. Optimization points are subsequently derived and fed back into the hardware development. With this Concurrent Engineering strategy a fast development of this robotic hand is possible. The paper outlines the relevant key strategies and gives insight into the design process. At the end, the final hand with its capabilities is presented and discussed.","","978-0-7695-5144-9","10.1109/ICMLA.2013.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784614","Soft Robotic Hand;Reinforcement Learning;Additive Manufacturing;SLS;Concurrent Engineering;Compliant;LearningGripper","Thumb;Robots;Hardware;Actuators;Bellows","concurrent engineering;control engineering computing;dexterous manipulators;laser sintering;learning (artificial intelligence);optimisation;rapid prototyping (industrial);service robots","SLS-generated soft robotic hand;additive manufacturing;reinforcement learning;multifinger soft robotic hand;bellow actuator;selective laser sintering;machine learning;optimization point;concurrent engineering","","15","5","11","IEEE","10 Apr 2014","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Production Scheduling in Industrial Internet of Things","Z. Luo; C. Jiang; L. Liu; X. Zheng; H. Ma; F. Dong; F. Li","Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; Nanjing Nangang Iron and Steel United Co, LTD, Nanjing, China","IEEE Internet of Things Journal","","2023","PP","99","1","1","The unprecedented prosperity of the Industrial Internet of Things (IIoT) promotes the traditional industry transforming into intelligent manufacturing so that the whole production process can be comprehensively controlled to achieve flexible production. Intelligent scheduling, as one of the key enabling techniques, is desired to allocate the production of several machines by an efficient solution with minimum makespan. Existing approaches adopt a fixed search paradigm based on expert knowledge to seek satisfactory solutions. However, considering the varying data distribution and large-sized of the practical problems, these methods fail to guarantee the quality of the obtained solution under the real-time requirement. To address this challenge, we formulate the production scheduling problem as a Markov decision process (MDP) and specifically design a job scheduling model made up of a job batching module for the hybrid flow-shop scheduling problem on batch processing machines (HFSP-BPM). Our proposed model consists of an actor network that learns the action under different conditions and a critic network that evaluates the action of the actor. We analyze the convergence of the model under different parameter settings to determine the optimal parameter. Extensive numerical experiments on both publicly available dataset and real steel plant production dataset demonstrate that the proposed deep reinforcement learning (DRL) approach compared with other baselines, more than 6% average improvements can be observed in many instances.","2327-4662","","10.1109/JIOT.2023.3283056","A3 Foresight Program of NSFC(grant numbers:No. 62061146002); Funds for Creative Research Groups of China(grant numbers:No. 61921003); National Natural Science Foundation of China(grant numbers:No. 62225204); National Key Research and Development Program of China(grant numbers:No. 2021YFB2900100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10144304","Deep reinforcement learning (DRL);flexible production;Industrial Internet of Things (IIoT);intelligent manufacturing;hybrid flow-shop scheduling problem on batch processing machines (HFSP-BPM)","Production;Job shop scheduling;Industrial Internet of Things;Steel;Batch production systems;Iron;Approximation algorithms","","","","","","","IEEE","5 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Deep Reinforcement Learning for Queue-Time Management in Semiconductor Manufacturing","H. Yedidsion; P. Dawadi; D. Norman; E. Zarifoglu","AI/ML Team Applied Materials Inc, Santa Clara, CA, USA; AI/ML Team Applied Materials Inc, Santa Clara, CA, USA; AI/ML Team Applied Materials Inc, Santa Clara, CA, USA; AI/ML Team Applied Materials Inc, Santa Clara, CA, USA","2022 Winter Simulation Conference (WSC)","23 Jan 2023","2022","","","3275","3284","Queue-time constraints (QTC) define a limit on the time that a lot can wait between two process steps in its flow. In semiconductor manufacturing, lots that exceed that time limit experience yield loss, need rework, or get scraped. QTCs are difficult to schedule, since a lot needs to wait to be released to the first process step until there is available capacity to process the final step. However, exactly calculating if there is enough capacity is computationally expensive. In this work we propose a deep Reinforcement Learning (RL) method to manage releasing lots into the queue time constraint. We analyze the performance of our RL method and compare it to seven baseline solutions. Our empirical evaluation shows that the RL method outperforms the baselines in five performance metrics including the number of queue-time violations and makespan, while requiring negligible online compute time.","1558-4305","978-1-6654-7661-4","10.1109/WSC57314.2022.10015463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015463","","Deep learning;Measurement;Schedules;Computational modeling;Reinforcement learning;Semiconductor device manufacture;Benchmark testing","deep learning (artificial intelligence);production engineering computing;queueing theory;reinforcement learning;scheduling;semiconductor device manufacture;semiconductor industry","deep reinforcement learning;performance metrics;QTC;queue time constraint;queue-time makespan;queue-time management;queue-time violations;RL method;semiconductor manufacturing","","","","26","IEEE","23 Jan 2023","","","IEEE","IEEE Conferences"
"Predictive Maintenance Model for IIoT-Based Manufacturing: A Transferable Deep Reinforcement Learning Approach","K. S. H. Ong; W. Wang; N. Q. Hieu; D. Niyato; T. Friedrichs","Ong and Dusit Niyato are with the School of Computer Science and Engineering, Nanyang Technological University, Singapore; Faculty of Engineering, Bar Ilan University, Ramat Gan, Israel; School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; Ong and Dusit Niyato are with the School of Computer Science and Engineering, Nanyang Technological University, Singapore; IT Strategy and Innovation Asia–Pacific, Robert Bosch (SEA) Pte. Ltd., Singapore","IEEE Internet of Things Journal","24 Aug 2022","2022","9","17","15725","15741","The Industrial Internet of Things (IIoT) is crucial for accurately assessing the state of complex equipment in order to perform predictive maintenance (PdM) successfully. However, existing IIoT-based PdM frameworks do not consider the influence of various practical yet complex system factors, such as the real-time production states, machine health, and maintenance manpower resources. For this reason, we propose a generic PdM optimization framework to assist maintenance teams in prioritizing and resolving maintenance task conflicts under real-world manufacturing conditions. Specifically, the PdM framework aims to jointly optimize the edge-based machine network uptime and the allocation of manpower resources in a stochastic IIoT-enabled manufacturing environment using the model-free deep reinforcement learning (DRL) methods. Since DRL requires a significant amount of training data, we propose and demonstrate the use of the transfer learning (TL) method to assist DRL in learning more efficiently by incorporating expert demonstrations, termed TL with demonstrations (TLDs). TLD reduces training wall time by 58% compared to baseline methods, and we conduct numerous experiments to illustrate the performance, robustness, and scalability of TLD. Finally, we discuss the general benefits and limitations of the proposed TL method, which are not well addressed in the existing literature but could be beneficial to both researchers and industry practitioners.","2327-4662","","10.1109/JIOT.2022.3151862","Computer Networks and Communications Lab, Nanyang Technological University of Singapore and Robert Bosch (SEA) Pte. Ltd; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714509","Decision support;deep reinforcement learning (DRL);Industrial Internet of Things (IIoT);predictive maintenance (PdM);resource management;transfer learning (TL)","Maintenance engineering;Resource management;Production;Manufacturing;Transfer learning;Task analysis;Industrial Internet of Things","condition monitoring;deep learning (artificial intelligence);Internet of Things;maintenance engineering;production engineering computing","model-free deep reinforcement learning methods;DRL;expert demonstrations;predictive maintenance model;transferable deep reinforcement learning approach;complex equipment;IIoT-based PdM frameworks;real-time production states;machine health;maintenance manpower resources;generic PdM optimization framework;edge-based machine network uptime;Industrial Internet of Things;maintenance task conflicts;stochastic IIoT-enabled manufacturing environment;TL with demonstration;TLD","","4","","42","IEEE","16 Feb 2022","","","IEEE","IEEE Journals"
"Contextual reinforcement learning","J. Langford","Microsoft Research New York, USA","2017 IEEE International Conference on Big Data (Big Data)","15 Jan 2018","2017","","","3","3","I will discuss a decade long research project to create the foundations of reinforcement learning with context (aka features). This research project has multiple threads including Contextual Bandits, Learning to Search, and Contextual Decision Processes. The most mature of these (Contextual Bandits) is now driving many real-world RL applications while the least mature (CDPs) is a fascinating theoretician's toy.","","978-1-5386-2715-0","10.1109/BigData.2017.8257902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257902","","Reinforcement learning;Conferences;Computer science;Toy manufacturing industry;Physics;Context;Biographies","","","","","","","IEEE","15 Jan 2018","","","IEEE","IEEE Conferences"
"Control of Shared Production Buffers: A Reinforcement Learning Approach","N. Krippendorff; C. Schwindt","Institute of Management and Economics, Clausthal University for Technology, Clausthal-Zellerfeld, Germany; Institute of Management and Economics, Clausthal University for Technology, Clausthal-Zellerfeld, Germany","2021 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)","19 Jan 2022","2021","","","703","707","We consider a buffer control problem arising in stochastic flow lines with shared production buffers. Buffer control relies on decision rules which determine transfers of items between buffers and machines at the release or completion times of parts on the different production stages. We devise a conceptual model of the problem for a basic scenario with one central buffer and explain how general system configurations and a tactical buffer allocation problem can be modeled within this framework. Assuming that the flow line can be represented as a Markovian production system, we provide a formulation as a continuous-time Markov decision problem admitting an optimal stationary policy. By applying a uniformization approach from literature, the Markov decision problem is discretized in time and thus amenable to standard algorithms. We propose a simple Q-learning implementation of reinforcement learning converging to an optimal stationary policy and validate the approach in a numerical experiment with a small toy problem.","","978-1-6654-3771-4","10.1109/IEEM50564.2021.9673034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9673034","Buffer allocation;buffer control;Markov decision process;Q-learning;unpaced asynchronous flow line","Production systems;Q-learning;Engineering management;Toy manufacturing industry;Process control;Markov processes;Industrial engineering","learning (artificial intelligence);Markov processes;stochastic processes","shared production buffers;reinforcement learning approach;buffer control problem;stochastic flow lines;decision rules;different production stages;central buffer;general system configurations;tactical buffer allocation problem;flow line;Markovian production system;continuous-time Markov decision problem;optimal stationary policy;toy problem","","","","13","IEEE","19 Jan 2022","","","IEEE","IEEE Conferences"
"Enhancing Digital Twins through Reinforcement Learning","C. Cronrath; A. R. Aderiani; B. Lennartson","Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Department of Industrial and Material Science, Chalmers University of Technology, Gothenburg, Sweden; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden","2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)","19 Sep 2019","2019","","","293","298","Digital Twins are core enablers of smart and autonomous manufacturing systems. Although they strive to represent their physical counterpart as accurately as possible, slight model or data errors will remain. We present an algorithm to compensate for those residual errors through Reinforcement Learning (RL) and data fed back from the manufacturing system. When learning, the Digital Twin acts as teacher and safety policy to ensure minimal performance. We test the algorithm in a sheet metal assembly context, in which locators of the fixture are optimally adjusted for individual assemblies. Our results show a fast adaption and improved performance of the autonomous system.","2161-8089","978-1-7281-0356-3","10.1109/COASE.2019.8842888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8842888","","Reinforcement learning;Data models;Manufacturing;Optimization;Deep learning;Neural networks","assembling;fixtures;learning (artificial intelligence);manufacturing systems;production engineering computing;sheet metal processing","slight model;residual errors;manufacturing system;autonomous system;core enablers;smart manufacturing systems;autonomous manufacturing systems;physical counterpart;digital twin;reinforcement learning;safety policy","","26","","24","IEEE","19 Sep 2019","","","IEEE","IEEE Conferences"
"Investigation of Energy Management and Optimization Using Penalty Based Reinforcement Learning Algorithms for Textile Industry","P. P. R.; R. Menon","School of Computer Science and Engineering(SCOPE), VIT University, Vellore, India; School of Computer Science and Engineering(SCOPE), VIT University, Vellore, India","2020 International Conference on Innovative Trends in Information Technology (ICITIIT)","20 Apr 2020","2020","","","1","8","Energy, today, has become one of the most important factors in almost every industry domain. The energy reserves of a country largely influences its growth rate. Particularly in case of the textile industry, this contributes 2% to the world GDP. This paper aims at demonstrating through experimentation, how reinforcement learning, a branch of machine learning, can be adopted to help manage and optimize energy usage in a cotton textile mill. The models explored in this paper are established as nonlinear programming mode with the experiment result showing the performance of the penalty based reinforcement-learning algorithms in comparison with results obtained in [1] per annum. It has been calculated that there is a reduction of approximately 1.3% using Thompson Sampling and UCB algorithms, and 1.26% in energy consumption using random selection.","","978-1-7281-4210-4","10.1109/ICITIIT49094.2020.9071554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9071554","industry 4.0;spinning process;optimization;markov chain;cyber physical systems","Textiles;Energy consumption;Petri nets;Spinning;Textile industry;Optimization;Production","energy consumption;energy management systems;learning (artificial intelligence);nonlinear programming;textile industry","energy consumption;energy management;textile industry;machine learning;cotton textile mill;penalty based reinforcement learning;nonlinear programming;Thompson sampling algorithm;UCB algorithm","","","","35","IEEE","20 Apr 2020","","","IEEE","IEEE Conferences"
"Strategies for Developing a Supervisory Controller with Deep Reinforcement Learning in a Production Context","J. Harb; S. Riedmann; S. Wegenkittl","Salzburg University of Applied Sciences GmbH, Puch bei Hallein, Austria; Salzburg University of Applied Sciences GmbH, Puch bei Hallein, Austria; Salzburg University of Applied Sciences GmbH, Puch bei Hallein, Austria","2022 IEEE Conference on Control Technology and Applications (CCTA)","8 Dec 2022","2022","","","869","874","Deep reinforcement learning (RL) algorithms are a promising optimisation tool in changing industrial production systems. We implement a supervisory controller using deep Q-learning and a Petri net simulation model. Furthermore, we identify challenges for using RL in a production context and propose three generally applicable strategies for using deep RL with production systems. Firstly, reward shaping may be used to deal with multiple goals and constraints, by allowing the RL agent to slowly adapt to the constraints. Secondly, an existing RL agent can be adapted to a different task using transfer learning and thus reducing training times. Lastly, including varying starting conditions increases the number of states the RL agent encounters during training. This increases the generalisation capabilities of the deep-Rlagent and allows the agent to react to unseen states more robustly. We present a setup for solving a sorting task using deep Q-learning and conduct several experiments to evaluate the proposed strategies.","2768-0770","978-1-6654-7338-5","10.1109/CCTA49430.2022.9966086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9966086","","Training;Deep learning;Production systems;Adaptation models;Q-learning;Transfer learning;Petri nets","deep learning (artificial intelligence);manufacturing systems;Petri nets;production engineering computing;reinforcement learning","deep Q-learning;deep reinforcement learning algorithms;deep RL agent;industrial production systems;Petri net simulation model;production context;supervisory controller;transfer learning","","1","","16","IEEE","8 Dec 2022","","","IEEE","IEEE Conferences"
"Complexity analysis of reinforcement learning and its application to robotics","B. Li; L. Xia; Q. Zhao","Department of Automation, TNList Tsinghua University, Beijing, China; Department of Automation, TNList Tsinghua University, Beijing, China; Department of Automation, TNList Tsinghua University, Beijing, China","2017 13th IEEE Conference on Automation Science and Engineering (CASE)","15 Jan 2018","2017","","","1425","1426","Reinforcement learning (RL) is a widely adopted theory in machine learning, which aims to handle the optimal decision of intelligent agent interacting with the stochastic dynamic environment. Its origin may come from the motivation of phycological observations since 1960's [1]. It blooms recently as the emerging of large sample data and powerful computation facility, especially the AlphaGo's beat over the human top Go player in 2016 [2].","2161-8089","978-1-5090-6781-7","10.1109/COASE.2017.8256303","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8256303","Reinforcement learning;algorithmic complexity;robotics;learning control","Complexity theory;Robots;Learning (artificial intelligence);Heuristic algorithms;Algorithm design and analysis;Training data;Automation","computational complexity;learning (artificial intelligence);optimal control;robots;software agents;stochastic systems","machine learning;optimal decision;stochastic dynamic environment;complexity analysis;reinforcement learning;robotics;intelligent agent interaction;phycological observations;large sample data;AlphaGo","","2","","4","IEEE","15 Jan 2018","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Method for Mobile Robot Path Planning in Unknown Environments","W. Zhang; W. Wang; H. Zhai; Q. Li","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","5898","5902","In this paper, a path planning method without global maps for mobile robots is proposed. At present, the traditional path planning methods have become mature. However, most of these methods are based on map information, and complex models need to be defined. To address these issues, we propose an end-to-end path planning model based on deep reinforcement learning, which takes the lidar sequence and depth image as the inputs, and the robot's velocity and angular velocity as the outputs. With the proposed method, the mobile robot can plan the motion path from the initial position to the target position without colliding with the obstacles in the environment. Besides, we propose a neural network that has the ability of multi-sensor information fusion from different dimensions. We build a simulation environment for mobile robot path planning in the gazebo based on Robot Operating System(ROS). Simulation results show the effectiveness of the proposed method.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727670","deep reinforcement learning;mobile robot;path planning;collision-free","Laser radar;Automation;Simulation;Neural networks;Reinforcement learning;Robot sensing systems;Path planning","collision avoidance;control engineering computing;learning (artificial intelligence);mobile robots;path planning;sensor fusion","mobile robot path planning;path planning method;traditional path planning methods;end-to-end path;deep reinforcement learning;motion path","","1","","12","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Adaptive Arbitration for Minimal Intervention Shared Control via Deep Reinforcement Learning","S. You; Y. Kang; Y. -B. Zhao; Q. Zhang","The Department of Automation, University of Science and Technology of China, Hefei, China; The Department of Automation, University of Science and Technology of China, Hefei, China; The Department of Automation, University of Science and Technology of China, Hefei, China; The Department of Automation, University of Science and Technology of China, Hefei, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","743","748","In shared control, humans and intelligent robots jointly complete real-time control tasks with their complementary capabilities for improved performance unavailable by neither side on its own, which is attracting more and more attentions in recent years. Arbitration, as an indispensable part of shared control, determines how control authority is allocated between the human and robot, and the definition of that policy has always been one of the fundamental problems. In this paper, we propose an adaptive arbitration method for shared control systems, which minimizes the deviation from the human inputs while ensuring the system performance based on deep reinforcement learning. We provide humans the maximum assistance with the minimal intervention, in order to balance human’s need for control authority and need for performance. We apply our method to real-time control tasks, and the results show that our method achieves high task success rate and shorter task completion time with less human workload, while maintaining higher human satisfaction.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727522","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727522","Arbitration;Shared Control;Minimal Intervention;Deep Reinforcement Learning","Adaptive systems;Automation;System performance;Reinforcement learning;Control systems;Real-time systems;Task analysis","adaptive control;deep learning (artificial intelligence);human-robot interaction;intelligent robots;reinforcement learning;resource allocation","deep reinforcement learning;control authority allocation;real-time control tasks;task completion time;minimal intervention shared control;intelligent robots;adaptive arbitration method","","","","20","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Self-Attention based Temporal Intrinsic Reward for Reinforcement Learning","Z. Jiang; D. Tian; Q. Yang; Z. Peng","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","2022","2026","This paper proposes a self-attention based temporal intrinsic reward model for reinforcement learning (RL), to synthesize the control policy for the agent constrained by the sparse reward in partially observable environments. This approach can solve the problem of temporal credit assignment to some extent and deal with the low efficiency of exploration. We first introduce a sequence-based self-attention mechanism to generate the temporary features, which can effectively capture the temporal property of the task for the agent. During the training process, the temporary features are employed in each sampled episode to elaborate the intrinsic rewards, which is combined with the extrinsic reward to help the agent learn a feasible policy. Then we use the meta-gradient to update this intrinsic reward model in order that the agent can achieve better performance. Experiments are given to demonstrate the superiority of the proposed method.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727314","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727314","reinforcement learning;sparse reward;self-attention;intrinsic motivation","Training;Automation;Reinforcement learning;Task analysis","gradient methods;reinforcement learning;software agents","control policy;extrinsic reward;meta-gradient;partially observable environments;reinforcement learning;RL;self-attention based temporal intrinsic reward model;sequence-based self-attention mechanism;sparse reward;temporal credit assignment problem;temporary features","","","","24","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Optimal Output Regulation of Linear Discrete-Time Systems With Unknown Dynamics Using Reinforcement Learning","Y. Jiang; B. Kiumarsi; J. Fan; T. Chai; J. Li; F. L. Lewis","State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; Department of Electrical and Computer Engineering, Michigan State University, East Lansing, USA; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; School of Information and Control Engineering, Liaoning Shihua University, Fushun, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China","IEEE Transactions on Cybernetics","17 Jun 2020","2020","50","7","3147","3156","This paper presents a model-free optimal approach based on reinforcement learning for solving the output regulation problem for discrete-time systems under disturbances. This problem is first broken down into two optimization problems: 1) a constrained static optimization problem is established to find the solution to the output regulator equations (i.e., the feedforward control input) and 2) a dynamic optimization problem is established to find the optimal feedback control input. Solving these optimization problems requires the knowledge of the system dynamics. To obviate this requirement, a model-free off-policy algorithm is presented to find the solution to the dynamic optimization problem using only measured data. Then, based on the solution to the dynamic optimization problem, a model-free approach is provided for the static optimization problem. It is shown that the proposed algorithm is insensitive to the probing noise added to the control input for satisfying the persistence of excitation condition. Simulation results are provided to verify the effectiveness of the proposed approach.","2168-2275","","10.1109/TCYB.2018.2890046","National Natural Science Foundation of China(grant numbers:61333012,61533015,61304028,61673280); Higher Education Discipline Innovation Project(grant numbers:B08015); Northeastern University Doctoral Research and Innovation Project; Fundamental Research Funds for the Central Universities(grant numbers:N160804001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8626769","Discrete-time (DT) systems;model-free;optimal output regulation;reinforcement learning (RL)","Optimization;Heuristic algorithms;Mathematical model;System dynamics;Optimal control;Automation;Reinforcement learning","discrete time systems;feedback;learning (artificial intelligence);linear systems;optimal control;optimisation","system dynamics;dynamic optimization problem;optimal output regulation;linear discrete-time systems;reinforcement learning;model-free optimal approach;output regulation problem;constrained static optimization problem;output regulator equations;optimal feedback control input;model-free off-policy algorithm","","58","","40","IEEE","25 Jan 2019","","","IEEE","IEEE Journals"
"USV Path Planning Under Marine Environment Simulation Using DWA and Safe Reinforcement Learning","T. Qu; G. Xiong; H. Ali; X. Dong; Y. Han; Z. Shen","The State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Beijing Engineering Research Center of Intelligent Systems and Technology, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","6","It is a challenge for USV navigation due to uncertainties and disturbances in the complex marine environment, which may lead to tilting or collision. However, current path planning methods for USVs lack dynamic environment adaptivity and timeliness. We formulate this problem as a Markov Decision Process (MDP) and combine the advantages of Dynamic Window Approach (DWA) and Safe Reinforcement Learning (RL) to solve it. First, the state encoding output by DWA is employed as observation to reduce the state dimension. Second, a safety-ensured reward function is designed to avoid collision areas. Then, the actor-critic network based on RL algorithm is used. Along with the RL training, the parameters in DWA is adaptively adjusted. Simulation experiments with dynamic waves and ocean currents demonstrate the adaptivity of our method, with avg. trajectory length decreased by 10%, avg. time cost decreased by 17%, and avg. speed increased by 18%, compared to the case using only DWA.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260584","National Natural Science Foundation of China(grant numbers:U19B2029,U1909204); Chinese Academy of Sciences Key Technology Talent Program (Zhen Shen); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260584","","Training;Uncertainty;Costs;Navigation;Oceans;Heuristic algorithms;Reinforcement learning","collision avoidance;control engineering computing;learning (artificial intelligence);Markov processes;mobile robots;path planning;reinforcement learning;unmanned surface vehicles","collision areas;complex marine environment;current path;DWA;dynamic waves;Dynamic Window Approach;marine environment simulation;Markov Decision Process;ocean currents;RL algorithm;RL training;Safe Reinforcement Learning;safety-ensured reward function;simulation experiments;state dimension;state encoding output;tilting collision;USV navigation;USV path planning;USVs lack dynamic environment adaptivity","","","","25","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Offline Reinforcement Learning via Sequence Modeling for Vision-Based Robotic Grasping","S. Zeng; X. Sun; K. Chen; W. Huang; Y. Chen; D. Chen; Z. Xu","Education Department of Fujian Province, Key Laboratory of Industrial Automation Control Technology and Information Processing, College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Evomotion Co., Ltd., Shenzhen, Guangdong, China; Education Department of Fujian Province, Key Laboratory of Industrial Automation Control Technology and Information Processing, College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Evomotion Co., Ltd., Shenzhen, Guangdong, China; Education Department of Fujian Province, Key Laboratory of Industrial Automation Control Technology and Information Processing, College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Education Department of Fujian Province, Key Laboratory of Industrial Automation Control Technology and Information Processing, College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Education Department of Fujian Province, Key Laboratory of Industrial Automation Control Technology and Information Processing, College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China","2023 IEEE 13th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","28 Sep 2023","2023","","","159","163","High cost of environmental interaction and low data efficiency limit the development of reinforcement learning in robotic grasping. This paper proposes an end-to-end robotic grasping method based on offline reinforcement learning via sequence modeling. It considers the most recent n-step history to assist the agent in making decisions, where a predictive model learns to directly predict actions from raw image inputs. The experimental results show that our method can achieve higher grasping success rate with less training data than traditional reinforcement learning algorithms in offline setting.","2642-6633","979-8-3503-1519-6","10.1109/CYBER59472.2023.10256592","NSF of China(grant numbers:61973085); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256592","","Training;Visualization;Training data;Grasping;Reinforcement learning;Predictive models;Prediction algorithms","deep learning (artificial intelligence);grippers;image sequences;manipulators;motion control;reinforcement learning;robot vision","action prediction;data efficiency;decision making;end-to-end robotic grasping method;environmental interaction;grasping success rate;image input;offline reinforcement learning;predictive model;sequence modeling;Transformer architecture;vision-based robotic grasping","","","","21","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Health-aware hierarchical control for smart manufacturing using reinforcement learning","B. Y. Choo; S. Adams; P. Beling","Department of Systems and Information Engineering, University of Virginia, Charlottesville, Virginia; Department of Systems and Information Engineering, University of Virginia, Charlottesville, Virginia; Department of Systems and Information Engineering, University of Virginia, Charlottesville, Virginia","2017 IEEE International Conference on Prognostics and Health Management (ICPHM)","3 Aug 2017","2017","","","40","47","Manufacturing facilities are laid out in a natural hierarchy of assembly lines, work cells, machines, and components. Currently, prognostics and health management (PHM) information is confined to the lowest levels of this hierarchy and finds primary use in decisions and control policies for machine maintenance and replacement. For the smart manufacturing systems of the future, however, PHM information should be passed to all levels of the hierarchy and incorporated into high level decision making about production quantities, rates, and locations. This paper proposes a hierarchical control methodology that passes PHM health estimates up the hierarchy and optimization objectives down the hierarchy. Individual nodes in the hierarchy are modeled as Markov decision processes (MDPs) and reinforcement learning is used to estimate optimal policies. This work makes several contributions to the PHM community. First, we propose a novel model of a control system that makes uses of health information throughout the manufacturing hierarchy. Second, we define a reinforcement learning based approach to solving the MDPs for optimal or near-optimal policies. Third, we illustrate the method on a numerical example based on a simulation of a real-world manufacturing environment.","","978-1-5090-5710-8","10.1109/ICPHM.2017.7998303","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7998303","","Prognostics and health management;Maintenance engineering;Learning (artificial intelligence);Numerical models;Manufacturing systems","fault diagnosis;intelligent manufacturing systems;learning (artificial intelligence);maintenance engineering;Markov processes;production engineering computing","health-aware hierarchical control;prognostics and health management;smart manufacturing systems;PHM information;decision making;Markov decision processes;reinforcement learning;MDP","","3","","50","IEEE","3 Aug 2017","","","IEEE","IEEE Conferences"
"Distributed Real-Time Scheduling in Cloud Manufacturing by Deep Reinforcement Learning","L. Zhang; C. Yang; Y. Yan; Y. Hu","Laboratory of Industrial and Intelligent Systems, School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; Laboratory of Industrial and Intelligent Systems, School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; Laboratory of Industrial and Intelligent Systems, School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Industrial Informatics","30 Sep 2022","2022","18","12","8999","9007","With the extensive application of automated guided vehicles, real-time production scheduling considering logistics services in cloud manufacturing (CM) becomes an urgent problem. Thus, this study focuses on the distributed real-time scheduling (DRTS) of multiple services to respond to dynamic and customized orders. First, a DRTS framework with cloud–edge collaboration is proposed to improve performance and satisfy responsiveness, where distributed actors and one centralized learner are deployed in the edge and cloud layer, respectively. And, the DRTS problem is modeled as a semi-Markov decision process, where the processing services sequencing and logistics services assignment are considered simultaneously. Then, we developed a distributed dueling deep Q network (D3QN) with cloud–edge collaboration to optimize the weighted tardiness of jobs. The experimental results show that the proposed D3QN obtains lower weighted tardiness and shorter flow-time than other state-of-the-art algorithms. It indicates the proposed DRTS method has significant potential to provide efficient real-time decision-making in CM.","1941-0050","","10.1109/TII.2022.3178410","National Key R&D Program of China(grant numbers:2021YFB1715700); National Natural Science Foundation of China(grant numbers:52175451); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9783015","Cloud–edge collaboration;cloud manufacturing;deep reinforcement learning;distributed;real-time scheduling","Job shop scheduling;Real-time systems;Cloud computing;Logistics;Dynamic scheduling;Processor scheduling;Decision making","automatic guided vehicles;cloud computing;decision making;learning (artificial intelligence);logistics;Markov processes;production control;production engineering computing;scheduling","distributed real-time scheduling;cloud manufacturing;deep reinforcement learning;extensive application;automated guided vehicles;real-time production scheduling;logistics services;dynamic orders;customized orders;DRTS framework;cloud-edge collaboration;distributed actors;cloud layer;DRTS problem;semiMarkov decision process;processing services sequencing;distributed dueling deep Q network;D3QN obtains lower weighted tardiness;shorter flow-time;DRTS method;real-time decision-making","","16","","30","IEEE","27 May 2022","","","IEEE","IEEE Journals"
"Digital-Twin-Assisted Resource Allocation for Network Slicing in Industry 4.0 and Beyond Using Distributed Deep Reinforcement Learning","L. Tang; Y. Du; Q. Liu; J. Li; S. Li; Q. Chen","School of Communication and Information Engineering and the Chongqing Key Laboratory of Mobile Communication, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering and the Chongqing Key Laboratory of Mobile Communication, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering and the Chongqing Key Laboratory of Mobile Communication, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering and the Chongqing Key Laboratory of Mobile Communication, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering and the Chongqing Key Laboratory of Mobile Communication, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering and the Chongqing Key Laboratory of Mobile Communication, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Internet of Things Journal","22 Sep 2023","2023","10","19","16989","17006","Personalization is one of the primary emerging trends in Industry 4.0 and Beyond. Highly personalized services will present a significant challenge to the existing algorithms for network slicing (NS) and resource allocation, leading to issues, such as nonequilibratory resource allocation, in which some services are sacrificed for the maximum total reward of the algorithm, excessive cost, and slow algorithm convergence. A digital twin network (DTN) is offered as a novel solution to the challenges listed above. By integrating the DTN and IIoT NS, we propose a DTN-assisted industry Internet of Things NS (DTN-IIoT NS) architecture for personalized IIoT services in Industry 4.0 and Beyond. The DTN-IIoT NS architecture consists of three layers, three modules, and two closed loops. On the basis of the aforementioned architecture, we focus on the resource allocation process in DTN-IIoT NS, model the DT-assisted resource allocation for highly personalized IIoT services, propose the service equilibrium rate, and formulate the optimization problem aiming at maximizing the equilibrium rate weighted net profit of network providers. Then, we propose a dual-channel weighted (DCW) Critic network for service equilibrium in DTN-IIoT NS resource allocation and the matching Improved prioritized experience replay (PER) to enhance convergent speed. In addition, we present a distributed DT-assisted DCW-PER multiagent deep deterministic policy gradient (PER-DCW MADDPG) algorithm for the resource allocation process in DTN-IIoT NS. Simulation results indicate that the PER-DCW MADDPG algorithm can produce a better service equilibrium and accelerate the convergence speed of the algorithm.","2327-4662","","10.1109/JIOT.2023.3274163","National Natural Science Foundation of China(grant numbers:62071078); Science and Technology Research Program of Chongqing Municipal Education Commission(grant numbers:KJZD-M201800601); Sichuan Science and Technology Program(grant numbers:2021YFQ0053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10121776","Digital twin network (DTN);Industry 4.0 and Beyond;Industry Internet of Things;prioritized experience replay (PER);resource allocation;service equilibrium","Resource management;Fourth Industrial Revolution;Industrial Internet of Things;Digital twins;Optimization;Network slicing;Convergence","convergence;deep learning (artificial intelligence);digital twins;gradient methods;Internet of Things;multi-agent systems;optimisation;production engineering computing;reinforcement learning;resource allocation","algorithm convergence;closed loops;convergence speed;digital twin network;digital-twin-assisted resource allocation;distributed deep reinforcement learning;distributed DT-assisted DCW-PER multiagent deep deterministic policy gradient algorithm;DT-assisted resource allocation;DTN-assisted industry Internet of Things NS architecture;DTN-IIoT NS architecture;DTN-IIoT NS resource allocation;dual-channel weighted critic network;equilibrium rate weighted net profit;highly personalized IIoT services;Industry 4.0;network slicing;nonequilibratory resource allocation;optimization problem;PER-DCW MADDPG;prioritized experience replay;service equilibrium rate","","","","35","IEEE","9 May 2023","","","IEEE","IEEE Journals"
"Learning Effective Communication for Cooperative Pursuit with Multi-Agent Reinforcement Learning","Y. Deng; X. Cao; Q. Yang","School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; College of Control Science and Engineering, Zhejiang University, Hangzhou, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","3835","3840","In a multi-agent environment, agents need to frequently communicate with each other. However, due to resource constraints such as limited bandwidth, they may not be able to keep broadcasting messages. Therefore, agents are required to analyze the importance of their local observations and broadcast their messages only when necessary. To address these issues, we propose a multi-agent reinforcement learning algorithm with communication called Proximal Policy Optimization with Gated Attention Communication (PPO-GAC). It follows the Centralized Training and Decentralized Execution (CTDE) framework and is capable of deciding when to broadcast and how to handle messages received from other agents. Furthermore, we evaluate our algorithm in a multi-agent pursuit task. The simulation result shows that pursuers with PPO-GAC have the best performance in capturing all evaders compared to other baseline algorithms.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055979","National Natural Science Foundation of China; Technology Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055979","multi-agent reinforcement learning;attention mechanism;gating mechanism;pursuit","Training;Automation;Simulation;Aggregates;Reinforcement learning;Bandwidth;Logic gates","multi-agent systems;optimisation;reinforcement learning","centralized training and decentralized execution framework;cooperative pursuit;CTDE;gated attention communication;local observations;multiagent pursuit task;multiagent reinforcement learning;PPO-GAC;proximal policy optimization;resource constraints","","","","19","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Distributed Deep Reinforcement Learning Resource Allocation Scheme For Industry 4.0 Device-To-Device Scenarios","J. Burgueño; R. Adeogun; R. L. Bruun; C. S. Morejón García; I. de-la-Bandera; R. Barco","Department of Electronic Systems, Aalborg University, Denmark; Department of Electronic Systems, Aalborg University, Denmark; Department of Electronic Systems, Aalborg University, Denmark; Department of Electronic Systems, Aalborg University, Denmark; Instituto Universitario de Investigación en Telecomunicación (TELMA), Universidad de Málaga, CEI Andalucía TECH, Málaga, Spain; Instituto Universitario de Investigación en Telecomunicación (TELMA), Universidad de Málaga, CEI Andalucía TECH, Málaga, Spain","2021 IEEE 94th Vehicular Technology Conference (VTC2021-Fall)","10 Dec 2021","2021","","","1","7","This paper proposes a distributed deep reinforcement learning (DRL) methodology for autonomous mobile robots (AMRs) to manage radio resources in an indoor factory with no network infrastructure. Hence, deep neural networks (DNN) are used to optimize the decision policy of the robots, which will make decisions in a distributed manner without signalling exchange. To speed up the learning phase, a centralized training is adopted in which a single DNN is trained using the experience from all robots. Once completed, the pre-trained DNN is deployed at all robots for distributed selection of resources. The performance of this approach is evaluated and compared to 5G NR sidelink mode 2 via simulations. The results show that the proposed method achieves up to 5% higher probability of successful reception when the density of robots in the scenario is high.","2577-2465","978-1-6654-1368-8","10.1109/VTC2021-Fall52928.2021.9625582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9625582","Industry 4.0;deep reinforcement learning;device-to-device;resource allocation;decentralized communications","Training;Industries;Deep learning;Vehicular and wireless technologies;Service robots;Reinforcement learning;Production facilities","5G mobile communication;deep learning (artificial intelligence);factory automation;industrial robots;mobile robots;neural nets;probability;reinforcement learning;resource allocation","distributed deep reinforcement learning resource allocation scheme;autonomous mobile robots;radio resources;indoor factory;network infrastructure;deep neural networks;decision policy;learning phase;centralized training;pre-trained DNN;Industry 4.0 device-to-device scenarios;AMR;distributed DRL methodology;5G NR sidelink mode 2","","2","","20","IEEE","10 Dec 2021","","","IEEE","IEEE Conferences"
"On-line Energy Optimization of Hybrid Production Systems Using Actor-Critic Reinforcement Learning","D. Schwung; A. Schwung; S. X. Ding","Dept. of Electrical Engineering, South Westfalia University of Applied Sciences, Soest, Germany; Dept. of Electrical Engineering, South Westfalia University of Applied Sciences, Soest, Germany; Dept. of Automation and Komplex Systems, University of Duisburg-Essen, Duisburg, Germany","2018 International Conference on Intelligent Systems (IS)","9 May 2019","2018","","","147","154","This paper presents a novel approach for energy optimization in large scale industrial production systems based on an actor-critic reinforcement learning (ACRL) framework. The objective of the on-line capable self-learning algorithm is the optimization of the energy consumption of the whole production process. Our central ACRL framework is realized by artificial neural network (ANN) function approximation using Gaussian radial-basis functions (RBF) for the critic and the actor, respectively, and gives the opportunity to cover not only a discrete but also continuous state and action space, which is necessary for hybrid systems where discrete and continuous actuator behavior is combined. For testing and validation purposes we develop a software model of our bulk good laboratory plant as application example for the developed ACRL algorithm. The model is based on mass-flow equations for a continuous bulk good supply whereas the energy consumption is modeled by functions dependent on the actuators behavior. The capability of our machine learning (ML) approach for energy optimization is underlined by simulation results for the task of supplying bulk good to a subsequent dosing section.","1541-1672","978-1-5386-7097-2","10.1109/IS.2018.8710466","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8710466","Machine learning;actor-critic reinforcement learning;radial-basis function neural networks;hybrid systems;energy optimization","Optimization;Production;Energy consumption;Actuators;Reinforcement learning;Function approximation;Mathematical model","function approximation;learning (artificial intelligence);optimisation;production engineering computing;radial basis function networks","hybrid production systems;actor-critic reinforcement learning framework;production process;central ACRL framework;artificial neural network function approximation;Gaussian radial-basis functions;continuous state;action space;hybrid systems;continuous actuator behavior;bulk good laboratory plant;continuous bulk good supply;machine learning approach;online energy optimization;large-scale industrial production systems;online capable self-learning algorithm;energy consumption optimization;ANN function approximation","","4","","19","IEEE","9 May 2019","","","IEEE","IEEE Conferences"
"Reinforcement Learning to Optimize Voltage in 3 to 10-KV Factory Grids","A. Malafeev; Y. Kashkarova","Dept. of Electric Power Supply of Industrial Enterprises, Nosov Magnitogorsk State Technical University, Magnitogorsk, Russia; Dept. of Electric Power Supply of Industrial Enterprises, Nosov Magnitogorsk State Technical University, Magnitogorsk, Russia","2020 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)","9 Jun 2020","2020","","","1","6","The paper dwells upon automatic voltage control of industrial grids carrying daily-variable loads. Modal automation tools can be used to solve energy saving problems in the energy complex of an enterprise. The solution can use grid APCS that implements local parametric automations at 3 to 10-kV substations. The system is able to control the steady-state parameters for a large number of circuit elements and provides multicriteria voltage control. The main criteria are power loss minimization and minimum wear of transformer UVR. The paper proposes reinforcement learning to optimize real-time UVR controls; reinforcement learning can generate control actions for the current time interval while adjusting them to the projected upcoming parametric changes.","","978-1-7281-4590-7","10.1109/ICIEAM48468.2020.9112015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9112015","voltage control;power loss;UVR wear;value function;reward function;scaled benefit;dynamic programming","Substations;Automation;Reinforcement learning;Switches;Programming;Transformers;Automatic voltage control","learning (artificial intelligence);minimisation;power grids;power system control;substations;voltage control","reinforcement learning;industrial grids;modal automation tools;energy saving problems;grid APCS;steady-state parameters;circuit elements;multicriteria voltage control;power loss minimization;control actions;factory grids;UVR controls","","","","26","IEEE","9 Jun 2020","","","IEEE","IEEE Conferences"
"Reinforcement adaptive learning neural-net-based friction compensation control for high speed and precision","Young Ho Kim; F. L. Lewis","Korea Army, Daejeon, South Korea; Automation and Robotics Research Institute, University of Technology, Arlington, TX, USA","IEEE Transactions on Control Systems Technology","6 Aug 2002","2000","8","1","118","126","There is an increasing number of applications in high-precision motion control systems in manufacturing, i.e., ultra-precision machining, assembly of small components and micro devices. It is very difficult to assure such accuracy due to many factors affecting the precision of motion, such as frictions and disturbances in the drive system. The standard proportional-integral-derivative (PID) type servo control algorithms are not capable of delivering the desired precision under the influence of frictions and disturbances. In this paper, the frictions are identified by a neural net, which has a critic element to measure the system performance. Then, the weight adaptation rule, defined as reinforcement adaptive learning, is derived from the Lyapunov stability theory. Therefore the proposed scheme can be applicable to a wide class of mechanical systems. The simulation results on a 1-degree-of-freedom mechanical system verify the effectiveness of the proposed algorithm.","1558-0865","","10.1109/87.817697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=817697","","Friction;Mechanical systems;Motion control;Manufacturing;Machining;Assembly systems;Drives;Three-term control;Servosystems;Neural networks","friction;compensation;motion control;feedback;neurocontrollers;learning (artificial intelligence);servomechanisms;intelligent control;Lyapunov methods;learning systems","reinforcement adaptive learning neural-net-based friction compensation control;high-precision motion control systems;ultra-precision machining;micro devices;small components;critic element;weight adaptation rule;Lyapunov stability theory;mechanical systems;1 DOF mechanical system","","92","","22","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"A physics-guided reinforcement learning framework for an autonomous manufacturing system with expensive data","M. F. Alam; M. Shtein; K. Barton; D. J. Hoelzle","Department of Mechanical and Aerospace Engineering, Ohio State University, Columbus, OH, USA; Department of Materials Science and Engineering, University of Michigan, Ann Arbor, Michigan, USA; Department of Mechanical Engineering, University of Michigan, Ann Arbor, Michigan, USA; Department of Mechanical and Aerospace Engineering, Ohio State University, Columbus, OH, USA","2021 American Control Conference (ACC)","28 Jul 2021","2021","","","484","490","Making intelligent decisions is the biggest challenge in building an autonomous manufacturing system that can build artifacts with desired properties without human intervention. Although reinforcement learning (RL) has favorable characteristics for such a task, the sample efficiency of RL is poor, which makes it difficult to implement on a manufacturing system due to the expense of producing parts to collect reward and action data. This paper focuses on building a framework for implementation of RL on manufacturing systems with expensive data and presents the framework for autonomous manufacturing of Phononic Crystals (PnCs), a type of acoustic metamaterial. Leveraging knowledge from physics-guided models and temporal abstraction ideas, we detail a framework that reduces the task of finding optimal manufacturing parameters from thousands of manufacturing samples to the order of 50 samples. The method is applied in simulation to a stochastic model of PnC production. Critically, we show that by using a long temporal abstraction horizon and order of 50 sample budget, the RL algorithm finds the optimal region greater than 95% of the time.","2378-5861","978-1-6654-4197-1","10.23919/ACC50511.2021.9482944","NSF(grant numbers:CMMI-1727894); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9482944","","Simulation;Buildings;Stochastic processes;Reinforcement learning;Production;Robustness;Task analysis","learning (artificial intelligence);manufacturing systems;phononic crystals","manufacturing samples;RL algorithm;physics-guided reinforcement learning framework;autonomous manufacturing system;expensive data;sample efficiency;action data;physics-guided models;optimal manufacturing parameters","","2","","25","","28 Jul 2021","","","IEEE","IEEE Conferences"
"Hierarchical Granular Computing-Based Model and Its Reinforcement Structural Learning for Construction of Long-Term Prediction Intervals","Z. Han; W. Pedrycz; J. Zhao; W. Wang","School of Control Sciences and Engineering, Dalian University of Technology, Dalian, China; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Canada; School of Control Sciences and Engineering, Dalian University of Technology, Dalian, China; School of Control Sciences and Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Cybernetics","11 Jan 2022","2022","52","1","666","676","As one of the most essential sources of energy, byproduct gas plays a pivotal role in the steel industry, for which the flow tendency is generally regarded as the guidance for planning and scheduling in real production. In order to obtain the numeric estimation along with its reliability, the construction of prediction intervals (PIs) is highly demanded by any practical applications as well as being long term for providing more information on future trends. Bearing this in mind, in this article, a hierarchical granular computing (HGrC)-based model is established for constructing long-term PIs, in which probabilistic modeling gives rise to a long horizon of numeric prediction, and the deployment of information granularities hierarchically extends the result to be interval-valued format. Considering that the structure of this model has a direct impact on its performance, Monte-Carlo search with a policy gradient technique is then applied for reinforcement structure learning. Compared with the existing methods, the size (length) of the granules in the proposed approach is unequal so that it becomes effective for not only periodic but also nonperiodic data. Furthermore, with the use of parallel strategy, the efficiency can be also guaranteed for real-world applications. The experimental results demonstrate that the proposed method is superior to other commonly encountered techniques, and the stability of the structure learning process behaves better when compared with other reinforcement learning approaches.","2168-2275","","10.1109/TCYB.2020.2964011","National Key Research and Development Program(grant numbers:2017YFA0700300); National Natural Science Foundation of China(grant numbers:61533005,61703071,61833003,61773085); Fundamental Research Funds for the Central Universities of China(grant numbers:DUT18RC(3)074); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8972350","Byproduct gas;hierarchical granular computing (HGrC);prediction intervals (PIs);reinforcement structure learning;steel industry","Computational modeling;Predictive models;Adaptation models;Steel industry;Numerical models;Data models;Artificial neural networks","gradient methods;granular computing;Monte Carlo methods;probability;production engineering computing;reinforcement learning;scheduling;steel industry;strategic planning","numeric estimation;hierarchical granular computing-based model;probabilistic modeling;numeric prediction;information granularities;Monte-Carlo search;policy gradient technique;reinforcement structure learning;structure learning process;reinforcement learning approaches;steel industry;production planning;production scheduling;byproduct gas","","11","","37","IEEE","28 Jan 2020","","","IEEE","IEEE Journals"
"Routability-Driven Detailed Placement Using Reinforcement Learning","S. F. Almeida; J. Luís Güntzel; L. Behjat; C. Meinhardt","Department of Informatics and Statistics, Federal University of Santa Catarina, Brazil; Department of Informatics and Statistics, Federal University of Santa Catarina, Brazil; Department of Electrical and Software Engineering, University of Calgary, Canada; Department of Informatics and Statistics, Federal University of Santa Catarina, Brazil","2022 IFIP/IEEE 30th International Conference on Very Large Scale Integration (VLSI-SoC)","8 Nov 2022","2022","","","1","2","Technology advancements have enabled us to manufacture integrated circuits composed of a sheer number of gates onto a single chip. However, these enhancements have also introduced new challenges. In physical synthesis, the placement and routing steps have to satisfy even more complex design rules while optimizing the solution quality. However, the search for wirelength optimization may lead the placement engine to produce an infeasible routing solution, making it necessary to repeat previous steps and increase the overall project cost. Traditionally, placement algorithms estimate routability using pin density because of its low computational cost. Nonetheless, in advanced technology nodes, this has become inefficient due to more restrictive manufacturing constraints and complex standard cell layouts. Although many placement techniques propose to address routability, the problem is that these models rely on specific heuristics or designer experience. Therefore, we propose a machine learning-based framework for addressing routability during the placement step.","2324-8440","978-1-6654-9005-4","10.1109/VLSI-SoC54400.2022.9939602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9939602","Electronic Design Automation;Detailed Placement;Routing Violation;Reinforcement Learning","Layout;Reinforcement learning;Very large scale integration;Logic gates;Lead;Routing;Pins","electronic engineering computing;integrated circuit layout;integrated circuit manufacture;network routing;optimisation;reinforcement learning","routability-driven detailed placement;reinforcement learning;technology advancements;integrated circuits manufacture;single chip;physical synthesis;routing steps;complex design rules;solution quality;wirelength optimization;placement engine;infeasible routing solution;placement algorithms;pin density;low computational cost;advanced technology nodes;restrictive manufacturing constraints;complex standard cell layouts;placement techniques;specific heuristics;machine learning-based framework","","","","14","IEEE","8 Nov 2022","","","IEEE","IEEE Conferences"
"Evolvable Motion-planning Method using Deep Reinforcement Learning","K. Nishi; N. Nakasu","Production Systems Research Department, Center for Technology Innovation - Production Engineering, Hitachi, Ltd. Research and Development Group, Yokohama, Japan; Production Systems Research Department, Center for Technology Innovation - Production Engineering, Hitachi, Ltd. Research and Development Group, Yokohama, Japan","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","4079","4085","A motion-planning method that can adapt to changes in the surrounding environment is proposed and evaluated. Automation of work is progressing in factories and distribution warehouses due to labor shortages. However, utilizing robots for transport operations in a distribution warehouse faces a problem; that is, tasks for setting up a robot, such as adjustment of acceleration for stabilization of the transportation operation, are time consuming. To solve that problem, we developed an ""evolvable robot motion-planning method."" The aim of this method is to reduce the preparation cost by allowing the robot to automatically learn the optimized acceleration according to the weight and center of gravity of the objects to be transported. It was experimentally demonstrated that the proposed method can learn the optimized acceleration control from time-series data such as sensor information. The proposed method was evaluated in a simulator environment, and the results of the evaluation demonstrate that the learned model reduced the inertial force due to the acceleration of robot motion and shortened the transport time by 35% compared with the conventional method of manual adjustment. The proposed method was also evaluated in a real machine environment, and the evaluation results demonstrate that the method can be applied to a real robot. Since the speed of the robot does not need to be adjusted in the case of the proposed method, the adjustment man-hours can be reduced.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561602","","Robot motion;Automation;Transportation;Reinforcement learning;Robot sensing systems;Manipulators;Production facilities","acceleration control;image motion analysis;learning (artificial intelligence);mobile robots;path planning;robots","evolvable motion-planning method;deep reinforcement learning;distribution warehouse;labor shortages;transport operations;transportation operation;evolvable robot motion-planning method;optimized acceleration control;time-series data;simulator environment;transport time;machine environment","","1","","32","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Development and research of learning algorithms for neural networks with reinforcement in the gaming industry","D. Ulyanov; D. Savelyev","Samara National Research University, Samara, Russia; “Crystallography and Photonics” RAS, Samara National Research University; Image Processing Systems Institute of RAS – Branch of the FSRC, Samara, Russia","2021 International Conference on Information Technology and Nanotechnology (ITNT)","24 Dec 2021","2021","","","1","6","This paper provides a test of the hypothesis about increasing the effectiveness of agents specialized for certain aspects over universal ones. Within the framework of the modified game environment Starcraft 2, a series of experiments was carried out, the analysis of the results of which was the conclusion about the confirmation of the hypothesis.","","978-1-6654-3217-7","10.1109/ITNT52450.2021.9649189","Ministry of Science and Higher Education of the Russian Federation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649189","Starcraft 2;reinforcement learning;multiagent learning","Training;Industries;Economics;Games;Learning (artificial intelligence);Artificial neural networks;Data mining","computer games;neural nets;reinforcement learning","neural networks;gaming industry;Starcraft 2 game environment;learning algorithms","","","","31","IEEE","24 Dec 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Robotic Assembly of Mixed Deformable and Rigid Objects","J. Luo; E. Solowjow; C. Wen; J. A. Ojea; A. M. Agogino","Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Siemens Corporate Technology, Berkeley, CA, USA; Siemens Corporate Technology, Berkeley, CA, USA; Siemens Corporate Technology, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA","2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","6 Jan 2019","2018","","","2062","2069","Reinforcement learning for assembly tasks can yield powerful robot control algorithms for applications that are challenging or even impossible for “conventional” feedback control methods. Insertion of a rigid peg into a deformable hole of smaller diameter is such a task. In this contribution we solve this task with Deep Reinforcement Learning. Force-torque measurements from a robot arm wrist sensor are thereby incorporated two-fold; they are integrated into the policy learning process and they are exploited in an admittance controller that is coupled to the neural network. This enables robot learning of contact-rich assembly tasks without explicit joint torque control or passive mechanical compliance. We demonstrate our approach in experiments with an industrial robot.","2153-0866","978-1-5386-8094-0","10.1109/IROS.2018.8594353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8594353","","Robot sensing systems;Task analysis;Reinforcement learning;Neural networks;Service robots;Robotic assembly","control engineering computing;feedback;industrial robots;learning (artificial intelligence);neural nets;position control;robot programming;robotic assembly;torque control;torque measurement","neural network;force torque measurements;passive mechanical compliance;deep reinforcement learning;torque control;robot control algorithms;assembly tasks;feedback control methods;robotic assembly;industrial robot;robot learning;admittance controller;policy learning process;robot arm wrist sensor;deformable hole;rigid peg","","45","","28","IEEE","6 Jan 2019","","","IEEE","IEEE Conferences"
"Novelty Search for Neuroevolutionary Reinforcement Learning of Deceptive Systems: An Application to Control of Colloidal Self-assembly","J. O’Leary; M. M. Khare; A. Mesbah","Department of Chemical and Biomolecular Engineering, University of California, Berkeley, CA, USA; Department of Chemical and Biomolecular Engineering, University of California, Berkeley, CA, USA; Department of Chemical and Biomolecular Engineering, University of California, Berkeley, CA, USA","2023 American Control Conference (ACC)","3 Jul 2023","2023","","","2776","2781","Colloidal self-assembly systems are generally difficult to control due to their highly nonlinear and stochastic dynamics and sparse rewards. These systems are also inherently deceptive, as successful control policies must be able to smooth out unavoidable defects and therefore temporarily move farther away from their goal in order to eventually realize the desired goal. This paper investigates the viability of evolutionary reinforcement learning (RL) based on novelty search, wherein behavioral novelty alone is used to learn control policies that can systematically mitigate deceptive dynamics. As such, for stochastic nonlinear systems that are prone to a deceptive behavior, novelty search is a promising alternative to the widely used objective search RL, where merely progress towards a pre-defined goal is used to learn and update control policies. In this work, we pair novelty search RL with a complexifying algorithm that simultaneously learns the neural network architecture and parameters of a control policy. The complexifying algorithm principles the novelty search by ensuring that simple behaviors must be discovered before more complex ones. We evaluate the performance of evolutionary RL based on objective search and novelty search on a benchmark in-silico colloidal self-assembly problem.","2378-5861","979-8-3503-2806-6","10.23919/ACC55779.2023.10155994","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155994","","Self-assembly;Neural networks;Reinforcement learning;Benchmark testing;Search problems;Behavioral sciences;Nonlinear systems","colloids;evolutionary computation;learning (artificial intelligence);neural nets;reinforcement learning;search problems;self-assembly","behavioral novelty;benchmark in-silico colloidal self-assembly problem;colloidal self-assembly systems;control policy;deceptive behavior;deceptive dynamics;deceptive systems;evolutionary reinforcement learning;highly nonlinear dynamics;neuroevolutionary reinforcement learning;objective search;pair novelty search RL;stochastic dynamics;stochastic nonlinear systems;successful control policies","","","","34","","3 Jul 2023","","","IEEE","IEEE Conferences"
"Rules-PPO-QMIX: Multi-Agent Reinforcement Learning with Mixed Rules for Large Scene Tasks","Z. -Z. Shen; R. Yu; Y. -Y. Chen","School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","6731","6736","Under the StarCraft multi-agent challenge (SMAC), the large scenes with multiple task objectives are obstacles for directly applying the existing multi-agent reinforcement learning (MARL) algorithms. The main challenge lies in that the recent MARL algorithms have a poor performance in these situations. A novel Rules-PPO-QMIX MARL algorithm is designed to 1) determine the optimal target and paths with tools of manual rules and proximal policy optimization (PPO) and 2) perform decentralised micromanagement near the target with monotonic value function factorisation MARL algorithm (QMIX). In such way, the complete decision-making process is divided into several parts, that is, path planning, target selecting and micromanagement, which is yielded by manual rules, single-agent reinforcement learning and MARL, respectively. The effectiveness of Rules-PPO-QMIX is validated by a testing map of SMAC, with QMIX as baseline. To demonstrate the feasibility on small scenes, a classical 8m SMAC environment is also used to test the proposed algorithm.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728241","Multi-agent reinforcement learning;manual rules;PPO;QMIX;large scene tasks;SMAC","Automation;Decision making;Reinforcement learning;Manuals;Path planning;Task analysis;Optimization","computer games;decision making;multi-agent systems;reinforcement learning","single-agent reinforcement learning;SMAC;mixed rules;scene tasks;StarCraft multiagent challenge;multiple task objectives;multiagent reinforcement learning;optimal target;proximal policy optimization;monotonic value function factorisation MARL algorithm;Rules-PPO-QMIX MARL algorithm","","","","17","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Local Sensing based Multi-agent Pursuit-evasion with Deep Reinforcement Learning","S. Wang; B. Wang; Z. Han; Z. Lin","School of Automation (Artifical Intellengence), Hangzhou Dianzi University, Hangzhou, China; School of Automation (Artifical Intellengence), Hangzhou Dianzi University, Hangzhou, China; School of Automation (Artifical Intellengence), Hangzhou Dianzi University, Hangzhou, China; Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","6748","6752","In this paper, a problem of multi-agent pursuit-evasion in which multiple pursuers try to round up a single evader as soon as possible in a 2D limited space is considered. A cooperative approach of pursuit strategy through sensing among pursuits is developed under the multi-agent deep deterministic policy gradient (MADDPG) reinforcement learning (RL) method, which adopts the centralized training and distributed execution to deal with the pursuit problem by a fully distributed approach. Instead of using the communication information, each pursuer is supposed that can only obtain the sensing information of its neighbors and the evader. By introducing the sensing range segment and relative-position average strategy, we allow that the number of the pursuer can be changeable, which is different from some existing results that assume that the number of pursuers is fixed. To demonstrate the feasibility of the proposed method above, we implement it in a simulation environment. Simulation results show that the pursuer can learn a highly cooperative control strategy and capture the evader with a high success rate.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055841","National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055841","multi-agent;pursuit-evasion;reinforce learning","Training;Deep learning;Analytical models;Automation;Simulation;Reinforcement learning;Sensors","deep learning (artificial intelligence);gradient methods;learning (artificial intelligence);multi-agent systems;reinforcement learning;telecommunication computing","deep reinforcement learning;evader;fully distributed approach;local sensing;multiagent deep deterministic policy gradient reinforcement learning method;multiagent pursuit-evasion;multiple pursuers;pursuer;pursuit problem;pursuit strategy;pursuits;relative-position average strategy;sensing information;sensing range segment","","","","21","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Global Routing Under a Congestion-Aware Reinforcement Learning Model","L. Li; Y. Cai; Q. Zhou","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","2023 International Symposium of Electronics Design Automation (ISEDA)","25 Aug 2023","2023","","","213","218","As one of the challenging problems in VLSI physical design, global routing is facing increasing difficulties and more and more algorithms attempt to introduce machine learning-based solutions. While most of these solutions lack high enough routability and routing efficiency. In this paper, we propose a congestion-aware reinforcement learning model for global routing. The global routing problem is solved based on pattern routing and a reinforcement learning technique is introduced for better searching global optimal routing decisions. We convert the network relation into a graph structure and introduce a graph attention network to provide global congestion features. Moreover, a net segment-based feature extraction mode for pattern routing is proposed to enhance the routing efficiency. Experimental results on ISPD18 benchmark show that the proposed algorithm can route different unseen layouts with an average routability over 99.9% and the average number of the failed nets is around 6.","","979-8-3503-0451-0","10.1109/ISEDA59274.2023.10218371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10218371","VLSI;Global Routing;Deep Reinforcement Learning;Graph Attention Network","Machine learning algorithms;Design automation;Layout;Reinforcement learning;Very large scale integration;Benchmark testing;Routing","electronic design automation;feature extraction;graph theory;network routing;reinforcement learning;VLSI","congestion-aware reinforcement learning model;global congestion features;global optimal routing decisions;global routing problem;ISPD18 benchmark;machine learning-based solutions;net segment-based feature extraction mode;pattern routing;reinforcement learning technique;route different unseen layouts;routing efficiency;VLSI physical design","","","","17","IEEE","25 Aug 2023","","","IEEE","IEEE Conferences"
"Reinforcement learning techniques applied to the motion planning of a robotic manipulator","F. M. Ribeiro; V. H. Pinto","FEUP - Faculty of Engineering, University of Porto, Portugal; FEUP - Faculty of Engineering, University of Porto, Portugal","2022 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)","1 Jun 2022","2022","","","173","178","Throughout this article the execution of the motion planning for a robotic manipulator by means of Reinforcement Learning methods is studied. Towards this, an implementation based on a “Wire and loop” game is used as an example case to be solved. The loop is controlled in a single plane as the end-effector of the manipulator. The modeling of the problem and the process of training the agent is detailed. This allowed for the verification of the capacity of a learning based method, having produced, under the considered abstractions, satisfying results by gaining the capability of completing the path imposed by the wire in 23 seconds.","","978-1-6654-8217-2","10.1109/ICARSC55462.2022.9784814","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9784814","Robotic Manipulator;Reinforcement Learning;Industry 4.0;Motion Planning;Simulation","Training;Learning systems;Welding;Wires;Reinforcement learning;Production;Games","control engineering computing;end effectors;multi-agent systems;path planning;reinforcement learning","motion planning;robotic manipulator;reinforcement learning;end-effector;agent training;capacity verification","","","","10","IEEE","1 Jun 2022","","","IEEE","IEEE Conferences"
"Reinforcement-Learning-Based Active Disturbance Rejection Control of Piezoelectric Actuators","M. -L. Yi; Y. -L. Zhang; X. Huang; H. -T. Zhang","And State Key Lab of Digital Manufacturing Equipment and Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, P. R. China; And State Key Lab of Digital Manufacturing Equipment and Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, P. R. China; And State Key Lab of Digital Manufacturing Equipment and Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, P. R. China; And State Key Lab of Digital Manufacturing Equipment and Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, P. R. China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","2303","2307","Piezoelectric actuators (PEAs) are crutial components in nano-positioning processes due to their high positioning precision. However, the application of PEAs is hindered by their intrinsic rate-dependent hysteretic nonlinearity. This paper proposes a reinforcement-learning-based active disturbance rejection control (RLBADRC) scheme for controling PEAs. Unlike conventional preexisting linear active disturbance rejection control (LADRC) for PEAs, control parameters are adaptively learned with Q learning in the present RLBADRC instead. Moreover, the tracking errors are regarded as control states, whereas different control parameters are regarded as control actions. By this means, a Q table is established offline and parameters are adaptively learned with the well-trained Q table model online. Comparative experiments are conducted to verify the effectiveness of the approach.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902520","Active disturbance rejection control (ADRC);Reinforcement Learning (RL);Piezoelectric actuator(PEA)","Robust control;Adaptation models;Q-learning;Process control;Piezoelectric actuators;Robustness;Optimization","active disturbance rejection control;control engineering computing;control nonlinearities;hysteresis;nanopositioning;piezoelectric actuators;reinforcement learning","control parameters;control states;intrinsic rate-dependent hysteretic nonlinearity;nanopositioning;PEAs;piezoelectric actuators;positioning precision;Q learning;Q table;reinforcement-learning-based active disturbance rejection control;RLBADRC;tracking errors","","","","11","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Adaptive Weight Tuning of EWMA Controller via Model-Free Deep Reinforcement Learning","Z. Ma; T. Pan","School of Computer Science and Technology and the School of Electrical Engineering and Automation, Anhui University, Hefei, China; School of Electrical Engineering and Automation, Anhui University, Hefei, China","IEEE Transactions on Semiconductor Manufacturing","3 Feb 2023","2023","36","1","91","99","Exponentially weighted moving average (EWMA) controllers have been extensively studied for run-to-run (RtR) control in semiconductor manufacturing processes. However, the EWMA controller with a fixed weight struggles to achieve excellent performance under unknown stochastic disturbances. To improve the performance of EMWA via online parameter tuning, an intelligent strategy using deep reinforcement learning (DRL) technique is developed in this work. To begin with, the weight adjusting problem is established as a Markov decision process. Meanwhile, simple state space, action space and reward function are designed. Then, the classical deep deterministic policy gradient (DDPG) algorithm is utilized to adjust the weight online. Moreover, a quantile regression-based DDPG (QR-DDPG) algorithm is further verified the effectiveness of the proposed method. Finally, the developed scheme is implemented on a deep reactive ion etching process. Comparisons are conducted to show the superiority of the presented approach in terms of disturbance rejection and target tracking.","1558-2345","","10.1109/TSM.2022.3225480","National Natural Science Foundation of China(grant numbers:62273002,61873113); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9965409","Run-to-run control;adaptive control;exponentially weighted moving average;deep reinforcement learning;semiconductor manufacturing process","Semiconductor device modeling;Process control;Deep learning;Reinforcement learning;Adaptation models;Semiconductor device manufacture;Aerospace electronics","deep learning (artificial intelligence);electronic engineering computing;gradient methods;Markov processes;moving average processes;regression analysis;reinforcement learning;semiconductor technology;state-space methods;target tracking","action space;adaptive weight tuning;deep deterministic policy gradient algorithm;deep reactive ion etching process;deep reinforcement learning;DRL technique;EWMA controller;exponentially weighted moving average controllers;Markov decision process;model-free deep reinforcement learning;online parameter tuning;QR-DDPG algorithm;quantile regression-based DDPG algorithm;reward function;run-to-run control;semiconductor manufacturing processes;simple state space;unknown stochastic disturbances;weight adjusting problem","","","","28","IEEE","28 Nov 2022","","","IEEE","IEEE Journals"
