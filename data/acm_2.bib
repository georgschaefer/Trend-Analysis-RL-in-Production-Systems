@inproceedings{10.1145/3520304.3528937,
author = {Abramowitz, Sasha and Nitschke, Geoff},
title = {Scalable Evolutionary Hierarchical Reinforcement Learning},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3528937},
doi = {10.1145/3520304.3528937},
abstract = {This paper investigates a novel method combining Scalable Evolution Strategies (S-ES) and Hierarchical Reinforcement Learning (HRL). S-ES, named for its excellent scalability, was popularised with demonstrated performance comparable to state-of-the-art policy gradient methods. However, S-ES has not been tested in conjunction with HRL methods, which empower temporal abstraction thus allowing agents to tackle more challenging problems. We introduce a novel method merging S-ES and HRL, which creates a highly scalable and efficient (compute time) algorithm. We demonstrate that the proposed method benefits from S-ES's scalability and indifference to delayed rewards. This results in our main contribution: significantly higher learning speed and competitive performance compared to gradient-based HRL methods, across a range of tasks.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {272–275},
numpages = {4},
keywords = {evolution strategies, hierarchical reinforcement learning},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1109/IAT.2007.86,
author = {Bergeron, Dany and Desjardins, Charles and Laumonier, Julien and Chaib-draa, Brahim},
title = {Reinforcement Learning with Inertial Exploration},
year = {2007},
isbn = {0769530273},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IAT.2007.86},
doi = {10.1109/IAT.2007.86},
abstract = {In the Q-Learning framework, the exploration of large environment is influenced by the time credit assignment problem. In this context, abstraction techniques may be used. Thus, multi-step actions (MSA) Q-Learning has been proposed to take advantage of the fact that few action switches are usually required in optimal policies. In this article, we propose the concept of inertial exploration, we apply a log-selection of the scales to MSA Q-Learning and we go further by proposing a dynamic time scale approach. We demonstrate that the same improvement in learning speed can be achieved without the full scales set. This improvement is shown on the mountain car problem and on a more realistic application of vehicle control.},
booktitle = {Proceedings of the 2007 IEEE/WIC/ACM International Conference on Intelligent Agent Technology},
pages = {277–280},
numpages = {4},
series = {IAT '07}
}

@inproceedings{10.1145/3551349.3556962,
author = {Liu, Junrui and Chen, Yanju and Tan, Bryan and Dillig, Isil and Feng, Yu},
title = {Learning Contract Invariants Using Reinforcement Learning},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556962},
doi = {10.1145/3551349.3556962},
abstract = {Due to the popularity of smart contracts in the modern financial ecosystem, there has been growing interest in formally verifying their correctness and security properties. Most existing techniques in this space focus on common vulnerabilities like arithmetic overflows and perform verification by leveraging contract invariants (i.e., logical formulas hold at transaction boundaries). In this paper, we propose a new technique, based on deep reinforcement learning, for automatically learning contract invariants that are useful for proving arithmetic safety. Our method incorporates an off-line training phase in which the verifier uses its own verification attempts to learn a policy for contract invariant generation. This learned (neural) policy is then used at verification time to predict likely invariants that are also useful for proving arithmetic safety. We implemented this idea in a tool called Cider and incorporated it into an existing verifier (based on refinement type checking) for proving arithmetic safety. Our evaluation shows that Cider improves both the quality of the inferred invariants as well as inference time, leading to faster verification and hardened contracts with fewer run-time assertions.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {63},
numpages = {11},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3278186.3278187,
author = {Adamo, David and Khan, Md Khorrom and Koppula, Sreedevi and Bryce, Ren\'{e}e},
title = {Reinforcement Learning for Android GUI Testing},
year = {2018},
isbn = {9781450360531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278186.3278187},
doi = {10.1145/3278186.3278187},
abstract = {This paper presents a reinforcement learning approach to automated GUI testing of Android apps. We use a test generation algorithm based on Q-learning to systematically select events and explore the GUI of an application under test without requiring a preexisting abstract model. We empirically evaluate the algorithm on eight Android applications and find that the proposed approach generates test suites that achieve between 3.31\% to 18.83\% better block-level code coverage than random test generation.},
booktitle = {Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation},
pages = {2–8},
numpages = {7},
keywords = {GUI Testing, Q-learning, Android, Mobile application testing},
location = {Lake Buena Vista, FL, USA},
series = {A-TEST 2018}
}

@inproceedings{10.1145/1687399.1687486,
author = {Tan, Ying and Liu, Wei and Qiu, Qinru},
title = {Adaptive Power Management Using Reinforcement Learning},
year = {2009},
isbn = {9781605588001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1687399.1687486},
doi = {10.1145/1687399.1687486},
abstract = {System level power management must consider the uncertainty and variability that comes from the environment, the application and the hardware. A robust power management technique must be able to learn the optimal decision from past history and improve itself as the environment changes. This paper presents a novel online power management technique based on model-free constrained reinforcement learning (RL). It learns the best power management policy that gives the minimum power consumption for a given performance constraint without any prior information of workload. Compared with existing machine learning based power management techniques, the RL based learning is capable of exploring the trade-off in the power-performance design space and converging to a better power management policy. Experimental results show that the proposed RL based power management achieves 24\% and 3\% reduction in power and latency respectively comparing to the existing expert based power management.},
booktitle = {Proceedings of the 2009 International Conference on Computer-Aided Design},
pages = {461–467},
numpages = {7},
keywords = {power management, model-free, reinforcement learning, Q-learning},
location = {San Jose, California},
series = {ICCAD '09}
}

@inproceedings{10.1145/3452296.3472902,
author = {Zhu, Hang and Gupta, Varun and Ahuja, Satyajeet Singh and Tian, Yuandong and Zhang, Ying and Jin, Xin},
title = {Network Planning with Deep Reinforcement Learning},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472902},
doi = {10.1145/3452296.3472902},
abstract = {Network planning is critical to the performance, reliability and cost of web services. This problem is typically formulated as an Integer Linear Programming (ILP) problem. Today's practice relies on hand-tuned heuristics from human experts to address the scalability challenge of ILP solvers.In this paper, we propose NeuroPlan, a deep reinforcement learning (RL) approach to solve the network planning problem. This problem involves multi-step decision making and cost minimization, which can be naturally cast as a deep RL problem. We develop two important domain-specific techniques. First, we use a graph neural network (GNN) and a novel domain-specific node-link transformation for state encoding, in order to handle the dynamic nature of the evolving network topology during planning decision making. Second, we leverage a two-stage hybrid approach that first uses deep RL to prune the search space and then uses an ILP solver to find the optimal solution. This approach resembles today's practice, but avoids human experts with an RL agent in the first stage. Evaluation on real topologies and setups from large production networks demonstrates that NeuroPlan scales to large topologies beyond the capability of ILP solvers, and reduces the cost by up to 17\% compared to hand-tuned heuristics.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {258–271},
numpages = {14},
keywords = {reinforcement learning, graph neural network, network planning},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@inproceedings{10.1145/3578360.3580273,
author = {VenkataKeerthy, S. and Jain, Siddharth and Kundu, Anilava and Aggarwal, Rohit and Cohen, Albert and Upadrasta, Ramakrishna},
title = {RL4ReAl: Reinforcement Learning for Register Allocation},
year = {2023},
isbn = {9798400700880},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578360.3580273},
doi = {10.1145/3578360.3580273},
abstract = {We aim to automate decades of research and experience in register allocation, leveraging machine learning. We tackle this problem by embedding a multi-agent reinforcement learning algorithm within LLVM, training it with the state of the art techniques. We formalize the constraints that precisely define the problem for a given instruction-set architecture, while ensuring that the generated code preserves semantic correctness. We also develop a gRPC based framework providing a modular and efficient compiler interface for training and inference. Our approach is architecture independent: we show experimental results targeting Intel x86 and ARM AArch64. Our results match or out-perform the heavily tuned, production-grade register allocators of LLVM.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction},
pages = {133–144},
numpages = {12},
keywords = {Register Allocation, Reinforcement Learning},
location = {Montr\'{e}al, QC, Canada},
series = {CC 2023}
}

@inproceedings{10.1145/3372780.3378174,
author = {Goldie, Anna and Mirhoseini, Azalia},
title = {Placement Optimization with Deep Reinforcement Learning},
year = {2020},
isbn = {9781450370912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372780.3378174},
doi = {10.1145/3372780.3378174},
abstract = {Placement Optimization is an important problem in systems and chip design, which consists of mapping the nodes of a graph onto a limited set of resources to optimize for an objective, subject to constraints. In this paper, we start by motivating reinforcement learning as a solution to the placement problem. We then give an overview of what deep reinforcement learning is. We next formulate the placement problem as a reinforcement learning problem, and show how this problem can be solved with policy gradient optimization. Finally, we describe lessons we have learned from training deep reinforcement learning policies across a variety of placement optimization problems.},
booktitle = {Proceedings of the 2020 International Symposium on Physical Design},
pages = {3–7},
numpages = {5},
keywords = {rl for combinatorial optimization, placement optimization, reinforcement learning, deep learning, device placement},
location = {Taipei, Taiwan},
series = {ISPD '20}
}

@inproceedings{10.1145/3511808.3557083,
author = {Yuan, Yiping and Muralidharan, Ajith and Nandy, Preetam and Cheng, Miao and Prabhakar, Prakruthi},
title = {Offline Reinforcement Learning for Mobile Notifications},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557083},
doi = {10.1145/3511808.3557083},
abstract = {Mobile notification systems have taken a major role in driving and maintaining user engagement for online platforms. They are interesting recommender systems to machine learning practitioners with more sequential and long-term feedback considerations. Most machine learning applications in notification systems are built around response-prediction models, trying to attribute both short-term impact and long-term impact to a notification decision. However, a user's experience depends on a sequence of notifications and attributing impact to a single notification is not always accurate, if not impossible. In this paper, we argue that reinforcement learning is a better framework for notification systems in terms of performance and iteration speed. We propose an offline reinforcement learning framework to optimize sequential notification decisions for driving user engagement. We describe a state-marginalized importance sampling policy evaluation approach, which can be used to evaluate the policy offline and tune learning hyperparameters. Through simulations that approximate the notifications ecosystem, we demonstrate the performance and benefits of the offline evaluation approach as a part of the reinforcement learning modeling approach. Finally, we collect data through online exploration in the production system, train an offline Double Deep Q-Network and launch a successful policy online. We also discuss the practical considerations and results obtained by deploying these policies for a large-scale recommendation system use-case.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {3614–3623},
numpages = {10},
keywords = {mobile notifications, reinforcement learning, off-policy evaluation},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3345629.3345636,
author = {Nguyen, An and Le, Bach and Nguyen, Vu},
title = {Prioritizing Automated User Interface Tests Using Reinforcement Learning},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345636},
doi = {10.1145/3345629.3345636},
abstract = {User interface testing validates the correctness of an application through visual cues and interactive events emitted in real world usages. Performing user interface tests is a time-consuming process, and thus, many studies have focused on prioritizing test cases to help maintain the effectiveness of testing while reducing the need for a full execution. This paper describes a novel prioritization method that combines Reinforcement Learning and interaction coverage testing concepts. While Reinforcement Learning has been found to be suitable for rapid changing projects with abundant historical data, interaction coverage considers in depth the event-based aspects of user interface testing and provides a granular level at which the Reinforcement Learning system can gain more insights into individual test cases. We experiment and assess the proposed method using five data sets, finding that the method outperforms related methods and has the potential to be used in practice.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {56–65},
numpages = {10},
keywords = {test prioritization, automation testing, reinforcement learning},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@inproceedings{10.1145/3351917.3351989,
author = {Cao, LiChun and ZhiMin},
title = {An Overview of Deep Reinforcement Learning},
year = {2019},
isbn = {9781450371865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351917.3351989},
doi = {10.1145/3351917.3351989},
abstract = {As a new machine learning method, deep reinforcement learning has made important progress in various fields of people's production and life since it was proposed. However, there are still many difficulties in function design and other aspects. Therefore, further research on deep reinforcement learning is of great significance for promoting the progress of the whole science and society. Based on the basic theory of deep learning, this paper introduces the basic theory, research method, main network model and successful application in various fields of deep reinforcement learning.},
booktitle = {Proceedings of the 2019 4th International Conference on Automation, Control and Robotics Engineering},
articleno = {17},
numpages = {9},
keywords = {Deep learning, machine learning, Deep reinforcement learning, method},
location = {Shenzhen, China},
series = {CACRE2019}
}

@inproceedings{10.5555/3400397.3400631,
author = {Shitole, Vivswan and Louis, Joseph and Tadepalli, Prasad},
title = {Optimizing Earth Moving Operations via Reinforcement Learning},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Earth moving operations are a critical component of construction and mining industries with a lot of potential for optimization and improved productivity. In this paper we combine discrete event simulation with reinforcement learning (RL) and neural networks to optimize these operations that tend to be cyclical and equipment-intensive. One advantage of RL is that it can learn near-optimal policies from the simulators with little human guidance. We compare three different RL methods including Q-learning, actor-critic, and trust region policy optimization and show that they all converge to significantly better policies than human-designed heuristics. We conclude that RL is a promising approach to automate and optimize earth moving and other similar expensive operations in construction, mining, and manufacturing industries.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2954–2965},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/3299815.3314427,
author = {Govindaiah, Swetha and Petty, Mikel D.},
title = {Applying Reinforcement Learning to Plan Manufacturing Material Handling Part 2: Experimentation and Results},
year = {2019},
isbn = {9781450362511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299815.3314427},
doi = {10.1145/3299815.3314427},
abstract = {Applying machine learning to improve the efficiency of complex manufacturing processes, particularly logistics and material handling, can be a challenging problem. The interconnectedness of the multiple components that compose such processes and the typically large number of variables required to specify procedures and plans within those processes combine to make it very difficult to map the details of real-world manufacturing processes to an abstract mathematical representation suitable for machine learning methods. In this paper, we report on the application of machine learning methods, in particular reinforcement learning, to generate increasingly efficient plans for material handling to satisfy temporally varying product demands in a representative manufacturing facility. The essential steps in the research included defining a formal representation of a realistically complex material handling plan, defining a set of suitable two-stage plan change operators as reinforcement learning actions, implementing a simulation-based multi-objective reward function that considers multiple components of material handling costs, and abstracting the many possible material handling plans into a state set small enough to enable reinforcement learning. Extensive experimentation with multiple starting plans showed that the reinforcement learning process could consistently reduce the material handling plans' costs over time. This work may be one of the first applications of reinforcement learning with a multi-objective reward function to a realistically complex material handling process. This paper first provides an explanation of how the material handling plans and rewards were abstracted into a manageable state set. It then details the various initial plans and experimental trials used to test the plans. Finally, it reports the results of those experimental trials, including the plan change policies learned and the reductions in material handling costs achieved.},
booktitle = {Proceedings of the 2019 ACM Southeast Conference},
pages = {16–23},
numpages = {8},
keywords = {multi-objective learning, planning, reinforcement learning, Material handling, machine learning},
location = {Kennesaw, GA, USA},
series = {ACM SE '19}
}

@inproceedings{10.1145/3274247.3274510,
author = {Lee, Jaedong and Won, Jungdam and Lee, Jehee},
title = {Crowd Simulation by Deep Reinforcement Learning},
year = {2018},
isbn = {9781450360159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274247.3274510},
doi = {10.1145/3274247.3274510},
abstract = {Simulating believable virtual crowds has been an important research topic in many research fields such as industry films, computer games, urban engineering, and behavioral science. One of the key capabilities agents should have is navigation, which is reaching goals without colliding with other agents or obstacles. The key challenge here is that the environment changes dynamically, where the current decision of an agent can largely affect the state of other agents as well as the agent in the future. Recently, reinforcement learning with deep neural networks has shown remarkable results in sequential decision-making problems. With the power of convolution neural networks, elaborate control with visual sensory inputs has also become possible. In this paper, we present an agent-based deep reinforcement learning approach for navigation, where only a simple reward function enables agents to navigate in various complex scenarios. Our method is also able to do that with a single unified policy for every scenario, where the scenario-specific parameter tuning is unnecessary. We will show the effectiveness of our method through a variety of scenarios and settings.},
booktitle = {Proceedings of the 11th ACM SIGGRAPH Conference on Motion, Interaction and Games},
articleno = {2},
numpages = {7},
keywords = {animation, reinforcement learning, collision avoidance, crowd simulation},
location = {Limassol, Cyprus},
series = {MIG '18}
}

@inproceedings{10.5555/2936924.2937239,
author = {Narvekar, Sanmit},
title = {Curriculum Learning in Reinforcement Learning: (Doctoral Consortium)},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Transfer learning in reinforcement learning is an area of research that seeks to speed up or improve learning of a complex target task, by leveraging knowledge from one or more source tasks. This thesis will extend the concept of transfer learning to curriculum learning, where the goal is to design a sequence of source tasks for an agent to train on, such that final performance or learning speed is improved. We discuss completed work on this topic, including an approach for modeling transferability between tasks, and methods for semi-automatically generating source tasks tailored to an agent and the characteristics of a target domain. Finally, we also present ideas for future work.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {1528–1529},
numpages = {2},
keywords = {curriculum learning, reinforcement learning, transfer learning},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.1145/1362622.1362666,
author = {Bar-Hillel, Aharon and Di-Nur, Amir and Ein-Dor, Liat and Gilad-Bachrach, Ran and Ittach, Yossi},
title = {Workstation Capacity Tuning Using Reinforcement Learning},
year = {2007},
isbn = {9781595937643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1362622.1362666},
doi = {10.1145/1362622.1362666},
abstract = {Computer grids are complex, heterogeneous, and dynamic systems, whose behavior is governed by hundreds of manually-tuned parameters. As the complexity of these systems grows, automating the procedure of parameter tuning becomes indispensable. In this paper, we consider the problem of auto-tuning server capacity, i.e. the number of jobs a server runs in parallel. We present three different reinforcement learning algorithms, which generate a dynamic policy by changing the number of concurrent running jobs according to the job types and machine state. The algorithms outperform manually-tuned policies for the entire range of checked workloads, with average throughput improvement greater than 20\%. On multi-core servers, the average throughput improvement is approximately 40\%, which hints at the enormous improvement potential of such a tuning mechanism with the gradual transition to multi-core machines.},
booktitle = {Proceedings of the 2007 ACM/IEEE Conference on Supercomputing},
articleno = {32},
numpages = {11},
location = {Reno, Nevada},
series = {SC '07}
}

@article{10.5555/3322706.3361995,
author = {H\"{u}ttenrauch, Maximilian and \v{S}o\v{s}i\'{c}, Adrian and Neumann, Gerhard},
title = {Deep Reinforcement Learning for Swarm Systems},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, the observation vector for decentralized decision making is represented by a concatenation of the (local) information an agent gathers about other agents. However, concatenation scales poorly to swarm systems with a large number of homogeneous agents as it does not exploit the fundamental properties inherent to these systems: (i) the agents in the swarm are interchangeable and (ii) the exact number of agents in the swarm is irrelevant. Therefore, we propose a new state representation for deep multi-agent RL based on mean embeddings of distributions, where we treat the agents as samples and use the empirical mean embedding as input for a decentralized policy. We define different feature spaces of the mean embedding using histograms, radial basis functions and neural networks trained end-to-end. We evaluate the representation on two well-known problems from the swarm literature--rendezvous and pursuit evasion--in a globally and locally observable setup. For the local setup we furthermore introduce simple communication protocols. Of all approaches, the mean embedding representation using neural network features enables the richest information exchange between neighboring agents, facilitating the development of complex collective strategies.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1966–1996},
numpages = {31},
keywords = {multi-agent systems, neural networks, deep reinforcement learning, mean embeddings, swarm systems}
}

@inproceedings{10.1145/3472410.3472437,
author = {Orozov, Bozhan and Orozova, Daniela},
title = {Rule-Based System Against Reinforcement Learning*},
year = {2021},
isbn = {9781450389822},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472410.3472437},
doi = {10.1145/3472410.3472437},
abstract = {Reinforcement Learning is becoming an increasingly popular type of machine learning. In it, artificial intelligence learns what the best action in each given situation is and over time optimizes the decisions it makes. On the other hand, provided that sufficiently good rules are created, a Rule-Based System is a program task that can support very complex behavior. The aim of the authors in this study is to analyze and compare the behavior of two agents, realized on the basis of these two approaches.},
booktitle = {Proceedings of the 22nd International Conference on Computer Systems and Technologies},
pages = {67–70},
numpages = {4},
keywords = {Intelligent Agents, Artificial Intelligence, Agent-Oriented Programming, Inference Engine},
location = {Ruse, Bulgaria},
series = {CompSysTech '21}
}

@inproceedings{10.5555/3091125.3091320,
author = {\v{S}o\v{s}i\'{c}, Adrian and KhudaBukhsh, Wasiur R. and Zoubir, Abdelhak M. and Koeppl, Heinz},
title = {Inverse Reinforcement Learning in Swarm Systems},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Inverse reinforcement learning (IRL) has become a useful tool for learning behavioral models from demonstration data. However, IRL remains mostly unexplored for multi-agent systems. In this paper, we show how the principle of IRL can be extended to homogeneous large-scale problems, inspired by the collective swarming behavior of natural systems. In particular, we make the following contributions to the field: 1) We introduce the swarMDP framework, a sub-class of decentralized partially observable Markov decision processes endowed with a swarm characterization. 2) Exploiting the inherent homogeneity of this framework, we reduce the resulting multi-agent IRL problem to a single-agent one by proving that the agent-specific value functions in this model coincide. 3) To solve the corresponding control problem, we propose a novel heterogeneous learning scheme that is particularly tailored to the swarm setting. Results on two example systems demonstrate that our framework is able to produce meaningful local reward models from which we can replicate the observed global system dynamics.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {1413–1421},
numpages = {9},
keywords = {multi-agent systems, inverse reinforcement learning, swarms},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@inproceedings{10.1145/3299815.3314451,
author = {Govindaiah, Swetha and Petty, Mikel D.},
title = {Applying Reinforcement Learning to Plan Manufacturing Material Handling Part 1: Background and Formal Problem Specification},
year = {2019},
isbn = {9781450362511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299815.3314451},
doi = {10.1145/3299815.3314451},
abstract = {Applying machine learning to improve the efficiency of complex manufacturing processes, particularly logistics and material handling, can be a challenging problem. The interconnectedness of the multiple components that compose such processes and the typically large number of variables required to specify procedures and plans within those processes combine to make it very difficult to map the details of real-world manufacturing processes to an abstract mathematical representation suitable for machine learning methods. In this paper, we report on the application of machine learning methods, in particular reinforcement learning, to generate increasingly efficient plans for material handling to satisfy temporally varying product demands in a representative manufacturing facility. The essential steps in the research included defining a formal representation of a realistically complex material handling plan, defining a set of suitable two-stage plan change operators as reinforcement learning actions, implementing a simulation-based multi-objective reward function that considers multiple components of material handling costs, and abstracting the many possible material handling plans into a state set small enough to enable reinforcement learning. Extensive experimentation with multiple starting plans showed that the reinforcement learning process could consistently reduce the material handling plans' costs over time. This work may be one of the first applications of reinforcement learning with a multi-objective reward function to a realistically complex material handling process. This paper first provides brief background on material handling and reinforcement learning. It then details the formal mathematical representation of a realistic material handling plan for a manufacturing facility and specifies the reinforcement learning operators to be applied to the plan representation. This paper (part 1 of 2) is the first of two companion papers; the second describes the experimentation and results [1].},
booktitle = {Proceedings of the 2019 ACM Southeast Conference},
pages = {168–171},
numpages = {4},
keywords = {Reinforcement Learning, Material Handling, Machine Learning, Multi-Objective Learning, Planning},
location = {Kennesaw, GA, USA},
series = {ACM SE '19}
}

@article{10.5555/944919.944955,
author = {Perkins, Theodore J. and Barto, Andrew G.},
title = {Lyapunov Design for Safe Reinforcement Learning},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {Lyapunov design methods are used widely in control engineering to design controllers that achieve qualitative objectives, such as stabilizing a system or maintaining a system's state in a desired operating range. We propose a method for constructing safe, reliable reinforcement learning agents based on Lyapunov design principles. In our approach, an agent learns to control a system by switching among a number of given, base-level controllers. These controllers are designed using Lyapunov domain knowledge so that any switching policy is safe and enjoys basic performance guarantees. Our approach thus ensures qualitatively satisfactory agent behavior for virtually any reinforcement learning algorithm and at all times, including while the agent is learning and taking exploratory actions. We demonstrate the process of designing safe agents for four different control problems. In simulation experiments, we find that our theoretically motivated designs also enjoy a number of practical benefits, including reasonable performance initially and throughout learning, and accelerated learning.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {803–832},
numpages = {30}
}

@inproceedings{10.1145/3543507.3583467,
author = {Liu, Ziru and Tian, Jiejie and Cai, Qingpeng and Zhao, Xiangyu and Gao, Jingtong and Liu, Shuchang and Chen, Dayou and He, Tonghao and Zheng, Dong and Jiang, Peng and Gai, Kun},
title = {Multi-Task Recommendations with Reinforcement Learning},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583467},
doi = {10.1145/3543507.3583467},
abstract = {In recent years, Multi-task Learning (MTL) has yielded immense success in Recommender System (RS) applications [40]. However, current MTL-based recommendation models tend to disregard the session-wise patterns of user-item interactions because they are predominantly constructed based on item-wise datasets. Moreover, balancing multiple objectives has always been a challenge in this field, which is typically avoided via linear estimations in existing works. To address these issues, in this paper, we propose a Reinforcement Learning (RL) enhanced MTL framework, namely RMTL, to combine the losses of different recommendation tasks using dynamic weights. To be specific, the RMTL structure can address the two aforementioned issues by (i) constructing an MTL environment from session-wise interactions and (ii) training multi-task actor-critic network structure, which is compatible with most existing MTL-based recommendation models, and (iii) optimizing and fine-tuning the MTL loss function using the weights generated by critic networks. Experiments on two real-world public datasets demonstrate the effectiveness of RMTL with a higher AUC against state-of-the-art MTL-based recommendation models. Additionally, we evaluate and validate RMTL’s compatibility and transferability across various MTL models.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1273–1282},
numpages = {10},
keywords = {Recommendation, Multi-task Learning, Reinforcement Learning},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.5555/3545946.3599126,
author = {Gajcin, Jasmina},
title = {Counterfactual Explanations for Reinforcement Learning Agents},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reinforcement learning (RL) algorithms often use neural networks to represent agent's policy, making them difficult to interpret. Counterfactual explanations are human-friendly explanations which offer users actionable advice on how to change their features to obtain a desired output from a black-box model. However, methods for generating counterfactuals in RL ignore the stochastic and sequential nature of RL tasks, and can generate counterfactuals which are difficult to obtain, affecting user effort and trust. My dissertation focuses on developing methods that take into account the complexities of RL framework and provide counterfactual explanations that are easy to reach and confidently produce the desired output},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2925–2927},
numpages = {3},
keywords = {contrastive explanations, reinforcement learning, explainability, causality, counterfactual explanations},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.1145/3605950,
author = {Hahn, Ernst Moritz and Perez, Mateo and Schewe, Sven and Somenzi, Fabio and Trivedi, Ashutosh and Wojtczak, Dominik},
title = {Multi-Objective ω-Regular Reinforcement Learning},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {2},
issn = {0934-5043},
url = {https://doi.org/10.1145/3605950},
doi = {10.1145/3605950},
abstract = {The expanding role of reinforcement learning (RL) in safety-critical system design has promoted ω-automata as a way to express learning requirements—often non-Markovian—with greater ease of expression and interpretation than scalar reward signals. However, real-world sequential decision making situations often involve multiple, potentially conflicting, objectives. Two dominant approaches to express relative preferences over multiple objectives are: (1) weighted preference, where the decision maker provides scalar weights for various objectives, and (2) lexicographic preference, where the decision maker provides an order over the objectives such that any amount of satisfaction of a higher-ordered objective is preferable to any amount of a lower-ordered one. In this article, we study and develop RL algorithms to compute optimal strategies in Markov decision processes against multiple ω-regular objectives under weighted and lexicographic preferences. We provide a translation from multiple ω-regular objectives to a scalar reward signal that is both faithful (maximising reward means maximising probability of achieving the objectives under the corresponding preference) and effective (RL quickly converges to optimal strategies). We have implemented the translations in a formal reinforcement learning tool, Mungojerrie, and we present an experimental evaluation of our technique on benchmark learning problems.},
journal = {Form. Asp. Comput.},
month = {jul},
articleno = {12},
numpages = {24},
keywords = {weighted preference, Multi-objective reinforcement learning, lexicographic preference, ω-regular objectives, automata-theoretic reinforcement learning}
}

@inproceedings{10.1109/WI-IAT.2010.160,
author = {Garcia-Pardo, Juan A. and Soler, J. and Carrascosa, C.},
title = {Social Reinforcement Learning for Changing Environments},
year = {2010},
isbn = {9780769541914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2010.160},
doi = {10.1109/WI-IAT.2010.160},
abstract = {If we imagine a dynamic environment whose behavior may change in time we can figure out the difficulties that agents located there will have trying to solve problems related to this environment. Changes in an environment e.g. a market, can be quite drastic: from changing the dependencies of some products to add new actions to build new products. The agents should try to cooperate or compete against others, when appropriated, to reach their goals faster than in an individual fashion, showing an always desirable emergent behavior. In this paper a reinforcement learning method proposal, guided by social interaction between agents, is presented. The proposal aims to show that adaptation is performed independently by the society where which these AI-controlled players belong, without explicitly reporting that changes have occurred by a central authority, or even by trying to recognize those changes.},
booktitle = {Proceedings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {269–272},
numpages = {4},
keywords = {Reinforcement Learning, Emergent Behavior, Multi-Agent Systems, Social Agents, Multi-Agent Learning},
series = {WI-IAT '10}
}

@inproceedings{10.1109/IAT.2007.75,
author = {Ribeiro, Richardson and Koerich, Alessandro L. and Enembreck, Fabricio},
title = {Noise Tolerance in Reinforcement Learning Algorithms},
year = {2007},
isbn = {0769530273},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IAT.2007.75},
doi = {10.1109/IAT.2007.75},
abstract = {This paper proposes a mechanism of noise tolerance for reinforcement learning algorithms. An adaptive agent that employs reinforcement learning algorithms may receive and accumulate many rewards for its actions. However, the amount of rewards received by the agent is not a guarantee of convergence to an optimal policy of action due to the noises produced by the environment. Therefore, we propose a noise tolerance mechanism which is able to estimate convergent policies without causing delays or an unexpected speedup in the agent's learning. Experimental results have shown that the proposed mechanism is able to speed up the convergence of the agent achieving good action policies very fast even in dynamic and noisy environments.},
booktitle = {Proceedings of the 2007 IEEE/WIC/ACM International Conference on Intelligent Agent Technology},
pages = {265–268},
numpages = {4},
keywords = {Adaptive Autonomous Agents, Reinforcement Learning and Noise Tolerant Learning.},
series = {IAT '07}
}

@inproceedings{10.1145/3350546.3352501,
author = {den Hengst, Floris and Hoogendoorn, Mark and van Harmelen, Frank and Bosman, Joost},
title = {Reinforcement Learning for Personalized Dialogue Management},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352501},
doi = {10.1145/3350546.3352501},
abstract = {Language systems have been of great interest to the research community and have recently reached the mass market through various assistant platforms on the web. Reinforcement Learning methods that optimize dialogue policies have seen successes in past years and have recently been extended into methods that personalize the dialogue, e.g. take the personal context of users into account. These works, however, are limited to personalization to a single user with whom they require multiple interactions and do not generalize the usage of context across users. This work introduces a problem where a generalized usage of context is relevant and proposes two Reinforcement Learning (RL)-based approaches to this problem. The first approach uses a single learner and extends the traditional POMDP formulation of dialogue state with features that describe the user context. The second approach segments users by context and then employs a learner per context. We compare these approaches in a benchmark of existing non-RL and RL-based methods in three established and one novel application domain of financial product recommendation. We compare the influence of context and training experiences on performance and find that learning approaches generally outperform a handcrafted gold standard.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {59–67},
numpages = {9},
keywords = {Personalization, Recommendation, Dialogue Management, Adaptive Virtual Assistants, Reinforcement Learning},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@article{10.5555/1248547.1248578,
author = {Whiteson, Shimon and Stone, Peter},
title = {Evolutionary Function Approximation for Reinforcement Learning},
year = {2006},
issue_date = {12/1/2006},
publisher = {JMLR.org},
volume = {7},
issn = {1532-4435},
abstract = {Temporal difference methods are theoretically grounded and empirically effective methods for addressing reinforcement learning problems. In most real-world reinforcement learning tasks, TD methods require a function approximator to represent the value function. However, using function approximators requires manually making crucial representational decisions. This paper investigates evolutionary function approximation, a novel approach to automatically selecting function approximator representations that enable efficient individual learning. This method evolves individuals that are better able to learn. We present a fully implemented instantiation of evolutionary function approximation which combines NEAT, a neuroevolutionary optimization technique, with Q-learning, a popular TD method. The resulting NEAT+Q algorithm automatically discovers effective representations for neural network function approximators. This paper also presents on-line evolutionary computation, which improves the on-line performance of evolutionary computation by borrowing selection mechanisms used in TD methods to choose individual actions and using them in evolutionary computation to select policies for evaluation. We evaluate these contributions with extended empirical studies in two domains: 1) the mountain car task, a standard reinforcement learning benchmark on which neural network function approximators have previously performed poorly and 2) server job scheduling, a large probabilistic domain drawn from the field of autonomic computing. The results demonstrate that evolutionary function approximation can significantly improve the performance of TD methods and on-line evolutionary computation can significantly improve evolutionary methods. This paper also presents additional tests that offer insight into what factors can make neural network function approximation difficult in practice.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {877–917},
numpages = {41}
}

@inproceedings{10.5555/3398761.3399018,
author = {Kumar, Pankaj},
title = {Deep Reinforcement Learning for Market Making},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Market Making is high frequency trading strategy in which an agent provides liquidity simultaneously quoting a bid price and an ask price on an asset. Market Makers reaps profits in the form of the spread between the quoted price placed on the buy and sell prices. Due to complexity in inventory risk, counterparties to trades and information asymmetry, understanding of market making algorithms is relatively unexplored by academicians across disciplines. In this paper, we develop realistic simulations of limit order markets and use it to design a market making agent using Deep Recurrent Q-Networks. Our approach outperforms a prominent benchmark strategy from literature, which uses temporal-difference reinforcement learning to design market maker agents. The agents successfully reproduce stylized facts in historical trade data from each simulation.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1892–1894},
numpages = {3},
keywords = {high frequecy tading stratergies, limit order books, agent based models, market making, deep reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3383455.3422519,
author = {Vadori, Nelson and Ganesh, Sumitra and Reddy, Prashant and Veloso, Manuela},
title = {Risk-Sensitive Reinforcement Learning: A Martingale Approach to Reward Uncertainty},
year = {2021},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383455.3422519},
doi = {10.1145/3383455.3422519},
abstract = {We introduce a novel framework to account for sensitivity to rewards uncertainty in sequential decision-making problems. While risk-sensitive formulations for Markov decision processes studied so far focus on the distribution of the cumulative reward as a whole, we aim at learning policies sensitive to the uncertain/stochastic nature of the rewards, which has the advantage of being conceptually more meaningful in some cases. To this end, we present a new decomposition of the randomness contained in the cumulative reward based on the Doob decomposition of a stochastic process, and introduce a new conceptual tool - the chaotic variation - which can rigorously be interpreted as the risk measure of the martingale component associated to the cumulative reward process. We innovate on the reinforcement learning side by incorporating this new risk-sensitive approach into model-free algorithms, both policy gradient and value function based, and illustrate its relevance on grid world and portfolio optimization problems.},
booktitle = {Proceedings of the First ACM International Conference on AI in Finance},
articleno = {28},
numpages = {9},
keywords = {policy gradient algorithms, reinforcement learning, risk-sensitive, reward uncertainty, risk-sensitive reinforcement learning, doob decomposition, martingale, actor-critic algorithms, markov decision process},
location = {New York, New York},
series = {ICAIF '20}
}

@inproceedings{10.1145/301136.301226,
author = {Sun, Ron and Sessions, Chad},
title = {Bidding in Reinforcement Learning: A Paradigm for Multi-Agent Systems},
year = {1999},
isbn = {158113066X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/301136.301226},
doi = {10.1145/301136.301226},
booktitle = {Proceedings of the Third Annual Conference on Autonomous Agents},
pages = {344–345},
numpages = {2},
location = {Seattle, Washington, USA},
series = {AGENTS '99}
}

@inproceedings{10.1145/3394486.3403231,
author = {Siddique, A. B. and Oymak, Samet and Hristidis, Vagelis},
title = {Unsupervised Paraphrasing via Deep Reinforcement Learning},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403231},
doi = {10.1145/3394486.3403231},
abstract = {Paraphrasing is expressing the meaning of an input sentence in different wording while maintaining fluency (i.e., grammatical and syntactical correctness). Most existing work on paraphrasing use supervised models that are limited to specific domains (e.g., image captions). Such models can neither be straightforwardly transferred to other domains nor generalize well, and creating labeled training data for new domains is expensive and laborious. The need for paraphrasing across different domains and the scarcity of labeled training data in many such domains call for exploring unsupervised paraphrase generation methods. We propose Progressive Unsupervised Paraphrasing (PUP): a novel unsupervised paraphrase generation method based on deep reinforcement learning (DRL). PUP uses a variational autoencoder (trained using a non-parallel corpus) to generate a seed paraphrase that warm-starts the DRL model. Then, PUP progressively tunes the seed paraphrase guided by our novel reward function which combines semantic adequacy, language fluency, and expression diversity measures to quantify the quality of the generated paraphrases in each iteration without needing parallel sentences. Our extensive experimental evaluation shows that PUP outperforms unsupervised state-of-the-art paraphrasing techniques in terms of both automatic metrics and user studies on four real datasets. We also show that PUP outperforms domain-adapted supervised algorithms on several datasets. Our evaluation also shows that PUP achieves a great trade-off between semantic similarity and diversity of expression.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {1800–1809},
numpages = {10},
keywords = {deep reinforcement learning, natural language processing, unsupervised paraphrasing, natural language generation},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.5555/3535850.3535963,
author = {Pan, Xinlei and Xiao, Chaowei and He, Warren and Yang, Shuang and Peng, Jian and Sun, Mingjie and Liu, Mingyan and Li, Bo and Song, Dawn},
title = {Characterizing Attacks on Deep Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent studies show that Deep Reinforcement Learning (DRL) models are vulnerable to adversarial attacks, which attack DRL models by adding small perturbations to the observations. However, some attacks assume full availability of the victim model, and some require a huge amount of computation, making them less feasible for real world applications. In this work, we make further explorations of the vulnerabilities of DRL by studying other aspects of attacks on DRL using realistic and efficient attacks. First, we adapt and propose efficient black-box attacks when we do not have access to DRL model parameters. Second, to address the high computational demands of existing attacks, we introduce efficient online sequential attacks that exploit temporal consistency across consecutive steps. Third, we explore the possibility of an attacker perturbing other aspects in the DRL setting, such as the environment dynamics. Finally, to account for imperfections in how an attacker would inject perturbations in the physical world, we devise a method for generating a robust physical perturbations to be printed. The attack is evaluated on a real-world robot under various conditions. We conduct extensive experiments both in simulation such as Atari games, robotics and autonomous driving, and on real-world robotics, to compare the effectiveness of the proposed attacks with baseline approaches. To the best of our knowledge, we are the first to apply adversarial attacks on DRL systems to physical robots.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1010–1018},
numpages = {9},
keywords = {reinforcement learning, robotics, adversarial machine learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3472735.3473390,
author = {Fr\"{o}hlich, Piotr and Gelenbe, Erol and Nowak, Mateusz},
title = {Reinforcement Learning and Energy-Aware Routing},
year = {2021},
isbn = {9781450386340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472735.3473390},
doi = {10.1145/3472735.3473390},
abstract = {We present an approach that uses Reinforcement Learning (RL) with the Random Neural Network (RNN) acting as an adaptive critic, to route traffic in a SDN network, so as to minimize a composite Goal function that includes both packet delay and energy consumption per packet. We directly measure the traffic dependent energy consumption characeristics of the hardware that we use (including energy expended per packet) so as to parametrize the Goal function. The RL based algorithm with the RNN is implemented in a SDN controller that manages a multi-hop network which assigns service requests to specific servers so as to minimize the desired Goal. The overall system's performance is evaluated through experimental measurements of packet delay and energy consumption under different traffic load values, demonstrating the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 4th FlexNets Workshop on Flexible Networks Artificial Intelligence Supported Network Flexibility and Agility},
pages = {26–31},
numpages = {6},
keywords = {reinforcement learning, machine learning, quality of service, energy consumption, optimization, smart networks, neural networks},
location = {Virtual Event, USA},
series = {FlexNets '21}
}

@inproceedings{10.5555/3306127.3331818,
author = {Arora, Saurabh and Doshi, Prashant and Banerjee, Bikramjit},
title = {Online Inverse Reinforcement Learning Under Occlusion},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Inverse reinforcement learning (IRL) is the problem of learning the preferences of an agent from observing its behavior on a task. While this problem is witnessing sustained attention, the related problem of online IRL - where the observations are incrementally accrued, yet the real-time demands of the application often prohibit a full rerun of an IRL method - has received much less attention. We introduce a formal framework for online IRL, called incremental IRL (I2RL), and a new method that advances maximum entropy IRL with hidden variables, to this setting. Our formal analysis shows that the new method has a monotonically improving performance with more demonstration data, as well as probabilistically bounded error, both under full and partial observability. Experiments in a simulated robotic application, which involves learning under occlusion, show the significantly improved performance of online IRL as compared to both batch IRL and an online imitation learning method.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1170–1178},
numpages = {9},
keywords = {reinforcement learning, robotics, online learning, inverse reinforcement learning, robot learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3243734.3243838,
author = {Heo, Kihong and Lee, Woosuk and Pashakhanloo, Pardis and Naik, Mayur},
title = {Effective Program Debloating via Reinforcement Learning},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243838},
doi = {10.1145/3243734.3243838},
abstract = {Prevalent software engineering practices such as code reuse and the "one-size-fits-all" methodology have contributed to significant and widespread increases in the size and complexity of software. The resulting software bloat has led to decreased performance and increased security vulnerabilities. We propose a system called Chisel to enable programmers to effectively customize and debloat programs. Chisel takes as input a program to be debloated and a high-level specification of its desired functionality. The output is a reduced version of the program that is correct with respect to the specification. Chisel significantly improves upon existing program reduction systems by using a novel reinforcement learning-based approach to accelerate the search for the reduced program and scale to large programs. Our evaluation on a suite of 10 widely used UNIX utility programs each comprising 13-90 KLOC of C source code demonstrates that Chisel is able to successfully remove all unwanted functionalities and reduce attack surfaces. Compared to two state-of-the-art program reducers C-Reduce and Perses, which time out on 6 programs and 2 programs espectively in 12 hours, Chisel runs up to 7.1x and 3.7x faster and finishes on all programs.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {380–394},
numpages = {15},
keywords = {program debloating, reinforcement learning},
location = {Toronto, Canada},
series = {CCS '18}
}

@article{10.1145/3476777,
author = {Zini, Floriano and Le Piane, Fabio and Gaspari, Mauro},
title = {Adaptive Cognitive Training with Reinforcement Learning},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {2160-6455},
url = {https://doi.org/10.1145/3476777},
doi = {10.1145/3476777},
abstract = {Computer-assisted cognitive training can help patients affected by several illnesses alleviate their cognitive deficits or healthy people improve their mental performance. In most computer-based systems, training sessions consist of graded exercises, which should ideally be able to gradually improve the trainee’s cognitive functions. Indeed, adapting the difficulty of the exercises to how individuals perform in their execution is crucial to improve the effectiveness of cognitive training activities. In this article, we propose the use of reinforcement learning (RL) to learn how to automatically adapt the difficulty of computerized exercises for cognitive training. In our approach, trainees’ performance in performed exercises is used as a reward to learn a policy that changes over time the values of the parameters that determine exercise difficulty. We illustrate a method to be initially used to learn difficulty-variation policies tailored for specific categories of trainees, and then to refine these policies for single individuals. We present the results of two user studies that provide evidence for the effectiveness of our method: a first study, in which a student category policy obtained via RL was found to have better effects on the cognitive function than a standard baseline training that adopts a mechanism to vary the difficulty proposed by neuropsychologists, and a second study, demonstrating that adding an RL-based individual customization further improves the training process.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {mar},
articleno = {3},
numpages = {29},
keywords = {adaptivity, personalization, Computerized cognitive training}
}

@inproceedings{10.1145/1089827.1089829,
author = {Kapoor, Aloak and Greiner, Russell},
title = {Reinforcement Learning for Active Model Selection},
year = {2005},
isbn = {1595932089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1089827.1089829},
doi = {10.1145/1089827.1089829},
abstract = {In many practical Machine Learning tasks, there are costs associated with acquiring the feature values of training instances, as well as a hard learning budget which limits the number of feature values that can be purchased. In this budgeted learning scenario, it is important to use an effective "data acquisition policy", that specifies how to spend the budget acquiring training data to produce an accurate classifier. This paper examines a simplified version of this problem, "active model selection" [10]. As this is a Markov decision problem, we consider applying reinforcement learning (RL) techniques to learn an effective spending policy. Despite extensive training, our experiments on various versions of the problem show that the performance of RL techniques is inferior to existing, simpler spending policies.},
booktitle = {Proceedings of the 1st International Workshop on Utility-Based Data Mining},
pages = {17–23},
numpages = {7},
keywords = {data acquisition, learning costs, training costs, budgeted learning},
location = {Chicago, Illinois},
series = {UBDM '05}
}

@article{10.1145/3375714,
author = {Li, Yuhao and Sun, Dan and Lee, Benjamin C.},
title = {Dynamic Colocation Policies with Reinforcement Learning},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3375714},
doi = {10.1145/3375714},
abstract = {We draw on reinforcement learning frameworks to design and implement an adaptive controller for managing resource contention. During runtime, the controller observes the dynamic system conditions and optimizes control policies that satisfy latency targets yet improve server utilization. We evaluate a physical prototype that guarantees 95th percentile latencies for a search engine and improves server utilization by up to 70\%, compared to exclusively reserving servers for interactive services, for varied batch workloads in machine learning.},
journal = {ACM Trans. Archit. Code Optim.},
month = {mar},
articleno = {1},
numpages = {25},
keywords = {adaptive control, Resource contention, machine learning}
}

@inproceedings{10.1145/1143844.1143929,
author = {Nevmyvaka, Yuriy and Feng, Yi and Kearns, Michael},
title = {Reinforcement Learning for Optimized Trade Execution},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143929},
doi = {10.1145/1143844.1143929},
abstract = {We present the first large-scale empirical application of reinforcement learning to the important problem of optimized trade execution in modern financial markets. Our experiments are based on 1.5 years of millisecond time-scale limit order data from NASDAQ, and demonstrate the promise of reinforcement learning methods to market microstructure problems. Our learning algorithm introduces and exploits a natural "low-impact" factorization of the state space.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {673–680},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}

@article{10.1145/3301273,
author = {Koch, William and Mancuso, Renato and West, Richard and Bestavros, Azer},
title = {Reinforcement Learning for UAV Attitude Control},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2378-962X},
url = {https://doi.org/10.1145/3301273},
doi = {10.1145/3301273},
abstract = {Autopilot systems are typically composed of an “inner loop” providing stability and control, whereas an “outer loop” is responsible for mission-level objectives, such as way-point navigation. Autopilot systems for unmanned aerial vehicles are predominately implemented using Proportional-Integral-Derivative&nbsp;(PID) control systems, which have demonstrated exceptional performance in stable environments. However, more sophisticated control is required to operate in unpredictable and harsh environments. Intelligent flight control systems is an active area of research addressing limitations of PID control most recently through the use of reinforcement learning&nbsp;(RL), which has had success in other applications, such as robotics. Yet previous work has focused primarily on using RL at the mission-level controller. In this work, we investigate the performance and accuracy of the inner control loop providing attitude control when using intelligent flight control systems trained with state-of-the-art RL algorithms—Deep Deterministic Policy Gradient, Trust Region Policy Optimization, and Proximal Policy Optimization. To investigate these unknowns, we first developed an open source high-fidelity simulation environment to train a flight controller attitude control of a quadrotor through RL. We then used our environment to compare their performance to that of a PID controller to identify if using RL is appropriate in high-precision, time-critical flight control.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {feb},
articleno = {22},
numpages = {21},
keywords = {machine learning, intelligent control, reinforcement learning, quadcopter, autopilot, PID, Attitude control, UAV, adaptive control}
}

@inproceedings{10.1145/3377929.3389859,
author = {Ha, David},
title = {Neuroevolution for Deep Reinforcement Learning Problems},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3389859},
doi = {10.1145/3377929.3389859},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {404–427},
numpages = {24},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.5555/3535850.3536113,
author = {M\"{u}ller, Robert and Illium, Steffen and Phan, Thomy and Haider, Tom and Linnhoff-Popien, Claudia},
title = {Towards Anomaly Detection in Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Identifying datapoints that substantially differ from normality is the task of anomaly detection (AD). While AD has gained widespread attention in rich data domains such as images, videos, audio and text, it has has been studied less frequently in the context of reinforcement learning (RL). This is due to the additional layer of complexity that RL introduces through sequential decision making. Developing suitable anomaly detectors for RL is of particular importance in safety-critical scenarios where acting on anomalous data could result in hazardous situations. In this work, we address the question of what AD means in the context of RL. We found that current research trains and evaluates on overly simplistic and unrealistic scenarios which reduce to classic pattern recognition tasks. We link AD in RL to various fields in RL such as lifelong RL and generalization. We discuss their similarities, differences, and how the fields can benefit from each other. Moreover, we identify non-stationarity to be one of the key drivers for future research on AD in RL and make a first step towards a more formal treatment of the problem by framing it in terms of the recently introduced block contextual Markov decision process. Finally, we define a list of practical desiderata for future problems.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1799–1803},
numpages = {5},
keywords = {reinforcement learning, AI safety, anomaly detection},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3061639.3062224,
author = {Wei, Tianshu and Wang, Yanzhi and Zhu, Qi},
title = {Deep Reinforcement Learning for Building HVAC Control},
year = {2017},
isbn = {9781450349277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3061639.3062224},
doi = {10.1145/3061639.3062224},
abstract = {Buildings account for nearly 40\% of the total energy consumption in the United States, about half of which is used by the HVAC (heating, ventilation, and air conditioning) system. Intelligent scheduling of building HVAC systems has the potential to significantly reduce the energy cost. However, the traditional rule-based and model-based strategies are often inefficient in practice, due to the complexity in building thermal dynamics and heterogeneous environment disturbances. In this work, we develop a data-driven approach that leverages the deep reinforcement learning (DRL) technique, to intelligently learn the effective strategy for operating the building HVAC systems. We evaluate the performance of our DRL algorithm through simulations using the widely-adopted EnergyPlus tool. Experiments demonstrate that our DRL-based algorithm is more effective in energy cost reduction compared with the traditional rule-based approach, while maintaining the room temperature within desired range.},
booktitle = {Proceedings of the 54th Annual Design Automation Conference 2017},
articleno = {22},
numpages = {6},
location = {Austin, TX, USA},
series = {DAC '17}
}

@inproceedings{10.1145/3474085.3478323,
author = {Ding, Zihan and Yu, Tianyang and Zhang, Hongming and Huang, Yanhua and Li, Guo and Guo, Quancheng and Mai, Luo and Dong, Hao},
title = {Efficient Reinforcement Learning Development with RLzoo},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3478323},
doi = {10.1145/3474085.3478323},
abstract = {Many multimedia developers are exploring for adopting Deep Reinforcement Learning (DRL) techniques in their applications. They however often find such an adoption challenging. Existing DRL libraries provide poor support for prototyping DRL agents (i.e., models), customising the agents, and comparing the performance of DRL agents. As a result, the developers often report low efficiency in developing DRL agents. In this paper, we introduce RLzoo, a new DRL library that aims to make the development of DRL agents efficient. RLzoo provides developers with (i) high-level yet flexible APIs for prototyping DRL agents, and further customising the agents for best performance, (ii) a model zoo where users can import a wide range of DRL agents and easily compare their performance, and (iii) an algorithm that can automatically construct DRL agents with custom components (which are critical to improve agent's performance in custom applications). Evaluation results show that RLzoo can effectively reduce the development cost of DRL agents, while achieving comparable performance with existing DRL libraries.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {3759–3762},
numpages = {4},
keywords = {flexibility, deep reinforcement learning, agent configuration},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.5555/3306127.3331670,
author = {Narvekar, Sanmit and Stone, Peter},
title = {Learning Curriculum Policies for Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Curriculum learning in reinforcement learning is a training methodology that seeks to speed up learning of a difficult target task, by first training on a series of simpler tasks and transferring the knowledge acquired to the target task. Automatically choosing a sequence of such tasks (i.e., a curriculum) is an open problem that has been the subject of much recent work in this area. In this paper, we build upon a recent method for curriculum design, which formulates the curriculum sequencing problem as a Markov Decision Process. We extend this model to handle multiple transfer learning algorithms, and show for the first time that a curriculum policy over this MDP can be learned from experience. We explore various representations that make this possible, and evaluate our approach by learning curriculum policies for multiple agents in two different domains. The results show that our method produces curricula that can train agents to perform on a target task as fast or faster than existing methods.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {25–33},
numpages = {9},
keywords = {curriculum learning, transfer learning, reinforcement learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3426826.3426839,
author = {Arques Corrales, Pilar and Aznar Gregori, Fidel},
title = {Swarm AGV Optimization Using Deep Reinforcement Learning},
year = {2020},
isbn = {9781450388344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426826.3426839},
doi = {10.1145/3426826.3426839},
abstract = {Behavior design for Automated Guided Vehicles (AGV) systems is an active research area, fundamental for robotics, industrial systems automation. The rise of machine learning neural systems and deep learning make promising results in a multitude of areas including warehouse environments.In this paper, several different policies will be obtained by using reinforcement learning on a heterogeneous swarm robotic system, applied for solving logistical tasks in Automated Guided Vehicles. More specifically, two different types of agents will be used: the vehicles that collect, transport and deposit their package and the traffic lights that regulate the number of vehicles that circulate on the tracks. The main objective of our work is to learn simultaneously two different control policies, one for each kind of agent.The obtained policies have shown their ability to correctly learn the package transport behavior in addition to balance traffic flow to facilitate agent mobility and avoid collisions. Furthermore, the scalability of the system and the behavior performance for different number of vehicles has been shown.},
booktitle = {Proceedings of the 2020 3rd International Conference on Machine Learning and Machine Intelligence},
pages = {65–69},
numpages = {5},
keywords = {Multi Agent Reinforcement Learning, Deep Reinforcement Learning, Automated Guided Vehicles},
location = {Hangzhou, China},
series = {MLMI '20}
}

@inproceedings{10.1145/301136.301195,
author = {Stone, Peter and Veloso, Manuela},
title = {Team-Partitioned, Opaque-Transition Reinforcement Learning},
year = {1999},
isbn = {158113066X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/301136.301195},
doi = {10.1145/301136.301195},
booktitle = {Proceedings of the Third Annual Conference on Autonomous Agents},
pages = {206–212},
numpages = {7},
location = {Seattle, Washington, USA},
series = {AGENTS '99}
}

@inproceedings{10.5555/3306127.3331673,
author = {Liu, Yang and Zeng, Yifeng and Chen, Yingke and Tang, Jing and Pan, Yinghui},
title = {Self-Improving Generative Adversarial Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The lack of data efficiency and stability is one of the main challenges in end-to-end model free reinforcement learning (RL) methods. Recent researches solve the problem resort to supervised learning methods by utilizing human expert demonstrations, e.g. imitation learning. In this paper we present a novel framework which builds a self-improving process upon a policy improvement operator, which is used as a black box such that it has multiple implementation options for various applications. An agent is trained to iteratively imitate behaviors that are generated by the operator. Hence the agent can learn by itself without domain knowledge from human. We employ generative adversarial networks (GAN) to implement the imitation module in the new framework. We evaluate the framework performance over multiple application domains and provide comparison results in support.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {52–60},
numpages = {9},
keywords = {generative adversarial nets, policy distillation, imitation learning, policy iteration, reinforcement learning, deep learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.5555/2031678.2031728,
author = {Taylor, Matthew E. and Kulis, Brian and Sha, Fei},
title = {Metric Learning for Reinforcement Learning Agents},
year = {2011},
isbn = {0982657161},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {A key component of any reinforcement learning algorithm is the underlying representation used by the agent. While reinforcement learning (RL) agents have typically relied on hand-coded state representations, there has been a growing interest in learning this representation. While inputs to an agent are typically fixed (i.e., state variables represent sensors on a robot), it is desirable to automatically determine the optimal relative scaling of such inputs, as well as to diminish the impact of irrelevant features. This work introduces Holler, a novel distance metric learning algorithm, and combines it with an existing instance-based RL algorithm to achieve precisely these goals. The algorithms' success is highlighted via empirical measurements on a set of six tasks within the mountain car domain.},
booktitle = {The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 2},
pages = {777–784},
numpages = {8},
keywords = {reinforcement learning, learning state representations, autonomous feature selection, distance metric learning},
location = {Taipei, Taiwan},
series = {AAMAS '11}
}

