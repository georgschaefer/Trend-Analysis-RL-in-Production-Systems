@article{LIU2023104477,
title = {Nonlinear relationships in soybean commodities Pairs trading-test by deep reinforcement learning},
journal = {Finance Research Letters},
volume = {58},
pages = {104477},
year = {2023},
issn = {1544-6123},
doi = {https://doi.org/10.1016/j.frl.2023.104477},
url = {https://www.sciencedirect.com/science/article/pii/S1544612323008498},
author = {Jianhe Liu and Luze Lu and Xiangyu Zong and Baao Xie},
keywords = {Pairs trading, DRL, Nonlinear relationships, Soybean futures},
abstract = {The pairs trading strategy involves selecting two highly correlated securities to profit from mean reversion. However, the traditional simple threshold method is subjective, random, and ignores nonlinear relationships. This paper proposes a new cointegration deep reinforcement learning (DRL) pairs trading model applied to Dalian Commodity Exchange futures to capture nonlinear relationships and gain profits. The CA-DRL model outperforms other models in terms of efficiency and performance.}
}
@article{WANG2023101787,
title = {Generative inverse reinforcement learning for learning 2-opt heuristics without extrinsic rewards in routing problems},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {9},
pages = {101787},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101787},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823003415},
author = {Qi Wang and Yongsheng Hao and Jiawei Zhang},
keywords = {Routing problems, Deep reinforcement learning, Generative adversarial networks, 2-opt heuristics, Inverse reinforcement learning},
abstract = {Deep reinforcement learning (DRL) has shown promise in solving challenging combinatorial optimization (CO) problems, such as the traveling salesman problem (TSP) and vehicle routing problem (VRP). However, existing DRL methods rely on manually designed reward functions, which may be inaccurate or unrealistic. Moreover, traditional DRL algorithms suffer from unstable training and sparse reward problems. This paper proposes GIRL (Generative Inverse Reinforcement Learning), a method to learn 2-opt heuristics without explicit extrinsic rewards to address these limitations. GIRL combines generative adversarial networks (GANs) and DRL to learn effective policies and reward functions in a reverse end-to-end fashion, improving generalization capabilities. Furthermore, we introduce a self-attentional policy network tailored for 2-opt heuristics and train the framework using a soft actor-critic algorithm along with a discriminator in the GAN.Extensive experiments on various TSP and VRP instances demonstrate superior performance compared to state-of-the-art methods. Moreover, integrating GANs and DRL enables data-driven reward functions, improving accuracy and realism. Using self-attentional networks and the soft actor-critic algorithm enhances training stability and addresses the sparse reward problem. This work advances reinforcement learning techniques in CO, enabling more accurate and practical optimization methods in real-world applications.}
}
@article{GUO2021479,
title = {UAV navigation in high dynamic environments: A deep reinforcement learning approach},
journal = {Chinese Journal of Aeronautics},
volume = {34},
number = {2},
pages = {479-489},
year = {2021},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2020.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S1000936120302247},
author = {Tong GUO and Nan JIANG and Biyue LI and Xi ZHU and Ya WANG and Wenbo DU},
keywords = {Autonomous vehicles, Deep learning, Motion planning, Navigation, Reinforcement learning, Unmanned Aerial Vehicle (UAV)},
abstract = {Unmanned Aerial Vehicle (UAV) navigation is aimed at guiding a UAV to the desired destinations along a collision-free and efficient path without human interventions, and it plays a crucial role in autonomous missions in harsh environments. The recently emerging Deep Reinforcement Learning (DRL) methods have shown promise for addressing the UAV navigation problem, but most of these methods cannot converge due to the massive amounts of interactive data when a UAV is navigating in high dynamic environments, where there are numerous obstacles moving fast. In this work, we propose an improved DRL-based method to tackle these fundamental limitations. To be specific, we develop a distributed DRL framework to decompose the UAV navigation task into two simpler sub-tasks, each of which is solved through the designed Long Short-Term Memory (LSTM) based DRL network by using only part of the interactive data. Furthermore, a clipped DRL loss function is proposed to closely stack the two sub-solutions into one integral for the UAV navigation problem. Extensive simulation results are provided to corroborate the superiority of the proposed method in terms of the convergence and effectiveness compared with those of the state-of-the-art DRL methods.}
}
@article{SUN2020115660,
title = {Optimal carbon storage reservoir management through deep reinforcement learning},
journal = {Applied Energy},
volume = {278},
pages = {115660},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.115660},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920311569},
author = {Alexander Y. Sun},
keywords = {Reinforcement learning, Multistage decision-making, Deep autoregressive model, Deep Q network, Surrogate modeling, Markov decision process, Geological carbon sequestration},
abstract = {Model-based optimization plays a central role in energy system design and management. The complexity and high-dimensionality of many process-level models, especially those used for geosystem energy exploration and utilization, often lead to formidable computational costs when the dimension of decision space is also large. This work adopts elements of recently advanced deep learning techniques to solve a sequential decision-making problem in applied geosystem management. Specifically, a deep reinforcement learning framework was formed for optimal multiperiod planning, in which a deep Q-learning network (DQN) agent was trained to maximize rewards by learning from high-dimensional inputs and from exploitation of its past experiences. To expedite computation, deep multitask learning was used to approximate high-dimensional, multistate transition functions. Both DQN and deep multitask learning are pattern based. As a demonstration, the framework was applied to optimal carbon sequestration reservoir planning using two different types of management strategies: monitoring only and brine extraction. Both strategies are designed to mitigate potential risks due to pressure buildup. Results show that the DQN agent can identify the optimal policies to maximize the reward for given risk and cost constraints. Experiments also show that knowledge the agent gained from interacting with one environment is largely preserved when deploying the same agent in other similar environments.}
}
@article{GOMES2022100516,
title = {A modeling environment for reinforcement learning in games},
journal = {Entertainment Computing},
volume = {43},
pages = {100516},
year = {2022},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2022.100516},
url = {https://www.sciencedirect.com/science/article/pii/S1875952122000404},
author = {Gilzamir Gomes and Creto A. Vidal and Joaquim B. Cavalcante-Neto and Yuri L.B. Nogueira},
keywords = {Reinforcement Learning, Games, Non-Player Characters, Navigation Problem},
abstract = {Developing Non-Player Characters, i.e., game characters that interact with the game’s environment autonomously with flexibility to experiment different behavior configurations is not a trivial task. Traditionally, this has been done with techniques that limit the complexity of the behavior of Non-Player Characters (NPCs), as in the case of using Navigation Mesh (NavMesh) for navigation behaviors. For this problem, it has been shown that reinforcement learning can be more efficient and flexible than traditional techniques. However, integrating reinforcement learning into current game development tools is laborious, given that a great deal of experimentation and coding is required. For that, we have developed a modeling environment that integrates with a game development tool and allows the direct specification of reward functions and NPC agent components with maximum code reuse and automatic code generation.}
}
@article{FERNANDEZGAUNA2014107,
title = {Reinforcement learning of ball screw feed drive controllers},
journal = {Engineering Applications of Artificial Intelligence},
volume = {30},
pages = {107-117},
year = {2014},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2014.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0952197614000220},
author = {Borja Fernandez-Gauna and Igor Ansoategui and Ismael Etxeberria-Agiriano and Manuel Graña},
keywords = {Reinforcement learning, Feedback control, Ball screw feed drive},
abstract = {Feedback controllers for ball screw feed drives may provide great accuracy in positioning, but have no close analytical solution to derive the desired controller. Reinforcement Learning (RL) is proposed to provide autonomous adaptation and learning of them. The RL paradigm allows different approaches, which are tested in this paper looking for the best suited for the ball screw drivers. Specifically, five algorithms are compared on an accurate simulation model of a commercial device, with and without a noisy disturbance on the state observation values. Benchmark results are provided by a double-loop PID controller, whose parameters have been tuned by a random search optimization. Action-critic methods with continuous action space (Policy-Gradient and CACLA) outperform the PID controller in the computational experiments, encouraging future research.}
}
@article{PARK2023121996,
title = {50% reduction in energy consumption in an actual cold storage facility using a deep reinforcement learning-based control algorithm},
journal = {Applied Energy},
volume = {352},
pages = {121996},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121996},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923013600},
author = {Jong-Whi Park and Young-Min Ju and You-Gwon Kim and Hak-Sung Kim},
keywords = {Reinforcement learning, Deep deterministic policy gradient, Temperature control, Actions, States, Cold storage},
abstract = {This study presents a unique application of a temperature control algorithm, specifically modified deep deterministic policy gradient (DDPG), in an actual 2.8 m2 cold storage facility, contrasting the majority of research that leverages theoretical validations using simulation tools. The primary goal was to minimize energy consumption while maintaining the desired temperature range. To achieve this, thermocouples and a watt-hour meter were installed to collect real-time data on temperature and power consumption, subsequently transmitted to a deep-learning computing and control system for processing. Utilizing the gathered data, the algorithm was trained to simultaneously maintain the temperature and minimize power consumption. The temperature setting served as a control variable, and a deep deterministic policy gradient algorithm was used. A hyperparameter with a dominant influence on learning outcomes was optimized. Furthermore, the algorithm was exposed to various complex scenarios that occur during actual cold storage operations, such as door opening, reinforcing its practical viability. The study findings revealed that our real-world application of the DDPG algorithm significantly reduced energy consumption by 47.64% compared to conventional proportional-integral-derivative control algorithms, whilst maintaining the target temperature range.}
}
@article{HAN2023108686,
title = {An autonomous control technology based on deep reinforcement learning for optimal active power dispatch},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {145},
pages = {108686},
year = {2023},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2022.108686},
url = {https://www.sciencedirect.com/science/article/pii/S0142061522006822},
author = {Xiaoyun Han and Chaoxu Mu and Jun Yan and Zeyuan Niu},
keywords = {Active power dispatch, Renewable energy penetration, Soft actor–critic (SAC), Imitation learning (IL), Lagrange multiplier method, Robustness},
abstract = {The large-scale renewable energy integration has brought challenges to energy management in modern power systems. Due to the strong randomness and volatility of renewable energy, traditional model-based methods may become insufficient for optimal active power dispatch. To tackle the challenge, this paper proposes an autonomous control method based on soft actor–critic (SAC), a deep-reinforcement learning (DRL) strategy recently developed, which provides an optimal solution for active power dispatch without a mathematical model while improving the renewable energy consumption rate under stable operation. A Lagrange multiplier is introduced to the SAC (LM-SAC) to promote algorithm performance in optimal active power dispatch. A pre-trained scheme based on imitation learning (IL-SAC) is also designed to further improve the training efficiency and robustness of the DRL agent. Simulations on the IEEE 118-bus system with the open platform Grid2Op verify that the proposed algorithm effectively achieves better renewable energy consumption rate and robustness compared with existing DRL algorithms.}
}
@article{CUI2023121334,
title = {Dynamic pricing for fast charging stations with deep reinforcement learning},
journal = {Applied Energy},
volume = {346},
pages = {121334},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121334},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923006980},
author = {Li Cui and Qingyuan Wang and Hongquan Qu and Mingshen Wang and Yile Wu and Le Ge},
keywords = {Electric vehicle (EV), Fast charging station (FCST), Dynamic pricing, User satisfaction, deep reinforcement learning (DRL)},
abstract = {With the rapid development of electric vehicles (EVs) and charging infrastructures, the unbalanced utilization rate of fast charging stations (FCSTs) and the long waiting time for charging have aroused considerable attention. The incurred low operation profit of FCSTs and low satisfaction of EVs impose difficulties on the further development of EV industry. Existing literature ignored the influence of real-time charging price changes on traffic flow variation and EV charging determination during the dynamic price regulating process. This paper focuses on solving these crucial issues in the dynamic pricing for FCSTs with deep reinforcement learning (DRL). Firstly, considering the spatial–temporal interactions of different roads, a traffic flow prediction model is proposed based on the LSTM combined with the GNN-FiLM. Then, the Origin-Destination (OD) estimation is used to estimate the charging requirements of EVs based on the predicted traffic flow, and a charging demand prediction method for FCSTs is developed by converting the EV satisfaction into economic costs with different dimensions. Then, the vehicle–road learning environment is built with the Markov decision process (MDP), and a dynamic pricing strategy based on the Deep Deterministic Policy Gradient (DDPG) learning is proposed to achieve the optimal charging prices of FCSTs with maximum operation profit. Moreover, during the learning process, the real-time charging price is renewed based on the predicted charging demand, and the future charging demand is further predicted under the renewed charging price until the optimal price is achieved. Finally, simulation results validate that the proposed dynamic pricing strategy effectively improves the profit of FCSTs, alleviates the road congestion, and improves the users’ satisfaction.}
}
@article{YE2019155,
title = {Automated vehicle’s behavior decision making using deep reinforcement learning and high-fidelity simulation environment},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {107},
pages = {155-170},
year = {2019},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2019.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X19311301},
author = {Yingjun Ye and Xiaohui Zhang and Jian Sun},
keywords = {Automated vehicle, Decision making, Deep reinforcement learning, Reward function},
abstract = {Automated vehicles (AVs) are deemed to be the key element for the intelligent transportation system in the future. Many studies have been made to improve AVs’ ability of environment recognition and vehicle control, while the attention paid to decision making is not enough and the existing decision algorithms are very preliminary. Therefore, a framework of the decision-making training and learning is put forward in this paper. It consists of two parts: the deep reinforcement learning (DRL) training program and the high-fidelity virtual simulation environment. Then the basic microscopic behavior, car-following (CF), is trained within this framework. In addition, theoretical analysis and experiments were conducted to evaluate the proposed reward functions for accelerating training using DRL. The results show that on the premise of driving comfort, the efficiency of the trained AV increases 7.9% and 3.8% respectively compared to the classical adaptive cruise control models, intelligent driver model and constant-time headway policy. Moreover, on a more complex three-lane section, we trained an integrated model combining both CF and lane-changing behavior, with the average speed further growing 2.4%. It indicates that our framework is effective for AV’s decision-making learning.}
}
@article{ZHANG2023103257,
title = {Dynamic spectrum access for Internet-of-Things with hierarchical federated deep reinforcement learning},
journal = {Ad Hoc Networks},
volume = {149},
pages = {103257},
year = {2023},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2023.103257},
url = {https://www.sciencedirect.com/science/article/pii/S1570870523001774},
author = {Songbo Zhang and Kwok-Yan Lam and Bowen Shen and Li Wang and Feng Li},
keywords = {Dynamic spectrum access (DSA), Hierarchical federated learning, Deep reinforcement learning, Internet-of-Things (IoT)},
abstract = {In Internet-of-Things (IoT), blooming IoT terminals along with growing demand for broadband services, such as video stream and virtual reality (VR) applications, have led the enhancement of spectrum efficiency to the research focus in IoT. The technology of dynamic spectrum access or sharing is commonly utilized to address this challenge. With the development of artificial intelligence (AI), there has been an increasing amount of research on dynamic spectrum access based on machine learning models, such as Q-learning, deep reinforcement learning and federated learning. When using federated learning to secure user privacy in dynamic spectrum access, several key challenges such as the lack of personalized model selection and parameter updates, and the difficulty of meeting the needs of different devices with a single global model are still under-investigated. In this paper, the heterogeneity of IoT users is considered and a hierarchical and personalized federated deep reinforcement learning approach is proposed to fulfill dynamic spectrum access. First, IoT users are trained locally by using deep reinforcement learning to obtain optimal rewards and spectrum access rate. Second, considering that users have their personalized characteristics, a client-edge-cloud federated learning framework is proposed to accelerate the model convergence and reduce communication overhead between cloud servers and clients. Accordingly, user parameters are divided into personalized and basic parameters. Basic parameters are individually uploaded for global model training, meanwhile personalized parameters are retained to guarantee the applicability of the model. Lastly, the proposed framework is used for dynamic spectrum access, thereby improving the efficiency of spectrum access and decrease the bandwidth occupied by exploiting the global optimization capabilities for federated learning. Only basic parameters are uploaded to substantially protect user’s privacy. Simulation results show that the proposed framework can improve user communication performances and the accuracy of spectrum access. The average convergency time for IoT users can be reduced by about 40% compared with common federated-learning-oriented dynamic spectrum access.}
}
@article{LEE2004577,
title = {Adaptive state space partitioning for reinforcement learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {17},
number = {6},
pages = {577-588},
year = {2004},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2004.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0952197604000879},
author = {Ivan S.K. Lee and Henry Y.K. Lau},
keywords = {Reinforcement learning, Nearest neighbor quantizer, State space partitioning, Peg-in-hole, Navigation},
abstract = {The convergence property of reinforcement learning has been extensively investigated in the field of machine learning, however, its applications to real-world problems are still constrained due to its computational complexity. A novel algorithm to improve the applicability and efficacy of reinforcement learning algorithms via adaptive state space partitioning is presented. The proposed temporal difference learning with adaptive vector quantization (TD-AVQ) is an online algorithm and does not assume any a priori knowledge with respect to the learning task and environment. It utilizes the information generated from the reinforcement learning algorithms. Therefore, no additional computations on the decisions of how to partition a particular state space are required. A series of simulations are provided to demonstrate the practical values and performance of the proposed algorithms in solving robot motion planning problems.}
}
@article{CARDARILLI2022107749,
title = {An FPGA-based multi-agent Reinforcement Learning timing synchronizer},
journal = {Computers and Electrical Engineering},
volume = {99},
pages = {107749},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107749},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622000581},
author = {Gian Carlo Cardarilli and Luca {Di Nunzio} and Rocco Fazzolari and Daniele Giardino and Marco Re and Andrea Ricci and Sergio Spanò},
keywords = {Reinforcement learning, Machine learning, Multi-agent, Timing recovery, FPGA, Symbol synchronization},
abstract = {In this paper we propose a Timing Recovery Loop for PSK and QAM modulations based on swarm Reinforcement Learning, suitable for FPGA implementation. We apply the Q-RTS algorithm, a hardware-oriented multi-agent version of Q-Learning, to a symbol synchronizer. One agent is in charge to synchronize the In-phase component and a second agent is applied to the Quadrature component. If compared to a loop based on a single-agent Q-Learning, we obtain improved synchronization capabilities in terms of recovery time and immunity to sub-optimal sampling. The Q-RTS timing recovery is up to 3 times faster than its Q-Learning counterpart. The implementation results show a low power consumption and a high throughput allowing the proposed synchronizer to be used in high-speed telecommunications systems.}
}
@article{YI2023119639,
title = {Automated design of search algorithms based on reinforcement learning},
journal = {Information Sciences},
volume = {649},
pages = {119639},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119639},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523012240},
author = {Wenjie Yi and Rong Qu},
keywords = {Automated algorithm design, Reinforcement learning, Vehicle routing problem with time windows},
abstract = {Automated algorithm design has attracted increasing research attention recently in the evolutionary computation community. The main design decisions include selection heuristics and evolution operators in the search algorithms. Most existing studies, however, have focused on the automated design of evolution operators, neglecting selection heuristics for evolution and for replacement, not to mention considering all of the design decisions. This limited the scope of the algorithms under consideration. This study aims to systematically investigate automated design of search algorithms by exploring the impact of individual algorithmic components within a general search framework and the synergy among these multiple algorithmic components utilising a reinforcement learning technique. Comprehensive computational experiments are conducted on different benchmark instances of the capacitated vehicle routing problem with time windows to evaluate the effectiveness and generality of the proposed method. This study contributes to knowledge discovery in automated algorithm design using machine learning towards significantly enhanced generality of search algorithms.}
}
@article{HEIN2018158,
title = {Interpretable policies for reinforcement learning by genetic programming},
journal = {Engineering Applications of Artificial Intelligence},
volume = {76},
pages = {158-169},
year = {2018},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2018.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0952197618301933},
author = {Daniel Hein and Steffen Udluft and Thomas A. Runkler},
keywords = {Interpretable, Reinforcement learning, Genetic programming, Model-based, Symbolic regression, Industrial benchmark},
abstract = {The search for interpretable reinforcement learning policies is of high academic and industrial interest. Especially for industrial systems, domain experts are more likely to deploy autonomously learned controllers if they are understandable and convenient to evaluate. Basic algebraic equations are supposed to meet these requirements, as long as they are restricted to an adequate complexity. Here we introduce the genetic programming for reinforcement learning (GPRL) approach based on model-based batch reinforcement learning and genetic programming, which autonomously learns policy equations from pre-existing default state–action trajectory samples. GPRL is compared to a straightforward method which utilizes genetic programming for symbolic regression, yielding policies imitating an existing well-performing, but non-interpretable policy. Experiments on three reinforcement learning benchmarks, i.e., mountain car, cart–pole balancing, and industrial benchmark, demonstrate the superiority of our GPRL approach compared to the symbolic regression method. GPRL is capable of producing well-performing interpretable reinforcement learning policies from pre-existing default trajectory data.}
}
@article{CHALITA201645,
title = {Reinforcement learning in a bio-connectionist model based in the thalamo-cortical neural circuit},
journal = {Biologically Inspired Cognitive Architectures},
volume = {16},
pages = {45-63},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300159},
author = {Mario Andrés Chalita and Diego Lis and Agustín Caverzasi},
keywords = {Reinforcement learning, Sparse coding, Thalamo-cortical, Cognition, Connectionism, Fuzzy neural network},
abstract = {In a previous study, we presented a program to simulate a particular dynamic of the thalamo-cortical biological system. The method used was called bio-connectionism which linked the thalamo-cortical mechanism reproduced with animal perception. In this presentation, a reinforcement learning program is supported by this mechanism. In a game world designed to test the model developed, the agent is assigned to a character that must learn by trial and error from its own experience upon recognition of aversive and appetitive patterns. The results confirm, support and extend the notion of configuration, a term familiar with sparse coding principles. If, as it is documented, this mechanism observed in sensory areas can be thought as condition of perception, the brain areas taken together – each in its interaction with a respective sub-thalamic nucleus – are suspected to be considered as condition of cognition. We introduce some philosophical questions derived from the experimental results in the discussion section.}
}
@article{DOROKHOVA2021117504,
title = {Deep reinforcement learning control of electric vehicle charging in the presence of photovoltaic generation},
journal = {Applied Energy},
volume = {301},
pages = {117504},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117504},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921008874},
author = {Marina Dorokhova and Yann Martinson and Christophe Ballif and Nicolas Wyrsch},
keywords = {Electric vehicles, EV charging, Model-free control, PV self-consumption, Reinforcement learning, State-of-charge},
abstract = {In recent years, the importance of electric mobility has increased in response to climate change. The fast-growing deployment of electric vehicles (EVs) worldwide is expected to decrease transportation-related CO2 emissions, facilitate the integration of renewables, and support the grid through demand–response services. Simultaneously, inadequate EV charging patterns can lead to undesirable effects in grid operation, such as high peak-loads or low self-consumption of solar electricity, thus calling for novel methods of control. This work focuses on applying deep reinforcement learning (RL) to the EV charging control problem with the objectives to increase photovoltaic self-consumption and EV state of charge at departure. Particularly, we propose mathematical formulations of environments with discrete, continuous, and parametrized action spaces and respective deep RL algorithms to resolve them. The benchmarking of the deep RL control against naive, rule-based, deterministic optimization, and model-predictive control demonstrates that the suggested methodology can produce consistent and employable EV charging strategies, while its performance holds a great promise for real-time implementations.}
}
@article{YUN2023,
title = {Price incentive strategy for the E-scooter sharing service using deep reinforcement learning},
journal = {Journal of Intelligent Transportation Systems},
year = {2023},
issn = {1547-2450},
doi = {https://doi.org/10.1080/15472450.2022.2135437},
url = {https://www.sciencedirect.com/science/article/pii/S1547245023000257},
author = {Hyunsoo Yun and Eui-Jin Kim and Seung Woo Ham and Dong-Kyu Kim},
keywords = {Deep reinforcement learning, electric-scooter (e-scooter), price incentive strategy, resolving imbalance},
abstract = {The electric-scooter (e-scooter) has become a popular mode of transportation with the proliferation of shared mobility services. As with other shared mobility services, the operation of the e-scooter sharing service has a recurring problem of imbalance in supply and demand. Various strategies have been studied to resolve the imbalance problems, including demand prediction and relocation strategies. However, the difficulty of accurately predicting the fluctuating demand and the excessive cost-labor consumption of relocation are major limitations of these strategies. As a remedy, we propose a deep reinforcement learning algorithm that suggests price incentives and an alternative rental location for users who find it difficult to acquire e-scooters at their desired boarding locations. A proximal policy optimization algorithm considering temporal dependencies is applied to develop a reinforcement learning agent that allocates the given initial budget to provide price incentives in a cost-efficient manner. We allow the proposed algorithm to re-use a portion of the operating profit as price incentives, which brings higher efficiency compared to the same initial budget. Our proposed algorithm is capable of reducing as much as 56% of the unmet demands by efficiently distributing price incentives. The result of the geographical analysis shows that the proposed algorithm can provide benefits to both users and service providers by promoting the use of idle e-scooters with a price incentive. Through experimental analysis, optimal budget, i.e., the most efficient initial budget, is suggested, which can contribute to e-scooter operators developing efficient e-scooter sharing services.}
}
@article{KABIR2023120629,
title = {Deep reinforcement learning-based two-timescale Volt-VAR control with degradation-aware smart inverters in power distribution systems},
journal = {Applied Energy},
volume = {335},
pages = {120629},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120629},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922018864},
author = {Farzana Kabir and Nanpeng Yu and Yuanqi Gao and Wenyu Wang},
keywords = {Two-timescale, Volt-VAR control, Smart inverters, High solar PV penetration, Reinforcement learning},
abstract = {Higher penetration of intermittent solar photovoltaic (PV) systems in the distribution grid results in frequent voltage fluctuations. The conventional voltage regulating devices operating on a slow-timescale need to be supplemented with the fast-operating smart inverters with adjustable reactive power setpoints. Complete and accurate information about distribution network topology and line parameters is necessary for conventional model-based Volt-VAR control (VVC) methods. However, such information is often unavailable. To tackle these challenges, a reinforcement learning-based two-timescale VVC algorithm is proposed in this paper that jointly controls the conventional voltage regulating devices at the slow-timescale and the smart inverters at the fast-timescale. Our proposed VVC algorithm simultaneously minimizes voltage violation costs and system operation costs in a model-free manner utilizing historical operational data. Two hierarchically organized agents are set up for the slow-timescale and fast-timescale problems, which are coupled through a communication scheme. The two sets of control policies are learned concurrently by a deep deterministic policy gradient and multi-agent soft actor-critic algorithm respectively. Comprehensive numerical studies performed with the IEEE 123-bus distribution test feeder show that the proposed framework can identify near optimal control actions of voltage regulating devices and smart inverters in real-time operations.}
}
@article{LI2015457,
title = {Continuous probabilistic model building genetic network programming using reinforcement learning},
journal = {Applied Soft Computing},
volume = {27},
pages = {457-467},
year = {2015},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2014.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S156849461400533X},
author = {Xianneng Li and Kotaro Hirasawa},
keywords = {Estimation of distribution algorithm, Probabilistic model building genetic network programming, Continuous optimization, Reinforcement learning},
abstract = {Recently, a novel probabilistic model-building evolutionary algorithm (so called estimation of distribution algorithm, or EDA), named probabilistic model building genetic network programming (PMBGNP), has been proposed. PMBGNP uses graph structures for its individual representation, which shows higher expression ability than the classical EDAs. Hence, it extends EDAs to solve a range of problems, such as data mining and agent control. This paper is dedicated to propose a continuous version of PMBGNP for continuous optimization in agent control problems. Different from the other continuous EDAs, the proposed algorithm evolves the continuous variables by reinforcement learning (RL). We compare the performance with several state-of-the-art algorithms on a real mobile robot control problem. The results show that the proposed algorithm outperforms the others with statistically significant differences.}
}
@article{SMITH20213667,
title = {Propulsionless planar phasing of multiple satellites using deep reinforcement learning},
journal = {Advances in Space Research},
volume = {67},
number = {11},
pages = {3667-3682},
year = {2021},
note = {Satellite Constellations and Formation Flying},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2020.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S0273117720306724},
author = {Brenton Smith and Rasit Abay and Joshua Abbey and Sudantha Balage and Melrose Brown and Russell Boyce},
keywords = {Reinforcement learning, Formation control},
abstract = {This work creates a framework for solving highly non-linear satellite formation control problems by using model-free policy optimisation deep reinforcement learning (DRL) methods. This work considers, believed to be for the first time, DRL methods, such as advantage actor-critic method (A2C) and proximal policy optimisation (PPO), to solve the example satellite formation problem of propellantless planar phasing of multiple satellites. Three degree-of-freedom simulations, including a novel surrogate propagation model, are used to train the deep reinforcement learning agents. During training, the agents actuated their motion through cross-sectional area changes which altered the environmental accelerations acting on them. The DRL framework designed in this work successfully coordinated three spacecraft to achieve a propellantless planar phasing manoeuvre. This work has created a DRL framework that can be used to solve complex satellite formation flying problems, such as planar phasing of multiple satellites and in doing so provides key insights into achieving optimal and robust formation control using reinforcement learning.}
}
@incollection{MALIK2021193,
title = {Chapter 9 - Intelligent Data Analytics for Time-Series Load Forecasting Using Fuzzy Reinforcement Learning (FRL)},
editor = {Hasmat Malik and Nuzhat Fatema and Atif Iqbal},
booktitle = {Intelligent Data-Analytics for Condition Monitoring},
publisher = {Academic Press},
pages = {193-213},
year = {2021},
isbn = {978-0-323-85510-5},
doi = {https://doi.org/10.1016/B978-0-323-85510-5.00009-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323855105000090},
author = {Hasmat Malik and Nuzhat Fatema and Atif Iqbal},
keywords = {forecasting, prediction, vertical power plant, fuzzy reinforcement learning, data analytics},
abstract = {In recent years, load forecasting is becoming more-and-more important due to its numerous applications in the modern power system as well as in the virtual power plant (VPP) by virtue of the real-time analysis of ESS (Energy Storage System), DERs (Distributed Energy Resources), DSM (demand side management), and EVs (Electric vehicles). To overcome the real-time created/generated challenges and ensure accurate, reliable, and stable power generation for continuous time horizons in different time steps (i.e., yearly, monthly, weekly, daily, and hourly, etc.), an advanced intelligent model is proposed by using FQL (fuzzy-Q-learning) based FRL (fuzzy reinforcement learning) approach. In this chapter, a short-term load predictor, which is able to forecast the load for next 24 h, is presented. The proposed approach is tested by using real-time recorded historical data collected from GEFCom2012 and GEFCom2014 and simulated results show accurate and highly satisfactory performance. In this chapter four case studies have been performed for 1 month-ahead, week-ahead, day-ahead, and hour-ahead load forecasting. The proposed approach can predict the load for 1 month, 1 week, 1 day, as well as 1 h in advance, which shows high prediction accuracy with acceptable range of MAPE (mean absolute percentage error) for all four case studies.}
}
@article{SELLAMI2022363,
title = {Deep Reinforcement Learning for energy-aware task offloading in join SDN-Blockchain 5G massive IoT edge network},
journal = {Future Generation Computer Systems},
volume = {137},
pages = {363-379},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22002588},
author = {Bassem Sellami and Akram Hakiri and Sadok {Ben Yahia}},
keywords = {Blockchain, SDN, Task scheduling, IoT, Deep Reinforcement Learning, Fog computing},
abstract = {The Internet-of-Things (IoT) edge allows cloud computing services for topology and location-sensitive distributed computing. As an immediate benefit, it improves network reliability and latency by enabling data access and processing rapidly and efficiently near IoT devices. However, it comes with several issues stemming from the complexity, the security, the energy consumption, and the instability due to the decentralization of service localization. Furthermore, the multi-resource allocation and task scheduling make this task the furthest from being straightforward. Blockchain has been envisioned to enforce trustworthiness in diverse IoT environments. However, high latency and high energy costs are incurred to process IoT transactions. This paper introduces a novel Blockchain-based Deep Reinforcement Learning (DRL) approach to enable energy-aware task scheduling and offloading in an Software Defined Networking (SDN)-enabled IoT network. The Asynchronous Actor–Critic Agent (A3C) DRL-based policy achieves efficient task scheduling and offloading. The latter is in symbiosis with Proof-of-Authority Blockchain consensus to validate IoT transactions and blocks. By doing so, we improve reliability and low latency and achieve energy efficiency for SDN-enabled IoT networks. The A3C policy combined with the Blockchain is proved theoretically. Carried out experiments put forth that our approach offers 50% better energy efficiency, which outperforms traditional consensus algorithms, i.e., Proof of Work and PBFT, in terms of throughput and network latency and offers better scheduling performance.}
}
@article{ZONG2022107960,
title = {Reinforcement learning based framework for COVID-19 resource allocation},
journal = {Computers & Industrial Engineering},
volume = {167},
pages = {107960},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.107960},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222000304},
author = {Kai Zong and Cuicui Luo},
keywords = {COVID-19, Reinforcement Learning, Agent-based Model, Resource Allocation},
abstract = {In this paper, a reinforcement learning based framework is developed for COVID-19 resource allocation. We first construct an agent-based epidemic environment to model the transmission dynamics in multiple states. Then, a multi-agent reinforcement-learning algorithm is proposed based on the time-varying properties of the environment, and the performance of the algorithm is compared with other algorithms. According to the age distribution of populations and their economic conditions, the optimal lockdown resource allocation strategies of Arizona, California, Nevada, and Utah in the United States are determined using the proposed reinforcement-learning algorithm. Experimental results show that the framework can adopt more flexible resource allocation strategies and help decision makers to determine the optimal deployment of limited resources in infection prevention.}
}
@article{WANG2023355,
title = {Reinforcement learning-based cost-efficient service function chaining with CoMP zero-forcing beamforming in edge networks},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {355-368},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003831},
author = {Kan Wang and Xuan Liu and Hongfang Zhou and Dapeng Lan and Zhen Gao and Amir Taherkordi and Yujie Ye and Yuan Gao},
keywords = {Service function chaining, Mobile edge computing, Actor–critic, Zero-forcing beamforming, Proximal center dual decomposition},
abstract = {As two promising paradigms in emerging 6G wireless systems, service function chaining (SFC) and mobile edge computing (MEC) have attracted insensitive attentions from both industry and academia, and would bring more close-proximity services to 6G users with communication, computing and caching (3C) resources, yet also faced with challenges arising in time-varying channel conditions and resource dynamics. In this work, boosted by recent advents in artificial intelligence and reinforcement learning, we investigate the on-line SFC deployment in the edge of 6G wireless systems via the actor–critic learning framework. First, one long-run cost-efficient SFC deployment problem is investigated, and the coordinated multiple points (CoMP)-based zero-forcing beamforming is utilized to cancel the interference across SFCs. Then, by exploiting the Markov decision processes (MDP) property of long-run SFC deployment, one natural gradient-based actor–critic framework is proposed to characterize edge network dynamics, and meanwhile facilitates the training of neural networks to the global optimum. Next, to lower the size of action space, we follow the principle that a subproblem is embedded into each state–action pair’s critic to solve the reward function, and then utilize both the ℓp (0<p<1) norm-based successive convex approximation (SCA) and proximal center-based dual decomposition to approach the global optimum and accelerate the convergence. Finally, numerical results are used to validate proposed actor–critic approach, showing that the communication resource management deserves special attentions in the SFC deployment in the edge of 6G wireless systems.}
}
@article{PHAM2022103463,
title = {Deep reinforcement learning based path stretch vector resolution in dense traffic with uncertainties},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {135},
pages = {103463},
year = {2022},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103463},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21004514},
author = {Duc-Thinh Pham and Phu N. Tran and Sameer Alam and Vu Duong and Daniel Delahaye},
keywords = {Reinforcement learning, Air traffic control, Deep deterministic policy gradient, Conflict resolution, Actor-critic, Learning environment},
abstract = {With the continuous growth in the air transportation demand, air traffic controllers will have to handle increased traffic and consequently, more potential conflicts. This gives rise to the need for conflict resolution advisory tools that can perform well in high-density traffic scenarios given a noisy environment. Unlike model-based approaches, learning-based approaches can take advantage of historical traffic data and flexibly encapsulate environmental uncertainty. In this study, we propose a reinforcement learning approach that is capable of resolving conflicts, in the presence of traffic and inherent uncertainties in conflict resolution maneuvers, without the need for prior knowledge about a set of rules mapping from conflict scenarios to expected actions. The conflict resolution task is formulated as a decision-making problem in a large and complex action space. The research also includes the development of a learning environment, scenario state representation, reward function, and a reinforcement learning algorithm inspired from Q-learning and Deep Deterministic Policy Gradient algorithms. The proposed algorithm, with two stages decision-making process, is used to train an agent that can serves as an advisory tool for air traffic controllers in resolving air traffic conflicts where it can learn from historical data by evolving overtime. Our findings show that the proposed model gives the agent the capability to suggest high quality conflict resolutions under different environmental conditions. It outperforms two baseline algorithms. The trained model has high performance under low uncertainty level (success rate ≥95% ) and medium uncertainty level (success rate ≥87%) with high traffic density. The detailed analysis of different impact factors such as environment’s uncertainty and traffic density on learning performance are investigated and discussed. The environment’s uncertainty is the most important factor which affects the performance. Moreover, the combination of high-density traffic and high uncertainty will be the challenge for any learning models.}
}
@article{KIM2023120038,
title = {Process design and optimization of single mixed-refrigerant processes with the application of deep reinforcement learning},
journal = {Applied Thermal Engineering},
volume = {223},
pages = {120038},
year = {2023},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2023.120038},
url = {https://www.sciencedirect.com/science/article/pii/S1359431123000674},
author = {Sam Kim and Mun-Gi Jang and Jin-Kuk Kim},
keywords = {Deep Q-Network, Deep reinforced learning, Process design, Process optimization, Natural gas liquefaction},
abstract = {Deep reinforcement learning approach is considered for the design and optimization of SMR (Single Mixed Refrigerant) cycles with which the refrigeration power is minimized. Deep Q-Network (DQN) agent is programmed with Python®, which is interacted with a process simulator UniSim Design®, as an environment, through the delivery of decision with MATLAB®. The optimization goal is achieved by designing a reward function such that specific power requirement is minimized, subject to the constraint of minimum temperature difference for the heat transfer. The case study focuses on the design of the SMR cycle for the liquefaction of lean and rich natural gas feeds. GA (Generic Algorithm) optimization framework is also constructed and applied for the case study. Almost the same or similar computational performance between the DQN and the GA methods is observed for finding optimal solutions. For the case study, the specific power required for the liquefaction process is reduced by 15.7% and 13.4% through DQN optimization for the cases of lean and rich feeds, respectively, compared to base cases. The case study clearly demonstrates the applicability of the reinforcement learning which can effectively deal with the optimization problem having complex interactions and high nonlinearities.}
}
@article{DONG2021116928,
title = {Intelligent wind farm control via deep reinforcement learning and high-fidelity simulations},
journal = {Applied Energy},
volume = {292},
pages = {116928},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116928},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921004086},
author = {Hongyang Dong and Jincheng Zhang and Xiaowei Zhao},
keywords = {Wind energy, Wind farm control, Power generation optimization, Deep reinforcement learning, CFD simulation},
abstract = {Wind farms’ power-generation efficiency is constrained by the high system complexity. A novel deep reinforcement learning (RL)-based wind farm control scheme is proposed to handle this challenge and achieve power generation optimization. A reward regularization (RR) module is designed to estimate wind turbines’ normalized power outputs under different yaw settings and uncertain wind conditions, which brings strong robustness and adaptability to the proposed control scheme. The RR module is then combined with the deep deterministic policy gradient algorithm to evaluate the optimal yaw settings for all the wind turbines within the farm. The proposed wind farm control scheme is data-driven and model-free, which addresses the limitations of current approaches, including reliance on accurate analytical/parametric models and lack of adaptability to uncertain wind conditions. In addition, a novel composite learning-based controller for each turbine is designed to achieve closed-loop yaw tracking, which can guarantee the exponential convergence of tracking errors in the presence of uncertainties of yaw actuators. The whole control system can be pre-trained offline and fine-tuned online, providing an easy-to-apply solution with enhanced generality and flexibility for wind farms. High-fidelity simulations with SOWFA (simulator for offshore wind farm applications) and Tensorflow show that the proposed scheme can significantly improve the wind farm’s power generation by exploiting a sparse data set without requiring any wake model.}
}
@article{WEI2020101906,
title = {Optimal policy for structure maintenance: A deep reinforcement learning framework},
journal = {Structural Safety},
volume = {83},
pages = {101906},
year = {2020},
issn = {0167-4730},
doi = {https://doi.org/10.1016/j.strusafe.2019.101906},
url = {https://www.sciencedirect.com/science/article/pii/S0167473019303704},
author = {Shiyin Wei and Yuequan Bao and Hui Li},
keywords = {Bridge maintenance policy, Deep reinforcement learning (DRL), Markov decision process (MDP), Deep Q-network (DQN), Convolutional neural network (CNN)},
abstract = {The cost-effective management of aged infrastructure is an issue of worldwide concern. Markov decision process (MDP) models have been used in developing structural maintenance policies. Recent advances in the artificial intelligence (AI) community have shown that deep reinforcement learning (DRL) has the potential to solve large MDP optimization tasks. This paper proposes a novel automated DRL framework to obtain an optimized structural maintenance policy. The DRL framework contains a decision maker (AI agent) and the structure that needs to be maintained (AI task environment). The agent outputs maintenance policies and chooses maintenance actions, and the task environment determines the state transition of the structure and returns rewards to the agent under given maintenance actions. The advantages of the DRL framework include: (1) a deep neural network (DNN) is employed to learn the state-action Q value (defined as the predicted discounted expectation of the return for consequences under a given state-action pair), either based on simulations or historical data, and the policy is then obtained from the Q value; (2) optimization of the learning process is sample-based so that it can learn directly from real historical data collected from multiple bridges (i.e., big data from a large number of bridges); and (3) a general framework is used for different structure maintenance tasks with minimal changes to the neural network architecture. Case studies for a simple bridge deck with seven components and a long-span cable-stayed bridge with 263 components are performed to demonstrate the proposed procedure. The results show that the DRL is efficient at finding the optimal policy for maintenance tasks for both simple and complex structures.}
}
@article{DONG2022109917,
title = {An approach for automatic parameters evaluation in unconventional oil reservoirs with deep reinforcement learning},
journal = {Journal of Petroleum Science and Engineering},
volume = {209},
pages = {109917},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109917},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521015321},
author = {Peng Dong and Xinwei Liao and Zhiming Chen},
keywords = {Pressure transient analysis, Trilinear flow model, Deep reinforcement learning, Automatic interpretation, Parameter evaluation},
abstract = {Accurate estimation of unconventional reservoir parameters is of great significance to improve the development effect and prolong the life cycle of production wells. Reservoir parameter estimation based on pressure transient analysis (PTA) is a mainstream method due to its ease of use. However, the non-unique solution and human bias make the reliability of the method less than ideal. Therefore, a robust automatic interpretation method is urgently needed to alleviate these problems. In this work, we propose an automatic parameter evaluation method of unconventional reservoirs based on a combination of deep reinforcement learning (DRL) and PTA. Our key insight is to treat the PTA process, namely the pressure derivative curve matching process, as Markov decision process (MDP) and solve the optimal matching policy through DRL algorithm. Based on this idea, we trained an agent to automatically adjust the parameters of the trilinear flow model, a classic PTA model, and finally complete the pressure derivative curve matching to evaluate the unconventional oil reservoirs parameters. To make the training converge, branch deep Q-network with independent rewards strategy (IR-BDQ) was proposed to train the agent. Results show that IR-BDQ algorithm can effectively improve the convergence speed and parameter evaluation accuracy compared with other DRL algorithms. The results of 1000 curve matching tests showed that the mean average relative errors of parameters is 13.1%. In addition, comparison with the supervised learning algorithm reveals that the proposed method has the smallest variance of parameter inversion errors, indicating that the method has good robustness. Finally, the case study shows that the proposed method can effectively alleviate the non-unique solution problem, which is of great significance to improve the repeatability of parameter evaluation results in unconventional oil reservoirs.}
}
@article{BAI2022100047,
title = {Hierarchical policy with deep-reinforcement learning for nonprehensile multiobject rearrangement},
journal = {Biomimetic Intelligence and Robotics},
volume = {2},
number = {3},
pages = {100047},
year = {2022},
issn = {2667-3797},
doi = {https://doi.org/10.1016/j.birob.2022.100047},
url = {https://www.sciencedirect.com/science/article/pii/S2667379722000134},
author = {Fan Bai and Fei Meng and Jianbang Liu and Jiankun Wang and Max Q.-H. Meng},
keywords = {Rearrangement, Reinforcement learning, Monte Carlo tree search},
abstract = {Nonprehensile multiobject rearrangement is the robotic task of planning feasible paths and transferring multiple objects to their predefined target poses without grasping. It must consider how each object reaches the target and the order in which objects move, considerably increasing the complexity of the problem. Thus, we propose a hierarchical policy for nonprehensile multiobject rearrangement based on deep-reinforcement learning. We use imitation learning and reinforcement learning to train a rollout policy. In a high-level policy, the policy network directs the Monte Carlo tree search algorithm to efficiently seek the ideal rearrangement sequence for several items. In a low-level policy, the robot plans the paths according to the order of path primitives and manipulates the objects to approach the target poses one by one. Our experiments show that the proposed method has a higher success rate, fewer steps, and shorter path length than the state-of-the-art methods.}
}
@article{LEI20223506,
title = {Active object tracking of free floating space manipulators based on deep reinforcement learning},
journal = {Advances in Space Research},
volume = {70},
number = {11},
pages = {3506-3519},
year = {2022},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2022.08.041},
url = {https://www.sciencedirect.com/science/article/pii/S0273117722007773},
author = {Wenxiao Lei and Hao Fu and Guanghui Sun},
keywords = {Active object tracking, Free-floating space manipulator, DRL},
abstract = {Free-floating space manipulators(FFSM) are more and more widely used in various space tasks, and active object tracking(AOT) is the basis of many missions in space. AOT of FFSM systems has two main difficulties: modeling and control of FFSM systems and tracking motion planning of space manipulators. To deal with these problems, the paper proposed an active object tracking proposal of FFSM systems using deep reinforcement learning(DRL) algorithm, Proximal Policy Optimization(PPO). Our approach is completely data-driven, which avoids the complex modeling process of FFSM and does not require motion planning for space manipulators, which is more concise than traditional algorithms. We trained and tested the algorithm by building a simulation environment in CoppeliaSim, and compared with the resolved motion rate control(RMRC). The results showed that our approach achieved good results in active object tracking of FFSM systems and demonstrated the great potential of DRL algorithms in solving space tasks.}
}
@article{NASERI2022108560,
title = {Dynamic retail market tariff design for an electricity aggregator using reinforcement learning},
journal = {Electric Power Systems Research},
volume = {212},
pages = {108560},
year = {2022},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2022.108560},
url = {https://www.sciencedirect.com/science/article/pii/S0378779622006447},
author = {Nastaran Naseri and Saber Talari and Wolfgang Ketter and John Collins},
keywords = {Artificial intelligence, Electricity markets, Demand prediction, Reinforcement learning, Tariff design},
abstract = {The role of retailers, as energy providers for end-users, in restructured retail electricity markets becomes substantial. The increasing share of distributed energy resources and electrification in different sectors bring several challenges to retailers. Among these challenges, procuring electricity and maintaining system reliability during peak times induce high costs to retailers. Therefore, they need to accurately predict customers’ demand to participate in the wholesale market and develop proper tariff mechanisms considering other retailers’ behavior to maximize their profit. This paper develops the design of an autonomous retailer in which a Sequence-to-Sequence (Seq2Seq) algorithm is employed to predict customers’ net demand. Furthermore, using Reinforcement Learning (RL), the proposed retailer designs tariff mechanisms based on other retailers’ behavior and customers’ load profiles. The proposed design of the retailer is evaluated on a retail market simulation platform called Power TAC, in which autonomous retailers compete in retail, wholesale, and balancing markets to maximize their profits. The results show the accuracy of the proposed load prediction method compared with other methods and successful profit growth with a drop in fixed costs and balancing costs.}
}
@article{WU2023120474,
title = {A hybrid stock market prediction model based on GNG and reinforcement learning},
journal = {Expert Systems with Applications},
volume = {228},
pages = {120474},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120474},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423009764},
author = {Yongming Wu and Zijun Fu and ·Xiaoxuan Liu and ·Yuan Bing},
keywords = {Reinforcement learning, Stock market prediction, Growing neural gas, Reward function, Triple Q-learning},
abstract = {The stock market is a dynamic, complex, and chaotic environment, which makes predictions for the stock market difficult. Many prediction methods are applied to the stock market, but most are supervised learning and cannot effectively parse the trading information present in the stock market. This paper proposes a prediction model that combines unsupervised learning with reinforcement learning to address this problem. Firstly, we capture the stock trend from historical stock data and construct the trading environment state of the market by the growing neural gas (GNG) algorithm in unsupervised learning. Secondly, the reward function is restructured to provide timely feedback on the trading information present in the stock trading market. Finally, a novel trading agent algorithm, Triple Q-learning, is designed to execute the corresponding trading behavior and make comprehensive predictions of the stock market based on the environment state constructed by GNG. Experimental results on several stock datasets demonstrate that the proposed model outperforms other comparative models in this paper.}
}
@article{UPRETI2023170309,
title = {Enhanced algorithmic modelling and architecture in deep reinforcement learning based on wireless communication Fintech technology},
journal = {Optik},
volume = {272},
pages = {170309},
year = {2023},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2022.170309},
url = {https://www.sciencedirect.com/science/article/pii/S0030402622015674},
author = {Kamal Upreti and Mohammad Haider Syed and Mohiuddin Ali Khan and Huda Fatima and Mohammad Shabbir Alam and A.K. Sharma},
keywords = {FinTech, Wireless communication, AI, ML, Fuzzy rule-based secure transmission, DRNN},
abstract = {Financial technology” or “FinTech” refers to use of information technologies to derive financial solutions. FinTech is now widely regarded as a hotly debated blend of financial services and information technology. A combination of AI and IoT approaches will significantly increase the extraction of valuable financial data as well as provide better services to customers. This research proposed a novel protocol in data transmission using fuzzy rule-based secure transmission with data optimization technique using deep reinforcement neural network by wireless communication. This technique for data transmission will reduce the credit risk and enhances the data optimization by fuzzy rule-based protocol with DRNN. The logic utilized is fuzzy logic, which is a multi-valued logic with truth values for variables ranging from 0 to 1. When truth values range from entirely true to completely false, it is used. The end user's searching experience is improved by fuzzy logic-based semantic search, which finds and retrieves exact matching files for relevant search files provided by user. When exact matches aren't possible, method leverages semantic similarities to determine most relevant matches. A grading method is utilized to minimize number of false positives. For this, proposed technique was used, which may decide on a collection of suitable storage servers on which the data must be saved and resulting in a reduction in execution time while maintaining a higher level of security. The experimental results show the execution time of 54 %, network performance of 96 %, the overall complexity of 71 %, data optimization of 95 % and end-end delay of 67 %.}
}
@article{ERGUN2023109934,
title = {A survey on how network simulators serve reinforcement learning in wireless networks},
journal = {Computer Networks},
volume = {234},
pages = {109934},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109934},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623003791},
author = {Serap Ergun and Ibrahim Sammour and Gerard Chalhoub},
keywords = {Wireless networks, Reinforcement learning, Network simulators},
abstract = {Rapid adoption of mobile devices, coupled with the increase in prominence of mobile applications and services, resulted in unprecedented infrastructure requirements for mobile and wireless networks. To improve user experience, future 5G and wireless network systems evolve to support increased mobile traffic, real-time precision analysis, and adaptable network resource management. As mobile environments become more complex, heterogeneous, and evolving, these tasks become more difficult. In order to solve these problems, many researchers rely on reinforcement learning. The success of reinforcement learning stems from its support for new and powerful tools that solve problems. Nodes mobility, instability of wireless connections, the coexistence of multiple wireless technologies, and resource sharing among users are a few examples of what makes a wireless network a dynamic system. Learning, which is the main feature of reinforcement learning, enables wireless nodes to adapt to the dynamics of the system over time. For the learning to be efficient, it should be done over realistic and varied conditions. This is where network simulation tools can be useful. Network simulators are extensively used when it comes to studying wireless network protocols. They offer the advantage of scaling up scenarios at minimum cost and the ability to test many possible configurations quicker under a controlled environment. The main purpose of this survey is to show how network simulators help in developing reinforcement learning techniques in wireless networks. We emphasize how these tools can be used in the learning process and which problems they can solve. In the end, we discuss open issues related to this topic and highlight some best practice guidelines when it comes to mixing network simulators, reinforcement learning, and wireless protocols.}
}
@article{LI2023128284,
title = {Dynamic pricing based electric vehicle charging station location strategy using reinforcement learning},
journal = {Energy},
volume = {281},
pages = {128284},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.128284},
url = {https://www.sciencedirect.com/science/article/pii/S036054422301678X},
author = {Yanbin Li and Jiani Wang and Weiye Wang and Chang Liu and Yun Li},
keywords = {Electric vehicles, Charging station location strategy, Dynamic pricing, Reinforcement learning},
abstract = {To accommodate the increased demand for electric vehicle(EV) charging in China, this paper examines electric vehicle charging station(EVCS) location strategies from the perspective of private investors, taking full account of the competitive environment of existing EVCSs in the region. First of all, a three-level location model considering dynamic pricing is developed, which includes user decisions, EVCS pricing, and EVCS location decisions. Then, the soft actor-critic(SAC) reinforcement learning algorithm is used to train the optimal pricing strategy for EVCS to guarantee the maximum cumulative revenue. The proposed methodology is verified through case studies based on an industrial park in China. The results show that the proposed methodology can make more economical and scientific location decisions than the traditional method. The dynamic pricing method based on reinforcement learning can provide a reference for the location and operation of more EVCSs.}
}
@article{ARANGO2023101109,
title = {Deep reinforcement learning approaches for the hydro-thermal economic dispatch problem considering the uncertainties of the context},
journal = {Sustainable Energy, Grids and Networks},
volume = {35},
pages = {101109},
year = {2023},
issn = {2352-4677},
doi = {https://doi.org/10.1016/j.segan.2023.101109},
url = {https://www.sciencedirect.com/science/article/pii/S2352467723001170},
author = {Alejandro Ramírez Arango and Jose Aguilar and Maria D. R-Moreno},
keywords = {Hydro-thermal economic dispatch, Energy market, Deep reinforcement learning, Optimization problem},
abstract = {Hydro-thermal economic dispatch is a widely analyzed energy optimization problem, which seeks to make the best use of available energy resources to meet demand at minimum cost. This problem has great complexity in its solution due to the uncertainty of multiple parameters. In this paper, we view hydro-thermal economic dispatch as a multistage decision-making problem, and propose several Deep Reinforcement Learning approaches to solve it due to their abilities to handle uncertainty and sequential decisions. We test our approaches considering several hydrological scenarios, especially the cases of hydrological uncertainty due to the high dependence on hydroelectric plants, and the unpredictability of energy demand. The policy performance of our algorithms is compared with a classic deterministic method. The main advantage is that our methods can learn a robust policy to deal with different inflow and load demand scenarios, and particularly, the uncertainties of the environment such as hydrological and energy demand, something that the deterministic approach cannot do.}
}
@article{ZHOU2022103520,
title = {Deep reinforcement learning-based algorithms selectors for the resource scheduling in hierarchical Cloud computing},
journal = {Journal of Network and Computer Applications},
volume = {208},
pages = {103520},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103520},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522001618},
author = {Guangyao Zhou and Ruiming Wen and Wenhong Tian and Rajkumar Buyya},
keywords = {Hierarchical cloud computing, Algorithm selection, Subsystem, DL-based selector, DRL-based selector},
abstract = {Cloud computing environment is becoming increasingly complex due to its large-scale information growth and increasing heterogeneity of computing resources. Hierarchical Cloud computing dividing the system into multi-levels with multiple subsystems to support the adaptability to abundant requests from users has been widely applied and brings great challenges to resource scheduling. It is critical to find an effective way to address the complex scheduling problems in hierarchical Cloud computing, whose scenarios and optimization objectives often change with the types of subsystems. In this paper, we propose a scheduling framework to select the scheduling algorithms (SFSSA) for different scheduling scenarios considering no algorithm well suitable to all scenarios. To concretize SFSSA, we propose deep learning-based algorithms selectors (DLS) trained by labeled data and deep reinforcement learning-based algorithms selectors (DRLS) trained by feedback from dynamic scenarios to complete the algorithms selection regarding the scheduling algorithms as selectable tools. Then, we apply strategies including pre-trained model, long experience reply and joint training to improve the performance of DRLS. To enable the quantitative comparison of selectors, we introduce a weighted cost model for the trade-off between solution and complexity. Through multiple sets of experiments in hierarchical Cloud computing with multi subsystems for five types of scheduling problems and varying weights of cost, we demonstrate DLS and DRLS outperform baseline strategies. Compared with random selector, greedy selector, round-robin selector, single best selector, virtual best selector and single fast selector, DLS reduces the cost by 47.4%, 46.1%, 33.9%, 47.9%, 19.3%, 18.8% under stable parameter ranges, and DRLS reduces the cost by 41.1%, 40.6%, 11.7%, 42.3%, 11.5%, 12.5% in dynamic scenarios respectively. In experiments, we also validate DRLS has stronger adaptability than DLS in dynamic scheduling scenarios and DRLS using all of strategies achieves the best performance.}
}
@article{ZANON20205213,
title = {Reinforcement Learning Based on Real-Time Iteration NMPC},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {5213-5218},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1195},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320315901},
author = {Mario Zanon and Vyacheslav Kungurtsev and Sébastien Gros},
keywords = {Reinforcement Learning, Model Predictive Control},
abstract = {Reinforcement Learning (RL) has proven a stunning ability to learn optimal policies from data without any prior knowledge on the process. The main drawback of RL is that it is typically very difficult to guarantee stability and safety. On the other hand, Nonlinear Model Predictive Control (NMPC) is an advanced model-based control technique which does guarantee safety and stability, but only yields optimality for the nominal model. Therefore, it has been recently proposed to use NMPC as a function approximator within RL. While the ability of this approach to yield good performance has been demonstrated, the main drawback hindering its applicability is related to the computational burden of NMPC, which has to be solved to full convergence. In practice, however, computationally efficient algorithms such as the Real-Time Iteration (RTI) scheme are deployed in order to return an approximate NMPC solution in very short time. In this paper we bridge this gap by extending the existing theoretical framework to also cover RL based on RTI NMPC. We demonstrate the effectiveness of this new RL approach with a nontrivial example modeling a challenging nonlinear system subject to stochastic perturbations with the objective of optimizing an economic cost.}
}
@article{LAU2023100684,
title = {ADFPA – A Deep Reinforcement Learning-based Flow Priority Allocation Scheme for Throughput Optimization in FANETs},
journal = {Vehicular Communications},
volume = {44},
pages = {100684},
year = {2023},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2023.100684},
url = {https://www.sciencedirect.com/science/article/pii/S2214209623001146},
author = {Wei Jian Lau and Joanne Mun-Yee Lim and Chun Yong Chong and Nee Shen Ho and Thomas Wei Min Ooi},
keywords = {Unmanned Aerial Vehicle, Reinforcement Learning, Anticipatory Networking},
abstract = {Flying ad hoc networks (FANETs) are easy to deploy and cost-efficient, however they are limited by the static protocols used in 802.11 and CSMA-based networks to support high bandwidth multi-UAV applications. This work proposes an Anticipatory Dynamic Flow Priority Allocation (ADFPA) scheme to optimize the priority levels of outgoing traffic flows for a transmitting node to maximize the total network throughput. Unlike other deep reinforcement learning (DRL)-based schemes in centralized networks, ADFPA is designed to be distributed, multi-agent, and proactive. It uses current and forecasted multi-context information to optimize the priority levels of traffic flows in a decentralized and dynamic FANET. Furthermore, a traffic flow sampling and padding algorithm is proposed so that a trained agent can be redeployed in different environments without retraining to address the practicality issue. Our evaluations show that ADFPA outperforms other state-of-the-art schemes by a maximum of 37% and 59.4% in terms of the network throughput in the single and multi-transmitting nodes environment, respectively, while achieving the best fairness amongst all schemes. These improvements translate to better data transmission capabilities in a conventional FANET, and the proposed scheme can enable the use of a FANET architecture in more demanding applications without switching to centralized solutions.}
}
@article{HERNANDEZDELOLMO20189,
title = {Tackling the start-up of a reinforcement learning agent for the control of wastewater treatment plants},
journal = {Knowledge-Based Systems},
volume = {144},
pages = {9-15},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S095070511730597X},
author = {Félix Hernández-del-Olmo and Elena Gaudioso and Raquel Dormido and Natividad Duro},
keywords = {Reinforcement learning, Wastewater systems, Intelligent agent, Adaptive control},
abstract = {Reinforcement learning problems involve learning by doing. Therefore, a reinforcement learning agent will have to fail sometimes (while doing) in order to learn. Nevertheless, even with this starting error, introduced at least during the non-optimal learning stage, reinforcement learning can be affordable in some domains like the control of a wastewater treatment plant. However, in wastewater treatment plants, trying to solve the day-to-day problems, plant operators will usually not risk to leave their plant in the hands of an inexperienced and untrained reinforcement learning agent. In fact, it is somewhat obvious that plant operators will require firstly to check that the agent has been trained and that it works as it should at their particular plant. In this paper, we present a solution to this problem by giving a previous instruction to the reinforcement learning agent before we let it act on the plant. In fact, this previous instruction is the key point of the paper. In addition, this instruction is given effortlessly by the plant operator. As we will see, this solution does not just solve the starting up problem of leaving the plant in the hands of an untrained agent, but it also improves the future performance of the agent.}
}
@article{DANGUT2022108873,
title = {Application of deep reinforcement learning for extremely rare failure prediction in aircraft maintenance},
journal = {Mechanical Systems and Signal Processing},
volume = {171},
pages = {108873},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.108873},
url = {https://www.sciencedirect.com/science/article/pii/S0888327022000693},
author = {Maren David Dangut and Ian K. Jennions and Steve King and Zakwan Skaf},
keywords = {Extremely rare event, Deep reinforcement learning, Imbalance classification, Aircraft maintenance},
abstract = {The use of aircraft operational logs to predict potential failure that may lead to disruption poses many challenges and has yet to be fully explored. Given that aircraft are high-integrity assets, failures are extremely rare, and hence the distribution of relevant log data containing prior indicators will be highly skewed to the normal (healthy) case. This will present a significant challenge in using data-driven techniques because the model will be biased to the heavily weighted no-fault outcomes. This paper presents a novel approach for predicting unscheduled aircraft maintenance action based on deep reinforcement learning techniques using aircraft central maintenance system logs. The algorithm transforms the rare failure prediction problem into a sequential decision-making process that is optimised using a reward system that penalises proposed predictions that result in a false diagnosis and preferentially favours predictions that result in the right diagnosis. The validation data is directly associated with the physical health aspects of the aircraft components. The influence of extremely rare failure prediction on the proposed method is analysed. The effectiveness of the new approach is verified by comparison with previous studies, cost-sensitive and oversampling methods. Performance was evaluated based on G-mean and false-positives rates. The proposed approach shows the superior performance of 20.3% improvement in G-mean and 97% reduction in false-positive rate.}
}
@article{LAWRENCE2022105046,
title = {Deep reinforcement learning with shallow controllers: An experimental application to PID tuning},
journal = {Control Engineering Practice},
volume = {121},
pages = {105046},
year = {2022},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2021.105046},
url = {https://www.sciencedirect.com/science/article/pii/S0967066121002963},
author = {Nathan P. Lawrence and Michael G. Forbes and Philip D. Loewen and Daniel G. McClement and Johan U. Backström and R. Bhushan Gopaluni},
keywords = {Reinforcement learning, Deep learning, PID control, Process control, Process systems engineering},
abstract = {Deep reinforcement learning (RL) is an optimization-driven framework for producing control strategies for general dynamical systems without explicit reliance on process models. Good results have been reported in simulation. Here we demonstrate the challenges in implementing a state of the art deep RL algorithm on a real physical system. Aspects include the interplay between software and existing hardware; experiment design and sample efficiency; training subject to input constraints; and interpretability of the algorithm and control law. At the core of our approach is the use of a PID controller as the trainable RL policy. In addition to its simplicity, this approach has several appealing features: No additional hardware needs to be added to the control system, since a PID controller can easily be implemented through a standard programmable logic controller; the control law can easily be initialized in a “safe” region of the parameter space; and the final product—a well-tuned PID controller—has a form that practitioners can reason about and deploy with confidence.}
}
@article{TOTARO2021121035,
title = {Lifelong control of off-grid microgrid with model-based reinforcement learning},
journal = {Energy},
volume = {232},
pages = {121035},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121035},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221012834},
author = {Simone Totaro and Ioannis Boukas and Anders Jonsson and Bertrand Cornélusse},
keywords = {Microgrid control, Optimization, Reinforcement learning},
abstract = {Off-grid microgrids are receiving a growing interest for rural electrification purposes in developing countries due to their ability to ensure affordable, sustainable and reliable energy services. Off-grid microgrids rely on renewable energy sources (RES) coupled with storage systems to supply the electrical consumption. The inherent uncertainty introduced by RES as well as the stochastic nature of the electrical demand in rural contexts pose significant challenges to the efficient control of off-grid microgrids throughout their entire life span. In this paper, we address the lifelong control problem of an isolated microgrid. We categorize the set of changes that may occur over its life span in progressive and abrupt changes. We propose a novel model-based reinforcement learning algorithm that is able to address both types of changes. In particular, the proposed algorithm demonstrates generalisation properties, transfer capabilities and better robustness in case of fast-changing system dynamics. The proposed algorithm is compared against a rule-based policy and a model predictive controller with look-ahead. The results show that the trained agent is able to outperform both benchmarks in the lifelong setting where the system dynamics are changing over time.}
}
@article{WANG2019106,
title = {Data-driven dynamic resource scheduling for network slicing: A Deep reinforcement learning approach},
journal = {Information Sciences},
volume = {498},
pages = {106-116},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519303986},
author = {Haozhe Wang and Yulei Wu and Geyong Min and Jie Xu and Pengcheng Tang},
keywords = {Data-driving, End-to-End, Deep reinforcement learning, Network slicing},
abstract = {Network slicing is designed to support a variety of emerging applications with diverse performance and flexibility requirements, by dividing the physical network into multiple logical networks. These applications along with a massive number of mobile phones produce large amounts of data, bringing tremendous challenges for network slicing performance. From another perspective, this huge amount of data also offers a new opportunity for the management of network slicing resources. Leveraging the knowledge and insights retrieved from the data, we develop a novel Machine Learning-based scheme for dynamic resource scheduling for networks slicing, aiming to achieve automatic and efficient resource optimisation and End-to-End (E2E) service reliability. However, it is difficult to obtain the user-related data, which is crucial to understand the user behaviour and requests, due to the privacy issue. Therefore, Deep Reinforcement Learning (DRL) is leveraged to extract knowledge from experience by interacting with the network and enable dynamic adjustment of the resources allocated to various slices in order to maximise the resource utilisation while guaranteeing the Quality-of-Service (QoS). The experiment results demonstrate that the proposed resource scheduling scheme can dynamically allocate resources for multiple slices and meet the corresponding QoS requirements.}
}
@article{HUANG2023121358,
title = {Training-efficient and cost-optimal energy management for fuel cell hybrid electric bus based on a novel distributed deep reinforcement learning framework},
journal = {Applied Energy},
volume = {346},
pages = {121358},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121358},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923007225},
author = {Ruchen Huang and Hongwen He and Miaojue Gao},
keywords = {Fuel cell hybrid electric bus, Energy management strategy, Distributed deep reinforcement learning, Asynchronous advantage actor-critic (A3C), Multi-process parallel computation},
abstract = {Deep reinforcement learning (DRL) has become the mainstream method to design intelligent energy management strategies (EMSs) for fuel cell hybrid electric vehicles with the prosperity of artificial intelligence in recent years. Conventional DRL algorithms are suffering from low sampling efficiency and unsatisfactory utilization of computing resources. Combined with distributed architecture and parallel computation, DRL algorithms can be more efficient. Given that, this paper proposes a novel distributed DRL-based energy management framework for a fuel cell hybrid electric bus (FCHEB) to shorten the development cycle of DRL-based EMSs while reducing the total operation cost of the FCHEB. To begin, to make full use of the limited computing resources, a novel asynchronous advantage actor-critic (A3C)-based energy management framework is designed by innovatively integrating with the multi-process parallel computation technique. Then, a promising EMS considering the extra operation cost caused by fuel cell degradation and battery aging is designed based on this novel framework. Furthermore, EMSs based on a conventional DRL algorithm, advantage actor-critic (A2C), and another conventional distributed DRL framework, multi-thread A3C, are employed as baselines, and the performance of the proposed EMS is evaluated by training and testing using different driving cycles. Simulation results indicate that compared to EMSs based on A2C and multi-thread A3C, the proposed EMS can efficiently accelerate the convergence speed respectively by 87.46% and 88.92%, and reduce the total operation cost respectively by 44.83% and 41.19%. The main contribution of this article is to explore the integration of multi-process parallel computation in a distributed DRL-based EMS for a fuel cell vehicle for more efficient utilization of hydrogen energy in the transportation sector.}
}
@article{KANG2023113655,
title = {Multi-objective sizing and real-time scheduling of battery energy storage in energy-sharing community based on reinforcement learning},
journal = {Renewable and Sustainable Energy Reviews},
volume = {185},
pages = {113655},
year = {2023},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2023.113655},
url = {https://www.sciencedirect.com/science/article/pii/S1364032123005129},
author = {Hyuna Kang and Seunghoon Jung and Hakpyeong Kim and Juwon Hong and Jaewon Jeoung and Taehoon Hong},
keywords = {Battery energy storage system, Multi-objective scheduling, Optimal sizing, Energy community, Reinforcement learning},
abstract = {Reducing peak demand and enhancing self-sufficiency have made the battery energy storage system (BESS) essential in microgrids when combined with photovoltaic (PV) systems. Previous studies have shown that BESS usage remains expensive, and it is more practical to share it within a community rather than installing individual systems. Therefore, this study aimed to propose a framework for BESS sizing and scheduling in an energy-sharing community based on reinforcement learning. To validate the proposed framework, a case study was conducted based on the BESS ownership scenarios (i.e., individual-owned BESS (IOB) and community-shared BESS (CSB)) considering the following purposes of potential stakeholders: (i) enhancing SSR; (ii) reducing peak demand; and (iii) increasing economic profit. The maximum SSR of CSB was 12.4% higher than IOB, while the peak load of CSB was 0.07% lower than IOB. As the BESS size increased, the total cost of BESS increased, and the total profits on electricity decreased. The economic profit increased by about 38% when CSB was used compared to IOBs. The proposed framework offers more promise for a broader range of building loads and PV generation within the energy-sharing community. Ultimately, this study would allow prosumers, community practitioners, and policymakers to better understand the sharing mechanisms in the community and provide decision guidelines for ownership types, operational strategies, and sizing of BESS and PV systems.}
}
@article{CHEN2023121710,
title = {The predictive management in campus heating system based on deep reinforcement learning and probabilistic heat demands forecasting},
journal = {Applied Energy},
volume = {350},
pages = {121710},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121710},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923010747},
author = {Minghao Chen and Zhiyuan Xie and Yi Sun and Shunlin Zheng},
keywords = {Deep reinforcement learning, Campus heating system, Probabilistic forecasting, Long short-term memory, Twin delayed deep deterministic policy gradient},
abstract = {As a promising technology for replacing the rule-based decision-making in region heating systems (RHS), deep reinforcement learning (DRL) is a practical solution to identify the optimal control for heating equipment. However, as residential customers perform more casual energy-consumption behaviors, the intermittency and volatility of heat demands make managing heat supply and storage much harder for DRL agents. This study proposes a novel predictive management method for campus heating systems (CHS) with air-source heat pumps (AHP) and thermostatic water tanks. The novelty of the proposed method lies in the combination of the heat demands forecasting model and DRL-based adaptively controlling for heat supply equipment, which is firstly proposed to improve the heating supply reliability and reduce the storage dependence for CHS. Specifically, an enhanced rule, namely minimum length hamming encoding, and an input array constructing method is introduced to deal with discrete feature data and then improve the accuracy of deterministic heat demands forecasting based on long-short term memory (LSTM), and the Kernel density estimation (KDE) are employed to obtain the prediction intervals (PIs) from H-step ahead heat demands forecasting series. Followed by these, the twin delayed deep deterministic policy gradient, a model-free DRL control algorithm, is adopted for adaptively adjusting the output flow rate of AHP and then the storage of the hot water tank. To demonstrate the validity of the proposed method, a case study is presented where a campus heat demands forecasting achieves a maximum accuracy gain of 4.52%, and an optimal AHP operating controlling determined from PIs achieves a better cost reduction and supply reliability, which is superior over the conventional method using real-time heat demands or deterministic forecasting results as input.}
}
@article{ZENG2022108546,
title = {A reinforcement learning approach to parameter selection for distributed optimal power flow},
journal = {Electric Power Systems Research},
volume = {212},
pages = {108546},
year = {2022},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2022.108546},
url = {https://www.sciencedirect.com/science/article/pii/S0378779622006319},
author = {Sihan Zeng and Alyssa Kody and Youngdae Kim and Kibaek Kim and Daniel K. Molzahn},
keywords = {Alternating direction method of multipliers, Alternating current optimal power flow, Distributed optimization, Reinforcement learning, Deep Q-learning},
abstract = {With the increasing penetration of distributed energy resources, distributed optimization algorithms have attracted significant attention for power systems applications due to their potential for superior scalability, privacy, and robustness to a single point-of-failure. The Alternating Direction Method of Multipliers (ADMM) is a popular distributed optimization algorithm; however, its convergence performance is highly dependent on the selection of penalty parameters, which are usually chosen heuristically. In this work, we use reinforcement learning (RL) to develop an adaptive penalty parameter selection policy for alternating current optimal power flow (ACOPF) problem solved via ADMM with the goal of minimizing the number of iterations until convergence. We train our RL policy using deep Q-learning and show that this policy can result in significantly accelerated convergence (up to a 59% reduction in the number of iterations compared to existing, curvature-informed penalty parameter selection methods). Furthermore, we show that our RL policy demonstrates promise for generalizability, performing well under unseen loading schemes as well as under unseen losses of lines and generators (up to a 50% reduction in iterations). This work thus provides a proof-of-concept for using RL for parameter selection in ADMM for power systems applications.}
}
@article{NAUMAN202113,
title = {Reinforcement learning-enabled Intelligent Device-to-Device (I-D2D) communication in Narrowband Internet of Things (NB-IoT)},
journal = {Computer Communications},
volume = {176},
pages = {13-22},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001912},
author = {Ali Nauman and Muhammad Ali Jamshed and Rashid Ali and Korhan Cengiz and  Zulqarnain and Sung Won Kim},
keywords = {Reinforcement Learning (RL), Intelligent communication, Device-to-Device (D2D) communication, Narrowband Internet of Things (NB-IoT), th Generation (5G) networks},
abstract = {The 5th Generation (5G) and Beyond 5G (B5G) are expected to be the enabling technologies for Internet-of-Everything (IoE). The quality-of-service (QoS) for IoE in the context of uplink data delivery of the content is of prime importance. The 3rd Generation Partnership Project (3GPP) standardizes the Narrowband Internet-of-Things (NB-IoT) in 5G, which is Low Power Wide Area (LPWA) technology to enhance the coverage and to optimize the power consumption for the IoT devices. Repetitions of control and data signals between NB-IoT User Equipment (UE) and the evolved NodeB/Base Station (eNB/BS), is one of the most prominent characteristics in NB-IoT. These repetitions ensure high reliability in the context of data delivery of time-sensitive applications, e.g., healthcare applications. However, these repetitions degrade the performance of the resource-constrained IoT network in terms of energy consumption. Device-to-Device (D2D) communication standardized in Long Term Evolution-Advanced (LTE-A) offers a key solution for NB-IoT UE to transmit in two hops route instead of direct uplink, which augments the efficiency of the system. In an effort to improve the data packet delivery, this study investigates D2D communication for NB-IoT delay-sensitive applications, such as healthcare-IoT services. This study formulates the selection of D2D communication relay as Multi-Armed Bandit (MAB) problem and incorporates Upper Confidence Bound (UCB) based Reinforcement Learning (RL) to solve MAB problem. The proposed Intelligent-D2D (I-D2D) communication methodology selects the optimum relay with a maximum Packet Delivery Ratio (PDR) with minimum End-to-End Delay (EED), which ultimately augments energy efficiency.}
}
@article{OH2022109877,
title = {Effective data-driven precision medicine by cluster-applied deep reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {256},
pages = {109877},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109877},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122009704},
author = {Sang Ho Oh and Su Jin Lee and Jongyoul Park},
keywords = {Deep reinforcement learning, Precision medicine, Clustering, Recommendation system, Healthcare management},
abstract = {The significance of machine-learning approaches in the healthcare domain has grown rapidly owing to the existence of enormous amounts of data and well-established simulation models and algorithms. The digitization of health-related data, as well as rapid technological advancements are accelerating the development and application of machine learning in healthcare, particularly in precision medicine. The ultimate goal of precision medicine is to provide personalized medicine, which requires tailoring medical decisions to each patient based on their projected disease response. In this study, we propose a cluster-applied deep reinforcement learning-based type 2 diabetes treatment recommendation model based on the electronic health records of South Koreans. The purpose of applying a clustering algorithm is to group patients who are in a similar state, to boost the performance of deep reinforcement learning, build a more realistic treatment recommendation model to support clinicians, and develop expert systems in the field of healthcare. The proposed model demonstrated significant performance by decreasing diabetes-related medical checkup measurements. Furthermore, the proposed model delivered high-quality performance when compared with existing reinforcement-learning methods. Finally, the recommendation outcomes of the proposed model were validated against real-life prescriptions to ensure the accuracy of the findings.}
}
@article{WANG2021108078,
title = {Transfer reinforcement learning-based road object detection in next generation IoT domain},
journal = {Computer Networks},
volume = {193},
pages = {108078},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108078},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621001687},
author = {Ke Wang and Chien-Ming Chen and M. Shamim Hossain and Ghulam Muhammad and Sachin Kumar and Saru Kumari},
keywords = {Deep learning, Transfer reinforcement learning, 5G, Internet of Things},
abstract = {The landscape of fifth generation (5G) and beyond 5G (B5G)-enabled Internet of Things(IoT) is expected to seamlessly and ubiquitously connect everything, which includes 5G, cloud computing, artificial intelligence and other cutting-edge technologies to realize truly intelligent applications in smart cities. In this paper, we present an important key technology for smart city, which is a road target recognition algorithm for smart city applications and designs a set of corresponding programs to assist automatic drivers, pedestrians and visually impaired people in road safety, or to manage city infrastructure. The system can connect robots in cars, wearable devices and body area network in pedestrians or blind people. A target recognition algorithm based on scene fusion is designed to recognize the specific target in the road environment, and transfer reinforcement learning method is used to improve the accuracy and real-time performance of target recognition. The system provides them with travel assistance, identify dangerous or useful objects for them through high-performance target recognition services. It can collect the road visual scene data by road cameras and transmit it to edge devices for training model. The model is collaborated trained in the edge devices and aggregated by the cloud. Based on the transfer reinforcement learning method, the vision-based road target recognition has been implemented, and the accurate and reliable target recognition can be realized. Many details of experiments verify the effectiveness of our technology.}
}
@article{ESILVAVIEIRA2023100526,
title = {Exploring reinforcement learning approaches for drafting in collectible card games},
journal = {Entertainment Computing},
volume = {44},
pages = {100526},
year = {2023},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2022.100526},
url = {https://www.sciencedirect.com/science/article/pii/S1875952122000490},
author = {Ronaldo {e Silva Vieira} and Anderson {Rocha Tavares} and Luiz Chaimowicz},
keywords = {Reinforcement learning, Collectible card game, Deck-building},
abstract = {Collectible card games (CCGs), such as Magic: the Gathering and Hearthstone, are played by tens of millions worldwide and are known for their usually large and intricate rules. From an artificial intelligence (AI) standpoint, playing CCGs consists of two interdependent tasks: deck-building and battling. This paper presents three deep reinforcement learning approaches for deck-building in the arena mode of CCGs. Our approaches are trained in self-play and differ in the handling of past information when drafting new cards for a deck. We formulate the problem in a game-agnostic manner and perform experiments on Legends of Code and Magic, a CCG designed for AI research. Considering the win rate of the decks when used by fixed battling agents, the results show that our trained drafting agents outperform the best ones available for the game and do so by building very different decks. We also reenact the Strategy Card Game AI competition and show that our best drafting strategy improves the win rate of a baseline competitor by 14.9 and 12.7 percentage points in the 2019 and 2020 editions.}
}
@article{CARLUCHO2020280,
title = {An adaptive deep reinforcement learning approach for MIMO PID control of mobile robots},
journal = {ISA Transactions},
volume = {102},
pages = {280-294},
year = {2020},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2020.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0019057820300781},
author = {Ignacio Carlucho and Mariano {De Paula} and Gerardo G. Acosta},
keywords = {Reinforcement learning, Adaptive control, Policy gradient, Mobile robots, Multi-platforms},
abstract = {Intelligent control systems are being developed for the control of plants with complex dynamics. However, the simplicity of the PID (proportional–integrative–derivative) controller makes it still widely used in industrial applications and robotics. This paper proposes an intelligent control system based on a deep reinforcement learning approach for self-adaptive multiple PID controllers for mobile robots. The proposed hybrid control strategy uses an actor–critic structure and it only receives low-level dynamic information as input and simultaneously estimates the multiple parameters or gains of the PID controllers. The proposed approach was tested in several simulated environments and in a real time robotic platform showing the feasibility of the approach for the low-level control of mobile robots. From the simulation and experimental results, our proposed approach demonstrated that it can be of aid by providing with behavior that can compensate or even adapt to changes in the uncertain environments providing a model free unsupervised solution. Also, a comparative study against other adaptive methods for multiple PIDs tuning is presented, showing a successful performance of the approach.}
}
@article{LI202289,
title = {Multi-objective reinforcement learning for fed-batch fermentation process control},
journal = {Journal of Process Control},
volume = {115},
pages = {89-99},
year = {2022},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2022.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959152422000804},
author = {Dazi Li and Fuqiang Zhu and Xiao Wang and Qibing Jin},
keywords = {Multi-objective, Reinforcement learning, Fed-batch fermentation, Process control},
abstract = {Many real-world control problems involve conflicting objectives. For different objectives, it is necessary to obtain Pareto optimal solution sets for each one. Over recent years, multi-objective reinforcement learning (MORL) has been extensively studied to solve this problem. However, the multi-objective optimization problem of complex continuous control processes still requires exploration. Soft proximal policy optimization algorithms have also been proposed to be combined with a hybrid weight-generation method for application to find the Pareto front approximation of the fed-batch fermentation process. This algorithm intends to initially find a single policy for the multi-objective reinforcement learning problem. A hybrid weight-generation method is then used to change the weights between different objectives to find a set of Pareto optimal solutions. In addition, we analyzed the mechanism of the fed-batch process, established the kinetic model, and designed the experimental environment based on the OpenAI Gym library. Experimental results showed that the proposed algorithm is effective and efficient in approaching the Pareto front of the fed-batch fermentation problem.}
}
@article{LI2023113581,
title = {Active fault-tolerant coordination energy management for a proton exchange membrane fuel cell using curriculum-based multiagent deep meta-reinforcement learning},
journal = {Renewable and Sustainable Energy Reviews},
volume = {185},
pages = {113581},
year = {2023},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2023.113581},
url = {https://www.sciencedirect.com/science/article/pii/S1364032123004380},
author = {Jiawen Li and Tao Zhou},
keywords = {Active fault tolerant control, meta-Reinforcement learning, Proton exchange membrane fuel cell, Operating variables, Meta-learner},
abstract = {This paper addresses the challenge of active fault-tolerant coordination control (AFTCC) for proton exchange membrane fuel cells (PEMFCs), which are complex nonlinear systems with multiple inputs and outputs. Conventional fault-tolerant control methods cannot properly coordinate multiple operating variables and prevent constraint violations in PEMFCs. Our proposed AFTCC method seeks to stabilize the output performance of four operating variables and avoid PEMFC operating constraint violations during failure scenarios. Our method is supported by a curriculum-based multiagent deep meta-deterministic policy gradient (CMA-DMDPG) algorithm, which integrates meta-reinforcement learning, multiagent reinforcement learning and curriculum learning to achieve multitask collaboration of multiple agents, thereby enhancing PEMFC robustness. The algorithm consists of a meta-learner and a base learner. The base learner regards the hydrogen controller, oxygen controller, pump controller and radiator controller as four independent agents and thus achieves a cooperative control policy. The meta-learner detects PEMFC faults and selects an appropriate cooperative control policy. The performance of AFTCC under various stochastic and fault conditions is evaluated using a 75 kW PEMFC model. The results showed that the performance of AFTCC surpassed 11 other fault-tolerant control methods in terms of output voltage, oxygen excess ratio, and stack temperature, and avoided constraint violations.}
}
@article{YUN20211,
title = {Distributed deep reinforcement learning for autonomous aerial eVTOL mobility in drone taxi applications},
journal = {ICT Express},
volume = {7},
number = {1},
pages = {1-4},
year = {2021},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2021.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405959521000059},
author = {Won Joon Yun and Soyi Jung and Joongheon Kim and Jae-Hyun Kim},
keywords = {eVTOL, Drone taxi, Air taxi, Distributed deep reinforcement learning, Urban aerial mobility},
abstract = {The urban aerial mobility (UAM) system, such as drone taxi or air taxi, is one of future on-demand transportation networks. Among them, electric vertical takeoff and landing (eVTOL) is one of UAM systems that is for identifying the locations of passengers, flying to the positions where the passengers are located, loading the passengers, and delivering the passengers to their destinations. In this paper, we propose a distributed deep reinforcement learning where the agents are formulated as eVTOL vehicles that can compute the optimal passenger transportation routes under the consideration of passenger behaviors, collisions among eVTOL, and eVTOL battery status.}
}
@incollection{WU2023795,
title = {Reinforcement Learning for inventory management in multi-echelon supply chains},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {795-800},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50127-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044315274050127X},
author = {Guoquan Wu and Miguel Ángel {de Carvalho Servia} and Max Mowbray},
keywords = {Reinforcement Learning, multi-echelon supply chain management, inventory optimization},
abstract = {Reinforcement Learning (RL) describes a set of model-free learning rules that enable identification of an approximately optimal feedback control policy function approximation for discrete, time uncertain process systems purely from data. This has led to interest in application of RL to supply chain management problems. However, many RL methods are dependent on estimating noisy first-order directions for policy improvement, and have been developed for video games and computer science applications. Here, we propose a deep RL method tailored for supply chain management. The algorithm deploys a derivative free approach to balance exploration and exploitation of a neural policy’s parameter space, providing means to avoid low quality local optima. Additionally, the policy inherits ease in posing risk-sensitive formulations to learn a policy that respects constraints on the conditional value-at-risk. The performance of our algorithm is tested on a benchmark multi-echelon supply chain inventory management problem. The results demonstrate empirical improvements in performance over the first-order RL method, proximal policy optimization, and competitive performance with mathematical programming. Additionally, the risk-sensitive decisions generated can effectively handle low probability, high severity scenarios.}
}
@article{LAN2022112226,
title = {Path planning for underwater gliders in time-varying ocean current using deep reinforcement learning},
journal = {Ocean Engineering},
volume = {262},
pages = {112226},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.112226},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822015372},
author = {Wei Lan and Xiang Jin and Xin Chang and Tianlin Wang and Han Zhou and Wei Tian and Lilei Zhou},
keywords = {Deep reinforcement learning, Underwater glider, Multi-agent systems, Time-varying ocean, Path planning},
abstract = {The objective of this paper is to solve the application research of underwater glider (UG) and UGs formation, it is aiming to solve the path planning of gliders in ocean current environment by deep deterministic policy gradient (DDPG). Gliders can be deployed individually or collectively to execute ocean missions. Using the existing glider model and the interactions between gliders and environment, models close to the practical application of UGs are established. The deep reinforcement learning (DRL) based planning algorithm by integrating artificial intelligence, and solution to planning problem of UGs is provided. For a single UG planning, the designed RL algorithm can solve the compliance of UG motion constraints. The algorithm can calculate the appropriate path for the UGs formation, and change the shape of formation as necessary, which is useful for navigation in the environment of dense obstacles. With the same reward function, the improved DDPG outperforms the deep Q-network (DQN). Based on Tokyo Bay geography and unacquainted ocean, the developed algorithm is tested in ocean current environments.}
}
@article{BENBRAHIM1997283,
title = {Biped dynamic walking using reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {22},
number = {3},
pages = {283-302},
year = {1997},
note = {Robot Learning: The New Wave},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(97)00043-2},
url = {https://www.sciencedirect.com/science/article/pii/S0921889097000432},
author = {Hamid Benbrahim and Judy A. Franklin},
keywords = {Biped walking, Reinforcement learning, Robot learning, Biped robot, Legged robot},
abstract = {This paper presents some results from a study of biped dynamic walking using reinforcement learning. During this study a hardware biped robot was built, a new reinforcement learning algorithm as well as a new learning architecture were developed. The biped learned dynamic walking without any previous knowledge about its dynamic model. The self scaling reinforcement (SSR) learning algorithm was developed in order to deal with the problem of reinforcement learning in continuous action domains. The learning architecture was developed in order to solve complex control problems. It uses different modules that consist of simple controllers and small neural networks. The architecture allows for easy incorporation of new modules that represent new knowledge, or new requirements for the desired task.}
}
@article{HERASYMOVYCH2019105697,
title = {Using reinforcement learning to optimize the acceptance threshold of a credit scoring model},
journal = {Applied Soft Computing},
volume = {84},
pages = {105697},
year = {2019},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2019.105697},
url = {https://www.sciencedirect.com/science/article/pii/S1568494619304788},
author = {Mykola Herasymovych and Karl Märka and Oliver Lukason},
keywords = {Credit scoring, Acceptance threshold, Cut-off optimization, Reinforcement learning, Profit maximization},
abstract = {This paper aims to study whether the reinforcement learning approach to optimizing the acceptance threshold of a credit score leads to higher profits for the lender compared to the state-of-the-art cost-sensitive optimization approach. We show that static methods, such as the latter do not ensure the optimality of the threshold leading to biased results and significant losses for the firm. We develop a dynamic reinforcement learning system that constantly adapts the threshold in response to live data feedback in order to maximize a company’s profits. The developed algorithm is shown to outperform the traditional approach in terms of profits both in various simulated scenarios and using real data from an international consumer credit company.}
}
@article{THEATE2021114632,
title = {An application of deep reinforcement learning to algorithmic trading},
journal = {Expert Systems with Applications},
volume = {173},
pages = {114632},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114632},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421000737},
author = {Thibaut Théate and Damien Ernst},
keywords = {Artificial intelligence, Deep reinforcement learning, Algorithmic trading, Trading policy},
abstract = {This scientific research paper presents an innovative approach based on deep reinforcement learning (DRL) to solve the algorithmic trading problem of determining the optimal trading position at any point in time during a trading activity in the stock market. It proposes a novel DRL trading policy so as to maximise the resulting Sharpe ratio performance indicator on a broad range of stock markets. Denominated the Trading Deep Q-Network algorithm (TDQN), this new DRL approach is inspired from the popular DQN algorithm and significantly adapted to the specific algorithmic trading problem at hand. The training of the resulting reinforcement learning (RL) agent is entirely based on the generation of artificial trajectories from a limited set of stock market historical data. In order to objectively assess the performance of trading strategies, the research paper also proposes a novel, more rigorous performance assessment methodology. Following this new performance assessment approach, promising results are reported for the TDQN algorithm.}
}
@article{SONG2022110947,
title = {Guidance and control of autonomous surface underwater vehicles for target tracking in ocean environment by deep reinforcement learning},
journal = {Ocean Engineering},
volume = {250},
pages = {110947},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.110947},
url = {https://www.sciencedirect.com/science/article/pii/S002980182200378X},
author = {Dalei Song and Wenhao Gan and Peng Yao and Wenchuan Zang and Zhixuan Zhang and Xiuqing Qu},
keywords = {Autonomous surface underwater vehicle, Standoff tracking, Guidance and control, Deep reinforcement learning},
abstract = {This paper studies a guidance and control framework of multiple autonomous surface underwater vehicles (multi-ASUV) based on deep reinforcement learning (DRL) for target tracking. The framework enables the vehicles to complete the standoff tracking and sampling tasks in a predetermined circular trajectory centered on the target and maintain predetermined relative positions during the process to obtain high-precision spatio-temporal synchronization data. We design an end-to-end architecture that maps the sensor inputs to control commands and develop autonomous capable of achieving the hybrid objective of cooperative guidance, standoff tracking, and dynamic obstacle avoidance without having prior knowledge about the goal or the environment. The results demonstrate the feasibility of the end-to-end DRL method with higher accuracy than the traditional “guidance-control” two-step method. Meanwhile, the obstacle avoidance and standoff tracking experiment for swarm and the sampling experiment in the mesoscale eddy area are stimulated further to verify the proposed framework’s effectiveness and robust ability.}
}
@article{LIU2020109675,
title = {Study on deep reinforcement learning techniques for building energy consumption forecasting},
journal = {Energy and Buildings},
volume = {208},
pages = {109675},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2019.109675},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819324740},
author = {Tao Liu and Zehan Tan and Chengliang Xu and Huanxin Chen and Zhengfei Li},
keywords = {Energy consumption prediction, Ground source heat pump, Deep reinforcement learning, Asynchronous advantage Actor-Critic, Deep deterministic Policy gradient, Recurrent deterministic Policy gradient},
abstract = {Reliable and accurate building energy consumption prediction is becoming increasingly pivotal in building energy management. Currently, data-driven approach has shown promising performances and gained lots of research attention due to its efficiency and flexibility. As a combination of reinforcement learning and deep learning, deep reinforcement learning (DRL) techniques are expected to solve nonlinear and complex issues. However, very little is known about DRL techniques in forecasting building energy consumption. Therefore, this paper presents a case study of an office building using three commonly-used DRL techniques to forecast building energy consumption, namely Asynchronous Advantage Actor-Critic (A3C), Deep Deterministic Policy Gradient (DDPG) and Recurrent Deterministic Policy Gradient (RDPG). The objective is to investigate the potential of DRL techniques in building energy consumption prediction field. A comprehensive comparison between DRL models and common supervised models is also provided. The results demonstrate that the proposed DDPG and RDPG models have obvious advantages in forecasting building energy consumption compared to common supervised models, while accounting for more computation time for model training. Their prediction performances measured by mean absolute error (MAE) can be improved by 16%-24% for single-step ahead prediction, and 19%-32% for multi-step ahead prediction. The results also indicate that A3C performs poor prediction accuracy and shows much slower convergence speed than DDPG and RDPG. However, A3C is still the most efficient technique among these three DRL methods. The findings are enlightening and the proposed DRL methodologies can be positively extended to other prediction problems, e.g., wind speed prediction and electricity load prediction.}
}
@article{GUAN2022108258,
title = {Structural dominant failure modes searching method based on deep reinforcement learning},
journal = {Reliability Engineering & System Safety},
volume = {219},
pages = {108258},
year = {2022},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2021.108258},
url = {https://www.sciencedirect.com/science/article/pii/S0951832021007341},
author = {Xiaoshu Guan and Zhengliang Xiang and Yuequan Bao and Hui Li},
keywords = {Structural system, Deep reinforcement learning, Deep neural network, Dominant failure mode},
abstract = {The dominant failure modes (DFMs) of a structural system are significant for structural analysis and failure probability estimation. However, existing failure modes (FMs) searching methods often face problems of combinatorial explosion. To address this issue, a deep reinforcement learning (DRL)-based method is proposed for DFMs searching, which transforms the probability-based failure component selection process into a sequential decision process. First, the failure stages and the selected failure components of a structural system are transformed to be the states and actions in the DRL. Second, a deep neural network (DNN) is established to observe the failure stages and select failure components. Finally, a new reward function is designed to guide the network to learn the failure component selection policy. The proposed method was tested through a roof truss structure and a truss bridge structure. It was demonstrated that the trained DNN could learn to observe the failure stages and select the most critical components in a completely unknown testing set. High accuracy of the identified DFMs can be achieved. In comparison with the calculation results of Monte Carlo Simulation (MCS) and β-unzipping method, this proposed method shows significant computational efficiency advantages with high accuracy in dealing with combinatorial explosion.}
}
@article{ZHANG2022121926,
title = {Bi-level stochastic real-time pricing model in multi-energy generation system: A reinforcement learning approach},
journal = {Energy},
volume = {239},
pages = {121926},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121926},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221021745},
author = {Li Zhang and Yan Gao and Hongbo Zhu and Li Tao},
keywords = {Smart grid, Real-time pricing, Bilevel programming, Reinforcement learning, Markov decision process, Multi-energy generation},
abstract = {With the penetration of intermittent renewable energy sources, greater uncertainty has been brought to the power generation system, creating increased challenges to real-time pricing (RTP). Different from the existing studies, this paper aims to design an RTP strategy for the smart grid which integrates multi-energy generation on the supply side. Without loss of generality, small-scale distributed energy generation and power storage devices for users are also considered. Taking the interests of both supply and demand sides into consideration, a bilevel stochastic model for real-time demand response in the framework of Markov decision process (MDP) is formulated. The model well captures the interactive characters of both sides. Regarding the difficulty of collecting exact information from users in a centralized way in practice, a novel distributed online multi-agent reinforcement learning algorithm is proposed to solve the MDP model without acquisition of the transition probabilities. Through the information interaction between the upper and lower levels, the real-time electricity prices are decided adaptively, meanwhile, the optimal strategy of power supply and consumption is obtained. Simulation results demonstrate that the proposed pricing method and algorithm have a good performance in cutting peak and filling the valley and guarantee the benefits of both supply and demand.}
}
@article{REN2023120813,
title = {Reinforcement Learning-Based Bi-Level strategic bidding model of Gas-fired unit in integrated electricity and natural gas markets preventing market manipulation},
journal = {Applied Energy},
volume = {336},
pages = {120813},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.120813},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923001770},
author = {Kezheng Ren and Jun Liu and Xinglei Liu and Yongxin Nie},
keywords = {Gas-fired unit, Anti-market manipulation, Strategic bidding, Deep reinforcement learing (DRL), Demand response},
abstract = {Due to its efficient operation and environment-friendly characteristic, gas-fired unit (GFU) plays a more and more important role in electric power systems and natural gas systems. To investigate the performance of GFU's participation in integrated electricity and natural gas markets, a bi-level strategic bidding model considering price and quantity factors is proposed. With the increasing participation of consumers in electricity markets, demand response (DR) management is implemented in the electricity market clearing process and user comfort level (UCL) is considered in the market clearing model. Since GFU participates in both the electricity market and the natural gas market, a local marginal price penalty (LMPP) variable is defined in this paper to prevent potential market manipulation (MM) of GFU. Then a modified reinforcement learning (RL)-based method is proposed to solve the model, combining deep deterministic policy gradient (DDPG) algorithm with autocorrelated noise. Test results on an integrated electricity-gas system show that the proposed method can reflect the strategic behaviors of GFU effectively. The proposed method has better performance than traditional DDPG algorithm with Gaussian noise and the Deep Q-Network (DQN) algorithm. And electricity markets with LMPP can save about 3.03% in generation cost by preventing MM of GFU.}
}
@article{AKAM202174,
title = {What is dopamine doing in model-based reinforcement learning?},
journal = {Current Opinion in Behavioral Sciences},
volume = {38},
pages = {74-82},
year = {2021},
note = {Computational cognitive neuroscience},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2020.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S2352154620301558},
author = {Thomas Akam and Mark E Walton},
abstract = {Experiments have implicated dopamine in model-based reinforcement learning (RL). These findings are unexpected as dopamine is thought to encode a reward prediction error (RPE), which is the key teaching signal in model-free RL. Here we examine two possible accounts for dopamine’s involvement in model-based RL: the first that dopamine neurons carry a prediction error used to update a type of predictive state representation called a successor representation, the second that two well established aspects of dopaminergic activity, RPEs and surprise signals, can together explain dopamine’s involvement in model-based RL.}
}
@article{CHATTERJEE2023110766,
title = {Dynamic indoor thermal environment using Reinforcement Learning-based controls: Opportunities and challenges},
journal = {Building and Environment},
volume = {244},
pages = {110766},
year = {2023},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2023.110766},
url = {https://www.sciencedirect.com/science/article/pii/S036013232300793X},
author = {Arnab Chatterjee and Dolaana Khovalyg},
keywords = {Reinforcement Learning, Dynamic indoor environment, HVAC controls, Energy efficiency, Thermal comfort, Temperature drifting},
abstract = {Currently, the indoor thermal environment in many buildings is controlled by conventional control techniques that maintain the indoor temperature within a prescribed deadband. The latest research provides evidence that more dynamic variations of the indoor thermal environment can promote health and trigger positive thermal alliesthesia , but such an environment requires a flexible and responsive control system that can adapt to the changes in real-time. As an emerging control technique, Reinforcement Learning (RL) has attracted growing research interest and demonstrated its potential to enhance building performance while addressing some limitations of other advanced control techniques. Thus, a comprehensive review explored the boundaries and limitations of a dynamic indoor environment and the possibilities to apply RL for building controls suitable for varying the indoor thermal environment. The first part discussed the studies on the permissible limits of temperature step changes and acceptable drifts to human occupants. It also debated the flexibility of the range of human thermal comfort and adaptation. In the next part, studies on RL for HVAC controls were explored, focusing on their application in creating a dynamic indoor thermal environment. The different algorithms, HVAC systems, co-simulation environment, action spaces, and energy-saving potentials were discussed. Overall, based on the review, this work outlined a potential pathway for the RL-based controller that can dynamically vary the indoor temperature. Suitable environmental parameters to be controlled, a choice of the RL-based algorithm, action space, and co-simulation environment are discussed.}
}
@article{BRANDONISIO2023,
title = {Deep reinforcement learning spacecraft guidance with state uncertainty for autonomous shape reconstruction of uncooperative target},
journal = {Advances in Space Research},
year = {2023},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2023.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0273117723005276},
author = {Andrea Brandonisio and Lorenzo Capra and Michèle Lavagna},
keywords = {On-orbit servicing, Relative dynamics, Reinforcement learning, State uncertainty, Shape reconstruction},
abstract = {In recent years, space research has shifted heavily its focus towards enhanced autonomy on-board spacecrafts for on-orbit servicing activities (OOS). OOS and proximity operations include a variety of activities: the focal point of this work is the autonomous guidance of a chaser spacecraft for the shape reconstruction of an artificial uncooperative object. Adaptive guidance depends on the ability of the system to build a map of the uncertain environment, figuring out its location inside of it and accordingly determining the control law. Thus, autonomous navigation is framed as an active Simultaneous Localization and Mapping (SLAM) problem and modeled as a Partially Observable Markov Decision Process (POMDP). A state-of-the-art Deep Reinforcement Learning (DRL) method, Proximal Policy Optimization (PPO), is investigated to develop an agent capable of cleverly planning the shape reconstruction of the target. Starting from previous research on the topic, this work develops further proposing a continuous action space, such that the agent is no more forced to choose between a predefined set of possible discrete actions, fixed both in magnitude and direction. In this way any combination of the three-dimensional thrust vector components is available. The chaser spacecraft is a small satellite mounting an electric propulsion engine defining the action space range, in linearized eccentric relative motion with the selected uncooperative object. Through a rendered triangular mesh, the agent capabilities of geometry reconstruction and mapping are evaluated, considering the number of quality pictures made for each face. Extensive training tests are performed with random initial conditions to verify the generalizing capability of the DRL agent. The results are then validated in a comprehensive testing campaign, whose primary focus is the introduction of noisy measurements coming from navigation, affecting pose estimation. The sensitivity of the proposed method to this condition is analyzed and the efficiency of a retraining procedure is examined. The applicability of DRL methods and neural networks to support autonomous guidance in a close proximity scenario is corroborated and the technique employed is vastly tested and verified.}
}
@article{IKONEN2020106994,
title = {Reinforcement learning of adaptive online rescheduling timing and computing time allocation},
journal = {Computers & Chemical Engineering},
volume = {141},
pages = {106994},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106994},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419310774},
author = {Teemu J. Ikonen and Keijo Heljanko and Iiro Harjunkoski},
keywords = {Online scheduling, Rescheduling procedures, Reinforcement learning, Decision-making, Timing, Computing resource allocation},
abstract = {Mathematical optimization methods have been developed to a vast variety of complex problems in the field of process systems engineering (e.g., the scheduling of chemical batch processes). However, the use of these methods in online scheduling is hindered by the stochastic nature of the processes and prohibitively long solution times when optimized over long time horizons. The following questions are raised: When to trigger a rescheduling, how much computing resources to allocate, what optimization strategy to use, and how far ahead to schedule? We propose an approach where a reinforcement learning agent is trained to make the first two decisions (i.e., rescheduling timing and computing time allocation). Using neuroevolution of augmenting topologies (NEAT) as the reinforcement learning algorithm, the approach yields, on average, better closed-loop solutions than conventional rescheduling methods on three out of four studied routing problems. We also reflect on expanding the agent’s decision-making to all four decisions.}
}
@article{LIU2023117728,
title = {Coordinated energy management for integrated energy system incorporating multiple flexibility measures of supply and demand sides: A deep reinforcement learning approach},
journal = {Energy Conversion and Management},
volume = {297},
pages = {117728},
year = {2023},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2023.117728},
url = {https://www.sciencedirect.com/science/article/pii/S0196890423010749},
author = {Jiejie Liu and Yao Li and Yanan Ma and Ruomu Qin and Xianyang Meng and Jiangtao Wu},
keywords = {Integrated energy system (IES), Flexibility measure, Operation optimization, Deep reinforcement learning (DRL), Reward shaping},
abstract = {With the development of energy Internet and intelligent buildings, the interactions of supply and demand sides of integrated energy system (IES) offer an attractive route for flexible energy management in buildings. However, traditional model-based control methods over-rely on precise mathematical modeling, difficult to flexibly deal with complex and changeable operating environments of IES. Therefore, this work proposes a coordinated operation optimization framework based on the deep reinforcement learning (DRL) algorithm for optimal scheduling of IES. Firstly, two supply-side and three demand-side flexibility measures are considered to tap the potential of flexible scheduling, including active adjustment of energy conversion equipment, energy storage, incentive-based demand response, electric vehicles and thermal inertia of building. Secondly, the coordinated optimization is formulated as a partially-observable Markov decision process. The twin delayed deep deterministic policy (TD3) algorithm is employed to solve the optimal energy management problem of IES, aiming at operation cost and user satisfaction. Thirdly, a hierarchical reward shaping (HRS) mechanism is proposed to improve the training performance of DRL, which could evaluate the current and final performance of agent and return the underway reward at each step and final reward. The developed optimization methodology is used for a case study in the building. The results show that the proposed HRS-TD3 algorithm achieves the fastest convergence and has the best economic performance compared with the other baseline algorithms. The operation cost of coordinated optimization is superior to those of the three baseline scenarios and achieves an improvement of 33.1%, 3.5% and 29.8%, respectively.}
}
@article{TSURUMINE201972,
title = {Deep reinforcement learning with smooth policy update: Application to robotic cloth manipulation},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {72-83},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303245},
author = {Yoshihisa Tsurumine and Yunduan Cui and Eiji Uchibe and Takamitsu Matsubara},
keywords = {Deep reinforcement learning, Robotic cloth manipulation, Dynamic policy programming},
abstract = {Deep Reinforcement Learning (DRL), which can learn complex policies with high-dimensional observations as inputs, e.g., images, has been successfully applied to various tasks. Therefore, it may be suitable to apply them for robots to learn and perform daily activities like washing and folding clothes, cooking, and cleaning since such tasks are difficult for non-DRL methods that often require either (1) direct access to state variables or (2) well-designed hand-engineered features extracted from sensory inputs. However, applying DRL to real robots remains very challenging because conventional DRL algorithms require a huge number of training samples for learning, which is arduous in real robots. To alleviate this dilemma, in this paper, we propose two sample efficient DRL algorithms: Deep P-Network (DPN) and Dueling Deep P-Network (DDPN). The core idea is to combine the nature of smooth policy update with the capability of automatic feature extraction in deep neural networks to enhance the sample efficiency and learning stability with fewer samples. The proposed methods were first investigated by a robot-arm reaching task in the simulation that compared previous DRL methods and applied to two real robotic cloth manipulation tasks: (1) flipping a handkerchief and (2) folding a t-shirt with a limited number of samples. All the results suggest that our method outperformed the previous DRL methods.}
}
@article{LIU202289,
title = {A context-based meta-reinforcement learning approach to efficient hyperparameter optimization},
journal = {Neurocomputing},
volume = {478},
pages = {89-103},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.12.086},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221019421},
author = {Xiyuan Liu and Jia Wu and Senpeng Chen},
keywords = {Hyperparameter optimization, Reinforcement learning, Meta-learning, Deep learning},
abstract = {In this paper, we present a context-based meta-reinforcement learning approach to tackle the challenging data-inefficiency problem of Hyperparameter Optimization (HPO). Specifically, we design an agent which sequentially selects hyperparameters to maximize the expected accuracy of the machine learning algorithm on the validation set. First, we design a context variable that learns the latent embedding of prior experience, and the agent can solve the new tasks efficiently conditioned on it. Second, we employ a multi-task objective method that aims to maximize the average reward across all the meta-training tasks to meta-train the agent. Third, in the adaptation phase, we introduce a quadratic penalty technique to achieve better performance of the agent. Finally, to further improve the efficiency in the adaptation phase, we use a predictive model to evaluate the accuracy of machine learning algorithm instead of training it. We evaluate our approach on 18 real-world datasets and the results demonstrate that our approach outperforms other state-of-the-art optimization methods in terms of test set accuracy and runtime performance.}
}
@article{CHOI2023104049,
title = {Framework for Connected and Automated Bus Rapid Transit with Sectionalized Speed Guidance based on deep reinforcement learning: Field test in Sejong City},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {148},
pages = {104049},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2023.104049},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X23000384},
author = {Seongjin Choi and Donghoun Lee and Sari Kim and Sehyun Tak},
keywords = {Cooperative intelligent transportation system, Connected and automated vehicles, Bus rapid transit speed guidance, Deep reinforcement learning},
abstract = {Nowadays, Automated Vehicle (AV) technology is gaining attention as a candidate to improve the efficiency of Bus Rapid Transit (BRT) systems. However, there are still some challenges in AV technology including limited perception range and lack of cooperation capability in mixed traffic situations with drivers. The emerging Connected and Automated Vehicles (CAVs) and Cooperative Intelligent Transportation System (C-ITS) offer an unprecedented opportunity to solve such challenges. As a result, this study presents a framework for Connected and Automated BRT (CA-BRT), including a cloud-based architecture and a deep reinforcement learning system for Sectionalized Speed Guidance (SSG) system designed for CAVs. The proposed framework is field-tested in Sejong City in South Korea, where there are various road environments such as bus stops, overpasses, underground tunnels, intersections, and crosswalks. The driving performance of the proposed system is compared with different types of control scenarios, and the results from the field tests show that the proposed system improves the driving performance of the AVs in various aspects including driving safety, ride comfort, and energy efficiency with downstream information obtained from road infrastructures.}
}
@article{LI2021150,
title = {A Deep Reinforcement Learning Based Energy Management Strategy for Hybrid Electric Vehicles in Connected Traffic Environment},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {10},
pages = {150-156},
year = {2021},
note = {6th IFAC Conference on Engine Powertrain Control, Simulation and Modeling E-COSM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.156},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321015585},
author = {Jie Li and Xiaodong Wu and Sunan Hu and Jiawei Fan},
keywords = {Hybrid electric vehicles, Connected traffic environment, Car following, Energy management, Reinforcement learning control},
abstract = {This paper proposed a deep reinforcement learning based energy management strategy in the connected traffic environment. At the upper layer, a deep deterministic policy gradient algorithm is used to systematically integrate ego vehicle reference speed planning into the energy management strategy in the car following scenarios. At the bottom layer, a driver model and an adaptive equivalent consumption minimization strategy are implemented to conduct the optimal power split control based on the planned reference speed. As distance headway, fuel consumption, and terrain information are taken into consideration by the deep reinforcement learning controller, the proposed strategy can not only maintain a safe distance headway but also improve the energy efficiency of the entire driving cycle. Finally, the fuel consumption score of the proposed strategy is reduced by 3.5% compared with a car following strategy based on a proportional-integral controller. The simulation results show that the proposed strategy can learn a reasonable car following policy through the training process. In addition, the simulation duration for each reference speed planning step is only about 4.1 ms, which proves the proposed strategy can be applied online.}
}
@article{SWARUP202142,
title = {Task Scheduling in Cloud Using Deep Reinforcement Learning},
journal = {Procedia Computer Science},
volume = {184},
pages = {42-51},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921006281},
author = {Shashank Swarup and Elhadi M. Shakshuki and Ansar Yasar},
keywords = {task scheduling, computational cost, energy consumption, deep reinforcement learning, Clipped Double Deep Q-learning (CDDQL)},
abstract = {Cloud computing is an emerging technology used in many applications such as data analysis, storage, and Internet of Things (IoT). Due to the increasing number of users in the cloud and the IoT devices that are being integrated with the cloud, the amount of data generated by these users and these devices is increasing ceaselessly. Managing this data over the cloud is no longer an easy task. It is almost impossible to move all data to the cloud datacenters, and this will lead to excessive bandwidth usage, latency, cost, and energy consumption. This makes it evident that allocating resources to users’ tasks is an essential quality feature in cloud computing. This is because it provides the customers or the users with high Quality of Service (QoS) with the best response time, and it also respects the established Service Level Agreement. Therefore, there is a great importance of efficient utilization of computing resources for which an optimal strategy for task scheduling is required. This paper focuses on the problem of task scheduling of cloud-based applications and aims to minimize the computational cost under resource and deadline constraints. Towards this end, we propose a clipped double deep Q-learning algorithm utilizing the target network and experience relay techniques, as we as using the reinforcement learning approach.}
}
@article{ASLAN2023102167,
title = {Development of Push-Recovery control system for humanoid robots using deep reinforcement learning},
journal = {Ain Shams Engineering Journal},
volume = {14},
number = {10},
pages = {102167},
year = {2023},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2023.102167},
url = {https://www.sciencedirect.com/science/article/pii/S2090447923000564},
author = {Emrah Aslan and Muhammet Ali Arserim and Ayşegül Uçar},
keywords = {Deep reinforcement learning, deep q network(DQN), double deep q network(DDQN), Humanoid robot, Robotis op2, Push-recovery},
abstract = {This paper focuses on the push-recovery problem of bipedal humanoid robots affected by external forces and pushes. Since they are structurally unstable, balance is the most important problem in humanoid robots. Our purpose is to design and implement a completely independent push-recovery control system that can imitate the actions of a human. For humanoid robots to be able to stay in balance while standing or walking, and to prevent balance disorders that may be caused by external forces, an active balance control has been presented. Push-recovery controllers consist of three strategies: ankle strategy, hip strategy, and step strategy. These strategies are biomechanical responses that people show in cases of balance disorder. In our application, both simulation and real-world tests have been performed. The simulation tests of the study were carried out with 3D models in the Webots environment. Real-world tests were performed on the Robotis-OP2 humanoid robot. The gyroscope, accelerometer and motor data from the sensors in our robot were recorded and external pushing force was applied to the robot. The balance of the robot was achieved by using the recorded data and the ankle strategy. To make the robot completely autonomous, Deep Q Network (DQN) and Double Deep Q Network (DDQN) methods from Deep Reinforcement Learning (DPL) algorithms have been applied. The results obtained with the DDQN algorithm yielded 21.03% more successful results compared to the DQN algorithm. The results obtained in the real environment tests showed parallelism to the simulation results.}
}
@article{DRUGAN2019228,
title = {Reinforcement learning versus evolutionary computation: A survey on hybrid algorithms},
journal = {Swarm and Evolutionary Computation},
volume = {44},
pages = {228-246},
year = {2019},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2018.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S2210650217302766},
author = {Madalina M. Drugan},
keywords = {Reinforcement learning, Evolutionary computation, Natural paradigms, Hybrid algorithms, Survey},
abstract = {A variety of Reinforcement Learning (RL) techniques blends with one or more techniques from Evolutionary Computation (EC) resulting in hybrid methods classified according to their goal, new focus, and their component methodologies. We denote this class of hybrid algorithmic techniques as the evolutionary computation versus reinforcement learning (ECRL) paradigm. This overview considers the entire spectrum of algorithmic aspects and proposes a novel methodology that analyses the technical resemblances and differences in ECRL. Our design analyses the motivation for each ECRL paradigm, the underlying natural models, the sub-component algorithmic techniques, as well as the properties of their ensemble.}
}
@article{ABE2023134636,
title = {Integration of deep reinforcement learning to simple microfluidic system toward intelligent control: Demonstration of simultaneous microbeads manipulation},
journal = {Sensors and Actuators B: Chemical},
volume = {397},
pages = {134636},
year = {2023},
issn = {0925-4005},
doi = {https://doi.org/10.1016/j.snb.2023.134636},
url = {https://www.sciencedirect.com/science/article/pii/S0925400523013515},
author = {Takaaki Abe and Shinsuke Oh-hara and Yoshiaki Ukita},
keywords = {Microfluidics, Deep reinforcement learning, Micromanipulation, Microvalves, Fluid dynamics},
abstract = {In this paper, we report a novel intelligent microfluidic system that realizes automated micromanipulation of particles in a two-dimensional plane. The system manipulates particles using the flow generated by the operation of on-chip microvalves. The valves were controlled autonomously using a Double Deep Q network agent, whose parameters were optimized based on deep reinforcement learning. We deployed a policy learned from a simulator in a real environment. To realize a predictive model of particle displacement, which is necessary for building a simulator, we modeled the relationship between the valve operation and particle displacement in a liquid using a neural network and incorporated it into the simulator. The network for the simulation was trained by supervised learning using sampled data from a real microfluidic system. This enabled building the simulator for any device structure without using complicated physics theories. After confirming that the simulator could predict the displacement of particles, the manipulation system was trained through reinforcement learning, and experiments were conducted in a real environment. Single- and multiple-particle manipulations were performed, and we confirmed that the system had achieved the acquisition of particle manipulation capability.}
}
@article{HIRSCH2022381,
title = {Multi-objective pruning of dense neural networks using deep reinforcement learning},
journal = {Information Sciences},
volume = {610},
pages = {381-400},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.07.134},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522008222},
author = {Lior Hirsch and Gilad Katz},
keywords = {Pruning, Deep reinforcement learning},
abstract = {Network pruning aims to reduce the inference cost of large models and enable neural architectures to run on end devices such as mobile phones. We present NEON, a novel iterative pruning approach using deep reinforcement learning (DRL). While most reinforcement learning-based pruning solutions only analyze the one network they aim to prune, we train a DRL agent on a large set of randomly-generated architectures. Therefore, our proposed solution is more generic and less prone to overfitting. To avoid the long-running times often required to train DRL models for each new dataset, we train NEON offline on multiple datasets and then apply it to additional datasets without additional training. This setup makes NEON more efficient than other DRL-based pruning methods. Additionally, we propose a novel reward function that enables users to clearly define their pruning/performance trade-off preferences. Our evaluation, conducted on a set of 28 diverse datasets, shows that the proposed method significantly outperforms recent top-performing solutions in the pruning of fully-connected networks. Specifically, our top configuration reduces the average size of the pruned architecture by ×24.59, compared to ×13.26 by the leading baseline, while actually improving accuracy by 0.5%.}
}
@article{JIANG2023119490,
title = {Time series compression based on reinforcement learning},
journal = {Information Sciences},
volume = {648},
pages = {119490},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119490},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523010757},
author = {Nan Jiang and Qingping Xiang and Hongzhi Wang and Bo Zheng},
keywords = {Time series database, Data compression storage, Self-adaptive, Reinforcement learning},
abstract = {Nowadays, sensors and signal catchers in various fields are capturing time-series data all the time, and time-series data are exploding. Due to the large storage space requirements and redundancy, many compression techniques for time series have been proposed. However, the existing compression algorithms still face the challenge of the contradiction between random access and compression ratio. That is, in a time series database, large-scale time series data have high requirements on compression ratio, while large pieces of data need to be decompressed during the access process, resulting in poor query efficiency. In this paper, a proper solution is proposed to resolve such a contradiction. We propose a data compression method based on reinforcement learning, and use the idea of data deduplication to design the data compression method, so that the queries can be processed without decompression. We theoretically show that the proposed approach is effective and could ensure random accessing. To efficiently implement the reinforcement-learning-based solution, we develop a data compression method based on DQN network. Experiments show that the proposed algorithm performs well in time series data sets with large amount of data and strong regularity, performs well in compression ratio and compression time. Besides, since no decompression is required, the query processing time is much less than the competitors.}
}
@article{SALEHI2022103271,
title = {A reinforcement learning development of the FRAM for functional reward-based assessments of complex systems performance},
journal = {International Journal of Industrial Ergonomics},
volume = {88},
pages = {103271},
year = {2022},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2022.103271},
url = {https://www.sciencedirect.com/science/article/pii/S0169814122000129},
author = {V. Salehi and T.T. Tran and B. Veitch and D. Smith},
keywords = {Functional resonance analysis method (FRAM), Functional route exploration, Reinforcement learning (RL), Artificial intelligent (AI) agent, Accumulated reward},
abstract = {Although the Functional Resonance Analysis Method (FRAM) is a well-established approach to visualizing complex systems' operations in terms of functions, further improvements are required to examine systems' performance through functionality. This study aims to develop an approach to couple the FRAM to reinforcement learning (RL) to explore complex operations. The developed approach is called the functional RL approach and constitutes a novel way of using a FRAM model to explore functionality using an artificial intelligent (AI) agent who responds to reward values assigned to functional parameters. To exemplify the approach, an agent is employed to perform the role of a patient and explore a functional environment generated by the FRAM. Reward values are considered to motivate the agent in order to explore the environment to achieve its objective. The ability of the developed approach is examined using different scenarios implemented in healthcare operations. The results of using the functional RL approach indicate that the approach is able to specify the functional route taken by the agent and to examine the performance of the system based on accumulated rewards. The outcomes of this study demonstrate that the developed functional RL approach provides a novel means to explore operational environments to identify the routes that have potential to affect the system performance. This method can be used as a powerful way to assess how a system performs under different management structures.}
}
@article{JAFARI2020100096,
title = {A biologically-inspired reinforcement learning based intelligent distributed flocking control for Multi-Agent Systems in presence of uncertain system and dynamic environment},
journal = {IFAC Journal of Systems and Control},
volume = {13},
pages = {100096},
year = {2020},
issn = {2468-6018},
doi = {https://doi.org/10.1016/j.ifacsc.2020.100096},
url = {https://www.sciencedirect.com/science/article/pii/S2468601820300146},
author = {Mohammad Jafari and Hao Xu and Luis Rodolfo Garcia Carrillo},
keywords = {Biologically-inspired reinforcement learning based intelligent control, BELBIC, Flocking control, Multi-Agent Systems},
abstract = {In this paper, we investigate the real-time flocking control of Multi-Agent Systems (MAS) in the presence of system uncertainties and dynamic environment. To handle the impacts from system uncertainties and dynamic environment, a novel reinforcement learning technique, which is appropriate for real-time implementation, has been integrated with multi-agent flocking control in this paper. The Brain Emotional Learning Based Intelligent Controller (BELBIC) is a biologically-inspired reinforcement learning-based controller relying on a computational model of emotional learning in the mammalian limbic system. The learning capabilities, multi-objective properties, and low computational complexity of BELBIC make it a very promising learning technique for implementation in real-time applications. Firstly, a novel brain emotional learning-based flocking control structure is proposed. Then, the real-time update laws are developed to tune the emotional signals based on real-time operational data. It is important to note that this data-driven reinforcement learning approach relaxes the requirement for system dynamics and effectively handle the uncertain impacts of the environment. Using the tuned emotional signals, the optimal flocking control can be obtained. The Lyapunov analysis has been used to prove the convergence of the proposed design. The effectiveness of the proposed design is also demonstrated through numerical and experimental results based on the coordination of multiple Unmanned Aerial Vehicles (UAVs).}
}
@incollection{CAMPOS20221597,
title = {Deep Reinforcement Learning for Continuous Process Scheduling with Storage, Day-Ahead Pricing and Demand Uncertainty},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {1597-1602},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50266-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596502669},
author = {Gustavo Campos and Simge Yildiz and Nael H. El-Farra and Ahmet Palazoglu},
keywords = {Deep Reinforcement Learning, Operation, Optimization, Energy Systems, Demand Response},
abstract = {In this work, we evaluate the application of a Deep Reinforcement Learning (DRL) method for the scheduling of continuous process/energy systems under day-ahead electricity rate and demand forecast uncertainty. We employ the Soft Actor Critic (SAC) method, a stochastic, off-policy, actor-critic method with built-in entropy maximization that balances exploration and exploitation. We choose as a case study the dispatching of energy systems with storage, which can be posed as a continuous scheduling problem. Results from the computational case study demonstrate that the DRL agent is able to surpass a heuristic policy using very little data, and ultimately reaches a performance comparable to a model predictive control (MPC) solution. The effect of demand forecast uncertainty is further analysed and it is shown that, while the MPC performance degrades steadily as the forecast error and recalculation period increase, the DRL method exhibits a more robust performance.}
}
@article{SANAYHA2022107625,
title = {Model-based deep reinforcement learning for wind energy bidding},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {136},
pages = {107625},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107625},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521008577},
author = {Manassakan Sanayha and Peerapon Vateekul},
keywords = {Machine learning, Model-based reinforcement learning, Deep learning, Energy bidding strategy},
abstract = {Wind energy is an important source of clean energy. Due to the common trade through bidding, many attempts have been made to apply deep reinforcement learning techniques to generate appropriate bidding policies to maximize profits. However, these algorithms are based entirely on a model-free strategy. The present study aims to develop a dynamic model capable of strategic bidding for wind energy. Thus, the model MB-A3C is implemented and proves to be quite resilient. Herein, “Nord Pool”, a conventional benchmark that comprises six datasets representing each wind power site in Denmark and Sweden is duly investigated. Results show that the policies generated by MB-A3C are less costly than those produced by both previous model-free and model-based algorithms i.e. Conv-A3C, DPPO, DDPG, and MBPG. The optimal bidding approach demonstrated in this study can be utilized to optimize profits and overcome the uncertainties in both the energy and reserve markets.}
}
@article{LEI2022119742,
title = {A practical deep reinforcement learning framework for multivariate occupant-centric control in buildings},
journal = {Applied Energy},
volume = {324},
pages = {119742},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119742},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922010297},
author = {Yue Lei and Sicheng Zhan and Eikichi Ono and Yuzhen Peng and Zhiang Zhang and Takamasa Hasama and Adrian Chong},
keywords = {Occupant-centric control, Deep learning, Reinforcement learning, Thermal comfort, Energy efficiency},
abstract = {Reinforcement learning (RL) has been shown to have the potential for optimal control of heating, ventilation, and air conditioning (HVAC) systems. Although research on RL-based building control has received extensive attention in recent years, there is limited real-world implementation to evaluate its performance while keeping occupants in the loop. Additionally, many HVAC systems consist of multiple subsystems, but conventional RL algorithms face significant challenges when dealing with high-dimensional action spaces. This study proposes a practical deep reinforcement learning (DRL) based multivariate occupant-centric control framework that considers personalized thermal comfort and occupant presence. Specifically, Branching Dueling Q-network (BDQ) is leveraged as the learning agent to efficiently solve the multi-dimensional control task, and a tabular-based personal comfort modeling method is applied that is naturally integrated into human-in-the-loop operations. The BDQ agent is pre-trained in a virtual environment, followed by online deployment in a real office space for 5-dimensional action control. Based on the actual deployment and real-time comfort votes, our results showed a 14% reduction in cooling energy and an 11% improvement in total thermal acceptability.}
}
@article{MARTINEZ20001187,
title = {Batch process modeling for optimization using reinforcement learning},
journal = {Computers & Chemical Engineering},
volume = {24},
number = {2},
pages = {1187-1193},
year = {2000},
issn = {0098-1354},
doi = {https://doi.org/10.1016/S0098-1354(00)00354-9},
url = {https://www.sciencedirect.com/science/article/pii/S0098135400003549},
author = {E.C. Martinez},
keywords = {Batch process, Reaction kinetics, Modeling for optimization},
abstract = {Imperfect and incomplete understanding of reaction kinetics compounded with uncontrollable variations not only prevent achieving an optimal operation of batch and semi-batch reactors, but also give rise to potential risks of violating product end-use properties, ecological or safety constraints. This paper proposes a sequential experiment design strategy based on reinforcement learning to accomplish the specific goal of modeling for optimization in batch reactors by making the most effective use of cumulative data and an approximate model. Reactor operating condition is incrementally improved over runs by integrating together estimation of a probabilistic measure of success using an imperfect model and a gradient-based approach so as to trade off exploitation with exploration. An improved operating policy is found by incrementally shrinking the region of interest for policy parameters. The solution strategy focuses on ‘learning by doing’ using a value function that accounts for endpoint performance and feasibility. Simulation results reveal the robustness of reinforcement learning to parametric and structural modeling errors.}
}
@article{LIU2023111004,
title = {Improved reinforcement learning-based real-time energy scheduling for prosumer with elastic loads in smart grid},
journal = {Knowledge-Based Systems},
volume = {280},
pages = {111004},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111004},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123007542},
author = {Didi Liu and Pengpeng Cheng and Jun Cheng and Junxiu Liu and Meiqu Lu and Frank Jiang},
keywords = {Prosumer, Energy storage device, Real-time energy scheduling, Elastic loads, Reinforcement learning},
abstract = {In this study, we investigate the dynamic energy-scheduling problem of a prosumer (producer/consumer) with an energy storage device (EDS) and elastic loads. The goal is to develop efficient real-time scheduling strategies for prosumers, and to minimise their total long-term costs (i.e. cost of energy purchased from the external grid and depreciation cost of the EDS). The challenges are twofold: the uncertainty of the energy output of the prosumer and the time-coupling constraints of the EDS and elastic loads. To address these challenges, we first describe the dynamic energy-scheduling problem as a Markov decision process. Then, an approximate state dual-agent Q-learning algorithm is proposed to solve the optimal dynamic scheduling problem by improving the model-free reinforcement learning(RL) method. Compared with the traditional RL method, the proposed algorithm reduces the system-state dimensions and exhibits improved performance. The proposed algorithm can only be assisted by mutual interactions between the environment with a reward feedback mechanism to dynamically respond to uncertain changes in the environments, without modelling or predicting the system environments. Finally, extensive empirical evaluations using real-world traces are conducted to study the effectiveness of the proposed algorithm. The results show that the proposed algorithm can reduce the total cost of the prosumer by up to 6.3%, 11.7% and 22.4% compared with the traditional RL method, Lyapunov optimisation and greedy algorithm, respectively.}
}
@article{KAZMI20191022,
title = {Multi-agent reinforcement learning for modeling and control of thermostatically controlled loads},
journal = {Applied Energy},
volume = {238},
pages = {1022-1035},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.01.140},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919301564},
author = {Hussain Kazmi and Johan Suykens and Attila Balint and Johan Driesen},
keywords = {Multi agent reinforcement learning, Distributed learning, Optimal control, Thermostatically controlled loads, Domestic hot water storage vessel, Heat pumps},
abstract = {Increasing energy efficiency of thermostatically controlled loads has the potential to substantially reduce domestic energy demand. However, optimizing the efficiency of thermostatically controlled loads requires either an existing model or detailed data from sensors to learn it online. Often, neither is practical because of real-world constraints. In this paper, we demonstrate that this problem can benefit greatly from multi-agent learning and collaboration. Starting with no thermostatically controlled load specific information, the multi-agent modelling and control framework is evaluated over an entire year of operation in a large scale pilot in The Netherlands, constituting over 50 houses, resulting in energy savings of almost 200 kW h per household (or 20% of the energy required for hot water production). Theoretically, these savings can be even higher, a result also validated using simulations. In these experiments, model accuracy in the multi-agent frameworks scales linearly with the number of agents and provides compelling evidence for increased agency as an alternative to additional sensing, domain knowledge or data gathering time. In fact, multi-agent systems can accelerate learning of a thermostatically controlled load’s behaviour by multiple orders of magnitude over single-agent systems, enabling active control faster. These findings hold even when learning is carried out in a distributed manner to address privacy issues arising from multi-agent cooperation.}
}
@article{ALIPIO2023100846,
title = {Deep Reinforcement Learning Perspectives on Improving Reliable Transmissions in IoT Networks: Problem Formulation, Parameter Choices, Challenges, and Future Directions},
journal = {Internet of Things},
volume = {23},
pages = {100846},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100846},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523001695},
author = {Melchizedek Alipio and Miroslav Bures},
keywords = {Artificial Intelligence, Caching, Congestion control, Deep Reinforcement Learning, Internet of Things, Machine Learning},
abstract = {The majority of communication protocols used in IoT networks for caching and congestion control techniques were rule-based which implies that these protocols are dependent on explicitly stated static models. To solve this issue, techniques are becoming more adaptive to changes in the network environment by incorporating a learning-based approach using Machine Learning (ML) and Deep Learning (DL). Recent surveys and review papers have covered topics on the use of ML and DL in either caching or congestion control techniques used in various types of networks. However, there is not an article in the literature dedicated to surveying the design of caching and congestion control mechanisms in IoT networks from the perspective of a Deep Reinforcement Learning (DRL) problem. Hence, this work aimed to survey the state-of-the-art DRL-based caching and congestion control techniques in IoT networks from 2019 to 2023. It also presented general frameworks for DRL-based caching and congestion control techniques based on surveyed works as a baseline for designing future protocols in IoT networks. Moreover, this paper classified the parameter choices of surveyed DRL-based techniques and identified the issues and challenges behind these techniques. Finally, a discussion of the possible future directions of this research domain was presented.}
}
@article{DOSHIVELEZ2012115,
title = {Reinforcement learning with limited reinforcement: Using Bayes risk for active learning in POMDPs},
journal = {Artificial Intelligence},
volume = {187-188},
pages = {115-132},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000458},
author = {Finale Doshi-Velez and Joelle Pineau and Nicholas Roy},
keywords = {Partially observable Markov decision process, Reinforcement learning, Bayesian methods},
abstract = {Acting in domains where an agent must plan several steps ahead to achieve a goal can be a challenging task, especially if the agentʼs sensors provide only noisy or partial information. In this setting, Partially Observable Markov Decision Processes (POMDPs) provide a planning framework that optimally trades between actions that contribute to the agentʼs knowledge and actions that increase the agentʼs immediate reward. However, the task of specifying the POMDPʼs parameters is often onerous. In particular, setting the immediate rewards to achieve a desired balance between information-gathering and acting is often not intuitive. In this work, we propose an approximation based on minimizing the immediate Bayes risk for choosing actions when transition, observation, and reward models are uncertain. The Bayes-risk criterion avoids the computational intractability of solving a POMDP with a multi-dimensional continuous state space; we show it performs well in a variety of problems. We use policy queries—in which we ask an expert for the correct action—to infer the consequences of a potential pitfall without experiencing its effects. More important for human–robot interaction settings, policy queries allow the agent to learn the reward model without the reward values ever being specified.}
}
@article{PADULLAPARTHI2022445,
title = {FALCON- FArm Level CONtrol for wind turbines using multi-agent deep reinforcement learning},
journal = {Renewable Energy},
volume = {181},
pages = {445-456},
year = {2022},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2021.09.023},
url = {https://www.sciencedirect.com/science/article/pii/S0960148121013227},
author = {Venkata Ramakrishna Padullaparthi and Srinarayana Nagarathinam and Arunchandar Vasan and Vishnu Menon and Depak Sudarsanam},
keywords = {Wind farm control, Coordinated control, Reinforcement learning, Fatigue, Wake, Auto-encoder},
abstract = {Turbines in a wind farm dynamically influence each other through wakes. Therefore trade-offs exist between energy output of upstream turbines and the health of downstream turbines. Using both model-based predictive control (MPC) and machine learning techniques, existing works have explored the energy-fatigue trade-off either in a single turbine or only with few turbines due to issues of scalability and complexity. To address this gap, this paper proposes a multi-agent deep reinforcement learning-based coordinated control for wind farms, called FALCON. FALCON addresses the multi-objective optimization problem of maximizing energy while minimizing fatigue damage by jointly controlling pitch and yaw of all turbines. FALCON achieves scale by using multiple reinforcement learning agents; capturing the global state-space efficiently using an auto-encoder; and pruning the action-space using domain knowledge. FALCON is evaluated through a real-world wind-farm case study with 21 turbines; and performs better than the default baseline PID controller and a learning-based distributed control.}
}
@article{HUANG2011127,
title = {Reinforcement learning based resource allocation in business process management},
journal = {Data & Knowledge Engineering},
volume = {70},
number = {1},
pages = {127-145},
year = {2011},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2010.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X1000114X},
author = {Zhengxing Huang and W.M.P. {van der Aalst} and Xudong Lu and Huilong Duan},
keywords = {Resource allocation, Business process, Markov decision process, Reinforcement learning, Q-learning},
abstract = {Efficient resource allocation is a complex and dynamic task in business process management. Although a wide variety of mechanisms are emerging to support resource allocation in business process execution, these approaches do not consider performance optimization. This paper introduces a mechanism in which the resource allocation optimization problem is modeled as Markov decision processes and solved using reinforcement learning. The proposed mechanism observes its environment to learn appropriate policies which optimize resource allocation in business process execution. The experimental results indicate that the proposed approach outperforms well known heuristic or hand-coded strategies, and may improve the current state of business process management.}
}
@article{SUN2023106197,
title = {Event-triggered reconfigurable reinforcement learning motion-planning approach for mobile robot in unknown dynamic environments},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106197},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106197},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623003810},
author = {Huihui Sun and Changchun Zhang and Chunhe Hu and Junguo Zhang},
keywords = {Mobile robot, Reinforcement learning, Actor–critic, Reconfigurable structure, Sample pretreatment, Event-triggered},
abstract = {Deep reinforcement learning (DRL) is an essential technique for autonomous motion planning of mobile robots in dynamic and uncertain environments. In attempting to acquire a satisfactory DRL-based motion planning strategy, the mobile robots encountered several difficulties, including poor convergence, insufficient sample information, and low learning efficiency. These problems not only consume plenty of training time, but also bring a negative impact on motion planning performance. One promising research direction is to provide a more effective network framework for DRL-based policies. Along this line of thinking, our paper presents a novel DRL-based motion planning approach called Reconfigurable Structure of Deep Deterministic Policy Gradient (RS-DDPG) for mobile robots. To account for the poor convergence, the proposed approach first introduces an event-triggered reconfigurable actor–critic network framework for motion policy that adaptively changes its network structure to suppress the overestimation of action value. Then, the time convergence of the motion policy can be enhanced based on the value actions with minor valuation deviation. Afterwards, an adaptive reward mechanism is designed for reconfigurable networks to compensate for the lack of sample information. To deal with the problem of low learning efficiency, we developed a sample pretreatment method for the experience samples, which employs three novel techniques to improve the sample utilization, including a double experience memory buffer, a variable proportional sampling principle, and a similarity judgment mechanism. In extensive experiments, the proposed method outperforms the compared approaches.}
}
@article{WANG2023101871,
title = {Multi-granularity fusion resource allocation algorithm based on dual-attention deep reinforcement learning and lifelong learning architecture in heterogeneous IIoT},
journal = {Information Fusion},
volume = {99},
pages = {101871},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101871},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523001872},
author = {Ying Wang and Fengjun Shang and Jianjun Lei},
keywords = {Deep reinforcement learning, Resource allocation, Heterogeneous industrial Internet of Things, Lifelong learning, Federated meta learning},
abstract = {Deep reinforcement learning (DRL) is a promising technology to address the resource allocation problem for efficient data transmission in complex network environments. However, most DRL-based resource allocation algorithms suffer from limited feature extraction capabilities and lack scalability and generalization, especially in heterogeneous Industrial Internet of Things (IIoT) environments. In this paper, we develop a lifelong learning architecture that can integrate artificial intelligence (AI) algorithms into the heterogeneous IIoT network for efficient data transmission. Based on this, we propose an intelligent resource allocation algorithm based on dual-attention DRL (DADR) for forwarding node selection and channel access slot allocation in a specific network environment. The proposed DADR algorithm combines the advantages of multi-dimension convolutional attention and multi-head self-attention mechanisms. It can provide local- and global-feature fusion capabilities for distributed nodes while maximizing the performance of data transmission. Furthermore, we present a lifelong federated meta reinforcement learning (LFMRL) that can effectively utilize prior knowledge and enable the DRL agent quickly adapt to a new environment. Specifically, LFMRL adopts a federated meta learning-based knowledge fusion algorithm to fuse the knowledge of learned DADR algorithms and iteratively update the shared model, thereby improving the scalability and generalization of the shared model in heterogeneous IIoT environments. In addition, a simple and efficient knowledge transfer mechanism is enabled to accelerate the DRL model convergence by transferring the knowledge of the shared model to the new environment. Simulation results demonstrate the effectiveness of the proposed algorithms in terms of energy efficiency, data transmission reliability, and network stability. Compared to DADR and FedAvg algorithms, LFMRL algorithm can further reduce the energy consumption, training time, and average forwarding node switching times, while improving packet delivery rate to 99.2%.}
}
@article{YAN2022170167,
title = {Secure wireless network system based on deep reinforcement learning network},
journal = {Optik},
volume = {271},
pages = {170167},
year = {2022},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2022.170167},
url = {https://www.sciencedirect.com/science/article/pii/S0030402622014255},
author = {Xiaolong Yan and Yingying Feng},
keywords = {Deep reinforcement learning network, Security, Wireless network, System performance},
abstract = {In order to improve the security of the secure wireless network system, this paper studies the security of the wireless network system based on the deep reinforcement learning network. Moreover, this paper divides the entire network into a domain structure, and each domain structure includes ordinary nodes, council nodes, key nodes, and dynamic gateway nodes. The key node selects the dynamic gateway node in the domain, and after the dynamic gateway node processes the message, it is forwarded to the gateway node in the adjacent domain. In addition, this paper studies the application of multi-objective optimization in trust management mechanism in secure routing. Furthermore, this paper builds a secure wireless network system based on the deep reinforcement learning network based on the deep reinforcement learning network, and builds a system architecture. Finally, this paper tests the secure wireless network system based on deep reinforcement learning network through simulation experiments. The research results show that the safety factor of the system is relatively high.}
}
@article{MANEE2022111,
title = {Learning to navigate a crystallization model with Deep Reinforcement Learning},
journal = {Chemical Engineering Research and Design},
volume = {178},
pages = {111-123},
year = {2022},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2021.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0263876221005037},
author = {Vidhyadhar Manee and Roberto Baratti and Jose A. Romagnoli},
keywords = {Deep Reinforcement Learning, Convolutional Neural Networks, Crystallization, Process control},
abstract = {In this work, a combination of a Convolutional Neural Network (CNN) based measurement sensor and a reinforcement learning (RL) framework that speeds up the control loop is presented. The objective of the controller is to reach a target mean size and to reduce the variability of the crystal sizes. The CNN based sensor improves the quality of crystal size measurement and reduces the time to process images while the RL framework learns to navigate the crystallization model optimally even in the face of disturbances. The proposed data driven strategy is validated against an unseeded crystallization of sodium chloride in water using ethanol as antisolvent in an experimental bench-scale semi-batch crystallizer. We find that the RL-based controller can be trained offline to optimize multiple target conditions while the CNN provides accurate feedback for the controller to recompute the optimal actions in the face of disturbances and guide the system towards the target.}
}
@article{WU2020381,
title = {Efficient hyperparameter optimization through model-based reinforcement learning},
journal = {Neurocomputing},
volume = {409},
pages = {381-393},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.06.064},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220310523},
author = {Jia Wu and SenPeng Chen and XiYuan Liu},
keywords = {Hyperparameter optimization, Machine learning, Reinforcement learning},
abstract = {Hyperparameter tuning is critical for the performance of machine learning algorithms. However, a noticeable limitation is the high computational cost of algorithm evaluation for complex models or for large datasets, which makes the tuning process highly inefficient. In this paper, we propose a novel model-based method for efficient hyperparameter optimization. Firstly, we frame this optimization process as a reinforcement learning problem and then employ an agent to tune hyperparameters sequentially. In addition, a model that learns how to evaluate an algorithm is used to speed up the training. However, model inaccuracy is further exacerbated by long-term use, resulting in collapse performance. We propose a novel method for controlling the model use by measuring the impact of the model on the policy and limiting it to a proper range. Thus, the horizon of the model use can be dynamically adjusted. We apply the proposed method to tune the hyperparameters of the extreme gradient boosting and convolutional neural networks on 101 tasks. The experimental results verify that the proposed method achieves the highest accuracy on 86.1% of the tasks, compared with other state-of-the-art methods and the average ranking of runtime is significant lower than all methods by using the predictive model.}
}