"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"A decision-making method for autonomous vehicles based on simulation and reinforcement learning","Rui Zheng; Chunming Liu; Qi Guo","College of Mechatronics and Automation, National University of Defense Technology, Changsha, P. R. China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, P. R. China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, P. R. China","2013 International Conference on Machine Learning and Cybernetics","8 Sep 2014","2013","01","","362","369","There are still some problems need to be solved though there are a lot of achievements in the field of automatic driving. One of those problems is the difficulty of designing a decision-making system for complex traffic conditions. In recent years, reinforcement learning (RL) shows the potential in solving sequential decision optimization problems, which can be modeled as Markov decision processes (MDPs). In this paper, we establish a 14-DOF dynamic model of an autonomous vehicle and use RL to build a decision-making system for autonomous driving based on simulation. The decision-making process of the vehicle is modeled as an MDP, and the performance of the MDP is improved using an approximate RL. At last, we show the efficiency of the proposed method by simulation in a highway environment.","2160-1348","978-1-4799-0260-6","10.1109/ICMLC.2013.6890495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890495","Autonomous Vehicles;Reinforcement learning;Markov Decision Process;Autonomous driving;Decision-making","Vehicles;Three-dimensional displays;Abstracts;Markov processes;DSL","decision making;learning (artificial intelligence);learning systems;Markov processes;mobile robots;road vehicles;robot dynamics","decision-making method;autonomous vehicles;reinforcement learning;automatic driving;decision-making system;complex traffic conditions;sequential decision optimization problem;Markov decision process;MDP;14-DOF dynamic model;autonomous driving;highway environment","","16","","15","IEEE","8 Sep 2014","","","IEEE","IEEE Conferences"
"Optimal Synchronization Control of Heterogeneous Asymmetric Input-Constrained Unknown Nonlinear MASs via Reinforcement Learning","L. Xia; Q. Li; R. Song; H. Modares","Beijing Engineering Research Center of Industrial Spectrum Imaging, School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Beijing Engineering Research Center of Industrial Spectrum Imaging, School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Beijing Engineering Research Center of Industrial Spectrum Imaging, School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Department of Mechanical Engineering, Michigan State University, East Lansing, MI, USA","IEEE/CAA Journal of Automatica Sinica","28 Dec 2021","2022","9","3","520","532","The asymmetric input-constrained optimal synchronization problem of heterogeneous unknown nonlinear multiagent systems (MASs) is considered in the paper. Intuitively, a state-space transformation is performed such that satisfaction of symmetric input constraints for the transformed system guarantees satisfaction of asymmetric input constraints for the original system. Then, considering that the leader's information is not available to every follower, a novel distributed observer is designed to estimate the leader's state using only exchange of information among neighboring followers. After that, a network of augmented systems is constructed by combining observers and followers dynamics. A nonquadratic cost function is then leveraged for each augmented system (agent) for which its optimization satisfies input constraints and its corresponding constrained Hamilton-Jacobi-Bellman (HJB) equation is solved in a data-based fashion. More specifically, a data-based off-policy reinforcement learning (RL) algorithm is presented to learn the solution to the constrained HJB equation without requiring the complete knowledge of the agents' dynamics. Convergence of the improved RL algorithm to the solution to the constrained HJB equation is also demonstrated. Finally, the correctness and validity of the theoretical results are demonstrated by a simulation example.","2329-9274","","10.1109/JAS.2021.1004359","National Natural Science Foundation of China(grant numbers:61873300,61722312); Fundamental Research Funds for the Central Universities(grant numbers:FRF-MP-20-11); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9646177","Asymmetric input-constrained;heterogeneous nonlinear multiagent systems (MASs);Hamilton-Jacobi-Bellman (HJB) equation;novel observer;reinforcement learning (RL)","Heuristic algorithms;Reinforcement learning;Artificial neural networks;Observers;Approximation algorithms;Cost function;Prediction algorithms","control system synthesis;convergence;learning (artificial intelligence);Lyapunov methods;multi-agent systems;nonlinear control systems;optimal control;synchronisation;time-varying systems","symmetric input constraints;transformed system;asymmetric input constraints;leader;novel distributed observer;neighboring followers;augmented system;observers;followers dynamics;corresponding constrained Hamilton-Jacobi-Bellman equation;data-based fashion;data-based off-policy reinforcement learning;constrained HJB equation;optimal synchronization control;heterogeneous asymmetric input-constrained;nonlinear MASs;asymmetric input-constrained optimal synchronization problem;heterogeneous unknown nonlinear multiagent systems;state-space transformation","","12","","58","","10 Dec 2021","","","IEEE","IEEE Journals"
"Self-Tuning MPPT Scheme Based on Reinforcement Learning and Beta Parameter in Photovoltaic Power Systems","D. Lin; X. Li; S. Ding; H. Wen; Y. Du; W. Xiao","School of Electrical and Automation Engineering, Nanjing Normal University, Nanjing, China; School of Electrical and Automation Engineering, Nanjing Normal University, Nanjing, China; School of Electrical and Automation Engineering, Nanjing Normal University, Nanjing, China; Xi’an Jiaotong–Liverpool University, Suzhou, China; College of Science and Engineering, James Cook University, Cairns, QLD, Australia; University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Power Electronics","20 Aug 2021","2021","36","12","13826","13838","Maximum power point tracking (MPPT) is required in PV power systems for the highest solar energy harvest. This article proposes a self-tuning scheme to improve the MPPT performance in terms of high accuracy and speed. The scheme adopts the reinforcement learning (RL) and Beta parameter for the highest MPPT performance. The tracking speed and accuracy are significantly improved since the RL algorithm is enhanced for high convergence speed, meanwhile, the guiding variable $\beta$ is introduced to constrain the exploration space. Simulation and experimental test are applied to validate the superior performance of the proposed solution following the EN50530 dynamic test procedure.","1941-0107","","10.1109/TPEL.2021.3089707","National Natural Science Foundation of China(grant numbers:51977112); Natural Science Research Project of Jiangsu Higher Education Institutions(grant numbers:20KJB470020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9457135","Control engineering;maximum power point tracking (MPPT);optimization;photovoltaic power system;reinforcement learning (RL);self-tuning","Reinforcement learning;Frequency selective surfaces;Space exploration;Maximum power point trackers;Tuning;Ions;Heuristic algorithms","energy harvesting;learning (artificial intelligence);maximum power point trackers;photovoltaic power systems;tuning","convergence speed;self-tuning MPPT scheme;reinforcement learning;beta parameter;photovoltaic power systems;maximum power point tracking;PV power systems;solar energy harvest;tracking speed;RL algorithm;EN50530 dynamic test","","11","","32","IEEE","16 Jun 2021","","","IEEE","IEEE Journals"
"A comparative study of urban traffic signal control with reinforcement learning and Adaptive Dynamic Programming","Y. Dai; D. Zhao; J. Yi","Laboratory of Complex Systems and Intelligence Science, Institute of Automation, Chinese Academy and Sciences, Beijing, China; Laboratory of Complex Systems and Intelligence Science, Institute of Automation, Chinese Academy and Sciences, Beijing, China; Laboratory of Complex Systems and Intelligence Science, Institute of Automation, Chinese Academy and Sciences, Beijing, China","The 2010 International Joint Conference on Neural Networks (IJCNN)","14 Oct 2010","2010","","","1","7","This paper proposes a new algorithm that employs Adaptive Dynamic Programming(ADP) to solve the distributed control problem of urban traffic with an infinite horizon. Urban traffic congestions lead to a lot of time consumption and exhaust emissions. So alleviating congested situation will have a good impact on both economy and environment. The signal control at urban intersections is an effective and most important way to reduce the traffic jams and collisions. A lot of control theories including traditional mathematical ways and modern artificial intelligent ways have been exploited. ADP is an effective and amiable intelligent control method. We proposed an algorithm to adjust the signal time plan at urban traffic intersections based on ADP theory. Simulations are taken under a microscopic traffic simulation software, TSIS(Traffic Software Integrated System). Several criteria named MOEs(Measures of Effectiveness) are collected to compare with the widely used pre-timed control, actuated control, also with a machine learning method Q-learning control. Results show that ADP control method have a better adaptability to the various traffic simulating real traffic flows.","2161-4407","978-1-4244-6918-5","10.1109/IJCNN.2010.5596480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5596480","","Vehicles;Machine learning algorithms;Heuristic algorithms;Software algorithms;Software;Dynamic programming;Green products","dynamic programming;learning (artificial intelligence);traffic control;traffic engineering computing","urban traffic signal control;reinforcement learning;adaptive dynamic programming;urban traffic congestion;artificial intelligent;traffic software integrated system;microscopic traffic simulation software;machine learning method;Q-learning control","","11","","18","IEEE","14 Oct 2010","","","IEEE","IEEE Conferences"
"Attention-Based Meta-Reinforcement Learning for Tracking Control of AUV With Time-Varying Dynamics","P. Jiang; S. Song; G. Huang","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","27 Oct 2022","2022","33","11","6388","6401","Reinforcement learning (RL) is a promising technique for designing a model-free controller by interacting with the environment. Several researchers have applied RL to autonomous underwater vehicles (AUVs) for motion control, such as trajectory tracking. However, the existing RL-based controller usually assumes that the unknown AUV dynamics keep invariant during the operation period, limiting its further application in the complex underwater environment. In this article, a novel meta-RL-based control scheme is proposed for trajectory tracking control of AUV in the presence of unknown and time-varying dynamics. To this end, we divide the tracking task for AUV with time-varying dynamics into multiple specific tasks with fixed time-varying dynamics, to which we apply meta-RL for training to distill the general control policy. The obtained control policy can transfer to the testing phase with high adaptability. Inspired by the line-of-sight (LOS) tracking rule, we formulate each specific task as a Markov decision process (MDP) with a well-designed state and reward function. Furthermore, a novel policy network with an attention module is proposed to extract the hidden information of AUV dynamics. The simulation environment with time-varying dynamics is established, and the simulation results reveal the effectiveness of our proposed method.","2162-2388","","10.1109/TNNLS.2021.3079148","Major Research Plan of the National Major Research Program(grant numbers:2018AAA0101604); Key Research and Development Special Project of Guangdong Province(grant numbers:2020B1111500002); National Natural Science Foundation of China(grant numbers:62022048); THUBosch JCML Center; Beijing Academy of Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439903","Attention mechanism;meta-reinforcement learning (meta-RL);time-varying dynamics;trajectory tracking","Vehicle dynamics;Task analysis;Trajectory tracking;Trajectory;Tracking;Heuristic algorithms;Adaptation models","autonomous underwater vehicles;learning (artificial intelligence);Markov processes;mobile robots;motion control;remotely operated vehicles;underwater vehicles","complex underwater environment;existing RL-based controller;fixed time-varying dynamics;general control policy;line-of-sight tracking rule;meta-reinforcement learning;model-free controller;motion control;novel meta-RL-based control scheme;tracking task;trajectory tracking control;unknown AUV dynamics","","10","","47","IEEE","24 May 2021","","","IEEE","IEEE Journals"
"Multiagent Meta-Reinforcement Learning for Adaptive Multipath Routing Optimization","L. Chen; B. Hu; Z. -H. Guan; L. Zhao; X. Shen","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; Department of Electrical, Computer and Biomedical Engineering, Ryerson University, Toronto, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada","IEEE Transactions on Neural Networks and Learning Systems","5 Oct 2022","2022","33","10","5374","5386","In this article, we investigate the routing problem of packet networks through multiagent reinforcement learning (RL), which is a very challenging topic in distributed and autonomous networked systems. In specific, the routing problem is modeled as a networked multiagent partially observable Markov decision process (MDP). Since the MDP of a network node is not only affected by its neighboring nodes’ policies but also the network traffic demand, it becomes a multitask learning problem. Inspired by recent success of RL and metalearning, we propose two novel model-free multiagent RL algorithms, named multiagent proximal policy optimization (MAPPO) and multiagent metaproximal policy optimization (meta-MAPPO), to optimize the network performances under fixed and time-varying traffic demand, respectively. A practicable distributed implementation framework is designed based on the separability of exploration and exploitation in training MAPPO. Compared with the existing routing optimization policies, our simulation results demonstrate the excellent performances of the proposed algorithms.","2162-2388","","10.1109/TNNLS.2021.3070584","National Natural Science Foundation of China(grant numbers:61976100,61976099,61772086,61633011,61873287); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9410247","Adaptive routing;metapolicy gradient;multiagent;reinforcement learning (RL)","Routing;Optimization;Task analysis;Heuristic algorithms;Spread spectrum communication;Routing protocols;Training","Markov processes;multi-agent systems;optimisation;reinforcement learning;telecommunication computing;telecommunication network routing;telecommunication traffic","multiagent meta-reinforcement learning;adaptive multipath routing optimization;packet networks;distributed systems;autonomous networked systems;networked multiagent partially observable Markov decision process;MDP;network node;neighboring nodes;network traffic demand;multitask learning problem;model-free multiagent RL algorithms;multiagent metaproximal policy optimization;network performances;time-varying traffic demand;practicable distributed implementation framework;routing optimization policies;meta-MAPPO","","10","","49","IEEE","21 Apr 2021","","","IEEE","IEEE Journals"
"Event-Triggered Communication Network With Limited-Bandwidth Constraint for Multi-Agent Reinforcement Learning","G. Hu; Y. Zhu; D. Zhao; M. Zhao; J. Hao","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Noah’s Ark Laboratory, Huawei, Beijing, China; Noah’s Ark Laboratory, Huawei, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","4 Aug 2023","2023","34","8","3966","3978","Communicating agents with each other in a distributed manner and behaving as a group are essential in multi-agent reinforcement learning. However, real-world multi-agent systems suffer from restrictions on limited bandwidth communication. If the bandwidth is fully occupied, some agents are not able to send messages promptly to others, causing decision delay and impairing cooperative effects. Recent related work has started to address the problem but still fails in maximally reducing the consumption of communication resources. In this article, we propose an event-triggered communication network (ETCNet) to enhance communication efficiency in multi-agent systems by communicating only when necessary. For different task requirements, two paradigms of the ETCNet framework, event-triggered sending network (ETSNet) and event-triggered receiving network (ETRNet), are proposed for learning efficient sending and receiving protocols, respectively. Leveraging the information theory, the limited bandwidth is translated to the penalty threshold of an event-triggered strategy, which determines whether an agent at each step participates in communication or not. Then, the design of the event-triggered strategy is formulated as a constrained Markov decision problem and reinforcement learning finds the feasible and optimal communication protocol that satisfies the limited bandwidth constraint. Experiments on typical multi-agent tasks demonstrate that ETCNet outperforms other methods in reducing bandwidth occupancy and still preserves the cooperative performance of multi-agent systems at the most.","2162-2388","","10.1109/TNNLS.2021.3121546","National Key Research and Development Program of China(grant numbers:2018AAA0102404); National Natural Science Foundation of China(grant numbers:62136008); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDA27030400); Youth Innovation Promotion Association CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9597491","Event trigger;limited bandwidth;multi-agent communication;multi-agent reinforcement learning (MARL)","Bandwidth;Protocols;Reinforcement learning;Task analysis;Optimization;Communication networks;Multi-agent systems","computer networks;Markov processes;multi-agent systems;protocols;reinforcement learning","bandwidth communication;bandwidth occupancy;communicating agents;communication efficiency;communication resources;efficient sending receiving protocols;event-triggered communication network;event-triggered strategy;limited-bandwidth constraint;multiagent reinforcement learning;multiagent tasks;optimal communication protocol;real-world multiagent systems","","9","","41","IEEE","1 Nov 2021","","","IEEE","IEEE Journals"
"Dynamie management of data center resources using reinforcement learning","O. Rolik; E. Zharikov; A. Koval; S. Telenyk","Department of Automation and Control in Technical Systems, Igor Sikorsky Kyiv Polytechnic Institute, Kyiv, Ukraine; Department of Automation and Control in Technical Systems, Igor Sikorsky Kyiv Polytechnic Institute, Kyiv, Ukraine; Department of Automation and Control in Technical Systems, Igor Sikorsky Kyiv Polytechnic Institute, Kyiv, Ukraine; Department of Automatic Control and Information Technology, Cracow University of Technology Cracow, Poland","2018 14th International Conference on Advanced Trends in Radioelecrtronics, Telecommunications and Computer Engineering (TCSET)","12 Apr 2018","2018","","","237","244","For effective management of cloud data center resources there is a need to develop and implement simple and robust methods that take into account the data center power consumption, user Service Level Agreement compliance, and different conditions of resource utilization. In this paper, the authors analyze the possibility of application of the reinforcement learning method to cloud data center resource management. Due to the intensive changes of the workloads and different conditions of resource utilization the data center resource management problem should be solved dynamically in an online manner. To address such problem, the authors propose the method of dynamic virtual machine placement based on the reinforcement learning. The proposed method takes into account the power consumption and the number of SLA violations while producing control impacts. Besides, the method considers the changes in the resource utilizations to make a decision on whether to switch underloaded physical servers to the sleep mode in order to reduce the power consumption. The proposed reinforcement learning agent is based on the Q-learning approach which allows to determine the optimal policy for managing the physical servers without creating an environment model and preliminary information about the workload.","","978-1-5386-2556-9","10.1109/TCSET.2018.8336194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8336194","data center;resource management;learning agent;energy efficiency","Virtual machine monitors;Resource management;Energy efficiency","cloud computing;computer centres;contracts;learning (artificial intelligence);power aware computing;virtual machines","resource utilization;data center resource management problem;dynamic virtual machine placement;reinforcement learning agent;Q-learning approach;cloud data center resources;reinforcement learning method;data center power consumption;user service level agreement compliance","","7","","17","IEEE","12 Apr 2018","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Lane Change Decision-Making with Imaginary Sampling","D. Li; D. Zhao; Q. Zhang","The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2019 IEEE Symposium Series on Computational Intelligence (SSCI)","20 Feb 2020","2019","","","16","21","This paper investigates the autonomous driving lane change problem with reinforcement learning methods. A two-stage control method is proposed which includes a decision-making module computing the high-level lane change action and a lateral control module outputting the low-level steering angle. Due to the decoupling system framework, the proposed controller can flexibly cancel the previous improper lane change command. To improve the data efficiency in the lateral control module, the Gaussian process is employed to modeling the local time-interval system models. These models are used to generate imaginary samples and facilitate the reinforcement learning training process. The lateral control experiments show that the imaginary samples can improve the data efficiency and speed up the training process. Finally, the two-lane scene and three-lane scene experiments validate the effectiveness of the proposed two-stage lane change method.","","978-1-7281-2485-8","10.1109/SSCI44817.2019.9003029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003029","reinforcement learning;gaussian process;autonomous driving;Lane-Changing","Decision making;Learning (artificial intelligence);Gaussian processes;Uncertainty;Estimation;Autonomous vehicles;Roads","Gaussian processes;learning (artificial intelligence);road safety;road traffic;traffic engineering computing","high-level lane change action;lateral control module;low-level steering angle;decoupling system framework;data efficiency;Gaussian process;reinforcement learning training process;lateral control experiments;three-lane scene experiments;two-stage lane change method;lane change decision-making;autonomous driving lane change problem;reinforcement learning methods;two-stage control method;decision-making module;two-lane scene experiments;improper lane change command","","6","","14","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"Automated Model Generation for Machinery Fault Diagnosis Based on Reinforcement Learning and Neural Architecture Search","J. Zhou; L. Zheng; Y. Wang; C. Wang; R. X. Gao","School of Mechanical Engineering and Automation, Beihang University, Beijing, China; School of Mechanical Engineering and Automation, Beihang University, Beijing, China; School of Mechanical Engineering and Automation, Beihang University, Beijing, China; Key Laboratory of Vehicle Transmission, China North Vehicle Research Institute, Beijing, China; Department of Mechanical and Aerospace Engineering, Case Western Reserve University, Cleveland, OH, USA","IEEE Transactions on Instrumentation and Measurement","23 Feb 2022","2022","71","","1","12","In recent years, deep learning (DL)-based fault diagnosis methods have demonstrated significant success in various domains due to their high accuracy. Similar to other data-driven techniques, DL models are application-specific and strongly depend on the data which they are developed upon. Different DL models need to be designed for different tasks. In addition, manual-tuning of the DL structures and associated parameters as part of the model design is a trial-and-error optimization process, which is time-consuming and sensitive to changes. To address these limitations, this article presents a reinforcement learning (RL) and neural architecture search (NAS)-based automatic modeling framework (RL-NAS) for fault diagnosis of machinery. The RL-NAS method can adaptively and automatically design high-accuracy network models according to the intended diagnosis tasks. A weight-sharing mechanism has been developed to alleviate the bottleneck of NAS where the computation time increases exponentially with the expansion of the search space and the deepening of the model structure. To evaluate its effectiveness, efficiency, and reproducibility, the proposed framework is validated on five datasets from different institutes, spanning the applications of rolling element bearings, gears, and ball screws. The results show that for all five applications, the RL-NAS method successfully searched out diagnostics models with over 97% accuracy within a short time, from 40 s to 30 min depending on the complexity of the diagnosis task, showing good performance of the proposed method.","1557-9662","","10.1109/TIM.2022.3141166","National Key Research and Development Program of China(grant numbers:2020YFB1708400); National Natural Science Foundation of China(grant numbers:51805262); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9673794","Automatic modeling;fault diagnosis;neural architecture search (NAS);reinforcement learning (RL);rotating machinery;weight sharing","Fault diagnosis;Computational modeling;Task analysis;Data models;Training;Process control;Feature extraction","ball screws;deep learning (artificial intelligence);error analysis;fault diagnosis;gears;mechanical engineering computing;optimisation;reinforcement learning;rolling bearings","high-accuracy network models;RL-NAS method;diagnostics models;machinery fault diagnosis;reinforcement learning;deep learning-based fault diagnosis methods;DL models;manual-tuning;model design;trial-and-error optimization process;neural architecture search model;automated model generation;automatic modeling framework;weight-sharing mechanism;rolling element bearings;gears;ball screws;time 40.0 s to 30.0 min","","6","","38","IEEE","7 Jan 2022","","","IEEE","IEEE Journals"
"Output Resilient Containment Control of Heterogeneous Systems With Active Leaders Using Reinforcement Learning Under Attack Inputs","Q. Li; L. Xia; R. Song","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China","IEEE Access","13 Nov 2019","2019","7","","162219","162228","The optimal solution to the distributed output containment control problem of heterogeneous multiple-agent systems (MASs) with unknown active leaders under attack inputs by using data-based off-policy reinforcement learning (RL) is proposed. Assume that the control input of each leader is bounded and non-zero. Moreover, followers are vulnerable to attack signals in real-world application. Firstly, distributed observers are designed such that the state and output of observers fall into the convex hull formed by leaders. Then, the output containment problem is converted into  $H_{\infty }$  tracking problem by minimizing value function, Algebraic Riccati equations (AREs) are obtained in solving optimal  $H_{\infty }$  tracking problem for each follower, which are computed by a data-based off-policy RL algorithm without using agents’ dynamics. At last, the effectiveness of the algorithm is verified by a simulation example.","2169-3536","","10.1109/ACCESS.2019.2947558","National Natural Science Foundation of China(grant numbers:61873300,61722312); Northwestern Polytechnical University(grant numbers:FRF-GF-17-B45); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8869870","Heterogeneous systems;distributed observer;algebraic riccati equations (AREs);reinforcement learning;H∞ control","Observers;Heuristic algorithms;Synchronization;Control systems;Reinforcement learning;Riccati equations","distributed control;H∞ control;learning (artificial intelligence);linear systems;matrix algebra;multi-agent systems;multi-robot systems","output resilient containment control;heterogeneous systems;reinforcement learning;attack inputs;optimal solution;distributed output containment control problem;heterogeneous multiple-agent systems;unknown active leaders;data-based off-policy reinforcement;control input;attack signals;distributed observers;output containment problem;tracking problem;data-based off-policy RL algorithm","","6","","47","CCBY","16 Oct 2019","","","IEEE","IEEE Journals"
"Energy Scheduling for Multi-Energy Systems via Deep Reinforcement Learning","Z. Wang; S. Zhu; T. Ding; B. Yang","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; School of Electrical Engineering, Xi’an Jiaotong University, Xi’an, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2020 IEEE Power & Energy Society General Meeting (PESGM)","16 Dec 2020","2020","","","1","5","With the development of smart infrastructures, especially energy hubs (EHs), traditional power systems transform into the multi-energy systems. This paper investigates a long term profit maximizing energy scheduling problem for multi-energy systems from the perspective of prosumers. Most existing methods assume that future market prices or demand information of prosumers are known to the decision makers. In this paper, we model the multi-energy scheduling strategy in the presence of unknown information as a Markov Decision Process (MDP) problem. We first establish an energy scheduling mechanism by exploring the unique features of EHs. The concept of valid actions is then proposed to ensure the balance between supply and demand. A deep Q-learning algorithm is developed to obtain the scheduling strategy without any prior information. Simulation results demonstrate the effectiveness and efficiency of the proposed strategy.","1944-9933","978-1-7281-5508-1","10.1109/PESGM41954.2020.9281483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9281483","","Supply and demand;Simulation;Transforms;Reinforcement learning;Markov processes;Scheduling;Power systems","decision making;deep learning (artificial intelligence);Markov processes;power engineering computing;power generation scheduling;power markets;pricing;wind power plants;wind turbines","multienergy systems;power systems;multienergy scheduling strategy;deep reinforcement learning;energy hubs;smart infrastructures;Markov decision process problem;MDP problem;deep Q-learning algorithm;market prices;decision makers","","6","","15","IEEE","16 Dec 2020","","","IEEE","IEEE Conferences"
"Safe Intermittent Reinforcement Learning for Nonlinear Systems","Y. Yang; K. G. Vamvoudakis; H. Modares; W. He; Y. Yin; D. C. Wunsch","School of Automation and Electrical Engineering, university of Science and Technology Beijing, Beijing, China; Daniel Guggenheim School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Mechanical Engineering, Michigan State University, East Lansing, USA; School of Automation and Electrical Engineering, university of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, university of Science and Technology Beijing, Beijing, China; Department of Electrical and Computer Engineering, Missouri University of Science and Technology, Rolla, MO, USA","2019 IEEE 58th Conference on Decision and Control (CDC)","12 Mar 2020","2019","","","690","697","In this paper, an online intermittent actor-critic reinforcement learning method is used to stabilize nonlinear systems optimally while also guaranteeing safety. A barrier function-based transformation is introduced to ensure that the system does not violate the user-defined safety constraints. It is shown that the safety constraints of the original system can be guaranteed by assuring the stability of the equilibrium point of an appropriately transformed system. Then, an online intermittent actor-critic learning framework is developed to learn the optimal safe intermittent controller. Also, Zeno behavior is guaranteed to be excluded. Finally, numerical examples are conducted to verify the efficacy of the learning algorithm.","2576-2370","978-1-7281-1398-2","10.1109/CDC40024.2019.9030210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9030210","Safety control;intermittent feedback;reinforcement learning.","Safety;Reinforcement learning;Feedback control;Stability;Learning (artificial intelligence);Nonlinear control systems","learning (artificial intelligence);nonlinear control systems;stability","safe intermittent reinforcement learning;nonlinear system stability;barrier function-based transformation;user-defined safety constraints;optimal safe intermittent controller;online intermittent actor-critic reinforcement learning;Zeno behavior","","5","","29","IEEE","12 Mar 2020","","","IEEE","IEEE Conferences"
"Containment control of heterogeneous systems with active leaders of bounded unknown control using reinforcement learning","Y. Yang; R. Song; Y. Yin; D. C. Wunsch; H. Modares","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Department of Electrical and Computer Engineering, Missouri University of Science and Technology, Rolla, MO, USA; Department of Electrical and Computer Engineering, Missouri University of Science and Technology, Rolla, MO, USA","2017 IEEE Symposium Series on Computational Intelligence (SSCI)","8 Feb 2018","2017","","","1","7","This paper solves the containment problem of multi-agent systems on undirected graph with multiple active leaders using off-policy reinforcement learning (RL). The leaders are active in the sense that there exists bounded control input in the dynamics which is unknown to all followers and the followers are heterogeneous with different dynamics. Not only the steady states of agent i but also the transient trajectories are taken into account to impose optimality to the proposed containment control. Inhomogeneous algebraic Riccati equations (ARE) are derived to solve the optimal containment control protocol. To avoid the requirement of agents' dynamics to obtain containment control, an off-policy RL algorithm is developed to solve the inhomogeneous AREs online in real time and without requiring any knowledge of the agents' dynamics. Finally, a simulation example is presented to illustrate the effectiveness of the proposed algorithm.","","978-1-5386-2726-6","10.1109/SSCI.2017.8285254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8285254","Containment control;Heterogeneous systems;Active Leader;Distributed observer;Reinforcement learning;Model-free","Observers;Nonhomogeneous media;Multi-agent systems;Learning (artificial intelligence);Trajectory;Heuristic algorithms","distributed control;graph theory;learning (artificial intelligence);multi-robot systems;nonlinear control systems;optimal control;Riccati equations","heterogeneous systems;bounded unknown control;multiagent systems;multiple active leaders;off-policy reinforcement learning;control input;optimal containment control protocol;off-policy RL algorithm;ARE;inhomogeneous algebraic Riccati equations;transient trajectories;undirected graph","","4","","37","IEEE","8 Feb 2018","","","IEEE","IEEE Conferences"
"A Simulation-Constraint Graph Reinforcement Learning Method for Line Flow Control","P. Xu; Y. Pei; X. Zheng; J. Zhang","School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, USA; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China","2020 IEEE 4th Conference on Energy Internet and Energy System Integration (EI2)","15 Feb 2021","2020","","","319","324","Line flow control plays an essential role in maintaining the stability of power system. Considering the randomness and uncertainties in the grid, a simulation-constraint graph reinforcement learning method is proposed to provide a potential solution for the real-time dispatch via topology control. To improve the training efficiency and deployment stability of reinforcement learning agent in the power system with various constraints and large action space, an adaptive simulation regulation strategy and a step-wise control mechanism are utilized. Power system simulation is introduced to keep the agent from actions with invalidity or severe simulated outcomes. Then, graph neural network is applied to help the agent learn a better representation of power system state by dealing with the information hidden in topology. Finally, a numeric example on IEEE 14-bus system is employed to demonstrate the feasibility of the proposed method.","","978-1-7281-9606-0","10.1109/EI250167.2020.9347305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9347305","power flow;topology control;reinforcement learning;graph neural network","Training;Power system simulation;Reinforcement learning;Power system stability;Stability analysis;Real-time systems;Topology","flow control;graph theory;learning (artificial intelligence);neural nets;power system stability","power system state;severe simulated outcomes;power system simulation;step-wise control mechanism;adaptive simulation regulation strategy;reinforcement learning agent;deployment stability;topology control;simulation-constraint graph reinforcement learning method;line flow control","","4","","18","IEEE","15 Feb 2021","","","IEEE","IEEE Conferences"
"Suspension Regulation of Medium-Low-Speed Maglev Trains Via Deep Reinforcement Learning","F. Zhao; K. You; S. Song; W. Zhang; L. Tong","Department of Automation, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Department of Automation, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Department of Automation, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; CRRC Zhuzhou Locomotive Co. Ltd., Zhuzhou, China; CRRC Zhuzhou Locomotive Co. Ltd., Zhuzhou, China","IEEE Transactions on Artificial Intelligence","14 Oct 2021","2021","2","4","341","351","This article studies the suspension regulation problem of medium-low-speed maglev trains (mlsMTs), which is not well solved by most model-based controllers. We propose a sample-based controller by reformulating it as a continuous Markov decision process (MDP) with unknown transition probabilities. Then, we propose a reinforcement learning algorithm with actor-critic neural networks to solve the MDP. To reduce estimation errors in the Q-function, we adopt a double Q-learning scheme and propose a novel initialization method to accelerate the convergence by exploiting the PID controller. Finally, we illustrate that the proposed controllers outperform the existing PID controller with a real dataset from the mlsMT in Changsha, China, and are even comparable to model-based controllers, which, however, assume that the complete information of the model is known, via simulations. Impact Statement—The control problem of levitation systems is essential for maglev trains. The advanced control methods require an exact dynamical model, which is difficult to establish in practice due to uncertainties and complex dynamics. Reinforcement learning (RL), as a model-free method, learns a controller directly from data. We are the first to propose deep RL algorithms to address the levitation control problem. By learning the real dataset provided by CRRC, the proposed algorithms outperform the well-known PID controller significantly. In particular, our controller responses 50 times faster than PID even without additional efforts on modeling. Moreover, the proposed initialization method can be applied in a variety of RL-based control problems to improve performance.","2691-4581","","10.1109/TAI.2021.3097313","National Natural Science Foundation of China(grant numbers:62033006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484843","Deep reinforcement learning;markov decision procession;neural network;suspension regulation;the levitation control system","Regulation;Electromagnets;Air gaps;Atmospheric modeling;Reinforcement learning;Rails;Predictive models","adaptive control;control system synthesis;learning (artificial intelligence);magnetic levitation;Markov processes;neurocontrollers;optimal control;three-term control","medium-low-speed maglev trains;model-based controllers;sample-based controller;continuous Markov decision process;MDP;unknown transition probabilities;reinforcement learning algorithm;actor-critic neural networks;double Q-learning scheme;novel initialization method;existing PID controller;Impact Statement-The control problem;control methods;exact dynamical model;model-free method;deep RL algorithms;levitation control problem;controller responses 50 times;RL-based control problems;deep reinforcement learning;suspension regulation problem","","4","","35","IEEE","14 Jul 2021","","","IEEE","IEEE Journals"
"Multi-Agent Reinforcement Learning Based on Clustering in Two-Player Games","W. Li; Y. Zhuand; D. Zhao","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Beijing, China","2019 IEEE Symposium Series on Computational Intelligence (SSCI)","20 Feb 2020","2019","","","57","63","Non-stationary environment is general in real environment, including adversarial environment and multi-agent problem. Multi-agent environment is a typical non-stationary environment. Each agent of the shared environment must learn a efficient interaction for maximizing the expected reward. Independent reinforcement learning (InRL) is the simplest form in which each agent treats other agents as part of environment. In this paper, we present Max-Mean-Learning-Win-or-Learn-Fast (MML-WoLF), which is an independent on-policy learning algorithm based on reinforcement clustering. A variational auto-encoder method based on reinforcement learning is proposed to extract features for unsupervised clustering. Based on clustering results, MML-WoLF uses statistics and the dominated factor to calculate the values of the states that belong to a certain category. The agent policy is iteratively updated by the value. We apply our algorithm to multi-agent problems including matrix-game, grid world, and continuous world game. The clustering results are able to show the strategies distribution under the agent’s current policy. The experiment results suggest that our method significantly improves average performance over other independent learning algorithms in multi-agent problems.","","978-1-7281-2485-8","10.1109/SSCI44817.2019.9003120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003120","reinforcement learning;unsupervised clustering;matrix game;multi-agent","Games;Clustering algorithms;Learning (artificial intelligence);Probability distribution;Nash equilibrium;Feature extraction;Approximation algorithms","game theory;learning (artificial intelligence);multi-agent systems;pattern clustering","multiagent reinforcement;nonstationary environment;adversarial environment;multiagent environment;shared environment;independent reinforcement learning;MML-WoLF;on-policy learning algorithm;reinforcement clustering;unsupervised clustering;agent policy;independent learning algorithms;statistics;dominated factor;two-player games;matrix-game;grid world;continuous world game;max-mean-learning-win-or-learn-fast","","3","","35","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"Model-Free Event-Triggered Consensus Algorithm for Multiagent Systems Using Reinforcement Learning Method","M. Long; H. Su; Z. Zeng","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","18 Jul 2022","2022","52","8","5212","5221","In this article, we study the consensus issues of multiagent systems (MASs) without any information of the system model by using the reinforcement learning (RL) method and event-based control strategy. First, we design an adaptive event-based consensus control protocol using the local sampled state information so that the consensus errors of all agents are uniformly ultimately bounded. The validity of the above event-triggered adaptive control protocol is confirmed by excluding the Zeno behavior within finite time. Then, based on the RL approach, we present a model-free algorithm to get the feedback gain matrix, and accomplish constructing the adaptive event-triggered control strategy without the knowledge of model information. Distinct with the existing related works, this RL-based event-triggered adaptive control algorithm only relies on the local sampled state information, irrelevant to any model information or global network information. Finally, we provide some examples to demonstrate the validity of the above adaptive event-based consensus algorithm.","2168-2232","","10.1109/TSMC.2021.3120008","National Natural Science Foundation of China(grant numbers:61991412,U1913602,61873318); 111 Project on Computational Intelligence and Intelligent Control(grant numbers:B18024); Program for HUST Academic Frontier Youth Team(grant numbers:2018QYTD07); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582745","Consensus;event triggered;model-free algorithm;multiagent systems (MASs);reinforcement learning (RL)","Adaptation models;Consensus algorithm;Heuristic algorithms;Protocols;Computational modeling;Multi-agent systems;Adaptive control","adaptive control;continuous time systems;distributed control;feedback;learning (artificial intelligence);multi-agent systems;multi-robot systems;nonlinear control systems","RL-based event-triggered adaptive control algorithm;local sampled state information;model information;global network information;adaptive event-based consensus algorithm;model-free event-triggered consensus algorithm;multiagent systems;reinforcement learning method;consensus issues;system model;event-based control strategy;adaptive event-based consensus control protocol;consensus errors;event-triggered adaptive control protocol;RL approach;model-free algorithm;adaptive event-triggered control strategy","","3","","37","IEEE","20 Oct 2021","","","IEEE","IEEE Journals"
"Vibration Control based on Reinforcement Learning for a Flexible Building-like Structure System with Active Mass Damper against Disturbance Effects","H. Gao; W. He; Y. Zhang; C. Sun","Institute of Artificial Intelligence, University of Science and Technology Beijing, Beijing, China; Institute of Artificial Intelligence, University of Science and Technology Beijing, Beijing, China; Department of Mechanical, Industrial, and Aerospace Engineering & Concordia, Institute of Aerospace Design and Innovation, Concordia University, Montreal, Quebec, Canada; School of Automation, Southeast University, Nanjing, China","2020 59th IEEE Conference on Decision and Control (CDC)","11 Jan 2021","2020","","","2380","2385","Vibration and displacement control are of critical importance for both high-rise and ultra high-rise building systems. A single-floor building-like structure equipped with an active mass damper (AMD) is investigated in this paper. Optimal vibration control, while dealing with system uncertainties, is realized by the reinforcement learning (RL) technique. When the unexpected natural disasters (such as strong wind excitation) occur, the proposed controller applying to the active mass damper can compensate the increase of the system vibration caused by external disturbances. In addition, a Lyapunov candidate is used to derive a semi-global uniformly ultimately bounded (SGUUB) property. The experimental platform is mainly composed of one flexible floor and a linear cart system. Both the acceleration and the displacement responses of the floor are provided and compared separately for passive mode, proportional-velocity (PV) control and RL control. The experimental results in the form of graphics and tables have shown the effectiveness of the proposed control algorithm.","2576-2370","978-1-7281-7447-1","10.1109/CDC42340.2020.9304455","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9304455","","Buildings;Shock absorbers;Vibration control;Artificial neural networks;Uncertainty;Vibrations;Reinforcement learning","buildings (structures);control engineering computing;learning (artificial intelligence);Lyapunov methods;optimal control;shock absorbers;structural engineering;structural engineering computing;uncertain systems;vibration control","reinforcement learning;optimal vibration control;single-floor building-like structure;ultra high-rise building systems;displacement control;flexible building-like structure system;control algorithm;RL control;proportional-velocity control;linear cart system;flexible floor;system vibration;active mass damper","","3","","24","IEEE","11 Jan 2021","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Benchmark for Autonomous Driving in Intersection Scenarios","Y. Liu; Q. Zhang; D. Zhao","The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences","2021 IEEE Symposium Series on Computational Intelligence (SSCI)","24 Jan 2022","2021","","","1","8","In recent years, control under urban intersection scenarios has become an emerging research topic. In such scenarios, the autonomous vehicle confronts complicated situations since it must deal with the interaction with social vehicles timely while obeying the traffic rules. Generally, the autonomous vehicle is supposed to avoid collisions while pursuing better efficiency. The existing work fails to provide a framework that emphasizes the integrity of the scenarios while deploying and testing reinforcement learning(RL) methods. Specifically, we propose a benchmark for training and testing RL-based autonomous driving agents in complex intersection scenarios, which is called RL-CIS. Then, a set of baselines consisting various algorithms are deployed. The test benchmark and baselines provide a fair and comprehensive training and testing platform for the study of RL for autonomous driving in the intersection scenario, advancing RL-based methods for autonomous driving control. The code of our proposed framework can be found at https://github.com/liuyufi123/ComplexUrbanScenarios.","","978-1-7281-9048-8","10.1109/SSCI50451.2021.9660172","National Natural Science Foundation of China (NSFC)(grant numbers:61803371); Beijing Science and Technology Plan(grant numbers:Z191100007419002); Beijing Municipal Natural Science Foundation(grant numbers:L191002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9660172","autonomous driving;reinforcement learning;intersection scenarios;decision-making","Training;Codes;Stochastic processes;Reinforcement learning;Benchmark testing;Sampling methods;Concrete","collision avoidance;learning (artificial intelligence);mobile robots;road vehicles","reinforcement learning benchmark;intersection scenario;urban intersection scenarios;emerging research topic;autonomous vehicle;social vehicles;traffic rules;complex intersection scenarios;RL-CIS;test benchmark;comprehensive training;testing platform;RL-based methods;autonomous driving control","","3","","26","IEEE","24 Jan 2022","","","IEEE","IEEE Conferences"
"Adjacency Constraint for Efficient Hierarchical Reinforcement Learning","T. Zhang; S. Guo; T. Tan; X. Hu; F. Chen","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Civil and Environmental Engineering, Stanford University, Stanford, CA, USA; Department of Computer Science and Technology, Institute for Artificial Intelligence, Beijing National Research Center for Information Science and Technology, State Key Laboratory of Intelligent Technology and Systems, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","7 Mar 2023","2023","45","4","4152","4166","Goal-conditioned Hierarchical Reinforcement Learning (HRL) is a promising approach for scaling up reinforcement learning (RL) techniques. However, it often suffers from training inefficiency as the action space of the high-level, i.e., the goal space, is large. Searching in a large goal space poses difficulty for both high-level subgoal generation and low-level policy learning. In this article, we show that this problem can be effectively alleviated by restricting the high-level action space from the whole goal space to a $k$k-step adjacent region of the current state using an adjacency constraint. We theoretically prove that in a deterministic Markov Decision Process (MDP), the proposed adjacency constraint preserves the optimal hierarchical policy, while in a stochastic MDP the adjacency constraint induces a bounded state-value suboptimality determined by the MDP's transition structure. We further show that this constraint can be practically implemented by training an adjacency network that can discriminate between adjacent and non-adjacent subgoals. Experimental results on discrete and continuous control tasks including challenging simulated robot locomotion and manipulation tasks show that incorporating the adjacency constraint significantly boosts the performance of state-of-the-art goal-conditioned HRL approaches.","1939-3539","","10.1109/TPAMI.2022.3192418","National Natural Science Foundation of China(grant numbers:62176133,61836004); Tsinghua-Guoqiang Research Program(grant numbers:2019GQG0006); National Key Research and Development Program of China(grant numbers:2021ZD0200300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833270","Hierarchical reinforcement learning (HRL);reinforcement learning (RL);goal-conditioning;subgoal generation;adjacency constraint","Task analysis;Reinforcement learning;Training;Random variables;Postal services;Markov processes;Games","learning (artificial intelligence);manipulators;Markov processes;mobile robots;reinforcement learning","adjacency constraint;adjacency network;efficient Hierarchical Reinforcement Learning;goal space;Goal-conditioned Hierarchical Reinforcement Learning;high-level action space;high-level subgoal generation;low-level policy learning;nonadjacent subgoals;optimal hierarchical policy;reinforcement learning techniques;state-of-the-art goal-conditioned HRL approaches;step adjacent region","","3","","59","IEEE","19 Jul 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Brachiation Control for Two-Link Bio-Primate Robot","Z. Cheng; H. Cheng; H. Xu","Department of Mechanical Engineering and Automation, Northeastern University, China; Department of Mechanical Engineering and Automation, Northeastern University, China; Department of Mechanical Engineering and Automation, Northeastern University, China","2018 IEEE International Conference on Robotics and Biomimetics (ROBIO)","14 Mar 2019","2018","","","856","861","Manually designing an effective and efficient controller for complex mechanics, such as bio-inspired robots or underactuated mechanical system, typically are very difficult. It requires precise motion planning and dynamic control. Reinforcement learning or genetic algorithm based learning methods suffers from representing the high dimensional models. The combination of deep learning and reinforcement learning provide a feasible way to handle such difficulties. However, priori-less searching sometimes tends to be low efficient and usually finds the “mechanic” solution instead of the “natural” one. In this paper, the traditional nonlinear control concept is integrated into the deep reinforcement learning (DRL) framework. The whole process is implemented on the brachiation control problem of a two link bio-primate robot. Deep Deterministic Policy Gradient (DDPG) is used to search for the optimal control policy. The searching process is realized by interacting with the dynamic model instead of real robot. The energy based planning and control concept is adopted, which utilize the fact that when the shoulder joint angle is fixed, energy of the whole system keeps constant. By regulating the angle and energy, the robot can be restricted on a particular trajectory. The energy concept is encoded within the reward function and trained in the Gym environment. For varying targets point-to-point control, the network structure is also modified to accept the target coordinates. Effectiveness of the proposed methods are verified by simulation and experimental results.","","978-1-7281-0377-8","10.1109/ROBIO.2018.8665274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8665274","","Conferences;Robots;Biomimetics","genetic algorithms;learning (artificial intelligence);mobile robots;motion control;nonlinear control systems;optimal control;path planning","deep learning;mechanic solution;deep reinforcement learning framework;brachiation control problem;Deep Deterministic Policy Gradient;optimal control policy;searching process;dynamic model;energy concept;targets point-to-point control;two-link bio-primate robot;complex mechanics;bio-inspired robots;underactuated mechanical system;precise motion planning;dynamic control;genetic algorithm;high dimensional models;nonlinear control concept","","2","","17","IEEE","14 Mar 2019","","","IEEE","IEEE Conferences"
"Adaptive Control of a Marine Vessel Based on Reinforcement Learning","Z. Yin; W. He; C. Sun; G. Li; C. Yang","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation, Southeast University, Nanjing, China; Queen Mary, University of London, London, UK; Key Laboratory of Autonomous Systems and Networked Control, South China University of Technology, Guangzhou, China","2018 37th Chinese Control Conference (CCC)","7 Oct 2018","2018","","","2735","2740","In this paper, our main goal is to solve optimal control problem by using reinforcement learning (RL) algorithm for marine surface vessel system with known dynamic. And this algorithm is an optimal control algorithm based on policy iteration (PI), and it can obtain the suitable approximations of cost function and the optimized control policy. There are two neural networks (NNs), where critic NN aims to estimate the cost-to-go and actor NN is utilized to design suitable input controller and minimize the tracking error. A novel tuning method is given for critic NN and actor NN. The stability and convergence are proven by Lyapunov's direct method. Finally, the numerical simulations are conducted to demonstrate the feasibility and superiority of presented algorithm.","1934-1768","978-988-15639-5-8","10.23919/ChiCC.2018.8482656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8482656","Reinforcement Learning;Critic Neural Networks;Actor Neural Networks;Lyapunov Method;Marine Vessel","Artificial neural networks;Cost function;Adaptive systems;Learning (artificial intelligence);Symmetric matrices;Optimal control","adaptive control;convergence;function approximation;learning (artificial intelligence);Lyapunov methods;marine control;neurocontrollers;optimal control;ships;stability","critic NN;actor NN;adaptive control;reinforcement learning algorithm;marine surface vessel system;optimal control algorithm;policy iteration;optimized control policy;neural networks;input controller;RL algorithm;cost function approximation;tracking error minimization;tuning method;stability;convergence;Lyapunovs direct method","","2","","29","","7 Oct 2018","","","IEEE","IEEE Conferences"
"Robust Control of Uncertain Linear Systems Based on Reinforcement Learning Principles","D. Xu; Q. Wang; Y. Li","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China","IEEE Access","11 Feb 2019","2019","7","","16431","16443","In this paper, a reinforcement learning (RL) approach is developed to solve the robust control for uncertain continuous-time linear systems. The objective is to find a feedback control law for the uncertain linear system using an online policy iteration algorithm. The robust control problem is solved by constructing an extended algebraic Riccati equation with properly defined weighting matrices for a general uncertain linear system. An online policy iteration algorithm is developed to solve the robust control problem based on RL principles without knowing the nominal system matrix. The convergence of the algorithm to the robust control solution for uncertain linear systems is proved. The simulation examples are given to demonstrate the effectiveness of the proposed algorithm. The results extend the design method of robust control to uncertain linear systems.","2169-3536","","10.1109/ACCESS.2019.2894594","National Natural Science Foundation of China(grant numbers:61472037,61463002,61375100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625521","Reinforcement learning;uncertain linear system;robust control;algebraic Riccati equation","Linear systems;Robust control;Uncertain systems;Optimal control;Uncertainty;Mathematical model;Design methodology","continuous time systems;control system synthesis;feedback;iterative methods;learning (artificial intelligence);linear systems;matrix algebra;Riccati equations;robust control;uncertain systems","robust control problem;nominal system matrix;reinforcement learning principles;uncertain continuous-time linear systems;feedback control law;online policy iteration algorithm;uncertain linear system;extended algebraic Riccati equation;RL principles;design method","","2","","55","OAPA","24 Jan 2019","","","IEEE","IEEE Journals"
"Research on Naval Air Defense Intelligent Operations on Deep Reinforcement Learning","J. Wang; J. Wang; J. He; G. Wang; M. Wang","School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, China; Jiangsu Automation Research Institute, Lianyungang, China; Jiangsu Automation Research Institute, Lianyungang, China; School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, China; School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, China","2022 34th Chinese Control and Decision Conference (CCDC)","14 Feb 2023","2022","","","2246","2252","Anti-air ability is one of the most important combat strengths of a warship. How to manage soft weapon, hard weapon and warship maneuver to achieve the optimal solution is the problem that every warship commander is facing during the process of navy air defense. This article applies the artificial intelligence to achieve the navy air intelligent defense. The entire process of navy air defense is analyzed, Actor-Critic algorithm and A3C algorithm are studied, and the simulation platform based on STAGE is constructed. Based on this, a navy air intelligent defense training framework based on STAGE and deep reinforcement learning is raised to guide the navy air defense intelligent learning. This work provides new insights for the application of artificial intelligence in the navy command and control system.","1948-9447","978-1-6654-7896-0","10.1109/CCDC55256.2022.10034115","Natural Science Foundation of Shanghai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10034115","Navy Air Defense;Deep Reinforcement Learning;Simulation;Training Framework","Deep learning;Training;Command and control systems;Analytical models;Weapons;Atmospheric modeling;Reinforcement learning","command and control systems;deep learning (artificial intelligence);military vehicles;reinforcement learning;ships;weapons","anti-air ability;artificial intelligence;combat strengths;control system;deep reinforcement learning;hard weapon;naval air defense intelligent operations;navy air defense intelligent learning;navy air intelligent defense training framework;navy command;soft weapon;warship commander","","1","","20","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Synthesis of Controllers for Co-Safe Linear Temporal Logic Specifications using Reinforcement Learning","X. Ren; X. Yin; S. Li","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","2304","2309","Recently, the interest in controller synthesis for complex tasks is rapidly growing [1], [2], and in most cases, environments are unknown, which limits applications of traditional control methods. In this paper, we use reinforcement learning method to learn to optimally achieve complex tasks under unknown environments. Specifically, we model the uncertain environments using the Markov Decision Processes (MDPs). The high-level control objective is described by the syntactically co-safe Linear Temporal Logics (scLTLs). Under such settings, we propose a new method for the reward design procedure. The proposed new reward function utilizes the information of automata which are induced from scLTL tasks. Furthermore, we compare the proposed reward function with existing approaches in the standard grid world environments. We show that, by using our reward function, the learning process converges faster and finally optimally achieves scLTL tasks.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9549746","National Key Research and Development Program of China; National Natural Science Foundation of China; Innovation Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9549746","Reinforcement learning;Syntactically co-safe Linear Temporal Logics;Reward design","Learning automata;Design methodology;Reinforcement learning;Markov processes;Aerospace electronics;Task analysis;Standards","convex programming;discrete time systems;learning (artificial intelligence);Markov processes;optimal control;temporal logic","reinforcement learning method;uncertain environments;Markov Decision Processes;high-level control objective;syntactically co-safe Linear Temporal Logics;reward design procedure;reward function;scLTL tasks;standard grid world environments;learning process;Co-Safe Linear Temporal Logic Specifications;controller synthesis;traditional control methods","","1","","19","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning Driving Strategy based on Auxiliary Task for Multi-Scenarios Autonomous Driving","J. Sun; X. Fang; Q. Zhang","Peng Cheng Labrary, Shenzhen, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Chinese Academy of Sciences, Institute of Automation, Beijing, China; Peng Cheng Labrary, Shenzhen, China","2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)","7 Jul 2023","2023","","","1337","1342","Reinforcement learning (RL) has made great progress in autonomous driving applications. However, using one RL based driving policy for multi-scenarios autonomous driving is still challenging for RL in autonomous driving. There are different observations and reward measurements in different scenarios. At the same time, there is also the problem of multi-source heterogeneous observation in autonomous driving. To address the problems above, we propose a reinforcement learning framework based on the auxiliary task. Firstly, we designed a reward function to enable vehicles to learn safe and efficient strategies. Further, an auxiliary task is designed to learn the characteristics of different scenarios so that the ego agent can adopt different strategies for different scenarios. Finally, in order to handle the driving problem in multiple scenarios, we propose a representation network based on Multi-layer perceptron (MLP), Convolutional neural network (CNN), and Transformer networks to learn multi-source heterogeneous observation. The multi-source heterogeneous observation consists of the ego vehicle state, the bird's eye view (BEV) state and neighbour vehicle states. Experiments show that our method achieves a higher success rate compared to a popular reinforcement learning algorithm.","2767-9861","979-8-3503-2105-0","10.1109/DDCLS58216.2023.10166271","National Key Research and Development Program of China(grant numbers:2022YFA1004000); National Natural Science Foundation of China (NSFC)(grant numbers:62173325); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10166271","Reinforcement learning;Autonomous Driving;Multiple Scenarios;Auxiliary Task","Learning systems;Reinforcement learning;Transformers;Control systems;Convolutional neural networks;Task analysis;Autonomous vehicles","convolutional neural nets;deep learning (artificial intelligence);learning (artificial intelligence);multilayer perceptrons;reinforcement learning;traffic engineering computing","autonomous driving applications;auxiliary task;different observations;driving policy;driving problem;driving strategy;Multilayer perceptron;multiscenarios autonomous driving;multisource heterogeneous observation;popular reinforcement learning algorithm;reward measurements;RL","","1","","18","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"A Self-Attention-Based Deep Reinforcement Learning Approach for AGV Dispatching Systems","Q. Wei; Y. Yan; J. Zhang; J. Xiao; C. Wang","Institute of Automation, State Key Laboratory of Management and Control for Complex Systems, Chinese Academy of Sciences, Beijing, China; Institute of Automation, State Key Laboratory of Management and Control for Complex Systems, Chinese Academy of Sciences, Beijing, China; Institute of Automation, State Key Laboratory of Management and Control for Complex Systems, Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Hong Kong Applied Science and Technology Research Institute Company Ltd, Hong Kong, China","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","12","The automated guided vehicle (AGV) dispatching problem is to develop a rule to assign transportation tasks to certain vehicles. This article proposes a new deep reinforcement learning approach with a self-attention mechanism to dynamically dispatch the tasks to AGV. The AGV dispatching system is modeled as a less complicated Markov decision process (MDP) using vehicle-initiated rules to dispatch a workcenter to an idle AGV. In order to deal with the highly dynamical environment, the self-attention mechanism is introduced to calculate the importance of different information. The invalid action masking technique is performed to alleviate false actions. A multimodal structure is employed to mix the features of various sources. Comparative experiments are performed to show the effectiveness of the proposed method. The properties of the learned policies are also investigated under different environment settings. It is discovered that the policies explore and learn the properties of different systems, and also smooth the traffic congestion. Under certain environment settings, the policy converges to a heuristic rule that assigns the idle AGV to the workcenter with the shortest queue length, which shows the adaptiveness of the proposed method.","2162-2388","","10.1109/TNNLS.2022.3222206","National Key Research and Development Program of China(grant numbers:2021YFE0206100); National Natural Science Foundation of China(grant numbers:62073321); National Defense Basic Scientific Research Program of China(grant numbers:JCKY2019203C029); Science and Technology Development Fund, Macau SAR(grant numbers:0060/2021/A2,0015/2020/AMJ); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9967762","Automated guided vehicle (AGV) dispatching;deep learning;reinforcement learning (RL);self-attention","Dispatching;Task analysis;Genetic algorithms;Deep learning;Costs;Vehicle dynamics;Heuristic algorithms","","","","1","","","IEEE","30 Nov 2022","","","IEEE","IEEE Early Access Articles"
"The Effect of Different Types of Internal Rewards in Distributed Multi-Agent Deep Reinforcement Learning","H. Zhang; D. Li; Y. He","State Key Laboratory of Robotics, Shenyang Institute of Automation Chinese Academy of Sciences, Shenyang, Liaoning, China; State Key Laboratory of Robotics, Shenyang Institute of Automation Chinese Academy of Sciences, Shenyang, Liaoning, China; State Key Laboratory of Robotics, Shenyang Institute of Automation Chinese Academy of Sciences, Shenyang, Liaoning, China","2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)","20 Jan 2020","2019","","","2890","2895","Distributed multiagent reinforcement learning in the same environment is prohibitively hard, due to the difficulty of assigning credit for the individual actions of the agent, especially when the agent is a member of a team. Meanwhile, the sparse delayed reward about the team from the environment such as winning makes the learning progress more challenging. To solve the credit assignment and sparse delayed reward problems which are common in multiagent reinforcement learning, researchers usually construct or learn an internal reward signal that acts as a proxy for winning and provides denser rewards for individual agent. To improve the learning effect of a typical multiagent learning task, we conducted three types of internal rewards for multiagent team members and evaluated the effect of these rewards. The results show that not all internal reward can improve the learning effect of multiagent reinforcement learning, it seems that when the learning task is not very complex and the time of finishing the task for the team is not very long, the sparse reward such as winning have the best learning effect, and the learning effect of the other two forms of reward is not as good as that of the simple sparse reward. To some extent, our research results can provide a reference for the design of reward function in the application of distributed multi-agent reinforcement learning.","","978-1-7281-6321-5","10.1109/ROBIO49542.2019.8961464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8961464","Internal reward;credit assignment;multiagent;deep reinforcement learning;distributed","Conferences;Biomimetics;Reinforcement learning;Task analysis;Robots;Finishing","learning (artificial intelligence);mobile robots;multi-robot systems","distributed multiagent deep reinforcement learning;learning progress;sparse delayed reward problems;internal reward signal;denser rewards;individual agent;learning effect;typical multiagent learning task;multiagent team members;simple sparse reward;reward function","","1","","11","IEEE","20 Jan 2020","","","IEEE","IEEE Conferences"
"An Optimal Control-Based Distributed Reinforcement Learning Framework for a Class of Non-Convex Objective Functionals of the Multi-Agent Network","Z. Chen; N. Li","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","IEEE/CAA Journal of Automatica Sinica","28 Sep 2023","2023","10","11","2081","2093","This paper studies a novel distributed optimization problem that aims to minimize the sum of the non-convex objective functionals of the multi-agent network under privacy protection, which means that the local objective of each agent is unknown to others. The above problem involves complexity simultaneously in the time and space aspects. Yet existing works about distributed optimization mainly consider privacy protection in the space aspect where the decision variable is a vector with finite dimensions. In contrast, when the time aspect is considered in this paper, the decision variable is a continuous function concerning time. Hence, the minimization of the overall functional belongs to the calculus of variations. Traditional works usually aim to seek the optimal decision function. Due to privacy protection and non-convexity, the Euler-Lagrange equation of the proposed problem is a complicated partial differential equation. Hence, we seek the optimal decision derivative function rather than the decision function. This manner can be regarded as seeking the control input for an optimal control problem, for which we propose a centralized reinforcement learning (RL) framework. In the space aspect, we further present a distributed reinforcement learning framework to deal with the impact of privacy protection. Finally, rigorous theoretical analysis and simulation validate the effectiveness of our framework.","2329-9274","","10.1109/JAS.2022.105992","National Natural Science Foundation of China (NSFC)(grant numbers:61773260); Ministry of Science and Technology(grant numbers:2018YFB 130590); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9938378","Distributed optimization;multi-agent;optimal control;reinforcement learning (RL)","Optimization;Privacy;Optimal control;Mathematical models;Calculus;Reinforcement learning;Protocols","concave programming;data protection;distributed processing;multi-agent systems;optimal control;partial differential equations;reinforcement learning","centralized reinforcement learning framework;continuous function;decision variable;distributed optimization problem;Euler-Lagrange equation;finite dimensions;local objective;multiagent network;nonconvex objective functionals;nonconvexity;optimal control-based distributed reinforcement learning framework;optimal decision derivative function;partial differential equation;privacy protection;RL framework;space aspect;time aspect","","1","","39","","3 Nov 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning With a Stage Incentive Mechanism of Dense Reward for Robotic Trajectory Planning","G. Peng; J. Yang; X. Li; M. O. Khyam","School of Artificial Intelligence and Automation and the Key Laboratory of Image Processing and Intelligent Control, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Automation, Southeast University, Nanjing, China; School of Engineering and Technology, Central Queensland University, Melbourne, VIC, Australia","IEEE Transactions on Systems, Man, and Cybernetics: Systems","17 May 2023","2023","53","6","3566","3573","To improve the efficiency of deep reinforcement learning (DRL)-based methods for robot manipulator trajectory planning in random working environments, we present three dense reward functions. These rewards differ from the traditional sparse reward. First, a posture reward function is proposed to speed up the learning process with a more reasonable trajectory by modeling the distance and direction constraints, which can reduce the blindness of exploration. Second, a stride reward function is proposed to improve the stability of the learning process by modeling the distance and movement distance of joint constraints. Finally, in order to further improve learning efficiency, we are inspired by the cognitive process of human behavior and propose a stage incentive mechanism, including a hard-stage incentive reward function and a soft-stage incentive reward function. Extensive experiments show that the soft-stage incentive reward function is able to improve the convergence rate, get higher mean reward and lower standard deviation after convergence.","2168-2232","","10.1109/TSMC.2022.3228901","National Natural Science Foundation of China(grant numbers:91748106); Hubei Province Natural Science Foundation of China(grant numbers:2019CFB526); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004017","Deep reinforcement learning (DRL);dense reward function;stage incentive mechanism;trajectory planning","Robots;Manipulators;Task analysis;Trajectory planning;Robot kinematics;Collision avoidance;Planning","cognition;deep learning (artificial intelligence);learning (artificial intelligence);manipulators;path planning;reinforcement learning;trajectory control","deep reinforcement learning-based methods;dense reward functions;direction constraints;hard-stage incentive reward function;higher mean reward;learning process;movement distance;posture reward function;random working environments;reasonable trajectory;robot manipulator trajectory planning;robotic trajectory planning;soft-stage incentive reward function;stage incentive mechanism;stride reward function;traditional sparse reward","","1","","25","CCBY","30 Dec 2022","","","IEEE","IEEE Journals"
"Active Fault-Tolerant Control Integrated with Reinforcement Learning Application to Robotic Manipulator","Z. Yan; J. Tan; B. Liang; H. Liu; J. Yang","Tsinghua Shenzhen International Graduate School, Center of Intelligent Control and Telescience, Tsinghua University, Shenzhen, China; Tsinghua Shenzhen International Graduate School, Center of Intelligent Control and Telescience, Tsinghua University, Shenzhen, China; Tsinghua Shenzhen International Graduate School, Center of Intelligent Control and Telescience, Tsinghua University, Shenzhen, China; Tsinghua Shenzhen International Graduate School, Center of Intelligent Control and Telescience, Tsinghua University, Shenzhen, China; Department of Automation, Navigation and Control Research Center, Tsinghua University, Beijing, China","2022 American Control Conference (ACC)","5 Sep 2022","2022","","","2656","2662","In this paper, we propose an active fault-tolerant control framework for robotic manipulators subjected to joint actuator faults. The proposed active fault-tolerant control scheme includes a neural network-based fault diagnosis module and a reinforcement learning-based fault-tolerant control module. Once the actuator fault is detected and diagnosed, an additive reinforcement learning controller will produce compensation torques to guarantee the system safety and maintain the control performance. Compared to traditional methods, our strategy avoids overly relying on the exact system models, which has potential for wider applications. The scheme is evaluated on a 7-DOF Panda manipulator for tracking control tasks in the MuJoCo simulator. Simulation results demonstrate the effectiveness of the proposed framework in dealing with single and multiple actuator faults.","2378-5861","978-1-6654-5196-3","10.23919/ACC53348.2022.9867641","National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9867641","","Fault tolerance;Actuators;Simulation;Fault tolerant systems;Reinforcement learning;Reliability engineering;Safety","actuators;control system synthesis;fault diagnosis;fault tolerant control;manipulators;neurocontrollers;nonlinear control systems;reinforcement learning","reinforcement learning application;robotic manipulator;active fault-tolerant control framework;joint actuator faults;neural network-based fault diagnosis module;reinforcement learning-based fault-tolerant control module;additive reinforcement learning controller;tracking control tasks;single actuator faults;multiple actuator faults;7-DOF Panda manipulator;MuJoCo simulator","","1","","25","","5 Sep 2022","","","IEEE","IEEE Conferences"
"Decentralized Multi-agent Reinforcement Learning with Multi-time Scale of Decision Epochs","J. Wu; K. Li; Q. -S. Jia","Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), Center for Intelligent and Networked Systems, Tsinghua University, Beijing, P. R. China; Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), Center for Intelligent and Networked Systems, Tsinghua University, Beijing, P. R. China; Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), Center for Intelligent and Networked Systems, Tsinghua University, Beijing, P. R. China","2020 59th IEEE Conference on Decision and Control (CDC)","11 Jan 2021","2020","","","578","584","Multi-agent reinforcement learning (MARL) has attracted more and more attention in recent years. It is now widely applied in various fields, including cyber physical systems, smart grid, finance, social network, and among others. The current researches on MARL mainly focus single-time scale, in which the agents have the same decision epoch. While in real applications, it is common that the agents make decisions by different frequencies. In addition, different agents may have separate roles in the system. In this paper, we propose a more general MARL framework by introducing multi-time scale of decision epochs. We assume that agents share information with their neighbors, including state, action, and reward. The global observability of state and action, which is a common assumption, is not required. We propose a decentralized Q-learning algorithm and a modified MADDPG algorithm to solve the problem. The main contributions of this paper are as follows. First, we formulate the multi-time scale multi-agent reinforcement learning (MTMARL) problem. This provides a general framework for the related systems and problems. Second, we provide a networked decentralized multi-time scale multi-agent Q-learning algorithm to solve the problem and prove its convergence. Third, we test the algorithm numerically. The results show that the proposed algorithm performs better than the previous QD-learning and is only slightly worse than the centralized algorithm.","2576-2370","978-1-7281-7447-1","10.1109/CDC42340.2020.9304323","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9304323","Multi-time scale;multiple decision epochs;multi-agent;reinforcement learning","Reinforcement learning;Q-factor;Observability;Transforms;Convergence;Communication networks;Smart grids","learning (artificial intelligence);multi-agent systems","decentralized multiagent reinforcement;decision epoch;cyber physical systems;single-time scale;general MARL framework;decentralized Q-learning algorithm;multitime scale multiagent reinforcement learning problem;networked decentralized multitime scale multiagent Q-learning algorithm;QD-learning;modified MADDPG algorithm;multiagent reinforcement learning;MTMARL;smart grid","","1","","24","IEEE","11 Jan 2021","","","IEEE","IEEE Conferences"
"Gradient Monitored Reinforcement Learning","M. S. Abdul Hameed; G. S. Chadha; A. Schwung; S. X. Ding","Department of Automation Technology, South Westphalia University of Applied Sciences, Soest, Germany; Department of Automation Technology, South Westphalia University of Applied Sciences, Soest, Germany; Department of Automation Technology, South Westphalia University of Applied Sciences, Soest, Germany; Department of Automatic Control and Complex Systems, University of Duisburg-Essen, Duisburg, Germany","IEEE Transactions on Neural Networks and Learning Systems","4 Aug 2023","2023","34","8","4106","4119","This article presents a novel neural network training approach for faster convergence and better generalization abilities in deep reinforcement learning (RL). Particularly, we focus on the enhancement of training and evaluation performance in RL algorithms by systematically reducing gradient’s variance and, thereby, providing a more targeted learning process. The proposed method, which we term gradient monitoring (GM), is a method to steer the learning in the weight parameters of a neural network based on the dynamic development and feedback from the training process itself. We propose different variants of the GM method that we prove to increase the underlying performance of the model. One of the proposed variants, momentum with GM (M-WGM), allows for a continuous adjustment of the quantum of backpropagated gradients in the network based on certain learning parameters. We further enhance the method with the adaptive M-WGM (AM-WGM) method, which allows for automatic adjustment between focused learning of certain weights versus more dispersed learning depending on the feedback from the rewards collected. As a by-product, it also allows for automatic derivation of the required deep network sizes during training as the method automatically freezes trained weights. The method is applied to two discrete (real-world multirobot coordination problems and Atari games) and one continuous control task (MuJoCo) using advantage actor–critic (A2C) and proximal policy optimization (PPO), respectively. The results obtained particularly underline the applicability and performance improvements of the methods in terms of generalization capability.","2162-2388","","10.1109/TNNLS.2021.3119853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585302","Atari games;deep neural networks (DNNs);gradient monitoring (GM);MuJoCo;multirobot coordination;OpenAI GYM;reinforcement learning (RL)","Training;Monitoring;Neural networks;Reinforcement learning;Optimization;Games;Task analysis","backpropagation;computer games;control engineering computing;deep learning (artificial intelligence);gradient methods;multi-robot systems;optimisation;reinforcement learning","adaptive M-WGM;AM-WGM;automatic adjustment;automatic derivation;backpropagated gradients;continuous adjustment;continuous control task using advantage actor-critic;deep network;deep reinforcement learning;dynamic development;evaluation performance;generalization abilities;GM method;gradient monitored reinforcement learning;gradient monitoring;learning parameters;neural network training approach;performance improvements;RL algorithms;systematically reducing gradient;targeted learning process;trained weights;training process;weight parameters","","1","","65","IEEE","25 Oct 2021","","","IEEE","IEEE Journals"
"Off-Policy Integral Reinforcement Learning for Semi-Global Constrained Output Regulation of Continuous-Time Linear Systems","Y. Yang; X. Chen; Y. Yin; D. C. Wunsch","School of Automation & Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation & Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation & Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Department of Electrical & Computer Engineering, Missouri University of Science & Technology, Rolla, USA","2018 International Joint Conference on Neural Networks (IJCNN)","14 Oct 2018","2018","","","1","6","This paper presents a data-driven method based on off-policy integral reinforcement learning to solve the semi-global output regulation of continuous-time linear systems with input saturation. A family of state feedback laws for the input constrained output regulation problem is designed based on solving an algebraic Riccati equation. In contrast to the existing methods, complete knowledge of the system dynamics is no longer required in this paper. Instead, the data collected from online implementation is efficiently utilized to design the controller. Therefore, the controller design in this paper is data-driven. It is shown that the presented method can find feedback control inputs with constraint of amplitude saturation and stabilize a given linear system with all its poles inside or on the imaginary axis. Finally, a simulation example is conducted to show the validity of the presented approach to solve the semi-global output regulation of continuous-time linear systems with input saturation.","2161-4407","978-1-5090-6014-6","10.1109/IJCNN.2018.8489343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489343","output regulation;input saturation;Algebraic Riccati equation;model-free;reinforcement learning","Mathematical model;Linear systems;Learning (artificial intelligence);State feedback;Riccati equations;Optimal control","continuous time systems;control system synthesis;learning (artificial intelligence);linear systems;Riccati equations;stability;state feedback","semiglobal output regulation;continuous-time linear systems;input saturation;state feedback laws;input constrained output regulation problem;system dynamics;controller design;feedback control inputs;policy integral reinforcement learning;semiglobal constrained output regulation;data-driven method;off-policy integral reinforcement learning;algebraic Riccati equation;amplitude saturation;stabilization","","1","","30","IEEE","14 Oct 2018","","","IEEE","IEEE Conferences"
"End-to-End Autonomous Exploration for Mobile Robots in Unknown Environments through Deep Reinforcement Learning","Z. Li; J. Xin; N. Li","Department of Automation, Shanghai Jiao Tong University, Shanghai, P.R. China; Department of Automation, Shanghai Jiao Tong University, Shanghai, P.R. China; Department of Automation, Shanghai Jiao Tong University, Shanghai, P.R. China","2022 IEEE International Conference on Real-time Computing and Robotics (RCAR)","5 Sep 2022","2022","","","475","480","Autonomous exploration in unknown environments is a significant capability for mobile robots. In this paper, we present an end-to-end autonomous exploration model based on deep reinforcement learning (DRL), which takes the sensor data and a novel exploration map as inputs, and directly outputs the motion control commands of the robot. In contrast to the existing DRL-based exploration methods, the proposed model has no requirements to be combined with the traditional exploration or navigation algorithms, resulting in lower computational complexity. We directly transfer the DRL-based model trained in the training map to four test maps with different sizes and layouts, and the results show that the robot can rapidly adapt to unknown scenes. Besides, a comparison study with RRT-exploration algorithm indicates that the proposed model can reach a higher map exploration rate within less distance and time. Furthermore, we also conduct experiments on the real physical robot to demonstrate the transferability of learned policy from simulation to reality. A video of our experiments in the Gazebo simulator and real world can be found here1","","978-1-6654-6983-8","10.1109/RCAR54675.2022.9872253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9872253","","Training;Adaptation models;Navigation;Computational modeling;Layout;Reinforcement learning;Robot sensing systems","collision avoidance;computational complexity;control engineering computing;learning (artificial intelligence);mobile robots;motion control;path planning","motion control commands;existing DRL-based exploration methods;traditional exploration;DRL-based model;training map;test maps;unknown scenes;RRT-exploration algorithm;higher map exploration rate;physical robot;learned policy;mobile robots;deep reinforcement learning;significant capability;end-to-end autonomous exploration model;novel exploration map","","1","","23","IEEE","5 Sep 2022","","","IEEE","IEEE Conferences"
"A Hierarchical Reinforcement Learning Framework based on Soft Actor-Critic for Quadruped Gait Generation","Y. Wang; W. Jia; Y. Sun","School of Mechatronics Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronics Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronics Engineering and Automation, Shanghai University, Shanghai, China","2022 IEEE International Conference on Robotics and Biomimetics (ROBIO)","18 Jan 2023","2022","","","1970","1975","Recently, reinforcement learning has become a promising control method of legged robot. However, it is challenging to train from scratch which requires perfect networks and reward design. In this paper, a hierarchical reinforcement learning framework based on Soft Actor-Critic has been proposed to find the appropriate gait of quadruped robot in the environment. The framework is composed of a low-level policy for generating joint reference trajectory and a high-level policy for gait optimization. In low-level policy, we use radial basis network and evolutionary computation solver to change the shape of reference trajectory in order to search for a better reference trajectory. In high-level policy, joint angle increment is learned to optimize gait. The experimental results show that the hierarchical framework is better than that of using Soft Actor-Critic only.","","978-1-6654-8109-0","10.1109/ROBIO55434.2022.10011919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011919","","Training;Shape;Reinforcement learning;Evolutionary computation;Radial basis function networks;Trajectory;Space exploration","evolutionary computation;humanoid robots;learning (artificial intelligence);legged locomotion;radial basis function networks;robot dynamics","appropriate gait;gait optimization;hierarchical framework;hierarchical reinforcement learning framework;high-level policy;joint reference trajectory;legged robot;low-level policy;perfect networks;promising control method;quadruped gait generation;quadruped robot;radial basis network;reward design;Soft Actor-Critic","","1","","16","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"Online Adaptive Decoding of Motor Imagery Based on Reinforcement Learning","J. Liu; S. Qu; W. Chen; J. Chu; Y. Sun","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Neurosurgery, Tiantan Hospital, Beijing, China; College of Biomedical Engineering & Instrument Science, Zhejiang University, Hangzhou, China","2019 14th IEEE Conference on Industrial Electronics and Applications (ICIEA)","16 Sep 2019","2019","","","522","527","The development of electronic and computer technology makes non-invasive acquisition systems of EEG more universal adoption. Therefore this paper focus on the BCI based on motor imagery, which is a popular studied spontaneous EEG, producing without external stimulus. 6 healthy people participate in the examination, 5 of which are saved while one of which is thrown due to disturbing. Independent component correlation algorithm (ICA) was used to extract motor related component, meanwhile other filters also designed for better signal quality. Taking the motor related components PSD distribution after ICA as features, signal modal decoding based on reinforcement learning is processed in two algorithms. Both of them have good performance. Finally, on-line decoding model is descripted on the basis of the off-line decoding model. The result shows the accuracy of training model has significant difference with random level. Furthermore, the conclusion can be developed into human-computer integration based on EEG signals.","2158-2297","978-1-5386-9490-9","10.1109/ICIEA.2019.8833778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8833778","Brain-Computer Interface;motor imagery;reinforment learning;online decoding;electroencephalography","Electroencephalography;Electrodes;Task analysis;Decoding;Reinforcement learning;Brain modeling;Brain-computer interfaces","brain-computer interfaces;electroencephalography;learning (artificial intelligence);medical signal processing","human-computer integration;off-line decoding model;on-line decoding model;signal modal;motor related components PSD distribution;signal quality;motor related component;ICA;independent component correlation algorithm;6 healthy people;external stimulus;EEG more universal adoption;noninvasive acquisition systems;electronic computer technology;reinforcement learning;motor imagery;online adaptive decoding","","1","","18","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"Morphing Strategy Design for UAV based on Prioritized Sweeping Reinforcement Learning","R. Li; Q. Wang; Y. Liu; C. Dong","School of Automation Science and Electrical Engineering, Beihang University, Beijing, People's Republic of China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, People's Republic of China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, People's Republic of China; School of Aeronautic Science and Engineering, Beihang University, Beijing, People's Republic of China","IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society","18 Nov 2020","2020","","","2786","2791","This paper proposes an improved deep deterministic policy gradient (DDPG) algorithm in the morphing policy designing for a kind of morphing unmanned aerial vehicles (UAVs) Considering that random selection in reinforcement learning structure is not always an efficient iterative update method, prioritized sweeping approach is introduced into the DDPG-based deep reinforcement learning framework, and the original DDPG algorithm is optimized to avoid random selection of state action pairs (SAPs). Consequently, the efficiency reduction problem in the traditional reinforcement learning structure is weakened. The proposed improved DDPG algorithm has better learning performance and can make reasonable decisions about environmental changes. A simulation experiment is carried out on the designed algorithm. By building a reinforcement learning model of the Markov decision process, the simulation results verify the effectiveness and superiority of the designed algorithm.","2577-1647","978-1-7281-5414-5","10.1109/IECON43393.2020.9254664","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9254664","deep deterministic policy gradient (DDPG);prioritized sweeping;state action pairs (SAPs);morphing determination","Reinforcement learning;Approximation algorithms;Unmanned aerial vehicles;Strain;Optimization;Stability analysis;Electrical engineering","autonomous aerial vehicles;control engineering computing;learning (artificial intelligence);Markov processes;path planning;remotely operated vehicles","state action pairs;efficiency reduction problem;traditional reinforcement learning structure;learning performance;reinforcement learning model;morphing strategy design;UAV;prioritized sweeping reinforcement learning;improved deep deterministic policy gradient algorithm;morphing policy;morphing unmanned aerial vehicles;random selection;efficient iterative update method;prioritized sweeping approach;DDPG-based deep reinforcement learning framework;original DDPG algorithm;SAP;Markov decision process","","1","","16","IEEE","18 Nov 2020","","","IEEE","IEEE Conferences"
"Event-Triggered Deep Reinforcement Learning for Dynamic Task Scheduling in Multisatellite Resource Allocation","K. Cui; J. Song; L. Zhang; Y. Tao; W. Liu; D. Shi","MIIT Key Laboratory of Servo Motion System Drive and Control, School of Automation, Beijing Institute of Technology, Beijing, China; MIIT Key Laboratory of Servo Motion System Drive and Control, School of Automation, Beijing Institute of Technology, Beijing, China; China Academy of Space Technology, Beijing, China; China Academy of Space Technology, Beijing, China; China Academy of Space Technology, Beijing, China; MIIT Key Laboratory of Servo Motion System Drive and Control, School of Automation, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Aerospace and Electronic Systems","8 Aug 2023","2023","59","4","3766","3777","In this work, we investigate the problem of multisatellite resource allocation for expected long-term performance optimization with a dynamic task network model, where communication tasks generated by task satellites are expected to be transmitted by resource satellites in the application layer, and the set of tasks changes with satellite orbital motions. The features of the tasks include priority, execution duration, visible time, etc. Since the feature information has a high dimension and changes with time, the scheduling problem is formulated as a dynamic combinatorial optimization problem and a receding-horizon task scheduling algorithm based on the event-triggered deep reinforcement learning is proposed. A residual-fully connected network is designed to extract the features of the complex task network model, and a deep double Q-learning iteration with the experience replay memory mechanism is employed to change the allocation strategy by evaluated rewards adaptively. An event-triggered strategy is then proposed to handle urgent tasks online. Numerical simulations show the performance improvement of the proposed algorithm. For the scenario of 50 task satellites and ten resource satellites, the proposed algorithm achieves 4.1%, 5.9%, and 11.4% higher reward scores than the static deep reinforcement learning algorithm, the data-driven parallel scheduling algorithm, and the improved genetic algorithm, respectively. The computation time of the proposed algorithm is only 34.7% and 21.3% of that of the latter two algorithms, and is similar to that of the static deep reinforcement learning algorithm.","1557-9603","","10.1109/TAES.2022.3231239","National Natural Science Foundation of China(grant numbers:62261160575); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9998480","Dynamic combinatorial optimization;event-triggered deep reinforcement learning;receding-horizon optimization;residual-fully connected network;resource allocation","Task analysis;Satellites;Heuristic algorithms;Dynamic scheduling;Satellite broadcasting;Optimization;Aerodynamics","aerospace computing;artificial satellites;combinatorial mathematics;deep learning (artificial intelligence);feature extraction;genetic algorithms;reinforcement learning;resource allocation;scheduling","complex task network model;deep double Q-learning iteration;dynamic combinatorial optimization problem;dynamic task network model;dynamic task scheduling;event-triggered deep reinforcement learning;expected long-term performance optimization;experience replay memory mechanism;feature extraction;feature information;multisatellite resource allocation;receding-horizon task;residual-fully connected network;resource satellites;satellite orbital motions","","1","","53","IEEE","23 Dec 2022","","","IEEE","IEEE Journals"
"Multi-Task Reinforcement Learning based Mobile Manipulation Control for Dynamic Object Tracking and Grasping","C. Wang; Q. Zhang; X. Wang; S. Xu; Y. Petillot; S. Wang","State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Edinburgh Centre for Robotics Edinburgh Centre for Robotics, Heriot-Watt University, Edinburgh, UK; Edinburgh Centre for Robotics Edinburgh Centre for Robotics, Heriot-Watt University, Edinburgh, UK; Edinburgh Centre for Robotics Edinburgh Centre for Robotics, Heriot-Watt University, Edinburgh, UK","2022 7th Asia-Pacific Conference on Intelligent Robot Systems (ACIRS)","18 Aug 2022","2022","","","34","40","Agile control of mobile manipulator is challenging because of the high complexity coupled by the robotic system and the unstructured working environment. Tracking and grasping a dynamic object with a random trajectory is even harder. In this paper, a multi-task reinforcement learning-based mobile manipulation control framework is proposed to achieve general dynamic object tracking and grasping. Several basic types of dynamic trajectories are chosen as the training set for the task. To improve policy generalization in practice, random noise and dynamics randomization are introduced during the training process. Extensive experiments show that our trained policy can adapt to unseen random dynamic trajectories with about 0.1 m tracking error and 75% grasping success rate for dynamic objects. The trained policy can also be successfully deployed on a real mobile manipulator.","","978-1-6654-8519-7","10.1109/ACIRS55390.2022.9845515","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9845515","Reinforcement Learning;Mobile Manipulation;Dynamic Object","Training;Grasping;Reinforcement learning;Multitasking;Trajectory;Complexity theory;Object tracking","learning (artificial intelligence);manipulators;mobile robots;object tracking;random processes;trajectory control","multitask reinforcement learning;general dynamic object tracking;dynamic object grasping;random noise;dynamics randomization;unseen random dynamic trajectories;agile control;unstructured working environment;random trajectory;mobile manipulation control framework","","1","","31","IEEE","18 Aug 2022","","","IEEE","IEEE Conferences"
"Application of Reinforcement Learning in the Autonomous Driving Platform of the DeepRacer","W. Zhu; H. Du; M. Zhu; Y. Liu; C. Lin; S. Wang; W. Sun; H. Yan","Department of Electrical & Automation of SEIEE, Shanghai Jiaotong University, Shanghai; Department of Automation of SEIEE, Shanghai Jiaotong University, Shanghai; Department of Electrical & Automation of SEIEE, Shanghai Jiaotong University, Shanghai; Teaching development and Student Innovation Center of SEIEE, Shanghai Jiao Tong University, Shanghai; AWS BD Consultant, Amazon Web Service Inc, Beijing; Chinese Electronics Technology Group 54Th Institute, Shi Jiazhuang; Xin Dong Interactive Entertainment Company Ltd, Shanghai; Summer Son Smart Technology Company Ltd, Shanghai","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","5345","5352","This article revolves around autonomous driving, mainly introducing the autonomous driving cloud platform based on the reinforcement learning to improve the autonomous driving of the car on the Deep Racer using the AWS (Amazon Web Service). Applying the sample codes provided by the DeepRacer platform, the training and completion of the car requires a long time. Therefore, we applied path planning into DeepRacer based on reinforcement learning. Several breakthroughs have been shown as follows: The formulation and solution of RL are completed on DeepRacer. The model of vehicle is simplified as the bicycle and thus reality gap between perception and joints was narrowed. A novel system framework VNARM(Vehicle Network Autonomous Racing Model) is introduced. Reward functions were set properly for tracing in RL. The vehicle's performance of finishing one lap is increased from nearly 30 seconds to less than 9 seconds, while maintaining a high percentage of completion.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902325","National Natural Science Foundation of China(grant numbers:61873163); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902325","Autonomous driving;Amazon's cloud platform;Reinforcement learning;Sarsa;Action space;Path planning","Training;Cloud computing;Analytical models;Web services;Reinforcement learning;Path planning;Data models","advanced driver assistance systems;bicycles;cloud computing;path planning;reinforcement learning;vehicular ad hoc networks;Web services","Amazon Web Service;autonomous driving cloud platform;AWS;bicycle;DeepRacer platform;path planning;reinforcement learning;reward functions;RL;vehicle network autonomous racing model;VNARM","","1","","15","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Nonlinear inverse reinforcement learning with mutual information and Gaussian process","D. C. Li; Y. Q. He; F. Fu","Shenyang Institute of Automation Chinese Academy of Sciences, Shenyang, Liaoning, CN; Shenyang Institute of Automation the Chinese Academy of Sciences, Shenyang, China; Shenyang Institute of Automation the Chinese Academy of Sciences, Shenyang, China","2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)","23 Apr 2015","2014","","","1445","1450","In this paper, a mutual information (MI) and Extreme Learning Machine (ELM) based inverse reinforcement learning (IRL) algorithm, which termed as MEIRL, is proposed to construct nonlinear reward function. The basic idea of MIIRL is that, similar to GPIRL, the reward function is learned by using Gaussian process and the importance of each feature is obtained by using automatic relevance determination (ARD). Then mutual information is employed to evaluate the impact of each feature to the reward function, based on which extreme learning machine is introduced along with an adaptive model construction procedure to choose the optimal subset of features and the performance of the original GPIRL algorithm is enhanced as well. Furthermore, to demonstrate the effectiveness of MEIRL, a simulation called highway driving is constructed. The simulation results show that MEIRL is comparable with the state of art IRL algorithms in terms of generalization capability, but more efficient while the number of features is large.","","978-1-4799-7397-2","10.1109/ROBIO.2014.7090537","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7090537","","Mutual information;Learning (artificial intelligence);Gaussian processes;Approximation error;Bayes methods;Adaptation models;Algorithm design and analysis","Gaussian processes;learning (artificial intelligence)","nonlinear inverse reinforcement learning;mutual information;Gaussian process;extreme learning machine;ELM;inverse reinforcement learning algorithm;MEIRL;nonlinear reward function;MIIRL;automatic relevance determination;ARD;adaptive model construction procedure;GPIRL algorithm;highway driving;generalization capability","","1","","16","IEEE","23 Apr 2015","","","IEEE","IEEE Conferences"
"Policy Evaluation and Seeking for Multiagent Reinforcement Learning via Best Response","R. Yan; X. Duan; Z. Shi; Y. Zhong; J. R. Marden; F. Bullo","Department of Automation, Tsinghua University, Beijing, China; Mechanical Engineering Department and the Center of Control, Dynamical Systems and Computation, University of California Santa Barbara, CA, USA; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, University of California Santa Barbara, CA, USA; Mechanical Engineering Department and the Center of Control, Dynamical Systems and Computation, University of California Santa Barbara, CA, USA","IEEE Transactions on Automatic Control","28 Mar 2022","2022","67","4","1898","1913","Multiagent policy evaluation and seeking are long-standing challenges in developing theories for multiagent reinforcement learning (MARL), due to multidimensional learning goals, nonstationary environment, and scalability issues in the joint policy space. This article introduces two metrics grounded on a game-theoretic solution concept called sink equilibrium, for the evaluation, ranking, and computation of policies in multiagent learning. We adopt strict best response dynamics (SBRDs) to model selfish behaviors at a meta-level for MARL. Our approach can deal with dynamical cyclical behaviors (unlike approaches based on Nash equilibria and Elo ratings), and is more compatible with single-agent reinforcement learning than $\alpha$-rank, which relies on weakly better responses. We first consider settings where the difference between the largest and second largest equilibrium metric has a known lower bound. With this knowledge, we propose a class of perturbed SBRD with the following property: only policies with maximum metric are observed with nonzero probability for a broad class of stochastic games with finite memory. We then consider settings where the lower bound for the difference is unknown. For this setting, we propose a class of perturbed SBRD such that the metrics of the policies observed with nonzero probability differ from the optimal by any given tolerance. The proposed perturbed SBRD addresses the scalability issue and opponent-induced nonstationarity by fixing the strategies of others for the learning agent, and uses empirical game-theoretic analysis to estimate payoffs for each strategy profile obtained due to the perturbation.","1558-2523","","10.1109/TAC.2021.3085171","National Natural Science Foundation of China(grant numbers:61374034); China Scholarship Council; Air Force Office of Scientific Research(grant numbers:FA9550-15-1-0138); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444583","Best response;multiagent reinforcement learning;policy evaluation and seeking;sink equilibrium;stochastic stability","Games;Measurement;Convergence;Nash equilibrium;Heuristic algorithms;Perturbation methods;Scalability","game theory;learning (artificial intelligence);multi-agent systems;stochastic games","joint policy space;game-theoretic solution concept;multiagent learning;strict best response dynamics;MARL;dynamical cyclical behaviors;single-agent reinforcement learning;weakly better responses;second largest equilibrium metric;perturbed SBRD;nonzero probability;scalability issue;learning agent;empirical game-theoretic analysis;multiagent reinforcement learning;multiagent policy evaluation;long-standing challenges;multidimensional learning goals","","1","","51","IEEE","31 May 2021","","","IEEE","IEEE Journals"
"Orientation-Preserving Rewards’ Balancing in Reinforcement Learning","J. Ren; S. Guo; F. Chen","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","27 Oct 2022","2022","33","11","6458","6472","Auxiliary rewards are widely used in complex reinforcement learning tasks. However, previous work can hardly avoid the interference of auxiliary rewards on pursuing the main rewards, which leads to the destruction of the optimal policy. Thus, it is challenging but essential to balance the main and auxiliary rewards. In this article, we explicitly formulate the problem of rewards’ balancing as searching for a Pareto optimal solution, with the overall objective of preserving the policy’s optimization orientation for the main rewards (i.e., the policy driven by the balanced rewards is consistent with the policy driven by the main rewards). To this end, we propose a variant Pareto and show that it can effectively guide the policy search toward more main rewards. Furthermore, we establish an iterative learning framework for rewards’ balancing and theoretically analyze its convergence and time complexity. Experiments in both discrete (grid word) and continuous (Doom) environments demonstrated that our algorithm can effectively balance rewards, and achieve remarkable performance compared with those RLs with heuristically designed rewards. In the ViZDoom platform, our algorithm can learn expert-level policies.","2162-2388","","10.1109/TNNLS.2021.3080521","National Natural Science Foundation of China(grant numbers:61671266,61836004); Tsinghua-Guoqiang Research Program(grant numbers:2019GQG006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9452798","Automatic rewards’ balancing;auxiliary rewards;Pareto solutions;reinforcement learning;reward design","Trajectory;Task analysis;Reinforcement learning;Optimization;Switches;Search problems;Pareto optimization","iterative methods;learning (artificial intelligence);Pareto optimisation","auxiliary rewards;balanced rewards;complex reinforcement learning tasks;heuristically designed rewards;main rewards;optimal policy;orientation-preserving rewards","Computer Simulation;Neural Networks, Computer;Reinforcement, Psychology;Reward;Learning","1","","46","IEEE","11 Jun 2021","","","IEEE","IEEE Journals"
"Adaptive beamforming based on the deep reinforcement learning","C. Hao; X. Sun; Y. Liu","College of Electrical Engineering and Automation, Shandong University of Science and Technology, Qingdao, China; College of Electrical Engineering and Automation, Shandong University of Science and Technology, Qingdao, China; College of Electrical Engineering and Automation, Shandong University of Science and Technology, Qingdao, China","2022 IEEE International Conference on Networking, Sensing and Control (ICNSC)","12 Jan 2023","2022","","","1","6","In the downlink transmission, the legitimate users are also vulnerable to malicious jamming when the desired signal is launched by multiple antenna array. Then, this paper investigates an anti-jamming defense criteria through a deep reinforcement learning (DRL) to suppress the jamming at transmitting information. Firstly, a dual-polarized antenna array is designed to achieve the global positioning system (GPS) signals model, and the information of designed model can be directly used into the anti-jamming criteria more general. Then, we set up the preprocessing framework based on a cyclic convolutional neural network (CNN), which is used to preprocesses the objective function by approximating the evaluated variable to the target value, i.e., the dynamic objective function of beamforming. Subsequently, a DRL algorithm based on the deep Q-network is proposed to achieve the optical null-steering strategy in dynamic jamming environment. Finally, some simulation results validate the proposed DRL can effectively improve both the system sum-rate and combating dynamic jamming level compared with other approaches.","","978-1-6654-7243-2","10.1109/ICNSC55942.2022.10004128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004128","Adaptive beamforming;Convolutional neural network;Anti-jamming;Deep reinforcement learning;Deep Q-network","Deep learning;Array signal processing;Heuristic algorithms;Data preprocessing;Reinforcement learning;Prediction algorithms;Linear programming","array signal processing;convolutional neural nets;deep learning (artificial intelligence);Global Positioning System;jamming;reinforcement learning","anti-jamming defense criteria;beamforming;combating dynamic jamming level;cyclic convolutional neural network;deep Q-network;deep reinforcement learning;downlink transmission;DRL algorithm;dual-polarized antenna array;dynamic jamming environment;dynamic objective function;global positioning system signals model;legitimate users;malicious jamming;multiple antenna array;preprocessing framework","","1","","15","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"Self-Attention-Based Temporary Curiosity in Reinforcement Learning Exploration","H. Hu; S. Song; G. Huang","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","17 Aug 2021","2021","51","9","5773","5784","In many real-world scenarios, extrinsic rewards provided by the environment are sparse. An agent trained with classic reinforcement learning algorithm fails to explore these environments in a sufficient and effective way. To address this problem, the exploration bonus which derives from environmental novelty serves as intrinsic motivation for the agent. In recent years, curiosity-driven exploration is a mainstream approach to describe environmental novelty through prediction errors of dynamics models. Due to the expressive ability limitations of curiosity-based environmental novelty and the difficulty of finding appropriate feature space, most curiosity-driven exploration methods have the problem of overprotection against repetition. This problem can reduce the efficiency of exploration and lead the agent into a trap with local optimality. In this article, we propose a combination of persisting curiosity and temporary curiosity framework to deal with the problem of overprotection against repetition. We introduce the self-attention mechanism from the field of computer vision and propose a sequence-based self-attention mechanism for temporary curiosity generation. We compare our framework with some previous exploration methods in hard-exploration environments, provide a series of comprehensive analysis of the proposed framework and investigate the effect of the individual components of our method. The experimental results indicate that the proposed framework delivers superior performance than existing methods.","2168-2232","","10.1109/TSMC.2019.2957051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8936518","Attention mechanism;curiosity-driven;exploration;intrinsic reward;reinforcement learning","Reinforcement learning;Task analysis;Dynamics;Computer vision;Neural networks;Predictive models;Games","computer vision;learning (artificial intelligence)","intrinsic motivation;prediction errors;dynamics models;expressive ability limitations;curiosity-based environmental novelty;feature space;curiosity-driven exploration;overprotection;sequence-based self-attention mechanism;temporary curiosity generation;hard-exploration environments;reinforcement learning exploration;extrinsic rewards","","1","","36","IEEE","18 Dec 2019","","","IEEE","IEEE Journals"
"High-speed Train Timetabling Based on Reinforcement Learning","W. Yang; P. Jiang; S. Song","Department of Automation and BNRist, Tsinghua University, Beijing, China; Department of Automation and BNRist, Tsinghua University, Beijing, China; Department of Automation and BNRist, Tsinghua University, Beijing, China","2022 IEEE Symposium Series on Computational Intelligence (SSCI)","30 Jan 2023","2022","","","1187","1193","Chinese high-speed railway has developed rapidly in the more intelligent and automatic direction over the past few decades. In this paper, we consider the optimization problem of the train timetable for the high-speed railway to minimize the total train waiting time and total station occupied time. To deal with time-related constraints, we first establish the train operation environment based on Discrete Event Dynamic System (DEDS). Then, we reformulate the timetabling problem as a Markov Decision Process (MDP) problem and propose an improved Q-learning approach by redesigning Q-value function to solve the problem. Finally, we consider the Beijing-Shanghai high-speed railway as a numerical example, where the passenger flow and train running time are stochastic. We empirically show that our Q-learning method reduces over 30% total waiting time and 1.9% total occupied time compared with the well-known First-Come-First-Service (FCFS) scheduling strategy.","","978-1-6654-8768-9","10.1109/SSCI51031.2022.10022145","National Natural Science Foundation of China(grant numbers:61936009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10022145","High-speed railway;Train timetable;Markov decision process;Q-learning algorithm","Q-learning;Processor scheduling;Markov processes;Rail transportation;Dynamical systems;Optimization;Computational intelligence","discrete event systems;learning (artificial intelligence);Markov processes;optimisation;rail traffic;railways;scheduling","30% total waiting time;automatic direction;Beijing-Shanghai high-speed railway;chinese high-speed railway;Discrete Event Dynamic System;improved Q-learning approach;intelligent direction;Markov Decision Process problem;optimization problem;passenger flow;Q-learning method;reinforcement learning;speed train timetabling;time-related constraints;timetabling problem;total station;total train waiting time;train operation environment;train running time;train timetable","","1","","21","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Obstacle Avoidance Algorithm via Hierarchical Interaction Deep Reinforcement Learning","Z. Ding; C. Song; J. Xu","School of Automation, Beijing Institute of Technology, Beijing, P. R. China; School of Automation, Beijing Institute of Technology, Beijing, P. R. China; School of Automation, Beijing Institute of Technology, Beijing, P. R. China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","3680","3685","The navigation task in a complex scenario is an essential problem in mobile robot technology. The mobile robot obstacle avoidance algorithm plays a vital role in navigation. In the navigation task, the mobile robot has to select the optimal action under different conditions in real-time. This research proposes a novel obstacle avoidance algorithm based on deep reinforcement learning. The proposed algorithm utilizes interacting with the environment in the simulation to update the decision network. The decision network includes the feature extraction module and the hierarchical interaction module. The feature extraction module can extract and identify the features of dynamic obstacles in the scenario. And the hierarchical interaction module can handle the interaction features between the mobile robot and obstacles. Furthermore, a safety module is applied in the algorithm to guarantee mobile robot collision-free. Finally, the experiment is conducted to evaluate the proposed method in the simulation environment. The experiment result verified the safety and effectiveness of the proposed method and proved that the proposed method could ensure the mobile robot completes the task.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9901935","Beijing Institute of Technology(grant numbers:3060021212104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9901935","reinforcement learning;moving obstacle avoidance;motion planning","Navigation;Heuristic algorithms;Estimation;Reinforcement learning;Feature extraction;Real-time systems;Safety","collision avoidance;feature extraction;learning (artificial intelligence);mobile robots;navigation","complex scenario;decision network;dynamic obstacles;feature extraction module;hierarchical interaction deep reinforcement learning;hierarchical interaction module;interaction features;mobile robot collision-free;mobile robot obstacle avoidance algorithm;mobile robot technology;navigation task;safety module","","","","18","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Formation Control using Simplified Reinforcement Learning for Multi-agent systems with State Delay","W. Shao; Y. Chen; J. Huang","College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","2269","2274","In this paper, a simplified reinforcement learning (RL) of identifier-actor-critic architecture is proposed to solve the formation problem of multiagent state-delay system. The dynamics of multi-agent systems includes the uncertainties and state delay, which is more practical in applications. In the multiagent system formation control, the system uncertainties are counteracted through the fuzzy logic system; the state delay is offset by applying a Lyapunov-Krasovskii functional. The updating laws of RL are derived from a simple equation, which is equivalent to the gradient of the HJB equation. Finally, a simulation example is given to demonstrate the satisfactory performance of the proposed method.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9549357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9549357","Fuzzy logic systems (FLSs);Multi-agent formation;Identifier-Actor-Critic architecture;simplified reinforcement learning (RL);state delay","Fuzzy logic;Uncertainty;Simulation;Reinforcement learning;Control systems;Mathematical models;Delays","delay systems;delays;learning (artificial intelligence);Lyapunov methods;multi-agent systems;time-varying systems","RL;simplified reinforcement learning;multiagent systems;state delay;identifier-actor-critic architecture;formation problem;multiagent state-delay system;multiagent system formation control;system uncertainties;fuzzy logic system","","","","13","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Mapless Path Planning of Multi-robot Systems in Complex Environments via Deep Reinforcement Learning","W. Han; C. Fang; J. He","Department of Automation, Ministry of Education of China, Shanghai Jiao Tong University, and Key Laboratory of System Control and Information Processing, Shanghai, China; Department of Automation, Ministry of Education of China, Shanghai Jiao Tong University, and Key Laboratory of System Control and Information Processing, Shanghai, China; Department of Automation, Ministry of Education of China, Shanghai Jiao Tong University, and Key Laboratory of System Control and Information Processing, Shanghai, China","2022 4th International Conference on Data-driven Optimization of Complex Systems (DOCS)","5 Dec 2022","2022","","","1","6","As mobile robots are becoming more and more widely used, it is of great significance to design an efficient path planning method for multi-robot systems (MRS) that can adapt to complex and unknown environments. In this paper, we present a deep reinforcement learning (DRL) based method to navigate the MRS with collision avoidance in unknown dynamic environments, which consists of centralized learning and decentralized executing paradigm. The proposed policy maps original laser information into robot control commands without constructing global maps. The learned policy is tested in Gazebo environments with three robot systems, which shows the effective performance in terms of success rate, extra time rate, and formation maintenance rate.","","978-1-6654-5982-2","10.1109/DOCS55193.2022.9967756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9967756","","Deep learning;Simulation;Lasers;Robot control;Reinforcement learning;Maintenance engineering;Path planning","collision avoidance;control engineering computing;learning (artificial intelligence);mobile robots;multi-robot systems;navigation;path planning;robot vision","centralized learning;complex environments;deep reinforcement learning based method;efficient path planning method;Gazebo environments;learned policy;mapless path planning;mobile robots;MRS;multirobot systems;policy maps original laser information;robot control;robot systems;unknown dynamic environments","","","","17","IEEE","5 Dec 2022","","","IEEE","IEEE Conferences"
"Autonomous Vehicles Roundup Strategy by Reinforcement Learning with Prediction Trajectory","J. Ni; R. Ma; H. Zhong; B. Wang","School of Automation, Beijing Institute of Technology, Beijing; School of Automation, Beijing Institute of Technology, Beijing; Beijing Aerospace Control Center, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","3370","3375","Autonomous vehicles are increasingly applied on many situations, but their autonomous decision-making ability needs to be improved. Multi-Agent Deep Deterministic Policy Gradient(MADDPG) adopts the method of centralized evaluation and decentralized execution, so that the autonomous vehicle can obtain the whole-field status information and make decisions through the companion information. In the process of autonomous vehicle training, we introduce artificial potential field, action guidance and other methods to alleviate the problem of sparse rewards. At the same time, we add a repulsion function to consider the relationship between team vehicles. Extended Kalman Filter(EKF) is also applied to predict the autonomous vehicle trajectory, changing the training network state input information. At the same time, secondary correction of the predicted autonomous vehicle trajectory is made to change the prediction range with the training time, and improve the training convergence speed while the speed of opposite agents increases. Simulation experiments show that the convergence speed and win rate of MADDPG algorithm based on trajectory prediction and artificial potential field is significantly improved, and it also has strong adaptability to various task scenarios.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902245","autonomous vehicle roundup;reinforcement learning;artificial potential field;trajectory prediction","Training;Process control;Reinforcement learning;Predictive models;Real-time systems;Trajectory;Artificial intelligence","collision avoidance;Kalman filters;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;nonlinear filters;path planning","artificial potential field;autonomous decision-making ability needs;autonomous vehicle training;autonomous vehicles roundup strategy;companion information;opposite agents increases;predicted autonomous vehicle trajectory;prediction range;prediction trajectory;reinforcement learning;team vehicles;training convergence speed;training network state input information;training time;trajectory prediction;whole-field status information","","","","11","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Reinforcement learning-based scheduling of multi-battery energy storage system","G. Cheng; L. Dong; X. Yuan; C. Sun","School of Automation, Southeast University, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China","Journal of Systems Engineering and Electronics","10 Mar 2023","2023","34","1","117","128","In this paper, a reinforcement learning-based multi-battery energy storage system (MBESS) scheduling policy is proposed to minimize the consumers' electricity cost. The MBESS scheduling problem is modeled as a Markov decision process (MDP) with unknown transition probability. However, the optimal value function is time-dependent and difficult to obtain because of the periodicity of the electricity price and residential load. Therefore, a series of time-independent action-value functions are proposed to describe every period of a day. To approximate every action-value function, a corresponding critic network is established, which is cascaded with other critic networks according to the time sequence. Then, the continuous management strategy is obtained from the related action network. Moreover, a two-stage learning protocol including offline and online learning stages is provided for detailed implementation in real-time battery management. Numerical experimental examples are given to demonstrate the effectiveness of the developed algorithm.","1004-4132","","10.23919/JSEE.2023.000036","National Key R&D Program of China(grant numbers:2018AAA0101400); National Natural Science Foundation of China(grant numbers:61921004,62173251,U1713209,62236002); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10066213","multi-battery energy storage system (MBESS);reinforcement learning;periodic value iteration;data-driven","Costs;Protocols;Battery management systems;Markov processes;Real-time systems;Energy storage;Load modeling","battery management systems;battery storage plants;Markov processes;power engineering computing;power generation economics;power generation scheduling;probability;reinforcement learning","electricity price;Markov decision process;MBESS scheduling problem;MDP;multibattery energy storage system;real-time battery management;reinforcement learning-based scheduling;transition probability","","","","38","","10 Mar 2023","","","BIAI","BIAI Journals"
"Anti-collision Trajectory Planning for Satellite Formation Reconstruction Based on Deep Reinforcement Learning","H. Li; Q. Zong; X. Zhang","School of Electrical Engineering and Automation, Tianjin, P. R. China; School of Electrical Engineering and Automation, Tianjin, P. R. China; School of Electrical Engineering and Automation, Tianjin, P. R. China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","4672","4677","This paper proposes an optimal trajectory planning method for satellite formation reconstruction based on deep reinforcement learning. To begin, the action space, state space, and reward function of satellite formation reconstruction are created, with the collision avoidance constraint taken into account. Second, the algorithm's essential parameters' learning rate and appropriate noise are determined. What's more, the Unity program is developed to create the training environment, so the real satellite dynamics model are embed in the environment. The optimal trajectory of formation satellite reconstruction obtained by this method can better meet the constraints such as collision avoidance, and the calculation speed is fast, which makes the autonomous real-time reconstruction of formation satellite possible. Finally, 1 simulation example is carried out to verify the proposed algorithm, showing that the formation reconfiguration task can be executed successfully while achieving rapid convergence.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9901660","National Natural Science Foundation of China(grant numbers:62003236,62073234,61903349,62022060); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9901660","formation reconstruction;trajectory planning;collision avoidance;deep reinforcement learning","Training;Satellites;Trajectory planning;Simulation;Reinforcement learning;Real-time systems;Robustness","artificial satellites;attitude control;collision avoidance;learning (artificial intelligence);path planning;position control;trajectory control","anti-collision trajectory planning;collision avoidance constraint;deep reinforcement learning;formation reconfiguration task;formation satellite possible;formation satellite reconstruction;optimal trajectory planning method;real-time reconstruction;satellite dynamics model;satellite formation reconstruction","","","","13","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Online Operational Decision-making for Integrated Electric-Gas Systems with Safe Reinforcement Learning","A. R. Sayed; X. Zhang; Y. Wang; G. Wang; J. Qiu; C. Wang","School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; College of Mechatronics and Control Engineering, Shenzhen University, Shenzhen, China; School of Electrical and Information Engineering, University of Sydney, NSW, Australia; School of Electrical and Electronic Engineering, North China Electric Power University, Beijing, China","IEEE Transactions on Power Systems","","2023","PP","99","1","14","Increasing interdependencies between power and gas systems and integrating large-scale intermittent renewable energy increase the complexity of energy management problems. This paper proposes a model-free safe deep reinforcement learning (DRL) approach to find fast optimal energy flow (OEF), guaranteeing its feasibility in real-time operation with high computational efficiency. A constrained Markov decision process model is standardized for the optimization problem of OEF with a limited number of state and control actions and developing a robust integrated environment. Because state-of-the-art DRL algorithms lack safety guarantees, this paper develops a soft-constraint enforcement method to adaptively encourage the control policy in the safety direction with non-conservative control actions. The overall procedure, namely the constrained soft actor-critic (C-SAC) algorithm, is off-policy, entropy maximization-based, sample-efficient, and scalable with low hyper-parameter sensitivity. The proposed C-SAC algorithm validates its superiority over the existing learning-based safety ones and OEF solution methods by finding fast OEF decisions with near-zero degrees of constraint violations. The proposed approach indicates its practicability for real-time energy system operation and extensions for other potential applications.","1558-0679","","10.1109/TPWRS.2023.3320172","China Postdoctoral Science Foundation(grant numbers:2023M730847); National Natural Science Foundation of China(grant numbers:72001058,72171155); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2023A1515010724,2022A1515240051); General Program of Foundation of Shenzhen Science and Technology Committee(grant numbers:GXWD20201230155427003-20200822103658001,JCYJ20190808141019317); Major Science and Technology Special Projects in Xinjiang Autonomous Region(grant numbers:2022A01007); Xinjiang Autonomous Region Key Research and Development(grant numbers:2022B01016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10266735","Safe learning;Optimal energy flow;Reinforcement learning;Fast control;Integrated energy systems;Soft actor-critic","Mathematical models;Safety;Compressors;Process control;Pipelines;Entropy;Uncertainty","","","","","","","IEEE","28 Sep 2023","","","IEEE","IEEE Early Access Articles"
"A Reinforcement Learning Based on Multi-Node Charging Path Planning Method for Wireless Rechargeable Sensor Networks","H. Wang; J. Li; J. Wang; C. Li","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, P. R. China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, P. R. China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, P. R. China; Department of Electrical and Information Engineering, Applied Technology of Dalian Ocean University, Dalin, P. R. China","2023 IEEE 18th Conference on Industrial Electronics and Applications (ICIEA)","11 Sep 2023","2023","","","1017","1022","Due to the limitations of sensor node battery capacity, network size, and cost, energy provision is the primary challenge in wireless sensor networks (WSNs). To solve the sensor energy limitation problem, the use of mobile chargers (MC) to supplement the energy of wireless sensor nodes has become a practical and effective approach, which is known as wireless rechargeable sensor networks (WRSNs). Compared to the one-to-one charging scheme, the one-to-many charging scheme that allows multiple sensor nodes to be charged simultaneously by a single charger can well address the charging scalability and efficiency issues. In a multi-node charging scheme, the charging locations of the MC need to be determined first before the charging path can be determined, which adds to the complexity of the charging scheme design. To address these challenges, this paper proposed a charging point determination method based on the greedy strategy and used reinforcement learning (RL) to solve the charging planning problem in WRSNs. Finally, the effectiveness of the proposed method is verified by simulation.","2158-2297","979-8-3503-1220-1","10.1109/ICIEA58696.2023.10241778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10241778","WRSN;one-to-many charging scheme;reinforcement learning","Wireless communication;Industrial electronics;Wireless sensor networks;Costs;Scalability;Reinforcement learning;Path planning","battery chargers;battery powered vehicles;electric vehicle charging;inductive power transmission;path planning;power engineering computing;reinforcement learning;wireless sensor networks","charging point determination method;charging scheme design;energy provision;mobile chargers;multinode charging path planning method;reinforcement learning;sensor energy limitation problem;sensor node battery capacity;wireless rechargeable sensor networks;WRSN","","","","13","IEEE","11 Sep 2023","","","IEEE","IEEE Conferences"
"Research on Autonomous Operation Control of Microgrid Based on Deep Reinforcement Learning","L. Xie; Y. Li; J. Xiao; J. Yang; B. Xu; Y. Ye","School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; State Grid Shanghai Municipal Electric Power Company, State Grid of China, Shanghai, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; State Grid Shanghai Municipal Electric Power Company, State Grid of China, Shanghai, China; State Grid Shanghai Municipal Electric Power Company, State Grid of China, Shanghai, China","2021 IEEE 5th Conference on Energy Internet and Energy System Integration (EI2)","25 Feb 2022","2021","","","2503","2507","In order to solve the problem that distributed power is difficult to use on a large scale, microgrid as a solution has developed rapidly in recent years. According to different operation modes, microgrid can be divided into two operation modes: grid-connected operation and island operation. When the microgrid is operating on an island mode, in order to provide better power quality for the loads, it is necessary to design a reasonable control strategy. This paper takes isolated microgrid as the research objective and uses deep reinforcement learning to control the voltage of microgrid with a constant value. A classic isolated microgrid model, which is combined with the depth determination strategy gradient (DDPG) algorithm in deep reinforcement learning, is established to maintain the load voltage within an acceptable level. Finally, the simulation experiment is compared with the classical controller and the model predictive controller to verify the effectiveness of the proposed control method.","","978-1-6654-3425-6","10.1109/EI252483.2021.9713298","State Grid Corporation of China(grant numbers:52093220000H); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9713298","microgrid;deep reinforcement learning;depth determination strategy gradient;constant voltage control","PI control;Power quality;Microgrids;Reinforcement learning;System integration;Predictive models;Prediction algorithms","distributed power generation;learning (artificial intelligence);power distribution faults;power grids;predictive control","depth determination strategy gradient algorithm;deep reinforcement learning;classical controller;model predictive controller;control method;autonomous operation control;distributed power;grid-connected operation;island operation;island mode;power quality;reasonable control strategy;classic isolated microgrid model","","","","22","IEEE","25 Feb 2022","","","IEEE","IEEE Conferences"
"Duty Cycle Optimization for Blood Pressure Sensors in Wireless Body Area Networks Based on Reinforcement Learning","L. Wang; S. Xi; W. Liu; Q. Zhou","Automation School, Nanjing University of Science and Technology, Nanjing, China; Automation School, Nanjing University of Science and Technology, Nanjing, China; School of electronic information, Jiangsu University of science and technology, Zhenjiang, China; Automation School, Nanjing University of Science and Technology, Nanjing, China","2021 4th IEEE International Conference on Industrial Cyber-Physical Systems (ICPS)","5 Jul 2021","2021","","","799","804","Blood Pressure (BP) is a rhythmic biological parameter which reflects the health state of individual's cardiovascular system. In this paper, the issue of duty cycle optimization for BP sensor in a Wireless Body Area Network (WBAN) scenario is studied. The sticking point is how to achieve a balance between monitoring reliability and energy conservation according to the BP status. Motivated by this, we use the peak and the trend of BP values over a period of time to characterize the BP status of a human, and establishes a two-dimensional BP evolution model based on the circadian and the seasonal variation of BP. Then we formulate the duty cycle optimization problem as a Markov Decision Process (MDP) and a Q-learning based algorithm is proposed as the solution. To test the validity of the algorithm, BP values of 180 days are generated based on the basic medical statistics. Simulation results show that the proposed algorithm significantly reduces energy consumption and effectively restrains delay in sensing abnormality.","","978-1-7281-6207-2","10.1109/ICPS49255.2021.9468195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468195","wireless body area network;duty cycle;reinforcement learning;optimization","Wireless communication;Wireless sensor networks;Energy consumption;Markov processes;Body area networks;Sensors;Delays","blood pressure measurement;body area networks;body sensor networks;cardiovascular system;energy conservation;learning (artificial intelligence);Markov processes;medical computing;optimisation;pressure sensors","blood pressure sensors;wireless body area networks;reinforcement learning;rhythmic biological parameter;health state;BP sensor;sticking point;monitoring reliability;energy conservation;BP status;BP values;two-dimensional BP evolution model;duty cycle optimization problem;Q-learning based algorithm;cardiovascular system;circadian;Markov decision process;medical statistics;sensing abnormality","","","","9","IEEE","5 Jul 2021","","","IEEE","IEEE Conferences"
"Integration of Adaptive Control and Reinforcement Learning for Real-time Control and Learning","A. M. Annaswamy; A. Guha; Y. Cui; S. Tang; P. A. Fisher; J. E. Gaudio","MIT, Cambridge, MA, USA; MIT, Cambridge, MA, USA; MIT, Cambridge, MA, USA; MIT, Cambridge, MA, USA; MIT, Cambridge, MA, USA; Aurora Flight Sciences, a Boeing Company, Cambridge, MA, USA","IEEE Transactions on Automatic Control","","2023","PP","99","1","16","This paper considers the problem of real-time control and learning in dynamic systems subjected to parametric uncertainties. We propose a combination of a Reinforcement Learning (RL) based policy in the outer loop suitably chosen to ensure stability and optimality for the nominal dynamics, together with Adaptive Control (AC) in the inner loop so that in real-time AC contracts the closed-loop dynamics towards a stable trajectory traced out by RL. Two classes of nonlinear dynamic systems are considered, both of which are control-affine. The first class of dynamic systems utilizes equilibrium points and a Lyapunov approach while second class of nonlinear systems uses contraction theory. AC-RL controllers are proposed for both classes of systems and shown to lead to online policies that guarantee stability using a high-order tuner and accommodate parametric uncertainties and magnitude limits on the input. In addition to establishing a stability guarantee with real-time control, the AC-RL controller is also shown to lead to parameter learning with persistent excitation for the first class of systems. Numerical validations of all algorithms are carried out using a quadrotor landing task on a moving platform.","1558-2523","","10.1109/TAC.2023.3290037","Boeing Strategic University Initiative and the Air Force Research Laboratory, Collaborative Research and Development for Innovative Aerospace Leadership (CRDInAL), Thrust 3 - Control Automation and Mechanization(grant numbers:FA 8650-16-C-2642); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10164143","","Uncertainty;Real-time systems;Convergence;Stability analysis;Numerical stability;Nonlinear dynamical systems;Adaptive control","","","","","","","IEEE","27 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Policy evaluation for reinforcement learning over asynchronous multi-agent networks","X. Sha; J. Zhang; K. You","Department of Automation, and BNRist, Tsinghua University, Beijing, P. R. China; Department of Automation, and BNRist, Tsinghua University, Beijing, P. R. China; Department of Automation, and BNRist, Tsinghua University, Beijing, P. R. China","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","5373","5378","This paper proposes a fully asynchronous algorithm for policy evaluation of multi-agent reinforcement learning over networks. Without any form of coordination, agents can communicate with neighbors and compute their local variables using (possibly) delayed information at any time. Thus, the proposed scheme fully takes advantage of the distributed setting. We prove that our method converges to a neighborhood of the optimum at a linear rate, showing the computational advantage by reducing the amount of synchronization. Numerical experiments show that our method is robust to straggler agents.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9550466","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9550466","Multi-agent reinforcement learning;multi-agent networks;fully asynchronous updates;policy evaluation","Handheld computers;Simulation;Reinforcement learning;Synchronization","learning (artificial intelligence);multi-agent systems","policy evaluation;reinforcement learning;asynchronous multiagent networks;fully asynchronous algorithm;multiagent reinforcement;local variables;delayed information;distributed setting;computational advantage;straggler agents","","","","35","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Multiagent Deep Reinforcement Learning for Automated Truck Platooning Control","R. Lian; Z. Li; B. Wen; J. Wei; J. Zhang; L. Li","Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Pengcheng National Lab, Shenzhen, China; DiDi Chuxing, Beijing, China; DiDi Chuxing, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Intelligent Transportation Systems Magazine","","2023","PP","99","2","17","Human-leading automated truck platooning has been an effective technique to improve traffic capacity and fuel economy and eliminate uncertainties of the traffic environment. Aiming for a tradeoff between the dynamic response of car following and energy-efficient platooning control, a predictive information multiagent soft actor–critic (PI-MASAC) control framework is proposed for a human-leading automated heavy-duty-truck platoon. In this framework, predictive information of environmental dynamics is modeled as the state representation of a deep reinforcement learning algorithm to address the uncertainties of a partially observable environment. In the truck model, the impact of intraplatoon aerodynamic interactions is modeled, which is used to design a constant spacing policy for platooning control. We demonstrate the effectiveness of our approach by testing the human-leading truck platoon under multiple scenarios compared to proximal policy optimization, an intelligent driver model, and linear-based cooperative adaptive cruise control. Our results show that the PI-MASAC learns a novel car-following strategy of peak shaving and valley filling and therefore significantly enhances energy savings by reducing high-intensity accelerations and decelerations. In addition, the PI-MASAC demonstrates its adaptability to various initial scenarios and exhibits good generalization to a larger platoon size.","1941-1197","","10.1109/MITS.2023.3319091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10273625","","Aerodynamics;Atmospheric modeling;Vehicle dynamics;Drag;Adaptation models;Uncertainty;Stability analysis","","","","","","","IEEE","6 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Exploration With Task Information for Meta Reinforcement Learning","P. Jiang; S. Song; G. Huang","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","4 Aug 2023","2023","34","8","4033","4046","Meta reinforcement learning (meta-RL) is a promising technique for fast task adaptation by leveraging prior knowledge from previous tasks. Recently, context-based meta-RL has been proposed to improve data efficiency by applying a principled framework, dividing the learning procedure into task inference and task execution. However, the task information is not adequately leveraged in this approach, thus leading to inefficient exploration. To address this problem, we propose a novel context-based meta-RL framework with an improved exploration mechanism. For the existing exploration and execution problem in context-based meta-RL, we propose a novel objective that employs two exploration terms to encourage better exploration in action and task embedding space, respectively. The first term pushes for improving the diversity of task inference, while the second term, named action information, works as sharing or hiding task information in different exploration stages. We divide the meta-training procedure into task-independent exploration and task-relevant exploration stages according to the utilization of action information. By decoupling task inference and task execution and proposing the respective optimization objectives in the two exploration stages, we can efficiently learn policy and task inference networks. We compare our algorithm with several popular meta-RL methods on MuJoco benchmarks with both dense and sparse reward settings. The empirical results show that our method significantly outperforms baselines on the benchmarks in terms of sample efficiency and task performance.","2162-2388","","10.1109/TNNLS.2021.3121432","National Science and Technology Innovation 2030 Major Project of the Ministry of Science and Technology of China(grant numbers:2018AAA0101604); National Natural Science Foundation of China(grant numbers:61936009,61906106,62022048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9604770","Exploration;meta reinforcement learning (meta-RL);mutual information;task information","Task analysis;Optimization;Reinforcement learning;Mutual information;Entropy;Context modeling;Inference algorithms","inference mechanisms;reinforcement learning","context-based meta-RL framework;data efficiency;exploration stages;exploration terms;fast task adaptation;improved exploration mechanism;inefficient exploration;learning procedure;meta reinforcement learning;meta-training procedure;named action information;popular meta-RL methods;sample efficiency;sharing hiding task information;task embedding space;task execution;task inference;task performance;task-independent exploration;task-relevant exploration stages","","","","40","IEEE","5 Nov 2021","","","IEEE","IEEE Journals"
"MO-MIX: Multi-Objective Multi-Agent Cooperative Decision-Making With Deep Reinforcement Learning","T. Hu; B. Luo; C. Yang; T. Huang","School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; Texas A&M University at Qatar, Doha, Qatar","IEEE Transactions on Pattern Analysis and Machine Intelligence","5 Sep 2023","2023","45","10","12098","12112","Deep reinforcement learning (RL) has been applied extensively to solve complex decision-making problems. In many real-world scenarios, tasks often have several conflicting objectives and may require multiple agents to cooperate, which are the multi-objective multi-agent decision-making problems. However, only few works have been conducted on this intersection. Existing approaches are limited to separate fields and can only handle multi-agent decision-making with a single objective, or multi-objective decision-making with a single agent. In this paper, we propose MO-MIX to solve the multi-objective multi-agent reinforcement learning (MOMARL) problem. Our approach is based on the centralized training with decentralized execution (CTDE) framework. A weight vector representing preference over the objectives is fed into the decentralized agent network as a condition for local action-value function estimation, while a mixing network with parallel architecture is used to estimate the joint action-value function. In addition, an exploration guide approach is applied to improve the uniformity of the final non-dominated solutions. Experiments demonstrate that the proposed method can effectively solve the multi-objective multi-agent cooperative decision-making problem and generate an approximation of the Pareto set. Our approach not only significantly outperforms the baseline method in all four kinds of evaluation metrics, but also requires less computational cost.","1939-3539","","10.1109/TPAMI.2023.3283537","National Natural Science Foundation of China(grant numbers:62022094,61988101); Zhejiang Lab(grant numbers:2021NB0AB01); Open Fund of the Laboratory of Cognition and Decision Intelligence for Complex Systems; Chinese Academy of Sciences(grant numbers:CASIA-KFKT-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10145811","Deep reinforcement learning;multi-agent;multi-objective;decision-making;Pareto","Decision making;Reinforcement learning;Approximation algorithms;Training;Task analysis;Behavioral sciences;Multi-agent systems","decision making;deep learning (artificial intelligence);learning (artificial intelligence);multi-agent systems;Pareto optimisation;reinforcement learning","complex decision-making problems;decentralized agent network;decision-making problem;deep reinforcement learning;multiobjective decision-making;multiobjective multiagent decision-making problems;multiobjective multiagent reinforcement learning problem;multiple agents","","","","61","IEEE","7 Jun 2023","","","IEEE","IEEE Journals"
"Advantage Constrained Proximal Policy Optimization in Multi-Agent Reinforcement Learning","W. Li; Y. Zhu; D. Zhao","State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2023 International Joint Conference on Neural Networks (IJCNN)","2 Aug 2023","2023","","","1","8","We investigate the integration of value-based and policy gradient methods in multi-agent reinforcement learning (MARL). The Individual-Global-Max (IGM) principle plays an important role in value-based MARL, as it ensures consistency between joint and local action values. IGM is difficult to guarantee in multi-agent policy gradient methods due to stochastic exploration and conflicting gradient directions. In this paper, we propose a novel multi-agent policy gradient algorithm called Advantage Constrained Proximal Policy Optimization (ACPPO). ACPPO calculates each agent's current local state-action advantage based on their advantage network and estimates the joint state-action advantage based on multi-agent advantage decomposition lemma. According to the consistency of the estimated joint-action advantage and local advantage, the coefficient of each agent constrains the joint-action advantage. ACPPO, unlike previous policy gradient MARL algorithms, does not require an additional sampled baseline to reduce variance or a sequential scheme to improve accuracy. The proposed method is evaluated using the continuous matrix game, the Starcraft Multi-Agent Challenge, and the Multi-Agent MuJoCo task. ACPPO outperforms baselines such as MAPPO, MADDPG, and HATRPO, according to the results.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10191652","National Key Research and Development Program of China(grant numbers:2018AAA0102404); National Natural Science Foundation of China(grant numbers:62136008); Youth Innovation Promotion Association CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191652","multi-agent;reinforcement learning;policy gradient","Gradient methods;Neural networks;Reinforcement learning;Games;Matrix decomposition;Task analysis;Optimization","computer games;deep learning (artificial intelligence);gradient methods;learning (artificial intelligence);multi-agent systems;optimisation;reinforcement learning","ACPPO;Advantage Constrained Proximal Policy Optimization;advantage network;conflicting gradient directions;current local state-action advantage;IGM;Individual-Global-Max principle;joint action values;joint state-action advantage;joint-action advantage;local action values;local advantage;multiagent advantage decomposition lemma;MultiAgent MuJoCo task;multiagent policy gradient methods;multiagent reinforcement learning;novel multiagent policy gradient algorithm;previous policy gradient MARL algorithms;Starcraft MultiAgent Challenge;stochastic exploration;value-based MARL","","","","25","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Efficiency-Power Density Multi-Objective Optimization Design of LLC Resonant Converter Based on Deep Reinforcement Learning","J. Wang; Z. Yao; R. Yang","School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China","2023 IEEE 6th International Electrical and Energy Conference (CIEEC)","10 Jul 2023","2023","","","1808","1813","As all knows, the dramatic improvement in the performance of wide band gap devices, such as SiC devices, can greatly increase the power density or efficiency of converters. However, with the increase of the switching frequency, the volume of the electrical electronic device has decreased significantly, but the loss has greatly increased. Therefore, the conflict between the converter device design indicators is exacerbated, and a comprehensive optimization design needs to be performed. The existing converter design method may only consider the optimization of the single goal, or the order design method that depends on artificial experience. There are defects for the optimized design between multiple targets and changes when the demand changes. In this paper, a deep reinforcement learning (DRL) based multi-objective optimization design method for the efficiency-power density of a single-phase LLC resonant converter is presented as an example, which enables the optimal design parameters to be obtained quickly at design time according to the design objectives. When the design requirements change, it is possible to achieve rapid access to the best design parameters according to the design objectives. First, a framework for DRL-based efficiency-power density optimization is presented; then the efficiency and power density of the converter are modelled. Afterwards, the agent is continuously trained by self-learning with deep deterministic policy gradients (DDPG) and an optimization strategy that minimizes power loss as well as volume is obtained. Finally, the strategy is able to respond quickly to changes in design specifications and provide design parameters that optimize efficiency-power density.","","979-8-3503-4667-1","10.1109/CIEEC58067.2023.10165893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10165893","multi-objective optimization;LLC resonance converter;deep reinforcement learning;DDPG algorithm","Performance evaluation;Deep learning;Power system measurements;Density measurement;Silicon carbide;Design methodology;Switching frequency","deep learning (artificial intelligence);gradient methods;optimisation;power engineering computing;reinforcement learning;resonant power convertors","comprehensive optimization design;converter design method;converter device design indicators;DDPG;deep deterministic policy gradients;deep reinforcement learning based multiobjective optimization design method;design requirements change;DRL;DRL-based efficiency-power density optimization;efficiency-power density multiobjective optimization design;electrical electronic device;optimal design parameters;optimization strategy;optimize efficiency-power density;order design method;SiC devices;single-phase LLC resonant converter;wide band gap devices","","","","16","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"Counterfactual Evolutionary Reasoning for Virtual Driver Reinforcement Learning in Safe Driving","P. Ye; H. Qi; F. Zhu; Y. Lv","State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Rail Transportation, Shandong Jiaotong University, Jinan, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Intelligent Vehicles","","2023","PP","99","1","10","Safety is the primary concern in the motion planning and decision-making of the virtual driver that provides prescriptions to the real human driver and even performs self-driving in the absence of human take-over. For such an issue, traditional reinforcement learning methods, limited by their learning mechanisms, suffer from a slow convergence of model training as well as a less consideration for early warning of possible accidents. To address the above deficiency, this paper proposes a new method based on counterfactual evolutionary reasoning that can be used to build the virtual driver. The method treats safe driving as a sequential decision-making problem with sparse rewards, and employs counterfactual evolutionary reasoning to guide the searching direction as well as to accelerate the model training. An intervention mechanism from outlier distributions is further introduced to enhance the model's ability of exploration. Experiments in the virtual test environment indicate that the proposed method, compared with other typical reinforcement learning techniques, both achieves a higher safe arrival rate and a faster convergence speed.","2379-8904","","10.1109/TIV.2023.3322694","National Natural Science Foundation of China(grant numbers:62076237,T2192933,U1909204); Youth Innovation Promotion Association Chinese Academy of Sciences(grant numbers:2021130); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10274149","Safe driving;reinforcement learning;counter factual reasoning;evolutionary search","Vehicles;Safety;Accidents;Decision making;Cognition;Trajectory;Convergence","","","","","","","IEEE","9 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Hypothesis-Driven Skill Discovery for Hierarchical Deep Reinforcement Learning","C. Chuck; S. Chockchowwat; S. Niekum",The University of Texas at Austin Personal Robotics and Automation Lab.; The University of Texas at Austin Personal Robotics and Automation Lab.; The University of Texas at Austin Personal Robotics and Automation Lab.,"2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","10 Feb 2021","2020","","","5572","5579","Deep reinforcement learning (DRL) is capable of learning high-performing policies on a variety of complex high-dimensional tasks, ranging from video games to robotic manipulation. However, standard DRL methods often suffer from poor sample efficiency, partially because they aim to be entirely problem-agnostic. In this work, we introduce a novel approach to exploration and hierarchical skill learning that derives its sample efficiency from intuitive assumptions it makes about the behavior of objects both in the physical world and simulations which mimic physics. Specifically, we propose the Hypothesis Proposal and Evaluation (HyPE) algorithm, which discovers objects from raw pixel data, generates hypotheses about the controllability of observed changes in object state, and learns a hierarchy of skills to test these hypotheses. We demonstrate that HyPE can dramatically improve the sample efficiency of policy learning in two different domains: a simulated robotic blockpushing domain, and a popular benchmark task: Breakout. In these domains, HyPE learns high-scoring policies an order of magnitude faster than several state-of-the-art reinforcement learning methods.","2153-0866","978-1-7281-6212-6","10.1109/IROS45743.2020.9340891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340891","","Transfer learning;Reinforcement learning;Proposals;Task analysis;Standards;Physics;Intelligent robots","deep learning (artificial intelligence)","problem-agnostic;HyPE;raw pixel data;policy learning;popular benchmark task;high-scoring policies;hierarchical deep reinforcement learning;high-dimensional tasks;video games;robotic manipulation;DRL methods;hypothesis proposal and evaluation algorithm;robotic blockpushing domain simulation;hypothesis-driven skill discovery","","","","53","IEEE","10 Feb 2021","","","IEEE","IEEE Conferences"
"Adaptive Optimal Tracking Control for Uncertain Unmanned Surface Vessel via Reinforcement Learning","L. Chen; M. Wang; S. -L. Dai","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","8398","8403","This paper presents a reinforcement-learning (RL)-based adaptive optimal tracking control scheme for unmanned surface vessel (USV) in the presence of modeling uncertainties and time-varying disturbances. By backstepping technique, the virtual and actual controls are designed as the optimal solutions of the corresponding sub-systems such that the overall optimization control is realized for the USV system. To improve the robustness on modeling uncertainties and time-varying disturbances, we employ the neural network (NN) approximation to approximate the modeling uncertainties and construct a disturbance observer to compensate for the external disturbances. An adaptive optimal control algorithm is presented by employing the actor-critic RL algorithm. The theoretical analysis shows that the desired optimized performance can be obtained via Lyapunov stability theory. Simulation results demonstrate the effectiveness of the proposed adaptive optimal control scheme.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9550299","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9550299","Adaptive optimal tracking control;neural network (NN);reinforcement learning (RL);unmanned surface vessel (USV)","Adaptation models;Uncertainty;Simulation;Optimal control;Artificial neural networks;Approximation algorithms;Disturbance observers","adaptive control;control system synthesis;learning (artificial intelligence);learning systems;Lyapunov methods;neurocontrollers;observers;optimal control;optimisation;robust control;time-varying systems;uncertain systems;unmanned surface vehicles","uncertain unmanned surface vessel;time-varying disturbances;virtual control design;optimization control;modeling uncertainties;adaptive optimal control algorithm;performance optimization;reinforcement-learning-based adaptive optimal tracking control;USV system;robustness;disturbance observer;external disturbance compensation;actor-critic RL algorithm;Lyapunov stability theory;neural network approximation","","","","19","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Adaptive Model-Based Reinforcement Learning for Fast Charging Optimization of Lithium-Ion Batteries","Y. Hao; Q. Lu; X. Wang; B. Jiang","Department of Automation, Tsinghua University, Beijing, China; Department of Chemical Engineering, Texas Tech University, Lubbock, TX, USA; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Industrial Informatics","","2023","PP","99","1","10","Fast charging problem of lithium-ion batteries with minimum-charging time while limiting battery degradation, has been receiving increasing attention and is a critical challenge to battery community. Difficulties in this optimization lie in that: (i) The parameter space of charging strategies is high dimensional while the budget of the experimental cost is often limited; (ii) The evaluation of charging strategies' performance is expensive, and (iii) the degradation process of battery is strongly nonlinear and multiple degradation mechanisms occur simultaneously leading to difficulties for establishing accurate first-principles models. Current methods to address these difficulties are mainly electrochemical model-based optimization and grid search, which are rarely adaptive to battery degradation and/or are of low sample efficiency. In this work, we propose an adaptive model-based reinforcement learning (RL) approach for fast charging optimization while limiting battery degradation, in which a probabilistic surrogate model of differential Gaussian process (GP) is adopted to adaptively describe the degradation of cells. The effectiveness of the proposed approach is demonstrated on PETLION, a high-performance PET-based battery simulator. The results show that (i) compared with the model-free RL method, the proposed adaptive GP-based RL approach possesses superior charging performance and high sample efficiency, and (ii) the proposed method performs well in the handling of degradation constraints on voltage and temperature for dynamically aging batteries with its adaptability to the variations of environment.","1941-0050","","10.1109/TII.2023.3257299","Tsinghua-Toyota Joint Research Fund, the National Natural Science Foundation of China(grant numbers:62273197); National Key Research and Development Program of China(grant numbers:2022YFE0197600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070849","Fast charging optimization;Lithium-ion battery;Reinforcement learning;Machine learning;Gaussian process","Batteries;Adaptation models;Degradation;Optimization;Computational modeling;Reinforcement learning;Probabilistic logic","","","","","","","IEEE","15 Mar 2023","","","IEEE","IEEE Early Access Articles"
"Extended Kalman Filter Based Resilient Formation Tracking Control of Multiple Unmanned Vehicles via Game-Theoretical Reinforcement Learning","L. Xue; B. Ma; J. Liu; C. Mu; D. C. Wunsch","Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education; School of Automation, Southeast University, Nanjing, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education; School of Automation, Southeast University, Nanjing, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education; School of Automation, Southeast University, Nanjing, China; School of of Electrical and Information Engineering, Tianjin University, Tianjin, China; Department of Electrical and Computer Engineering, Missouri University of Science and Technology, Rolla, MO, USA","IEEE Transactions on Intelligent Vehicles","27 Apr 2023","2023","8","3","2307","2318","In this paper, we discuss the resilient formation tracking control problem of multiple unmanned vehicles (MUV). A dynamic leader-follower distributed control structure is utilized to optimize the performance of the formation tracking. For the follower of the MUV, the leader is a cooperative unmanned vehicle, and the target of formation tracking is a non-cooperative unmanned vehicle with a nonlinear trajectory. Therefore, an extended Kalman filter (EKF) observer is designed to estimate the state of the target. Then the leader of the MUV is adjusted dynamically according to the state of the target. In order to describe the interactions between the follower and dynamic leader, a Stackelberg game model is constructed to handle the hierarchical decision problems. At the lower layer, each follower responds by observing the leader's strategy, and the potential game is used to prove a Nash equilibrium among all followers. At the upper layer, the dynamic leader makes decisions depending on the response of all followers to reaching the Stackelberg equilibrium. Moreover, the Stackelberg-Nash equilibrium of the designed game theoretical model is proven. A novel reinforcement learning-based algorithm is designed to achieve the Stackelberg-Nash equilibrium of the game. Finally, the effectiveness of the method is verified by a variety of formation tracking simulation experiments.","2379-8904","","10.1109/TIV.2023.3237790","National Key Research and Development Program of China(grant numbers:2021YFB1714700); National Natural Science Foundation of China(grant numbers:62022061,62273094); Zhishan Scholars Programs of Southeast University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018835","Extended Kalman filter;leader-switching;formation tracking;Stackelberg-Nash equilibrium;reinforcement learning","Target tracking;Games;Trajectory;Heuristic algorithms;Vehicle dynamics;Mathematical models;Observers","distributed control;game theory;Kalman filters;nonlinear filters;reinforcement learning;remotely operated vehicles","designed game theoretical model;dynamic leader-follower distributed control structure;extended Kalman filter observer;follower responds;formation tracking simulation experiments;game-theoretical reinforcement learning;hierarchical decision problems;multiple unmanned vehicles;MUV;potential game;reinforcement learning-based algorithm;resilient formation tracking control problem;Stackelberg equilibrium;Stackelberg game model;Stackelberg-Nash equilibrium;unmanned vehicle","","","","44","IEEE","17 Jan 2023","","","IEEE","IEEE Journals"
"NVIF: Neighboring Variational Information Flow for Cooperative Large-Scale Multiagent Reinforcement Learning","J. Chai; Y. Zhu; D. Zhao","Institute of Automation, State Key Laboratory of Multimodal Artificial Intelligence Systems, Chinese Academy of Sciences, Beijing, China; Institute of Automation, State Key Laboratory of Multimodal Artificial Intelligence Systems, Chinese Academy of Sciences, Beijing, China; Institute of Automation, State Key Laboratory of Multimodal Artificial Intelligence Systems, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","13","Communication-based multiagent reinforcement learning (MARL) has shown promising results in promoting cooperation by enabling agents to exchange information. However, the existing methods have limitations in large-scale multiagent systems due to high information redundancy, and they tend to overlook the unstable training process caused by the online-trained communication protocol. In this work, we propose a novel method called neighboring variational information flow (NVIF), which enhances communication among neighboring agents by providing them with the maximum information set (MIS) containing more information than the existing methods. NVIF compresses the MIS into a compact latent state while adopting neighboring communication. To stabilize the overall training process, we introduce a two-stage training mechanism. We first pretrain the NVIF module using a randomly sampled offline dataset to create a task-agnostic and stable communication protocol, and then use the pretrained protocol to perform online policy training with RL algorithms. Our theoretical analysis indicates that NVIF-proximal policy optimization (PPO), which combines NVIF with PPO, has the potential to promote cooperation with agent-specific rewards. Experiment results demonstrate the superiority of our method in both heterogeneous and homogeneous settings. Additional experiment results also demonstrate the potential of our method for multitask learning.","2162-2388","","10.1109/TNNLS.2023.3309608","Strategic Priority Research Program of Chinese Academy of Sciences (CAS)(grant numbers:XDA27030400); National Natural Science Foundation of China(grant numbers:62293541,62136008); National Key Research and Development Program of China(grant numbers:2018AAA0102404); Youth Innovation Promotion Association of CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10242075","Large-scale multiagent;neighboring communication;reinforcement learning (RL);variational information flow","Training;Protocols;Multi-agent systems;Task analysis;Redundancy;Reinforcement learning;Scalability","","","","","","","IEEE","6 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Aircraft Anti-Skid Braking Control Based on Reinforcement Learning","Z. Jiao; N. Bai; D. Sun; X. Liu; J. Li; Y. Shi; Y. Hou; C. Chen","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Beijing Institute of Control Engineering, Beijing, China; Ningbo Institute of Technology, Beihang University, Ningbo, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; China Academy of Space Technology, Beijing, China; China Academy of Space Technology, Beijing, China; China Academy of Space Technology, Beijing, China","IEEE Transactions on Aerospace and Electronic Systems","","2023","PP","99","1","14","Reliability and efficiency under complex working conditions are the ultimate goal of aircraft wheel brake system. In order to ensure the safety under different working conditions, traditional control methods for aircraft braking tend to be conservative in parameter adjustment, resulting in loss of efficiency. In this paper, an anti-skid brake control algorithm based on reinforcement learning (RL) is designed, which takes both safety and efficiency into consideration. Meanwhile, to solve the problems of generalization difficulty of different working conditions, designing difficulty of the reward function and the high failure rate in training brought by direct application of RL, we adopt task decomposition, dimension reduction of state features, action discretization and inverse reinforcement learning (IRL) strategies. Finally, to avoid human-computer interaction disputes, we design a strategy in which the RL agent is pilot-activated without changing the existing hardware. The efficiency and robustness of the algorithm are proved in simulation under three typical disturbance conditions. Besides, the ground inertial bench test results show that the average deceleration rate of the proposed algorithm is 25% higher than that of the traditional algorithm, which is of engineering application value.","1557-9603","","10.1109/TAES.2023.3319600","National Natural Science Foundation of China(grant numbers:52205045); National Key Research and Development Program of China(grant numbers:2021YFB2011300); Young Elite Scientists Sponsorship Program by CAST(grant numbers:YESS20200063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10266979","Aircraft;anti-disturbance;aviation engineering;brake system;reinforcement learning","Brakes;Wheels;Aircraft;Aerospace control;Torque;Aircraft propulsion;Control systems","","","","","","","IEEE","28 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Attention-based Highway Safety Planner for Autonomous Driving via Deep Reinforcement Learning","G. Chen; Y. Zhang; X. Li","School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China","IEEE Transactions on Vehicular Technology","","2023","PP","99","1","14","In this paper, a motion planning for autonomous driving on highway is studied. A high-level motion planning controller with discrete action space is designed based on deep Q network (DQN). An occupancy grid based state presentation aiming at specific scenarios is proposed and then a novel attention mechanism named external spatial attention (ESA) is designed for occupancy grid to improve the network performance. Con-sidering both computational complexity and interpretability, a lightweight data-driven safety layer consisting of two-dimensional linear biased support vector machine (2D-LBSVM) is proposed to improve safety. The advantages of this controller and the role of each module are illustrated by experiments. In addition, the superior performance of occupancy grid state and the interpretability of safety layer are further analyzed.","1939-9359","","10.1109/TVT.2023.3304530","National Key R&D Program of China(grant numbers:2021ZD0112700); National Natural Science Foundation (NNSF) of China(grant numbers:61973082,62233003); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20202006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214640","Autonomous vehicles;deep reinforcement learn-ing;attention;safety layer","Safety;Planning;Autonomous vehicles;Training;Deep learning;Vehicle dynamics;Trajectory","","","","","","","IEEE","11 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Safe Reinforcement Learning for Signal Temporal Logic Tasks Using Robust Control Barrier Functions","J. Chen; Y. Zou; S. Li","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","8627","8632","In this paper, a control synthesis problem based on reinforcement learning (RL) for continuous system under temporal logic tasks is studied. Since the systems are expected to satisfy diverse safety and liveness properties under temporal logic tasks, traditional RL algorithm does not work well due to the heuristic design of rewards and non-existent security guarantee. In this work, a novel framework is proposed to synthesize a safe and optimal controller based on RL for temporal logic tasks. First, signal temporal logic (STL) is adopted to formally describe the temporal logic tasks. Based on robust semantics of STL formula, a reference reward curve is designed to determine the reward according to the current instant and state. Then, the shield layer is designed by robust control barrier function which renders the system in a safe set when training the RL policy. In the proposed method, a time-dependent policy for STL tasks in continuous state space is achieved. We demonstrate that this approach both ensures safety and guides exploration effectively during training by several robot motion case studies.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240080","Signal temporal logic;Reinforcement learning;Robot Motion","Training;Robust control;Robot motion;Heuristic algorithms;Semantics;Reinforcement learning;Continuous time systems","continuous systems;learning (artificial intelligence);mobile robots;optimal control;reinforcement learning;robust control;temporal logic","optimal controller;robust control barrier function;safe controller;safe reinforcement learning;signal temporal logic tasks;STL tasks","","","","17","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Knowledge transfer in multi-agent reinforcement learning with incremental number of agents","W. Liu; L. Dong; J. Liu; C. Sun","School of Automation, Southeast University, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China","Journal of Systems Engineering and Electronics","13 May 2022","2022","33","2","447","460","In this paper, the reinforcement learning method for cooperative multi-agent systems (MAS) with incremental number of agents is studied. The existing multi-agent reinforcement learning approaches deal with the MAS with a specific number of agents, and can learn well-performed policies. However, if there is an increasing number of agents, the previously learned in may not perform well in the current scenario. The new agents need to learn from scratch to find optimal policies with others, which may slow down the learning speed of the whole team. To solve that problem, in this paper, we propose a new algorithm to take full advantage of the historical knowledge which was learned before, and transfer it from the previous agents to the new agents. Since the previous agents have been trained well in the source environment, they are treated as teacher agents in the target environment. Correspondingly, the new agents are called student agents. To enable the student agents to learn from the teacher agents, we first modify the input nodes of the networks for teacher agents to adapt to the current environment. Then, the teacher agents take the observations of the student agents as input, and output the advised actions and values as supervising information. Finally, the student agents combine the reward from the environment and the supervising information from the teacher agents, and learn the optimal policies with modified loss functions. By taking full advantage of the knowledge of teacher agents, the search space for the student agents will be reduced significantly, which can accelerate the learning speed of the holistic system. The proposed algorithm is verified in some multi-agent simulation environments, and its efficiency has been demonstrated by the experiment results.","1004-4132","","10.23919/JSEE.2022.000045","National Natural Science Foundation of China(grant numbers:62173251,61921004,U1713209); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9775069","knowledge transfer;multi-agent reinforcement learning (MARL);new agents","Knowledge engineering;Accelerated aging;Simulation;Reinforcement learning;Task analysis;Knowledge transfer;Multi-agent systems","multi-agent systems;reinforcement learning","student agents;teacher agents;incremental number;multiagent systems;multiagent reinforcement learning approaches;MAS;supervising information;optimal policies","","","","41","","13 May 2022","","","BIAI","BIAI Journals"
"Confidence estimation transformer for long-term renewable energy forecasting in reinforcement learning-based power grid dispatching","X. Li; N. Yang; Z. Li; Y. Huang; Z. Yuan; X. Song; L. Li; L. Zhang","Beijing University of Posts and Telecommunications, Beijing 100876, China; Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology (China Electric Power Research Institute), Beijing 100192, China; Beijing University of Posts and Telecommunications, Beijing 100876, China; Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology (China Electric Power Research Institute), Beijing 100192, China; Beijing University of Posts and Telecommunications, Beijing 100876, China; Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology (China Electric Power Research Institute), Beijing 100192, China; Beijing University of Posts and Telecommunications, Beijing 100876, China; Beijing University of Posts and Telecommunications, Beijing 100876, China","CSEE Journal of Power and Energy Systems","","2022","PP","99","1","12","The expansion of renewable energy could help realize the goals of peaking carbon dioxide emissions and carbon neutralization. Some existing grid dispatching methods integrating short-term renewable energy prediction and reinforcementlearning(RL)havebeenprovedtoalleviatetheadverse impact of energy fluctuations risk. However, these methods omit the long-term output prediction, which leads to stability and security problems on the optimal power flow. This paper proposes a confidence estimation Transformer for long-term renewable energy forecasting in reinforcement learning-based power grid dispatching (Conformer-RLpatching). ConformerRLpatching predicts long-term active output of each renewable energy generator with an enhanced Transformer to ensure the stable operation of the hybrid energy grid and improve the utilization rate of renewable energy, thus boosting the dispatching performance. Furthermore, a confidence estimation method is proposed to reduce the prediction error of renewable energy. Meanwhile, a dispatching necessity evaluation mechanism is put forward to decide whether the active output of a generator needs to be adjusted. Experiments carried out on the SG-126 power grid simulator show that Conformer-RLpatching achieves great improvement over the second best algorithm DDPG in security score by 25.8% and achieves a better total reward compared with the golden medal team in the power grid dispatching competition sponsored by State Grid Corporation of China under the same simulation environment. Codes are outsourced in https://github.com/BUPT-ANTlab/Conformer-RLpatching.","2096-0042","","10.17775/CSEEJPES.2022.02050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9979735","optimal power flow;reinforcement learning;renewable energy prediction;Conformer-RLpatching","Dispatching;Renewable energy sources;Transformers;Power grids;Estimation;Forecasting;Prediction algorithms","","","","","","","","9 Dec 2022","","","CSEE","CSEE Early Access Articles"
"Multi-Agent Deep Reinforcement Learning for Dynamic Avatar Migration in AIoT-Enabled Vehicular Metaverses With Trajectory Prediction","J. Chen; J. Kang; M. Xu; Z. Xiong; D. Niyato; C. Chen; A. Jamalipour; S. Xie","School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Nanyang Technological University, Singapore; School of Singapore University of Technology and Design, Singapore; School of Nanyang Technological University, Singapore; School of Sun Yat-sen University, Guangzhou, China; School of Electrical and Information Engineering, The University of Sydney, Sydney, Australia; School of Automation, Guangdong University of Technology, Guangzhou, China","IEEE Internet of Things Journal","","2023","PP","99","1","1","Avatars, as promising digital assistants in Vehicular Metaverses, can enable drivers and passengers to immerse in 3D virtual spaces, serving as a practical emerging example of Artificial Intelligence of Things (AIoT) in intelligent vehicular environments. The immersive experience is achieved through seamless human-avatar interaction, e.g., augmented reality navigation, which requires intensive resources that are inefficient and impractical to process on intelligent vehicles locally. Fortunately, offloading avatar tasks to RoadSide Units (RSUs) or cloud servers for remote execution can effectively reduce resource consumption. However, the high mobility of vehicles, the dynamic workload of RSUs, and the heterogeneity of RSUs pose novel challenges to making avatar migration decisions. To address these challenges, in this paper, we propose a dynamic migration framework for avatar tasks based on real-time trajectory prediction and Multi-Agent Deep Reinforcement Learning (MADRL). Specifically, we propose a model to predict the future trajectories of intelligent vehicles based on their historical data, indicating the future workloads of RSUs.Based on the expected workloads of RSUs, we formulate the avatar task migration problem as a long-term mixed integer programming problem. To tackle this problem efficiently, the problem is transformed into a Partially Observable Markov Decision Process (POMDP) and solved by multiple DRL agents with hybrid continuous and discrete actions in decentralized. Numerical results demonstrate that our proposed algorithm can effectively reduce the latency of executing avatar tasks by around 25% without prediction and 30% with prediction and enhance user immersive experiences in the AIoT-enabled Vehicular Metaverse (AeVeM).","2327-4662","","10.1109/JIOT.2023.3296075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10185562","Metaverses;avatar;service migration;trajectory prediction;multi-agent deep reinforcement learning;Artificial Intelligence of Things","Avatars;Task analysis;Intelligent vehicles;Trajectory;Vehicle dynamics;Internet of Things;Predictive models","","","","","","","IEEE","18 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Implicit Posteriori Parameter Distribution Optimization in Reinforcement Learning","T. Li; G. Yang; J. Chu","Ningbo Artificial Intelligence Institute and the Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Ningbo Artificial Intelligence Institute and the Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Ningbo Artificial Intelligence Institute and the Department of Automation, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Cybernetics","","2023","PP","99","1","14","Efficient and intelligent exploration remains a major challenge in the field of deep reinforcement learning (DRL). Bayesian inference with a distributional representation is usually an effective way to improve the exploration ability of the RL agent. However, when optimizing Bayesian neural networks (BNNs), most algorithms need to specify an explicit parameter distribution such as a multivariate Gaussian distribution. This may reduce the flexibility of model representation and affect the algorithm performance. Therefore, to improve sample efficiency and exploration based on Bayesian methods, we propose a novel implicit posteriori parameter distribution optimization (IPPDO) algorithm. First, we adopt a distributional perspective on the parameter and model it with an implicit distribution, which is approximated by generative models. Each model corresponds to a learned latent space, providing structured stochasticity for each layer in the network. Next, to make it possible to optimize an implicit posteriori parameter distribution, we build an energy-based model (EBM) with value function to represent the implicit distribution which is not constrained by any analytic density function. Then, we design a training algorithm based on amortized Stein variational gradient descent (SVGD) to improve the model learning efficiency. We compare IPPDO with other prevailing DRL algorithms on the OpenAI Gym, MuJoCo, and Box2D platforms. Experiments on various tasks demonstrate that the proposed algorithm can represent the parameter uncertainty implicitly for a learned policy and can consistently outperform competing approaches.","2168-2275","","10.1109/TCYB.2023.3254596","China National Research and Development Key Research Program(grant numbers:2020YFB1711204,2019YFB1705700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10078231","Bayesian inference;exploration;parameter distribution;reinforcement learning (RL)","Bayes methods;Uncertainty;Artificial neural networks;Task analysis;Mathematical models;Inference algorithms;Generators","","","","","","","IEEE","22 Mar 2023","","","IEEE","IEEE Early Access Articles"
"Energy Management of Networked Microgrids with Real-Time Pricing by Reinforcement Learning","G. Cui; Q. -S. Jia; X. Guan","CFINS, Department of Automation, BNRist, Tsinghua University, Beijing, China; CFINS, Department of Automation, BNRist, Tsinghua University, Beijing, China; CFINS, Department of Automation, BNRist, Tsinghua University, Beijing, China","IEEE Transactions on Smart Grid","","2023","PP","99","1","1","Coordinating the microgrids (MGs) in the distribution network is a critical task for the distribution system operator (DSO), which could be achieved by setting prices as incentive signals. The high uncertainty of loads and renewable resources motivates the DSO to adopt real-time prices. The MGs require reference price sequences for a long time horizon in advance to make generation plans. However, due to privacy concerns in practice, the MGs may not provide adequate information for the DSO to build a closed-form model. This causes challenges to the implementation of the conventional model-based methods. In this paper, the framework of the coordination system through real-time prices is proposed. In this bi-level framework, the DSO sets real-time reference price sequences as the incentive signals, based on which the MGs make the generation and charging plan. The model-free reinforcement learning (RL) is applied to optimize the pricing policy when the response behavior of the MGs is unknown to the DSO. To deal with the large action space of this problem, the reference policy is incorporated into the RL algorithm for efficiency improvement. The numerical result shows that the minimized cost obtained by the developed model-free RL algorithm is close to the model-based method while the private information is preserved.","1949-3061","","10.1109/TSG.2023.3281935","National Natural Science Foundation of China(grant numbers:62073182,62125304,62192751); 111 International Collaboration Project(grant numbers:BP2018006); National Key Research and Development Program of China(grant numbers:2022YFA1004600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10142166","Reinforcement learning;Microgrids;Distribution network;Energy management","Pricing;Real-time systems;Optimization;Generators;Task analysis;Load modeling;Energy management","","","","","","","CCBY","1 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Leveraging Joint-action Embedding in Multi-agent Reinforcement Learning for Cooperative Games","X. Lou; J. Zhang; Y. Du; C. Yu; Z. He; K. Huang","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Department of Informatics, King's College London, England; School of Data and Computer Science, Sun Yat-sen University, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Games","","2023","PP","99","1","14","State-of-the-art multi-agent policy gradient (MAPG) methods have demonstrated convincing capability in many cooperative games. However, the exponentially growing joint-action space severely challenges the critic's value evaluation and hinders performance of MAPG methods. To address this issue, we augment Central-Q policy gradient with a joint-action embedding function and propose Mutual-information Maximization MAPG (M3APG). The joint-action embedding function makes joint-actions contain information of state transitions, which will improve the critic's generalization over the joint-action space by allowing it to infer joint-actions' outcomes. We theoretically prove that with a fixed joint-action embedding function, the convergence of M3APG is guaranteed. Experiment results on the StarCraft Multi-Agent Challenge (SMAC) demonstrate that M3APG gives evaluation results with better accuracy and outperform other MAPG basic models across various maps of multiple difficulty levels. We empirically show that our joint-action embedding model can be extended to value-based multi-agent reinforcement learning methods and state-of-the-art MAPG methods. Finally, we run ablation study to show that the usage of mutual information in our method is necessary and effective.","2475-1510","","10.1109/TG.2023.3302694","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10210002","Multi-agent;Reinforcement Learning;Policy Gradient;Joint-action Embedding","Games;Predictive models;Reinforcement learning;Convergence;Task analysis;Correlation;Training","","","","","","","IEEE","7 Aug 2023","","","IEEE","IEEE Early Access Articles"
"A Cooperation-Free Resource Allocation Algorithm Enhanced by Reinforcement Learning for Coexisting IIoTs","J. Zhang; W. Liang; B. Yang; H. Shi; Q. Wang; Z. Pang","Chinese Academy of Sciences, Shenyang Institute of Automation, Shenyang, China; Chinese Academy of Sciences, Shenyang Institute of Automation, Shenyang, China; College of Information Engineering, Northwest A&F University, Yangling, China; School of Artificial Intelligence, Henan University, Zhengzhou, China; Chinese Academy of Sciences, Shenyang Institute of Automation, Shenyang, China; ABB, Corporate Research, Västerås, Sweden","2023 IEEE 19th International Conference on Factory Communication Systems (WFCS)","7 Jun 2023","2023","","","1","7","The Industrial Internet of Things (IIoTs) plays an important role in various industrial applications, which require multiple time-critical networks to be deployed in the same region. The limited communication resources inevitably incur network coexistence problems. For scenarios where coexisting networks cannot coordinate effectively, the centralized or partial-information-based decentralized resource allocation methods cannot be implemented. To address this concern, we propose a Cooperation-Free Reinforcement Learning (CF-RL) algorithm for the fully distributed resource allocation problem in coexisting IIoT systems. Each network adopts the proposed algorithm to minimize collisions through a trial-and-error approach without any information interaction. To resist the influence of environmental dynamics, each coexisting network learns the state transition probability of the resource block instead of the resource block's position. Moreover, to potentially ensure the overall system performance, each network additionally considers the period offset in the initialization phase and action selection phase, so that the coexisting networks have different preferences for different state transitions. We conduct extensive simulations to verify the convergence performance. Evaluation results show that the CF-RL algorithm almost achieves (more than 99.88%) the effect of centralized resource allocation and has obvious superiorities over other cooperation-free algorithms in terms of the convergence rate, the number of collisions, and the resource utilization ratio.","2835-8414","978-1-6654-6432-1","10.1109/WFCS57264.2023.10144246","Chinese Academy of Sciences(grant numbers:XDC02020600); National key research and development program(grant numbers:2021YFB3301001); Program for Science & Technology Development of Henan Province, China(grant numbers:222102210022); China Scholarship Council; Swedish Foundation for Strategic Research (SSF)(grant numbers:APR20-0023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10144246","Industrial Internet of Things (IIoTs);heuristic-based reinforcement learning;cooperation-free coexistence resource allocation","Wireless communication;Schedules;Heuristic algorithms;System performance;Reinforcement learning;Resists;Resource management","cooperative communication;Internet of Things;learning (artificial intelligence);reinforcement learning;resource allocation","centralized information-based decentralized resource allocation methods;centralized resource allocation;CF-RL algorithm;coexisting IIoT systems;coexisting IIoTs;coexisting network;communication resources;cooperation-free algorithms;Cooperation-Free Reinforcement;Cooperation-Free resource allocation algorithm enhanced;fully distributed resource allocation problem;industrial applications;multiple time-critical networks;network coexistence problems;partial-information-based decentralized resource allocation methods;Reinforcement Learning;resource block;resource utilization ratio","","","","18","IEEE","7 Jun 2023","","","IEEE","IEEE Conferences"
"Dynamic Event-Triggered Robust Optimal Attitude Control of QUAV Using Reinforcement Learning","P. Jin; Q. Ma; S. Xu","School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Aerospace and Electronic Systems","","2023","PP","99","1","10","In this article, a dynamic event-triggered robust optimal attitude tracking control problem for quadrotor unmanned aerial vehicle in uncertain environment is investigated. First, an augmented system consisting of tracking error signal and reference signal is developed to transform the tracking problem into a stabilization problem. Then, in order to handle the random disturbances in the design of the optimal controller, a new Hamilton-Jacobi-Bellman equation is proposed. Subsequently, the dynamic event-triggered mechanism is designed to reduce the communication pressure, and an event-based critic-only reinforcement learning algorithm is proposed to implement the optimal controller design. Remarkably, by combining concurrent learning technique and gradient descent algorithm, the adaptive weight update law is derived to tune the critic neural network, thus erasing the demand on the persistent excitation condition. After that, we demonstrate that the closed-loop system is semi-globally uniformly ultimately bounded in mean square and prove the zeno-free behavior. Finally, the simulation results are given to show the effectiveness of our control strategy.","1557-9603","","10.1109/TAES.2023.3294893","NSFC(grant numbers:62173183); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10182298","Attitude control;dynamic event-triggered;quadrotor unmanned aerial vehicle;reinforcement learning","Optimal control;Attitude control;Vehicle dynamics;Aerodynamics;Trajectory;Rotors;Reinforcement learning","","","","","","","IEEE","13 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Data-Based Optimal Synchronization of Heterogeneous Multiagent Systems in Graphical Games via Reinforcement Learning","C. Xiong; Q. Ma; J. Guo; F. L. Lewis","School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Nanjing University of Science and Technology, Nanjing, China; UTA Research Institute, University of Texas at Arlington, Fort Worth, TX, USA","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","9","This article studies the optimal synchronization of linear heterogeneous multiagent systems (MASs) with partial unknown knowledge of the system dynamics. The object is to realize system synchronization as well as minimize the performance index of each agent. A framework of heterogeneous multiagent graphical games is formulated first. In the graphical games, it is proved that the optimal control policy relying on the solution of the Hamilton–Jacobian–Bellmen (HJB) equation is not only in Nash equilibrium, but also the best response to fixed control policies of its neighbors. To solve the optimal control policy and the minimum value of the performance index, a model-based policy iteration (PI) algorithm is proposed. Then, according to the model-based algorithm, a data-based off-policy integral reinforcement learning (IRL) algorithm is put forward to handle the partially unknown system dynamics. Furthermore, a single-critic neural network (NN) structure is used to implement the data-based algorithm. Based on the data collected by the behavior policy of the data-based off-policy algorithm, the gradient descent method is used to train NNs to approach the ideal weights. In addition, it is proved that all the proposed algorithms are convergent, and the weight-tuning law of the single-critic NNs can promote optimal synchronization. Finally, a numerical example is proposed to show the effectiveness of the theoretical analysis.","2162-2388","","10.1109/TNNLS.2023.3291542","NSFC(grant numbers:62173183,62073169,62221004); Postgraduate Research and Practice Innovation Program of Jiangsu Province(grant numbers:KYCX22_0447); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10186232","Graphical games;heterogeneous MASs;Nash equilibrium;optimal synchronization;reinforcement learning (RL)","Games;Synchronization;Heuristic algorithms;System dynamics;Optimal control;Artificial neural networks;Performance analysis","","","","","","","IEEE","18 Jul 2023","","","IEEE","IEEE Early Access Articles"
"pTLC: Personalized Traffic Light Control Based on Deep Reinforcement Learning Approach","Z. Pan; X. Pan; Y. Zhan; Y. Xia","Yangtze Delta Region Academy of Beijing Institute of Technology, Jiaxing, China; CATARC Intelligent and connected technology CO., LTd., Tianjin, China; Yangtze Delta Region Academy of Beijing Institute of Technology, Jiaxing, China; School of Automation, Beijing Institute of Technology, Beijing, China","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","6580","6585","Traffic signal control is a critical aspect of improving urban traffic efficiency and reducing road accidents. Reinforcement learning (RL) has been applied to solve the intersection traffic control problem, which is flexible and can be adapted to dynamic traffic conditions. However, most existing RL methods only consider a single vehicle type, ignoring the impact of multiple vehicle types on traffic and safety. To address this gap, we propose a new RL method called pTLC, which aims to allow trucks to cross intersections quickly while minimizing the impact on other vehicles. In this method, vehicle type is added as a state variable to capture the influence of different vehicle types on traffic flow. At the same time, a new state representation method is proposed that divides the road into several road sections to balance the environmental information and the size of the state space. We design simulation experiments under different traffic demand scenarios, including cars and trucks, to verify the effectiveness of the method. The results show significant improvements in traffic efficiency and safety, especially for mixed vehicle types.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240702","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240702","Personalized Traffic Control;Traffic Signal;Deep Reinforcement Learning","Space vehicles;Road accidents;Roads;Transportation;Reinforcement learning;Aerospace electronics;Traffic control","deep learning (artificial intelligence);reinforcement learning;road accidents;road safety;road traffic;road traffic control;road vehicles;traffic control;traffic engineering computing","deep reinforcement learning approach;different traffic demand scenarios;different vehicle types;dynamic traffic conditions;existing RL methods;intersection traffic control problem;mixed vehicle types;multiple vehicle types;personalized traffic light control;pTLC;reducing road accidents;RL method;road sections;safety;single vehicle type;state representation method;state space;traffic flow;traffic signal control;trucks;urban traffic efficiency","","","","17","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Character Behavior Automation Using Deep Reinforcement Learning","H. Lee; M. K. Dahouda; I. Joe","Department of Computer Science, Hanyang University, Seoul, South Korea; Department of Computer Science, Hanyang University, Seoul, South Korea; Department of Computer Science, Hanyang University, Seoul, South Korea","IEEE Access","22 Sep 2023","2023","11","","101435","101442","Recently, various new attempts are being made to improve the quality of media content according to the expansion of the media market. Pre-visualization is one of those attempts, and the behavior of characters (agents) in virtual space is essential for pre-visualization. In this paper, a study was conducted to automatically generate behaviors of virtual characters for more efficient visualization in pre-visualization. In particular, we propose a method to automatically produce an appropriate behavior by detecting the state of the surrounding environment with a deep reinforcement learning technique. A virtual environment is created using a game engine to configure space for reinforcement learning, and a reinforcement learning model of the training environment is configured with Python and PyTorch. The virtual environment and the model training environment are communicated with the ML-agents toolkit. In the virtual environment, the character basically moves in a straight line, and three obstacles appear at random locations in front of the character. The character senses 9 states and allows 5 actions. After that, a reward is offered according to the action to proceed with learning. For performance evaluation, reinforcement learning training was conducted using the Proximal Policy Optimization (PPO) algorithm and Soft Actor-Critic (SAC) algorithm, and performance comparisons were also conducted according to the batch size. As a result, we are able to secure a reinforcement learning model with obstacle avoidance capability. Applying the model to the character proved that the character can automatically animate according to the state of the surrounding environment without explicit programming.","2169-3536","","10.1109/ACCESS.2023.3313737","Institute of Information Communications Technology Planning Evaluation (IITP); Korean Government [Ministry of Science and ICT (MSIT)] (Development of the Technology to Automate the Recommendations for Big Data Analytic Models that Define Data Characteristics and Problems)(grant numbers:2020-0-00107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10246267","Pre-visualization;deep reinforcement learning;behavior","Reinforcement learning;Behavioral sciences;Media;Training;Games;Deep learning;Virtual environments","collision avoidance;computer animation;computer games;deep learning (artificial intelligence);learning (artificial intelligence);optimisation;Python;reinforcement learning","appropriate behavior;character behavior;character senses 9 states;deep reinforcement learning technique;efficient visualization;media market;ML-agents toolkit;model training environment;pre-visualization;reinforcement learning model;reinforcement learning training;virtual characters;virtual environment;virtual space","","","","19","CCBYNCND","11 Sep 2023","","","IEEE","IEEE Journals"
"Knowledge-Based Reinforcement Learning and Estimation of Distribution Algorithm for Flexible Job Shop Scheduling Problem","Y. Du; J. -q. Li; X. -l. Chen; P. -y. Duan; Q. -k. Pan","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Mathematics and Information Sciences, Yantai University, Yantai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China","IEEE Transactions on Emerging Topics in Computational Intelligence","21 Jul 2023","2023","7","4","1036","1050","Inthis study, a flexible job shop scheduling problem with time-of-use electricity price constraint is considered. The problem includes machine processing speed, setup time, idle time, and the transportation time between machines. Both maximum completion time and total electricity price are optimized simultaneously. A hybrid multi-objective optimization algorithm of estimation of distribution algorithm and deep Q-network is proposed to solve this. The processing sequence, machine assignment, and processing speed assignment are all described using a three-dimensional solution representation. Two knowledge-based initialization strategies are designed for better performance. In the estimation of distribution algorithm component, three probability matrices corresponding to solution representation are provided. In the deep Q-network component, 34 state features are selected to describe the scheduling situation, while nine knowledge-based actions are defined to refine the scheduling solution, and the reward based on the two objectives is designed. As the knowledge for initialization and optimization strategies, five properties of the considered problem are proposed. The proposed mixed integer linear programming model of the problem is validated by exact solver CPLEX. The results of the numerical testing on wide-range scale instances show that the proposed hybrid algorithm is efficient and effective at solving the integrated flexible job shop scheduling problem.","2471-285X","","10.1109/TETCI.2022.3145706","National Natural Science Foundation of China(grant numbers:62173216,61773192,61803192); Natural Science Foundation of Shandong Province(grant numbers:ZR2018ZB0419); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9707606","Flexible job shop scheduling problem;deep reinforcement learning;estimation of distribution algorithm;multi-objective optimization","Transportation;Optimization;Job shop scheduling;Standards;Knowledge based systems;Indexes;Estimation","deep learning (artificial intelligence);distributed algorithms;integer programming;job shop scheduling;pricing;probability;production engineering computing","deep Q-network component;distribution algorithm component;hybrid multiobjective optimization algorithm;integrated flexible job shop scheduling problem;knowledge-based actions;knowledge-based initialization strategies;knowledge-based reinforcement learning;machine assignment;machine processing speed;mixed integer linear programming model;optimization strategies;probability matrices corresponding;processing sequence;setup time;solver CPLEX;speed assignment;three-dimensional solution representation;time-of-use electricity price constraint;transportation time","","29","","56","IEEE","8 Feb 2022","","","IEEE","IEEE Journals"
"Battery Scheduling in a Residential Multi-Carrier Energy System Using Reinforcement Learning","B. V. Mbuwir; M. Kaffash; G. Deconinck","ELECTA-ESAT, KU Leuven, Belgium; ELECTA-ESAT, KU Leuven, Belgium; ELECTA-ESAT, KU Leuven, Belgium","2018 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)","27 Dec 2018","2018","","","1","6","Motivated by the recent developments in machine learning and artificial intelligence, this work contributes to the application of reinforcement learning in Multi-Carrier Energy Systems (MCESs) to provide flexibility at the residential level. The work addresses the problem of providing flexibility through the operation of a storage device, and flexibility of supply by considering several infrastructures to meet the residential thermal and electrical demand in a MCES with a photovoltaic (PV) installation. The problem of providing flexibility using a battery is formulated as a sequential decision making problem under uncertainty where, at every time step, the uncertainty is due to the lack of knowledge about future electricity demand and weather dependent PV production. This paper proposes to address this problem using fitted Q-iteration, a batch Reinforcement Learning (RL) algorithm. The proposed method is tested using data from a typical Belgian residential household. Simulation results show that, an optimal interaction of the different energy carriers in the system can be obtained using RL and without providing a detailed model of the MCES.","","978-1-5386-7954-8","10.1109/SmartGridComm.2018.8587412","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8587412","flexibility;fitted Q-iteration;reinforcement learning.","Batteries;Production;Resistance heating;Heat pumps;Uncertainty;Smart grids","battery storage plants;buildings (structures);decision making;demand side management;electric heating;hybrid power systems;iterative methods;learning (artificial intelligence);photovoltaic power systems;power grids;secondary cells","PV production;fitted Q-iteration;Belgian residential household;energy carriers;electricity demand;residential MultiCarrier Energy system;battery scheduling;batch Reinforcement;sequential decision making problem;photovoltaic installation;MCES;electrical demand;residential thermal demand;storage device;reinforcement learning;artificial intelligence;machine learning","","13","","25","IEEE","27 Dec 2018","","","IEEE","IEEE Conferences"
"Grid Differentiated Services: A Reinforcement Learning Approach","J. Perez; C. Germain-Renaud; B. Kégl; C. Loomis","Laboratoire de Recherche en Informatique, CNRS and Université Paris-Sud, France; Laboratoire de l'Accélérateur Linéaire, CNRS and Université Paris-Sud, France; Laboratoire de Recherche en Informatique, CNRS and Université Paris-Sud, France; Laboratoire de l'Accélérateur Linéaire, CNRS and Université Paris-Sud, France","2008 Eighth IEEE International Symposium on Cluster Computing and the Grid (CCGRID)","30 May 2008","2008","","","287","294","Large scale production grids are a major case for autonomic computing. Following the classical definition of Kephart, an autonomic computing system should optimize its own behavior in accordance with high level guidance from humans. This central tenet of this paper is that the combination of utility functions and reinforcement learning (RL) can provide a general and efficient method for dynamically allocating grid resources in order to optimize the satisfaction of both end-users and participating institutions. The flexibility of an RL-based system allows to model the state of the grid,the jobs to be scheduled, and the high-level objectives of the various actors on the grid. RL-based scheduling can seamlessly adapt its decisions to changes in the distributions ofinter-arrival time, QoS requirements, and resource availability. Moreover, it requires minimal prior knowledge about thetarget environment, including user requests and infrastructure. Our experimental results, both on a synthetic workloadand a real trace, show that RL is not only a realistic alternative to empirical scheduler design, but is able to outperform them.","","978-0-7695-3156-4","10.1109/CCGRID.2008.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4534230","","Learning;Grid computing;Job shop scheduling;Processor scheduling;Quality of service;Large-scale systems;Production;Humans;Resource management;Optimization methods","grid computing;learning (artificial intelligence);quality of service","grid differentiated services;large scale production grids;autonomic computing system;reinforcement learning;grid resources;QoS requirements","","5","","11","IEEE","30 May 2008","","","IEEE","IEEE Conferences"
"Reinforcement learning with classifier systems","R. E. Smith; D. E. Goldberg","Department of Engineering Mechanics, University of Alabama, Tuscaloosa, AL, USA; Department of Engineering Mechanics, University of Alabama, Tuscaloosa, AL, USA","Proceedings [1990]. AI, Simulation and Planning in High Autonomy Systems","6 Aug 2002","1990","","","184","192","Consideration is given to the learning classifier system (LCS) as an approach to reinforcement learning problems. An LCS is a type of adaptive expert system that uses a knowledge base of production rules in a low-level syntax that can be manipulated by a genetic algorithm (GA). GAs are a class of computerized search procedures that are based on the mechanics of natural genetics. An important feature of the LCS paradigm is the possible adaptive formation of default hierarchies (layered sets of default and exception rules). An examination is made of the problem of default hierarchy formation under the conventional bid competition method of LCS conflict resolution and the necessity auction and a separate priority factor are suggested as modifications to this method. Simulations show the utility of this method.<>","","0-8186-2043-9","10.1109/AIHAS.1990.93934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=93934","","Learning;Chromium;Production systems;Genetic algorithms;Adaptive systems;Expert systems;Knowledge based systems;Control systems;Robustness;Feedback","adaptive systems;expert systems;genetic algorithms;learning systems;search problems","learning classifier system;reinforcement learning problems;adaptive expert system;knowledge base;production rules;low-level syntax;genetic algorithm;computerized search procedures;natural genetics;LCS paradigm;adaptive formation;default hierarchies;layered sets;exception rules;default hierarchy formation;conventional bid competition method;LCS conflict resolution;separate priority factor","","3","","21","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Dynamic Feature Selection for Solar Irradiance Forecasting Based on Deep Reinforcement Learning","C. Lyu; S. Eftekharnejad; S. Basumallik; C. Xu","Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, USA; Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, USA; Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA; Department of Electrical and Computer Engineering, University of Washington, Seattle, WA, USA","IEEE Transactions on Industry Applications","19 Jan 2023","2023","59","1","533","543","A large volume of data is typically needed to achieve an accurate solar generation prediction. However, not all types of data are consistently available. Various research efforts have addressed this challenge by developing methods that identify the most relevant features for predicting solar generation. However, the optimal features vary with different weather patterns, making it impossible to select a fixed set of optimal features for all weather patterns. This study develops a new framework to accurately predict solar irradiance using dynamically changing optimal features. The developed model first incorporates feature extraction with clustering techniques to identify representative weather data from a dataset. Next, using deep reinforcement learning (DRL), a new feature selection method is developed to yield the minimum features required to accurately forecast solar irradiance from representative data. Benefiting from the model-free nature of DRL, the developed method is adaptive to various weather conditions, and dynamically alters the selected features. Case studies using real-world data have shown that the developed model significantly reduces the volume of data required for accurate irradiance forecasting for different weather patterns.","1939-9367","","10.1109/TIA.2022.3206731","National Science Foundation(grant numbers:2144918); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893318","Data analytics;data clustering;deep reinforcement learning;feature extraction;solar generation forecast","Predictive models;Forecasting;Feature extraction;Meteorology;Weather forecasting;Data models;Computational modeling","deep learning (artificial intelligence);feature extraction;feature selection;image representation;pattern clustering;power engineering computing;reinforcement learning;solar power;sunlight;weather forecasting","accurate solar generation prediction;deep reinforcement learning;dynamic feature selection;feature extraction;feature selection method;optimal features;real-world data;representative data;representative weather data;solar irradiance forecasting;weather conditions;weather patterns","","2","","50","IEEE","15 Sep 2022","","","IEEE","IEEE Journals"
"A Reinforcement-Learning Approach to Failure-Detection Scheduling","F. Zeng","BEA Systems, Inc., Liberty Corner, NJ, USA","Seventh International Conference on Quality Software (QSIC 2007)","21 Nov 2007","2007","","","161","170","A failure-detection scheduler for an online production system must strike a tradeoff between performance and reliability. If failure-detection processes are run too frequently, valuable system resources are spent checking and rechecking for failures. However, if failure-detection processes are run too rarely, a failure can remain undetected for a long time. In both cases, system performability suffers. We present a model-based learning approach that estimates the failure rate and then performs an optimization to find the tradeoff that maximizes system performability. We show that our approach is not only theoretically sound but practically effective, and we demonstrate its use in an implemented automated deadlock-detection system for Java.","2332-662X","978-0-7695-3035-2","10.1109/QSIC.2007.4385492","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385492","","Costs;Scheduling;Frequency;Exponential distribution;System recovery;Java;System performance;Upper bound;Production systems;Convergence","decision theory;learning (artificial intelligence);scheduling;software reliability;system recovery","reinforcement learning;failure detection scheduling;online production system;optimization;automated deadlock detection;Java;decision theory;software reliability","","1","3","26","IEEE","21 Nov 2007","","","IEEE","IEEE Conferences"
"Service Restoration Using Deep Reinforcement Learning and Dynamic Microgrid Formation in Distribution Networks","M. A. Igder; X. Liang","Department of Electrical and Computer Engineering, University of Saskatchewan, Saskatoon, SK, Canada; Department of Electrical and Computer Engineering, University of Saskatchewan, Saskatoon, SK, Canada","IEEE Transactions on Industry Applications","19 Sep 2023","2023","59","5","5453","5472","A resilient power distribution network can reduce length and impact of power outages, maintain continuous services, and improve reliability. One effective way to enhance the system's resilience is to form microgrids during outages. In this article, a novel dynamic microgrid formation-based service restoration method using deep reinforcement learning is proposed, and it is treated as a Markov decision process (MDP) while taking operational and structural limitations of microgrids into account. The deep Q-network is employed to obtain optimal control strategies for microgrid formation. We have introduced a new way for the agent to choose actions when building a microgrid using the deep Q-learning method, which ensures that the microgrid has a feasible radial structure. The proposed service restoration method enables real-time computing to facilitate online formation of dynamic microgrids and adapts to changing conditions. The influence of optimal switch placement on service restoration using proposed method is also investigated. The effectiveness of proposed service restoration method is validated by case studies using the modified IEEE 33-node test system and a real 404-node distribution system operated by Saskatoon Light and Power in Saskatoon, Canada.","1939-9367","","10.1109/TIA.2023.3287944","University of Saskatchewan; City of Saskatoon; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10158030","Deep Q-learning;deep reinforcement learning;distribution network;microgrid formation;service restoration","Microgrids;Distribution networks;Deep learning;Voltage;Power system dynamics;Load modeling;Topology","deep learning (artificial intelligence);distributed power generation;Markov processes;optimal control;power distribution control;power distribution reliability;power engineering computing;power generation control;power generation reliability;power system restoration;reinforcement learning","404-node distribution system;Canada;deep Q-learning method;deep Q-network;deep reinforcement learning;dynamic microgrid formation-based service restoration method;IEEE 33-node test system;Markov decision process;MDP;online formation;operational limitations;optimal control;optimal switch placement;power outages;radial structure;real-time computing;resilient power distribution network;Saskatoon;structural limitations;systems resilience","","1","","32","IEEE","20 Jun 2023","","","IEEE","IEEE Journals"
"Augmenting practical cross-layer MAC schedulers via offline reinforcement learning","F. Pianese; P. J. Danielsen",Nokia Bell Labs; Nokia Bell Labs,"2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","15 Feb 2018","2017","","","1","6","An automated offline design process for optimized cross-layer schedulers can produce augmented scheduling algorithms tailored to a target deployment scenario. We discuss the application of ODDS, a reinforcement learning technique we introduced, for augmenting LTE MAC scheduler algorithms of practical significance. ODDS observes the correlation between the value of a utility function and the cross-layer state parameters seen by an instrumented baseline scheduler, determining the best actions via an offline Monte Carlo exploration of the problem space. The result of the ODDS process is a compact definition of a scheduling policy that has been optimized for a target scenario and utility function. In this paper we instrument a production scheduler definition to evaluate the potential of augmented schedulers in practical use, and experiment with awareness to application traffic properties by using a multi-class utility function, yielding scheduling policies that behave differently depending on the features of individual traffic classes.","2166-9589","978-1-5386-3531-5","10.1109/PIMRC.2017.8292409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292409","","Optimization;Algorithm design and analysis;Training;Measurement;Production;Throughput;Scheduling algorithms","access protocols;learning (artificial intelligence);Long Term Evolution;Monte Carlo methods;telecommunication scheduling;telecommunication traffic","offline reinforcement learning;multiclass utility function;augmented schedulers;production scheduler definition;scheduling policy;ODDS process;offline Monte Carlo exploration;instrumented baseline scheduler;cross-layer state parameters;LTE MAC scheduler algorithms;reinforcement learning technique;target deployment scenario;augmented scheduling algorithms;optimized cross-layer schedulers;automated offline design process;practical cross-layer MAC schedulers","","","","13","IEEE","15 Feb 2018","","","IEEE","IEEE Conferences"
"CAPES: Unsupervised Storage Performance Tuning Using Neural Network-Based Deep Reinforcement Learning","Y. Li; K. Chang; O. Bel; E. L. Miller; D. D. E. Long","University of California, Santa Cruz; University of California, Santa Cruz; University of California, Santa Cruz; University of California, Santa Cruz Pure Storage; University of California, Santa Cruz","SC17: International Conference for High Performance Computing, Networking, Storage and Analysis","27 Oct 2022","2017","","","1","14","Parameter tuning is an important task of storage performance optimization. Current practice usually involves numerous tweak-benchmark cycles that are slow and costly. To address this issue, we developed CAPES, a model-less deep reinforcement learning-based unsupervised parameter tuning system driven by a deep neural network (DNN). It is designed to find the optimal values of tunable parameters in computer systems, from a simple client-server system to a large data center, where human tuning can be costly and often cannot achieve optimal performance. CAPES takes periodic measurements of a target computer system’s state, and trains a DNN which uses Q-learning to suggest changes to the system’s current parameter values. CAPES is minimally intrusive, and can be deployed into a production system to collect training data and suggest tuning actions during the system’s daily operation. Evaluation of a prototype on a Lustre file system demonstrates an increase in I/O throughput up to 45% at saturation point. CCS CONCEPTS • Information systems → Storage management; Distributed storage; • Computing methodologies →Neural networks;","2167-4337","978-1-4503-5114-0","","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926120","performance tuning;deep learning;q-learning","Production systems;Q-learning;Storage management;Neural networks;Training data;Prototypes;Throughput","client-server systems;deep learning (artificial intelligence);storage management;unsupervised learning","CAPES;model-less deep reinforcement learning-based unsupervised parameter tuning system;deep neural network;DNN;tunable parameters;computer systems;client-server system;human tuning;optimal performance;target computer system;Q-learning;production system;tuning actions;Lustre file system;Information systems;storage management;distributed storage;storage performance optimization;tweak-benchmark cycles;unsupervised storage performance tuning;neural network-based deep reinforcement","","","","33","","27 Oct 2022","","","IEEE","IEEE Conferences"
"Resilient Event Detection Algorithm for Non-intrusive Load Monitoring under Non-ideal Conditions using Reinforcement Learning","M. Etezadifar; H. Karimi; A. G. Aghdam; J. Mahseredjian","Polytechnique Montréal, Montreal, Quebec, Canada; Electrical Engineering and Computer Science, York University, Toronto, Ontario, Canada; Electrical Engineering and Computer Science, York University, Toronto, Ontario, Canada; Electrical Engineering, Ecole Polytechnique de Montreal, Montreal, Quebec, Canada","IEEE Transactions on Industry Applications","","2023","PP","99","1","10","Event detection is critical in a non-intrusive load monitoring (NILM) solution. NILM is essential for the imple-mentation of some demand-side management (DSM) techniques. This paper proposes an event detection algorithm based on reinforcement learning for NILM purposes (RLNILM). The proposed method employs a number of simpler traditional event detection algorithms, e.g., LLR voting, or SWDC, to train the RLNILM agent through a feedback system that separates the RLNILM agent from directly accessing consumers' data. The performance of the proposed RLNILM method is validated using the real-world data from the iAWE dataset under ideal and non-ideal conditions. Four test scenario groups include cases where the frequency, input electric signals, or access to crucial grid information varies significantly. In all scenarios, the RLNILM agent outperforms traditional event detection algorithms used in the feedback system. The results show not only the proposed architecture increases the cyber security of the customers con-nected to NILM services, but also it improves the performance of real-time event detection algorithms in non-ideal grid conditions.","1939-9367","","10.1109/TIA.2023.3307347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10226288","Non-intrusive load monitoring;reinforcement learning;smart grid;event detection;federated learning;machine learning;NILM","Event detection;Power grids;Machine learning algorithms;Heuristic algorithms;Load monitoring;Training;Real-time systems","","","","","","","IEEE","22 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Deep Reinforcement Learning-Based Spatiotemporal Decision of Utility-Scale Highway Portable Energy Storage Systems","Y. Ding; G. Qu; X. Chen; J. Wang; J. Song; G. He","Department of Industrial Engineering and Management, Peking University, Beijing, China; Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Industrial Engineering and Management, Peking University, Beijing, China; National Engineering Laboratory for Big Data Analysis and Applications, Peking University, Beijing, China; Department of Industrial Engineering and Management, Peking University, Beijing, China; Department of Industrial Engineering and Management, Peking University, Beijing, China","IEEE Transactions on Industry Applications","","2023","PP","99","1","10","Mobile charging is an efficient solution to meet peak charging demand on highways. In this paper we propose a deep reinforcement learning (DRL)-based approach to maximize the revenue of a utility-scale highway portable energy storage system (PESS) for on-demand electric vehicle charging. We consider a PESS that consists of an electric truck, battery, and charging stations on highways. Actions include the selection of stations and routes, the selection of charging and discharging, and the corresponding charging/discharging power selection. The first two are discrete, while the last is continuous. To deal with the hybrid action space and time-varying state space of PESS, we design an action space, observation space, and reward function according to its characteristics; in addition, we develop a state-of-the-art DRL model for online decision-making considering the uncertainty in the real-time market price of electricity. Through numerical simulations with real-world electricity price data in California, we demonstrate the effectiveness of the proposed method.","1939-9367","","10.1109/TIA.2023.3274729","National Key Research and Development Program(grant numbers:2022YFB2405600); National Natural Science Foundation of China(grant numbers:72271008,72131001,T2121002,52277092); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10122695","Portable Energy storage;deep reinforcement learning;neural network;hybrid action space;on-demand charging","Road transportation;Costs;Optimization;Batteries;Transportation;Spatiotemporal phenomena;Real-time systems","","","","","","","IEEE","10 May 2023","","","IEEE","IEEE Early Access Articles"
"A Cooperative Scatter Search With Reinforcement Learning Mechanism for the Distributed Permutation Flowshop Scheduling Problem With Sequence-Dependent Setup Times","F. Zhao; G. Zhou; L. Wang","School of Computer and Communication Technology, Lanzhou University of Technology, Lanzhou, China; School of Computer and Communication Technology, Lanzhou University of Technology, Lanzhou, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","18 Jul 2023","2023","53","8","4899","4911","The integration of reinforcement learning technology into meta-heuristic algorithms to address complex combinatorial optimization problems has attracted much attention in recent years. A cooperative scatter search with  $Q$ -learning mechanism (QCSS) is proposed for solving the DPFSP-SDST. In the diversification generation method, two effective heuristic algorithms are designed to construct an initial population with high quality and diversity. In the improved method, eight domain knowledge-guided perturbation operators are combined with  $Q$ -learning to balance the exploration and exploitation capabilities of the QCSS algorithm. The reference set (RefSet) is divided into two subpopulations, and adaptive competition is adopted between the subpopulations to enhance search efficiency. In addition, a restart mechanism is proposed in the RefSet update phase to ensure the diversity of solutions. The performance of the QCSS algorithm is verified on the benchmark set, and the experimental results demonstrate the robustness and effectiveness of the QCSS algorithm.","2168-2232","","10.1109/TSMC.2023.3256484","National Natural Science Foundation of China(grant numbers:62063021,62273193); High-Level Foreign Experts Project of Gansu Province(grant numbers:22JR10KA007); Key Research Programs of Science and Technology Commission Foundation of Gansu Province(grant numbers:21YF5WA086); Lanzhou Science Bureau Project(grant numbers:2018-rc-98); Project of Gansu Natural Science Foundation(grant numbers:21JR7RA204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10086534","Distributed permutation flow shop scheduling;reinforcement learning (RL);scatter search (SS) algorithm;sequence-dependent setup times","Heuristic algorithms;Job shop scheduling;Scheduling;Q-learning;Search problems;Metaheuristics;Statistics","flow shop scheduling;metaheuristics;production engineering computing;reinforcement learning;search problems","combinatorial optimization problems;distributed permutation flowshop scheduling problem;diversification generation method;domain knowledge-guided perturbation operators;DPFSP-SDST;metaheuristic algorithms;QCSS algorithm;reinforcement learning mechanism;restart mechanism;scatter search Q-learning mechanism;sequence-dependent setup times","","","","52","IEEE","29 Mar 2023","","","IEEE","IEEE Journals"
"Fair Reinforcement Learning Algorithm for PV Active Control in LV Distribution Networks","M. Vassallo; A. Benzerga; A. Bahmanyar; D. Ernst","Department of Electrical Engineering and Computer Science, Liège, Belgium; Department of Electrical Engineering and Computer Science, Liège, Belgium; Department of Electrical Engineering and Computer Science, Liège, Belgium; Department of Electrical Engineering and Computer Science, Liège, Belgium","2023 International Conference on Clean Electrical Power (ICCEP)","13 Sep 2023","2023","","","796","802","The increasing adoption of distributed energy resources, particularly photovoltaic (PV) panels, has presented new and complex challenges for power network control. With the significant energy production from PV panels, voltage issues in the network have become a problem. Currently, PV smart inverters (SIs) are introduced to mitigate the voltage problems by controlling their active power generation and reactive power injection or absorption. However, reducing the active power output of PV panels can be perceived as unfair to some customers, discouraging the future installations. In this paper, a reinforcement learning technique is proposed to address voltage issues in a distribution network, while considering fairness in active power curtailment among customers. The feasibility of the proposed approach is explored through experiments, demonstrating its ability to effectively control voltage in a fair and efficient manner.","2474-9664","979-8-3503-4837-8","10.1109/ICCEP57914.2023.10247485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247485","Distributed Generation;Fair Curtailment;Low-Voltage Distribution Network;Reinforcement Learning;Voltage Control","Photovoltaic systems;Reactive power;Costs;Reinforcement learning;Distribution networks;Production;Silicon","invertors;photovoltaic power systems;power distribution control;power engineering computing;power generation control;reactive power control;reinforcement learning;voltage control","active power curtailment;active power generation;active power output;control voltage issues;distributed energy resources;energy production;fair reinforcement learning algorithm;LV distribution networks;photovoltaic panels;power network control;PV active control;PV panels;PV smart inverters;reactive power injection;voltage problems","","","","22","IEEE","13 Sep 2023","","","IEEE","IEEE Conferences"
"Tool Path Optimization for Complex Cavity Milling Based on Reinforcement Learning Approach","Y. Wan; W. Xu; T. -Y. Zuo","School of Environmental Science, Nanjing Xiaozhuang University, Nanjing, China; School of Mechanical and Electrical Engineering, Sanjiang University, Nanjing, China; School of Automation, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Access","10 Jul 2023","2023","11","","66793","66807","In the machining of parts, tool paths for complex cavity milling often have different generation options, as opposed to simple machining features. The different tool path generation options influence the machining time and cost of the part during the machining process. Decision makers prefer tool path solutions that have fewer blanking lengths, which means that the machining process is more efficient. Therefore, in order to reduce costs and increase efficiency, it is necessary to carefully design the tool path generation for the features to be machined on the part, especially for complex cavity milling features. However, solutions to the problem of optimal design of tool paths for complex cavity milling features have not been well developed in current research work. In this paper, we present a systematic solution for complex cavity milling tool path generation based on reinforcement learning. First, a grid converter is executed for converting the 3D geometry of the cavity milling feature into a matrix of planar grid points recognisable by the program, set according to the cutting parameters. Afterwards, the tool path generation process is refined and modelled as a Markov decision process. Ultimately, a tool path generation solution combining the A* algorithm with the Q-learning algorithm is executed. The agent iterates through trial and error to construct an optimal tool path for a given cavity milling task. Three case experiments demonstrate the feasibility of the proposed approach. The superiority of the reinforcement learning-based approach in terms of solution speed and solution quality is further demonstrated by comparing the proposed approach with the evolutionary computational techniques currently popular in research for solving tool path optimisation design problems.","2169-3536","","10.1109/ACCESS.2023.3262169","Natural Science Foundation of the Jiangsu Higher Education Institutions of China(grant numbers:19KJB460006); China Postdoctoral Science Foundation(grant numbers:2019M651642); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10081313","Tool path optimization;cavity milling;path planning algorithm;reinforcement learning;Q-learning algorithm","Milling;Optimization;Task analysis;Q-learning;Markov processes;Three-dimensional displays;Iterative methods","cutting;decision making;evolutionary computation;Markov processes;milling;optimisation;production engineering computing;reinforcement learning","A* algorithm;complex cavity milling tool path generation;machining process;Markov decision process;matrix;optimal design;planar grid points;Q-learning algorithm;reinforcement learning;tool path generation options;tool path optimisation design problems","","","","41","CCBYNCND","27 Mar 2023","","","IEEE","IEEE Journals"
"Mobile Robot Planner with Low-cost Cameras Using Deep Reinforcement Learning","M. Q. Tran; N. Q. Ly","Faculty of Information Technology, VNUHCM-University of Science, Ho Chi Minh City, Vietnam; Computer Vision & Cognitive Cybernetics Dept., VNUHCM-University of Science, Ho Chi Minh City, Vietnam","2020 7th NAFOSTED Conference on Information and Computer Science (NICS)","2 Feb 2021","2020","","","54","59","This study develops a robot mobility policy based on deep reinforcement learning. Since traditional methods of conventional robotic navigation depend on accurate map reproduction as well as require high-end sensors, learning-based methods are positive trends, especially deep reinforcement learning. The problem is modeled in the form of a Markov Decision Process (MDP) with the agent being a mobile robot. Its state of view is obtained by the input sensors such as laser findings or cameras and the purpose is navigating to the goal without any collision. There have been many deep learning methods that solve this problem. However, in order to bring robots to market, low-cost mass production is also an issue that needs to be addressed. Therefore, this work attempts to construct a pseudo laser findings system based on direct depth matrix prediction from a single camera image while still retaining stable performances. Experiment results show that they are directly comparable with others using high-priced sensors.","","978-0-7381-0553-6","10.1109/NICS51282.2020.9335852","Honors Program, University of Science, Vietnam National University - Ho Chi Minh City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9335852","mapless planner;deep reinforcement learning;vision-based navigation","Mass production;Navigation;Robot vision systems;Reinforcement learning;Cameras;Sensors;Mobile robots","collision avoidance;control engineering computing;deep learning (artificial intelligence);Markov processes;mobile robots;navigation;path planning;robot vision","low-cost mass production;mobile robot planner;low-cost cameras;deep reinforcement learning;robot mobility policy;robotic navigation;high-end sensors;learning-based methods;Markov decision process;direct depth matrix prediction;pseudo laser findings system","","","","41","IEEE","2 Feb 2021","","","IEEE","IEEE Conferences"
