"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"An modeling processing method for video games based on deep reinforcement learning","R. Tan; J. Zhou; H. Du; S. Shang; L. Dai","School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, Anhui, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, Anhui, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, Anhui, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, Anhui, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, Anhui, China","2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","5 Aug 2019","2019","","","939","942","In the reinforcement learning field, the traditional Q-Learning method can make the agent get a good grades on some simple games whose states are limited. Meanwhile, if the model of game is relatively simple, the continuous states can also be transformed into finite states through discretization so that the agent can achieve good results. However, traditional Q-Learning will face many difficulties for video games with image output, such as too complicated image states to store. In this paper, an algorithm of deep reinforcement learning(DRL), called Deep Q-Network(DQN), which is originated from Q-Learning and combining with artificial neural network, is used to model the video games with its image output. Then in the model training process, some image processing optimization methods and neural network structure are put forward for the preparation of test. Finally, from the experimental results, it is concluded that DQN can make the agent get high scores from video game, and the human game data can make the model training faster and better.","","978-1-5386-8178-7","10.1109/ITAIC.2019.8785463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8785463","Deep reinforcement learning;Image processing;Artificial intelligence","Games;Training;Reinforcement learning;Neural networks;Deep learning;Automobiles","computer games;image processing;learning (artificial intelligence);neural nets","modeling processing method;video game;reinforcement learning field;Q-Learning method;continuous states;finite states;image output;complicated image states;model training process;image processing optimization methods;human game data;DQN;deep Q-network;deep reinforcement learning","","6","","13","IEEE","5 Aug 2019","","","IEEE","IEEE Conferences"
"A strategy for push recovery in quadruped robot based on reinforcement learning","Y. -z. Chen; W. -Q. Hou; J. Wang; J. -W. Wang; H. -x. Ma","College of Mechatronics and Automation, National University of Defense Technology, Changsha, P. R. China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, P. R. China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, P. R. China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, P. R. China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, P. R. China","2015 34th Chinese Control Conference (CCC)","14 Sep 2015","2015","","","3145","3151","In this paper, a strategy for push recovery in quadruped robot based on reinforcement learning(RL) is proposed. At first, this strategy makes use of the simplified model of quadruped robot to reduce the dimensions of the action and state space for the RL framework, then it enhance the efficiency of the arithmetic by using the prior knowledge provided by the simplified model. Through learning process, this strategy can provide a foot placement estimate to the quadruped robot to restore balance while being pushed. By compared with the traditional arithmetic on a united simulation platform, we prove that this arithmetic is available, and can converge at the result quickly.","1934-1768","978-9-8815-6389-7","10.1109/ChiCC.2015.7260125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7260125","Reinforcement Learning;Push Recovery;Quadruped Robot;Foot Placement Estimate","Legged locomotion;Learning (artificial intelligence);Mathematical model;Foot;Approximation algorithms;Joints","learning (artificial intelligence);legged locomotion","push recovery;quadruped robot;reinforcement learning;RL;learning process;foot placement estimate;united simulation platform","","5","","24","","14 Sep 2015","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-Based Voltage Control Method for Distribution Network with High Penetration of Renewable Energy","S. Liu; C. Ding; Y. Wang; Z. Zhang; M. Chu; M. Wang","College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China","2021 IEEE Sustainable Power and Energy Conference (iSPEC)","24 Mar 2022","2021","","","287","291","In order to improve the stable operation of the distribution network that is connected to renewable energy and solve the problem of the distribution network with voltage out of limit, an optimal control method of distributed generation participating in voltage regulation based on Deep Deterministic Policy Gradient (DDPG) algorithm is proposed in this paper. Firstly, the application basis of DDPG algorithm is presented, after that, a mathematical model of the environment of the distribution network with distributed generations based on DDPG algorithm is established. Finally, based on the principle of reactive power compensation as the mainstay and active power reduction as a supplement, the voltage regulation control process is given. The system can continuously accumulate experience through off-line learning, the optimal control is given according to the real-time measurement data of the distribution network. Meanwhile, the speedability and effectiveness of the approach proposed are verified by simulation.","","978-1-6654-1439-5","10.1109/iSPEC53008.2021.9736086","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9736086","Distribution network;Distributed generation;Voltage control;Deep Deterministic Policy Gradient","Renewable energy sources;Reactive power;Optimal control;Process control;Distribution networks;Real-time systems;Distributed power generation","distributed power generation;distribution networks;learning (artificial intelligence);optimal control;reactive power;voltage control;voltage regulators","distribution network;distributed generation;DDPG algorithm;voltage regulation control process;voltage control method;renewable energy;optimal control method;Deep Deterministic Policy Gradient algorithm","","2","","14","IEEE","24 Mar 2022","","","IEEE","IEEE Conferences"
"Emotion Regulation Based on Multi-objective Weighted Reinforcement Learning for Human-robot Interaction","M. Hao; W. Cao; Z. Liu; M. Wu; Y. Yuan","Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China","2019 12th Asian Control Conference (ASCC)","18 Jul 2019","2019","","","1402","1406","Given emotion is important in maintaining mental and physical well-being. A multi-objective weighted reinforcement learning (MOW-RL) decision method is proposed to execute emotion regulation in human-robot interaction. The goal of emotion regulation is to minimize the cost of executing the service while eliminate user's negative emotions or maintain user's positive emotions. Considering the coordination problem of two objectives, fuzzy analytic hierarchy process (FAHP) is used to calculate the weight of each target reward and punishment function under different conditions. In addition, the influence factors of different personality on the difficulty level of emotion transfer are calculated by FAHP. Experiments are performed by 20 experimenters in the laboratory scenario, from which the results show that experimenters' satisfaction is 2.3, which is close to satisfaction.","","978-4-88898-300-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8764941","","Human-robot interaction;External stimuli;Service robots;Reinforcement learning;Robot kinematics;Emotion recognition","analytic hierarchy process;emotion recognition;fuzzy set theory;human-robot interaction;learning (artificial intelligence);minimisation","multiobjective weighted reinforcement learning decision method;emotion regulation;human-robot interaction;emotion transfer;cost minimization;user negative emotions;user positive emotions;fuzzy analytic hierarchy process;target reward;punishment function;FAHP;experimenter satisfaction","","1","","19","","18 Jul 2019","","","IEEE","IEEE Conferences"
"Toward Obstacle Avoidance for Mobile Robots Using Deep Reinforcement Learning Algorithm","X. Gao; L. Yan; G. Wang; T. Wang; N. Du; C. Gerada","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation, Beijing University of Posts and Telecommunications, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Power Electronics, Machines and Control Group, University of Nottingham, Nottingham, U.K","2021 IEEE 16th Conference on Industrial Electronics and Applications (ICIEA)","30 Aug 2021","2021","","","2136","2139","The state-of-the-art deep reinforcement learning algorithm, i.e., the deep deterministic policy gradient (DDPG), has achieved good performance in continuous control problems for the robotics. However, the conventional experience replay mechanism of the DDPG algorithm stores the experience explored by the mobile robot in the bufer pool, and trains the neural network through random sampling, without considering whether the transition is valuable, which can probably influence the network performance. To overcome the limitation, the DDPG framework with separating experience is developed for mobile robot collision-free navigation in this study, to replay the transitions of valuable and the failed experience discretely. Additionally, environment state vector is designed including mobile robot and obstacles, the reward function and action space are also designed. The simulation results show that the proposed model can possess the collision-free navigation capacity to deal with multiple obstacles.","2158-2297","978-1-6654-2248-2","10.1109/ICIEA51954.2021.9516114","National Natural Science Foundation of China(grant numbers:51875013); National Key R&D Program of China(grant numbers:2017YFB1300400); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516114","mobile robot;obstacle avoidance;deep deterministic policy gradient","Navigation;Service robots;Heuristic algorithms;Simulation;Conferences;Neural networks;Reinforcement learning","collision avoidance;deep learning (artificial intelligence);gradient methods;mobile robots","obstacle avoidance;deep reinforcement learning algorithm;deep deterministic policy gradient;continuous control problems;experience replay mechanism;neural network;DDPG framework;mobile robot collision-free navigation;environment state vector","","1","","17","IEEE","30 Aug 2021","","","IEEE","IEEE Conferences"
"Filter Design for Single-Phase Grid-Connected Inverter Based on Reinforcement Learning","W. Yan; M. Dong; L. Li; R. Liang; C. Xu","School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China","2022 IEEE 17th Conference on Industrial Electronics and Applications (ICIEA)","12 Jan 2023","2022","","","261","266","As an important part of the grid-connected inverter, the filtering characteristics directly determine the gridconnected performance of the inverter. In this paper, the two fifth-order filter topologies are used as templates for Reinforcement learning selection based on the performance of the filters. The Reinforcement learning algorithm rewards the performance of the filters under different templates accordingly, and then continuously trains the neural network to obtain the optimal filter templates and component parameters. The simulation results validate the rationality of the proposed reinforcement learning-based filter design method.","2158-2297","978-1-6654-0984-1","10.1109/ICIEA54703.2022.10006121","National Natural Science Foundation of China; Natural Science Foundation of Hunan Province; Central South University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10006121","grid-connected inverter;fifth-order filter;Reinforcement learning","Network topology;Simulation;Design methodology;Neural networks;Reinforcement learning;Filtering algorithms;Inverters","filtering theory;invertors;power engineering computing;power grids;reinforcement learning","different templates;fifth-order filter topologies;filtering characteristics;gridconnected performance;optimal filter templates;Reinforcement learning algorithm;Reinforcement learning selection;reinforcement learning-based filter design method;single-phase grid-connected inverter","","1","","12","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"Single leg operational space control of quadruped robot based on reinforcement learning","Jinhui Rao; Honglei An; Taihui Zhang; Yangzhen Chen; Hongxu Ma","College of Mechatronics Engineering and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics Engineering and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics Engineering and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics Engineering and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics Engineering and Automation, National University of Defense Technology, Changsha, China","2016 IEEE Chinese Guidance, Navigation and Control Conference (CGNCC)","23 Jan 2017","2016","","","597","602","Due to the strongly environmental adaptation, quadruped robot becomes a research hot spot. The single leg control lays foundation for the control of quadruped robot. The operational space control which is used in the quadruped robot single leg control algorithms usually strongly depends on the accuracy of the model. This paper applies RBF (radial basis function) neural network adaptive control on the single leg, and a kind of control parameters adjustment method which is based on reinforcement learning is proposed. The result shows the algorithm effectively improves the control accuracy and convergence speed under the high-dynamic condition.","","978-1-4673-8318-9","10.1109/CGNCC.2016.7828853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828853","","Legged locomotion;Learning (artificial intelligence);Adaptation models;Adaptive control;Biological neural networks","adaptive control;learning (artificial intelligence);legged locomotion;neurocontrollers;radial basis function networks","radial basis function neural network adaptive control;RBF neural network adaptive control;reinforcement learning;quadruped robot;single leg operational space control","","","","16","IEEE","23 Jan 2017","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-based Multi-AMR Path Planning Algorithm","Z. Shen; Y. Duan; Y. Cong; W. Li; H. Du; W. Zhu","School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, Anhui, P. R. China; The 2nd Research Institute of China Electronics Technology group Corporation, Taiyuan, Shanxi, P. R. China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, Anhui, P. R. China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, Anhui, P. R. China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, Anhui, P. R. China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, Anhui, P. R. China","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","1984","1987","Aiming at the scheduling problem of multi-autonomous mobile robots (AMR) in the box storage environment, the traditional dynamic programming (DP) algorithm has the disadvantage of low efficiency in solving the feasible path. To solve this problem, this paper establishes a reinforcement learning (RL) algorithm model with the goal of time optimization, which is used to improve the speed of path planning for multi-AMR simultaneous scheduling. In addition, combined with the advantages of the deep learning (DL) algorithm, the deep reinforcement learning (DRL) algorithm is used to effectively shorten the convergence time of the RL algorithm model training under high-dimensional and complex working conditions. The effectiveness of the DRL method is verified by comparing DP, RL, and DRL algorithm models in the simulation platform.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240541","Deep reinforcement learning;AMR;Path planning;Task scheduling","Deep learning;Training;Heuristic algorithms;Reinforcement learning;Dynamic scheduling;Path planning;Planning","deep learning (artificial intelligence);dynamic programming;learning (artificial intelligence);mobile robots;path planning;reinforcement learning","AMR;box storage environment;deep learning algorithm;deep reinforcement learning algorithm;DRL algorithm models;multiAMR path planning algorithm;multiAMR simultaneous scheduling;multiautonomous mobile robots;reinforcement learning algorithm model;RL algorithm model training;scheduling problem;time optimization;traditional dynamic programming algorithm","","","","23","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Power Electronics Converters Topology Derivation with Combination of TopoDiffVAE and Reinforcement Learning","C. Xu; M. Dong; L. Li; R. Liang; W. Yan","School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China","2022 4th International Conference on Smart Power & Internet Energy Systems (SPIES)","3 Apr 2023","2022","","","780","783","With the widespread use and development of power electronics, the study of power electronics, especially power electronics topology, has become increasingly compelling. With the study and expansion of graph theory on power electronics topology, it has become an idea to describe and reveal the topology by graph structures. We propose a framework combining the TopoDiffVAE model and reinforcement learning to reveal and learn the intrinsic connection rules of a certain class of topology and generate new topology based on them. Our model can be used not only to generate new topology, but also to accelerate the generation of other neural network-based topology.","","978-1-6654-8957-7","10.1109/SPIES55999.2022.10082077","Nature; Central South University; Natural Science Foundation of Hunan Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10082077","Topology generation;TopoDiffVAE;Reinforcement learning;Graph theory","Network topology;Energy;Reinforcement learning;Power electronics;Graph theory;Topology;Internet","graph theory;neural nets;power convertors;power electronics;power engineering computing;reinforcement learning","graph structures;intrinsic connection rules;neural network-based topology;power electronics converters topology derivation;reinforcement learning;TopoDiffVAE model","","","","9","IEEE","3 Apr 2023","","","IEEE","IEEE Conferences"
"The Optimality Analysis of Hybrid Reinforcement Learning Combined with SVMs","X. -n. Wang; W. Chen; D. -x. Liu; T. Wu; H. -g. He","College of Mechatronics Engineering and automation, National University of Defense Technology, Changsha, China; College of Mechatronics Engineering and automation, National University of Defense Technology, Changsha, China; College of Mechatronics Engineering and automation, National University of Defense Technology, Changsha, China; College of Mechatronics Engineering and automation, National University of Defense Technology, Changsha, China; College of Mechatronics Engineering and automation, National University of Defense Technology, Changsha, China","Sixth International Conference on Intelligent Systems Design and Applications","11 Dec 2006","2006","1","","936","941","To reduce the learning time of reinforcement learning (RL), hybrid algorithms that combine reinforcement learning with various supervised learning methods have attracted many research interests. However, the global convergence and optimality become one of the main problems for hybrid reinforcement learning algorithms. In this paper, the convergence of a hybrid RL algorithm, which is combined with support vector machines (SVMs) is analyzed theoretically. It is shown that by making use of policy gradient learning and the SVM regression, the hybrid algorithm can easily escape from local optima","2164-7151","0-7695-2528-8","10.1109/ISDA.2006.268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021565","","Learning;Algorithm design and analysis;Convergence;Support vector machines;Stochastic processes;Gradient methods;State estimation;State-space methods;Helium;Educational institutions","convergence;gradient methods;learning (artificial intelligence);regression analysis;support vector machines","optimality analysis;hybrid reinforcement learning;global convergence;support vector machine;policy gradient learning;SVM regression","","","","12","IEEE","11 Dec 2006","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Bi-stage Control for Semi-enclosed Berth","K. Hu; W. Li; J. Wu; C. Long; S. Li","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2022 13th Asian Control Conference (ASCC)","20 Jul 2022","2022","","","80","85","In this paper, reinforcement learning is used to study the problem of semi-enclosed berth with bilateral obstacles, where the unmanned surface vessel can berthing from any universal initial position. A bi-stage path planning is developed, where Stage 1 is to arrive the boundary quickly while keeping the connection between berth center and USV mass center consistent with yaw angle. Stage 2 ensures a successful berthing with no collision. By doing so, a stable, rapid, and safe berthing can be guaranteed. Finally, simulations verify the effectiveness and robustness of our proposed method.","2770-8373","978-89-93215-23-6","10.23919/ASCC56756.2022.9828102","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9828102","reinforcement learning;bi-stage control;automatic berthing;semi-enclosed","Process control;Reinforcement learning;Kinematics;Propulsion;Robustness;Path planning;Safety","collision avoidance;control engineering computing;marine vehicles;reinforcement learning;ships","yaw angle;bistage control;bistage path planning;rapid berthing;safe berthing;stable berthing;successful berthing;USV mass center;berth center;universal initial position;unmanned surface vessel;bilateral obstacles;semienclosed berth;reinforcement learning","","","","16","","20 Jul 2022","","","IEEE","IEEE Conferences"
"Finding the optimal sequence of features selection based on reinforcement learning","Song Bi; Lei Liu; Cunwu Han; Dehui Sun","The Beijing Key Laboratory of Fieldbus Technology and Automation, North China University of Technology, Beijing, China; The Beijing Key Laboratory of Fieldbus Technology and Automation, North China University of Technology, Beijing, China; The Beijing Key Laboratory of Fieldbus Technology and Automation, North China University of Technology, Beijing, China; The Beijing Key Laboratory of Fieldbus Technology and Automation, North China University of Technology, Beijing, China","2014 IEEE 3rd International Conference on Cloud Computing and Intelligence Systems","6 Aug 2015","2014","","","347","350","This paper proposes a method for generating an optimal feature selecting sequence which is cost-effective for pattern classification. The sequence describes the order that feature selects for the process like classification. We model the procedure of feature selecting using Markov decision process (MDP), and use dynamic programming (DP) to learn a strategy to generate the orders only with the feedback of circumstance. To simplify the problem, we design a simple test scene that classifying three objects, whose values of synthetic features are generated randomly, into three classes. The results of experiments show that our method can reduce the computational time of extracting features.","2376-595X","978-1-4799-4719-5","10.1109/CCIS.2014.7175757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7175757","Reinforcement learning;Optimal Sequence;Feature selection","Feature extraction;Learning (artificial intelligence);Computational efficiency;Object recognition;Pattern classification;Service robots;Robot sensing systems","dynamic programming;feature extraction;feature selection;image classification;image sequences;learning (artificial intelligence);Markov processes","reinforcement learning;optimal feature selecting sequence generation;pattern classification;Markov decision process;MDP;dynamic programming;DP;test scene design;object classification;randomly-generated synthetic features;computational time reduction;feature extraction","","","","6","IEEE","6 Aug 2015","","","IEEE","IEEE Conferences"
"Ramp Jump Control of Single-track Two-wheeled Robot using Reinforcement Learning with Demonstration Data","Q. Zheng; X. Zhu; B. Wang; Y. Deng; Z. Chen; B. Liang","Department of Automation, Tsinghua University, Beijing, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","2022 IEEE International Conference on Robotics and Biomimetics (ROBIO)","18 Jan 2023","2022","","","1769","1774","The single-track two-wheeled (STTW) robot ramp jump is a challenging task. This task not only requires the STTW robot to maintain its balance during the various phases of the jump but also requires it to use different speeds for different ramp terrains. This article proposes a deep reinforcement learning method to generate a controller for the STTW robot to complete the ramp jump task. First, the corresponding state space and action space are designed based on the characteristics of ramp jumping. Second, an effective reward function is developed to help train the agent. Then, the demonstration data are introduced into the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm to obtain better training results. Finally, the proposed control method is verified in a dynamic simulation environment. Simulation results show that the proposed control method can enable the STTW robot to complete the ramp jump task on the training terrain and on some other test terrains.","","978-1-6654-8109-0","10.1109/ROBIO55434.2022.10011757","National Natural Science Foundation of China(grant numbers:62073183); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011757","","Training;Deep learning;Simulation;Heuristic algorithms;Biomimetics;Prototypes;Reinforcement learning","gradient methods;mobile robots;reinforcement learning;state-space methods","control method;deep reinforcement learning method;ramp jump control;ramp jump task;ramp jumping;ramp terrain;single-track;STTW robot;TD3 algorithm;twin delayed deep deterministic policy gradient algorithm;two-wheeled robot ramp jump","","","","24","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"High maneuverability control of Single-track Two-wheeled Robot in Narrow Terrain based on Reinforcement Learning","Q. Zheng; X. Zhu; Y. Deng; Y. Tian; Z. Chen; B. Liang","Department of Automation, Tsinghua University, Beijing, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","2022 IEEE International Conference on Robotics and Biomimetics (ROBIO)","18 Jan 2023","2022","","","1775","1780","The single-track two-wheeled (STTW) robot has the advantages of small size and flexibility, and it is designed to travel in narrow terrain such as mountains or jungles. In this article, a reinforcement learning-based control method is proposed to solve the problem that STTW robots drive fast in narrow terrain with limited visibility. This control method integrates STTW robot path planning, trajectory tracking, and balance control in a single framework. Based on this framework, we define state, action, and reward function for narrow terrain passing tasks. At the same time, we design the actor network and the critic network structures and use the Twin Delayed Deep Deterministic Policy Gradient (TD3) to train these neural networks to construct a controller. Next, we verify the proposed control method on a simulation platform. The simulation results show that the obtained controller allows the STTW robot to effectively pass the training terrain. In addition, this article conducts a simulation comparison to prove advantages of training with the TD3 algorithm and the effectiveness of the reward function.","","978-1-6654-8109-0","10.1109/ROBIO55434.2022.10011769","National Natural Science Foundation of China(grant numbers:62073183); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011769","","Training;Adaptation models;Trajectory tracking;Simulation;Biomimetics;Robot control;Neural networks","learning (artificial intelligence);mobile robots;path planning","balance control;critic network structures;high maneuverability control;narrow terrain passing tasks;reinforcement learning-based control method;reward function;single framework;single-track;STTW robot path;training terrain;trajectory tracking;Twin Delayed Deep Deterministic Policy Gradient;two-wheeled robot","","","","32","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"Task Offloading Based-on Deep Reinforcement Learning for Microgrid","Y. Wang; X. Jin; R. Xu; W. Shao; F. Lin","School of Information and Automation Engineering, Qilu University of Technology(Shandong Academy of Sciences), Jinan, China; School of Information and Automation Engineering, Qilu University of Technology(Shandong Academy of Sciences), Jinan, China; School of Information and Automation Engineering, Qilu University of Technology(Shandong Academy of Sciences), Jinan, China; School of Information and Automation Engineering, Qilu University of Technology(Shandong Academy of Sciences), Jinan, China; School of Information and Automation Engineering, Qilu University of Technology(Shandong Academy of Sciences), Jinan, China","2022 IEEE 10th International Conference on Information, Communication and Networks (ICICN)","12 Jan 2023","2022","","","281","285","Nowadays, microgrid plays an important role in building local energy system to generate and consume power. However, there are numerous computing tasks and high delay in the data processing from users. Mobile edge computing (MEC) technology emerges as a new method, especially through the way of the edge-terminal collaborative. Furthermore, we propose a new MEC architecture in the background of microgrid, and then use deep Q network (DQN) to minimize the total processing latency to optimize offloading decision and resource allocation. Experiments results show that the proposed DQN algorithm can effectively reduce latency and improve convergence compared to MEC-only algorithms and local-only algorithms.","","978-1-6654-9082-5","10.1109/ICICN56848.2022.10006429","National Natural Science Foundation of China(grant numbers:U2006222); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10006429","Microgrid;mobile edge computing;partial offloading;deep Q network","Multi-access edge computing;Delay effects;Simulation;Collaboration;Microgrids;Reinforcement learning;Internet","cloud computing;distributed power generation;learning (artificial intelligence);mobile computing;resource allocation","data processing;deep Q network;deep reinforcement learning;DQN algorithm;edge-terminal collaborative;high delay;local energy system;MEC architecture;microgrid;mobile edge computing;numerous computing tasks;offloading decision;resource allocation;task offloading;total processing","","","","14","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Load Control Strategy for Combined Heat and Power Units","G. Xie; C. Cui; H. Zhao; J. Yang; Y. Shi","College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China","2021 11th International Conference on Power and Energy Systems (ICPES)","26 Jan 2022","2021","","","280","284","A deep reinforcement learning (DRL) control strategy is proposed for automatic generation control (AGC) operation of the combined heat and power (CHP) units under varying power system dynamic conditions. Firstly, a Markov decision process (MDP) model and a deep deterministic strategy (DDPG) algorithm are used to enhance the load control stability of the CHP units. Secondly, a reward and punishment mechanism is proposed to ensure the control performances of the main steam pressure, the power output, and the extraction pressure in CHP units. Due to the rapid adaptability of DRL, the stability time and the fluctuation range of these control performances are significantly reduced when the AGC disturbance occurs. Finally, the control effect of DRL has more superiority compared with the traditional PID feedback control strategy.","2767-732X","978-1-6654-1325-1","10.1109/ICPES53652.2021.9683825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9683825","combined heat and power plant;deep reinforcement learning;DDPG;Markov decision process","Fluctuations;Cogeneration;Reinforcement learning;Automatic generation control;Power system stability;Valves;Stability analysis","cogeneration;feedback;learning (artificial intelligence);load regulation;Markov processes;power generation control;three-term control","load control strategy;power units;deep reinforcement learning control strategy;automatic generation control operation;power system dynamic conditions;Markov decision process model;deep deterministic strategy algorithm;load control stability;CHP units;punishment mechanism;main steam pressure;power output;traditional PID feedback control strategy","","","","12","IEEE","26 Jan 2022","","","IEEE","IEEE Conferences"
"Dynamic Control of a Cable-Driven Parallel Robot Allowing Wrapping Phenomenon through Sim-to-Real Deep Reinforcement Learning","H. Zhu; K. Ouyang; H. Xiong; W. Lu","School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China","2023 International Conference on Advanced Robotics and Mechatronics (ICARM)","25 Aug 2023","2023","","","1102","1106","Cable-Driven Parallel Robots (CDPRs) have attracted the attention of a large number of researchers in recent years. Researchers proposed to allow wrapping phenomenon to expand the workspace of CDPRs, leading to challenges in the dynamic control of CDPRs. To this end, this study develops a sim-to-real Deep Reinforcement Learning (DRL)-based dynamic control approach for a CDPR allowing wrapping phenomenon. A planner CDPR prototype allowing wrapping phenomenon is established and the DRL-based dynamic control approach is evaluated based on the CDPR prototype. The effectiveness of the DRL-based dynamic control approach in controlling the pose of a CDPR allowing wrapping phenomenon is verified.","","979-8-3503-0017-8","10.1109/ICARM58088.2023.10218923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10218923","","Deep learning;Parallel robots;Mechatronics;Prototypes;Reinforcement learning;Wrapping","cables (mechanical);deep learning (artificial intelligence);end effectors;manipulator dynamics;reinforcement learning;robot programming","cable-driven parallel robots;CDPR pose control;DRL-based dynamic control;end effector;planner CDPR prototype;sim-to-real deep reinforcement learning;wrapping phenomenon","","","","11","IEEE","25 Aug 2023","","","IEEE","IEEE Conferences"
"A Generalized Dynamic Nonsmooth Control Design for DC Microgrids based on Deep Reinforcement Learning","L. Zhou; C. Zhang; J. Ge; X. Dong; C. Cui","College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China; College of Automation Engineering, Shanghai University of Electric Power, Shanghai, China","2023 IEEE 18th Conference on Industrial Electronics and Applications (ICIEA)","11 Sep 2023","2023","","","1257","1262","A novel generalized dynamic nonsmooth control framework, integrated with the deep reinforcement learning (DRL) algorithm, is developed in this paper for the primary layer control of DC microgrids. Aiming to strike a balance between the adaptiveness and robustness in traditional nonsmooth control, first, a benchmark nonsmooth controller is designed with reference to the existing literature. Second, the synthesis dynamic nonsmooth control strategy is presented, in which a soft actor-critic (SAC) algorithm is utilized to realize parameter self-configuration according to the current operating condition. The contributions of the proposed control framework can be summarized as the following twofold: the robustness and adaptiveness of the closed-loop system can be feasibly endowed. Besides, the large-signal stability of the DC/DC converter system can be guaranteed. The efficacy of the investigated approach is verified by simulation studies based on MATLAB/Simulation platform.","2158-2297","979-8-3503-1220-1","10.1109/ICIEA58696.2023.10241895","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10241895","nonsmooth control;DC microgrids;constant power loads;deep reinforcement learning;soft actor-critic","Deep learning;Heuristic algorithms;Reinforcement learning;Microgrids;Robustness;Stability analysis;Steady-state","closed loop systems;control system synthesis;deep learning (artificial intelligence);distributed power generation;power generation control;reinforcement learning","adaptiveness;benchmark nonsmooth controller;DC microgrids;deep reinforcement learning algorithm;dynamic nonsmooth control framework;generalized dynamic nonsmooth control design;primary layer control;robustness;soft actor-critic algorithm;synthesis dynamic nonsmooth control strategy;traditional nonsmooth control","","","","18","IEEE","11 Sep 2023","","","IEEE","IEEE Conferences"
"Application of Fuzzy Inference System to Average Reward Reinforcement Learning","W. Chen; Z. Zhai; X. Li; J. Guo","Faculty O Automation, Guandong University of Technology, Guangzhou, China; Faculty O Automation, Guandong University of Technology, Guangzhou, China; Faculty O Automation, Guandong University of Technology, Guangzhou, China; Faculty O Automation, Guandong University of Technology, Guangzhou, China","2009 International Conference on Information Technology and Computer Science","4 Aug 2009","2009","1","","374","377","To enhance the generalization ability of average reward reinforcement in continuous state space, this paper proposes an improved algorithm based on fuzzy inference system. In reinforcement learning, agent accesses to knowledge through the interaction with the environment. The reward signal from the environment carries out comprehensive appraisal of the effect of the action, which is operated by the agent. However, when the learning environment is complex and continuous, it will increase the difficulty of learning process and reduce the learning efficiency, even can not accomplish. In response to these disadvantages, we use fuzzy inference system as the approximator to generalize the continuous state space. Duo to this solution, the application scope of reinforcement learning can be effectively expanded. To verify the reasonableness of algorithm, we apply new version to Robocup simulation soccer. Through the analysis of experimental data, the results show that the performance of the improved algorithm is better than the original algorithm.","","978-0-7695-3688-0","10.1109/ITCS.2009.80","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190090","reinforcement learning;fuzzy inference system;generalization;R-learning","Fuzzy systems;Learning;Information technology;Computer science","function approximation;fuzzy reasoning;fuzzy systems;generalisation (artificial intelligence);learning (artificial intelligence);mobile robots;multi-robot systems;state-space methods","fuzzy inference system;average reward reinforcement learning algorithm;generalization ability;continuous state space function approximator;agent interaction;Robocup simulation soccer","","","","8","IEEE","4 Aug 2009","","","IEEE","IEEE Conferences"
"Formation Control With Collision Avoidance Through Deep Reinforcement Learning Using Model-Guided Demonstration","Z. Sui; Z. Pu; J. Yi; S. Wu","Chinese Academy of Sciences, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","2 Jun 2021","2021","32","6","2358","2372","Generating collision-free, time-efficient paths in an uncertain dynamic environment poses huge challenges for the formation control with collision avoidance (FCCA) problem in a leader-follower structure. In particular, the followers have to take both formation maintenance and collision avoidance into account simultaneously. Unfortunately, most of the existing works are simple combinations of methods dealing with the two problems separately. In this article, a new method based on deep reinforcement learning (RL) is proposed to solve the problem of FCCA. Especially, the learning-based policy is extended to the field of formation control, which involves a two-stage training framework: an imitation learning (IL) and later an RL. In the IL stage, a model-guided method consisting of a consensus theory-based formation controller and an optimal reciprocal collision avoidance strategy is designed to speed up training and increase efficiency. In the RL stage, a compound reward function is presented to guide the training. In addition, we design a formation-oriented network structure to perceive the environment. Long short-term memory is adopted to enable the network structure to perceive the information of obstacles of an uncertain number, and a transfer training approach is adopted to improve the generalization of the network in different scenarios. Numerous representative simulations are conducted, and our method is further deployed to an experimental platform based on a multiomnidirectional-wheeled car system. The effectiveness and practicability of our proposed method are validated through both the simulation and experiment results.","2162-2388","","10.1109/TNNLS.2020.3004893","National Key Research and Development Program of China(grant numbers:2018AAA0102404); Innovation Academy for Light-Duty Gas Turbine, Chinese Academy of Sciences(grant numbers:CXYJJ19-ZD-02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9142429","Collision avoidance;deep reinforcement learning (DRL);formation control;leader–follower","Collision avoidance;Training;Maintenance engineering;Machine learning;Multi-agent systems;Task analysis","collision avoidance;control engineering computing;deep learning (artificial intelligence);mobile robots;multi-robot systems","optimal reciprocal collision avoidance strategy;deep RL;formation-oriented network structure;deep reinforcement learning;model-guided demonstration;leader-follower structure;formation maintenance;learning-based policy;imitation learning;consensus theory;formation control with collision avoidance;collision-free generation;FCCA;long short-term memory;multiomnidirectional-wheeled car system;transfer training","","37","","49","IEEE","16 Jul 2020","","","IEEE","IEEE Journals"
"Multistep Multiagent Reinforcement Learning for Optimal Energy Schedule Strategy of Charging Stations in Smart Grid","Y. Zhang; Q. Yang; D. An; D. Li; Z. Wu","School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory for Manufacturing System Engineering, the School of Automation Science and Engineering, and the Ministry of Education Key Laboratory for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory for Manufacturing System Engineering, the School of Automation Science and Engineering, and the Ministry of Education Key Laboratory for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, China; School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China; College of Mechatronics and Control Engineering and the Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen University, Shenzhen, China","IEEE Transactions on Cybernetics","15 Jun 2023","2023","53","7","4292","4305","An efficient energy scheduling strategy of a charging station is crucial for stabilizing the electricity market and accommodating the charging demand of electric vehicles (EVs). Most of the existing studies on energy scheduling strategies fail to coordinate the process of energy purchasing and distribution and, thus, cannot balance the energy supply and demand. Besides, the existence of multiple charging stations in a complex scenario makes it difficult to develop a unified schedule strategy for different charging stations. In order to solve these problems, we propose a multiagent reinforcement learning (MARL) method to learn the optimal energy purchasing strategy and an online heuristic dispatching scheme to develop a energy distribution strategy in this article. Unlike the traditional scheduling methods, the two proposed strategies are coordinated with each other in both temporal and spatial dimensions to develop the unified energy scheduling strategy for charging stations. Specifically, the proposed MARL method combines the multiagent deep deterministic policy gradient (MADDPG) principles for learning purchasing strategy and a long short-term memory (LSTM) neural network for predicting the charging demand of EVs. Moreover, a multistep reward function is developed to accelerate the learning process. The proposed method is verified by comprehensive simulation experiments based on real data of the electricity market in Chicago. The experiment results show that the proposed method can achieve better performance than other state-of-the-art energy scheduling methods in the charging market in terms of the economic profits and users’ satisfaction ratio.","2168-2275","","10.1109/TCYB.2022.3165074","National Natural Science Foundation of China(grant numbers:61973247,62173268); China Postdoctoral Science Foundation(grant numbers:2021M692566); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9764664","Charging stations;energy scheduling;reinforcement learning (RL);smart grid","Charging stations;Schedules;Mathematical models;Optimization;Distribution strategy;Job shop scheduling;Dynamic scheduling","deep learning (artificial intelligence);electric vehicle charging;gradient methods;learning (artificial intelligence);multi-agent systems;optimisation;power engineering computing;power markets;purchasing;recurrent neural nets;reinforcement learning;scheduling;smart power grids;telecommunication computing","charging demand;charging market;charging station;different charging stations;efficient energy scheduling strategy;electric vehicles;electricity market;energy distribution strategy;energy scheduling strategies;energy supply;learning process;multiagent deep deterministic policy gradient principles;multiagent reinforcement learning method;multiple charging stations;optimal energy purchasing strategy;optimal energy schedule strategy;state-of-the-art energy;traditional scheduling methods;unified energy scheduling strategy;unified schedule strategy","Learning;Reinforcement, Psychology;Reward;Computer Simulation;Computer Systems","7","","43","IEEE","27 Apr 2022","","","IEEE","IEEE Journals"
"Reactive Power Optimization Using Feed Forward Neural Deep Reinforcement Learning Method : (Deep Reinforcement Learning DQN algorithm)","M. Ali; A. Mujeeb; H. Ullah; S. Zeb","School of Electrical and Electronic Engineering, North China Electric Power University, Beijing, China; School of Electrical and Electronic Engineering, North China Electric Power University, Beijing, China; School of Electrical and Electronic Engineering, North China Electric Power University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","2020 Asia Energy and Electrical Engineering Symposium (AEEES)","19 Jun 2020","2020","","","497","501","This paper focuses on reactive power optimization based on deep reinforcement learning. A deep reinforcement learning algorithm DQN is selected as an improvement of the reinforcement learning algorithm, this algorithm uses a deep neural network instead of a Q value table in the reinforcement learning algorithm, the method of power flow calculation in the Matlab toolbox is executed to verify the optimization effect. The single-section training results of the 14-BUS IEEE power grid model are tested and the parameters of the power grid model are modified after the optimization is confirmed. The DQN model is trained in a simulated complex environment, and the performance of the optimization effect is tested after the training is completed. Reactive power optimization of the power grid based on deep reinforcement learning is validated by experimental results.","","978-1-7281-6782-4","10.1109/AEEES48850.2020.9121492","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9121492","component;Deep reinforcement Learning (DQN);artificial intelligence;reactive power optimization","Training;Reactive power;Analytical models;Reinforcement learning;Mathematical models;Power grids;Data models","feedforward neural nets;learning (artificial intelligence);load flow;optimisation;power engineering computing;power grids;reactive power","reactive power optimization;deep neural network;power flow calculation;14-BUS IEEE power grid model;DQN algorithm;feed forward neural deep reinforcement learning;Matlab toolbox","","10","","10","IEEE","19 Jun 2020","","","IEEE","IEEE Conferences"
"Resource Allocation Based on Deep Reinforcement Learning for Wideband Cognitive Radio Networks","F. Zhou; Y. Wu; Q. Wu","Nanjing University of Aeronautics and Astronautics, Nanjing, China; Nanjing University of Aeronautics and Astronautics, Nanjing, China; Nanjing University of Aeronautics and Astronautics, Nanjing, China","2021 XXXIVth General Assembly and Scientific Symposium of the International Union of Radio Science (URSI GASS)","14 Oct 2021","2021","","","01","04","A wideband cognitive radio network relying on energy harvesting is studied under a practically non-linear energy harvesting model. In order to efficiently exploit the harvesting energy and enhance the performance of the secondary users (SUs), an intelligent resource allocation scheme based on deep reinforcement learning is proposed. The scheme intelligently selects the operation mode and the transmit power of SUs as well as allocates the sub-channels to maximize the defined reward function. Simulation results demonstrate the efficiency of our proposed resource allocation scheme.","2642-4339","978-9-4639-6-8027","10.23919/URSIGASS51995.2021.9560596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560596","","Simulation;Reinforcement learning;Resource management;Energy harvesting;Cognitive radio;Wideband","broadband networks;cognitive radio;deep learning (artificial intelligence);energy harvesting;reinforcement learning;resource allocation;telecommunication computing;telecommunication power management","deep reinforcement learning;wideband cognitive radio network;nonlinear energy harvesting model;intelligent resource allocation scheme;secondary users;transmit power;reward function","","3","","14","","14 Oct 2021","","","IEEE","IEEE Conferences"
"Control of Fab Lifters via Deep Reinforcement Learning: A Semi-MDP Approach","G. Kim; J. Shin; G. Kim; J. Kim; I. Yang","Department of Electrical and Computer Engineering, ASRI, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, ASRI, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, ASRI, Seoul National University, Seoul, South Korea; SK Hynix, Icheon, South Korea; Department of Electrical and Computer Engineering, ASRI, Seoul National University, Seoul, South Korea","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","13","In multi-floor fabrication facilities (fabs), lifters’ efficient transportation of resources is crucial for the overall productivity of the modern semiconductor industry. Unfortunately, most of the existing methods for controlling fab lifters have difficulty obtaining exact system models and applying traditional numerical schemes. In this paper, we propose two off-policy deep reinforcement learning algorithms that can learn to efficiently control the lifters in complex multi-floor fab environments with limited prior knowledge. The proposed algorithms exploit a novel semi-Markov decision process (semi-MDP) model and resolve several challenges that arise from the complex structures of multi-floor fabs to achieve highly efficient transportation. Extensive empirical analyses confirm that the controllers trained with our method automatically learn the intricate structure of the problem and effectively react to the real-time information flow of the multi-floor fab without referring to any domain knowledge. We empirically show that the proposed methods outperform advanced techniques including model predictive control and proximal policy optimization in terms of the computation time and transportation efficiency in various simulated fab environments. Note to Practitioners—In terms of implementation, our method is readily integrated into any fab that tracks real-time operational data, which is a common requirement in modern fabs. The method is particularly well-suited for facilities where the manufacturing processes are consistent over time. The crucial aspect of the lifter control problem is that the arrival of lots is independent of the lifter’s behavior. This makes it possible to collect data from real-world fabs and build multiple scenarios for training the reinforcement learning controllers. Furthermore, the robustness of the proposed method to small perturbations in the scenarios makes it applicable even when the frequencies of the lot arrivals change slightly. However, if there is a significant shift in lot arrival frequencies from the training data, the method may not perform as expected and may need to be extended using an advanced technique such as meta-reinforcement learning.","1558-3783","","10.1109/TASE.2023.3308849","MSIT(grant numbers:2021R1A4A2001824); MSIT(grant numbers:2020-0-00857,2022-0-00124); SK Hynix AICC (D20; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10239211","Reinforcement learning;industrial control;manufacturing automation","Transportation;Dispatching;Floors;Belts;Real-time systems;Q-learning;Elevators","","","","","","","IEEE","4 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Simulation-optimization using a reinforcement learning approach","C. D. Paternina-Arboleda; J. R. Montoya-Torres; A. Fabregas-Ariza","Department of Industrial Engineering, Universidad del Norte, Barranquilla, Colombia; School of Economics and Management Sciences, Universidad de La Sabana, Chia, Colombia; Department of Industrial and Management Engineering, University of South Florida, Tampa, FL, USA","2008 Winter Simulation Conference","30 Dec 2008","2008","","","1376","1383","The global optimization of complex systems such as industrial systems often necessitates the use of computer simulation. In this paper, we suggest the use of reinforcement learning (RL) algorithms and artificial neural networks for the optimization of simulation models. Several types of variables are taken into account in order to find global optimum values. After a first evaluation through mathematical functions with known optima, the benefits of our approach are illustrated through the example of an inventory control problem frequently found in manufacturing systems. Single-item and multi-item inventory cases are considered. The efficiency of the proposed procedure is compared against a commercial tool.","1558-4305","978-1-4244-2707-9","10.1109/WSC.2008.4736213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4736213","","Learning;Optimization methods;Artificial intelligence;Computational modeling;Computer simulation;Computer industry;Inventory control;Artificial neural networks;Manufacturing systems;Engineering management","digital simulation;learning (artificial intelligence);manufacturing data processing;neural nets;optimisation","simulation-optimization;reinforcement learning approach;global optimization;complex systems;industrial systems;computer simulation;artificial neural networks;inventory control problem;manufacturing systems","","6","","23","IEEE","30 Dec 2008","","","IEEE","IEEE Conferences"
"Data-Driven Deep Reinforcement Learning Control: Application to New Energy Aircraft PMSM","X. Li; Y. Qi; T. Zhao; Y. Liu; L. Zhang; H. Xu; W. Yu; X. Zhao; C. Zhang","Rhyxeon General Aircraft Co., Ltd., Shenyang, China; Rhyxeon General Aircraft Co., Ltd., Shenyang, China; Rhyxeon General Aircraft Co., Ltd., Shenyang, China; Rhyxeon General Aircraft Co., Ltd., Shenyang, China; Shenyang Aircraft Airworthiness Certification Center of CAAC, Shenyang, China; Shenyang Aircraft Airworthiness Certification Center of CAAC, Shenyang, China; School of Automation, Shenyang Aerospace University, Shenyang, China; School of Automation, Shenyang Aerospace University, Shenyang, China; School of Automation, Shenyang Aerospace University, Shenyang, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","7172","7132","As the power of the new energy aircraft, the performance of permanent magnet synchronous motor (PMSM) directly determines the reliability and stability of the flight state. The complex coupling characteristics (strong nonlinearity, time-varying parameters, multiple working modes) essentially exist in the PMSM speed control system, which makes it difficult for traditional control methods to meet application requirements. Aiming at the PMSM speed control, this paper proposes a novel data-driven deep reinforcement learning method. The system operating data is utilized, and a deep deterministic policy gradient (DDPG) agent is established to fully characterize the system data relationship. Instead of the speed loop and current loop in traditional PID control, DDPG intelligent controller directly outputs the quadrature axis voltage. The problem of poor torque dynamic performance caused by the slow response speed of the current loop in the traditional speed control system is effectively solved. Simulation results demonstrate that, under three different typical operating conditions, the proposed method can meet high requirements of PMSM control system. Compared with the traditional PID controller, the DDPG controller shows excellent dynamic and steady-state characteristics, and has stronger robustness and stability.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728191","National Natural Science Foundation of China; Technology Development; Liaoning Revitalization Talents Program; Scientific Research Fund of Liaoning Provincial Education Department; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728191","Data driven control;DDPG;PMSM;New energy aircraft","Torque;Atmospheric modeling;Velocity control;Reinforcement learning;Data models;Steady-state;Aircraft","aircraft power systems;angular velocity control;control engineering computing;deep learning (artificial intelligence);electric machine analysis computing;intelligent control;machine control;permanent magnet motors;reinforcement learning;synchronous motors;three-term control;torque control","complex coupling characteristics;current loop;data-driven deep reinforcement learning control;DDPG controller;DDPG intelligent controller;deep deterministic policy gradient agent;energy aircraft PMSM;flight state;multiple working modes;novel data-driven deep reinforcement learning method;permanent magnet synchronous motor;PMSM control system;PMSM speed control system;poor torque dynamic performance;reliability;slow response speed;speed loop;stability;steady-state characteristics;system data relationship;system operating data;time-varying parameters","","2","","11","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"An Intelligent Planning Method For The Multi-Rotor Manipulation Robot With Reinforcement Learning","H. Liu; P. Guo; X. Jin; H. Deng; K. Xu; X. Ding","Robotics Institute, School of Mechanical Engineering and Automation, Beihang University, Beijing, China; Robotics Institute, School of Mechanical Engineering and Automation, Beihang University, Beijing, China; Robotics Institute, School of Mechanical Engineering and Automation, Beihang University, Beijing, China; Robotics Institute, School of Mechanical Engineering and Automation, Beihang University, Beijing, China; Robotics Institute, School of Mechanical Engineering and Automation, Beihang University, Beijing, China; Robotics Institute, School of Mechanical Engineering and Automation, Beihang University, Beijing, China","2022 IEEE International Conference on Mechatronics and Automation (ICMA)","22 Aug 2022","2022","","","1028","1033","In this paper, an intelligent planning method based on proximal policy optimization algorithm for the multi-rotor aerial manipulation robot is presented. This method can not only avoid the complexity of the dynamic analysis and modeling but also the large disturbance of the manipulator to the robot’s body produced by the independent kinematic planning method. The disadvantage of kinematic planning method in aerial manipulation is given. The detailed structure of the adopted training and simulation environment is introduced. A deep reinforcement learning formulation is proposed to deal with the aerial manipulation of the robot. The particulars of setup in training and simulation are illustrated and the practical training and a series tests are carried out. The results of simulation proved the feasibility of this intelligent planning method and its advantages in real-time planning or replanning to enhance the stability of the robot in manipulation.","2152-744X","978-1-6654-0853-0","10.1109/ICMA54519.2022.9856385","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9856385","Reinforcement Learning;Robot Planning;UAV;Aerial Manipulation","Training;Mechatronics;Heuristic algorithms;Reinforcement learning;Kinematics;Stability analysis;Real-time systems","aerospace robotics;deep learning (artificial intelligence);manipulator dynamics;manipulator kinematics;optimisation;path planning;reinforcement learning","intelligent planning method;multirotor manipulation robot;proximal policy optimization algorithm;multirotor aerial manipulation robot;independent kinematic planning method;simulation environment;deep reinforcement learning formulation;replanning","","","","16","IEEE","22 Aug 2022","","","IEEE","IEEE Conferences"
"Optimal Scheduling of Integrated Energy System Based on Multi-agent Reinforcement Learning","F. Meng; H. Wang; L. Xu; C. Yuan; H. Wang; Y. Niu","College Of Automation & College Of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","1179","1184","The different interests pursued by each entity and the complex coupling relationship between multiple energy sources pose many challenges to the optimal operation of multi-park integrated energy systems(IESs). To this end, this paper proposes a data-driven based multi-agent proximal policy optimization (MAPPO) algorithm for the optimal scheduling of multi-park IESs. Firstly, the real-time optimal scheduling problem of multi-park IESs considering energy trading and carbon market trading is modeled as a Markov decision process (MDP). Secondly, the joint evaluation of MAPPO centralized training decentralized execution(CTDE) framework and global value function is used to achieve distributed autonomous learning for each agent to improve the quality of scheduling policies and privacy security of each park. The simulation results show that the proposed method has a faster convergence speed and can reduce the operating cost of the system under the influence of random fluctuations of new energy output and load.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055447","multi-park integrated energy systems;optimal scheduling;reinforcement learning;multi-agent proximal policy optimization","Couplings;Training;Privacy;Fluctuations;Costs;Uncertainty;Simulation","data analysis;energy management systems;Markov processes;multi-agent systems;optimisation;power markets;reinforcement learning;scheduling","carbon market trading;complex coupling relationship;distributed autonomous learning;global value function;MAPPO centralized training decentralized execution framework;Markov decision process;multiagent proximal policy optimization algorithm;multiagent reinforcement learning;multipark IESs;multipark integrated energy system;multiple energy sources;random fluctuations;real-time optimal scheduling problem","","","","16","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Primary Frequency Control of Renewable Energy Based on Deep Reinforcement Learning","Y. Wang; S. Liu; M. Chu; L. Liu; M. Wang; G. Dong","College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College Of Automation & College Of Artifical Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","5183","5188","Proportion of wind power, photovoltaic, and other renewable energy sources in the power system gradually increasing, the capability of system primary frequency regulation has a trend of weakening. To improve this situation, renewable energy sources are required to have the same ability to participate in system frequency regulation as conventional power sources. This paper analyzes the dynamic performance of the interconnected power system composed of wind farms based on doubly-fed induction generators (DFIG), then proposes a control method based on the deep reinforcement learning algorithm-Deep Deterministic Policy Gradient Agents (DDPG), according to the power generation characteristics of wind farms. The reward function, input states, and output actions of the DDPG algorithm are designed in conjunction with the control objectives, thus the DDPG algorithm is effectively applied to the primary frequency regulation optimization scheme to achieve the control goals which is adaptively acquiring the optimal coordination control strategies of the controllers of multiple renewable energy power plant. Numerical simulations on a two- area power system demonstrate that the design scheme can effectively mitigate the regional frequency deviation problem of each area containing in the power system, to maintain the stable operation of the system.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727948","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727948","deep deterministic policy gradient;renewable energy;primary frequency regulation;deep reinforcement learning;coordinative optimization","Renewable energy sources;Heuristic algorithms;Power system dynamics;Doubly fed induction generators;Reinforcement learning;Power system stability;Wind farms","asynchronous generators;frequency control;learning (artificial intelligence);power generation control;power grids;power system interconnection;renewable energy sources;wind power plants","primary frequency control;wind power;renewable energy sources;system primary frequency regulation;system frequency regulation;conventional power sources;interconnected power system;wind farms;doubly-fed induction generators;control method;deep reinforcement learning algorithm-Deep Deterministic Policy Gradient Agents;power generation characteristics;DDPG algorithm;control objectives;primary frequency regulation optimization scheme;control goals;optimal coordination control strategies;multiple renewable energy power plant;two- area power system;regional frequency deviation problem","","","","24","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Learning to Rock-and-Walk: Dynamic, Non-Prehensile, and Underactuated Object Locomotion Through Reinforcement Learning","A. Nazir; X. Pu; J. Rojas; J. Seo","The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong; The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","4487","4493","When moving objects that are too bulky or heavy to be grasped or lifted, robotic manipulation can benefit from the object's interaction with the support surface and its natural dynamics under gravity. In this work, we show that such dynamic, underactuated manipulation capability can be acquired through reinforcement learning and deployed on real robot systems. First, we present a framework to learn a control policy for object transport in a dynamic simulation environment, featuring the object and the support surface. We then demonstrate successful object locomotion with the learned policy through a set of simulated and real-world experiments, performed with a robot arm and an aerial robot interacting with the object in a non-prehensile manner. While the object, which is in contact with the support surface, oscillates sideways passively under gravity, the robot uses the learned policy to move the object forward with a steady gait by regulating the mechanical energy and the posture of the object. Our experiment results show that the learned policy can transport the object through unmodeled effects of terrain and perturbation.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811554","","Automation;Perturbation methods;Reinforcement learning;Autonomous aerial vehicles;Mechanical energy;Manipulator dynamics;Gravity","gait analysis;learning (artificial intelligence);legged locomotion;manipulators;mobile robots;motion control;path planning;position control;robot dynamics","rock-and-walk;underactuated object locomotion;reinforcement learning;moving objects;robotic manipulation;support surface;natural dynamics;dynamic manipulation capability;underactuated manipulation capability;robot systems;control policy;object transport;dynamic simulation environment;successful object locomotion;learned policy;robot arm;aerial robot interacting;nonprehensile manner","","","","29","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"A new multi-agent reinforcement learning algorithm and its application in wastewater reclamation by IBAC reactor","Haiyan Yang; Fang Ma; Fuyi Cui; Yu Zhong","School of Municipal and Environment Engineering, Harbin Institute of Technology, Harbin, Heilongjiang, China; School of Municipal and Environment Engineering, Harbin Institute of Technology, Harbin, Heilongjiang, China; School of Municipal and Environment Engineering, Harbin Institute of Technology, Harbin, Heilongjiang, China; Department of Software, Beijing Aerospace Automatic Control Institute, Beijing, China","Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)","18 Oct 2004","2004","3","","2671","2675 Vol.3","In multi-agent systems, joint-action must be employed to achieve cooperation because the evaluation to the behavior of an agent often depends on the other agents' behaviors. However, joint-action reinforcement learning suffers the slow convergence rate because of the enormous learning space produced by joint-action. In this article, a prediction-based reinforcement learning algorithm is presented for multi-agent cooperation tasks, which demands all agents to learn predicting the probabilities of actions that other agents may execute. An Immobilized Biological Activated Carbon (IBAC) reactor is run to test the efficacy of the new algorithm, and the result shows that the new algorithm can achieve high biodegradation efficiency much faster than the primitive reinforcement learning algorithm.","","0-7803-8273-0","10.1109/WCICA.2004.1342082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342082","","Wastewater;Inductors;Machine learning algorithms;Space technology;Multiagent systems;Acceleration;Collaboration;Convergence;Prediction algorithms;Testing","multi-agent systems;intelligent control;learning systems;adaptive control;learning (artificial intelligence);wastewater treatment;bioreactors;convergence;probability","multiagent systems;multiagent reinforcement learning;wastewater reclamation;immobilized biological activated carbon reactor;convergence rate;joint action reinforcement learning;multiagent cooperation tasks;probability;biodegradation efficiency","","1","","12","IEEE","18 Oct 2004","","","IEEE","IEEE Conferences"
"Relevant Safety Falsification by Automata Constrained Reinforcement Learning","C. Cronrath; T. P. Huck; C. Ledermann; T. Kröger; B. Lennartson","Department of Electrical Engineering, Division of Systems and Control, Chalmers University of Technology, Gothenburg, Sweden; Intelligent Process Automation and Robotics Lab, Institute of Anthropomatics and Robotics (IAR-IPR), Karlsruhe Institute of Technology, Germany; Intelligent Process Automation and Robotics Lab, Institute of Anthropomatics and Robotics (IAR-IPR), Karlsruhe Institute of Technology, Germany; Intelligent Process Automation and Robotics Lab, Institute of Anthropomatics and Robotics (IAR-IPR), Karlsruhe Institute of Technology, Germany; Department of Electrical Engineering, Division of Systems and Control, Chalmers University of Technology, Gothenburg, Sweden","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","2273","2280","Complex safety-critical cyber-physical systems, such as autonomous cars or collaborative robots, are becoming increasingly common. Simulation-based falsification is a testing method for uncovering safety hazards of such systems already in the design phase. Conventionally, the falsification method takes the form of a static optimization. Recently, dynamic optimization methods such as reinforcement learning have gained interest for their ability to uncover harder-to-find safety hazards. However, these methods may converge to risk-maximising, but irrelevant behaviors. This paper proposes a principled formulation and solution of the falsification problem by automata constrained reinforcement learning, in which rewards for relevant behavior are tuned via Lagrangian relaxation. The challenges and proposed methods are demonstrated in a use-case example from the domain of industrial human-robot collaboration, where falsification is used to identify hazardous human worker behaviors that result in human-robot collisions. Compared to random sampling and conventional approximate Q-learning, we show that the proposed method generates equally hazardous, but at the same time more relevant testing conditions that expose safety flaws.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926460","Vinnova; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926460","","Q-learning;Computer aided software engineering;Learning automata;Collaboration;Optimization methods;Cyber-physical systems;Hazards","automata theory;collision avoidance;control engineering computing;cyber-physical systems;human-robot interaction;industrial robots;mobile robots;occupational safety;optimisation;production engineering computing;reinforcement learning;safety-critical software","simulation-based falsification;safety hazards;design phase;static optimization;dynamic optimization methods;automata constrained reinforcement learning;relevant behavior;industrial human-robot collaboration;hazardous human worker;human-robot collisions;Q-learning;safety flaws;complex safety-critical cyber-physical systems;autonomous cars;collaborative robots;safety falsification","","","","32","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"An application of reinforcement learning to voltage control in power system","Hongxia Guo; Jie Wu; Yongqiang Liu; Chunru Wang","Electrical Power College, South China University of Technology, Guangzhou, China; Electrical Power College, South China University of Technology, Guangzhou, China; Electrical Power College, South China University of Technology, Guangzhou, China; Electrical Power College, South China University of Technology, Guangzhou, China","Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)","18 Oct 2004","2004","3","","2681","2685 Vol.3","A novel scheme based on the principle of reinforcement learning algorithm was proposed for the voltage control in power system. The controller adopted the adaptive heuristic critic (AHC) algorithm, which consists of two networks: action selection network and action critic network. The controller parameters are adjusted by given reinforcement signal of system. The numerical simulation example of single-machine infinite power system is given, and the result shows the designed control scheme can maintain desired voltage profile and prevent/stop a developing voltage collapse in system contingencies.","","0-7803-8273-0","10.1109/WCICA.2004.1342084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342084","","Learning;Voltage control;Power system control;Power systems;Power system simulation;Programmable control;Adaptive control;Heuristic algorithms;Control systems;Numerical simulation","learning (artificial intelligence);voltage control;power system control;optimisation;numerical analysis;static VAr compensators;control system synthesis;neurocontrollers","reinforcement learning;voltage control;adaptive heuristic critic algorithm;action selection network;action critic network;reinforcement signal;numerical simulation;single machine infinite power system;static VAr compensators;neurocontrollers;control system synthesis","","","","7","IEEE","18 Oct 2004","","","IEEE","IEEE Conferences"
"Sparse Markov Decision Processes With Causal Sparse Tsallis Entropy Regularization for Reinforcement Learning","K. Lee; S. Choi; S. Oh","Department of Electrical and Computer Engineering and Automation and Systems Research Institute (ASRI), Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering and Automation and Systems Research Institute (ASRI), Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering and Automation and Systems Research Institute (ASRI), Seoul National University, Seoul, South Korea","IEEE Robotics and Automation Letters","23 Feb 2018","2018","3","3","1466","1473","In this letter, a sparse Markov decision process (MDP) with novel causal sparse Tsallis entropy regularization is proposed. The proposed policy regularization induces a sparse and multimodal optimal policy distribution of a sparse MDP. The full mathematical analysis of the proposed sparse MDP is provided. We first analyze the optimality condition of a sparse MDP. Then, we propose a sparse value iteration method that solves a sparse MDP and then prove the convergence and optimality of sparse value iteration using the Banach fixed-point theorem. The proposed sparse MDP is compared to soft MDPs that utilize causal entropy regularization. We show that the performance error of a sparse MDP has a constant bound, while the error of a soft MDP increases logarithmically with respect to the number of actions, where this performance error is caused by the introduced regularization term. In experiments, we apply sparse MDPs to reinforcement learning problems. The proposed method outperforms existing methods in terms of the convergence speed and performance.","2377-3766","","10.1109/LRA.2018.2800085","Basic Science Research Program; National Research Foundation of Korea (NRF); Ministry of Science and ICT(grant numbers:(NRF-2017R1A2B2006136)); Next-Generation Information Computing Development Program; NRF; Ministry of Science and ICT(grant numbers:(2017M3C4A7065926)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8276246","Autonomous agents;deep learning in robotics and automation;learning and adaptive systems","Entropy;Markov processes;Learning (artificial intelligence);Convergence;Mathematical analysis;Autonomous vehicles","Banach spaces;entropy;learning (artificial intelligence);Markov processes;mathematical analysis","sparse Markov decision process;sparse policy distribution;multimodal optimal policy distribution;sparse MDP;causal sparse Tsallis entropy regularization;Banach fixed-point theorem;Reinforcement Learning","","16","","23","IEEE","31 Jan 2018","","","IEEE","IEEE Journals"
"Distributed Robust Process Monitoring Based on Optimized Denoising Autoencoder With Reinforcement Learning","S. Chen; Q. Jiang","Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai, China","IEEE Transactions on Instrumentation and Measurement","24 Feb 2022","2022","71","","1","11","Global monitoring for complex large-scale chemical processes is often challenging because of complex correlations among variables. This article proposes an optimized denoising autoencoder (DAE)-based distributed monitoring method to achieve efficient and robust monitoring of multiunit, nonlinear processes. First, a process is decomposed into multiple units, and then stacked DAE is used to extract the robust features of each unit and represent variable correlations within each unit. Second, deep regression neural networks are established between a local unit and its neighboring units to represent the correlations among units. A reinforcement learning-based neural architecture search method is proposed to avoid the tedious manual tuning process and obtain a high-performance neural network. Finally, a numerical simulation example, the Tennessee–Eastman benchmark process, and a laboratory-scale distillation process are used to verify the effectiveness of the proposed method.","1557-9662","","10.1109/TIM.2022.3147887","National Natural Science Foundation of China(grant numbers:61973119); Shanghai Rising-Star Program(grant numbers:20QA1402600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9698017","Denoising autoencoder (DAE);distributed monitoring;multiunit process;Q-learning;robust monitoring","Monitoring;Correlation;Feature extraction;Noise reduction;Q-learning;Optimization;Process monitoring","chemical industry;deep learning (artificial intelligence);distillation;feature extraction;neural net architecture;optimisation;process monitoring;production engineering computing;regression analysis;reinforcement learning","reinforcement learning-based neural architecture search;correlations representation;distillation process;Tennessee-Eastman benchmark process;deep regression neural networks;nonlinear processes;DAE;denoising autoencoder;chemical processes;distributed robust process monitoring","","5","","33","IEEE","31 Jan 2022","","","IEEE","IEEE Journals"
"Priority-Aware Reinforcement-Learning-Based Integrated Design of Networking and Control for Industrial Internet of Things","H. Xu; X. Liu; W. G. Hatcher; G. Xu; W. Liao; W. Yu","Department of Computer and Information Sciences, Towson University, Towson, MD, USA; Department of Computer and Information Sciences, Towson University, Towson, MD, USA; Department of Computer and Information Sciences, Towson University, Towson, MD, USA; Department of Computer Science and Information Technologies, Frostburg State University, Frostburg, MD, USA; Department of Computer and Information Sciences, Towson University, Towson, MD, USA; Department of Computer and Information Sciences, Towson University, Towson, MD, USA","IEEE Internet of Things Journal","5 Mar 2021","2021","8","6","4668","4680","Industrial Internet of Things (IIoT) envisions the tight coupling of numerous critical industrial manufacturing subsystems, such as control, networking, and computing through the ubiquitous Internet of Things technologies. Nonetheless, such interconnectivity poses significant challenges to the successful management and operation of massively distributed industrial manufacturing systems. Without carefully integrated system design, the nonoptimal management and operation of highly intertwined subsystems can lead to the loss of productivity and ultimately the value of factories and plants. To address this issue, in this article, we conduct the integrated design that is capable of simultaneously configuring both control and networking subsystems in IIoT with consideration for their inherent interdependencies. We first analyze the performance of the dynamic backoff exponential (BE) in IEEE 802.15.4 carrier-sense multiple access (CSMA) and show the performance improvement of dynamic BE. We then design a model-free reinforcement learning algorithm to configure the control and networking subsystems automatically via systematic trial and error, as it is impractical to build a model for a highly intertwined complex IIoT system. Considering the time-sensitive characteristics of IIoT systems, we design priority-aware policies based on importance among networking traffic (i.e., sensing traffic and actuation traffic) to improve the convergence speed. The experimental results demonstrate that our priority-aware reinforcement-learning-based integrated design can successfully reconfigure the complex and highly intertwined IIoT system at runtime with a minimal convergence time. Besides, our approach reduces convergence time by 37.5%, and energy consumption by 9.2%, compared to the standard reinforcement learning approach.","2327-4662","","10.1109/JIOT.2020.3027506","U.S. National Science Foundation(grant numbers:CNS 1350145); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9209038","Control and networking co-design;Industrial Internet of Things (IIoT);priority awareness;reinforcement learning","Sensors;IEEE 802.15 Standard;Integrated design;Multiaccess communication;Learning (artificial intelligence);Heuristic algorithms;Internet of Things","carrier sense multiple access;control engineering computing;Internet of Things;learning (artificial intelligence);manufacturing systems;Markov processes;production engineering computing;telecommunication congestion control;telecommunication traffic","highly intertwined IIoT system;priority-aware reinforcement-learning-based integrated design;ubiquitous Internet;massively distributed industrial manufacturing systems;nonoptimal management;highly intertwined subsystems;networking subsystems;IEEE 802.15.4 carrier-sense multiple access;model-free reinforcement learning algorithm;highly intertwined complex IIoT system;design priority-aware policies;networking traffic;complex intertwined IIoT system;critical industrial manufacturing subsystems","","8","","37","IEEE","29 Sep 2020","","","IEEE","IEEE Journals"
"Model-Free Reinforcement Learning based Lateral Control for Lane Keeping","Q. Zhang; R. Luo; D. Zhao; C. Luo; D. Qian","University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Synthetical Automation for Process Industries, North China Electric Power University, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Department of Electrical and Computer Engineering, University of Detroit Mercy, Michigan, USA; School of Control and Computer Engineering, North China Electric Power Unverisity, Beijing, China","2019 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2019","2019","","","1","7","In this paper, the lateral control strategy for lane keeping task, which is an important module in the advanced assistant driver systems, is proposed based on the model-free reinforcement learning. Different from the model-based methods, our method only requires the generated data rather than the accurate system model. Furthermore, the lateral control strategy for driver model lane keeping is given, where driver controller and direct yaw controller (DYC) are working at the same time to maintain the vehicle stability. Note that the dynamic game theory is considered for this task, where the steering wheel controller for driver and the DYC compensated controller are obtained based on Nash game theory. Finally, we give simulation examples to prove the validity of the proposed schemes.","2161-4407","978-1-7281-1985-4","10.1109/IJCNN.2019.8851766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8851766","Lateral control;lane keeping;reinforcement learning","Vehicles;Vehicle dynamics;Mathematical model;Optimal control;Reinforcement learning;Wheels;Task analysis","game theory;learning systems;motion control;position control;road vehicles;stability;steering systems;wheels","direct yaw controller;steering wheel controller;DYC compensated controller;Nash game theory;vehicle stability;driver model lane keeping;accurate system model;model-based methods;advanced assistant driver systems;lateral control strategy;model-free reinforcement learning","","7","","13","IEEE","30 Sep 2019","","","IEEE","IEEE Conferences"
"Energy Management for a Series-Parallel Plug-In Hybrid Electric Truck Based on Reinforcement Learning","Z. Wang; J. Xie; M. Kang; Y. Zhang","School of Mechanical Engineering, Yanshan University, Qinhuangdao, China; China North Engine Research Institute Genter of Test, Tianjin, China; State Key Laboratory of Synthetical Automation for Process Industries (SAPI), Northeastern University, Shenyang, China; School of Mechanical Engineering, Yanshan University, Qinhuangdao, China","2022 13th Asian Control Conference (ASCC)","20 Jul 2022","2022","","","590","596","Plug-in hybrid electric vehicles offer an opportunity to reduce emissions and Energy management strategies (EMS) are the key to improving the fuel economy of PHEVs. With the rise of artificial intelligence, deep reinforcement learning (DRL) is a promising approach for building energy management strategies for hybrid electric vehicles. For a novel single-shaft series-parallel powertrain, we propose a deep deterministic policy gradient energy management strategy combined with pre-optimization. Firstly, we regard the power distribution of the two motors in this powertrain as a static optimization problem to perform pre-optimization of power distribution and gear position, and integrate these two motors as a equivalent motor. Secondly, DDPG is used for mode selection and power distribution between the engine and the equivalent motor.","2770-8373","978-89-93215-23-6","10.23919/ASCC56756.2022.9828206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9828206","Energy management;Plug-in hybrid electric vehicle;Deep reinforcement learning","Gears;Buildings;Power distribution;Reinforcement learning;Mechanical power transmission;Fuel economy;Energy management","energy management systems;fuel economy;gears;hybrid electric vehicles;learning (artificial intelligence);optimisation;power transmission (mechanical)","series-parallel plug-in hybrid electric truck;hybrid electric vehicles;energy management strategies;deep reinforcement learning;novel single-shaft series-parallel powertrain;deep deterministic policy gradient energy management strategy;pre-optimization;power distribution","","","","13","","20 Jul 2022","","","IEEE","IEEE Conferences"
"Reinforcement learning-based tracking control for wheeled mobile robot","N. T. Luy","Division of Automation Electronics, Ho Chi Minh University of Industry, Vietnam","2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","13 Dec 2012","2012","","","462","467","This paper proposes a new method to design a reinforcement learning-based integrated kinematic and dynamic tracking control scheme for a nonholonomic wheeled mobile robot. The scheme uses just only one neural network to design an online adaptive synchronous policy iteration algorithm implemented as an actor critic structure. Our tuning law for the single neural network not only learns online a tracking-HJB equation to approximate both the optimal cost and the optimal adaptive control law but also guarantees closed-loop stability in real-time. The convergence and stability of the overall system are proven by Lyapunov theory. The simulation results for wheeled mobile robot verify the effectiveness of the proposed controller.","1062-922X","978-1-4673-1714-6","10.1109/ICSMC.2012.6377767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6377767","Adaptive critic;actor critic;policy iteration;neural network;mobile robot","Artificial neural networks;Tuning;Equations;Vectors;Mathematical model;Transmission line matrix methods;Kinematics","adaptive control;closed loop systems;iterative methods;learning (artificial intelligence);Lyapunov methods;mobile robots;neurocontrollers;optimal control;robot dynamics;robot kinematics;stability;wheels","reinforcement learning-based integrated kinematic tracking control scheme;reinforcement learning-based integrated dynamic tracking control scheme;nonholonomic wheeled mobile robot;neural network;online adaptive synchronous policy iteration algorithm;actor critic structure;tracking-HJB equation;optimal cost;optimal adaptive control law;closed-loop stability;Lyapunov theory","","","","17","IEEE","13 Dec 2012","","","IEEE","IEEE Conferences"
"Outperformance of Mall-Receptionist Android as Inverse Reinforcement Learning is Transitioned to Reinforcement Learning","Z. Chen; Y. Nakamura; H. Ishiguro","Behavior Learning Research Team, Guardian Robot Project, RIKEN, Saitama, Japan; Behavior Learning Research Team, Guardian Robot Project, RIKEN, Saitama, Japan; Graduate School of Engineering Science, Osaka University, Osaka, Japan","IEEE Robotics and Automation Letters","24 Apr 2023","2023","8","6","3350","3357","Robots can tackle human–robot interaction (HRI) tasks through inverse reinforcement learning (IRL). However, offline IRL agents' performance is upper-bounded by experts. Limited demonstration fails to provide an overall picture of the environment, especially in real-world applications. To further enhance IRL's performance, we implement a cross-modal inverse reinforcement learning to reinforcement learning (IRL-to-RL) transition framework for a real-world HRI interaction task, in which a mall receptionist android promotes sanitizer usage. During the 10-day experiment, the android develops a more proactive and effective strategy than the human expert. Furthermore, we explore four decay modes of prior knowledge supervision and suggest a preferable pattern for practical use. Our results demonstrate the feasibility of the framework to assist robots in switching to diverse modalities, learning incrementally with a sparse reward function, and eventually outperforming the human expert. We anticipate our framework to inspire more IRL-to-better learning paradigms, facilitating robots to outcompete teachers in more real-world HRI applications.","2377-3766","","10.1109/LRA.2023.3267385","JST, Moonshot R&D(grant numbers:JPMJMS2011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10102484","Social HRI;imitation learning;reinforcement learning","Robots;Task analysis;Human-robot interaction;Training;Switches;Reinforcement learning;Behavioral sciences","human-robot interaction;learning (artificial intelligence);mobile robots;reinforcement learning","cross-modal inverse reinforcement;human-robot interaction tasks;inverse reinforcement learning;IRL's performance;IRL-to-better learning paradigms;IRL-to-RL;mall receptionist android;mall-receptionist android;offline IRL agents;real-world HRI applications;real-world HRI interaction task","","","","22","IEEE","14 Apr 2023","","","IEEE","IEEE Journals"
"Optimal Control of a Gold Cyanide Leaching Process Based on Reinforcement Learning","J. Zheng; R. Jia; F. Chu","College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; Engineering Research Center of Intelligent Control for Underground Space of the Ministry of Education, School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China","2022 34th Chinese Control and Decision Conference (CCDC)","14 Feb 2023","2022","","","309","314","Leaching by cyanide solutions in aerated alkaline slurry is the main process for gold extraction from ores. In this study, the model-based reinforcement learning (MBRL) is used to design an optimal controller for a gold cyanidation leaching process (GCLP) by automatically learning a control policy. The simulation experiments are performed on a first principle model to verify the effectiveness of the proposed method. The results show that the MBRL method can learn an optimal control policy for the GCLP.","1948-9447","978-1-6654-7896-0","10.1109/CCDC55256.2022.10034081","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10034081","Optimal control;Gold cyanide leaching process;Probabilistic inference and learning for control;Reinforcement learning","Gold;Process control;Optimal control;Reinforcement learning;Leaching;Slurries","gold;leaching;learning (artificial intelligence);optimal control;reinforcement learning;slurries","aerated alkaline slurry;cyanide solutions;gold cyanidation leaching process;gold cyanide leaching process;gold extraction;main process;model-based reinforcement learning;optimal control policy;optimal controller","","","","12","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"A Novel Dynamic Operation Optimization Method Based on Multiobjective Deep Reinforcement Learning for Steelmaking Process","C. Liu; L. Tang; C. Zhao","Ministry of Education, National Frontiers Science Center for Industrial Intelligence and Systems Optimization, and the Key Laboratory of Data Analytics and Optimization for Smart Industry, Northeastern University, Shenyang, China; National Frontiers Science Center for Industrial Intelligence and Systems Optimization, Northeastern University, Shenyang, China; Liaoning Engineering Laboratory of Data Analytics and Optimization for Smart Industry, and the Liaoning Key Laboratory of Manufacturing System and Logistics Optimization, Northeastern University, Shenyang, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","15","This article studies a dynamic operation optimization problem for a steelmaking process. The problem is defined to determine optimal operation parameters that bring smelting process indices close to their desired values. The operation optimization technologies have been applied successfully for endpoint steelmaking, but it is still challenging for the dynamic smelting process because of the high temperature and complex physical and chemical reactions. A framework of deep deterministic policy gradient is applied to solve the dynamic operation optimization problem in the steelmaking process. Then, an energy-informed restricted Boltzmann machine method with physical interpretability is developed to construct the actor and critic networks in reinforcement learning (RL) for dynamic decision-making operations. It can provide a posterior probability for each action to guide training in each state. Furthermore, in terms of the design of neural network (NN) architecture, a multiobjective evolutionary algorithm is used to optimize the model hyperparameters, and a knee solution strategy is designed to balance the model accuracy and complexity of neural networks. Experiments are conducted on real data from a steelmaking production process to verify the practicability of the developed model. The experimental results show the advantages and effectiveness of the proposed method compared with other methods. It can meet the requirements of the specified quality of molten steel.","2162-2388","","10.1109/TNNLS.2023.3244945","Major Program of National Natural Science Foundation of China(grant numbers:72192830,72192831,72101052); Higher Education Discipline Innovation Project(grant numbers:B16009); Post-Doctoral Science Foundation of China(grant numbers:2021M700720); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10050437","Deep reinforcement learning (DRL);dynamic operation optimization;energy-informed restricted Boltzmann machine (EIRBM);multiobjective optimization;steelmaking process","Optimization;Steel;Furnaces;Smelting;Production;Process control;Closed box","","","","","","","IEEE","22 Feb 2023","","","IEEE","IEEE Early Access Articles"
"Deep Reinforcement Learning for Practical Phase-Shift Optimization in RIS-Aided MISO URLLC Systems","R. Hashemi; S. Ali; N. H. Mahmood; M. Latva-Aho","Centre for Wireless Communications, University of Oulu, Oulu, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Centre for Wireless Communications, University of Oulu, Oulu, Finland","IEEE Internet of Things Journal","5 May 2023","2023","10","10","8931","8943","We study the joint active/passive beamforming and channel blocklength (CBL) allocation in a nonideal reconfigurable intelligent surface (RIS)-aided ultrareliable and low-latency communication (URLLC) system. The considered scenario is a finite blocklength (FBL) regime and the problem is solved by leveraging a deep reinforcement learning (DRL) algorithm named twin-delayed deep deterministic policy gradient (TD3). First, assuming an industrial automation system, the signal-to-interference-plus-noise ratio and achievable rate in the FBL regime are identified for each actuator. Next, the joint active/passive beamforming and CBL optimization problem (OP) is formulated where the objective is to maximize the total achievable FBL rate in all actuators, subject to nonlinear amplitude response at the RIS elements, BS transmit power budget, and total available CBL. Since the formulated problem is highly nonconvex and nonlinear, we resort to employing an actor–critic policy gradient DRL algorithm based on TD3. The considered method relies on interacting RIS with the industrial automation environment by taking actions which are the phase shifts at the RIS elements, CBL variables, and BS beamforming to maximize the expected observed reward, i.e., the total FBL rate. We assess the performance loss of the system when the RIS is nonideal, i.e., with nonlinear amplitude response, and compare it with ideal RIS without impairments. The numerical results show that optimizing the RIS phase shifts, BS beamforming, and CBL variables via the TD3 method with deterministic policy outperforms conventional methods and it is highly beneficial for improving the network total FBL rate considering finite CBL size.","2327-4662","","10.1109/JIOT.2022.3232962","Academy of Finland, 6G Flagship Program(grant numbers:346208); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002901","Block error probability (BLER);deep reinforcement learning (DRL);finite blocklength (FBL);industrial automation;reconfigurable intelligent surface (RIS);ultrareliable low-latency communications (URLLC)","Ultra reliable low latency communication;Optimization;Array signal processing;Wireless communication;Resource management;Automation;Actuators","array signal processing;deep learning (artificial intelligence);gradient methods;optimisation;radiocommunication;reinforcement learning;telecommunication computing;telecommunication network reliability;telecommunication power management","achievable rate;actor-critic policy gradient DRL algorithm;actuator;BS beamforming;BS transmit power budget;CBL variables;deep deterministic policy gradient;deep reinforcement learning algorithm;FBL regime;finite blocklength regime;finite CBL size;ideal RIS;industrial automation environment;industrial automation system;low-latency communication system;network total FBL rate;nonlinear amplitude response;practical phase-shift optimization;RIS elements;RIS phase shifts;RIS-aided MISO URLLC systems;signal-to-interference-plus-noise ratio;total achievable FBL rate;total available CBL","","1","","50","CCBY","29 Dec 2022","","","IEEE","IEEE Journals"
"Automated Saturation Mitigation Controlled by Deep Reinforcement Learning","E. Aguas; A. Lambert; G. Blanc; H. Debar","Orange Labs, Châtillon, France; Orange Labs, Châtillon, France; Télécom SudParis, Institut Polytechnique de Paris, Evry-Courcouronnes, France; Télécom SudParis, Institut Polytechnique de Paris, Evry-Courcouronnes, France","2020 IEEE 28th International Conference on Network Protocols (ICNP)","20 Nov 2020","2020","","","1","6","Recent developments in orchestration and machine learning have made network automation more feasible, allowing the transition from error-prone and time-consuming manual manipulations to fast and refined automated responses in areas such as security and management. This article investigates the capabilities of a deep reinforcement learning agent to learn how to automatically share prefix announcements of an Autonomous System to its neighbors, in order to mitigate undesired network behaviors and therefore increase network resiliency and security. Our work focuses on network saturation, tackling the problem of network responsiveness in today's massive content delivery context. Results not only prove feasibility of such an agent, but also demonstrate its ability to minimize traffic loss as well as the number of actions to be performed by the automation process.","2643-3303","978-1-7281-6992-7","10.1109/ICNP49622.2020.9259356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9259356","deep reinforcement learning;network;automation;security;management;network resiliency;saturation","Automation;Reinforcement learning;Servers;Computer architecture;Cognition;Service-oriented architecture;Web and internet services","learning (artificial intelligence);telecommunication traffic","automation process;machine learning;network automation;time-consuming manual manipulations;automated responses;security;deep reinforcement learning agent;prefix announcements;autonomous system;undesired network behaviors;network resiliency;network saturation;network responsiveness;massive content delivery context;automated saturation mitigation;error-prone manual manipulations","","","","17","IEEE","20 Nov 2020","","","IEEE","IEEE Conferences"
"Autonomous construction of structures in a dynamic environment using Reinforcement Learning","S. R. B. dos Santos; S. N. Givigi; C. L. Nascimento","Division of Electronic Engineering, Instituto Tecnológico de Aeronáutica, Sao Jose dos Campos, Brazil; Department of Electrical and Computer Engineering, Royal Military College of Canada, Kingston, ONT, Canada; Division of Electronic Engineering, Instituto Tecnológico de Aeronáutica, Sao Jose dos Campos, Brazil","2013 IEEE International Systems Conference (SysCon)","1 Jul 2013","2013","","","452","459","This paper presents an adaptive approach based on the Reinforcement Learning (RL) method to manipulate and transport parts and also assemble 3-D structures in a moderately constrained and dynamic environment using a quad-rotor. Nowadays, complex construction tasks using mobile robots are characterized by two fundamental problems such as task planning and motion planning. However, to obtain the task and path planning that define a specific sequence of operations for construction of a given structure is generally very complex. In this context, we propose and investigate a system in which an aerial robot learns the assembly and construction tasks of multiple 3-D structures. This process involves the learning of the sequence of maneuvers of a vehicle, the assembly sequence of the parts and also the correct types of structural elements for each assembly point of the structure. A heuristic search algorithm is used in the learning process to find the optimal path for the quad-rotor so that its navigation through the dynamic environment is performed. The experimental results show that a 3-D structure can be built using the task planning approach derived from a learning algorithm combined with a heuristic search method.","","978-1-4673-3108-1","10.1109/SysCon.2013.6549922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549922","Robotic Construction;Reinforcement Learning;Learning Automata;Quad-rotor Robot;Task Planning","Assembly;Learning automata;Planning;Robot kinematics;Heuristic algorithms;Path planning","autonomous aerial vehicles;helicopters;learning (artificial intelligence);mobile robots;path planning;robotic assembly","reinforcement learning;RL method;quadrotor;3D structure assembly;mobile robot;task planning;motion planning;aerial robot;heuristic search algorithm","","9","","22","IEEE","1 Jul 2013","","","IEEE","IEEE Conferences"
"Sub-Resolution Assist Feature Generation with Reinforcement Learning and Transfer Learning","G. -T. Liu; W. -C. Tai; Y. -T. Lin; I. H. -R. Jiang; J. P. Shiely; P. -J. Cheng","Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan; Synopsys, Inc, Hillsboro, OR, United States; Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan","2022 IEEE/ACM International Conference On Computer Aided Design (ICCAD)","22 Mar 2023","2022","","","1","9","As modern photolithography feature sizes continue to shrink, sub-resolution assist feature (SRAF) generation has become a key resolution enhancement technique to improve the manufacturing process window. State-of-the-art works resort to machine learning to overcome the deficiencies of model-based and rule-based approaches. Nevertheless, these machine learning-based methods do not consider or implicitly consider the optical interference between SRAFs, and highly rely on post-processing to satisfy SRAF mask manufacturing rules. In this paper, we are the first to generate SRAFs using reinforcement learning to address SRAF interference and produce mask-rule-compliant results directly. In this way, our two-phase learning enables us to emulate the style of model-based SRAFs while further improving the process variation (PV) band. A state alignment and action transformation mechanism is proposed to achieve orientation equivariance while expediting the training process. We also propose a transfer learning framework, allowing SRAF generation under different light sources without retraining the model. Compared with state-of-the-art works, our method improves the solution quality in terms of PV band and edge placement error (EPE) while reducing the overall runtime.","1558-2434","978-1-4503-9217-4","","National Science and Technology Council; Synopsys; MediaTek; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10069974","Design for Manufacturability;Sub-Resolution Assist Feature;Markov Decision Process;Reinforcement Learning;Transfer Learning","Training;Runtime;Manufacturing processes;Transfer learning;Layout;Metals;Interference","masks;photolithography;proximity effect (lithography);reinforcement learning","action transformation mechanism;feature generation;key resolution enhancement technique;machine learning;manufacturing process window;mask-rule-compliant results;model-based SRAFs;modern photolithography feature sizes;optical interference;process variation band;reinforcement learning;SRAF generation;SRAF interference;SRAF mask manufacturing rules;state alignment;training process;transfer learning framework;two-phase learning","","","","19","","22 Mar 2023","","","IEEE","IEEE Conferences"
"A novel decision-making algorithm for beyond visual range air combat based on deep reinforcement learning","Y. Jiang; J. Yu; Q. Li","School of Automation Science and Electrical Engineering, Science and Technology on Aircraft Control Laboratory Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Science and Technology on Aircraft Control Laboratory Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Science and Technology on Aircraft Control Laboratory Beihang University, Beijing, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","516","521","In this paper, a novel decision-making algorithm based on deep reinforcement learning(DRL) is proposed for the decision-making problem in beyond visual range(BVR) air combat. Firstly, the relative kinematics model of 1 vs 1 air combat is established, and the state space and action space of the fighter are designed. Then, a new reward function is designed according to the situation of the BVR air combat, which is suitable for a wider range of BVR confrontation scenarios, and the construction of a decision-making method based on DRL is completed. Finally, several sets of experimental data are given to verify the effectiveness of the algorithm.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023870","deep reinforcement learning;beyond visual range air combat;decision-making algorithm","Deep learning;Visualization;Adaptation models;Automation;Atmospheric modeling;Decision making;Reinforcement learning","decision making;deep learning (artificial intelligence);military aircraft;military computing;reinforcement learning","action space;BVR air combat;BVR confrontation scenarios;decision-making problem;deep reinforcement learning;relative kinematics model;reward function;state space;visual range air combat","","1","","19","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Mobile Robot Navigation in Unknown Environment with Continuous Action Space","P. Phueakthong; J. Varagul; N. Pinrath","School of Mechatronics Engineering, Suranaree University of Technology, Nakhon Ratchasima, Thailand; School of Manufacturing Automation and Robotics Engineering Suranaree University of Technology, Nakhon Ratchasima, Thailand; School of Industrial Engineering Suranaree University of Technology, Nakhon Ratchasima, Thailand","2022 5th International Conference on Intelligent Autonomous Systems (ICoIAS)","8 Nov 2022","2022","","","154","158","This work aims to propose the use of deep reinforcement learning for mobile robot navigation and obstacle avoidance in previously unknown areas or without pre-made maps with continuous action control to increase the capabilities of mobile robots beyond conventional map-based navigation. Deep reinforcement learning is used to enable the robot to learn how to make decisions and interact with the environment observed from sensor data to safely navigate itself to its destination. The robot has a two-dimensional laser scanner, ultrasonic sensors and odometry sensor. Deep Deterministic Policy Gradient, which can function in continuous action space, was chosen as the deep reinforcement learning model. The robot is trained and tested in a Gazebo simulator with Robot Operating System. After the training process, the robot is put to the challenge to complete a waypoint navigation mission in four unknown areas as part of an assessment. The results indicate that the mobile robot is adaptable and has the capability of traveling to the specified waypoints and completing the job without the need for a pre-drawn route or an obstacle map in unidentified environments with the minimum success rate of 69.7 percent.","","978-1-6654-9838-8","10.1109/ICoIAS56028.2022.9931272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931272","deep reinforcement learning;autonomous navigation;DDPG","Training;Navigation;Autonomous systems;Operating systems;Reinforcement learning;Robot sensing systems;Laser modes","collision avoidance;control engineering computing;deep learning (artificial intelligence);mobile robots;navigation;reinforcement learning;robot programming","continuous action control;continuous action space;deep deterministic policy gradient;deep reinforcement learning model;Gazebo simulator;mobile robot navigation;obstacle avoidance;odometry sensor;robot operating system;training process;two-dimensional laser scanner;ultrasonic sensors;unknown areas;unknown environment;waypoint navigation mission","","1","","16","IEEE","8 Nov 2022","","","IEEE","IEEE Conferences"
"AMI: Adaptive Motion Imitation Algorithm Based on Deep Reinforcement Learning","N. Taghavi; M. H. A. Alqatamin; D. O. Popa","Louisville Automation & Robotics Research Institute, University of Louisville, KY; Louisville Automation & Robotics Research Institute, University of Louisville, KY; Louisville Automation & Robotics Research Institute, University of Louisville, KY","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","798","804","In this paper, we develop a novel adaptive motion imitation algorithm (AMI) for robotic systems. Although AMI can be used in a variety of human-robot interaction scenarios, we are particularly interested in robotic rehabilitation where the robot plays the role of demonstrating and practicing challenging motion physiotherapy. During therapy, the robot first demonstrates a reference trajectory to the patient that needs to be repeated during practice and then adapts its motion to a cyclic speed and amplitude based on the patient's abilities. Using this algorithm, the robotic system learns an upper-body motion of the human user and performs a unique, similar, and easier motion based on the learned trajectory from the user. Adaptation in the AMI is based on deep reinforcement learning with deep deterministic policy gradient implemented in the Robot Operating System (ROS) environment. Experimental data collected from 11 users during upper body human-robot imitation sessions with social robot Zeno was used to show that the algorithm can learn reference elbow joint trajectories of the user in an off-line manner after just a few cycles. Finally, we also implemented the algorithm online using the Baxter robot to demonstrate its learning and playback performance.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812121","National Science Foundation(grant numbers:1838808); EPSCoR(grant numbers:1849213); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812121","","Automation;Adaptive systems;Operating systems;Social robots;Medical treatment;Reinforcement learning;Trajectory","biomechanics;humanoid robots;human-robot interaction;image motion analysis;learning (artificial intelligence);medical robotics;mobile robots;patient rehabilitation;patient treatment","AMI;deep reinforcement learning;novel adaptive motion imitation algorithm;robotic system;human-robot interaction scenarios;robotic rehabilitation;challenging motion physiotherapy;reference trajectory;patient;upper-body motion;human user;unique motion;similar, motion;easier motion;learned trajectory;deep deterministic policy gradient;Robot Operating System environment;upper body human-robot imitation sessions;social robot Zeno;reference elbow joint trajectories;Baxter robot;playback performance","","1","","23","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Efficient Policy Transfer in Large-Scale Traffic Light Control via Multi-Agent Hierarchical Reinforcement Learning","C. Li; H. Yan; Q. Zhao","Dept Automation, Center for Intelligent and Networked System, Tsinghua University, Beijing, China; Dept Automation, Center for Intelligent and Networked System, Tsinghua University, Beijing, China; Dept Automation, Center for Intelligent and Networked System, Tsinghua University, Beijing, China","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","6","Multi-agent reinforcement learning (MARL) is increasingly being used for traffic light control in traffic networks. However, applying MARL to large-scale traffic light control faces challenges due to the time-consuming and resource-intensive nature of learning in such environments. This paper addresses this challenge by leveraging transfer learning, utilizing policies trained in small-scale environments to effectively control large-scale traffic light systems. To enhance the zero-shot transferability of trained policies, we introduce communication channels and sub-task decomposition, while ensuring transferability by sharing all neural network parameters. In our experiments, we assess the zero-shot transferability of policies trained in different small-scale traffic networks, both with and without sub-task decomposition. The results demonstrate the significance of establishing a hierarchical structure through sub-task decomposition for zero-shot transfer.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260400","National Natural Science Foundation of China(grant numbers:62192751); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260400","","Training;Computer aided software engineering;Automation;Transfer learning;Neural networks;Reinforcement learning;Communication channels","control engineering computing;lighting control;multi-agent systems;reinforcement learning;road traffic control","large-scale traffic light control;large-scale traffic light systems;MARL;multiagent hierarchical reinforcement learning;neural network parameter;policy transfer;small-scale environments;small-scale traffic networks;subtask decomposition;trained policies;transfer learning;zero-shot transferability","","","","16","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Adaptive Transient Stepping Policy on Reinforcement Learning","L. Xu; D. Niu; Y. Yang; A. Wang; Z. Jin; Y. Dong; C. Zhang","School of Microelectronics, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Microelectronics, Southeast University, Nanjing, China; School of Information Science and Engineering, China University of Petroleum-Beijing, Beijing, Beijing, China; School of Automation, Southeast University, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China","2023 International Symposium of Electronics Design Automation (ISEDA)","25 Aug 2023","2023","","","46","51","Transient analysis (TA) is the foundation for nonlinear electronic circuit simulation, which determines the time-domain response of the circuit over a specified time interval. However, it is usually computationally intensive and quite time-consuming without careful parameter tuning and proper stepping policy. In this paper, reinforcement learning (RL) is introduced to design a effective transient stepping policy (TSP). The online RL-based TSP works with bidirectional agents and state-switch samples, enabling our model to adaptively and intelligently adjust the bidirectional step sizes in TA. The proposed RL-based TSP has been implemented in an open source SPICE-like simulator, and has achieved remarkable simulation efficiency speedup while maintaining reasonable simulation accuracy.","","979-8-3503-0451-0","10.1109/ISEDA59274.2023.10218535","Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20202006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10218535","Transient analysis;stepping policy;LTE;rein-forcement learning","Adaptation models;Design automation;Computational modeling;Reinforcement learning;Transient analysis;Time-domain analysis;Tuning","circuit simulation;reinforcement learning;SPICE;time-domain analysis;transient analysis","adaptive transient stepping policy;bidirectional agents;bidirectional step sizes;nonlinear electronic circuit simulation;online RL-based TSP;parameter tuning;reinforcement learning;simulation accuracy;simulation efficiency speedup;SPICE-like simulator;state-switch samples;time-domain response;transient analysis","","","","18","IEEE","25 Aug 2023","","","IEEE","IEEE Conferences"
"Towards Autonomous Robotic Ultrasound Scanning Using the Reinforcement Learning-Based Volumetric Data Navigation Method","C. Shen; Z. Deng; J. Wang; S. Wang; C. Chen","Department of Mechanics and Engineering Science, College of Engineering, State Key Laboratory for Turbulence and Complex Systems, Peking University, Beijing, P. R. China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Department of Mechanics and Engineering Science, College of Engineering, State Key Laboratory for Turbulence and Complex Systems, Peking University, Beijing, P. R. China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2023 WRC Symposium on Advanced Robotics and Automation (WRC SARA)","27 Sep 2023","2023","","","335","340","Ultrasound scanning is an indispensable diagnostic tool in modern medicine, but obtaining an accurate standard plane relies heavily on the sonographer’s skill and expertise. This paper presents a novel approach toward autonomous robotic ultrasound scanning using a reinforcement learning-based navigation method with volumetric ultrasound data. The proposed method enables an ultrasound robot to autonomously acquire high-quality 2D standard planes from patients, thereby reducing the need for human intervention. The core of the approach is a deep reinforcement learning algorithm that detects 2D standard planes within the 3D ultrasound volume and guides the ultrasound probe to extract high-quality standard planes. To assess the effectiveness of the 2D standard plane detection method, an evaluation was conducted using a realworld ultrasound dataset, and a comparison was made between its performance and that of other state-of-the-art autonomous approaches. The simulation experiment was also conducted to validate the autonomous robotic ultrasound acquisition workflow. Results demonstrate that the reinforcement learning-based volumetric data navigation method outperforms existing techniques in terms of standard plane accuracy and is very effective for volumetric data-based robotic navigation.","2835-3358","979-8-3503-0732-0","10.1109/WRCSARA60131.2023.10261822","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261822","medical robotics;autonomous robotic ultrasound;deep reinforcement learning","Deep learning;Ultrasonic imaging;Three-dimensional displays;Medical robotics;Automation;Navigation;Reinforcement learning","biomedical ultrasonics;deep learning (artificial intelligence);medical image processing;mobile robots;navigation;path planning;reinforcement learning","2D standard plane detection method;accurate standard plane;autonomous robotic ultrasound acquisition workflow;deep reinforcement learning algorithm;high-quality 2D standard planes;high-quality standard planes;realworld ultrasound dataset;reinforcement learning-based navigation method;reinforcement learning-based volumetric data navigation method;standard plane accuracy;state-of-the-art autonomous approaches;towards autonomous robotic ultrasound scanning;ultrasound probe;ultrasound robot;volumetric data-based robotic navigation;volumetric ultrasound data","","","","21","IEEE","27 Sep 2023","","","IEEE","IEEE Conferences"
"A plume-tracing strategy via continuous state-action reinforcement learning","L. Niu; S. Song; K. You","Dept. of Automation, Tsinghua University, Beijing, China; Dept. of Automation, Tsinghua University, Beijing, China; Dept. of Automation, Tsinghua University, Beijing, China","2017 Chinese Automation Congress (CAC)","1 Jan 2018","2017","","","759","764","This paper proposes plume-tracing strategy for an autonomous underwater vehicle (AUV) in the deep-sea environment. In order to dynamically adapt the complex environment and optimize the policy during interaction, reinforcement learning (RL) with continuous state and action domain is applied in this problem. Different from traditional strategies which have predesigned and stationary actions, this learning-based approach can smooth the searching trajectory and reduce the risk of losing plume. To achieve this, this paper models the tracing problem as a Markov decision process (MDP) with unknown transition matrix. Continuous temporal difference and deterministic policy gradient method are used to estimate and improve the policy. Moreover, supervised initialization, reward shaping and modified exploration technology are proposed to accelerate learning. The effectiveness and efficiency of the proposed strategy is validated in the simulation experiment.","","978-1-5386-3524-7","10.1109/CAC.2017.8242868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8242868","","Vents;Chemicals;Learning (artificial intelligence);Trajectory;Acceleration;Automation;Electronic mail","autonomous underwater vehicles;control engineering computing;gradient methods;learning (artificial intelligence);marine control;Markov processes;matrix algebra;mobile robots;trajectory control","stationary actions;AUV;policy optimization;RL;Markov decision process;MDP;unknown transition matrix;learning-based approach;searching trajectory;complex environment;deep-sea environment;autonomous underwater vehicle;continuous state-action reinforcement learning;plume-tracing strategy;deterministic policy gradient method;continuous temporal difference","","","","17","IEEE","1 Jan 2018","","","IEEE","IEEE Conferences"
"Analysis of Evolutionary Dynamics for Bidding Strategy Driven by Multi-Agent Reinforcement Learning","Z. Zhu; K. W. Chan; S. Bu; S. W. Or; X. Gao; S. Xia","Hong Kong Branch of National Rail Transit Electrification and Automation Engineering Technology Research Center, and Department of Electrical Engineering, The Hong Kong Polytechnic University, Hong Kong; Department of Electrical Engineering, The Hong Kong Polytechnic University, Hong Kong; Department of Electrical Engineering, The Hong Kong Polytechnic University, Hong Kong; Hong Kong Branch of National Rail Transit Electrification and Automation Engineering Technology Research Center, and Department of Electrical Engineering, The Hong Kong Polytechnic University, Hong Kong; Department of Electrical Engineering, The Hong Kong Polytechnic University, Hong Kong; State Key Laboratory of Alternate Electrical Power System with Renewable Energy Sources, School of Electrical and Electronic Engineering, North China Electric Power University, Beijing, China","IEEE Transactions on Power Systems","19 Oct 2021","2021","36","6","5975","5978","In this letter, the evolutionary game theory (EGT) with replication dynamic equations (RDEs) is adopted to explicitly determine the factors affecting energy providers’ (EPs) willingness of using the market power to uplift the price in the bidding procedure, which could be simulated using the win-or-learn-fast policy hill climbing (WoLF-PHC) algorithm as a multi-agent reinforcement learning (MARL) method. Firstly, empirical and numerical connections between WoLF-PHC and RDEs is proved. Then, by formulating RDEs of the bidding procedure, three factors affecting the bidding strategy preference are revealed, including the load demand, severity of congestion, and the price cap. Finally, the impact of these factors on the converged bidding price is demonstrated in case studies, by simulating the bidding procedure driven by WoLF-PHC.","1558-0679","","10.1109/TPWRS.2021.3099693","Research Grants Council of the HKSAR Government(grant numbers:R5020-18); Innovation and Technology Commission; Hong Kong Branch of National Rail Transit Electrification and Automation Engineering Technology Research Center(grant numbers:K-BBY1); National Natural Science Foundation of China(grant numbers:52077075); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9496217","Market power;multi-agent reinforcement learning;evolutionary game theory;bidding strategy","Power system dynamics;Game theory;Multi-agent systems;Stability criteria;Reinforcement learning","evolutionary computation;game theory;learning (artificial intelligence);multi-agent systems;pricing;stock markets;tendering","RDEs;bidding strategy preference;bidding price;WoLF-PHC;evolutionary dynamics;bidding strategy driven;evolutionary game theory;replication dynamic equations;energy providers;market power;multiagent reinforcement learning method;empirical connections;numerical connections;win-or-learn-fast policy hill climbing","","4","","10","IEEE","26 Jul 2021","","","IEEE","IEEE Journals"
"Influence of Sensor Noise and Latency on Navigational Safety of Deep-Reinforcement-Learning-based Planners","S. Liu; A. T. Nguyen; K. Wang; J. Jiang; C. Liu; L. Kästner; J. Lambrecht","Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Department of Computer Science, Artificial Intelligence Group, Humboldt University of Berlin; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany","2022 IEEE/SICE International Symposium on System Integration (SII)","16 Feb 2022","2022","","","285","290","Recently, mobile robots have become important tools in various industries, like logistics, healthcare, and delivery. Deep Reinforcement Learning (DRL) emerged as an end-to-end approach, which maps raw laser scan observations to robot actions and promises flexible and efficient navigation. Various works have incorporated DRL for navigation in highly dynamic environments. However, these approaches are often trained within simulation with optimal assumptions and perfect modeling of observations, which makes the simulationto-reality gap a relevant issue. In this work, we evaluate the impact of noise and delay modules on the performance of DRL-based navigation approaches. Therefore, we introduce noise and delay modules into a 2D simulation environment - arena-rosnav, which makes it feasible to train and test DRL-based approaches under realistic circumstances. Subsequently, we evaluate the approaches with the different noise modules extensively and demonstrate a strong correlation between the level of noise and the success rate. The results could be valuable to design safe DRL-based navigation approaches.","2474-2325","978-1-6654-4540-5","10.1109/SII52469.2022.9708738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9708738","","Navigation;Service robots;System integration;Reinforcement learning;Medical services;Robot sensing systems;Delays","computer simulation;deep learning (artificial intelligence);mobile robots;navigation;path planning;reinforcement learning;robot programming","sensor noise;navigational safety;mobile robots;end-to-end approach;robot actions;delay modules;DRL based navigation;deep reinforcement learning based planners;robot navigation;2D simulation environment;arena-rosnav","","","","20","IEEE","16 Feb 2022","","","IEEE","IEEE Conferences"
"A reinforcement learning approach to network routing based on adaptive learning rates and route memory","M. Kavalerov; Y. Likhacheva; Y. Shilova","Department of Automation and Telemechanics, Perm National Research Polytechnic University, Perm, Russia; Department of Automation and Telemechanics, Perm National Research Polytechnic University, Perm, Russia; Department of Automation and Telemechanics, Perm National Research Polytechnic University, Perm, Russia","SoutheastCon 2017","11 May 2017","2017","","","1","6","Mobile ad hoc networks may dynamically change their topology and system parameters. They require efficient routing techniques that provide reasonably low delivery times for packets even under high loads. A routing algorithm based on Full Echo Q-routing scheme is proposed. It uses adaptive learning rates and route memory to reduce instability of routing under high load conditions and improve performance in terms of overshoot and settling time of the learning.","1558-058X","978-1-5386-1539-3","10.1109/SECON.2017.7925316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7925316","mobile ad hoc networks;irregular grid network;routing;delivery time;learning time;overshoot;Q-routing;full echo;multi-agent modeling","Routing;Oscillators;Heuristic algorithms;Learning (artificial intelligence);Adaptive systems;Ad hoc networks;Load modeling","learning (artificial intelligence);mobile ad hoc networks;telecommunication computing;telecommunication network routing;telecommunication network topology","reinforcement learning approach;network routing;adaptive learning rates;route memory;mobile ad hoc networks;topology;system parameters;routing algorithm;full echo q-routing scheme","","8","","16","IEEE","11 May 2017","","","IEEE","IEEE Conferences"
"Learning to Solve Nonlinear Optimization Problem with Deep Reinforcement Learning","Y. Gao; Q. Yang; H. Wu; M. Sun","MoE Key Lab of Artificial Intelligence and AI Institute, Shanghai Jiao Tong University, Shanghai, P.R. China; Department of Automation, Shanghai Jiao Tong University, Shanghai, P.R. China; Department of Automation, Shanghai Jiao Tong University, Shanghai, P.R. China; Department of Automation, Shanghai Jiao Tong University, Shanghai, P.R. China","2022 IEEE International Conference on Robotics and Biomimetics (ROBIO)","18 Jan 2023","2022","","","909","914","Nonlinear least-squares problems (NLS) are pop-ular in engineering and scientific fields. Traditional optimization methods such as Newton's method and Gaussian-Newton method (GN) suffer from the sensibility to initial values and the high computational complexity. In this paper, we propose LS-DDPG, a robust optimization method utilizing deep rein-forcement learning algorithms to solve nonlinear least-squares problems. The experiment results on synthetic data demonstrate that the proposed method outperforms Newton's method in terms of computation cost, convergence speed and initial values sensibility. In addition, LS-DDPG is utilized on model predictive control (MPC) problems for trajectory planning and tracking tasks in self-driving with longer prediction horizon and higher accuracy than baseline methods.","","978-1-6654-8109-0","10.1109/ROBIO55434.2022.10011977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011977","","Trajectory planning;Optimization methods;Reinforcement learning;Robustness;Newton method;Task analysis;Robots","deep learning (artificial intelligence);least squares approximations;mathematics computing;nonlinear programming;reinforcement learning","deep reinforcement learning;LS-DDPG;NLS;nonlinear least-squares;nonlinear optimization","","","","15","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"Learn Effective Representation for Deep Reinforcement Learning","Y. Zhan; Z. Xu; G. Fan","Institute of Automation, Chinese Academy of Sciences; Institute of Automation, Chinese Academy of Sciences; Institute of Automation, Chinese Academy of Sciences","2022 IEEE International Conference on Multimedia and Expo (ICME)","26 Aug 2022","2022","","","1","6","Recent years have witnessed an increasing application of deep reinforcement learning (DRL) on video games. While deeper and wider neural networks have played a crucial role in computer vision and natural language processing, such capacity remain under-explored in most DRL works. Under the fact that feature propagation together with large networks contributes to learning a good representation, we propose an end-to-end Large Feature Extractor Network (LFENet) that uses large neural networks with dense connections to train a high-capacity encoder. Even though the increased dimensionality of input is usually thought to result in poor performance for RL agents, we introduce the information bottleneck to alleviate the problem. Finally, we combine LFENet with Proximal Policy Optimization (PPO) algorithm. Through numerical experiments on Atari 2600 video games, we demonstrate our method matches or outperforms state-of-the-art algorithms.","1945-788X","978-1-6654-8563-0","10.1109/ICME52920.2022.9859768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9859768","Representation Learning;Reinforcement Learning;Video Game","Representation learning;Computer vision;Neural networks;Games;Reinforcement learning;Feature extraction;Natural language processing","computer games;deep learning (artificial intelligence);feature extraction;neural nets;optimisation;reinforcement learning","deep reinforcement learning;computer vision;natural language processing;DRL works;feature propagation;feature extractor network;Atari 2600 video games;proximal policy optimization;PPO algorithm","","","","28","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"Self-optimization of energy consumption in complex bulk good processes using reinforcement learning","D. Schwung; T. Kempe; A. Schwung; S. X. Ding","South Westfalia University of Applied Sciences, Soest, Germany; South Westfalia University of Applied Sciences, Soest, Germany; South Westfalia University of Applied Sciences, Soest, Germany; University of Duisburg-Essen, Duisburg, Germany","2017 IEEE 15th International Conference on Industrial Informatics (INDIN)","13 Nov 2017","2017","","","231","236","This paper presents a novel approach to the optimization of energy consumption in large scale industrial bulk good processes. The approach is based on a model-free self-learning algorithm solely based on available process data using ideas from the well known reinforcement learning framework. To this end energy consumers of the plant are integrated in the optimization framework such that each consumer learns its own optimal energy profile for a given production task. The approach is implemented on a laboratory size testbed where the task is the supply of bulk good to a subsequent dosing section. The capability of the approach is underlined by the results obtained at the testbed.","2378-363X","978-1-5386-0837-1","10.1109/INDIN.2017.8104776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8104776","","Production;Energy consumption;Optimization;Actuators;Industries;Learning (artificial intelligence);Energy management","energy consumption;goods distribution;learning (artificial intelligence);production engineering computing","energy consumption;model-free self-learning algorithm;optimization framework;reinforcement learning framework;industrial bulk good processes;production task;energy consumers;optimal energy profile","","10","","18","IEEE","13 Nov 2017","","","IEEE","IEEE Conferences"
"Residual Reinforcement Learning for Robot Control","T. Johannink; S. Bahl; A. Nair; J. Luo; A. Kumar; M. Loskyll; J. A. Ojea; E. Solowjow; S. Levine","Hamburg University of Technology; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; Siemens Corporation; Siemens Corporation; Siemens Corporation; Siemens Corporation; University of California, Berkeley","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","6023","6029","Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8794127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794127","","Robots;Task analysis;Feedback control;Reinforcement learning;Mathematical model;Manufacturing;Adaptive control","continuous systems;control system synthesis;feedback;friction;industrial robots;learning (artificial intelligence);learning systems;mechanical contact;motion control;robot dynamics","first-order physical modeling;brittle controllers;inaccurate controllers;reinforcement learning methods;continuous robot controllers;control signals;robot control problems;modern manufacturing;control design;feedback control methods;control policy;residual reinforcement learning;rigid body equations of motion;contacts;friction;robot learning;unstable objects;block assembly task","","128","","32","IEEE","12 Aug 2019","","","IEEE","IEEE Conferences"
"Socially Compliant Navigation in Indoor Corridors Based on Reinforcement Learning","C. -H. G. Li; Y. -H. Chang","Graduate Institute of Manufacturing Technology, National Taipei University of Technology, Taipei, Taiwan ROC; Graduate Institute of Manufacturing Technology, National Taipei University of Technology, Taipei, Taiwan ROC","2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)","5 Oct 2021","2021","","","1985","1990","In this paper, the policy function of a mobile robot navigating in indoor corridor environments was obtained through reinforcement learning (RL). Assuming the scenario where right-passing rules are enforced, target paths associated with different corridor widths were defined; the robot's actions of navigating into the target paths were defined as RL rewards to encourage the common social consensus regarding corridor passing. Specifically, the robot was trained to render proper reactions according to the width of the corridor and the robot's speed, pose, and relative position in the corridor. The RL models were trained in Gazebo and ROS; the effectiveness of the navigation policy was validated by various tests of different conditions. It was found that different speeds need different strategies; the RL models trained for each specific speed category appear to be optimal. Such results were supported by the cross-examinations on the success rate and the number of corrective actions.","2161-8089","978-1-6654-1873-7","10.1109/CASE49439.2021.9551393","National Taipei University of Technology; King Mongkut's University of Technology Thonburi(grant numbers:NTUT-KMUTT-107-02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551393","","Computer aided software engineering;Automation;Navigation;Conferences;Reinforcement learning;Mobile robots","learning (artificial intelligence);mobile robots;navigation","RL rewards;right-passing rules;indoor corridor environments;mobile robot;policy function;reinforcement learning;compliant navigation;RL models;common social consensus","","","","12","IEEE","5 Oct 2021","","","IEEE","IEEE Conferences"
"Autonomous Navigation in Complex Environments using Memory-Aided Deep Reinforcement Learning","L. Kästner; Z. Shen; C. Marx; J. Lambrecht","Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany","2021 IEEE/SICE International Symposium on System Integration (SII)","24 Mar 2021","2021","","","170","175","Mobile robots have gained increased importance within industrial tasks such as commissioning, delivery or operation in hazardous environments. The ability to navigate in unknown and complex environments is paramount in industrial robotics. Reinforcement learning approaches have shown remarkable success in dealing with unknown situations and react accordingly without manually engineered guidelines and overconservative measures. However, these approaches are often restricted to short range navigation and are prone to local minima due to a lack of a memory module. Thus, the navigation in complex environments such as mazes, long corridors or concave areas is still an open frontier. In this paper, we incorporate a variety of recurrent neural networks to cope with these challenges. We train a reinforcement learning based agent within a 2D simulation environment of our previous work and extend it with a memory module. The agent is able to navigate solely on sensor data observations which are directly mapped to actions. We evaluate the performance on different complex environments and achieve enhanced results within complex environments compared to memory-free baseline approaches.","2474-2325","978-1-7281-7658-1","10.1109/IEEECONF49454.2021.9382661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382661","","Training;Navigation;Two dimensional displays;Reinforcement learning;Memory modules;System integration;Task analysis","control engineering computing;deep learning (artificial intelligence);industrial robots;mobile robots;path planning;production engineering computing","memory-aided deep reinforcement learning;complex environments;2D simulation environment;industrial robotics;mobile robots;autonomous navigation","","","","22","IEEE","24 Mar 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Laser Welding Speed Control Minimizing Bead Width Error","T. Kaneko; G. Minamoto; Y. Hirose; T. Sakai","Media AI Laboratory, Corporate Research and Development Center, Toshiba Corporation, Kawasaki, Japan; Media AI Laboratory, Corporate Research and Development Center, Toshiba Corporation, Kawasaki, Japan; Optics & Inspection Technology Research Department, Corporate Manufacturing Engineering Center, Toshiba Corporation, Yokohama, Japan; Optics & Inspection Technology Research Department, Corporate Manufacturing Engineering Center, Toshiba Corporation, Yokohama, Japan","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","12275","12281","In this paper, we propose a method for reinforcement learning-based laser welding control. Conventional methods apply standard reinforcement learning formulations to welding tasks, but we show that this formulation can minimize bead width or penetration depth errors only when the welding speed is constant. Therefore, conventional methods are suboptimal for training control parameters including the welding speed. The proposed method discounts future rewards with respect to the welding length instead of time steps to solve this issue. This is easily implemented by (1) modifying the discount factor used for $Q$-function updates in existing reinforcement learning algorithms and (2) using an appropriate reward function. Experimental results using simulators show that the proposed method achieves performance that is superior to conventional methods.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161334","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161334","","Training;Automation;Welding;Lasers;Velocity control;Reinforcement learning;Task analysis","control engineering computing;laser beam welding;reinforcement learning;velocity control;welds","bead width error;discount factor;laser welding speed control;penetration depth errors;reinforcement learning algorithms;reward function","","","","20","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Reset-Free Reinforcement Learning via Multi-Task Learning: Learning Dexterous Manipulation Behaviors without Human Intervention","A. Gupta; J. Yu; T. Z. Zhao; V. Kumar; A. Rovinsky; K. Xu; T. Devlin; S. Levine",UC Berkeley; UC Berkeley; UC Berkeley; University of Washington; UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley,"2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","6664","6671","Reinforcement Learning (RL) algorithms can in principle acquire complex robotic skills by learning from large amounts of data in the real world, collected via trial and error. However, most RL algorithms use a carefully engineered setup in order to collect data, requiring human supervision and intervention to provide episodic resets. This is particularly evident in challenging robotics problems, such as dexterous manipulation. To make data collection scalable, such applications require reset-free algorithms that are able to learn autonomously, without explicit instrumentation or human intervention. Most prior work in this area handles single-task learning. However, we might also want robots that can perform large repertoires of skills. At first, this would appear to only make the problem harder. However, the key observation we make in this work is that an appropriately chosen multi-task RL setting actually alleviates the reset-free learning challenge, with minimal additional machinery required. In effect, solving a multi-task problem can directly solve the reset-free problem since different combinations of tasks can serve to perform resets for other tasks. By learning multiple tasks together and appropriately sequencing them, we can effectively learn all of the tasks together reset-free. This type of multi-task learning can effectively scale reset-free learning schemes to much more complex problems, as we demonstrate in our experiments. We propose a simple scheme for multi-task learning that tackles the reset-free learning problem, and show its effectiveness at learning to solve complex dexterous manipulation tasks in both hardware and simulation without any explicit resets. This work shows the ability to learn in-hand manipulation behaviors in the real world with RL without any human intervention.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561384","Office of Naval Research; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561384","","Sequential analysis;Automation;Instruments;Conferences;Reinforcement learning;Data collection;Hardware","dexterous manipulators;reinforcement learning","human intervention;single-task learning;multitask RL;multitask learning;explicit resets;reset-free reinforcement;complex robotic skills;human supervision;episodic resets;reset-free learning;complex dexterous manipulation","","14","","62","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Evaluation of a Reinforcement Learning Algorithm for Vascular Intervention Surgery","F. Meng; S. Guo; W. Zhou; Z. Chen","Key Laboratory of Convergence Biomedical Engineering System and Healthcare Technology, The Ministry of Industry and Information Technology, School of Life Science, Beijing Institute of Technology, Beijing, China; Key Laboratory of Convergence Biomedical Engineering System and Healthcare Technology, The Ministry of Industry and Information Technology, School of Life Science, Beijing Institute of Technology, Beijing, China; Key Laboratory of Convergence Biomedical Engineering System and Healthcare Technology, The Ministry of Industry and Information Technology, School of Life Science, Beijing Institute of Technology, Beijing, China; Key Laboratory of Convergence Biomedical Engineering System and Healthcare Technology, The Ministry of Industry and Information Technology, School of Life Science, Beijing Institute of Technology, Beijing, China","2021 IEEE International Conference on Mechatronics and Automation (ICMA)","27 Aug 2021","2021","","","1033","1037","With the increasing use of vascular interventions, catheter navigation in complex vessels has become even more critical. Vascular intervention surgeries also require more precise manipulation and a more intelligent system to ensure the safety of the patients. In this paper, a virtual training model based on deep reinforcement learning was designed to navigate the catheter into the aortic arch. The whole experiment was carried out in a virtual environment, and a reinforcement learning method was used to test the performance of catheter autonomous navigation in vessels. Finally, the model was successfully trained and results were analyzed basing on previous work. The results obtained would be more convincing if the model was more complex and closer to the actual vessels.","2152-744X","978-1-6654-4101-8","10.1109/ICMA52036.2021.9512675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512675","Vascular interventional surgery;Reinforcement learning;Catheter navigation","Training;Mechatronics;Navigation;Heart beat;Virtual environments;Surgery;Reinforcement learning","blood vessels;catheters;control system synthesis;intelligent control;learning systems;manipulators;medical robotics;navigation;surgery","vascular intervention surgery;catheter navigation;complex vessels;precise manipulation;virtual training model;deep reinforcement learning;catheter autonomous navigation;intelligent system;patient safety;medical robots","","3","","24","IEEE","27 Aug 2021","","","IEEE","IEEE Conferences"
"Automating Reinforcement Learning With Example-Based Resets","J. Kim; J. h. Park; D. Cho; H. J. Kim","Department of Mechanical and Aerospace Engineering, Seoul National University, Seoul, South Korea; Department of Mechanical and Aerospace Engineering, Seoul National University, Seoul, South Korea; Department of Mechanical and Aerospace Engineering, Seoul National University, Seoul, South Korea; Department of Mechanical and Aerospace Engineering, Seoul National University, Seoul, South Korea","IEEE Robotics and Automation Letters","25 May 2022","2022","7","3","6606","6613","Deep reinforcement learning has enabled robots to learn motor skills from environmental interactions with minimal to no prior knowledge. However, existing reinforcement learning algorithms assume an episodic setting, in which the agent resets to a fixed initial state distribution at the end of each episode, to successfully train the agents from repeated trials. Such reset mechanism, while trivial for simulated tasks, can be challenging to provide for real-world robotics tasks. Resets in robotic systems often require extensive human supervision and task-specific workarounds, which contradicts the goal of autonomous robot learning. In this paper, we propose an extension to conventional reinforcement learning towards greater autonomy by introducing an additional agent that learns to reset in a self-supervised manner. The reset agent preemptively triggers a reset to prevent manual resets and implicitly imposes a curriculum for the forward agent. We apply our method to learn from scratch on a suite of simulated and real-world continuous control tasks and demonstrate that the reset agent successfully learns to reduce manual resets whilst also allowing the forward policy to improve gradually over time.","2377-3766","","10.1109/LRA.2022.3173039","Korea Medical Device Development Fund; Ministry of Science and ICT; Ministry of Trade, Industry and Energy; Ministry of Health & Welfare; Ministry of Food and Drug Safety(grant numbers:202013D05); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770380","Deep learning methods;reinforcement learning;autonomous agents;incremental learning","Task analysis;Robots;Training;Reinforcement learning;Manuals;Autonomous robots;Trajectory tracking","control engineering computing;deep learning (artificial intelligence);reinforcement learning;robot programming","real-world continuous control tasks;reset agent;manual resets;example-based resets;deep reinforcement learning;motor skills;environmental interactions;fixed initial state distribution;repeated trials;reset mechanism;real-world robotics tasks;robotic systems;autonomous robot learning;forward agent","","1","","30","IEEE","6 May 2022","","","IEEE","IEEE Journals"
"Self-Organized Routing for Autonomous Vehicles via Deep Reinforcement Learning","H. Pei; J. Zhang; Y. Zhang; H. Xu; L. Li","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, BNRist, Tsinghua University, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Automation, BNRist, Tsinghua University, Beijing, China","IEEE Transactions on Vehicular Technology","","2023","PP","99","1","12","Routing for autonomous vehicles with global traffic information and sufficient direct cooperation among vehicles has been widely studied to relieve traffic congestion in recent years. However, the assembly rate of Vehicle-to-Everything (V2X) equipment in practical traffic systems is currently and could be at a low level in near future. Accordingly, autonomous vehicles can only access localized traffic information, and direct cooperation among them cannot always be guaranteed. Thus, how to optimize the routing choices in such scenarios is worthy of particular attention. In this paper, we propose a self-organized routing strategy based on deep reinforcement learning (DRL). Under the condition of limited traffic information, the proposed self-organized mechanism well organizes localized traffic conditions through vehicle-level routing decisions, which are able to achieve network-wide benefits gains. In the specified DRL, we propose a novel reward mechanism to harmonize indirect interactions among vehicles by jointly learning individual and overall efficiency, even if each vehicle is modified to make individual decisions independently, rather than only focusing on individual interests as in the greedy strategy. Numerical experiments demonstrate that the proposed self-organized strategy is promising to resolve the routing problem from the perspective of individual decision-making with limited traffic information.","1939-9359","","10.1109/TVT.2023.3311198","National Key Research and Development Program of China(grant numbers:2020AAA0108104); National Natural Science Foundation of China(grant numbers:52272420); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10244078","Routing;self-organized;deep reinforcement learning;autonomous vehicle","Routing;Autonomous vehicles;Vehicle-to-everything;Vehicle dynamics;Estimation;Automation;Traffic congestion","","","","","","","IEEE","8 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Optimization of Apparel Supply Chain Using Deep Reinforcement Learning","J. W. Chong; W. Kim; J. Hong","Department of Industrial Engineering, Yonsei University, Seodaemun-gu, Seoul, South Korea; Department of Industrial Engineering, Yonsei University, Seodaemun-gu, Seoul, South Korea; Department of Management Information Systems, Kyonggi University, Yeongtong-gu, Suwon-si, South Korea","IEEE Access","28 Sep 2022","2022","10","","100367","100375","An effective supply chain management system is indispensable for an enterprise with a supply chain network in several aspects. Especially, organized control over the production and transportation of its products is a key success factor for the enterprise to stay active without damaging its reputation. This case is also highly relevant to garment industries. In this study, an extensive Deep Reinforcement Learning study for apparel supply chain optimization is proposed and undertaken, with focus given to Soft Actor-Critic. Six models are experimented with in this study and are compared with respect to the sell-through rate, service level, and inventory-to-sales ratio. Soft Actor-Critic outperformed several other state-of-the-art Actor Critic models in managing inventories and fulfilling demands. Furthermore, explicit indicators are calculated to assess the performance of the models in the experiment. Soft Actor-Critic achieved a better balance between service level and sell-through rate by ensuring higher availability of the stocks to sell without overstocking. From numerical experiments, it has been shown that S-policy, Trust Region Policy Optimization, and Twin Delayed Deep Deterministic Policy Gradient have a good balance between service level and sell-through rate. Additionally, Soft Actor-Critic achieved a 7%, 41.6%, and 42.8% lower inventory sales ratio than the S-policy, Twin Delayed Deep Deterministic Policy Gradient, and Trust Region Policy Optimization models, indicating its superior ability in making the inventory stocks available to make sales and profit from them.","2169-3536","","10.1109/ACCESS.2022.3205720","Jungseok Logistics Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9885193","Deep reinforcement learning;inventory management;markov decision process;supply chain management;soft actor critic","Inventory management;Optimization;Inventory control;Transportation;Supply chains;Supply chain management;Reinforcement learning;Deep learning;Markov processes","clothing;clothing industry;deep learning (artificial intelligence);inventory management;optimisation;production engineering computing;profitability;reinforcement learning;supply chain management;supply chains","supply chain management system;supply chain network;apparel supply chain optimization;soft actor-critic;sell-through rate;twin delayed deep deterministic policy gradient;trust region policy optimization models;effective supply chain management system;deep reinforcement learning;state-of-the-art actor critic models;inventories management;demands fulfilling;products production;products transportation;garment industries;inventory sales ratio;inventory stocks;profitability","","","","27","CCBYNCND","12 Sep 2022","","","IEEE","IEEE Journals"
"Reinforcement learning approach to learning human experience in tuning cavity filters","Z. Wang; J. Yang; J. Hu; W. Feng; Y. Ou","Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Chinese Academy of Sciences, Shenzhen, P.R.China; Shenzhen College of Advanced Technology, University of Chinese Academy of Sciences, Shenzhen, P.R.China; Shenzhen College of Advanced Technology, University of Chinese Academy of Sciences, Shenzhen, P.R.China; Key Laboratory of Human-Machine Intelligence-Synergy Systems, Chinese Academy of Sciences, Shenzhen, P.R.China; The Chinese University of Hong Kong, Hong Kong","2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)","25 Feb 2016","2015","","","2145","2150","Owing to the rapid development of the communication industry, various kinds of radio frequency components are in great demand and put into mass production. Among them, passive devices such as microwave cavity filters, duplexers and combiners have experienced fast and unexpected upgrades. However, the tuning process of these products, which is always manually operated, still seems hard to be automatically replaced or improved because of the difficulties in extracting human experience. In this study, we make deep investigations into some previous automatic cavity filter tuning solutions, especially the ones using intelligent algorithms. In addition, we propose the method of intelligent tuning based on the reinforcement learning algorithm which dynamically extracts the human strategies during the tuning process. The experimental results prove the powerful performance of reinforcement learning in mastering human skills.","","978-1-4673-9675-2","10.1109/ROBIO.2015.7419091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7419091","","Tuning;Scattering parameters;Fasteners;Cavity resonators;Games;Robots;Industries","electronic engineering computing;learning (artificial intelligence);microwave filters;radiofrequency filters","human experience learning;communication industry;radio frequency components;mass production;passive devices;microwave cavity filters;duplexers;combiners;tuning process;automatic cavity filter tuning solutions;intelligent tuning;reinforcement learning algorithm;human strategies;human skills","","13","1","17","IEEE","25 Feb 2016","","","IEEE","IEEE Conferences"
"Character Animation Using Reinforcement Learning and Imitation Learning Algorithms","T. Tahmid; M. A. Lobabah; M. Ahsan; R. Zarin; S. S. Anis; F. B. Ashraf","Department of Computer Science and Engineering, BRAC University; Department of Computer Science and Engineering, BRAC University; Department of Computer Science and Engineering, BRAC University; Department of Computer Science and Engineering, BRAC University; Department of Computer Science and Engineering, BRAC University; Department of Computer Science and Engineering, BRAC University","2021 Joint 10th International Conference on Informatics, Electronics & Vision (ICIEV) and 2021 5th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)","18 Oct 2021","2021","","","1","6","Real-time character animation for gaming and film industries is challenging and achieving production-ready quality requires a huge amount of time and resources. Animation through marker-based motion capture is quite a tiresome process that requires costly motion-capture suits, multiple cameras, and a large database. In this paper, we propose a model that aims to generate real-time character animation for biped locomotion in Unity ML(Machine Learning) agents using RL(Reinforcement learning) and IL(Imitation learning) algorithms. We first evaluate the training process with solely the state-of-the-art RL algorithm, PPO(Proximal Policy Optimization). Then we analyze the combination of IL algorithms BC(Behavioral Cloning) and GAIL(Generative Adversarial Imitation Learning) in conjunction with PPO. We further discuss the comparison between the two training results and show that our model can generate animations in real-time avoiding all the tedious work and large databases. We demonstrate that our approach is effortlessly easy to implement while maintaining the quality of the animation.","","978-1-6654-4923-6","10.1109/ICIEVicIVPR52578.2021.9564143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564143","Animation;Reinforcement Learning;Imitation Learning;PPO;BC;GAIL;Unity ML agents","Training;Legged locomotion;Databases;Entertainment industry;Cloning;Reinforcement learning;Animation","computer animation;image motion analysis;learning (artificial intelligence);legged locomotion","production-ready quality;marker-based motion capture;tiresome process;costly motion-capture suits;real-time character animation;training process;solely the state-of-the-art RL algorithm;Generative Adversarial Imitation;Imitation Learning algorithms;film industries","","","","18","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Intelligent Fault Quantitative Identification for Industrial Internet of Things (IIoT) via a Novel Deep Dual Reinforcement Learning Model Accompanied With Insufficient Samples","Y. Chang; J. Chen; W. Wu; T. Pan; Z. Zhou; S. He","State Key Laboratory for Manufacturing and Systems Engineering, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory for Manufacturing and Systems Engineering, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory for Manufacturing and Systems Engineering, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory for Manufacturing and Systems Engineering, Xi’an Jiaotong University, Xi’an, China; FAST Auto Drive Transmission Engineering Research Institute, Shaanxi Fast Gear Company Ltd, Xi’an, China; School of Mechanical and Electrical Engineering, Guilin University of Electronic Technology, Guilin, China","IEEE Internet of Things Journal","6 Oct 2022","2022","9","20","19811","19822","Industrial Internet of Things (IIoT) is mainly a data-oriented network, so intelligent processing of massive data is desiderated to realize the interconnection between machines. Currently, deep-learning-based methods are widely applied for intelligent construction of the IIoT, so as to maximize the self-monitoring and self-management capabilities of various machines. However, the quantity and quality of data and the optimization of parameters greatly limit the properties of such methods. As a breakthrough of artificial intelligence (AI), deep reinforcement learning (DRL) provides inspiration and direction, which combines the advantages of deep learning and reinforcement learning to construct an end-to-end fault identification system. Therefore, a novel deep dual reinforcement learning model was proposed, which consisted of an actor model and a critic model. The dual structures avoid the over-self-optimization of the network. The action model continually learns the knowledge of identifying unknown samples by the  $\varepsilon $ - $greedy$  algorithm, while the critic model dynamically adjusts the policy to guide the action model in right training direction. The effectiveness of the proposed method was verified by three bearing data sets. The results indicate that the proposed method enables agents to independently realize precise fault quantitative identification. The establishment of an experience storage unit overcomes the problem of insufficient samples, which avoids blind trial and error of the proposed mode.","2327-4662","","10.1109/JIOT.2022.3168317","National Natural Science Foundation of China(grant numbers:51875436,91960106,U1933101,51965013); National Key Research and Development Program of China(grant numbers:2019YFF0302204); Scientific Research and Technology Development in Liuzhou(grant numbers:2021AAA0112); Open Fund of State Key Laboratory(grant numbers:sklms2022005); Guangxi Natural Science Foundation Program(grant numbers:2020GXNSFAA159081); Fundamental Research Funds for the Central Universities(grant numbers:XZY022020007,XZY022021006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9759483","Deep reinforcement learning (DRL);fault quantitative identification;insufficient samples;Internet of Things","Reinforcement learning;Industrial Internet of Things;Fault diagnosis;Feature extraction;Training;Deep learning;Predictive models","deep learning (artificial intelligence);fault diagnosis;greedy algorithms;Internet of Things;optimisation;production engineering computing;reinforcement learning","intelligent fault quantitative identification;IIoT;insufficient samples;data-oriented network;intelligent processing;massive data;deep-learning-based methods;intelligent construction;deep reinforcement learning;deep learning;end-to-end fault identification system;actor model;critic model;dual structures;action model;unknown samples;bearing data sets;precise fault quantitative identification;deep dual reinforcement learning model;DRL","","11","","30","IEEE","19 Apr 2022","","","IEEE","IEEE Journals"
"Multi-Agent Deep Reinforcement Learning for Solving Large-scale Air Traffic Flow Management Problem: A Time-Step Sequential Decision Approach","Y. Tang; Y. Xu","School of Aerospace, Transport and Manufacturing, Cranfield University, Bedford, United Kingdom; School of Aerospace, Transport and Manufacturing, Cranfield University, Bedford, United Kingdom","2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC)","15 Nov 2021","2021","","","1","10","In this paper, we focus on the demand-capacity balancing (DCB) problem in air traffic flow management, which is considered as a fully cooperative multi-agent learning task. First, a rule-based time-step environment is designed to mimic the DCB process. In this environment, each agent ‘flight’ decides its action at valid time steps. Three different rules are defined, based on the remaining capacity and the number of cooperative flights in each sector, to ease the learning process. Second, a multi-agent reinforcement learning framework, built on the proximal policy optimization (MAPPO), is proposed by using the parameter sharing mechanism and the mean-field approximation method, where an inherent feature of all other agents is extracted to address the credit assignment problem. Moreover, a supervisor integrated MAPPO framework is proposed, where a supervisor is designed to generate supervised actions, in such a way to further improve the learning performance. In the experiments, two performance indices, Search Capability and Generalization Capability, are considered. Both indices are assessed with the evaluation of two toy cases and a real-world case study. Results suggest that, the supervisor integrated MAPPO with supervised actions achieves the best performance across the different cases; other proposed methods also show some promising Search Capability, but only prove an acceptable Generalization Capability in simpler cases than the training cases.","2155-7209","978-1-6654-3420-1","10.1109/DASC52595.2021.9594329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9594329","Air Traffic Flow Management;Demand-capacity Balance;Multi-agent Reinforcement Learning;Proximal Policy Optimization","Training;Toy manufacturing industry;Supervised learning;Reinforcement learning;Aerospace electronics;Feature extraction;Delays","air traffic;approximation theory;decision support systems;knowledge based systems;learning (artificial intelligence);multi-agent systems;optimisation","proximal policy optimization;parameter sharing mechanism;mean-field approximation method;credit assignment problem;MAPPO framework;supervised actions;multiagent deep reinforcement learning;large-scale air traffic flow management problem;time-step sequential decision approach;demand-capacity balancing problem;rule-based time-step environment;DCB process;generalization capability","","1","","30","IEEE","15 Nov 2021","","","IEEE","IEEE Conferences"
"UAV countermeasure maneuver decision based on deep reinforcement learning","W. Li; J. Wu; J. Chen; K. Lia; X. Cai; C. Wang; Y. Guo; S. Jia; W. Chen; F. Luo; W. Wang","School of automation, Wuhan University of Technology, Wuhan, China; Shenzhen Zhongxing Microelectronics Technology Co.,Ltd, Shenzhen, China; Chinese People’s Liberation Army Naval Research Institute, Beijing, China; Wuhan Second Ship Design Institute, Wuhan, China; Wuhan Second Ship Design Institute, Wuhan, China; Harbin Industrial Robot International Innovation Research Institute (Wuhan) Co., Ltd, Wuan, China; School of Automation, Northwestern Polytechnical University; School of Intelligent Science, National University of Defense Technology; China Ship Development and Design Center, Wuhan, China; School of automation, Wuhan University of Technology, Wuhan, China; Sino-German College of Intelligent Manufacturing, Shenzhen Technology University, Shenzhen, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","92","96","The struggle for air supremacy is the core issue in modern military confrontation. In recent years, deep reinforcement learning has developed rapidly in the field of decision-making. Compared with traditional reinforcement learning methods, deep reinforcement learning can effectively solve continuous decisionmaking problems in large-scale state spaces. Based on the theory of deep reinforcement learning, this paper conducts research on UAV confrontation decision-making, combined with neural network to fit the action value function of the large-scale state space in the aerial combat problem, and implements it based on the exploration strategy combining E-value and $\epsilon$-greedy reinforce the balance of exploration and utilization in the learning process, and conduct a comparative experiment. Compared with the DQN maneuver decision algorithm based on the attenuated greedy algorithm, it improves the convergence speed of the aerial combat algorithm by 100% and greatly reduces the loss function.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023761","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023761","UCAV;Maneuver decision;Exploration strategy;Reinforcement learning","Deep learning;Training;Greedy algorithms;Correlation;Decision making;Neural networks;Reinforcement learning","autonomous aerial vehicles;decision making;deep learning (artificial intelligence);greedy algorithms;military computing;reinforcement learning","action value function;aerial combat problem;air supremacy;convergence speed;deep reinforcement learning;E-value;large-scale state space;large-scale state spaces;loss function;military confrontation;neural network;UAV confrontation decision-making;UAV countermeasure maneuver decision;ϵ-greedy","","1","","13","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"A Multi-action Reinforcement Learning Algorithm for Energy-efficiency Blocking Flow-shop Scheduling Problem","H. Bao; Q. Pan; M. Rong; A. Yang; X. Wang","School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China","2023 26th International Conference on Computer Supported Cooperative Work in Design (CSCWD)","22 Jun 2023","2023","","","492","497","With the increasingly serious ecological problems, energy-efficient scheduling, an effective approach to achieve sustainable development and green manufacturing, has attracted much attention by taking both economic effect and energy conservation into account. This paper addresses an energy-efficient scheduling of the distributed blocking flow-shop problem (EDBFSP) to minimize both makespan and total energy consumption. The mixed-integer linear programming (MILP) model of EDBFSP is designed. A multi-action reinforcement learning algorithm based on problem-specific knowledge called multi-greedy policy optimization (multi-GPO) is proposed to solve the EDBFSP. In addition, after analyzing the characteristics of the problem, an energy-saving strategy and an acceleration strategy are designed to further optimize the solution. Experiments in a large number of benchmark tests have testified that the multi-GPO is superior to the state-of-the-art algorithms in terms of efficiency and importance in solving EDBFSP.","2768-1904","979-8-3503-3168-4","10.1109/CSCWD57460.2023.10152003","Nature; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10152003","blocking flow-shop;energy-efficient distributed scheduling;problem-specific knowledge;reinforcement learning;multi-objective optimization","Green manufacturing;Energy consumption;Job shop scheduling;Federated learning;Energy conservation;Reinforcement learning;Energy efficiency","energy conservation;energy consumption;flow shop scheduling;greedy algorithms;integer programming;linear programming;reinforcement learning;scheduling;sustainable development","distributed blocking flow-shop problem;economic effect;EDBFSP;energy conservation;energy-efficiency blocking flow-shop scheduling problem;energy-efficient scheduling;energy-saving strategy;increasingly serious ecological problems;mixed-integer linear programming model;multiaction reinforcement learning algorithm;multiGPO;problem-specific knowledge called multigreedy policy optimization;total energy consumption","","","","14","IEEE","22 Jun 2023","","","IEEE","IEEE Conferences"
"Visuomotor Reinforcement Learning for Multirobot Cooperative Navigation","Z. Liu; Q. Liu; L. Tang; K. Jin; H. Wang; M. Liu; H. Wang","Department of Computer Science and Technology, University of Cambridge, Cambridge, U.K.; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; MOE Key Laboratory of Marine Intelligent Equipment and System and the State Key Laboratory of Ocean Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key Laboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Engineering Research Center of Intelligent Control and Management, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Automation Science and Engineering","13 Oct 2022","2022","19","4","3234","3245","This article investigates the multirobot cooperative navigation problem based on raw visual observations. A fully end-to-end learning framework is presented, which leverages graph neural networks to learn local motion coordination and utilizes deep reinforcement learning to generate visuomotor policy that enables each robot to move to its goal without the need of environment map and global positioning information. Experimental results show that, with a few tens of robots, our approach achieves comparable performance with the state-of-the-art imitation learning-based approaches with bird-view state inputs. We also illustrate our generalizability to crowded and large environments and our scalability to ten times number of the training robots. In addition, we demonstrate that our model trained for multirobot case can also improve the success rate in the single-robot navigation task in unseen environments. Note to Practitioners—With the development of intelligent industrial and logistic systems, robotic transportation systems are widely implemented. However, existing multirobot path coordination and navigation approaches are basically under some unreasonable assumptions, which are very hard to be implemented in practical scenarios. This article aims to greatly promote the real application of learning-based multirobot cooperative navigation approach, in order to achieve the following. First, we introduce an end-to-end reinforcement learning framework instead of the commonly used imitation learning strategy, as the latter one needs exhaustive training data to cover all the scenarios and does not have the required generalizability. Second, we directly use the raw sensor data instead of the commonly used bird-eye-view semantic observations, as the latter one is generally not representative of practical application scenario from the robot perspective and cannot solve the occlusion issue. Third, we interpret our learned model to illustrate which parts of the input and shared observations contribute most to the robots’ final actions. The above interpretability ensures predictability (thus safety) of our visuomotor policy in practical applications. Our learned visuomotor policy has the ability to coordinate dozens of robots by only using raw visual observations in unknown environments without map nor global localization information, this is the first time in the literature. Our future work includes solving the sim-to-real issue and conducting physical experiments.","1558-3783","","10.1109/TASE.2021.3114327","Natural Science Foundation of China(grant numbers:62073222,U1913204); Shanghai Municipal Education Commission and Shanghai Education Development Foundation through “Shu Guang” Project(grant numbers:19SG08); Shenzhen Science and Technology Program(grant numbers:JSGG20201103094400002); Science and Technology Commission of Shanghai Municipality(grant numbers:21511101900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555922","Cooperative navigation;multirobot system;reinforcement learning (RL);visuomotor","Multi-robot systems;Robot kinematics;Navigation;Cooperative systems;Collision avoidance;Reinforcement learning","deep learning (artificial intelligence);graph theory;learning systems;mobile robots;motion control;multi-robot systems;navigation;path planning;reinforcement learning;robot vision","visuomotor reinforcement learning;raw visual observations;graph neural network;local motion coordination;deep reinforcement learning;environment map;global positioning information;crowded environments;single-robot navigation task;unseen environment;intelligent industrial system;logistic system;multirobot path coordination;end-to-end reinforcement learning;imitation learning;raw sensor data;bird-eye-view;robot perspective;visuomotor policy learning;global localization information;learning-based multirobot cooperative navigation;robotic transportation system;bird-view state input;occlusion","","6","","29","IEEE","1 Oct 2021","","","IEEE","IEEE Journals"
"Learning Representation with Q-irrelevance Abstraction for Reinforcement Learning","S. Hao; L. Li; M. Liu; Y. Zhu; D. Zhao","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2021 11th International Conference on Intelligent Control and Information Processing (ICICIP)","16 Dec 2021","2021","","","367","373","In order to improve the performance of deep reinforcement learning (DRL) algorithm in high-dimensional observation environments, we propose a new auxiliary task to learn representations to aggregate task-relevant information of observations. Inspired by Q-irrelevance abstraction, our auxiliary task trains a deep Q-network (DQN) to predict the true Q value distribution over all discrete actions. Then we use the output of DQN to train the encoder to discriminate states with different Q values. The encoder is used as the representation of proximal policy optimization (PPO). The resulting algorithm is called as Q-irrelevance Abstraction for Reinforcement Learning (QIARL). After training, the encoder can aggregate states with similar Q value distributions together for any policy and any action. Thus the encoder can encode the important information that is relevant to reinforcement learning task. We test QIARL in four Procgen environments compare with PPO, A2C and Rainbow. The experimental results show QIARL outperforms the other three algorithms.","","978-1-6654-2515-5","10.1109/ICICIP53388.2021.9642160","Research and Development; Chinese Academy of Sciences; Youth Innovation Promotion Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9642160","reinforcement learning;state abstraction;proximal policy optimization;auxiliary task","Training;Measurement;Learning systems;Aggregates;Process control;Reinforcement learning;Prediction algorithms","deep learning (artificial intelligence);image representation;optimisation","Q-irrelevance abstraction for reinforcement learning;encoder;proximal policy optimization;QIARL;representation learning;deep reinforcement learning algorithm;high-dimensional observation environments;auxiliary task;deep Q-network;DQN;Q value distribution;discrete actions;task-relevant information aggregation;PPO;Procgen environments;A2C;Rainbow","","1","","24","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Learning Superior Cooperative Policy in Adversarial Multi-Team Reinforcement Learning","Q. Fu; T. Qiu; Z. Pu; J. Yi; X. Ai; W. Yuan","*University of Chinese Academy of Sciences, Beijing, China; *University of Chinese Academy of Sciences, Beijing, China; *University of Chinese Academy of Sciences, Beijing, China; *University of Chinese Academy of Sciences, Beijing, China; *University of Chinese Academy of Sciences, Beijing, China; Electronics Technology Group Corporation, Information Science Academy of China, Beijing, China","2023 International Joint Conference on Neural Networks (IJCNN)","2 Aug 2023","2023","","","1","7","Multi-agent Reinforcement Learning (MARL) has become a powerful tool for addressing multi-agent challenges. Existing studies have explored numerous models to use MARL to solve single-team cooperation (competition) problems and adversarial problems with opponents controlled by static knowledge-based policies. However, most studies in the literature often ignore adversarial multi-team problems involving dynamically evolving opponents. We investigate adversarial multi-team problems where all participating teams use MARL learners to learn policies against each other. Two objectives are achieved in this study. Firstly, we design an adversarial team-versus-team learning framework to generate cooperative multi-agent policies to compete against opponents without preprogrammed opponent partners or any supervision. Secondly, we explore the key factors to achieve win-rate superiority during dynamic competitions. Then we put forward a novel FeedBack MARL (FBMARL) algorithm that takes advantage of feedback loops to adjust optimizer hyper-parameters based on real-time game statistics. Finally, the effectiveness of our FBMARL model is tested in a benchmark environment named Multi-Team Decentralized Collective Assault (MT-DCA). The results demonstrate that our feedback MARL model can achieve superior performance over baseline competitor MARL learners in 2-team and 3-team dynamic competitions.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10191422","National Key Research and Development Program of China(grant numbers:2018AAA0102404); National Natural Science Foundation of China(grant numbers:62073323); Beijing Nova Program(grant numbers:20220484077); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191422","Multi-agent system;reinforcement learning;competitive RL","Training;Feedback loop;Heuristic algorithms;Neural networks;Knowledge based systems;Reinforcement learning;Games","game theory;multi-agent systems;optimisation;reinforcement learning","3-team dynamic competitions;adversarial multiteam problems;adversarial problems;adversarial team-versus-team learning framework;baseline competitor MARL learners;feedback MARL model;game statistics;multiagent policies;multiagent reinforcement learning;multiteam decentralized collective assault;opponents;static knowledge-based policies;superior cooperative policy","","","","25","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Application of Deep Reinforcement Learning in Residential Preconditioning for Radiation Temperature","T. Morinibu; T. Noda; T. Shota","Technology and Innovation Center, Daikin Industries, Ltd., Osaka, Japan; Technology and Innovation Center, Daikin Industries, Ltd., Osaka, Japan; Technology and Innovation Center, DAIKIN INDUSTRIES, Ltd., Osaka, Japan","2019 8th International Congress on Advanced Applied Informatics (IIAI-AAI)","13 Feb 2020","2019","","","561","566","In Heating, Ventilation, and Air Conditioning (HVAC) control, there are many studies to control air conditioners to improve comfort when people are present, but this paper proposes a method of pre-air conditioning. It is to control the radiant temperature of a room in advance by controlling wind directions of an air conditioner. It has been investigated how the fixed wind direction control of the air conditioner causes the radiation temperature non-uniformity in the room. Reinforcement learning is used as a method to solve it, the effectiveness of which has been verified in the residential environment. Here, thermography is adopted as a sensor for acquiring the state. The proposed method has reduced the nonuniformity of radiation temperature in the room more than the random and normal control. The feature of this study is that all learning and verifications are performed in a real environment, and image data is taken as an input value.","","978-1-7281-2627-2","10.1109/IIAI-AAI.2019.00120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8992750","deep reinforcement learning, HVAC control, thermal comfort, home heating","","building management systems;buildings (structures);HVAC;infrared imaging;learning (artificial intelligence);thermal comfort","deep reinforcement learning;residential preconditioning;radiation temperature nonuniformity;random control;wind direction control;HVAC control;thermography;sensor","","3","","19","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Trustworthy Dynamic Object Tracking Using Deep Reinforcement Learning with the Self-Attention Mechanism","M. Li; H. Liu; H. Wang; M. Xia","Faculty of Materials and Manufacturing, College of Intelligent Machinery, Beijing University of Technology, Beijing, China; Faculty of Materials and Manufacturing, College of Intelligent Machinery, Beijing University of Technology, Beijing, China; Faculty of Materials and Manufacturing, College of Intelligent Machinery, Beijing University of Technology, Beijing, China; Faculty of Materials and Manufacturing, College of Intelligent Machinery, Beijing University of Technology, Beijing, China","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","6","Object tracking serves as a prerequisite and foundation for higher-level driving tasks and has broad application prospects in various fields, including intelligent logistics and autonomous driving. In this paper, we propose a deep reinforcement learning-based object tracking control network model, that incorporates attention and Long Short-Term Memory (LSTM) mechanisms. The Asynchronous Advantage Actor-Critic (A3C) algorithm is employed for multi-threaded synchronous unsupervised training of the tracking network model, resulting in end-to-end dynamic object tracking control. Additionally, the Gradient-weighted Class Activation Mapping (Grad-CAM) method is utilized to analyze the interpretability of the network model. Experimental results demonstrate that by introducing the attention salience mechanism and LSTM temporal mechanism, the network can effectively focus on obstacle and target locations, thus enhancing its attentional capacity and interpretability. The trustworthiness of the object tracking model can be improved in terms of both tracking performance and interpretability.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260504","National Key Research and Development Program of China(grant numbers:2021YFB1716200); Beijing Municipal Education Commission(grant numbers:KM202310005033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260504","","Deep learning;Training;Analytical models;Visualization;Target tracking;Three-dimensional displays;Heuristic algorithms","deep learning (artificial intelligence);learning (artificial intelligence);multi-threading;object tracking;recurrent neural nets;reinforcement learning;unsupervised learning","Asynchronous Advantage Actor-Critic algorithm;attention salience mechanism;attentional capacity;autonomous driving;deep reinforcement learning-based object tracking control network model;end-to-end dynamic object tracking control;Gradient-weighted Class Activation Mapping method;higher-level driving tasks;Long Short-Term Memory mechanisms;LSTM temporal mechanism;multithreaded synchronous unsupervised training;object tracking model;self-attention mechanism;tracking network model;tracking performance;trustworthy dynamic object tracking","","","","20","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Digital Twin Enhanced Reinforcement Learning for Integrated Scheduling in Automated Container Terminals*","Y. Zhang; X. Bao; L. Zhang; L. Chen; X. Tang; Z. Zhang; Y. Zheng","Shanghai Key lab of Advanced Manufacturing Environment, School of Mechanical Engineering, Shanghai Jiaotong University, Shanghai, PR, China; Shanghai Key lab of Advanced Manufacturing Environment, School of Mechanical Engineering, Shanghai Jiaotong University, Shanghai, PR, China; Qingdao Port International Co., Ltd., Qingdao, RP, China; Shanghai Key lab of Advanced Manufacturing Environment, School of Mechanical Engineering, Shanghai Jiaotong University, Shanghai, PR, China; Qingdao Port International Co., Ltd., Qingdao, RP, China; Shandong Port Technology Group Qingdao Co., Ltd., Qingdao, RP, China; Shanghai Key lab of Advanced Manufacturing Environment, School of Mechanical Engineering, Shanghai Jiaotong University, Shanghai, PR, China","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","6","Container logistics in automated container terminals (ACTs) involves multiple interconnected operational processes. The integrated scheduling of these processes is crucial for the efficiency of the system. Current reinforcement learning (RL) methods struggle to interact effectively with the real world. Digital twin (DT) presents a promising solution to this challenge by providing real-time synchronized simulation. We propose a RL method enhanced by DT to address the integrated scheduling problem in ACTs. The RL model is trained in a DT-based simulation environment, which is more accurate and real-time than traditional simulations. This approach allows for easy transfer of the RL model to a real-world environment. To demonstrate the effectiveness of this approach, we present an experiment in this paper. Our proposed method shows promising results and has the potential to increase container logistics efficiency in ACTs.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260497","Automated container terminal;digital twin;reinforcement learning;simulation optimization","Reinforcement learning;Containers;Multilayer perceptrons;Real-time systems;Data models;Digital twins;Synchronization","digital twins;learning (artificial intelligence);logistics;reinforcement learning;scheduling;sea ports;synchronisation","automated container terminals;container logistics efficiency;current reinforcement learning methods struggle;digital twin enhanced reinforcement learning;DT-based simulation environment;integrated scheduling problem;multiple interconnected operational processes;real-time synchronized simulation;RL method;RL model;traditional simulations","","","","26","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Experimental and Computational Study on the Ground Forces CGF Automation of Wargame Models Using Reinforcement Learning","M. Choi; H. Moon; S. Han; Y. Choi; M. Lee; N. Cho","Department of Operation Research, Korea National Defense University, Nonsan, South Korea; Department of Operation Research, Korea National Defense University, Nonsan, South Korea; Agency for Defense Development, Daejeon, South Korea; Department of Economics and Management, Korea Army Academy, Yeongcheon, South Korea; Department of Operation Research, Korea National Defense University, Nonsan, South Korea; Department of Operation Research, Korea National Defense University, Nonsan, South Korea","IEEE Access","14 Dec 2022","2022","10","","128970","128982","Wargame is an important tool that enables training units to develop various strategies by allowing them to experience unexpected situations. There are three methodologies that determine the behavior of the Computer Generated Forces(CGF) in wargame—rule-based, agent-based, and learning-based methodologies. The military determines the behaviors of the CGF mainly based on the rules because a doctrine and an operation plan are well established. However, the advent of intelligent weapons and the accompanying changes in tactics will make it difficult to expect an environment and situations of the future battlefield. Therefore, we studied the automation of CGF through reinforcement learning in order to give unexpected situations, so that the training unit would be able to establish various strategies and tactics through the wargame model. Based on the combat functions of the ground forces, we configured multiple environments that the ground forces CGFs will learn in. First, infantry and artillery CGFs learned in the close combat environment, which is the basis of ground forces combat. Second, the trainee CGF learned in the context of military training. Third, the drone CGF learned how to reconnaissance and attack in a multi-drone environment, and finally, the combat service support CGF learned under the mission of supplying ammunition. As a result, we confirmed that the reinforcement learning methodology is applicable to CGF through these experiments.","2169-3536","","10.1109/ACCESS.2022.3227797","Agency for Defense Development by the Korean Government(grant numbers:UE202075ID); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9976053","Wargame;CGF;ground forces combat;reinforcement learning;artificial intelligence","Reinforcement learning;Behavioral sciences;Games;Training data;Automation;Weapons;Computational modeling","computer based training;computer games;digital simulation;military computing;military systems;multi-agent systems;reinforcement learning;weapons","agent-based methodology;combat environment;combat functions;combat service support CGF;drone CGF;future battlefield;ground Forces CGF automation;ground forces CGFs;ground forces combat;infantry;intelligent weapons;learning-based methodology;military training;multidrone environment;multiple environments;operation plan;reinforcement learning methodology;rule-based methodology;tactics;trainee CGF;training unit;unexpected situations;wargame model","","1","","37","CCBY","8 Dec 2022","","","IEEE","IEEE Journals"
"Improvement of Reinforcement Learning With Supermodularity","Y. Meng; F. Shi; L. Tang; D. Sun","National Frontiers Science Center for Industrial Intelligence and Systems Optimization and the Key Laboratory of Data Analytics and Optimization for Smart Industry, Ministry of Education, Northeastern University, Shenyang, China; Key Laboratory of Data Analytics and Optimization for Smart Industry, Ministry of Education, Liaoning Engineering Laboratory of Data Analytics and Optimization for Smart Industry, Northeastern University, Shenyang, China; National Frontiers Science Center for Industrial Intelligence and Systems Optimization, Northeastern University, Shenyang, China; National Frontiers Science Center for Industrial Intelligence and Systems Optimization and the Key Laboratory of Data Analytics and Optimization for Smart Industry, Ministry of Education, Northeastern University, Shenyang, China","IEEE Transactions on Neural Networks and Learning Systems","1 Sep 2023","2023","34","9","5298","5309","Reinforcement learning (RL) is a promising approach to tackling learning and decision-making problems in a dynamic environment. Most studies on RL focus on the improvement of state evaluation or action evaluation. In this article, we investigate how to reduce action space by using supermodularity. We consider the decision tasks in the multistage decision process as a collection of parameterized optimization problems, where state parameters dynamically vary along with the time or stage. The optimal solutions of these parameterized optimization problems correspond to the optimal actions in RL. For a given Markov decision process (MDP) with supermodularity, the monotonicity of the optimal action set and the optimal selection with respect to state parameters can be obtained by using the monotone comparative statics. Accordingly, we propose a monotonicity cut to remove unpromising actions from the action space. Taking bin packing problem (BPP) as an example, we show how the supermodularity and monotonicity cut work in RL. Finally, we evaluate the monotonicity cut on the benchmark datasets reported in the literature and compare the proposed RL with some popular baseline algorithms. The results show that the monotonicity cut can effectively improve the performance of RL.","2162-2388","","10.1109/TNNLS.2023.3244024","Major Program of National Natural Science Foundation of China(grant numbers:72192830,72192831); National Natural Science Foundation of China(grant numbers:72002028); 111 Project(grant numbers:B16009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049751","Dynamic parameter;monotone comparative statics;optimization;reinforcement learning (RL);supermodularity","Optimization;Dynamic programming;Industries;Heuristic algorithms;Data analysis;Approximation algorithms;Sufficient conditions","bin packing;decision making;decision theory;learning (artificial intelligence);Markov processes;optimisation;reinforcement learning","action space;bin packing problem;decision tasks;decision-making problems;dynamic environment;given Markov decision process;monotone comparative statics;monotonicity cut;multistage decision process;optimal action;optimal selection;optimal solutions;parameterized optimization problems;reinforcement learning;RL focus;state evaluation;state parameters;supermodularity;unpromising actions","","","","43","IEEE","22 Feb 2023","","","IEEE","IEEE Journals"
"A simple reinforcement learning algorithm for biped walking","J. Morimoto; G. Cheng; C. G. Atkeson; G. Zeglin","Department of Humanoid Robotics and Computational Neuroscience, ATR Computational Neuroscience Laboratories, Japan; Department of Humanoid Robotics and Computational Neuroscience, ATR Computational Neuroscience Laboratories, Japan; The Robotics Institute, Carnegie Mellon University, USA; The Robotics Institute, Carnegie Mellon University, USA","IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004","6 Jul 2004","2004","3","","3030","3035 Vol.3","We propose a model-based reinforcement learning algorithm for biped walking in which the robot learns to appropriately place the swing leg. This decision is based on a learned model of the Poincare map of the periodic walking pattern. The model maps from a state at the middle of a step and foot placement to a state at next middle of a step. We also modify the desired walking cycle frequency based on online measurements. We present simulation results, and are currently implementing this approach on an actual biped robot.","1050-4729","0-7803-8232-3","10.1109/ROBOT.2004.1307522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307522","","Learning;Legged locomotion;Frequency estimation;Foot;Timing;Phase estimation;Humanoid robots;Leg;Humans;Oscillators","legged locomotion;learning (artificial intelligence);Poincare mapping","biped walking;model-based reinforcement learning algorithm;swing leg;Poincare map;periodic walking pattern;walking cycle frequency modification;online measurements;biped robot","","45","","24","IEEE","6 Jul 2004","","","IEEE","IEEE Conferences"
"Robot path planning by artificial potential field optimization based on reinforcement learning with fuzzy state","Xiaodong Zhuang; Qingchun Meng; Bo Yin; Hanping Wang","Department of Electronics & Engineering, Ocean University of China, Qingdao, Shandong, China; Department of Electronics & Engineering, Ocean University of China, Qingdao, Shandong, China; Department of Electronics & Engineering, Ocean University of China, Qingdao, Shandong, China; Department of Electronics & Engineering, Ocean University of China, Qingdao, Shandong, China","Proceedings of the 4th World Congress on Intelligent Control and Automation (Cat. No.02EX527)","7 Nov 2002","2002","2","","1166","1170 vol.2","Temporal difference (TD) learning with fuzzy state is applied to robot navigation in a multi-obstacle environment. An interpretation of the state evaluation function is given by regarding the state evaluation as a discrete artificial potential field (APF). Global optimal path planning is implemented with the APF obtained by TD learning. The APF obtained is globally optimal and avoids the local minimum areas, which always appear in traditional APF methods. Fuzzy state is introduced to improve the learning efficiency. A computer evaluation experiment shows the method's effectiveness and efficiency.","","0-7803-7268-9","10.1109/WCICA.2002.1020763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1020763","","Path planning;Control systems;Navigation;Stochastic processes;Optimal control;Mobile robots;Fuzzy control;Intelligent control;Learning systems;Automatic control","path planning;learning (artificial intelligence);optimal control;Markov processes;decision theory;probability;fuzzy set theory;digital simulation;mobile robots","robot path planning;artificial potential field optimization;reinforcement learning;fuzzy state;temporal difference learning;robot navigation;multi-obstacle environment;state evaluation function;discrete artificial potential field;global optimal path planning;learning efficiency","","2","","9","IEEE","7 Nov 2002","","","IEEE","IEEE Conferences"
"Research on dynamic team formation for multirobot based on reinforcement learning","Wang Xing-ce; Zhang Ru-bo; Gu Guo-chang","College of Computer Science and Technology, Harbin Engineering of Technology, Harbin, China; Robotics Laboratory, Chinese Academy and Sciences, China; Robotics Laboratory, Chinese Academy and Sciences, China","Proceedings of the 4th World Congress on Intelligent Control and Automation (Cat. No.02EX527)","7 Nov 2002","2002","4","","2959","2963 vol.4","In the field of the artificial intelligence, more and more attention has been paid to the reinforcement learning algorithm with the advantage of its self-learning and self-adaptability. With the development of the multiagent theory in distributed artificial intelligence, the distributed reinforcement learning is becoming the focus of this research. A model of the multirobots' team formation is used as the study model to illuminate the high-level behavior control of the robots with the usage of the reinforcement learning. Now, few people apply this way to solve such problem. In the reinforcement learning algorithm explained here, the inside reinforcement signals and outside reinforcement signals are applied to show the interests of the robot and its whole group. The control system of the robot is composed of the high-level behavior control and the low-level action control. With this multilayer control, the task of every part is clear. In the low-level action control, the fuzzy control is used here for the mechanical character of the robot. After using the multilayer architecture and fuzzy control algorithm, the speed of learning and convergence of the reinforcement is faster.","","0-7803-7268-9","10.1109/WCICA.2002.1020069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1020069","","Learning;Artificial intelligence;Control systems;Fuzzy control;Educational institutions;Computer science;Robot control;Convergence;Animals;Adaptive control","multi-robot systems;learning (artificial intelligence);adaptive control;multi-agent systems;distributed control;multilayer perceptrons;fuzzy control;convergence","dynamic team formation;multirobot;reinforcement learning;AI;self-learning;self-adaptability;multiagent theory;distributed artificial intelligence;robot control system;high-level behavior control;low-level action control;multilayer control;fuzzy control;convergence","","1","","14","IEEE","7 Nov 2002","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Build-Order Production in StarCraft II","Z. Tang; D. Zhao; Y. Zhu; P. Guo","The State Key Laboratory of Management and Control for Complex Systems, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China","2018 Eighth International Conference on Information Science and Technology (ICIST)","9 Aug 2018","2018","","","153","158","StarCraft II is one of the most popular real-time strategy games and has become an important benchmark for AI research as it provides a complex environment with numerous challenges. The build order problem is one of the key challenges which concern the order and type of buildings and units to produce based on current game situation. In contrast to existing hand-craft methods, we propose two reinforcement learning based models: Neural Network Fitted Q-Learning (NNFQ) and Convolutional Neural Network Fitted Q-Learning (CNNFQ). NNFQ and CNNFQ have been applied into a simple bot for fighting against the enemy race. Experimental results show that both these two models are capable of finding the most effective production sequence to defeat the opponent.","2573-3311","978-1-5386-3782-1","10.1109/ICIST.2018.8426160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8426160","reinforcement learning;deep learning;build-order;production;StarCraft II","Games;Buildings;Data structures;Boolean functions;Training;Learning (artificial intelligence);Neural networks","computer games;learning (artificial intelligence);neural nets","NNFQ;Convolutional Neural Network Fitted Q-Learning;CNNFQ;build-order production;StarCraft II;real-time strategy games;AI research;game situation;Reinforcement Learning;and-craft methods","","10","","28","IEEE","9 Aug 2018","","","IEEE","IEEE Conferences"
"Data-Driven Optimal Bipartite Consensus Control for Second-Order Multiagent Systems via Policy Gradient Reinforcement Learning","Q. Liu; H. Yan; M. Wang; Z. Li; S. Liu","Key Laboratory of Smart Manufacturing in Energy Chemical Process of Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process of Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process of Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Smart Manufacturing in Energy Chemical Process of Ministry of Education, East China University of Science and Technology, Shanghai, China; School of Control Science and Engineering, Shandong University, Jinan, China","IEEE Transactions on Cybernetics","","2023","PP","99","1","11","This article investigates the optimal bipartite consensus control (OBCC) problem for unknown second-order discrete-time multiagent systems (MASs). First, the coopetition network is constructed to describe the cooperative and competitive relationships between agents, and the OBCC problem is proposed by the tracking error and related performance index function. Based on the distributed policy gradient reinforcement learning (RL) theory, a data-driven distributed optimal control strategy is obtained to guarantee the bipartite consensus of all agents’ position and velocity states. In addition, the offline data sets ensure the learning efficiency of the system. These data sets are generated by running the system in real time. Besides, the designed algorithm is an asynchronous version, which is essential to solve the challenge caused by the computational ability difference between nodes in MASs. Then, by means of the functional analysis and Lyapunov theory, the stability of the proposed MASs and the convergence of the learning process are analyzed. Furthermore, an actor–critic structure containing two neural networks is used to implement the proposed methods. Finally, a numerical simulation shows the effectiveness and validity of the results.","2168-2275","","10.1109/TCYB.2023.3276797","National Natural Science Foundation of China(grant numbers:62073143,62273255,62003139); Shanghai International Science and Technology Cooperation Project(grant numbers:18510711100); Innovation Program of Shanghai Municipal Education Commission(grant numbers:2021-01-07-00-02-E00107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148621","Data-driven;optimal bipartite consensus control (OBCC);policy gradient;reinforcement learning (RL);second-order multiagent systems (MASs)","Consensus control;Optimal control;Costs;Real-time systems;Neural networks;Multi-agent systems;Smart manufacturing","","","","","","","IEEE","12 Jun 2023","","","IEEE","IEEE Early Access Articles"
"APD: Learning Diverse Behaviors for Reinforcement Learning Through Unsupervised Active Pre-Training","K. Zeng; Q. Zhang; B. Chen; B. Liang; J. Yang","School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Robotics and Automation Letters","27 Oct 2022","2022","7","4","12251","12258","Unsupervised pre-training in reinforcement learning enables the agent to gain prior environmental knowledge, which is then fine-tuned in the supervised stage to quickly adapt to various downstream tasks. In the absence of task-related rewards, pre-training aims to acquire policies (i.e., behaviors) that generate different trajectories to explore and master the environment. Previous research categorizes states into their associated behaviors by learning a supervised discriminator. However, an underlying problem persists: such discriminator is trained in lack of relevant data, leading to an underestimation of reward for new states and inadequate exploration. To this end, we introduce an unsupervised active pre-training algorithm for diverse behavior induction (APD). We explicitly characterize the behavior variables with a state-dependent sampling method, and the agent can decompose the entire state space into parts for fine-grained and diverse behavior learning. Specifically, a particle-based entropy estimator is applied to optimize a combination of behavioral entropy and mutual information objective. Moreover, we develop behavior-based representation learning to compress states into the latent space. Experiments show that our method can improve exploration efficiency and outperforms most state-of-the-art unsupervised algorithms on a number of continuous control tasks in the DeepMind Control Suite.","2377-3766","","10.1109/LRA.2022.3214057","National Natural Science Foundation of China(grant numbers:62073033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9917354","Contrastive Representation Learning;Robotic Control;Unsupervised Reinforcement Learning","Behavioral sciences;Entropy;Mutual information;Task analysis;Uncertainty;Reinforcement learning;Avalanche photodiodes","entropy;learning (artificial intelligence);sampling methods;unsupervised learning","behavior variables;state-dependent sampling method;entire state space;particle-based entropy estimator;behavioral entropy;mutual information objective;behavior-based representation;exploration efficiency;state-of-the-art unsupervised algorithms;continuous control tasks;APD;reinforcement learning;unsupervised pre-training;prior environmental knowledge;supervised stage;downstream tasks;task-related rewards;different trajectories;associated behaviors;supervised discriminator;underlying problem persists;inadequate exploration;unsupervised active pre-training algorithm;diverse behavior induction","","","","31","IEEE","12 Oct 2022","","","IEEE","IEEE Journals"
"Connecting Deep-Reinforcement-Learning-based Obstacle Avoidance with Conventional Global Planners using Waypoint Generators","L. Kästner; X. Zhao; T. Buiyan; J. Li; Z. Shen; J. Lambrecht; C. Marx","Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany","2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","16 Dec 2021","2021","","","1213","1220","Deep Reinforcement Learning has emerged as an efficient dynamic obstacle avoidance method in highly dynamic environments. It has the potential to replace overly conservative or inefficient navigation approaches. However, integrating Deep Reinforcement Learning into existing navigation systems is still an open frontier due to the myopic nature of Deep-Reinforcement-Learning-based navigation, which hinders its widespread integration into current navigation systems. In this paper, we propose the concept of an intermediate planner to interconnect novel Deep-Reinforcement-Learning-based obstacle avoidance with conventional global planning methods using waypoint generation. Therefore, we integrate different waypoint generators into existing navigation systems and compare the joint system against traditional ones. We found an increased performance in terms of safety, efficiency and path smoothness, especially in highly dynamic environments.","2153-0866","978-1-6654-1714-3","10.1109/IROS51168.2021.9636039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636039","","Navigation;Buildings;Reinforcement learning;Robot sensing systems;Generators;Safety;Planning","collision avoidance;deep learning (artificial intelligence);mobile robots;navigation","conventional global planners;reinforcement learning;efficient dynamic obstacle avoidance method;highly dynamic environments;overly conservative navigation approaches;inefficient navigation approaches;widespread integration;current navigation systems;intermediate planner;deep-reinforcement-learning-based obstacle avoidance;conventional global planning methods;waypoint generation;deep-reinforcement-learning-based navigation;waypoint generators","","10","","21","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Linear Quadratic Tracker with Integrator using Integral Reinforcement Learning","O. Park; H. Shin; A. Tsourdos","School of Aerospace Technology and Manufacturing (SATM), Cranfield University, Mk43 0AL, United Kingdom; School of Aerospace Technology and Manufacturing (SATM), Cranfield University, Mk43 0AL, United Kingdom; School of Aerospace Technology and Manufacturing (SATM), Cranfield University, Mk43 0AL, United Kingdom","2019 Workshop on Research, Education and Development of Unmanned Aerial Systems (RED UAS)","17 Feb 2020","2019","","","31","36","This paper describes a Reinforcement Learning (RL) application using Linear Quadratic Regulator (LQR) based tracking controller, which is augmented with a tracking error term. In order to deal with the steady-state errors, Linear Quadratic Tracker with Integrator (LQTI) is designed by adding an integration term of the tracking error in the state variable. Based on the LQTI, an online learning using the Integral Reinforcement Learning (IRL) is applied for the tracking problem to find the optimal control on the partially unknown continuous-time systems by regulating the augmented state variable. The optimal control solution and the performance of the method are verified through numerical simulation on two applications.","","978-1-7281-6600-1","10.1109/REDUAS47371.2019.8999679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999679","","","continuous time systems;control engineering computing;control system synthesis;learning (artificial intelligence);linear quadratic control;optimal control;tracking","LQTI;online learning;integral reinforcement learning;augmented state variable;optimal control solution;linear quadratic tracker;linear quadratic regulator","","2","","12","IEEE","17 Feb 2020","","","IEEE","IEEE Conferences"
"Deep Convolutional AutoEncoders as a Minimal State Representation for Reinforcement Learning in Industrial Robot Manipulators","A. Maldonado-Ramirez; I. Lopez-Juarez; R. Rios-Cabrera","Robotics and Advanced Manufacturing, CINVESTAV-IPN, Mexico; Robotics and Advanced Manufacturing, CINVESTAV-IPN, Mexico; Robotics and Advanced Manufacturing, CINVESTAV-IPN, Mexico","2018 XX Congreso Mexicano de Robótica (COMRob)","14 Apr 2019","2018","","","1","6","Reinforcement Learning has become a very powerful technique in the last decade thanks to the technological advances in computational power and efficient learning algorithms in neuronal networks. Reinforcement Learning allows an autonomous agent to learn through experience how to solve a problem in an optimal way, with a minimal information of its environment, just a reward signal. This approach has lightened the way to an artificial intelligence capable of reaching superhuman performance in games such as Go and Backgammon, beating world champion players. Unfortunately this is far from a real world robot application due to many technological challenges. So in this paper we propose how to get a low-dimensional state representation for industrial robot manipulators, using visual information and Convolutional Autoencoders to speed up the information processing in the Reinforcement Learning training phase.","","978-1-5386-9220-2","10.1109/COMROB.2018.8689419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8689419","","Service robots;Robot kinematics;Robot sensing systems;Reinforcement learning;Manipulators;Games","industrial manipulators;learning (artificial intelligence);neural nets","industrial robot manipulators;powerful technique;computational power;efficient learning algorithms;minimal information;world robot application;low-dimensional state representation;minimal state representation;deep convolutional autoencoders;reinforcement learning training phase","","1","","15","IEEE","14 Apr 2019","","","IEEE","IEEE Conferences"
"Context - Enhanced Meta-Reinforcement Learning with Data-Reused Adaptation for Urban Autonomous Driving","Q. Deng; Y. Zhao; R. Li; Q. Hu; T. Liu; R. Li","Inspur Electronic Information Industry Co., Ltd, Beijing, China; Inspur Electronic Information Industry Co., Ltd, Beijing, China; Inspur Electronic Information Industry Co., Ltd, Beijing, China; Inspur Electronic Information Industry Co., Ltd, Beijing, China; Inspur Electronic Information Industry Co., Ltd, Beijing, China; Inspur Electronic Information Industry Co., Ltd, Beijing, China","2023 International Joint Conference on Neural Networks (IJCNN)","2 Aug 2023","2023","","","1","8","Autonomous driving (AD) has experienced rapid development in recent years, and the reinforcement learning (RL) pipeline in trial-and-error manner can surpass human driving ability. However, the poor performance in sample efficiency and generalization limits RL applying in the challenging urban traffic scenarios. In this paper, we build a context-enhanced meta-RL framework with data-reused adaptation for challenging urban AD. At both the meta-learning and adaptation stages, the context-enhanced state representation is designed to reduce the perceptual gap in variant urban scenarios, improving the sample efficiency and robustness. At adaptation stage, the meta-training data with context-enhanced features are reused through propensity estimation to constrain the optimization objective of new tasks, aiming to maintain the good driving performance of meta-trained policy and fast adapt to the new tasks. Extensive experiments are conducted in CARLA simulator with various urban environments and task settings. The learning curves and quantitative comparisons validate the good sample efficiency and generalization of our proposed method, with state-of-the-art driving performance on urban AD benchmarks.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10191187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191187","Autonomous driving;Meta-reinforcement learning;State encoding;Policy adaptation","Metalearning;Training;Urban areas;Estimation;Reinforcement learning;Robustness;Safety","mobile robots;reinforcement learning;traffic engineering computing","context-enhanced meta-RL framework;context-enhanced state representation;data-reused adaptation;enhanced meta-reinforcement learning;meta-learning;meta-training data;urban AD benchmarks;urban autonomous driving","","","","36","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Towards Learning-automation IoT Attack Detection through Reinforcement Learning","T. Gu; A. Abhishek; H. Fu; H. Zhang; D. Basu; P. Mohapatra","Department of Computer Science, University of California, Davis, CA, USA; ARM Research, Austin, TX, USA; Department of Computer Science, University of California, Davis, CA, USA; Department of Computer Science, University of California, Davis, CA, USA; Department of Computer Science, University of California, Davis, CA, USA; Department of Computer Science, University of California, Davis, CA, USA","2020 IEEE 21st International Symposium on "A World of Wireless, Mobile and Multimedia Networks" (WoWMoM)","9 Oct 2020","2020","","","88","97","As a massive number of the Internet of Things (IoT) devices are deployed, the security and privacy issues in IoT arouse more and more attention. The IoT attacks are causing tremendous loss to the IoT networks and even threatening human safety. Compared to traditional networks, IoT networks have unique characteristics, which make the attack detection more challenging. First, the heterogeneity of platforms, protocols, software, and hardware exposes various vulnerabilities. Second, in addition to the traditional high-rate attacks, the low-rate attacks are also extensively used by IoT attackers to obfuscate the legitimate and malicious traffic. These low-rate attacks are challenging to detect and can persist in the networks. Last, the attackers are evolving to be more intelligent and can dynamically change their attack strategies based on the environment feedback to avoid being detected, making it more challenging for the defender to discover a consistent pattern to identify the attack. In order to adapt to the new characteristics in IoT attacks, we propose a reinforcement learning-based attack detection model that can automatically learn and recognize the transformation of the attack pattern. Therefore, we can continuously detect IoT attacks with less human intervention. In this paper, we explore the crucial features of IoT traffics and utilize the entropy-based metrics to detect both the high-rate and low-rate IoT attacks. Afterward, we leverage the reinforcement learning technique to continuously adjust the attack detection threshold based on the detection feedback, which optimizes the detection and the false alarm rate. We conduct extensive experiments over a real IoT attack data set and demonstrate the effectiveness of our IoT attack detection framework.","","978-1-7281-7374-0","10.1109/WoWMoM49955.2020.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9217785","Internet of Things;Intrusion detection;Reinforcement learning;Anomaly detection;IoT security;Entropy;Wireless traffic;Artificial Intelligence","Entropy;Internet of Things;Measurement;Protocols;Intrusion detection;Learning (artificial intelligence)","data privacy;entropy;Internet of Things;learning (artificial intelligence);security of data;telecommunication traffic","IoT traffics;IoT attack data;learning-automation IoT attack detection;Internet of Things devices;reinforcement learning;environment feedback;entropy-based metrics;privacy issue;security issue","","13","","32","IEEE","9 Oct 2020","","","IEEE","IEEE Conferences"
"Range-Aware Impact Angle Guidance Law With Deep Reinforcement Meta-Learning","C. Liang; W. Wang; Z. Liu; C. Lai; S. Wang","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Navigation and Control Technology Research Institute of China North Industries Group Corporation, Beijing, China; Navigation and Control Technology Research Institute of China North Industries Group Corporation, Beijing, China","IEEE Access","26 Aug 2020","2020","8","","152093","152104","In this article, a new guidance law is proposed for impact angle constrained missile with time-varying velocity against a maneuvering target. The proposed guidance law is based on model-based deep reinforcement learning (RL) technique where a deep neural network is trained to be a predictive model used in model predictive path integral (MPPI) control. Tube-MPPI, a robust approach utilizing ancillary controller for disturbance rejection, is introduced in guidance law design in this work to deal with the MPPI degradation of robustness when the deep predictive model differs with actual environment. To further improve the performance, meta-learning is utilized to enable the deep neural dynamics adapt to environment changes online. With this approach the model mismatch of the nominal controller is reduced to improve tube-MPPI performance. Furthermore, a range-aware hyperbolic function is proposed as an adaptive function in the MPPI performance index design. Thus, reduced initial acceleration command and increased terminal velocity benefit guidance performance. Numerical simulations under various conditions demonstrate the effectiveness of proposed guidance law.","2169-3536","","10.1109/ACCESS.2020.3017480","Defense Industrial Technology Development Program(grant numbers:JCKY2018601B101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9170500","Missile guidance;tube model predictive control;meta-learning;deep reinforcement learning;impact angle constraint","Adaptation models;Missiles;Acceleration;Predictive models;Reinforcement learning;Navigation;Neural networks","adaptive systems;control system synthesis;hyperbolic equations;learning (artificial intelligence);missile guidance;neurocontrollers;predictive control;robust control;time-varying systems","adaptive function;disturbance rejection;impact angle constrained missile;tube-MPPI performance improvement;range-aware impact angle guidance law;terminal velocity benefit guidance performance;MPPI performance index design;range-aware hyperbolic function;deep neural dynamics;deep predictive model;guidance law design;ancillary controller;model predictive path integral control;deep neural network;model-based deep reinforcement learning technique;time-varying velocity;deep reinforcement meta-learning","","6","","41","CCBY","18 Aug 2020","","","IEEE","IEEE Journals"
"Double-deck elevator systems using Genetic Network Programming with reinforcement learning","Jin Zhou; Lu Yu; Shingo Mabu; Kotaro Hirasawa; Jinglu Hu; S. Markon","Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Big Wing, Fujitec Company Limited, Hikone, Shiga, Japan","2007 IEEE Congress on Evolutionary Computation","7 Jan 2008","2007","","","2025","2031","In order to increase the transportation capability of elevator group systems in high-rise buildings without adding elevator installation space, double-deck elevator system (DDES) is developed as one of the next generation elevator group systems. Artificial intelligence (AI) technologies have been employed to find some efficient solutions in the elevator group control systems during the late 20th century. Genetic Network Programming (GNP), a new evolutionary computation method, is reported to be employed as the elevator group system controller in some studies of recent years. Moreover, reinforcement learning (RL) is also verified to be useful for more improvements of elevator group performances when it is combined with GNP. In this paper, we proposed a new approach of DDES using GNP with RL, and did some experiments on a simulated elevator group system of a typical office building to check its efficiency. Simulation results show that the DDES using GNP with RL performs better than the one without RL in regular and down-peak time, while both of them outperforms a conventional approach and a heuristic approach in all three traffic patterns.","1941-0026","978-1-4244-1339-3","10.1109/CEC.2007.4424722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424722","","Elevators;Learning;Economic indicators;Space technology;Artificial intelligence;Control systems;Transportation;Genetic programming;Evolutionary computation;Traffic control","genetic algorithms;learning (artificial intelligence);lifts","double-deck elevator systems;genetic network programming;reinforcement learning;transportation;elevator group systems;high-rise buildings;elevator installation space;artificial intelligence;elevator group control;evolutionary computation","","4","","7","IEEE","7 Jan 2008","","","IEEE","IEEE Conferences"
"A study of applying Genetic Network Programming with Reinforcement Learning to Elevator Group Supervisory Control System","Jin Zhou; T. Eguchi; S. Mabu; K. Hirasawa; Jinglu Hu; S. Markon","Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; World Headquarters, Fujitec Company Limited, Ibaraki, Osaka, Japan","2006 IEEE International Conference on Evolutionary Computation","11 Sep 2006","2006","","","3035","3041","Elevator Group Supervisory Control System (EGSCS) is a very large scale stochastic dynamic optimization problem. Due to its vast state space, significant uncertainty, and numerous resource constraints such as finite car capacities and registered hall/car calls, it is hard to manage EGSCS using conventional control methods. Recently, many solutions for EGSCS using Artificial Intelligence (AI) technologies have been reported. Genetic Network Programming (GNP), which is proposed as a new evolutionary computation method several years ago, is also proved to be efficient when applied to EGSCS problem. In this paper, we propose an extended algorithm for EGSCS by introducing Reinforcement Learning (RL) into GNP framework, and expect to make an improvement of the EGSCS’ performances since the efficiency of GNP with RL has been clarified in some other studies like tile-world problem. Simulation tests using traffic flows in a typical office building have been made, and the results show an actual improvement of the EGSCS’ performances comparing to the algorithms using original GNP and conventional control methods.","1941-0026","0-7803-9487-9","10.1109/CEC.2006.1688692","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688692","","Genetics;Learning;Elevators;Supervisory control;Economic indicators;Artificial intelligence;Large-scale systems;Stochastic systems;State-space methods;Uncertainty","control system analysis computing;genetic algorithms;learning (artificial intelligence);lifts","genetic network programming;GNP;reinforcement learning;elevator group supervisory control system;EGSCS;very large scale stochastic dynamic optimization;artificial intelligence technology;evolutionary computation method;traffic flows;office building","","2","","17","IEEE","11 Sep 2006","","","IEEE","IEEE Conferences"
"Edge Encoded Attention Mechanism to Solve Capacitated Vehicle Routing Problem with Reinforcement Learning","G. Fellek; G. Gebreyesus; A. Farid; S. Fujimura; O. Yoshie","Graduate School of Information, Production and Systems, Waseda University, Kitakyushu City, Japan; Graduate School of Information, Production and Systems, Waseda University, Kitakyushu City, Japan; Graduate School of Information, Production and Systems, Waseda University, Kitakyushu City, Japan; Graduate School of Information, Production and Systems, Waseda University, Kitakyushu City, Japan; Graduate School of Information, Production and Systems, Waseda University, Kitakyushu City, Japan","2022 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)","26 Dec 2022","2022","","","576","582","The capacitated vehicle routing problem (CVRP), which is referred as NP-hard problem is a variant of Traveling Salesman Problem (TSP). CVRP constructs the route with the lowest cost without violating vehicle capacity constraints to meet demands of customer nodes. Following the advent of artificial intelligence and deep learning, the use of deep reinforcement learning (DRL) to solve CVRP is giving promising results. In this paper we proposed DRL model to solve CVRP. The transformer-based encoder of our proposed model fuses node and edge information to construct a rich graph embedding. The proposed architecture is trained using proximal policy optimization (PPO). Experiments using randomly generated test instances show that the proposed model gives rise to better results in comparison with the existing DRL methods. In addition, we also tested our model on locally generated real-world data to verify its performance. Accordingly, the results show that our model has a good generalization performance for both of random instance testing to real-world instance testing.","","978-1-6654-8687-3","10.1109/IEEM55944.2022.9989997","Japan International Cooperation Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9989997","Attention mechanism;transformer;vehicle routing problem","Deep learning;Vehicle routing;Reinforcement learning;Traveling salesman problems;Transformers;Feature extraction;Routing","computational complexity;graph theory;optimisation;reinforcement learning;search problems;traffic engineering computing;travelling salesman problems;vehicle routing","artificial intelligence;capacitated vehicle routing problem;customer nodes;CVRP;deep learning;deep reinforcement learning;DRL model;edge encoded attention mechanism;edge information;existing DRL methods;model fuses node;NP-hard problem;random instance testing;randomly generated test instances;rich graph embedding;transformer-based encoder;vehicle capacity constraints","","","","26","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Robotic Packaging Optimization with Reinforcement Learning","E. Drijver; R. Pérez-Dattari; J. Kober; C. D. Santina; Z. Ajanović","Cognitive Robotics Department, Delft University of Technology, Delft, CD, The Netherlands; Cognitive Robotics Department, Delft University of Technology, Delft, CD, The Netherlands; Cognitive Robotics Department, Delft University of Technology, Delft, CD, The Netherlands; Cognitive Robotics Department, Delft University of Technology, Delft, CD, The Netherlands; Cognitive Robotics Department, Delft University of Technology, Delft, CD, The Netherlands","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","7","Intelligent manufacturing is becoming increasingly important due to the growing demand for maximizing productivity and flexibility while minimizing waste and lead times. This work investigates automated secondary robotic food packaging solutions that transfer food products from the conveyor belt into containers. A major problem in these solutions is varying product supply which can cause drastic productivity drops. Conventional rule-based approaches, used to address this issue, are often inadequate, leading to violation of the industry's requirements. Reinforcement learning, on the other hand, has the potential of solving this problem by learning responsive and predictive policy, based on experience. However, it is challenging to utilize it in highly complex control schemes. In this paper, we propose a reinforcement learning framework, designed to optimize the conveyor belt speed while minimizing interference with the rest of the control system. When tested on real-world data, the framework exceeds the performance requirements (99.8% packed products) and maintains quality (100% filled boxes). Compared to the existing solution, our proposed framework improves productivity, has smoother control, and reduces computation time.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260406","","Productivity;Industries;Training;Service robots;Reinforcement learning;Interference;Belts","belts;conveyors;food packaging;food products;learning (artificial intelligence);optimisation;reinforcement learning","99.8% packed products;computation time;control system;conventional rule-based approaches;conveyor belt speed;drastic productivity drops;existing solution;highly complex control schemes;industry;intelligent manufacturing;lead times;maximizing productivity;performance requirements;predictive policy;product supply;reinforcement learning framework;responsive policy;robotic packaging optimization;secondary robotic food packaging solutions;transfer food products","","","","23","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Deep reinforcement learning with experience replay based on SARSA","D. Zhao; Haitao Wang; Kun Shao; Y. Zhu","Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","13 Feb 2017","2016","","","1","6","SARSA, as one kind of on-policy reinforcement learning methods, is integrated with deep learning to solve the video games control problems in this paper. We use deep convolutional neural network to estimate the state-action value, and SARSA learning to update it. Besides, experience replay is introduced to make the training process suitable to scalable machine learning problems. In this way, a new deep reinforcement learning method, called deep SARSA is proposed to solve complicated control problems such as imitating human to play video games. From the experiments results, we can conclude that the deep SARSA learning shows better performances in some aspects than deep Q learning.","","978-1-5090-4240-1","10.1109/SSCI.2016.7849837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849837","SARSA learning;Q learning;experience replay;deep reinforcement learning;deep learning","Learning (artificial intelligence);Games;Training;Neural networks;Machine learning;Feature extraction","computer games;learning (artificial intelligence);neural nets","deep reinforcement learning;on-policy reinforcement learning;video games control problems;deep convolutional neural network;scalable machine learning problems;deep SARSA learning;Q learning","","59","","20","IEEE","13 Feb 2017","","","IEEE","IEEE Conferences"
"A Model-Based Reinforcement Learning and Correction Framework for Process Control of Robotic Wire Arc Additive Manufacturing","A. G. Dharmawan; Y. Xiong; S. Foong; G. Song Soh","Engineering Product Development, Singapore University of Technology and Design, Pillar, Singapore; Engineering Product Development, Singapore University of Technology and Design, Pillar, Singapore; Engineering Product Development, Singapore University of Technology and Design, Pillar, Singapore; Engineering Product Development, Singapore University of Technology and Design, Pillar, Singapore","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","4030","4036","Robotic Wire Arc Additive Manufacturing (WAAM) utilizes a robot arm as a motion system to build 3D metallic objects by depositing weld beads one above the other in a layer by layer fashion. A key part of this approach is the process study and control of Multi-Layer Multi-Bead (MLMB) deposition, which is very sensitive to process parameters and prone to error stacking. Despite its importance, it has been receiving less attention than its single bead counterpart in literature, probably due to the higher experimental overhead and complexity of modeling. To address these challenges, this paper proposes an integrated learning-correction framework, adapted from Model-Based Reinforcement Learning, to iteratively learn the direct effect of process parameters on MLMB print while simultaneously correct for any inter-layer geometric digression such that the final output is still satisfactory. The advantage is that this learning architecture can be used in conjunction with actual parts printing (hence, in-situ study), thus minimizing the required training time and material wastage. The proposed learning framework is implemented on an actual robotic WAAM system and experimentally evaluated.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197222","","Training;Predictive models;Process control;Adaptation models;Printing;Robots;Three-dimensional displays","learning (artificial intelligence);process control;rapid prototyping (industrial);robotic welding;three-dimensional printing;welds;wires","integrated learning-correction framework;model-based reinforcement learning;process parameters;inter-layer geometric digression;process control;robot arm;3D metallic objects;layer by layer fashion;multilayer multibead deposition control;robotic wire arc additive manufacturing;weld beads;error stacking;material wastage reduction;MLMB print","","16","","27","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
