"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Neural-network-based reinforcement learning control for path following of underactuated ships","L. Zhang; L. Qiao; J. Chen; W. Zhang","Department of Automation, Shanghai Jiao Tong University, Shanghai, CN; Department of Automation, Shanghai Jiao Tong University, Shanghai, CN; Department of Automation, Shanghai Jiao Tong University, Shanghai, CN; Department of Automation, Shanghai Jiao Tong University, Shanghai, CN","2016 35th Chinese Control Conference (CCC)","29 Aug 2016","2016","","","5786","5791","This paper considers the effectiveness of neural-network-based reinforcement learning (RLNN) controller in discrete time for the underactuated ships. The path following problem of underactuated ships is solved by utilizing RLNN controller together with the Line-of-Sight (LOS) guidance law. The proper form of variables in RLNN controller compatible with the variables in ship maneuvering model is derived and the calculation of desired yaw angle in LOS is implemented in a concise way. The proposed controller is model independent with the consideration of output constraints. Our approach is evaluated by simulations on different ship models with different way-point generated paths.","1934-1768","978-9-8815-6391-0","10.1109/ChiCC.2016.7554262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7554262","Path following;LOS (Line-of-Sight) Guidance Law;Neural Networks;Reinforcement Learning;Underactuated ships","Marine vehicles;MATLAB;Surges","discrete time systems;learning (artificial intelligence);learning systems;neurocontrollers;path planning;ships","neural-network-based reinforcement learning control;path following;underactuated ships;RLNN controller;discrete time;line-of-sight guidance law;LOS guidance law;ship maneuvering model;yaw angle;output constraints;ship models","","13","","16","","29 Aug 2016","","","IEEE","IEEE Conferences"
"Adaptive impedance control of robotic exoskeletons using reinforcement learning","Z. Huang; J. Liu; Z. Li; C. -Y. Su","College of Automation Science and Engineering, South China University of Technology, Guangzhou, China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, China","2016 International Conference on Advanced Robotics and Mechatronics (ICARM)","27 Oct 2016","2016","","","243","248","A robotic exoskeleton with adjustable robot behavior is proposed in this paper. The assistive exoskeleton helps the people to execute the task and optimizes the system's performance. We propose a novel adaptive impedance control for the robotic exoskeleton, whose end-effector's motions are prescribed by the relative trajectory and constrained by the physical limits. With the purpose of shaping the dynamics of the robotic exoskeleton to minimize motion tracking errors and human effort, the linear quadratic regulation (LQR) approach is employed to acquire an optimal impedance model. Integral reinforcement learning (IRL) is employed to deal with the given LQR design without the human model's information. The impedance control incorporating adaptive parameter learning technique makes tracking errors convergence while the constrained region is not transgressed by the end-effectors. Experiment results show that the proposed controller is effective in tracking the desired trajectories.","","978-1-5090-3364-5","10.1109/ICARM.2016.7606926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7606926","Linear quadratic regulation (LQR);integral reinforcement learning (IRL);adaptive impedance control","Robots;Mechatronics;Trajectory;Algorithm design and analysis;Integrated circuits","adaptive control;assisted living;convergence;end effectors;learning (artificial intelligence);linear quadratic control;manipulator dynamics;minimisation;trajectory control","adaptive impedance control;assistive exoskeleton;end-effector;robotic exoskeleton dynamics;motion tracking error minimization;human effort minimization;linear quadratic regulation;optimal impedance model;integral reinforcement learning;IRL;LQR design;adaptive parameter learning;tracking error convergence","","6","","17","IEEE","27 Oct 2016","","","IEEE","IEEE Conferences"
"GA+DDPG+HER: Genetic Algorithm-Based Function Optimizer in Deep Reinforcement Learning for Robotic Manipulation Tasks","A. Sehgal; N. Ward; H. M. La; C. Papachristos; S. Louis","Advanced Robotics and Automation (ARA) Laboratory; Advanced Robotics and Automation (ARA) Laboratory; Advanced Robotics and Automation (ARA) Laboratory; Department of Computer Science and Engineering, University of Nevada, Reno, NV, USA; Department of Computer Science and Engineering, University of Nevada, Reno, NV, USA","2022 Sixth IEEE International Conference on Robotic Computing (IRC)","24 Jan 2023","2022","","","85","86","Agents can base decisions made using reinforcement learning (RL) on a reward function. The selection of values for the learning algorithm parameters can, nevertheless, have a substantial impact on the overall learning process. In order to discover values for the learning parameters that are close to optimal, we extended our previously proposed genetic algorithm-based Deep Deterministic Policy Gradient and Hindsight Experience Replay approach (referred to as GA+DDPG+HER) in this study. On the robotic manipulation tasks of FetchReach, FetchSlide, FetchPush, FetchPick&Place, and DoorOpening, we applied the GA+DDPG+HER methodology. Our technique GA+DDPG+HER was also used in the AuboReach environment with a few adjustments. Our experimental analysis demonstrates that our method produces performance that is noticeably better and occurs faster than the original algorithm. We also offer proof that GA+DDPG+HER beat the current approaches. The final results support our assertion and offer sufficient proof that automating the parameter tuning procedure is crucial and does cut down learning time by as much as 57%.","","978-1-6654-7260-9","10.1109/IRC55401.2022.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023883","DRL;DDPG+HER;Reinforcement Learning;Genetic Algorithm;GA+DDPG+HER;DDPG;HER","Deep learning;Reinforcement learning;Task analysis;Robots;Tuning;Genetic algorithms","control engineering computing;deep learning (artificial intelligence);genetic algorithms;gradient methods;manipulators;multi-agent systems;reinforcement learning;robot programming","agents;AuboReach environment;deep reinforcement learning;DoorOpening;FetchPick&Place;FetchPush;FetchReach;FetchSlide;GA+DDPG+HER;genetic algorithm-based deep deterministic policy gradient and hindsight experience replay;genetic algorithm-based function optimizer;learning algorithm parameters;learning process;parameter tuning procedure;reward function;robotic manipulation tasks","","5","","7","IEEE","24 Jan 2023","","","IEEE","IEEE Conferences"
"Cartesian tasks oriented friction compensation through a reinforcement learning approach","L. Roveda; G. Pallucca; N. Pedrocchi; F. Braghin; L. M. Tosatti","Institute of Industrial Technologies and Automation (ITIA), Italian National Research Council (CNR), Milan, Italy; Institute of Industrial Technologies and Automation (ITIA), Italian National Research Council (CNR), Milan, Italy; Institute of Industrial Technologies and Automation (ITIA), Italian National Research Council (CNR), Milan, Italy; Department of Mechanical Engineering, Politecnico di Milano, Milan, Italy; Institute of Industrial Technologies and Automation (ITIA), Italian National Research Council (CNR), Milan, Italy","2016 IEEE International Conference on Advanced Intelligent Mechatronics (AIM)","29 Sep 2016","2016","","","895","900","The paper describes an algorithm to compensate for the friction in the robot joints, while executing a target impedance controlled Cartesian task. The proposed method relies on the reinforcement learning procedure: given a target task in the Cartesian space and a joint space friction model, the algorithm is capable to adapt the friction model parameters based on a specified error function. The proposed error function correlates the Cartesian position tracking error to the joint space friction torques, allowing to independently learn the friction parameters for each joint. In such a way, the friction model parameters can be updated in subsequent iterations, compensating for the friction effects. The proposed algorithm has been validated through experiments. A target Cartesian motion has been specified (such as in a pick and place operation) and the proposed method has allowed to learn the friction model parameters. A Universal Robot UR10 has been used as a test platform, developing the impedance control with robot dynamics compensation.","","978-1-5090-2065-2","10.1109/AIM.2016.7576882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7576882","","Friction;Impedance;Learning (artificial intelligence);Adaptation models;Jacobian matrices;Manipulators","friction;iterative methods;learning (artificial intelligence);position control;robot dynamics;robot kinematics","Cartesian task oriented friction compensation;reinforcement learning approach;robot joint friction;target impedance controlled Cartesian task;Cartesian space;joint space friction model;error function;Cartesian position tracking error;joint space friction torques;target Cartesian motion;pick and place operation;Universal Robot UR10;robot dynamics compensation;impedance control","","5","","24","IEEE","29 Sep 2016","","","IEEE","IEEE Conferences"
"Smart bidding strategy of the demand-side loads based on the reinforcement learning","J. Zhou; K. Wang; W. Mao; Y. Wang; P. Huang","Power Automation Department, China Electric Power Research Institute, Nan Jing, China; Power Automation Department, China Electric Power Research Institute, Nan Jing, China; Power Automation Department, China Electric Power Research Institute, Nan Jing, China; Power Automation Department, China Electric Power Research Institute, Nan Jing, China; Distribution network dispatch department, State Grid Anhui Electrical Power Company, He Fei, China","2017 IEEE Conference on Energy Internet and Energy System Integration (EI2)","4 Jan 2018","2017","","","1","5","With the rapid reform of the electricity market in China, a more practical and smart bidding strategy is needed for the demand-side loads when they become the market participants. Based on the different regulation characteristics, a piecewise regulation cost model is established for continuous or discrete regulation demand loads. Meanwhile, considering the factors besides market demand and historical data, the paper proposes the intelligent bidding strategy based on an adaptive reinforcement learning model. The flowcharts for the process of bidding strategy was also proposed. Finally, the simulation results show that the bidding strategy is simple and easy to operate, which will further improve the decision-making ability of the demand side loads.","","978-1-5386-1427-3","10.1109/EI2.2017.8245286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8245286","Electricity market;Demand-side bidding;Reinforcement learning;Bidding acceptance probability","Load modeling;Probability;Learning (artificial intelligence);Adaptation models;Analytical models;Data models;Power demand","costing;decision making;demand side management;learning (artificial intelligence);power engineering computing;power markets","discrete regulation demand loads;market demand;intelligent bidding strategy;adaptive reinforcement learning model;demand side loads;smart bidding strategy;demand-side loads;electricity market;market participants;piecewise regulation cost model;continuous regulation demand loads;regulation characteristics","","4","","13","IEEE","4 Jan 2018","","","IEEE","IEEE Conferences"
"A Soft Graph Attention Reinforcement Learning for Multi-Agent Cooperation","H. Wang; Z. Pu; Z. Liu; J. Yi; T. Qiu","Scholl of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Scholl of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Scholl of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China","2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)","8 Oct 2020","2020","","","1257","1262","The multi-agent reinforcement learning (MARL) suffers from several issues when it is applied to large-scale environments. Specifically, the communication among the agents is limited by the communication distance or bandwidth. Besides, the interactions among the agents are complex in large-scale environments, which makes each agent hard to take different influences of the other agents into consideration and to learn a stable policy. To address these issues, a soft graph attention reinforcement learning (SGA-RL) is proposed. By taking the advantage of the chain propagation characteristics of graph neural networks, stacked graph convolution layers can overcome the limitation of the communication and enlarge the agents' receptive field to promote the cooperation behavior among the agents. Moreover, unlike traditional multi-head attention mechanism which takes all the heads into consideration equally, a soft attention mechanism is designed to learn each attention head's importance, which means that each agent can learn how to treat the other agents' influence more effectively during large-scale environments. The results of the simulations indicate that the agents can learn stable and complicated cooperative strategies with SGA-RL in large-scale environments.","2161-8089","978-1-7281-6904-0","10.1109/CASE48305.2020.9216877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9216877","","Convolution;Learning (artificial intelligence);Neural networks;Optimization;Bandwidth;Multi-agent systems;Games","graph theory;learning (artificial intelligence);multi-agent systems;neural nets","stacked graph convolution layers;multihead attention mechanism;large-scale environments;soft graph attention reinforcement learning;multiagent cooperation;multiagent reinforcement learning;communication distance;graph neural networks;SGA-RL","","3","","21","IEEE","8 Oct 2020","","","IEEE","IEEE Conferences"
"Fuzzy C-means method for representation policy iteration in reinforcement learning","Z. Huang; X. Xu; J. Wu; L. Zuo","College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China","Proceedings of 2012 9th IEEE International Conference on Networking, Sensing and Control","28 May 2012","2012","","","175","180","This paper introduces a Fuzzy C-means method as the subsampling method for Representation Policy Iteration (RPI) in Reinforcement Learning. RPI is a new class of algorithm that automatically learns both basis functions and approximately optimal policy. In this paper the procedures of the RPI algorithm are as follows. Firstly samples are collected using a random or guided policy. The subset samples are obtained from the original ones subsequently by using the Fuzzy C-means (FCM) method as the subsampling method. Then global basis functions called proto-value functions (PVFs) are formed by using the eigenfunctions of the graph Laplacian operator on an undirected graph constructed from the subset samples. Finally, the least-square policy iteration (LSPI) as the parameter estimation method is used for learning an approximately optimal policy. Illustrative experiments on an Inverted Pendulum problem were accomplished to compare the performance of RPI using the FCM method as the subsampling method with that using the previous subsampling method.","","978-1-4673-0390-3","10.1109/ICNSC.2012.6204912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6204912","Reinforcement learning;representation policy iteration;subsampling;fuzzy c-means;proto-value function;graph laplacian operator","Economic indicators;Laplace equations;Function approximation;Approximation algorithms;Clustering algorithms;Eigenvalues and eigenfunctions","fuzzy set theory;iterative methods;learning (artificial intelligence);least squares approximations","fuzzy C-means method;policy iteration representation;reinforcement learning;subsampling method;representation policy iteration;RPI;optimal policy approximation;FCM;proto value functions;PVF;graph Laplacian operator;eigenfunctions;undirected graph construction;subset samples;least square policy iteration;LSPI;parameter estimation method;inverted pendulum problem","","2","","10","IEEE","28 May 2012","","","IEEE","IEEE Conferences"
"Research on path planning of robot based on deep reinforcement learning","F. Liu; C. Chen; Z. Li; Z. -H. Guan; H. O. Wang","School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; College of Automation, Huazhong University of Science and Technology, Wuhan, China; Department of Mechanical Engineering, Boston University, Boston, MA, USA","2020 39th Chinese Control Conference (CCC)","9 Sep 2020","2020","","","3730","3734","In this paper, to avoid the problem of local optimization and slow convergence in complex environment, a reinforcement learning algorithm is proposed to solve the problem. A robot path planning model is built and its feasibility is verified by simulation. In addition, this paper proposes a deep environment to neural network for robot camera to establish a deep reinforcement learning path planning model, and establishes a deep recursive Q-network (DRQN) and Deep Dueling Q-network(DDQN) respectively. In the comparison of the final simulation results, DRQN needs to consume more computation time, but can achieve better results with higher accuracy.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9188890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9188890","Reinforcement learning;path planning;robots;deep reinforcement learning","Path planning;Neural networks;Learning (artificial intelligence);Markov processes;Cameras;Robot vision systems","learning (artificial intelligence);mobile robots;path planning","local optimization;complex environment;robot path planning;deep environment;neural network;robot camera;deep reinforcement learning path;deep recursive Q-network;DRQN;deep dueling Q-network;DDQN","","2","","7","","9 Sep 2020","","","IEEE","IEEE Conferences"
"Adaptive Reinforcement Learning Tracking Control for Second-Order Multi-Agent Systems","W. Bai; L. Cao; G. Dong; H. Li","School of Automation, Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, P. R. China; School of Automation, Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, P. R. China; School of Automation, Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, P. R. China; School of Automation, Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, P. R. China","2019 IEEE 8th Data Driven Control and Learning Systems Conference (DDCLS)","25 Nov 2019","2019","","","202","207","In this paper, the adaptive reinforcement learning tracking control problem is studied for second-order pure-feedback multi-agent systems (MASs). The pure-feedback MASs are transformed into strict-feedback form by using the mean value theorem. The reinforcement learning approach is applied to handle the unknown functions and system control performance index. Moreover, the error terms are introduced to the controller, which can improve the robust of the control scheme. The theoretical analysis indicates that all the signals and tracking errors in close-loop system are semi-global uniformly ultimately bounded (SGUUB), and the numerical simulation are conducted to verify the superiority of this scheme.","","978-1-7281-1454-5","10.1109/DDCLS.2019.8908978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8908978","Adaptive Reinforcement Learning;Multi-Agent Systems;and Tracking Control","Reinforcement learning;Multi-agent systems;Performance analysis;Cost function;Backstepping;Control systems","adaptive control;closed loop systems;control system synthesis;feedback;learning (artificial intelligence);multi-agent systems;performance index","close-loop system;adaptive reinforcement learning tracking control problem;second-order pure-feedback multiagent systems;pure-feedback MASs;strict-feedback form;mean value theorem;system control performance index;unknown functions;semiglobal uniformly ultimately bounded;SGUUB;control design","","2","","12","IEEE","25 Nov 2019","","","IEEE","IEEE Conferences"
"Asynchronous Localization with Stratification Effect for Underwater Target: A Reinforcement Learning-based Approach","Y. Gong; X. Li; J. Yan; X. Luo","Department of Automation, Yanshan University, Qinhuangdao, China; Department of Automation, Yanshan University, Qinhuangdao, China; Department of Automation, Yanshan University, Qinhuangdao, China; Department of Automation, Yanshan University, Qinhuangdao, China","2019 3rd International Symposium on Autonomous Systems (ISAS)","11 Jul 2019","2019","","","91","96","In this paper, we are concerned with the localization of underwater target under the asynchronous clock and stratification effect. A network architecture is established that comprises of surface buoys, sensor nodes and the target. Sensor nodes act as anchor nodes and communicate with target. With the collected localization messages, the relationship of time differences and propagation delay is established. Then the reinforcement learning-based approach is designed to solve the localization optimization problem. The value iteration process is given to determine the optimal policy. Finally, simulation results are presented to show the effectiveness of the proposed method.","","978-1-7281-1298-5","10.1109/ISASS.2019.8757772","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8757772","Underwater wireless sensor networks;localization;asynchronous;stratification effect;reinforcement learning","Optical buffering;5G mobile communication;Optical network units;Minimization;Resource management","iterative methods;learning (artificial intelligence);optimisation;telecommunication computing;underwater acoustic communication;wireless sensor networks","asynchronous clock;stratification effect;network architecture;surface buoys;sensor nodes;anchor nodes;collected localization messages;reinforcement learning-based approach;localization optimization problem;asynchronous localization;underwater target localization;propagation delay;time differences;value iteration process;optimal policy","","1","","17","IEEE","11 Jul 2019","","","IEEE","IEEE Conferences"
"Control of Ski Robot Based on Deep Reinforcement Learning","Z. Wu; J. Ye; X. Wang; F. Li","School of Automation Engineering, University of Electronic Science and Technology of China; School of Automation Engineering, University of Electronic Science and Technology of China; School of Automation Engineering, University of Electronic Science and Technology of China; School of Automation Engineering, University of Electronic Science and Technology of China","2021 International Conference on Security, Pattern Analysis, and Cybernetics（SPAC)","27 Sep 2021","2021","","","211","215","This paper describes a humanoid robot developed for the 2020 Beijing Ski Robot Challenge. The goal is to design a skiing robot that can independently perform skiing movements to reach a designated destination. Aiming at the biped alpine skiing robot, we proposed a skiing control algorithm based on DDPG reinforcement learning. In this paper, the approximate method is used to establish the relationship between tilting angle, skateboard cutting angle and turning radius of the robot. In order to simplify the dimension of the output of the control algorithm, the relationship among turning radius and foot length and toe distance is established. We established the relationship among the turning radius, the length of the feet and the distance between the toes, and also controlled the turning radius by controlling the length of the feet and the distance between the feet, moreover, we obtained the angles of each joint of the humanoid robot by establishing the kinematics model of the humanoid robot. The control algorithm uses critic network to evaluate the state-action value, and uses actor network to generate the parameters of foot length and foot spacing in real time. In the process of DDPG network training, the concept of zero torque point (ZMP) of the robot is introduced, and the ZMP point of the robot is used to judge whether the robot falls or not, and the termination time of the sequence is determined, and the training is carried out on the gym simulation platform.","","978-1-6654-4322-7","10.1109/SPAC53836.2021.9539926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9539926","skiing robot;ZMP;reinforcement learning;DDPG","Training;Analytical models;Torque;Heuristic algorithms;Humanoid robots;Reinforcement learning;Kinematics","humanoid robots;learning (artificial intelligence);mobile robots;robot kinematics;sport","turning radius;humanoid robot;state action value;zero torque point;kinematics model;toe distance;tilting angle;DDPG reinforcement learning;skiing control algorithm;biped alpine skiing robot;deep reinforcement learning","","1","","12","IEEE","27 Sep 2021","","","IEEE","IEEE Conferences"
"Multi-Agent Cooperative-Competitive Environment with Reinforcement Learning","S. -Y. Huang; B. Hu; R. -Q. Liao; J. -W. Xiao; D. -X. He; Z. -H. Guan","School of Artificial intelligence and Automation, Huazhong University of Science and Technology, Wuhan, PR China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, PR China; Petroleum and Engineering College, Yangtze University, Jingzhou, PR China; School of Artificial intelligence and Automation, Huazhong University of Science and Technology, Wuhan, PR China; School of Artificial intelligence and Automation, Huazhong University of Science and Technology, Wuhan, PR China; School of Artificial intelligence and Automation, Huazhong University of Science and Technology, Wuhan, PR China","2019 IEEE 8th Data Driven Control and Learning Systems Conference (DDCLS)","25 Nov 2019","2019","","","1382","1386","This paper studies the multi-agent pursuit-evasion problem. When the mathematical model of agent is unknown, it's effective to use machine learning algorithm to design the policy of each agent. According to the cooperation among pursuers and competition between evaders and pursuers, we choose deterministic policy gradient of reinforcement learning as our basic approach. In this study, we redesign the reward function and the structure of neural network to adapt to the actual environment where evader has greater speed and accelerated speed than pursuers. The character of this algorithm is that it only takes coordinates of agents as controller input without other information like speed, in particular, this algorithm would keep effective even the environment transform to higher dimensional space. Finally, we verify the validity of our algorithm in experiment.","","978-1-7281-1454-5","10.1109/DDCLS.2019.8909048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8909048","Multi-agent;pursuit-evasion problem;Reinforcement Learning;Deterministic Policy Gradient;Actor-Critic","Reinforcement learning;Neural networks;Training;Games;Markov processes;Transforms;Task analysis","gradient methods;learning (artificial intelligence);multi-agent systems;neural nets","reinforcement learning;multiagent pursuit-evasion problem;mathematical model;machine learning algorithm;deterministic policy gradient;reward function;neural network;multiagent cooperative-competitive environment","","1","","14","IEEE","25 Nov 2019","","","IEEE","IEEE Conferences"
"Prediction of Boiler Combustion Energy Efficiency via Deep Reinforcement Learning","H. Jiang; Z. Cai; T. Zhang; C. Peng","College of Automation & College of Articial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Automation & College of Articial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Automation & College of Articial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","2658","2662","Aiming at the problem of energy efficiency prediction during boiler combustion, a prediction method is proposed. The boiler coal feed speed, steam value, feed water speed and power output are used as state variables, and the fuel utilization and rate of boiler combustion are used as the reward function to perform deep reinforcement learning algorithms. The experimental results show that the boiler combustion energy efficiency prediction system based on deep reinforcement learning proposed in this paper is effective and accurate, and performs well in the prediction process.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9549779","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9549779","Boiler combustion;Deep Q-Learning;Prediction","Linear systems;Reinforcement learning;Coal;Predictive models;Boilers;Prediction algorithms;Combustion","boilers;coal;combustion;deep learning (artificial intelligence);mechanical engineering computing","prediction method;boiler coal feed speed;steam value;feed water speed;deep reinforcement learning algorithms;boiler combustion energy efficiency prediction system;prediction process","","","","11","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Optimal Tracking Control for Hypersonic Flight Vehicle: A Model Free Approach","X. Hu; K. Dong; T. Yang; B. Xiao","School of Automation, Northwestern Polytechnical University, Xi’an, P. R. China; School of Automation, Northwestern Polytechnical University, Xi’an, P. R. China; School of Automation, Northwestern Polytechnical University, Xi’an, P. R. China; School of Automation, Northwestern Polytechnical University, Xi’an, P. R. China","2022 IEEE 20th International Conference on Industrial Informatics (INDIN)","15 Dec 2022","2022","","","711","717","The tracking control of hypersonic flight vehicle (HFV) is discussed in this paper, and the nonlinear model of HFV is assumed to be completely unknown. This problem is surely challenging because of the missing prior knowledge, but is more closer to reality since the exact mode of HFV is difficult to be obtained. A reinforcement learning (RL) based optimal controller is proposed for the tracking control of HFV. A model based RL algorithm is firstly proposed and then, based on this algorithm, a model free algorithm is constructed. For relaxing the environmental conditions, neural network (NN) is adopted for the approximation of Critic and Actor, and then a Greedy Policy based updated learning law for NN is derived. The presented RL based control strategy is carried on the nonlinear model of HFV to show its effectiveness.","","978-1-7281-7568-3","10.1109/INDIN51773.2022.9976071","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9976071","Hypersonic flight vehicles (HFV);reinforcement learning(RL);Model free","Simulation;Process control;Reinforcement learning;Artificial neural networks;Approximation algorithms;History;Informatics","aircraft control;approximation theory;control engineering computing;greedy algorithms;hypersonic vehicles;neural nets;nonlinear control systems;optimal control;reinforcement learning","Critic and Actor approximation;greedy policy;HFV;hypersonic flight vehicle;model free approach;neural network;NN;nonlinear model;optimal controller;optimal tracking control;reinforcement learning;RL","","","","16","IEEE","15 Dec 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning Consensus Control for Discrete-Time Multi-Agent Systems","X. Zhu; X. Yuan; Y. Wang; C. Sun","School of Automation, Southeast University, Nanjing, Jiangsu, P. R. China; School of Automation, Southeast University, Nanjing, Jiangsu, P. R. China; School of Automation, Southeast University, Nanjing, Jiangsu, P. R. China; School of Automation, Southeast University, Nanjing, Jiangsu, P. R. China","2019 Chinese Control Conference (CCC)","17 Oct 2019","2019","","","6178","6182","In this paper, the consensus control of leader-follower multi-agent systems is investigated. To achieve the consensus of the discrete-time multi-agent systems, the data-driven iterative neighbor and target Q-learning algorithm is proposed. To implement the proposed method, the actor-critic architecture with neighbor and target networks are employed to approximate the Q-function and control signal. The reasonable reinforcement signal and cost function are chosen from the environment. This method is independent on the accurate system model where most practical systems are too complicated to build the accurate models. Finally, the simulation example is given to demonstrate the effectiveness of the proposed approach.","1934-1768","978-9-8815-6397-2","10.23919/ChiCC.2019.8865975","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8865975","Consensus;Reinforcement Learning;Actor-Critic Networks;Neighbor Networks;Target Networks","Multi-agent systems;Topology;Reinforcement learning;Iterative algorithms;Control systems;Consensus algorithm;Games","discrete time systems;iterative methods;learning systems;multi-agent systems","consensus control;discrete-time multiagent systems;leader-follower multiagent systems;target networks;reasonable reinforcement signal;cost function;accurate system model;Q-function;control signal;actor-critic architecture","","","","29","","17 Oct 2019","","","IEEE","IEEE Conferences"
"Policy gradient fuzzy reinforcement learning","Xue-Ning Wang; Xin Xu; Han-Gen He","Institute of Automation, National University of Defense Technology, Changsha, China; Institute of Automation, National University of Defense Technology, Changsha, China; Institute of Automation, National University of Defense Technology, Changsha, China","Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826)","24 Jan 2005","2004","2","","992","995 vol.2","This work presents a new approach for tuning conclusions of fuzzy rules based on reinforcement learning. Unlike the most of existing fuzzy reinforcement learning algorithms, which are based on value function, while our approach called policy gradient fuzzy reinforcement learning (PGFRL) bases on gradient estimate. In PGFRL, the algorithm GPOMDP is employed to estimate the performance gradient with respect to the parameters of fuzzy rules. In our work we prove the convergence of fuzzy rules' parameters to a local optimum given necessary conditions. The experiment results show the effectiveness of PGFRL.","","0-7803-8403-2","10.1109/ICMLC.2004.1382332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1382332","","Learning;Fuzzy control;Fuzzy systems;Control systems;State estimation;Fuzzy neural networks;Convergence;Computational modeling;Equations;Helium","fuzzy control;learning (artificial intelligence);gradient methods","fuzzy rules;fuzzy control;policy gradient fuzzy reinforcement learning;gradient estimate","","","","7","IEEE","24 Jan 2005","","","IEEE","IEEE Conferences"
"Omnidirectional Drift Control of an Underwater Biomimetic Vehicle-Manipulator System via Reinforcement Learning","R. Ma; Y. Wang; R. Wang; S. Wang","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS)","25 Jun 2021","2021","","","254","258","This paper proposes an omnidirectional drift control on an underwater biomimetic vehicle-manipulator system (UB-VMS). The UBVMS has two biomimetic propellers, they obtain propulsive force by actuating their undulating fins. A configuration of the system, the dynamics of the UBVMS and the kinematics of the fins are given respectively. Then the control problem is designed as a Markov decision process (MDP). A reinforcement learning method based on the twin delayed deep deterministic policy gradient (TD3) is proposed for this MDP. A control policy is well trained by the reinforcement learning method and tested in eight different simulations. An analysis of the simulation results is also given.","2767-9861","978-1-6654-2423-3","10.1109/DDCLS52934.2021.9455641","National Key Research and Development Program of China(grant numbers:2020YFC1512202); Youth Innovation Promotion Association CAS(grant numbers:2018162); National Natural Science Foundation of China(grant numbers:U1713222,62073316,U1806204,62033013); Chinese Academy of Science(grant numbers:XDB32050100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9455641","Omnidirectional Drift Control;Undulating Fin;Underwater Biomimetic Vehicle-manipulator System (UBVMS);Reinforcement Learning;Twin Delayed Deep Deterministic policy gradient (TD3)","Training;Learning systems;Propellers;Simulation;Biological system modeling;Process control;Reinforcement learning","biomimetics;control system synthesis;decision theory;delays;gradient methods;learning (artificial intelligence);learning systems;manipulator dynamics;manipulator kinematics;marine propulsion;Markov processes;mobile robots;motion control;position control;propellers;underwater vehicles","reinforcement learning method;omnidirectional drift control;underwater biomimetic vehicle-manipulator system;biomimetic propellers;propulsive force;undulating fins;UBVMS dynamics;fin kinematics;control design;Markov decision process;MDP;twin delayed deep deterministic policy gradient;control policy training","","","","12","IEEE","25 Jun 2021","","","IEEE","IEEE Conferences"
"Cooperative Target Pursuit by Multiple Fixed-wing UAVs Based on Deep Reinforcement Learning and Artificial Potential Field","F. Zhao; Y. Hua; H. Zheng; J. Yu; X. Dong; Q. Li; Z. Ren","The School of Automation Science and Electrical Engineering,Beihang University, Beijing, China; Institute of Artificial Intelligence,Beihang University, Beijing, China; Beijing Academy of Blockchain and Edge Computing, Beijing, China; The School of Automation Science and Electrical Engineering,Beihang University, Beijing, China; Institute of Artificial Intelligence,Beihang University, Beijing, China; The School of Automation Science and Electrical Engineering,Beihang University, Beijing, China; The School of Automation Science and Electrical Engineering,Beihang University, Beijing, China","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","5693","5698","This paper presents a cooperative target pursuit algorithm based on deep reinforcement learning and artificial potential field for multiple fixed-wing unmanned aerial vehicles (UAVs) in the complex environment. In the proposed algorithm, decentralized deep deterministic policy gradient is employed to learn cooperative target pursuit policies that are adaptive to complex environments including static obstacles and dynamic evader. The artificial potential field method is combined into the learning process to avoid obstacles and collision. In order to cooperatively pursuit, the reward is designed to incentivize the capture of target and encourage a beneficial formation of pursuers. Finally, the simulation shows the feasibility of the proposed algorithm in learning multiple cooperative pursuit strategies.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10241187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10241187","Cooperative Target Pursuit;Decentralized Deep Deterministic Policy Gradient;Artificial Potential Field","Deep learning;Heuristic algorithms;Simulation;Reinforcement learning;Learning (artificial intelligence);Autonomous aerial vehicles;Safety","autonomous aerial vehicles;collision avoidance;deep learning (artificial intelligence);gradient methods;learning (artificial intelligence);mobile robots;reinforcement learning","artificial potential field method;complex environment;cooperatively pursuit;decentralized deep deterministic policy gradient;deep reinforcement learning;fixed-wing unmanned aerial vehicles;learning process;multiple cooperative pursuit strategies;multiple fixed-wing UAVs;target pursuit algorithm;target pursuit policies","","","","17","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Quantum Reinforcement Learning for Multi-Armed Bandits","Y. -P. Liu; K. Li; X. Cao; Q. -S. Jia; X. Wang","Department of Automation,BNRist, CFIN, Tsinghua University, Beijing, China; Department of Automation,BNRist, CFIN, Tsinghua University, Beijing, China; Department of Automation,BNRist, CFIN, Tsinghua University, Beijing, China; Department of Automation,BNRist, CFIN, Tsinghua University, Beijing, China; Guizhou University, Guiyang, China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","5675","5680","This work focuses on the multi-armed bandits (MAB) problem and proposes a quantum reinforcement learning (RL) algorithm for action selection. Existing quantum RL algorithms generally assume that some prior information about the optimal action is known, and initial probability is set unequally. Our algorithm can be executed with equal initial probability on each action, and can greatly accelerate the learning process.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902595","Natural Science Foundation of China(grant numbers:62125304,62073182); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902595","quantum computation;reinforcement learning;multi-arm bandits","Quantum computing;Reinforcement learning;Computational efficiency","probability;quantum computing;quantum theory;reinforcement learning","action selection;equal initial probability;multiarmed bandits problem;optimal action;quantum reinforcement learning algorithm;quantum RL algorithms","","","","22","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Cooperative multi-agent reinforcement learning based on online heuristic extraction","J. Wu; X. Xu; Zhen-ping Sun; Yan Huang","College of Mechatronics and Automation, National University of Defense Technology, Changsha, P.R China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, P.R China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, P.R China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, P.R China","2011 Seventh International Conference on Natural Computation","19 Sep 2011","2011","3","","1299","1303","Reinforcement learning has been an important technique for adaptive decision-making of multi-agent systems in uncertain environments. However, the curse of dimensionality in multi-agent reinforcement learning usually causes the slow learning convergence or even failure. In this paper, a novel Online Heuristics Extraction method, which can integrate the prior heuristic policy with a learned heuristic policy, is presented. The new method can be incorporated into a tabular or approximate cooperative multi-agent reinforcement learning algorithm so as to speed up the learning process. Simulation results on a cooperative learning task show that, with the new method, a much better learning convergence can be achieved.","2157-9563","978-1-4244-9953-3","10.1109/ICNC.2011.6022301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6022301","multi-agent;reinforcement learning;heuristic policy;cooperative;policy iteration","Approximation algorithms;Learning systems;Heuristic algorithms;Learning;Joints;Robots;Convergence","iterative methods;learning (artificial intelligence);multi-agent systems","cooperative multiagent reinforcement learning algorithm;adaptive decision making;online heuristic extraction method;heuristic policy","","","","11","IEEE","19 Sep 2011","","","IEEE","IEEE Conferences"
"Use of Reinforcement Learning in the Modeling of Ring-Type Water Networks","A. Buchkovski; Z. Markov; V. Iliev; D. Babunski","Department of Hydraulic Engineering and Automation, Faculty of Mechanical Engineering, University St. Cyril and Methodius, Skopje, North Macedonia; Department of Hydraulic Engineering and Automation, Faculty of Mechanical Engineering, University St. Cyril and Methodius, Skopje, North Macedonia; Department of Hydraulic Engineering and Automation, Faculty of Mechanical Engineering, University St. Cyril and Methodius, Skopje, North Macedonia; Department of Hydraulic Engineering and Automation, Faculty of Mechanical Engineering, University St. Cyril and Methodius, Skopje, North Macedonia","2023 12th Mediterranean Conference on Embedded Computing (MECO)","26 Jun 2023","2023","","","1","4","In this paper it is proposed to use reinforcement learning (RL) based predictive control of ring-type water supply network for finding the optimal path from point of water supply to point of water consumption avoiding some obstacles that may arise along the way. An example of an obstruction may be a defect in a section point (consumer) through which the water cannot flow. Ring-type water supply network consists of a number of closed rings surrounding the consumers while supplying them with water through sections. In this paper, a computer program was developed in LabVIEW in order to calculate the optimal path from point of water supply to point of water consumption based on reinforcement learning techniques. In order to achieve the goal, Markov Decision Processes, Q-Learning, Bellman Equations and Policy Gradients were used. The results show the optimal path.","2637-9511","979-8-3503-2291-0","10.1109/MECO58584.2023.10154904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10154904","Reinforcement learning;LabVIEW;Ring-type water supply networks","Training;Embedded computing;Q-learning;Computational modeling;Predictive models;Markov processes;Prediction algorithms","learning (artificial intelligence);Markov processes;optimal control;predictive control;reinforcement learning;water supply","closed rings;optimal path;reinforcement learning techniques;ring-type water networks;ring-type water supply network;section point;water consumption","","","","18","IEEE","26 Jun 2023","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning for Distributed Cooperative Targets Search","Y. Sun; Z. Wu; Q. Zhang; Z. Shi; Y. Zhong","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","2021 IEEE International Conference on Unmanned Systems (ICUS)","22 Dec 2021","2021","","","711","716","Many real-world applications require intelligent agents to coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of multi-agent cooperative search has emerged as an important challenge for swarm intelligence research, owing to its endogenous complicated behavior mechanism and interaction with the uncertain dynamic environment. We choose to address the challenge by proposing a decentralized reinforcement learning method DELTAS for a team of unmanned aerial vehicles to perform an efficient search of an area of interest. Specifically, we study the problem that multiple searchers cooperatively search for an unknown number of targets in a partially known environment with inaccurate prior distribution on the targets' locations. It is assumed that both autonomous searchers and targets only have limited kinematics and sensing capabilities. In the designed network, the total value function is fed with the global state to stabilize its learning process, which then factored as a weighted summation of local action values across agents to alleviate the bias in value estimation. Finally, two sets of simulated experiments are designed to verify the advantages of the proposed method.","","978-1-6654-3885-8","10.1109/ICUS52573.2021.9641124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9641124","unmanned systems;cooperative search;multi-agent reinforcement learning","Conferences;Estimation;Reinforcement learning;Kinematics;Search problems;Autonomous aerial vehicles;Sensors","autonomous aerial vehicles;multi-agent systems;reinforcement learning","decentralized reinforcement learning method;unmanned aerial vehicles;multiple searchers;autonomous searchers;total value function;multiagent reinforcement learning;distributed cooperative targets search;real-world applications;intelligent agents;complex environments;swarm intelligence research;endogenous complicated behavior mechanism;uncertain dynamic environment;DELTAS","","","","7","IEEE","22 Dec 2021","","","IEEE","IEEE Conferences"
"Optimal dispatch of an integrated energy system based on deep reinforcement learning considering new energy uncertainty","Y. Zhou; L. Jia; Y. Zhao; Z. Zhan","School of Mechatronics Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronics Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronics Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronics Engineering and Automation, Shanghai University, Shanghai, China","2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)","7 Jul 2023","2023","","","804","809","As the uncertainties of intermittent energy and load in the integrated energy system gradually increase, traditional dispatch methods are limited to fixed physical models and parameter settings that can hardly respond to the random fluctuations in the dynamic system with source-load. In this paper, a deep reinforcement learning-based dynamic dispatch method for the integrated energy system is proposed to address this problem. First, a data-driven deep reinforcement learning model is constructed for the integrated energy system. Through the continuous interaction between the agent and the integrated energy system, the dispatch strategies are learned adaptively to reduce dependence on the physical models. Secondly, the variations of source-load uncertainties are characterized by adding random disturbances. Pivotal aspects such as state spaces, action spaces, reward mechanisms, and the training process of the deep reinforcement learning model are improved according to the characteristics of uncertainties. Then a proximal policy optimization algorithm is used to solve the problem, and the dynamic dispatch decisions of the integrated energy system are realized. Finally, simulation results verify the feasibility and effectiveness of the proposed method over different time scales and in uncertain environments.","2767-9861","979-8-3503-2105-0","10.1109/DDCLS58216.2023.10166885","National Natural Science Foundation of China(grant numbers:92067105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10166885","Integrated energy system;dynamic dispatch;deep reinforcement learning;proximal policy optimization","Deep learning;Adaptation models;Analytical models;Uncertainty;Biological system modeling;Simulation;Heuristic algorithms","deep learning (artificial intelligence);learning (artificial intelligence);optimisation;power generation dispatch;reinforcement learning","data-driven deep reinforcement learning model;deep reinforcement learning-based dynamic dispatch method;integrated energy system;intermittent energy","","","","13","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Optimization Algorithm Designed for IRS-Assisted Edge Computing","K. Liu; F. Lin; Y. Zhao; J. Zhang","School of Information and Automation Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; School of Information and Automation Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; School of Information and Automation Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; School of Information and Automation Qilu University of Technology (Shandong Academy of Sciences), Jinan, China","2023 IEEE 6th International Conference on Electronic Information and Communication Technology (ICEICT)","22 Sep 2023","2023","","","835","840","To reduce the computing delay of the terminals and improve the poor wireless channel environment, in this paper, a computing resource allocation algorithm for IRS-assisted edge computing is proposed. By introducing the intelligent reflecting surface (IRS) technique to reduce the edge computing latency, the computing offloading variables are optimized through an Alternating optimization approach, and a partial offloading strategy for MEC offloading is adopted. Deep reinforcement learning (DRL) is applied for coarse phase control of the IRS, and fine phase control of the IRS is achieved through fine phase optimization. In simulations, the proposed scheme is demonstrated to be effective and low latency.","2836-7782","979-8-3503-9905-9","10.1109/ICEICT57916.2023.10245923","National Natural Science Foundation of China(grant numbers:U2006222); Natural Science Foundation of Shandong province(grant numbers:ZR2020MF138); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10245923","edge computing;intelligent reflecting surface (IRS);deep reinforcement learning (DRL)","Deep learning;Wireless communication;Simulation;Reinforcement learning;Phase control;Servers;Low latency communication","deep learning (artificial intelligence);edge computing;mobile computing;optimisation;reinforcement learning;resource allocation;telecommunication computing","alternating optimization approach;computing delay reduction;computing offloading variables;computing resource allocation algorithm;deep reinforcement learning optimization algorithm;DRL;edge computing latency reduction;fine phase optimization;intelligent reflecting surface technique;IRS-assisted edge computing;MEC offloading","","","","8","IEEE","22 Sep 2023","","","IEEE","IEEE Conferences"
"Device Placement Optimization for Deep Neural Networks via One-shot Model and Reinforcement Learning","Z. Ding; Y. Chen; N. Li; D. Zhao","State Key Laboratory of Management and Control for Complex Systems Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems Institute of Automation, Chinese Academy of Sciences, Beijing, China","2020 IEEE Symposium Series on Computational Intelligence (SSCI)","5 Jan 2021","2020","","","1478","1484","With the development of deep learning that employs deep neural networks (DNN) as powerful tool, its computational requirement grows rapid together with the increasing size (e.g. depth and parameter) of DNN. Currently, model and data parallelism are employed for accelerating the training and inference process of DNN. However, the above techniques make placement decision on devices for DNN based on heuristics and intuitions by machine learning experts. In this paper, we propose an novel approach for designing device placement of DNN in an automatic way. For a DNN, we employ a sequence-to-sequence model as controller to sample device placement from a one-shot model, which contains all possible device placements with respect to a specific hardware environment (e.g. CPU and GPU). Then, reinforcement learning treats the execution time of sampled device placement as reward to guide the sequence-to-sequence model for finding better one. The proposed approach is employed to optimize the device placement for both model and data parallelism of Inception-V3 on ImageNet. Experimental results show that the optimal placements discovered by our method can outperform hand-crafted one.","","978-1-7281-2547-3","10.1109/SSCI47803.2020.9308141","Chinese Academy of Sciences; Huawei Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9308141","deep neural networks;device placement;controller;one-shot model;reinforcement learning","Parallel processing;Data models;Computational modeling;Optimization;Reinforcement learning;Hardware;Training","decision making;deep learning (artificial intelligence);image classification;inference mechanisms;neural nets;optimisation;parallel processing","device placement optimization;deep neural networks;one-shot model;reinforcement learning;deep learning;DNN;data parallelism;inference process;placement decision making;machine learning experts;sequence-to-sequence model;sample device placement;sampled device placement;optimal placements;Inception-V3;ImageNet","","","","23","IEEE","5 Jan 2021","","","IEEE","IEEE Conferences"
"An efficient reinforcement learning algorithm for continuous actions","F. Bo; C. Xin; H. Yong; W. Min","Institute of Advanced Control and Intelligent Automation, School of Information Science and Engineering, Central South University, Changsha, China; Institute of Advanced Control and Intelligent Automation, School of Information Science and Engineering, Central South University, Changsha, China; Institute of Advanced Control and Intelligent Automation, School of Information Science and Engineering, Central South University, Changsha, China; Institute of Advanced Control and Intelligent Automation, School of Information Science and Engineering, Central South University, Changsha, China","2013 25th Chinese Control and Decision Conference (CCDC)","18 Jul 2013","2013","","","80","85","In this paper a fast and effective reinforcement learning algorism named Dyna-CA in which the learning agent or agents can get the continuous action has been proposed to get the generalization of reinforcement learning methods to large-scale or continuous space. Firstly, the set of k states around the current state will be observed and the probability distribution of k states on condition current state can be calculated by functional mapping. Secondly the selection action-making in the current state for agent is recommended the weighted sum of best actions taken in the neighbor states to guarantee the learning with continuous actions. Then the Q value will be updated by the rules of Dyna algorism, which are not only based on the current neighbor states' practical knowledge but also the priori neighbor states' experience. Computer simulations involving the Maze and Acrobat problems illustrate the validity of the proposed reinforcement learning method and fast convergence in learning an optimal policy.","1948-9447","978-1-4673-5534-6","10.1109/CCDC.2013.6560898","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6560898","reinforcement learning;continuous actions;Q-Iearning;Dyna","Learning (artificial intelligence);Algorithm design and analysis;Planning;Probability distribution;Computational modeling;Machine learning algorithms;Classification algorithms","learning (artificial intelligence);multi-agent systems;statistical distributions","reinforcement learning algorithm;continuous actions;Dyna-CA algorithm;learning agent;large-scale;continuous space;probability distribution;functional mapping;selection action-making;neighbor states;computer simulations;maze problem;acrobat problem;machine learning","","","","14","IEEE","18 Jul 2013","","","IEEE","IEEE Conferences"
"Dynamics-Aligned Transfer Reinforcement Learning For Autonomous Underwater Vehicle Control","K. Cheng; W. Lu; H. Xiong; H. Liu","school of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), ShenZhen, China; school of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), ShenZhen, China; school of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), ShenZhen, China; school of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), ShenZhen, China","2022 International Conference on Advanced Robotics and Mechatronics (ICARM)","29 Nov 2022","2022","","","1040","1045","Autonomous Underwater Vehicles (AUVs) in shallow waters usually are subject to model uncertainties from loading and unloading objects, thruster malfunction due to thruster caught by waterweed, etc. Learning-based controllers are suitable for uncertainty attenuation control. But excessive uncertainties heavily affect the state transition in Markov Decision Process (MDP) or Partially Observable Markov Decision Process (POMDP). In addition, learning procedures on real AUV system requires a large number of training samples, which may damage systems. Therefore, a policy trained exclusively on a simulated model usually results in unsatisfactory performance on a real AUV system due to dynamics model mismatch (transition model mismatch). In this paper, we propose a transfer reinforcement learning approach that adapts a control policy to an underwater robot under dynamics model mismatch, for the purpose of uncertainty rejection. In particular, A policy trained on a source dynamics model is transferred to target dynamics models via dynamics alignment, by proposing a distance metric between two dynamics models. We have tested the proposed approach on a pose regulation task through numerical simulations of three scenarios, mainly including changes in thruster characteristics. The results have demonstrated the effectiveness of the proposed dynamics-aligned transfer reinforcement learning approach.","","978-1-6654-8306-3","10.1109/ICARM54641.2022.9959635","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9959635","","Adaptation models;Autonomous underwater vehicles;Uncertainty;Attitude control;Motion estimation;Reinforcement learning;Numerical models","autonomous underwater vehicles;decision theory;Markov processes;mobile robots;reinforcement learning","autonomous underwater vehicle control;AUV system;AUVs;dynamics model mismatch;dynamics-aligned transfer reinforcement learning approach;learning-based controllers;model uncertainties;partially observable Markov decision process;shallow waters;source dynamics model;state transition;transition model mismatch;uncertainty attenuation control;uncertainty rejection;underwater robot","","","","16","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"A Motion Planning Method for Visual Servoing Using Deep Reinforcement Learning in Autonomous Robotic Assembly","Z. Liu; K. Wang; D. Liu; Q. Wang; J. Tan","State Key Laboratory of Computer-Aided Design and Computer Graphics (CAD&CG), Zhejiang University, Hangzhou, China; State Key Laboratory of Computer-Aided Design and Computer Graphics (CAD&CG), Zhejiang University, Hangzhou, China; State Key Laboratory of Computer-Aided Design and Computer Graphics (CAD&CG), Zhejiang University, Hangzhou, China; State Key Laboratory of Computer-Aided Design and Computer Graphics (CAD&CG), Zhejiang University, Hangzhou, China; State Key Laboratory of Computer-Aided Design and Computer Graphics (CAD&CG), Zhejiang University, Hangzhou, China","IEEE/ASME Transactions on Mechatronics","","2023","PP","99","1","12","Assembly positioning by visual servoing (VS) is a basis for autonomous robotic assembly. In practice, VS control suffers potential stability and convergence problems due to image and physical constraints, e.g., field of view constraints, image local minima, obstacle collisions, and occlusion. Therefore, this article proposes a novel deep reinforcement learning-based hybrid visual servoing (DRL-HVS) controller for motion planning of VS tasks. DRL-HVS controller takes current observed image features and camera pose as inputs, and the core parameters of hybrid VS are dynamically optimized using a deep deterministic policy gradient (DDPG) algorithm to obtain an optimal motion scheme, considering image/physical constraints and robot motion performance. In addition, an adaptive exploration strategy is proposed to further improve the training efficiency by adaptively tuning the exploration noise parameters. In this way, the offline pretrained DRL-HVS controller in the virtual environment, where the DDPG actor–critic network is continuously optimized, can be quickly deployed to a real robot system for real-time control. Experiments based on an eye-in-hand VS system are conducted with a calibrated HIKVISION RGB camera mounted on the end-effector of a GSK-RB03A1 six degree-of-freedom (6-DoF) robot. Basic VS task experiments show that the proposed controller achieves better performance than the existing methods: the servoing time is 24% smaller than that of the five-dimensional VS method, a 100% success rate with the perturbed ranges of the initial position within 25 mm for translation and 20° for rotation, and a 48% efficiency improvement. Moreover, a planetary gear component assembly process case study, where the robot aims to automatically put the gears on the gear shafts, is conducted to demonstrate the applicability of the proposed method in practice.","1941-014X","","10.1109/TMECH.2023.3275854","National Key Research and Development Program of China(grant numbers:2019YFB1312600); National Natural Science Foundation of China(grant numbers:52075480); Key Research and Development Program of Zhejiang Province(grant numbers:2021C01008); High-Level Talent Special Support Plan of Zhejiang Province(grant numbers:2020R52004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138316","Deep reinforcement learning (DRL);motion planning;robotic assembly;visual servoing (VS)","Robots;Task analysis;Cameras;Training;Planning;Visualization;Gears","","","","","","","IEEE","29 May 2023","","","IEEE","IEEE Early Access Articles"
"A Matching Algorithm with Reinforcement Learning and Decoupling Strategy for Order Dispatching in On-Demand Food Delivery","J. Chen; L. Wang; Z. Pan; Y. Wu; J. Zheng; X. Ding","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Delivery Technology, Beijing, China","Tsinghua Science and Technology","22 Sep 2023","2024","29","2","386","399","The on-demand food delivery (OFD) service has gained rapid development in the past decades but meanwhile encounters challenges for further improving operation quality. The order dispatching problem is one of the most concerning issues for the OFD platforms, which refer to dynamically dispatching a large number of orders to riders reasonably in very limited decision time. To solve such a challenging combinatorial optimization problem, an effective matching algorithm is proposed by fusing the reinforcement learning technique and the optimization method. First, to deal with the large-scale complexity, a decoupling method is designed by reducing the matching space between new orders and riders. Second, to overcome the high dynamism and satisfy the stringent requirements on decision time, a reinforcement learning based dispatching heuristic is presented. To be specific, a sequence-to-sequence neural network is constructed based on the problem characteristic to generate an order priority sequence. Besides, a training approach is specially designed to improve learning performance. Furthermore, a greedy heuristic is employed to effectively dispatch new orders according to the order priority sequence. On real-world datasets, numerical experiments are conducted to validate the effectiveness of the proposed algorithm. Statistical results show that the proposed algorithm can effectively solve the problem by improving delivery efficiency and maintaining customer satisfaction.","1007-0214","","10.26599/TST.2023.9010069","National Natural Science Foundation of China(grant numbers:62273193); Tsinghua University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10258151","order dispatching;on-demand delivery;reinforcement learning;decoupling strategy;sequence-to-sequence neural network","Training;Uncertainty;Heuristic algorithms;Neural networks;Customer satisfaction;Optimization methods;Reinforcement learning","combinatorial mathematics;computational complexity;customer satisfaction;greedy algorithms;neural nets;optimisation;pattern matching;reinforcement learning;service industries","combinatorial optimization problem;customer satisfaction;decoupling method;delivery efficiency;dispatching heuristic;large-scale complexity;matching algorithm;OFD platforms;on-demand food delivery service;operation quality;order dispatching problem;order priority sequence;reinforcement learning;sequence-to-sequence neural network","","","","31","","22 Sep 2023","","","TUP","TUP Journals"
"Infusing Model Predictive Control Into Meta-Reinforcement Learning for Mobile Robots in Dynamic Environments","J. Shin; A. Hakobyan; M. Park; Y. Kim; G. Kim; I. Yang","Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National University, Seoul, South Korea","IEEE Robotics and Automation Letters","1 Aug 2022","2022","7","4","10065","10072","The successful operation of mobile robots requires them to adapt rapidly to environmental changes. To develop an adaptive decision-making tool for mobile robots, we propose a novel algorithm that combines meta-reinforcement learning (meta-RL) with model predictive control (MPC). Our method employs an off-policy meta-RL algorithm as a baseline to train a policy using transition samples generated by MPC when the robot detects certain events that can be effectively handled by MPC, with its explicit use of robot dynamics. The key idea of our method is to switch between the meta-learned policy and the MPC controller in a randomized and event-triggered fashion to make up for suboptimal MPC actions caused by the limited prediction horizon. During meta-testing, the MPC module is deactivated to significantly reduce computation time in motion control. We further propose an online adaptation scheme that enables the robot to infer and adapt to a new task within a single trajectory. The performance of our method has been demonstrated through simulations using a nonlinear car-like vehicle model with $(i)$ synthetic movements of obstacles, and $(ii)$ real-world pedestrian motion data. The simulation results indicate that our method outperforms other algorithms in terms of learning efficiency and navigation quality.","2377-3766","","10.1109/LRA.2022.3191234","National Research Foundation of Korea(grant numbers:MSIT2020R1C1C1009766,MSIT2021R1A4A2001824); Information and Communications Technology Planning and Evaluation(grant numbers:MSIT2020-0-00857,MSIT2022-0-00480); Samsung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9830833","Machine learning for robot control;motion control;reinforcement learning","Task analysis;Robots;Dynamics;Vehicle dynamics;Navigation;Heuristic algorithms;Mobile robots","decision making;learning (artificial intelligence);mobile robots;motion control;multi-robot systems;nonlinear control systems;predictive control;robot dynamics;vehicles","infusing model predictive control;mobile robots;dynamic environments;adaptive decision-making tool;meta-reinforcement learning;off-policy meta-RL algorithm;robot dynamics;meta-learned policy;MPC controller;randomized event-triggered fashion;suboptimal MPC actions;prediction horizon;meta-testing;MPC module;motion control;online adaptation scheme;nonlinear car-like vehicle model","","4","","29","IEEE","15 Jul 2022","","","IEEE","IEEE Journals"
"A Knowledge-based reinforcement learning control approach using deep Q network for cooling tower in HVAC systems","Z. Yu; X. Yang; F. Gao; J. Huang; R. Tu; J. Cui","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of civil and resources engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","1721","1726","In order to improve the control performance of the complex mechatronic system, like the cooling tower in HVAC systems, a data-driven control approach based on model-free deep reinforcement learning method is proposed. The deep Q network (DQN) method, as one of the approaches in reinforcement learning (RL) scheme, can learn and obtain robust feedback control laws by using direct interaction data from the environment. At the same time, radial basis function neural network (RBFNN) is used for the design of deep Q network. Furthermore, the prior knowledge is used to develop an exploration strategy for the RL controller. And the strategy can guide the RL controller to explore the action space, which leads reduction on the training time as compared to traditional DQN controller. At last, the results of simulation show that the DQN controller can achieve lower Integral Absolute Error (IAE) and Integral Square Error (ISE) compared to Proportional integral (PI) controller.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327385","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327385","reinforcement learning;deep Q network;knowledge-based exploration strategy;HVAC Systems","Neural networks;Poles and towers;Cooling;Reinforcement learning;Space exploration;Aerospace electronics;HVAC","control engineering computing;control system synthesis;deep learning (artificial intelligence);feedback;HVAC;learning systems;mechatronics;neurocontrollers;radial basis function networks;robust control","cooling tower;HVAC systems;control performance;complex mechatronic system;data-driven control;model-free deep reinforcement learning;deep Q network;robust feedback control laws;direct interaction data;radial basis function neural network;exploration strategy;RL controller;proportional integral controller;DQN controller;knowledge-based reinforcement learning control","","3","","19","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Dynamic Obstacle Avoidance for Cable-Driven Parallel Robots With Mobile Bases via Sim-to-Real Reinforcement Learning","Y. Liu; Z. Cao; H. Xiong; J. Du; H. Cao; L. Zhang","Guangdong Key Laboratory of Intelligent Morphing Mechanisms and Adaptive Robotics and the School of Mechanical Engineering and Automation, the Harbin Institute of Technology Shenzhen, Shenzhen, China; Guangdong Key Laboratory of Intelligent Morphing Mechanisms and Adaptive Robotics and the School of Mechanical Engineering and Automation, the Harbin Institute of Technology Shenzhen, Shenzhen, China; Guangdong Key Laboratory of Intelligent Morphing Mechanisms and Adaptive Robotics and the School of Mechanical Engineering and Automation, the Harbin Institute of Technology Shenzhen, Shenzhen, China; Guangdong Key Laboratory of Intelligent Morphing Mechanisms and Adaptive Robotics and the School of Mechanical Engineering and Automation, the Harbin Institute of Technology Shenzhen, Shenzhen, China; Guangdong Key Laboratory of Intelligent Morphing Mechanisms and Adaptive Robotics and the School of Mechanical Engineering and Automation, the Harbin Institute of Technology Shenzhen, Shenzhen, China; Department of Physics & Astronomy, University of Central Arkansas, Conway, AR, USA","IEEE Robotics and Automation Letters","9 Feb 2023","2023","8","3","1683","1690","A Cable-Driven Parallel Robot (CDPR) with Mobile Bases (MBs) can modify its geometric architecture and is suitable for manipulation tasks in constrained environments. In manipulation tasks, a CDPR with MBs inevitably encounters obstacles, including dynamic obstacles. However, the high dimensional state space and a considerable number of constraints caused by multiple cables and MBs make the real-time dynamic obstacle avoidance of a CDPR with MBs challenging. This letter proposes a Reinforcement Learning (RL)-based dynamic obstacle avoidance method for a CDPR with MBs to deal with dynamic obstacles in real time. To explain the RL-based dynamic obstacle avoidance method, this letter focuses on a CDPR with four fixed-length cables connected to four MBs. An RL-based Obstacle Avoidance Controller (OAC) is developed and integrated into a trajectory tracking controller to address the dynamic obstacle avoidance problem of a CDPR with MBs tracking a target trajectory. To explain and evaluate the RL-based dynamic obstacle avoidance method further, an RL-based OAC is trained in a Mujoco simulator and transferred to a CDPR with four fixed-length cables connected to four MBs in the real world.","2377-3766","","10.1109/LRA.2023.3241801","Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515110021); Shenzhen Science and Technology Program(grant numbers:RCBS20210609103819024); Research Foundation for Advanced Talents; Harbin Institute of Technology Shenzhen(grant numbers:CA11409019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035491","Collision avoidance;machine learning for robot control;parallel robots;wire mechanism","Collision avoidance;Trajectory;Trajectory planning;Real-time systems;Trajectory tracking;Reinforcement learning;Parallel robots","cables (mechanical);collision avoidance;manipulators;mobile robots;reinforcement learning;trajectory control","Cable-Driven Parallel Robot;Cable-Driven Parallel robots;CDPR;dynamic obstacle avoidance problem;dynamic obstacles;fixed-length cables;manipulation tasks;MBs;Mobile Bases;real-time dynamic obstacle avoidance;RL-based dynamic obstacle avoidance method;RL-based Obstacle Avoidance Controller","","3","","32","IEEE","2 Feb 2023","","","IEEE","IEEE Journals"
"Optimal balancing control of bipedal robots using reinforcement learning","F. Peng; L. Ding; Z. Li; C. Yang; C. -Y. Su","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Zienkiewicz Centre for Computational Engineering, Swansea University, UK; Department of Mechanical and Industrial Engineering, Concordia University, Canada","2016 12th World Congress on Intelligent Control and Automation (WCICA)","29 Sep 2016","2016","","","2186","2191","The balance control a bipedal robot in the presence of external disturbances is still a challenge. In this paper, a novel optimal ankle stiffness regulation for humanoid robot balancing problem is proposed. The presented techniques are developed based on the integral reinforcement learning (IRL) algorithm, which is designed for unknown continuous-time systems using only partial knowledge of the system dynamics. A linear mathematical model of an inverted pendulum model (IPM) is employed to study robot balance, and optimization is achieved by using linear quadratic regulator (LQR). Because the internal system dynamics is nonlinear and time-varying, IRL algorithm is proposed to solve the algebraic Riccati equation online without knowledge of the internal system dynamics. Nonlinear stiffness stabilizer based on both IRL and the fixed stiffness are studied in the simulation, the comparative results demonstrate the superior balance ability of the proposed method. In addition, dynamics changing is also discussed in the simulation to test robustness of the proposed method.","","978-1-4673-8414-8","10.1109/WCICA.2016.7578463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7578463","","Impedance;Mathematical model;Cost function;Robot kinematics;Torque;Riccati equations","elastic constants;humanoid robots;learning (artificial intelligence);linear quadratic control;nonlinear control systems;pendulums;Riccati equations;time-varying systems","optimal balancing control;bipedal robots;reinforcement learning;external disturbances;optimal ankle stiffness regulation;humanoid robot balancing problem;integral reinforcement learning;unknown continuous-time systems;linear mathematical model;inverted pendulum model;IPM;linear quadratic regulator;LQR;internal system dynamics;IRL algorithm;algebraic Riccati equation;nonlinear stiffness stabilizer","","2","","16","IEEE","29 Sep 2016","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Charging Electric Vehicles With Uncertain Photovoltaic Power","H. Liang; Z. Wang; G. Hao; L. Xiao; Z. Sun","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","1225","1231","Electric vehicles (EVs) are some of the major consumers of renewable energy generation in recent years. However, uncertainties associated with the photovoltaic (PV) output and EV users’ charging behaviors have imposed great challenges on PV power consumption. This study presents a real-time EV charging strategy based on deep reinforcement learning (DRL) to reduce the PV curtailment and charging costs of EV users. A mathematical model is constructed to describe the charging control process of EVs, in which the uncertainties of PV outputs and users’ charging behaviors are considered. This EV charging control problem is then formulated as a Markov Decision Process, and DRL is developed to learn the optimal real-time charging strategy in a dynamic environment. A policy search algorithm based on Proximal Policy Optimization is used to train the neural network. Moreover, a novel allocation criterion is proposed for the charging behavior of a single EV, thus ensuring that each EV is fully charged before departure. The proposed approach is tested on simulation cases, and the results verify the effectiveness in improving PV accommodation with uncertainties.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055292","electric vehicle;photovoltaic power consumption;deep reinforcement learning;allocation criterion","Photovoltaic systems;Uncertainty;Costs;Process control;Reinforcement learning;Electric vehicle charging;Real-time systems","deep learning (artificial intelligence);demand side management;electric vehicle charging;Markov processes;optimisation;photovoltaic power systems;power engineering computing;power grids;reinforcement learning","allocation criterion;charging behavior;charging control process;charging electric vehicles;deep reinforcement learning;DRL;EV charging control problem;Markov decision process;neural network;optimal real-time charging strategy;photovoltaic output;policy search algorithm;proximal policy optimization;PV accommodation;PV curtailment;PV outputs;PV power consumption;real-time EV charging strategy;renewable energy generation;uncertain photovoltaic power","","","","15","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Keyframe Selection Via Deep Reinforcement Learning for Skeleton-Based Gesture Recognition","M. Gan; J. Liu; Y. He; A. Chen; Q. Ma","Key Laboratory of Complex System Intelligent Control and Decision, School of Automation, Beijing Institute of Technology, Beijing, China; Key Laboratory of Complex System Intelligent Control and Decision, School of Automation, Beijing Institute of Technology, Beijing, China; Key Laboratory of Complex System Intelligent Control and Decision, School of Automation, Beijing Institute of Technology, Beijing, China; Key Laboratory of Complex System Intelligent Control and Decision, School of Automation, Beijing Institute of Technology, Beijing, China; Key Laboratory of Complex System Intelligent Control and Decision, School of Automation, Beijing Institute of Technology, Beijing, China","IEEE Robotics and Automation Letters","16 Oct 2023","2023","8","11","7807","7814","Skeleton-based gesture recognition has attracted extensive attention and has made great progress. However, mainstream methods generally treat all frames as equally important, which may limit performance, especially when dealing with high inter-class variance in gesture. To tackle this issue, we propose an approach that models a Markov decision process to identify keyframes while discarding irrelevant ones. This article proposes a deep reinforcement learning double-feature double-motion network comprising two main components: a baseline gesture recognition model and a frame selection network. These two components mutually influence each other, resulting in enhanced overall performance. Following the evaluation of the SHREC-17 and F-PHAB datasets, our proposed method demonstrates superior performance.","2377-3766","","10.1109/LRA.2023.3322645","National Key Research and Development Program of China(grant numbers:2020YFB1708500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10274843","Skeleton-based gesture recognition;Markov decision process;deep reinforcement learning;frame selection network","Skeleton;Reinforcement learning;Deep learning;Gesture recognition;Training;Markov processes;Feature extraction","","","","","","35","IEEE","9 Oct 2023","","","IEEE","IEEE Journals"
"Robust Adaptive PID Control based on Reinforcement Learning for MIMO Nonlinear Six-joint Manipulator","J. Wang; J. Zhu; C. Zou; L. Ou; X. Yu","Department of Automation, Zhejiang University of Technology, Hangzhou, CN; Department of Automation, Zhejiang University of Technology, Hangzhou, CN; Department of Automation, Zhejiang University of Technology, Hangzhou, CN; Department of Automation, Zhejiang University of Technology, Hangzhou, CN; Department of Automation, Zhejiang University of Technology, Hangzhou, CN","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","2633","2638","The problem of designing a robust and adaptive PID controller for MIMO nonlinear robotic systems with tiny static error and overshooting has been a long-lasting open challenge. This paper presents a novel adaptive reinforcement learning-based control design for a robot with six joints based on a simple structured PID-like control. The method of proposed hybrid control uses an actor-critic structure and automatically estimates the multiple parameters of the PID controllers. The proposed scheme has been verified in a simulation environment. A comparative experiment demonstrated the successful performance of the method against other adaptive methods for MIMO has indicated.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727605","PID control;reinforcement learning;manipulator;MIMO systems","Adaptation models;PI control;Control design;Reinforcement learning;Manipulators;Stability analysis;PD control","adaptive control;control system synthesis;learning (artificial intelligence);neurocontrollers;nonlinear control systems;robust control;three-term control","mimo nonlinear six-joint manipulator;robust PID controller;adaptive PID controller;MIMO nonlinear robotic systems;tiny static error;overshooting;novel adaptive reinforcement;control design;simple structured PID-like control;proposed hybrid control;actor-critic structure;PID controllers;adaptive methods;robust adaptive PID control;reinforcement learning","","","","18","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Multi-agent reinforcement learning based coordinated control of PEMFC gas supply system","G. Wang; X. Wang; L. Wang; L. Jia; M. Shao; Y. Yu","College of Control Science and Engineering, Shandong University, Jinan, China; College of Control Science and Engineering, Shandong University, Jinan, China; College of Control Science and Engineering, Shandong University, Jinan, China; College of Control Science and Engineering, Shandong University, Jinan, China; Haihui New Energy Motor Co., Ltd., Rizhao, China; Haihui New Energy Motor Co., Ltd., Rizhao, China","2022 IEEE 17th Conference on Industrial Electronics and Applications (ICIEA)","12 Jan 2023","2022","","","132","137","This paper proposes a multi-agent reinforcement learning algorithm for the coordinated gas supply control of PEMFC to improve efficiency and extend service life by regulating differential pressure of anode and cathode, oxygen excess ratio and anode inlet humidity. The coordinated gas supply model of PMEFC is firstly established to provide the environment used to interact with agents, and then the control frame considering air supply, hydrogen supply and anode circulation gas supply is designed to realize coordinated control. In the structure of multi-agent reinforcement learning, each agent has a pair of critic networks and actor networks for action value estimation and action selection respectively. By designing appropriate reward function and giving reward feedback, the agents update their actor and critic networks with the method of centralized training and decentralized execution. The control algorithm is finally evaluated in the matlab&simulink environment, and compared with the traditional PID. The results show that the proposed algorithm is superior to traditional PID in steady-state error and dynamic response.","2158-2297","978-1-6654-0984-1","10.1109/ICIEA54703.2022.10006221","Nature; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10006221","PEMFC;coordinated control;gas supply system;multi-agent reinforcement learning","Training;Heuristic algorithms;Atmospheric modeling;Reinforcement learning;Humidity;Mathematical models;Anodes","control engineering computing;estimation theory;humidity;hydrogen production;multi-agent systems;production engineering computing;proton exchange membrane fuel cells;reinforcement learning","action selection;action value estimation;actor networks;air supply;anode circulation gas supply;anode inlet humidity;coordinated control;critic networks;hydrogen supply;Matlab;multiagent reinforcement learning;oxygen excess ratio;PEMFC gas supply system;proton exchange membrane fuel cell;reward feedback;reward function;Simulink","","","","10","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"A novel incremental learning scheme for reinforcement learning in dynamic environments","Z. Wang; C. Chen; H. -X. Li; D. Dong; T. -J. Tarn","Department of Control and Systems Engineering, University of Hong Kong, Nanjing, China; Department of Control and Systems Engineering, Nanjing University, Nanjing, China; Department of System Engineering and Engineering Management, Central South University, Hong Kong, Changsha, Hunan, China; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia; Department of Electrical and Systems Engineering, Washington University in St. Louis, St. Louis, MO, USA","2016 12th World Congress on Intelligent Control and Automation (WCICA)","29 Sep 2016","2016","","","2426","2431","In this paper, we develop a novel incremental learning scheme for reinforcement learning (RL) in dynamic environments, where the reward functions may change over time instead of being static. The proposed incremental learning scheme aims at automatically adjusting the optimal policy in order to adapt to the ever-changing environment. We evaluate the proposed scheme on a classical maze navigation problem and an intelligent warehouse system in simulated dynamic environments. Simulation results show that the proposed scheme can greatly improve the adaptability and applicability of RL in dynamic environments compared to several other direct methods.","","978-1-4673-8414-8","10.1109/WCICA.2016.7578530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7578530","","Systems engineering and theory;Electronic mail;Heuristic algorithms;Intelligent control;Learning (artificial intelligence);Machine learning algorithms;Dynamic programming","learning (artificial intelligence)","incremental learning scheme;reinforcement learning;reward functions;optimal policy;classical maze navigation problem;intelligent warehouse system;simulated dynamic environments","","13","","23","IEEE","29 Sep 2016","","","IEEE","IEEE Conferences"
"Improved Reinforcement Learning through Imitation Learning Pretraining Towards Image-based Autonomous Driving","T. Wang; D. E. Chang","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea","2019 19th International Conference on Control, Automation and Systems (ICCAS)","30 Jan 2020","2019","","","1306","1310","We present a training pipeline for the autonomous driving task given the current camera image and vehicle speed as the input to produce the throttle, brake, and steering control output. The simulator Airsim's [1] convenient weather and lighting API provides a sufficient diversity during training which can be very helpful to increase the trained policy's robustness. In order to not limit the possible policy's performance, we use a continuous and deterministic control policy setting. We utilize ResNet-34 [2] as our actor and critic networks with some slight changes in the fully connected layers. Considering human's mastery of this task and the high-complexity nature of this task, we first use imitation learning to mimic the given human policy and then leverage the trained policy and its weights to the reinforcement learning phase for which we use DDPG [3]. This combination shows a considerable performance boost comparing to both pure imitation learning and pure DDPG for the autonomous driving task.","2642-3901","978-89-93215-17-5","10.23919/ICCAS47443.2019.8971737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8971737","Autonomous driving;reinforcement learning;imitation learning","","application program interfaces;convolutional neural nets;learning (artificial intelligence);mobile robots;road vehicles;robot vision","critic networks;fully connected layers;reinforcement learning;imitation learning;image-based autonomous driving;camera image;vehicle speed;steering control output;brake;throttle;simulator Airsim convenient weather;lighting API;DDPG;convolutional neural network","","9","","15","","30 Jan 2020","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Edge Device Selection Using Social Attribute Perception in Industry 4.0","P. Zhang; P. Gan; G. S. Aujla; R. S. Batth","College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; School of Computing Science, Durham University, Durham, U.K.; School of Computer Science and Engineering, Lovely Professional University, Phagwara, India","IEEE Internet of Things Journal","6 Feb 2023","2023","10","4","2784","2792","In the 5G era, the problem of data islands in various industries restricts the development of artificial intelligence technology, so data sharing is proposed. High-quality data sharing directly affects the effectiveness of machine learning models, but data leakage and abuse will inevitably occur in the process. As a consequence, in order to solve this problem, federated learning is proposed. This method uses the personalized data of multiple edge devices to train the model. The central server collects the training results of the edge devices and updates the global model, and then iteratively tests and updates the model through the edge devices. However, edge devices may have problems, such as unbalanced load and exit from the training process, which makes the training time of the model long and the effect is poor. Therefore, in the process of federated learning, the selection of reliable and high-quality edge devices becomes crucial. On this basis, in this article, we introduce reinforcement learning (RL) to preselect edge devices and obtain a set of candidate devices and then determine reliable edge devices through social attribute perception. The simulation experiment data analysis demonstrates that this scheme can improve the reliability of federated learning and complete the training process in a shorter time, the efficiency of federated learning increased by approximately 10.3%.","2327-4662","","10.1109/JIOT.2021.3088577","Start-Up Fund from Durham University; Major Scientific and Technological Projects of CNPC(grant numbers:ZD2019-183-006); Shandong Provincial Natural Science Foundation, China(grant numbers:ZR2020MF006); “the Fundamental Research Funds for the Central Universities” of China University of Petroleum (East China)(grant numbers:20CX05017A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9452166","Data sharing;edge computing;federated learning;reinforcement learning (RL);social attribute perception","Collaborative work;Training;Data models;Load modeling;Servers;Reliability;Reinforcement learning","data analysis;data integrity;edge computing;production engineering computing;reinforcement learning;security of data","data abuse;data analysis;data islands;data leakage;data sharing;edge device selection;edge devices;federated learning;Industry 4.0;machine learning;reinforcement learning;RL;social attribute perception","","7","","45","IEEE","11 Jun 2021","","","IEEE","IEEE Journals"
"Learning Pushing Skills Using Object Detection and Deep Reinforcement Learning","W. Guo; G. Dong; C. Chen; M. Li","School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, Heilongjiang Province, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, Heilongjiang Province, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, Heilongjiang Province, China; School of Mechatronics Engineering, Harbin Institute of Technology, Harbin, Heilongjiang Province, China","2019 IEEE International Conference on Mechatronics and Automation (ICMA)","29 Aug 2019","2019","","","469","474","Pushing objects to the target place on a tabletop surface is a task which requires skillful interaction with the physical world. Usually, this is achieved by precisely modeling physical properties of the objects, robot, and the environment for explicit planning. In contrast, as explicitly modeling the physical environment is not always feasible and involves various uncertainties, we learn robotic pushing skills with deep reinforcement learning and based on only visual feedback. For this, we model the task with rewards and use deep deterministic policy gradient (DDPG) algorithms to update the control strategy. And we combine You Only Look Once (YOLO) which is one of state-of-the-art object detection algorithm with DDPG algorithm, the method reduces the exploring space of the robot, produces more effective learning samples, and improves the learning efficiency greatly. In simulation experiments, the robot learned pushing skills after training 400 episodes, the success rate is around 95% and the learning efficiency is much higher than using DDPG alone. In real-world experiments, the robot learned pushing skills after training 800 episodes and the success rate is around 85%.","2152-744X","978-1-7281-1699-0","10.1109/ICMA.2019.8816481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816481","DDPG;YoloV3;UR5","Robots;Reinforcement learning;Object detection;Training;Task analysis;Neural networks;Space exploration","control engineering computing;gradient methods;learning (artificial intelligence);neural nets;object detection;robot programming;robot vision","robotic pushing skills;deep reinforcement learning;deep deterministic policy gradient algorithms;DDPG algorithm;tabletop surface;skillful interaction;object detection algorithm;You Only Look Once","","3","","31","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Receding Horizon Cache and Extreme Learning Machine based Reinforcement Learning","Z. Shao; M. J. Er; G. -B. Huang","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","2012 12th International Conference on Control Automation Robotics & Vision (ICARCV)","25 Mar 2013","2012","","","1591","1596","Function approximators have been extensively used in Reinforcement Learning (RL) to deal with large or continuous space problems. However, batch learning Neural Networks (NN), one of the most common approximators, has been rarely applied to RL. In this paper, possible reasons for this are laid out and a solution is proposed. Specifically, a Receding Horizon Cache (RHC) structure is designed to collect training data for NN by dynamically archiving state-action pairs and actively updating their Q-values, which makes batch learning NN much easier to implement. Together with Extreme Learning Machine (ELM), a new RL with function approximation algorithm termed as RHC and ELM based RL (RHC-ELM-RL) is proposed. A mountain car task was carried out to test RHC-ELM-RL and compare its performance with other algorithms.","","978-1-4673-1872-3","10.1109/ICARCV.2012.6485384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6485384","","Artificial neural networks;Training;Heuristic algorithms;Training data;Educational institutions;Function approximation;Approximation algorithms","approximation theory;learning (artificial intelligence);neural nets","extreme learning machine based reinforcement learning;RL;continuous space problems;batch learning Neural Networks;NN;receding horizon cache structure;RHC structure;ELM;mountain car task;function approximation algorithm","","2","","18","IEEE","25 Mar 2013","","","IEEE","IEEE Conferences"
"Motor learning model using reinforcement learning with neural internal model","J. Izawa; T. Kondo; K. Ito","Department of Computational Intelligence and Systems, Tokyo Institute of Technology, Yokohama, Kanagawa, Japan; Department of Computational Intelligence and Systems, Tokyo Institute of Technology, Yokohama, Kanagawa, Japan; Department of Computational Intelligence and Systems, Tokyo Institute of Technology, Yokohama, Kanagawa, Japan","2003 IEEE International Conference on Robotics and Automation (Cat. No.03CH37422)","10 Nov 2003","2003","3","","3146","3151 vol.3","The present paper proposes a learning control method for the musculoskeletal system of arm based on reinforcement learning. An optimization for the hand trajectory and muscle's force distribution is needed to acquire the reaching motion. The proposed architecture can acquire an optimized motion through learning the task. However, the biological control system composed of musculoskeletal system is not able to sense the state without time delay. The time delay causes instability of learning. The proposed scheme consists of the reinforcement learning part and neural internal model. Neural internal model is employed to compensate for the time delay by estimating the state of musculoskeletal system. Then, there must be a modeling error if some noise is included. Thus we introduce the minimum modeling error criterion for reinforcement learning, which gives not only the reduction of total muscle level but also the smoothness of the hand trajectory. The effectiveness and the biological plausibility of the present model is demonstrated by several simulations.","1050-4729","0-7803-7736-2","10.1109/ROBOT.2003.1242074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1242074","","Muscles;Biological system modeling;Delay effects;Musculoskeletal system;Learning systems;Biological control systems;Elasticity;Viscosity;Jacobian matrices;Indium tin oxide","learning (artificial intelligence);delays;optimisation;neural nets;state estimation;muscle;physiological models;biocontrol;biomechanics","motor learning model;reinforcement learning;neural internal model;learning control;arm musculoskeletal system;hand trajectory optimisation;muscles force distribution;biological control system;time delay;state estimation","","2","","7","IEEE","10 Nov 2003","","","IEEE","IEEE Conferences"
"Learning to Solve Multiple-TSP With Time Window and Rejections via Deep Reinforcement Learning","R. Zhang; C. Zhang; Z. Cao; W. Song; P. S. Tan; J. Zhang; B. Wen; J. Dauwels","School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; Singapore Institute of Manufacturing Technology (SIMTech), Agency for Science Technology and Research (A*STAR), Fusionopolis, Singapore; Institute of Marine Science and Technology, Shandong University, Qingdao, China; Singapore Institute of Manufacturing Technology (SIMTech), Agency for Science Technology and Research (A*STAR), Fusionopolis, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore; Department of Microelectronics, Faculty EEMCS, Technische Universiteit Delft, Delft, The Netherlands","IEEE Transactions on Intelligent Transportation Systems","26 Jan 2023","2023","24","1","1325","1336","We propose a manager-worker framework (the implementation of our model is publically available at: https://github.com/zcaicaros/manager-worker-mtsptwr) based on deep reinforcement learning to tackle a hard yet nontrivial variant of Travelling Salesman Problem (TSP), i.e., multiple-vehicle TSP with time window and rejections (mTSPTWR), where customers who cannot be served before the deadline are subject to rejections. Particularly, in the proposed framework, a manager agent learns to divide mTSPTWR into sub-routing tasks by assigning customers to each vehicle via a Graph Isomorphism Network (GIN) based policy network. A worker agent learns to solve sub-routing tasks by minimizing the cost in terms of both tour length and rejection rate for each vehicle, the maximum of which is then fed back to the manager agent to learn better assignments. Experimental results demonstrate that the proposed framework outperforms strong baselines in terms of higher solution quality and shorter computation time. More importantly, the trained agents also achieve competitive performance for solving unseen larger instances.","1558-0016","","10.1109/TITS.2022.3207011","Agency for Science Technology and Research Career Development Fund(grant numbers:222D8235,C222812027); IEO Decentralized GAP Project of Continuous Last-Mile Logistics (CLML) at the Singapore Institute of Manufacturing Technology(grant numbers:I22D1AG003); Rapid-Rich Object Search (ROSE) Laboratory, Nanyang Technological University, Singapore; Ministry of Education, Singapore, under its Academic Research Fund Tier 1(grant numbers:RG61/22); Start-Up Grant; National Natural Science Foundation of China(grant numbers:62102228); Shandong Provincial Natural Science Foundation(grant numbers:ZR2021QF063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9901466","Travelling salesman problem;graph neural network;deep reinforcement learning","Task analysis;Costs;Routing;Reinforcement learning;Time factors;Training data;Market research","deep learning (artificial intelligence);graph theory;reinforcement learning;travelling salesman problems","cost minimisation;deep reinforcement learning;GIN;graph isomorphism network-based policy network;hard yet nontrivial variant;manager agent;manager-worker framework;mTSPTWR;multiple-vehicle TSP;rejection rate;shorter computation time;sub-routing tasks;time window;tour length;travelling salesman problem;worker agent","","2","","36","IEEE","23 Sep 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning Method with Dynamic Learning Rate for Real-Time Route Guidance Based on SUMO","Y. Li; J. Tang; H. Zhao; R. Luo","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Physics Electronic Science, Changsha University of Science Technology; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","2022 17th International Conference on Control, Automation, Robotics and Vision (ICARCV)","10 Jan 2023","2022","","","820","824","The increasing number of vehicles and dynamic changes in traffic situations make real-time route planning strongly necessary. The route-guiding method is supposed to cope with dynamic traffic situations. In addition, the ability to adapt to the second fastest route is very important when traffic congestion suddenly occurs on the fastest path. This paper proposes a method of using reinforcement learning to solve dynamic route planning problems, and the adaptation from a static learning rate to a dynamic learning rate enhances the capability to deal with emergent congestion. Meanwhile, the waiting time before each traffic light also is considered as a reward factor in the proposed algorithm. Contrast experiments have been conducted on the simulation network by SUMO, which has demonstrated well that our proposed method has better performance than other methods.","","978-1-6654-7687-4","10.1109/ICARCV57592.2022.10004227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004227","","Adaptive systems;Navigation;Heuristic algorithms;Reinforcement learning;Real-time systems;Mathematical models;Planning","data analysis;digital simulation;learning (artificial intelligence);path planning;road traffic;telecommunication network routing;traffic engineering computing","dynamic changes;dynamic learning rate;dynamic route planning problems;dynamic traffic situations;fastest path;fastest route;real-time route guidance;real-time route planning;reinforcement learning;route-guiding method;static learning rate;SUMO;traffic congestion;traffic light;waiting time","","1","","25","IEEE","10 Jan 2023","","","IEEE","IEEE Conferences"
"Off-line path integral reinforcement learning using stochastic robot dynamics approximated by sparse pseudo-input Gaussian processes: Application to humanoid robot motor learning in the real environment","N. Sugimoto; J. Morimoto","Department of Center for Information and Neural Networks, NICT, Kyoto, Japan; Department of Brain Robot Interface, ATR Computational Neuroscience Laboratories, Kyoto, Japan","2013 IEEE International Conference on Robotics and Automation","17 Oct 2013","2013","","","1311","1316","We develop fast reinforcement learning (RL) framework using the approximated dynamics of a humanoid robot. Although RL is a useful non-linear optimizer, applying it to real robotic systems is usually difficult due to the large number of iterations required to acquire suitable policies. In this study, we approximate the dynamics using data from a real robot with sparse pseudo-input Gaussian processes (SPGPs). By using SPGPs, we estimated the probability distribution considering both the input vector and output signal variances. In real environments, since the observations from robotic sensors include large noise, SPGPs can suitably approximate the stochastic dynamics of a real humanoid robot. We use the approximated dynamics to improve the performance of a movement task in a path integral RL framework, which updates a policy from the sampled trajectories of the state and action vectors and the cost. We implemented our proposed method on a real humanoid robot and tested on a via-point reaching task. The robot achieved successful performance with fewer number of interactions with the real environment by using the proposed method than a conventional approach which dose not use the simulated dynamics.","1050-4729","978-1-4673-5643-5","10.1109/ICRA.2013.6630740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630740","","Humanoid robots;Vectors;Trajectory;Gaussian processes;Approximation methods;Joints","Gaussian processes;humanoid robots;learning (artificial intelligence);robot dynamics;statistical distributions","off-line path integral reinforcement learning;stochastic robot dynamics;sparse pseudo-input Gaussian processes;humanoid robot motor learning;fast reinforcement learning;RL framework;probability distribution;input vector;output signal variances;SPGP;action vectors;state vectors;via-point reaching task","","1","","12","IEEE","17 Oct 2013","","","IEEE","IEEE Conferences"
"Variational value learning in advantage actor-critic reinforcement learning","Y. Zhang; J. Han; X. Hu; S. Dan","School of Computer and Information Science, Southwest University, Chongqing, China; School of Hanhong, Southwest University, Chongqing, China; College of Artiflcial Intelligence, Southwest University, Chongqing, China; School of Computer and Information Science, Southwest University, Chongqing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","1955","1960","The performance and applications of reinforcement learning based on artificial neural networks (ANNs) have been limited by the overfitting problems of ANNs. With probability distributional weights and the variational inference technique, Bayesian neural networks (BNNs) can reduce the overfitting of data and thus improve the generalization ability of models. This paper proposes an advantage actor-variational-critic reinforcement learning algorithm (called A2VC) based on advantage actor-critic reinforcement learning (A2C). We model the value functions as a probability distribution and implement the distribution by the critic BNN. Based on the variational inference technique and the reparameterization trick, the weights of the critic BNN are well optimized. On the other hand, weights of the actor ANN are optimized with the stochastic policy gradient. Simulations in the lunar lander and cart-pole environments show the effectiveness and advantages of the proposed scheme over conventional A2C algorithm on the learning and decision-making capacity of agents.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327530","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327530","Bayesian Neural Networks;variational inference;reinforcement learning","Reinforcement learning;Inference algorithms;Task analysis;Probability distribution;Engines;Bayes methods;Artificial neural networks","gradient methods;learning (artificial intelligence);neural nets;statistical distributions","variational value;advantage actor-critic reinforcement learning;artificial neural networks;probability distributional weights;variational inference technique;Bayesian neural networks;advantage actor-variational-critic reinforcement learning algorithm;probability distribution;critic BNN;actor ANN;A2C algorithm","","","","14","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Deep reinforcement learning for IRS-assisted UAV covert communications","S. Bi; L. Hu; Q. Liu; J. Wu; R. Yang; L. Wu","School of Electronic Engineering and Intelligent Manufacturing, Anqing Normal University, Anqing 246133, China; School of Electronic Engineering and Intelligent Manufacturing, Anqing Normal University, Anqing 246133, China; School of Electronic Engineering and Intelligent Manufacturing, Anqing Normal University, Anqing 246133, China; School of Electronic Engineering and Intelligent Manufacturing, Anqing Normal University, Anqing 246133, China; School of Electronic Engineering and Intelligent Manufacturing, Anqing Normal University, Anqing 246133, China; School of Electronic Engineering and Intelligent Manufacturing, Anqing Normal University, Anqing 246133, China","China Communications","","2023","PP","99","1","11","Covert communications can hide the existence of a transmission from the transmitter to receiver. This paper considers an intelligent reflecting surface (IRS) assisted unmanned aerial vehicle (UAV) covert communication system. It was inspired by the high-dimensional data processing and decision-making capabilities of the deep reinforcement learning (DRL) algorithm. In order to improve the covert communication performance, an UAV 3D trajectory and IRS phase optimization algorithm based on double deep Q network (TAP-DDQN) is proposed. The simulations show that TAP-DDQN can significantly improve the covert performance of the IRS-assisted UAV covert communication system, compared with benchmark solutions.","1673-5447","","10.23919/JCC.ea.2022-0336.202302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10122840","UAV;covert communication;deep reinforcement learning;intelligent reflective surface","Interference;Signal to noise ratio;Trajectory;Autonomous aerial vehicles;Communication systems;Three-dimensional displays;Wireless communication","","","","","","","","10 May 2023","","","IEEE","IEEE Early Access Articles"
"Some theoretical backgrounds for reinforcement learning model of supply chain management under stochastic demand","D. Filatova; C. El-Nouty; R. V. Fedorenko","Ephe Chart 4 Rue Ferrus, Paris, France; Laga, Université Sorbonne Paris Nord 99 Avenue J-B Clément, Villetaneuse, France; Department of Research Degree, Samara State University of Economics Ulitsa Sovetskoy Armii, 141, Samara, Russian Federation","2021 International Conference on Information and Digital Technologies (IDT)","30 Jul 2021","2021","","","24","30","Integrated and collaborative decision-making systems have been increasingly employed by all companies involved in the supply chain. However, steadfast fact-based decision-making tools are still difficult to get as far as these require adapted methodology. This paper studies the sustainable management problem of a two-echelon supply chain in which a supplier has a limited production capacity to satisfy the quality and quantity requirements of a retailer under stochastic demand. We motivate the model selection, which can be used in reinforcement learning, as well as sustainability constraints for a one-product manufacturing system taking into account production, maintenance, quality, and inventory operations and explore the supplier's production behavior with a required quality level. Pursuing the profit maximization, formed as the difference between the sales revenue and the expenditures on the abovementioned operations, we determine and characterize the optimal conditions for the supply chain management in the presence of the stochastic demand and quality requirements. Our study shows that the stochastic formulation of the optimal control problem and its solution allow taking into account the short-range and long-range dependences exhibited by consumers' behavior. In this context, we characterize the optimal policy for the supplier and the retailer. Finally, we provide managerial insights concerning the sustainable coordination conditions of the supply chain under and quality requirements.","2575-677X","978-1-6654-3692-2","10.1109/IDT52577.2021.9497569","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9497569","","Supply chain management;Supply chains;Decision making;Stochastic processes;Collaboration;Reinforcement learning;Production","decision making;learning (artificial intelligence);optimal control;profitability;retailing;stochastic processes;supply chain management;supply chains","supply chain management;stochastic demand;quality requirements;stochastic formulation;optimal control problem;sustainable coordination conditions;theoretical backgrounds;reinforcement learning model;decision-making systems;steadfast fact-based decision-making tools;sustainable management problem;two-echelon supply chain;production capacity;quantity requirements;model selection;sustainability constraints;one-product manufacturing system;account production;supplier;required quality level","","1","","17","IEEE","30 Jul 2021","","","IEEE","IEEE Conferences"
"Frame-Correlation Transfers Trigger Economical Attacks on Deep Reinforcement Learning Policies","X. Qu; Y. -S. Ong; A. Gupta","Computational Intelligence Lab, School of Computer Science and Engineering, Nanyang Technological University, Singapore; Data Science and Artificial Intelligence Research Centre, School of Computer Science and Engineering, Nanyang Technological University, Singapore; Singapore Institute of Manufacturing Technology (SIMTech), Agency for Science, Technology and Research, Singapore","IEEE Transactions on Cybernetics","19 Jul 2022","2022","52","8","7577","7590","Adversarial attack can be deemed as a necessary prerequisite evaluation procedure before the deployment of any reinforcement learning (RL) policy. Most existing approaches for generating adversarial attacks are gradient based and are extensive, viz., perturbing every pixel of every frame. In contrast, recent advances show that gradient-free selective perturbations (i.e., attacking only selected pixels and frames) could be a more realistic adversary. However, these attacks treat every frame in isolation, ignoring the relationship between neighboring states of a Markov decision process; thus resulting in high computational complexity that tends to limit their real-world plausibility due to the tight time constraint in RL. Given the above, this article showcases the first study of how transferability across frames could be exploited for boosting the creation of minimal yet powerful attacks in image-based RL. To this end, we introduce three types of frame-correlation transfers (FCTs) (i.e., anterior case transfer, random projection-based transfer, and principal components-based transfer) with varying degrees of computational complexity in generating adversaries via a genetic algorithm. We empirically demonstrate the tradeoff between the complexity and potency of the transfer mechanism by exploring four fully trained state-of-the-art policies on six Atari games. Our FCTs dramatically speed up the attack generation compared to existing methods, often reducing the computation time required to nearly zero; thus, shedding light on the real threat of real-time attacks in RL.","2168-2275","","10.1109/TCYB.2020.3041265","Singtel Cognitive and Artificial Intelligence Lab for Enterprises (SCALE@NTU), which is a Collaboration Between Singapore Telecommunications Limited; Nanyang Technological University (NTU); Singapore Government Through the Industry Alignment Fund—Industry Collaboration Projects Grant; A*STAR Cyber-Physical Production System—Toward Contextual and Intelligent Response Research Program(grant numbers:RIE2020 IAF-PP,A19C1a0018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9318536","Adversarial attack;deep reinforcement learning (DRL);transfer optimization","Optimization;Perturbation methods;Task analysis;Reinforcement learning;Games;Correlation;Computational complexity","computational complexity;computer crime;computer games;decision theory;deep learning (artificial intelligence);genetic algorithms;gradient methods;Markov processes;principal component analysis;reinforcement learning","economical attacks;deep reinforcement learning policies;adversarial attack;reinforcement learning policy;gradient-free selective perturbations;Markov decision process;computational complexity;image-based RL;frame-correlation transfers;anterior case transfer;random projection-based transfer;principal component-based transfer;Atari games;genetic algorithm","Learning;Machine Learning;Markov Chains;Policy;Reinforcement, Psychology","2","","56","IEEE","8 Jan 2021","","","IEEE","IEEE Journals"
"Cooperative behavior acquisition mechanism for a multi-robot system based on reinforcement learning in continuous space","T. Yasuda; K. Ohkura; T. Taura","Department of Mechanical Engineering, Faculty of Engineering, Kobe University, Kobe, Hyogo, Japan; Department of Mechanical Engineering, Faculty of Engineering, Kobe University, Kobe, Hyogo, Japan; Department of Mechanical Engineering, Faculty of Engineering, Kobe University, Kobe, Hyogo, Japan","Proceedings 2003 IEEE International Symposium on Computational Intelligence in Robotics and Automation. Computational Intelligence in Robotics and Automation for the New Millennium (Cat. No.03EX694)","18 Aug 2003","2003","3","","1539","1544 vol.3","This paper describes an approach to controlling an autonomous multi-robot system. One of the most important issues for this type of system is how to design an on-line autonomous behavior acquisition mechanism which is capable of developing each robot's role in an embedded environment. Our approach is applying reinforcement learning that uses Bayesian discrimination method for segmenting the continuous state and action spaces simultaneously. In addition to this, neural networks are provided for predicting the other robots' moves at the next time step in order to support the learning in a dynamic environment that originates from the other learning robots. The output signals are utilized as the sensory information for the reinforcement learning to increase the stability of the learning problem. A homogeneous multi-robot system is built for evaluation.","","0-7803-7866-0","10.1109/CIRA.2003.1222226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222226","","Multirobot systems;Learning;Orbital robotics;Robot sensing systems;Control systems;Stability;Computational intelligence;Robotics and automation;Mechanical engineering;Bayesian methods","multi-robot systems;learning (artificial intelligence);cooperative systems;neural nets","cooperative behavior acquisition;multirobot system;reinforcement learning;continuous space;Bayesian discrimination method;neural network;sensory information;stability","","","","18","IEEE","18 Aug 2003","","","IEEE","IEEE Conferences"
"Research on Data Distribution for VANET Based on Deep Reinforcement Learning","Y. Yang; R. Zhao; X. Wei","College of Information Science and Engineering, Huaqiao University Key laboratory of mobile multimedia in Xiamen, Xiamen, China; College of Information Science and Engineering, Huaqiao University Key laboratory of mobile multimedia in Xiamen, Xiamen, China; College of Information Science and Engineering, Huaqiao University Key laboratory of mobile multimedia in Xiamen, Xiamen, China","2019 International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM)","9 Jan 2020","2019","","","484","487","In this paper, we propose a vehicular ad hoc network (VANET) data distribution scheme based on artificial intelligence software-defined network (SDN) controller. Our aim is to improve the throughput of VANET by selecting edge cluster head nodes and gateway cluster head nodes based on deep reinforcement learning. In the entire VANET network, SDN determines the edge cluster head through neural episodic control (NEC). The gateway cluster head is determined by Q-learning. Then the gateway cluster head nodes act as the role that download the data from road side units (RSU) and then transmit the data to vehicles which need these data. Numerical results have shown the proposed scheme can provide greater throughput in VANET compared with the existing scheme.","","978-1-7281-4691-1","10.1109/AIAM48774.2019.00102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950857","data distribution, deep reinforcement learning, reinforcement learning, VANET.","","learning (artificial intelligence);software defined networking;telecommunication control;vehicular ad hoc networks","vehicular ad hoc network;artificial intelligence;SDN;edge cluster head nodes;gateway cluster head nodes;deep reinforcement learning;neural episodic control;VANET;Q-learning;software-defined network controller;data distribution scheme;road side units","","","","16","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Efficient 2D Simulators for Deep-Reinforcement-Learning-based Training of Navigation Approaches","H. Zeng; L. Kästner; J. Lambrecht","The Chair of Robotics, Artificial Intelligence and Real-time Systems, Technical University Munich (TUM), Germany; The Chair of Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; The Chair of Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany","2023 20th International Conference on Ubiquitous Robots (UR)","8 Aug 2023","2023","","","275","280","In recent years, Deep Reinforcement Learning (DRL) has emerged as a competitive approach for mobile robot navigation. However, training DRL agents often comes at the cost of difficult and tedious training procedures in which powerful hardware is required to conduct oftentimes long training runs. Especially, for complex environments, this proves to be a major bottleneck for widespread adoption of DRL approaches into industries. In this paper we integrate an efficient 2D simulator into the Arena-Rosnav framework of our previous work as an alternative simulation platform to train and develop DRL agents. Therefore, we utilized the provided API to integrate necessary components into the ecosystem of Arena-Rosnav. We evaluated our simulator by training a DRL agent within that platform and compared the training and navigational performance against the baseline 2D simulator Flatland, which is the default simulating platform of Arena-Rosnav. Results demonstrate that using our Arena2D simulator results in substantially faster training times and in some scenarios better agents. This proves to be an important step towards resource-efficient DRL training, which accelerates training times and improve the development cycle of DRL agents for navigation tasks. We made our simulator openly available at https://github.com/Arena-Rosnav/arena2d.","","979-8-3503-3517-0","10.1109/UR57808.2023.10202268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10202268","","Training;Deep learning;Visualization;Navigation;Pipelines;Reinforcement learning;Reliability","control engineering computing;deep learning (artificial intelligence);mobile robots;navigation;path planning;reinforcement learning;robot programming","alternative simulation platform;Arena-Rosnav framework;Arena2D simulator;baseline 2D simulator;competitive approach;deep-reinforcement-learning-based training;default simulating platform;difficult training procedures;DRL agent;DRL agents training;DRL approaches;efficient 2D simulators;mobile robot navigation;navigation approaches;navigation tasks;navigational performance;resource-efficient DRL training;tedious training procedures","","","","20","IEEE","8 Aug 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning Algorithm of Intelligent Cultural and Creative Product Design Based on Data Mining Technology","J. Xu; Y. Cao","College of Cultural and Creative Industry Shandong Institute of Commerce and Technology, Jinan, Shandong, China; College of Digital Marketing Industry, Shandong Institute of Commerce and Technology, Jinan, Shandong, China","2022 International Conference on Knowledge Engineering and Communication Systems (ICKES)","17 Mar 2023","2022","","","1","4","With the progress of science and technology and the development of society, the emotional needs of users have been paid more and more attention by academia and industry. At the same time, the market has also put forward new requirements for the rapid iteration of intelligent cultural and creative products. The emotional design of intelligent cultural and creative products can endow intelligent cultural and creative products with emotional characteristics and bring users a beautiful and pleasant experience. A lot of research work has been carried out at home and abroad. This paper aims to study the reinforcement learning algorithm of intelligent cultural and creative product design based on data mining technology. This paper will firstly guide the content of industrial design in the information age, aiming at comfort and ease of use, briefly summarize the research of domestic and foreign scholars in the field of traditional industrial design of intelligent cultural and creative products, and take these research contents as For reference, study the design elements of intelligent cultural and creative from the aspects of shape, material and color, and analyze the influencing factors of related elements; and then study the intelligent cultural and creative system from the three dimensions of intelligent cultural and creative interaction, interactive content, and interactive behavior elements of interaction design. Secondly, design and practice the appearance of intelligent cultural and creative based on the elements of appearance design, analyze and research intelligent cultural and creative from the perspective of ergonomics and product aesthetics, design corresponding conceptual sketches, screen modeling and rendering to obtain plans, and evaluate corresponding design scheme and complete the physical construction of the scheme. Finally, based on the development trend of interaction design in the future, the interaction method and interaction content in this intelligent cultural and creative solution are designed, and an easy-to-use application product is designed based on the above design elements. Experiments have shown that the user satisfaction of intelligent cultural and creative products designed by algorithms has reached more than 90%.","","978-1-6654-5637-1","10.1109/ICKECS56523.2022.10060711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10060711","creative products;data mining;industrial design;intelligent cultural;machine learning","Knowledge engineering;Image recognition;Shape;Reinforcement learning;Rendering (computer graphics);Market research;Information age","data mining;design engineering;human computer interaction;product design;reinforcement learning;user experience","creative interaction;creative product design;data mining technology;emotional design;ergonomics;industrial design;intelligent cultural product design;product aesthetics;reinforcement learning algorithm;user experience","","","","12","IEEE","17 Mar 2023","","","IEEE","IEEE Conferences"
"Multiagent reinforcement learning with organizational-learning oriented classifier system","K. Takadama; S. Nakasuka; T. Terano","Graduate School of Engineering, University of Tokyo, Japan; Research Center for AdvancedScience and Technology, University of Tokyo, Japan; Graduate School of SystemsManagement, University of Tsukuba, Japan","1998 IEEE International Conference on Evolutionary Computation Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98TH8360)","6 Aug 2002","1998","","","63","68","Organizational learning oriented classifier system (OCS) is a new architecture proposed by us for an evolutionary computational model. We have shown its effectiveness in large scale problems with printed circuit board (PCB) redesign using computer aided design (CAD). The paper proposes a novel reinforcement learning method for multiagents with OCS for more practical and engineering use. To validate the effectiveness of our method, we have conducted experiments on real scale PCB design problems for electric appliances. The experimental results have suggested that: (1) our method has found feasible solutions with the same quality of those by human experts; (2) the solutions are globally better than those by the conventional reinforcement learning methods with regard to both the total wiring length and the number of iterations.","","0-7803-4869-9","10.1109/ICEC.1998.699138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=699138","","Printed circuits;Large-scale systems;Design automation;Humans;Markov processes;Computational modeling;Learning systems;Wiring;Algorithm design and analysis;Evolutionary computation","learning (artificial intelligence);software agents;genetic algorithms;printed circuit design;circuit CAD","multiagent reinforcement learning;organizational learning oriented classifier system;evolutionary computational model;large scale problems;printed circuit board redesign;computer aided design;reinforcement learning method;real scale PCB design problems;electric appliances;feasible solutions;total wiring length","","6","","17","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Behavior hierarchy learning in a behavior-based system using reinforcement learning","A. M. Farahmand; M. N. Ahmadabadi; B. N. Araabi","Control and Intelligent Processing Center of Excellence, Department of Elect and Computer Engineering, School of Cognitive Sciences, IPM, University of Tehran, Tehran, Iran; Control and Intelligent Processing Center of Excellence, Department of Elect and Computer Engineering, School of Cognitive Sciences, IPM, University of Tehran, Tehran, Iran; Control and Intelligent Processing Center of Excellence, Department of Elect and Computer Engineering, School of Cognitive Sciences, IPM, University of Tehran, Tehran, Iran","2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)","14 Feb 2005","2004","2","","2050","2055 vol.2","Hand-design of an intelligent agent's behaviors and their hierarchy is a very hard task. One of the most important steps toward creating intelligent agents is providing them with capability to learn the required behaviors and their architecture. Architecture learning in a behavior-based agent with subsumption architecture is considered in this paper. Overall value function is decomposed into easily calculate-able parts in order to learn the behavior hierarchy. Using probabilistic formulations, two different decomposition methods are discussed: storing the estimated value of each behavior in each layer, and storing the ordering of behaviors in the architecture. Using defined decompositions, two appropriate credit assignment methods are designed. Finally, the proposed methods are tested in a multi-robot object-lifting task that results in satisfactory performance.","","0-7803-8463-6","10.1109/IROS.2004.1389699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1389699","","Learning;Robotics and automation;Intelligent agent;Design methodology;Testing;Multiagent systems;State-space methods;Process control;Control systems;Intelligent control","multi-agent systems;learning (artificial intelligence)","behavior hierarchy learning;behavior system;reinforcement learning;architecture learning;intelligent agents;multi-robot object-lifting task","","2","","15","IEEE","14 Feb 2005","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-based Auto-tuning Algorithm for Cavity Filters","D. P. Mtowe; S. -H. Son; D. Ahn; D. M. Kim","Department of ICT Convergence, Soonchunhyang University, Asan, South Korea; Department of ICT Convergence, Soonchunhyang University, Asan, South Korea; Department of ICT Convergence, Soonchunhyang University, Asan, South Korea; Department of ICT Convergence, Soonchunhyang University, Asan, South Korea","2023 Photonics & Electromagnetics Research Symposium (PIERS)","28 Aug 2023","2023","","","1040","1042","Over the past few decades, the tuning of cavity filters has often been done by trial and error, using human experience and intuition, due to the imprecision of the design and manufacturing tolerances, which often results in detuning the filters and requiring costly post-production fine-tuning. Various techniques using optimization and machine learning have been investigated to automate the process. The superiority of a deep reinforcement learning approach, which can properly explore various possibilities and operate them in the desired way according to the well-defined reward, has motivated us to apply it to our problem. To meet the demand for an automatic tuning algorithm for cavity filters with high accuracy and efficiency, this study proposes an automatic tuning algorithm for cavity filters based on the deep reinforcement learning. For the efficiency of the tuning process, we limit the order of the elements to be tuned, inspired by the experience of experts based on domain knowledge. In addition, the coarse tuning process is performed first, followed by the fine tuning process to improve the tuning accuracy. The proposed method has demonstrated the ability of the deep reinforcement learning to learn the complex relationship between impedance values of equivalent circuit elements and S-parameters to effectively satisfy filter design requirements within an acceptable time range. The performance of the proposed automatic tuning algorithm has been evaluated through simulation experiments. The effectiveness of the proposed algorithm is demonstrated by the fact that it is able to tune a detuned filter from random starting point to meet its design requirements.","2831-5804","979-8-3503-1284-3","10.1109/PIERS59004.2023.10221259","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10221259","","Deep learning;Reinforcement learning;Filtering algorithms;Scattering parameters;Manufacturing;Impedance;Electromagnetics","computational electromagnetics;deep learning (artificial intelligence);equivalent circuits;reinforcement learning;tuning","automatic tuning algorithm;autotuning algorithm;cavity filters;coarse tuning process;deep reinforcement learning approach;detuned filter;equivalent circuit elements;filter design requirements;fine tuning process;human experience;machine learning;optimization;postproduction fine-tuning;random starting point;S-parameters","","","","5","IEEE","28 Aug 2023","","","IEEE","IEEE Conferences"
"Modeling of Plant Dynamics and Control based on Reinforcement learning","T. Maeda; M. Nakayama; A. Kitaura","Production System Research Laboratory, Kobe Steel, Ltd., Kobe, Japan; Production System Research Laboratory, Kobe Steel, Ltd., Kobe, Japan; Department of Information and Knowledge Engineering, Tottori University, Tottori, Japan","2006 SICE-ICASE International Joint Conference","26 Feb 2007","2006","","","6027","6030","The dynamics modeling of a plant was developed by using Q-learning, which is one method of reinforcement learning. We thought the modeling of the dynamical system to be the function approximation problem for the system output response signal, and enhanced reinforcement learning to the modeling method of the dynamical system. We describe that this modeling method guarantee to offer highly accurate dynamics models by numerical samples, which deals with incinerator's combustion. Results of numerical simulation show that the predictive control method using these models has robust tracking property","","89-950038-4-7","10.1109/SICE.2006.315850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4108658","reinforcement learing;dynamical systems;modeling;predictive control","Learning;Combustion;Predictive models;Temperature control;Numerical models;Predictive control;Error correction;Function approximation;Process control;Uncertainty","learning (artificial intelligence);nonlinear dynamical systems;predictive control;process control","plant dynamics model;reinforcement learning;predictive control","","","","3","","26 Feb 2007","","","IEEE","IEEE Conferences"
"Learning to Shape by Grinding: Cutting-Surface-Aware Model-Based Reinforcement Learning","T. Hachimine; J. Morimoto; T. Matsubara","Division of Information Science, Graduate School of Science and Technology, Nara Institute of Science and Technology, Nara, Japan; Department of Systems Science, Graduate School of Informatics, Kyoto University, Kyoto, Japan; Division of Information Science, Graduate School of Science and Technology, Nara Institute of Science and Technology, Nara, Japan","IEEE Robotics and Automation Letters","21 Aug 2023","2023","8","10","6235","6242","Object shaping by grinding is a crucial industrial process in which a rotating grinding belt removes material. Object-shape transition models are essential to achieving automation by robots; however, learning such a complex model that depends on process conditions is challenging because it requires a significant amount of data, and the irreversible nature of the removal process makes data collection expensive. This letter proposes a cutting-surface-aware Model-Based Reinforcement Learning (MBRL) method for robotic grinding. Our method employs a cutting-surface-aware model as the object's shape transition model, which in turn is composed of a geometric cutting model and a cutting-surface-deviation model, based on the assumption that the robot action can specify the cutting surface made by the tool. Furthermore, according to the grinding resistance theory, the cutting-surface-deviation model does not require raw shape information, making the model's dimensions smaller and easier to learn than a naive shape transition model directly mapping the shapes. Through evaluation and comparison by simulation and real robot experiments, we confirm that our MBRL method can achieve high data efficiency for learning object shaping by grinding and also provide generalization capability for initial and target shapes that differ from the training data.","2377-3766","","10.1109/LRA.2023.3303721","JST-Mirai Program(grant numbers:JPMJMI21B1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214100","Manipulation planning;model learning for control;reinforcement learning","Shape;Robots;Surface treatment;Predictive models;Data models;Surface resistance;Belts","belts;grinding;production engineering computing;reinforcement learning","complex model;cutting-surface-aware model-based reinforcement;cutting-surface-aware model-based reinforcement learning;cutting-surface-deviation model;data collection;geometric cutting model;grinding resistance theory;industrial process;initial target shapes;MBRL method;naive shape transition model;object shaping;object-shape transition models;process conditions;raw shape information;removal process;robotic grinding;rotating grinding belt","","","","27","IEEE","9 Aug 2023","","","IEEE","IEEE Journals"
"Dynamic Control of a Fiber Manufacturing Process Using Deep Reinforcement Learning","S. Kim; D. D. Kim; B. W. Anthony","Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE/ASME Transactions on Mechatronics","15 Apr 2022","2022","27","2","1128","1137","This article presents a model-free deep reinforcement learning (DRL) approach for controlling a fiber drawing system. The custom DRL-based control system predictively regulates fiber diameter and produces a fiber with a desired, constant or nonconstant, diameter trajectory, i.e., diameter variation along the fiber length. Physical models of the system are not used. The system was trained and tested on a compact fiber drawing system, which has nonlinear delayed dynamics and stochastic behaviors. For a reference trajectory with random step changes, after 1 h of training, the DRL controller showed the same root-mean-squared error (RMSE) as an optimized PI controller; after 3 h of training, it achieved the performance of a quadratic dynamic matrix controller (QDMC). While the PI feedback controller showed 3.5 s of time lag in a step response, the DRL controller showed less than a second of time lag. Controller performance tests on trajectories not used in the training process are conducted; for a sine sweep reference trajectory, the DRL controller maintained an RMSE under 40 $\mu \text{m}$ up to a frequency of 45 mHz, compared to 25 mHz for QDMC.","1941-014X","","10.1109/TMECH.2021.3070973","Massachusetts Institute of Technology Skoltech Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9437478","Fiber fabrication;learning control systems;neural network (NN) applications;process control","Heating systems;Process control;Trajectory;Optical fiber sensors;Instruction sets;History;Training","control system synthesis;delays;feedback;manufacturing processes;mean square error methods;nonlinear control systems;PI control;predictive control;reinforcement learning;step response;trajectory control","fiber manufacturing process;model-free deep reinforcement learning approach;custom DRL-based control system;fiber diameter;constant diameter trajectory;nonconstant diameter trajectory;diameter variation;fiber length;physical models;compact fiber drawing system;nonlinear delayed dynamics;optimized PI controller;quadratic dynamic matrix controller;PI feedback controller;controller performance tests;sine sweep reference trajectory;root-mean-squared error;diameter trajectory","","1","","31","IEEE","20 May 2021","","","IEEE","IEEE Journals"
"Performance of Reinforcement Learning on Traditional Video Games","Y. Sun","New York University Center for Data Science, New York, the United States","2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM)","8 Mar 2022","2021","","","276","279","Reinforcement learning is a model structure that learns and updates the model based on receiving the reward and feedback of certain environments. In recent researches, reinforcement learning has been shown to be very effective in a well- defined or simulated environment to identify optimal strategies for solving given problems. This work test the performance of several Deep Reinforcement Learning model structures on a classic video game, Flappy bird. This project wants to further analyze the performance of different settings of deep Q networks and policy gradient models, to see how different model structures affect the training speed, accuracy, and efficiency. The author has found that the Double DQN model can achieve the best score among all deep learning models, whereas the Q-table model can achieve a better score with a proper feature extraction method, as the environment is rather simple.","","978-1-6654-1732-7","10.1109/AIAM54119.2021.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9724774","reinforcement learning;q-learning;dqn;double dqn;dueling dqn;mobile game","Training;Deep learning;Analytical models;Reinforcement learning;Games;Birds;Feature extraction","computer games;deep learning (artificial intelligence);feature extraction;optimisation;reinforcement learning","policy gradient models;double DQN model;Q-table model;video games;deep reinforcement learning model;optimal strategies;deep Q networks;feature extraction method","","1","","11","IEEE","8 Mar 2022","","","IEEE","IEEE Conferences"
"Autonomous Navigation of UAVs in Resource Limited Environment Using Deep Reinforcement Learning","P. Sha; Q. Wang","School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","36","41","In recent years, we can see the number of applications of UAVs in various situations are growing, such as accomplishing missions in resource limited environments where the space and time to perform tasks are limited. In this paper, we propose a new framework for leveraging machine learning technologies such as deep reinforcement learning methods to let the UAV to accomplish navigation tasks in complex resource limited environments. The proposed framework adopt PID algorithm to control the UAV’s attitude and position during flight and use PPO algorithm to optimize the navigation planning. Technical details involve the use of domain specific knowledge and well-designed reward function and state representation. We make some general tests with a single quadcopter UAV in a simulated pybullet environment using the developed framework. This experimental results show that the proposed framework can achieve high performance in navigation tasks in resource limited environments. This will enable continuing research and active development on the deep reinforcement learning based frameworks for UAV autonomous navigation in more complex applications and environments.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023581","Deep reinforcement learning;resource limited environments;UAV autonomous navigation","Deep learning;Machine learning algorithms;Automation;Navigation;Space missions;Reinforcement learning;Planning","autonomous aerial vehicles;helicopters;learning (artificial intelligence);three-term control","complex resource limited environments;deep reinforcement learning;flight;navigation planning;navigation tasks;resource limited environment;simulated pybullet environment;single quadcopter UAV;UAV autonomous navigation","","","","16","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Algorithm Optimization of Deep Reinforcement Learning for Traffic Signal Control of Municipal Road Engineering","H. Ma","College of Management, Tianjin University of Technology, Tianjin, China","2022 4th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM)","27 Mar 2023","2022","","","829","833","Urban road traffic signal control is the key factor affecting the road capacity, but due to the strong randomness of traffic flow, the traditional inductive control method only considers the current phase of vehicle arrival information, which has great limitations. In the stage of intelligent traffic signal control and regulation of traffic engineering, we should do a good job in traffic flow prediction, intelligent traffic signal regulation and control, as well as hardware and software control, so as to improve the quality of intelligent traffic signal control and regulation of traffic engineering. Taking the municipal road at a single intersection as an example, this paper applies deep learning to predict the municipal traffic flow, and then optimizes the deep reinforcement learning algorithm for its traffic signal control and carries out information modeling. The conclusion is that the traffic signal control effect based on algorithm optimization is significantly better than the traditional timing control method, which can effectively reduce the probability of traffic accidents and realize the intellectualization of municipal traffic engineering.","","978-1-6654-6399-7","10.1109/AIAM57466.2022.00168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071498","Deep Reinforcement Learning;Municipal Traffic Engineering;Algorithm Optimization;Traffic Signal Control;Traffic Flow Prediction","Deep learning;Pollution;Roads;Reinforcement learning;Prediction algorithms;Regulation;Automobiles","deep learning (artificial intelligence);reinforcement learning;road traffic;road traffic control;roads;traffic engineering computing","algorithm optimization;deep reinforcement learning algorithm;intelligent traffic signal control;intelligent traffic signal regulation;municipal road engineering;municipal traffic engineering;municipal traffic flow;road capacity;software control;traditional inductive control method;traditional timing control method;traffic accidents;traffic flow prediction;traffic signal control effect;urban road traffic signal control","","","","6","IEEE","27 Mar 2023","","","IEEE","IEEE Conferences"
"Compiler Optimization for Quantum Computing Using Reinforcement Learning","N. Quetschlich; L. Burgholzer; R. Wille","Chair for Design Automation, Technical University of Munich, Germany; Institute for Integrated Circuits, Johannes Kepler University, Linz, Austria; Chair for Design Automation, Technical University of Munich, Germany","2023 60th ACM/IEEE Design Automation Conference (DAC)","15 Sep 2023","2023","","","1","6","Any quantum computing application, once encoded as a quantum circuit, must be compiled before being executable on a quantum computer. Similar to classical compilation, quantum compilation is a sequential process with many compilation steps and numerous possible optimization passes. Despite the similarities, the development of compilers for quantum computing is still in its infancy—lacking mutual consolidation on the best sequence of passes, compatibility, adaptability, and flexibility. In this work, we take advantage of decades of classical compiler optimization and propose a reinforcement learning framework for developing optimized quantum circuit compilation flows. Through distinct constraints and a unifying interface, the framework supports the combination of techniques from different compilers and optimization tools in a single compilation flow. Experimental evaluations show that the proposed framework—set up with a selection of compilation passes from IBM’s Qiskit and Quantinuum’s TKET—significantly outperforms both individual compilers in 73% of cases regarding the expected fidelity. The framework is available on GitHub (https://github.com/cda-tum/MQTPredictor) as part of the Munich Quantum Toolkit (MQT).","","979-8-3503-2348-1","10.1109/DAC56929.2023.10248002","European Research Council; Bayer; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10248002","","Adaptation models;Design automation;Reinforcement learning;Markov processes;Quantum circuit;Integrated circuit modeling;Optimization","learning (artificial intelligence);optimising compilers;program compilers;quantum computing","classical compilation;classical compiler optimization;compilation passes;compilation steps;Munich Quantum Toolkit;optimization tools;optimized quantum circuit compilation;quantum compilation;quantum computer;quantum computing application;reinforcement learning framework","","","","38","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
"TDNCIB: Target-Driven Navigation with Convolutional Information Branch Based on Deep Reinforcement Learning","X. Zhao; T. Wang; K. Liu; M. Zhang; P. Shi; H. Snoussi","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Institute of Artificial Intelligence, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; College of Electrical Engineering and Control Science, Nanjing Tech University, Nanjing, China; School of Mathematics and Computer Science, Fujian Normal University, Fujian, China; Institute Charles Delaunay-LM2S, University of Technology of Troyes, Troyes, France","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","1421","1426","Target-driven navigation has been a significant application field of reinforcement learning. However, sparse reward and lack of useful environment information are still existing problems. Therefore, in this paper, we propose a novel design to the target-driven visual navigation based on deep reinforcement learning: Target-Driven Navigation with Convolutional Information Branch (TDNCIB), which enriches the high-dimensional information learned by the network from the environment. It is an improvement of A2C algorithm. CIB can help make full use of environmental information and serve as a complement to the baseline. Besides, a novel reward design using similarity is introduced to relieve sparse reward. The ablation study is implemented in 4 different scenes of AI2-THOR framework comparing the baseline and our TDNCIB. According to our evaluation, TDNCIB converges faster than the baseline and achieves better performance in 5 common metrics with shorter trajectory length, less collision and higher success rate.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728402","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728402","","Measurement;Visualization;Automation;Navigation;Reinforcement learning;Trajectory","convolutional neural nets;deep learning (artificial intelligence);mobile robots;reinforcement learning;robot vision","deep reinforcement learning;sparse reward;target-driven visual navigation;TDNCIB;high-dimensional information;environmental information;convolutional information branch;A2C algorithm;AI2-THOR framework","","","","21","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"A Comparative Study of Situation Awareness-Based Decision-Making Model Reinforcement Learning Adaptive Automation in Evolving Conditions","R. D. Costa; C. M. Hirata; V. U. Pugliese","Department of Computer Science, Instituto Tecnológico de Aeronáutica, São José dos Campos, São Paulo, Brazil; Department of Computer Science, Instituto Tecnológico de Aeronáutica, São José dos Campos, São Paulo, Brazil; Institute of Science and Technology, Universidade Federal de Sao Paulo, São José dos Campos, São Paulo, Brazil","IEEE Access","22 Feb 2023","2023","11","","16166","16182","Situation-awareness-based decision-making (SABDM) models constructed using cognitive maps and goal-direct task analysis techniques have been successfully used in decision support systems in safety-critical and mission-critical environments such as air traffic control and electrical energy distribution. Reinforcement learning (RL) and other machine learning techniques are used to automate situational awareness mental model parameter adjustments, reducing expert work on the initial configuration and long-term maintenance without affecting the mental model’s structure and maintaining the situation-awareness-based decision-making model’s cognitive and explainability characteristics. Real-world models should evolve to cope with changes in the environmental conditions. This study evaluates the application of reinforcement learning as an online adaptive technique to adjust the situation-awareness mental model parameters under evolving conditions, a technique we named SABDM/RL. We conducted evaluation experiments using real-world public datasets to compare the performance of the SABDM/RL technique with that of other adaptive machine learning methods under distinct concept drift-evolving conditions. We measured the overall and dynamic performances of these techniques to understand how well they adapt to evolving environmental conditions. The experiments show that the SABDM/RL performs similarly to modern online adaptive machine learning classification methods with the support of concept drift detection techniques while maintaining the mental model strength of the situation-awareness-based decision-making systems.","2169-3536","","10.1109/ACCESS.2023.3245055","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES) foundation from the Ministério da Educação (MEC) of the Brazilian Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10044646","Adaptive systems;artificial intelligence;decision support systems;evolving behavior;explainable artificial intelligence;reinforcement learning;situation awareness","Reinforcement learning;Decision making;Adaptation models;Automation;Task analysis;Cognitive science;Analytical models","decision making;decision support systems;pattern classification;reinforcement learning;task analysis","adaptive machine learning method;cognitive map;concept drift-evolving conditions;decision support systems;goal-direct task analysis techniques;mental model strength;mission-critical environments;online adaptive machine learning classification method;SABDM-RL technique;safety-critical environments;situation awareness-based decision-making model reinforcement learning adaptive automation;situation-awareness-based decision-making model cognitive characteristics;situation-awareness-based decision-making model explainability characteristics;situational awareness mental model parameter adjustments","","1","","65","CCBYNCND","14 Feb 2023","","","IEEE","IEEE Journals"
"Adaptive Storage Optimization Scheme for Blockchain-IIoT Applications Using Deep Reinforcement Learning","N. K. Akrasi-Mensah; A. S. Agbemenu; H. Nunoo-Mensah; E. T. Tchao; A. -R. Ahmed; E. Keelson; A. Sikora; D. Welte; J. J. Kponyo","Distributed IoT Platforms, Privacy and Edge-Intelligence Research (DIPPER) Laboratory, Faculty of Electrical and Computer Engineering, Kwame Nkrumah University of Science and Technology, PMB, Kumasi, Ghana; Distributed IoT Platforms, Privacy and Edge-Intelligence Research (DIPPER) Laboratory, Faculty of Electrical and Computer Engineering, Kwame Nkrumah University of Science and Technology, PMB, Kumasi, Ghana; Distributed IoT Platforms, Privacy and Edge-Intelligence Research (DIPPER) Laboratory, Faculty of Electrical and Computer Engineering, Kwame Nkrumah University of Science and Technology, PMB, Kumasi, Ghana; Distributed IoT Platforms, Privacy and Edge-Intelligence Research (DIPPER) Laboratory, Faculty of Electrical and Computer Engineering, Kwame Nkrumah University of Science and Technology, PMB, Kumasi, Ghana; Distributed IoT Platforms, Privacy and Edge-Intelligence Research (DIPPER) Laboratory, Faculty of Electrical and Computer Engineering, Kwame Nkrumah University of Science and Technology, PMB, Kumasi, Ghana; Distributed IoT Platforms, Privacy and Edge-Intelligence Research (DIPPER) Laboratory, Faculty of Electrical and Computer Engineering, Kwame Nkrumah University of Science and Technology, PMB, Kumasi, Ghana; Institute of Reliable Embedded Systems and Communication Electronics (ivESK), Offenburg University of Applied Sciences, Offenburg, Germany; Institute of Reliable Embedded Systems and Communication Electronics (ivESK), Offenburg University of Applied Sciences, Offenburg, Germany; Responsible Artificial Intelligence Laboratory (RAIL), Faculty of Electrical and Computer Engineering, Kwame Nkrumah University of Science and Technology, PMB, Kumasi, Ghana","IEEE Access","6 Jan 2023","2023","11","","1372","1385","Blockchain-IIoT integration into industrial processes promises greater security, transparency, and traceability. However, this advancement faces significant storage and scalability issues with existing blockchain technologies. Each peer in the blockchain network maintains a full copy of the ledger which is updated through consensus. This full replication approach places a burden on the storage space of the peers and would quickly outstrip the storage capacity of resource-constrained IIoT devices. Various solutions utilizing compression, summarization or different storage schemes have been proposed in literature. The use of cloud resources for blockchain storage has been extensively studied in recent years. Nonetheless, block selection remains a substantial challenge associated with cloud resources and blockchain integration. This paper proposes a deep reinforcement learning (DRL) approach as an alternative to solving the block selection problem, which involves identifying the blocks to be transferred to the cloud. We propose a DRL approach to solve our problem by converting the multi-objective optimization of block selection into a Markov decision process (MDP). We design a simulated blockchain environment for training and testing our proposed DRL approach. We utilize two DRL algorithms, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO) to solve the block selection problem and analyze their performance gains. PPO and A2C achieve 47.8% and 42.9% storage reduction on the blockchain peer compared to the full replication approach of conventional blockchain systems. The slowest DRL algorithm, A2C, achieves a run-time 7.2 times shorter than the benchmark evolutionary algorithms used in earlier works, which validates the gains introduced by the DRL algorithms. The simulation results further show that our DRL algorithms provide an adaptive and dynamic solution to the time-sensitive blockchain-IIoT environment.","2169-3536","","10.1109/ACCESS.2022.3233474","German Federal Ministry of Research and Education (Bundesministerium für Bildung und Forschung, BMBF) and the German Academic Exchange Service (Deutscher Akademischer Austauschdienst, DAAD); Distributed Internet of Things (IoT)-Platforms for Safe Food Production in Education, Research and Industry (DIPPER) Project, which is co-financed by the BMBF (Förderkennzeichen: 01DG21017) and DAAD(grant numbers:57557211); Responsible Artificial Intelligence Laboratory Project which is being co-financed by the International Development Research Centre (IDRC)(grant numbers:109832-001); Deutsche Gesellschaft für Internationale Zusammenarbeit (GIZ)(grant numbers:81280702); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004557","Blockchain;IIoT;reinforcement learning;scalability;storage efficiency;storage optimization","Blockchains;Optimization;Cloud computing;Heuristic algorithms;Reinforcement learning;Deep learning;Scalability","blockchains;deep learning (artificial intelligence);evolutionary computation;Internet of Things;manufacturing systems;Markov processes;production engineering computing;reinforcement learning","A2C;adaptive solution;adaptive storage optimization scheme;benchmark evolutionary algorithm;block selection problem;blockchain environment;blockchain network;blockchain peer;blockchain storage;blockchain technologies;blockchain-IIoT integration;cloud resources;deep reinforcement learning approach;DRL algorithms;DRL approach;dynamic solution;industrial processes;ledger;Markov decision process;multiobjective optimization;PPO;proximal policy optimization;replication approach;resource-constrained IIoT devices;scalability issues;storage capacity;storage reduction;storage space;time-sensitive blockchain-IIoT environment","","2","","44","CCBY","30 Dec 2022","","","IEEE","IEEE Journals"
"Iterative Learning Control (ILC) Guided Reinforcement Learning Control (RLC) Scheme for Batch Processes","X. Xu; H. Xie; J. Shi","Department of Chemical & Biochemical Engineering, School of Chemistry & Chemical Engineering, Xiamen University, Xiamen, P. R. China; Department of Chemical & Biochemical Engineering, School of Chemistry & Chemical Engineering, Xiamen University, Xiamen, P. R. China; Department of Chemical & Biochemical Engineering, School of Chemistry & Chemical Engineering, Xiamen University, Xiamen, P. R. China","2020 IEEE 9th Data Driven Control and Learning Systems Conference (DDCLS)","7 Dec 2020","2020","","","241","246","Iterative learning control (ILC) is a kind of effective learning control scheme which is mainly designed to solve the problems in controlling a batch or repetitive process. Although the control performances of ILC systems can improved from batch to batch, it still strongly depends on the repeatability of the process and control target. Reinforcement learning (RL) is another learning based optimization algorithm which can be applied to many complicated decision-making scenarios. Data-driven based RL algorithms have good robustness due to the generalization of the policy neural network, however, it is low-data efficiency in network training. In this paper, for batch process control we propose a new reinforcement learning control (RLC) scheme which is guided by classical iterative leaning control. On the one hand, this RLC scheme has capability to optimize the policy network faster than RL algorithm without guidance, on the other hand, the generalization of deep policy network improves the robustness of the control system. Based on the numerical simulations, the effectiveness of the proposed control scheme is demonstrated by comparing with the conventional reinforcement learning algorithm and the P-type iterative learning control scheme. This paper provides a new way for the application of reinforcement learning algorithm to batch process control.","","978-1-7281-5922-5","10.1109/DDCLS49620.2020.9275065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9275065","Iterative learning control (ILC);Deep reinforcement learning;Batch/repetitive processes","Process control;Reinforcement learning;Control systems;Batch production systems;Iterative learning control;Heuristic algorithms;Robustness","batch processing (industrial);iterative learning control;process control","iterative learning control guided reinforcement learning control scheme;ILC;effective learning control scheme;control target;learning based optimization algorithm;RL algorithm;batch process control;classical iterative leaning control;conventional reinforcement learning algorithm;decision-making scenarios;network training;deep policy network;numerical simulations","","2","","16","IEEE","7 Dec 2020","","","IEEE","IEEE Conferences"
"Fuzzy-Based Adaptive Optimization of Unknown Discrete-Time Nonlinear Markov Jump Systems With Off-Policy Reinforcement Learning","H. Fang; Y. Tu; H. Wang; S. He; F. Liu; Z. Ding; S. S. Cheng","Anhui Engineering Laboratory of Human-Robot Integration System and Intelligent Equipment, School of Electrical Engineering and Automation, Anhui University, Hefei, China; Anhui Engineering Laboratory of Human-Robot Integration System and Intelligent Equipment, School of Electrical Engineering and Automation, Anhui University, Hefei, China; Discipline of Engineering and Energy, and the Centre for Water, Energy and Waste, Murdoch University, Murdoch, Australia; Anhui Engineering Laboratory of Human-Robot Integration System and Intelligent Equipment, School of Electrical Engineering and Automation, Anhui University, Hefei, China; Key Laboratory of Advanced Process Control for Light Industry (Ministry of Education), Jiangnan University, Wuxi, China; School of Electrical and Electronic Engineering, The University of Manchester, Manchester, U.K.; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Fuzzy Systems","30 Nov 2022","2022","30","12","5276","5290","This article explores a novel adaptive optimal control strategy for a class of sophisticated discrete-time nonlinear Markov jump systems (DTNMJSs) via Takagi–Sugeno fuzzy models and reinforcement learning (RL) techniques. First, the original nonlinear system model is represented by fuzzy approximation, while the relevant optimal control problem is equivalent to designing fuzzy controllers for linear fuzzy systems with Markov jumping parameters. Subsequently, we derive the fuzzy coupled algebraic Riccati equations for the fuzzy-based discrete-time linear Markov jump systems by using Hamiltonian–Bellman methods. Following this, an online fuzzy optimization algorithm for DTNMJSs as well as the associated equivalence proof is given. Then, a fully model-free off-policy fuzzy RL algorithm is derived with proved convergence for the DTNMJSs without using the information of system dynamics and transition probability. Finally, two simulation examples, respectively, related to the single-link robotic arm and the half-car active suspension are given to verify the effectiveness and good performance of the proposed approach.","1941-0034","","10.1109/TFUZZ.2022.3171844","National Natural Science Foundation of China(grant numbers:62073001); University Synergy Innovation Program of Anhui Province(grant numbers:GXXT-2021-010); State Key Program of National Natural Science Foundation of China(grant numbers:61833007); Key Support Program of University Outstanding Youth Talent of Anhui Province(grant numbers:gxydZD2017001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9767748","Fuzzy coupled algebraic Riccati equations (FCAREs);off-policy iteration;reinforcement learning (RL);Takagi–Sugeno (T–S) fuzzy models","Optimal control;Mathematical models;Nonlinear systems;Markov processes;Optimization;Heuristic algorithms;System dynamics","adaptive control;control system synthesis;convergence;discrete time systems;fuzzy control;fuzzy set theory;learning systems;linear systems;Markov processes;nonlinear control systems;optimal control;optimisation;probability;reinforcement learning;Riccati equations;stochastic systems","adaptive optimal control strategy;convergence;discrete-time linear Markov jump system;DTNMJS;equivalence proof;fully model-free off-policy fuzzy RL algorithm;fuzzy approximation;fuzzy controller design;fuzzy coupled algebraic Riccati equations;fuzzy-based adaptive optimization;half-car active suspension;Hamiltonian-Bellman method;linear fuzzy systems;Markov jumping parameters;nonlinear system model;off-policy reinforcement learning;online fuzzy optimization algorithm;optimal control problem;single-link robotic arm;system dynamics;Takagi-Sugeno fuzzy model;transition probability;unknown discrete-time nonlinear Markov jump systems","","6","","34","IEEE","3 May 2022","","","IEEE","IEEE Journals"
"Enhancing Sparse Data Performance in E-Commerce Dynamic Pricing with Reinforcement Learning and Pre-Trained Learning","Y. Liu; K. L. Man; G. Li; T. Payne; Y. Yue","Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China; Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China; Faculty of Creative Arts, Technologies and Science, University of Bedfordshire, Luton, United Kingdom; Department of Computer Science, University of Liverpool, Liverpool, United Kingdom; Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China","2023 International Conference on Platform Technology and Service (PlatCon)","25 Sep 2023","2023","","","39","42","This paper introduces a reinforcement learning-based framework designed to tackle dynamic pricing challenges in e-commerce. Prior research has predominantly concentrated on algorithm selection to enhance performance in dense data scenarios. However, many of these models fail to robustly address sparse data structures, such as low-traffic products, leading to the ‘cold-start’ problem [4]. Through numerical analysis, our framework offers innovative insights derived from the design of the reward function and integrates product clustering with pre-trained learning to mitigate this issue. As a result of this optimization, the performance of predictive models on sparse data is expected to see substantial improvement.","2766-4198","979-8-3503-0599-9","10.1109/PlatCon60102.2023.10255211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255211","Dynamic Pricing;Reinforcement Learning;Clustering;K-means;Sarsa;Markov decision process;Price elasticity of demand","Industries;Numerical analysis;Transfer learning;Pricing;Reinforcement learning;Predictive models;Data structures","data structures;electronic commerce;learning (artificial intelligence);pricing;reinforcement learning","address sparse data structures;algorithm selection;cold-start problem;dense data scenarios;dynamic pricing challenges;e-commerce dynamic pricing;innovative insights;integrates product clustering;low-traffic products;numerical analysis;pre-trained;predictive models;reinforcement learning-based framework;reward function;sparse data performance","","","","9","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"Distributed learning of energy contracts negotiation strategies with collaborative reinforcement learning","T. Pinto; Z. Vale","GECAD Research Group, Polytechnic of Porto (ISEP/IPP), Porto, Portugal; Polytechnic of Porto (ISEP/IPP), Porto, Portugal","2019 16th International Conference on the European Energy Market (EEM)","28 Nov 2019","2019","","","1","6","The evolution of electricity markets towards local energy trading models, including peer-to-peer transactions, is bringing by multiple challenges for the involved players. In particular, small consumers, prosumers and generators, with no experience on participating in competitive energy markets, are not prepared for facing such an environment. This paper addresses this problem by proposing a decision support solution for small players negotiations in local transactions. The collaborative reinforcement learning concept is applied to combine different learning processes and reached an enhanced final decision for players actions in bilateral negotiations. The reinforcement learning process is based on the application of the Q-Learning algorithm; and the continuous combination of the different learning results applies and compares several collaborative learning algorithms, namely BEST-Q, Average (AVE)-Q; Particle Swarm Optimization (PSO)-Q, and Weighted Strategy Sharing (WSS)-Q and uses a model to aggregate these results. Results show that the collaborative learning process enables players' to correctly identify the negotiation strategy to apply in each moment, context and against each opponent.","2165-4093","978-1-7281-1257-2","10.1109/EEM.2019.8916342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8916342","Collaborative reinforcement learning;Electricity Markets;Energy Contracts;Negotiation Strategies;Q-Learning","Learning (artificial intelligence);Collaborative work;Contracts;Collaboration;Adaptation models;Particle swarm optimization;Electricity supply industry","decision support systems;groupware;learning (artificial intelligence);particle swarm optimisation;power engineering computing;power markets","distributed learning;energy contracts negotiation strategies;electricity markets;local energy trading models;peer-to-peer transactions;competitive energy markets;decision support solution;collaborative reinforcement learning concept;particle swarm optimization;weighted strategy sharing","","","","10","IEEE","28 Nov 2019","","","IEEE","IEEE Conferences"
"Multiagent-Reinforcement-Learning-Based Stable Path Tracking Control for a Bionic Robotic Fish With Reaction Wheel","C. Qiu; Z. Wu; J. Wang; M. Tan; J. Yu","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Turbulence and Complex Systems, Department of Advanced Manufacturing and Robotics, College of Engineering, Peking University, Beijing, China","IEEE Transactions on Industrial Electronics","15 Jun 2023","2023","70","12","12670","12679","The path tracking of the robotic fish is a hotspot with its high maneuverability and environmental friendliness. However, the periodic oscillation generated by bionic fish-like propulsion mode may lead to unstable control. To this end, this article proposes a novel framework involving a newly designed platform and multiagent reinforcement learning (MARL) method. First, a bionic robotic fish equipped with a reaction wheel is developed to enhance the stability. Second, an MARL-based control framework is proposed for the cooperative control of tail-beating and reaction wheel. Correspondingly, a hierarchical training method including initial training and iterative training is designed to deal with the control coupling and frequency difference between two agents. Finally, extensive simulations and experiments indicate that the developed robotic fish and the proposed MARL-based control framework can effectively improve the accuracy and stability of path tracking. Remarkably, headshaking is reduced about 40%. It provides a promising reference for the stability optimization and cooperative control of bionic swimming robots featuring oscillatory motions.","1557-9948","","10.1109/TIE.2023.3239937","National Natural Science Foundation of China(grant numbers:62233001,62203436,62022090,62273351,62203015); Joint Fund of Ministry of Education for Equipment Pre-Research(grant numbers:8091B022134); S&T Program of Hebei(grant numbers:F2020203037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10034492","Multiagent reinforcement learning (MARL);path tracking control;reaction wheel;robotic fish;underwater robot","Robots;Wheels;Mobile robots;Robot kinematics;Biological system modeling;Training;Stability analysis","learning (artificial intelligence);marine control;mobile robots;multi-agent systems;reinforcement learning;telecommunication computing","bionic fish-like propulsion mode;bionic robotic fish;bionic swimming robots;control coupling;cooperative control;developed robotic fish;hierarchical training method;high maneuverability;initial training;iterative training;MARL-based control framework;multiagent reinforcement learning method;multiagent-reinforcement-learning-based stable path tracking control;newly designed platform;reaction wheel;unstable control","","","","34","IEEE","31 Jan 2023","","","IEEE","IEEE Journals"
"Reinforcement Learning Based Algorithm for the Maximization of EV Charging Station Revenue","S. Dimitrov; R. Lguensat","Sofia University; Telecom Bretagne, Institut Mines-Telecom","2014 International Conference on Mathematics and Computers in Sciences and in Industry","23 Feb 2015","2014","","","235","239","This paper presents an online reinforcement learning based application which increases the revenue of one particular electric vehicles (EV) station, connected to a renewable source of energy. Moreover, the proposed application adapts to changes in the trends of the station's average number of customers and their types. Most of the parameters in the model are simulated stochastically and the algorithm used is the Q-learning algorithm. A computer simulation was implemented which demonstrates and confirms the utility of the model.","","978-1-4799-4324-1","10.1109/MCSI.2014.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7046189","Reinforcement learning;electric vehicles;charging stations;renewable energy;Q-learning","Vehicles;Learning (artificial intelligence);System-on-chip;Charging stations;Electricity;Renewable energy sources;Batteries","electric vehicles;learning (artificial intelligence);optimisation;power engineering computing","reinforcement learning based algorithm;EV charging station revenue maximization;electric vehicles station;EV station;renewable energy source;Q-learning algorithm","","10","","9","IEEE","23 Feb 2015","","","IEEE","IEEE Conferences"
"A Novel Model-Free Actor-Critic Reinforcement Learning Approach for Dynamic Target Tracking","A. Elhussein; M. S. Miah","Electrical & Computer Eng., Bradley University, Peoria, Illinois, USA; Electrical & Computer Eng., Bradley University, Peoria, Illinois, USA","2020 IEEE Midwest Industry Conference (MIC)","30 Sep 2020","2020","1","","1","6","Addressing the trajectory tracking problem of a mobile robot in tracking a dynamic target is still one of the challenging problems in the field of robotics. In this paper, we address the position tracking problem of a mobile robot where it is supposed to track the position of a mobile target whose dynamics is unknown a priori. This problem is even more challenging when the dynamics of the mobile robot is also assumed to be unknown, which is indeed a realistic assumption. Most of the trajectory tracking solutions proposed in the literature in the field of mobile robotics are either focused on algorithms that rely on mathematical models of the robots or driven by a overwhelming degree of computational complexity. This paper proposes a model-free actor-critic reinforcement learning strategy to determine appropriate actuator commands for the robot to track the position of the target. We emphasize that mathematical models of both mobile robot and the target are not required in the current approach. Moreover, Bellman's principle of optimality is utilized to minimize the energy required for the robot to track the target. The performance of the proposed actor-critic reinforcement learning approach is backed by a set of computer experiments with various complexities using a virtual circular-shaped mobile robot and a point target modeled by an integrator.","","978-1-7281-8386-2","10.1109/MIC50194.2020.9209618","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9209618","Mobile robots;policy iteration;reinforcement learning;target tracking;trajectory tracking","Target tracking;Mobile robots;Learning (artificial intelligence);Robot kinematics;Mathematical model;Trajectory","actuators;learning systems;mobile robots;target tracking;trajectory control","dynamic target tracking;position tracking problem;mobile target;trajectory tracking solutions;mathematical models;model-free actor-critic reinforcement learning strategy;circular-shaped mobile robot","","2","","20","IEEE","30 Sep 2020","","","IEEE","IEEE Conferences"
"Co-Evolution With Deep Reinforcement Learning for Energy-Aware Distributed Heterogeneous Flexible Job Shop Scheduling","R. Li; W. Gong; L. Wang; C. Lu; C. Dong","Department of Automation, Tsinghua University, Beijing, China; School of Computer Science, China University of Geosciences, Wuhan, China; Department of Automation, Tsinghua University, Beijing, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Mechanical and Automotive Engineering, Qingdao Hengxing University of Science and Technology, Qingdao, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2023","PP","99","1","11","Energy-aware distributed heterogeneous flexible job shop scheduling (DHFJS) problem is an extension of the traditional FJS, which is harder to solve. This work aims to minimize total energy consumption (TEC) and makespan for DHFJS. A deep  $Q$ -networks-based co-evolution algorithm (DQCE) is proposed to solve this NP-hard problem, which includes four parts: First, a new co-evolutionary framework is proposed, which allocates sufficient computation to global searching and executes local search surrounding elite solutions. Next, nine problem features-based local search operators are designed to accelerate convergence. Moreover, deep  $Q$ -networks are applied to learn and select the best operator for each solution. Furthermore, an efficient heuristic method is proposed to reduce TEC. Finally,  $20$  instances and a real-world case are employed to evaluate the effectiveness of DQCE. Experimental results indicate that DQCE outperforms the six state-of-the-art algorithms for DHFJS.","2168-2232","","10.1109/TSMC.2023.3305541","National Natural Science Foundation of China(grant numbers:62076225,62273193); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10242078","Co-evolution;deep  $Q$ -networks (DQNs);distributed heterogeneous flexible job shop scheduling (DHFJS) problem;energy-saving;multiobjective optimization","Production facilities;Job shop scheduling;Search problems;Manufacturing;Statistics;Sociology;Behavioral sciences","","","","","","","IEEE","6 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Intelligent Adjustment of Temperature Control Parameters Based on Deep Reinforcement Learning for Stretch Blow Molding Machine","P. -C. Hsieh; G. -L. Su; Y. -T. Lin; W. -B. Lin","Department of Mechatronics Engineering, National Changhua University of Education, Changhua, Taiwan; Department of Mechanical and Computer-Aided Engineering, Feng Chia University, Taichung, Taiwan; Ph.D. Program of Mechanical and Aeronautical Engineering, Feng Chia University, Taichung, Taiwan; Department of Mechatronics Engineering, National Changhua University of Education, Changhua, Taiwan","2022 25th International Conference on Mechatronics Technology (ICMT)","30 Dec 2022","2022","","","1","4","Stretch blow molding is the main technology used in the production of PET bottles. The stretch blow molding machine is usually composed of preform infeed system, transfer system, heating system, molding system, bottle discharge system, etc. Among them, the temperature control of the heating system is one of the key factors affecting the quality of PET bottle, especially in the face of the environmental temperature changes greatly between morning and evening in some seasons. The on-site operators of the stretch blow molding machine often need to adjust the infrared heating lamps in the heating system several times. The adjustment process is highly dependent on experience of the personnel. It has become a production challenge for the bottle manufacturers. Therefore, this paper takes the heating system of the stretch blow molding machine as the object and uses the deep reinforcement learning method to develop an intelligent adjustment technique of temperature control parameters. The proposed method can improve the problems such as the interference of environmental temperature changes and the aging variation of infrared heating lamps. The experimental results show that the proposed method can adjust the temperature control parameters automatically in the heating process to eliminate the effect of the environmental temperature change and to control the surface temperature of the preforms stably within ±3 °C of the target temperature (the requirement is within±5 °C).","","978-1-6654-6195-5","10.1109/ICMT56556.2022.9997684","National Science and Technology Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9997684","stretch blow molding;reinforcement learning;deep learning;intelligent temperature control","Deep learning;Temperature;Mechatronics;Preforms;Infrared heating;Process control;Reinforcement learning","blow moulding;bottles;design engineering;injection moulding;learning (artificial intelligence);moulding equipment;plastic products;production engineering computing;temperature control","bottle discharge system;environmental temperature change;heating system;infrared heating lamps;molding system;PET bottle;preform infeed system;stretch blow molding machine;temperature 3.0 degC;temperature 5.0 degC;temperature control parameters;transfer system","","","","12","IEEE","30 Dec 2022","","","IEEE","IEEE Conferences"
"Iterative Learning Procedure With Reinforcement for High-Accuracy Force Tracking in Robotized Tasks","L. Roveda; G. Pallucca; N. Pedrocchi; F. Braghin; L. M. Tosatti","IRAS, Istituto di Tecnologie Industriali ed Automazione Consiglio Nazionale delle Ricerche, Milano, Italy; IRAS, Istituto di Tecnologie Industriali ed Automazione Consiglio Nazionale delle Ricerche, Milano, Italy; IRAS, Istituto di Tecnologie Industriali ed Automazione Consiglio Nazionale delle Ricerche, Milano, Italy; Department of Mechanical Engineering, Politecnico di Milano, Milano, Italy; IRAS, Istituto di Tecnologie Industriali ed Automazione Consiglio Nazionale delle Ricerche, Milano, Italy","IEEE Transactions on Industrial Informatics","4 Apr 2018","2018","14","4","1753","1763","The paper focuses on industrial interaction robotics tasks, investigating a control approach involving multiples learning levels for training the manipulator to execute a repetitive (partially) changeable task, accurately controlling the interaction. Based on compliance control, the proposed approach consists of two main control levels: 1) iterative friction learning compensation controller with reinforcement and 2) iterative force-tracking learning controller with reinforcement. The learning algorithms rely on the iterative learning and reinforcement learning procedures to automatize the controllers parameters tuning. The proposed procedure has been applied to an automotive industrial assembly task. A standard industrial UR 10 Universal Robot has been used, equipped by a compliant pneumatic gripper and a force/torque sensor at the robot end-effector.","1941-0050","","10.1109/TII.2017.2748236","HORIZON-2020 research and innovation programme(grant numbers:637095); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8024058","Cooperative robot design;dual-driven actuation;empowering humans;industrial applications;shoulder joint actuation","Friction;Service robots;Force;Impedance;Manipulators;Robot sensing systems","automobile industry;compliance control;end effectors;force control;force sensors;grippers;iterative learning control;learning (artificial intelligence);pneumatic actuators;robotic assembly","industrial interaction robotics tasks;compliance control;controllers parameters tuning;automotive industrial assembly task;standard industrial UR 10 Universal Robot;robot end-effector;iterative force-tracking learning controller;iterative friction learning compensation controller;torque sensor;manipulator;force sensor;reinforcement learning;compliant pneumatic gripper","","62","","33","IEEE","1 Sep 2017","","","IEEE","IEEE Journals"
"A new approach to the design of reinforcement schemes for learning automata: stochastic estimator learning algorithms","A. V. Vasilakos; G. I. Papadimitriou; C. T. Paximadis","Department of Computer Engineering, University of Patras, Patras, Greece; Department of Computer Engineering, University of Patras, Patras, Greece; Department of Computer Engineering, University of Patras, Patras, Greece","Conference Proceedings 1991 IEEE International Conference on Systems, Man, and Cybernetics","6 Aug 2002","1991","","","1387","1392 vol.2","A new approach to the design of S-model ergodic reinforcement learning algorithms is introduced. The new scheme utilizes a stochastic estimator and can operate in nonstationary environments with high accuracy and high adaptation rate. The performance of the presented stochastic estimator learning automation (SELA) is superior over previous well-known S-model ergodic schemes. Furthermore it is proved that SELA is epsilon -optimal in every S-model random environment.<>","","0-7803-0233-8","10.1109/ICSMC.1991.169882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=169882","","Learning automata;Stochastic processes;Algorithm design and analysis;Design engineering;Feedback;Probability distribution","learning systems;stochastic automata","S-model ergodic reinforcement learning;stochastic estimator learning automation;epsilon -optimal","","5","","15","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"RL-CSL: A Combinatorial Optimization Method Using Reinforcement Learning and Contrastive Self-Supervised Learning","Z. Yuan; G. Li; Z. Wang; J. Sun; R. Cheng","School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, Shenzhen, China; School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, Shenzhen, China; School of System Design and Intelligent Manufacturing and also with the Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, Shaanxi Province, China; Guangdong Provincial Key Laboratory of Brain-inspired Intelligent Computation, Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China","IEEE Transactions on Emerging Topics in Computational Intelligence","21 Jul 2023","2023","7","4","1010","1024","Reinforcement learning-based methods have shown great potential in solving combinatorial optimization problems. However, the related research has not been mature in terms of both models and training methods. This paper proposes a method based on reinforcement learning and contrastive self-supervised learning. To be specific, the proposed method uses an attention model to learn a policy for generating solutions and combines a contrastive self-supervised learning model to learn the attention encoder in the way of node-by-node. Correspondingly, a two-phase learning method, including node-wise learning and solution-wise learning, is adopted to train the attention model and the contrastive self-supervised model jointly and collaboratively. The performance of the proposed method has been verified by numerical experiments on various combinatorial optimization problems.","2471-285X","","10.1109/TETCI.2021.3139802","National Natural Science Foundation of China(grant numbers:11991023,62076197,62106096); Guangdong Provincial Key Laboratory(grant numbers:2020B121201001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9690950","Combinatorial optimization;reinforcement learning;attention model;self-supervised learning;contrastive learning","Learning systems;Optimization;Reinforcement learning;Computational modeling;Costs;Vehicle routing;Computational intelligence","combinatorial mathematics;optimisation;reinforcement learning;supervised learning","attention model;combinatorial optimization method;contrastive self-supervised learning;node-by-node;node-wise learning;reinforcement learning-based methods;RL-CSL;solution-wise learning;two-phase learning method","","4","","59","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"Automatic Modeling of High-Temperature Superconducting Cable Using Reinforcement Learning","S. S. Bang; G. -Y. Kwon","Department of Electronics Engineering, Tech University of Korea, Siheung, South Korea; Department of Smart Safety Engineering, Dongguk University, Gyeongju, South Korea","IEEE Transactions on Applied Superconductivity","16 Feb 2023","2023","33","5","1","5","This paper proposes a technique of automatic modeling for high-temperature superconducting (HTS) cables. Reinforcement learning (RL), which is a representative methodology for automation and intelligence, is applied for the automation of the proposed modeling. To reflect the high-frequency characteristics of the HTS cables in the proposed modeling, reflectometry-based cable modeling is used. In addition, for agent training, an environment that combines simulation and experiment results is proposed, and detailed techniques for the process of the proposed RL model are introduced. The proposed technique is demonstrated by experiment using an actual HTS cable under 77 K and 300 K conditions. It is expected that the proposed technique will allow anyone without the related knowledge to perform the modeling of the HTS cables.","1558-2515","","10.1109/TASC.2023.3240379","Ministry of Science and ICT, South Korea; ICT Challenge and Advanced Network of HRD Program(grant numbers:IITP-2023-RS-2022-00156326); Institute of Information Communications Technology Planning Evaluation; National Research Foundation of Korea funded by the Korea Government, MSIT(grant numbers:NRF-2022R1C1C1003894); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10027198","Automation;frequency characteristic;high temperature superconducting (HTS) cable;modeling;reflectometry;reinforcement learning","Superconducting cables;High-temperature superconductors;Power cables;Training;Temperature measurement;Data models;Communication cables","high-temperature superconductors;power engineering computing;reflectometry;reinforcement learning;superconducting cables","high-frequency characteristics;high-temperature superconducting cable;HTS cables;reflectometry-based cable modeling;reinforcement learning;RL","","","","23","IEEE","27 Jan 2023","","","IEEE","IEEE Journals"
"An Approach Based on Quantum Reinforcement Learning for Navigation Problems","N. F. Bar; H. Yetis; M. Karakose","Department of Computer Engineering, Engineering Faculty, Firat University, Elazig, Turkey; Department of Computer Engineering, Engineering Faculty, Firat University, Elazig, Turkey; Department of Computer Engineering, Engineering Faculty, Firat University, Elazig, Turkey","2022 International Conference on Data Analytics for Business and Industry (ICDABI)","14 Feb 2023","2022","","","593","597","The power of classical computers is still insufficient for deep reinforcement learning (DRL) problems which has large state space. Thanks to entanglement and superposition, quantum computers have a high computational power. The concept of using this high computing power to solve problems that would be difficult for classical computers is fairly common. In this study, a hybrid approach is proposed to take advantage of the benefits of quantum computers. Deep Q-Network (DQN) algorithm for DRL, optimization operations, and storage operations are performed on the classical computer side of this hybrid approach. In the quantum side, a variational quantum circuit (VQC) is proposed. The proposed method is applied to a navigation problem. The proposed approach is evaluated in terms of target success rate, collision (going out) rate. The proposed approach is compared to DRL solutions in the literature for the navigation problem with classical computers. According to the number of parameters used, the proposed approach appears to be successful. As a result, the performance of the proposed approach has been validated.","","978-1-6654-9058-0","10.1109/ICDABI56818.2022.10041570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10041570","quantum computing;quantum machine learning;deep reinforcement learning;variational quantum circuit","Computers;Training;Quantum computing;Navigation;Quantum entanglement;Simulation;Qubit","deep learning (artificial intelligence);learning (artificial intelligence);quantum computing;reinforcement learning;telecommunication computing","classical computer side;classical computers;deep Q-Network algorithm;deep reinforcement learning problems;DRL;high computational power;high computing power;hybrid approach;navigation problem;quantum computers;quantum reinforcement learning;variational quantum circuit","","2","","19","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Virtual Commissioning Simulation as Reinforcement Learning Environment for Robot Cable Handling","F. Jaensch; A. Verl","ISW, University of Stuttgart, Stuttgart, Germany; ISW, University of Stuttgart, Stuttgart, Germany","2020 Third International Conference on Artificial Intelligence for Industries (AI4I)","13 Nov 2020","2020","","","27","31","The paper introduces an approach to generate control strategies for cable handling tasks automatically, using Reinforcement Learning. Cable handling tasks require human like sensomotoric and cognitive abilities. Humans develop an understanding of the cable behaviour and are able to derive control strategies, how to handle the cable. Transferring these strategies to an industrial robot control system with the usual programming interface is very difficult. The introduced approach addresses this problem by giving the robot control the possibility to learn the cable handling strategies by itself. Therefore an Virtual Commissioning simulation environment was adapted to model Reinforcement Learning problems using the OpenAI Gym and Functional Mock-Up Interface standard.","","978-1-7281-8701-3","10.1109/AI4I49448.2020.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9253103","reinforcement learning;virtual commissioning simulation;soft tissue robotics","Training;Adaptation models;Reinforcement learning;Tools;Robot sensing systems;Task analysis;Standards","cables (mechanical);control engineering computing;learning (artificial intelligence);mobile robots","reinforcement learning environment;robot cable handling;control strategies;sensomotoric abilities;cognitive abilities;cable behaviour;virtual commissioning simulation environment","","1","","26","IEEE","13 Nov 2020","","","IEEE","IEEE Conferences"
"A Robust Offline Reinforcement Learning Algorithm Based on Behavior Regularization Methods","Y. Zhang; T. Gao; Q. Mi","Software College, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China","2022 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)","16 Sep 2022","2022","","","150","154","Offline deep reinforcement learning algorithms are still in developing. Some existing algorithms have shown that it is feasible to learn directly without using environmental interaction under the condition of sufficient datasets. In this paper, we combine an offline reinforcement learning method through behavior regularization with a robust offline reinforcement learning algorithm. Moreover, the algorithm is verified and analyzed with a high-quality but limited dataset. The experimental results show that it is feasible to combine the behavior regularization method with the robust offline reinforcement learning algorithm, to gain better performance under the condition of limited data compared with the baseline algorithms.","","978-1-6654-5126-0","10.1109/IAICT55358.2022.9887435","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887435","offline deep reinforcement learning;limited data;behavior regularization","Reinforcement learning;Generators;Communications technology;Behavioral sciences;Fourth Industrial Revolution;Artificial intelligence;Standards","deep learning (artificial intelligence);reinforcement learning","behavior regularization method;robust offline reinforcement learning algorithm;offline deep reinforcement learning algorithms","","","","15","IEEE","16 Sep 2022","","","IEEE","IEEE Conferences"
"An Obstacle Avoidance Method Using Asynchronous Policy-based Deep Reinforcement Learning with Discrete Action","Y. Wang; F. Yao; L. Cui; S. Chai","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China","2022 34th Chinese Control and Decision Conference (CCDC)","14 Feb 2023","2022","","","6235","6241","With the increasing application of mobile robots in manufacturing, service, and military fields, the demand of intelligent autonomous decision is also growing. In this paper, the state-of-the-art policy-based deep reinforcement learning (DRL) algorithm is applied to the mobile robot obstacle avoidance task. To solve the strong coupling in the existing DRL-based training process of mobile robot decision, an asynchronous decoupling architecture is proposed in this paper, which greatly improves the scalability and sample generation efficiency of the DRL algorithm. At the same time, based on the asynchronous decoupling architecture and Soft Actor-Critic (SAC) algorithm, we design an obstacle avoidance method named asynchronous dueling-based discrete action SAC (ADDSAC). Experiments show both action discretization and dueling network are simple and powerful techniques to improve obstacle avoidance performance. Finally, the reasons are also analyzed from different perspectives.","1948-9447","978-1-6654-7896-0","10.1109/CCDC55256.2022.10033562","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10033562","Mobile robot;obstacle avoidance;deep reinforcement learning;Soft Actor-Critic","Training;Deep learning;Couplings;Scalability;Reinforcement learning;Aerospace electronics;Manufacturing","collision avoidance;deep learning (artificial intelligence);mobile robots;reinforcement learning;telecommunication computing","action discretization;asynchronous decoupling architecture;asynchronous dueling-based discrete action SAC;asynchronous policy-based deep reinforcement;DRL algorithm;existing DRL-based training process;increasing application;intelligent autonomous decision;military fields;mobile robot decision;mobile robot obstacle avoidance task;mobile robots;obstacle avoidance method;obstacle avoidance performance;sample generation efficiency;Soft Actor-Critic algorithm;state-of-the-art policy-based deep reinforcement learning algorithm","","","","20","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Trading Agent for the Indian Stock Market scenario using Actor-Critic based Reinforcement Learning","M. Vishal; Y. Satija; B. S. Babu","Dept. of Computer Science and Engineering, R.V. College Of Engineering, Bengaluru, India; Dept. of Computer Science and Engineering, R.V. College Of Engineering, Bengaluru, India; Dept. of Computer Science and Engineering, R.V. College Of Engineering, Bengaluru, India","2021 IEEE International Conference on Computation System and Information Technology for Sustainable Solutions (CSITSS)","25 Jan 2022","2021","","","1","5","Financial trading is about buying, holding, and selling securities in the hope of making a profit. Automation is a trending area in the engineering domain that can help maximize the outcome of interest. Machine learning approaches like reinforcement learning have a great potential to solve automation in certain business domains, thereby maximizing the work outcome. Reinforcement learning has the sole objective of attaining maximum profit or reward in the given environment where the agent acts. Hence, the proposed work deals with leveraging the state of the art Actor-Critic Reinforcement learning algorithms like Proximal Policy optimization (PPO), Deep Deterministic Policy Gradient (DDPG), Advantage Actor Critic (A2C), and Twin Delayed DDPG (TD3) in developing a trading agent which can make decisions based on the trading environment whether the stock has to be bought, sold or held at the given instant in the Indian Stock Market scenario.","","978-1-6654-0610-9","10.1109/CSITSS54238.2021.9683467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9683467","Deep Reinforcement Learning (DRL);Actor-Critic approach;Q Learning;Proximal Policy Optimization (PPO);Deep Deterministic Policy Gradient (DDPG);Advantage Actor Critic (A2C);Twin Delayed DDPG (TD3);Moving Average Convergence Divergence (MACD)","Measurement;Automation;Statistical analysis;Reinforcement learning;Mathematical models;Stakeholders;Security","financial data processing;gradient methods;learning (artificial intelligence);optimisation;stock markets","Deep Deterministic Policy Gradient;Advantage Actor Critic;trading agent;trading environment;Indian Stock Market scenario;Actor-Critic based Reinforcement learning;financial trading;engineering domain;machine learning;business domains;work outcome;Proximal Policy optimization;Twin Delayed DDPG","","3","","23","IEEE","25 Jan 2022","","","IEEE","IEEE Conferences"
"Integral Reinforcement Learning for Linear Continuous-Time Zero-Sum Games With Completely Unknown Dynamics","H. Li; D. Liu; D. Wang","The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Beijing, China","IEEE Transactions on Automation Science and Engineering","30 Jun 2014","2014","11","3","706","714","In this paper, we develop an integral reinforcement learning algorithm based on policy iteration to learn online the Nash equilibrium solution for a two-player zero-sum differential game with completely unknown linear continuous-time dynamics. This algorithm is a fully model-free method solving the game algebraic Riccati equation forward in time. The developed algorithm updates value function, control and disturbance policies simultaneously. The convergence of the algorithm is demonstrated to be equivalent to Newton's method. To implement this algorithm, one critic network and two action networks are used to approximate the game value function, control and disturbance policies, respectively, and the least squares method is used to estimate the unknown parameters. The effectiveness of the developed scheme is demonstrated in the simulation by designing an H∞ state feedback controller for a power system.","1558-3783","","10.1109/TASE.2014.2300532","National Natural Science Foundation of China(grant numbers:61034002,61233001,61273140,61304086); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6730707","Adaptive critic designs;adaptive dynamic programming;approximate dynamic programming;reinforcement learning;policy iteration;zero-sum games","Heuristic algorithms;Games;Game theory;Algorithm design and analysis;Power system dynamics;Approximation algorithms;Mathematical model","game theory;learning (artificial intelligence);least squares approximations;linear systems;network theory (graphs);parameter estimation;Riccati equations","controller design;H∞ state feedback controller;parameter estimation;least squares method;action networks;critic network;Newton method;disturbance policy;control policy;value function;algebraic Riccati equation;model-free method;two-player zero-sum differential game;Nash equilibrium solution;policy iteration;linear continuous-time zero-sum games;integral reinforcement learning algorithm","","142","","49","IEEE","31 Jan 2014","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning in a Dynamic Environment: A Case Study in the Telecommunication Industry","H. Zhang; J. Li; Z. Qi; X. Lin; A. Aronsson; J. Bosch; H. H. Olsson","Chalmers University of Technology, Gothenburg, Sweden; Ericsson Research, Ericsson; Ericsson Research, Ericsson; Ericsson Research, Ericsson; Ericsson Research, Ericsson; Chalmers University of Technology, Gothenburg, Sweden; Malmö University, Malmö, Sweden","2022 48th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)","16 Jan 2023","2022","","","68","75","Reinforcement learning, particularly deep reinforcement learning, has made remarkable progress in recent years and is now used not only in simulators and games but is also making its way into embedded systems as another software-intensive domain. However, when implemented in a real-world context, reinforcement learning is typically shown to be fragile and incapable of adapting to dynamic environments. In this paper, we provide a novel dynamic reinforcement learning algorithm for adapting to complex industrial situations. We apply and validate our approach using a telecommunications use case. The proposed algorithm can dynamically adjust the position and antenna tilt of a drone-based base station to maintain reliable wireless connectivity for mission-critical users. When compared to traditional reinforcement learning approaches, the dynamic reinforcement learning algorithm improves the overall service performance of a drone-based base station by roughly 20%. Our results demonstrate that the algorithm can quickly evolve and continuously adapt to the complex dynamic industrial environment.","","978-1-6654-6152-8","10.1109/SEAA56994.2022.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011518","Reinforcement Learning;Machine Learning;Software Engineering;Emergency Communication Network","Deep learning;Wireless communication;Base stations;Heuristic algorithms;Software algorithms;Mission critical systems;Reinforcement learning","antennas;deep learning (artificial intelligence);embedded systems;mobile radio;reinforcement learning;remotely operated vehicles;telecommunication industry","antenna tilt;complex dynamic industrial environment;deep reinforcement learning;drone-based base station;dynamic environment;dynamic reinforcement learning algorithm;embedded systems;reinforcement learning approaches;telecommunication industry;telecommunications use case","","1","","16","IEEE","16 Jan 2023","","","IEEE","IEEE Conferences"
"Learning to Play Football from Sports Domain Perspective: A Knowledge-embedded Deep Reinforcement Learning Framework","B. Liu; Z. Pu; T. Zhang; H. Wang; J. Yi; J. Mi","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; JD.COM, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, Beijing Technology and Business University, Beijing, China","IEEE Transactions on Games","","2022","PP","99","1","10","Applying deep reinforcement learning to football games has recently received extensive attention. However, this remains challenging due to the excessively high complexity of the football environment such as high-dynamical game states, sparse rewards, and multiple roles with different capabilities. Existing works aim to address these problems without considering abundant domain knowledge of football. In this paper, a football knowledge-embedded learning framework is proposed. Specifically, the pitch control concept is innovatively introduced to design a knowledge-embedded state representation. As a result, a novel pitch control model is designed that quantitatively provides space influence values of a single player, the whole team, and the ball. Different from existing models, this model additionally considers each player's various capabilities, including flexibility, explosive force, and stamina. Furthermore, the deformable convolution network is adopted for state representation extracting, which is used to process the geometric transformation of the players' positions and spatial influence values generated by the pitch control model. Then, based on this comprehensive state representation, a PPO-based reinforcement learning scheme is adopted to generate the final policy. Finally, extensive simulations, including learning against a fixed opponent and learning from self-play, clearly show the effectiveness and adaptability of our proposed framework.","2475-1510","","10.1109/TG.2022.3207068","National Key Research, Development Program of China(grant numbers:2020AAA0103404); National Natural Science Foundation of China(grant numbers:62073323); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDA27030204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893354","Reinforcement learning;football analysis;pitch control;deformable convolution","Sports;Games;Convolutional neural networks;Strain;Training;Reinforcement learning;Feature extraction","","","","","","","IEEE","15 Sep 2022","","","IEEE","IEEE Early Access Articles"
"Safely Learn to Fly Aircraft From Human: An Offline-Online Reinforcement Learning Strategy and Its Application to Aircraft Stall Recovery","H. Jiang; H. Xiong; W. Zeng; Y. Ou","School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, Shenzhen, China","IEEE Transactions on Aerospace and Electronic Systems","","2023","PP","99","1","14","Researchers have made many attempts to apply Reinforcement Learning (RL) to learn to fly aircraft in recent years. However, existing RL strategies usually are not safe (e.g., can lead to crash) in the initial stage of the training of an RL-based policy. For increasingly complex piloting tasks whose representative models are hard to establish, it is not safe to apply the existing RL strategies to learn an RL-based policy by interacting with an aircraft. To enhance the safety and feasibility of applying an RL-based policy to an aircraft, this study develops an Offline-Online RL strategy. The Offline-Online RL strategy learns an effective initialization for an RL-based flight control policy from human pilots without interacting with an aircraft through offline RL. Then, the Offline-Online RL strategy can further improve the RL-based flight control policy safely without leading to crash by interacting with the aircraft according to regular online RL, requiring no or very little intervention performed by a human pilot. To demonstrate and evaluate the Offline-Online RL strategy, the strategy is utilized to address the stall recovery problem of aircraft based on a flight simulator.","1557-9603","","10.1109/TAES.2023.3299913","Research Foundation for Advanced Talents, Harbin Institute of Technology Shenzhen(grant numbers:CA11409019); Shenzhen Science and Technology Program(grant numbers:RCBS20210609103819024); Natural Science Foundation of Guangdong Province China(grant numbers:2023A1515011010); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2021A1515110021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10197466","Aircraft;human pilot;reinforcement learning;safety;stall recovery","Aircraft;Aerospace electronics;Aerospace control;Behavioral sciences;Atmospheric modeling;Task analysis;Training","","","","","","","IEEE","31 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Learning Skills to Navigate without a Master: A Sequential Multi-Policy Reinforcement Learning Algorithm","A. Dukkipati; R. Banerjee; R. S. Ayyagari; D. P. Udaybhai","Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India","2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","26 Dec 2022","2022","","","2483","2489","Solving complex problems using reinforcement learning necessitates breaking down the problem into manageable tasks, and learning policies to solve these tasks. These policies, in turn, have to be controlled by a master policy that takes high-level decisions. Hence learning policies involves hierarchical decision structures. However, training such methods in practice may lead to poor generalization, with either sub-policies executing actions for too few time steps or devolving into a single policy altogether. In our work, we introduce an alternative approach to learn such skills sequentially without using an overarching hierarchical policy. We propose this method in the context of environments where a major component of the objective of a learning agent is to prolong the episode for as long as possible. We refer to our proposed method as Sequential Soft Option Critic. We demonstrate the utility of our approach on navigation and goal-based tasks in a flexible simulated 3D navigation environment that we have developed. We also show that our method outperforms prior methods such as Soft Actor-Critic and Soft Option Critic on various environments, including the Atari River Raid environment and the Gym-Duckietown self-driving car simulator.","2153-0866","978-1-6654-7927-1","10.1109/IROS47612.2022.9981607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981607","","Training;Three-dimensional displays;Navigation;Heuristic algorithms;Reinforcement learning;Rivers;Autonomous automobiles","computer simulation;reinforcement learning","Atari river raid environment;complex problems;flexible simulated 3D navigation environment;goal-based tasks;Gym-Duckietown self-driving car simulator;hierarchical decision structures;high-level decisions;learning agent;manageable tasks;master policy;overarching hierarchical policy;sequential multipolicy reinforcement learning algorithm;sequential soft option critic","","","","32","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Learn to efficiently exploit cost maps by combining RRT* with Reinforcement Learning","R. Franceschini; M. Fumagalli; J. C. Becerra","Eurecat, Centre Tecnològic de Catalunya, Robotics and Automation Unit, Cerdanyola del Valles, Barcelona, Spain; Department of Electrical Engineering, Automation and Control group, Danish Technical University, Elektrovej, Lyngby, Denmark; Eurecat, Centre Tecnològic de Catalunya, Robotics and Automation Unit, Cerdanyola del Valles, Barcelona, Spain","2022 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","24 Jan 2023","2022","","","251","256","Safe autonomous navigation of robots in complex and cluttered environments is a crucial task and is still an open challenge even in 2D environments. Being able to efficiently minimize multiple constraints such as safety or battery drain requires the ability to understand and leverage information from different cost maps. Rapid-exploring random trees (RRT) methods are often used in current path planning methods, thanks to their efficiency in finding a quick path to the goal. However, these approaches suffer from a slow convergence towards an optimal solution, especially when the planner's goal must consider other aspects like safety or battery consumption besides simply achieving the goal. Therefore, it is proposed a sample-efficient and cost-aware sampling RRT* method that can overcome previous methods by exploiting the information gathered from map analysis. In particular, the use of a Reinforcement Learning agent is leveraged to guide the RRT* sampling toward an almost optimal solution. The performance of the proposed method is demonstrated against different RRT* implementations in multiple synthetic environments.","2475-8426","978-1-6654-5680-7","10.1109/SSRR56537.2022.10018735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018735","","Costs;Three-dimensional displays;Reinforcement learning;Robot sensing systems;Path planning;Safety;Batteries","collision avoidance;learning (artificial intelligence);mobile robots;path planning;trees (mathematics)","cluttered environments;complex environments;cost-aware sampling RRT* method;current path planning methods;different cost maps;different RRT* implementations;leverage information;map analysis;multiple synthetic environments;optimal solution;planner;quick path;rapid-exploring random trees methods;Reinforcement Learning agent;RRT* sampling;safety;sample-efficient","","","","28","IEEE","24 Jan 2023","","","IEEE","IEEE Conferences"
"Soft Contrastive Learning With Q-Irrelevance Abstraction for Reinforcement Learning","M. Liu; L. Li; S. Hao; Y. Zhu; D. Zhao","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Software, Beihang University, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Cognitive and Developmental Systems","7 Sep 2023","2023","15","3","1463","1473","The difference between training and testing environments is a huge challenge to generalizing reinforcement learning (RL) algorithms. We propose a soft contrastive learning with a coarser approximate  $Q$ -irrelevance abstraction for RL (SCQRL) to increase RL generalization. Specifically, we specify the coarser approximate  $Q$ -irrelevance abstraction as the feature of the state with a theoretical analysis for better generalization ability. We construct a positive and negative sample selection mechanism based on the  $Q$  value for contrastive learning to achieve efficient representation learning. Considering the selection error of positive and negative samples, we design soft contrastive learning and combine it with RL in the form of an auxiliary task to propose SCQRL. The generalization experiments on several Procgen environments demonstrate that SCQRL outperforms the excellent generalized RL algorithm.","2379-8939","","10.1109/TCDS.2022.3218940","National Key Research and Development Program of China(grant numbers:2018AAA0101005); National Natural Science Foundation of China(grant numbers:62136008); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDA27030400); Youth Innovation Promotion Association CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9939161","Contrastive learning;generalization;reinforcement learning (RL);representation learning","Task analysis;Reinforcement learning;Training;Representation learning;Testing;Feature extraction;Estimation","","","","","","38","IEEE","4 Nov 2022","","","IEEE","IEEE Journals"
"LILAC: Learning a Leader for Cooperative Reinforcement Learning","Y. Fu; J. Chai; Y. Zhu; D. Zhao","The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China","2022 IEEE Conference on Games (CoG)","20 Sep 2022","2022","","","49","55","In cooperative multi-agent reinforcement learning,role-based learning promises to reach satisfactory policy learning through the decomposition of complicated tasks using roles. Different roles are responsible for different aspects of the task. However, how this group of roles can be quickly identified is not clear. To address this problem, we propose a novel framework, LearnIng a LeAder for Cooperative reinforcement learning (LILAC), which introduces a leader to integrate information to assign roles. Leaders take a broad view of the whole task and feed the integrated information into a Gaussian mixture model to sample role embedding distribution. It enables LILAC to assign appropriate roles to different agents and improves cooperative performance. In order to evaluate the cooperation of multiple agents, a mixing network, inputted by individual local utility networks, is constructed to estimate the global action value. Two loss functions, temporal difference loss and mean divergence loss, are adopted by LILAC to learn network parameters and to encourage diversity of policies for different roles. By virtue of the leader module, LILAC outperforms the StarCraft II micromanagement benchmark in our experiments, especially on challenging tasks.","2325-4289","978-1-6654-5989-1","10.1109/CoG51982.2022.9893619","Research and Development; National Natural Science Foundation of China; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893619","multi-agent reinforcement learning;role-based method;deep learning;game AI","Training;Dimensionality reduction;Collaboration;Reinforcement learning;Games;Color;Benchmark testing","Gaussian processes;mixture models;multi-agent systems;reinforcement learning","LILAC;policy learning;multiple agents;temporal difference loss;leader module;cooperative multiagent reinforcement learning;role-based learning;learning a leader for cooperative reinforcement learning;Gaussian mixture model;role embedding distribution;mixing network;local utility networks","","","","30","IEEE","20 Sep 2022","","","IEEE","IEEE Conferences"
"Deep-Reinforcement-Learning-Based Semantic Navigation of Mobile Robots in Dynamic Environments","L. Kästner; C. Marx; J. Lambrecht","Chair Industry Grade Networks and Clouds Department, Technical University of Berlin, Berlin, Germany; Chair Industry Grade Networks and Clouds Department, Technical University of Berlin, Berlin, Germany; Chair Industry Grade Networks and Clouds Department, Technical University of Berlin, Berlin, Germany","2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)","8 Oct 2020","2020","","","1110","1115","Mobile robots have gained increased importance within industrial tasks such as commissioning, delivery or operation in hazardous environments. The ability to autonomously navigate safely especially within dynamic environments, is paramount in industrial mobile robotics. Current navigation methods depend on preexisting static maps and are error-prone in dynamic environments. Furthermore, for safety reasons, they often rely on hand-crafted safety guidelines, which makes the system less flexible and slow. Visual based navigation and high level semantics bear the potential to enhance the safety of path planing by creating links the agent can reason about for a more flexible navigation. On this account, we propose a reinforcement learning based local navigation system which learns navigation behavior based solely on visual observations to cope with highly dynamic environments. Therefore, we develop a simple yet efficient simulator - ARENA2D- which is able to generate highly randomized training environments and provide semantic information to train our agent. We demonstrate enhanced results in terms of safety and robustness over a traditional baseline approach based on the dynamic window approach.","2161-8089","978-1-7281-6904-0","10.1109/CASE48305.2020.9216798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9216798","","Navigation;Robots;Semantics;Training;Safety;Visualization;Neural networks","control engineering computing;learning (artificial intelligence);mobile robots;neural nets;path planning;robot vision","local navigation system;highly dynamic environments;highly randomized training environments;dynamic window approach;deep-reinforcement-learning-based semantic navigation;hazardous environments;industrial mobile robotics;hand-crafted safety guidelines;visual based navigation;high level semantics;flexible navigation","","12","","22","IEEE","8 Oct 2020","","","IEEE","IEEE Conferences"
"Agent-Based Simulation of Power Markets under Uniform and Pay-as-Bid Pricing Rules using Reinforcement Learning","A. G. Bakirtzis; A. C. Tellidou","Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece","2006 IEEE PES Power Systems Conference and Exposition","5 Feb 2007","2006","","","1168","1173","In this paper agent-based simulation is employed to study the power market operation under two alternative pricing systems: uniform and discriminatory (pay-as-bid). Power suppliers are modeled as adaptive agents capable of learning through the interaction with their environment, following a reinforcement learning algorithm. The SA-Q-learning algorithm, a slightly changed version of the popular Q-Learning, is used in this paper; it proposes a solution to the difficult problem of the balance between exploration and exploitation and it has been chosen for its quick convergence. A test system with five supplier-agents is used to study the suppliers' behavior under the uniform and the pay-as-bid pricing systems","","1-4244-0177-1","10.1109/PSCE.2006.296473","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4075912","","Power markets;Pricing;Learning;Power system modeling;Electricity supply industry;Power supplies;System testing;Electricity supply industry deregulation;Costs;Predictive models","learning (artificial intelligence);power engineering computing;power markets;pricing;simulated annealing;software agents","agent-based simulation;power market operation;uniform pricing rules;pay-as-bid pricing rules;reinforcement learning algorithm;discriminatory pricing;simulated annealing-Q-learning algorithm;convergence;test system;supplier-agents;suppliers behavior;electricity spot markets;multiagent modeling","","11","","12","IEEE","5 Feb 2007","","","IEEE","IEEE Conferences"
"Generalized Policy Iteration-based Reinforcement Learning Algorithm for Optimal Control of Unknown Discrete-time Systems","M. Lin; B. Zhao; D. Liu; X. Liu; F. Luo","School of Automation, Guangdong University of Technology, Guangzhou, China; School of Systems Science, Beijing Normal University, Beijing, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China","2021 33rd Chinese Control and Decision Conference (CCDC)","30 Nov 2021","2021","","","3650","3655","This paper presents a novel generalized policy iteration-based reinforcement learning (RL) algorithm to deal with infinite-horizon optimal control problems of nonlinear discrete-time systems with completely unknown dynamics. In the present iterative algorithm, two iteration procedures are utilized to obtain the iterative Q-function and the iterative control policy. Furthermore, the iterative Q-function is obtained by the temporal difference learning and the policy gradient method is utilized to directly optimize the iterative control policy. Then, the convergence and optimality analysis of the generalized policy iteration-based RL algorithm are provided. To implement this algorithm, two neural networks, including a critic network and an action network, are used to approximate the iterative Q-function and the iterative control policy. Finally, a numerical simulation example is provided to illustrate the effectiveness of the proposed control method.","1948-9447","978-1-6654-4089-9","10.1109/CCDC52312.2021.9601467","National Natural Science Foundation of China(grant numbers:62073085,61973330,61773075,61533017); Beijing Natural Science Foundation(grant numbers:4212038); Guangdong Introducing Innovative and Enterpreneurial Teams of “The Pearl River Talent Recruitment Program”(grant numbers:2019ZT08X340); State Key Laboratory of Synthetical Automation for Process Industries(grant numbers:2019-KF-23-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9601467","Adaptive dynamic programming;Reinforcement learning;Generalized policy iteration;Neural networks;Optimal control;Unknown Discrete-time Systems","Discrete-time systems;Gradient methods;Heuristic algorithms;Simulation;Neural networks;Optimal control;Reinforcement learning","discrete time systems;dynamic programming;gradient methods;iterative methods;learning (artificial intelligence);neurocontrollers;nonlinear control systems;optimal control","novel generalized policy iteration-based reinforcement learning algorithm;infinite-horizon optimal control problems;iterative algorithm;iteration procedures;iterative Q-function;iterative control policy;policy gradient method;unknown discrete-time systems","","","","28","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Elastic O-RAN Slicing for Industrial Monitoring and Control: A Distributed Matching Game and Deep Reinforcement Learning Approach","S. F. Abedin; A. Mahmood; N. H. Tran; Z. Han; M. Gidlund","Department of Information Systems and Technology, Mid Sweden University, Sundsvall, Sweden; Department of Information Systems and Technology, Mid Sweden University, Sundsvall, Sweden; School of Computer Science, The University of Sydney, Sydney, NSW, Australia; Computer Science Department, University of Houston, Houston, TX, USA; Department of Information Systems and Technology, Mid Sweden University, Sundsvall, Sweden","IEEE Transactions on Vehicular Technology","17 Oct 2022","2022","71","10","10808","10822","In this work, we design an elastic open radio access network (O-RAN) slicing for the Industrial Internet of things (IIoT). Due to the rapid spread of IoT in the industrial use-cases such as safety and mobile robot communications, the IIoT landscape has been shifted from static manufacturing processes towards dynamic manufacturing workflows (e.g., Modular Production System). But unlike IoT, IIoT poses additional challenges such as severe communication environment, network-slice resource demand variations, and on-time information update from the IIoT devices during industrial production. First, we formulate the O-RAN slicing problem for on-time industrial monitoring and control where the objective is to minimize the cost of fresh information updates (i.e., age of information (AoI)) from the IIoT devices (i.e., sensors) with the device energy consumption and O-RAN slice isolation constraints. Second, we propose the intelligent O-RAN framework based on game theory and machine learning to mitigate the problem’s complexity. We propose a two-sided distributed matching game in the O-RAN control layer that captures the IIoT channel characteristics and the IIoT service priorities to create IIoT device and small cell base station (SBS) preference lists. We then employ an actor-critic model with a deep deterministic policy gradient (DDPG) in the O-RAN service management layer to solve the resource allocation problem for optimizing the network slice configuration policy under time-varying slicing demand. Furthermore, the proposed matching game within the actor-critic model training helps to enforce the long-term policy-based guidance for resource allocation that reflects the trends of all IIoT devices and SBSs satisfactions with the assignment. Finally, the simulation results show that the proposed solution enhances the performance gain for the IIoT services by serving an average of 50% and 43.64% more IIoT devices than the baseline approaches.","1939-9359","","10.1109/TVT.2022.3188217","KKS Research Profile NIIT; MERIT; NSF(grant numbers:CNS-2107216,CNS-2128368); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9813589","Industrial IoT;open RAN slicing;age of information;game theory;deep reinforcement learning","Industrial Internet of Things;Network slicing;Quality of service;Resource management;Monitoring;Energy efficiency;5G mobile communication","control engineering computing;deep learning (artificial intelligence);game theory;gradient methods;Internet of Things;mobile robots;radio access networks;reinforcement learning;resource allocation;telecommunication control","deep reinforcement learning approach;device energy consumption;distributed matching game;dynamic manufacturing workflows;elastic O-RAN slicing;fresh information updates;game theory;IIoT channel characteristics;IIoT devices;IIoT landscape;IIoT service priorities;IIoT services;industrial production;intelligent O-RAN framework;machine learning;mobile robot communications;modular production system;network slice configuration policy;network slice resource demand variations;O-RAN control layer;O-RAN service management layer;O-RAN slice isolation constraints;O-RAN slicing problem;on-time information update;ontime industrial monitoring;open radio access network;severe communication environment;static manufacturing processes;time varying slicing demand","","7","","53","IEEE","4 Jul 2022","","","IEEE","IEEE Journals"
"An OCBA-Based Method for Efficient Sample Collection in Reinforcement Learning","K. Li; X. Jin; Q. -S. Jia; D. Ren; H. Xia","Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), Center for Intelligent and Networked Systems (CFINS), Tsinghua University, Beijing, China; Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), Center for Intelligent and Networked Systems (CFINS), Tsinghua University, Beijing, China; Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), Center for Intelligent and Networked Systems (CFINS), Tsinghua University, Beijing, China; Meituan Group, Beijing, China; Meituan Group, Beijing, China","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","12","This work focuses on the sample collection in reinforcement learning (RL), where the interaction with the environment is typically time-consuming and extravagantly expensive. In order to collect samples in a more valuable way, we propose a confidence-based sampling strategy based on the optimal computing budget allocation algorithm (OCBA), which actively allocates the computing efforts to actions with different predictive uncertainties. We estimate the uncertainty with ensembles and generalize them from tabular representations to function approximations. The OCBA-based sampling strategy could be easily integrated into various off-policy RL algorithms, where we take Q-learning, DQN, and SAC as examples to show the incorporation. Besides, we provide the theoretical analysis towards convergence and evaluate the algorithms experimentally. According to the experiments, the incorporated algorithms obtain remarkable gains compared with modern ensemble-based RL algorithms. Note to Practitioners—Reinforcement learning is a powerful tool for handling sequential decision-making problems, e.g., autonomous driving and robotics control, where the behaviors typically have a long-term effect on future events. However, although RL achieves human-level control in some tasks, it severely suffers from low sample efficiency. Therefore, implementing RL in some practical areas, e.g., healthcare and rescue, is extremely hard due to the requirement of massive samples. This work aims to enhance the exploration of RL by incorporating OCBA, which provides an asymptotically optimal data-collection strategy for simulation-based optimization. Based on ensemble-based uncertainty estimation and OCBA-based action selection, the incorporated RL algorithms show competitive performance on many benchmarks and significantly reduce the sampling efforts during iterations.","1558-3783","","10.1109/TASE.2023.3282257","NSFC(grant numbers:62125304,62192751,62073182); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148580","Reinforcement learning;OCBA;ensemble;uncertainty;exploration","Uncertainty;Prediction algorithms;Resource management;Approximation algorithms;Q-learning;Function approximation;Task analysis","","","","","","","CCBY","12 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Deep Reinforcement Learning Based Trajectory Planning for Hopping on Low-Gravity Asteroid Surface","L. Chang; L. Zixuan; Z. Shengying","School of Aerospace Engineering, Beijing Institute of Technology, Key Laboratory of Autonomous Navigation and Control for Deep Space Exploration, Ministry of Industry and Information Technology, Beijing, China; School of Aerospace Engineering, Beijing Institute of Technology, Key Laboratory of Autonomous Navigation and Control for Deep Space Exploration, Ministry of Industry and Information Technology, Beijing, China; School of Aerospace Engineering, Beijing Institute of Technology, Key Laboratory of Autonomous Navigation and Control for Deep Space Exploration, Ministry of Industry and Information Technology, Beijing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","5353","5358","In a small-body exploration mission, the rover may deviate from the expected target point due to the dispersion of delivery. This paper proposes a hopping trajectory planning method based on deep reinforcement learning to achieve a precision landing on a low-gravity surface. First, the dynamic model of the hopping rover is established. Then, the hopping scheme is proposed with the attitude angle and angular velocity as control variables. In order to rapidly solve the control variables, the deep reinforcement learning algorithm is utilized for the autonomous hopping trajectory planning. The landing process is divided into an approach and a deceleration stage, and two agents are trained according to the reward functions of the two stages. To achieve the expected attitude angle and angular velocity given by the agents’ outputs, the control torque is solved using sliding mode control method. Finally, the hopping trajectory planning method are verified in a landing mission on low-gravity surface. The results show that the rover can reach and stop at the target by intelligent hopping under various initial conditions.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728343","National Natural Science Foundation of China; Beijing Institute of Technology Research Fund Program for Young Scholars; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728343","Hopping rover;Low-gravity surface;Trajectory planning;Deep reinforcement learning;Sliding mode control","Torque;Trajectory planning;Attitude control;Reinforcement learning;Angular velocity;Trajectory;Solar system","aerospace control;aerospace robotics;asteroids;attitude control;learning (artificial intelligence);mobile robots;optimal control;path planning;planetary rovers;position control;space vehicles;trajectory control;variable structure systems","hopping scheme;angular velocity;control variables;deep reinforcement learning algorithm;autonomous hopping trajectory planning;expected attitude angle;sliding mode control method;hopping trajectory planning method;landing mission;low-gravity surface;intelligent hopping;low-gravity asteroid surface;small-body exploration mission;expected target point;precision landing;hopping rover","","","","12","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Effective Strategies of Positive Reinforcement Learning For Non-Compliant Behavior","S. Mazhar; S. Rasheed; M. R. Naqvi","Dept of Software Engineering, The Superior University, Lahore, Pakistan; Dept of Software Engineering, The Superior University, Lahore, Pakistan; Le Laboratoire Génie de Production, INP-ENIT, Tarbes, France","2022 International Conference on Data Analytics for Business and Industry (ICDABI)","14 Feb 2023","2022","","","1","5","This Research was conducted to test the effect of positive reinforcement on non-compliant behavior. And also, compare the effectiveness of different kinds of reinforcements on academic achievements. This accomplishes that positive reinforcement did reduce this student’s noncompliant behavior. In this study we test the effect of positive reinforcement on a boy who is facing ASD autism spectrum disorder and then after four weeks of observation when the complaint behavior was ignored, we implement positive reinforcement and as a result,it is clear that positive reinforcement reduces the non-compliant behavior. Positive reinforcement is different for every student but the theory of hypothesis regarding no difference in compliant behavior before and after this implementation is wrong and rejected because the t-test is showing a clear difference. It is suggested that further contemplates are directed at other evaluation and age levels, just as in an entire bunch setting.","","978-1-6654-9058-0","10.1109/ICDABI56818.2022.10041587","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10041587","Reinforcement Learning;Non-Complaint Behavior;Autsim","Industries;Autism;Data analysis;Reinforcement learning;Behavioral sciences;Business","computer aided instruction;handicapped aids;medical disorders;reinforcement learning","ASD autism spectrum disorder;noncompliant behavior;positive reinforcement learning","","","","21","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
