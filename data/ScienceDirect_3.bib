@article{QIU2023103063,
title = {Heterogeneous reinforcement learning vibration control of coupling system with four flexible beams connected by springs},
journal = {Mechatronics},
volume = {95},
pages = {103063},
year = {2023},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2023.103063},
url = {https://www.sciencedirect.com/science/article/pii/S0957415823001198},
author = {Zhi-cheng Qiu and Yang Yang and Xian-min Zhang},
keywords = {Four-flexible beam coupling system, Vibration control, RL algorithm, Experimental identification, HATRPO},
abstract = {Aiming at studying the vibration characteristics and active control of a coupling system with four flexible beams connected by springs, an experimental platform is built. The dynamic equation of the system is solved by finite element method (FEM), and the parameter model based on state space equation is deduced. In order to ensure the accuracy of the parameter model, an experimental identification method based on wavelet transform and optimization algorithm is adopted. The state matrix, observation matrix and control force coefficient matrix in the parameterized model are solved in turn. A multi-agent based Heterogeneous-Agent Trust Region Policy Optimization (HATRPO) reinforcement learning (RL) algorithm is designed. The HATRPO RL algorithm interacts with the identified parameter model. After several rounds of training, the HATRPO RL vibration controller is finally obtained. The simulation and experimental results show that the HATRPO RL controller can well compensate for the nonlinearity and uncertainty in the multi-flexible beam coupling system. In addition, the nonlinear characteristics of the HATRPO RL algorithm effectively solve the problem of insufficient control power of traditional linear controller in small vibration amplitude, and realize faster vibration suppression.}
}
@article{WANG200573,
title = {Application of reinforcement learning for agent-based production scheduling},
journal = {Engineering Applications of Artificial Intelligence},
volume = {18},
number = {1},
pages = {73-82},
year = {2005},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2004.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0952197604001034},
author = {Yi-Chi Wang and John M. Usher},
keywords = {Reinforcement learning, -learning, Dispatching rule selection},
abstract = {Reinforcement learning (RL) has received some attention in recent years from agent-based researchers because it deals with the problem of how an autonomous agent can learn to select proper actions for achieving its goals through interacting with its environment. Although there have been several successful examples demonstrating the usefulness of RL, its application to manufacturing systems has not been fully explored yet. In this paper, Q-learning, a popular RL algorithm, is applied to a single machine dispatching rule selection problem. This paper investigates the application potential of Q-learning, a widely used RL algorithm to a dispatching rule selection problem on a single machine to determine if it can be used to enable a single machine agent to learn commonly accepted dispatching rules for three example cases in which the best dispatching rules have been previously defined. This study provided encouraging results that show the potential of RL for application to agent-based production scheduling.}
}
@article{HAO2023109231,
title = {A sequential decision problem formulation and deep reinforcement learning solution of the optimization of O&M of cyber-physical energy systems (CPESs) for reliable and safe power production and supply},
journal = {Reliability Engineering & System Safety},
volume = {235},
pages = {109231},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109231},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023001461},
author = {Zhaojun Hao and Francesco {Di Maio} and Enrico Zio},
keywords = {Cyber-Physical Energy System (CPES), Operation & Maintenance (O&M), Deep Reinforcement Learning (DRL), Nuclear Power Plant (NPP), Optimization, Advanced Lead-cooled Fast Reactor European Demonstrator (ALFRED)},
abstract = {The Operation & Maintenance (O&M) of Cyber-Physical Energy Systems (CPESs) is driven by reliable and safe production and supply, that need to account for flexibility to respond to the uncertainty in energy demand and also supply due to the stochasticity of Renewable Energy Sources (RESs); at the same time, accidents of severe consequences must be avoided for safety reasons. In this paper, we consider O&M strategies for CPES reliable and safe production and supply, and develop a Deep Reinforcement Learning (DRL) approach to search for the best strategy, considering the system components health conditions, their Remaining Useful Life (RUL), and possible accident scenarios. The approach integrates Proximal Policy Optimization (PPO) and Imitation Learning (IL) for training RL agent, with a CPES model that embeds the components RUL estimator and their failure process model. The novelty of the work lies in i) taking production plan into O&M decisions to implement maintenance and operate flexibly; ii) embedding the reliability model into CPES model to recognize safety related components and set proper maintenance RUL thresholds. An application, the Advanced Lead-cooled Fast Reactor European Demonstrator (ALFRED), is provided. The optimal solution found by DRL is shown to outperform those provided by state-of-the-art O&M policies.}
}
@article{WANG2023101849,
title = {High-efficient view planning for surface inspection based on parallel deep reinforcement learning},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101849},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101849},
url = {https://www.sciencedirect.com/science/article/pii/S147403462200307X},
author = {Yuanbin Wang and Tao Peng and Wenhu Wang and Ming Luo},
keywords = {Automatic surface inspection, View planning, Deep reinforcement learning, Intelligent manufacturing},
abstract = {Machine vision, especially deep learning methods, has become a hot topic for product surface inspection. In practice, capturing high quality images is a base for defect detection. It turns out to be challenging for complex products as image quality suffers from occlusion, illumination, and other issues. Multiple images from different viewpoints are often required in this scenario to cover all the important areas of the products. Reducing the viewpoints while ensuring the coverage is the key to make the inspection system more efficient in production. This paper proposes a high-efficient view planning method based on deep reinforcement learning to solve this problem. First, visibility estimation method is developed so that the visible areas can be quickly identified for a given viewpoint. Then, a new reward function is designed, and the Asynchronous Advantage Actor-Critic method is applied to solve the view planning problem. The effectiveness and efficiency of the proposed method is verified with a set of experiments. The proposed method could also be potentially applied to other similar vision-based tasks.}
}
@article{WU2021108004,
title = {Deep reinforcement learning for blockchain in industrial IoT: A survey},
journal = {Computer Networks},
volume = {191},
pages = {108004},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108004},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621001213},
author = {Yulei Wu and Zehua Wang and Yuxiang Ma and Victor C.M. Leung},
keywords = {Blockchain, Industrial Internet-of-Things, Consensus, Storage, Communication, Security},
abstract = {With the ambitious plans of renewal and expansion of industrialization in many countries, the efficiency, agility and cost savings potentially resulting from the application of industrial Internet of Things (IIoT) are drawing attentions. Although blockchain and machine learning technologies (especially deep learning and deep reinforcement learning) may provide the next promising use case for IIoT, they are working in an adversarial way to some extent. While blockchain facilitates the data collection that is critical for machine learning under the premise of data regulation rules like privacy protection, it may suffer from privacy leakage due to big data analytics with the help of machine learning. To make machine learning and blockchain useful and practical for diversified services in industrial sectors, it is of paramount importance to have a comprehensive understanding of the development of both technologies in the context of IIoT. To this end, in this paper we summarize and analyze the applications of blockchain and machine learning in IIoT from three important aspects, i.e., consensus mechanism, storage and communication. This survey provides a deeper understanding on the security and privacy risks of the key components of a blockchain from the perspective of machine learning, which is useful in the design of practical blockchain solutions for IIoT. In addition, we provide useful guidance for future research in the area by identifying interesting open problems that need to be addressed before large-scale deployments of IIoT applications are practicable.}
}
@article{KUMAR2021107644,
title = {Production scheduling in industrial mining complexes with incoming new information using tree search and deep reinforcement learning},
journal = {Applied Soft Computing},
volume = {110},
pages = {107644},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107644},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621005652},
author = {Ashish Kumar and Roussos Dimitrakopoulos},
keywords = {Artificial intelligence, Production scheduling, Mining complexes, Reinforcement learning, New information},
abstract = {Industrial mining complexes have implemented digital technologies and advanced sensors to monitor and gather real-time data about their different operational aspects, starting from the supply of materials from the mineral deposits involved to the products provided to customers. However, technologies are not available to respond in real-time to the incoming new information to adapt the short-term production schedule of a mining complex. A short-term production schedule determines the daily/weekly/monthly sequence of extraction, the destination of materials and utilization of processing streams. This paper presents a novel self-learning artificial intelligence algorithm for mining complexes that learns, from its own experience, to adapt the short-term production scheduling decisions by responding to incoming new information. The algorithm plays the game of short-term production scheduling on its own using a Monte Carlo tree search to train a deep neural network agent that adapts the short-term production schedule with incoming new information. The deep neural network agent evaluates the short-term production scheduling decisions and, in parallel, performs searches using the Monte Carlo tree search to generate experiences. The experiences are then used to train the agent. The agent improves the strength of the tree search, which results in an even stronger self-play to generate better experiences. An application of the proposed algorithm at a real-world copper mining complex shows its exceptional performance to adapt the 13-week short-term production schedule almost in real-time. The adapted production schedule successfully meets the different production requirements and makes better use of the processing capabilities, while also increasing copper concentrate production by 7% and cash flows by 12% compared to the initial production schedule. A video of the proposed algorithm can be found at https://youtu.be/_gSbzxMc_W8.}
}
@article{TAO2022117,
title = {Reinforcement Learning for Dynamic Mutation Process Control in Multi-Objective Differential Evolution},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {15},
pages = {117-122},
year = {2022},
note = {6th IFAC Conference on Intelligent Control and Automation Sciences ICONS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.07.618},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322010291},
author = {Lue Tao and Gongshu Wang and Yang Yang and Yun Dong and Lijie Su},
keywords = {Reinforcement Learning, Evolutionary Algorithm, Multi-Objective Deferential Evolution, Mutation Process Control, Operator Selection},
abstract = {To improve the search performance of the multi-objective differential evolution algorithm, we use a reinforcement learning agent to control the dynamic mutation process. First, a novel multi-objective optimization framework with new mutation operators is developed. Then, a Q-learning algorithm is introduced to select mutation operators with different characteristics adaptively. Specifically, we design three feature functions to describe the state of the population observed by the agent. Based on the observed state, the agent dynamically selects the mutation operator in each generation, which strongly improved the searching ability. We compare the performance of the proposed method with two state-of-art algorithms on benchmark functions and find that the proposed algorithm has a better performance in multiple indicators. At the same time, the component validity analysis also shows the effectiveness of the Q-learning and the framework introduced in this paper. Finally, the learning process shows that the agent can achieve asymptotic convergence.}
}
@article{KERAMATI2022123112,
title = {Deep reinforcement learning for heat exchanger shape optimization},
journal = {International Journal of Heat and Mass Transfer},
volume = {194},
pages = {123112},
year = {2022},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2022.123112},
url = {https://www.sciencedirect.com/science/article/pii/S001793102200583X},
author = {Hadi Keramati and Feridun Hamdullahpur and Mojtaba Barzegari},
keywords = {Reinforcement learning, Deep neural network, Heat exchanger, Geometric constraints, BREP},
abstract = {We present a parametric approach for heat exchanger shape optimization utilizing Deep Reinforcement Learning (Deep RL) and Boundary Representation (BREP). In this study, we show that continuous geometric representation of the fluid and solid domain facilitates the implementation of boundary conditions and design space exploration in contrast to traditional Topology Optimization such as density-based methods. The proposed framework consists of a Deep Neural Network (DNN), a Computational Fluid Dynamics (CFD) solver with an automatic body-fitted mesh generation to solve a single fin shape optimization. The learning is performed using Proximal Policy Optimization (PPO) in combination with a CFD environment in FEniCS. The RL agent successfully explores the design space and maximizes heat transfer and minimizes pressure drop for geometric design with as low as 12 degrees of freedom represented by composite Bézier curves. Higher degree of freedom results in higher reward of the agent. This method alleviates the curse of dimensionality compared to voxel and pixel-based optimization of coupled thermal fluid-structure. Results show the manufacturability and efficiency of the output of our framework. Over 30 percent improvement in overall heat transfer while lowering the pressure drop by more than 60 percent compared to the rectangle reference geometry is achieved.}
}
@article{HUANG2023104691,
title = {To imitate or not to imitate: Boosting reinforcement learning-based construction robotic control for long-horizon tasks using virtual demonstrations},
journal = {Automation in Construction},
volume = {146},
pages = {104691},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104691},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522005611},
author = {Lei Huang and Zihan Zhu and Zhengbo Zou},
keywords = {Construction robot, Reinforcement learning, Virtual reality},
abstract = {Construction robots controlled using reinforcement learning (RL) have recently emerged, showing higher adaptability and self-learning intelligence over pre-programmed and teleoperated robots. This work aims to train RL-based construction robots to learn long-horizon tasks with few exploration steps. We propose an approach that collects expert demonstrations in virtual reality, which are then added in the RL loop to assist with learning long-horizon construction tasks. Additionally, we utilize a hierarchical training strategy to generalize control policies to new policies that handle complex scenarios. For evaluation, we implement the approach for picking and installing window panels in simulation. In experiments, all 10 agents trained with virtual demonstrations delivered the task with success rates over 95%. Moreover, these 10 agents generalized their control policies and handled randomized window panels with success rates over 90%. The results confirm the effectiveness of our approach in boosting construction robots’ performance over long-horizon tasks.}
}
@article{WESENDRUP2023109216,
title = {Post-prognostics demand management, production, spare parts and maintenance planning for a single-machine system using Reinforcement Learning},
journal = {Computers & Industrial Engineering},
volume = {179},
pages = {109216},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109216},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223002401},
author = {Kevin Wesendrup and Bernd Hellingrath},
keywords = {Production Planning and Control, Condition-based Maintenance, Prognostics and Health Management, Post-prognostics decision-making, Reinforcement Learning},
abstract = {Production Planning and Control (PPC) is crucial for any manufacturer and comprises steps such as demand management, production, or source planning. Manufacturers achieve competitive advantage by sustaining continuous production, which can be realised through Condition-based Maintenance and Prognostics and Health Management. Hereby, the machine’s health can be predicted, and post-prognostics decision-making allows to optimise PPC to meet customer demands and minimise costs. Unfortunately, the complex dynamic, stochastic and intransparent nature of post-prognostics PPC makes it intractable to use ‘traditional’ static or deterministic optimisation techniques or approaches that require an exact mathematical model or objective function. To tackle this, a data-driven post-prognostics Reinforcement Learning model is developed to plan and control the sourcing of spare parts, production, and maintenance of a single-machine production system to maximise production revenue by meeting customer demands and minimising costs. In a case study, Proximal Policy Optimisation, which is well-known from OpenAI’s ChatGPT, is applied to a post-prognostics PPC decision-making problem. The Proximal Policy Optimisation is compared to other state-of-the-art learners, and the performance and robustness are evaluated. Analyses show that our model outperforms other learners, as well as reactive and scheduled preventive maintenance strategies and is robust to noise and cost changes.}
}
@article{RADAIDEH2023112423,
title = {NEORL: NeuroEvolution Optimization with Reinforcement Learning—Applications to carbon-free energy systems},
journal = {Nuclear Engineering and Design},
volume = {412},
pages = {112423},
year = {2023},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2023.112423},
url = {https://www.sciencedirect.com/science/article/pii/S0029549323002728},
author = {Majdi I. Radaideh and Katelin Du and Paul Seurin and Devin Seyler and Xubo Gu and Haijia Wang and Koroush Shirvan},
keywords = {Optimization, Deep reinforcement learning, Evolutionary computation, Neuroevolution, Nuclear reactor design, Carbon-free energy},
abstract = {We present an open-source Python framework for NeuroEvolution Optimization with Reinforcement Learning (NEORL) developed at the Massachusetts Institute of Technology. NEORL offers a global optimization interface of state-of-the-art algorithms in the field of evolutionary computation, neural networks through reinforcement learning, and hybrid neuroevolution algorithms. NEORL features diverse set of algorithms, user-friendly interface, parallel computing support, automatic hyperparameter tuning, detailed documentation, and demonstration of applications in mathematical and real-world engineering optimization. NEORL encompasses various optimization problems from combinatorial, continuous, mixed discrete/continuous, to high-dimensional, expensive, and constrained engineering optimization. In this paper, NEORL is tested in a variety of engineering applications relevant to low carbon energy research in addressing solutions to climate change. The examples include nuclear reactor control, nuclear fuel optimization, mechanical and structural design optimization, and fuel cell power production. The results demonstrate NEORL competitiveness against other algorithms and optimization frameworks in the literature, and a potential tool to solve large-scale optimization problems. More details about NEORL can be found here: https://neorl.readthedocs.io/en/latest/index.html.}
}
@article{WANG2023261,
title = {Evolutionary-assisted reinforcement learning for reservoir real-time production optimization under uncertainty},
journal = {Petroleum Science},
volume = {20},
number = {1},
pages = {261-276},
year = {2023},
issn = {1995-8226},
doi = {https://doi.org/10.1016/j.petsci.2022.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S1995822622001959},
author = {Zhong-Zheng Wang and Kai Zhang and Guo-Dong Chen and Jin-Ding Zhang and Wen-Dong Wang and Hao-Chen Wang and Li-Ming Zhang and Xia Yan and Jun Yao},
keywords = {Production optimization, Deep reinforcement learning, Evolutionary algorithm, Real-time optimization, Optimization under uncertainty},
abstract = {Production optimization has gained increasing attention from the smart oilfield community because it can increase economic benefits and oil recovery substantially. While existing methods could produce high-optimality results, they cannot be applied to real-time optimization for large-scale reservoirs due to high computational demands. In addition, most methods generally assume that the reservoir model is deterministic and ignore the uncertainty of the subsurface environment, making the obtained scheme unreliable for practical deployment. In this work, an efficient and robust method, namely evolutionary-assisted reinforcement learning (EARL), is proposed to achieve real-time production optimization under uncertainty. Specifically, the production optimization problem is modeled as a Markov decision process in which a reinforcement learning agent interacts with the reservoir simulator to train a control policy that maximizes the specified goals. To deal with the problems of brittle convergence properties and lack of efficient exploration strategies of reinforcement learning approaches, a population-based evolutionary algorithm is introduced to assist the training of agents, which provides diverse exploration experiences and promotes stability and robustness due to its inherent redundancy. Compared with prior methods that only optimize a solution for a particular scenario, the proposed approach trains a policy that can adapt to uncertain environments and make real-time decisions to cope with unknown changes. The trained policy, represented by a deep convolutional neural network, can adaptively adjust the well controls based on different reservoir states. Simulation results on two reservoir models show that the proposed approach not only outperforms the RL and EA methods in terms of optimization efficiency but also has strong robustness and real-time decision capacity.}
}
@article{GUO2019105828,
title = {A reinforcement learning decision model for online process parameters optimization from offline data in injection molding},
journal = {Applied Soft Computing},
volume = {85},
pages = {105828},
year = {2019},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2019.105828},
url = {https://www.sciencedirect.com/science/article/pii/S156849461930609X},
author = {Fei Guo and Xiaowei Zhou and Jiahuan Liu and Yun Zhang and Dequn Li and Huamin Zhou},
keywords = {Intelligent manufacturing, Injection molding, Neural network, Reinforcement learning},
abstract = {Injection molding is widely used owing to its ability to form high precision products. Good dimensional accuracy control depends on appropriate process parameters settings. However, existing optimization methods fail in producing ultra-high precision products due to their narrow process windows. In order to address the problem, an online decision system which consists of a novel reinforcement learning framework and a self-prediction artificial neural network model is developed. This decision system utilizes the knowledge learned from offline data to dynamically optimize the process of ultra-high precision products. Process optimization of an optical lens is dedicated to validating the proposed system. The experimental results show that the proposed system has excellent convergence performance in producing lens with deviation not exceeding ± 5μm. Comparison with the static optimization method prove that the decision model is more robust and effective in online production environment. And it achieves superior results in continuous production with the process capability index of 1.720 compared to 0.315 in fuzzy inference system. There is great potential for utilizing the proposed data-driven decision system in similar manufacturing process.}
}
@article{LIAN2022110524,
title = {Inverse reinforcement learning for multi-player noncooperative apprentice games},
journal = {Automatica},
volume = {145},
pages = {110524},
year = {2022},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2022.110524},
url = {https://www.sciencedirect.com/science/article/pii/S0005109822003843},
author = {Bosen Lian and Wenqian Xue and Frank L. Lewis and Tianyou Chai},
keywords = {Inverse reinforcement learning, Inverse optimal control, Optimal control, Multi-player noncooperative games},
abstract = {In this paper, we devise inverse reinforcement learning (RL) algorithms for nonlinear continuous-time systems described by multiplayer differential equations. We define a new class of Multi-player Noncooperative Apprentice Games, in which both the expert and the learner have N-player control inputs. The games are solved by the learners reconstructing unknown performance reward functions of the experts from the experts’ trajectories, i.e., states and optimal control inputs. We first develop a model-based inverse RL algorithm that involves two learning stages: an optimal control learning stage and a second inverse optimal control (IOC) learning stage. Our algorithm solves IOC as a subproblem. We therefore provide one possible unified framework for inverse RL and IOC in multiplayer differential dynamic systems. We then develop two inverse RL algorithms using neural networks: completely model-free for homogeneous control inputs; and partially model-free for heterogeneous control inputs. Finally, we present the results of simulations, which verify the validity of our proposed algorithms.}
}
@article{LIU2022110,
title = {Output synchronization of multi-agent systems via reinforcement learning},
journal = {Neurocomputing},
volume = {508},
pages = {110-119},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222009808},
author = {Yingying Liu and Zhanshan Wang},
keywords = {Output synchronization, Multi-agent systems, Reinforcement learning, Input–output data sequences with pinning gain},
abstract = {In this paper, the measured input–output data sequences with pinning gain are proposed via the topology of multi-agent systems (MAS), where the requirement of reinforcement learning algorithm on internal state is avoided by pinning gain and the measured data of leader and neighbors. Besides, a data-based tracking state is given, which can be applied to MAS with different control matrices. According to the sequences and tracking state, a distributed control policy and corresponding reinforcement learning algorithm are proposed for the output synchronization. The proposed algorithm overcomes the shortcoming that previous algorithms can not be applied to MAS with different control matrices in the absence of model information and full-state vector. Finally, the effectiveness of proposed algorithm is verified by simulation examples.}
}
@article{HEINBACH202340,
title = {Deep reinforcement learning for layout planning – An MDP-based approach for the facility layout problem},
journal = {Manufacturing Letters},
volume = {38},
pages = {40-43},
year = {2023},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2023.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S2213846323002134},
author = {Benjamin Heinbach and Peter Burggräf and Johannes Wagner},
keywords = {Reinforcement learning, Markov decision process, Facility layout problem, Machine learning, Optimization},
abstract = {Deep Reinforcement Learning (DRL) has demonstrated operational excellence in several production-related problems. This paper applies DRL to facility layout problems (FLP) using Proximal Policy Optimisation, Advantage Actor-Critic and Deep Q-Networks. We show that the proposed approach produces an improved arrangement of facilities. The contribution of this work is the proof of concept that DRL can optimise layouts with respect to material handling costs using only an image representation of the layout and a reward signal. The approach shows potential to generalise to new layouts without the need to model or train, thus significantly speeding up layout design procedures.}
}
@article{OGOKE2021102033,
title = {Thermal control of laser powder bed fusion using deep reinforcement learning},
journal = {Additive Manufacturing},
volume = {46},
pages = {102033},
year = {2021},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2021.102033},
url = {https://www.sciencedirect.com/science/article/pii/S2214860421001986},
author = {Francis Ogoke and Amir Barati Farimani},
keywords = {Deep Reinforcement Learning, Additive Manufacturing, Powder Bed Fusion},
abstract = {Powder-based additive manufacturing techniques provide tools to construct intricate structures that are difficult to manufacture using conventional methods. In Laser Powder Bed Fusion, components are iteratively built by selectively melting specific areas of the powder bed to form the two-dimensional cross section of the specific part. However, the high occurrence of defects impact the adoption of this method for precision applications. Therefore, a control policy for dynamically altering process parameters to avoid phenomena that lead to defect occurrences is necessary. A Deep Reinforcement Learning (DRL) framework that derives a versatile control strategy for minimizing the likelihood of these defects is presented. The generated control policy alters either the velocity or power of the laser during the melting process to ensure the consistency of the melt pool and reduce overheating in the generated product. The control policy is trained and validated on efficient simulations of the continuum temperature distribution of the powder bed layer under various laser trajectories.}
}
@article{GAO2023108131,
title = {Reinforcement learning based optimization algorithm for maintenance tasks scheduling in coalbed methane gas field},
journal = {Computers & Chemical Engineering},
volume = {170},
pages = {108131},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.108131},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422004641},
author = {Xiaoyong Gao and Diao Peng and Guofeng Kui and Jun Pan and Xin Zuo and Feifei Li},
keywords = {Coalbed methane gas field, Maintenance tasks scheduling, Mathematical model, Reinforcement learning, Q-learning},
abstract = {The Coalbed Methane (CBM) well maintenance tasks scheduling optimization is of importance for improving the CBM production efficiency. Traditionally, this problem is addressed using mathematical model based classical optimization method or some meta-heuristic algorithms. However, due to the large-scale nature of this problem, these trials often fail in practical use. Therefore, the Q-learning algorithm based solving method is proposed in this paper. An interactive environment for reinforcement learning is constructed. To validate the effectiveness of proposed method, scenarios with different scales are provided. For the cases with 10, 14, 20, 30, 47, 60, 80 and 100 maintenance tasks respectively, the required solution time is 3.66 s, 4.94 s, 7.66 s, 12.48 s, 21.82 s, 30.82 s, 48.33 s, 73.17 s respectively. The proposed Q-learning algorithm is insensitive with the problem scale, which is promising. Moreover, the Q-learning based algorithm is more efficient than the traditional algorithm along with the increase of the number of tasks.}
}
@article{WU2021392,
title = {A fast decision-making method for process planning with dynamic machining resources via deep reinforcement learning},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {392-411},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520302296},
author = {Wenbo Wu and Zhengdong Huang and Jiani Zeng and Kuan Fan},
keywords = {Deep reinforcement learning, Computer-aided process planning, Combinatorial optimization, Decision making},
abstract = {Mass customized production brings great uncertainty to the computer-aided process planning (CAPP). Current CAPP methods based on heuristic optimization assume in advance that manufacturing resources are static and make a deterministic plan that cannot cope with the uncertainty of the manufacture environment. As a promising method in solving complex and dynamic decision-making problems, deep reinforcement learning is employed in this paper for process planning, aiming at promoting the response speed by exploiting the reusability and expandability of past decision-making experiences. To simplify the decision procedure, two different types of decisions, operation sequencing and resource selection, are fused into one by integrating environment states and agent behaviors in a matrix manner. Then, a masking algorithm is developed to screen out currently inexecutable machining operations at each decision step and process planning datasets are generated for training and testing according to the actual processing logic. Next, the Monte Carlo method and the deep learning algorithm are utilized to evaluate and improve the process policy, respectively. Finally, the searching capability of the proposed method for both static and dynamic manufacturing resources are tested in case studies, and the results are discussed. It is shown that the proposed approach can solve the planning problem more efficiently compared with current optimization-based approaches.}
}
@article{SCHREIBER2020110490,
title = {Application of two promising Reinforcement Learning algorithms for load shifting in a cooling supply system},
journal = {Energy and Buildings},
volume = {229},
pages = {110490},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.110490},
url = {https://www.sciencedirect.com/science/article/pii/S0378778820320922},
author = {Thomas Schreiber and Sören Eschweiler and Marc Baranski and Dirk Müller},
keywords = {Reinforcement Learning, Load shifting, Optimal control, Thermal systems, Building automation and control, Simulation},
abstract = {With the increasing use of volatile renewable energies, the requirements for building automation and control systems (BACS) are increasing. Load shifting within local energy systems stabilizes fluctuations in the grid and can be triggered by price signals. The energy purchase can thus be considered and solved as an optimal control problem. Classical approaches, often based on the optimization of mathematical models, are uneconomical in many cases, due to the high effort involved in the model creation. Algorithms from the field of Reinforcement Learning (RL), on the other hand, have a high potential for the automation of energy system optimization, due to their model-free and data-driven characteristics. However, there is still a lack of studies that examine algorithms for BACS-related applications in a structured way. Therefore, we present a study, investigating the potential of two different RL algorithms for load shifting in a cooling supply system. We combine the benefits of Modelica, a powerful modeling language, with RL algorithms and demonstrate how generalized relationships and control decisions can be learned. The case study is modeled according to a cooling supply system in Berlin, Germany. The two different algorithms (DQN and DDPG) are used to control the operation parameters of a central compression chiller, with respect to a price signal. While real monitoring data are used as exogenous influences, the thermal dynamics of the cooling network are simulated. With the learned policies, flexibility in the network is used which leads on average to weekly cost savings of 14 %, compared to direct load coverage. Our results suggest that, under certain conditions, RL is a suitable alternative to established methods. However, we also acknowledge that there are still research questions to address before RL can be applied in real BACS.}
}
@article{XIONG2023102028,
title = {Towards reliable robot packing system based on deep reinforcement learning},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102028},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102028},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623001568},
author = {Heng Xiong and Kai Ding and Wan Ding and Jian Peng and Jianfeng Xu},
keywords = {Robotics, Online bin packing, Reinforcement learning, Manipulation},
abstract = {Object packing by a robot has a wide range of applications in the logistics industry. This task requires the variable size items to be picked from piles one by one and then packed into another container immediately without the information about the unpicked items, modeled as an Online 3D Bin Packing Problem (3D-BPP). Due to limited information, it is a challenging problem to obtain an optimal solution for maximizing space utilization. Furthermore, existing studies do not consider practical constraints and assume an ideal perception and robotic packing manipulation. In this paper, we present a robot packing system with high performance and reliability. First, the Online 3D-BPP is formulated as a Markov decision process. A deep reinforcement learning (DRL) approach is proposed to tackle the problem utilizing the observations of the container and the current item. Specifically, a candidate map that indicates the potentially feasible placements based on heuristics is introduced to balance the exploration and exploitation in the considerable discrete action space. Second, we develop a physical robotic system to bridge the DRL agent from simulation to practical application. To make the packing manipulation resilient to uncertainties from the physical system, we design a motion primitive by moving the picked item close to its target placement from a collision-free area within the container. Experiments demonstrate that our method delivers superior performance against the baselines on two datasets, improving space utilization by over 2.7% and 3.8%, respectively, and the performance is not limited by the container size. Moreover, our robotic system can facilitate DRL to perform well in the real world.}
}
@article{PELECH2022401,
title = {Planning lunar In-Situ Resource Utilisation with a reinforcement learning agent},
journal = {Acta Astronautica},
volume = {201},
pages = {401-419},
year = {2022},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2022.09.040},
url = {https://www.sciencedirect.com/science/article/pii/S0094576522005057},
author = {T. Pelech and L. Yao and S. Saydam},
abstract = {In-situ Resource Utilisation (ISRU) is considered necessary to enable Off-Earth settlement. It is also regularly compared with terrestrial mining operations. Optimisation of the resource extraction sequence and cut-off grades for product and waste streams are critical for both terrestrial mining operations and ISRU. However, the traditional methods used in the terrestrial mining industry are not directly compatible with ISRU. This paper outlines the differences between terrestrial mining and ISRU and develops a new method for ISRU planning based on Reinforcement Learning (RL). An RL agent is trained and evaluated for extraction sequencing, sometimes showing the ability to outperform a human expert. The generalised RL agent can also be used to run multiple scenarios to determine optimal cut-off grades and conduct risk analysis on varying geological and equipment reliability inputs. Future ISRU projects and assessments will benefit from this method by reducing the human effort required to achieve production optimality.}
}
@article{WEKWETE2023200286,
title = {Application of deep reinforcement learning in asset liability management},
journal = {Intelligent Systems with Applications},
volume = {20},
pages = {200286},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200286},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323001114},
author = {Takura Asael Wekwete and Rodwell Kufakunesu and Gusti {van Zyl}},
keywords = {Reinforcement learning, Deep learning, Deep reinforcement learning, Asset liability management, Duration matching, Redington immunisation, Deep hedging},
abstract = {Asset Liability Management (ALM) is an essential risk management technique in Quantitative Finance and Actuarial Science. It aims to maximise a risk-taker's ability to fulfil future liabilities. ALM is especially critical in environments of elevated interest rate changes, as has been experienced globally between 2021 and 2023. Traditional ALM implementation is still heavily dependent on the judgement of professionals such as Quants, Actuaries or Investment Managers. This over-reliance on human input critically limits ALM performance due to restricted automation, human irrationality and restricted scope for multi-objective optimisation. This paper addressed these limitations by applying Deep Reinforcement Learning (DRL), which optimises through trial, and error and continuous feedback from the environment. We defined the Reinforcement Learning (RL) components for the ALM application: the RL decision-making Agent, Environment, Actions, States and Reward Functions. The results demonstrated that DRL ALM can achieve duration-matching outcomes within 1% of the theoretical ALM at a 95% confidence level. Furthermore, compared to a benchmark weekly rebalancing traditional ALM regime, DRL ALM achieved superior outcomes of net portfolios which are, on average, 3 times less sensitive to interest rate changes. DRL also allows for increased automation, flexibility, and multi-objective optimisation in ALM, reducing the negative impact of human limitations and improving risk management outcomes. The findings and principles presented in this study apply to various institutional risk-takers, including insurers, banks, pension funds, and asset managers. Overall, DRL ALM provides a promising Artificial Intelligence (AI) avenue for improving risk management outcomes compared to the traditional approaches.}
}
@article{KRISTENSEN2019225,
title = {Towards a Robot Simulation Framework for E-waste Disassembly Using Reinforcement Learning},
journal = {Procedia Manufacturing},
volume = {38},
pages = {225-232},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.030},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920300317},
author = {Christoffer B. Kristensen and Frederik A. Sørensen and Hjalte B. Nielsen and Martin S. Andersen and Søren P. Bendtsen and Simon Bøgh},
keywords = {E-waste, Robotic Disassembly, Reinforcement Learning},
abstract = {The purpose of this paper is to introduce a new framework for training and testing Reinforcement Learning (RL) algorithms for robotic unscrewing tasks. The paper investigates current disassembly technologies through a state-of-the-art analysis, and the basic concepts of reinforcement learning are studied. A comparable framework exists as an extension for OpenAI gym called Gym-Gazebo, which is tested and analysed. Based on this analysis, a design for a new framework is made to specifically support unscrewing operations in robotics disassembly of electronics waste. The proposed simulation architecture uses ROS as data middleware, Gazebo (with the ODE physics solver) for simulating the robot environment, and MoveIt as a controller. The Gazebo simulation consists of a minimalistic setup in order to stay focused on the architecture and usability of the framework. The simulation world interfaces with the RL-agent, using OpenAI Gym and ROS-topics, which can be adapted to interface with a real robot. Lastly, the work demonstrates the functionality of the system by implementing an application example using a Q-learning algorithm, and the results of this are presented.}
}
@article{LUO2023110071,
title = {A novel Congestion Control algorithm based on inverse reinforcement learning with parallel training},
journal = {Computer Networks},
volume = {237},
pages = {110071},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110071},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623005169},
author = {Pengcheng Luo and Yuan Liu and Zekun Wang and Jian Chu and Genke Yang},
keywords = {Congestion Control algorithm, Inverse reinforcement learning, Parallel training},
abstract = {With the growing impact of the Internet, computer network communication has become an essential component for various industries. Congestion Control (CC) algorithms serve as the backbone of network communication, significantly affecting network quality. However, designing a CC algorithm that performs optimally across diverse network environments presents substantial challenges. The task of redesigning and optimizing CC algorithms for specific network environments demands both expert experience and a substantial workforce. In this paper, we proposed an inverse reinforcement learning (IRL) algorithm that can use expert data to guide the self-optimization of the CC model in specific network environments. To enhance model training efficiency, we propose a parallel training framework and a visualization analysis tool, enabling distributed training and real-time control level analysis. In the experiments, we assess the performance of 16 algorithms across 3 network scenarios using Pantheon. Our IRL model achieves the optimal level of network performance in satellite network, enhancing throughput by 10%–23%. For delay performance, it ranks 2nd in wired network and achieves a 21%–67% improvement over traditional TCP algorithms in regular network.}
}
@article{DONG202013190,
title = {An Optimal Day-ahead Bidding Strategy and Operation for Battery Energy Storage System by Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {13190-13195},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.144},
url = {https://www.sciencedirect.com/science/article/pii/S240589632030402X},
author = {Yi Dong and Tianqiao Zhao and Zhengtao Ding},
keywords = {Battery Energy Storage System (BESS), optimal bidding, reinforcement learning},
abstract = {The Battery Energy Storage System (BESS) plays an important role in the smart grid and the ancillary market offers high revenues. It is reasonable for the owner of the BESS to maximise their profits by deciding how to bid with their rivals and balance between the different market offers. Therefore, this paper proposes an optimal bidding model of the BESS to maximise the total profit from the Automation Generation Control (AGC) market and the energy market, while taking the charging/discharging losses and the life of the BESS into consideration. Taking advantages of function approximation approaches, a reinforcement learning algorithm is introduced to the designed model, which can cope with the continuous and massive states of the proposed model and avoid the dimension curse. The resultant novel bidding model would help the BESS owners to decide their biddings and operational schedules profitably. Several case studies illustrate the effectiveness and validity of the proposed model.}
}
@article{HUBBS2020106982,
title = {A deep reinforcement learning approach for chemical production scheduling},
journal = {Computers & Chemical Engineering},
volume = {141},
pages = {106982},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106982},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420301599},
author = {Christian D. Hubbs and Can Li and Nikolaos V. Sahinidis and Ignacio E. Grossmann and John M. Wassick},
keywords = {Machine learning, Reinforcement learning, Optimization, Scheduling, Stochastic programming},
abstract = {This work examines applying deep reinforcement learning to a chemical production scheduling process to account for uncertainty and achieve online, dynamic scheduling, and benchmarks the results with a mixed-integer linear programming (MILP) model that schedules each time interval on a receding horizon basis. An industrial example is used as a case study for comparing the differing approaches. Results show that the reinforcement learning method outperforms the naive MILP approaches and is competitive with a shrinking horizon MILP approach in terms of profitability, inventory levels, and customer service. The speed and flexibility of the reinforcement learning system is promising for achieving real-time optimization of a scheduling system, but there is reason to pursue integration of data-driven deep reinforcement learning methods and model-based mathematical optimization approaches.}
}
@article{RADAIDEH2021106836,
title = {Rule-based reinforcement learning methodology to inform evolutionary algorithms for constrained optimization of engineering applications},
journal = {Knowledge-Based Systems},
volume = {217},
pages = {106836},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106836},
url = {https://www.sciencedirect.com/science/article/pii/S095070512100099X},
author = {Majdi I. Radaideh and Koroush Shirvan},
keywords = {Reinforcement learning, RL-guided evolutionary computation, proximal policy optimization, constrained combinatorial optimization, nuclear fuel assembly},
abstract = {For practical engineering optimization problems, the design space is typically narrow, given all the real-world constraints. Reinforcement Learning (RL) has commonly been guided by stochastic algorithms to tune hyperparameters and leverage exploration. Conversely in this work, we propose a rule-based RL methodology to guide evolutionary algorithms (EA) in constrained optimization. First, RL proximal policy optimization agents are trained to master matching some of the problem rules/constraints, then RL is used to inject experiences to guide various evolutionary/stochastic algorithms such as genetic algorithms, simulated annealing, particle swarm optimization, differential evolution, and natural evolution strategies. Accordingly, we develop RL-guided EAs, which are benchmarked against their standalone counterparts. RL-guided EA in continuous optimization demonstrates significant improvement over standalone EA for two engineering benchmarks. The main problem analyzed is nuclear fuel assembly combinatorial optimization with high-dimensional and computationally expensive physics. The results demonstrate the ability of RL to efficiently learn the rules that nuclear fuel engineers follow to realize candidate solutions. Without these rules, the design space is large for RL/EA to find many candidates. With imposing the rule-based RL methodology, we found that RL-guided EA outperforms standalone algorithms by a wide margin, with >10 times improvement in exploration capabilities and computational efficiency. These insights imply that when facing a constrained problem with numerous local optima, RL can be useful in focusing the search space in the areas where expert knowledge has demonstrated merit, while evolutionary/stochastic algorithms utilize their exploratory features to improve the number of feasible solutions.}
}
@article{OSINENKO2022123,
title = {Reinforcement learning with guarantees: a review},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {15},
pages = {123-128},
year = {2022},
note = {6th IFAC Conference on Intelligent Control and Automation Sciences ICONS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.07.619},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322010308},
author = {Pavel Osinenko and Dmitrii Dobriborsci and Wolfgang Aumer},
keywords = {Reinforcement learning, stability, safe control},
abstract = {Reinforcement learning is concerned with a generic concept of an agent acting in an environment. From the control theory standpoint, reinforcement learning may be considered as an adaptive optimal control scheme. Despite accumulating evidence of effectiveness of reinforcement learning in various applications, which range from video games to robotics, this control scheme in its bare-bones version provides no guarantees on the performance of the agent-environment closed loop. Measures have to be taken to provide the said guarantees. This survey gives a brief picture of the current progress in this direction. Three major groups of approaches are overviewed: supervisor-based, Lyapunov reinforcement learning and fusion with model-predictive control. The central message of this survey is that a synergy with classical model-based control seems the most promising direction of research in reinforcement learning, as long as it is to become an industry standard.}
}
@article{SU2023110596,
title = {Evolution strategies-based optimized graph reinforcement learning for solving dynamic job shop scheduling problem},
journal = {Applied Soft Computing},
volume = {145},
pages = {110596},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110596},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623006142},
author = {Chupeng Su and Cong Zhang and Dan Xia and Baoan Han and Chuang Wang and Gang Chen and Longhan Xie},
keywords = {Evolution strategies, Graph neural network, Graph reinforcement learning, Job shop scheduling, Reinforcement learning},
abstract = {The job shop scheduling problem (JSSP) with dynamic events and uncertainty is a strongly NP-hard combinatorial optimization problem (COP) with extensive applications in the manufacturing system. Recently, growing interest has been aroused in utilizing machine learning techniques to solve the JSSP. However, most prior arts cannot handle dynamic events and barely consider uncertainties. To close this gap, this paper proposes a framework to solve a dynamic JSSP (DJSP) with machine breakdown and stochastic processing time based on Graph Neural Network (GNN) and deep reinforcement learning (DRL). To this end, we first formulate the DJSP as a Markov Decision Process (MDP), where disjunctive graph represent the states. Secondly, we propose a GNN-based model to effectively extract the embeddings of the state by considering the features of the dynamic events and the stochasticity of the problem, e.g., the machine breakdown and stochastic processing time. Then, the model constructs solutions by dispatching optimal operations to machines based on the learned embeddings. Notably, we propose to use the evolution strategies (ES) to find optimal policies that are more stable and robust than conventional DRL algorithms. The extensive experiments show that our method substantially outperforms existing reinforcement learning-based and traditional methods on multiple classic benchmarks.}
}
@article{DREHER2022115401,
title = {AI agents envisioning the future: Forecast-based operation of renewable energy storage systems using hydrogen with Deep Reinforcement Learning},
journal = {Energy Conversion and Management},
volume = {258},
pages = {115401},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.115401},
url = {https://www.sciencedirect.com/science/article/pii/S0196890422001972},
author = {Alexander Dreher and Thomas Bexten and Tobias Sieker and Malte Lehna and Jonathan Schütt and Christoph Scholz and Manfred Wirsum},
keywords = {Hydrogen, Renewable energy storage, Energy management, Deep reinforcement learning, Dynamic programming},
abstract = {Hydrogen-based energy storage has the potential to compensate for the volatility of renewable power generation in energy systems with a high renewable penetration. The operation of these storage facilities can be optimized using automated energy management systems. This work presents a Reinforcement Learning-based energy management approach in the context of CO2-neutral hydrogen production and storage for an industrial combined heat and power application. The economic performance of the presented approach is compared to a rule-based energy management strategy as a lower benchmark and a Dynamic Programming-based unit commitment as an upper benchmark. The comparative analysis highlights both the potential benefits and drawbacks of the implemented Reinforcement Learning approach. The simulation results indicate a promising potential of Reinforcement Learning-based algorithms for hydrogen production planning, outperforming the lower benchmark. Furthermore, a novel approach in the scientific literature demonstrates that including energy and price forecasts in the Reinforcement Learning observation space significantly improves optimization results and allows the algorithm to take variable prices into account. An unresolved challenge, however, is balancing multiple conflicting objectives in a setting with few degrees of freedom. As a result, no parameterization of the reward function could be found that fully satisfied all predefined targets, highlighting one of the major challenges for Reinforcement Learning -based energy management algorithms to overcome.}
}
@article{KOPF201714902,
title = {Inverse Reinforcement Learning for Identification in Linear-Quadratic Dynamic Games},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {14902-14908},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.2537},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317334596},
author = {Florian Köpf and Jairo Inga and Simon Rothfuß and Michael Flad and Sören Hohmann},
keywords = {Game Theory, Identification, Inverse Reinforcement Learning, Inverse Optimal Control, Maximum Entropy, Shared Control},
abstract = {The theory of dynamic games has received considerable attention in a wide range of fields. While great effort has been made to develop new algorithms for finding Nash equilibria in dynamic games, the identification of cost functions has received little attention. We present an identification algorithm for linear quadratic dynamic games, a framework which can be applied in the field of shared control between a human and an automatic controller. In this application, the cost function describing human behavior is identified, taking into account the influence of the automation. Furthermore, we consider that human movement underlies certain variability by using a probabilistic Inverse Reinforcement Learning approach. As identification is performed in a single optimization step, the proposed method is suited for real-time applications. A simulation example shows that the algorithm successfully identifies the cost function of the first player which—in combination with the second player—reproduces the observed system output.}
}
@article{JOGLEKAR2022400,
title = {Deep Reinforcement Learning Based Adaptation of Pure-Pursuit Path-Tracking Control for Skid-Steered Vehicles},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {37},
pages = {400-407},
year = {2022},
note = {2nd Modeling, Estimation and Control Conference MECC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.11.216},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322028592},
author = {Ajinkya Joglekar and Sumedh Sathe and Nicola Misurati and Srivatsan Srinivasan and Matthias J. Schmid and Venkat Krovi},
keywords = {Adaptive control, Path tracking, Offroad systems, Pure pursuit, Deep reinforcement learning, Data-driven control},
abstract = {The growing need for autonomous vehicles in the offroad space raises certain complexities that need to be considered more rigorously in comparison to onroad vehicle automation. Popular path control frameworks in onroad autonomy deployments such as the pure-pursuit controller use geometric and kinematic motion models to generate reference trajectories. However in the offroad settings these controllers, despite their merits (low design and computation requirements), could compute dynamically infeasible trajectories as several of the nominal assumptions made by these models don't hold true when operating in a 2.5D terrain. Outside of the notable challenges such as uncertainties and non-linearities/disturbances introduced by the unknown/unmapped 2.5D terrains, additional complexities arise from the use of vehicle architectures such as the skid-steer that experience lateral skidding for achieving simple curvilinear motion. Additionally, linear models of skid-steer vehicles often consist of high modeling uncertainty which renders traditional linear optimal and robust control techniques inadequate given their sensitivity to modeling errors. Nonlinear MPC has emerged as an upgrade, but needs to overcome realtime deployment challenges (including slow sampling time, design complexity, and limited computational resources). This provides an unique opportunity to utilize data-driven adaptive control methods in tailored application spaces to implicitly learn and hence compensate for the unmodeled aspects of the robot operation. In this study, we build an adaptive control framework called Deep Reinforcement Learning based Adaptive Pure Pursuit (DRAPP) where the base structure is that of a geometric Pure-Pursuit (PP) algorithm which is adapted through a policy learned using Deep Reinforcement Learning (DRL). An additional law that enforces a mechanism to account for the rough terrain is added to the DRL policy to prioritize smoother reference-trajectory generation (and thereby more feasible trajectories for lower-level controllers). The adaptive framework converges quickly and generates smoother references relative to a pure 2D-kinematic path tracking controller. This work includes extensive simulations and a bench marking of the DRAPP framework against Nonlinear Model Predictive Control (NMPC) that is an alternate popular choice in literature for this application.}
}
@article{SALAZAR202339,
title = {Dynamic customer demand management: A reinforcement learning model based on real-time pricing and incentives},
journal = {Renewable Energy Focus},
volume = {46},
pages = {39-56},
year = {2023},
issn = {1755-0084},
doi = {https://doi.org/10.1016/j.ref.2023.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1755008423000443},
author = {Eduardo J. Salazar and Mauricio E. Samper and H. Daniel Patiño},
keywords = {Price-based demand response, Incentive-based demand response, Reinforcement Q-learning, K-Means algorithm},
abstract = {The demand response model proposed in this work offers a game-changing solution to the challenges posed by the unpredictability of renewable energy sources. By combining both pricing and incentives, this model significantly improves the accuracy of demand response strategies, leading to more effective modulation of customer demand. The real-time and time-of-use pricing options presented to customers incentivize them to actively increase or decrease their energy consumption, thereby contributing to the stability of the energy grid. This work also sheds light on the crucial role that characteristic parameters such as the internal or external coincidence factor play in the classification of customers using the k-means algorithm. The reinforcement learning method used in the model not only optimizes prices and incentives, but also ensures that both customers and energy distribution companies benefit equally. A sensitivity analysis of customer elasticity highlights the dynamic interplay between clustering and reinforcement learning algorithms and customer behavior, demonstrating the power and effectiveness of this model. With its innovative approach and cutting-edge techniques, this work sets a new model for demand response and makes a compelling case for the inclusion of prices and incentives in future models.}
}
@article{HAN2023138758,
title = {Deep reinforcement learning-based approach for dynamic disassembly scheduling of end-of-life products with stimuli-activated self-disassembly},
journal = {Journal of Cleaner Production},
volume = {423},
pages = {138758},
year = {2023},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2023.138758},
url = {https://www.sciencedirect.com/science/article/pii/S0959652623029165},
author = {Muyue Han and Lingxiang Yun and Lin Li},
keywords = {End-of-life management, Dynamic scheduling, Multi-agent deep reinforcement learning, Stimuli-activated self-disassembly},
abstract = {Remanufacturing is one of the most critical strategies for end-of-life product management to promote a circular economy; however, it has been seen very limited implementation due to the labor-intensive and time-consuming disassembly processes for component retrieval. The newly emerged 4D printing technology enables the fabrication of stimuli-responsive reconfigurable structures, outlining new ways to achieve non-destructive and simultaneous self-disassembly of components with different geometry. However, large uncertainties and increased process dynamics have also emerged directly pertaining to the real-time scheduling in disassembly lines with self-disassembly workstations, which the existing scheduling methods are not equipped to handle. In this study, a constrained multi-agent deep reinforcement learning approach is proposed to maximize the disassembly profit by dynamically changing the batch mixing ratios of different-sized components in self-disassembly workstations and adapting real-time scheduling to stochastic product quality, changes in operational sequences, and self-disassembly failures. The proposed approach is validated on a disassembly line for hand pulse detectors that contain heat-activated self-disassembly components. Numerical results show that the proposed achieves stable convergence under uncertainties, and the implementation of a dynamic batch mixing scheme in self-disassembly operations yields a substantial improvement in disassembly profit over the scheduling period. In addition, sensitivity analyses are conducted to evaluate the impacts of system uncertainties on the profitability of the disassembly line.}
}
@article{ZHOU202329,
title = {Digital twin application for reinforcement learning based optimal scheduling and reliability management enhancement of systems},
journal = {Solar Energy},
volume = {252},
pages = {29-38},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.01.042},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23000506},
author = {Jun Zhou and Mei Yang and Yong Zhan and Li Xu},
keywords = {Demand response, Photovoltaic, Energy storages, Smart home scheduling, Reinforcement learning, digital twin simulation},
abstract = {Increasing populations and economic expansion have substantially increased the energy requirements of residential consumers. Energy storage system (ESS) and distributed generation (DGs) are key tools for tackling this problem in smart homes. This study investigates the cost of electricity for residential consumers as a result of the combination of distributed photovoltaics (PVs) and ESSs for IoT-based smart home. Moreover, this paper examines energy management advantages due to bidirectional energy flow (H2G). In order to formulate the home energy management issue, PV and ESS end-user satisfaction limitations are taken into account. This study exploits a Q value-enabled reinforcement learning (RL) method to optimize home appliance scheduling (HAS) according to end-user priority. According to simulation outcomes, the suggested scheduling for household appliances performs well, and demand response (DR) measures have been implemented. It can be seen that the cost of electricity consumption as well as the uncertainty of the system have decreased in digital twin real-based application.}
}
@article{MOOSMANN2021881,
title = {Separating Entangled Workpieces in Random Bin Picking using Deep Reinforcement Learning},
journal = {Procedia CIRP},
volume = {104},
pages = {881-886},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.148},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121010465},
author = {Marius Moosmann and Marco Kulig and Felix Spenrath and Manuel Mönnig and Simon Roggendorf and Oliver Petrovic and Richard Bormann and Marco F. Huber},
keywords = {bin picking, deep reinforcement learning, entanglement, grasping, machine learning},
abstract = {Entangled workpiece situations often occur in random bin picking of chaotically stored objects and are a common source of problem in the bin picking process. Previous methods for averting this problem, such as randomly shaking the gripper over the bin, lead to decreasing production efficiency and an increase in cycle time. A promising new strategy uses supervised learning and deep neural networks to learn the separation. However, this approach requires a large amount of labeled data. To overcome this issue, this paper proposes a deep reinforcement learning approach to separate entangled workpieces and to minimize the setup effort.}
}
@article{BOUNI2022825,
title = {Towards an Efficient Recommender Systems in Smart Agriculture: A deep reinforcement learning approach},
journal = {Procedia Computer Science},
volume = {203},
pages = {825-830},
year = {2022},
note = {17th International Conference on Future Networks and Communications / 19th International Conference on Mobile Systems and Pervasive Computing / 12th International Conference on Sustainable Energy Information Technology (FNC/MobiSPC/SEIT 2022), August 9-11, 2022, Niagara Falls, Ontario, Canada},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.07.124},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922007293},
author = {Mohamed Bouni and Badr Hssina and Khadija Douzi and Samira Douzi},
keywords = {Agriculture, deep neural network, IoT, recommendation system, KNN, Random Tree, Naive Bayes, reinforcement learning, SVM},
abstract = {A profitable agriculture system is the fundamental foundation of a rising economy. Precise prediction of crop yield focuses primarily on agriculture research that has a significant effect on making decisions such as import-export, pricing, and distribution of specific crops. There is a severe need to utilize advanced technologies in order to improve yield quality and creation, anticipate crop yields, and study crop diseases/infections. The most prevalent issue among farmers is that they do not select the appropriate crop based on their soil needs. As a result, they see a significant decrease in production. In this paper, we presented a deep reinforcement learning (DRL)-based crop classification system for precision agriculture selection to solve the farmers' dilemma. DRL-based advanced agriculture techniques eliminate bad options and boost production in the crop recommendation system. We compared the proposed recommendation system with the various machine learning algorithms, such as Random Tree, Naive Bayes, and K-Nearest Neighbor, for a site-specific crop with effective accuracy.}
}
@article{AISSANI20091102,
title = {Multi-agent reinforcement learning for adaptive scheduling: application to multi-site company},
journal = {IFAC Proceedings Volumes},
volume = {42},
number = {4},
pages = {1102-1107},
year = {2009},
note = {13th IFAC Symposium on Information Control Problems in Manufacturing},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20090603-3-RU-2001.0280},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016339428},
author = {N. Aissani and D. Trentesaux and B. Beldjilali},
keywords = {manufacturing control, scheduling algorithms, multi-agent system, reinforcement learning},
abstract = {In recent years, most companies have resorted to multi-site organization in an effort to improve their competitiveness and to adapt to current conditions. In this article, we propose a model for adaptive scheduling in multi-site companies. We adopt a multi-agent approach in which intelligent agents have reactive learning ability. This allows them to make accurate short-term decisions. Our model is implemented on a 3-tier architecture that ensures the security of the data exchanged between the various company sites. Experimentations on a real case study demonstrate the applicability and the effectiveness of our model concerning both optimality and reactivity.}
}
@article{SVETOZAREVIC2022118127,
title = {Data-driven control of room temperature and bidirectional EV charging using deep reinforcement learning: Simulations and experiments},
journal = {Applied Energy},
volume = {307},
pages = {118127},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.118127},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921014045},
author = {B. Svetozarevic and C. Baumann and S. Muntwiler and L. {Di Natale} and M.N. Zeilinger and P. Heer},
keywords = {Data-driven building control, Deep reinforcement learning, Room temperature control, Thermal comfort, EV charging, Recurrent neural networks},
abstract = {The control of modern buildings is a complex multi-loop problem due to the integration of renewable energy generation, storage devices, and electric vehicles (EVs). Additionally, it is a complex multi-criteria problem due to the need to optimize overall energy use while satisfying users’ comfort. Both conventional rule-based (RB) controllers, which are difficult to apply in multi-loop settings, and advanced model-based controllers, which require an accurate building model, cannot fulfil the requirements of the building automation industry to solve this problem optimally at low development and commissioning costs. This work presents a fully data-driven pipeline to obtain an optimal control policy from historical building and weather data, thus avoiding the need for complex physics-based modelling. We demonstrate the potential of this method by jointly controlling a room temperature and an EV to minimize the cost of electricity while retaining the comfort of the occupants. We model the room temperature with a recurrent neural network and use it as a simulation environment to learn a deep reinforcement learning (DRL) control policy. It achieves on average 17% energy savings and 19% better comfort satisfaction than a standard RB room temperature controller. When a bidirectional EV is connected additionally and a two-tariff electricity pricing is applied, it successfully leverages the battery and decreases the overall cost of electricity. Finally, we deployed it on a real building, where it achieved up to 30% energy savings while maintaining similar comfort levels compared to a conventional RB room temperature controller.}
}
@article{CHANG20208181,
title = {Cascade Attribute Network: Decomposing Reinforcement Learning Control Policies using Hierarchical Neural Networks},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {8181-8186},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2317},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320329840},
author = {Haonan Chang and Zhuo Xu and Masayoshi Tomizuka},
keywords = {Reinforcement learning control, Deep Learning, Artificial Intelligence},
abstract = {Reinforcement learning methods have been developed to achieve great success in training control policies in various automation tasks. However, a main challenge of the wider application of reinforcement learning in practical automation is that the training process is hard and the pretrained policy networks are hardly reusable in other similar cases. To address this problem, we propose the cascade attribute network (CAN), which utilizes its hierarchical structure to decompose a complicated control policy in terms of the requirement constraints, which we call attributes, encoded in the control tasks. We validated the effectiveness of our proposed method on two robot control scenarios with various add-on attributes. For some control tasks with more than one add-on attribute attribute, by directly assembling the attribute modules in cascade, the CAN can provide ideal control policies in a zero-shot manner.}
}
@article{KOSASIH20221539,
title = {Reinforcement Learning Provides a Flexible Approach for Realistic Supply Chain Safety Stock Optimisation},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1539-1544},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.609},
url = {https://www.sciencedirect.com/science/article/pii/S240589632201919X},
author = {Edward Elson Kosasih and Alexandra Brintrup},
keywords = {Supply Chain, Safety Stock, Artificial Intelligence, Inventory Control, Simulation Optimisation, Reinforcement Learning},
abstract = {Although safety stock optimisation has been studied for more than 60 years, most companies still use simplistic means to calculate necessary safety stock levels, partly due to the mismatch between existing analytical methods’ emphases on deriving provably optimal solutions and companies’ preferences to sacrifice optimal results in favour of more realistic problem settings. A newly emerging method from the field of Artificial Intelligence (AI), namely Reinforcement Learning (RL), offers promise in finding optimal solutions while accommodating more realistic problem features. Unlike analytical-based models, RL treats the problem as a black-box simulation environment mitigating against the problem of oversimplifying reality. As such, assumptions on stock keeping policy can be relaxed and a higher number of problem variables can be accommodated. While RL has been popular in other domains, its applications in safety stock optimisation remain scarce. In this paper we investigate three RL methods, namely, Q-Learning, Advantage Actor-Critic and Multi-agent Advantage Actor-Critic for optimising safety stock in a linear chain of independent agents. We find that RL can simultaneously optimise both safety stock level and order quantity parameters of an inventory policy, unlike classical safety stock optimisation models where only safety stock level is optimised while order quantity is predetermined based on simple rules. This allows RL to model more complex supply chain procurement behaviour. However, RL takes longer time to arrive at solutions, necessitating future research on identifying and improving trade-offs between the use of AI and mathematical models are needed.}
}
@article{JEONG20231995,
title = {Processing parameters optimization in hot forging of AISI 4340 steel using instability map and reinforcement learning},
journal = {Journal of Materials Research and Technology},
volume = {23},
pages = {1995-2009},
year = {2023},
issn = {2238-7854},
doi = {https://doi.org/10.1016/j.jmrt.2023.01.106},
url = {https://www.sciencedirect.com/science/article/pii/S2238785423001060},
author = {Ho Young Jeong and Joonhee Park and Yosep Kim and Sang Yun Shin and Naksoo Kim},
keywords = {Hot deformation, Instability map, Hot forging, Reinforcement learning, AISI 4340},
abstract = {High-temperature compression tests of the AISI 4340 alloy were conducted at temperatures of 900–1200 °C, strain rates of 0.01–10 s−1, and true strain ranges between 0.01 and 1.00 to explore the hot deformation properties. The experiment result has been used to obtain stress-strain curves of the material. The instability map is built at each true strain based on the dynamic materials model and Prasad's instability criterion. On the top of the instability map, the deformation behavior obtained from finite element analysis (FEA) is schematically investigated in the deformation flow instability. A reinforcement learning-based optimization algorithm has been developed to derive the optimal processing parameter, including workpiece temperature and stroke speed in the manufacturing process. Q-learning was used to learn and explore the solution environment to achieve the optimum processing parameters that prevent defects from instability traits. The experiment results show that the algorithm successfully avoided the unstable domains. Optical photography and electron backscatter diffraction (EBSD) has verified the proposed approach's practicality with experimental observations of flow instability signs.}
}
@article{PALOMBARINI2019231,
title = {Closed-loop Rescheduling using Deep Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {1},
pages = {231-236},
year = {2019},
note = {12th IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems DYCOPS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.06.067},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319301521},
author = {Jorge A. Palombarini and Ernesto C. Martínez},
keywords = {Real-time Rescheduling, Manufacturing Systems, Uncertainty, Deep Reinforcement Learning},
abstract = {Modern socio-technical manufacturing systems are characterized by high levels of variability which gives rise to poor predictability of environmental conditions at the shop-floor. Therefore, a closed-loop rescheduling strategy for handling unforeseen events and unplanned disturbances has become a key element for any real-time disruption management strategy in order to guarantee highly efficient production in uncertain and dynamic conditions. In this work, a real-time rescheduling task is modeled as a closed-loop control problem in which an artificial intelligent agent implements control knowledge generated offline using a schedule simulator to learn schedule repair policies directly from high-dimensional sensory inputs. The rescheduling control knowledge is stored in a deep Q-network, which is used closed-loop to select repair actions to achieve a small set of repaired goal states. The network is trained using the deep Q-learning algorithm with experience replay over a variety of simulated transitions between schedule states based on color-rich Gantt chart images and negligible prior knowledge as inputs. An industrial example is discussed to highlight that the proposed approach enables end-to-end learning of comprehensive rescheduling policies and encoding plant-specific knowledge that can be understood by human experts.}
}
@article{KOCH2023105477,
title = {Automated function development for emission control with deep reinforcement learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {117},
pages = {105477},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105477},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622004675},
author = {Lucas Koch and Mario Picerno and Kevin Badalian and Sung-Yong Lee and Jakob Andert},
keywords = {Automation, Reinforcement learning, Embedded systems, Emission control},
abstract = {The conventional automotive development process for embedded systems today is still time- and data-inefficient, and requires highly experienced software developers and calibration engineers. Consequently, it is cost-intensive and at the same time prone to sub-optimal solutions. Reinforcement Learning offers a promising approach to address these challenges. The evolved agents have proven their ability to master complex control tasks in a close-to-optimal manner without any human intervention, but the training procedures are hardly compatible with current development processes. As a result, Reinforcement Learning has rarely been used in powertrain development until now. This work describes an integration of Reinforcement Learning in the embedded system development process to automatically train and deploy agents in transient driving cycles. Using the example of exhaust gas re-circulation control for a Diesel engine, an agent is successfully trained in a fully virtualized environment, achieving emission reductions of up to 10% in comparison to a state-of-the-art controller. Further investigations are carried out to quantify the impact of the driving cycle and ambient conditions on the agent’s performance. To demonstrate the transferability between different levels of virtualization, the experienced agent is then tested in closed-loop with a real hardware controller to operate the physical actuator. By confirming the reproducibility of the learned strategy on real hardware, this article serves as proof-of-concept for a sustainable, Reinforcement Learning based path to automatically develop embedded controllers for complex control problems.}
}
@article{ZHAO2023180,
title = {Dynamic sparse coding-based value estimation network for deep reinforcement learning},
journal = {Neural Networks},
volume = {168},
pages = {180-193},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023005063},
author = {Haoli Zhao and Zhenni Li and Wensheng Su and Shengli Xie},
keywords = {Deep reinforcement learning, Value estimation network, Dynamic sparse coding},
abstract = {Deep Reinforcement Learning (DRL) is one powerful tool for varied control automation problems. Performances of DRL highly depend on the accuracy of value estimation for states from environments. However, the Value Estimation Network (VEN) in DRL can be easily influenced by the phenomenon of catastrophic interference from environments and training. In this paper, we propose a Dynamic Sparse Coding-based (DSC) VEN model to obtain precise sparse representations for accurate value prediction and sparse parameters for efficient training, which is not only applicable in Q-learning structured discrete-action DRL but also in actor–critic structured continuous-action DRL. In detail, to alleviate interference in VEN, we propose to employ DSC to learn sparse representations for accurate value estimation with dynamic gradients beyond the conventional ℓ1 norm that provides same-value gradients. To avoid influences from redundant parameters, we employ DSC to prune weights with dynamic thresholds more efficiently than static thresholds like ℓ1 norm. Experiments demonstrate that the proposed algorithms with dynamic sparse coding can obtain higher control performances than existing benchmark DRL algorithms in both discrete-action and continuous-action environments, e.g., over 25% increase in Puddle World and about 10% increase in Hopper. Moreover, the proposed algorithm can reach convergence efficiently with fewer episodes in different environments.}
}
@article{LIU2021101360,
title = {Deep reinforcement learning-based safe interaction for industrial human-robot collaboration using intrinsic reward function},
journal = {Advanced Engineering Informatics},
volume = {49},
pages = {101360},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101360},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001130},
author = {Quan Liu and Zhihao Liu and Bo Xiong and Wenjun Xu and Yang Liu},
keywords = {Industrial human-robot collaboration, Collision avoidance, Deep reinforcement learning, Intrinsic reward function},
abstract = {Aiming at human-robot collaboration in manufacturing, the operator's safety is the primary issue during the manufacturing operations. This paper presents a deep reinforcement learning approach to realize the real-time collision-free motion planning of an industrial robot for human-robot collaboration. Firstly, the safe human-robot collaboration manufacturing problem is formulated into a Markov decision process, and the mathematical expression of the reward function design problem is given. The goal is that the robot can autonomously learn a policy to reduce the accumulated risk and assure the task completion time during human-robot collaboration. To transform our optimization object into a reward function to guide the robot to learn the expected behaviour, a reward function optimizing approach based on the deterministic policy gradient is proposed to learn a parameterized intrinsic reward function. The reward function for the agent to learn the policy is the sum of the intrinsic reward function and the extrinsic reward function. Then, a deep reinforcement learning algorithm intrinsic reward-deep deterministic policy gradient (IRDDPG), which is the combination of the DDPG algorithm and the reward function optimizing approach, is proposed to learn the expected collision avoidance policy. Finally, the proposed algorithm is tested in a simulation environment, and the results show that the industrial robot can learn the expected policy to achieve the safety assurance for industrial human-robot collaboration without missing the original target. Moreover, the reward function optimizing approach can help make up for the designed reward function and improve policy performance.}
}
@article{WANG20222144,
title = {Dynamic Selection of Priority Rules Based on Deep Reinforcement Learning for Rescheduling of RCPSP},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2144-2149},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322020341},
author = {Teng Wang and Wei Cheng and Yahui Zhang and Xiaofeng Hu},
keywords = {reinforcement learning, project rescheduling, priority rule, transfer learning},
abstract = {Due to the uncertainties in the project execution process, the original plan often cannot be carried out correctly and needs to be rescheduled to repair the plan. In this case, rescheduling is required to repair the plan. Priority rules are the most common method for rescheduling because of their known advantages such as simplicity and fast. Although numerous papers have conducted comparative studies on different priority rules, managers often do not know which rules should be used for project rescheduling in specific situations. In this paper, we propose a reinforcement learning based approach for adaptive selection of priority rules in dynamic environments, which includes off-line phase and on-line phase. Reinforcement learning is used to learn scheduling knowledge and obtain the scheduling model in the off-line phase. Transfer learning can be used to reuse scheduling models between different cases in this phase. In the online phase, the scheduling model is used to adaptively select appropriate rules for rescheduling when the initial plan is infeasible due to unexpected disturbance. Experiments show that the proposed method has better rescheduling performance than other heuristic algorithms based on priority rules under different disturbances. Besides, we find that the time consumption of off-line training can be greatly reduced by using transfer learning, which also proves that our method can indeed learn some essential scheduling knowledge.}
}
@article{GAO2023129092,
title = {A health-aware energy management strategy for fuel cell hybrid electric UAVs based on safe reinforcement learning},
journal = {Energy},
volume = {283},
pages = {129092},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.129092},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223024866},
author = {Qinxiang Gao and Tao Lei and Wenli Yao and Xingyu Zhang and Xiaobin Zhang},
keywords = {Energy management strategy, Fuel cell, Hybrid electric UAV, Safe reinforcement learning, Degradation},
abstract = {Energy management strategies (EMSs) are crucial for the hydrogen economy and energy component lifetimes of fuel cell hybrid electric unmanned aerial vehicles (UAVs). Reinforcement learning (RL)-based schemes have been a hotspot for EMSs, but most of RL-based EMSs focus on the energy-saving performance and rarely consider energy component durability and safe exploration. This paper proposes a health-aware energy management strategy based on a safe RL framework to minimize the overall flight cost and achieve safe operation of UAVs. In this framework, a universal three-dimensional environment that integrates the UAV kinematics and dynamics model is developed. In addition, wind disturbances and random loading of the mission payload during flight are considered for robust training. The energy management problem is formulated as a constrained Markov decision process, where both hydrogen consumption and energy component degradation are incorporated in the multi-objective reward function. A safety optimizer is then designed to satisfy operation constraints by correcting the action through analytical optimization. The results indicate that the safety of the explored action is guaranteed, maintaining zero constraint violations in both training and real-time control scenarios. Compared with other RL-based methods, the proposed method had better convergence capability and reduced the training time. Furthermore, the simulation showed that the proposed method can reduce the total flight cost and fuel cell degradation by 14.6% and 15.3%, respectively, compared with the online benchmark method.}
}
@article{TAN2023,
title = {Strengthening Network Slicing for Industrial Internet with Deep Reinforcement Learning},
journal = {Digital Communications and Networks},
year = {2023},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2023.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S2352864823001153},
author = {Yawen Tan and Jiadai Wang and Jiajia Liu},
keywords = {Industrial Internet, network slicing, deep reinforcement learning, graph neural network},
abstract = {Industrial Internet combines the industrial system with Internet connectivity to build a new manufacturing and service system covering the entire industry chain and value chain. Its highly heterogeneous network structure and diversified application requirements call for the applying of network slicing technology. Guaranteeing robust network slicing is essential for Industrial Internet, but it faces the challenge of complex slice topologies caused by the intricate interaction relationships among Network Functions (NFs) composing the slice. Existing works have not concerned the strengthening problem of industrial network slicing regarding its complex network properties. Towards this end, we aim to study this issue by intelligently selecting a subset of most valuable NFs with the minimum cost to satisfy the strengthening requirements. State-of-the-art AlphaGo series of algorithms and the advanced graph neural network technology are combined to build the solution. Simulation results demonstrate the superior performance of our scheme compared to the benchmark schemes.}
}
@article{DING2023e17919,
title = {A reinforcement learning method for optimal control of oil well production using cropped well group samples},
journal = {Heliyon},
volume = {9},
number = {7},
pages = {e17919},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e17919},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023051277},
author = {Yangyang Ding and Xiang Wang and Xiaopeng Cao and Huifang Hu and Yahui Bu},
keywords = {Production optimization, Reinforcement learning, Image enhancement, Optimal control, Generalization capability},
abstract = {The influence of geological development factors such as reservoir heterogeneity needs to be comprehensively considered in the determination of oil well production control strategy. In the past, many optimization algorithms are introduced and coupled with numerical simulation for well control problems. However, these methods require a large number of simulations, and the experience of these simulations is not preserved by the algorithm. For each new reservoir, the optimization algorithm needs to start over again. To address the above problems, two reinforcement learning methods are introduced in this research. A personalized Deep Q-Network (DQN) algorithm and a personalized Soft Actor-Critic (SAC)algorithm are designed for optimal control determination of oil wells. The inputs of the algorithms are matrix of reservoir properties, including reservoir saturation, permeability, etc., which can be treated as images. The output is the oil well production strategy. A series of samples are cut from two different reservoirs to form a dataset. Each sample is a square area that takes an oil well at the center, with different permeability and saturation distribution, and different oil-water well patterns. Moreover, all samples are expanded by using image enhancement technology to further increase the number of samples and improve the coverage of the samples to the reservoir conditions. During the training process, two training strategies are investigated for each personalized algorithm. The second strategy uses 4 times more samples than the first strategy. At last, a new set of samples is designed to verify the model’s accuracy and generalization ability. Results show that both the trained DQN and SAC models can learn and store historical experience, and push appropriate control strategies based on reservoir characteristics of new oil wells. The agreement between the optimal control strategy obtained by both algorithms and the global optimal strategy obtained by the exhaustive method is more than 95%. The personalized SAC algorithm shows better performance compared to the personalized DQN algorithm. Compared to the traditional Particle Swarm Optimization (PSO), the personalized models were faster and better at capturing complex patterns and adapting to different geological conditions, making them effective for real-time decision-making and optimizing oil well production strategies. Since a large amount of historical experience has been learned and stored in the algorithm, the proposed method requires only 1 simulation for a new oil well control optimization problem, which showing the superiority in computational efficiency.}
}
@article{ZHANG2022109766,
title = {Training effective deep reinforcement learning agents for real-time life-cycle production optimization},
journal = {Journal of Petroleum Science and Engineering},
volume = {208},
pages = {109766},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109766},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521013887},
author = {Kai Zhang and Zhongzheng Wang and Guodong Chen and Liming Zhang and Yongfei Yang and Chuanjin Yao and Jian Wang and Jun Yao},
keywords = {Production optimization, Deep reinforcement learning, Optimal control, Goal-directed interaction, Model free},
abstract = {Life-cycle production optimization aims to obtain the optimal well control scheme at each time control step to maximize financial profit and hydrocarbon production. However, searching for the optimal policy under the limited number of simulation evaluations is a challenging task. In this paper, a novel production optimization method is presented, which maximizes the net present value (NPV) over the entire life-cycle and achieves real-time well control scheme adjustment. The proposed method models the life-cycle production optimization problem as a finite-horizon Markov decision process (MDP), where the well control scheme can be viewed as sequence decisions. Soft actor-critic, known as the state-of-the-art model-free deep reinforcement learning (DRL) algorithm, is subsequently utilized to train DRL agents that can solve the above MDP. The DRL agent strives to maximize long-term NPV rewards as well as the control scheme randomness by training a stochastic policy that maps reservoir states to well control variables and an action-value function that estimates the objective value of the current policy. Since the trained policy is an explicit function structure, the DRL agent can adjust the well control scheme in real-time under different reservoir states. Different from most existing methods that introduce task-specific sensitive parameters or construct complex supplementary structures, the DRL agent learns adaptively by executing goal-directed interactions with an uncertain reservoir environment and making use of accumulated well control experience, which is similar to the actual field well control mode. The key insight here is that the DRL method's ability to utilize gradients information (well-control experience) for higher sample efficiency. The simulation results based on two reservoir models indicate that compared to other optimization methods, the proposed method can attain higher NPV and access excellent performance in terms of oil displacement.}
}
@article{KANNAN2023108258,
title = {An Efficient Reinforcement Learning Approach to Optimal Control with Application to Biodiesel Production},
journal = {Computers & Chemical Engineering},
volume = {174},
pages = {108258},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108258},
url = {https://www.sciencedirect.com/science/article/pii/S009813542300128X},
author = {Shiam Kannan and Urmila Diwekar},
keywords = {Optimal control, biodiesel production, temperature profile, reinforcement learning, Hammersley sequence sampling},
abstract = {Optimal control problems are one of the most challenging problems in optimization. This paper presents a new and efficient Reinforcement Learning approach to optimal control problems based on the Batch Q-learning algorithm. To improve the convergence of the RL algorithm, we use k-dimensional uniformity of advanced sampling procedures, namely employing Hamersley sequences (HSS). HSS is used to randomly sample the state variables and discrete controls from the action space for the RL optimal control problem. The Neural-fitted Q-iterative algorithm is applied to solve an optimal control problem for a first-order state dynamical system. A real-world application of optimal temperature profile determination for biodiesel production in a batch reactor is presented. We present the comparison of our HSS-RL algorithm with that of the maximum principle.}
}
@article{BAO2023101399,
title = {A collaborative iterated greedy algorithm with reinforcement learning for energy-aware distributed blocking flow-shop scheduling},
journal = {Swarm and Evolutionary Computation},
volume = {83},
pages = {101399},
year = {2023},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2023.101399},
url = {https://www.sciencedirect.com/science/article/pii/S2210650223001724},
author = {Haizhu Bao and Quanke Pan and Rubén Ruiz and Liang Gao},
keywords = {Energy-aware scheduling, Flow-shop, Q-learning, Iterated greedy, Multi-objective optimization},
abstract = {Energy-aware scheduling has attracted increasing attention mainly due to economic benefits as well as reducing the carbon footprint at companies. In this paper, an energy-aware scheduling problem in a distributed blocking flow-shop with sequence-dependent setup times is investigated to minimize both makespan and total energy consumption. A mixed-integer linear programming model is constructed and a cooperative iterated greedy algorithm based on Q-learning (CIG) is proposed. In the CIG, a top-level Q-learning is focused on enhancing the utilization ratio of machines to minimize makespan by finding a scheduling policy from four sequence-related operations. A bottom-level Q-learning is centered on improving energy efficiency to reduce total energy consumption by learning the optimal speed governing policy from four speed-related operations. According to the structure characteristics of solutions, several properties are explored to design an energy-saving strategy and acceleration strategy. The experimental results and statistical analysis prove that the CIG is superior to the state-of-the-art competitors with improvement percentages of 20.16 % over 2880 instances from the well-known benchmark set in the literature.}
}
@article{JIANG2023110768,
title = {Reinforcement learning and cooperative H∞ output regulation of linear continuous-time multi-agent systems},
journal = {Automatica},
volume = {148},
pages = {110768},
year = {2023},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2022.110768},
url = {https://www.sciencedirect.com/science/article/pii/S0005109822006343},
author = {Yi Jiang and Weinan Gao and Jin Wu and Tianyou Chai and Frank L. Lewis},
keywords = {Cooperative  output regulation, Multi-agent system, Value iteration, Reinforcement learning},
abstract = {This paper proposes a novel control approach to solve the cooperative H∞ output regulation problem for linear continuous-time multi-agent systems (MASs). Different from existing solutions to cooperative output regulation problems, a distributed feedforward-feedback controller is developed to achieve asymptotic tracking and reject both modeled and unmodeled disturbances. The feedforward control policy is computed via solving regulator equations, and the optimal feedback control policy is obtained through handling a zero-sum game. Instead of relying on the knowledge of system matrices in the state equations of the followers’ dynamics and initial stabilizing feedback control gains, a value iteration (VI) algorithm is proposed to learn the optimal feedback control gain and feedforward control gain using online data. To the best of our knowledge, this paper is the first to show that the proposed VI algorithm can approximate the solution to continuous-time game algebraic Riccati equations with guaranteed convergence. Finally, the numerical analysis is provided to show the effectiveness of the proposed approach.}
}
@article{SHIUE2018604,
title = {Real-time scheduling for a smart factory using a reinforcement learning approach},
journal = {Computers & Industrial Engineering},
volume = {125},
pages = {604-614},
year = {2018},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S036083521830130X},
author = {Yeou-Ren Shiue and Ken-Chuan Lee and Chao-Ton Su},
keywords = {Machine learning, Q-learning, Real-time scheduling, Reinforcement learning, Shop floor control},
abstract = {Previous studies of the real-time scheduling (RTS) problem domain indicate that using a multiple dispatching rules (MDRs) strategy for the various zones in the system can enhance the production performance to a greater extent than using a single dispatching rule (SDR) over a given scheduling interval for all the machines in the shop floor control system. This approach is feasible but the drawback of the previously proposed MDRs method is its inability to respond to changes in the shop floor environment. The RTS knowledge base (KB) is not static, so it would be useful to establish a procedure that maintains the KB incrementally if important changes occur in the manufacturing system. To address this issue, we propose reinforcement learning (RL)-based RTS using the MDRs mechanism by incorporating two main mechanisms: (1) an off-line learning module and (2) a Q-learning-based RL module. According to various performance criteria over a long period, the proposed approach performs better than the previously proposed MDRs method, the machine learning-based RTS using the SDR approach, and heuristic individual dispatching rules.}
}
@article{KRASHENINNIKOVA20198,
title = {Reinforcement learning for pricing strategy optimization in the insurance industry},
journal = {Engineering Applications of Artificial Intelligence},
volume = {80},
pages = {8-19},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619300107},
author = {Elena Krasheninnikova and Javier García and Roberto Maestre and Fernando Fernández},
keywords = {Pricing strategy optimization, Reinforcement learning},
abstract = {Pricing is a fundamental problem in the banking sector, and is closely related to a number of financial products such as credit scoring or insurance. In the insurance industry an important question arises, namely: how can insurance renewal prices be adjusted? Such an adjustment has two conflicting objectives. On the one hand, insurers are forced to retain existing customers, while on the other hand insurers are also forced to increase revenue. Intuitively, one might assume that revenue increases by offering high renewal prices, however this might also cause many customers to terminate their contracts. Contrarily, low renewal prices help retain most existing customers, but could negatively affect revenue. Therefore, adjusting renewal prices is a non-trivial problem for the insurance industry. In this paper, we propose a novel modelization of the renewal price adjustment problem as a sequential decision problem and, consequently, as a Markov decision process (MDP). In particular, this study analyzes two different strategies to carry out this adjustment. The first is about maximizing revenue analyzing the effect of this maximization on customer retention, while the second is about maximizing revenue subject to the client retention level not falling below a given threshold. The former case is related to MDPs with a single criterion to be optimized. The latter case is related to Constrained MDPs (CMDPs) with two criteria, where the first one is related to optimization, while the second is subject to a constraint. This paper also contributes with the resolution of these models by means of a model-free Reinforcement Learning algorithm. Results have been reported using real data from the insurance division of BBVA, one of the largest Spanish companies in the banking sector.}
}
@article{KARIMIMAJD201787,
title = {A reinforcement learning methodology for a human resource planning problem considering knowledge-based promotion},
journal = {Simulation Modelling Practice and Theory},
volume = {79},
pages = {87-99},
year = {2017},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2015.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X15001082},
author = {Amir-Mohsen Karimi-Majd and Masoud Mahootchi and Amir Zakery},
keywords = {Reinforcement learning, Production-inventory control, Human resource planning, Stochastic dynamic programming, Knowledge-intensive},
abstract = {This paper addresses a combined problem of human resource planning (HRP) and production-inventory control for a high-tech industry, wherein the human resource plays a critical role. The main characteristics of this resource are the levels of “knowledge” and the learning process. The learning occurs during the production process in which a worker can promote to the upper knowledge level. Workers in upper levels have more productivity in the production. The objective is to maximize the expected profit by deciding on the optimal numbers of workers in various knowledge levels to fulfill both production and training requirement. As taking an action affects next periods’ decisions, the main problem is to find the optimal hiring policy of non-skilled workers in long-time horizon. Thus, we develop a reinforcement learning (RL) model to obtain the optimal decision for hiring workers under the demand uncertainty. The proposed interval-based policy of our RL model, in which for each state there are multiple choices, makes it more flexible. We also embed some managerial issues such as layoff and overtime-working hours into the model. To evaluate the proposed methodology, stochastic dynamic programming (SDP) and a conservative method implemented in a real case study are used. We study all these methods in terms of four criteria: average obtained profit, average obtained cost, the number of new-hired workers, and the standard deviation of hiring policies. The numerical results confirm that our developed method end up with satisfactory results compared to two other approaches.}
}
@article{ZHANG2022110009,
title = {KAiPP: An interaction recommendation approach for knowledge aided intelligent process planning with reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {258},
pages = {110009},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110009},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122011029},
author = {Chao Zhang and Guanghui Zhou and Jingjing Li and Tianyu Qin and Kai Ding and Fengtian Chang},
keywords = {Intelligent process planning, Interaction recommendation, Reinforcement learning, Knowledge reuse, Industry 4.0},
abstract = {Effective reuse of historical process knowledge has been one of the most important technical roadmaps driving traditional human experience-based process planning all the way to intelligent process planning in the context of Industry 4.0. However, effective knowledge reuse in process planning is still made difficult nowadays due to the lack of means to capture the technologists’ preference embedded in the interaction history of technologists with the system, and the mutual influence of process decision-making activities for future knowledge recommendations. To bridge the gap, this work takes in-depth integration of deep learning and reinforcement learning in process planning and proposes a novel dynamic interaction recommendation approach for knowledge aided intelligent process planning (KAiPP) towards Industry 4.0. This work first formalizes KAiPP as a sequential interaction knowledge recommendation scenario, to which a Markov decision process is employed to model the KAiPP scenario. On that basis, a knowledge-aware reinforcement learning model is designed to learn a KAiPP agent through interacting with the KAiPP environment for effective interaction knowledge recommendation in process planning. Here, the technologists’ preference-task information model and dynamic knowledge sub-graph are elaborated as the KAiPP environment to capture the preference of the technologist and mutual influence of process decision-making activities. Finally, experimental results show that the proposed approach achieves a very high recommendation performance that outperforms the state-of-the-art approaches in the process planning domain. In addition, a thus developed KAiPP prototype system provides a reference for industrial implementation of the approach.}
}
@article{YANG2022,
title = {Reinforcement learning based edge computing in B5G},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000293},
author = {Jiachen Yang and Yiwen Sun and Yutian Lei and Zhuo Zhang and Yang Li and Yongjun Bao and Zhihan Lv},
keywords = {Reinforcement learning, Edge computing, Beyond 5G, Vehicle-to-pedestrian},
abstract = {The development of communication technology will promote the application of Internet of things, and Beyond 5G will become a new technology promoter. At the same time, Beyond 5G will become one of the important supports for the development of edge computing technology. This paper proposes a communication task allocation algorithm based on deep reinforcement learning for vehicle-to-pedestrian communication scenarios in edge computing. Through trial and error learning of agent, the optimal spectrum and power can be determined for transmission without global information, so as to balance the communication between vehicle-to-pedestrian and vehicle-to-infrastructure. The results show that the agent can effectively improve vehicle-to-infrastructure communication rate as well as meeting the delay constraints on the vehicle-to-pedestrian link.}
}
@article{RUPPRECHT202213,
title = {A survey for deep reinforcement learning in markovian cyber–physical systems: Common problems and solutions},
journal = {Neural Networks},
volume = {153},
pages = {13-36},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022001873},
author = {Timothy Rupprecht and Yanzhi Wang},
keywords = {Deep reinforcement learning, Cyber–physical systems, Motor control, Resource allocation, HVAC},
abstract = {Deep Reinforcement Learning (DRL) is increasingly applied in cyber–physical systems for automation tasks. It is important to record the developing trends in DRL’s applications to help researchers overcome common problems using common solutions. This survey investigates trends seen within two applied settings: motor control tasks, and resource allocation tasks. The common problems include intractability of the action space, or state space, as well as hurdles associated with the prohibitive cost of training systems from scratch in the real-world. Real-world training data is sparse and difficult to derive and training in real-world can damage real-world learning systems. Researchers have provided a set of common as well as unique solutions. Tackling the problem of intractability, researchers have succeeded in guiding network training with handcrafted reward functions, auxiliary learning, and by simplifying the state or action spaces before performing transfer learning to more complex systems. Many state-of-the-art algorithms reformulate problems to use multi-agent or hierarchical learning to reduce the intractability of the state or action spaces for a single agent. Common solutions to the prohibitive cost of training include using benchmarks and simulations. This requires a shared feature space common to both simulation and the real world; without that you introduce what is known as the reality gap problem. This is the first survey, to our knowledge, that studies DRL as it is applied in the real world at this scope. It is our hope that the common solutions surveyed become common practice.}
}
@article{HUA2023121128,
title = {Digital twin based reinforcement learning for extracting network structures and load patterns in planning and operation of distribution systems},
journal = {Applied Energy},
volume = {342},
pages = {121128},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121128},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923004920},
author = {Weiqi Hua and Bruce Stephen and David C.H. Wallom},
keywords = {Digital twin, Distribution network, Fitted Q-iteration, Load pattern, Network configuration, Reinforcement learning},
abstract = {Low voltage distribution networks deliver power to the last mile of the network, but are often legacy assets from a time when low carbon technologies, e.g., electrified heat, storage, and electric vehicles, were not envisaged. Furthermore, exploiting emerging data from distribution networks to provide decision support for adapting planning and operational strategies with system transitions presents a challenge. To overcome these challenges, this paper proposes a novel application of digital twins based reinforcement learning to improve decision making by a distribution system operator, with key metrics of predictability, responsiveness, interoperability, and automation. The power system states, i.e., network configurations, technological combinations, and load patterns, are captured via a convolutional neural network, chosen for its pattern recognition capability with high-dimensional inputs. The convolutional neural networks are iteratively trained through the fitted Q-iteration algorithm, as a batch mode reinforcement learning, to adapt the planning and operational decisions with the dynamic system transitions. Case studies demonstrate the effectiveness of the proposed model by reducing 50% of the investment cost when the system transitions towards the winter and maintaining the power loss and loss of load within 5% compared to the benchmark optimisation. Doubled power consumption was observed in winter under future energy scenarios due to the electrification of heat. The trained model can accurately adapt optimal decisions according to the system changes while reducing the computational time of solving optimisation problems, for a range of scales of distribution systems, demonstrating its potential for scalable deployment by a system operator.}
}
@article{LIU20221459,
title = {A Reinforcement Learning Variable Neighborhood Search for the Robust Dynamic Bayesian Network Optimization Problem under the Supply Chain Ripple Effect},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1459-1464},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.596},
url = {https://www.sciencedirect.com/science/article/pii/S240589632201905X},
author = {Ming Liu and Hao Tang and Feng Chu and Feifeng Zheng and Chengbin Chu},
keywords = {Ripple effect, Robust DBN, Variable neighborhood search, Reinforcement learning, Q-learning},
abstract = {Due to the impact of the global COVID-19, numerous industries have suffered from the disruption propagating along the supply chain, i.e. the ripple effect. To reduce adverse impact of the ripple effect, supply chain (SC) risk management under it is becoming an increasingly hot topic in both practice and research. In our former research, a robust dynamic bayesian network (DBN) approach has been developed for disruption risk assessment, whereas the solution methods adopted before (commercial solvers and simulated annealling algorithm) are not efficient enough, especially for large-size instances. For this reason, a new reinforcement learning variable neighborhood search (QVNS) is developed for solving the robust DBN optimization model, where the Q-learning algorithm is implemented to select the most efficient neighborhood structure in different stages of the search process. We conduct computational experiments on randomly generated instances, which indicates that Q-learning algorithm can improve significantly the performance of the VNS on large-size instances of the robust DBN optimization problem.}
}
@article{HA2023,
title = {Leveraging vehicle connectivity and autonomy for highway bottleneck congestion mitigation using reinforcement learning},
journal = {Transportmetrica A Transport Science},
year = {2023},
issn = {2324-9935},
doi = {https://doi.org/10.1080/23249935.2023.2215338},
url = {https://www.sciencedirect.com/science/article/pii/S2324993523001914},
author = {Paul (Young Joun) Ha and Sikai Chen and Jiqian Dong and Samuel Labi},
keywords = {Speed harmonisation, congestion, bottleneck, deep reinforcement learning, connected and autonomous vehicles, active traffic management},
abstract = {ABSTRACT
Automation and connectivity based platforms have great potential for managing highway traffic congestion including bottlenecks. Speed harmonisation (SH), one of such platforms, is an Active Traffic Management (ATM) strategy that addresses flow breakdown in real-time by adjusting upstream traffic speeds. However, SH has limitations including the need for supporting roadway infrastructure that is immovable and has limited coverage; the inability to enact control beyond its range; and the dependence on human driver compliance. These issues could be addressed by leveraging connected and automated vehicles (CAVs), which can collect information and execute control along their trajectories, irrespective of drivers’ awareness or compliance. In addressing this objective, this study utilises reinforcement learning to present a CAV control model to achieve efficient speed harmonisation. The results suggest that even at low market penetration, CAVs can significantly mitigate traffic congestion bottlenecks to a greater extent compared to traditional SH approaches.}
}
@article{ALHAMED2022108273,
title = {Building construction based on video surveillance and deep reinforcement learning using smart grid power system},
journal = {Computers and Electrical Engineering},
volume = {103},
pages = {108273},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108273},
url = {https://www.sciencedirect.com/science/article/pii/S004579062200502X},
author = {Khalid M Alhamed and Celestine Iwendi and Ashit Kumar Dutta and Badr Almutairi and Hisham Alsaghier and Sultan Almotairi},
keywords = {Deep reinforcement learning, Camcorder, Video monitoring, IoT, Smart building, Smart power grid},
abstract = {New trendy neighborhoods require trimming scientific and technological methods and equipment. Smart buildings (SB) use resources efficiently, save energy, and provide services to the community more easily for their occupants while reducing their environmental footprint. Smart cities have benefited from this growth in terms of smart buildings. Maximum accuracy and reduced latency are both required for smart building monitoring systems. Poor scheduling rules can lead to network congestion and latency that is too high for real-time monitoring on construction sites, which have restricted computing and networking capabilities. These devices can collect the data on on-site actions, achievements, and circumstances and send it back to the central dashboard for analysis. Model predictive control and Deep Reinforcement Learning (DRL) have significant drawbacks, and DRL addresses some drawbacks. Researchers are intrigued by DRL, a brand-new approach to quality control. The most important considerations for developing smart power grid systems are energy conservation, renewable energy integration, and a streamlined control system. Experiments have shown that the new video surveillance has a low loss rate and a consistent latency. The DRL-SB-IoT technique can successfully track multiple cameras in a wide monitoring situation. This technique results in excellent tracking performance and meets the criteria for developing an intelligent campus in the best way possible. Researchers analyzed studies using supervised learning to solve common building issues, such as health monitoring, security on building sites, accommodation modeling, and energy consumption prediction. Reinforcement learning has been used to solve these issues. The proposed method advances the smart gateway channel of 97.5%, the energy storage ratio of 96.9%, and the overall surveillance performance ratio of 98.6%.}
}
@article{JI2022102209,
title = {Optimal shape morphing control of 4D printed shape memory polymer based on reinforcement learning},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {73},
pages = {102209},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102209},
url = {https://www.sciencedirect.com/science/article/pii/S0736584521000922},
author = {Qinglei Ji and Mo Chen and Xi Vincent Wang and Lihui Wang and Lei Feng},
keywords = {4D printing, Shape Memory Polymer, Closed loop control, Reinforcement learning},
abstract = {4D printing technology, as a new generation of Additive Manufacturing methods, enables printed objects to further change their shapes or other properties upon external stimuli. One main category of 4D printing research is 4D printed thermal Shape Memory Polymer (SMP). Its morphing process has large time delay, is nonlinear time variant, and susceptible to unpredictable disturbances. Reaching an arbitrary position with high precision is an active research question. This paper applies the Reinforcement Learning (RL) method to develop an optimal control method to perform closed loop control of the SMP actuation. Precise and prompt shape morphing is achieved compared with previous control methods using a PI controller. The training efforts of RL are further reduced by simplifying the optimal control policy using the structural property of the prior trained results. Customized protective visors against COVID-19 are fabricated using the proposed control method.}
}
@article{LIU2023102568,
title = {A mixed perception-based human-robot collaborative maintenance approach driven by augmented reality and online deep reinforcement learning},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {83},
pages = {102568},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102568},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000443},
author = {Changchun Liu and Zequn Zhang and Dunbing Tang and Qingwei Nie and Linqi Zhang and Jiaye Song},
keywords = {Human-robot collaborative maintenance, Mixed perception, Decision-making, Online deep reinforcement learning, Augmented reality},
abstract = {Owing to the fact that the number and complexity of machines is increasing in Industry 4.0, the maintenance process is more time-consuming and labor-intensive, which contains plenty of refined maintenance operations. Fortunately, human-robot collaboration (HRC) can integrate human intelligence into the collaborative robot (cobot), which can realize not merely the nimble and sapiential maintenance operations of personnel but also the reliable and repeated maintenance manipulation of cobots. However, the existing HRC maintenance lacks the precise understand of the maintenance intention, the efficient HRC decision-making for executing robotized maintenance tasks (e.g., repetitive manual tasks) and the convenient interaction interface for executing cognitive tasks (e.g., maintenance preparation and guidance job). Hence, a mixed perception-based human-robot collaborative maintenance approach consisting of three-hierarchy structures is proposed in this paper, which can help reduce the severity of the mentioned problems. In the first stage, a mixed perception module is proposed to help the cobot recognize human safety and maintenance request according to human actions and gestures separately. During the second stage, an improved online deep reinforcement learning (DRL)-enabled decision-making module with the asynchronous structure and the function of anti-disturbance is proposed in this paper, which can realize the execution of robotized maintenance tasks. In the third stage, an augmented reality-assisted (AR) user-friendly interaction interface is designed to help the personnel interact with the cobot and execute the auxiliary maintenance task without the limitation of spatial and human factors. In addition, the auxiliary of maintenance operation can also be supported by the AR-assisted visible guidance. Finally, comparative numerical experiments are implemented in a typical machining workshop, and the experimental results show a competitive performance of the proposed HRC maintenance approach compared with other state-of-the-art methods.}
}
@incollection{CASSOL2018553,
title = {Reinforcement Learning Applied to Process Control: A Van der Vusse Reactor Case Study},
editor = {Mario R. Eden and Marianthi G. Ierapetritou and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {44},
pages = {553-558},
year = {2018},
booktitle = {13th International Symposium on Process Systems Engineering (PSE 2018)},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64241-7.50087-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642417500872},
author = {G.O. Cassol and G.V.K. Campos and D.M. Thomaz and B.D.O. Capron and A.R. Secchi},
keywords = {Reinforcement Learning, Model Predictive Control, Adaptive Control},
abstract = {With recent advances in industrial automation, data acquisition, and successful applications of Machine Learning methods to real-life problems, data-based methods can be expected to grow in use within the process control community in the near future. Model-based control methods rely on accurate models of the process to be effective. However, such models may be laborious to obtain and, even when available, the optimization problem underlying the online control problem may be too computationally demanding. Furthermore, the process degradation with time imposes that the model should be periodically updated to stay reliable. One way to address these drawbacks is through the merging of Reinforcement Learning (RL) techniques into the classical process control framework. In this work, a methodology to tackle the control of nonlinear chemical processes with RL techniques is proposed and tested on the wellknown benchmark problem of the non-isothermal CSTR with the Van de Vusse reaction. The controller proposed herein is based on the implementation of a policy that associates each state of the process to a certain control action. This policy is directly deduced from a measure of the expected performance gain, given by a value function dependent on the states and actions. In other words, in a given state, the action that provides the highest expected performance gain is chosen and implemented. The value function is approximated by a neural network that can be trained with pre-simulated data and adapted online with the continuous inclusion of new process data through the implementation of an RL algorithm. The results show that the proposed adaptive RLbased controller successfully manages to control and optimize the Van de Vusse reactor against unmeasured disturbances.}
}
@article{KLAR202310,
title = {Performance comparison of reinforcement learning and metaheuristics for factory layout planning},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {45},
pages = {10-25},
year = {2023},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2023.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1755581723000718},
author = {Matthias Klar and Moritz Glatt and Jan C. Aurich},
keywords = {Reinforcement learning, Factory layout planning, Facility layout problem, Machine learning, Optimization},
abstract = {Factory layout planning is a time-consuming process that has a large impact on the operational performance of a future factory. Besides, changing technologies and market requirements result in a frequent reconfiguration of the factory layout. Automated planning approaches can generate high-quality layout solutions and reduce the planning time compared to mere manual planning. Recent studies indicate that reinforcement learning is a suitable approach to support the early phase of the layout planning process. In this context, reinforcement learning shows potential performance-related advantages by learning the problem-related interdependencies compared to current metaheuristic approaches, which are commonly applied to the regarded problem. However, recent studies only consider a low number of reinforcement learning approaches and regarded application scenarios. In consequence, the performance in different problem sizes and of various existing reinforcement learning approaches has not been investigated. Besides, no comparison between reinforcement learning approaches and existing metaheuristics was performed for factory layout planning. As a consequence, the potential of reinforcement learning based factory layout panning can not be evaluated appropriately. Therefore, an encompassing comparison to metaheuristics is still an open research question. Regarding this background, the performance of 13 different reinforcement learning and 7 commonly used metaheuristics for three layout planning problems with different sizes is investigated in this paper. The approaches are applied to all three layout planning problems in order to compare their performance capabilities. The results indicate that the best-performing reinforcement learning approach is able to find similar or superior solutions compared to the best-performing metaheuristics.}
}
@article{ZHANG2022100150,
title = {Reinforcement learning-driven local transactive energy market for distributed energy resources},
journal = {Energy and AI},
volume = {8},
pages = {100150},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2022.100150},
url = {https://www.sciencedirect.com/science/article/pii/S2666546822000118},
author = {Steven Zhang and Daniel May and Mustafa Gül and Petr Musilek},
keywords = {Transactive energy, Demand response, Distributed energy resources (DER), DER integration, Local energy market, Reinforcement learning},
abstract = {Local energy markets are emerging as a tool for coordinating generation, storage, and consumption of energy from distributed resources. In combination with automation, they promise to provide an effective energy management framework that is fair and brings system-level savings. The cooperative–competitive nature of energy markets calls for multi-agent based automation with learning energy trading agents. However, depending on the dynamics of the agent–environment interaction, this approach may yield unintended behavior of market participants. Thus, the design of market mechanisms suitable for reinforcement learning agents must take into account this interplay. This article introduces autonomous local energy exchange (ALEX) as an experimental framework that combines multi-agent learning and double auction mechanism. Participants determine their internal price signals and make energy management decisions through market interactions, rather than relying on predetermined external price signals. The main contribution of this article is examination of compatibility between specific market elements and independent learning agents. Effects of different market properties are evaluated through simulation experiments, and the results are used for determine a suitable market design. The results show that market truthfulness maintains demand-response functionality, while weak budget balancing provides a strong reinforcement signal for the learning agents. The resulting agent behavior is compared with two baselines: net billing and time-of-use rates. The ALEX-based pricing is more responsive to fluctuations in the community net load compared to the time-of-use. The more accurate accounting of renewable energy usage reduced bills by a median 38.8% compared to net billing, confirming the ability to better facilitate demand response.}
}
@article{DONG202083,
title = {Principled reward shaping for reinforcement learning via lyapunov stability theory},
journal = {Neurocomputing},
volume = {393},
pages = {83-90},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220301831},
author = {Yunlong Dong and Xiuchuan Tang and Ye Yuan},
keywords = {Reinforcement learning, Principled reward shaping, Lyapunov stability theory, Stochastic approximation},
abstract = {Reinforcement learning (RL) suffers from the designation in reward function and the large computational iterating steps until convergence. How to accelerate the training process in RL plays a vital role. In this paper, we proposed a Lyapunov function based approach to shape the reward function which can effectively accelerate the training. Furthermore, the shaped reward function leads to convergence guarantee via stochastic approximation, an invariant optimality condition using Bellman Equation and an asymptotical unbiased policy. Moreover, sufficient RL benchmarks have been experimented to demonstrate the effectiveness of our proposed method. It has been verified that our proposed method substantially accelerates the convergence process as well as improves the performance in terms of a higher accumulated reward.}
}
@article{TEJER2024107300,
title = {Robust and efficient task scheduling for robotics applications with reinforcement learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {127},
pages = {107300},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107300},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623014847},
author = {Mateusz Tejer and Rafal Szczepanski and Tomasz Tarczewski},
keywords = {Task scheduling, Robotic arm, Sorting problem, Reinforcement learning algorithm, Performance analysis, Predictive strategies},
abstract = {Effective task scheduling can significantly impact on performance, productivity, and profitability in many real-world settings, such as production lines, logistics, and transportation systems. Traditional approaches to task scheduling rely on heuristics or simple rule-based methods. However, with the emergence of machine learning and artificial intelligence, there is growing interest in using these methods to optimize task scheduling. In particular, reinforcement learning is a promising task scheduling approach, because it can learn from experience and adapt to changing conditions. One step often missed or neglected is choosing optimal algorithm parameters and different ways the environment could be implemented. The study analyzes the performance possibilities of task scheduling using reinforcement learning. The deep analysis allows to select highly efficient environment models and Q-learning parameters. Moreover, automatic selection based on optimization algorithms has been proposed. Regardless of the selected optimal parameters, the resilience to environmental changes seems poor. The deducted analysis motivated the Authors to develop a novel Hybrid Q-learning approach. It allows to provide superior efficiency regardless of the environmental parameters.}
}
@article{GUI2023109255,
title = {Dynamic scheduling for flexible job shop using a deep reinforcement learning approach},
journal = {Computers & Industrial Engineering},
volume = {180},
pages = {109255},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109255},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223002796},
author = {Yong Gui and Dunbing Tang and Haihua Zhu and Yi Zhang and Zequn Zhang},
keywords = {Dynamic flexible job-shop scheduling problem, Single dispatching rule, Markov decision process, Deep reinforcement learning, Deep deterministic policy gradient},
abstract = {Due to the influence of dynamic changes in the manufacturing environment, a single dispatching rule (SDR) cannot consistently attain better results than other rules for dynamic scheduling problems. Dynamic selection of the most appropriate rule from several SDRs based on the Deep Q-Network (DQN) offers better scheduling performance than using an individual SDR. However, the discreteness of action space caused by the DQN and the simplicity of the action as an SDR limit the selection range and restrict performance improvement. Thus, in this paper, we propose a scheduling method based on deep reinforcement learning for the dynamic flexible job-shop scheduling problem (DFJSP), aiming to minimize the mean tardiness. Firstly, a Markov decision process with composite scheduling action is provided to elaborate the flexible job-shop dynamic scheduling process and transform the DFJSP into an RL task. Subsequently, a composite scheduling action aggregated by SDRs and continuous weight variables is designed to provide a continuous rule space and SDR weight selection. Moreover, a reward function related to mean tardiness performance criteria is designed such that maximizing the cumulative reward is equivalent to minimizing the mean tardiness. Finally, a policy network with states as inputs and weights as outputs is constructed to generate the scheduling decision at each decision point. Also, the deep deterministic policy gradient (DDPG) algorithm is used to train the policy network to select the most appropriate weights at each decision point, thereby aggregating the SDRs into a better rule. Results from numerical experiments reveal that the proposed scheduling method achieves significantly better scheduling results than an SDR and the DQN-based method in dynamically changeable manufacturing environments.}
}
@article{LUO2021107489,
title = {Dynamic multi-objective scheduling for flexible job shop by deep reinforcement learning},
journal = {Computers & Industrial Engineering},
volume = {159},
pages = {107489},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107489},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221003934},
author = {Shu Luo and Linxuan Zhang and Yushun Fan},
keywords = {Flexible job shop scheduling, Multi-objective, New job insertion, Dispatching rules, Deep reinforcement learning},
abstract = {In modern volatile and complex manufacturing environment, dynamic events such as new job insertions and machine breakdowns may randomly occur at any time and different objectives in conflict with each other should be optimized simultaneously, leading to an urgent requirement of real-time multi-objective rescheduling methods that can achieve both time efficiency and solution quality. In this regard, this paper proposes an on-line rescheduling framework named as two-hierarchy deep Q network (THDQN) for the dynamic multi-objective flexible job shop scheduling problem (DMOFJSP) with new job insertions. Two practical objectives including total weighted tardiness and average machine utilization rate are optimized. The THDQN model contains two deep Q network (DQN) based agents. The higher-level DQN is a controller determining the temporary optimization goal for the lower DQN. At each rescheduling point, it takes the current state features as input and chooses a feasible goal to guide the behaviour of the lower DQN. Four different goals corresponding to four different forms of reward functions are suggested, each of which optimizes an indicator of tardiness or machine utilization rate. The lower-level DQN acts as an actuator. It takes the current state features together with the higher optimization goal as input and chooses a proper dispatching rule to achieve the given goal. Six composite dispatching rules are developed to select an available operation and assign it on a feasible machine, which serve as the candidate action set for the lower DQN. A novel training framework based on double DQN (DDQN) is designed. The trained THDQN is compared with each proposed composite dispatching rule, existing well-known dispatching rules as well as other reinforcement learning based scheduling methods on a wide range of test instances. Results of numerical experiments have confirmed both the effectiveness and generality of the proposed THDQN.}
}
@article{TEWARI2020106988,
title = {Information-theoretic sensor planning for large-scale production surveillance via deep reinforcement learning},
journal = {Computers & Chemical Engineering},
volume = {141},
pages = {106988},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106988},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419311974},
author = {Ashutosh Tewari and Kuang-Hung Liu and Dimitri Papageorgiou},
keywords = {Active sensing, Deep reinforcement learning, Markov decision process, Production surveillance, Sensor resource management},
abstract = {Production surveillance is the task of monitoring oil and gas production from every well in a hydrocarbon field. Accurate surveillance is a basic necessity for several reasons that include improved resource management, better equipment health monitoring, reduced operational cost, and ultimately optimal hydrocarbon production. A key challenge in this task, especially for large fields with many wells, is the measurement of multiphase fluid flow using a limited number of noisy sensors of varying characteristics. Current surveillance practices are based on fixed utilization schedules of such flow sensors, which rarely change over time. Such a passive mode of sensing is completely agnostic to surveillance performance and thus often fails to achieve a desired accuracy. Here we propose an active surveillance approach, underpinned by the concept of value of information-based sensing. Borrowing some well-known concepts from Markov decision processes, reinforcement learning and artificial neural networks, we demonstrate that a practical active surveillance strategy can be devised, which can not only improve surveillance performance significantly, but also reduce usage of flow sensors.}
}
@article{SINGH2022100094,
title = {How are reinforcement learning and deep learning algorithms used for big data based decision making in financial industries–A review and research agenda},
journal = {International Journal of Information Management Data Insights},
volume = {2},
number = {2},
pages = {100094},
year = {2022},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2022.100094},
url = {https://www.sciencedirect.com/science/article/pii/S2667096822000374},
author = {Vinay Singh and Shiuann-Shuoh Chen and Minal Singhania and Brijesh Nanavati and Arpan kumar kar and Agam Gupta},
keywords = {Big data, Markov decision process, Online learning, Reinforcement learning, Financial applications, Deep reinforcement learning},
abstract = {Data availability and accessibility have brought in unseen changes in the finance systems and new theoretical and computational challenges. For example, in contrast to classical stochastic control theory and other analytical approaches for solving financial decision-making problems that rely heavily on model assumptions, new developments from reinforcement learning (RL) can make full use of a large amount of financial data with fewer model assumptions and improve decisions in complex economic environments. This paper reviews the developments and use of Deep Learning(DL), RL, and Deep Reinforcement Learning (DRL)methods in information-based decision-making in financial industries. Therefore, it is necessary to understand the variety of learning methods, related terminology, and their applicability in the financial field. First, we introduce Markov decision processes, followed by Various algorithms focusing on value and policy-based methods that do not require any model assumptions. Next, connections are made with neural networks to extend the framework to encompass deep RL algorithms. Finally, the paper concludes by discussing the application of these RL and DRL algorithms in various decision-making problems in finance, including optimal execution, portfolio optimization, option pricing, hedging, and market-making. The survey results indicate that RL and DRL can provide better performance and higher efficiency than traditional algorithms while facing real economic problems in risk parameters and ever-increasing uncertainties. Moreover, it offers academics and practitioners insight and direction on the state-of-the-art application of deep learning models in finance.}
}
@article{ZIMMERLING2020847,
title = {Estimating Optimum Process Parameters in Textile Draping of Variable Part Geometries - A Reinforcement Learning Approach},
journal = {Procedia Manufacturing},
volume = {47},
pages = {847-854},
year = {2020},
note = {23rd International Conference on Material Forming},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.04.263},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920313299},
author = {Clemens Zimmerling and Christian Poppe and Luise Kärger},
keywords = {Manufacturing Process Optimisation, Textile Forming, Neural Networks, Machine Learning, Reinforcement Learning},
abstract = {Fine-tuning of manufacturing processes for optimum part quality requires many resource-intensive trial experiments in practice. To reduce the experimental effort, physics-based process simulations in conjunction with optimisation algorithms can be applied, e.g. finite-element-models and evolutionary algorithms. However, they generally require considerable numerical expertise and long computation times. Efficient optimisation of such expensive-to-evaluate models often employs surrogate-based optimisation (SBO). SBO constructs numerically inexpensive approximations of the original model, which guide the optimiser in the parameter space. This allows concentrating costly simulations on the most promising regions. While SBO significantly reduces the computational load in many cases, current SBO-strategies are inevitably problem-specific and cannot be reused in other, even similar situations. Consequently, subtle problem variations, e.g. minor geometry changes in material forming, require an entirely new optimisation and all previous numerical effort is in vain. Thus, surrogate techniques with generalised applicability are an open field of research. Machine Learning techniques using convolutional neural networks (CNNs) are capable of ‘learning’ complex system dynamics from data. In this work, CNNs are used to extend the predictive capabilities of SBO towards variable instead of fixed manufacturing settings. Specifically, material draw-in optimisation in textile forming (‘draping’) for variable geometries is studied. Using reinforcement learning, a CNN is trained to estimate optimum positions of pressure pads during draping of a pre-specified class of box-shaped geometries. Once trained, the CNN interprets a forming result and infers beneficial pad positions. Unlike conventional SBO strategies, it can also give recommendations for variable geometries from the selected geometry class. The paper shows that, in principle, CNNs are able to extract information from a range of different forming tasks and apply it to a new, unknown situation. Since they reuse information gained from previous simulations, they are considered a viable option for future, generalised SBO-strategies.}
}
@article{LI2021116311,
title = {Reinforcement learning based automated history matching for improved hydrocarbon production forecast},
journal = {Applied Energy},
volume = {284},
pages = {116311},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116311},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920316950},
author = {Hao Li and Siddharth Misra},
keywords = {Reinforcement Learning, History Matching, Reservoir Simulation, Deep Learning, Neural Network},
abstract = {History matching aims to find a numerical reservoir model that can be used to predict the reservoir performance. An engineer and model calibration (data inversion) method are required to adjust various parameters/properties of the numerical model in order to match the reservoir production history. In this study, we develop deep neural networks within the reinforcement learning framework to achieve automated history matching that will reduce engineers’ efforts, human bias, automatically and intelligently explore the parameter space, and remove the need of large set of labeled training data. To that end, a fast-marching-based reservoir simulator is encapsulated as an environment for the proposed reinforcement learning. The deep neural-network-based learning agent interacts with the reservoir simulator within reinforcement learning framework to achieve the automated history matching. Reinforcement learning techniques, such as discrete Deep Q Network and continuous Deep Deterministic Policy Gradients, are used toth, used to train the learning agents. The continuous actions enable the Deep Deterministic Policy Gradients to explore more states at each iteration in a a learning episode; consequently, a better history matching is achieved using this algorithm as compared to Deep Q Network. For simplified dual-target composite reservoir models, the best history-matching performances of the discrete and continuous learning methods in terms of normalized root mean square errors are 0.0447 and 0.0038, respectively. Our study shows that continuous action space achieved by the deep deterministic policy gradient drastically outperforms deep Q network.}
}
@article{LIN2023197,
title = {Accelerating reinforcement learning with case-based model-assisted experience augmentation for process control},
journal = {Neural Networks},
volume = {158},
pages = {197-215},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022004129},
author = {Runze Lin and Junghui Chen and Lei Xie and Hongye Su},
keywords = {Reinforcement learning, Process control, Experience augmentation, Local dynamic model, Case-based reasoning},
abstract = {In the context of intelligent manufacturing in the process industry, traditional model-based optimization control methods cannot adapt to the situation of drastic changes in working conditions or operating modes. Reinforcement learning (RL) directly achieves the control objective by interacting with the environment, and has significant advantages in the presence of uncertainty since it does not require an explicit model of the operating plant. However, most RL algorithms fail to retain transfer learning capabilities in the presence of mode variation, which becomes a practical obstacle to industrial process control applications. To address these issues, we design a framework that uses local data augmentation to improve the training efficiency and transfer learning (adaptability) performance. Therefore, this paper proposes a novel RL control algorithm, CBR-MA-DDPG, organically integrating case-based reasoning (CBR), model-assisted (MA) experience augmentation, and deep deterministic policy gradient (DDPG). When the operating mode changes, CBR-MA-DDPG can quickly adapt to the varying environment and achieve the desired control performance within several training episodes. Experimental analyses on a continuous stirred tank reactor (CSTR) and an organic Rankine cycle (ORC) demonstrate the superiority of the proposed method in terms of both adaptability and control performance/robustness. The results show that the control performance of the CBR-MA-DDPG agent outperforms the conventional PI and MPC control schemes, and that it has higher training efficiency than the state-of-the-art DDPG, TD3, and PPO algorithms in transfer learning scenarios with mode shift situations.}
}
@article{ZHONG2022361,
title = {A New Neuro-Optimal Nonlinear Tracking Control Method via Integral Reinforcement Learning with Applications to Nuclear Systems},
journal = {Neurocomputing},
volume = {483},
pages = {361-369},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.01.034},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222000558},
author = {Weifeng Zhong and Mengxuan Wang and Qinglai Wei and Jingwei Lu},
keywords = {Integral reinforcement learning, Nuclear power reactor, Nonlinear system, Optimal tracking control, Neural networks},
abstract = {In this paper, a new infinite horizon optimal tracking control method for continuous-time nonlinear systems is given using an actor-critic structure. This present integral reinforcement learning (IRL) method is a novelty method in adaptive dynamic programming (ADP) algorithms and an online policy iteration algorithm. For the optimal tracking problem, the cost function is defined by tracking errors. Consequently, the goal is to minimize tracking errors toward desired trajectories. Since it is hard to solve the Hamilton-Jacobi-Bellman (HJB) equation for continuous-time nonlinear systems control problems, leveraging the actor-critic architecture with neural networks (NNs) to approximate the tracking error performance index and error control law is necessary. Instead of using conventional neural networks, we employ higher-order polynomials in the whole actor-critic architecture. Finally, we apply this new neuro-optimal tracking method to the 2500MW pressurized water reactor (PWR) nuclear power plant, and simulation results are given to demonstrate the effectiveness of the developed method.}
}
@article{BOUGIE2022863,
title = {Data-Efficient Reinforcement Learning from Controller Guidance with Integrated Self-Supervision for Process Control},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {7},
pages = {863-868},
year = {2022},
note = {13th IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems DYCOPS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.07.553},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322009594},
author = {Nicolas Bougie and Takashi Onishi and Yoshimasa Tsuruoka},
keywords = {Reinforcement learning control, Process control, Chemical plant control, Co-Learning, self-learning, Artificial intelligence},
abstract = {Model-free reinforcement learning methods have achieved significant success in a variety of decision-making problems. In fact, they traditionally rely on large amounts of data generated by sample-efficient simulators. However, many process control industries involve complex and costly computations, which limits the applicability of model-free reinforcement learning. In addition, extrinsic rewards are naturally sparse in the real world, further increasing the amount of necessary interactions with the environment. This paper presents a sample-efficient model-free algorithm for process control, which massively accelerates the learning process even when rewards are extremely sparse. To achieve this, we leverage existing controllers to guide the agent's learning — controller guidance is used to drive exploration towards key regions of the state space. To further mitigate the above-mentioned challenges, we propose a strategy for self-supervision learning that lets us improve the agent's policy via its own successful experience. Notably, the method we develop is able to leverage guidance that does not include the actions and remains effective when the existing controllers are suboptimal. We present an empirical evaluation on a vinyl acetate monomer (VAM) chemical plant under disturbances. The proposed method exhibits better performance than baselines approaches and higher sample efficiency. Besides, empirical results show that our method outperforms the existing controllers for controlling the plant and canceling disturbances, mitigating the drop in the production load.}
}
@article{KIM2024108962,
title = {Non-iterative generation of an optimal mesh for a blade passage using deep reinforcement learning},
journal = {Computer Physics Communications},
volume = {294},
pages = {108962},
year = {2024},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2023.108962},
url = {https://www.sciencedirect.com/science/article/pii/S0010465523003077},
author = {Innyoung Kim and Sejin Kim and Donghyun You},
keywords = {Mesh generation, Multi-condition optimization, Deep reinforcement learning, Structured mesh generation, Blade passage},
abstract = {A method using deep reinforcement learning (DRL) to non-iteratively generate an optimal mesh for an arbitrary blade passage is developed. Despite automation in mesh generation using either an empirical approach or an optimization algorithm, repeated tuning of meshing parameters is still required for a new geometry. The method developed herein employs a DRL-based multi-condition optimization technique to define optimal meshing parameters as a function of the blade geometry, attaining automation, minimization of human intervention, and computational efficiency. The meshing parameters are optimized by training an elliptic mesh generator which generates a structured mesh for a blade passage with an arbitrary blade geometry. During each episode of the DRL process, the mesh generator is trained to produce an optimal mesh for a randomly selected blade passage by updating the meshing parameters until the mesh quality, as measured by the ratio of determinants of the Jacobian matrices and the skewness, reaches the highest level. Once the training is completed, the mesh generator creates an optimal mesh for a new arbitrary blade passage in a single try without an repetitive process for the parameter tuning for mesh generation from the scratch. The effectiveness and robustness of the proposed method are demonstrated through the generation of meshes for various blade passages.}
}
@article{DENG2023117031,
title = {Decentralized yaw optimization for maximizing wind farm production based on deep reinforcement learning},
journal = {Energy Conversion and Management},
volume = {286},
pages = {117031},
year = {2023},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2023.117031},
url = {https://www.sciencedirect.com/science/article/pii/S0196890423003771},
author = {Zhiwen Deng and Chang Xu and Xingxing Han and Zhe Cheng and Feifei Xue},
keywords = {Wind farm, Deep reinforcement learning, Dynamic wake model, Production maximization, Yaw meandering},
abstract = {This study describes a deep reinforcement learning (DRL) based decentralized yaw optimization method to maximize the power production of wind farms. Specifically, we apply the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm to design agents separately for each turbine in the wind farm to make yaw decisions independently. This design allows setting different yaw rewards for the agents to promote faster and better convergence of MADDPG. To test the control effect of the proposed method, a novel analytical dynamic wake model is derived first, which can dynamically reflect the wake propagation of the wind turbine after the inflow wind speed, axial induction factor, and yaw angle change. Then, the static and dynamic characteristics of the proposed wake model are verified. The wind farm simulation is realized through the dynamic wake model, which provides an interactive environment for the simulation experiments to test the effect of the proposed DRL-based decentralized yaw optimization method. The results show that the proposed method can significantly increase the power generation of the wind farm, and the set yaw reward helps to guide MADDPG to converge to the optimal strategy.}
}
@article{ZHU2021498,
title = {Control of A Polyol Process Using Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {3},
pages = {498-503},
year = {2021},
note = {16th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.291},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321010648},
author = {Wenbo Zhu and Ricardo Rendall and Ivan Castillo and Zhenyu Wang and Leo H. Chiang and Philippe Hayot and Jose A. Romagnoli},
keywords = {Deep Learning, Reinforcement Learning, Batch Process},
abstract = {Reinforcement learning is a branch of machine learning, where an agent gradually learns a control policy via a combination of exploration and interactions with a system. Recent successes of model-free reinforcement learning (RL) has attracted tremendous attention from the process control community. For instance, RL has been successfully applied in very complex control tasks (e.g., games such as chess or Go that contain large state spaces) and is shown to be robust to uncertainties. These findings indicate that there is a significant potential to leverage RL methods to improve the control of chemical processes. In this work, RL was applied to a detailed and accurate simulation of an industrial polyol process. To manufacture the desired product, the RL controller is required to achieve the target ending conditions determined by four key parameters; meanwhile, economic factors are also considered in this process, including batch reaction time and total feed amounts. The obtained results show a high consistency between RL and the current optimal operating conditions. Additionally, an improvement opportunity was identified by extending current control bounds of the manipulated variables. This work illustrates that RL is capable of handling complicated industrial systems, even under realistic operating constraints.}
}
@article{YOO2021108,
title = {Reinforcement learning for batch process control: Review and perspectives},
journal = {Annual Reviews in Control},
volume = {52},
pages = {108-119},
year = {2021},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2021.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S136757882100081X},
author = {Haeun Yoo and Ha Eun Byun and Dongho Han and Jay H. Lee},
keywords = {Reinforcement learning, Batch process, Batch optimization, Batch control},
abstract = {Batch or semi-batch processing is becoming more prevalent in industrial chemical manufacturing but it has not benefited from advanced control technologies to a same degree as continuous processing. This is due to its several unique aspects which pose challenges to implementing model-based optimal control, such as its highly nonstationary operation and significant run-to-run variabilities. While existing advanced control methods like model predictive control (MPC) have been extended to address some of the challenges, they still suffer from certain limitations which have prevented their widespread industrial adoption. Reinforcement learning (RL) where the agent learns the optimal policy by interacting with the system offers an alternative to the existing model-based methods and has potential for bringing significant improvements to industrial batch process control practice. With such motivation, this paper examines the advantages that RL offers over the traditional model-based optimal control methods and how it can be tailored to better address the characteristics of industrial batch process control problems. After a brief review of the existing batch control methods, the basic concepts and algorithms of RL are introduced and issues for applying them to batch process control problems are discussed. The nascent literature on the use of RL in batch process control is briefly reviewed, both in recipe optimization and tracking control, and our perspectives on future research directions are shared.}
}
@article{WEI2022282,
title = {Monte Carlo-based reinforcement learning control for unmanned aerial vehicle systems},
journal = {Neurocomputing},
volume = {507},
pages = {282-291},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222009869},
author = {Qinglai Wei and Zesheng Yang and Huaizhong Su and Lijian Wang},
keywords = {Reinforcement learning, Adaptive dynamic programming (ADP), UAV control, Monte Carlo simulation, Neural networks},
abstract = {In this paper, a new data-driven reinforcement learning method based on Monte Carlo simulation is developed to solve the optimal control problem of unmanned aerial vehicle (UAV) systems. Based on the data which are generated by Monte Carlo simulation, neural network (NN) is used to construct the dynamics of the UAV system with unknown disturbances, where the mathematical model of the UAV system is unnecessary. An effective iterative framework of action and critic is constructed to obtain the optimal control law. The convergence property is developed to guarantee that the iterative performance cost function converges to a finite neighborhood of the optimal performance cost function. Finally, numerical results are given to illustrate the effectiveness of the developed method.}
}
@article{TOMEDEANDRADEESILVA2022105854,
title = {Self-adapting WIP parameter setting using deep reinforcement learning},
journal = {Computers & Operations Research},
volume = {144},
pages = {105854},
year = {2022},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2022.105854},
url = {https://www.sciencedirect.com/science/article/pii/S0305054822001290},
author = {Manuel {Tomé De Andrade e Silva} and Américo Azevedo},
keywords = {WIP reduction, CONWIP, Deep reinforcement learning},
abstract = {This study investigates the potential of dynamically adjusting WIP cap levels to maximize the throughput (TH) performance and minimize work in process (WIP), according to real-time system state arising from process variability associated with low volume and high-variety production systems. Using an innovative approach based on state-of-the-art deep reinforcement learning (proximal policy optimization algorithm), we attain WIP reductions of up to 50% and 30%, with practically no losses in throughput, against pure-push systems and the statistical throughput control method (STC), respectively. An exploratory study based on simulation experiments was performed to provide support to our research. The reinforcement learning agent’s performance was shown to be robust to variability changes within the production systems.}
}
@article{SHEN202011704,
title = {Burden Control Strategy Based on Reinforcement Learning for Gas Utilization Rate in Blast Furnace⁎⁎This work was supported in part by the National Natural Science Foundation of China under Grants 61973287 and 61333002, and the 111 project under Grant B17040.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {11704-11709},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.667},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320309782},
author = {Xiaoling Shen and Jianqi An and Min Wu and Jinhua She},
keywords = {Blast furnace, gas utilization rate, burden control strategy, reinforcement learning algorithm, long-time scale},
abstract = {Gas utilization rate (GUR) is an important state parameter to reflect the energy consumption, the quality and production of the pig iron, and the distribution of the gas flow in a blast furnace. The GUR is mainly adjusted by burden distribution and hot-blast supply. According to the analysis of mechanism and data, burden distribution and hot-blast supply affect the GUR on a long-time scale and short-time scale, respectively. However, few of the previous researches proposed the control method for the GUR and they did not consider multi-time-scale characteristics. Thus, it is necessary to design a control strategy or system for the GUR considering the multi-time-scale characteristics, which can make the GUR have a reasonable development trend. This paper presented a burden control strategy based on a reinforcement learning algorithm for the GUR. The method improved the development trend of the GUR on a long-time scale. The experimental results demonstrated that the sequence of the parameters of the burden distribution given by the presented method ensured a reasonable development trend of the GUR on a long-time scale.}
}
@article{RIESENER2020127,
title = {Applying Supervised and Reinforcement Learning to Design Product Portfolios in Accordance with Corporate Goals},
journal = {Procedia CIRP},
volume = {91},
pages = {127-133},
year = {2020},
note = {Enhancing design through the 4th Industrial Revolution Thinking},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.02.157},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120307952},
author = {Michael Riesener and Christian Dölle and Christopher Dierkes and Merle-Hendrikje Jank},
keywords = {Product portfolio management, Neural networks, Corporate goals, Reinforcement learning},
abstract = {Faced with rapidly changing technologies, diminishing product life cycles and heightened global competition, product portfolio managers across all industries encounter increasing challenges within portfolio design processes. Aligning the product portfolio with corporate strategies is central to sustain long-term company success. However, regarding volatile environments, this is becoming increasingly challenging. In the last decades, product portfolio decisions were based on subjective experience, but this is no longer sufficient. Nowadays, as portfolio complexity grows constantly, data-based decision support procedures are needed to enable effective decisions in product portfolio management. Regarding the field of portfolio management, only little research has been conducted on the usage of data-based analytical methods. Additionally, the alignment of product portfolios with corporate strategies is still largely unexplored and, in this context, the application of analytical methods has been largely omitted until now. This paper proposes a methodology that uses neural networks with supervised learning to model correlations among product portfolio control parameters and corporate goal indicators. Based on this, reinforcement learning is applied to derive goal-conform recommendations for product portfolio managers. For both supervised and reinforcement learning, the presented methodology includes generic steps for implementation. Moreover, for both machine learning methods, requirements regarding necessary product portfolio data are elaborated. The methodology is validated using a case study.}
}
@article{PANE2019236,
title = {Reinforcement learning based compensation methods for robot manipulators},
journal = {Engineering Applications of Artificial Intelligence},
volume = {78},
pages = {236-247},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2018.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0952197618302446},
author = {Yudha P. Pane and Subramanya P. Nageshrao and Jens Kober and Robert Babuška},
keywords = {Reinforcement learning, Tracking control, Robotics, Actor-critic scheme},
abstract = {Smart robotics will be a core feature while migrating from Industry 3.0 (i.e., mass manufacturing) to Industry 4.0 (i.e., customized or social manufacturing). A key characteristic of a smart system is its ability to learn. For smart manufacturing, this means incorporating learning capabilities into the current fixed, repetitive, task-oriented industrial manipulators, thus rendering them ‘smart’. In this paper we introduce two reinforcement learning (RL) based compensation methods. The learned correction signal, which compensates for unmodeled aberrations, is added to the existing nominal input with an objective to enhance the control performance. The proposed learning algorithms are evaluated on a 6-DoF industrial robotic manipulator arm to follow different kinds of reference paths, such as square or a circular path, or to track a trajectory on a three dimensional surface. In an extensive experimental study we compare the performance of our learning-based methods with well-known tracking controllers, namely, proportional-derivative (PD), model predictive control (MPC), and iterative learning control (ILC). The experimental results show a considerable performance improvement thanks to our RL-based methods when compared to PD, MPC, and ILC.}
}
@article{OU20191,
title = {Simulation study on reward function of reinforcement learning in gantry work cell scheduling},
journal = {Journal of Manufacturing Systems},
volume = {50},
pages = {1-8},
year = {2019},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0278612518304503},
author = {Xinyan Ou and Qing Chang and Nilanjan Chakraborty},
keywords = {Gantry scheduling, Reinforcement learning, Q-learning, Reward function},
abstract = {In a work cell with material handling gantries, gantry movements constrain the production of the work cell. Due to the fact that the gantry real-time scheduling and the material flow are highly coupled, modeling of the gantry work cell is very challenging. In this paper, we formulate the gantry real-time scheduling problem as a reinforcement learning problem, carried out by Q-learning algorithm. To build a learning model, the definition of reward function is instrumental. To study the learning performance of Q-learning algorithm, we perform simulation experiments with five different reward functions based on different understandings of the production system. It is shown by simulation experiments that the learning performance varies with reward functions and only the reward demonstrating a better understanding of the system outperforms other reward functions. In addition, the results further validate the effectiveness and practicality of the theories and conclusions from the systematic analyses of the gantry work cell.}
}
@article{SKOCAJ2022403,
title = {Cellular Network Capacity and Coverage Enhancement with MDT Data and Deep Reinforcement Learning},
journal = {Computer Communications},
volume = {195},
pages = {403-415},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422003449},
author = {Marco Skocaj and Lorenzo M. Amorosa and Giorgio Ghinamo and Giuliano Muratore and Davide Micheli and Flavio Zabini and Roberto Verdone},
keywords = {Mobile networks, Deep Reinforcement Learning, Capacity and coverage optimization, Network optimization, Minimization of Drive Test},
abstract = {Recent years witnessed a remarkable increase in the availability of data and computing resources in communication networks. This contributed to the rise of data-driven over model-driven algorithms for network automation. This paper investigates a Minimization of Drive Tests (MDT)-driven Deep Reinforcement Learning (DRL) algorithm to optimize coverage and capacity by tuning antennas tilts on a cluster of cells from TIM’s cellular network. We jointly utilize MDT data, electromagnetic simulations, and network Key Performance indicators (KPIs) to define a simulated network environment for the training of a Deep Q-Network (DQN) agent. Some tweaks have been introduced to the classical DQN formulation to improve the agent’s sample efficiency, stability and performance. In particular, a custom exploration policy is designed to introduce soft constraints at training time. Results show that the proposed algorithm outperforms baseline approaches like DQN and best-first search in terms of long-term reward and sample efficiency. Our results indicate that MDT-driven approaches constitute a valuable tool for autonomous coverage and capacity optimization of mobile radio networks.}
}
@article{SZARSKI2021106235,
title = {Composite temperature profile and tooling optimization via Deep Reinforcement Learning},
journal = {Composites Part A: Applied Science and Manufacturing},
volume = {142},
pages = {106235},
year = {2021},
issn = {1359-835X},
doi = {https://doi.org/10.1016/j.compositesa.2020.106235},
url = {https://www.sciencedirect.com/science/article/pii/S1359835X20304681},
author = {Martin Szarski and Sunita Chauhan},
keywords = {Process simulation, Optimization, Cure, Tooling, Thermal analysis},
abstract = {Carbon Fibre Reinforced Plastic (CFRP) manufacturing cycle time is a major driver of production rate and cost for aerospace manufacturers and manufacturers in other composite heavy industries. One major thread in optimizing liquid moulding manufacturing processes such as VARTM involves modifying variables affecting heat transfer while the composite is being processed. Modelling and control of these curing processes is known to be challenging as the temperature response during cure is dependent on many variables including resin cure kinetics, carbon perform thickness and material properties, tooling thickness and material properties, and autoclave or oven heat transfer coefficients. In this work, we introduce a novel optimization method for composite cures taking into account both air temperature and tooling. Framing oven air profile optimization as optimal control, we are able to learn an adaptive and generalizable temperature controller via Deep Reinforcement Learning in simulation. Using 5 1D heat diffusion models at different locations, this controller learns from experience to anticipate the thermochemical dynamics of composite curing, and makes use of both heating and cooling to reduce cycle time while adhering to soft constraints on the maximum temperature of the system and avoid thermal runaway. This adaptive controller is then used in a Bayesian Optimization loop to optimize tooling geometry, adding and removing thermal mass in different locations to ensure the resultant cure adheres to hard constraints and has minimal overall cycle time. On two realistic aerospace parts with complex geometry and varying thicknesses we are able to reduce ramp to cure time by 27 to 40%.}
}
@article{BRAMMER2021107704,
title = {Solving the mixed model sequencing problem with reinforcement learning and metaheuristics},
journal = {Computers & Industrial Engineering},
volume = {162},
pages = {107704},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107704},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221006082},
author = {Janis Brammer and Bernhard Lutz and Dirk Neumann},
keywords = {Scheduling, Mixed model sequencing, Reinforcement learning, Metaheuristics, Mixed-integer linear programming},
abstract = {This study presents a reinforcement learning (RL) approach for the mixed model sequencing (MMS) problem with a minimization of work overload situations. The proposed approach generates the sequence in a constructive way, so that an action denotes the model to be sequenced next. The trained policy quickly creates an initial sequence, which allows us to use the cutoff time to further improve the solution quality with a metaheuristic. Our numerical evaluation based on an existing benchmark dataset shows that our approach is superior to established methods if the demand plan follows its expected distribution from the learning process.}
}
@article{ZHANG2023526,
title = {Multi-objective Reinforcement Learning – Concept, Approaches and Applications},
journal = {Procedia Computer Science},
volume = {221},
pages = {526-532},
year = {2023},
note = {Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923007767},
author = {Linzi Zhang and Zhiquan Qi and Yong Shi},
keywords = {Multi-objective decision making, Reinforcement Learning, Survey},
abstract = {Real-world decision-making tasks are generally complicated and require trade-offs between multiple, even conflicting, objectives. As the advent and great development of advanced information technology, it has evolved into using reinforcement learning (RL) algorithms to tackle the multi-objective decision making (MODM) problems. In this paper, we will first identify the basic concepts and factors when modelling the MODM tasks with reinforcement learning, and then review the traditional RL, such as Sarsa, Q-Learning, Policy Gradients, Actor-Critic, Monte-Carlo learning, and modern deep RL algorithms applied in this process. Furthermore, the specific practical scenarios described in MODM problems will be summarized through analyzing some typical articles. Finally, the future trends of multi-objective reinforcement learning will be discussed.}
}
@article{ZAMBIANCO2022100,
title = {A reinforcement learning agent for mixed-numerology interference-aware slice spectrum allocation with non-deterministic and deterministic traffic},
journal = {Computer Communications},
volume = {189},
pages = {100-109},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422000858},
author = {Marco Zambianco and Giacomo Verticale},
keywords = {Network slicing, Reinforcement learning, Interference},
abstract = {5G RAN slicing is an essential tool to support the simultaneous coexistence of enhanced mobile broadband (eMBB) and ultra-reliable low-latency communications (URLLC) network slices on a shared mixed-numerology physical layer. Moreover, due to recent advance of the private network paradigm, RAN slicing assumes a central role to provide dedicated radio coverage to industry 4.0 applications as a standalone RAN. Unlike the stochastic traffic behavior characterizing URLLC slices in classical mobile networks, industrial networks support URLLC services with deterministic and periodic traffic patterns. Based on this alternative network characterization, we design a deep reinforcement learning (DRL) agent that simultaneously provides a spectrum allocation fulfilling the eMBB and URLLC service requirements and mitigates the inter-numerology interference (INI). Furthermore, by exploiting the information about the deterministic traffic patterns, we specialize the agent reward function to improve the spectrum allocation effectiveness for URLLC slices deployed in industrial environments. We assess the agent performance with respect to resource allocation schemes that are INI agnostic. Results reveal that the proposed solution outperforms the benchmark schemes in terms of service provisioning performance in both network scenarios (e.g. mobile and industrial) and showcase the benefit of INI mitigation.}
}
@article{YANG2021107713,
title = {Joint optimization of preventive maintenance and production scheduling for multi-state production systems based on reinforcement learning},
journal = {Reliability Engineering & System Safety},
volume = {214},
pages = {107713},
year = {2021},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2021.107713},
url = {https://www.sciencedirect.com/science/article/pii/S0951832021002489},
author = {Hongbing Yang and Wenchao Li and Bin Wang},
keywords = {Preventive maintenance, Production scheduling, Reinforcement learning, Markov decision process, Expected average rewards},
abstract = {Preventive maintenance and production scheduling are two important and interactive activities in production systems. In this work, the integrated optimization problem of production scheduling for multi-state single-machine production systems experiencing degradation processes is investigated. Preventive maintenance tasks and jobs scheduling are jointly considered to find the optimal production policy by considering the processing costs, the maintenance costs, and the completion rewards, simultaneously. We formulate the integrated optimization problem as Markov decision process framework. R-learning algorithm is introduced to maximize the long-run expected average rewards per time unit over infinite horizon. On the basis of the analysis of the optimal stationary policy, the appropriate condition to perform preventive maintenance following optimal stationary policy is presented. This provides the basis for the improvement in R-learning algorithm. Furthermore, a novel heuristic reinforcement learning method is proposed to deal with the integrated model more efficiently. Finally, we present the simulation results and analysis of the proposed algorithm's performance in terms of the number of job types and machine states. The simulation results and analysis show the effectiveness of the proposed approach for solving the integrated problems.}
}
@article{DAPOLITO2021251,
title = {Flight Control of a Multicopter using Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {13},
pages = {251-255},
year = {2021},
note = {20th IFAC Conference on Technology, Culture, and International Stability TECIS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.454},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321018917},
author = {Francesco d’Apolito and Christoph Sulzbachner},
keywords = {Artificial Intelligence, Applications, Intelligent Systems, Robotics, Handling Devices, Models, Simulation, Mechatronic Systems},
abstract = {Machine Learning, and in particular Reinforcement Learning, is a persistent trend in automation and robotics in recent years. Many researchers worldwide are developing intelligent controllers using Reinforcement Learning techniques. This paper aims to present a proof-of-concept Reinforcement Learning flight controller for a multicopter. The agent has been trained in the Airsim simulation environment to achieve stable flight conditions by controlling its roll, pitch, yaw and throttle. After training, the agent has been tested on the same environment to prove its ability to maintain stable flight conditions while following a determined route.}
}
@article{SINGH2023109678,
title = {Dispatching AGVs with battery constraints using deep reinforcement learning},
journal = {Computers & Industrial Engineering},
pages = {109678},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109678},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223007027},
author = {Nitish Singh and Alp Akcay and Quang-Vinh Dang and Tugce Martagan and Ivo Adan},
keywords = {Dispatching, Automated guided vehicles, Deep reinforcement learning, Mixed-integer linear programming},
abstract = {This paper considers the problem of real-time dispatching of a fleet of automated guided vehicles (AGVs) with battery constraints. AGVs must be immediately assigned to transport requests, which arrive randomly. In addition, the AGVs must be repositioned and recharged, awaiting future transport requests. Each transport request has a soft time window with late delivery incurring a tardiness cost. This research aims to minimize the total costs, consisting of tardiness costs of transport requests and travel costs of AGVs. We extend the existing literature by making a distinction between parking and charging nodes, where AGVs wait idle for incoming transporting requests and satisfy their charging needs, respectively. Also, we formulate this online decision-making problem as a Markov decision process and propose a solution approach based on deep reinforcement learning. To assess the quality of the proposed approach, we compare it with the optimal solution of a mixed-integer linear programming model that assumes full knowledge of transport requests in hindsight and hence serves as a lower-bound on the costs. We also compare our solution with a heuristic policy used in practice. We assess the performance of the proposed solutions in an industry case study using real-world data.}
}
@article{NAGY2023110435,
title = {Ten questions concerning reinforcement learning for building energy management},
journal = {Building and Environment},
volume = {241},
pages = {110435},
year = {2023},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2023.110435},
url = {https://www.sciencedirect.com/science/article/pii/S0360132323004626},
author = {Zoltan Nagy and Gregor Henze and Sourav Dey and Javier Arroyo and Lieve Helsen and Xiangyu Zhang and Bingqing Chen and Kadir Amasyali and Kuldeep Kurte and Ahmed Zamzam and Helia Zandi and Ján Drgoňa and Matias Quintana and Steven McCullogh and June Young Park and Han Li and Tianzhen Hong and Silvio Brandi and Giuseppe Pinto and Alfonso Capozzoli and Draguna Vrabie and Mario Bergés and Kingsley Nweye and Thibault Marzullo and Andrey Bernstein},
keywords = {Open AI Gym},
abstract = {As buildings account for approximately 40% of global energy consumption and associated greenhouse gas emissions, their role in decarbonizing the power grid is crucial. The increased integration of variable energy sources, such as renewables, introduces uncertainties and unprecedented flexibilities, necessitating buildings to adapt their energy demand to enhance grid resiliency. Consequently, buildings must transition from passive energy consumers to active grid assets, providing demand flexibility and energy elasticity while maintaining occupant comfort and health. This fundamental shift demands advanced optimal control methods to manage escalating energy demand and avert power outages. Reinforcement learning (RL) emerges as a promising method to address these challenges. In this paper, we explore ten questions related to the application of RL in buildings, specifically targeting flexible energy management. We consider the growing availability of data, advancements in machine learning algorithms, open-source tools, and the practical deployment aspects associated with software and hardware requirements. Our objective is to deliver a comprehensive introduction to RL, present an overview of existing research and accomplishments, underscore the challenges and opportunities, and propose potential future research directions to expedite the adoption of RL for building energy management.}
}