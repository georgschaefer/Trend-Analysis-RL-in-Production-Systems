"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Motion simulation of robot arm using reinforcement learning","Takahito Oshiro; Kajiro Watanabe","Hosei University, Koganei, Tokyo, Japan; Hosei University, Koganei, Tokyo, Japan","SICE Annual Conference 2007","7 Jan 2008","2007","","","1077","1080","This paper describes the learning of robot arm action by reinforcement learning. We used Q-learning, which is a typical method of reinforcement learning, and which was programmed via MATLAB software. Simulations demonstrated the shortest path of robot arm motion to reach the target location.","","978-4-907764-27-2","10.1109/SICE.2007.4421144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4421144","simulation;reinforcement learning;robot arm;Q-learning","Learning;Service robots;Robot kinematics;Medical robotics;Manufacturing industries;Medical simulation;Computer errors;Error correction;MATLAB;Application software","control engineering computing;learning (artificial intelligence);manipulators","motion simulation;robot arm;reinforcement learning;Q-learning;MATLAB software","","","","2","","7 Jan 2008","","","IEEE","IEEE Conferences"
"Proactive Action Visual Residual Reinforcement Learning for Contact-Rich Tasks Using a Torque-Controlled Robot","Y. Shi; Z. Chen; H. Liu; S. Riedel; C. Gao; Q. Feng; J. Deng; J. Zhang","Department of Informatics, TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg; Agile Robots AG; Technische Universität München; Agile Robots AG; Agile Robots AG; Technische Universität München; Agile Robots AG; Department of Informatics, TAMS (Technical Aspects of Multimodal Systems), Universität Hamburg","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","765","771","Contact-rich manipulation tasks are commonly found in modern manufacturing settings. However, manually designing a robot controller is considered hard for traditional control methods as the controller requires an effective combination of modalities and vastly different characteristics. In this paper, we first consider incorporating operational space visual and haptic information into a reinforcement learning (RL) method to solve the target uncertainty problems in unstructured environments. Moreover, we propose a novel idea of introducing a proactive action to solve a partially observable Markov decision process (POMDP) problem. With these two ideas, our method can either adapt to reasonable variations in unstructured environments or improve the sample efficiency of policy learning. We evaluated our method on a task that involved inserting a random-access memory (RAM) using a torque-controlled robot and tested the success rates of different baselines used in the traditional methods. We proved that our method is robust and can tolerate environmental variations.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561162","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561162","","Visualization;Uncertainty;Conferences;Random access memory;Reinforcement learning;Markov processes;Manufacturing","industrial manipulators;Markov processes;reinforcement learning;robotic assembly;torque control","proactive action visual residual reinforcement learning;torque-controlled robot;contact-rich manipulation;robot controller design;haptic information;target uncertainty problems;partially observable Markov decision process problem;policy learning;random access memory;industrial robots;assembly tasks","","3","","35","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement learning combined with radial basis function neural network to solve Job-Shop scheduling problem","R. S. Williem; K. Setiawan","Faculty of Business Universitas Pelita Harapan Surabaya, Waru - Surabaya, Indonesia; Faculty of Computer Science Hill University Ormskirk, Lancashire, UK. Universitas Pelita Harapan Surabaya, Waru-Surabaya, Indonesia","2011 IEEE International Summer Conference of Asia Pacific Business Innovation and Technology Management","22 Aug 2011","2011","","","29","32","To complete jobs/tasks within their designated time periods, manufacturing companies utilize multiple machines. Job-shop scheduling is a critical element in job/task completion. This schedule consists of a sequence of doing consecutive jobs in a minimum amount of time. In addition, any conflict between the raw materials used in each job and its resource pool are to be avoided. This research applied the Reinforcement Learning (RL) method which is implemented in Temporal Difference Learning (TDL). Furthermore, the TDL focused on the Gradient-Descent method in which the Radial Basis Function Neural Network served as the approximation function. The input of this research was an initial critical path with no conflict-free schedule. Using the above methods, the conflict(s) could be eliminated gradually. Thus, the flexible job-shop scheduling can readily be made by any manufacturing company. Language used for this research is the Borland Delphi 7.0. All object structure and methods are made as easy as possible so that it can be implemented on the same problem with different application.","","978-1-4244-9655-6","10.1109/APBITM.2011.5996285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5996285","machine;industry;job-shop scheduling;reinforcement learning;radial basis function neural network","Schedules;Learning;Indexes;Resource description framework;Job shop scheduling;Resource management;Processor scheduling","approximation theory;gradient methods;job shop scheduling;learning (artificial intelligence);radial basis function networks;raw materials","reinforcement learning;radial basis function neural network;Job-Shop Scheduling Problem;manufacturing companies;multiple machines;raw materials;gradient-descent method;approximation function;conflict-free schedule;Borland Delphi 7.0","","4","","8","IEEE","22 Aug 2011","","","IEEE","IEEE Conferences"
"Online Hybrid Learning to Speed Up Deep Reinforcement Learning Method for Commercial Aircraft Control","M. Xin; Y. Gao; T. Mou; J. Ye","Dept. Automation, Shanghai Jiao Tong University, Shanghai, China; Dept. of Automation Shanghai Jiao Tong University, MoE Key Lab of Artificial Intelligence, Shanghai, China; Dept. Automation, Shanghai Jiao Tong University, Shanghai, China; Dept. Automation, Shanghai Jiao Tong University, Shanghai, China","2019 3rd International Symposium on Autonomous Systems (ISAS)","11 Jul 2019","2019","","","305","310","We propose an online hybrid learning algorithm that enables deep reinforcement learning agents to learn in environments where the cost of exploration is expensive. Our algorithm adopts ideas from imitation learning and Deep Deterministic Policy Gradient (DDPG). It utilizes an existing baseline controller to speed up the process of learning as well as lower the exploration cost. Our algorithm is validated on classic pendulum swing-up problem and shows faster convergence speed and lower exploration cost. Furthermore, the algorithm can also be applied in learning a controller for commercial aircraft cruising. While DDPG fails to learn a decent policy, our hybrid learning algorithm is able to learn quickly in an online manner with low cost. Our experiments show that the learned policy network is more robust than the baseline PID controller.","","978-1-7281-1298-5","10.1109/ISASS.2019.8757756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8757756","Hybrid Learning;Deep Reinforcement Learning;Commercial Aircraft Control","Training;Aerospace control;Aircraft;Reinforcement learning;Process control;Neural networks;Automation","aircraft control;gradient methods;learning (artificial intelligence);learning systems;pendulums;three-term control","DDPG;deep reinforcement;commercial aircraft control;online hybrid learning algorithm;imitation learning;deep deterministic policy gradient;PID controller;pendulum swing-up problem","","2","","22","IEEE","11 Jul 2019","","","IEEE","IEEE Conferences"
"Curiosity-Driven Exploration for Off-Policy Reinforcement Learning Methods","B. Li; T. Lu; J. Li; N. Lu; Y. Cai; S. Wang","Research Center on Intelligent Robotic Systems, Institute of Automation, Chinese Academy of Sciences; Research Center on Intelligent Robotic Systems, Institute of Automation, Chinese Academy of Sciences; Research Center on Intelligent Robotic Systems, Institute of Automation, Chinese Academy of Sciences; Research Center on Intelligent Robotic Systems, Institute of Automation, Chinese Academy of Sciences; Research Center on Intelligent Robotic Systems, Institute of Automation, Chinese Academy of Sciences; Research Center on Intelligent Robotic Systems, Institute of Automation, Chinese Academy of Sciences","2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)","20 Jan 2020","2019","","","1109","1114","Deep reinforcement learning (DRL) has achieved remarkable results in many high-dimensional continuous control tasks. However, the RL agent still explores the environment randomly, resulting in low exploration efficiency and learning performance, especially in robotic manipulation tasks with sparse rewards. To address this problem, in this paper, we intro-duce a simplified Intrinsic Curiosity Module (S-ICM) into the off-policy RL methods to encourage the agent to pursue novel and surprising states for improving the exploration competence. This method can be combined with an arbitrary off-policy RL algorithm. We evaluate our approach on three challenging robotic manipulation tasks provided by OpenAI Gym. In our experiments, we combined our method with Deep Deterministic Policy Gradient (DDPG) with and without Hindsight Experience Replay (HER). The empirical results show that our proposed method significantly outperforms vanilla RL algorithms both in sample-efficiency and learning performance.","","978-1-7281-6321-5","10.1109/ROBIO49542.2019.8961529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8961529","Intrinsic Curiosity Module;Deep Deterministic Policy Gradient;Robotic Manipulation Tasks","Conferences;Biomimetics;Biological system modeling;Reinforcement learning;Task analysis;Robots","learning (artificial intelligence);manipulators","deep deterministic policy gradient;vanilla RL;sample-efficiency;learning performance;curiosity-driven exploration;DRL;high-dimensional continuous control tasks;RL agent;sparse rewards;exploration competence;off-policy RL algorithm;robotic manipulation tasks;simplified intrinsic curiosity module;off-Policy reinforcement learning methods;off-policy reinforcement learning methods","","6","","22","IEEE","20 Jan 2020","","","IEEE","IEEE Conferences"
"Attention Enhanced Reinforcement Learning for Multi agent Cooperation","Z. Pu; H. Wang; Z. Liu; J. Yi; S. Wu","Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China.; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China, and also with the Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China; Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China.; Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China.; Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China.","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","15","In this article, a novel method, called attention enhanced reinforcement learning (AERL), is proposed to address issues including complex interaction, limited communication range, and time-varying communication topology for multi agent cooperation. AERL includes a communication enhanced network (CEN), a graph spatiotemporal long short-term memory network (GST-LSTM), and parameters sharing multi-pseudo critic proximal policy optimization (PS-MPC-PPO). Specifically, CEN based on graph attention mechanism is designed to enlarge the agents' communication range and to deal with complex interaction among the agents. GST-LSTM, which replaces the standard fully connected (FC) operator in LSTM with graph attention operator, is designed to capture the temporal dependence while maintaining the spatial structure learned by CEN. PS-MPC-PPO, which extends proximal policy optimization (PPO) in multi agent systems with parameters' sharing to scale to environments with a large number of agents in training, is designed with multi-pseudo critics to mitigate the bias problem in training and accelerate the convergence process. Simulation results for three groups of representative scenarios including formation control, group containment, and predator-prey games demonstrate the effectiveness and robustness of AERL.","2162-2388","","10.1109/TNNLS.2022.3146858","National Key Research and Development Program of China(grant numbers:2018AAA0102404); National Natural Science Foundation of China(grant numbers:62073323,61806199); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDA27030403); External Cooperation Key Project of Chinese Academy Sciences(grant numbers:173211KYSB20200002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9716772","Attention mechanism;deep reinforcement learning (DRL);graph convolutional networks;multi agent systems.","Training;Reinforcement learning;Games;Scalability;Task analysis;Standards;Optimization","","","","4","","","IEEE","18 Feb 2022","","","IEEE","IEEE Early Access Articles"
"Evolutionary Deep Reinforcement Learning for Volt-VAR Control in Distribution Network","R. Si; T. Gao; Y. Dai; Y. Bai; Y. Jiang; J. Zhang","School of Electrical Engineering and Automation, Wuhan University; School of Electrical Engineering and Automation, Wuhan University; School of Electrical Engineering and Automation, Wuhan University; School of Electrical Engineering and Automation, Wuhan University; School of Electrical Engineering and Automation, Wuhan University; School of Electrical Engineering and Automation, Wuhan University","2022 IEEE 2nd International Conference on Digital Twins and Parallel Intelligence (DTPI)","3 Jan 2023","2022","","","1","6","As an important form of renewable energy integrated to the power system, distribution network is being challenged by voltage violation and network loss increase. Currently, model-based Vol-Var control (VVC) methods are widely used to reduce voltage violation and network loss. However, model-based methods need accurate parameters of distribution network. In practice, accurate model is difficult to obtain. In this paper, we propose a model-free evolutionary deep reinforcement learning (E-DRL) algorithm to solve the VVC problem. Based on E-DRL, the agent evolves autonomously by continuously interacting with the environment learning control strategy. Inverter-based PVs and SVGs are used to provide fast and continuous control. VVC problem is solved by soft actor-critic algorithm, which uses the maximum entropy technique to balance the exploration and exploitation. Numerical simulations on IEEE 13-bus system demonstrate that the proposed method has satisfied performance.","","978-1-6654-9227-0","10.1109/DTPI55838.2022.9998947","National Key R&D Program of China(grant numbers:2021ZD0112700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9998947","Volt-Var control;reinforcement learning;soft actorcritic;PV inverters;reactive power","Deep learning;Renewable energy sources;Reactive power;Distribution networks;Reinforcement learning;Numerical simulation;Entropy","control engineering computing;deep learning (artificial intelligence);evolutionary computation;power distribution control;power engineering computing;reinforcement learning;voltage control","continuous control;control strategy;distribution network;E-DRL;environment learning control;IEEE 13-bus system;model-free evolutionary deep reinforcement learning;network loss;soft actor-critic algorithm;volt-VAR control;voltage violation;VVC problem","","","","17","IEEE","3 Jan 2023","","","IEEE","IEEE Conferences"
"Design and implementation of power grid operation environment simulation system based on Reinforcement Learning","Y. Ling; W. Jiaqi; Y. Nan; X. Wenyue; L. Dapeng; F. Qiong","Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology(China Electric Power Research Institute), Beijing, China; Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology(China Electric Power Research Institute), Beijing, China; Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology(China Electric Power Research Institute), Beijing, China; Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology(China Electric Power Research Institute), Beijing, China; Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology(China Electric Power Research Institute), Beijing, China; Beijing Key Laboratory of Research and System Evaluation of Power Dispatching Automation Technology(China Electric Power Research Institute), Beijing, China","2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)","3 Feb 2022","2021","2","","935","939","With the rapid and profound changes in all aspects of power system, the control scale increases exponentially, the characteristics of control objects vary greatly, the bilateral uncertainty of source load increases, and the power grid real-time dispatching will be more complex and frequent. In this paper, we combine current advanced artificial intelligence technology and power grid dispatching business needs, design and develop power grid operation environment simulation system based on reinforcement learning. The system determines the power grid scenario according to the user's choice, applies the real-time power grid operation data, directly interacts with the decision agent, realizes the agent's autonomous learning and continuously improves its regulation and decision-making ability. The system has good feasibility, practicability and scalability, realizes the standardization of workflow, and provides an effective decision support tool for power grid dispatching.","","978-1-6654-2877-4","10.1109/ICIBA52610.2021.9688106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688106","artificial intelligence;reinforcement learning;power dispatching;aid decision making","Training;Uncertainty;Decision making;Reinforcement learning;Power grids;Real-time systems;Dispatching","decision making;decision support systems;power engineering computing;power grids;reinforcement learning","power grid operation environment simulation system;reinforcement learning;power grid real-time dispatching;power grid dispatching business needs;power grid scenario;real-time power grid operation data;decision support tool","","","","5","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"Evaluating Renewable Energy Policies Using a Multi-agent Reinforcement Learning Model","M. Suzuki; M. Ito; R. Takashima","Dept. of Industrial Administration, Tokyo University of Science, Chiba, Japan; Dept. of Industrial Administration, Tokyo University of Science, Chiba, Japan; Dept. of Industrial Administration, Tokyo University of Science, Chiba, Japan","2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","17 Jan 2019","2018","","","959","963","Even as governments combat greenhouse emissions through a range of initiatives, it has yet to be clarified how renewable energy policy, energy market structure, and number of energy producers impact social welfare. We model a deregulated market for electricity as a blind single-price call auction and construct a multi-agent system with reinforcement learning that facilitates more realistic market evaluations and observation of equilibrium processes. We validate our simulation by comparing its results with the results from theoretical analysis in a simplified market.","2577-1655","978-1-5386-6650-0","10.1109/SMC.2018.00170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8616166","renewable energy;multi-agent simulation;reinforcement learning","Renewable energy sources;Portfolios;Standards;Analytical models;Production;Reinforcement learning;Electricity supply industry","learning (artificial intelligence);multi-agent systems;power engineering computing;power markets;pricing;renewable energy sources","renewable energy policy;energy market structure;energy producers;deregulated market;blind single-price call auction;multiagent system;realistic market evaluations;simplified market;multiagent reinforcement learning model;greenhouse emissions","","1","","5","IEEE","17 Jan 2019","","","IEEE","IEEE Conferences"
"Moving Object Flexible Grasping Based on Deep Reinforcement Learning","Z. Tu; C. Yang; X. Wu; Y. Zhu; W. Wu; N. Jia","State Key Laboratory of Fluid, Power and Mechatronic Systems, Zhejiang University, Hangzhou, China; State Key Laboratory of Fluid, Power and Mechatronic Systems, Zhejiang University, Hangzhou, China; State Key Laboratory of Fluid, Power and Mechatronic Systems, Zhejiang University, Hangzhou, China; State Key Laboratory of Fluid, Power and Mechatronic Systems, Zhejiang University, Hangzhou, China; State Key Laboratory of Fluid, Power and Mechatronic Systems, Zhejiang University, Hangzhou, China; Training Platform of Robots and Intelligent, Manufacturing from Polytechnic Institute, Zhejiang University, Hangzhou, China","2022 8th International Conference on Control, Automation and Robotics (ICCAR)","31 May 2022","2022","","","34","39","High manipulability can improve the movement ability of the robot end-effector in various degrees of freedom when it is engaged in autonomous grasping. In this paper, we propose a flexible grasping method based on deep reinforcement learning for moving objects. We use the deep deterministic policy gradient (DDPG) algorithm to train and control a six degrees of freedom manipulator, and introduce the manipulability index to optimize the grasping pose of the manipulator. A moving ball grasping mission was conducted using the Robot Operating System (ROS) and the Gazebo simulator to verify the effectiveness of the method. Compared with the DDPG algorithm without optimizing manipulability and the traditional tracking method, experimental results indicate that the proposed method maintains the high manipulability of the manipulator.","2251-2454","978-1-6654-8116-8","10.1109/ICCAR55106.2022.9782595","National Natural Science Foundation of China(grant numbers:52071292); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782595","Moving object grasping;Deep reinforcement learning;Manipulability","Tracking;Operating systems;Grasping;Reinforcement learning;Manipulators;Approximation algorithms;End effectors","control engineering computing;deep learning (artificial intelligence);end effectors;operating systems (computers);path planning;reinforcement learning;robot programming","DDPG algorithm;high manipulability;manipulator;object flexible grasping;deep reinforcement learning;movement ability;robot end-effector;autonomous grasping;flexible grasping method;moving objects;deep deterministic policy gradient;manipulability index;moving ball grasping mission;Robot Operating System;Gazebo simulator","","2","","18","IEEE","31 May 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Pushing and Grasping Objects from Ungraspable Poses","H. Zhang; H. Liang; L. Cong; J. Lyu; L. Zeng; P. Feng; J. Zhang","Division of Advanced Manufacturing, Shenzhen International Graduate School, Tsinghua University; Dept. of Informatics, Group TAMS, Universität Hamburg; Dept. of Informatics, Group TAMS, Universität Hamburg; Dept. of Informatics, Group TAMS, Universität Hamburg; Division of Advanced Manufacturing, Shenzhen International Graduate School, Tsinghua University; Division of Advanced Manufacturing, Shenzhen International Graduate School, Tsinghua University; Dept. of Informatics, Group TAMS, Universität Hamburg","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","3860","3866","Grasping an object when it is in an ungraspable pose is a challenging task, such as books or other large flat objects placed horizontally on a table. Inspired by human manipulation, we address this problem by pushing the object to the edge of the table and then grasping it from the hanging part. In this paper, we develop a model-free Deep Reinforcement Learning framework to synergize pushing and grasping actions. We first pre-train a Variational Autoencoder to extract high-dimensional features of input scenario images. One Proximal Policy Optimization algorithm with the common reward and sharing layers of Actor-Critic is employed to learn both pushing and grasping actions with high data efficiency. Experiments show that our one network policy can converge 2.5 times faster than the policy using two parallel networks. Moreover, the experiments on unseen objects show that our policy can generalize to the challenging case of objects with curved surfaces and off-center irregularly shaped objects. Lastly, our policy can be transferred to a real robot without fine-tuning by using CycleGAN for domain adaption and outperforms the push-to-wall baseline.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160491","German Research Foundation (DFG)(grant numbers:DFG TRR-169); National Science Foundation of China (NSFC)(grant numbers:NSFC 62061136001,61972220); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160491","","Training;Shape;Stacking;Grasping;Reinforcement learning;Feature extraction;Trajectory","deep learning (artificial intelligence);learning (artificial intelligence);optimisation;reinforcement learning","common reward;flat objects;high data efficiency;high-dimensional features;human manipulation;input scenario images;model-free Deep Reinforcement;off-center irregularly shaped objects;one network policy;Proximal Policy Optimization algorithm;push-to-wall baseline;pushing grasping actions;sharing layers;ungraspable poses;unseen objects","","","","24","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Constrained Reinforcement Learning for Dynamic Material Handling","C. Hu; Z. Wang; J. Liu; J. Wen; B. Mao; X. Yao","Research Institute of Trustworthy Autonomous Systems (RITAS), Southern University of Science and Technology, Shenzhen, China; Research Institute of Trustworthy Autonomous Systems (RITAS), Southern University of Science and Technology, Shenzhen, China; Department of Computer Science and Engineering, Guangdong Key Laboratory of Brain-inspired Intelligent Computation, Southern University of Science and Technology, Shenzhen, China; Trustworthiness Theory Research Center, Huawei Technologies Co., Ltd, Shenzhen, China; Trustworthiness Theory Research Center, Huawei Technologies Co., Ltd, Shenzhen, China; Department of Computer Science and Engineering, Guangdong Key Laboratory of Brain-inspired Intelligent Computation, Southern University of Science and Technology, Shenzhen, China","2023 International Joint Conference on Neural Networks (IJCNN)","2 Aug 2023","2023","","","1","9","As one of the core parts of flexible manufacturing systems, material handling involves storage and transportation of materials between workstations with automated vehicles. The improvement in material handling can impulse the overall efficiency of the manufacturing system. However, the occurrence of dynamic events during the optimisation of task arrangements poses a challenge that requires adaptability and effectiveness. In this paper, we aim at the scheduling of automated guided vehicles for dynamic material handling. Motivated by some real-world scenarios, unknown new tasks and unexpected vehicle breakdowns are regarded as dynamic events in our problem. We formulate the problem as a constrained Markov decision process which takes into account tardiness and available vehicles as cumulative and instantaneous constraints, respectively. An adaptive constrained reinforcement learning algorithm that combines Lagrangian relaxation and invalid action masking, named RCPOM, is proposed to address the problem with two hybrid constraints. Moreover, a gym-like dynamic material handling simulator, named DMH-GYM, is developed and equipped with diverse problem instances, which can be used as benchmarks for dynamic material handling. Experimental results on the problem instances demonstrate the outstanding performance of our proposed approach compared with eight state-of-the-art constrained and non-constrained reinforcement learning algorithms, and widely used dispatching rules for material handling.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10191999","National Natural Science Foundation of China(grant numbers:62250710682,61906083); Shenzhen Fundamental Research Program(grant numbers:JCYJ20190809121403553); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191999","Dynamic material handling;constrained reinforcement learning;automated guided vehicle;manufacturing system;benchmark","Electric breakdown;Heuristic algorithms;Materials handling;Reinforcement learning;Markov processes;Dynamic scheduling;Dispatching","automatic guided vehicles;dispatching;flexible manufacturing systems;Markov processes;materials handling equipment;optimisation;production engineering computing;reinforcement learning;scheduling;transportation","adaptive constrained reinforcement learning algorithm;automated guided vehicles;dispatching rules;DMH-GYM;dynamic material handling;flexible manufacturing systems;Lagrangian relaxation;Markov decision process;materials storage;optimisation;RCPOM;scheduling;task arrangements;transportation;workstations","","","","37","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning With Adversarial Training for Automated Excavation Using Depth Images","T. Osa; M. Aizawa","Department of Human Intelligence Systems, Kyushu Institute of Technology, Fukuoka, Japan; Komatsu Ltd., Kanagawa, Japan","IEEE Access","17 Jan 2022","2022","10","","4523","4535","Excavation, which is one of the most frequently performed tasks during construction often poses danger to human operators. To reduce potential risks and address the problem of workforce shortage, automation of excavation is essential. Although previous studies have yielded promising results based on the use of reinforcement learning (RL) for automated excavation, the properties of excavation task in the context of RL have not been sufficiently investigated. In this study, we investigate Qt-Opt, which is a variant of Q-learning algorithms for continuous action space, for learning the excavation task using depth images. Inspired by virtual adversarial training in supervised learning, we propose a regularization method that uses virtual adversarial samples to reduce overestimation of Q-values in a Q-learning algorithm. Our results reveal that Qt-Opt is more sample-efficient than state-of-the-art actor-critic methods in our problem setting, and we verify that the proposed method further improves the sample efficiency of Qt-Opt. Our results demonstrate that multiple optimal actions often exist within the process of excavation and the choice of policy representation is crucial for satisfactory performance.","2169-3536","","10.1109/ACCESS.2022.3140781","Kyushu Institute of Technology and Komatsu Company Ltd; JSPS KAKENHI(grant numbers:JP19K20370); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672107","Construction industry;automation;machine learning;reinforcement learning","Excavation;Construction industry;Training data;Q-learning;Automation;Approximation algorithms;Machine learning;Reinforcement learning","deep learning (artificial intelligence);excavators;image processing;reinforcement learning;structural engineering computing","sample efficiency;Qt-Opt;deep reinforcement learning;automated excavation;depth images;frequently performed tasks;human operators;workforce shortage;excavation task;Q-learning algorithm;continuous action space;virtual adversarial training;supervised learning;virtual adversarial samples;state-of-the-art actor-critic methods;RL","","8","","50","CCBY","6 Jan 2022","","","IEEE","IEEE Journals"
"Gantry Work Cell Scheduling through Reinforcement Learning with Knowledge-guided Reward Setting","X. Ou; Q. Chang; J. Arinez; J. Zou","Stony Brook University, Stony Brook, NY, USA; Stony Brook University, Stony Brook, NY, USA; Research and Development Center, General Motors, Warren, MI, USA; Stony Brook University, Stony Brook, NY, USA","IEEE Access","2 Apr 2018","2018","6","","14699","14709","In this paper, a manufacturing work cell utilizing gantries to move between machines for loading and unloading materials/parts is considered. The production performance of the gantry work cell highly depends on the gantry movements in real operation. This paper formulates the gantry scheduling problem as a reinforcement learning problem, in which an optimal gantry moving policy is solved to maximize the system output. The problem is carried out by the Q-learning algorithm. The gantry system is analyzed and its real-time performance is evaluated by permanent production loss and production loss risk, which provide a theoretical base for defining reward function in the Q-learning algorithm. A numerical study is performed to demonstrate the effectiveness of the proposed policy by comparing with the first-comefirst-served policy.","2169-3536","","10.1109/ACCESS.2018.2800641","U.S. National Science Foundation(grant numbers:CMMI 1435534,1351160); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8283823","Equivalent serial line;gantry scheduling;production loss risk;Q-learning;reinforcement learning","Job shop scheduling;Real-time systems;Learning (artificial intelligence);Service robots","cranes;learning (artificial intelligence);manufacturing systems;numerical analysis;real-time systems;risk management;scheduling","real-time performance;production loss risk;Q-learning algorithm;gantry work cell scheduling;reinforcement learning problem;optimal gantry moving policy;knowledge-guided reward setting;numerical analysis;materials loading;materials unloading;manufacturing systems","","16","","31","OAPA","8 Feb 2018","","","IEEE","IEEE Journals"
"Co-Design of Control, Computation and Network Scheduling Based on Reinforcement Learning","Y. Liu; P. Zeng; J. Cui; C. Xia","Key Laboratory of Networked Control Systems, Chinese Academy of Sciences, Shenyang, China; Key Laboratory of Networked Control Systems, Chinese Academy of Sciences, Shenyang, China; Jilin University, Changchun, China; Key Laboratory of Networked Control Systems, Chinese Academy of Sciences, Shenyang, China","IEEE Internet of Things Journal","","2023","PP","99","1","1","Computationally intensive control tasks, especially the control of image-based mobile controllers, usually require more computational capacity and network transmission resources than traditional control tasks due to the assembly of camera-based visual perception sensors. As a result, the collaborative design of network, computation, and control is of great importance to improve the efficiency of resource usage while ensuring control performance, thus achieving overall system optimality. In this paper, we proposed a synergistic algorithm for network and computational resource scheduling through Deep Deterministic Policy Gradient and control based on self-triggered model predictive control, which provides optimal resource scheduling instructions after observing the system state in real-time, and uses these resources to complete control tasks in the controller with a self-triggering mechanism to achieve the goal of ensuring control performance and reducing energy consumption. Finally, we designed numerical simulation experiments to verify the effectiveness of the algorithm proposed in this paper.","2327-4662","","10.1109/JIOT.2023.3305708","National Natural Science Foundation of China(grant numbers:62003079,92067205); Natural Science Foundation of Liaoning Province(grant numbers:2023-BS-029); Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2020207); National Key Research and Development Program of China(grant numbers:2022YFB3304000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10220202","Co-design framework;reinforcement learning;self-triggered;model predictive control;resource scheduling","Processor scheduling;Optimization;Task analysis;Control systems;State estimation;Sensors;Real-time systems","","","","","","","IEEE","16 Aug 2023","","","IEEE","IEEE Early Access Articles"
"SmartHART: A Priority-aware Scheduling and Routing Scheme for IIoT Networks using Deep Reinforcement Learning","S. Chilukuri; A. Gupta; H. S. Sai Pulamolu","Department of Computer Science and Engineering, GITAM School of Technology, GITAM (deemed to be University), Visakhapatnam, India; Department of Computer Science and Engineering, GITAM School of Technology, GITAM (deemed to be University), Visakhapatnam, India; Department of Computer Science and Engineering, GITAM School of Technology, GITAM (deemed to be University), Visakhapatnam, India","2023 15th International Conference on COMmunication Systems & NETworkS (COMSNETS)","15 Feb 2023","2023","","","452","456","The aims of the Industrial Internet of Things (IIoT) are improved efficiency of production facilities and operations in factories and better logistics and supply chain management. Communication of data in the IIoT can be challenging due to diverse and critical demands on the Quality of Service (QoS). Medium Access Control (MAC) in such networks is typically using Time Division Multiple Access as recommended by stan-dards such as the WirelessHART. In this paper, we propose and evaluate a Deep Reinforcement Learning (DRL)-based scheduling and routing scheme (called SmartHART) for WirelessHART networks. SmartHART leverages on redundancy in the network and chooses schedules and routes that minimize the end-to-end delay of data, taking priority of data into consideration. The reward design of the SmartHART agent also minimizes the maximum packet queue length in a network, making it suitable for memory-constrained field devices. Simulation results show that SmartHART can reduce the end-to-end delay by as much as 18% and the maximum packet queue size by up to 60%.","2155-2509","978-1-6654-7706-2","10.1109/COMSNETS56262.2023.10041352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10041352","Deep Reinforcement Learning;Industrial IoT;scheduling and routing","Deep learning;Time division multiple access;Job shop scheduling;Supply chain management;Reinforcement learning;Quality of service;Routing","access protocols;deep learning (artificial intelligence);Internet of Things;production engineering computing;quality of service;queueing theory;reinforcement learning;scheduling;supply chain management;telecommunication network routing;telecommunication scheduling;time division multiple access","critical demands;Deep Reinforcement Learning;diverse demands;end-to-end delay;IIoT networks;logistics;maximum packet queue length;Medium Access Control;production facilities;routing scheme;SmartHART agent;SmartHART leverages;stan-dards;supply chain management;Time Division Multiple Access;WirelessHART networks","","","","13","IEEE","15 Feb 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Device Scheduling for Renewable Energy-Powered Federated Learning","Y. Cui; K. Cao; T. Wei","School of Computer Science and Technology, Shanghai Key Laboratory of Trustworthy Computing, and Shanghai Trusted Industry Internet Software Collaborative Innovation Center, East China Normal University, Shanghai, China; College of Information Science and Technology, Jinan University, Guangzhou, China; School of Computer Science and Technology, Shanghai Key Laboratory of Trustworthy Computing, and Shanghai Trusted Industry Internet Software Collaborative Innovation Center, East China Normal University, Shanghai, China","IEEE Transactions on Industrial Informatics","4 May 2023","2023","19","5","6264","6274","Due to its unique privacy protection advantages, emerging federated learning (FL) is regarded as a significant technique to enable Industry 4.0. However, the industrial deployment of FL encounters the primary obstacles of limited device energy and system communication resources. Nowadays, renewable energy-powered devices have been deployed in various industrial fields to tackle the challenges of unsustainable and limited energy of battery-powered devices. Inspired by this, this article proposes a novel FL protocol to groundbreakingly improve the performance of renewable energy-powered FL systems. Specifically, with the underlying theory of FL as the guide, the proposed protocol features a reinforcement learning-based device scheduling solution to adapt to intermittent renewable energy supply. Following this device scheduling solution, an integer linear programming-based bandwidth management scheme is introduced to optimize communication efficiency. Experimental results on two representative data distribution situations demonstrate that compared with the state-of-the-art schemes, our FL protocol can boost up to 36.63% and 50.99% accuracy, respectively.","1941-0050","","10.1109/TII.2022.3210008","National Natural Science Foundation of China(grant numbers:62272169,62102164); China Postdoctoral Science Foundation(grant numbers:2021T140272,2021M691240); Science and Technology Project of Guangzhou(grant numbers:202201010573); Fundamental Research Funds for the Central Universities(grant numbers:21621025); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9904320","Bandwidth management;device scheduling;federated learning (FL);reinforcement learning (RL);renewable energy","Training;Renewable energy sources;Internet of Things;Protocols;Data models;Energy consumption;Job shop scheduling","data privacy;integer programming;learning (artificial intelligence);linear programming;production engineering computing;protocols;reinforcement learning","battery-powered devices;device energy;industrial deployment;industrial fields;intermittent renewable energy supply;limited energy;novel FL protocol;primary obstacles;reinforcement learning-based device scheduling solution;renewable energy-powered devices;renewable energy-powered federated learning;renewable energy-powered FL systems;significant technique;system communication resources;unique privacy protection advantages;unsustainable energy","","1","","29","IEEE","27 Sep 2022","","","IEEE","IEEE Journals"
"Object surveillance using reinforcement learning based sensor dispatching","M. D. Naish; E. A. Croft; B. Banhabib","Dept. of Mechanical and Materials Engineering, University of Western Ontario, London, Ontario, Canada; Industrial Automation Laboratory, University of British Columbia, Vancouver, B.C., Canada; NA","IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004","6 Jul 2004","2004","1","","71","76 Vol.1","This paper outlines an approach to the coordination of multiple mobile sensors for the surveillance of a single moving target. A real-time dispatching algorithm is used to select and position groups of sensors in response to the observed object motion. The aim is to provide robust, high-quality data while ensuring that the system can react to unexpected object manoeuvres. Sensors are assigned to collect data at specific points on the object trajectory. A dispatching strategy learned via reinforcement learning is used to control the sensor poses with respect to these points. In using the learned strategy, each sensor adopts an egocentric view of the system state to determine the most appropriate action. Simulations demonstrate the performance of the RL-based dispatcher, in comparison to similar static-sensor systems.","1050-4729","0-7803-8232-3","10.1109/ROBOT.2004.1307131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307131","","Surveillance;Learning;Dispatching;Sensor systems;Robustness;Sensor phenomena and characterization;Sensor fusion;Mechanical sensors;Laboratories;Manufacturing automation","surveillance;learning (artificial intelligence);dispatching;sensor fusion","object surveillance;reinforcement learning;sensor dispatching;multiple mobile sensor;static-sensor system;object trajectory","","","","14","IEEE","6 Jul 2004","","","IEEE","IEEE Conferences"
"Enhancing Data Monitoring Scheme Based on Reinforcement Learning in IIoT Systems","Y. Dong; G. Qin; H. Tian","China Academy of Information and Communications Technology, Institute of Security Research, Beijing, China; China Academy of Information and Communications Technology, Institute of Security Research, Beijing, China; China Academy of Information and Communications Technology, Institute of Security Research, Beijing, China","2020 12th International Conference on Communication Software and Networks (ICCSN)","13 Jul 2020","2020","","","69","72","The high-efficiency and security-guarantee of Industrial Internet of Things systems are critical in industry area. In this paper, the novel data collection procedure is established to monitor and classify the data flow in IIoT systems firstly. Then, a Q-learning algorithm based on reinforcement learning is introduced to detect the system fault and equipment error in IIoT systems. Simulation results show that the proposed data collection procedure and data analysis method can detect the equipment fault more efficiently than before.","2472-8489","978-1-7281-9815-6","10.1109/ICCSN49894.2020.9139067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139067","Industrial Internet of Things;Data monitoring;Reinforcement learning;Q-learning","Data analysis;Q-learning;Simulation;Software algorithms;Data collection;Software;Classification algorithms","data analysis;Internet of Things;learning (artificial intelligence);production engineering computing","data monitoring;reinforcement learning;IIoT systems;security-guarantee;industry area;data collection procedure;data flow;system fault;equipment error;data analysis;Industrial Internet of Things","","2","","8","IEEE","13 Jul 2020","","","IEEE","IEEE Conferences"
"Sim-to-Real Surgical Robot Learning and Autonomous Planning for Internal Tissue Points Manipulation Using Reinforcement Learning","Y. Ou; M. Tavakoli","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada","IEEE Robotics and Automation Letters","20 Mar 2023","2023","8","5","2502","2509","Indirect simultaneous positioning (ISP), where internal tissue points are placed at desired locations indirectly through the manipulation of boundary points, is a type of subtask frequently performed in robotic surgeries. Although challenging due to complex tissue dynamics, automating the task can potentially reduce the workload of surgeons. This letter presents a sim-to-real framework for learning to automate the task without interacting with a real environment, and for planning preoperatively to find the grasping points that minimize local tissue deformation. A control policy is learned using deep reinforcement learning (DRL) in the FEM-based simulation environment and transferred to real-world situation. Grasping points are planned in the simulator by utilizing the trained policy using Bayesian optimization (BO). Inconsistent simulation performance is overcome by formulating the problem as a state augmented Markov decision process (MDP). Experimental results show that the learned policy places the internal tissue points accurately, and that the planned grasping points yield small tissue deformation among the trials. The proposed learning and planning scheme is able to automate internal tissue point manipulation in surgeries and has the potential to be generalized to complex surgical scenarios.","2377-3766","","10.1109/LRA.2023.3254860","Natural Sciences and Engineering Research Council of Canada; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10065553","Medical robots and systems;reinforcement learning;dual arm manipulation;task and motion planning;surgical automation","Grasping;Surgery;Task analysis;Planning;Deformable models;Automation;Finite element analysis","deep learning (artificial intelligence);manipulators;Markov processes;medical computing;medical robotics;position control;reinforcement learning;robot programming;surgery","autonomous planning;boundary points;complex tissue dynamics;deep reinforcement learning;FEM-based simulation environment;inconsistent simulation performance;indirect simultaneous positioning;internal tissue point manipulation;internal tissue points manipulation;learned policy places;local tissue deformation;planned grasping points;planning scheme;robotic surgeries;sim-to-real framework;sim-to-real surgical robot learning","","","","26","IEEE","10 Mar 2023","","","IEEE","IEEE Journals"
"PRIMAL$_2$: Pathfinding Via Reinforcement and Imitation Multi-Agent Learning - Lifelong","M. Damani; Z. Luo; E. Wenzel; G. Sartoretti","Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore","IEEE Robotics and Automation Letters","18 Mar 2021","2021","6","2","2666","2673","Multi-agent path finding (MAPF) is an indispensable component of large-scale robot deployments in numerous domains ranging from airport management to warehouse automation. In particular, this work addresses lifelong MAPF (LMAPF) - an online variant of the problem where agents are immediately assigned a new goal upon reaching their current one - in dense and highly structured environments, typical of real-world warehouse operations. Effectively solving LMAPF in such environments requires expensive coordination between agents as well as frequent replanning abilities, a daunting task for existing coupled and decoupled approaches alike. With the purpose of achieving considerable agent coordination without any compromise on reactivity and scalability, we introduce PRIMAL2, a distributed reinforcement learning framework for LMAPF where agents learn fully decentralized policies to reactively plan paths online in a partially observable world. We extend our previous work, which was effective in low-density sparsely occupied worlds, to highly structured and constrained worlds by identifying behaviors and conventions which improve implicit agent coordination, and enable their learning through the construction of a novel local agent observation and various training aids. We present extensive results of PRIMAL2 in both MAPF and LMAPF environments and compare its performance to state-of-the-art planners in terms of makespan and throughput. We show that PRIMAL2 significantly surpasses our previous work and performs comparably to these baselines, while allowing real-time re-planning and scaling up to 2048 agents.","2377-3766","","10.1109/LRA.2021.3062803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9366340","Deep learning in robotics and automation;distributed robot systems;multi-robot systems","Robots;Robot kinematics;Task analysis;Planning;Scalability;Reinforcement learning;Training","learning (artificial intelligence);multi-agent systems;multi-robot systems;path planning;planning (artificial intelligence);warehouse automation","local agent observation;implicit agent coordination;highly structured constrained worlds;partially observable world;distributed reinforcement learning framework;reactivity;considerable agent coordination;frequent replanning abilities;expensive coordination;real-world warehouse operations;highly structured environments;dense environments;online variant;lifelong MAPF;warehouse automation;airport management;numerous domains;large-scale robot deployments;multiagent path finding;imitation multiagent learning;pathfinding via reinforcement;LMAPF environments;PRIMAL","","37","","31","IEEE","1 Mar 2021","","","IEEE","IEEE Journals"
"Deep-Reinforcement-Learning-Based Path Planning for Industrial Robots Using Distance Sensors as Observation","T. Bhuiyan; L. Kästner; Y. Hu; B. Kutschank; J. Lambrecht","Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany","2023 8th International Conference on Control and Robotics Engineering (ICCRE)","26 Jun 2023","2023","","","204","210","Traditionally, collision-free path planning for industrial robots is realized by sampling-based algorithms such as RRT (Rapidly-exploring Random Tree), PRM (Probabilistic Roadmap), etc. Sampling-based algorithms require long computation times, especially in complex environments. Furthermore, the environment in which they are employed needs to be known beforehand. When utilizing these approaches in new environments, a tedious engineering effort in setting hyperparameters needs to be conducted, which is time- and cost-intensive. On the other hand, DRL (Deep Reinforcement Learning) has shown remarkable results in dealing with complex environments, generalizing new problem instances, and solving motion planning problems efficiently. On that account, this paper proposes a Deep-Reinforcement-Learning-based motion planner for robotic manipulators. We propose an easily reproducible method to train an agent in randomized scenarios achieving generalization for unknown environments. We evaluated our model against state-of-the-art sampling- and DRL-based planners in several experiments containing static and dynamic obstacles. Results show the adaptability of our agent in new environments and the superiority in terms of path length and execution time compared to conventional methods. Our code is available on GitHub [1].","2835-3722","979-8-3503-4565-0","10.1109/ICCRE57112.2023.10155608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155608","Deep Reinforcement Learning;Path Planning;Industrial Robots;Automation","Adaptation models;Stars;Reinforcement learning;Industrial robots;Probabilistic logic;Planning;Trajectory","collision avoidance;deep learning (artificial intelligence);industrial robots;learning (artificial intelligence);manipulators;mobile robots;path planning;reinforcement learning;trees (mathematics)","collision-free path;complex environments;Deep Reinforcement;Deep-Reinforcement-Learning-based motion planner;Deep-Reinforcement-Learning-based path planning;distance sensors;DRL-based planners;execution time;industrial robots;long computation times;motion planning problems;path length;Probabilistic Roadmap;randomized scenarios;Rapidly-exploring Random Tree;robotic manipulators;sampling-based algorithms;state-of-the-art sampling;tedious engineering effort","","1","","30","IEEE","26 Jun 2023","","","IEEE","IEEE Conferences"
"Automatic Generation Control with Competing GENCOs-A Reinforcement Learning Based Approach","D. Jay; K. S. Swarup","Department of Electrical Engineering, Indian Institute Of Technology, Chennai; Department of Electrical Engineering, Indian Institute Of Technology, Chennai","2018 20th National Power Systems Conference (NPSC)","25 Jul 2019","2018","","","1","5","Under deregulation, Automatic Generation Control (AGC) scheme may consider economic dispatch of Generating Companies (GENCOs) as well in addition to minimising the system frequency deviation. This will result in increased participation from GENCOs in AGC. GENCOs being private utilities, will have the private information like cost function etc which will not be shared with the system operator. This imposes a challenge on implementing classical economic dispatch problem in AGC scheme. The contribution of the paper is a game theoretic based model of AGC scheme that will ensure an optimal dispatch to competing GENCOs. This is achieved by defining an optimal participation factor for GENCOs that will minimize their cost of production. A game theoretic approach towards AGC as a dynamic game is presented and GENCOs are considered as Reinforcement learning agents. A Single Agent Q-Learning method along with pursuit algorithm is used at each GENCO agent to achieve the equilibrium point in AGC. The algorithm was studied with three competing GENCOs. Results show the suitability of the proposed AGC model and its ability to minimize the deviation in system frequency while ensuring economic operation of the system.","","978-1-5386-6159-8","10.1109/NPSC.2018.8771789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8771789","Automatic Generation Control;Economic Dispatch;Deregulation;Stochasticcollaborationgames;Nash Equilibrium;Reinforcement Q-Learning;PursuitAlgorithm","","electricity supply industry;game theory;learning (artificial intelligence);multi-agent systems;optimisation;power engineering computing;power generation control;power generation dispatch;power generation economics","generating companies;optimal economic dispatch problem;reinforcement learning agents;automatic generation control scheme;private utilities;pursuit algorithm;single agent Q-learning method;production cost minimization;GENCO agent;game theoretic approach;optimal participation factor;AGC scheme;system frequency deviation","","","","10","IEEE","25 Jul 2019","","","IEEE","IEEE Conferences"
"Path Planning Algorithms for USVs via Deep Reinforcement Learning","H. Zhai; W. Wang; W. Zhang; Q. Li","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","4281","4286","With the application of Unmanned Surface Vehicles (USVs) in complex and changeable environments, therefore it is particularly significant to enhance its autonomous navigation ability. Deep reinforcement learning (DRL) is a emerging method that combines the perception ability of deep learning (DL) with the decision-making ability of reinforcement learning (RL). Since it was put forward, DRL has achieved remarkable results in theory and application. In addition, DRL has the ability of perception and decision-making simultaneously, and shows strong learning ability. Therefore, DRL algorithms can be applied to solve the path planning problem of USV. In this paper, pointing at solving the path planning problem of USV, the horizontal 3-DOF motion model of USV is established, and the DQN algorithm is applied to settle USV path planning problem. Finally, the above algorithm is verified on the simulation platform.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728038","path planning;unmanned surface vehicle;obstacle avoidance;deep reinforcement learning","Deep learning;Automation;Decision making;Reinforcement learning;Path planning;Autonomous robots","decision making;deep learning (artificial intelligence);mobile robots;path planning;reinforcement learning;remotely operated vehicles","decision-making ability;strong learning ability;DRL algorithms;DQN algorithm;USV path planning problem;path planning algorithms;deep reinforcement learning;Unmanned Surface Vehicles;complex environments;changeable environments;autonomous navigation ability;perception ability;deep learning","","1","","11","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Security-Aware Reinforcement Learning under Linear Temporal Logic Specifications","B. Cui; K. Zhu; S. Li; X. Yin","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","12367","12373","In this paper, we investigate the problem of reinforcement learning under linear temporal logic (LTL) specifications for Markov decision processes (MDPs) with security constraints. We consider an outside passive intruder (observer) that can observe the external output behavior of the system through an output projection. We assume that the secret of the system is a subset of the initial states. The security constraint requires that the observer can never infer for sure that the agent was initiated from a secret state. Our objective is to learn a control policy that achieves the LTL task while ensuring security. To solve the problem of shaping the reward for reinforcement learning, we propose an approach based on the initial-state estimator and the limit deterministic Büchi automata. We illustrate the proposed approach by a case study of mobile robot example.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160753","National Natural Science Foundation of China(grant numbers:62061136004,62173226,61803259); National Key Research and Development Program of China(grant numbers:2018AAA0101700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160753","","Automation;Learning automata;Reinforcement learning;Observers;Markov processes;Behavioral sciences;Security","automata theory;learning (artificial intelligence);Markov processes;mobile robots;reinforcement learning;state estimation;temporal logic","external output behavior;initial states;initial-state estimator;linear temporal logic specifications;LTL task;Markov decision processes;observer;output projection;passive intruder;secret state;security constraint;security-aware reinforcement learning","","1","","26","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"HR-Planner: A Hierarchical Highway Tactical Planner based on Residual Reinforcement Learning","H. Wu; Y. Li; H. Zhuang; C. Wang; M. Yang","Department of Automation, Shanghai Jiao Tong University, Shanghai; Department of Automation, Shanghai Jiao Tong University, Shanghai; University of Michigan-Shanghai Jiao Tong University Joint Institute, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai; Department of Automation, Shanghai Jiao Tong University, Shanghai","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","7263","7269","Tactical planning is crucial for safe and efficient driving on the highway. However, the problem is complicated by the uncertain intention of surrounding vehicles, as well as observation noise caused by measurement noise and perception errors. Rule-based tactical planning methods are ineffective in handling dynamic scenarios with uncertainty, and susceptible to observation noise. To tackle this problem, we propose a hierarchical tactical planning framework based on residual reinforcement learning. Besides, a new reinforcement learning from demonstrations scheme that views rule-based methods as soft guidance is developed to combine prior knowledge with data-driven methods. Based on the framework and the training scheme, rule-based methods not only can be improved in highway scenarios with uncertainty and observation noise, but also will guide the training procedure for increased sampling efficiency. Additionally, to boost in-depth and consistent exploration in a vehicle system with inertia, we employ noisy networks to explore the optimal policy. The proposed method is validated in a stochastic and uncertain simulation environment, and the results reveal that our method outperforms both rule-based methods and pure data-driven methods in terms of safety and driving efficiency under noisy observations and uncertainty.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812400","","Road transportation;Training;Uncertainty;Automation;Measurement uncertainty;Reinforcement learning;Planning","learning (artificial intelligence);optimisation;planning (artificial intelligence);stochastic processes","rule-based methods;highway scenarios;observation noise;pure data-driven methods;hierarchical highway tactical planner;residual reinforcement learning;safe driving;efficient driving;measurement noise;perception errors;rule-based tactical planning methods;hierarchical tactical planning framework","","","","34","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning with Sparse Regularized Pruning and Compressing","W. Su; Z. Li; Z. Yang; J. Lu","Department of Automation, Guangdong University of Technology, Guangzhou, China; Department of Automation, Guangdong University of Technology, Guangzhou, China; Department of Automation, Guangdong University of Technology, Guangzhou, China; Department of Automation, Guangdong University of Technology, Guangzhou, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","8041","8046","Deep Reinforcement Learning (DRL) currently demonstrates distinguished performance in various fields of sequential decision tasks. Despite the great success, tremendous model size and large redundant parameters have become the major bottleneck on real-time deployments of DRL models. In this paper, we propose a novel model compression framework for DRL models with sparse regularized pruning method and policy shrinking technology, which can realize the superb balance be-tween the high sparsity and compression rate. In our framework, We first obtain the sparse student model by pruning method with the ℓ1-norm. Then, the shrinking technology is proposed to gain the dense student model for further compressing DRL models. Experiments on Cartpole and Lunarlander environment demonstrate that the proposed algorithm for DRL leads to an improvement state-of-the-art in terms of model size, iteration cycles, and parameters reduction.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727767","Deep Reinforcement Learning (DRL);sparse regularized pruning;policy shrinking","Automation;Reinforcement learning;Color;Market research;Real-time systems;Sparse matrices;Task analysis","deep learning (artificial intelligence);gradient methods;image coding;optimisation;reinforcement learning","deep reinforcement learning;sequential decision tasks;redundant parameters;real-time deployments;model compression framework;sparse regularized pruning method;policy shrinking technology;compression rate;sparse student model;dense student model;compressing DRL models;sparse regularized compressing;ℓ1-norm;Cartpole and Lunarlander environment;Cartpole and Lunarlander environment","","","","18","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Multitask reinforcement learning on the distribution of MDPs","F. Tanaka; M. Yamamura","Department of Computational Intelligence and Systems Science, Tokyo Institute of Technology, Japan; Department of Computational Intelligence and Systems Science, Tokyo Institute of Technology, Japan","Proceedings 2003 IEEE International Symposium on Computational Intelligence in Robotics and Automation. Computational Intelligence in Robotics and Automation for the New Millennium (Cat. No.03EX694)","18 Aug 2003","2003","3","","1108","1113 vol.3","In this paper we address a new problem in reinforcement learning. Here we consider an agent that faces multiple learning tasks within its lifetime. The agent's objective is to maximize its total reward in the lifetime as well as a conventional return in each task. To realize this, it has to be endowed an important ability to keep its past learning experiences and utilize them for improving future learning performance. This time we try to phrase this problem formally. The central idea is to introduce an environmental class, BV-MDPs that is defined with the distribution of MDPs. As an approach to exploiting past learning experiences, we focus on statistics (mean and deviation) about the agent's value tables. The mean can be used as initial values of the table when a new task is presented. The deviation can be viewed as measuring reliability of the mean, and we utilize it in calculating priority of simulated backups. We conduct experiments in computer simulation to evaluate the effectiveness.","","0-7803-7866-0","10.1109/CIRA.2003.1222152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222152","","Learning;Robots;Computational intelligence;Computer simulation;Cleaning;Computational modeling;Computer errors;Dogs;Cats;Uncertainty","learning (artificial intelligence);decision theory;Markov processes","multitask reinforcement learning;Markov decision processes;multiple learning tasks;past learning experiences;reliability measurement","","23","","11","IEEE","18 Aug 2003","","","IEEE","IEEE Conferences"
"A parameter control method inspired from neuromodulators in reinforcement learning","K. Murakoshi; J. Mizuno","Department of Knowledge-based Information Engineering, Toyohashi University of Technology, Japan; Department of Knowledge-based Information Engineering, Toyohashi University of Technology, Japan","Proceedings 2003 IEEE International Symposium on Computational Intelligence in Robotics and Automation. Computational Intelligence in Robotics and Automation for the New Millennium (Cat. No.03EX694)","18 Aug 2003","2003","1","","7","12 vol.1","The brain gains appropriate behaviors, which get rewards and escapes punishments by trial-and-error. Reinforcement learning models such a system by an engineering approach. Neuromodulators, which project widely in the brain and adjust functions in each brain part, are matched with parameters of reinforcement learning. We propose a reinforcement learning algorithm, which can follow sudden changes in environment by considering how neuromodulators affect behaviors. This algorithm improves actions by controlling the parameters of reinforcement learning after the obtained reward decreased as compared with the past. Computer simulation shows that the robots with the proposed algorithm are able to respond flexibly to sudden environmental changes.","","0-7803-7866-0","10.1109/CIRA.2003.1222054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222054","","Learning;Knowledge engineering;Appropriate technology;Systems engineering and theory;Computer simulation;Robot control;Neurons;Circuits;Humans;Animals","neural nets;learning (artificial intelligence);mobile robots","parameter control method;neuromodulators;reinforcement learning;sudden environmental changes;robots","","","","9","IEEE","18 Aug 2003","","","IEEE","IEEE Conferences"
"Reinforcement Learning of a Robot Cell Control Logic using a Software-in-the-Loop Simulation as Environment","F. Jaensch; A. Csiszar; J. Sarbandi; A. Verl","Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, Stuttgart, Germany; Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, Stuttgart, Germany; Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, Stuttgart, Germany; Institute of Control Engineering of Machine Tools and Manufacturing Units, University of Stuttgart, Stuttgart, Germany","2019 Second International Conference on Artificial Intelligence for Industries (AI4I)","9 Mar 2020","2019","","","79","84","This paper introduces a method for automatic robot programming of industrial robots using reinforcement learning on a Software-in-the-loop simulation. The focus of the the paper is on the higher levels of a hierarchical robot programming problem. While the lower levels the skills are stored as domain specific program code, the combination of the skills into a robot control program to solve a specific task is automated. The reinforcement learning learning approach allows the shopfloor workers and technicians just to define the end result of the manufacturing process through a reward function. The programming and process optimization is done within the learning procedure. The Software-in-the-loop simulation with the robot control software makes it possible to to interpret the real program code and generate the exact motion. The exact motion of the robot is needed in order to find not just an optimal but also a collision-free policy.","","978-1-7281-4087-2","10.1109/AI4I46381.2019.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9027783","Reinforcement Learning;Robot Control;Software in the Loop Simulation","Optimization;Learning (artificial intelligence);Service robots;Robot control;Software","control engineering computing;industrial robots;learning (artificial intelligence);mobile robots;robot programming","robot cell control logic;Software-in-the-loop simulation;automatic robot programming;industrial robots;hierarchical robot programming problem;domain specific program code;robot control program;reinforcement learning learning approach;process optimization;robot control software","","5","","15","IEEE","9 Mar 2020","","","IEEE","IEEE Conferences"
"Full Echo Q-routing with adaptive learning rates: A reinforcement learning approach to network routing","Y. Shilova; M. Kavalerov; I. Bezukladnikov","Department of Automation and Telemechanics, Perm National Research Polytechnic University, Perm, Russia; Department of Automation and Telemechanics, Perm National Research Polytechnic University, Perm, Russia; Department of Automation and Telemechanics, Perm National Research Polytechnic University, Perm, Russia","2016 IEEE NW Russia Young Researchers in Electrical and Electronic Engineering Conference (EIConRusNW)","7 Apr 2016","2016","","","341","344","Dynamically changing networks, such as mobile wireless sensor networks, Internet of Things networks, vehicular ad hoc networks etc., require efficient routing techniques. We present a routing algorithm, Adaptive Q-routing Full Echo, that is an extension of `full echo' modification of Q-routing algorithm and uses adaptive learning rates to improve exploration behaviour. The performance of the proposed algorithm is evaluated empirically in comparison to Q-routing and Dual Q-routing algorithms. The preliminary results suggest that the proposed algorithm represents a promising way of achieving good routing performance in dynamically changing networks.","","978-1-5090-0445-4","10.1109/EIConRusNW.2016.7448188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448188","network;nodes;Q-routing;learning rates","Routing;Heuristic algorithms;Learning (artificial intelligence);Oscillators;Algorithm design and analysis;Simulation;Prediction algorithms","Internet of Things;mobile radio;telecommunication network routing;vehicular ad hoc networks;wireless sensor networks","echo Q-routing;adaptive learning;network routing;mobile wireless sensor networks;Internet of Things;vehicular ad hoc networks;adaptive Q-routing full echo;dual Q-routing algorithms","","17","","14","IEEE","7 Apr 2016","","","IEEE","IEEE Conferences"
"Run-to-Run Control of Chemical Mechanical Polishing Process Based on Deep Reinforcement Learning","J. Yu; P. Guo","School of Mechanical Engineering, Tongji University, Shanghai, China; School of Mechanical Engineering, Tongji University, Shanghai, China","IEEE Transactions on Semiconductor Manufacturing","5 Aug 2020","2020","33","3","454","465","The chemical mechanical polishing (CMP) process usually suffers from drift and shift in the Run-to-Run material removal process due to the wear and replacement of the polishing pad, lacking of in-suit measurements of the product quality of interest and other environment variations. This paper proposed a deep reinforcement learning (DRL)-based run-to-run (R2R) controller for the CMP process. Firstly, deep reinforcement learning is effectively utilized as a training algorithm of the R2R controller, which is a model-free controller to take a decision with infinite horizon information and thus improves the control performance; Secondly, a novel policy network is embedded to the DRL model, which divides the network into linear and nonlinear part explicitly to improve the prediction performance of the R2R controller on process changes. Finally, a special reward function is proposed to improve the training of the R2R controller, which trades off between target tracing and fluctuations of production parameters. The effectiveness of the proposed controller is validated on a CMP process. The testing results illustrate that the DRL-based R2R controller can precisely trace the desired target of material removal rate (MRR) and is very effective to control various process variations online.","1558-2345","","10.1109/TSM.2020.3002896","National Natural Science Foundation of China(grant numbers:71777173); Science and Technology Innovation Action Plan of Shanghai Science and Technology Commission(grant numbers:19511106303); Fundamental Research Funds for the Central Universities of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9119169","Semiconductor manufacturing;chemical mecha-nical planarization;process control;Run-to-Run control;deep reinforcement learning","Process control;Reinforcement learning;Predictive models;Semiconductor device manufacture;Decision making;Planarization","chemical engineering computing;chemical mechanical polishing;control engineering computing;learning (artificial intelligence)","run-to-run controller;deep reinforcement learning;run-to-run material removal;DRL-based R2R;model-free controller;CMP process;polishing pad;chemical mechanical polishing","","12","","44","IEEE","16 Jun 2020","","","IEEE","IEEE Journals"
"Gait Self-learning for Damaged Robots Combining Bionic Inspiration and Deep Reinforcement Learning","M. Zeng; Y. Ma; Z. Wang; Q. Li","Institute of Robotics and Automation Systems, School of Electrical and Information Engineering, Tianjin University, Tianjin; Institute of Robotics and Automation Systems, School of Electrical and Information Engineering, Tianjin University, Tianjin; Institute of Robotics and Automation Systems, School of Electrical and Information Engineering, Tianjin University, Tianjin; Institute of Robotics and Automation Systems, School of Electrical and Information Engineering, Tianjin University, Tianjin","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","3978","3983","The gait self-learning for the Hexapod Robot in the damaged state is very important to improve its survivability in complex environments. Aiming at the damage of a Hexapod Robot with broken legs, this paper proposes a gait self-learning method for the damaged robot based on bionic inspiration and deep reinforcement learning. Due to various damage states of robots, it is difficult to accurately model the complex damage state. Using deep reinforcement learning can better solve this kind of model-free robot control problem. Besides, inspired by the moving gaits of the hexapod, the motion range of each leg joint of the Hexapod Robot is constrained, which further reduces the search range of the action space. The experimental results show that compared with the single deep reinforcement learning method under the same training episodes, the gaits generated by the proposed method are more adaptable and efficient.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9549968","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9549968","Gait self-learning;damaged robot;bionic inspiration;deep reinforcement learning","Legged locomotion;Training;Biological system modeling;Robot control;Reinforcement learning;Aerospace electronics;Robots","learning (artificial intelligence);legged locomotion;mobile robots;motion control;path planning;robot kinematics;robots","damaged robots combining bionic inspiration;Hexapod Robot;damaged state;gait self-learning method;damaged robot;complex damage state;model-free robot control problem;moving gaits;single deep reinforcement learning method","","","","24","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Reduced Operational Inhomogeneities in a Reconfigurable Parallelly-Connected Battery Pack Using DQN Reinforcement Learning Technique","A. Stevenson; M. Tariq; A. Sarwat","Department of Electrical and Computer Engineering, Florida International University, Miami, USA; Department of Electrical and Computer Engineering, Florida International University, Miami, USA; Department of Electrical and Computer Engineering, Florida International University, Miami, USA","2023 IEEE Transportation Electrification Conference & Expo (ITEC)","25 Jul 2023","2023","","","1","5","Battery cells that are placed in parallel in order to increase capacity are commonly considered single-series cells. In reality, there exist unavoidable variations between cells due to manufacturing processes as well as operational conditions that create current and State of Charge (SOC) inhomogeneities. If these inhomogeneities are not taken into consideration, accelerated degradation may occur causing early decommissioning of battery packs. Literature review reveals that reconfigurable battery packs are capable of dealing with these inhomogeneities, however, that a lack of demonstrated intelligent control methods exists. Thus in this work, a novel reconfigurable battery pack topology for reducing SOC and current inhomogeneities in a parallelly connected battery pack using a Reinforcement Learning (RL) Deep Q-Network (DQN) is presented. Results show that the RL-DQN based switch controller can reduce both current and SOC imbalances over time between parallel battery cells, especially in lower degradation variation battery packs and under lower operational current rates.","2473-7631","979-8-3503-9742-0","10.1109/ITEC55900.2023.10187040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10187040","Machine Learning;Reconfigurable Battery Pack;Battery Management Systems;Reinforcement Learning;Battery Inhomogeneities","Degradation;Manufacturing processes;Reinforcement learning;Switches;Nonhomogeneous media;Control systems;Batteries","battery management systems;deep learning (artificial intelligence);power engineering computing;reinforcement learning;secondary cells","accelerated degradation;current inhomogeneities;demonstrated intelligent control methods;DQN Reinforcement Learning technique;early decommissioning;lower degradation variation battery packs;lower operational current rates;manufacturing processes;novel reconfigurable battery pack topology;operational conditions;operational inhomogeneities;parallel battery cells;parallelly connected battery pack;reconfigurable battery packs;reconfigurable parallelly-connected battery pack;RL-DQN;single-series cells;SOC;state of charge inhomogeneities","","","","11","IEEE","25 Jul 2023","","","IEEE","IEEE Conferences"
"Learning to Update Engine Models with Deep Reinforcement Learning","R. Yu; B. Li; S. Shi; C. Peng; Y. Zhou; X. Du","School of Automation, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Automation, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Automation, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Automation, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Aviation Key Laboratory of Science and Technology on Aero Electromechanical System Integration, Nanjing Engineering Institute of Aircraft Systems, Nanjing, China; Aviation Key Laboratory of Science and Technology on Aero Electromechanical System Integration, Nanjing Engineering Institute of Aircraft Systems, Nanjing, China","2023 6th International Symposium on Autonomous Systems (ISAS)","3 Jul 2023","2023","","","1","6","Model updating can reduce the reality gap between the digital space and physical space, which is essential for accurately building digital models of aviation equipment. However, with data requirements and the complexity of application environments continue to increase, traditional update techniques show great limitations. The main reason is that traditional update technologies are limited by computational efficiency, resulting in the inability to balance accuracy and real-time. In this work, we propose a model update method based on reinforcement learning to infer model parameters. The proposed method does not require any ground truth parameters. Experimental results on the twin-rotor turbofan engine model verify the superiority of the proposed method compared with other state-of-the-art methods.","","979-8-3503-1615-5","10.1109/ISAS59543.2023.10164344","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10164344","model update;reinforcement learning;engine;digital model","Training;Degradation;Deep learning;Computational modeling;Buildings;Reinforcement learning;Real-time systems","deep learning (artificial intelligence);inference mechanisms;jet engines;learning (artificial intelligence);reinforcement learning","application environments;aviation equipment;computational efficiency;data requirements;deep reinforcement learning;digital models;digital space;great limitations;ground truth parameters;model parameters;model update method;model updating;physical space;reality gap;traditional update techniques;traditional update technologies;twin-rotor turbofan engine model;update engine models","","","","26","IEEE","3 Jul 2023","","","IEEE","IEEE Conferences"
"Learning to Manipulate Tools Using Deep Reinforcement Learning and Anchor Information","J. Wei; S. Cui; P. Hao; S. Wang","School of Future Technology, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2022 IEEE International Conference on Robotics and Biomimetics (ROBIO)","18 Jan 2023","2022","","","1420","1425","Endowing robots with tool manipulation skills helps them accomplish challenging tasks. While robots manipulate tools to achieve goals, the alignment of tools and targets is a noise-sensitive and contact-rich task. However, it is difficult to access the accurate pose of the tool and the target. When there is unknown noise in the observations, reinforcement learning can't be sure to perform well. In this paper, we define the easier-to-obtain accurate task-related information as anchor information and introduce a tool manipulation method based on reinforcement learning and anchor information, which can perform well when the observations include unknown noise. To evaluate the method, we build a simulated environment ToolGym, which includes four different kinds of tools and different noise sampling functions for each tool. Finally, we compare our method with baseline methods to show the effectiveness of the proposed method.","","978-1-6654-8109-0","10.1109/ROBIO55434.2022.10012027","National Natural Science Foundation of China(grant numbers:U1913201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10012027","","Deep learning;Biomimetics;Reinforcement learning;Grasping;Fasteners;End effectors;Task analysis","deep learning (artificial intelligence);manipulators;reinforcement learning","accurate task-related information;anchor information;contact-rich task;deep reinforcement learning;manipulate tools;noise sampling functions;noise-sensitive;tool manipulation method;tool manipulation skills;unknown noise","","","","20","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"Aircraft Engine Maintenance Based on Reinforcement Learning","X. Xu; X. Wang","School of Safety Engineering, Shenyang Aerospace University, Shenyang, China; School of Safety Engineering, Shenyang Aerospace University, Shenyang, China","2020 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)","8 Jun 2020","2020","","","958","960","In recent years, with the rapid development of China's civil aviation industry, the maintenance of aircraft engines has grown rapidly in terms of both the number of repairs and the repair capacity. The structure of aero engine is complex, and the technical requirements for maintenance are high. During the operation of the aircraft, the engine equipment will decay due to fatigue, wear, aging and other reasons. Running equipment in a decay state often causes increased costs, increased failure rates, and huge safety risks. Proper maintenance can prevent equipment from operating in poor conditions, but excessive maintenance can increase equipment downtime and maintenance costs. According to the development trend of the state and possible failure modes, adopting corresponding maintenance strategies, preparing maintenance in advance, and adjusting the flight plan reasonably, this is of great significance for reducing equipment maintenance costs, improving operating efficiency and system safety, and improving the revenue of aviation companies.","","978-1-7281-6698-8","10.1109/ICITBS49701.2020.00213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9110025","Aircraft engine;Decay;Maintenance strategy","Technical requirements;Costs;Smart cities;Reinforcement learning;Maintenance engineering;Sensor systems;Safety","aerospace engines;aerospace industry;aerospace safety;learning (artificial intelligence);maintenance engineering","aircraft engine maintenance;reinforcement learning;China's civil aviation industry;engine equipment;maintenance strategies;equipment maintenance costs;aeroengine structure;aviation companies","","1","","9","IEEE","8 Jun 2020","","","IEEE","IEEE Conferences"
"A Physiological Control System for Pulsatile Ventricular Assist Device Using an Energy-Efficient Deep Reinforcement Learning Method","T. Li; W. Cui; X. Liu; X. Li; N. Xie; Y. Wang","State Key Laboratory of High-performance Precision Manufacturing and the School of Mechanical Engineering, Dalian University of Technology, Dalian, China; State Key Laboratory of High-performance Precision Manufacturing and the School of Mechanical Engineering, Dalian University of Technology, Dalian, China; State Key Laboratory of High-performance Precision Manufacturing and the School of Mechanical Engineering, Dalian University of Technology, Dalian, China; State Key Laboratory of High-performance Precision Manufacturing and the School of Mechanical Engineering, Dalian University of Technology, Dalian, China; State Key Laboratory of High-performance Precision Manufacturing and the School of Mechanical Engineering, Dalian University of Technology, Dalian, China; State Key Laboratory of High-performance Precision Manufacturing and the School of Mechanical Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Instrumentation and Measurement","8 Jun 2023","2023","72","","1","10","Pulsatile ventricular assist device (PVAD) is a blood pump used to assist the circulation support of the native heart. Because of patients’ complex physiological environment, PVAD’s control system requires high adaptive ability and low computational energy. However, traditional PID controllers do not possess sufficient adaptive ability. Although neural network controllers are with high adaptive ability, their extensive energy cost limits the applications. In this study, a PVAD physiological control system based on deep reinforcement learning (DRL) is proposed, which significantly improves the system’s adaptive ability. To further reduce its energy cost, a new energy-efficient DRL method, AddTD3, is developed, in which the fully connected network (FC) with high computation complexity is replaced by energy-efficient AdderNet. Experimental results show that the proposed AddTD3 controller is with higher adaptive ability than the traditional PID controller (cumulative absolute error: 237.1 versus 484.3 mmHg) and can be migrated to the mock circulation system (MCS) without fine-tuning. It can reduce the energy cost of the traditional DRL algorithm TD3 to 44.8% without reducing the performance (8773.2 versus 22420.4 pJ).","1557-9662","","10.1109/TIM.2023.3277993","National Key Research and Development Project(grant numbers:2022ZD0116700); National Natural Science Foundation of China(grant numbers:52105008); Fundamental Research Funds for the Central Universities(grant numbers:DUT22GF301,DUT21RC(3)037); Foundation of State Key Laboratory of Digital Manufacturing Equipment and Technology(grant numbers:DMETKF2022009); Young Elite Scientists Sponsorship Program by CAST(grant numbers:YESS20220068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10130059","Adder neural network;deep reinforcement learning (DRL);heart failure;physiological control;pulsatile ventricular assist device (PVAD)","Physiology;Control systems;Costs;Training;Adaptive systems;Blood;Energy efficiency","cardiology;computational complexity;haemodynamics;medical computing;patient diagnosis;reinforcement learning;three-term control","AddTD3 controller;blood pump;circulation support;deep reinforcement learning;DRL algorithm TD;energy-efficient AdderNet;energy-efficient deep reinforcement;energy-efficient DRL method;mock circulation system;neural network controllers;PID controller;pulsatile ventricular assist device;PVAD control system;PVAD physiological control system","","","","37","IEEE","19 May 2023","","","IEEE","IEEE Journals"
"Application of Reinforcement Learning to Wind Farm Active Power Control Design","X. Zhang; H. Badihi; Z. Yu; M. Benbouzid; N. Lu; Y. Zhang","College of Automation Engineering Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering Nanjing University of Aeronautics and Astronautics, Nanjing, China; Institut de Recherche Dupuy de Lome (IRDL) University of Brest, Brest, France; College of Automation Engineering Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Mechanical, Industrial and Aerospace Engineering Concordia University, Montreal, Canada","2022 IEEE Electrical Power and Energy Conference (EPEC)","5 Jan 2023","2022","","","229","234","Wind power is becoming a key player of the world's energy landscape thanks to its cleanliness, abundance and huge potential. At the same time, it attracts an increasing attention when it comes to its efficient production and financial viability, which otherwise may restrict its development in the foreseeable future. Indeed, enhancing energy production efficiency to reduce costs and improve the ability of the system to resist faults are research areas of great interest. With the advancement of machine learning and artificial intelligence along with increasing computational power at hand, reinforcement learning enables achieving an optimal control solution in an application environment after continuous attempts and updates. In this paper, a novel solution based on reinforcement learning is applied to the control of wind farm. An intelligent agent is designed to explore the environment, and after training, it effectively maintains the necessary balance between power generation and load, which in turn regulates the wind farm grid frequency when enough wind is available. The trained agent is tested under different loads, realistic wind fields, as well as fault scenarios. All simulation results show that the agent accurately understands the environment and load requirements, mitigates the impact of faults, and thus, improves the stability of the grid frequency.","2381-2842","978-1-6654-6318-8","10.1109/EPEC56903.2022.10000071","National Natural Science Foundation of China(grant numbers:62003166); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10000071","Wind farm;reinforcement learning;active power;frequency control","Training;Wind energy;Simulation;Reinforcement learning;Production;Resists;Wind farms","learning (artificial intelligence);optimal control;power control;wind power plants","application environment;artificial intelligence;computational power;continuous attempts;energy production efficiency;financial viability;foreseeable future;intelligent agent;key player;machine learning;optimal control solution;power generation;realistic wind fields;reinforcement learning;wind farm active power control design;wind farm grid frequency;wind power;world","","","","18","IEEE","5 Jan 2023","","","IEEE","IEEE Conferences"
"Multi-Layer Attention-based State Representation for the Reinforcement Learning of Visual Servoing","H. Kitajima; S. Bounyong; M. Yoshioka","Panasonic Industry Co., Ltd., Osaka, Japan; Panasonic Industry Co., Ltd., Osaka, Japan; Panasonic Industry Co., Ltd., Osaka, Japan","2023 IEEE International Conference on Consumer Electronics (ICCE)","17 Feb 2023","2023","","","1","2","In this paper, we propose multi-layer attention-based state representation for robotic visual servoing using reinforcement learning. It is challenging for a robot to manipulate an object that it has never been trained on, which can occur in real use cases. Multiple attention layers allow the model to extract the spatial features of the object in the scene from high to low resolution in latent space. Using the extracted features allows the robotic controller model to learn more robust trajectory planning. We evaluated and compared the robustness of our method with that of other methods on a fetch-reach task in a simulation by randomly scaling the size of the target object. The results demonstrated that our method yielded the highest success rate, with the most efficient trajectory planning, which indicates that our state representation model provided more useful information that enabled visual servoing to perform more robustly for unseen objects.","2158-4001","978-1-6654-9130-3","10.1109/ICCE56470.2023.10043381","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10043381","representation learning;autoencoder;visual servoing;reinforcement learning","Trajectory planning;Reinforcement learning;Aerospace electronics;Feature extraction;Visual servoing;Robustness;Task analysis","feature extraction;learning (artificial intelligence);robot vision;visual servoing","multilayer attention-based state representation;multiple attention layers;reinforcement learning;robotic controller model;robotic visual servoing;state representation model","","","","3","IEEE","17 Feb 2023","","","IEEE","IEEE Conferences"
"Multi-agent cooperation based on behavior prediction and reinforcement learning","Dongyong Yang; Xuejiang Chen; Jingping Jiang","College of Information Engineering, Zhejiang University of Technology, Hangzhou, China; College of Information Engineering, Zhejiang University of Technology, Hangzhou, China; College of Electrical Engineering, University of Zhejiang, Hangzhou, China","Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)","18 Oct 2004","2004","6","","4869","4872 Vol.6","In multi-agent systems based on reinforcement learning, the evaluation to the behavior of an agent depends on the other agents' behaviors closely. The cooperation performance of multi-agent systems can be improved when each agent takes its action after it predicts the other agents' actions self-consciously. In this paper, several methods for predicting other agents' behaviors were presented, which demand all agents to evaluate the probabilities of actions that other agents may take, and joint-action was introduced to the traditional reinforcement learning. An experiment that three agents cooperate to raise an object was conducted to test the performance of multi-agent systems, and its results show that the cooperation process can be speeded by behavior prediction and joint-action is applied to the traditional reinforcement learning successfully.","","0-7803-8273-0","10.1109/WCICA.2004.1343636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1343636","","Learning;Multiagent systems;Educational institutions;System testing","multi-agent systems;learning (artificial intelligence)","multi-agent systems cooperation;behavior prediction;reinforcement learning;joint-action","","2","2","","IEEE","18 Oct 2004","","","IEEE","IEEE Conferences"
"Research of a heuristic reward function for reinforcement learning algorithms","Yingzi Wei; Mingyang Zhao; Feng Zhang; Yulan Hu","Graduate School of Chinese Academy of Sciences, Beijing, China; Shenyang Institute ofAutomation, Chinese Academy and Sciences, Shenyang, China; Graduate School of Chinese Academy of Sciences, Beijing, China; Graduate School of Chinese Academy of Sciences, Beijing, China","Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)","18 Oct 2004","2004","3","","2676","2680 Vol.3","The reward function is considered as the critical component for its effect of evaluating the action and guiding the reinforcement learning (RL) process. According to the distribution of rewards in the space of states, reward functions can have two basic forms, dense and sparse. We present an idea of designing a heuristic reward function in this paper. An additional reward is added to the traditional sparse reward function. The additional reward function F is a difference of potentials, which can provide more heuristic information for the learning system to progress rapidly. We also prove the convergence property of Q-value iteration. The heuristic reward function helps to implement an efficient reinforcement learning system on a real-time control or scheduling system.","","0-7803-8273-0","10.1109/WCICA.2004.1342083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342083","","Space technology;Convergence;Learning systems;Real time systems;Control systems","learning systems;learning (artificial intelligence);Markov processes;decision theory;optimisation;convergence of numerical methods;iterative methods","heuristic reward function;reinforcement learning algorithms;dense functions;sparse reward function;additional reward function;convergence property;Q-value iteration;real time control;real time scheduling system;Markov processes","","1","","","IEEE","18 Oct 2004","","","IEEE","IEEE Conferences"
"Rule driven multi objective dynamic scheduling by data envelopment analysis and reinforcement learning","Xili Chen; XinChang Hao; Hao Wen Lin; Tomohiro Murata","Graduate School of Information, Production and System, Waseda University of Japan, Fukuoka, Japan; Graduate School of Information, Production and System, Waseda University of Japan, Fukuoka, Japan; Graduate School of Information, Production and System, Waseda University of Japan, Fukuoka, Japan; Graduate School of Information, Production and System, Waseda University of Japan, Fukuoka, Japan","2010 IEEE International Conference on Automation and Logistics","23 Sep 2010","2010","","","396","401","This paper presents a rule driven method of developing composite dispatching rule for multi objective dynamic scheduling. Data envelopment analysis is adopted to select elementary dispatching rules, where each rule is justified as efficient for optimizing specific operational objectives of interest. The selected rules are subsequently combined into a single composite rule using the weighted aggregation manner. An intelligent agent is trained using reinforcement learning to acquire the scheduling knowledge of assigning the appropriate weighting values for building the composite rule to cope with the WIP fluctuation of a machine. Implementation of the proposed method in a two objective dynamic job shop scheduling problem is demonstrated and the results are satisfactory.","2161-816X","978-1-4244-8376-1","10.1109/ICAL.2010.5585316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5585316","multi objective scheduling;dynamic job shop;composite dispatching rule;data envelopment analysis;reinforcement learning","Dispatching;Dynamic scheduling;Job shop scheduling;Learning;Intelligent agent;Fluctuations;Data envelopment analysis","data envelopment analysis;job shop scheduling;learning (artificial intelligence)","multiobjective dynamic scheduling;data envelopment analysis;reinforcement learning;rule driven method;WIP fluctuation;job shop scheduling;composite dispatching rule","","7","","14","IEEE","23 Sep 2010","","","IEEE","IEEE Conferences"
"Adaptive V2X platform for guaranteed QoS/QoE service based on cloud computing and deep reinforcement learning","B. Jo; S. Jeong","Dept. IT application research center, Korea Electronics Technology Institute, Republic of Korea; Dept. IT application research center, Korea Electronics Technology Institute, Republic of Korea","2021 Twelfth International Conference on Ubiquitous and Future Networks (ICUFN)","13 Sep 2021","2021","","","23","25","An industry where technology pushes behind and the market pulls ahead succeeds. In this repect, autonomous driving is a global industry that is rapidly growing in line with the advancement of wireless communication, vision, AI technologies and the smart infrastructure market, including automobiles. In particular, V2X technology is an international communication standard for providing autonomous driving services at the level of mobility 4.0. In this paper, we propose an adaptive V2X service methodology. Autonomous driving is a technology belonging to the MCS field which is directly connected to human life, so we provide enhanced QoS/QoE through proposed method which is guaranteed the real-time based on V2X standard.","2165-8536","978-1-7281-6476-2","10.1109/ICUFN49451.2021.9528541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9528541","Mobility 4.0;autonomous driving;adaptive V2X;cloud computing;real-time streaming;MCS","Industries;Wireless communication;Cloud computing;Quality of service;Switches;Reinforcement learning;Real-time systems","cloud computing;learning (artificial intelligence);quality of service","Adaptive V2X platform;cloud computing;deep reinforcement learning;repect;global industry;wireless communication;AI technologies;smart infrastructure market;international communication standard;autonomous driving services;adaptive V2X service methodology","","","","7","IEEE","13 Sep 2021","","","IEEE","IEEE Conferences"
"A novel learning classifier system based on reinforcement learning","Yang Gao; Ning Li; Xin Lu; Shifu Chen","Nanjing University, China; Nanjing University, China; Nanjing University, China; Nanjing University, China","The 2002 International Conference on Control and Automation, 2002. ICCA. Final Program and Book of Abstracts.","8 Sep 2003","2002","","","132","132","","","0-7803-7412-6","10.1109/ICCA.2002.1229428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1229428","","Learning;Control systems;Neural networks;Remote monitoring;Three-term control;Java;Neurons;Internet;Field buses;Web server","","","","","","","IEEE","8 Sep 2003","","","IEEE","IEEE Conferences"
"Reinforcement learning based on on-line state features learning and bias exploration","Xueyong Li; Yaping Lin",Hunan University; Hunan University,"The 2002 International Conference on Control and Automation, 2002. ICCA. Final Program and Book of Abstracts.","8 Sep 2003","2002","","","245","246","","","0-7803-7412-6","10.1109/ICCA.2002.1229820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1229820","","Learning;Control systems;Optimal control;Iterative methods;Frequency domain analysis;Orbital robotics;Space shuttles;Nonlinear control systems;Testing;Performance analysis","","","","","","","IEEE","8 Sep 2003","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Condition-oriented Maintenance Scheduling for Flow Line Systems","R. Lamprecht; F. Wurst; M. F. Huber","Fraunhofer IPA, Center for Cyber Cognitive Intelligence (CCI), Stuttgart, Germany; Fraunhofer IPA, Center for Cyber Cognitive Intelligence (CCI), Stuttgart, Germany; Institute of Industrial Manufacturing and Management IFF, University of Stuttgart, Center for Cyber Cognitive Intelligence (CCI), Fraunhofer IPA, Stuttgart, Germany","2021 IEEE 19th International Conference on Industrial Informatics (INDIN)","11 Oct 2021","2021","","","1","7","Maintenance scheduling is a complex decision-making problem in the production domain, where a number of maintenance tasks and resources has to be assigned and scheduled to production entities in order to prevent unplanned production downtime. Intelligent maintenance strategies are required that are able to adapt to the dynamics and different conditions of production systems. The paper introduces a deep reinforcement learning approach for condition-oriented maintenance scheduling in flow line systems. Different policies are learned, analyzed and evaluated against a benchmark scheduling heuristic based on reward modelling. The evaluation of the learned policies shows that reinforcement learning based maintenance strategies meet the requirements of the presented use case and are suitable for maintenance scheduling in the shop floor.","","978-1-7281-4395-8","10.1109/INDIN45523.2021.9557373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9557373","Reinforcement Learning;DDQN;Maintenance Scheduling;Policy Evaluation","Production systems;Analytical models;Job shop scheduling;Conferences;Decision making;Reinforcement learning;Maintenance engineering","decision making;deep learning (artificial intelligence);maintenance engineering;production engineering computing;scheduling","deep reinforcement learning approach;flow line systems;based condition-oriented maintenance scheduling;complex decision-making problem;production domain systems;intelligent maintenance tasks;benchmark scheduling heuristic","","","","22","IEEE","11 Oct 2021","","","IEEE","IEEE Conferences"
"Research and Application of 45G Antenna Weight Optimization Based on Heuristic and Reinforcement Learning","C. Zhang; F. Gao; W. Zhu; Y. Wu","China Mobile Group Design Institute Co.,Ltd, Beijing, China; China Mobile Group Design Institute Co.,Ltd, Beijing, China; China Mobile Group Design Institute Co.,Ltd, Beijing, China; China Mobile Group Design Institute Co.,Ltd, Beijing, China","2022 IEEE International Symposium on Electromagnetic Compatibility & Signal/Power Integrity (EMCSI)","26 Sep 2022","2022","","","101","105","Currently, the main optimization strategies for 45G network antenna weight are to use manual or simple automated method to adjust weight parameters, however, the above two traditional ways have problems such as high maintenance cost, low optimization efficiency and large errors. Therefore, for the purpose of network production efficiency improvement, optimized maintenance cost reduction and achieving the goals of accurate, rapid, efficient and intelligent optimization of antenna weight, the paper focused on 45G massive multiple-input multiple-output antenna weight self-optimization methods based on artificial intelligence technology and massive multiple-input multiple-output array antenna technology. Meanwhile, network data were analyzed in depth and put forward the theoretical research idea by applying reinforcement learning and heuristic learning as the core strategy to drive the weight self-optimization, which is helpful to provide references for the promotion of self-optimization adjustment technology and the evolution of digital intelligence in the antenna field.","","978-1-6654-0929-2","10.1109/EMCSI39492.2022.9889424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9889424","communication technology;array antenna;weight optimization;intelligent algorithm;artificial intelligence","Costs;Heuristic algorithms;Reinforcement learning;Production;Manuals;Maintenance engineering;MIMO","4G mobile communication;5G mobile communication;artificial intelligence;cost reduction;MIMO communication;optimisation;reinforcement learning;telecommunication computing","4G network;5G antenna weight optimization application;5G network;accurate optimization;antenna field;artificial intelligence technology;efficient optimization;heuristic learning;intelligent optimization;low optimization efficiency;maintenance cost;manual weight parameter adjustment method;massive multiple-input multiple-output antenna weight self-optimization;massive multiple-input multiple-output array antenna technology;network data;network production efficiency improvement;optimized maintenance cost reduction;rapid optimization;reinforcement learning;self-optimization adjustment technology;simple automated weight parameter adjustment method;weight parameters","","1","","3","IEEE","26 Sep 2022","","","IEEE","IEEE Conferences"
"Optimal Task Offloading Decision in IIoT Enviornments Using Reinforcement Learning","S. Koo; Y. Lim","Dept. IT Engineering, Sookmyung Women’s Univ, Seoul, South Korea; Dept. IT Engineering, Sookmyung Women’s Univ, Seoul, South Korea","2021 IEEE 3rd Eurasia Conference on IOT, Communication and Engineering (ECICE)","23 Dec 2021","2021","","","86","89","In the Industrial Internet of Things (IIoT), various types of tasks are processed for the small quantity batch production. But there are many challenges due to the limited battery lifespan and computational capabilities of devices. To overcome the limitations, Mobile Edge Computing (MEC) has been introduced. In MEC, a task offloading technique to execute the tasks attracts much attention. A MEC server (MECS) has limited computational capability, which increases the burden on the server and a cellular network if a larger number of tasks are offloaded to the server. It can reduce the quality of service for task execution. Thus, offloading between nearby devices through device-to-device (D2D) communication is drawing attention. We propose the optimal task offloading decision strategy in the MEC and D2D communication architecture. We aim to minimize the energy consumption of devices and task execution delay under delay constraints. To solve the problem, we adopt Q-learning algorithm as one of Reinforcement Learning (RL). Simulation results show that the proposed algorithm outperforms the other methods in terms of energy consumption of devices and task execution delay.","","978-1-6654-4516-0","10.1109/ECICE52819.2021.9645710","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645710","Computation Offloading;Q-Learning;Mobile Edge Computing;Device-to-Device (D2D) Communication;Industrial Internet of Things","Energy consumption;Q-learning;Simulation;Quality of service;Production;Delays;Device-to-device communication","cellular radio;learning (artificial intelligence);mobile computing","IIoT enviornments;Reinforcement Learning;quantity batch production;battery lifespan;Mobile Edge Computing;task offloading technique;MEC server;MECS;computational capability;nearby devices;device-to-device communication;optimal task offloading decision strategy;D2D communication architecture;task execution delay;Q-learning algorithm","","1","","11","IEEE","23 Dec 2021","","","IEEE","IEEE Conferences"
"Simulation of Cross-border E-commerce Supply Chain Coordination Decision Model Based on Reinforcement Learning Algorithm","J. Liu","School of Economics, Zhejiang University of Technology, Hangzhou, China","2023 International Conference on Networking, Informatics and Computing (ICNETIC)","6 Sep 2023","2023","","","429","433","The management of supply chain is based on information sharing, and the supply chain lacking information exchange and sharing is bound to be in a state of management confusion and inefficiency. However, the profit of supply chain node enterprises is not only affected by the retail price, but also by the contract concluded between manufacturers and retailers. For domestic e-commerce, cross-border logistics needs to deliver goods from our warehouses to consumers in other countries, and the logistics cost is high. In this paper, a coordination decision model of cross-border e-commerce supply chain based on RL (Reinforcement Learning) algorithm is proposed. By using the pairing transaction model based on RL, the parameters of the training set data of the final pairing combination are iterated and optimized, and the optimal parameters are obtained, and then the test set data is used for back testing. The simulation results of Q-RL algorithm show that although there are fluctuations in the negotiation process, the overall negotiation result can reach the global optimum, the negotiation time can be greatly shortened, and the conflict resolution efficiency of supply chain production and marketing collaborative planning can be improved. When the benefits shared by customers to other entities are within the scope that integrators and subcontractors are willing to accept, the profits of cross-border e-commerce enterprises will increase with the increase of their own shared benefits, which will reduce the profits of cross-border e-commerce enterprises.","","979-8-3503-1331-4","10.1109/ICNETIC59568.2023.00095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10236673","Reinforcement learning;Cross-border e-commerce;Supply chain;Coordinated decision","Training;Fluctuations;Simulation;Supply chains;Collaboration;Production;Reinforcement learning","contracts;electronic commerce;logistics;pricing;profitability;supply chain management;supply chains","cross-border e-commerce enterprises;cross-border e-commerce supply chain coordination decision model;cross-border logistics;domestic e-commerce;information exchange;information sharing;management confusion;marketing collaborative planning;pairing transaction model;profit;Q-RL algorithm show;Reinforcement Learning algorithm;retail price;retailers;shared benefits;supply chain node enterprises;supply chain production","","","","12","IEEE","6 Sep 2023","","","IEEE","IEEE Conferences"
"Prolonging Robot Lifespan Using Fatigue Balancing with Reinforcement Learning","F. Costa; S. Mozaffari; R. Alirezaee; M. Ahmadi; S. Alirezaee","Electrical and Computer Engineering Department, University of Windsor, Windsor, Canada; Electrical and Computer Engineering Department, University of Windsor, Windsor, Canada; Electrical and Computer Engineering Department, University of Windsor, Windsor, Canada; Electrical and Computer Engineering Department, University of Windsor, Windsor, Canada; Electrical and Computer Engineering Department, University of Windsor, Windsor, Canada","2022 7th International Conference on Mechanical Engineering and Robotics Research (ICMERR)","12 Apr 2023","2022","","","45","50","Over the last quarter century, industrial robots have become an indispensable element in modern manufacturing and assembly. While the costs of implementing these robot systems can be high, over time these expenditures are offset by their longevity and productivity. One of the major costs of existing robotic systems is the maintenance and replacement of individual robots. Currently, process and manufacturing robots are programmed by robotic technicians with predetermined motions based on process requirements and product specifications. These repetitive motions tend to wear out specific joints faster than others, causing total robot failure and precipitating the replacement of the entire robot. This paper leverages machine learning algorithms to modify nonproduction critical robot movements to balance the physical load at each joint, with the goal of extending the useful life of these industrial robots in a manufacturing environment. Experimental results show that Q-Learning balanced the average load on robot joints by roughly 14%.","","978-1-6654-9051-1","10.1109/ICMERR56497.2022.10097795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10097795","Industrial Robots lifespan;Robot Fatigue;robot motor wear;load balancing on robot joints;Reinforcement learning in robotics","Productivity;Costs;Q-learning;Machine learning algorithms;Service robots;Maintenance engineering;Fatigue","control engineering computing;industrial robots;production engineering computing;reinforcement learning","assembly;fatigue balancing;industrial robots;machine learning;manufacturing environment;manufacturing robots;modern manufacturing;nonproduction critical robot movements;process requirements;product specifications;productivity;Q-learning;reinforcement learning;robot joints;robot lifespan;robotic systems;total robot failure","","","","18","IEEE","12 Apr 2023","","","IEEE","IEEE Conferences"
"Genetic reinforcement learning approach to the machine scheduling problem","G. H. Kim; C. S. G. Lee","School of Electrical Engineering, Purdue University, West Lafayette, IN, USA; School of Electrical Engineering, Purdue University, West Lafayette, IN, USA","Proceedings of 1995 IEEE International Conference on Robotics and Automation","6 Aug 2002","1995","1","","196","201 vol.1","This paper focuses on the development of a learning-based heuristic for the machine scheduling problem, which automatically captures the search control knowledge or the common features of good schedules while generating a number of schedules. Defining states and actions of the machine shop, the machine scheduling problem is transformed into a problem of reinforcement learning (RL) in which a learner or a scheduler will learn to select the right action at each state of the machine shop, using the reward from a schedule evaluator for executing the action. Implementing the proposed reinforcement learning with a genetic algorithm results in the genetic reinforcement learning (GRL) approach to the machine scheduling problem. Although the learning-based heuristic has the overhead of acquiring knowledge on the problem, it can be easily adapted for a wide variety of machine scheduling problems due to the weak dependence on the problem structures and objectives. A GRL-based scheduler, called EVIS (Evolutionary Intracell Scheduler), has been developed and applied to various classes of machine scheduling problems, such as the job-shop scheduling, the flow-shop scheduling and the open-shop scheduling problems, and even the processor scheduling problem, the performance evaluation of EVIS with a number of different problem instances has shown that the learning-based heuristic is robust and its performance is comparable with that of other problem-specific heuristics or search-oriented heuristics in the quality of solutions.","1050-4729","0-7803-1965-6","10.1109/ROBOT.1995.525285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=525285","","Job shop scheduling;Machine shops;Genetic algorithms;Machine learning;Robustness;Costs;Intelligent manufacturing systems;Robotics and automation;Design methodology;Problem-solving","production control;genetic algorithms;learning (artificial intelligence);heuristic programming;search problems","genetic reinforcement learning approach;machine scheduling problem;learning-based heuristic;search control knowledge;schedule evaluator;genetic algorithm;EVIS;Evolutionary Intracell Scheduler;job-shop scheduling;flow-shop scheduling;open-shop scheduling;processor scheduling","","12","","21","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Reinforcement Learning Based Manipulation Skill Transferring for Robot-assisted Minimally Invasive Surgery","H. Su; Y. Hu; Z. Li; A. Knoll; G. Ferrigno; E. De Momi","Dipartimento di Elettronica, Politecnico di Milano, Milano, Italy; Department of Informatics, Technical University of Munich, Munich, Germany; Department of Automation, University of Science and Technology of China, China; Department of Informatics, Technical University of Munich, Munich, Germany; Dipartimento di Elettronica, Politecnico di Milano, Milano, Italy; Dipartimento di Elettronica, Politecnico di Milano, Milano, Italy","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","2203","2208","The complexity of surgical operation can be released significantly if surgical robots can learn the manipulation skills by imitation from complex tasks demonstrations such as puncture, suturing, and knotting, etc.. This paper proposes a reinforcement learning algorithm based manipulation skill transferring technique for robot-assisted Minimally Invasive Surgery by Teaching by Demonstration. It employed Gaussian mixture model and Gaussian mixture Regression based dynamic movement primitive to model the high-dimensional human-like manipulation skill after multiple demonstrations. Furthermore, this approach fascinates the learning and trial phase performed offline, which reduces the risks and cost for the practical surgical operation. Finally, it is demonstrated by transferring manipulation skills for reaching and puncture using a KUKA LWR4+ robot in a lab setup environment. The results show the effectiveness of the proposed approach for modelling and learning of human manipulation skill.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196588","","Learning (artificial intelligence);Robots;Surgery;Trajectory;Task analysis;Shape;Education","end effectors;Gaussian processes;human-robot interaction;learning (artificial intelligence);manipulator dynamics;medical robotics;motion control;regression analysis;surgery","complex tasks demonstrations;reinforcement learning algorithm based manipulation skill transferring technique;robot-assisted minimally invasive surgery;Gaussian mixture model;Gaussian mixture regression;multiple demonstrations;trial phase performed offline;practical surgical operation;KUKA LWR4+ robot;human manipulation skill;surgical robots","","8","","22","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning based Mobility-Aware Service Migration for Multi-access Edge Computing Environment","Y. Zhang; R. Li; Y. Zhao; R. Li","Inspur (Beijing) Electronic Information Industry Co., Ltd, Beijing, China; Inspur (Beijing) Electronic Information Industry Co., Ltd, Beijing, China; Inspur (Beijing) Electronic Information Industry Co., Ltd, Beijing, China; Inspur (Beijing) Electronic Information Industry Co., Ltd, Beijing, China","2022 IEEE Symposium on Computers and Communications (ISCC)","19 Oct 2022","2022","","","1","6","Multi-access Edge Computing (MEC) plays an im-portant role for providing end users with high reliability and low latency services at the edge of mobile network. In the scenario of Internet of Vehicles (IoV), vehicle users continually access nearby base stations to offload real-time tasks for reducing their computing overhead, while the ongoing services on current deployed edge nodes may be far away from users with the vehicles moving, potentially resulting in a high delay of data transmission. To address this challenge, in this paper, we propose a Deep Reinforcement Learning (DRL)-based mobility-aware service migration mechanism for effectively reducing the service delay and migration delay of the network. The proposed technique is adopted by re-calibrating required services at edge locations near the mobile user. Edge network state and user movement information are considered to ensure the generation of real-time service migration decision. Extensive experiments are conducted, and evaluation results demonstrate that our proposed DRL-based technique can effectively reduce the long-term average delay of the MEC system, compared with the state-of-the-art techniques.","2642-7389","978-1-6654-9792-3","10.1109/ISCC55528.2022.9912842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9912842","Service Migration;Multi-access Edge Computing;Deep Reinforcement Learning;Mobility-Aware","Computers;Multi-access edge computing;Reinforcement learning;Real-time systems;Delays;Reliability;Data communication","cellular radio;Internet;learning (artificial intelligence);mobile computing;mobile radio;mobility management (mobile radio);quality of service;resource allocation","high reliability;low latency services;mobile network;vehicle users;nearby base stations;real-time tasks;computing overhead;ongoing services;current deployed edge nodes;high delay;Deep Reinforcement;service delay;migration delay;re-calibrating required services;edge locations;mobile user;user movement information;real-time service migration decision;DRL-based technique;long-term average delay;multiaccess Edge Computing environment;im-portant role;end users","","","","14","IEEE","19 Oct 2022","","","IEEE","IEEE Conferences"
"Multi-ASV Coordinated Tracking With Unknown Dynamics and Input Underactuation via Model-Reference Reinforcement Learning Control","W. Hu; F. Chen; L. Xiang; G. Chen","State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; Department of Electrical Engineering, City University of Hong Kong, Kowloon Tong, Hong Kong","IEEE Transactions on Cybernetics","15 Sep 2023","2023","53","10","6588","6597","This article studies coordinated tracking of underactuated and uncertain autonomous surface vehicles (ASVs) via model-reference reinforcement learning control. It considered how model-reference control can be incorporated with reinforcement learning to address the challenges caused by model uncertainties and input underactuation, and how existing results may be employed to realize adaptive communication amongst ASVs. It is demonstrated that the proposed algorithm has a better performance over baseline control and effectively improves the training efficiency over reinforcement learning.","2168-2275","","10.1109/TCYB.2022.3203507","National Science Foundation of China(grant numbers:61973061,61973064); Natural Science Foundation of Hebei Province of China(grant numbers:F2019501043,F2022501024,F2019501126); Natural Science Foundation of Liaoning Province of China(grant numbers:2020-KF-11-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9905234","Coordinated tracking;input underactuation;model-reference control;reinforcement learning;unknown dynamics","Reinforcement learning;Uncertainty;Vehicle dynamics;Protocols;Adaptation models;Surges;Damping","control system synthesis;learning (artificial intelligence);mobile robots;multi-robot systems;nonlinear control systems;reinforcement learning;unmanned surface vehicles","ASVs;baseline control;model uncertainties;model-reference control;model-reference reinforcement learning control;multiASV coordinated tracking;uncertain autonomous surface vehicles;underactuated surface vehicles","","4","","37","IEEE","28 Sep 2022","","","IEEE","IEEE Journals"
"R$^{2}$Fed: Resilient Reinforcement Federated Learning for Industrial Applications","W. Zhang; F. Yu; X. Wang; X. Zeng; H. Zhao; Y. Tian; F. -Y. Wang; L. Li; Z. Li","College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; School of Artificial Intelligence, Anhui University, Anhui, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Space Star Technology Co., Ltd., Beijing, China; Institute of Digital Research, ENN Group, Langfang, China","IEEE Transactions on Industrial Informatics","13 Jul 2023","2023","19","8","8829","8840","Federated learning has become an emerging hot research field in industry because of its ability to perform large-scale distributed learning while preserving data privacy. However, recent studies have shown that in the actual use of federated learning, there are device heterogeneity and data not identically and independently distributed (Non-IID) characteristics between client nodes, which will affect the effect of federated learning. In this work, we propose resilient reinforcement federated learning (R$^{2}$Fed), a R$^{2}$Fed method, which applies reinforcement learning to federated learning and uses reinforcement learning for weighted fusion of client models instead of average fusion. We conduct experiments on object detection, object classification, and sentiment classification tasks in the context of Non-IID and heterogeneity, and the experimental results show that the R$^{2}$Fed method outperforms traditional federated learning, increasing the average accuracy by 4.7%. Experiments also demonstrate that R$^{2}$Fed is resilient to federation attacks.","1941-0050","","10.1109/TII.2022.3222369","National Natural Science Foundation of China(grant numbers:62072469); Opening Project of the State Key Laboratory for Management and Control Complex Systems; Institute of Automation; Chinese Academy of Sciences(grant numbers:20210114); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950718","Federated learning;non-independent identically distributed (non-IID) data;reinforcement learning","Federated learning;Reinforcement learning;Data models;Distributed databases;Training;Computational modeling;Performance evaluation","data privacy;object detection;pattern classification;reinforcement learning;sentiment analysis","client models weighted fusion;data privacy preservation;federation attacks;industrial applications;industrial applications;large-scale distributed learning;non-IID characteristics;nonindependent identically distributed data;not identically and independently distributed characteristics;object classification;object detection;R2Fed method;resilient reinforcement federated learning;sentiment classification","","","","27","IEEE","15 Nov 2022","","","IEEE","IEEE Journals"
"Multiobjective Operation Optimization of Wastewater Treatment Process Based on Reinforcement Self-Learning and Knowledge Guidance","P. Zhou; X. Wang; T. Chai","State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China","IEEE Transactions on Cybernetics","17 Oct 2023","2023","53","11","6896","6909","This article proposes a multiobjective operation optimization method based on reinforcement self-learning and knowledge guidance for quality assurance and consumption reduction of wastewater treatment process (WWTP) with nonstationary time-varying dynamics. First, operation optimization models are developed by online sequential random vector functional-link (OS-RVFL) neural network, which can realize online sequential learning of model parameters. Then, a knowledge base is established to store typical optimization cases for knowledge guiding the subsequent optimizations. Based on it, a reinforcement self-learning-based multiobjective particle swarm optimization (RSL-MOPSO) algorithm is proposed to perform optimization calculation. In this algorithm, reinforcement self-learning is used for interaction learning between environment and action in optimization, and the particle motion trend of algorithm is adjusted according to the feedback information of the optimization process. The effects of wastewater state parameters on particles are recorded and reused to improve the solution quality and calculation efficiency of optimization. Moreover, to make good use of the information of the previous optimizations and balance the coordination between global search in the early stage and local search in the later stage, a selective information feedback mechanism is further proposed to ensure the diversity and convergence of the algorithm. Finally, prediction-based intelligent decision making is performed to select the final optimization solution as the final setpoints for the lower-level controllers from the Pareto frontier with considering specific technical requirements. Data experiments show that the proposed method can effectively reduce energy consumption and ensure effluent quality.","2168-2275","","10.1109/TCYB.2022.3164476","National Natural Science Foundation of China(grant numbers:61890934,61890930,61991400); Liaoning Revitalization Talents(grant numbers:XLYC1907132); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9766188","Information feedback;knowledge base;knowledge guidance;neural networks;operation optimization;reinforcement self-learning;wastewater treatment process (WWTP)","Optimization methods;Wastewater treatment;Wastewater;Employee welfare;Costs;Water resources;Standards","","","","","","35","IEEE","2 May 2022","","","IEEE","IEEE Journals"
"Simulation Based Multi-Objective Fab Scheduling by Using Reinforcement Learning","W. -J. Lee; B. -H. Kim; K. Ko; H. Shin","VMS Solutions Co.,Ltd., Yongin, REPUBLIC OF KOREA; VMS Solutions Co.,Ltd., Yongin, REPUBLIC OF KOREA; VMS Global, Inc., Vienna, VA, USA; Department of Industrial and Systems engineering, Korea Advanced Institute of Science and Technology(KAIST), REPUBLIC OF KOREA","2019 Winter Simulation Conference (WSC)","20 Feb 2020","2019","","","2236","2247","Semiconductor manufacturing fab is one of the most sophisticated man-made system, consisting of hundreds of very expensive equipment connected by highly automated material handling system. Operation schedule has huge impact on the productivity of the fab. Obtaining efficient schedule for numerous equipment is a very complex problem, which cannot be solved by conventional optimization techniques. Hence, heuristic dispatching rules combined with fab simulation is often used for generating fab operation schedule. In this paper, we formulate the fab scheduling problem as a semi-Markov decision process and propose a reinforcement learning method used in conjunction with the fab simulator to obtain the (near-)optimal dispatching policy. Resulting schedule obtained by the proposed method shows better performance than heuristic rules whose parameters are tuned by human experts.","1558-4305","978-1-7281-3283-9","10.1109/WSC40007.2019.9004886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9004886","","Dispatching;Mathematical model;Learning (artificial intelligence);Schedules;Job shop scheduling;Modeling;Tools","dispatching;heuristic programming;learning (artificial intelligence);Markov processes;materials handling;optimisation;production engineering computing;productivity;scheduling;semiconductor device manufacture;semiconductor industry","conventional optimization techniques;heuristic dispatching rules;semiMarkov decision process;reinforcement learning method;simulation based multiobjective fab scheduling;semiconductor manufacturing fab;man-made system;highly automated material handling system;productivity","","6","","20","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards","G. Schoettler; A. Nair; J. Luo; S. Bahl; J. Aparicio Ojea; E. Solowjow; S. Levine","Siemens Corporation; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; Siemens Corporation; Siemens Corporation; University of California, Berkeley","2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","10 Feb 2021","2020","","","5548","5555","Connector insertion and many other tasks commonly found in modern manufacturing settings involve complex contact dynamics and friction. Since it is difficult to capture related physical effects with first-order modeling, traditional control methods often result in brittle and inaccurate controllers, which have to be manually tuned. Reinforcement learning (RL) methods have been demonstrated to be capable of learning controllers in such environments from autonomous interaction with the environment, but running RL algorithms in the real world poses sample efficiency and safety challenges. Moreover, in practical real-world settings, we cannot assume access to perfect state information or dense reward signals. In this paper, we consider a variety of difficult industrial insertion tasks with visual inputs and different natural reward specifications, namely sparse rewards and goal images. We show that methods that combine RL with prior information, such as classical controllers or demonstrations, can solve these tasks from a reasonable amount of real-world interaction.","2153-0866","978-1-7281-6212-6","10.1109/IROS45743.2020.9341714","Siemens; Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341714","","Robotic assembly;Connectors;Visualization;Reinforcement learning;Manufacturing;Safety;Task analysis","computer vision;learning (artificial intelligence);production engineering computing","visual inputs;connector insertion;manufacturing settings;contact dynamics;friction;physical effects;first-order modeling;brittle controllers;reinforcement learning methods;RL;autonomous interaction;safety challenges;state information;dense reward signals;sparse rewards;natural reward specifications;industrial insertion tasks","","50","","42","IEEE","10 Feb 2021","","","IEEE","IEEE Conferences"
"Deep-Reinforcement-Learning-Based Spectrum Resource Management for Industrial Internet of Things","Z. Shi; X. Xie; H. Lu; H. Yang; M. Kadoch; M. Cheriet","School of Computer Science and Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Computer Networks and Communication Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Computer Networks and Communication Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Electrical Engineering, École de Technologie Supérieure, Montreal, Canada; Department of Automated Production Engineering, École de Technologie Supérieure, Montreal, Canada","IEEE Internet of Things Journal","18 Feb 2021","2021","8","5","3476","3489","The Industrial Internet of Things (IIoT) has attracted tremendous interest from both industry and academia as it can significantly improve production efficiency and system intelligence. However, with the explosive growth of various types of user equipment (UE) and data flow, IIoT experiences spectrum resource scarcity for wireless applications. In this article, we propose a solution for spectrum resource management for the IIoT network, with the objective of facilitating the limited spectrum sharing between different kinds of UEs. To overcome the challenges of unknown dynamic IIoT environments, a modified deep $Q$ -learning network (MDQN) is developed. Considering the cost effectiveness of IIoT devices, the base station (BS) acts as a single agent and centrally manages the spectrum resources, which can be executed without coordination or exchange between UEs. In this article, we first built a realistic IIoT model and design a simple medium access control (MAC) frame structure to facilitate the environment state observation. Then, a new reward function is designed to drive the learning process, which takes into account the different communication requirements of various types of UEs. In addition, to improve the learning efficiency, we compress the action space and propose a priority experience replay strategy based on decreasing temporal difference (TD) error. Finally, simulation results show that the proposed algorithm can successfully achieve dynamic spectrum resource management in the IIoT network. Compared with other algorithms, it can achieve superior network performance with a faster convergence rate.","2327-4662","","10.1109/JIOT.2020.3022861","National Nature Science Foundation of China(grant numbers:61502067); Key Research Project of Chongqing Education Commission(grant numbers:KJZD-K201800603); Key Project of Science and Technology Research of Chongqing Education Commission(grant numbers:KJZD-M201900602); Chongqing Nature Science Foundation(grant numbers:CSTC2018jcyjAX0432,CSTC2016jcyjA0455); Doctoral Candidate Innovative Talent Project of Chongqing University of Posts and Telecommunications(grant numbers:BYJS201912); Key Project on Anhui Provincial Natural Science Study by Colleges and Universities(grant numbers:KJ2019A0554); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187936","Deep reinforcement learning (DRL);Industrial Internet of Things (IIoT);learning efficiency;priority experience replay;spectrum resource management","Resource management;Internet of Things;Wireless communication;Sensors;Wireless sensor networks;Industries;Heuristic algorithms","access protocols;deep learning (artificial intelligence);Internet of Things;production engineering computing;radio spectrum management","production efficiency;system intelligence;data flow;spectrum resource scarcity;IIoT network;IIoT devices;base station;spectrum resources;environment state observation;dynamic spectrum resource management;dynamic IIoT environments;medium access control frame structure;modified deep Q-learning network;MAC frame structure;deep reinforcement learning;reward function;temporal difference error;priority experience replay strategy;spectrum sharing;industrial Internet of Things","","23","","30","IEEE","8 Sep 2020","","","IEEE","IEEE Journals"
"Moving Target Shooting Control Policy Based on Deep Reinforcement Learning","B. Li; T. Jin; Y. Zhu; H. Li; Y. Wu; D. Zhao","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Automation, Beijing Information Science and Technology University, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Automation, Beijing Information Science and Technology University, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2021 8th International Conference on Information, Cybernetics, and Computational Social Systems (ICCSS)","1 Mar 2022","2021","","","183","188","Robots are playing a more and more important role in people’s production and life, recently. However, robot control in dynamic environment is still a difficulty. With the great breakthrough of deep reinforcement learning in the field of video games, this method is also extended to the field of robots. Due to the gap between the simulation environment and the real environment, the deep reinforcement learning algorithm trained in the simulation environment is difficult to be applied to the real environment. Aiming at the gimbal control with two degrees of freedom (DOF), a pipline combining system identification and deep reinforcement learning is proposed. On the one hand, the shooting accuracy of the gimbal to moving objects is improved through deep reinforcement learning algorithm. On the other hand, the gap between simulation and reality is reduced through system identification. The method is verified in the RoboMaster University AI Challenge (RMUA) shooting system. The results show that the shooting accuracy is better than the classical control method.","2639-4235","978-1-6654-0245-3","10.1109/ICCSS53909.2021.9722012","Research and Development; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9722012","moving target;shooting control;deep reinforcement learning;system identification","Computational modeling;Heuristic algorithms;Robot control;Pipelines;Reinforcement learning;Production;Games","control engineering computing;deep learning (artificial intelligence);image motion analysis;manipulators;object tracking;reinforcement learning;robot vision","simulation environment;gimbal control;shooting accuracy;RoboMaster University AI Challenge shooting system;robot control;dynamic environment;deep reinforcement learning;moving target shooting control policy;degrees of freedom;system identification;moving objects;RMUA","","","","20","IEEE","1 Mar 2022","","","IEEE","IEEE Conferences"
"The implementation of reinforcement learning algorithms on the elevator control system","H. Li","Institute of Automation and Information System, Technische Universität München, Garching, Munich, Germany","2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA)","26 Oct 2015","2015","","","1","4","The machine learning technology has been implemented in control systems successfully especially in the stochastic process. Among the machine learning algorithms, the reinforcement learning algorithm is fairly significant. On the other hand, the elevator, as a facility in daily life, is implemented with several heuristic control methods in the past practice. Simultaneously, the elevator working procedure can be viewed as a stochastic process, so this paper tries to discuss the implementation of several reinforcement learning algorithms on the elevator control system. Through the application of the reinforcement learning algorithms, the elevator system can perform better.","1946-0759","978-1-4673-7929-8","10.1109/ETFA.2015.7301554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301554","reinforcement algorithms;elevator control system;Q-learning","Elevators;Floors;Learning (artificial intelligence);Machine learning algorithms;Control systems;Algorithm design and analysis;Stochastic processes","control engineering computing;learning (artificial intelligence);lifts;stochastic systems","reinforcement learning algorithms;elevator control system;machine learning technology;stochastic process;heuristic control methods;elevator working procedure","","5","","13","IEEE","26 Oct 2015","","","IEEE","IEEE Conferences"
"UAV online path planning technology based on deep reinforcement learning","J. Fan; Z. Wang; J. Ren; Y. Lu; Y. Liu","Research and Development Center of China Academy of Launch Vehicle Technology of Beijing, Beijing, China; Research and Development Center of China Academy of Launch Vehicle Technology of Beijing, Beijing, China; Research and Development Center of China Academy of Launch Vehicle Technology of Beijing, Beijing, China; Research and Development Center of China Academy of Launch Vehicle Technology of Beijing, Beijing, China; School of Automation Science and Electrical Engineering of Beihang, University of Beijing, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","5382","5386","This paper proposes a method for planning three-dimensional path for unmanned aerial vehicle (UAV) in complex airspace based on interfered fluid dynamical system (IFDS) and deep reinforcement learning. Firstly, the model of unmanned aerial vehicle under various constraints and the mathematical expression of threat zone are established. Secondly, in order to solve the problems of slow calculation speed and difficult to make the global optimal solution existed at present, an intelligent 3D path planning method on the basis of IFDS is proposed, and deep reinforcement learning is used to solve the coefficient of IFDS. The simulation results show that the path planned by the proposed method can avoid the threat zone effectively, meanwhile, the path is smooth, suitable and fuel saving for UAV.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327752","path planning;interfered fluid dynamical system (IFDS);unmanned aerial vehicle (UAV);deep reinforcement learning;Twin Delayed Deep Deterministic Policy Gradient (TD3)","Unmanned aerial vehicles;Path planning;Planning;Reinforcement learning;Atmospheric modeling;Mathematical model;Research and development","autonomous aerial vehicles;control engineering computing;deep learning (artificial intelligence);mobile robots;path planning;remotely operated vehicles","interfered fluid dynamical system;deep reinforcement learning;unmanned aerial vehicle;threat zone;UAV online path planning technology;three-dimensional path;IFDS","","3","","11","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Control of Quadrotor Drone with Partial State Observation via Reinforcement Learning","G. SHAN; Y. Zhang; Y. Gao; T. Wang; J. Chen","School of Instrument Sci. and Opto-electronic Engineering, Beihang University, Beijing, China; School of Instrument Sci. and Opto-electronic Engineering, Beihang University, Beijing, China; Department of Aviation Engineering, Naval Aviation University, Yantai, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Suzhou University of Science and Technology, Suzhou, China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","1965","1968","In this paper, we propose a quadrotor control algorithm which use the historical strengthened partial state observations as input information to control the quadrotor drone using reinforcement learning algorithm. Reinforcement learning method could enable the agent to learn a policy which could map the observations to control commands, which, in our work, is the actuator command of the quadrotor. Besides, we conduct our method in the control task via simulation, the results of which show excellent performance.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8996394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8996394","reinforcement learning;quadrotor drone;quadrotor control;partial state observation","Learning (artificial intelligence);Neural networks;Training;Optimization;Rotors;Computational modeling;Drones","aircraft control;autonomous aerial vehicles;control engineering computing;helicopters;learning (artificial intelligence)","quadrotor drone;partial state observation;quadrotor control algorithm;reinforcement learning;control commands;control task;actuator command","","2","","12","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"A stochastic adaptive traffic signal control model based on fuzzy reinforcement learning","Kaige Wen; Wugang Yang; Shiru Qu","School of Electronics and Control Engineering, Chang'an University, Xi'an, Shaanxi, China; School of Electronics and Control Engineering, Chang'an University, Xi'an, Shaanxi, China; School of Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, China","2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE)","19 Apr 2010","2010","5","","467","471","The signalized intersection system often exhibits severe nonlinear and time-varying characteristic due to the random fluctuation of traffic demand or some special event, therefore, it cannot be adequately controlled with some traditional ways. The traditional reinforcement learning was extended to the fuzzy pattern with defining the fuzzy reinforcement function by using the fuzzy state. A stochastic control scheme, based on fuzzy reinforcement learning, is introduced in the traffic signal control systems due to its powerful adaptability. The FRL-based adaptive controller can produced appropriate control policy to prevent the traffic network from becoming over-congested. The traditional intersection traffic model is extended to a new mode which taking some real aspects of traffic conditions into account, such as the turning fraction and the lanes scheme. The model is tested on a typical four-legged signalized intersection, and compared to both pre-timed control and full-actuated controller. Analyses of simulation results using this approach show significant improvement over traditional control, especially for the case of over-saturated traffic demand and special events such as incidents and blockages. Using the FRL model, the total mean delay of each vehicle has been reduced by 25.7% under the heavy demands compared to the FAC scheme.","","978-1-4244-5586-7","10.1109/ICCAE.2010.5451248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5451248","Reinforcement learning;fuzzy logic;traffic model;traffic signal control","Stochastic processes;Programmable control;Adaptive control;Traffic control;Fuzzy control;Learning;Communication system traffic control;Control systems;Time varying systems;Fluctuations","adaptive control;fuzzy systems;learning (artificial intelligence);stochastic systems;traffic control","stochastic adaptive traffic signal control model;fuzzy reinforcement learning;signalized intersection system;time-varying characteristic;traffic demand;fuzzy reinforcement function;fuzzy state;stochastic control;traffic signal control system;adaptive controller;traffic network;intersection traffic model;traffic conditions;turning fraction;lanes scheme","","1","","10","IEEE","19 Apr 2010","","","IEEE","IEEE Conferences"
"Vision Based Autonomous Tracking of UAVs Based on Reinforcement Learning","G. Xiong; L. Dong","School of Automation, Southeast University, Nanjing, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","2682","2686","Air-ground cooperation is of great importance in certain extreme environment missions, which require real-time tracking of the unmanned aerial vehicle (UAV) on the ground vehicle. As reinforcement learning (RL) has achieved great success in many challenges of planning and control, a research on path planning of UAVs for object tracking based on RL is presented, considering images from a visual sensor as the input. Convolutional neural networks (CNN) and a spatial soft-max layer are used to detect the object in the images. The tracking results of a filter CSR-DCF based on OpenCV are combined with the output of CNN to improve training efficiency and obtain better tracking performance. Three independent experiments are conducted with different conditions in simulated environments where a quadcopter is trained to track a ground robot based on V-REP. Valid results show that the UAV has good performance in the tracking work with MAE of 0.23m in x and 0.19m in y.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9326946","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326946","object tracking;reinforcement learning;UAV;vision based","Robots;Neural networks;Training;Target tracking;Reinforcement learning;Object tracking;Vision sensors","autonomous aerial vehicles;helicopters;learning (artificial intelligence);mobile robots;neural nets;object detection;object tracking;path planning;robot vision","unmanned aerial vehicle;UAV;ground vehicle;reinforcement learning;RL;object tracking;visual sensor;convolutional neural networks;CNN;spatial soft-max layer;filter CSR-DCF;tracking performance;simulated environments;ground robot;tracking work;air-ground cooperation;extreme environment missions;real-time tracking;vision based autonomous tracking;size 0.19 m;size 0.23 m","","1","","20","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for the Fighter Theater","G. Li; W. Zhang","School of Automation, Guangdong University of Technology, Guangzhou, China; Western Digital, Irvine, CA, USA","2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","13 Feb 2020","2019","","","1420","1425","The Fighter Theater is an AI game in which heroes of the two sides compete against the opposite side by finding energy stones or fighting against enemy heroes based on state-machine algorithm. We propose new strategies to heroes from each side and modify the state-machine algorithm to increase the confrontations of the game environment. Besides, inspired by deep reinforcement learning, we use convolutional neural network to model Q-function and implement Deep Q-Networks with experience replay to train one side of the game. Experimental result demonstrates despite the high dimensionality of raw pixel input, the side with Deep Q-learning can boost the performance of heroes by finding an optimal policy and is capable of outperforming the opposite side that is controlled by the state-machine algorithm.","2381-0947","978-1-7281-1907-6","10.1109/IAEAC47372.2019.8997984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8997984","intelligent gaming;deep reinforcement learning;convolutional neural network","Games;Green products;Machine learning;Neural networks;Learning (artificial intelligence);Training;Blood","computer games;convolutional neural nets;learning (artificial intelligence)","deep reinforcement learning;fighter theater;AI game;state-machine algorithm;game environment;convolutional neural network;deep Q-networks;deep Q-learning","","","","7","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Missile Attitude Control Based on Deep Reinforcement Learning","B. Li; F. Ma; Y. Wu","State Key Laboratory of Virtual Reality Technology and System, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Science and Technology on Aircraft Control Laboratory, Beihang University, Beijing, China","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","931","936","Deep reinforcement learning (DRL) has been one of the research hotspots in the areas of control. In this paper, we focus on the study of missile attitude control system using DRL. An novel PID controller based on deep deterministic policy gradient(DDPG) algorithm is presented, which could applied to the self-tuning of parameters. The framework of the adaptive DDPG-PID controller is given. The controller takes flight information as input and takes rudder angle as output. A reward function related to the system error is designed, which can be used to train the DDPG algorithm effectively. Simulation results show that the adaptive DDPG-PID controller has a faster convergence velocity, reduces the overshoot and oscillation, achieves higher accuracy tracking control to target.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264391","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264391","deep reinforcement learning;missile attitude control;DDPG;PID","Software algorithms;Missiles;Heuristic algorithms;Earth;Adaptation models;Task analysis;Solid modeling","attitude control;learning (artificial intelligence);missile control;neurocontrollers;three-term control","missile attitude control system;DRL;adaptive DDPG-PID controller;DDPG algorithm;tracking control;deep reinforcement learning;deep deterministic policy gradient algorithm","","","","13","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Delivery Route Optimization Based on Deep Reinforcement Learning","E. Xing; B. Cai","McDonald's (China) Co., Ltd.; School of Geography, Nanjing Normal University","2020 2nd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)","26 Feb 2021","2020","","","334","338","With the rapid development of fast food industry, the research on the algorithm of delivery problem becomes increasingly important. The delivery problem of takeout is essentially the optimal path problem, while the traditional algorithm optimization of delivery path has been unable to meet the needs of modern takeout development. Based on this, this paper carries out the research on the delivery path optimization based on Deep Reinforcement learning algorithm. In this paper, we use the improvement method Heuristics in Deep Reinforcement learning to optimize the delivery path. In addition, we compare this method with the traditional tabu search algorithm, three distribution locations are selected and compared from two aspects of delivery time and customer satisfaction. The results show that using Deep Reinforcement learning algorithm to optimize delivery path can effectively reduce delivery time and improve delivery efficiency.","","978-1-7281-9638-1","10.1109/MLBDBI51377.2020.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361015","Deep Reinforcement learning;Delivery;Route Optimization","Customer satisfaction;Food industry;Reinforcement learning;Big Data;Food products;Business intelligence;Optimization","customer satisfaction;genetic algorithms;goods distribution;learning (artificial intelligence);optimisation;search problems;telecommunication network routing","optimal path problem;traditional algorithm optimization;modern takeout development;delivery path optimization;Deep Reinforcement learning algorithm;traditional tabu search algorithm;delivery time;delivery efficiency;delivery route optimization;fast food industry;delivery problem","","4","","10","IEEE","26 Feb 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Quantitative Trading: Challenges and Opportunities","B. An; S. Sun; R. Wang","Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore","IEEE Intelligent Systems","20 May 2022","2022","37","2","23","26","Quantitative trading (QT) has been a popular topic in both academia and the financial industry since the 1970s. In the last decade, deep reinforcement learning (DRL) has garnered significant research interest with stellar performance in solving complex sequential decision-making problems, such as Go and video games. The impact of DRL is pervasive, recently demonstrating its ability to conquer some challenging QT tasks. In this article, we outline several key challenges and opportunities that manifest in DRL-based QT to shed light on future research in this field.","1941-1294","","10.1109/MIS.2022.3165994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779600","","Decision making;Reinforcement learning;Games;Task analysis;Intelligent systems;Financial industry;Deep learning","decision making;deep learning (artificial intelligence);reinforcement learning;stock markets","DRL-based QT;deep reinforcement learning;quantitative trading;financial industry;stellar performance;complex sequential decision-making problems","","3","","14","IEEE","20 May 2022","","","IEEE","IEEE Magazines"
"Depth-Based Reinforcement Learning Algorithm","X. Yi","Northwest Minzu University, LanZhou, GanSu, China","2022 International Conference on Artificial Intelligence of Things and Crowdsensing (AIoTCs)","20 Apr 2023","2022","","","211","215","With the continuous development of Internet, cloud computing, 5g technology and we media industry, people generate and obtain more and more information every day, and most of these information exists in the form of text. Automatic text technology can quickly help people obtain clear and readable information in massive text. Aiming at the problem that the traditional text technology based on deep learning can not generate high-quality long text summary, this paper explores a deep proxy text generation technology based on reinforcement learning. The model divides the long-term and short-term memory network into different agents, and each agent is responsible for encoding a part of the source text. After that, each agent will broadcast its own coding results, so that all agents can share the global context information. After the final coding, the model integrates the information from multiple agents through a context agent attention mechanism and transmits it to the decoder, so as to optimize the process of long text summary generation.","","979-8-3503-3410-4","10.1109/AIoTCs58181.2022.00111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10102192","Deep learning;Text generation;Reinforcement learning","Deep learning;Training;Industries;Computational modeling;Reinforcement learning;Learning (artificial intelligence);Media","5G mobile communication;cloud computing;deep learning (artificial intelligence);multi-agent systems;reinforcement learning;text analysis","5g technology;automatic text technology;cloud computing;context agent attention mechanism;deep learning;deep proxy text generation technology;depth-based reinforcement learning algorithm;global context information;Internet;long text summary generation;massive text;media industry;multiple agents;short-term memory network;source text;traditional text technology","","","","10","IEEE","20 Apr 2023","","","IEEE","IEEE Conferences"
"Path Planning for Unmanned Marine Vessels Based on Improved Reinforcement Learning","C. Wang; W. Zhang; Y. Wang","Navigation College, Dalian Maritime University, Dalian, China; Navigation College, Dalian Maritime University, Dalian, China; Navigation College, Dalian Maritime University, Dalian, China","2021 13th International Conference on Advanced Infocomm Technology (ICAIT)","9 Feb 2022","2021","","","76","80","With the increasing development of the shipping industry, the research on perfecting the route planning behavior of unmanned ships has become increasingly important. Path planning is an important part of unmanned ship navigation. Its goal is to enable the unmanned ship to find a collision-free path from the starting point to the end point in a more complicated environment. In this paper, by improving the Q-learning algorithm, it can search in multiple directions, avoiding the local optimum to a large extent, and making the turning trajectory smoother. By modifying the iteration parameters, the efficiency of the algorithm is improved, the optimal path planning can be made in a short time, and the results can be analyzed.","2770-1603","978-1-6654-3188-0","10.1109/ICAIT52638.2021.9702052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9702052","path planning;Q-learning;unmanned ship;reinforcement learning","Industries;Q-learning;Navigation;Turning;Trajectory;Planning;Marine vehicles","control engineering computing;path planning;reinforcement learning;ships;unmanned surface vehicles","route planning behavior;unmanned ship navigation;collision-free path;end point;Q-learning algorithm;optimal path planning;unmanned marine vessels;improved reinforcement learning;increasing development;shipping industry","","","","20","IEEE","9 Feb 2022","","","IEEE","IEEE Conferences"
"Safe, Fast and Explainable Online Reinforcement Learning for Continuous Process Control","K. M. Patel","Saudi Aramco, P O Box 5000, Dhahran, Saudi Arabia","2022 IEEE International Symposium on Advanced Control of Industrial Processes (AdCONIP)","21 Sep 2022","2022","","","54","60","Industrial process control using model-based technologies is well established. These technologies are typically non-adaptive and so have limitations. Reinforcement Learning (RL) provides a model-free adaptive alternative. RL is a type of machine learning (ML) where models or data sets of the environment are not necessary before learning can start. It generates data, by exploring the environment and then learn the behavior from it. Though RL has been successfully applied for learning and playing various games such as Go, Chess, Atari; its application to continuous process control problems is not trivial. There is a need for online RL implementation to be safe, fast learning and explainable when applied to industrial control problems. Rather than adding to the extensive research on augmenting existing RL algorithms, the work focuses on developing a unique systematic method of formulating the RL problem incorporating domain-specific knowledge about process constraints and objectives, reducing dimensionality and modifying the exploration process, applicable to any model free RL algorithm supporting continuous states and actions, to enhance safety, speed and explainability of online RL implementation without requiring a simulation model. The approach is successfully implemented on two multivariable processes: a simulated distillation column and a temperature control lab setup using the Deep Deterministic Policy Gradient (DDPG) algorithm. The work demonstrates that the developed method is applicable to multivariable, noisy, non-linear processes with disturbances. It will further the potential of introducing the advances in Artificial Intelligence and ML algorithms for intelligent process control capable of enabling autonomous operation in the process industry.","","978-1-6654-7174-9","10.1109/AdCONIP55568.2022.9894195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894195","","Industries;Heating systems;Adaptation models;Systematics;Process control;Reinforcement learning;Market research","industrial control;learning (artificial intelligence);process control;temperature control","explainable;industrial control problems;existing RL algorithms;unique systematic method;RL problem;process constraints;exploration process;model free RL;continuous states;explainability;online RL implementation;simulation model;multivariable processes;temperature control lab setup;Deep Deterministic Policy Gradient algorithm;nonlinear processes;ML;intelligent process control;process industry;industrial process control;model-based technologies;nonadaptive;Reinforcement Learning;model-free adaptive alternative;machine learning;continuous process control problems;fast learning","","","","15","IEEE","21 Sep 2022","","","IEEE","IEEE Conferences"
"Optimized Reward Function Based Deep Reinforcement Learning Approach for Object Detection Applications","Z. Tan; M. Karaköse","Erzincan Binali Yıldırım University, Erzincan, Turkey; Computer Engineering Department, Firat University, Elazig, Turkey","2022 International Conference on Decision Aid Sciences and Applications (DASA)","2 May 2022","2022","","","1367","1370","Reinforcement learning is considered a powerful artificial intelligence method that can be used to teach machines through interaction with the environment and learning from their mistakes. More and more applications are coming to the fore where Reinforcement learning has been newly and successfully implemented. It is frequently used especially in the game industry and robotics. In this article, a deep reinforcement learning approach, which uses our own developed neural network, is presented for object detection on the PASCAL Voc2012 dataset. Our approach is by moving a bounding box step-by-step towards the goal in order to fully frame the object in the picture. The created neural network consists of a 5-layer structure. In addition, it is aimed to maximize the mAP value by optimizing the reward function. The right choice in the reward policy will certainly affect the outcome and will play an important role in the training of the agent. Thanks to the optimized reward function, ground truth and the bounding box intersect at the highest rate, contributing positively to the result. As a result of the training that lasted for approximately 36 hours, the test results of 6 randomly selected classes were compared with the results of previous similar studies. Within the scope of this article, some artificial neural networks and basic studies in the literature using the Reinforcement learning approach for object detection are examined.","","978-1-6654-9501-1","10.1109/DASA54658.2022.9764979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9764979","Object detection;deep reinforcement learning;CNN;deep learning","Training;Industries;Deep learning;Service robots;Reinforcement learning;Object detection;Learning (artificial intelligence)","deep learning (artificial intelligence);image representation;neural nets;object detection","optimized reward function;object detection applications;artificial intelligence method;game industry;robotics;deep reinforcement learning approach;developed neural network;PASCAL Voc2012 dataset;reward policy;bounding box intersect;artificial neural networks;time 36.0 hour","","","","22","IEEE","2 May 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Driven Continuous and Crashless Load Test Architecture","T. Büyüktanır; B. Erbay; M. Altun","Loodos Tech, İstanbul; Bilgisayar Mühendisligĭ Bölümü, Yıldız Teknik Üniversitesi, İstanbul; Bilişim Teknolojileri Bölümü, Sabancı Üniversitesi, İstanbul","2022 30th Signal Processing and Communications Applications Conference (SIU)","29 Aug 2022","2022","","","1","4","Realistic load testing of software systems and continuous testing of the systems is an industry requirement. Within the scope of this study, a software architecture was presented, and prototype implementation was implemented for load testing of live systems with realistic loads continuously and without crashing the system. Experiments of the crashless load test module of the prototype were implemented, and successful results were obtained from experiments.","2165-0608","978-1-6654-5092-8","10.1109/SIU55565.2022.9864834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864834","reinforcement learning;load test;crashless load test;continuous load test","Industries;Software architecture;Prototypes;Computer architecture;Signal processing;Software systems;Computer crashes","program testing;reinforcement learning;software architecture","reinforcement learning-driven continuous;crashless load test architecture;realistic load testing;software systems;continuous testing;industry requirement;software architecture;live systems;crashless load test module","","","","0","IEEE","29 Aug 2022","","","IEEE","IEEE Conferences"
"Multi-component Maintenance Optimization: an Approach combining Genetic Algorithm and Multiagent Reinforcement Learning","B. Li; Y. Zhou","Dept. School of Mechanical Engineering, Southeast University, Nanjing, China; Dept. School of Mechanical Engineering, Southeast University, Nanjing, China","2020 Global Reliability and Prognostics and Health Management (PHM-Shanghai)","18 Dec 2020","2020","","","1","7","The maintenance strategy optimization of the systems with intermediate buffers is a typical maintenance optimization. As the number of components in the system is increased, the state space and action space of the maintenance optimization of the manufacturing system with buffer inventory increase exponentially. Multi-agent reinforcement learning is an effective method to optimize the maintenance decision making of large multi-component system. However, as the number of agents increases, the reward function of multi-agent reinforcement learning tends to be complicated, and each agent will receive a noisy reward signal, so it is difficult for multi-agent reinforcement learning to converge to the optimal strategy. Considering the excellent global optimization ability of genetic algorithm, this paper adopts genetic algorithm as the central unit to guide the decision-making of each agent, and establishes a bilateral interaction mechanism between multi-agent reinforcement learning and genetic algorithm, through which both genetic algorithm and multi-agent reinforcement learning can learn the solutions provided by the other party. Numerical research results show that the proposed method is superior to multi-agent reinforcement learning and genetic algorithm in terms of solution quality.","","978-1-7281-5946-1","10.1109/PHM-Shanghai49105.2020.9280997","National Natural Science Foundation of China(grant numbers:71671041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9280997","maintenance optimization;genetic algorithm;multiagent reinforcement learning;intermediate buffert","Decision making;Reinforcement learning;Maintenance engineering;Reliability;Optimization;Genetic algorithms;Manufacturing systems","decision making;genetic algorithms;learning (artificial intelligence);maintenance engineering;manufacturing systems;multi-agent systems;production engineering computing","genetic algorithm;multicomponent maintenance optimization;maintenance strategy optimization;multiagent reinforcement learning;multicomponent system;manufacturing system;bilateral interaction mechanism;decision-making;buffer inventory","","2","","20","IEEE","18 Dec 2020","","","IEEE","IEEE Conferences"
"Large-Scale Traffic Signal Control Using a Novel Multiagent Reinforcement Learning","X. Wang; L. Ke; Z. Qiao; X. Chai","State Key Laboratory for Manufacturing Systems Engineering, School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory for Manufacturing Systems Engineering, School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory for Manufacturing Systems Engineering, School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China; CETC Key Laboratory of Aerospace Information Applications, Shijiazhuang, China","IEEE Transactions on Cybernetics","22 Dec 2020","2021","51","1","174","187","Finding the optimal signal timing strategy is a difficult task for the problem of large-scale traffic signal control (TSC). Multiagent reinforcement learning (MARL) is a promising method to solve this problem. However, there is still room for improvement in extending to large-scale problems and modeling the behaviors of other agents for each individual agent. In this article, a new MARL, called cooperative double Q-learning (Co-DQL), is proposed, which has several prominent features. It uses a highly scalable independent double Q-learning method based on double estimators and the upper confidence bound (UCB) policy, which can eliminate the over-estimation problem existing in traditional independent Q-learning while ensuring exploration. It uses mean-field approximation to model the interaction among agents, thereby making agents learn a better cooperative strategy. In order to improve the stability and robustness of the learning process, we introduce a new reward allocation mechanism and a local state sharing method. In addition, we analyze the convergence properties of the proposed algorithm. Co-DQL is applied to TSC and tested on various traffic flow scenarios of TSC simulators. The results show that Co-DQL outperforms the state-of-the-art decentralized MARL algorithms in terms of multiple traffic metrics.","2168-2275","","10.1109/TCYB.2020.3015811","National Natural Science Foundation of China(grant numbers:61973244,61573277); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186324","Double estimators;mean-field approximation;multiagent reinforcement learning (MARL);traffic signal control (TSC)","Learning (artificial intelligence);Convergence;Games;Nash equilibrium;Standards;Markov processes","learning (artificial intelligence);multi-agent systems;optimisation;road traffic control","large-scale traffic signal control;multiagent reinforcement learning;optimal signal timing strategy;large-scale problems;individual agent;highly scalable independent double Q-learning method;double estimators;over-estimation problem;independent Q-learning;learning process;local state sharing method;traffic flow scenarios;decentralized MARL algorithms;multiple traffic metrics;cooperative double Q-learning;upper confidence bound policy;UCB;mean-field approximation;reward allocation mechanism;convergence properties;Co-DQL;TSC simulators","","62","","49","IEEE","3 Sep 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning Tracking Control for Robotic Manipulator With Kernel-Based Dynamic Model","Y. Hu; W. Wang; H. Liu; L. Liu","State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Department of Mathematics, Georgia Institute of Technology, Atlanta, USA; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China","IEEE Transactions on Neural Networks and Learning Systems","1 Sep 2020","2020","31","9","3570","3578","Reinforcement learning (RL) is an efficient learning approach to solving control problems for a robot by interacting with the environment to acquire the optimal control policy. However, there are many challenges for RL to execute continuous control tasks. In this article, without the need to know and learn the dynamic model of a robotic manipulator, a kernel-based dynamic model for RL is proposed. In addition, a new tuple is formed through kernel function sampling to describe a robotic RL control problem. In this algorithm, a reward function is defined according to the features of tracking control in order to speed up the learning process, and then an RL tracking controller with a kernel-based transition dynamic model is proposed. Finally, a critic system is presented to evaluate the policy whether it is good or bad to the RL control tasks. The simulation results illustrate that the proposed method can fulfill the robotic tracking tasks effectively and achieve similar and even better tracking performance with much smaller inputs of force/torque compared with other learning algorithms, demonstrating the effectiveness and efficiency of the proposed RL algorithm.","2162-2388","","10.1109/TNNLS.2019.2945019","National Key R & D Program of China(grant numbers:2016YFE0206200); Key R & D and Technology Transfer Program of Shenyang Science and Technology Plan(grant numbers:18-400-6-16); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8890006","Kernel function;reinforcement learning (RL);reward function;robotics tracking control","Manipulator dynamics;Heuristic algorithms;Task analysis;Kernel;Adaptation models","learning (artificial intelligence);learning systems;manipulator dynamics;optimal control;position control","robotic tracking tasks;learning algorithms;RL algorithm;reinforcement learning tracking control;robotic manipulator;kernel-based dynamic model;optimal control policy;continuous control tasks;kernel function sampling;robotic RL control problem;learning process;RL tracking controller;kernel-based transition dynamic model;RL control tasks","","20","","33","IEEE","1 Nov 2019","","","IEEE","IEEE Journals"
"Stochastic Economic Lot Scheduling via Self-Attention Based Deep Reinforcement Learning","W. Song; N. Mi; Q. Li; J. Zhuang; Z. Cao","Institute of Marine Sci-ence and Technology, Shandong University, Qingdao, China; Institute of Marine Sci-ence and Technology, Shandong University, Qingdao, China; Institute of Marine Sci-ence and Technology, Shandong University, Qingdao, China; Agency for Science Technology and Research (A*STAR), Singapore Institute of Manufacturing Technology (SIMTech), Singapore, Singapore; Agency for Science Technology and Research (A*STAR), Institute for Infocomm Research (I2R), Singapore, Singapore","IEEE Transactions on Automation Science and Engineering","","2023","PP","99","1","12","The Stochastic Economic Lot Scheduling Problem (SELSP) is a difficult dynamic optimization problem with wide industrial applications. Traditional methods such as hyper-heuristics are manually designed based on substantial expert knowledge, which may limit their optimization performance. Recently, Deep Reinforcement Learning (DRL) is shown to be promising in automatically learning scheduling policies for SELSP. However, its performance is still quite far from that of hyper-heuristics, due to the lack of suitable deep models. In this paper, we propose a novel DRL method to learn dynamic scheduling policies for SELSP in an end-to-end fashion. Based on self-attention, our method can effectively extract useful features from raw state information, and is flexible in handling different numbers of products, which is not viable for previous methods. Experiments on a complex biopharmaceutical manufacturing process show that our method outperforms a recent DRL method and state-of-the-art hyper-heuristics. Moreover, the trained policy performs better in environments different from training with demand forecast errors and varying number of products, showing its strong robustness and generalization ability. Note to Practitioners—The Stochastic Economic Lot Scheduling Problem (SELSP) is an important problem for manufacturing enterprises, which is to optimally balance the production and inventory so as to minimize the total cost. However, SELSP is very challenging to solve due to the involvement of uncertain factors such as customer demands and machine failures. Traditional methods for solving SELSP, such as heuristic policies and hyper-heuristics, heavily rely on human experiences to design and hence the performance could be limited. This paper proposes a Deep Reinforcement Learning (DRL) based method to automatically learn scheduling policy for solving SELSP, which could alleviate the above limitation through a self-attention based feature extraction mechanism and reward based training. Experimental results on a realistic manufacturing process show that our method can deliver higher revenue than conventional manual policy and an existing DRL based method.","1558-3783","","10.1109/TASE.2023.3248229","National Natural Science Foundation of China(grant numbers:62102228); Natural Science Foundation of Shandong Province(grant numbers:ZR2021QF063); Young Scholar Future Plan of Shandong University(grant numbers:62420089964188); First A*Star Career Development(grant numbers:202D800040); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054449","Deep reinforcement learning;stochastic economic lot scheduling;self-attention","Production;Job shop scheduling;Metaheuristics;Costs;Dynamic scheduling;Reinforcement learning;Deep learning","","","","3","","","IEEE","27 Feb 2023","","","IEEE","IEEE Early Access Articles"
"Imitation and Adaptation Based on Consistency: A Quadruped Robot Imitates Animals from Videos Using Deep Reinforcement Learning","Q. Yao; J. Wang; S. Yang; C. Wang; H. Zhang; Q. Zhang; D. Wang","State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; School of Engineering, Westlake University, Hangzhou, China; School of Engineering, Westlake University, Hangzhou, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; School of Engineering, Westlake University, Hangzhou, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; School of Engineering, Westlake University, Hangzhou, China","2022 IEEE International Conference on Robotics and Biomimetics (ROBIO)","18 Jan 2023","2022","","","1414","1419","The essence of quadruped movements is the move-ment of the center of gravity, which has a pattern in the movement of quadrupeds. However, planning the gait motion of the quadruped robot is time consuming. Animals in nature can provide a large amount of gait information for robots to imitate skills. Common methods learn the posture of animals with a motion capture system or numerous motion data points. In this paper, we propose a video imitation adaptation network that can imitate the action of animals from a few seconds of video and adapt skills to the robot. The deep learning model extracts key points of animal motion from videos. A motion adaptor that eliminates noise and extracts key information of motion is proposed; the information of the extracted movements was used as a motion pattern to help deep reinforcement learning complete the imitation of skills. To ensure similarity between the learning result and the animal motion in the video, we introduce rewards that are based on the consistency of the motion. The results show that our framework can help robots learn periodic and aperiodic skills from several seconds of videos of different types of animals. And we deploy the trained model to the real robot to complete the walking and backflip tasks without fine-tuning.","","978-1-6654-8109-0","10.1109/ROBIO55434.2022.10011737","NSFC(grant numbers:62176215); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011737","","Deep learning;Legged locomotion;Adaptation models;Animals;Reinforcement learning;Motion capture;Planning","computer animation;gait analysis;learning (artificial intelligence);legged locomotion;motion control","animal motion;aperiodic skills;complete the imitation;deep learning model extracts key points;deep reinforcement learning;extracted movements;extracts key information;gait information;gait motion;imitate skills;learning result;motion adaptor;motion capture system;motion pattern;numerous motion data points;periodic skills;quadruped movements;quadruped robot imitates animals;video imitation adaptation network","","2","","27","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"Robotic Tracking Control with Kernel Trick-based Reinforcement Learning","Y. Hu; W. Wang; H. Liu; L. Liu","State Key Laboratory of Robotics, Shenyang Institute of Automation, Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, China; department of mathematics, Georgia Institute of Technology, Atlanta, GA, USA; State Key Laboratory of Robotics, Shenyang Institute of Automation, Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, China","2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","28 Jan 2020","2019","","","997","1002","In recent years, reinforcement learning has been developed dramatically and is widely used to solve control problems, e.g., playing games. However, there are still some problems for reinforcement learning to perform robotic control tasks. Fortunately, the kernel trick-based methods provide a chance to deal with those challenges. This work aims at developing a kernel trick-based learning control method to carry out robotic tracking control tasks. A reward system, in this work, is presented in order to speed up the learning processes. And then, a kernel trick-based reinforcement learning tracking controller is presented to perform tracking control tasks on a robotic manipulator system. To evaluate the policy and assist the reward system to accelerate the speed of finding the optimal control policy, a critic system is introduced. Finally, from the comparison with the benchmark, the simulation results illustrate that our algorithm has faster convergence rate and can execute tracking control tasks effectively, the reward function and the critic system proposed in this work is efficient.","2153-0866","978-1-7281-4004-9","10.1109/IROS40897.2019.8968574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968574","","","learning (artificial intelligence);learning systems;manipulators;optimal control;robot programming","robotic tracking control tasks;reward system;robotic manipulator system;optimal control;critic system;kernel trick-based reinforcement learning tracking controller;machine learning","","1","","31","IEEE","28 Jan 2020","","","IEEE","IEEE Conferences"
"(Re)2H2O: Autonomous Driving Scenario Generation via Reversely Regularized Hybrid Offline-and-Online Reinforcement Learning","H. Niu; K. Ren; Y. Xu; Z. Yang; Y. Lin; Y. Zhang; J. Hu","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","2023 IEEE Intelligent Vehicles Symposium (IV)","27 Jul 2023","2023","","","1","8","Autonomous driving and its widespread adoption have long held tremendous promise. Nevertheless, without a trustworthy and thorough testing procedure, not only does the industry struggle to mass-produce autonomous vehicles (AV), but neither the general public nor policymakers are convinced to accept the innovations. Generating safety-critical scenarios that present significant challenges to AV is an essential first step in testing. Real-world datasets include naturalistic but overly safe driving behaviors, whereas simulation would allow for unrestricted exploration of diverse and aggressive traffic scenarios. Conversely, higher-dimensional searching space in simulation disables efficient scenario generation without real-world data distribution as implicit constraints. In order to marry the benefits of both, it seems appealing to learn to generate scenarios from both offline real-world and online simulation data simultaneously. Therefore, we tailor a Reversely Regularized Hybrid Offline-and-Online ((Re)2H2O) Reinforcement Learning recipe to additionally penalize Q-values on real-world data and reward Q-values on simulated data, which ensures the generated scenarios are both varied and adversarial. Through extensive experiments, our solution proves to produce more risky scenarios than competitive baselines and it can generalize to work with various autonomous driving models. In addition, these generated scenarios are also corroborated to be capable of fine-tuning AV performance.","2642-7214","979-8-3503-4691-6","10.1109/IV55152.2023.10186559","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10186559","","Industries;Technological innovation;Reinforcement learning;Hybrid power systems;Behavioral sciences;Autonomous vehicles;Testing","mobile robots;reinforcement learning;road vehicles","aggressive traffic scenarios;autonomous driving scenario generation;AV;driving behaviors;H2O;higher-dimensional searching space;mass-produce autonomous vehicles;offline real-world;online simulation data;policymakers;Q-values;real-world data distribution;reversely regularized hybrid offline-and-online reinforcement learning;risky scenarios;safety-critical scenarios","","","","58","IEEE","27 Jul 2023","","","IEEE","IEEE Conferences"
"Dynamic Graph Dismantling with Reinforcement Supervised Learning","Y. Wang; C. Tang; H. -T. Zhang","the Engineering Research Center of Autonomous Intelligent Unmanned Systems, the Key Laboratory of Image Processing and Intelligent Control, and the State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; the Engineering Research Center of Autonomous Intelligent Unmanned Systems, the Key Laboratory of Image Processing and Intelligent Control, and the State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; the Engineering Research Center of Autonomous Intelligent Unmanned Systems, the Key Laboratory of Image Processing and Intelligent Control, and the State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","2023 42nd Chinese Control Conference (CCC)","18 Sep 2023","2023","","","1189","1194","There are always some “key” nodes in a big complex network, which can joint the most connected subgraphs. How to identify these nodes, finding a minimum set of nodes to attack for reducing the size of residual network's Largest Connected Component(LCC) to break up the original network, has become a research hotspot. Therefore, a method for determining the “key” nodes based on reinforcement learning framework and supervised learning model is proposed. This algorithm can not only utilize the dynamic exploration ability of reinforcement learning to collect a rich training dataset, but also take advantage of the characteristics that supervised learning is adaptive and has strong generalization ability to possess high efficiency and strong robustness. In order to further improve the algorithm's performance, $\epsilon$-greedy mechanism is used to explore more network states. The experiment results show that given the same fraction of removed nodes, our algorithm can make the residual LCC smaller in various networks which is superior to the state-of-the-art algorithms in terms of effectiveness and generalization.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10240999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10240999","key nodes;complex network;deep learning;network dismantling","Training;Adaptation models;Heuristic algorithms;Supervised learning;Complex networks;Reinforcement learning;Markov processes","complex networks;graph theory;learning (artificial intelligence);reinforcement learning;supervised learning","$\epsilon$-greedy mechanism;big complex network;connected subgraphs;dynamic exploration ability;dynamic graph dismantling;key nodes;minimum set;network states;reinforcement learning framework;reinforcement supervised learning;removed nodes;research hotspot;residual LCC smaller;residual network;rich training dataset;strong generalization ability;strong robustness;supervised learning model","","","","21","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Utilizing Multi-Agent Deep Reinforcement Learning For Flexible Job Shop Scheduling Under Sustainable Viewpoints","J. Popper; W. Motsch; A. David; T. Petzsche; M. Ruskowski","German Research Center for Artifical Intelligence GmbH, Kaiserslautern, Germany; Technologie-Initiative SmartFactory KL e.V., Kaiserslautern, Germany; German Research Center for Artifical Intelligence GmbH, Kaiserslautern, Germany; Technologie-Initiative SmartFactory KL e.V., Kaiserslautern, Germany; German Research Center for Artifical Intelligence GmbH, Kaiserslautern, Germany","2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)","10 Nov 2021","2021","","","1","6","Current trends place great demands on the flexibility and sustainability of modern production facilities. The optimisation of these Flexible Job Shop Scheduling Problems (FJSSP) under multiple objective variables, such as the makespan or the consumed energy, is a great challenge for today's planning systems due to the constantly changing constraints. In this paper, we present a method for multi-criteria dynamic planning of production facilities under both common and sustainable target variables, based on a Multi-Agent Reinforcement Learning (MARL) procedure. This is experimentally applied to a planning problem in a series of trials and compared with common methods. Finally, the results and further research questions are presented.","","978-1-6654-1262-9","10.1109/ICECCME52200.2021.9590925","European Commission(grant numbers:H2020 ICT-38); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9590925","Flexible Job Shop Scheduling;Deep Reinforcement Learning;Multi Agent Systems;Sustainability;Energy management","Job shop scheduling;Mechatronics;Production planning;Reinforcement learning;Linear programming;Production facilities;Planning","job shop scheduling;learning (artificial intelligence);multi-agent systems;optimisation;production facilities","common target variables;sustainable target variables;MultiAgent Reinforcement Learning procedure;planning problem;MultiAgent deep Reinforcement Learning;sustainable viewpoints;current trends place great demands;flexibility;sustainability;modern production facilities;Flexible Job Shop Scheduling Problems;multiple objective variables;planning systems;constantly changing constraints;multicriteria dynamic planning","","5","","38","IEEE","10 Nov 2021","","","IEEE","IEEE Conferences"
"Fuzzy genetic Network Programming with Reinforcement Learning for mobile robot navigation","S. Sendari; S. Mabu; K. Hirasawa","Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Fukuoka, Japan","2011 IEEE International Conference on Systems, Man, and Cybernetics","21 Nov 2011","2011","","","2243","2248","This paper proposes Fuzzy Genetic Network Programming with Reinforcement Learning (Fuzzy GNP-RL). This method integrates fuzzy logic to the conventional GNP-RL. The new part of the proposed method is fuzzy judgment nodes. Fuzzy GNP-RL provides flexibility to determine the appropriate next node by the probabilistic transition instead of that by the threshold values on GNP-RL. The simulation of the wall following behavior of a Khepera robot is used to evaluate the performance of Fuzzy GNP-RL compared with that of GNP-RL. The result shows that Fuzzy GNP-RL is more robust than GNP-RL.","1062-922X","978-1-4577-0653-0","10.1109/ICSMC.2011.6084011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6084011","Fuzzy logic;Genetic Network Programming;Reinforcement Learning;Robustness;Wall following behavior","Robot sensing systems;Wheels;Economic indicators;Mobile robots;Training;Learning","fuzzy logic;genetic algorithms;learning (artificial intelligence);mobile robots;path planning;probability;robust control","fuzzy genetic network programming;reinforcement learning;mobile robot navigation;fuzzy logic;fuzzy judgment node;fuzzy GNP-RL;probabilistic transition;threshold value;Khepera robot","","3","","15","IEEE","21 Nov 2011","","","IEEE","IEEE Conferences"
"Control of HVAC-Systems Using Reinforcement Learning With Hysteresis and Tolerance Control","C. Blad; C. S. Kallesøe; S. Bøgh","Dept. of Materials & Production, Aalborg University, Denmark; Dept. of Materials & Production, Aalborg University, Denmark; Dept. of Materials & Production, Aalborg University, Denmark","2020 IEEE/SICE International Symposium on System Integration (SII)","9 Mar 2020","2020","","","938","942","This paper presents the idea of using tolerance control in Deep Reinforcement Learning to improve robustness and reduce training time. This paper is a continuation of [1] where it is shown that Reinforcement Learning (RL) can be used to control an underfloor heating (UFH) system. However, it is seen in the study that the initial training time is too high and that the performance during training is not fulfilling the requirements to a UFH system. In this paper the fundamental challenge regarding control of UFH systems is explained, how RL can be beneficial for control of UFH systems, and how the implementation is done. Furthermore, results are presented with a standard hysteresis control, an RL control, and an RL control with tolerance control. These results show that the effect of tolerance control in these types of systems is significant. Finally, we discuss the challenges there are for a real-world implementation of RL-based control in UFH system.","2474-2325","978-1-7281-6667-4","10.1109/SII46433.2020.9026189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9026189","Deep Reinforcement Learning;Underfloor Heating;HVAC-systems;Tolerance Control","Hysteresis;Learning (artificial intelligence);Heating systems;Temperature;Valves;Buildings","control engineering computing;HVAC;learning (artificial intelligence);power engineering computing","HVAC-systems;reinforcement learning;tolerance control;deep reinforcement;underfloor heating system;initial training time;UFH system;standard hysteresis control;RL-based control","","1","","13","IEEE","9 Mar 2020","","","IEEE","IEEE Conferences"
"Training a Robotic Arm Movement with Deep Reinforcement Learning","X. Ni; X. He; T. Matsumaru","Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan","2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)","28 Mar 2022","2021","","","595","600","This paper introduces a general experimental design scheme for conditions and parameter settings of robotic arm control under the specific task when using Deep Deterministic Policy Gradient(DDPG) algorithm to train the robotic arm for completing the control task. Based on the Coppelia simulation tool, this paper builds an interactive reinforcement learning environment for robotic arm control tasks, and designs two different control tasks to verify the validity of experimental design schemes. Conclusions in this paper provide an important reference for finding suitable environmental design and parameter settings for using DDPG to train a manipulator and improving the training effect.","","978-1-6654-0535-5","10.1109/ROBIO54168.2021.9739340","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739340","","Training;Green design;Conferences;Biomimetics;Reinforcement learning;Manipulators;Task analysis","control engineering computing;deep learning (artificial intelligence);design engineering;manipulators;mobile robots;reinforcement learning","Coppelia simulation tool;DDPG algorithm;deep deterministic policy gradient algorithm;deep reinforcement learning;experimental design schemes;interactive reinforcement learning environment;robotic arm control tasks;robotic arm movement","","1","","11","IEEE","28 Mar 2022","","","IEEE","IEEE Conferences"
"Two-Stage Reinforcement Learning based on Genetic Network Programming for mobile robot","S. Sendari; S. Mabu; K. Hirasawa","Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan","2012 Proceedings of SICE Annual Conference (SICE)","4 Oct 2012","2012","","","95","100","This paper studies the adaptability of Two-Stage Reinforcement Learning based on Genetic Network Programming for a mobile robot to cope with sudden changes in the environments, i.e., sensors break suddenly in the implementation. Two-Stage Reinforcement Learning (TSRL) uses two kinds of learning, that is, (1) sub node selection proposed in the conventional Genetic Network Programming with Reinforcement Learning and (2) branch connection selection. As a result, when the sudden changes occur in the environments, the proposed method can determine the actions more appropriately.","","978-1-4673-2259-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6318415","","Wheels;Robot sensing systems;Economic indicators;Mobile robots;Learning","control engineering computing;genetic algorithms;intelligent robots;learning (artificial intelligence);mobile robots","two-stage reinforcement learning;genetic network programming;mobile robot;subnode selection;branch connection selection","","","","17","","4 Oct 2012","","","IEEE","IEEE Conferences"
"Hierarchical Reinforcement Learning for Robot Navigation using the Intelligent Space Concept","L. A. Jeni; Z. Istenes; P. Korondi; H. Hashimoto","Faculty of Informatics, Eōtvōs Loránd University, Budapest, Hungary; Faculty of Informatics, Eōtvōs Loránd University, Budapest, Hungary; Department of Automation and Applied Informatics, Budapest University of Technology and Economics, Budapest, Hungary; Institute of Industrial Science, University of Tokyo, Meguro, Tokyo, Japan","2007 11th International Conference on Intelligent Engineering Systems","8 Aug 2007","2007","","","149","153","Navigation in an unknown environment is a difficult task, because mobile robots need topological maps in order to operate in the environment. Another fundamental problem is that robot programming is a time-consuming process, so it is better to use a learning method with reinforcement. In previous work we proposed a learning framework, which used the capability of the Intelligent Space in order to build a topological map of the environment. In this paper we present an extension of this framework to decompose the learning problem into sub-problems, which can be learned faster.","1543-9259","1-4244-1147-5","10.1109/INES.2007.4283689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4283689","","Intelligent robots;Learning;Orbital robotics;Navigation;Mobile robots;Robotics and automation;Space technology;Informatics;State-space methods;Environmental economics","intelligent robots;learning (artificial intelligence);mobile robots;path planning;robot programming","hierarchical reinforcement learning;robot navigation;intelligent space concept;mobile robot;robot programming","","9","","18","IEEE","8 Aug 2007","","","IEEE","IEEE Conferences"
"Strategy Entropy as a Measure of Strategy Convergence in Reinforcement Learning","X. Zhuang; Z. Chen","Electronics & Engineering Department Automation Engineering College, Qingdao University of China, Qin Huang-dao, China; College of Information Science & Technology, Qingdao University of Science and Technology, Qin Huang-dao, China","2008 First International Conference on Intelligent Networks and Intelligent Systems","21 Nov 2008","2008","","","81","84","The concept of entropy is introduced into reinforcement learning. The definitions of the local and global strategy entropy are presented. The global strategy entropy is experimentally proved to be the quantitative problem-independent measure of the strategypsilas convergence degree. The experimental results show that the learning based on the local strategy entropy improves the learning performance.","","978-0-7695-3391-9","10.1109/ICINIS.2008.94","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4683173","Reinforcement Learning;Strategy Entropy;Strategy Convergence;Adaptive Learning Rate","Entropy;Convergence;Learning;Control systems;Intelligent networks;Educational institutions;Stochastic processes;Intelligent systems;Automation;Information science","entropy;learning (artificial intelligence)","global strategy entropy;strategy convergence;reinforcement learning;local strategy entropy;quantitative problem-independent measure","","4","","16","IEEE","21 Nov 2008","","","IEEE","IEEE Conferences"
"Inverse Reinforcement Learning for Adversarial Apprentice Games","B. Lian; W. Xue; F. L. Lewis; T. Chai","University of Texas at Arlington Research Institute, Fort Worth, TX, USA; State Key Laboratory of Synthetical Automation for Process Industries and the International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; University of Texas at Arlington Research Institute, Fort Worth, TX, USA; State Key Laboratory of Synthetical Automation for Process Industries and the International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China","IEEE Transactions on Neural Networks and Learning Systems","4 Aug 2023","2023","34","8","4596","4609","This article proposes new inverse reinforcement learning (RL) algorithms to solve our defined Adversarial Apprentice Games for nonlinear learner and expert systems. The games are solved by extracting the unknown cost function of an expert by a learner using demonstrated expert’s behaviors. We first develop a model-based inverse RL algorithm that consists of two learning stages: an optimal control learning and a second learning based on inverse optimal control. This algorithm also clarifies the relationships between inverse RL and inverse optimal control. Then, we propose a new model-free integral inverse RL algorithm to reconstruct the unknown expert cost function. The model-free algorithm only needs online demonstration of the expert and learner’s trajectory data without knowing system dynamics of either the learner or the expert. These two algorithms are further implemented using neural networks (NNs). In Adversarial Apprentice Games, the learner and the expert are allowed to suffer from different adversarial attacks in the learning process. A two-player zero-sum game is formulated for each of these two agents and is solved as a subproblem for the learner in inverse RL. Furthermore, it is shown that the cost functions that the learner learns to mimic the expert’s behavior are stabilizing and not unique. Finally, simulations and comparisons show the effectiveness and the superiority of the proposed algorithms.","2162-2388","","10.1109/TNNLS.2021.3114612","Office of Naval Research (ONR)(grant numbers:N00014-18-1-2221); Army Research Office (ARO)(grant numbers:W911NF-20-1-0132); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565156","Adversarial games;apprentice games;inverse optimal control;inverse reinforcement learning (RL);neural networks (NNs);optimal control","Games;Cost function;Optimal control;Heuristic algorithms;Costs;Artificial neural networks;System dynamics","expert systems;game theory;learning (artificial intelligence);optimal control;reinforcement learning","cost functions;defined Adversarial Apprentice Games;demonstrated expert;different adversarial attacks;expert systems;inverse optimal control;inverse reinforcement learning algorithms;learning process;learning stages;model-based inverse RL algorithm;model-free algorithm;model-free integral inverse RL algorithm;nonlinear learner;optimal control learning;two-player zero-sum game;unknown cost function;unknown expert cost function","","10","","45","IEEE","8 Oct 2021","","","IEEE","IEEE Journals"
"Planning Irregular Object Packing via Hierarchical Reinforcement Learning","S. Huang; Z. Wang; J. Zhou; J. Lu","Beijing National Research Center for Information Science and Technology (BNRist), the Department of Automation, Tsinghua University, Beijing, China; Beijing National Research Center for Information Science and Technology (BNRist), the Department of Automation, Tsinghua University, Beijing, China; Beijing National Research Center for Information Science and Technology (BNRist), the Department of Automation, Tsinghua University, Beijing, China; Beijing National Research Center for Information Science and Technology (BNRist), the Department of Automation, Tsinghua University, Beijing, China","IEEE Robotics and Automation Letters","24 Nov 2022","2023","8","1","81","88","Object packing by autonomous robots is an important challenge in warehouses and logistics industry. Most conventional data-driven packing planning approaches focus on regular cuboid packing, which are usually heuristic and limit the practical use in realistic applications with everyday objects. In this paper, we propose a deep hierarchical reinforcement learning approach to simultaneously plan packing sequence and placement for irregular object packing. Specifically, the top manager network infers packing sequence from six principal view heightmaps of all objects, and then the bottom worker network receives heightmaps of the next object to predict the placement position and orientation. The two networks are trained hierarchically in a self-supervised Q-Learning framework, where the rewards are provided by the packing results based on the top height, object volume and placement stability in the box. The framework repeats sequence and placement planning iteratively until all objects have been packed into the box or no space is remained for unpacked items. We compare our approach with existing robotic packing methods for irregular objects in a physics simulator. Experiments show that our approach can pack more objects with less time cost than the state-of-the-art packing methods of irregular objects. We also implement our packing plan with a robotic manipulator to show the generalization ability in the real world.","2377-3766","","10.1109/LRA.2022.3222996","National Natural Science Foundation of China(grant numbers:62125603,U1813218); Beijing Academy of Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954127","Manipulation planning;reinforcement learning;robotic packing","Planning;Reinforcement learning;Search problems;Robots;Visualization;Manipulators;Pipelines","bin packing;learning (artificial intelligence);logistics;manipulators","autonomous robots;bottom worker network;conventional data-driven packing planning;deep hierarchical reinforcement learning approach;irregular objects;logistics industry;manager network;object volume;packing plan;packing results;placement planning;placement position;placement stability;planning irregular object packing;principal view heightmaps;regular cuboid packing;robotic packing methods;self-supervised Q-Learning framework;state-of-the-art packing methods;warehouses","","4","","44","IEEE","17 Nov 2022","","","IEEE","IEEE Journals"
"Solving the Zero-Sum Control Problem for Tidal Turbine System: An Online Reinforcement Learning Approach","H. Fang; M. Zhang; S. He; X. Luan; F. Liu; Z. Ding","School of Electrical Engineering and Automation, Anhui Engineering Laboratory of Human-Robot Integration System and Intelligent Equipment, Anhui University, Hefei, China; School of Electrical Engineering and Automation, Anhui Engineering Laboratory of Human-Robot Integration System and Intelligent Equipment, Anhui University, Hefei, China; School of Electrical Engineering and Automation, Anhui Engineering Laboratory of Human-Robot Integration System and Intelligent Equipment, Anhui University, Hefei, China; Key Laboratory of Advanced Process Control for Light Industry (Ministry of Education), Jiangnan University, Wuxi, China; Key Laboratory of Advanced Process Control for Light Industry (Ministry of Education), Jiangnan University, Wuxi, China; Department of Electrical and Electronic Engineering, The University of Manchester, Manchester, U.K.","IEEE Transactions on Cybernetics","","2022","PP","99","1","13","A novel completely mode-free integral reinforcement learning (CMFIRL)-based iteration algorithm is proposed in this article to compute the two-player zero-sum games and the Nash equilibrium problems, that is, the optimal control policy pairs, for tidal turbine system based on continuous-time Markov jump linear model with exact transition probability and completely unknown dynamics. First, the tidal turbine system is modeled into Markov jump linear systems, followed by a designed subsystem transformation technique to decouple the jumping modes. Then, a completely mode-free reinforcement learning algorithm is employed to address the game-coupled algebraic Riccati equations without using the information of the system dynamics, in order to reach the Nash equilibrium. The learning algorithm includes one iteration loop by updating the control policy and the disturbance policy simultaneously. Also, the exploration signal is added for motivating the system, and the convergence of the CMFIRL iteration algorithm is rigorously proved. Finally, a simulation example is given to illustrate the effectiveness and applicability of the control design approach.","2168-2275","","10.1109/TCYB.2022.3186886","National Natural Science Foundation of China(grant numbers:62073001); Anhui Provincial Key Research and Development Project(grant numbers:2022i01020013); State Key Program of National Natural Science Foundation of China(grant numbers:61833007); University Synergy Innovation Program of Anhui Province(grant numbers:GXXT-2021-010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831018","Game-coupled algebraic Riccati equations;integral reinforcement learning;Markov jump linear systems (MJLSs);tidal turbine;zero-sum games","Turbines;Reinforcement learning;Markov processes;Rotors;Optimal control;Games;Mathematical models","","","","3","","","IEEE","15 Jul 2022","","","IEEE","IEEE Early Access Articles"
"Event-Triggered Deep Reinforcement Learning Using Parallel Control: A Case Study in Autonomous Driving","J. Lu; L. Han; Q. Wei; X. Wang; X. Dai; F. -Y. Wang","Parallel Intelligence Innovation Research Center, Qingdao Academy of Intelligent Industries, Qingdao, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, Anhui University, Hefei, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Intelligent Vehicles","22 May 2023","2023","8","4","2821","2831","This paper utilizes parallel control to investigate the problem of event-triggered deep reinforcement learning and develops an event-triggered deep Q-network (ETDQN) for decision-making of autonomous driving, without training an explicit triggering condition. Based on the framework of parallel control, the developed ETDQN incorporates information of actions into the feedback and constructs a dynamic control policy. First, in the realization of the dynamic control policy, we integrate the current state and the previous action to construct the augmented state as well as the augmented Markov decision process. Meanwhile, it is shown theoretically that the goal of the developed dynamic control policy is to learn the variation rate of the action. The augmented state contains information of the current state and the previous action, which enables the developed ETDQN to directly design the immediate reward considering communication loss. Then, based on dueling double deep Q-network (dueling DDQN), we establish the augmented action-value, value, and advantage functions to directly learn the optimal event-triggered decision-making policy of autonomous driving without an explicit triggering condition. It is worth noticing that the developed ETDQN applies to various deep Q-networks (DQNs). Empirical results demonstrate that, in event-triggered control, the developed ETDQN outperforms dueling DDQN and reduces communication loss effectively.","2379-8904","","10.1109/TIV.2023.3262132","Key Research and Development Program 2020 of Guangzhou(grant numbers:202007050002); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B090921003); National Natural Science Foundation of China(grant numbers:U1811463); Motion G, Inc. Collaborative Research Project for Modeling, Decision and Control Algorithms of Servo Drive Systems; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10081497","Autonomous driving;deep reinforcement learning;deep Q-network;event-triggered control;parallel control","Autonomous vehicles;Decision making;Path planning;Training;Optimal control;Deep learning;Complex systems","decision making;deep learning (artificial intelligence);learning (artificial intelligence);Markov processes;reinforcement learning;telecommunication computing","augmented action-value;augmented Markov decision process;augmented state;autonomous driving driving,without;deep Q-networks;developed dynamic control policy;developed ETDQN;dueling double deep Q-network;event-triggered control;event-triggered deep Q-network;event-triggered deep reinforcement learning;explicit triggering condition;optimal event-triggered decision-making;parallel control;previous action","","1","","44","IEEE","27 Mar 2023","","","IEEE","IEEE Journals"
"A Deep Reinforcement Learning Based Energy Management Strategy for Fuel-Cell Electric UAV","Q. Gao; T. Lei; F. Deng; Z. Min; W. Yao; X. Zhang","Electrical Engineering Department, Automation School, Northwestern Polytechnical University, Xi’An, China; Electrical Engineering Department, Automation School, Northwestern Polytechnical University, Xi’An, China; Electrical Engineering Department, Automation School, Northwestern Polytechnical University, Xi’An, China; Electrical Engineering Department, Automation School, Northwestern Polytechnical University, Xi’An, China; Key Laboratory of Aircraft Electric Propulsion Technology, Ministry of Industry and Information Technology of China, Xi’An, China; Key Laboratory of Aircraft Electric Propulsion Technology, Ministry of Industry and Information Technology of China, Xi’An, China","2022 International Conference on Power Energy Systems and Applications (ICoPESA)","14 Apr 2022","2022","","","524","530","Electric propulsion UAV powered by hybrid power system consisting of fuel cells and lithium batteries have attracted significant attention for long endurance and zero emission. Different dynamic characteristics for variable power load demanding which can be stochastically affected by the UAV’s flight air dynamic disturbance are difficult to be modeled with energy management system (EMS). In this paper, a Deep Reinforcement Learning (DRL) algorithm, namely twin-delayed Deep Deterministic policy gradient (TD3), is adopted to derivate EMS for hybrid electric UAV which can avoid performance degradation from uncertainty of power system model and curse of dimensionality of traditional algorithm. The simulation results indicate that the TD3-based DRL strategy is able to coordinate multiple electric power sources based on their natural power characteristics, satisfy different flight profiles of UAV. Furthermore, the performances of TD3, Deep Q-Networks (DQN), Deep Deterministic policy gradient (DDPG) and Dynamic Programming (DP) algorithms with different parameters in EMS of hybrid electric UAV were compared and the effectiveness of the algorithm was verified by digital simulation. Comparative results also illustrate that the proposed TD3 method outperforms other two methods in solving multi-objective optimization energy management problem, in terms of hydrogen consumptions, system efficiency and battery’s state of charge (SOC) sustainability.","","978-1-6654-1097-7","10.1109/ICoPESA54515.2022.9754414","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9754414","Energy Management;Fuel-cell;Hybrid Electric UAV;Deep Reinforcement Learning","Training;Heuristic algorithms;Power system dynamics;Hydrogen;Reinforcement learning;Propulsion;Prediction algorithms","aerospace control;autonomous aerial vehicles;computer simulation;dynamic programming;electric propulsion;energy management systems;fuel cell vehicles;gradient methods;hybrid electric vehicles;hybrid power systems;hydrogen;lithium compounds;power engineering computing;primary cells;reinforcement learning","deep reinforcement learning based energy management strategy;fuel-cell electric UAV;electric propulsion;hybrid power system;lithium batteries;variable power load;EMS;deep deterministic policy gradient;hybrid electric UAV;DRL strategy;multiple electric power sources;natural power characteristics;dynamic programming algorithms;multiobjective optimization energy management problem;UAV flight air;deep Q-networks;DQN;DDPG;digital simulation;hydrogen consumptions;battery state of charge sustainability;SOC","","1","","20","IEEE","14 Apr 2022","","","IEEE","IEEE Conferences"
"Inverse Reinforcement Learning for Multi-player Apprentice Games in Continuous-Time Nonlinear Systems","B. Lian; W. Xue; F. L. Lewis; T. Chai; A. Davoudi","The University of Texas at Arlington, Arlington, TX, USA; The State Key Laboratory of Synthetical Automation for Process Industries and International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; The University of Texas at Arlington, Arlington, TX, USA; The State Key Laboratory of Synthetical Automation for Process Industries and International Joint Research Laboratory of Integrated Automation, Northeastern University, Shenyang, China; The University of Texas at Arlington, Arlington, TX, USA","2021 60th IEEE Conference on Decision and Control (CDC)","1 Feb 2022","2021","","","803","808","We extend the inverse reinforcement learning (inverse RL) algorithms to multi-player apprentice games described by nonlinear differential equations. In these games, both the expert and the learner have N-player control inputs. Inverse RL algorithms solve the games by learner reconstructing the unknown cost function of each expert player using the demonstration of expert’s behavior (states and control inputs of each player), thereby mimicking the given behaviors. We first develop a model-based inverse RL algorithm with two learning stages: an optimal control learning stage and an inverse optimal control learning stage. Then, a model-free off-policy integral inverse RL algorithm is developed by using online expert’s demonstrations and learner’s behavior trajectories without knowing system dynamics of either expert or the learner. Finally, simulations verify the effectiveness of proposed algorithms.","2576-2370","978-1-6654-3659-5","10.1109/CDC45484.2021.9682909","Office of Naval Research; Army Research Office; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9682909","","System dynamics;Heuristic algorithms;Optimal control;Games;Reinforcement learning;Cost function;Numerical simulation","continuous time systems;learning (artificial intelligence);nonlinear control systems;nonlinear differential equations;optimal control","multiplayer apprentice games;continuous-time nonlinear systems;inverse reinforcement learning algorithms;nonlinear differential equations;N-player control inputs;inverse RL algorithms;expert player;model-based inverse RL algorithm;learning stages;optimal control learning stage;inverse optimal control;model-free off-policy integral inverse RL","","","","34","IEEE","1 Feb 2022","","","IEEE","IEEE Conferences"
"The AGV Battery Swapping Policy Based on Reinforcement Learning","M. S. Lee; Y. Jae Jang","Department of Industrial and Systems Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Department of Industrial and Systems Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","1479","1484","The automated guided vehicle (AGV), a typical form of automated material handling system, generally utilizes electric power from an internally mounted battery pack. AGVs need to occasionally visit a battery station and swap the battery to manage their state of charge. An AGV system therefore needs a swapping policy, which determines when a vehicle should proceed to a battery station for battery replacement. In real industrial practice, most swapping policies are conservative and are based heuristically on the experiences of decision makers, which results in production inefficiency. The objective of this research is to develop a swapping strategy to improve the AGV system production efficiency. The proposed swapping policy is based on sequential decisions that consider current and future situations, and utilizes a Markov decision process framework and deep reinforcement learning. We present the results of numerical experiments to demonstrate the superior performance of the proposed swapping policy compared with heuristic policies. We also analyze the properties of the proposed swapping policy, and the results demonstrate its application potential for AGV systems.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926504","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926504","","Industries;Remotely guided vehicles;Materials handling;Reinforcement learning;Production;Markov processes;Batteries","automatic guided vehicles;battery powered vehicles;control engineering computing;decision making;deep learning (artificial intelligence);Markov processes;reinforcement learning","AGV battery swapping policy;automated guided vehicle;automated material handling system;internally mounted battery pack;battery station;battery replacement;AGV system production efficiency;heuristic policies;reinforcement learning;Markov decision process framework;deep reinforcement learning","","","","15","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"Towards Safe Control of Continuum Manipulator Using Shielded Multiagent Reinforcement Learning","G. Ji; J. Yan; J. Du; W. Yan; J. Chen; Y. Lu; J. Rojas; S. S. Cheng","CUHK T Stone Robotics Institute and Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong; CUHK T Stone Robotics Institute and Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong; CUHK T Stone Robotics Institute and Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong; CUHK T Stone Robotics Institute and Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong; CUHK T Stone Robotics Institute and Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong; CUHK T Stone Robotics Institute and Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong; CUHK T Stone Robotics Institute and Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong; CUHK T Stone Robotics Institute and Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Robotics and Automation Letters","3 Aug 2021","2021","6","4","7461","7468","Continuum robotic manipulators are increasingly adopted in minimal invasive surgery. However, their nonlinear behavior is challenging to model accurately, especially when subject to external interaction, potentially leading to poor control performance. In this letter, we investigate the feasibility of adopting a model-free multiagent reinforcement learning (RL), namely multiagent deep Q network (MADQN), to control a 2-degree of freedom (DoF) cable-driven continuum surgical manipulator. The control of the robot is formulated as a one DoF, one agent problem in the MADQN framework to improve the learning efficiency. Combined with a shielding scheme that enables dynamic variation of the action set boundary, MADQN leads to efficient and importantly safer control of the robot. Shielded MADQN enabled the robot to perform point and trajectory tracking with submillimeter root mean square errors under external loads, soft obstacles, and rigid collision, which are common interaction scenarios encountered by surgical manipulators. The controller was further proven to be effective in a miniature continuum robot with high structural nonlinearitiy, achieving trajectory tracking with submillimeter accuracy under external payload.","2377-3766","","10.1109/LRA.2021.3097660","Chinese University of Hong Kong; CUHK T Stone Robotics Institute(grant numbers:4930807); Innovation and Technology Commission(grant numbers:ITS/389/18,ITS/226/19,ITS/136/20); Research Grants Council(grant numbers:24201219,BME-p7-20); Shun Hing Institute of Advanced Engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9488207","Reinforcement learning;modeling;control;and learning for soft robots;model learning for control;robust/adaptive control;medical robots and systems","Robots;Collision avoidance;Robot kinematics;Reinforcement learning;Real-time systems;Medical robotics;Manipulator dynamics","end effectors;learning (artificial intelligence);manipulator dynamics;mean square error methods;medical robotics;multi-agent systems;surgery;trajectory control","model-free multiagent reinforcement learning;multiagent deep Q network;learning efficiency;safer control;shielded MADQN;trajectory tracking;submillimeter root mean square errors;miniature continuum robot;external payload;shielded multiagent reinforcement learning;continuum robotic manipulators;minimal invasive surgery;nonlinear behavior;external interaction;2-degree of freedom cable-driven continuum surgical manipulator;one agent problem;external loads;soft obstacles;rigid collision","","17","","30","IEEE","16 Jul 2021","","","IEEE","IEEE Journals"
"Notice of Violation of IEEE Publication Principles: Multiobjective Reinforcement Learning: A Comprehensive Overview","C. Liu; X. Xu; D. Hu","College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; Institute of Unmanned Systems, College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; Department of Automatic Control, College of Mechatronics and Automation, National University of Defense Technology, Changsha, China","IEEE Transactions on Cybernetics","","2013","PP","99","1","13","Reinforcement learning is a powerful mechanism for enabling agents to learn in an unknown environment, and most reinforcement learning algorithms aim to maximize some numerical value, which represents only one long-term objective. However, multiple long-term objectives are exhibited in many real-world decision and control problems; therefore, recently, there has been growing interest in solving multiobjective reinforcement learning (MORL) problems with multiple conflicting objectives. The aim of this paper is to present a comprehensive overview of MORL. The basic architecture, research topics, and naive solutions of MORL are introduced at first. Then, several representative MORL approaches and some important directions of recent research are reviewed. The relationships between MORL and other related research are also discussed, which include multiobjective optimization, hierarchical reinforcement learning, and multi-agent reinforcement learning. Finally, research challenges and open problems of MORL techniques are highlighted.;Notice of Violation of IEEE Publication Principles<br><br>""Multiobjective Reinforcement Learning: A Comprehensive Overview Authors""<br>by Chunming Liu, Xin Xu, and Dewen Hu<br>Submitted to IEEE Transactions on Cybernetics in May 2012<br><br>After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this submission has been found to not be in compliance with IEEE's Publication Principles.<br><br>This submission contains portions of original text from the papers cited below. While the authors reference the original papers, they did not sufficiently delineate the reused text from their own work and subsequently the submission was rejected from publication.<br><br>""Empirical Evaluation Methods for Multiobjective Reinforcement Learning Algorithms"",<br>by Peter Vamplew, Richard Dazeley, Adam Berry, Rustam Issabekov, and Evan Dekker<br>in Machine Learning (2011) 84, pp. 51-80.<br><br>""On the Limitations of Scalarisation for Multi-Objective Reinforcement Learning of Pareto Fronts""<br>by Peter Vamplew, John Yearwood, Richard Dazeley, and Adam Berry<br>in Lecture Notes in Computer Science, vol. 5360, 2008, pp. 372-378.<br><br>""Multiple-goal Reinforcement Learning with Modular Sarsa(0).""<br>by Nathan Sprague and Dana Ballard<br> in Proceedings of the International Joint Conference on Artificial Intelligence, vol. 18.<br> Lawrence Erlbaum Associates Ltd., Technical Report 798, 2004.<br><br>A revised version of this submission was published in the IEEE Transactions on Systems, Man, and Cybernetics: Systems, Vol. 45, Issue 3, pp. 385-398<br><br>http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6918520","2168-2275","","10.1109/TSMCC.2013.2249512","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509978","","","","","","9","1","","IEEE","29 Apr 2013","","","IEEE","IEEE Early Access Articles"
"Modeling and Optimization of Paper-making Wastewater Treatment Based on Reinforcement Learning","Z. Zhuang; Z. Sun; Y. Cheng; R. Yao; W. Zhang","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2018 37th Chinese Control Conference (CCC)","7 Oct 2018","2018","","","8342","8346","Environmental disturbances and system uncertainties are the main obstacles for wastewater treatment process of paper-making process. In this paper, a neural network based on on LSTM which is a neural network that can learn long-term dependencies is constructed to simulate the environment of the above process. A deep reinforcement learning method is then proposed to optimize the wastewater treatment process. With the proposed design, the control scheme could not only obtain a good performance of the control system, but also can enhance the robustness of the closed-loop system. Numerical simulations are given to demonstrate the effectiveness of the proposed method.","1934-1768","978-988-15639-5-8","10.23919/ChiCC.2018.8482733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8482733","Deep Q network;Markov process;Neural network;Paper-making;Reinforcement learning;Wastewater","Wastewater treatment;Neural networks;Correlation;Learning (artificial intelligence);Wastewater;Optimization;Machine learning","chemical variables control;closed loop systems;industrial waste;learning (artificial intelligence);neurocontrollers;paper making;recurrent neural nets;wastewater treatment;water pollution control","closed-loop system;paper-making wastewater treatment;system uncertainties;wastewater treatment process;paper-making process;neural network;deep reinforcement learning method;LSTM;robustness;coagulant dosage control system","","7","","17","","7 Oct 2018","","","IEEE","IEEE Conferences"
