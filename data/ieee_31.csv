"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Reinforcement Learning in Quasi-Continuous Time","P. Wawrzynski; A. Pacut","Institute of Control and Computation Engineering, Warsaw University of Technology, Warsaw, Poland; Institute of Control and Computation Engineering, Warsaw University of Technology, Warsaw, Poland","International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)","22 May 2006","2005","2","","1031","1036","Reinforcement Learning (RL) is used here as a tool for control systems optimization. State and action spaces are assumed to be continuous. Time is assumed to be discrete, yet the discretization may be arbitrarily fine. Within the proposed algorithm, a piece of information that leads to a policy improvement, is inferred from an experiment that lasts for several consecutive steps, rather than from a single step, as in more traditional RL methods. Simulations reveal that the algorithm is able to optimize the control policies for plants for which it is very difficult to apply the traditional methods.","","0-7695-2504-0","10.1109/CIMCA.2005.1631605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631605","Machine Learning;Reinforcement Learning;Adaptive Control.","Optimization methods;Process control;Control systems;Space technology;Adaptive control;Intelligent agent;Information resources;Control engineering computing;Machine learning algorithms;Machine learning","adaptive control;continuous time systems;control system analysis;learning (artificial intelligence);learning systems;optimal control","Machine Learning;Reinforcement Learning;Adaptive Control.","","","","13","IEEE","22 May 2006","","","IEEE","IEEE Conferences"
"Trade-Off Between Robustness and Rewards Adversarial Training for Deep Reinforcement Learning Under Large Perturbations","J. Huang; H. J. Choi; N. Figueroa","Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, USA; Department of Mechanical Engineering and Applied Mechanics, University of Pennsylvania, Philadelphia, PA, USA; Department of Mechanical Engineering and Applied Mechanics, University of Pennsylvania, Philadelphia, PA, USA","IEEE Robotics and Automation Letters","","2023","PP","99","1","8","Deep Reinforcement Learning (DRL) has become a popular approach for training robots due to its generalization promise, complex task capacity and minimal human intervention. Nevertheless, DRL-trained controllers are vulnerable to even the smallest of perturbations on its inputs which can lead to catastrophic failures in real-world human-centric environments with large and unexpected perturbations. In this work, we study the vulnerability of state-of-the-art DRL subject to large perturbations and propose a novel adversarial training framework for robust control. Our approach generates aggressive attacks on the state space and the expected state-action values to emulate real-world perturbations such as sensor noise, perception failures, physical perturbations, observations mismatch, etc. To achieve this, we reformulate the adversarial risk to yield a trade-off between rewards and robustness (TBRR). We show that TBRR-aided DRL training is robust to aggressive attacks and outperforms baselines on standard DRL benchmarks (Cartpole, Pendulum), Meta-World tasks (door manipulation) and a vision-based grasping task with a 7DoF manipulator. Finally, we show that the vision-based grasping task trained in simulation via TBRR transfers sim2real with 70% success rate subject to sensor impairment and physical perturbations without any retraining.","2377-3766","","10.1109/LRA.2023.3324590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10284990","","Perturbation methods;Training;Robustness;Task analysis;Robots;Robot sensing systems;Standards","","","","","","","IEEE","13 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Sim2Real Transfer of Reinforcement Learning for Concentric Tube Robots","K. Iyengar; S. M. H. Sadati; C. Bergeles; S. Spurgeon; D. Stoyanov","Wellcome/ EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, U.K.; School of Biomedical Engineering and Imaging Sciences, King's College London, London, U.K.; School of Biomedical Engineering and Imaging Sciences, King's College London, London, U.K.; Department of Electronic and Electrical Engineering, University College London, London, U.K.; Wellcome/ EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, U.K.","IEEE Robotics and Automation Letters","18 Aug 2023","2023","8","10","6147","6154","Concentric Tube Robots (CTRs) are promising for minimally invasive interventions due to their miniature diameter, high dexterity, and compliance with soft tissue. CTRs comprise individual pre-curved tubes usually composed of NiTi and are arranged concentrically. As each tube is relatively rotated and translated, the backbone elongates, twists, and bends with a dexterity that is advantageous for confined spaces. Tube interactions, unmodelled phenomena, and inaccurate tube parameter estimation make physical modeling of CTRs challenging, complicating in turn kinematics and control. Deep reinforcement learning (RL) has been investigated as a solution. However, hardware validation has remained a challenge due to differences between the simulation and hardware domains. With simulation-only data, in this work, domain randomization is proposed as a strategy for translation to hardware of a simulation policy with no additionally acquired physical training data. The differences in simulation and hardware forward kinematics accuracy and precision are characterized by errors of $14.74 \pm 8.87$ mm or $26.61 \pm 17.00$% robot length. We showcase that the proposed domain randomization approach reduces errors by 56% in mean errors as compared to no domain randomization. Furthermore, we demonstrate path following capability in hardware with a line path with resulting errors of $4.37 \pm 2.39$ mm or $5.61 \pm 3.11$% robot length.","2377-3766","","10.1109/LRA.2023.3303714","Wellcome / EPSRC Centre for Interventional and Surgical Sciences(grant numbers:203145Z/16/Z); Engineering and Physical Sciences Research Council(grant numbers:EP/P027938/1,EP/R004080/1); Wellcome/EPSRC Centre for Medical Engineering at KCL(grant numbers:WT 203148/Z/16/Z); ERC Starting(grant numbers:714562); NIHR Cardiovascular MIC Grant; Royal Academy of Engineering Chair in Emerging Technologies and an EPSRC Early Career Research Fellowship(grant numbers:EP/P012841/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214097","Surgical robotics;robot control;reinforcement learning;concentric tube robots","Hardware;Robots;Electron tubes;Kinematics;Deep learning;Visualization;Training","deep learning (artificial intelligence);medical robotics;parameter estimation;pipes;reinforcement learning;robot kinematics;surgery","concentric tube robots;CTR;deep reinforcement learning;domain randomization approach;hardware domains;hardware validation;inaccurate tube parameter estimation;minimally invasive interventions;physical training data;precurved tubes;robot length;sim2real transfer;Sim2Real transfer;simulation-only data;tube interactions;turn kinematics","","","","28","IEEE","9 Aug 2023","","","IEEE","IEEE Journals"
"Distilling deep neural networks with reinforcement learning","Y. Huang; Y. Yu","College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China","2018 IEEE International Conference on Information and Automation (ICIA)","26 Aug 2019","2018","","","133","138","Deep architecture can improve performance of neural networks whereas it increases the computational complexity. Compressing networks is the key to solve this problem. The framework Knowledge Distilling (KD) compresses cumbersome networks well. It improved mimic learning, enabling knowledge to be transferred from cumbersome networks to compressed networks without constraint of architectures. Inspired by AlphaGo Zero, this paper proposed an algorithm combining KD with reinforcement learning to compress networks on changing datasets. In this algorithm, the compressed networks interact with the environment made by KD to produce datasets that are appropriate w.r.t the model. Monte Carlo Tree Search (MCTS) of AlphaGo Zero is used to produce the datasets by making a trade-off between the prediction of compressed networks and the knowledge. In experiments, the algorithm proved to be effective in compressing networks by training ResNet on CIFAR datasets, with mean squared error as the object function.","","978-1-5386-8069-8","10.1109/ICInfA.2018.8812321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812321","Compressing neural networks;Knowledge Distilling;Policy Iteration;Monte Carlo Tree Search","Training;Feature extraction;Task analysis;Computer architecture;Data models;Reinforcement learning;Knowledge engineering","computational complexity;learning (artificial intelligence);Monte Carlo methods;neural nets;tree searching","deep neural networks;reinforcement learning;compressed networks;deep architecture;computational complexity;knowledge distilling;mimic learning;AlphaGo Zero;Monte Carlo tree search","","","","12","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"Intelligent Control Strategy of Vehicle Active Suspension Based on Deep Reinforcement Learning","X. Zhu; Z. Chen; S. Zhang; C. Zhang","Science and Technology Committee, Beijing Institute of Space Launch Technology, Beijing, China; Technology R&D Center of eDrive Special Vehicles, Beijing Institute of Space Launch Technology, Beijing, China; Technology R&D Center of eDrive Special Vehicles, Beijing Institute of Space Launch Technology, Beijing, China; Electrical and Computer Department, Pratt School of Engineering, Duke University, Durham, United States","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","4871","4876","Aiming at the uncertainty and nonlinearity of vehicle active suspension systems, this paper proposes a suspension vibration control strategy based on the twin delayed deep deterministic policy gradient (TD3) algorithm. A quarter-vehicle active suspension model and the time domain model of road excitation are established to develop the controller. Simulation results of vibration characteristics show that the intelligent control strategy has excellent performance and self-learning ability. Compared with the linear quadratic gaussian, proportional-integral-derivative, and sky-hook strategy, the deep reinforcement learning controller has better robustness and generalization ability, which can adapt to sophisticated driving conditions of the vehicle, and has an extensive application prospect in the development of the active control arithmetic for suspension.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10054782","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054782","Deep reinforcement learning;TD3;Vehicle active suspension;Intelligent control","Vibrations;Deep learning;Suspensions (mechanical systems);Uncertainty;Simulation;Roads;Reinforcement learning","control engineering computing;deep learning (artificial intelligence);gradient methods;intelligent control;reinforcement learning;road vehicles;suspensions (mechanical components);vehicle dynamics;vibration control;vibrations","active control arithmetic;deep deterministic policy gradient algorithm;deep reinforcement learning controller;intelligent control strategy;nonlinearity;proportional-integral;quarter-vehicle active suspension model;self-learning ability;sky-hook strategy;suspension vibration control strategy;time domain model;uncertainty;vehicle active suspension systems;vibration characteristics","","","","16","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Spatial-Temporal-Aware Safe Multi-Agent Reinforcement Learning of Connected Autonomous Vehicles in Challenging Scenarios","Z. Zhang; S. Han; J. Wang; F. Miao","Department of Computer Science and Engineering; Department of Computer Science and Engineering; Department of Electrical and Computer Engineering, University of Connecticut, Storrs Mansfield, CT, USA; Department of Computer Science and Engineering","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5574","5580","Communication technologies enable coordination among connected and autonomous vehicles (CAVs). However, it remains unclear how to utilize shared information to improve the safety and efficiency of the CAV system in dynamic and complicated driving scenarios. In this work, we propose a framework of constrained multi-agent reinforcement learning (MARL) with a parallel Safety Shield for CAVs in challenging driving scenarios that includes unconnected hazard vehicles. The coordination mechanisms of the proposed MARL include information sharing and cooperative policy learning, with Graph Convolutional Network (GCN)-Transformer as a spatial-temporal encoder that enhances the agent's environment awareness. The Safety Shield module with Control Barrier Functions (CBF)-based safety checking protects the agents from taking unsafe actions. We design a constrained multi-agent advantage actor-critic (CMAA2C) algorithm to train safe and cooperative policies for CAVs. With the experiment deployed in the CARLA simulator, we verify the performance of the safety checking, spatial-temporal encoder, and coordination mechanisms designed in our method by comparative experiments in several challenging scenarios with unconnected hazard vehicles. Results show that our proposed methodology significantly increases system safety and efficiency in challenging scenarios.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161216","NSF(grant numbers:1849246); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161216","","Connected vehicles;Information sharing;Reinforcement learning;Hazards;Robustness;Communications technology;Noise measurement","convolutional neural nets;deep learning (artificial intelligence);graph theory;learning (artificial intelligence);mobile robots;multi-agent systems;reinforcement learning;road vehicles;telecommunication computing;traffic engineering computing","agent;CAV system;communication technologies;complicated driving scenarios;connected autonomous vehicles;connected vehicles;constrained multiagent reinforcement learning;Control Barrier Functions-based safety checking;cooperative policies;cooperative policy learning;coordination mechanisms;dynamic scenarios;Graph Convolutional Network-Transformer;information sharing;MARL;multiagent advantage actor-critic;parallel Safety Shield;safe policies;Safety Shield module;spatial-temporal encoder;spatial-temporal-aware safe multiagent reinforcement learning;system safety;unconnected hazard vehicles","","","","37","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Efficient Multi-Task and Transfer Reinforcement Learning With Parameter-Compositional Framework","L. Sun; H. Zhang; W. Xu; M. Tomizuka","School of Mechanical Engineering, University of California, Berkeley, Albany, CA, USA; Horizon Robotics, Cupertino, CA, USA; Horizon Robotics, Cupertino, CA, USA; School of Mechanical Engineering, University of California, Berkeley, Albany, CA, USA","IEEE Robotics and Automation Letters","21 Jun 2023","2023","8","8","4569","4576","In this work, we investigate the potential of improving multi-task training and also leveraging it for transferring in the reinforcement learning setting. We identify several challenges towards this goal and propose a transferring approach with a parameter-compositional formulation. We investigate ways to improve the training of multi-task reinforcement learning which serves as the foundation for transferring. Then we conduct a number of transferring experiments on various manipulation tasks. Experimental results demonstrate that the proposed approach can have improved performance in the multi-task training stage, and further show effective transferring in terms of both sample efficiency and performance.","2377-3766","","10.1109/LRA.2023.3284660","Lingfeng Sun was interning at Horizon Robotics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10147360","Reinforcement learning;transfer learning","Task analysis;Multitasking;Training;Robots;Reinforcement learning;Transfer learning;Supervised learning","reinforcement learning","manipulation tasks;multitask reinforcement learning;multitask training stage;parameter-compositional formulation;parameter-compositional framework;reinforcement learning setting;transferring approach;transferring experiments","","","","35","IEEE","9 Jun 2023","","","IEEE","IEEE Journals"
"Reinforcement Learning-based Analog Circuit Optimizer using gm/ID for Sizing","M. Choi; Y. Choi; K. Lee; S. Kang","Samsung Advanced Institute of Technology, Suwon, Korea; Electrical Engineering POSTECH, Pohang, Korea; Electrical Engineering POSTECH, Pohang, Korea; Electrical Engineering POSTECH, Pohang, Korea","2023 60th ACM/IEEE Design Automation Conference (DAC)","15 Sep 2023","2023","","","1","6","Designing analog circuits incurs high time costs because designers must consider numerous design variables or trade-off relationships of circuit performance based on a lot of knowledge and experience. To reduce design time, various machine learning methods have been used to optimize analog circuits by learning the correlation between the device size and the circuit performance. However, it is difficult to train the correlation because of its high non-linearity and wide design space. In this paper, this study proposes a new framework to optimize analog circuit designs by combining reinforcement learning (RL) and the sensitivity analysis with gm/ID sizing, which is more intuitive for interpreting circuit performance. Furthermore, the universal value function approximator (UVFA), previously proposed in RL, is modified more simply to make it easier to find the target design. Additionally, the dataset is rearranged and sampled by the criteria that are established based on the principle of circuit operation, which helps to orient the agent to learn the circuit operation. Using the proposed methods, we optimize three types of differential amplifiers with common mode feedback circuits and obtain the best circuit design. Compared to baseline, we find the optimal point using modified UVFA, and moreover, reduce the number of iterations by 42.2%, 39.5%, and 37.5%, respectively, for the three test cases.","","979-8-3503-2348-1","10.1109/DAC56929.2023.10247739","IC Design Education Center; Samsung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247739","Analog circuit sizing;Automated design;Efficient search;Reinforcement learning","Training;Circuit optimization;Correlation;Sensitivity analysis;Feedback circuits;Reinforcement learning;Analog circuits","analogue integrated circuits;circuit feedback;circuit optimisation;differential amplifiers;electronic engineering computing;function approximation;integrated circuit design;reinforcement learning;sensitivity analysis","analog integrated circuits;analog circuit designs;analog circuit sizing;circuit operation;circuit performance;common mode feedback circuits;design time;design variables;device size;differential amplifiers;gm sizing;ID sizing;machine learning;reinforcement learning-based analog circuit optimizer;RL;sensitivity analysis;target design;universal value function approximator;UVFA","","","","20","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
"Reducing the Deployment-Time Inference Control Costs of Deep Reinforcement Learning Agents via an Asymmetric Architecture","C. -J. Chang; Y. -W. Chu; C. -H. Ting; H. -K. Liu; Z. -W. Hong; C. -Y. Lee","Department of Computer Science, Elsa Lab, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, Elsa Lab, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, Elsa Lab, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, Elsa Lab, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, Elsa Lab, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, Elsa Lab, National Tsing Hua University, Hsinchu, Taiwan","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","4762","4768","Deep reinforcement learning (DRL) has been demonstrated to provide promising results in several challenging decision making and control tasks. However, the required inference costs of deep neural networks (DNNs) could prevent DRL from being applied to mobile robots which cannot afford high energy-consuming computations. To enable DRL methods to be affordable in such energy-limited platforms, we propose an asymmetric architecture that reduces the overall inference costs via switching between a computationally expensive policy and an economic one. The experimental results evaluated on a number of representative benchmark suites for robotic control tasks demonstrate that our method is able to reduce the inference costs while retaining the agent’s overall performance.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9562026","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562026","","Deep learning;Costs;Conferences;Decision making;Training data;Computer architecture;Reinforcement learning","decision making;learning (artificial intelligence);mobile robots;neural nets","robotic control tasks;agent;deployment-time inference control costs;deep reinforcement learning agents;asymmetric architecture;challenging decision making;required inference costs;deep neural networks;mobile robots;high energy-consuming computations;DRL methods;energy-limited platforms;computationally expensive policy","","","","30","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Online Vulnerability Identification in Smart Grid under Load Uncertainty","L. Yu; Y. Tan; S. Qin; D. Yue","NUPT, College of Auto. & AI, Nanjing, China; NUPT, College of Auto. & AI, Nanjing, China; NUPT, College of IoT, Nanjing, China; NUPT, College of Auto. & AI, Nanjing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","4036","4041","The outages of transmission lines can trigger cascading failure in power grid, resulting in serious negative impacts. Identifying those critical lines can help to take precautionary measures and build robust power system. A group of existing works have developed approaches to identify those critical lines. However, the approaches haven’t consider the load uncertainty, which will lead to the changes of critical lines. In this paper, we investigate a critical line identification problem in smart grid considering load uncertainty. Specifically, we first formulate an optimal virtual attacking problem and its objective is to maximize the expected generation loss under the given attacking resources. Due to the existence of large solution space, unknown system dynamics model, and load uncertainty, solving the formulated problem is challenging. Thus, we design an online algorithm based on prioritized multi-agent-attention-actor-critic (PMA3C). Compared with optimization-based methods and evolutionary methods, the designed algorithm can identify critical lines instantly under load uncertainty without searching the whole or partial solution space. Simulation results indicate that the designed algorithm can identify critical lines efficiently.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055373","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Nanjing University of Posts and Telecommunications; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055373","Smart grid;vulnerability analysis;cascading failures;load uncertainty;deep reinforcement learning","Uncertainty;Power transmission lines;System dynamics;Heuristic algorithms;Simulation;Power system protection;Reinforcement learning","deep learning (artificial intelligence);multi-agent systems;power engineering computing;power system reliability;reinforcement learning;smart power grids","critical line identification problem;evolutionary methods;online vulnerability identification;optimal virtual attacking problem;optimization-based methods;PMA3C;power grid;prioritized multiagent-attention-actor-critic;robust power system;smart grid considering load uncertainty;transmission lines","","","","16","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Graph Topography-Aware Reinforcement Learning for Intelligent Traffic Signal Control","C. Yang; B. Zou; W. Huang; F. Sun; H. Liu","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","10 Nov 2021","2021","","","459","465","Scheduling of extensive traffic signal plays an essential role in relieving the traffic pressure. Massive-scale adaptive traffic-signal control involves dealing with combinatorial state and action spaces. Previous work attempts to address this challenge by distributing control to independent agents who only use its local state information. Considering the unstructured nature of transportation network data, we design a graph topography-aware signal control framework base on deep reinforcement learning, which explicitly utilizes the distribution of traffic intersections to extract global information. To further simulate the real-world diversified road network and its vehicle physical properties, we develop a flexible and realistic traffic intersection environment based on Unreal Engine [1]. Our graph adaptive signal controller can significantly improve traffic performance and considerably reduce traffic congestion delay compared to the traditional baseline. Additionally, our environment can lay a foundation for the subsequent optimization of transportation. Code and video are available at https://github.com/emigmo/InterSim.git.","2642-6633","978-1-6654-2527-8","10.1109/CYBER53097.2021.9588330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9588330","","Roads;Transportation;Reinforcement learning;Control systems;Delays;Data mining;Intelligent systems","adaptive control;deep learning (artificial intelligence);distributed control;graph theory;reinforcement learning;road traffic control;traffic engineering computing;transportation","graph topography-aware reinforcement learning;intelligent traffic signal control;extensive traffic signal;traffic pressure;massive-scale adaptive traffic-signal control;combinatorial state;action spaces;local state information;transportation network data;graph topography-aware signal control framework base;deep reinforcement learning;traffic intersections;global information;real-world diversified road network;flexible traffic intersection environment;realistic traffic intersection environment;graph adaptive signal controller;traffic performance;traffic congestion delay","","","","30","IEEE","10 Nov 2021","","","IEEE","IEEE Conferences"
"Adaptive Risk Sensitive Path Integral for Model Predictive Control via Reinforcement Learning","H. -J. Yoon; C. Tao; H. Kim; N. Hovakimyan; P. Voulgaris","Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, USA; Department of Mechanical Engineering, University of Nevada, Reno, USA; Department of Electrical and Computer Engineering, Mercer University, USA; Department of Mechanical Engineering, University of Nevada, Reno, USA; Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, USA","2023 31st Mediterranean Conference on Control and Automation (MED)","25 Jul 2023","2023","","","926","931","We propose a reinforcement learning framework where an agent uses an internal nominal model for stochastic model predictive control (MPC) while compensating for a disturbance. Our work builds on the existing risk-aware optimal control with stochastic differential equations (SDEs) that aims to deal with such disturbance. However, the risk sensitivity and the noise strength of the nominal SDE in the riskaware optimal control are often heuristically chosen. In the proposed framework, the risk-taking policy determines the behavior of the MPC to be risk-seeking (exploration) or risk-averse (exploitation). Specifically, we employ the risk-aware path integral control that can be implemented as a Monte-Carlo (MC) sampling with fast parallel simulations using a GPU. The MC sampling implementations of the MPC have been successful in robotic applications due to their real-time computation capability. The proposed framework that adapts the noise model and the risk sensitivity outperforms the standard model predictive path integral in simulation environments that have disturbances.","2473-3504","979-8-3503-1543-1","10.1109/MED59994.2023.10185876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10185876","","Adaptation models;Sensitivity;Computational modeling;Stochastic processes;Optimal control;Reinforcement learning;Predictive models","differential equations;Monte Carlo methods;optimal control;predictive control;reinforcement learning;risk analysis;stochastic processes","adaptive risk sensitive path integral;existing risk-aware;internal nominal model;MC sampling implementations;MPC;noise model;noise strength;nominal SDE;reinforcement learning framework;risk sensitivity;risk-averse;risk-aware path integral control;risk-seeking;risk-taking policy;riskaware optimal control;standard model predictive path integral;stochastic differential equations;stochastic model predictive control","","","","24","IEEE","25 Jul 2023","","","IEEE","IEEE Conferences"
"Automatic Cell Rotation Method Based on Deep Reinforcement Learning","H. Gong; Y. Zhang; Y. Liu; Q. Zhao; X. Zhao; M. Sun","Tianjin Key Laboratory of Intelligent Robotic (tjKLIR), Institute of Robotics and Automatic Information System (IRAIS), Nankai University, Tianjin, China; Tianjin Key Laboratory of Intelligent Robotic (tjKLIR), Institute of Robotics and Automatic Information System (IRAIS), Nankai University, Tianjin, China; Tianjin Key Laboratory of Intelligent Robotic (tjKLIR), Institute of Robotics and Automatic Information System (IRAIS), Nankai University, Tianjin, China; Tianjin Key Laboratory of Intelligent Robotic (tjKLIR), Institute of Robotics and Automatic Information System (IRAIS), Nankai University, Tianjin, China; Tianjin Key Laboratory of Intelligent Robotic (tjKLIR), Institute of Robotics and Automatic Information System (IRAIS), Nankai University, Tianjin, China; Tianjin Key Laboratory of Intelligent Robotic (tjKLIR), Institute of Robotics and Automatic Information System (IRAIS), Nankai University, Tianjin, China","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5452","5458","Cell rotation is widely used to adjust cell posture in sub-cellular micromanipulations. The trajectory planning of the injection micropipette is needed, so that the cells can be rotated with the minimum deformation to reduce cell damage and keep cell viability. Due to the uncertainty of cell properties and manipulation environment, it is difficult to identify the parameters of the mechanical models in traditional robotic cell rotation methods. In this paper, deep reinforcement learning is introduced into cell manipulation for the first time to perform trajectory planning of the micropipette. We first abstract the cell rotation process by using the mechanical model and microscopic vision techniques and build a cell rotation simulation environment. Then we design a reward function by combining various factors of cell rotation and implement a reinforcement learning framework based on deep Q-learning (DQL). Finally, we train the cell rotation process based on the deep reinforcement learning algorithm. The simulation results indicate the proposed DQL agent achieved an average success rate of 97% without useless exploration. Moreover, the proposed method rotated the cells in a way that causes less mechanical damage than humans, demonstrating the DRL ability for cell rotation with high efficiency and low cell damage.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161043","","Deep learning;Training;Uncertainty;Q-learning;Trajectory planning;Deformation;Microscopy","cellular biophysics;deep learning (artificial intelligence);learning (artificial intelligence);medical robotics;micromanipulators;motion control;reinforcement learning;robot kinematics;robot vision;telecommunication computing;trajectory control","automatic cell rotation method;cell manipulation;cell posture;cell properties;cell rotation process;cell rotation simulation environment;cell viability;deep Q-learning;deep reinforcement learning algorithm;low cell damage;mechanical model;reinforcement learning framework;traditional robotic cell rotation methods;trajectory planning","","","","33","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning-based Drift Control for Autonomous Vehicles","Y. Jiang; X. Xu; X. Zhang; J. Huang; S. Gao","College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; Department of Computer Science, Technical University of Munich, Garching, Germany; School of Instrumentation Science and Engineering, Harbin Institute of Technology, Harbin, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","4508","4513","Drift maneuvers show promising capabilities for vehicle control under unstable conditions, which is significant for improving the maneuverability and safety of autonomous vehicles. However, the implementation of autonomous drift is challenging due to the complex dynamics of the vehicle under drifting conditions and the strong coupling of lateral and longitudinal control. In this work, a customized drift simulator of the target vehicle is constructed to capture its dynamics characteristics through vehicle modeling and parameter identification. In addition, a reinforcement learning-based drift controller is designed using the vehicle dynamics model as prior knowledge. Simulation results demonstrate the effectiveness and efficiency of the proposed controller in both steady-state drift and drift cornering.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727514","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727514","","Couplings;Parameter estimation;Simulation;Heuristic algorithms;Design methodology;Steady-state;Safety","learning (artificial intelligence);mobile robots;motion control;road vehicles;robot dynamics;steering systems;vehicle dynamics","reinforcement learning-based drift control;autonomous vehicles;drift maneuvers;vehicle control;unstable conditions;autonomous drift;lateral control;longitudinal control;customized drift simulator;vehicle modeling;parameter identification;reinforcement learning-based drift controller;vehicle dynamics model;steady-state drift;drift cornering","","","","16","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"RESPECT: Reinforcement Learning based Edge Scheduling on Pipelined Coral Edge TPUs","J. Yin; Y. Li; D. Robinson; C. Yu","University of Utah, Salt Lake City, US; University of Utah, Salt Lake City, US; University of Utah, Salt Lake City, US; University of Utah, Salt Lake City, US","2023 60th ACM/IEEE Design Automation Conference (DAC)","15 Sep 2023","2023","","","1","6","Deep neural networks (DNNs) have substantial computational and memory requirements, and the compilation of its computational graphs has a great impact on the performance of resource-constrained (e.g., computation, I/O, and memory-bound) edge computing systems. While efficient execution of their computational graph requires an effective scheduling algorithm, generating the optimal scheduling solution is a challenging NP-hard problem. Furthermore, the complexity of scheduling DNN computational graphs will further increase on pipelined multi-core systems considering memory communication cost, as well as the increasing size of DNNs. Using the synthetic graph for the training dataset, this work presents a reinforcement learning (RL) based scheduling framework RESPECT, which learns the behaviors of optimal optimization algorithms and generates near-optimal scheduling results with short solving runtime overhead. Our framework has demonstrated up to ∼ 2.5 × real-world on-chip inference runtime speedups over the commercial compiler with ten popular ImageNet models deployed on the physical Coral Edge TPUs system. Moreover, compared to the exact optimization methods, the proposed RL scheduling improves the scheduling optimization runtime by up to 683× speedups compared to the commercial compiler and matches the exact optimal solutions with up to 930× speedups. Finally, we perform a comprehensive generalizability test, which demonstrates RESPECT successfully imitates optimal solving behaviors from small synthetic graphs to large real-world DNNs computational graphs.","","979-8-3503-2348-1","10.1109/DAC56929.2023.10247706","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247706","","Training;Runtime;Processor scheduling;Image edge detection;Optimization methods;Optimal scheduling;Reinforcement learning","computational complexity;deep learning (artificial intelligence);edge computing;graph theory;inference mechanisms;optimisation;power aware computing;reinforcement learning;scheduling","commercial compiler;computational graph;deep neural networks;DNN computational graphs;effective scheduling algorithm;exact optimal solutions;exact optimization methods;memory communication cost;memory-bound;near-optimal scheduling results;NP-hard problem;on-chip inference runtime speedups;optimal optimization algorithms;optimal scheduling solution;optimal solving behaviors;physical Coral Edge TPU system;pipelined coral edge;pipelined multicore systems;real-world DNN computational graphs;reinforcement learning based edge scheduling;resource-constrained;RESPECT;RL scheduling;scheduling optimization runtime;short solving runtime overhead;substantial computational memory requirements;synthetic graph","","","","24","IEEE","15 Sep 2023","","","IEEE","IEEE Conferences"
"Collision-Free Trajectory Planning of Mobile Robots by Integrating Deep Reinforcement Learning and Model Predictive Control","Z. Zhang; Y. Cai; K. Ceder; A. Enliden; O. Eriksson; S. Kylander; R. Sridhara; K. Åkesson","Chalmers University of Technology, Gothenburg, Sweden; Chalmers University of Technology, Gothenburg, Sweden; Chalmers University of Technology, Gothenburg, Sweden; Chalmers University of Technology, Gothenburg, Sweden; Chalmers University of Technology, Gothenburg, Sweden; Chalmers University of Technology, Gothenburg, Sweden; Chalmers University of Technology, Gothenburg, Sweden; Chalmers University of Technology, Gothenburg, Sweden","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","7","In this paper, we present an efficient approach to real-time collision-free navigation for mobile robots. By integrating deep reinforcement learning with model predictive control, our aim is to achieve both collision avoidance and computational efficiency. The methodology begins with training a preliminary agent using deep Q-learning, enabling it to generate actions for next time steps. Instead of executing these actions, a reference trajectory is generated based on them, which avoids obstacles present on the original reference path. Subsequently, this local trajectory is employed within an MPC trajectory-tracking framework to provide collision-free guidance for the mobile robot. Experimental results demonstrate that the proposed DQN-MPC hybrid approach outperforms pure MPC in terms of time efficiency and solution quality.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260515","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260515","","Deep learning;Training;Visualization;Trajectory tracking;Trajectory planning;Stability analysis;Real-time systems","collision avoidance;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;path planning;predictive control;reinforcement learning;trajectory control","collision avoidance;collision-free guidance;collision-free trajectory planning;computational efficiency;deep Q-learning;integrating deep reinforcement learning;local trajectory;mobile robot;model predictive control;MPC trajectory-tracking framework;real-time collision-free navigation;reference trajectory;solution quality;time efficiency;time steps","","","","25","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Aligning Human Preferences with Baseline Objectives in Reinforcement Learning","D. Marta; S. Holk; C. Pek; J. Tumova; I. Leite","Division of Robotics, Perception and Learning, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Robotics, Perception and Learning, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Robotics, Perception and Learning, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Robotics, Perception and Learning, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Robotics, Perception and Learning, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","7562","7568","Practical implementations of deep reinforcement learning (deep RL) have been challenging due to an amplitude of factors, such as designing reward functions that cover every possible interaction. To address the heavy burden of robot reward engineering, we aim to leverage subjective human preferences gathered in the context of human-robot interaction, while taking advantage of a baseline reward function when available. By considering baseline objectives to be designed beforehand, we are able to narrow down the policy space, solely requesting human attention when their input matters the most. To allow for control over the optimization of different objectives, our approach contemplates a multi-objective setting. We achieve human-compliant policies by sequentially training an optimal policy from a baseline specification and collecting queries on pairs of trajectories. These policies are obtained by training a reward estimator to generate Pareto optimal policies that include human preferred behaviours. Our approach ensures sample efficiency and we conducted a user study to collect real human preferences, which we utilized to obtain a policy on a social navigation environment.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161261","Swedish Foundation for Strategic Research(grant numbers:SSF FFL18-0199); Knut and Alice Wallenberg Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161261","","Training;Measurement;Navigation;Human-robot interaction;Reinforcement learning;Predictive models;Pareto optimization","deep learning (artificial intelligence);human-robot interaction;learning (artificial intelligence);Pareto optimisation;reinforcement learning","baseline objectives;baseline reward function;baseline specification;collecting queries;cover every possible interaction;deep reinforcement learning;deep RL;heavy burden;human attention;human preferred behaviours;human-compliant policies;human-robot interaction;leverage subjective human preferences;multiobjective setting;optimal policy;Pareto optimal policies;policy space;reward estimator;reward functions;robot reward engineering","","","","44","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Hierarchical Planning Through Goal-Conditioned Offline Reinforcement Learning","J. Li; C. Tang; M. Tomizuka; W. Zhan","Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA","IEEE Robotics and Automation Letters","1 Aug 2022","2022","7","4","10216","10223","Offline Reinforcement learning (RL) has shown potent in many safe-critical tasks in robotics where exploration is risky and expensive. However, it still struggles to acquire skills in temporally extended tasks. In this paper, we study the problem of offline RL for temporally extended tasks. We propose a hierarchical planning framework, consisting of a low-level goal-conditioned RL policy and a high-level goal planner. The low-level policy is trained via offline RL. We improve the offline training to deal with out-of-distribution goals by a perturbed goal sampling process. The high-level planner selects intermediate sub-goals by taking advantages of model-based planning methods. It plans over future sub-goal sequences based on the learned value function of the low-level policy. We adopt a Conditional Variational Autoencoder to sample meaningful high-dimensional sub-goal candidates and to solve the high-level long-term strategy optimization problem. We evaluate our proposed method in long-horizon driving and robot navigation tasks. Experiments show that our method outperforms baselines with different hierarchical designs and other regular planners without hierarchy in these complex tasks.","2377-3766","","10.1109/LRA.2022.3190100","Denso International America, Inc.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826807","Integrated planning and learning;reinforcement learning;autonomous agents","Task analysis;Time division multiplexing;Planning;Optimization;Reinforcement learning;Training;Robots","control engineering computing;learning (artificial intelligence);mobile robots;path planning;planning (artificial intelligence)","temporally extended tasks;hierarchical planning framework;low-level goal-conditioned RL policy;high-level goal planner;low-level policy;offline RL;offline training;out-of-distribution goals;perturbed goal sampling process;high-level planner;intermediate sub-goals;future sub-goal sequences;learned value function;Conditional Variational Autoencoder;sample meaningful high-dimensional sub-goal candidates;high-level long-term strategy optimization problem;long-horizon driving;robot navigation tasks;different hierarchical designs;goal-conditioned offline Reinforcement learning;safe-critical tasks","","","","35","IEEE","12 Jul 2022","","","IEEE","IEEE Journals"
"Air conditioner component optimum operation point search through a deep reinforcement learning algorithm","M. -S. YOON; W. -S. YOON; M. -K. SEO; S. -Y. RYU; J. -S. LEE","Energy Technology Center, Korea Testing Laboratory, Seoul, Korea; Energy Technology Center, Korea Testing Laboratory, Seoul, Korea; Engineering Team, Fläkt Korea, Seoul, Korea; System Department, Seungil Electronics, Bucheon, Korea; School of Integrated Technology, Yonsei University, Incheon, Korea","2020 20th International Conference on Control, Automation and Systems (ICCAS)","1 Dec 2020","2020","","","365","372","A deep reinforcement machine learning algorithm is applied to an energy-efficient optimal operation point search of an inverter air conditioner. The combination of the two factors of compressor output (COM) and electronic expansion valve (EEV) opening, which have a major influence on the efficiency (EER) of an air conditioner, is analyzed with the algorithm over 5 days. It displays a repetitive stabilization pattern 12 hours after the commencement of the deep learning algorithm, and finds the optimal (COM, EEV) combination with the maximum EER. An arbitrary case (600, 400) that satisfies the target cooling capacity (9200 W) is started with an initial value to reach (420, 230) with the optimal EER at a given condition (given product specification, standard test condition (T1 condition)). In this study, since the optimal point of (COM, EEV) exists at the boundary of the action domain, it inevitably has a repeating learning pattern. The repetitive stabilization pattern is examined for two cases of the discount factor of 0.5 and 0.99. When the discount factor is 0.5, it shows a shortsighted behavior to the present reward value more clearly than when it is 0.99. This kind of experimental study can be extended to find the optimum operating point when several components of an air conditioner are operating simultaneously.","2642-3901","978-89-93215-20-5","10.23919/ICCAS50221.2020.9268387","Korea Institute for Advancement of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268387","Air conditioner;Deep reinforcement learning;Machine learning;Inverter air conditioner;Smart HVAC","Cooling;Heating systems;Inverters;Reinforcement learning;Power demand;Artificial neural networks;Encoding","air conditioning;compressors;cooling;domestic appliances;learning (artificial intelligence);mechanical engineering computing;neural nets;valves","EEV;maximum EER;cooling capacity;optimal EER;standard test condition;T1 condition;repeating learning pattern;optimum operating point;air conditioner component optimum operation point search;energy-efficient optimal operation point search;inverter air conditioner;repetitive stabilization pattern;deep reinforcement machine learning;time 5.0 d;time 12.0 hour;power 9200.0 W","","","","36","","1 Dec 2020","","","IEEE","IEEE Conferences"
"Visual Reinforcement Learning With Self-Supervised 3D Representations","Y. Ze; N. Hansen; Y. Chen; M. Jain; X. Wang","Shanghai Jiao Tong University, Shanghai, China; University of California San Diego, San Diego, CA, USA; University of California San Diego, San Diego, CA, USA; University of California San Diego, San Diego, CA, USA; University of California San Diego, San Diego, CA, USA","IEEE Robotics and Automation Letters","6 Apr 2023","2023","8","5","2890","2897","A prominent approach to visual Reinforcement Learning (RL) is to learn an internal state representation using self-supervised methods, which has the potential benefit of improved sample-efficiency and generalization through additional learning signal and inductive biases. However, while the real world is inherently 3D, prior efforts have largely been focused on leveraging 2D computer vision techniques as auxiliary self-supervision. In this work, we present a unified framework for self-supervised learning of 3D representations for motor control. Our proposed framework consists of two phases: a pretraining phase where a deep voxel-based 3D autoencoder is pretrained on a large object-centric dataset, and a finetuning phase where the representation is jointly finetuned together with RL on in-domain data. We empirically show that our method enjoys improved sample efficiency compared to 2D representation learning methods. Additionally, our learned policies transfer zero-shot to a real robot setup with only approximate geometric correspondence, and successfully solve motor control tasks that involve grasping and lifting from a single, uncalibrated RGB camera.","2377-3766","","10.1109/LRA.2023.3259681","Amazon Research Award and gifts from Qualcomm; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10077386","Reinforcement learning;representation learning;deep learning for visual perception","Three-dimensional displays;Task analysis;Visualization;Cameras;Representation learning;Training;Robot vision systems","cameras;computer vision;deep learning (artificial intelligence);learning (artificial intelligence);reinforcement learning;robot vision;supervised learning;unsupervised learning","additional learning signal;auxiliary self-supervision;deep voxel-based 3D autoencoder;improved sample-efficiency;internal state representation;learned policies;leveraging 2D computer vision techniques;prominent approach;RL;sample efficiency;self-supervised 3D representations;self-supervised learning;self-supervised methods;visual Reinforcement Learning","","","","44","IEEE","20 Mar 2023","","","IEEE","IEEE Journals"
"Robot Navigation With Reinforcement Learned Path Generation and Fine-Tuned Motion Control","L. Zhang; Z. Hou; J. Wang; Z. Liu; W. Li","Academy for Engineering and Technology, Fudan University, Shanghai, China; Academy for Engineering and Technology, Fudan University, Shanghai, China; Academy for Engineering and Technology, Fudan University, Shanghai, China; Academy for Engineering and Technology, Fudan University, Shanghai, China; Academy for Engineering and Technology, Fudan University, Shanghai, China","IEEE Robotics and Automation Letters","16 Jun 2023","2023","8","8","4489","4496","In this letter, we propose a novel reinforcement learning (RL) based path generation (RL-PG) approach for mobile robot navigation without a prior exploration of an unknown environment. Multiple predictive path points are dynamically generated by a deep Markov model optimized using an RL approach for the robot to track. To ensure safety when tracking the predictive points, the robot's motion is fine-tuned by a motion fine-tuning module. Such an approach, using a deep Markov model with RL algorithm for planning, focuses on the relationship between adjacent path points. We analyze the benefits of our proposed approach and show it is more effective and has higher success rates than the RL-based approach DWA-RL (Patel et al. 2021) and a traditional navigation approach APF (Chen et al. 2021). We deploy our model on both simulation and physical platforms and demonstrate our model performs robot navigation effectively and safely.","2377-3766","","10.1109/LRA.2023.3284354","Shanghai Municipal Science and Technology Major Project(grant numbers:2021SHZDZX0103); Scientific Research Development Center in Higher Education Institutions; Ministry of Education, China(grant numbers:2021ITA10013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10146449","Motion and path planning;RL-based path generation;collision avoidance","Robots;Robot kinematics;Collision avoidance;Navigation;Trajectory;Robot sensing systems;Laser radar","deep learning (artificial intelligence);Markov processes;mobile robots;motion control;navigation;path planning;reinforcement learning","adjacent path points;deep Markov model;fine-tuned motion control;mobile robot navigation;model performs robot navigation;motion fine-tuning module;multiple predictive path points;predictive points;reinforcement learned path generation;reinforcement learning based path generation approach;RL algorithm;RL approach;RL-based approach DWA-RL;RL-PG;traditional navigation approach APF","","","","30","IEEE","8 Jun 2023","","","IEEE","IEEE Journals"
"On Deep Recurrent Reinforcement Learning for Active Visual Tracking of Space Noncooperative Objects","D. Zhou; G. Sun; Z. Zhang; L. Wu","Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China","IEEE Robotics and Automation Letters","14 Jun 2023","2023","8","8","4418","4425","Active tracking of space noncooperative object that merely relies on vision camera is greatly significant for autonomous rendezvous and debris removal. Considering its Partial Observable Markov Decision Process (POMDP) property, this letter proposes a novel deep recurrent neural network architecture, named as recurrent and attention module based active visual tracking (RAMAVT), incorporating Multi-Head Attention (MHA) module and Squeeze-and-Excitation (SE) layer that remarkably improve the representative ability of neural network with almost no extra computational cost. It has been successfully applied to value-based and policy gradient-based deep reinforcement learning algorithm, and learned to drive the chasing spacecraft to follow arbitrary space noncooperative object with high-frequency and near-optimal velocity control commands. Extensive experiments and robustness evaluations implemented on space non-cooperative object active tracking (SNCOAT) benchmark show the betterment and robustness of our method compared with other state-of-the-art active visual trackers. In addition, we make further ablation study and interpretability research on RAMAVT of which validity and rationality have been demonstrated.","2377-3766","","10.1109/LRA.2023.3282792","National Natural Science Foundation of China(grant numbers:62173107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10143684","Active visual tracking;deep recurrent reinforcement learning;space noncooperative object;multi-head attention","Target tracking;Visualization;Aerospace electronics;Space vehicles;Velocity control;Reinforcement learning;Computer architecture","deep learning (artificial intelligence);gradient methods;learning (artificial intelligence);Markov processes;recurrent neural nets;reinforcement learning;space vehicles;velocity control","active visual tracking;arbitrary space noncooperative object;autonomous rendezvous;debris removal;deep recurrent neural network architecture;deep recurrent reinforcement learning;extra computational cost;MultiHead Attention module;Partial Observable Markov Decision Process property;policy gradient-based deep reinforcement learning algorithm;recurrent attention module;space noncooperative object active tracking;Squeeze-and-Excitation layer;state-of-the-art active visual trackers;vision camera","","","","40","IEEE","5 Jun 2023","","","IEEE","IEEE Journals"
"Quantification of Joint Redundancy considering Dynamic Feasibility using Deep Reinforcement Learning","J. Chai; M. Hayashibe","Department of Robotics, Neuro-Robotics Lab, Graduate School of Engineering, Tohoku University, Sendai, Japan; Department of Robotics, Neuro-Robotics Lab, Graduate School of Engineering, Tohoku University, Sendai, Japan","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","10712","10718","The robotic joint redundancy for executing a task and the optimal usage of robotic joints given the redundant degrees of freedom are crucial for the performance of a robot. It is therefore of interest to quantify the joint redundancy to better understand the robotic dexterity considering the dynamic feasibility. To this end, model-based approaches have been among the most commonly used methods to quantify the joint redundancy of simple robots analytically. However, this classical approach fails when applied to non-conventional complex robots. In this study, we propose a new method based on a deep reinforcement learning-derived metric, the synergy exploration area (SEA) metric, for the quantification of redundancy with a given dynamic environment. We conducted various experiments with different robotic structures for different tasks, ranging from simple robotic arm manipulation to more complex robotic locomotion. The experimental results show that the SEA metric can effectively quantify the relative joint redundancy over different robotic structures with varying degrees of freedom under unknown dynamic situations.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561048","","Measurement;Analytical models;Redundancy;Dynamics;Reinforcement learning;Manipulators;Distance measurement","deep learning (artificial intelligence);dexterous manipulators;manipulator dynamics;redundant manipulators;reinforcement learning;robot programming","robotic structures;robotic arm manipulation;complex robotic locomotion;dynamic feasibility;robotic joint redundancy;robotic dexterity;deep reinforcement learning;synergy exploration area metric","","","","29","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Optimal Stabilization of Magnetically Suspended Balance Beam Subject to Input Delay","S. Ali Asad Rizvi; Y. Wei; Z. Lin","Electrical and Computer Engineering Department, Tennessee Technological University, Cookeville, TN, USA; Department of Electrical Engineering, University of North Texas, Denton, TX, USA; Charles L. Brown Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA, USA","2022 IEEE 17th International Conference on Control & Automation (ICCA)","25 Jul 2022","2022","","","237","342","In this paper, we present a delay compensating Q-learning algorithm for solving the optimal stabilization problem of a magnetically suspended balance beam system. The suspension by electromagnets mimics the operation of active magnetic bearings (AMBs) employed in industrial applications. We focus on the frequently encountered delay arising in the input current channel, which controls the magnetic field to produce the required force. We employ the recently proposed model-free reinforcement learning algorithm based on the idea of state augmentation to bring the balance beam system into a delay-free form. Compared to the recent works, the approach requires neither the knowledge of the system parameters nor the actual value of the delay. Only an upper bound of the delay is needed, which can be selected arbitrarily large as long as the learning time is acceptable. The stabilization of the balance beam and the convergence of the optimal control parameters are shown, confirming the efficacy of the proposed technique.","1948-3457","978-1-6654-9572-1","10.1109/ICCA54724.2022.9831866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831866","Active magnetic bearings;input delay;reinforcement learning;optimal adaptive control","Q-learning;Upper bound;MIMICs;Process control;Optimal control;Electromagnets;Stability analysis","beams (structures);delays;learning (artificial intelligence);magnetic bearings;optimal control;stability","magnetically suspended balance beam subject;input delay;Q-learning algorithm;optimal stabilization problem;magnetically suspended balance beam system;electromagnets mimics;active magnetic bearings;industrial applications;frequently encountered delay;input current channel;magnetic field;required force;model-free reinforcement learning algorithm;delay-free form;system parameters;optimal control parameters","","","","22","IEEE","25 Jul 2022","","","IEEE","IEEE Conferences"
"Tumbling Robot Control Using Reinforcement Learning: An Adaptive Control Policy That Transfers Well to the Real World","A. Schwartzwald; M. Tlachac; L. Guzman; A. Bacharis; N. Papanikolopoulos","SpaceX, El Segundo, California, USA; Amazon Robotics AI, Boulder, Colorado, USA; Amazon, Seattle, Washington, USA; Department of Computer Science and Engineering, University of Minnesota, Minneapolis, Minnesota, USA; Department of Computer Science and Engineering, University of Minnesota, Minneapolis, Minnesota, USA","IEEE Robotics & Automation Magazine","14 Jun 2023","2023","30","2","86","95","Tumbling robots are simple platforms that are able to traverse large obstacles relative to their size, at the cost of being difficult to control. Existing control methods apply only a subset of possible robot motions and make the assumption of flat terrain. Reinforcement learning (RL) allows for the development of sophisticated control schemes that can adapt to diverse environments. By utilizing domain randomization while training in simulation, a robust control policy can be learned that transfers well to the real world. In this article, we implement autonomous set point navigation on a tumbling robot prototype and evaluate it on flat, uneven, and valley–hill terrain. Our results demonstrate that RL-based control policies can generalize well to challenging environments that were not encountered during training. The flexibility of our system demonstrates the viability of nontraditional robots for navigational tasks.","1558-223X","","10.1109/MRA.2022.3188215","National Science Foundation(grant numbers:CNS-1439728,CNS-1531330,CNS-1939033); United States Department of Agriculture/National Institute of Food and Agriculture(grant numbers:2020-67021-30755); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9845375","","Robots;Collision avoidance;Friction;Servomotors;Prototypes;Numerical models;Legged locomotion;Reinforcement learning;Robust control;Robot motion;Adaptation models","adaptive control;collision avoidance;mobile robots;reinforcement learning;robust control","adaptive control policy;autonomous set point navigation;diverse environments;domain randomization;flat terrain;nontraditional robots;obstacles;reinforcement learning;RL-based control policies;robot motions;robust control policy;sophisticated control schemes;tumbling robot prototype;valley-hill terrain","","","","21","IEEE","29 Jul 2022","","","IEEE","IEEE Magazines"
"Model-Assisted Reinforcement Learning for Online Diagnostics in Stochastic Controlled Systems","E. Noorani; C. Somarakis; R. Goyal; A. Feldman; S. Rane","Palo Alto Research Center, A Xerox Company, Palo Alto, CA, USA; Palo Alto Research Center, A Xerox Company, Palo Alto, CA, USA; Palo Alto Research Center, A Xerox Company, Palo Alto, CA, USA; Palo Alto Research Center, A Xerox Company, Palo Alto, CA, USA; Palo Alto Research Center, A Xerox Company, Palo Alto, CA, USA","2022 IEEE 17th International Conference on Control & Automation (ICCA)","25 Jul 2022","2022","","","338","345","A mechanism to protect a controlled system in the event of a priori unknown abnormalities (e.g. faults, attacks) is the key to designing resilient and robust control systems. We explore bi-level control design architectures in which a supervisory Reinforcement Learning (RL) agent augments an over-observed controlled system. The RL agent monitors sensor signals, detects and takes action to mitigate unknown sensor faults. We use the system dynamics to extract features and develop a design method for the cost function of the RL module. We theoretically show that the designed cost function has a unique optimal policy that enables the diagnosis of arbitrary constant sensor faults. To conceptualize our architecture, we consider a linear version of an over-observed chemical process, controlled by a Linear Quadratic Gaussian (LQG) Servo-Controller with Integral Action. Our experimental results, coupled with our theoretical analysis, show that the RL-agent is successful in identifying and mitigating the faults in one or more sensors in an online fashion.","1948-3457","978-1-6654-9572-1","10.1109/ICCA54724.2022.9831833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831833","","Robust control;Fault diagnosis;System dynamics;Control design;Design methodology;Stochastic processes;Reinforcement learning","control system synthesis;intelligent control;learning (artificial intelligence);linear quadratic Gaussian control;observers;optimal control;robust control;sensors;stochastic systems","model-assisted Reinforcement Learning;online diagnostics;stochastic controlled systems;a priori unknown abnormalities;resilient control systems;robust control systems;bi-level control design architectures;supervisory Reinforcement;over-observed controlled system;RL agent monitors sensor signals;unknown sensor faults;system dynamics;design method;RL module;designed cost function;unique optimal policy;arbitrary constant sensor faults;over-observed chemical process;Linear Quadratic Gaussian Servo-Controller;RL-agent;online fashion","","","","19","IEEE","25 Jul 2022","","","IEEE","IEEE Conferences"
"Automated Test Generation for Hardware Trojan Detection using Reinforcement Learning","Z. Pan; P. Mishra","University of Florida, Gainesville, Florida, USA; University of Florida, Gainesville, Florida, USA","2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC)","11 Mar 2021","2021","","","408","413","Due to globalized semiconductor supply chain, there is an increasing risk of exposing System-on-Chip (SoC) designs to malicious implants, popularly known as hardware Trojans. Unfortunately, traditional simulation-based validation using millions of test vectors is unsuitable for detecting stealthy Trojans with extremely rare trigger conditions due to exponential input space complexity of modern SoCs. There is a critical need to develop eﬃcient Trojan detection techniques to ensure trustworthy SoCs. While there are promising test generation approaches, they have serious limitations in terms of scalability and detection accuracy. In this paper, we propose a novel logic testing approach for Trojan detection using an effective combination of testability analysis and reinforcement learning. Specifically, this paper makes three important contributions. 1) Unlike existing approaches, we utilize both controllability and observability analysis along with rareness of signals to significantly improve the trigger coverage. 2) Utilization of reinforcement learning considerably reduces the test generation time without sacrificing the test quality. 3) Experimental results demonstrate that our approach can drastically improve both trigger coverage (14.5% on average) and test generation time (6.5 times on average) compared to state-of-the-art techniques.","2153-697X","978-1-4503-7999-1","","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9371613","","Supply chains;Stochastic processes;Reinforcement learning;Hardware;System-on-chip;Test pattern generators;Trojan horses","","","","","","17","","11 Mar 2021","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Method for Motion Control With Constraints on an HPN Arm","Y. Gan; P. Li; H. Jiang; G. Wang; Y. Jin; X. Chen; J. Ji","School of Data Science, University of Science and Technology of China (USTC), Hefei, China; School of Computer Science, University of Science and Technology of China (USTC), Hefei, China; School of Computer Science, University of Science and Technology of China (USTC), Hefei, China; School of Physics, University of Science and Technology of China (USTC), Hefei, China; School of Computer Science, University of Science and Technology of China (USTC), Hefei, China; School of Computer Science, University of Science and Technology of China (USTC), Hefei, China; School of Computer Science, University of Science and Technology of China (USTC), Hefei, China","IEEE Robotics and Automation Letters","4 Oct 2022","2022","7","4","12006","12013","Soft robotic arms have shown great potential toward applications to human daily lives, which is mainly due to their infinite passive degrees of freedom and intrinsic safety. There are tasks in lives that require the motion of the robot to meet some certain pose constraints that have not been implemented through the soft arm, like delivering a glass of water. Because the workspace of the soft arm is affected by the loads or interaction, it is difficult to implement this task through the motion planning method. In this letter, we propose a Q-learning based approach to address the problem, directly achieving motion control with constraints under loads and interaction without planning. We first generate a controller for the soft arm based on Q-learning, which can operate the arm while satisfying the pose constraints when the arm is neither loaded nor interacted with the environment. Then, we introduce a process that adjusts corresponding Q values in the controller, which allows the controller to operate the arm with an unknown load or interaction while still satisfying the pose constraints. We implement the approach on our soft arm, i.e., the Honeycomb Pneumatic Network (HPN) Arm. The experiments show that the approach is effective, even when the arm reached an untrained situation or even beyond the workspace under the interaction.","2377-3766","","10.1109/LRA.2022.3196789","Shenzhen Institute of Artificial Intelligence and Robotics for Society(grant numbers:AC01202005010); National Natural Science Foundation of China(grant numbers:61573333); CAAI-Huawei MindSpore Open Fund; Guangdong Province R&D Program(grant numbers:2020B0909050001); Anhui Province Development and Reform Commission Project 2020 and 2021; Shenzhen Yijiahe Technology R&D Co., Ltd.; Huawei Cloud Computing Technologies Co., Ltd.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851517","Machine learning for robot control;modeling;control;and learning for soft robots;soft robot applications","Motion control;Q-learning;Manipulators;Task analysis;Soft robotics;Load modeling;Data models","humanoid robots;learning (artificial intelligence);manipulator dynamics;medical robotics;mobile robots;motion control;path planning;pneumatic actuators;robots","reinforcement learning method;motion control;HPN Arm;soft robotic arms;human daily lives;pose constraints;soft arm;motion planning method;Q-learning based approach;Honeycomb Pneumatic Network Arm","","","","19","IEEE","5 Aug 2022","","","IEEE","IEEE Journals"
"Safe Reinforcement Learning of Dynamic High-Dimensional Robotic Tasks: Navigation, Manipulation, Interaction","P. Liu; K. Zhang; D. Tateo; S. Jauhri; Z. Hu; J. Peters; G. Chalvatzaki","Computer Science Department, Technical University Darmstadt; Computer Science Department, Technical University Darmstadt; Computer Science Department, Technical University Darmstadt; Computer Science Department, Technical University Darmstadt; Computer Science Department, Technical University Darmstadt; Computer Science Department, Technical University Darmstadt; Computer Science Department, Technical University Darmstadt","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","9449","9456","Safety is a fundamental property for the real-world deployment of robotic platforms. Any control policy should avoid dangerous actions that could harm the environment, humans, or the robot itself. In reinforcement learning (RL), safety is crucial when exploring a new environment to learn a new skill. This paper introduces a new formulation of safe exploration for robotic RL in the tangent space of the constraint manifold that effectively transforms the action space of the RL agent for always respecting safety constraints locally. We show how to apply this approach to a wide range of robotic platforms and how to define safety constraints that represent dynamic articulated objects like humans in the context of robotic RL. Our proposed approach achieves state-of-the-art performance in simulated high-dimensional and dynamic tasks while avoiding collisions with the environment. We show safe real-world deployment of our learned controller on a $\text{TIAGo}++$ robot, achieving remarkable performance in manipulation and human-robot interaction tasks.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161548","China Scholarship Council(grant numbers:201908080039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161548","","Manifolds;Navigation;Shape;Reinforcement learning;Transforms;Aerospace electronics;Safety","control engineering computing;deep learning (artificial intelligence);human-robot interaction;learning (artificial intelligence);mobile robots;path planning;reinforcement learning","action space;constraint manifold;control policy;dangerous actions;dynamic articulated objects;dynamic tasks;fundamental property;high-dimensional robotic tasks;human-robot interaction tasks;learned controller;real-world deployment;RL agent;robotic platforms;robotic RL;safe exploration;safe reinforcement learning;safety constraints;simulated high-dimensional tasks;tangent space;TIAGo++ robot","","","","60","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Train Station Parking Approach Based on Fuzzy Reinforcement Learning Algorithms","J. Yin; S. Su; K. Li; T. Tang","State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China; National Engineering Research Center of Rail Transportation Operation and Control System, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China","2019 IEEE 15th International Conference on Control and Automation (ICCA)","14 Nov 2019","2019","","","1411","1416","Train station parking (TSP) accuracy is important to enhance the efficiency of train operation and the safety of passengers for urban rail transit, while TSP is always subject to a series of uncertain factors. To increase the parking accuracy, robustness and self-learning ability, we propose a new train parking approach by using the reinforcement learning (RL) theory. Three algorithms were developed, involving a stochastic optimal selection algorithm (SOSA), a Q-learning algorithm (QLA) and a fuzzy function based Q-learning algorithm (FQLA) in order to reduce the parking error in urban rail transit. Meanwhile, five braking rates are adopted as the action vector of the three algorithms and some statistical indices are developed to evaluate parking errors. Parking results show that the parking errors of the three algorithms are all within the ±30cm, which meet the requirement of urban rail transit.","1948-3457","978-1-7281-1164-3","10.1109/ICCA.2019.8899712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899712","Train station parking;reinforcement learning;Q-learning;fuzzy function.","Rails;Resistance;Safety;Adaptation models;Reinforcement learning;Computational modeling;Silicon","braking;fuzzy set theory;learning (artificial intelligence);railway engineering;railway safety;statistical analysis;stochastic processes","train station parking approach;fuzzy reinforcement learning algorithms;TSP;urban rail transit;self-learning ability;train parking approach;reinforcement learning theory;stochastic optimal selection algorithm;fuzzy function based Q-learning algorithm;statistical indices","","","","20","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Reward Redistribution for Reinforcement Learning of Dynamic Nonprehensile Manipulation","G. Sejnova; M. Mejdrechova; M. Otahal; N. Sokovnin; I. Farkas; M. Vavrecka","Czech Institute of Informatics, Robotics and Cybernetics Czech Technical University in Prague, Prague, Czech Republic; Czech Institute of Informatics, Robotics and Cybernetics Czech Technical University in Prague, Prague, Czech Republic; Czech Institute of Informatics, Robotics and Cybernetics Czech Technical University in Prague, Prague, Czech Republic; Czech Institute of Informatics, Robotics and Cybernetics Czech Technical University in Prague, Prague, Czech Republic; Faculty of Mathematics, Physics and Informatics, Comenius University in Bratislava, Bratislava, Slovakia; Czech Institute of Informatics, Robotics and Cybernetics Czech Technical University in Prague, Prague, Czech Republic","2021 7th International Conference on Control, Automation and Robotics (ICCAR)","25 Jun 2021","2021","","","326","331","Recent reinforcement learning (RL) systems can solve a wide variety of manipulation tasks even in real-world robotic implementations. However, in some nonprehensile manipulation tasks (e.g. poking, throwing), the classical reward system fails as the robot has to manipulate objects whose motion trajectory is partly uncontrollable. Such tasks require a specific type of reward that would reflect this temporal misalignment. We propose a novel method, based on a delayed reward redistribution, that allows a robot to fulfill goals in an only partially controllable environment. The reward system in our architecture combines information from other sensors together with inputs from an unsupervised vision module based on a variational autoencoder (VAE). This delayed reward system then controls the training of the motor module based on a Soft Actor-Critic (SAC) neural network. We compare results for a delayed and nondelayed version of our system in a simulated environment and show that the delayed reward greatly outperforms the nondelayed version.","2251-2454","978-1-6654-4986-1","10.1109/ICCAR52225.2021.9463495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463495","reinforcement learning;credit assignment;unsupervised learning","Training;Heuristic algorithms;Neural networks;Tactile sensors;Reinforcement learning;Sensor systems;Trajectory","dexterous manipulators;manipulator dynamics;mobile robots;motion control;neural nets;robot vision;trajectory control;unsupervised learning","RL;nonprehensile manipulation tasks;motion trajectory;temporal misalignment;delayed reward redistribution;partially controllable environment;unsupervised vision module;delayed reward system;delayed version;nondelayed version;dynamic nonprehensile manipulation;soft actor-critic neural network;variational autoencoder","","","","27","IEEE","25 Jun 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Autonomous Racing Car Control With Priori Knowledge","Z. Lu; C. Zhang; H. Zhang; Z. Wang; C. Huang; Y. Ji","College of Electronic and Information Engineering, Tongji University, Shanghai, China; College of Electronic and Information Engineering, Tongji University, Shanghai, China; College of Electronic and Information Engineering, Tongji University, Shanghai, China; College of Electronic and Information Engineering, Tongji University, Shanghai, China; College of Electronic and Information Engineering, Tongji University, Shanghai, China; College of Transportation Engineering, Tongji University, Shanghai, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","2241","2246","In the community of artificial intelligence, re-searchers have devoted much effort to the application of deep reinforcement learning algorithms for autonomous driving. Under deep reinforcement learning framework, it is important for the racing car agent to interact with its external environment to accumulate enough driving experience. However, the inter-action process is usually inefficient, risky and time-consuming. Furthermore, it is a common problem in relevant studies that brake policy is difficult to master. In this paper, we adopt some priori knowledge about vehicle dynamics to design the brake force and update it to the actor-critic network by soft-learning strategy. In addition, some effective strategies are developed to improve the training efficiency and control performance. The Open Racing Car Simulator(TORCS) is adopted to evaluate our algorithm. The simulation results show the effectiveness of our proposed algorithm with better learning efficiency, robustness and generalization performance.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728289","Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728289","autonomous racing car;deep reinforcement learning;priori knowledge;TD3","Training;Knowledge engineering;Smoothing methods;Simulation;Reinforcement learning;Stability analysis;Robustness","automobiles;computer games;learning (artificial intelligence);vehicle dynamics","deep reinforcement learning algorithms;autonomous driving;deep reinforcement learning framework;racing car agent;external environment;driving experience;inter-action process;brake policy;priori knowledge;brake force;actor-critic network;soft-learning strategy;control performance;learning efficiency;autonomous Racing Car control;artificial intelligence","","","","18","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Trajectory Planning for Capacitated Vehicle Routing Problem: A Deep Reinforcement Learning Approach","X. Fan; Y. Zhou; C. -Y. Liu","School of Mathematics, Southeast University No.2 Southeast University Road, Nanjing, Jiangsu, China; School of Cyber Science and Engineering, Southeast University No.2 Southeast University Road, Nanjing, Jiangsu, China; School of Cyber Science and Engineering, Southeast University No.2 Southeast University Road, Nanjing, Jiangsu, China","2023 IEEE International Conference on Mechatronics and Automation (ICMA)","22 Aug 2023","2023","","","760","765","Recently, deep reinforcement learning has been frequently used to deal with vehicle routing problems (VRPs), where the learned policy guides the selection of the next node to be visited. However, such a node-output strategy is not adequate for handling vehicle trajectory planning in complex road networks. To address this challenge, this article investigates the trajectory planning problem of automated guided vehicles (AGVs) with limited capacity and energy in a specific road network, which is a typical variant of VRP. We propose a new algorithm that integrates convolutional neural network (CNN) and double deep Q-learning, where CNN is used to extract the map information. Numerous experimental results indicate that the proposed method can make AGV effectively balance customer requirements, obstacle avoidance and path minimization.","2152-744X","979-8-3503-2084-8","10.1109/ICMA57826.2023.10215879","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10215879","Capacitated vehicle routing problem;trajectory planning;automated guided vehicle;deep reinforcement learning","Deep learning;Q-learning;Remotely guided vehicles;Mechatronics;Trajectory planning;Roads;Vehicle routing","automatic guided vehicles;collision avoidance;convolutional neural nets;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;reinforcement learning;road traffic;road vehicles;traffic engineering computing;transportation;vehicle routing","AGV;automated guided vehicles;capacitated vehicle routing problem;CNN;complex road networks;convolutional neural network;deep reinforcement learning;double deep Q-learning;learned policy guides;node-output strategy;specific road network;trajectory planning problem;vehicle routing problems;vehicle trajectory;VRPs","","","","14","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Model-Based Reinforcement Learning with LSTM Networks for Non-Prehensile Manipulation Planning","J. Fong; D. Campolo; C. Acar; K. P. Tee","School of Computing, National University of Singapore, Singapore; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research (A * STAR), Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research (A * STAR), Singapore","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","1152","1158","Solving non-prehensile manipulation tasks requires domain knowledge involving various interactions such as switching contact dynamics between the robot and the object, and the object-environment interactions. This results in a switched nonlinear dynamic system governing the physical interactions between the object and the environment. In this paper, we propose an interactive learning framework that allows a robot to autonomously learn and model an unknown object's dynamics, as well as utilise the learned model for efficient planning in completing re-positioning tasks using non-prehensile manipulation. First, we model the overall object dynamics using a Long Short-Term Memory (LSTM) neural network. We then assimilate the learned model into the Monte Carlo Tree Search (MCTS) algorithm with a dense reward function to generate an optimal sequence of push actions for task completion. We demonstrate the framework in both simulated and real robot that pushes objects on a table.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649940","Agency for Science, Technology and Research(grant numbers:A19E4a0101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649940","Manipulation planning;non-prehensile;reinforcement learning","Heuristic algorithms;Neural networks;Transfer learning;Switches;Reinforcement learning;Planning;Nonlinear dynamical systems","learning (artificial intelligence);manipulators;Monte Carlo methods;neural nets;nonlinear dynamical systems;path planning;tree searching","Monte Carlo Tree Search algorithm;task completion;model-based reinforcement learning;LSTM networks;nonprehensile manipulation planning;solving nonprehensile manipulation tasks;domain knowledge;contact dynamics;object-environment interactions;switched nonlinear dynamic system;physical interactions;interactive learning framework;unknown object;efficient planning;re-positioning tasks;object dynamics;Long Short-Term Memory neural network","","","","23","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Concurrent Carbon Footprint Reduction (C2FR) Reinforcement Learning Approach for Sustainable Data Center Digital Twin","S. Sarkar; A. Naug; A. Guillen; R. L. Gutierrez; S. Ghorbanpour; S. Mousavi; A. R. Babu; V. Gundecha","Hewlett Packard Labs at Hewlett Packard Enterprise, USA; Hewlett Packard Labs at Hewlett Packard Enterprise, USA; Hewlett Packard Labs at Hewlett Packard Enterprise, USA; Hewlett Packard Labs at Hewlett Packard Enterprise, USA; Hewlett Packard Labs at Hewlett Packard Enterprise, USA; Hewlett Packard Labs at Hewlett Packard Enterprise, USA; Hewlett Packard Labs at Hewlett Packard Enterprise, USA; Hewlett Packard Labs at Hewlett Packard Enterprise, USA","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","8","In recent years, the increasing emphasis on sustainability and carbon footprint reduction has required the exploration of innovative optimization techniques for data center operators. In this paper, we introduce a Concurrent Carbon Footprint Reduction (C2FR) Reinforcement Learning framework, designed to optimize data center energy consumption, load shifting, and battery operation decisions in real time. The C2FR framework utilizes short-term forecasts and incorporates Reinforcement Learning Energy ($A_{E}$), Battery ($A_{BAT}$) and Load-Shifting ($A_{LS}$) agents to optimize and effectively manage the intricate dependencies and information exchange between these individual optimization strategies, thus overcoming the limitations of existing isolated approaches. When compared to state-of-the-art algorithms, the C2FR framework demonstrates its effectiveness across various data center scenarios. The AE agent achieves a 7.9% reduction in pollutant emissions and a 7.8% reduction in energy cost on average. Moreover, the C2FR framework enables further emission reductions through the application of the battery and load-shifting optimization, leading to a total reduction of 10.17% in pollutant emissions on average over different data center configurations. This highlights the potential of the C2FR framework in addressing data center sustainability challenges and improving real-time carbon footprint optimization.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260633","","Data centers;Energy consumption;Reinforcement learning;Carbon dioxide;Real-time systems;Batteries;Sustainable development","computer centres;digital twins;energy consumption;power aware computing;reinforcement learning","battery operation decisions;C2FR framework;Concurrent Carbon Footprint Reduction Reinforcement Learning Approach;Concurrent Carbon Footprint Reduction Reinforcement Learning framework;data center energy consumption;data center operators;data center scenarios;data center sustainability challenges;different data center configurations;emission reductions;improving real-time carbon footprint optimization;individual optimization strategies;innovative optimization techniques;load-shifting optimization;Reinforcement Learning Energy;sustainable data center digital twin;total reduction","","","","24","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-based Dynamic Bandwidth Allocation in Weighted Fair Queues of Routers","J. Pan; G. Chen; H. Wu; X. Peng; L. Xia","School of Business, Sun Yat-Sen University, Guangzhou, P. R. China; School of Management, Guangzhou University, Guangzhou, P. R. China; School of Business, Sun Yat-Sen University, Guangzhou, P. R. China; Theory Lab, Hong Kong Research Center, Huawei Technologies Co. Ltd., Hong Kong, China; School of Business, Sun Yat-Sen University, Guangzhou, P. R. China","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","1580","1587","Motivated by a real problem of service mechanism in the output port of routers, this paper studies the dynamic bandwidth allocation in a G/G/1/K parallel queueing system, where weighted fair queueing (WFQ) scheduling discipline is applied to support differentiated services for different packet queues. The bursty and complicated characteristics of Internet traffic pose a challenge on the analytic solution for dynamic bandwidth allocation, which requires distributional information of traffic patterns. Since the distributional information of Internet traffic is always unavailable and varied with time, we propose a deep reinforcement learning (DRL) framework to train a bandwidth controller by adaptively interacting with the environment. The controller dynamically allocates bandwidth weights among multiple queues according to the instant queue lengths observed. We train the controller with two advanced DRL algorithms, DDPG and SAC, respectively. With real traffic data, experiment results show that our trained controllers achieve a lower average delay and packet loss rate than a rule-based policy. Our proposed WFQ-DRL algorithm is a first attempt to apply RL algorithms in real scenarios of routers, where the system has eight or more queues and a diversity of real traffic without Poisson assumption is applicable.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926628","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926628","","Computer aided software engineering;Heuristic algorithms;Packet loss;Bandwidth;Reinforcement learning;Channel allocation;Traffic control","bandwidth allocation;deep learning (artificial intelligence);Internet;queueing theory;reinforcement learning;telecommunication congestion control;telecommunication network routing;telecommunication traffic","multiple queues;instant queue;dynamic bandwidth allocation;weighted fair queues;queueing system;weighted fair queueing scheduling discipline;packet queues;Internet traffic;distributional information;deep reinforcement learning framework;bandwidth controller;bandwidth weights","","","","23","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"Monocular Reactive Collision Avoidance for MAV Teleoperation with Deep Reinforcement Learning","R. Brilli; M. Legittimo; F. Crocetti; M. Leomanni; M. L. Fravolini; G. Costante","Department of Engineering, University of Perugia, Perugia; Department of Engineering, University of Perugia, Perugia; Department of Engineering, University of Perugia, Perugia; Department of Engineering, University of Perugia, Perugia; Department of Engineering, University of Perugia, Perugia; Department of Engineering, University of Perugia, Perugia","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","12535","12541","Enabling Micro Aerial Vehicles (MAVs) with semi-autonomous capabilities to assist their teleoperation is crucial in several applications. Remote human operators do not have, in general, the situational awareness to perceive obstacles near the drone, nor the readiness to provide commands to avoid collisions. In this work, we devise a novel teleoperation setting that asks the operator to provide a simple high-level signal encoding the speed and the direction they expect the drone to follow. We then endow the MAV with an end-to-end Deep Reinforcement Learning (DRL) model that computes control commands to track the desired trajectory while performing collision avoidance. Differently from State-of-the-Art (SotA) works, it allows the robot to move freely in the 3D space, requires only the current RGB image captured by a monocular camera and the current robot position, and does not make any assumption about obstacle shape and size. We show the effectiveness and the generalization capabilities of our strategy by comparing it against a SotA baseline in photorealistic simulated environments.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160427","University of Perugia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160427","","Deep learning;Three-dimensional displays;Tracking;Shape;Robot vision systems;Reinforcement learning;Trajectory","autonomous aerial vehicles;cameras;collision avoidance;control engineering computing;deep learning (artificial intelligence);image capture;image colour analysis;mobile robots;reinforcement learning;robot vision;telerobotics","current RGB image;current robot position;Deep Reinforcement Learning;drone;Enabling MicroAerial Vehicles;end-to-end Deep Reinforcement;generalization capabilities;human operators;MAV teleoperation;monocular camera;monocular reactive collision avoidance;novel teleoperation setting;obstacle shape;semiautonomous capabilities;simple high-level signal;situational awareness","","","","41","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Efficient Planning of Multi-Robot Collective Transport using Graph Reinforcement Learning with Higher Order Topological Abstraction","S. Paul; W. Li; B. Smyth; Y. Chen; Y. Gel; S. Chowdhury","Department of Mechanical and Aerospace Engineering, University at Buffalo, Buffalo, NY, USA; Department of Mechanical and Aerospace Engineering, University at Buffalo, Buffalo, NY, USA; Department of Mechanical and Aerospace Engineering, University at Buffalo, Buffalo, NY, USA; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA; Department of Mathematical Sciences, University of Texas at Dallas, Dallas, TX, USA; Department of Mechanical and Aerospace Engineering, University at Buffalo, Buffalo, NY, USA","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","5779","5785","Efficient multi-robot task allocation (MRTA) is fundamental to various time-sensitive applications such as disaster response, warehouse operations, and construction. This paper tackles a particular class of these problems that we call MRTA-collective transport or MRTA-CT - here tasks present varying workloads and deadlines, and robots are subject to flight range, communication range, and payload constraints. For large instances of these problems involving 100s-1000's of tasks and 10s-100s of robots, traditional non-learning solvers are often time-inefficient, and emerging learning-based policies do not scale well to larger-sized problems without costly retraining. To address this gap, we use a recently proposed encoder-decoder graph neural network involving Capsule networks and multi-head attention mechanism, and innovatively add topological descriptors (TD) as new features to improve transferability to unseen problems of similar and larger size. Persistent homology is used to derive the TD, and proximal policy optimization is used to train our TD-augmented graph neural network. The resulting policy model compares favorably to state-of-the-art non-learning baselines while being much faster. The benefit of using TD is readily evident when scaling to test problems of size larger than those used in training.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161517","National Science Foundation(grant numbers:CMMI 2048020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161517","","Training;Scalability;Reinforcement learning;Graph neural networks;Real-time systems;Planning;Resource management","control engineering computing;deep learning (artificial intelligence);graph neural networks;multi-robot systems;optimisation;path planning;reinforcement learning","capsule networks;disaster response;efficient multirobot task allocation;efficient planning;encoder-decoder graph neural network;flight range;graph reinforcement learning;higher order topological abstraction;learning-based policies;MRTA-collective transport;MRTA-CT;multihead attention mechanism;multirobot collective transport;nonlearning baselines;nonlearning solvers;payload constraints;persistent homology;policy model;proximal policy optimization;TD-augmented graph neural network;time 10.0 s to 100.0 s;time 100.0 s;time-sensitive applications;topological descriptors;traditional nonlearning solvers;warehouse operations","","","","41","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Robotic Control of the Deformation of Soft Linear Objects Using Deep Reinforcement Learning","M. H. Daniel Zakaria; M. Aranda; L. Lequièvre; S. Lengagne; J. A. Corrales Ramón; Y. Mezouar","CNRS, Clermont Auvergne INP, Institut Pascal, Université Clermont Auvergne, Clermont-Ferrand, France; Instituto de Investigación en Ingeniería de Aragon, Universidad de Zaragoza, Zaragoza, Spain; CNRS, Clermont Auvergne INP, Institut Pascal, Université Clermont Auvergne, Clermont-Ferrand, France; CNRS, Clermont Auvergne INP, Institut Pascal, Université Clermont Auvergne, Clermont-Ferrand, France; Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Universidade de Santiago de Compostela, Santiago de Compostela, Spain; CNRS, Clermont Auvergne INP, Institut Pascal, Université Clermont Auvergne, Clermont-Ferrand, France","2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)","28 Oct 2022","2022","","","1516","1522","This paper proposes a new control framework for manipulating soft objects. A Deep Reinforcement Learning (DRL) approach is used to make the shape of a deformable object reach a set of desired points by controlling a robotic arm which manipulates it. Our framework is more easily generalizable than existing ones: it can work directly with different initial and desired final shapes without need for relearning. We achieve this by using learning parallelization, i.e., executing multiple agents in parallel on various environment instances. We focus our study on deformable linear objects. These objects are interesting in industrial and agricultural domains, yet their manipulation with robots, especially in 3D workspaces, remains challenging. We simulate the entire environment, i.e., the soft object and the robot, for the training and the testing using PyBullet and OpenAI Gym. We use a combination of state-of-the-art DRL techniques, the main ingredient being a training approach for the learning agent (i.e., the robot) based on Deep Deterministic Policy Gradient (DDPG). Our simulation results support the usefulness and enhanced generality of the proposed approach.","2161-8089","978-1-6654-9042-9","10.1109/CASE49997.2022.9926667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926667","","Training;Three-dimensional displays;Computer aided software engineering;Shape;Service robots;Simulation;Reinforcement learning","learning (artificial intelligence);manipulator dynamics;manipulators;multi-agent systems","robotic control;soft linear objects;Deep Reinforcement Learning;control framework;soft object;deformable object;desired points;robotic arm;final shapes;learning parallelization;multiple agents;environment instances;deformable linear objects;industrial domains;agricultural domains;state-of-the-art DRL techniques;training approach;learning agent;Deep Deterministic Policy Gradient","","","","34","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"Depth Control of Cable Patrol Autonomous Underwater Vehicle Based on Reinforcement Learning","Y. Zhang; X. Zhang; F. Zeng; S. Sun; J. Liu; Z. Zhou; C. Guo; D. Wang","School of Electronics and Information, Hangzhou Dianzi University, Hangzhou, China; School of Mechanical Engineering, Hangzhou Dianzi University, Hangzhou, China; School of Mechanical Engineering, Hangzhou Dianzi University, Hangzhou, China; School of Mechanical Engineering, Hangzhou Dianzi University, Hangzhou, China; School of Mechanical Engineering, Hangzhou Dianzi University, Hangzhou, China; School of Electronics and Information, Hangzhou Dianzi University, Hangzhou, China; School of Electronics and Information, Hangzhou Dianzi University, Hangzhou, China; School of Mechanical Engineering, Hangzhou Dianzi University, Hangzhou, China","2023 WRC Symposium on Advanced Robotics and Automation (WRC SARA)","27 Sep 2023","2023","","","534","540","In order to improve the depth control method of underwater vehicle and reduce the interference and influence of external factors such as currents, this paper analyzes the motion control and dynamics modeling for an independently designed cable patrol autonomous underwater vehicle(CP-AUV), and proposes a control algorithm for CP-AUV that integrates the Actor-Critic algorithm and PID control. The algorithm uses traditional PID control as the controller of the CP-AUV and uses the Actor-Critic algorithm to adjust the PID control parameters to make it better adapted to different disturbances and different adjustment depths. The effectiveness of the method is verified by simulation, and compared with the traditional PID control, the reinforcement learning-based PID control retains the simple and practical advantages of PID, but can also adjust the parameters according to the motion environment, which enhances the robustness of the algorithm and the accuracy of parameter tracking while improving the control accuracy.","2835-3358","979-8-3503-0732-0","10.1109/WRCSARA60131.2023.10261868","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261868","","Underwater cables;PI control;Tracking;Heuristic algorithms;Reinforcement learning;Interference;Software","autonomous underwater vehicles;motion control;three-term control;underwater vehicles","Actor-Critic algorithm;control accuracy;control algorithm;CP-AUV;depth control method;different adjustment depths;independently designed cable patrol autonomous underwater vehicle;motion control;PID control parameters;reinforcement learning-based PID control;traditional PID control","","","","15","IEEE","27 Sep 2023","","","IEEE","IEEE Conferences"
"Reinforcement learning based radio resource scheduling in LTE-advanced","I. S. Comşa; M. Aydin; S. Zhang; P. Kuonen; J. -F. Wagen","Institute of Information and Communication Technologies, University of Applied Sciences of Western Switzerland, Fribourg, Switzerland; Institute of Information and Communication Technologies, University of Applied Sciences of Western Switzerland, Fribourg, Switzerland; Institute of Research in Applicable Computing, University of Bedfordshire, Luton, UK; Institute of Information and Communication Technologies, University of Applied Sciences of Western Switzerland, Fribourg, Switzerland; Institute of Information and Communication Technologies, University of Applied Sciences of Western Switzerland, Fribourg, Switzerland","The 17th International Conference on Automation and Computing","21 Nov 2011","2011","","","219","224","In this paper, a novel radio resource scheduling policy for Long Term Evolution Advanced (LTE-A) radio access technology in downlink acceptance is proposed. The scheduling process works with dispatching rules which are various with different behaviors. In the literature, the scheduling disciplines are applied for the entire transmission sessions and the scheduler performance strongly depends on the exploited discipline. Our method provides a straightforward schedule within transmission time interval (TTI) frame. Hence, a mixture of disciplines can be used for each TTI instead of the single one adopted across the whole transmission. The grand objective is to bring real improvements in terms of system throughput, system capacity and spectral efficiency (operator benefit) assuring in the same time the best user fairness and Quality of Services (QoS) capabilities (user benefit). In order to meet this objective, each rule must to be called on the best matching conditions. The policy adoption and refinement are the best way to optimize the use of mixture of rules. The Q-III reinforcement learning algorithm is proposed for the policy adoption in order to transform the scheduling experiences into a permanent nature, facilitating the decision-making on which rules will be used for each TTI. The IQ-III reinforcement learning algorithm using multi-agent environments refines the policy adoption by considering the agents' opinions in order to reduce the policy convergence time.","","978-1-86218-098-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6084930","LTE-A;TTI;scheduling rule;policy adoption;Q-III learning;IQ-III reinforcement learning;multi agent systems","Throughput;Dynamic scheduling;Resource management;Scheduling algorithm;Indexes;Quality of service","decision making;learning (artificial intelligence);Long Term Evolution;mobile computing;multi-agent systems;optimisation;quality of service;radio access networks","radio resource scheduling;LTE-advanced;Long Term Evolution;radio access technology;dispatching rules;transmission time interval;quality of service;optimization;decision making;TTI;IQ-III reinforcement learning algorithm;multi-agent system;Q-III reinforcement learning","","","","21","","21 Nov 2011","","","IEEE","IEEE Conferences"
"Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing Local and Remote Computers","Y. Wang; G. Vasan; A. R. Mahmood","Department of Computing Science, University of Alberta, Edmonton, AB., Canada; Department of Computing Science, University of Alberta, Edmonton, AB., Canada; Department of Computing Science, University of Alberta, Edmonton, AB., Canada","2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","9435","9441","Real-time learning is crucial for robotic agents adapting to ever-changing, non-stationary environments. A common setup for a robotic agent is to have two different computers simultaneously: a resource-limited local computer tethered to the robot and a powerful remote computer connected wirelessly. Given such a setup, it is unclear to what extent the performance of a learning system can be affected by resource limitations and how to efficiently use the wirelessly connected powerful computer to compensate for any performance loss. In this paper, we implement a real-time learning system called the Remote-Local Distributed (ReLoD) system to distribute computations of two deep reinforcement learning (RL) algorithms, Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO), between a local and a remote computer. The performance of the system is evaluated on two vision-based control tasks developed using a robotic arm and a mobile robot. Our results show that SAC's performance degrades heavily on a resource-limited local computer. Strikingly, when all computations of the learning system are deployed on a remote workstation, SAC fails to compensate for the performance loss, indicating that, without careful consideration, using a powerful remote computer may not result in performance improvement. However, a carefully chosen distribution of computations of SAC consistently and substantially improves its performance on both tasks. On the other hand, the performance of PPO remains largely unaffected by the distribution of computations. In addition, when all computations happen solely on a powerful tethered computer, the performance of our system remains on par with an existing system that is well-tuned for using a single machine. ReLoD is the only publicly available system for real-time RL that applies to multiple robots for vision-based tasks. The source code can be found at https://github.com/rlai-lab/relod","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10160684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160684","","Computers;Learning systems;Source coding;Reinforcement learning;Real-time systems;Remote working;Mobile robots","deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;multi-robot systems;optimisation;reinforcement learning;robot vision","deep reinforcement learning algorithms;different computers;mobile robot;multiple robots;performance improvement;powerful computer;powerful remote computer;powerful tethered computer;real-time learning system;Remote computers;remote workstation;Remote-Local Distributed system;resource-limited local computer;robotic agent;robotic arm;time reinforcement learning;vision-based control tasks;vision-based robotics utilizing Local;vision-based tasks","","","","23","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"Reinforcement Q-learning for Closed-loop Hypnosis Depth Control in Anesthesia","G. Calvi; E. Manzoni; M. Rampazzo","Department of Information Engineering, Padova, Italy; Department of Information Engineering, Padova, Italy; Department of Information Engineering, Padova, Italy","2022 30th Mediterranean Conference on Control and Automation (MED)","1 Aug 2022","2022","","","164","169","The management of anesthesia is one of the most critical challenges since millions of subjects undergo surgeries every day. For instance, providing an inaccurate drug dose to the patient may entail adverse effects and postoperative complications. In this context, the use of computer-controlled drug dosing systems nowadays available provides significant ad-vantages to effectively regulate anesthesia. This paper presents the control of the depth of hypnosis in anesthesia through a Machine Learning approach, i.e. a Q-learning, that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences. The design and performance evaluation of the proposed solution are done in silico by exploiting an Open-source Patient Simulator, that can describe the main relevant system characteristics.","2473-3504","978-1-6654-0673-4","10.1109/MED54222.2022.9837170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837170","","Drugs;Performance evaluation;Q-learning;Surgery;Machine learning;Maintenance engineering;Anesthesia","closed loop systems;drug delivery systems;drugs;learning (artificial intelligence);medical computing;self-adjusting systems;surgery","reinforcement Q-learning;closed-loop hypnosis depth control;anesthesia;surgery;drug dose;postoperative complications;computer-controlled drug;machine learning approach;interactive environment;open-source patient simulator","","","","20","IEEE","1 Aug 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Next-Best-View Planning in Agricultural Applications","X. Zeng; T. Zaenker; M. Bennewitz","University of Bonn, Germany; University of Bonn, Germany; University of Bonn, Germany","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","2323","2329","Automated agricultural applications, i.e., fruit picking require spatial information about crops and, especially, their fruits. In this paper, we present a novel deep reinforcement learning (DRL) approach to determine the next best view for automatic exploration of 3D environments with a robotic arm equipped with an RGB-D camera. We process the obtained images into an octree with labeled regions of interest (ROIs), i.e., fruits. We use this octree to generate 3D observation maps that serve as encoded input to the DRL network. We hereby do not only rely on known information about the environment, but explicitly also represent information about the unknown space to force exploration. Our network takes as input the encoded 3D observation map and the temporal sequence of camera view pose changes, and outputs the most promising camera movement direction. Our experimental results show an improved ROI targeted exploration performance resulting from our learned network in comparison to a state-of-the-art method.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9811800","Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy(grant numbers:EXC 2070 - 390732324); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811800","","Three-dimensional displays;Robot vision systems;Octrees;Force;Crops;Reinforcement learning;Cameras","agriculture;cameras;image segmentation;image sensors;learning (artificial intelligence);mobile robots;octrees;robot vision","DRL network;known information;encoded 3D observation map;camera view;promising camera movement direction;improved ROI targeted exploration performance;learned network;next-best-view planning;automated agricultural applications;fruit picking;spatial information;crops;fruits;deep reinforcement learning approach;automatic exploration;robotic arm;RGB-D camera;octree;labeled regions;3D observation maps;encoded input","","","","31","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Visual Backtracking Teleoperation: A Data Collection Protocol for Offline Image-Based Reinforcement Learning","D. Brandfonbrener; S. Tu; A. Singh; S. Welker; C. Boodoo; N. Matni; J. Varley",New York University; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google,"2023 IEEE International Conference on Robotics and Automation (ICRA)","4 Jul 2023","2023","","","11336","11342","We consider how to most efficiently leverage teleoperator time to collect data for learning robust image-based value functions and policies for sparse reward robotic tasks. To accomplish this goal, we modify the process of data collection to include more than just successful demonstrations of the desired task. Instead we develop a novel protocol that we call Visual Backtracking Teleoperation (VBT), which deliberately collects a dataset of visually similar failures, recoveries, and successes. VBT data collection is particularly useful for efficiently learning accurate value functions from small datasets of image-based observations. We demonstrate VBT on a real robot to perform continuous control from image observations for the deformable manipulation task of T-shirt grasping. We find that by adjusting the data collection process we improve the quality of both the learned value functions and policies over a variety of baseline methods for data collection. Specifically, we find that offline reinforcement learning on VBT data outperforms standard behavior cloning on successful demonstration data by 13 % when both methods are given equal-sized datasets of 60 minutes of data from the real robot.","","979-8-3503-2365-8","10.1109/ICRA48891.2023.10161096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161096","","Visualization;Backtracking;Teleoperators;Protocols;Reinforcement learning;Grasping;Data collection","control engineering computing;data analysis;manipulators;mobile robots;reinforcement learning;robot vision;telecontrol;telerobotics","data collection protocol;deformable manipulation task;image-based observations;learned value functions;leverage teleoperator time;offline image-based reinforcement learning;robust image-based value functions;sparse reward robotic tasks;T-shirt grasping;VBT data collection;visual backtracking teleoperation","","","","25","IEEE","4 Jul 2023","","","IEEE","IEEE Conferences"
"RAPID-RL: A Reconfigurable Architecture with Preemptive-Exits for Efficient Deep-Reinforcement Learning","A. K. Kosta; M. A. Anwar; P. Panda; A. Raychowdhury; K. Roy","Purdue University, West Lafayette, IN, USA; Georgia Institute of Technology, Atlanta, GA, USA; Yale University, New Haven, CT, USA; Georgia Institute of Technology, Atlanta, GA, USA; Purdue University, West Lafayette, IN, USA","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","7492","7498","Present-day Deep Reinforcement Learning (RL) systems show great promise towards building intelligent agents surpassing human-level performance. However, the computational complexity associated with the underlying deep neural networks (DNNs) leads to power-hungry implementations. This makes deep RL systems unsuitable for deployment on resource-constrained edge devices. To address this challenge, we propose a reconfigurable architecture with preemptive exits for effi-cient deep RL (RAPID-RL). RAPID-RL enables conditional activation of DNN layers based on the difficulty level of inputs. This allows to dynamically adjust the compute effort during inference while maintaining competitive performance. We achieve this by augmenting a deep Q-network (DQN) with side-branches capable of generating intermediate predictions along with an associated confidence score. We also propose a novel training methodology for learning the actions and branch confidence scores in a dynamic RL setting. Our experiments evaluate the proposed framework for Atari 2600 gaming tasks and a realistic Drone navigation task on an open-source drone simulator (PEDRA). We show that RAPID-RL incurs 0.34 × (0.25 ×) number of operations (OPS) while maintaining performance above 0.88 × (0.91 ×) on Atari (Drone navigation) tasks, compared to a baseline-DQN without any side-branches. The reduction in OPS leads to fast and efficient inference, proving to be highly beneficial for the resource-constrained edge where making quick decisions with minimal compute is essential.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812320","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812320","","Performance evaluation;Training;Q-learning;Navigation;Neural networks;Reconfigurable architectures;Task analysis","computational complexity;computer games;learning (artificial intelligence);neural nets;reconfigurable architectures","branch confidence scores;dynamic RL setting;RAPID-RL incurs;Atari tasks;reconfigurable architecture;preemptive-exits;efficient Deep-Reinforcement Learning;Present-day Deep Reinforcement;intelligent agents surpassing human-level performance;underlying deep neural networks;power-hungry implementations;deep RL systems;resource-constrained edge devices;preemptive exits;effi-cient deep RL;deep Q-network;associated confidence score","","","","28","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Multi-agent Exploration with Reinforcement Learning","A. Sygkounas; D. Tsipianitis; G. Nikolakopoulos; C. P. Bechlioulis","Department of Electrical and Computer Engineering, University of Patras, Greece; Department of Electrical and Computer Engineering, University of Patras, Greece; Department of Robotics and AI, Lulea University of Technology, Sweden; Department of Electrical and Computer Engineering, University of Patras, Greece","2022 30th Mediterranean Conference on Control and Automation (MED)","1 Aug 2022","2022","","","630","635","Modern robots are used in many exploration, search and rescue applications nowadays. They are essentially coordinated by human operators and collaborate with inspection or rescue teams. Over time, robots (agents) have become more sophisticated with more autonomy, operating in complex environments. Therefore, the purpose of this paper is to present an approach for autonomous multi-agent coordination for exploring and covering unknown environments. The method we suggest combines reinforcement learning with multiple neural networks (Deep Learning) to plan the path for each agent separately and achieve collaborative behavior amongst them. Specifically, we have applied two recent techniques, namely the target neural network and the prioritized experience replay, which have been proven to stabilize and accelerate the training process. Agents should also avoid obstacles (walls, objects, etc.) throughout the exploration without prior information/knowledge about the environment; thus we use only local information available at any time instant to make the decision of each agent. Furthermore, two neural networks are used for generating actions, accompanied by an extra neural network with a switching logic that chooses one of them. The exploration of the unknown environment is conducted in a two-dimensional model (2D) using multiple agents for various maps, ranging from small to large size. Finally, the efficiency of the exploration is investigated for a different number of agents and various types of neural networks.","2473-3504","978-1-6654-0673-4","10.1109/MED54222.2022.9837168","University of Patras; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837168","","Training;Deep learning;Robot kinematics;Neural networks;Two dimensional displays;Collaboration;Reinforcement learning","collision avoidance;control engineering computing;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;neural nets","multiagent exploration;modern robots;rescue applications;human operators;collaborate;inspection;complex environments;autonomous multiagent coordination;exploring covering unknown environments;combines reinforcement learning;multiple neural networks;collaborative behavior;target neural network;prioritized experience replay;extra neural network;unknown environment;multiple agents","","","","11","IEEE","1 Aug 2022","","","IEEE","IEEE Conferences"
"Optimization of Volume & Brightness of Android Smartphone through Clustering & Reinforcement Learning (“RE-IN”)","J. S. D. M. D. S. Abeywardhane; E. M. W. N. de Silva; I. G. A. G. S. Gallanga; L. N. Rathnayake; J. Wickramarathne; D. Sriyaratna","Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Sri Lanka Institute of Information Technology, Malabe, Sri Lanka","2018 IEEE International Conference on Information and Automation for Sustainability (ICIAfS)","28 Nov 2019","2018","","","1","6","Smartphone has become one of the most significant piece of technology that humans were able to produce in the 21st century. It has become our life companion; hence the features of the smartphones have developed in advance. But, some features may not work as expected. For instance, auto brightness changing feature is now actualized with smartphones, yet we alter the brightness according to our preference. In the same manner, considering the volume of our smartphone it doesn't change according to our preference subsequently. This research will develop a mobile application (“RE-IN”) to overcome this issue for Android smartphones. Since android smartphones allow accessing its hardware layer we can roll out improvements as we need, yet Apple doesn't permit to proceed with its hardware layer thus hard to do this for the iPhone users. By utilizing the RE-IN mobile application users may have to encounter an optimal brightness and volume on their Android smartphones agreeing the present condition of smartphone users are in. RE- IN application will keep running as a background application on an Android smartphone. When the client changes the brightness and volume as his/her preference. At that point, the reinforcement learning algorithm over the time application will distinguish how to control user's smartphone's brightness and volume relying upon the user's circumstance. When client surrounding is loaded with light, the framework will modify brightness for his/her preference. The client doesn't need to do this manually. Moreover when the client is at the too much boisterous place all of a sudden gets a call from someone; client's smartphone amplifier volume will change consequently and solaces the client's discussion. To actualize this framework it is relied upon to reinforcement learning and machine learning as the research area. By finishing the literature review, research group unable to find an Android mobile application which automates the process of volume and brightness of the Android smartphone as per user preference. After using the reinforcement learning algorithm to learn the data set then distribute the process, using client-server model and come up with a clustering algorithm(K-means algorithm) to share common attributes by considering geographical area which they live in and variables like age, gender, how they interact with the device etc. In addition, this system will identify abnormal behaviors of some particular users. RE-IN will identify the users who are keeping volume level to the highest and brightness level to its maximum and notify them in advance.","2151-1810","978-1-5386-9418-3","10.1109/ICIAFS.2018.8913391","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8913391","Reinforcement learning;Optimal brightness and volume;Clustering algorithm;Machine learning","Smart phones;Brightness;Reinforcement learning;Clustering algorithms;Mobile applications;Data models","client-server systems;learning (artificial intelligence);mobile computing;pattern clustering;smart phones;telecommunication computing","smartphone users;mobile application users;volume & brightness;Android smartphone;Android mobile application","","","","15","IEEE","28 Nov 2019","","","IEEE","IEEE Conferences"
"High-dimensional Video Caching Selection Method Based on Deep Reinforcement Learning","R. Chen; B. Zhang; J. Zhou; L. Zhao; F. Xiao","College of Computer, Nanjing University of Posts and Telecommunications Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks, Nanjing, China; College of Computer, Nanjing University of Posts and Telecommunications Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks, Nanjing, China; College of Computer, Nanjing University of Posts and Telecommunications Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks, Nanjing, China; College of Computer, Nanjing University of Posts and Telecommunications Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks, Nanjing, China; College of Computer, Nanjing University of Posts and Telecommunications Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks, Nanjing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","4431","4436","At present, the quantity and the quality of the videos are increasing, which results in more and more video traffic. The combination of video caching and edge computing can improve the performance of multimedia services system. However, the popularity of the videos changes over time, so video caching selection has the dynamic characteristic. Besides, the edge server needs to select some videos from a large number of videos for caching, so video caching selection has the high-dimensional characteristic. In order to reduce the time delay and the traffic cost, we propose a high-dimensional video caching selection method based on deep reinforcement learning. First, the system model of the video caching selection problem is constructed. Second, the video caching action is selected based on the improved deep deterministic policy gradient. Finally, simulation results show that the proposed method can further reduce the time delay of video transmission and the traffic cost of user expense, compared with similar methods.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10054728","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054728","Video Caching Selection;Deep Reinforcement Learning;Edge Computing;Deep Deterministic Policy Gradient;Decoder","Deep learning;Costs;Delay effects;Simulation;Reinforcement learning;Multimedia computing;Streaming media","deep learning (artificial intelligence);edge computing;gradient methods;reinforcement learning;video signal processing","deep reinforcement learning;edge computing;high-dimensional video caching selection method;multimedia services system;user expense;video caching action;video caching selection problem;video quality;video quantity;video traffic;video transmission","","","","24","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Approach for Non-homogeneous Patrolling using Wi-Fi Fleet-restricted Autonomous Vehicles","S. Y. Luis; D. G. Reina; S. T. Marín","dept. of Electronics, University of Seville, Seville, Spain; dept. of Electronics, University of Seville, Seville, Spain; dept. of Electronics, University of Seville, Seville, Spain","2022 2nd International Conference on Robotics, Automation and Artificial Intelligence (RAAI)","10 Apr 2023","2022","","","92","98","The use of intelligent autonomous vehicles to monitor natural phenomena involves the optimization of multiple policies that must comply with physical restrictions of the environment. In the patrolling problem, typically addressed in the environmental surveillance of natural scenarios, it is required to fulfill the non-homogeneous coverage of an unknown scalar map, with limitations of navigable areas and communication. This work presents a framework based on deep reinforcement learning to deal with communication restrictions for online route planning and patrolling with multiple vehicles. This algorithm, based on the Deep Q-Learning algorithm, using a customized reward function and a fleet-informed deep network, is able to optimize every vehicle policy to maintain each vehicle’s distance from another within the limits of its wireless communication protocol (WiFi). The results show better performance than other path planning heuristics, while being a model-free approach and providing an effective method to use in similar patrolling scenarios.","","978-1-6654-5944-0","10.1109/RAAI56146.2022.10092959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10092959","Deep-Reinforcement Learning;Patrolling;Multi-Agent;Wireless Communications","Deep learning;Wireless communication;Q-learning;Surveillance;Routing protocols;Path planning;Planning","deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;optimisation;path planning;reinforcement learning;wireless LAN","communication restrictions;customized reward function;Deep Q-Learning algorithm;deep reinforcement learning;environmental surveillance;fleet-informed deep network;intelligent autonomous vehicles;model-free approach;multiple policies;multiple vehicles;natural phenomena;natural scenarios;navigable areas;nonhomogeneous coverage;nonhomogeneous patrolling;online route planning;path planning heuristics;patrolling problem;physical restrictions;similar patrolling scenarios;unknown scalar map;vehicle policy;Wi-Fi Fleet-restricted Autonomous;wireless communication protocol","","","","19","IEEE","10 Apr 2023","","","IEEE","IEEE Conferences"
"Goal-Conditioned Reinforcement Learning With Disentanglement-Based Reachability Planning","Z. Qian; M. You; H. Zhou; X. Xu; B. He","College of Electronic and Information Engineering, Frontiers Science Center for Intelligent Autonomous Systems, Tongji University, Shanghai, China; College of Electronic and Information Engineering, Frontiers Science Center for Intelligent Autonomous Systems, Tongji University, Shanghai, China; College of Electronic and Information Engineering, Frontiers Science Center for Intelligent Autonomous Systems, Tongji University, Shanghai, China; College of Electronic and Information Engineering, Frontiers Science Center for Intelligent Autonomous Systems, Tongji University, Shanghai, China; College of Electronic and Information Engineering, Frontiers Science Center for Intelligent Autonomous Systems, Tongji University, Shanghai, China","IEEE Robotics and Automation Letters","29 Jun 2023","2023","8","8","4721","4728","Goal-Conditioned Reinforcement Learning (GCRL) can enable agents to spontaneously set diverse goals to learn a set of skills. Despite the excellent works proposed in various fields, reaching distant goals in temporally extended tasks remains a challenge for GCRL. Current works tackled this problem by leveraging planning algorithms to plan intermediate subgoals to augment GCRL. Their methods need two crucial requirements: (i) A state representation space to search valid subgoals, and (ii) a distance function to measure the reachability of subgoals. However, they struggle to scale to high-dimensional state space due to their non-compact representations. Moreover, they cannot collect high-quality training data through standard GC policies, which results in an inaccurate distance function. Both affect the efficiency and performance of planning and policy learning. In the letter, we propose a goal-conditioned RL algorithm combined with Disentanglement-based Reachability Planning (REPlan) to solve temporally extended tasks. In REPlan, a Disentangled Representation Module (DRM) is proposed to learn compact representations which disentangle robot poses and object positions from high-dimensional observations in a self-supervised manner. A simple Reachability Discrimination Module (REM) is also designed to determine the temporal distance of subgoals. Moreover, REM computes intrinsic bonuses to encourage the collection of novel states for training. We evaluate our REPlan in three vision-based simulation tasks and one real-world task. The experiments demonstrate that our REPlan significantly outperforms the prior state-of-the-art methods in solving temporally extended tasks.","2377-3766","","10.1109/LRA.2023.3287362","National Natural Science Foundation of China(grant numbers:62073244); Shanghai Innovation Action Plan(grant numbers:20511100500); Innovation Program of Shanghai Municipal Education Commission(grant numbers:202101070007E00098); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155203","Goal-conditioned reinforcement learning;disentangled representation;reachability measure","Task analysis;Planning;Robots;Computational modeling;Training;Training data;Reinforcement learning","learning (artificial intelligence);path planning;reinforcement learning;supervised learning","compact representations;current works;Disentangled Representation Module;Disentanglement-based Reachability Planning;distant goals;diverse goals;excellent works;GCRL;goal-Conditioned Reinforcement Learning;Goal-Conditioned Reinforcement Learning;goal-conditioned RL algorithm;high-dimensional observations;high-dimensional state space;high-quality training data;inaccurate distance function;intermediate subgoals;leveraging planning algorithms;noncompact representations;policy learning;REPlan;simple Reachability Discrimination Module;state representation space;temporally extended tasks;valid subgoals;vision-based simulation tasks","","","","33","IEEE","19 Jun 2023","","","IEEE","IEEE Journals"
"Motion Simulation of Flying Quadruped Robot Based on Deep Reinforcement Learning","H. Zhou; Z. Dong; P. Zhai; L. Zhang","Academy for Engineering & Technology, Fudan University, Shanghai, China; Shanghai Engineering Research Center of AI & Robotics, Fudan University, Shanghai, China; Academy for Engineering & Technology, Fudan University, Shanghai, China; Shanghai Engineering Research Center of AI & Robotics, Fudan University, Shanghai, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","6747","6752","Traditional motion control methods for fixed-profile aircraft are difficult to satisfy the requirements of flying quadruped robot. However, the emergence of artificial intelligence provides new ideas for the design of variant aircraft systems. During the deformed flight, the flying quadruped robot is extremely susceptible to various internal and external disturbances. Therefore, it is challenging to ensure its stability. On this basis, the motion problem of flying quadruped robot is investigated by employing the new deep reinforcement learning technology. The twin delayed deep deterministic policy gradient algorithm (TD3) is adopted to acquire gait and flight strategy through interactive learning with the environment. In order to enhance the learning efficiency in the training process, the framework of actor-critic learning system is proposed. In the meanwhile, the extra domain-specific knowledge is applied to shape the reward function. In addition, simulation studies are also carried out to verify the performance of the proposed TD3-based controller.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728624","Science and Technology Commission of Shanghai Municipality; Guangxi University; Guangxi Normal University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728624","Deep reinforcement learning;flying robot;legged walking robot;motion simulation;twin delayed deep deterministic policy gradient","Legged locomotion;Training;Robot motion;Shape;Decision making;Reinforcement learning;Stability analysis","gradient methods;learning (artificial intelligence);learning systems;legged locomotion;motion control","flying quadruped robot;traditional motion control methods;deep reinforcement learning technology;twin delayed deep deterministic policy gradient algorithm","","","","15","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning Control for Consensus Problems of Uncertain Nonlinear Multi-Agent Systems","H. -y. Zhu; W. Mao","College of Control Science and Engineering, Zhejiang University, Hangzhou, China; College of Control Science and Engineering, Zhejiang University, Hangzhou, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","6561","6566","In this paper, we consider the consensus problem of distributed multi-agent systems with nonlinear dynamics and disturbances. The underlying system is described based upon the stochastic communication network topology. To solve this problem, we propose a multi-agent reinforcement learning-based method that converts the consensus problem with multiple nonlinear agents into one learning target and automatically learns effective strategies. The actor-critic method is adopted for updating learning policies. Moreover, we design a distributed communication method to ensure that each agent in the multi-agent system can obtain consensus information. Two examples are presented to show the effectiveness and potential of the proposed design technique.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728588","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728588","multi-agent systems;consensus;nonlinear;uncertainties;multi-agent reinforcement learning","Learning systems;Uncertainty;Network topology;Heuristic algorithms;Reinforcement learning;Information processing;Nonlinear dynamical systems","learning (artificial intelligence);multi-agent systems;multi-robot systems;nonlinear control systems;telecommunication network topology;uncertain systems","distributed multiagent systems;nonlinear dynamics;stochastic communication network topology;multiagent reinforcement learning-based method;consensus problem;multiple nonlinear agents;learning target;actor-critic method;learning policies;distributed communication method;multiagent system;consensus information;multiagent reinforcement learning control;uncertain nonlinear multiagent systems","","","","30","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Cooperative Emergent Swarming Through Deep Reinforcement Learning","T. X. Lin; D. M. Lofaro; D. A. Sofge","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Distributed Autonomous Systems Group, U.S. Naval Research Laboratory, Washington DC, USA; Distributed Autonomous Systems Group, U.S. Naval Research Laboratory, Washington DC, USA","2020 IEEE 16th International Conference on Control & Automation (ICCA)","30 Nov 2020","2020","","","1354","1359","This paper studies the problem of designing a decentralized controller that is able to induce a desired emergent behavior in robot swarms. We consider a robot swarm composing of agents that have identical dynamics and sensors and are unable to communicate among each other. Our proposed approach leverages deep reinforcement learning methods to search for an appropriate control policy that achieves our desired group behavior. We show that our method is able to capture targets in a predator-prey case-study. Simulation and experimental results using an autonomous blimp research platform are also provided.","1948-3457","978-1-7281-9093-8","10.1109/ICCA51439.2020.9264545","Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264545","","Reinforcement learning;Decentralized control;Neural networks;Predator prey systems;Vehicle dynamics;Training;Testing","control system synthesis;decentralised control;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;neurocontrollers;predator-prey systems;sensors","control policy;group behavior;predator-prey case-study;deep reinforcement learning;decentralized controller;emergent behavior;robot swarms;identical dynamics;sensors;cooperative emergent swarming;controller design;autonomous blimp research platform","","","","18","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Optimal Trajectory Tracking for Cyber-Physical Marine Vessel: Reinforcement Learning Approach","X. Li; J. Wang; X. Li; J. Yan","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical Engineering, Yanshan University, Qinhuangdao, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","2020 16th International Conference on Control, Automation, Robotics and Vision (ICARCV)","8 Jan 2021","2020","","","1230","1235","In this paper, we consider the optimal trajectory tracking control problem for a cyber-physical marine surface vessel system with unknown dynamics by using a reinforcement learning (RL) algorithm. The main contributions of the paper are threefold: (1) The dynamics of the marine vessel system considered in this paper is partially unknown. (2) The actor-critic-identifier (ACI) architecture based on fuzzy logical system (FLS) approximation is designed to handle the optimal tracking control problem; (3) The same Bellman error is chosen to update the actor and critic FLS weights at the same time. To be more specific, by using FLS approximating the unknown optimal value function, optimal policy, and partial unknown dynamic, a reinforcement learning algorithm is used to solve the optimal tracking problem of the marine surface vessel system. By designing a fuzzy identifier, the actor-critic weights are adaptively updated simultaneously. The rigorous proof is given to verify the correctness of the algorithm.","","978-1-7281-7709-0","10.1109/ICARCV50220.2020.9305422","National Natural Science Foundation of China(grant numbers:61903319); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305422","","Fuzzy logic;Trajectory;Heuristic algorithms;Optimal control;Nonlinear dynamical systems;Trajectory tracking;Reinforcement learning","adaptive control;continuous time systems;fuzzy control;fuzzy logic;learning (artificial intelligence);neurocontrollers;nonlinear control systems;optimal control;tracking","cyber-physical marine vessel;reinforcement learning approach;optimal trajectory tracking control problem;cyber-physical marine surface vessel system;unknown dynamics;reinforcement learning algorithm;marine vessel system;actor-critic-identifier architecture;fuzzy logical system approximation;FLS;optimal tracking control problem;unknown optimal value function;optimal policy;partial unknown dynamic;optimal tracking problem;fuzzy identifier;actor-critic weights","","","","12","IEEE","8 Jan 2021","","","IEEE","IEEE Conferences"
"Hierarchical Reinforcement Learning-based Mapless Navigation with Predictive Exploration Worthiness","Y. Gao; Z. Ji; J. Wu; C. Wei; R. Grech","School of Engineering, Cardiff University, Cardiff, United Kingdom; School of Engineering, Cardiff University, Cardiff, United Kingdom; School of Computer Science and Informatics, Cardiff University, Cardiff, United Kingdom; College of Mechanical and Electrical Engineering, Hohai University, Changzhou, China; Spirent Communications, Paignton, United Kingdom","2023 IEEE International Conference on Mechatronics and Automation (ICMA)","22 Aug 2023","2023","","","636","643","Hierarchical reinforcement learning (HRL) is a promising approach for complex mapless navigation tasks by decomposing the task into a hierarchy of subtasks. However, selecting appropriate subgoals is challenging. Existing methods predominantly rely on sensory inputs, which may contain inadequate information or excessive redundancy. Inspired by the cognitive processes underpinning human navigation, our aim is to enable the robot to leverage both ‘intrinsic and extrinsic factors’ to make informed decisions regarding subgoal selection. In this work, we propose a novel HRL-based mapless navigation framework. Specifically, we introduce a predictive module, named Predictive Exploration Worthiness (PEW), into the high-level (HL) decision-making policy. The hypothesis is that the worthiness of an area for further exploration is related to obstacle spatial distribution, such as the area of free space and the distribution of obstacles. The PEW is introduced as a compact representation for obstacle spatial distribution. Additionally, to incorporate ‘intrinsic factors’ in the subgoal selection process, a penalty element is introduced in the HL reward function, allowing the robot to take into account the capabilities of the low-level policy when selecting subgoals. Our method exhibits significant improvements in success rate when tested in unseen environments.","2152-744X","979-8-3503-2084-8","10.1109/ICMA57826.2023.10215569","Royal Academy of Engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10215569","Mapless navigation;Deep Reinforcement Learning;Motion Planning;Hierarchical Reinforcement Learning","Training;Laser radar;Graphical models;Navigation;Redundancy;Reinforcement learning;Robot sensing systems","collision avoidance;decision making;mobile robots;navigation;reinforcement learning;robot vision;SLAM (robots)","cognitive processes;complex mapless navigation tasks;hierarchical reinforcement learning-based mapless navigation;high-level decision-making policy;HL decision-making policy;HRL-based mapless navigation;human navigation;obstacle spatial distribution;PEW;predictive exploration worthiness;predictive module;subgoal selection process","","","","21","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"GNN-Based Hierarchical Deep Reinforcement Learning for NFV-Oriented Online Resource Orchestration in Elastic Optical DCIs","B. Li; Z. Zhu","School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China","Journal of Lightwave Technology","7 Feb 2022","2022","40","4","935","946","Network function virtualization (NFV) in elastic optical datacenter interconnections (EO-DCIs) enables flexible and timely deployment of network services. However, as the service provisioning of virtual network function service chains (vNF-SCs) in an EO-DCI needs to orchestrate the allocations of IT resources in datacenters (DCs) and spectrum resources on fiber links dynamically, it is a complex and challenging problem. In this work, we model the problem as a Markov decision process (MDP), and propose a hierarchical deep reinforcement learning (DRL) model based on graph neural network (GNN), namely, HRLOrch, to tackle it. To ensure its universality and scalability, we design the policy neural network (NN) in HRLOrch based on a GNN. As the GNN-based policy NN can operate on the graph-structured network state of an EO-DCI directly, it can adapt to an arbitrary EO-DCI topology without any structural changes. Then, through analysis, we find that the EO-DCI is a sparse reward environment if we want to train a DRL model to minimize the blocking probability of vNF-SCs in it directly. To address this issue, we design a hierarchical DRL with lower-level and upper-level models to improve the convergence performance of training. Specifically, we make the lower-level DRL optimize the provisioning scheme of each vNF-SC to minimize its resource usage, while the upper-level one coordinates the provisioning of all the active vNF-SCs to minimize the overall blocking probability. Hence, the lower-level and upper-level DRL models operate cooperatively in the training to optimize the dynamic provisioning of vNF-SCs. Our simulations demonstrate the universality and scalability of HRLOrch, and confirm that it can outperform the existing algorithms for vNF-SC provisioning in an EO-DCI.","1558-2213","","10.1109/JLT.2021.3125974","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2020YFB1806400); National Natural Science Foundation of China(grant numbers:61871357); SPR Program of CAS(grant numbers:XDC02070300); Fundamental Funds for Central Universities(grant numbers:WK3500000006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609608","Network function virtualization (NFV);service function chain;datacenter interconnection (DCI);elastic optical network (EON);graph neural network (GNN);deep reinforcement learning (DRL);network automation","Virtualization;Training;Optical fiber networks;Artificial neural networks;Adaptation models;Topology;Optical interconnections","computer centres;deep learning (artificial intelligence);graph theory;Markov processes;optical computing;optical fibre networks;optical interconnections;probability;resource allocation;virtualisation","blocking probability;vNF-SC;hierarchical DRL;upper-level models;lower-level DRL;resource usage;dynamic provisioning;HRLOrch;GNN-based hierarchical deep reinforcement learning;NFV-oriented online resource orchestration;network function virtualization;elastic optical datacenter interconnections;network services;service provisioning;virtual network function service chains;spectrum resources;complex problem;hierarchical deep reinforcement learning model;graph neural network;policy neural network;GNN-based policy NN;graph-structured network state;arbitrary EO-DCI topology;DRL model;elastic optical DCI;IT resources;Markov decision process;lower-level models","","15","","52","IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"Adversarial Reinforcement Learning for Procedural Content Generation","L. Gisslén; A. Eakins; C. Gordillo; J. Bergdahl; K. Tollmar",SEED - Electronic Arts (EA); Frostbite - Electronic Arts (EA); SEED - Electronic Arts (EA); SEED - Electronic Arts (EA); SEED - Electronic Arts (EA),"2021 IEEE Conference on Games (CoG)","7 Dec 2021","2021","","","1","8","Training RL agents to solve novel environments is a notoriously difficult task. Here we present a new approach ARLPCG: Adversarial Reinforcement Learning for Procedural Content Generation, which procedurally generates and tests previously unseen environments with an auxiliary input as a control variable. The procedurally generated environments induces state diversity which increases the generalizability of the trained agents. ARLPCG deploys an adversarial model with one PCG RL agent (called Generator) and one solving RL agent (called Solver). The Generator receives a reward signal based on the Solver's performance, which encourages the environment design to be challenging but not impossible. To further drive diversity and control of the environment generation, we propose using auxiliary inputs for the Generator. The benefit is two-fold: Firstly, the Solver achieves better generalization through the Generator's generated challenges. Secondly, the trained Generator can be used as a creator of novel environments that, together with the Solver, can be shown to be solvable. We create two types of 3D environments to validate our model, representing two popular game genres: a third-person platformer and a racing game. In these cases, we show that ARLPCG has a significantly better solve ratio, and that the auxiliary inputs renders the levels creation controllable to a certain degree. For a video compilation of the results please visit https://youtu.be/z7q2PtVsT0I.","2325-4289","978-1-6654-3886-5","10.1109/CoG52621.2021.9619053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619053","machine learning;game testing;procedural content generation;automation;computer games;reinforcement learning;adversarial","Training;Solid modeling;Three-dimensional displays;Conferences;Games;Reinforcement learning;Generators","computer games;reinforcement learning;software agents","adversarial reinforcement learning;procedural content generation;RL agents;adversarial model;PCG RL agent;environment design;environment generation;Solver performance;ARLPCG approach","","11","","20","IEEE","7 Dec 2021","","","IEEE","IEEE Conferences"
"Adaptation of a wheel loader automatic bucket filling neural network using reinforcement learning","S. Dadhich; F. Sandin; U. Bodin; U. Andersson; T. Martinsson","Luleå University of Technology, Luleå, Sweden; Luleå University of Technology, Luleå, Sweden; Luleå University of Technology, Luleå, Sweden; Luleå University of Technology, Luleå, Sweden; Volvo CE, Eskilstuna, Sweden","2020 International Joint Conference on Neural Networks (IJCNN)","28 Sep 2020","2020","","","1","9","Bucket-filling is a repetitive task in earth-moving operations with wheel-loaders, which needs to be automated to enable efficient remote control and autonomous operation. Ideally, an automated bucket-filling solution should work for different machine-pile environments, with a minimum of manual retraining. It has been shown that for a given machine-pile environment, a time-delay neural network can efficiently fill the bucket after imitation-based learning from 100 examples by one expert operator. Can such a bucket-filling network be automatically adapted to different machine-pile environments without further imitation learning by optimization of a utility or reward function? This paper investigates the use of a deterministic actor-critic reinforcement learning algorithm for automatic adaptation of a neural network in a new pile environment. The algorithm is used to automatically adapt a bucket-filling network for medium coarse gravel to a cobble-gravel pile environment. The experiments presented are performed with a Volvo L180H wheel-loader in a real-world setting. We found that the bucket-weights in the novel pile environment can improve by five to ten percent within one hour of reinforcement learning with less than 40 bucket-filling trials. This result was obtained after investigating two different reward functions motivated by domain knowledge.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9206849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206849","reinforcement learning;imitation learning;bucket filling;wheel loader;automation;construction","Adaptation models;Neural networks;Learning (artificial intelligence);Predictive models;Task analysis;Trajectory;Wheels","construction equipment;earthmoving equipment;foundations;learning (artificial intelligence);loading equipment;mechanical engineering computing;neural nets","earth-moving operations;automated bucket-filling solution;time-delay neural network;actor-critic reinforcement learning algorithm;cobble-gravel pile environment;reward functions;machine-pile environment;imitation-based learning;Volvo L180H wheel-loader","","8","","42","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
"From Specification to Topology: Automatic Power Converter Design via Reinforcement Learning","S. Fan; N. Cao; S. Zhang; J. Li; X. Guo; X. Zhang",New Jersey Institute of Technology; New Jersey Institute of Technology; New Jersey Institute of Technology; New Jersey Institute of Technology; IBM T. J. Watson Research Center; IBM T. J. Watson Research Center,"2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)","23 Dec 2021","2021","","","1","9","The tidal waves of modern electronic/electrical devices have led to increasing demands for ubiquitous application-specific power converters. A conventional manual design procedure of such power converters is computation- and labor-intensive, which involves selecting and connecting component devices, tuning component-wise parameters and control schemes, and iteratively evaluating and optimizing the design. To automate and speed up this design process, we propose an automatic framework that designs custom power converters from design specifications using reinforcement learning. Specifically, the framework embraces upper-confidence-bound-tree-based (UCT-based) reinforcement learning to automate topology space exploration with circuit design specification-encoded reward signals. Moreover, our UCT-based approach can exploit small offline data via the specially designed default policy to accelerate topology space exploration. Further, it utilizes a hybrid circuit evaluation strategy to substantially reduces design evaluation costs. Empirically, we demonstrated that our framework could generate energy-efficient circuit topologies for various target voltage conversion ratios. Compared to existing automatic topology optimization strategies, the proposed method is much more computationally efficient - it can generate topologies with the same quality while being up to 67% faster. Additionally, we discussed some interesting circuits discovered by our framework.","1558-2434","978-1-6654-4507-8","10.1109/ICCAD51958.2021.9643552","ARPA-E; U.S. Department of Energy(grant numbers:DE-AR0001210); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9643552","design automation;power converter topology design;upper-confidence-bound tree (UCT);reinforcement learning","Circuit topology;Reinforcement learning;Manuals;Energy efficiency;Topology;Space exploration;Computational efficiency","evolutionary computation;learning (artificial intelligence);network topology;power convertors;power engineering computing;search problems;topology;trees (mathematics)","reinforcement learning;upper-confidence-bound-tree-based reinforcement;circuit design specification-encoded reward signals;UCT-based approach;hybrid circuit evaluation strategy;automatic power converter design;control schemes;energy-efficient circuit topology;automatic topology optimization strategy;voltage conversion ratio","","4","","26","IEEE","23 Dec 2021","","","IEEE","IEEE Conferences"
"Automated Testing of Android Applications Integrating Residual Network and Deep Reinforcement Learning","L. Cai; J. Wang; M. Cheng; J. Wang","School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; Shanghai Key Laboratory of Computer Software Testing&Evaluating, Shanghai Development Center of Computer Software Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China","2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS)","10 Mar 2022","2021","","","189","196","With the improvements of Deep Reinforcement Learning (DRL), there have been tremendous interests in utilizing DRL for automated application testing. However, most automated testing methods based on reinforcement learning have the problem of interacting with invalid UI areas and invalid interactions with controls. To solve this problem, this paper extracts the page features, constructs the Interactive Control Feature Diagram(ICCD); improves the DDQN network structure, adds the residual network, makes the algorithm take the picture as the input, and splits the original single output action(n*w*h) into two successive outputs: the interaction(1,n) and the position(1,w*h); a new reward function which combines the interaction times and the image similarity of ICCD is proposed to explore different UIs and ensure that there will be more than one action will be executed under the same UI. Experiments are carried out on five open source applications. The experimental results show that the proposed method is superior to other methods in code coverage and branch coverage.","2693-9177","978-1-6654-5813-9","10.1109/QRS54544.2021.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9724895","Automation;Application Testing;Reinforcement Learning","Adaptation models;Software algorithms;Reinforcement learning;Software quality;Feature extraction;Software reliability;Security","learning (artificial intelligence);mobile computing;program testing","DDQN network structure;original single output action;successive outputs;interaction times;different UIs;open source applications;android applications integrating residual network;Deep Reinforcement Learning;DRL;tremendous interests;automated application testing;automated testing methods;invalid UI areas;invalid interactions","","3","","30","IEEE","10 Mar 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Optimization at Early Design Stages","L. Servadei; J. H. Lee; J. A. Arjona Medina; M. Werner; S. Hochreiter; W. Ecker; R. Wille","Infineon Technologies AG, Neubiberg, Germany; Technical University of Munich, Munich, Germany; Johannes Kepler University Linz, Linz, Austria; Infineon Technologies AG, Munich, Germany; Johannes Kepler University Linz, Linz, Austria; Infineon Technologies AG, Munich, Germany; Johannes Kepler University Linz, Linz, Austria","IEEE Design & Test","23 Jan 2023","2023","40","1","43","51","Deep reinforcement learning is shown to improve the design cost of hardware char63software interfaces within an industrial design framework. Based on optimization preferences specified by a designer, the proposed approach generates optimized solutions. ","2168-2364","","10.1109/MDAT.2022.3145344","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9687589","Machine Learning;Design Automation;Reinforcement Learning;Combinatorial Optimization;Early Design Stages","Optimization;Costs;Design methodology;User interfaces;Hardware;Reinforcement learning;Table lookup","deep learning (artificial intelligence);hardware-software codesign;optimisation;reinforcement learning","deep reinforcement learning;design cost;design stages;hardware-software interfaces;industrial design framework;optimization preferences","","2","","12","IEEE","20 Jan 2022","","","IEEE","IEEE Magazines"
"Deep Reinforcement Learning for URLLC in 5G Mission-Critical Cloud Robotic Application","T. M. Ho; T. T. Nguyen; K. -K. Nguyen; M. Cheriet","Synchromedia Lab, Ecole de Technologie Supéricure, Université du Québec, QC, Canada; Synchromedia Lab, Ecole de Technologie Supéricure, Université du Québec, QC, Canada; Synchromedia Lab, Ecole de Technologie Supéricure, Université du Québec, QC, Canada; Synchromedia Lab, Ecole de Technologie Supéricure, Université du Québec, QC, Canada","2021 IEEE Global Communications Conference (GLOBECOM)","2 Feb 2022","2021","","","1","6","In this paper, we investigate the problem of robot swarm control in 5G mission-critical robotic applications, i.e., in an automated grid-based warehouse scenario. Such application requires both the kinematic energy consumption of the robots and the ultra-reliable and low latency communication (URLLC) between the central controller and the robot swarm to be jointly optimized in real-time. The problem is formulated as a nonconvex optimization problem since the achievable rate and decoding error probability with short block-length are neither convex nor concave in bandwidth and transmit power. We propose a deep reinforcement learning (DRL) based approach that employs the deep deterministic policy gradient (DDPG) method and convolutional neural network (CNN) to achieve a stationary optimal control policy that consists of a number of continuous and discrete actions. Numerical results show that our proposed multi-agent DDPG algorithm achieves a performance close to the optimal baseline and outperforms the single-agent DDPG in terms of decoding error probability and energy efficiency.","","978-1-7281-8104-2","10.1109/GLOBECOM46510.2021.9685978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9685978","5G network;robotic network;industrial automation;URLLC;deep reinforcement learning","5G mobile communication;Error probability;Mission critical systems;Optimal control;Reinforcement learning;Kinematics;Ultra reliable low latency communication","cloud computing;concave programming;gradient methods;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;neural nets;optimal control;optimisation","URLLC;mission-critical cloud robotic application;robot swarm control;5G mission-critical robotic applications;automated grid-based warehouse scenario;kinematic energy consumption;ultra-reliable latency communication;low latency communication;central controller;nonconvex optimization problem;achievable rate;decoding error probability;short block-length;transmit power;deep reinforcement learning based approach;deep deterministic policy gradient method;stationary optimal control policy;optimal baseline","","1","","9","IEEE","2 Feb 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Selecting Custom Instructions under Area Constraint","S. Wang; C. Xiao","Department of Computer Science, Shantou University, China; Department of Computer Science, Shantou University, China","IEEE Transactions on Artificial Intelligence","","2023","PP","99","1","13","Extensible processors, which combine programma-bility and efficiency, are emerging as a promising approach in the field of embedded computing. Automated synthesis of custom instructions from high-level application descriptions is a vital step involved in the design of extensible processors. In automated custom instruction synthesis, selecting custom instructions from a large set of candidates under area constraint is essentially a difficult combinatorial optimization problem. In this paper, we show that the custom instruction selection problem can be formulated as a sequential decision-making problem. Based on this formulation, we present three reinforcement learning-based approaches, namely SARSA, Q-learning, and Double Q-Learning, for solving the custom instruction selection problem. Moreover, we also perform a comprehensive analysis and com-parison of various combinations of learning specifications: the algorithm type and the update strategy for ε-greedy policy. The experiments with 45 test instances reveal that the SARSA, Q-learning, and Double Q-Learning algorithms outperform the meta-heuristic algorithm in terms of the overall performance gains by 26.9%,26.1% and 26.4% respectively. Among the three reinforcement learning algorithms, the SARSA algorithm slightly overwhelms the other two reinforcement learning algorithms. Furthermore, the experimental results suggest that the strategy $F_3 : ε = κ^i,0 < κ < 1$ is generally the most effective one for controlling the exploration and the exploitation of the learning processes.","2691-4581","","10.1109/TAI.2023.3308099","Scientific Research Project of Col-leges and Universities in Guangdong Province(grant numbers:2021ZDZX1027); Guang-dong Basic and Applied Basic Research Foundation(grant numbers:2022A1515110712,2023A1515010077); STU Scientific Research Foundation for Talents(grant numbers:NTF20016,NTF20017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10229491","Embedded Systems;Design Automation;Exten-sible Processors;Custom Instruction Selection;Reinforcement Learning","Program processors;Heuristic algorithms;Artificial intelligence;Q-learning;Metaheuristics;Codes;Partitioning algorithms","","","","","","","IEEE","24 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Reinforcement Learning Based Framework for Real Time Fault Tolerance","Y. Kotb; M. Alakkoummi; H. Kanj","College of Engineering and Technology, American University of the Middle East, Kuwait; College of Engineering and Technology, American University of the Middle East, Kuwait; College of Engineering and Technology, American University of the Middle East, Kuwait","2020 11th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)","22 Dec 2020","2020","","","0357","0364","Smart autonomous systems are system that take decisions independently and on run time without the need for human interaction. One of the most important components that plays a big roll in system autonomy is fault tolerance and avoidance which is a basic capability that autonomous systems should have in order to be able to survive surrounding environment state change without the need for supervision. In this paper, a framework is proposed where fault tolerance and avoidance is achieved through a reinforcement learning based framework. The framework adapts to changes and learns new processes for fault tolerance and avoidance whenever environment states change. The framework has a set of predefined actions and an observable environment. Reinforcement learning is being applied in order to learn the sequence of actions that needs to be taken to avoid or tolerate failure. The outcome of the learning process is a sequence of actions to help the system reach a desired state while avoiding fault states. These are used for later execution when the same situation occurs when the agent is in similar environment state while having the similar readings. Two Theorems and a Lemma are proposed to define the validity and correctness of the framework. The proposed framework is then simulated and tested.","2644-3163","978-1-7281-8416-6","10.1109/IEMCON51383.2020.9284929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284929","Fault tolerance;Fault avoidance;reinforcement learning;intelligent agents;automation","Fault tolerance;Autonomous systems;Fault tolerant systems;Reinforcement learning;Time measurement;Real-time systems;Mathematical model","fault tolerant computing;learning (artificial intelligence)","real time fault tolerance;fault states;learning process;tolerate failure;environment states change;reinforcement learning based framework;surrounding environment state change;system autonomy;smart autonomous systems","","","","36","IEEE","22 Dec 2020","","","IEEE","IEEE Conferences"
"Training a robot with limited computing resources to crawl using reinforcement learning","M. P. Heimbach; J. Weber; M. Schmidt","Aerospace Information Technology, University of Würzburg, Würzburg, Germany; Campus Velbert-Heiligenhaus, Bochum University of Applied Sciences, Heiligenhaus, Germany; Aerospace Information Technology, University of Würzburg, Würzburg, Germany","2022 Sixth IEEE International Conference on Robotic Computing (IRC)","24 Jan 2023","2022","","","265","270","In recent years, new successes in artificial intelligence and machine learning have been continuously achieved. However, this progress is largely based on the use of simulations as well as numerous powerful computers. Due to the volume taken up and the necessary power to run these components, this is not feasible for mobile robotics. Nevertheless, the use of machine learning in mobile robots is desirable in order to adapt to unknown or changing environmental conditions.This paper evaluates the performance of different reinforcement learning methods on a physical robot platform. This robot has an arm with two degrees of freedom that can be used to move across a surface. The goal is to learn the correct motion sequence of the arm to move the robot. The focus here is exclusively on using the robot’s onboard computer, a Raspberry Pi 4 Model B. To learn forward motion, Value Iteration and variants of Q-learning from the field of reinforcement learning are used.It is shown that since the structure of some problems can be described by a very limited problem space, even when using a physical robot relatively simple algorithms can yield sufficient learning results. Furthermore, hardware limitations may prevent using more complex algorithms.","","978-1-6654-7260-9","10.1109/IRC55401.2022.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023868","Machine Learning for Robot Control;Reinforcement Learning;AI-Enabled Robotics;Continual Learning;Embedded Systems for Robotics and Automation","Learning systems;Computers;Training;Q-learning;Computational modeling;Neural networks;Manipulators","mobile robots;motion control;reinforcement learning","artificial intelligence;changing environmental conditions;computing resources;correct motion sequence;crawl;forward motion;machine learning;mobile robotics;onboard computer;physical robot platform;Q-learning;Raspberry Pi 4 Model B;reinforcement learning;value iteration","","","","11","IEEE","24 Jan 2023","","","IEEE","IEEE Conferences"
"Parameter Design Optimization for DC-DC Power Converters with Deep Reinforcement Learning","F. Tian; D. B. Cobaleda; H. Wouters; W. Martinez","Department of Electrical Engineering (ESAT), KU Leuven-EnergyVille, Diepenbeek-Genk, Belgium; Department of Electrical Engineering (ESAT), KU Leuven-EnergyVille, Diepenbeek-Genk, Belgium; Department of Electrical Engineering (ESAT), KU Leuven-EnergyVille, Diepenbeek-Genk, Belgium; Department of Electrical Engineering (ESAT), KU Leuven-EnergyVille, Diepenbeek-Genk, Belgium","2022 IEEE Energy Conversion Congress and Exposition (ECCE)","1 Dec 2022","2022","","","1","7","The deep deterministic policy gradient (DDPG) algorithm, a reinforcement learning (RL) technique which trains an agent to achieve the maximal reward by interacting with the environment, is applied for parameters optimization for a DC-DC power converter. A model for evaluating the efficiency of both semiconductor and magnetic components is developed. An artificial neural network (ANN) is trained by data from Spice simulation to establish a fast and accurate model for semiconductor losses. In the meantime, magnetic design is conducted by considering various core shapes, materials, winding turns, and airgaps. Finally, DDPG is trained by the combined efficiency evaluation model. The results indicate that the ANN model for semiconductor losses is accurate and the optimal design is obtained quickly by the trained DDPG algorithm.","2329-3748","978-1-7281-9387-8","10.1109/ECCE50734.2022.9948201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9948201","Artificial intelligence;power converters;reinforcement learning;deep deterministic policy gradient algorithm;design automation","Semiconductor device modeling;Atmospheric modeling;Magnetic cores;Windings;Artificial neural networks;Reinforcement learning;DC-DC power converters","air gaps;DC-DC power convertors;deep learning (artificial intelligence);losses;optimisation;power engineering computing;reinforcement learning;SPICE","airgaps;ANN model;artificial neural network;combined efficiency evaluation model;DC-DC power converter;deep deterministic policy gradient algorithm;deep reinforcement learning;magnetic components;magnetic design;parameter design optimization;semiconductor losses;Spice simulation;trained DDPG algorithm;winding turns","","","","13","IEEE","1 Dec 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for DC-DC Converter Parameters Optimization","F. Tian; D. B. Cobaleda; W. Martinez","Dept. of Electrical Engineering (ESAT), KU Leuven-EnergyVille, Diepenbeek, Genk, Belgium; Dept. of Electrical Engineering (ESAT), KU Leuven-EnergyVille, Diepenbeek, Genk, Belgium; Dept. of Electrical Engineering (ESAT), KU Leuven-EnergyVille, Diepenbeek, Genk, Belgium","2022 IEEE 31st International Symposium on Industrial Electronics (ISIE)","25 Jul 2022","2022","","","325","330","Reinforcement learning (RL) is a type of machine learning in which an agent teaches itself by interacting with the environment. A RL-based parameter optimization method is proposed to improve the efficiency of a DC-DC power converter. More specifically, deep Q network (DQN) methods are utilized to optimize the power converter's parameter designs under current, voltage ripple, and volume limitations. Spice simulation is used to determine power losses on semiconductors. Combined with an optimal design of the inductor, the overall efficiency of power converters is obtained. The results show that an optimal design is obtained by using the DQN algorithm.","2163-5145","978-1-6654-8240-0","10.1109/ISIE51582.2022.9831660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831660","Artificial intelligence;power converters;rein-forcement learning;design automation","Training;Power system measurements;Optimization methods;DC-DC power converters;Reinforcement learning;Voltage;Data models","DC-DC power convertors;deep learning (artificial intelligence);optimisation;power engineering computing;reinforcement learning","optimal parameter designs;DC-DC converter parameters optimization;DC-DC power converter;deep Q network algorithms;deep reinforcement learning;machine learning approach;DQN","","","","15","IEEE","25 Jul 2022","","","IEEE","IEEE Conferences"
"Prescriptive Maintenance of Freight Vehicles using Deep Reinforcement Learning","C. -K. Tham; W. Liu; R. Chattopadhyay","Department of Electrical and Computer Engineering, National University of Singapore; Department of Electrical and Computer Engineering, National University of Singapore; Department of Electrical and Computer Engineering, National University of Singapore","2023 IEEE 97th Vehicular Technology Conference (VTC2023-Spring)","14 Aug 2023","2023","","","1","5","Supply chain disruptions caused by breakdown of freight vehicles lead to delayed deliveries which cost companies millions of dollars and loss of customer goodwill. Breakdowns can be reduced through predictive maintenance, which has become a mature field with solutions offered by various vendors. In this paper, we propose a prescriptive maintenance approach that leverages deep reinforcement learning (DRL) to directly make maintenance decisions for a fleet of freight vehicles such as trucks. Proximal Policy Optimization (PPO) is a state-of-the-art reinforcement learning (RL) algorithm based on policy gradient and uses function approximators like deep neural networks (DNNs) to store the policy and value functions. In order to introduce long timescale memory that can lead to superior policies in complex problems, we integrate the PPO with a Long Short Term Memory (LSTM) network. The resulting PPO-LSTM scheme requires careful handling of sequences of observations. We investigated the performance of the PPO-DNN and PPO-LSTM schemes in making prescriptive maintenance decisions for a fleet of trucks transporting goods between factories. From sensor readings indicating the condition of the truck transmission system which deteriorate under use, good maintenance decisions are made that enable a large number of trucks to remain active. The performance of these schemes were evaluated in realistic simulations of different traffic conditions using the SUMO simulator working in conjunction with realistic truck transmission system and factory production simulators. Our results show that the proposed schemes outperformed baseline schemes and achieved a significant increase in production throughput under different traffic conditions and maintenance and repair durations.","2577-2465","979-8-3503-1114-3","10.1109/VTC2023-Spring57618.2023.10199753","National University of Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10199753","Intelligent Transportation;Prescriptive Maintenance;Deep Reinforcement Learning","Deep learning;Vehicular and wireless technologies;Electric breakdown;Supply chains;Reinforcement learning;Maintenance engineering;Throughput","decision making;deep learning (artificial intelligence);freight handling;function approximation;goods distribution;maintenance engineering;mechanical engineering computing;optimisation;power transmission (mechanical);recurrent neural nets;reinforcement learning;road vehicles","customer goodwill;deep neural network;deep reinforcement learning;delayed deliveries;factory production simulator;freight vehicle breakdown;function approximator;goods transport;long short term memory network;long timescale memory;policy gradient;PPO-DNN;PPO-LSTM scheme;predictive maintenance;prescriptive maintenance decisions;proximal policy optimization;repair duration;sensor readings;SUMO simulator;supply chain disruption;truck transmission system","","","","13","IEEE","14 Aug 2023","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach to Online Web Systems Auto-configuration","X. Bu; J. Rao; C. -Z. Xu","Department of Electrical & Computer Engineering, Wayne State University, Detroit, MI, USA; Department of Electrical & Computer Engineering, Wayne State University, Detroit, MI, USA; Department of Electrical & Computer Engineering, Wayne State University, Detroit, MI, USA","2009 29th IEEE International Conference on Distributed Computing Systems","7 Jul 2009","2009","","","2","11","In a web system, configuration is crucial to the performance and service availability. It is a challenge, not only because of the dynamics of Internet traffic, but also the dynamic virtual machine environment the system tends to be run on. In this paper, we propose a reinforcement learning approach for autonomic configuration and reconfiguration of multi-tier web systems. It is able to adapt performance parameter settings not only to the change of workload, but also to the change of virtual machine configurations. The RL approach is enhanced with an efficient initialization policy to reduce the learning time for online decision. The approach is evaluated using TPC-W benchmark on a three-tier website hosted on a Xen-based virtual machine environment. Experiment results demonstrate that the approach can auto-configure the web system dynamically in response to the change in both workload and VM resource. It can drive the system into a near-optimal configuration setting in less than 25 trial-and-error iterations.","1063-6927","978-0-7695-3659-0","10.1109/ICDCS.2009.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5158403","Reinforcement Learning;Auto-configuration;Web Systems","Learning;Virtual machining;Hardware;Virtual manufacturing;Web and internet services;Resource management;Distributed computing;Availability;System performance;Automatic control","Internet;learning (artificial intelligence);software fault tolerance;virtual machines","reinforcement learning;online Web systems auto-configuration;Internet traffic;dynamic virtual machine environment;autonomic configuration;multi-tier Web systems;TPC-W benchmark;Xen-based virtual machine environment","","65","2","20","IEEE","7 Jul 2009","","","IEEE","IEEE Conferences"
"Behavior coordination for a mobile robot using modular reinforcement learning","E. Uchibe; M. Asada; K. Hosoda","Department of Mechanical Engineering for Computer-Controlled Machinery, Osaka University, Suita, Osaka, Japan; Department of Mechanical Engineering for Computer-Controlled Machinery, Osaka University, Suita, Osaka, Japan; Department of Mechanical Engineering for Computer-Controlled Machinery, Osaka University, Suita, Osaka, Japan","Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS '96","6 Aug 2002","1996","3","","1329","1336 vol.3","Coordination of multiple behaviors independently obtained by a reinforcement learning method is one of the issues in order for the method to be scaled to larger and more complex robot learning tasks. Direct combination of all the state spaces for individual modules (subtasks) needs enormous learning time, and it causes hidden states. This paper presents a method of modular learning which coordinates multiple behaviors taking account of a trade-off between learning time and performance. First, in order to reduce the learning time the whole state space is classified into two categories based on the action values separately obtained by Q learning: the area where one of the learned behaviors is directly applicable (no more learning area), and the area where learning is necessary due to competition of multiple behaviors (re-learning area). Second, hidden states are detected by model fitting to the learned action values based on the information criterion. Finally, the initial action valves in the re-learning area are adjusted so that they can be consistent with the values in the no more learning area. The method is applied to one to one soccer playing robots. Computer simulation and real robot experiments are given, to show the validity of the proposed method.","","0-7803-3213-X","10.1109/IROS.1996.568989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=568989","","Mobile robots;Learning;Robot kinematics;State-space methods;Robot sensing systems;Orbital robotics;Robotics and automation;Computer simulation;Autonomous agents;Machinery","learning (artificial intelligence);mobile robots;robot programming","behavior coordination;mobile robot;modular reinforcement learning;multiple behaviors;action values;model fitting;one to one soccer playing robots","","47","","14","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Robot behavior adaptation for human-robot interaction based on policy gradient reinforcement learning","N. Mitsunaga; C. Smith; T. Kanda; H. Ishiguro; N. Hagita","ATR Intelligent Robotics and Communication Laboratories; ATR Intelligent Robotics and Communication Laboratories; ATR Intelligent Robotics and Communication Laboratories; Graduate School of Engineering, Osaka University, Japan; ATR Intelligent Robotics and Communication Laboratories","2005 IEEE/RSJ International Conference on Intelligent Robots and Systems","5 Dec 2005","2005","","","218","225","In this paper, we propose an adaptation mechanism for robot behaviors to make robot-human interactions run more smoothly. We propose such a mechanism based on reinforcement learning, which reads minute body signals from a human partner, and uses this information to adjust interaction distances, gaze meeting, and motion speed and timing in human-robot interaction. We show that this enables autonomous adaptation to individual preferences by an experiment with twelve subjects.","2153-0866","0-7803-8912-3","10.1109/IROS.2005.1545206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1545206","policy gradient reinforcement learning (PGRL);human-robot interaction;behavior adaptation;proxemics","Human robot interaction;Learning;Intelligent robots;Orbital robotics;Laboratories;Minutes;Timing;Context;Eyes;Robotics and automation","robots;learning (artificial intelligence);human computer interaction","robot behavior adaptation;human-robot interaction;interaction distance;gaze meeting;policy gradient reinforcement learning;proxemics","","38","2","11","IEEE","5 Dec 2005","","","IEEE","IEEE Conferences"
"Reinforcement Learning for Intelligent Penetration Testing","M. C. Ghanem; T. M. Chen","Research Centre for Systems and Control City, University of London, London, United Kingdom; Research Centre for Systems and Control City, University of London, London, United Kingdom","2018 Second World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4)","17 Jan 2019","2018","","","185","192","Penetration testing (PT) is an active method for assessing and evaluating the security of digital assets by planning, generating and executing all possible attacks that can exploit existing vulnerabilities. Current PT practice is becoming repetitive, complex and resource consuming despite the use of automated tools. The goal of this paper is to design an intelligent PT approach using reinforcement learning (RL) that will allow regular and systematic testing, saving human resources. The system is modelled as a partially observed Markov decision process (POMDP), and tested using an external POMDP-solver with different algorithms. Although this paper is limited to only the planning phase and not the entire PT process, the results support the hypothesis that reinforcement learning can enhance PT beyond the capabilities of any human expert in terms of accurate and reliable outputs.","","978-1-5386-7280-8","10.1109/WorldS4.2018.8611595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611595","Penetration testing;reinforcement learning;net-work security;vulnerabilities assessment;cyber attack;autonomous testing;intelligent testing","Security;Task analysis;Testing;Automation;Reinforcement learning;Planning;Tools","decision theory;learning (artificial intelligence);Markov processes;security of data","reinforcement learning;intelligent penetration testing;digital assets;resource consuming;automated tools;intelligent PT approach;systematic testing;human resources;partially observed Markov decision process;external POMDP-solver;planning phase","","28","","18","IEEE","17 Jan 2019","","","IEEE","IEEE Conferences"
"Application of Deep Reinforcement Learning in Maneuver Planning of Beyond-Visual-Range Air Combat","D. Hu; R. Yang; J. Zuo; Z. Zhang; J. Wu; Y. Wang","Air Force Engineering University, Xi’an, China; Air Force Engineering University, Xi’an, China; Air Force Engineering University, Xi’an, China; Air Force Engineering University, Xi’an, China; Air Force Engineering University, Xi’an, China; Air Force Engineering University, Xi’an, China","IEEE Access","26 Feb 2021","2021","9","","32282","32297","Beyond-visual-range (BVR) engagement becomes more and more popular in the modern air battlefield. The key and difficulty for pilots in the fight is maneuver planning, which reflects the tactical decision-making capacity of the both sides and determinates success or failure. In this paper, we propose an intelligent maneuver planning method for BVR combat with using an improved deep Q network (DQN). First, a basic combat environment builds, which mainly includes flight motion model, relative motion model and missile attack model. Then, we create a maneuver decision framework for agent interaction with the environment. Basic perceptive variables are constructed for agents to form continuous state space. Also, considering the threat of each side missile and the constraint of airfield, the reward function is designed for agents to training. Later, we introduce a training algorithm and propose perceptional situation layers and value fitting layers to replace policy network in DQN. Based on long short-term memory (LSTM) cell, the perceptional situation layer can convert basic state to high-dimensional perception situation. The fitting layer does well in mapping action. Finally, three combat scenarios are designed for agent training and testing. Simulation result shows the agent can avoid the threat of enemy and gather own advantages to threat the target. It also proves the models and methods of agents are valid and intelligent air combat can be realized.","2169-3536","","10.1109/ACCESS.2021.3060426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9358136","Beyond visual range;intelligent air combat;maneuver planning and decision;missile kill envelope;deep reinforcement learning;LSTM-DQN","Missiles;Aircraft;Atmospheric modeling;Planning;Decision making;Mathematical model;Aircraft manufacture","aerospace computing;aerospace simulation;decision making;deep learning (artificial intelligence);military aircraft;military computing;missiles;multi-agent systems;recurrent neural nets","deep reinforcement learning;beyond-visual-range air combat;beyond-visual-range engagement;tactical decision-making capacity;BVR combat;deep Q network;DQN;flight motion;relative motion;missile attack;maneuver decision framework;agent interaction;agent training;intelligent air combat;air battlefield;intelligent maneuver planning;long short-term memor cell;LSTM cell","","23","","32","CCBY","19 Feb 2021","","","IEEE","IEEE Journals"
"Automatic Feature Selection for Model-Based Reinforcement Learning in Factored MDPs","M. Kroon; S. Whiteson","Informatics Institute, University of Amsterdam, Amsterdam, Netherlands; Informatics Institute, University of Amsterdam, Amsterdam, Netherlands","2009 International Conference on Machine Learning and Applications","15 Jan 2010","2009","","","324","330","Feature selection is an important challenge in machine learning. Unfortunately, most methods for automating feature selection are designed for supervised learning tasks and are thus either inapplicable or impractical for reinforcement learning. This paper presents a new approach to feature selection specifically designed for the challenges of reinforcement learning. In our method, the agent learns a model, represented as a dynamic Bayesian network, of a factored Markov decision process, deduces a minimal feature set from this network, and efficiently computes a policy on this feature set using dynamic programming methods. Experiments in a stock-trading benchmark task demonstrate that this approach can reliably deduce minimal feature sets and that doing so can substantially improve performance and reduce the computational expense of planning.","","978-0-7695-3926-3","10.1109/ICMLA.2009.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381529","Reinforcement learning;feature selection;factored MDPs","Machine learning;Dynamic programming;Bayesian methods;Costs;Filters;Informatics;Supervised learning;Computer networks;Robot sensing systems;Robotics and automation","belief networks;learning (artificial intelligence);Markov processes;stock markets","feature selection;reinforcement learning;machine learning;supervised learning;dynamic Bayesian network;factored Markov decision process;stock trading benchmark","","21","","30","IEEE","15 Jan 2010","","","IEEE","IEEE Conferences"
"Physics Informed Deep Reinforcement Learning for Aircraft Conflict Resolution","P. Zhao; Y. Liu","School for Engineering of Matter, Transport & Energy, Arizona State University, Tempe, AZ, USA; School for Engineering of Matter, Transport & Energy, Arizona State University, Tempe, AZ, USA","IEEE Transactions on Intelligent Transportation Systems","11 Jul 2022","2022","23","7","8288","8301","A novel method for aircraft conflict resolution in air traffic management (ATM) using physics informed deep reinforcement learning (RL) is proposed. The motivation is to integrate prior physics understanding and model in the learning algorithm to facilitate the optimal policy searching and to present human-explainable results for display and decision-making. First, the information of intruders’ quantity, speeds, heading angles, and positions are integrated into an image using the solution space diagram (SSD), which is used in the ATM for conflict detection and mitigation. The SSD serves as the prior physics knowledge from the ATM domain which is the input features for learning. A convolution neural network is used with the SSD images for the deep reinforcement learning. Next, an actor-critic network is constructed to learn conflict resolution policy. Several numerical examples are used to illustrate the proposed methodology. Both discrete and continuous RL are explored using the proposed concept of physics informed learning. A detailed comparison and discussion of the proposed algorithm and classical RL-based conflict resolution is given. The proposed approach is able to handle arbitrary number of intruders and also shows faster convergence behavior due to the encoded prior physics understanding. In addition, the learned optimal policy is also beneficial for proper display to support decision-making. Several major conclusions and future work are presented based on the current investigation.","1558-0016","","10.1109/TITS.2021.3077572","National Aeronautics and Space Administration (NASA) University Leadership Initiative Program (Project Officer: Dr. Anupa Bajwa, Principal Investigator: Dr. Yongming Liu)(grant numbers:NNX17AJ86A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9430767","Conflict resolution;deep reinforcement learning;air traffic management","Aircraft;Image resolution;Reinforcement learning;Physics;Training;Aircraft manufacture;Visualization","air traffic;aircraft;convergence;convolutional neural nets;decision making;deep learning (artificial intelligence);image resolution;neural nets;object detection;optimisation;reinforcement learning;search problems","prior physics knowledge;conflict resolution policy;physics informed learning;decision-making;physics informed deep reinforcement learning;aircraft conflict resolution;ATM;SSD images;conflict detection;RL-based conflict resolution;air traffic management;optimal policy searching;heading angles;solution space diagram;conflict mitigation;convolution neural network;actor-critic network;convergence behavior","","12","","48","IEEE","13 May 2021","","","IEEE","IEEE Journals"
"Realtime reinforcement learning for a real robot in the real environment","T. Yamaguchi; M. Masubuchi; K. Fujihara; M. Yachida","Department of Systems Engineering, Faculty of Engineering Science, Osaka University, Toyonaka, Osaka, Japan; Department of Systems Engineering, Faculty of Engineering Science, Osaka University, Toyonaka, Osaka, Japan; Osaka Daigaku, Suita, Osaka, JP; NA","Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS '96","6 Aug 2002","1996","3","","1321","1328 vol.3","For a real robot to acquire behaviors, it is important for it to learn in a real environment. Most reinforcement learning research has been made by simulation because real-environment learning requires large computation costs as well as a lot of time. Realizing reinforcement learning of a physical robot in a real environment requires both an adaptation for the diversity of possible situations and a high-speed learning method that can learn from fewer trials. This paper describes the realtime reinforcement learning for a real robot in the real environment based on the exploitation oriented reinforcement learning method where the learning cost is very small and has strict incrementality to realize realtime reinforcement learning with an automated sub-rewards generation method achieved by the abstraction of the task state to accelerate the learning process. Successive learning experiments in the real environment for the ball pushing task for the real robot are performed.","","0-7803-3213-X","10.1109/IROS.1996.568988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=568988","","Virtual environment;Computational modeling;Intelligent robots;Computational efficiency;Learning systems;Costs;Robotics and automation;Artificial intelligence;Computer simulation;Systems engineering and theory","learning (artificial intelligence);intelligent control;robot programming","realtime reinforcement learning;real environment;behaviors acquisition;high-speed learning method;exploitation oriented reinforcement learning method;automated sub-rewards generation method;ball pushing task","","12","","19","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Reinforcement learning of sensor-based reaching strategies for a two-link manipulator","P. Martin; J. del R. Millan","Department of Computer Science, University of Jaume-I, Spain; Institute for Systems, Informatics and SafetyJoint Research Centre, European Commission, Varese, Italy","Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS '96","6 Aug 2002","1996","3","","1345","1352 vol.3","This paper presents a neural controller that learns goal-oriented obstacle-avoiding reaction strategies for a multilink robot arm. It acquires these strategies through reinforcement learning from local sensory data. The robot arm has rings of range sensors placed along its links. The neural controller achieves a good performance quite rapidly and shows good generalization abilities in the face of new environments. Suitable input and output codification schemes help greatly to attain these aims. The input codification exploits the inherent symmetry of the robot kinematics and the action given by the controller is interpreted with regard to the shortest path vector (SPV) to the closest goal in the configuration space. In order to avoid the SPV computation for multilink manipulators, we put forward the use of a module for differential inverse kinematics based on the inversion of a neural network that has been previously trained to approximate the manipulator forward kinematics. The use of this module does not only get round the SPV calculation, but also speeds up the learning process.","","0-7803-3213-X","10.1109/IROS.1996.568991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=568991","","Learning;Robot sensing systems;Manipulators;Orbital robotics;Robotics and automation;Control systems;Artificial neural networks;Computer science;Robot kinematics;Computer networks","learning (artificial intelligence);robot programming;intelligent control;manipulator kinematics;neurocontrollers;generalisation (artificial intelligence);path planning","reinforcement learning;sensor-based reaching strategies;two-link manipulator;neural controller;goal-oriented obstacle-avoiding reaction strategies;multilink robot arm;generalization abilities;robot kinematics;shortest path vector","","5","","15","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Development of an imitation behavior in humanoid Kenta with reinforcement learning algorithm based on the attention during imitation","T. Yoshikai; N. Otake; I. Mizuuchi; M. Inaba; H. Inoue","dept. Mechano-Informatics, University of Tokyo, Tokyo, Japan; dept. Mechano-Informatics, University of Tokyo, Tokyo, Japan; dept. Mechano-Informatics, University of Tokyo, Tokyo, Japan; dept. Mechano-Informatics, University of Tokyo, Tokyo, Japan; dept. Mechano-Informatics, University of Tokyo, Tokyo, Japan","2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)","14 Feb 2005","2004","2","","1192","1197 vol.2","Since an environment or body states of robots are very changeable, robotic imitation systems should have ability to develop their behaviors by themselves. For the development of the imitation behaviors, we assume that attention during imitation is the key information. In order to realize such imitation systems with evolving ability, the idea of reinforcement learning system based on the attention structure during imitation has been presented in this paper. First, for describing the attention during imitation behaviors, we define the term 'sensor-action attention pair' as the pair of the most important sensor information and the focused body parts during that behavior. Second, we introduce R-learning, the reinforcement learning method for continual tasks such as imitation behaviors. Third, the method to design the state-action space and the reward function based on the sensor-action attention pair is proposed. At last, for the confirmation of the function of the proposed imitation behavior system, we have done some experiments using actual humanoid Kenta. In those experiments, Kenta can develop imitation behavior that imitates the hand position of the human.","","0-7803-8463-6","10.1109/IROS.2004.1389558","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1389558","","Learning;Humans;Robotics and automation;Intelligent robots;Sensor phenomena and characterization;Design methodology;Robust control;Automatic control;Control systems;Uncertainty","humanoid robots;mobile robots;learning (artificial intelligence);state-space methods","reinforcement learning algorithm;humanoid Kenta;imitation behavior;robotic imitation system;state-action space;sensor-action attention pair","","3","","12","IEEE","14 Feb 2005","","","IEEE","IEEE Conferences"
"Haptic Assistance via Inverse Reinforcement Learning","D. R. R. Scobee; V. Rubies Royo; C. J. Tomlin; S. S. Sastry","Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley","2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","17 Jan 2019","2018","","","1510","1517","In assistive teleoperation, an autonomous agent uses a prediction about a human user's intent to attempt to align the behavior of a controlled system with the human's goal, even if the human's own inputs are not perfectly aligned to that goal. Haptic Assistance achieves this effect by influencing the human through forces/torques applied to the human's control interface. In this work, we describe our method for creating such haptic assistance via Inverse Reinforcement Learning applied to successful task demonstrations. We then use our assistance method to examine the role that haptic feedback plays in assistive teleoperation. Through our user study, we find that when the assistance incorrectly predicts a user's intent, aiding the user via haptic feedback on their control interface, rather than directly modifying their input signal, is preferable and provides the user with a significantly greater sense of control over the system.","2577-1655","978-1-5386-6650-0","10.1109/SMC.2018.00262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8616258","","Haptic interfaces;Task analysis;Trajectory;Automation;Robots;Force","haptic interfaces;human-robot interaction;learning (artificial intelligence);telerobotics","haptic feedback;assistive teleoperation;control interface;haptic assistance;Inverse Reinforcement Learning;human user;controlled system","","3","","36","IEEE","17 Jan 2019","","","IEEE","IEEE Conferences"
"A new two-layer reinforcement learning approach the control of a 2DOF manipulator","A. Albers; S. Schillo; D. Sonnleithner; M. Frietsch; P. Meckl","Institute of Product Development, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Product Development, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Product Development, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Product Development, Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Mechanical Engineering, Purdue University, West Lafayette, IN, USA","IEEE ICCA 2010","26 Jul 2010","2010","","","546","551","This paper presents a new machine learning approach, based on reinforcement learning, to control a highly nonlinear robot demonstrator. Learning is achieved using an on-policy temporal difference learning agent framework. The agent controls the movements of the robot by choosing torques for each joint and immediately receives a feedback signal. The implemented SARSA-agent uses an update rule that calculates the estimate of the current state-action by using the current state-action value, the received reward and the following state-action value. To handle the high number of state-action value pairs, a hash-table is used to efficiently store and access these values inside the lookup-table. To accelerate the learning process of more complex motions, a new second layer approach is introduced. In this approach, a library of simple motions is created in the first layer. The second layer-agent then combines the gathered experiences to achieve a faster solution for more complex motions. The evaluation of the two-layer agent shows that the combination of both layers dramatically increases the speed of finding a solution. Additionally, its solution is often better than the solution found by the pure reinforcement learning agent.","1948-3457","978-1-4244-5195-1","10.1109/ICCA.2010.5524384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5524384","","Automatic control;Manipulators;Robots;Machine learning;Acceleration;Control systems;Machine learning algorithms;Automation;Torque control;State estimation","control engineering computing;feedback;learning (artificial intelligence);manipulators;motion control;nonlinear control systems;table lookup;torque control","two-layer reinforcement learning approach;2DOF manipulator control;machine learning;nonlinear robot demonstrator;on-policy temporal difference learning agent;robot movements controls;torques;feedback signal;SARSA-agent;state-action value;hash-table;lookup-table;complex motions;motion control","","2","","15","IEEE","26 Jul 2010","","","IEEE","IEEE Conferences"
"Federated Reinforcement Learning for Automatic Control in SDN-based IoT Environments","H. -K. Lim; J. -B. Kim; S. -Y. Kim; Y. -H. Han","Department of Interdisciplinary Program in Creative Engineering, Korea University of Technology and Education; Department of Computer Science Engineering, Korea University of Technology and Education; Advanced Technology Research Center, Korea University of Technology and Education; Advanced Technology Research Center, Korea University of Technology and Education","2020 International Conference on Information and Communication Technology Convergence (ICTC)","21 Dec 2020","2020","","","1868","1873","Recently, reinforcement learning has been applied to various fields and shows better performance than humans. In particular, it is attracting attention in the fields of smart factories and robotics that require automatic control without human intervention. In this paper, we try to allow multiple reinforcement learning agents to learn optimal control policy on their own IoT devices of the same type. There is no guarantee that the reinforcement learning agent that has learned the optimal control policy using one IoT device will perform optimal control of other IoT devices. Therefore, since reinforcement learning must be performed individually for each IoT device, it takes a lot of time and cost. To solve this problem, we propose a new method of federated reinforcement learning. In the proposed federated reinforcement learning, multiple agents have independent IoT devices, perform learning at the same time, and federate with each other to improve learning performance. Therefore, we apply a new gradient sharing method and transfer learning to reinforcement learning. In addition, Actor-Critic PPO, which shows good performance in reinforcement learning algorithms, is used. And, for smooth learning in the IoT environment where numerous devices exist, we propose an architecture based on Software-Defined Networking. Using multiple rotary inverted pendulum devices interconnected via a SDN, we demonstrate that the proposed federated reinforcement learning scheme can effectively facilitate the learning process for multiple IoT devices.","2162-1233","978-1-7281-6758-9","10.1109/ICTC49870.2020.9289245","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9289245","Federated reinforcement learning;Multi-IoT device control;Software-Defined networking","Performance evaluation;Optimal control;Reinforcement learning;Information and communication technology;Robots;Smart manufacturing;Convergence","Internet of Things;learning (artificial intelligence);multi-agent systems;nonlinear control systems;optimal control;pendulums","automatic control;SDN-based IoT environments;multiple reinforcement;optimal control policy;IoT device;reinforcement learning agent;independent IoT devices;learning performance;reinforcement learning algorithms;smooth learning;IoT environment;federated reinforcement learning scheme;learning process;multiple IoT devices","","1","","23","IEEE","21 Dec 2020","","","IEEE","IEEE Conferences"
"Automated Driving Highway Traffic Merging using Deep Multi-Agent Reinforcement Learning in Continuous State-Action Spaces","L. Schester; L. E. Ortiz",NA; NA,"2021 IEEE Intelligent Vehicles Symposium (IV)","1 Nov 2021","2021","","","280","287","Achieving the highest levels of automated driving will require effective solutions to the key challenging maneuver of highway on-ramp merging. This paper extends our previous work on a multi-agent reinforcement-learning (MARL) approach to study the problem of highway on-ramp merging, with particular emphasis on the study of the behavior of the vehicle that is on the on-ramp with approaching traffic. Our previous model was based on a discretized space of states and actions. Here, we present results on a more sophisticated model based on a continuous space of states and actions. We exploit recent advances on deep reinforcement learning (deep RL) to train controllers for this task in an idealized environment using an implementation of our MARL approach. We specifically employ artificial neural network architectures for policy and function approximation within our multiagent Q-learning approach. We show the effectiveness of our trained controllers by demonstrating their collision-avoidance performance on interaction scenarios with different in-traffic behavior. We compare their performance to those obtained using a similar deep RL single-agent approach. We argue why the resulting MARL-based controllers are essentially optimal within the context, conditions, and parameters of the evaluation environment that we employ and our previously established fundamental performance limitations governing the highway on-ramp merging maneuver.","","978-1-7281-5394-0","10.1109/IV48863.2021.9575676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9575676","","Road transportation;Automation;Intelligent vehicles;Merging;Reinforcement learning;Artificial neural networks;Aerospace electronics","collision avoidance;function approximation;learning (artificial intelligence);multi-agent systems;neural nets","driving highway traffic merging;deep multiagent reinforcement learning;continuous state-action spaces;highest levels;automated driving;key challenging maneuver;multiagent reinforcement-learning approach;approaching traffic;discretized space;sophisticated model;continuous space;deep reinforcement learning;MARL approach;artificial neural network architectures;multiagent Q-learning approach;trained controllers;in-traffic behavior;similar deep RL single-agent approach;resulting MARL-based controllers;highway on-ramp merging maneuver","","1","","20","IEEE","1 Nov 2021","","","IEEE","IEEE Conferences"
"Prioritizing automated test cases of Web applications using reinforcement learning: an enhancement","H. -G. Nguyen; H. -D. Le; V. Nguyen","Faculty of Information Technology, University of Science, Ho Chi Minh city, Vietnam; Faculty of Information Technology, University of Science, Ho Chi Minh city, Vietnam; Faculty of Information Technology, University of Science, Ho Chi Minh city, Vietnam","2021 13th International Conference on Knowledge and Systems Engineering (KSE)","28 Dec 2021","2021","","","1","8","Test prioritization helps reduce the time needed to perform testing on the target application under test. It is even more critical when there are lots of tests to be tested within a short period. This paper presents a test prioritization method that enhances our previous method for prioritizing automated tests of Web-based applications using reinforcement learning. The main improvements are focused on the reward function of reinforcement learning and the graph merge-discount factor. We evaluate our method and other six recent test prioritization methods using eleven data sets. The results show that the proposed method outperforms the other methods on most data sets.","2694-4804","978-1-6654-9975-0","10.1109/KSE53942.2021.9648835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9648835","Test case ranking;coverage graph;reinforcement learning;graphical user interface;automated testing","Measurement;Knowledge engineering;Automation;Reinforcement learning;Systems engineering and theory;Optimization;Open source software","Internet;program testing;reinforcement learning","reinforcement learning;automated test cases;test prioritization method;Web-based applications","","","","21","IEEE","28 Dec 2021","","","IEEE","IEEE Conferences"
"Towards Scalable Verification of Deep Reinforcement Learning","G. Amir; M. Schapira; G. Katz","The Hebrew University of Jerusalem, Jerusalem, Israel; The Hebrew University of Jerusalem, Jerusalem, Israel; The Hebrew University of Jerusalem, Jerusalem, Israel","2021 Formal Methods in Computer Aided Design (FMCAD)","29 Nov 2021","2021","","","193","203","Deep neural networks (DNNs) have gained significant popularity in recent years, becoming the state of the art in a variety of domains. In particular, deep reinforcement learning (DRL) has recently been employed to train DNNs that realize control policies for various types of real-world systems. In this work, we present the whiRL 2.0 tool, which implements a new approach for verifying complex properties of interest for DRL systems. To demonstrate the benefits of whiRL 2.0, we apply it to case studies from the communication networks domain that have recently been used to motivate formal verification of DRL systems, and which exhibit characteristics that are conducive for scalable verification. We propose techniques for performing k-induction and semi-automated invariant inference on such systems, and leverage these techniques for proving safety and liveness properties that were previously impossible to verify due to the scalability barriers of prior approaches. Furthermore, we show how our proposed techniques provide insights into the inner workings and the generalizability of DRL systems. whiRL 2.0 is publicly available online.","2708-7824","978-3-85448-046-4","10.34727/2021/isbn.978-3-85448-046-4_28","Israel Science Foundation; Hebrew University of Jerusalem; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9617684","","Deep learning;Design automation;Scalability;Reinforcement learning;Tools;Safety;Communication networks","deep learning (artificial intelligence);program verification","whiRL 2.0 tool;scalability barriers;liveness properties;proving safety;k-induction;formal verification;communication networks domain;DRL systems;real-world systems;control policies;DNNs;deep neural networks;deep reinforcement learning;scalable verification","","","","60","","29 Nov 2021","","","IEEE","IEEE Conferences"
"Towards High-Quality CGRA Mapping with Graph Neural Networks and Reinforcement Learning","Y. Zhuang; Z. Zhang; D. Liu","College of Computer Science, Chongqing University; College of Computer Science, Chongqing University; College of Computer Science, Chongqing University","2022 IEEE/ACM International Conference On Computer Aided Design (ICCAD)","22 Mar 2023","2022","","","1","9","Coarse-Grained Reconfigurable Architectures (CGRA) is a promising solution to accelerate domain applications due to its good combination of energy-efficiency and flexibility. Loops, as computation-intensive parts of applications, are often mapped onto CGRA and modulo scheduling is commonly used to improve the execution performance. However, the actual performance using modulo scheduling is highly dependent on the mapping ability of the Data Dependency Graph (DDG) extracted from a loop. As existing approaches usually separate routing exploration of multi-cycle dependence from mapping for fast compilation, they may easily suffer from poor mapping quality. In this paper, we integrate the routing explorations into the mapping process and make it have more opportunities to find a globally optimized solution. Meanwhile, with a reduced resource graph defined, the searching space of the new mapping problem is not greatly increased. To efficiently solve the problem, we introduce graph neural network based reinforcement learning to predict a placement distribution over different resource nodes for all operations in a DDG. Using the routing connectivity as the reward signal, we optimize the parameters of neural network to find a valid mapping solution with a policy gradient method. Without much engineering and heuristic designing, our approach achieves 1.57× mapping quality, as compared to the state-of-the-art heuristic.","1558-2434","978-1-4503-9217-4","","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10069423","CGRA;Modulo Scheduling;Graph Neural Network;Reinforcement Learning","Gradient methods;Design automation;Processor scheduling;Reinforcement learning;Routing;Search problems;Graph neural networks","","","","","","23","","22 Mar 2023","","","IEEE","IEEE Conferences"
"Reinforcement learning in multi-dimensional state-action space using random rectangular coarse coding and gibbs sampling","Hajime Kimura","Department of Marine Engineering, Graduate School of Engineering, Kyushu University, Fukuoka, Japan","SICE Annual Conference 2007","7 Jan 2008","2007","","","2754","2761","This paper presents a coarse coding technique and an action selection scheme for reinforcement learning (RL) in multi-dimensional and continuous state-action spaces following conventional and sound RL manners. RL in highdimensional continuous domains includes two issues: One is a generalization problem for value-function approximation, and the other is a sampling problem for action selection over multi-dimensional continuous action spaces. The proposed method combines random rectangular coarse coding with an action selection scheme using Gibbs-sampling. The random rectangular coarse coding is very simple and quite suited both to approximate Q-functions in high-dimensional spaces and to execute Gibbs sampling. Gibbs sampling enables us to execute action selection following Boltsmann distribution over high-dimensional action space.","","978-4-907764-27-2","10.1109/SICE.2007.4421457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4421457","Reinforcement learning;Q-learning;continuous state-action spaces;Function approximation","Learning;Sampling methods;Function approximation;Costs;Approximation algorithms;Acoustical engineering;State-space methods;Orbital robotics;Robotics and automation;Automatic control","function approximation;learning (artificial intelligence);sampling methods","reinforcement learning;multidimensional state-action space;random rectangular coarse coding;Gibbs sampling;action selection scheme;continuous state-action spaces;value-function approximation;Q-function approximation;Boltsmann distribution","","","","11","","7 Jan 2008","","","IEEE","IEEE Conferences"
"Associative Memory Based Experience Replay for Deep Reinforcement Learning","M. Li; A. Kazemi; A. F. Laguna; X. S. Hu","Department of Computer Science and Engineering, University of Notre Dame, USA; Department of Computer Science and Engineering, University of Notre Dame, USA; De La Salle University, Manila, Philippines; Department of Computer Science and Engineering, University of Notre Dame, USA","2022 IEEE/ACM International Conference On Computer Aided Design (ICCAD)","22 Mar 2023","2022","","","1","9","Experience replay is an essential component in deep reinforcement learning (DRL), which stores the experiences and generates experiences for the agent to learn in real time. Recently, prioritized experience replay (PER) has been proven to be powerful and widely deployed in DRL agents. However, implementing PER on traditional CPU or GPU architectures incurs significant latency overhead due to its frequent and irregular memory accesses. This paper proposes a hardware-software co-design approach to design an associative memory (AM) based PER, AMPER, with an AM-friendly priority sampling operation. AMPER replaces the widely-used time-costly tree-traversal-based priority sampling in PER while preserving the learning performance. Further, we design an in-memory computing hardware architecture based on AM to support AMPER by leveraging parallel in-memory search operations. AMPER shows comparable learning performance while achieving 55× to 270× latency improvement when running on the proposed hardware compared to the state-of-the-art PER running on GPU.","1558-2434","978-1-4503-9217-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10069690","","Deep learning;Associative memory;Design automation;Accelerated aging;Graphics processing units;Computer architecture;Reinforcement learning","content-addressable storage;deep learning (artificial intelligence);graphics processing units;parallel processing;performance evaluation;real-time systems;reinforcement learning","AM friendly priority sampling operation;AMPER;associative memory;comparable learning performance;CPU artchitectures;deep reinforcement learning;DRL agents;frequent memory accesses;GPU architectures;hardware software codesign approach;in-memory computing hardware architecture;irregular memory accesses;learning performance;parallel in-memory search operations;PER;prioritized experience replay;priority sampling;time costly tree traversal","","","","22","","22 Mar 2023","","","IEEE","IEEE Conferences"
"A Scalable Privacy-Preserving Multi-Agent Deep Reinforcement Learning Approach for Large-Scale Peer-to-Peer Transactive Energy Trading","Y. Ye; Y. Tang; H. Wang; X. -P. Zhang; G. Strbac","School of Electrical Engineering, Southeast University, Nanjing, China; School of Electrical Engineering, Southeast University, Nanjing, China; School of Electrical Engineering, Southeast University, Nanjing, China; School of Electronic, Electrical and Computer Engineering, University of Birmingham, Birmingham, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.","IEEE Transactions on Smart Grid","20 Oct 2021","2021","12","6","5185","5200","Peer-to-peer (P2P) transactive energy trading has emerged as a promising paradigm towards maximizing the flexibility value of prosumers’ distributed energy resources (DERs). Despite reinforcement learning constitutes a well-suited model-free and data-driven methodological framework to optimize prosumers’ energy management decisions, its application to the large-scale coordinated management and P2P trading among multiple prosumers within an energy community is still challenging, due to the scalability, non-stationarity and privacy limitations of state-of-the-art multi-agent deep reinforcement learning (MADRL) approaches. This paper proposes a novel P2P transactive trading scheme based on the multi-actor-attention-critic (MAAC) algorithm, which addresses the above challenges individually. This method is complemented by a P2P trading platform that incentivizes prosumers to engage in local energy trading while also penalizes each prosumer’s addition to rebound peaks. Case studies involving a real-world, large-scale scenario with 300 residential prosumers demonstrate that the proposed method significantly outperforms the state-of-the-art MADRL methods in reducing the community’s cost and peak demand.","1949-3061","","10.1109/TSG.2021.3103917","National Natural Science Foundation of China(grant numbers:51877037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9509579","Distributed energy resources;energy management;local energy community;multi-agent deep reinforcement learning;peer-to-peer transactive energy trading","HVAC;Peer-to-peer computing;Transactive energy;Scalability;Uncertainty;Reinforcement learning;Production","data privacy;distributed power generation;learning (artificial intelligence);multi-agent systems;optimisation;peer-to-peer computing;smart power grids","scalable privacy-preserving multiagent deep reinforcement learning approach;peer-to-peer transactive energy trading;well-suited model-free;large-scale coordinated management;multiple prosumers;energy community;privacy limitations;state-of-the-art multiagent deep reinforcement learning;novel P2P transactive trading scheme;multiactor-attention-critic algorithm;P2P trading platform;local energy trading;prosumer;large-scale scenario;300 residential prosumers;state-of-the-art MADRL methods","","39","","54","IEEE","10 Aug 2021","","","IEEE","IEEE Journals"
"Multi-agent reinforcement learning for microgrids","A. L. Dimeas; N. D. Hatziargyriou","National and Technical University of Athens, Greece; National and Technical University of Athens, Greece","IEEE PES General Meeting","30 Sep 2010","2010","","","1","8","This paper presents a general framework for Microgrids control based on Multi Agent System Technology. The proposed architecture is capable to integrate several functionalities, adaptable to the complexity and the size of the Microgrid. To achieve this, the idea of layered learning is used, where the various controls and actions of the agents are grouped depending on their effect on the environment. Moreover this paper, focus on how the agent will cooperate in order to achieve their goals. The core of the cooperation is a Multi Agent Reinforcement Learning Algorithm that allows the system to operate autonomously in island mode.","1944-9925","978-1-4244-6551-4","10.1109/PES.2010.5589633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5589633","Distributed Generation;Microgrids;Multi Agent System;Reinforcement Learning;Layered Learning;Island Operation","Production;Learning;Water heating;Fuels;Complexity theory;Batteries","multi-agent systems;power grids;power system control","microgrid control;multiagent reinforcement learning;layered learning;autonomous operation;island mode","","32","","26","IEEE","30 Sep 2010","","","IEEE","IEEE Conferences"
"A Neural Reinforcement Learning Approach to Gas Turbine Control","A. M. Schaefer; D. Schneegass; V. Sterzing; S. Udluft","Department of Learning Systems, Information & Communications, Corporate Technology, Siemens AG, Munich, Germany; Department of Learning Systems, Information & Communications, Corporate Technology, Siemens AG, Munich, Germany; Department of Learning Systems, Information & Communications, Corporate Technology, Siemens AG, Munich, Germany; Department of Learning Systems, Information & Communications, Corporate Technology, Siemens AG, Munich, Germany","2007 International Joint Conference on Neural Networks","29 Oct 2007","2007","","","1691","1696","In this paper a new neural network based approach to control a gas turbine for stable operation on high load is presented. A combination of recurrent neural networks (RNN) and reinforcement learning (RL) is used. The authors start by applying an RNN to identify the minimal state space of a gas turbine's dynamics. Based on this the optimal control policy is determined by standard RL methods. The authors proceed to the recurrent control neural network, which combines these two steps into one integrated neural network. This approach has the advantage that by using neural networks one can easily deal with the high dimensions of a gas turbine. Due to the high system-identification quality of RNN one can further cope with the only limited amount of available data. The proposed methods are demonstrated on an exemplary gas turbine model where, compared to standard controllers, it strongly improves the performance.","2161-4407","978-1-4244-1379-9","10.1109/IJCNN.2007.4371212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4371212","","Learning;Turbines;Recurrent neural networks;Neural networks;State-space methods;Optimal control;System identification;Renewable energy resources;Power generation;Production","control engineering computing;gas turbines;learning (artificial intelligence);neural nets;power engineering computing","neural reinforcement learning;gas turbine control;recurrent control neural network;integrated neural network;RNN","","15","11","15","IEEE","29 Oct 2007","","","IEEE","IEEE Conferences"
"Interplay of Rhythmic and Discrete Manipulation Movements During Development: A Policy-Search Reinforcement-Learning Robot Model","V. C. Meola; D. Caligiore; V. Sperati; L. Zollo; A. L. Ciancio; F. Taffoni; E. Guglielmelli; G. Baldassarre","Biomedical Robotics and Biomicrosystem Lab, Universita’ Campus Bio-Medico di Roma, Roma, Italy; Laboratory of Computational Embodied Neuroscience, Consiglio Nazionale delle Ricerche (LOCEN-ISTC-CNR), Roma, Italy; Laboratory of Computational Embodied Neuroscience, Consiglio Nazionale delle Ricerche (LOCEN-ISTC-CNR), Roma, Italy; Biomedical Robotics and Biomicrosystem Lab, Universita’ Campus Bio-Medico di Roma, Roma, Italy; Biomedical Robotics and Biomicrosystem Lab, Universita’ Campus Bio-Medico di Roma, Roma, Italy; Biomedical Robotics and Biomicrosystem Lab, Universita’ Campus Bio-Medico di Roma, Roma, Italy; Biomedical Robotics and Biomicrosystem Lab, Universita’ Campus Bio-Medico di Roma, Roma, Italy; Laboratory of Computational Embodied Neuroscience, Consiglio Nazionale delle Ricerche (LOCEN-ISTC-CNR), Roma, Italy","IEEE Transactions on Cognitive and Developmental Systems","20 May 2017","2016","8","3","152","170","The flexibility of human motor behavior strongly relies on rhythmic and discrete movements. Developmental psychology has shown how these movements closely interplay during development, but the dynamics of that are largely unknown and we currently lack computational models suitable to investigate such interaction. This work initially presents an analysis of the problem from a computational and empirical perspective and then proposes a novel computational model to start to investigate it. The model is based on a movement primitive capable of producing both rhythmic and end-point discrete movements, and on a policy search reinforcement learning algorithm capable of mimicking trial-and-error learning processes underlying development and efficient enough to work on real robots. The model is tested with hand manipulation tasks (“touching,” “tapping,” and “rotating” an object). The results show how the system progressively shapes the initial rhythmic exploration into refined rhythmic or discrete movements depending on the task demand. The tests on the real robot also show how the system exploits the specific hand-object physical properties, some possibly shared with developing infants, to find effective solutions to the tasks. The results show that the model represents a useful tool to investigate the interplay of rhythmic and discrete movements during development.","2379-8939","","10.1109/TAMD.2015.2494460","European Commission(grant numbers:FP7-ICT-IP-231722); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7303905","Central patter generators;developmental robotics;iCub robot hand;motor babbling;motor system and development;policy search reinforcement learning;rhythmic and discrete movement primitives","Robot sensing systems;Computational modeling;Oscillators;Joints;Generators;Production","biomechanics;learning (artificial intelligence);manipulator dynamics","rhythmic interplay;discrete manipulation movements;policy-search reinforcement-learning robot model;human motor behavior;developmental psychology;end-point discrete movements;policy search reinforcement learning algorithm;mimicking trial-and-error learning processes;hand manipulation tasks;specific hand-object physical properties","","15","","128","OAPA","26 Oct 2015","","","IEEE","IEEE Journals"
"Wind Farm Power Generation Control Via Double-Network-Based Deep Reinforcement Learning","J. Xie; H. Dong; X. Zhao; A. Karcanias","Intelligent Control and Smart Energy (ICSE) Research Group, School of Engineering, University of Warwick, Coventry, U.K.; Intelligent Control and Smart Energy (ICSE) Research Group, School of Engineering, University of Warwick, Coventry, U.K.; Intelligent Control and Smart Energy (ICSE) Research Group, School of Engineering, University of Warwick, Coventry, U.K.; PA Consulting, London, U.K.","IEEE Transactions on Industrial Informatics","5 Jan 2022","2022","18","4","2321","2330","A model-free deep reinforcement learning (DRL) method is proposed in this article to maximize the total power generation of wind farms through the combination of induction control and yaw control. Specifically, a novel double-network (DN)-based DRL approach is designed to generate control policies for thrust coefficients and yaw angles simultaneously and separately. Two sets of critic-actor networks are constructed to this end. They are linked by a central power-related reward, providing a coordinated control structure while inheriting the critic-actor mechanism's advantages. Compared with conventional DRL methods, the proposed DN-based DRL strategy can adapt to the distinctive and incompatible features of different control inputs, guaranteeing a reliable training process and ensuring superior performance. Also, the prioritized experience replay strategy is utilized to improve the training efficiency of deep neural networks. Simulation tests based on a dynamic wind farm simulator show that the proposed method can significantly increase the power generation for wind farms with different layouts.","1941-0050","","10.1109/TII.2021.3095563","Engineering and Physical Sciences Research Council(grant numbers:EP/S000747/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9478204","Model-free control;power generation control;reinforcement learning (RL);wind farm control","Wind farms;Power generation;Informatics;Wind turbines;Training;Production;Optimal control","control engineering computing;deep learning (artificial intelligence);power generation control;wind power plants","yaw control;control policies;thrust coefficients;yaw angles;critic-actor networks;central power-related reward;coordinated control structure;DN-based DRL strategy;deep neural networks;wind farm power generation control;model-free deep reinforcement learning;total power generation;induction control;double-network-based DRL approach;dynamic wind farm simulator","","14","","34","IEEE","8 Jul 2021","","","IEEE","IEEE Journals"
"A Reinforcement Learning Approach for User Preference-Aware Energy Sharing Systems","A. Timilsina; A. R. Khamesi; V. Agate; S. Silvestri","Department of Computer Science, University of Kentucky, Lexington, KY, USA; Department of Computer Science, University of Kentucky, Lexington, KY, USA; Department of Engineering, University of Palermo, Palermo, Italy; Department of Computer Science, University of Kentucky, Lexington, KY, USA","IEEE Transactions on Green Communications and Networking","23 Aug 2021","2021","5","3","1138","1153","Energy Sharing Systems (ESS) are envisioned to be the future of power systems. In these systems, consumers equipped with renewable energy generation capabilities are able to participate in an energy market to sell their energy. This paper proposes an ESS that, differently from previous works, takes into account the consumers' preference, engagement, and bounded rationality. The problem of maximizing the energy exchange while considering such user modeling is formulated and shown to be NP-Hard. To learn the user behavior, two heuristics are proposed: 1) a Reinforcement Learning-based algorithm, which provides a bounded regret and 2) a more computationally efficient heuristic, named BPT- K, with guaranteed termination and correctness. A comprehensive experimental analysis is conducted against state-of-the-art solutions using realistic datasets. Results show that including user modeling and learning provides significant performance improvements compared to state-of-the-art approaches. Specifically, the proposed algorithms result in 25% higher efficiency and 27% more transferred energy. Furthermore, the learning algorithms converge to a value less than 5% of the optimal solution in less than 3 months of learning.","2473-2400","","10.1109/TGCN.2021.3077854","National Institute for Food and Agriculture (NIFA)(grant numbers:2017-67008-26145); NSF(grant numbers:EPCN 1936131); NSF CAREER(grant numbers:CPS-1943035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9424189","Energy sharing systems;virtual power plants;reinforcement learning;user preference","Power generation;Renewable energy sources;Production;Energy exchange;Reinforcement learning;Green products;Coal","learning (artificial intelligence);optimisation;power engineering computing;power markets;user modelling","reinforcement learning approach;renewable energy generation capabilities;energy market;energy exchange;user modeling;transferred energy;learning algorithms;user preference-aware energy sharing system;NP-hard;BPT- K;optimal solution","","10","","68","IEEE","5 May 2021","","","IEEE","IEEE Journals"
"Enabling peer-to-peer User-Preference-Aware Energy Sharing Through Reinforcement Learning","V. Agate; A. R. Khamesi; S. Silvestri; S. Gaglio","Department of Industrial and Digital Innovation, University of Palermo, Palermo, Italy; Department of Computer Science, University of Kentucky, Lexington, KY, USA; Department of Computer Science, University of Kentucky, Lexington, KY, USA; Department of Industrial and Digital Innovation, University of Palermo, Palermo, Italy","ICC 2020 - 2020 IEEE International Conference on Communications (ICC)","27 Jul 2020","2020","","","1","7","Renewable, heterogeneous and distributed energy resources are the future of power systems, as envisioned by the recent paradigm of Virtual Power Plants (VPPs). Residential electricity generation, e.g., through photovoltaic panels, plays a fundamental role in this paradigm, where users are able to participate in an energy sharing system and exchange energy resources among each other. In this work, we study energy sharing systems and, differently from previous approaches, we consider realistic user behaviors by taking into account the user preferences and level of engagement in the energy trades. We formulate the problem of matching energy resources while contemplating the user behavior as a Mixed Integer Linear Programming (MILP) problem, and show that the problem is NPHard. Since the solution of such problem requires the knowledge of the user behavioral model, we propose an heuristic based on reinforcement learning with bounded regret to learn such model while optimizing the system performance. Comparison with the state-of-the-art approaches using realistic simulations based on real traces shows that our method outperforms existing schemes in several efficiency metrics. Besides, the results reveal that increasing the amount of produced energy improves the learning ability of the system even in a short period. It gives practical insights for implementation of energy sharing systems.","1938-1883","978-1-7281-5089-5","10.1109/ICC40277.2020.9149337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9149337","Energy Sharing;Virtual Power Plant;Reinforcement Learning","Power generation;Learning (artificial intelligence);System performance;Production;Energy resources;Smart grids;Renewable energy sources","distributed power generation;integer programming;learning (artificial intelligence);linear programming;peer-to-peer computing","reinforcement learning;renewable distributed energy resources;heterogeneous distributed energy resources;power systems;virtual power plants;energy sharing system;realistic user behaviors;user preferences;energy trades;user behavior;user behavioral model;peer-to-peer user-preference-aware energy sharing;mixed integer linear programming problem;VPP;MILP","","9","","39","IEEE","27 Jul 2020","","","IEEE","IEEE Conferences"
"Distributed inter-domain SLA negotiation using Reinforcement Learning","T. Groléat; H. Pouyllau","Telecom Bretagne; Alcatel Lucent Bell Labs France, France","12th IFIP/IEEE International Symposium on Integrated Network Management (IM 2011) and Workshops","18 Aug 2011","2011","","","33","40","Applications requiring network Quality of Service (QoS) (e.g. telepresence, cloud computing, etc.) are becoming mainstream. To support their deployment, network operators must automatically negotiate end-to-end QoS contracts (aka. Service Level Agreements, SLAs) and configure their networks accordingly. Other crucial needs must be considered: QoS should provide incentives to network operators, and confidentiality on topologies, resource states and committed SLAs must be respected. To meet these requirements, we propose two distributed learning algorithms that will allow network operators to negotiate end-to-end SLAs and optimize revenues for several demands while treating requests in real-time: one algorithm minimizes the cooperation between providers while the other demands to exchange more information. Experiment results exhibit that the second algorithm satisfies better customers and providers while having worse runtime performances.","1573-0077","978-1-4244-9221-3","10.1109/INM.2011.5990671","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5990671","","Topology;Optimized production technology;Runtime","distributed algorithms;electronic data interchange;learning (artificial intelligence);quality of service;telecommunication computing;telecommunication network topology","distributed interdomain SLA negotiation;reinforcement learning;quality of service;telepresence;cloud computing;network operator;end-to-end QoS contract;service level agreement;network topology;distributed learning algorithm;end-to-end SLA;information exchange","","8","","16","IEEE","18 Aug 2011","","","IEEE","IEEE Conferences"
"Using Deep Reinforcement Learning to Improve Sensor Selection in the Internet of Things","H. Rashtian; S. Gopalakrishnan","Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada","IEEE Access","1 Jun 2020","2020","8","","95208","95222","We study the problem of handling timeliness and criticality trade-off when gathering data from multiple resources in complex environments. In IoT environments, where several sensors transmitting data packets - with various criticality and timeliness, the rate of data collection could be limited due to associated costs (e.g., bandwidth limitations and energy considerations). Besides, environment complexity regarding data generation could impose additional challenges to balance criticality and timeliness when gathering data. For instance, when data packets (either regarding criticality or timeliness) of two or more sensors are correlated, or there exists temporal dependency among sensors, incorporating such patterns can expose challenges to trivial policies for data gathering. Motivated by the success of the Asynchronous Advantage Actor-Critic (A3C) approach, we first mapped vanilla A3C into our problem to compare its performance in terms of criticality-weighted deadline miss ratio to the considered baselines in multiple scenarios. We observed degradation of the A3C performance in complex scenarios. Therefore, we modified the A3C network by embedding long short term memory (LSTM) to improve performance in cases that vanilla A3C could not capture repeating patterns in data streams. Simulation results show that the modified A3C reduces the criticality-weighted deadline miss ratio from 0.3 to 0.19.","2169-3536","","10.1109/ACCESS.2020.2994600","Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant; Peter Wall Institute for Advanced Studies through a research grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093816","Internet of Things;asynchronous advantage actor-critic networks;soft scheduling;deep reinforcement learning;soft real-time systems","Machine learning;Internet of Things;Correlation;Production facilities;Temperature sensors;Complexity theory;Temperature distribution","Internet of Things;learning (artificial intelligence);wireless sensor networks","complex environments;IoT environments;data collection;associated costs;asynchronous advantage actor-critic approach;multiple resources;criticality trade;sensor selection;deep reinforcement learning;data streams;criticality-weighted deadline;vanilla A3C;data gathering;data packets;data generation;environment complexity;energy considerations;bandwidth limitations","","4","","26","CCBY","14 May 2020","","","IEEE","IEEE Journals"
"Online distributed voltage control of an offshore MTdc network using reinforcement learning","S. Rodrigues; R. T. Pinto; P. Bauer; T. Brys; A. Nowé","DC systems, Delft University of Technology, Delft, The Netherlands; DC systems, Delft University of Technology, Delft, The Netherlands; DC systems, Delft University of Technology, Delft, The Netherlands; Artificial Intelligence Lab, Vrije Universiteit Brussel, Brussels, Belgium; DC systems, Delft University of Technology, Delft, The Netherlands","2015 IEEE Congress on Evolutionary Computation (CEC)","14 Sep 2015","2015","","","1769","1775","This paper addresses one of the main challenges on the way to an offshore transnational multi-terminal dc (MTdc) network: its control and operation. The main objective is to demonstrate the feasibility of using reinforcement learning (RL) techniques to control, in real time, a multi-terminal dc network aimed at integrating offshore wind farms (OWFs). This method of controlling MTdc networks using RL techniques is called Online Distributed Voltage Control (ODVC). The ODVC strategy uses Continuous Action Reinforcement Learning Automata (CARLA) to optimize power flows in real time. To validate the effectiveness of the proposed control method, dynamic simulations are carried out using a MTdc grid model composed of six nodes, interconnecting three offshore wind farms to three European countries. The results obtained demonstrate the advantages of implementing an online distributed voltage control strategy to obtain feasible controlled power flows with low transmission losses. The results obtained demonstrate the feasibility of the proposed method to control, in real time, MTdc networks and that the RL techniques are well-suited for this problem due to their inherent advantages of coping with stochastic environments.","1941-0026","978-1-4799-7492-4","10.1109/CEC.2015.7257101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7257101","","Voltage control;Optimization;Numerical models;Learning (artificial intelligence);Propagation losses;Production;Wind farms","control engineering computing;distributed control;learning (artificial intelligence);learning automata;load flow control;multiterminal networks;offshore installations;power engineering computing;voltage control;wind power plants","offshore MTdc network;offshore transnational multiterminal dc network;offshore wind farms;RL techniques;ODVC strategy;continuous action reinforcement learning automata;power flow optimization;MTdc grid model;online distributed voltage control;transmission losses","","3","","44","IEEE","14 Sep 2015","","","IEEE","IEEE Conferences"
"Group utility functions: learning equilibria between groups of agents in computer games by modifying the reinforcement signal","J. Bradley; G. Hayes","Institute of Perception, Action and Behaviour School of Informatics, University of Edinburgh, Edinburgh, UK; Institute of Perception, Action and Behaviour School of Informatics, University of Edinburgh, Edinburgh, UK","2005 IEEE Congress on Evolutionary Computation","12 Dec 2005","2005","2","","1914","1921 Vol. 2","Group utility functions are an expansion of the well known team utility function for providing multiple agents with a common reinforcement learning signal for learning collective behaviour. In this paper we describe what group utility functions are and use them with reinforcement learning to learn non-player character behaviours in a simple computer game. As yet, reinforcement learning techniques have rarely been used for computer game character behaviour specification. Using group utility functions, we can trade some optimality for some other desirable collective behaviour. As an example, in this paper we use group utility functions to learn an equilibrium between groups of agents performing a typical foraging task in a dynamic environment. Group utility functions act as filters on the reinforcement signal and sit between the reward function and the agents. We show several results demonstrating how group utility functions work in practice with varying learning parameters. An earlier paper describes our simpler initial results (Bradley et al., 2005). All our experiments are carried out using a commercial computer game engine.","1941-0026","0-7803-9363-5","10.1109/CEC.2005.1554921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1554921","","Learning;Humans;Informatics;Engines;Teamwork;Automata;Production;Process design","learning (artificial intelligence);multi-agent systems;computer games;formal specification;software agents","group utility functions;agent groups;computer games;reinforcement signal;team utility function;reinforcement learning signal;collective behaviour learning;character behaviour specification;reward function","","3","","22","IEEE","12 Dec 2005","","","IEEE","IEEE Conferences"
"Distributed Reinforcement Learning Framework for Resource Allocation in Disaster Response","C. L. Castellanos; J. R. Marti; S. Sarkaria","Electrical & Computer Engineering, University of British Columbia, Vancouver, Canada; Electrical & Computer Engineering, University of British Columbia, Vancouver, Canada; Electrical & Computer Engineering, University of British Columbia, Vancouver, Canada","2018 IEEE Global Humanitarian Technology Conference (GHTC)","6 Jan 2019","2018","","","1","8","Making decisions during a disaster can be challenging when human lives and infrastructures are exposed. An important factor to consider when allocating resources, in these situations, is the critical infrastructures' interdependencies. i2Sim, the Infrastructures' Interdependencies Simulator, is a tool build for that purpose. As a layered architecture, i2Sim includes a dedicated decision-making layer. The use of Reinforcement Learning (RL), a machine learning approach based on an agent learning from experience, has been successfully tested with some dimensionality constraints. This paper introduces two improvements to our previous tests aiming at increasing speed and allowing larger dimensionality problems. The first addition is an improved reward scheme for speeding up convergence while guaranteeing it. The correct application of shaping rewards requires a deep understanding of the problem and extensive convergence tests. The second improvement added is the implementation of a scheduler programmed to trigger multiple instances of the same model using different parameters. This scheduler partitions the state space, enabling the agent's training to be done in parallel via a distributed RL algorithm. With this idea, the state/action matrix representing knowledge is partitioned for training, assigned to computing nodes, populated with knowledge (trained) and collected/reconstructed for use. This work has tested on an IBM cluster with 24 computing nodes. The test model is an aggregated model of the City of Vancouver configured for a disaster with numerous consequences over the different critical infrastructures. Based on the model's configuration, 24 scenarios were identified, created and solved simultaneously. The scheduler automates the training by setting up model and learning parameters, looping execution of instances and gathering results from all nodes. The results verify a proof of concept and enable applicability to new models with highly increased dimensionalities.","2377-6919","978-1-5386-5566-5","10.1109/GHTC.2018.8601911","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8601911","disaster response;critical infrastructures;decision support;machine learning;reinforcement learning","Reinforcement learning;Ontologies;Critical infrastructure;Computer architecture;Computational modeling;Production;Tools","critical infrastructures;emergency management;learning (artificial intelligence);resource allocation","reward scheme;convergence tests;critical infrastructures;distributed reinforcement learning framework;infrastructures interdependencies simulator;learning parameters;state/action matrix;distributed RL algorithm;dedicated decision-making layer;layered architecture;i2Sim;human lives;disaster response;resource allocation","","2","","22","IEEE","6 Jan 2019","","","IEEE","IEEE Conferences"
