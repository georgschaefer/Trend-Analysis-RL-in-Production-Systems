@inproceedings{10.1145/3307772.3328281,
author = {Subramanian, Easwar and Bichpuriya, Yogesh and Achar, Avinash and Bhat, Sanjay and Singh, Abhay Pratap and Sarangan, Venkatesh and Natarajan, Akshaya},
title = {LEarn: A Reinforcement Learning Based Bidding Strategy for Generators in Single Sided Energy Markets},
year = {2019},
isbn = {9781450366717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307772.3328281},
doi = {10.1145/3307772.3328281},
abstract = {We aim to increase the profit of a given generator participating in a single-sided wholesale energy market. We model the market clearing mechanism and the behavior of other generators competing in the market. We utilize interesting structures in the data to classify generators and then build novel supervised competition models for each class of generators. We leverage these models to build an interactive system through which we discover better bidding strategies for the given generator using reinforcement learning (RL). We relax several assumptions made in existing works in order to make the problem more relevant to real life. Our MDP formulation enables us to tackle action space explosion in an efficient way. Further, our state formulation enables us to compute optimal actions across all time-steps of the day in parallel. We compare the performance of the proposed RL based bidding agent with the historical real world performance of a generator in a wholesale energy market.},
booktitle = {Proceedings of the Tenth ACM International Conference on Future Energy Systems},
pages = {121–127},
numpages = {7},
keywords = {Reinforcement learning, Energy markets, Bidding},
location = {Phoenix, AZ, USA},
series = {e-Energy '19}
}

@inproceedings{10.1145/3524273.3528184,
author = {Chen, Ke and Wang, Han and Fang, Shuwen and Li, Xiaotian and Ye, Minghao and Chao, H. Jonathan},
title = {RL-AFEC: Adaptive Forward Error Correction for Real-Time Video Communication Based on Reinforcement Learning},
year = {2022},
isbn = {9781450392839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524273.3528184},
doi = {10.1145/3524273.3528184},
abstract = {Real-time video communication is profoundly changing people's lives, especially in today's pandemic situation. However, packet loss during video transmission degrades reconstructed video quality, thus impairing users' Quality of Experience (QoE). Forward Error Correction (FEC) techniques are commonly employed in today's audio and video conferencing applications, such as Skype and Zoom, to mitigate the impact of packet loss. FEC helps recover the lost packets during transmissions at the receiver side, but the additional bandwidth consumption is also a concern. Since network conditions are highly dynamic, it is not trivial for FEC to maintain video quality with a fixed bandwidth overhead. In this paper, we propose RL-AFEC, an adaptive FEC scheme based on Reinforcement Learning (RL) to improve reconstructed video quality with an aim to mitigate bandwidth consumption for different network conditions. RL-AFEC learns to select a proper redundancy rate for each video frame, and then adds redundant packets based on the frame-level Reed-Solomon (RS) code. We also implement a novel packet-level Video Quality Assessment (VQA) method based on Video Multimethod Assessment Fusion (VMAF), which leverages Supervised Learning (SL) to generate video quality scores in real time by only extracting information from the packet stream without the need of visual contents. Extensive evaluations demonstrate the superiority of our scheme over other baseline FEC methods.},
booktitle = {Proceedings of the 13th ACM Multimedia Systems Conference},
pages = {96–108},
numpages = {13},
keywords = {reinforcement learning, forward error correction, real-time video communication, video quality assessment},
location = {Athlone, Ireland},
series = {MMSys '22}
}

@inproceedings{10.1145/3486635.3491073,
author = {Rao, Jinmeng and Gao, Song and Zhu, Xiaojin},
title = {VTSV: A Privacy-Preserving Vehicle Trajectory Simulation and Visualization Platform Using Deep Reinforcement Learning},
year = {2021},
isbn = {9781450391207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486635.3491073},
doi = {10.1145/3486635.3491073},
abstract = {Trajectory data is among the most sensitive data and the society increasingly raises privacy concerns. In this demo paper, we present a privacy-preserving Vehicle Trajectory Simulation and Visualization (VTSV) web platform (demo video: https://youtu.be/NY5L4bu2kTU), which automatically generates navigation routes between given pairs of origins and destinations and employs a deep reinforcement learning model to simulate vehicle trajectories with customized driving behaviors such as normal driving, overspeed, aggressive acceleration, and aggressive turning. The simulated vehicle trajectory data contain high-sample-rate of attributes including GPS location, speed, acceleration, and steering angle, and such data are visualized in VTSV using streetscape.gl, an autonomous driving data visualization framework. Location privacy protection methods such as origin-destination geomasking and trajectory k-anonymity are integrated into the platform to support privacy-preserving trajectory data generation and publication. We design two application scenarios to demonstrate how VTSV performs location privacy protection and customize driving behavior, respectively. The demonstration shows that VTSV is able to mitigate data privacy, sparsity, and imbalance sampling issues, which offers new insights into driving trajectory simulation and GeoAI-powered privacy-preserving data publication.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
pages = {43–46},
numpages = {4},
keywords = {transportation, vehicle trajectory, data visualization, privacy protection, reinforcement learning},
location = {Beijing, China},
series = {GEOAI '21}
}

@inproceedings{10.1145/1810479.1810515,
author = {Meraji, Sina and Zhang, Wei and Tropper, Carl},
title = {Brief Announcement: A Reinforcement Learning Approach for Dynamic Load-Balancing of Parallel Digital Logic Simulation},
year = {2010},
isbn = {9781450300797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1810479.1810515},
doi = {10.1145/1810479.1810515},
abstract = {In this paper, we present a dynamic load-balancing algorithm for parallel digital logic simulation making use of reinforcement learning. We first introduce two dynamic load-balancing algorithms oriented towards balancing the computational and communication load respectively and then utilize reinforcement learning to create an algorithm which is a combination of the first two algorithms. In addition, the algorithm determines the value of two important parameters-the number of processors which participate in the algorithm and the load which is exchanged during its execution. We investigate the algorithms on gate level simulations of several open source VLSI circuits.},
booktitle = {Proceedings of the Twenty-Second Annual ACM Symposium on Parallelism in Algorithms and Architectures},
pages = {181–182},
numpages = {2},
keywords = {dynamic load-balancing, digital logic simulation, time warp, verilog, reinforcement learning},
location = {Thira, Santorini, Greece},
series = {SPAA '10}
}

@article{10.1145/3408876,
author = {Meng, Lingheng and Lin, Daiwei and Francey, Adam and Gorbet, Rob and Beesley, Philip and Kuli\'{c}, Dana},
title = {Learning to Engage with Interactive Systems: A Field Study on Deep Reinforcement Learning in a Public Museum},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
url = {https://doi.org/10.1145/3408876},
doi = {10.1145/3408876},
abstract = {Physical agents that can autonomously generate engaging, life-like behavior will lead to more responsive and user-friendly robots and other autonomous systems. Although many advances have been made for one-to-one interactions in well-controlled settings, physical agents should be capable of interacting with humans in natural settings, including group interaction. To generate engaging behaviors, the autonomous system must first be able to estimate its human partners’ engagement level. In this article, we propose an approach for estimating engagement during group interaction by simultaneously taking into account active and passive interaction, and use the measure as the reward signal within a reinforcement learning framework to learn engaging interactive behaviors. The proposed approach is implemented in an interactive sculptural system in a museum setting. We compare the learning system to a baseline using pre-scripted interactive behaviors. Analysis based on sensory data and survey data shows that adaptable behaviors within an expert-designed action space can achieve higher engagement and likeability.},
journal = {J. Hum.-Robot Interact.},
month = {oct},
articleno = {5},
numpages = {29},
keywords = {adaptive system, reinforcement learning, robotic sculpture, human-robot interaction, group interaction, interactive system, robotic arts, Living architecture, open-world interaction, natural setting interaction, social robot, voluntary engagement, engagement}
}

@inproceedings{10.1145/3534678.3539481,
author = {Luo, Shuang and Li, Yinchuan and Li, Jiahui and Kuang, Kun and Liu, Furui and Shao, Yunfeng and Wu, Chao},
title = {S2RL: Do We Really Need to Perceive All States in Deep Multi-Agent Reinforcement Learning?},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539481},
doi = {10.1145/3534678.3539481},
abstract = {Collaborative multi-agent reinforcement learning (MARL) has been widely used in many practical applications, where each agent makes a decision based on its own observation. Most mainstream methods treat each local observation as an entirety when modeling the decentralized local utility functions. However, they ignore the fact that local observation information can be further divided into several entities, and only part of the entities is helpful to model inference. Moreover, the importance of different entities may change over time. To improve the performance of decentralized policies, the attention mechanism is used to capture features of local information. Nevertheless, existing attention models rely on dense fully connected graphs and cannot better perceive important states. To this end, we propose a sparse state based MARL (S2RL) framework, which utilizes a sparse attention mechanism to discard irrelevant information in local observations. The local utility functions are estimated through the self-attention and sparse attention mechanisms separately, then are combined into a standard joint value function and auxiliary joint value function in the central critic. We design the S2RL framework as a plug-and-play module, making it general enough to be applied to various methods. Extensive experiments on StarCraft II show that S2RL can significantly improve the performance of many state-of-the-art methods.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1183–1191},
numpages = {9},
keywords = {deep learning, sparse attention, multi-agent reinforcement learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3360322.3360849,
author = {Chen, Bingqing and Cai, Zicheng and Berg\'{e}s, Mario},
title = {Gnu-RL: A Precocial Reinforcement Learning Solution for Building HVAC Control Using a Differentiable MPC Policy},
year = {2019},
isbn = {9781450370059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360322.3360849},
doi = {10.1145/3360322.3360849},
abstract = {Reinforcement learning (RL) was first demonstrated to be a feasible approach to controlling heating, ventilation, and air conditioning (HVAC) systems more than a decade ago. However, there has been limited progress towards a practical and scalable RL solution for HVAC control. While one can train an RL agent in simulation, it is not cost-effective to create a model for each thermal zone or building. Likewise, existing RL agents generally take a long time to learn and are opaque to expert interrogation, making them unattractive for real-world deployment.To tackle these challenges, we propose Gnu-RL: a novel approach that enables practical deployment of RL for HVAC control and requires no prior information other than historical data from existing HVAC controllers. To achieve this, Gnu-RL adopts a recently-developed Differentiable Model Predictive Control (MPC) policy, which encodes domain knowledge on planning and system dynamics, making it both sample-efficient and interpretable. Prior to any interaction with the environment, a Gnu-RL agent is pre-trained on historical data using imitation learning, which enables it to match the behavior of the existing controller. Once it is put in charge of controlling the environment, the agent continues to improve its policy end-to-end, using a policy gradient algorithm.We evaluate Gnu-RL on both an EnergyPlus model and a real-world testbed. In both experiments, our agents were directly deployed in the environment after offline pre-training on expert demonstration. In the simulation experiment, our approach saved 6.6\% energy compared to the best published RL result for the same environment, while maintaining a higher level of occupant comfort. Next, Gnu-RL was deployed to control the HVAC of a real-world conference room for a three-week period. Our results show that Gnu-RL saved 16.7\% of cooling demand compared to the existing controller and tracked temperature set-point better.},
booktitle = {Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {316–325},
numpages = {10},
keywords = {HVAC Control, Deep Reinforcement Learning},
location = {New York, NY, USA},
series = {BuildSys '19}
}

@inproceedings{10.1145/3511808.3557389,
author = {Lee, Seonjae and Lee, Myoung Hoon and Moon, Jun},
title = {Maximum Norm Minimization: A Single-Policy Multi-Objective Reinforcement Learning to Expansion of the Pareto Front},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557389},
doi = {10.1145/3511808.3557389},
abstract = {In this paper, we propose Maximum Norm Minimization (MNM), a single-policy Multi-Objective Reinforcement Learning (MORL) algorithm to solve the multi-objective RL problem. The main objective of our MNM is to provide the Pareto optimal points constituting the Pareto front in the multi-objective space. First, MNM measures distances among the Pareto optimal points in the current Pareto front and then normalizes the distances based on maximum and minimum reward values for each objective in the multi-objective space. Second, MNM identifies the maximum norm, i.e., the maximum value of the normalized Pareto optimal distances. Then MNM seeks to find a new Pareto optimal point, which corresponds to the middle of the two Pareto optimal points constituting the maximum norm. By iterating these two processes, MNM is able to expand and densify the Pareto front with increasing summation of the Pareto front volumes and decreasing mean-squared distance of the Pareto optimal points. To validate the performance of MNM, we provide the experimental results of five complex robotic multi-objective environments. In particular, we compare the performance of MNM with those of other state-of-the-art methods in terms of the summation of volumes and the mean-squared distance of the Pareto optimal points.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {1064–1073},
numpages = {10},
keywords = {multi-objective reinforcement learning, weight vector selection, maximum norm minimization, pareto optimality},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3431379.3460648,
author = {Chen, Ruobing and Wu, Jinping and Shi, Haosen and Li, Yusen and Liu, Xiaoguang and Wang, Gang},
title = {DRLPart: A Deep Reinforcement Learning Framework for Optimally Efficient and Robust Resource Partitioning on Commodity Servers},
year = {2021},
isbn = {9781450382175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3431379.3460648},
doi = {10.1145/3431379.3460648},
abstract = {Workload consolidation is a commonly used approach for improving resource utilization of commodity servers. However, colocated workloads often suffer from significant performance degradations due to resource contention, which makes resource partitioning an important research problem. Partitioning multiple resources coordinately is particularly challenging due to the complex contention behaviors and huge solution space, which is not well-addressed in the literature.In this paper, we propose a deep reinforcement learning (DRL) framework, named DRLPart, for solving the problem of partitioning multiple resources coordinately. DRLPart learns the optimal partitioning decision from easy-to-collect real-time system state, without need of domain knowledge and handcrafted search heuristics. We solve two critical challenges of applying DRL to the resource partitioning problem. First, we build a deep-learning based performance model, which significantly reduces the training overhead, by estimating the rewards of actions without interacting with real system. Second, we propose a fine-tuning process to improve bad decisions occasionally made by the DRL model, which enhances the adaptivity to new situations. Results from extensive evaluations show that the proposed framework is optimally efficient and robust, which improves the system throughput by 13.3\%~18.5 compared to the state-of-the-art baselines.},
booktitle = {Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {175–188},
numpages = {14},
keywords = {resource partitioning, deep reinforcement learning, workload consolidation, performance interference},
location = {Virtual Event, Sweden},
series = {HPDC '21}
}

@inproceedings{10.1145/3450268.3453525,
author = {Elmalaki, Salma},
title = {FaiR-IoT: Fairness-Aware Human-in-the-Loop Reinforcement Learning for Harnessing Human Variability in Personalized IoT},
year = {2021},
isbn = {9781450383547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450268.3453525},
doi = {10.1145/3450268.3453525},
abstract = {Thanks to the rapid growth in wearable technologies, monitoring complex human context becomes feasible, paving the way to develop human-in-the-loop IoT systems that naturally evolve to adapt to the human and environment state autonomously. Nevertheless, a central challenge in designing such personalized IoT applications arises from human variability. Such variability stems from the fact that different humans exhibit different behaviors when interacting with IoT applications (intra-human variability), the same human may change the behavior over time when interacting with the same IoT application (inter-human variability), and human behavior may be affected by the behaviors of other people in the same environment (multi-human variability). To that end, we propose FaiR-IoT, a general reinforcement learning-based framework for adaptive and fairness-aware human-in-the-loop IoT applications. In FaiR-IoT, three levels of reinforcement learning agents interact to continuously learn human preferences and maximize the system's performance and fairness while taking into account the intra-, inter-, and multi-human variability. We validate the proposed framework on two applications, namely (i) Human-in-the-Loop Automotive Advanced Driver Assistance Systems and (ii) Human-in-the-Loop Smart House. Results obtained on these two applications validate the generality of FaiR-IoT and its ability to provide a personalized experience while enhancing the system's performance by 40\%-60\% compared to non-personalized systems and enhancing the fairness of the multi-human systems by 1.5 orders of magnitude.},
booktitle = {Proceedings of the International Conference on Internet-of-Things Design and Implementation},
pages = {119–132},
numpages = {14},
keywords = {Human-in-the-Loop, Fairness, Personalized IoT, Reinforcement Learning, Human Adaptation},
location = {Charlottesvle, VA, USA},
series = {IoTDI '21}
}

@inproceedings{10.1145/3459637.3481906,
author = {Wang, Zihao and Wang, Fudong and Zhang, Haipeng and Yang, Minghui and Cao, Shaosheng and Wen, Zujie and Zhang, Zhe},
title = { 'Could You Describe the Reason for the Transfer?': A Reinforcement Learning Based Voice-Enabled Bot Protecting Customers from Financial Frauds},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481906},
doi = {10.1145/3459637.3481906},
abstract = {With the booming of the Internet finance and e-payment business, telecom and online fraud has become a serious problem which grows rapidly. In China, 351 billion RMB (approximately 0.3\% of China's GDP) was lost in 2018 due to telecommunication and online fraud, influencing tens of millions of individual customers. Anti-fraud algorithms have been widely adopted by major Internet finance companies to detect and block transactions induced by scam. However, due to limited contextual information, most systems would probably mistakenly block the normal transactions, leading to poor user experience. On the other hand, if the transactions induced by scam are detected yet not fully explained to the users, the users will continue to pay, suffering from direct financial losses.To address these problems, we design a voice-enabled bot that interacts with the customers who are involved with potential telecommunication and online frauds decided by the back-end system. The bot seeks additional information from the customers through natural conversations to confirm whether the customers are scammed and identify the actual fraud types. The details about the frauds are then provided to convince the customers that they are on the edge of being scammed. Our bot adopts offline reinforcement learning (RL) to learn dialogue policies from real-world human-human chat logs. During the conversations, our bot also identifies fraud types every turn based on the dialogue state.The bot proposed outperforms baseline dialogue strategies by 2.8\% in terms of task success rate, and 5\% in terms of dialogue accuracy in offline evaluations. Furthermore, in the 8 months of real-world deployment, our bot lowers the dissatisfaction rate by 25\% and increases the fraud prevention rate by 135\% relatively, indicating a significant improvement in user experience as well as anti-fraud effectiveness. More importantly, we help prevent millions of users from being deceived, and avoid trillions of financial losses.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {4214–4223},
numpages = {10},
keywords = {dialogue system, reinforcement learning, outbound bot, dialogue risk detection, dialogue policy},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3205651.3205770,
author = {Shimada, Koki and Bentley, Peter},
title = {Learning How to Flock: Deriving Individual Behaviour from Collective Behaviour with Multi-Agent Reinforcement Learning and Natural Evolution Strategies},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3205770},
doi = {10.1145/3205651.3205770},
abstract = {This work proposes a method for predicting the internal mechanisms of individual agents using observed collective behaviours by multi-agent reinforcement learning (MARL). Since the emergence of group behaviour among many agents can undergo phase transitions, and the action space will not in general be smooth, natural evolution strategies were adopted for updating a policy function. We tested the approach using a well-known flocking algorithm as a target model for our system to learn. With the data obtained from this rule-based model, the MARL model was trained, and its acquired behaviour was compared to the original. In the process, we discovered that agents trained by MARL can self-organize flow patterns using only local information. The expressed pattern is robust to changes in the initial positions of agents, whilst being sensitive to the training conditions used.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {169–170},
numpages = {2},
keywords = {neural networks/deep learning, evolution strategies, reinforcement learning, multi-agent systems, swarm intelligence},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@article{10.1145/3338123,
author = {Banerjee, Suvadeep and Chatterjee, Abhijit},
title = {ALERA: Accelerated Reinforcement Learning Driven Adaptation to Electro-Mechanical Degradation in Nonlinear Control Systems Using Encoded State Space Error Signatures},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3338123},
doi = {10.1145/3338123},
abstract = {The successful deployment of autonomous real-time systems is contingent on their ability to recover from performance degradation of sensors, actuators, and other electro-mechanical subsystems with low latency. In this article, we introduce ALERA, a novel framework for real-time control law adaptation in nonlinear control systems assisted by system state encodings that generate an error signal when the code properties are violated in the presence of failures. The fundamental contributions of this methodology are twofold—first, we show that the time-domain error signal contains perturbed system parameters’ diagnostic information that can be used for quick control law adaptation to failure conditions and second, this quick adaptation is performed via reinforcement learning algorithms that relearn the control law of the perturbed system from a starting condition dictated by the diagnostic information, thus achieving significantly faster recovery. The fast (up to 80X faster than traditional reinforcement learning paradigms) performance recovery enabled by ALERA is demonstrated on an inverted pendulum balancing problem, a brake-by-wire system, and a self-balancing robot.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jul},
articleno = {44},
numpages = {25},
keywords = {real-time systems, Reinforcement learning, dependability, control systems}
}

