"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Inventory Pooling using Deep Reinforcement Learning","K. Sampath; S. Nishad; S. K. Reddy Danda; P. Dayama; S. Sankagiri","IBM Research, India; IBM Research, India; Avesha India Pvt Ltd, India; IBM Research, India; University of Illinois, USA","2022 IEEE International Conference on Services Computing (SCC)","24 Aug 2022","2022","","","259","267","Inventory pooling is a collaborative arrangement in which different agents share their inventories to reduce the total inventory cost. Each agent maintains its own inventory, which are shared periodically through lateral transshipment. Analysis of an inventory pooling system is typified by demand profiles, cost sharing mechanism, level of collaboration, type of transshipment etc. By varying the choice in each dimension, different possible configurations of the system can be defined. Extensive analysis of the possible configurations is required for the agents to agree on a particular configuration. Such an analysis would require a multi-disciplinary approach to study each of the dimensions in isolation. We propose deep reinforcement learning as a single framework to analyze the inventory pooling configurations. Extensive computational experiments illustrate the efficacy of deep reinforcement learning for inventory pooling with generic assumptions on system characteristics, that are intractable using existing models.","2474-2473","978-1-6654-8146-5","10.1109/SCC55611.2022.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860102","horizontal collaboration;lateral transshipments;cost allocation;dynamic demand scenarios","Industries;Costs;Computational modeling;Collaboration;Service computing;Reinforcement learning;Behavioral sciences","cost reduction;deep learning (artificial intelligence);inventory management;production engineering computing;reinforcement learning","deep reinforcement learning;total inventory cost reduction;inventory pooling system;cost sharing mechanism;multidisciplinary approach;computational experiments","","","","35","IEEE","24 Aug 2022","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Platform for Small and Medium-sized Enterprises in Logistics","J. P. S. Piest; M. -E. Iacob; M. van Sinderen; M. Gemmink; B. Goossens","Information Systems, Industrial Engineering and Business, University of Twente, Enschede, Netherlands; Information Systems, Industrial Engineering and Business, University of Twente, Enschede, Netherlands; Services and Cybersecurity, University of Twente, Enschede, Netherlands; Bullit Digital, Groenlo, Netherlands; Bullit Digital, Groenlo, Netherlands","2021 IEEE 25th International Enterprise Distributed Object Computing Workshop (EDOCW)","1 Dec 2021","2021","","","289","298","While real-time data and sophisticated Reinforcement Learning (RL) approaches are emerging, logistic organizations, in particular Small and Medium-sized Enterprises (SMEs), lack the tools and expertise to effectively identify whether (parts of) their business processes are suitable for using RL and adopt these approaches in their daily practice. This paper presents the results of our efforts to design, develop, test and implement a RL-based decision support platform based on the Open Trip Model (OTM) for the logistics industry. The main contribution of this paper is a potentially generalizable platform architecture and instantiation of the following main functional components: 1) a graphical user interface to access platform services, 2) APIs to automate the data collection based on the OTM, 3) an OTM compliant data storage, 4) a repository and tool for testing multiple algorithms and perform (hyper)parameter tuning, and 5) infrastructure provisioning to run and monitor agents. The platform is complemented with RL guidelines to transfer the platform, knowledge, and practices to SMEs. The platform architecture is validated with a panel of experts and demonstrated in use in a case study at a logistics services provider. The platform demonstration and case study contribute to increasing awareness of potential applications of RL in the logistics industry. The platform provides a foundation for empirical research and experimental development of RL approaches in logistics. Future research will focus on alternative and hybrid approaches, federated learning, and incorporating data sharing concepts as part of the envisioned federated data sharing infrastructure for the Dutch logistics industry.","2325-6605","978-1-6654-4488-0","10.1109/EDOCW52865.2021.00060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9626344","reinforcement learning;platform;logistics;small and medium sized enterprises;SMEs;open trip model;OTM","Industries;Reinforcement learning;Computer architecture;Organizations;Tools;Real-time systems;Tuning","application program interfaces;decision making;decision support systems;graphical user interfaces;learning (artificial intelligence);logistics;service-oriented architecture;small-to-medium enterprises","Reinforcement Learning platform;real-time data;sophisticated Reinforcement;logistic organizations;Medium-sized Enterprises;SMEs;business processes;daily practice;RL-based decision support platform;Open Trip Model;potentially generalizable platform architecture;main functional components;graphical user interface;access platform services;data collection;OTM compliant data storage;repository;RL guidelines;logistics services provider;platform demonstration;RL approaches;alternative approaches;hybrid approaches;federated learning;incorporating data;envisioned federated data;Dutch logistics industry","","","","37","IEEE","1 Dec 2021","","","IEEE","IEEE Conferences"
"Adaptive FPGA Placement Optimization via Reinforcement Learning","K. E. Murray; V. Betz","Dept. of Electrical & Computer Engineering, University of Toronto, Canada; Dept. of Electrical & Computer Engineering, University of Toronto, Canada","2019 ACM/IEEE 1st Workshop on Machine Learning for CAD (MLCAD)","16 Jul 2020","2019","","","1","6","Developing new or improved optimization heuristics for Computer Aided Design (CAD) tools is challenging and time consuming, relying on empirical experimentation and researcher experience. In this work we study how this process can be improved by using Reinforcement Learning (RL) to learn effective and adaptive heuristics. Applying these techniques to Field Programmable Gate Array (FPGA) placement, we show our RL-enhanced algorithm outperforms the standard VTR 8 placer, achieving a better run-time & quality trade-off (up to 2× faster for equivalent quality). RL enables the placer to more efficiently explore the solution space and enables it to dynamically adapt to the specific problem instance being solved. We expect further application of advanced RL methods will improve these results, which motivates further exploration of how RL could be applied in the CAD flow.","","978-1-7281-5758-0","10.1109/MLCAD48534.2019.9142079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9142079","Field Programmable Gate Array (FPGA);Electronic Design Automation (EDA);Computer Aided Design (CAD);Machine Learning (ML);Reinforcement Learning (RL)","Field programmable gate arrays;Optimization;Annealing;Learning (artificial intelligence);Design automation;Machine learning algorithms;Generators","circuit optimisation;field programmable gate arrays;learning (artificial intelligence);logic CAD","adaptive FPGA placement optimization;reinforcement learning;improved optimization heuristics;computer aided design tools;effective heuristics;adaptive heuristics;RL-enhanced algorithm;standard VTR 8 placer;quality trade-off;equivalent quality;advanced RL methods;CAD flow;field programmable gate array placement;CAD tools","","10","","16","IEEE","16 Jul 2020","","","IEEE","IEEE Conferences"
"Adaptive control for building energy management using reinforcement learning","L. Eller; L. C. Siafara; T. Sauter","Institute of Computer Technology, TU Wien, Vienna, Austria; Institute of Computer Technology, TU Wien, Vienna, Austria; Center for Integrated Sensor Systems, Danube University Krems, Wr. Neustadt, Austria","2018 IEEE International Conference on Industrial Technology (ICIT)","30 Apr 2018","2018","","","1562","1567","Efficient energy management of building operation shall consider the individual and time variant characteristics of the building and its systems to maximize the potential energy savings without compromising the comfort level of occupants. Model-free control approaches, such as Reinforcement Learning, process building operation data to find control actions to operate the building systems while integrating seamlessly into their decisions changes in the building dynamics. These methods, however, do not scale well to complex problems due to the curse of dimensionality, which limits their practical applicability. To address the state explosion problem we propose a Reinforcement Learning controller for a two zone building model that gets state approximation inputs from an Artificial Neural Network. The results show that the system is able to maintain comfort levels while achieving significant energy gains by finding untapped potential for energy performance improvements.","","978-1-5090-5949-2","10.1109/ICIT.2018.8352414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8352414","Building Automation Systems;Adaptive Control;Reinforcement Learning;Artificial Neural Networks","Heating systems;Mathematical model;Building automation;Ventilation;Data models","adaptive control;building management systems;buildings (structures);energy conservation;energy management systems;learning (artificial intelligence);neural nets;neurocontrollers;space heating","energy performance improvements;adaptive control;building energy management;individual time variant characteristics;potential energy savings;model-free control approaches;process building operation data;control actions;building systems;decisions changes;building dynamics;complex problems;state explosion problem;Reinforcement Learning controller;zone building model;state approximation inputs;comfort levels;energy gains","","8","","23","IEEE","30 Apr 2018","","","IEEE","IEEE Conferences"
"Nonholonomic Yaw Control of an Underactuated Flying Robot With Model-Based Reinforcement Learning","N. O. Lambert; C. B. Schindler; D. S. Drew; K. S. J. Pister","Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, Stanford University, Stanford, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA, USA","IEEE Robotics and Automation Letters","8 Jan 2021","2021","6","2","455","461","Nonholonomic control is a candidate to control nonlinear systems with path-dependant states. We investigate an underactuated flying micro-aerial-vehicle, the ionocraft, that requires nonholonomic control in the yaw-direction for complete attitude control. Deploying an analytical control law involves substantial engineering design and is sensitive to inaccuracy in the system model. With specific assumptions on assembly and system dynamics, we derive a Lie bracket for yaw control of the ionocraft. As a comparison to the significant engineering effort required for an analytic control law, we implement a data-driven model-based reinforcement learning yaw controller in a simulated flight task. We demonstrate that a simple model-based reinforcement learning framework can match the derived Lie bracket control - in yaw rate and chosen actions - in a few minutes of flight data, without a pre-defined dynamics function. This letter shows that learning-based approaches are useful as a tool for synthesis of nonlinear control laws previously only addressable through expert-based design.","2377-3766","","10.1109/LRA.2020.3045930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9300244","Reinforcement learning;nonholonomic motion planning;aerial systems: mechanics and control","Robots;Reinforcement learning;Attitude control;Vehicle dynamics;Planning;Manifolds;Analytical models","aerospace robotics;aircraft control;attitude control;control engineering computing;control system synthesis;learning (artificial intelligence);microrobots;nonlinear control systems;robot dynamics","nonlinear control law synthesis;flight data;simulated flight task;underactuated flying microaerial-vehicle;data-driven model-based reinforcement learning;expert-based design;learning-based approaches;yaw rate;Lie bracket control;simple model-based reinforcement learning framework;system dynamics;assembly;system model;engineering design;analytical control law;attitude control;ionocraft;path-dependant states;nonlinear systems;underactuated flying robot;nonholonomic yaw control","","4","","36","IEEE","21 Dec 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Attack Graph Analysis for Wastewater Treatment Plant","M. Ibrahim; R. Elhafiz; A. Al-Wadi","Department of Mechatronics Engineering, German Jordanian University, Amman, Jordan; Department of Mechatronics Engineering, German Jordanian University, Amman, Jordan; Department of Mechatronics Engineering, German Jordanian University, Amman, Jordan","IEEE Transactions on Industry Applications","","2023","PP","99","1","10","Automation frequently needs less human engagement, encouraging reliance on continually operating machinery and automated processes that perform a range of functions. The resultant predictable, repeated behavior can be exploited. Since the Internet of Things (IoT) has been included in aforesaid automatic operations, these Cyber-Physical Systems (CPSs) are exposed to cyberattacks, making it difficult to stop them and identify their patterns. Even though running Wastewater Treatment Plant (WTPs) might be difficult, they are necessary since both drinking water and water that may be recycled are in low supply. Thus, increasing their vulnerabilities to assaults caused by the exploitation of flaws while doing so. To secure such CPSs, one must be aware of system weaknesses and potential exploitation. This paper examines the attack graph of the treatment method, and its flaws to avoid similar incidents and lessen the harm. A Q-learning-based attack graph inquiry method is also presented, where the agent is considered to be the adversary and the attack graph that was formed mimics Q-environment. This method can help in figuring out the optimal path an attacker can follow to do the most damage with the fewest amount of actions.","1939-9367","","10.1109/TIA.2023.3298289","Deanship of Graduate Studies and Scientific Research at the German Jordanian University, Seed(grant numbers:SATS 03/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10192325","attack graph;reinforcement learning;security analysis;wastewater treatment","Computer crime;Wastewater treatment;Wastewater;Microorganisms;Internet of Things;Systems architecture;Solids","","","","","","","IEEE","24 Jul 2023","","","IEEE","IEEE Early Access Articles"
"A Knowledge-Guided End-to-End Optimization Framework based on Reinforcement Learning for Flow Shop Scheduling","Z. Pan; L. Wang; C. Dong; J. -f. Chen","Department of Automation, Tsinghua University, Beijing, P.R. China; Department of Automation, Tsinghua University, Beijing, P.R. China; School of Mechanical and Automotive Engineering, Qingdao Hengxing University of Science and Technology, Qingdao, P.R. China; Department of Automation, Tsinghua University, Beijing, P.R. China","IEEE Transactions on Industrial Informatics","","2023","PP","99","1","9","Designing an effective and efficient end-to-end optimization framework with good generalization for shop scheduling is an emerging topic in the informational manufacturing system. Existing end-to-end frameworks have achieved satisfactory results for COPs such as traveling salesman problem and vehicle routing problem. However, the performances of these methods in solving complex COPs such as shop scheduling need to be improved. In this paper, a knowledge-guided end-to-end optimization framework based on reinforcement learning is proposed to solve the permutation flow shop scheduling problem (PFSP). Firstly, a new policy network is designed based on the problem characteristics to deal with different scales of PFSPs and achieve iterative end-to-end generation. Secondly, an improved policy-based reinforcement learning algorithm by using the knowledge accumulated during the training process is designed to enhance the training quality. Thirdly, a knowledge-guided improvement strategy is introduced through the cooperation of local search and supervised learning to improve the learning of the policy. Simulation results and comparisons show that the knowledge-guided end-to-end optimization framework can obtain better results than different kinds of commonly-used optimization methods in limited computation time for solving the PFSP.","1941-0050","","10.1109/TII.2023.3282313","National Natural Science Foundation of China(grant numbers:62273193); National Science Fund for Distinguished Young Scholars of China(grant numbers:61525304); Research and Development Project of China Railway Signal & Communication Corp. (CRSC) Research and Design Institute Group Company Ltd.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10143317","Reinforcement Learning;Knowledge Guided;End-to-end Framework;Flow Shop Scheduling","Job shop scheduling;Optimization;Scheduling;Reinforcement learning;Training;Metaheuristics;Heuristic algorithms","","","","","","","IEEE","2 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Energy-Efficient Motion Planning and Control for Robotic Arms via Deep Reinforcement Learning","T. Shen; X. Liu; Y. Dong; Y. Yuan","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan; School of Information Science and Engineering, East China University of Science and Technology, Shanghai; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan","2022 34th Chinese Control and Decision Conference (CCDC)","14 Feb 2023","2022","","","5502","5507","With the optimization of China’s energy supply structure, the issue of efficient energy use has attracted widespread attention from researchers [1]. The robotic arm, which is widely used in industrial production, comes with high energy usage. In order to reduce energy usage, we propose an energy-efficient motion planning and control method based on Reinforcement Learning (RL) to reduce energy consumption during operation. The motion planning and control policy is learned in a model-free manner using RL, making it effective even in complex industrial environments. The main goal can be divided into three parts: moving to the target location, obstacle avoidance, and energy saving. From this, a comprehensive and effective reward function is designed using a distance reward, a velocity reward, an obstacle avoidance reward, and an energy-saving reward. Using this reward function, a control policy is trained which balances the aforementioned goals. Finally, the algorithm is verified by using the trained policy to avoid obstacles in a simulation environment. Extensive experimental results have demonstrated the effectiveness of the algorithm proposed, and show that the learned control policy can make the robot move with less energy whilst avoiding obstacles.","1948-9447","978-1-6654-7896-0","10.1109/CCDC55256.2022.10033563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10033563","Energy Efficient;Reinforcement Learning;Motion Planning","Energy consumption;Service robots;Reinforcement learning;Manipulators;Energy efficiency;Stability analysis;Robustness","collision avoidance;control engineering computing;deep learning (artificial intelligence);energy conservation;energy consumption;learning (artificial intelligence);mobile robots;path planning;reinforcement learning","comprehensive reward function;control method;deep Reinforcement Learning;effective reward function;efficient energy use;energy consumption;energy-efficient motion planning;energy-saving reward;high energy usage;learned control policy;obstacle avoidance reward;robotic arm","","","","30","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"An Isolation-aware Online Virtual Network Embedding via Deep Reinforcement Learning","A. Gohar; C. Rong; S. Lee","Department of Electrical Engineering and Computer Science, University of Stavanger, Stavanger, Norway; Department of Electrical Engineering and Computer Science, University of Stavanger, Stavanger, Norway; College of Computer Science, Kookmin University, Seoul, Korea","2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing Workshops (CCGridW)","19 Jul 2023","2023","","","89","95","Virtualization technologies are the foundation of modern ICT infrastructure, enabling service providers to create dedicated virtual networks (VNs) that can support a wide range of smart city applications. These VNs continuously generate massive amounts of data, necessitating stringent reliability and security requirements. In virtualized network environments, however, multiple VNs may coexist on the same physical infrastructure and, if not properly isolated, may interfere with or provide unauthorized access to one another. The former causes performance degradation, while the latter compromises the security of VNs. Service assurance for infrastructure providers becomes significantly more complicated when a specific VN violates the isolation requirement. In an effort to address the isolation issue, this paper proposes isolation during virtual network embedding (VNE), the procedure of allocating VNs onto physical infrastructure. We define a simple abstracted concept of isolation levels to capture the variations in isolation requirements and then formulate isolation-aware VNE as an optimization problem with resource and isolation constraints. A deep reinforcement learning (DRL)-based VNE algorithm ISO-DRL VNE, is proposed that considers resource and isolation constraints and is compared to the existing three state-of-the-art algorithms: NodeRank, Global Resource Capacity (GRC), and Mote-Carlo Tree Search (MCTS). Evaluation results show that the ISO-DRL VNE algorithm outperforms others in acceptance ratio, long-term average revenue, and long-term average revenue-to-cost ratio by 6%, 13%, and 15%.","","979-8-3503-0208-0","10.1109/CCGridW59191.2023.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10181179","Internet of Things;Isolation;Reinforcement Learning;Resource Allocation;Smart City;Virtual Network Embedding;Vertical Industries","Deep learning;Industries;Smart cities;Reinforcement learning;Security;Reliability;Virtualization","deep learning (artificial intelligence);Monte Carlo methods;reinforcement learning;resource allocation;smart cities;tree searching;virtualisation","causes performance degradation;dedicated virtual networks;deep reinforcement learning-based VNE algorithm ISO-DRL VNE;infrastructure providers;ISO-DRL VNE algorithm;isolation issue;isolation levels;isolation requirement;isolation-aware online virtual network embedding;isolation-aware VNE;modern ICT infrastructure;multiple VNs;necessitating stringent reliability;physical infrastructure;security requirements;service assurance;service providers;smart city applications;virtualization technologies;virtualized network environments","","","","16","IEEE","19 Jul 2023","","","IEEE","IEEE Conferences"
"Reinforcement-Learning- and Belief-Learning-Based Double Auction Mechanism for Edge Computing Resource Allocation","Q. Li; H. Yao; T. Mai; C. Jiang; Y. Zhang","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Tsinghua Space Center, Tsinghua University, Beijing, China; Department of Informatics, University of Oslo, Oslo, Norway","IEEE Internet of Things Journal","10 Jul 2020","2020","7","7","5976","5985","In recent years, we have witnessed the compelling application of the Internet of Things (IoT) in our daily life, ranging from daily living to industrial production. On account of the computation and power constraints, the IoT devices have to offload their tasks to the remote cloud services. However, the long-distance transmission poses significant challenges for latency-sensitive businesses, such as autonomous driving and industrial control. As a remedy, mobile edge computing (MEC) is deployed at the edge of the network to reduce the transmission delay. With the MEC joining in, how to allocate the limited computing resource of MEC is a critical problem to guarantee efficient working of the whole IoT system. In this article, we formulate the resource management among MEC and IoT devices as a double auction game. Also, for searching the Nash equilibrium, we introduce the experience-weighted attraction (EWA) algorithm performing behind each participant. With this AI method, auction participants acquire and accumulate experience by observing others' behavior and doing introspection, which accelerates the trading policy's learning process of each agent in such an opaque environment. Some simulation results are presented to evaluate the convergence and correctness of our architecture and algorithm.","2327-4662","","10.1109/JIOT.2019.2953108","National Key Research and Development Plan(grant numbers:2018YFB1800805); National Engineering Laboratory for Public Safety Risk Perception and Control by Big Data (PSRPC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8896972","Double auction game;experience-weighted attraction (EWA);latency-sensitive businesses;mobile edge computing (MEC)","Task analysis;Resource management;Games;Cloud computing;Heuristic algorithms;Reinforcement learning","cloud computing;game theory;learning (artificial intelligence);mobile computing;resource allocation","EWA algorithm;reinforcement-learning based double auction mechanism;trading policy;auction participants;experience-weighted attraction algorithm;double auction game;IoT system;transmission delay;MEC;mobile edge computing;latency-sensitive businesses;remote cloud services;IoT devices;industrial production;edge computing resource allocation;belief-learning-based double auction mechanism","","35","","31","IEEE","12 Nov 2019","","","IEEE","IEEE Journals"
"Reinforcement learning and instance-based learning approaches to modeling human decision making in a prognostic foraging task","S. E. Chelian; J. Paik; P. Pirolli; C. Lebiere; R. Bhattacharyya","LLC, HRL Laboratories, Malibu, CA; LG Electronics, Seoul, South Korea; PARC Institute, Palo Alto, CA; Carnegie Mellon University, Pittsburgh, PA; LLC, HRL Laboratories, Malibu, CA","2015 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)","7 Dec 2015","2015","","","116","122","Procedural memory and episodic memory are known to be distinct and both underlie the performance of many tasks. Reinforcement learning (RL) and instance-based learning (IBL) represent common approaches to modeling procedural and episodic memory in that order. In this work, we present a neural model utilizing RL dynamics and an ACT-R model utilizing IBL productions to the task of modeling human decision making in a prognostic foraging task. The task performed was derived from a geospatial intelligence domain wherein agents must choose among information sources to more accurately predict the actions of an adversary. Results from both models are compared to human data and suggest that information gain is an important component in modeling decision-making behavior using either memory system; with respect to the episodic memory approach, the procedural memory approach has a small but significant advantage in fitting human data. Finally, we discuss the interactions of multi-memory systems in complex decision-making tasks.","","978-1-4673-9320-1","10.1109/DEVLRN.2015.7346127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346127","Computational Neuroscience;Decision Making;Instance-based learning (IBL);Prognostic foraging;Reinforcement learning (RL)","Decision making;Training;Electronic mail;Learning (artificial intelligence);Roads;Geospatial analysis;Data models","cognition;decision making;learning (artificial intelligence)","instance-based learning approach;human decision making;prognostic foraging task;episodic memory;procedural memory;neural model;RL dynamics;ACT-R model;IBL productions;geospatial intelligence domain;information sources;complex decision-making tasks","","1","","27","IEEE","7 Dec 2015","","","IEEE","IEEE Conferences"
"Learning to Routing in UAV Swarm Network: A Multi-Agent Reinforcement Learning Approach","Z. Wang; H. Yao; T. Mai; Z. Xiong; X. Wu; D. Wu; S. Guo","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Pillar of Information Systems Technology and Design, Singapore University of Technology and Design, Singapore; Rocket Force Academy, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Department of Computing, Hong Kong Polytechnic University, Hong Kong SAR, China","IEEE Transactions on Vehicular Technology","15 May 2023","2023","72","5","6611","6624","The past few years have witnessed an exponential growth of compelling UAV swarm applications ranging from agricultural production, and intelligent transport, to disaster rescue. The high-speed mobility of the UAV swarm belongs to a new clan of networks, termed flying ad-hoc networks (FANETs). How to design an effective routing mechanism in such a dynamic network is challenging. Traditional flooding searching algorithms (e.g., OLSR, AODV) lead to huge communication overheads, while greedy searching algorithms (e.g., Geolocation-Based Routing protocol) pose low routing efficiency. In this paper, we propose an adaptive communication-based UAV swarm routing algorithm. In our algorithm, we design the Multilayer Perceptron algorithm to learn when routing flooding is required among different UAVs, and design the Gate Recurrent Unit algorithm to greatly compress the volume of communication data. Besides, we further adopt the multi-agent actor-critic algorithm to learn how to integrate shared information for cooperative routing decision-making. Extensive simulation results validate that our algorithms achieve efficient and effective routing under a partially observable distributed environment for large-scale UAV swarm cooperation.","1939-9359","","10.1109/TVT.2022.3232815","Artificial Intelligence and Smart City Joint Laboratory(grant numbers:B2020001); Future Intelligent Networking and Intelligent Transportation Joint Laboratory(grant numbers:B2019007); Intelligent Network Joint Laboratory(grant numbers:B2021006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002864","UAV swarm;network routing;multi-agent reinforcement learning","Routing;Routing protocols;Heuristic algorithms;Logic gates;AODV;Vehicle dynamics;Three-dimensional displays","ad hoc networks;autonomous aerial vehicles;decision making;disasters;learning (artificial intelligence);mobile ad hoc networks;mobile radio;multi-agent systems;multi-robot systems;multilayer perceptrons;recurrent neural nets;reinforcement learning;routing protocols;telecommunication computing;telecommunication network routing","adaptive communication-based UAV swarm routing algorithm;agricultural production;compelling UAV swarm applications;different UAVs;disaster rescue;dynamic network;effective routing mechanism;exponential growth;Gate Recurrent Unit algorithm;Geolocation-Based Routing protocol;greedy searching algorithms;high-speed mobility;huge communication overheads;intelligent transport;large-scale UAV swarm cooperation;low routing efficiency;multiagent actor-critic algorithm;multiagent reinforcement learning approach;Multilayer Perceptron algorithm;routing decision-making;routing flooding;traditional flooding searching algorithms;UAV swarm network","","","","35","IEEE","29 Dec 2022","","","IEEE","IEEE Journals"
"Fuzzy reinforcement learning for System of Systems (SOS)","H. Berenji; M. Jamshidi","IIS Corporation, Moffett Field, CA, USA; ACE Center, ECEC Department, University of Texas, San Antonio, San Antonio, TX, USA","2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)","1 Sep 2011","2011","","","1689","1694","The System of Systems (SOS) technology is an advanced technology for Intelligent Systems that is developed with multiple intelligent systems. Recently, there has been a growing interest in a class of complex systems (robotic swarm as an example) whose constituents are themselves complex. Performance optimization, robustness and reliability among an emerging group of heterogeneous systems in order to realize a common goal have become the focus of various applications including military, security, aerospace, space, manufacturing, service industry, environmental systems, and disaster management, to name a few. In this paper, we discuss how Fuzzy Reinforcement Learning (FRL) can be used in SOS.","1098-7584","978-1-4244-7317-5","10.1109/FUZZY.2011.6007325","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007325","System of Systems (SOS);Fuzzy Reinforcement Learning (FRL);robots;cooperation;Unmanned Ground Vehicles (UGVs)","Robot kinematics;Robot sensing systems;Mobile robots;XML;Software","fuzzy reasoning;intelligent robots;learning (artificial intelligence);optimisation;reliability;robot programming","fuzzy reinforcement learning;system of systems;SOS technology;multiple intelligent systems;complex systems;optimization;reliability","","7","","16","IEEE","1 Sep 2011","","","IEEE","IEEE Conferences"
"Robust Active Simultaneous Localization and Mapping Based on Bayesian Actor-Critic Reinforcement Learning","B. Pedraza; D. Dera","Department of Electrical and Computer Engineering, The University of Texas Rio Grande Valley; Department of Electrical and Computer Engineering, The University of Texas Rio Grande Valley","2023 IEEE Conference on Artificial Intelligence (CAI)","2 Aug 2023","2023","","","63","66","Autonomous mobile robots play vital roles in business, industry, manufacturing, e-commerce, and healthcare. Autonomous navigation and obstacle avoidance involve localizing a robot to actively explore and map an unknown environment autonomously without prior knowledge. Simultaneous localization and mapping (SLAM) present a severe challenge. This paper proposes a novel approach for robust navigation and robot action mapping based on Bayesian Actor-Critic (A2C) reinforcement learning. The principle of Actor-Critic combines policy-based and value-based learning by splitting the model into two: the policy model (Actor) computes the action based on the state, and the value model (Critic) tracks whether the agent is ahead or behind during the game. That feedback guides the training process, where both models participate in a game and optimize their output as time passes. We develop a Bayesian A2C model that generates robot actions and quantifies uncertainty on the actions toward robust exploration and collision-free navigation. We adopt the Bayesian inference and optimize the variational posterior distribution over the unknown model parameters using the evidence lower bound (ELBO) objective. The first-order Taylor series approximates the mean and covariance of the variational distribution passed through non-linear functions in the A2C model. The propagated covariance estimates the robot's action uncertainty at the output of the Actor-network. Experiments demonstrate the superior robustness of the proposed Bayesian A2C model exploring heavily noisy environments compared to deterministic homologs. The proposed framework can be applied to other fields of research (underwater robots, biomedical devices/robots, micro-robots, drones, etc.) where robustness and uncertainty quantification are critical.","","979-8-3503-3984-0","10.1109/CAI54212.2023.00035","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10195002","Simultaneous localization and mapping (SLAM);uncertainty;Actor-Critic (A2C);variational inference (VI);evidence lower bound (ELBO)","Simultaneous localization and mapping;Uncertainty;Navigation;Service robots;Computational modeling;Reinforcement learning;Robustness","approximation theory;Bayes methods;collision avoidance;control engineering computing;inference mechanisms;learning (artificial intelligence);mobile robots;path planning;reinforcement learning;SLAM (robots)","action uncertainty;Actor-Critic combines policy-based;Actor-network;autonomous mobile robots;Bayesian A2C;Bayesian Actor-Critic reinforcement learning;Bayesian inference;collision-free navigation;obstacle avoidance;policy model;robot actions;robust active simultaneous localization;robust exploration;robust navigation;severe challenge;superior robustness;underwater robots;unknown environment;unknown model parameters;value model;value-based;variational posterior distribution;vital roles","","","","22","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-Based Dynamic Resource Management for Mobile Edge Computing in Industrial Internet of Things","Y. Chen; Z. Liu; Y. Zhang; Y. Wu; X. Chen; L. Zhao","School of Computer, Beijing Information Science and Technology University, Beijing, China; School of Computer, Beijing Information Science and Technology University, Beijing, China; School of Computer, Beijing Information Science and Technology University, Beijing, China; State Key Lab of Internet of Things for Smart City, University of Macau, Macau; School of Computer, Beijing Information Science and Technology University, Beijing, China; Department of Electrical, Computer and Biomedical Engineering, Ryerson University, Toronto, ON, Canada","IEEE Transactions on Industrial Informatics","5 Apr 2021","2021","17","7","4925","4934","Nowadays, driven by the rapid development of smart mobile equipments and 5G network technologies, the application scenarios of Internet of Things (IoT) technology are becoming increasingly widespread. The integration of IoT and industrial manufacturing systems forms the industrial IoT (IIoT). Because of the limitation of resources, such as the computation unit and battery capacity in the IIoT equipments (IIEs), computation-intensive tasks need to be executed in the mobile edge computing (MEC) server. However, the dynamics and continuity of task generation lead to a severe challenge to the management of limited resources in IIoT. In this article, we investigate the dynamic resource management problem of joint power control and computing resource allocation for MEC in IIoT. In order to minimize the long-term average delay of the tasks, the original problem is transformed into a Markov decision process (MDP). Considering the dynamics and continuity of task generation, we propose a deep reinforcement learning-based dynamic resource management (DDRM) algorithm to solve the formulated MDP problem. Our DDRM algorithm exploits the deep deterministic policy gradient and can deal with the high-dimensional continuity of the action and state spaces. Extensive simulation results demonstrate that the DDRM can reduce the long-term average delay of the tasks effectively.","1941-0050","","10.1109/TII.2020.3028963","National Natural Science Foundation of China(grant numbers:61902029,61872044); Excellent Talents Projects of Beijing(grant numbers:9111923401); Scientific Research Project of Beijing Municipal Education Commission(grant numbers:KM202011232015); Science and Technology Development Fund of Macau SAR(grant numbers:0162/2019/A3); FDCT-MOST Joint Project(grant numbers:066/2019/AMJ); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214878","Deep reinforcement learning (DRL);dynamic resource management;industrial Internet of things (IIoT);mobile edge computing (MEC)","Task analysis;Delays;Resource management;Servers;Heuristic algorithms;Dynamic scheduling;Internet of Things","Internet of Things;learning (artificial intelligence);manufacturing systems;Markov processes;mobile computing;power control;resource allocation","deep deterministic policy gradient;long-term average delay;smart mobile equipments;5G network technologies;Things technology;industrial manufacturing systems;industrial IoT;computation unit;IIoT equipments;computation-intensive tasks;mobile edge computing server;MEC;task generation;dynamic resource management problem;computing resource allocation;deep reinforcement learning-based dynamic resource management algorithm;formulated MDP problem","","106","","27","IEEE","6 Oct 2020","","","IEEE","IEEE Journals"
"Human-Flow-Aware Long-Term Mobile Robot Task Planning Based on Hierarchical Reinforcement Learning","Y. Liu; L. Palmieri; I. Georgievski; M. Aiello","Corporate Sector Research and Advance Engineering, Robert Bosch GmbH, Renningen, Germany; Corporate Sector Research and Advance Engineering, Robert Bosch GmbH, Renningen, Germany; Service Computing Department, Institute of Architecture of Application Systems, University of Stuttgart, Stuttgart, Germany; Service Computing Department, Institute of Architecture of Application Systems, University of Stuttgart, Stuttgart, Germany","IEEE Robotics and Automation Letters","5 Jun 2023","2023","8","7","4068","4075","The difficulty in finding long-term planning policies for a mobile robot increases when operating in crowded and dynamic environments. State-of-the-art approaches do not consider cues of human-robot-shared dynamic environments. Aiming to fill this gap, we present a novel Human-Flow-Aware Guided Hierarchical Dyna-Q (HA-GHDQ) algorithm, which solves long-term robot task planning problems by using human motion patterns encoded in Maps of Dynamics (MoDs). To tackle the complexity of long-term robot operation in dynamic environments, we propose a combination of symbolic planning and Hierarchical Reinforcement Learning (HRL) that generates robot policies considering cost information derived from MoDs. We evaluated HA-GHDQ in a factory environment with two simulation and one real-world datasets to complete a transportation-and-assembly task. Our approach outperforms the baselines with respect to sample efficiency and final plan quality. Moreover, we show that it is more adaptable and robust against environmental changes than the baselines.","2377-3766","","10.1109/LRA.2023.3280816","EU Horizon 2020 Research and Innovation Program(grant numbers:101017274); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10137750","Task planning;integrated planning and learning;human-robot collaboration","Task analysis;Planning;Robots;Mobile robots;Costs;Reinforcement learning;Dynamics","human-robot interaction;learning (artificial intelligence);mobile robots;path planning;reinforcement learning","-assembly task;factory environment;final plan quality;HA-GHDQ;Hierarchical Dyna-Q algorithm;Hierarchical Reinforcement Learning;human motion patterns;human-robot-shared dynamic environments;long-term mobile robot task planning;long-term planning policies;long-term robot operation;long-term robot task planning problems;mobile robot increases;MoDs;novel Human-Flow-Aware;robot policies;symbolic planning;transportation","","1","","26","IEEE","29 May 2023","","","IEEE","IEEE Journals"
"Automatic HMI Structure Exploration Via Curiosity-Based Reinforcement Learning","Y. Cao; Y. Zheng; S. -W. Lin; Y. Liu; Y. S. Teo; Y. Toh; V. V. Adiga","Continental-NTU Corporate Lab, Nanyang Technological University, Singapore; Continental-NTU Corporate Lab, Nanyang Technological University, Singapore; Continental-NTU Corporate Lab, Nanyang Technological University, Singapore; Continental-NTU Corporate Lab, Nanyang Technological University, Singapore; Continental Automotive Singapore Pte. Ltd., Singapore; Continental Automotive Singapore Pte. Ltd., Singapore; Continental Automotive Singapore Pte. Ltd., Singapore","2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)","20 Jan 2022","2021","","","1151","1155","Discovering the underlying structure of HMI software efficiently and sufficiently for the purpose of testing without any prior knowledge on the software logic remains a difficult problem. The key challenge lies in the complexity of the HMI software and the high variance in the coverage of current methods. In this paper, we introduce the PathFinder, an effective and automatic HMI software exploration framework. PathFinder adopts a curiosity-based reinforcement learning framework to choose actions that lead to the discovery of more unknown states. Additionally, PathFinder progressively builds a navigation model during the exploration to further improve state coverage. We have conducted experiments on both simulations and real-world HMI software testing environment, which comprise a full tool chain of automobile dashboard instrument cluster. The exploration coverage outperforms manual and fuzzing methods which are the current industrial standards.","2643-1572","978-1-6654-0337-5","10.1109/ASE51524.2021.9678703","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678703","Test Automation;HMI Software Exploration;Reinforcement Learning;Curiosity","Knowledge engineering;Industries;Q-learning;Navigation;Instruments;Manuals;Software","automotive electronics;human computer interaction;program testing;reinforcement learning","exploration coverage;automatic HMI structure exploration;curiosity-based reinforcement learning;software logic;PathFinder;automatic HMI software exploration framework;state coverage;automobile dashboard instrument cluster","","1","","28","IEEE","20 Jan 2022","","","IEEE","IEEE Conferences"
"Multi-AGV Scheduling based on Hierarchical Intrinsically Rewarded Multi-Agent Reinforcement Learning","J. Zhang; B. Guo; Z. Sun; M. Li; J. Liu; Z. Yu; X. Fan","School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; Hangzhou Institute of Advanced Technology, Hangzhou, China","2022 IEEE 19th International Conference on Mobile Ad Hoc and Smart Systems (MASS)","19 Dec 2022","2022","","","155","161","Automated Guided Vehicle (AGV) has been widely used in automated warehouses and flexible manufacture systems for material delivery. As a flexible robot, AGV can finish automatic transportation of raw materials in different locations. The proper AGV scheduling strategy can effectively reduce the overall delivery time. To eliminate the large scheduling overhead from the centralized methods, we propose a multi-AGV distributed scheduling scheme in this paper. In particular, we design a Hierarchical Intrinsic Reward Mechanism (HIRM) for the multi-agent reinforcement learning to improve the convergence speed and the final policy level. Based on it, we propose the HIRM Bidirectionally-Coordinated Network (HIRM-BiCNet) based multi-AGV distributed scheduling scheme, to improve the scheduling success rate. The proposed scheme avoids the dependence on the global information and explicit communication. Experiment results demonstrate that our approach achieves impressive results at increase in scheduling success rate (30.75%) and decrease in scheduling time (16 time steps) compared to existing schemes.","2155-6814","978-1-6654-7180-0","10.1109/MASS56207.2022.00028","National Key R&D Program of China(grant numbers:2019YFB1703901); National Natural Science Foundation of China(grant numbers:62032020,61960206008,61725205,62102322); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9973692","Multi-agent Reinforcement Learning;AGVs;Distributed Scheduling;Intrinsic Motivation","Job shop scheduling;Remotely guided vehicles;Transportation;Collaboration;Reinforcement learning;Manipulators;Raw materials","automatic guided vehicles;flexible manufacturing systems;learning (artificial intelligence);multi-agent systems;multi-robot systems;scheduling","automated warehouses;flexible manufacture systems;Hierarchical Intrinsic Reward Mechanism;Hierarchical intrinsically;material delivery;multiagent reinforcement learning;multiAGV distributed scheduling scheme;multiAGV scheduling;proper AGV scheduling strategy;scheduling overhead;scheduling success rate;scheduling time","","","","28","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Greenhouse Climate Control","L. Wang; X. He; D. Luo","School of Computer Science and Technology, East China Normal University; School of Computer Science and Technology, East China Normal University; Tencent AI lab, Inc.","2020 IEEE International Conference on Knowledge Graph (ICKG)","11 Sep 2020","2020","","","474","480","Worldwide, the area of greenhouse production is increasing with the rapid growth of global population and demands for fresh food. However, the greenhouse industry encounters challenges to find automatic control policy. Reinforcement Learning (RL) is a powerful tool in solving the autonomous decision making problems. In this paper, we propose a novel Deep Reinforcement Learning framework for cucumber climate control. Although some machine learning methods have been proposed to address the dynamic climate control problem, these methods have two major issues. First, they only consider the current reward (e.g., the fruit weight of the cucumber). Second, previous study only considers one control variable. However, the growth of crops are impacted by multiple factors synchronously (e.g., CO2 and Temperature).To solve these challenges, we propose a Deep Reinforcement learning based climate control method, which can model future reward explicitly. We further consider the fruit weight and the cost of the planting in order to improve the cumulative fruit weight and reduce the costs.Extensive experiments are conducted on the cucumber simulator environment have shown the superior performance of our methods.","","978-1-7281-8156-1","10.1109/ICBK50248.2020.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9194467","On policy Reinforcement Learning;Cucumber Climate Control","Air pollution;Green products;Meteorology;Agriculture;Machine learning;Production;Trajectory","control engineering computing;decision making;greenhouses;learning (artificial intelligence)","greenhouse climate control;greenhouse production;automatic control;autonomous decision making problems;cucumber climate control;deep reinforcement learning;machine learning","","7","","30","IEEE","11 Sep 2020","","","IEEE","IEEE Conferences"
"Towards Optimal Attacks on Reinforcement Learning Policies","A. Russo; A. Proutiere","Division of Decision and Control Systems, KTH Royal Institute of Technology, Sweden; Division of Decision and Control Systems, KTH Royal Institute of Technology, Sweden","2021 American Control Conference (ACC)","28 Jul 2021","2021","","","4561","4567","Control policies, trained using Deep Reinforcement Learning, have been recently shown to be vulnerable to adversarial attacks introducing even minimal perturbations to the policy input. The attacks proposed so far have been designed using heuristics, based on existing adversarial example crafting techniques used to dupe classifiers in supervised learning. In contrast, this paper investigates the problem of devising optimal attacks, depending on a well-defined attacker's objective, e.g., to minimize the main agent average reward. When the policy and the system dynamics, as well as the rewards, are known to the attacker, a scenario referred to as a white-box attack, designing optimal attacks amounts to solving a Markov Decision Process. For what we call black-box attacks, where neither the policy nor the system is known, optimal attacks can be trained using Reinforcement Learning. We present numerical experiments demonstrating the efficiency of our attacks compared to existing attacks. We further quantify the potential impact of attacks and establish its connection to the smoothness of the policy under attack. Smooth policies are naturally less prone to attacks (e.g. Lipschitz policies, with respect to the state, are more resilient). Finally, we show that from the main agent perspective, the system uncertainties induced by the attack can be modelled using a Partially Observable Markov Decision Process (POMDP) framework. We demonstrate that using Reinforcement Learning methods tailored to POMDP (e.g. using Recurrent Neural Networks) leads to more resilient policies.","2378-5861","978-1-6654-4197-1","10.23919/ACC50511.2021.9483025","Swedish Foundation for Strategic Research(grant numbers:RIT17-0046); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9483025","","Uncertainty;Recurrent neural networks;System dynamics;Perturbation methods;Toy manufacturing industry;Supervised learning;Reinforcement learning","learning (artificial intelligence);Markov processes;recurrent neural nets","Reinforcement Learning policies;towards optimal attacks;existing attacks;black-box attacks;white-box attack;attacker;adversarial attacks;Deep Reinforcement Learning","","4","","25","","28 Jul 2021","","","IEEE","IEEE Conferences"
"Advanced Reinforcement Learning Solution for Clock Skew Engineering: Modified Q-Table Update Technique for Peak Current and IR Drop Minimization","S. A. Beheshti-Shirazi; N. Nazari; K. I. Gubbi; B. S. Latibari; S. Rafatirad; H. Homayoun; A. Sasan; P. D. S. Manoj","Department of ECE, George Mason University, Fairfax, VA, USA; Department of ECE, University of California, Davis, CA, USA; Department of ECE, University of California, Davis, CA, USA; Department of ECE, University of California, Davis, CA, USA; Department of ECE, University of California, Davis, CA, USA; Department of ECE, University of California, Davis, CA, USA; Department of ECE, University of California, Davis, CA, USA; Department of ECE, George Mason University, Fairfax, VA, USA","IEEE Access","22 Aug 2023","2023","11","","87869","87886","This paper discloses a Reinforcement Learning (RL) solution implemented to decrease the peak current by alteration of the clock skews. Clock skews are elements of the clock network calculated throughout the Clock Tree Synthesis (CTS) phase of physical design. Initially, the physical design tools targeted obtaining a balanced clock tree and decreasing the clock skew as low as possible. The resulting zero-skew clock tree caused a drastic increase in the current demand for the battery. The proposed solutions in this paper comprise a Reinforcement Learning agent that maneuvers throughout the design and updates the clock arrival time of each register by either adding, removing, or not changing it. The agent’s end game is to maximize the clock arrival distribution of the design. The Reinforcement learning solution allows the exploration and optimization of the clock tree synthesis process beyond the heuristic algorithms employed by traditional Electronic Design Automation (EDA) tools. This paper contains two experiments using the Reinforcement Learning algorithm. The first experiment results indicate a 35% reduction in peak current and a significant reduction in IR decrease (from package to transistor) in the chosen benchmarks. The second experiment modified the Q-table renewing technique, which resulted in another additional 10% improvement compared to the first experiment. In both experiments, the agent traverses the environment and explores different options despite creating timing violations and obtaining a substantial negative feedback reward for the actions taken. However, the timing violation fixed later results in the agent obtaining a future reward for modifying the clock arrival time of other registers. The overall process resulted in the broader spread of clock arrival distribution.","2169-3536","","10.1109/ACCESS.2023.3304534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214539","Clock tree synthesis (CTS);computer aided design;machine learning;peak current reduction;reinforcement learning;SARSA learning","Clocks;Timing;Reinforcement learning;Voltage;Transistors;Routing;Design automation;Learning systems","clocks;electronic engineering computing;optimisation;reinforcement learning","advanced reinforcement learning solution;clock arrival distribution;clock arrival time;clock skew engineering;clock tree synthesis process;CTS;EDA tools;electronic design automation tools;IR drop minimization;modified Q-table update technique;paper comprise;physical design tools;Q-table renewing technique;reinforcement learning agent algorithm;RL;zero-skew balanced clock tree synthesis phase","","","","57","CCBYNCND","11 Aug 2023","","","IEEE","IEEE Journals"
"Moral Reinforcement Learning Using Actual Causation","T. Herlau","DTU Compute, Technical University of Denmark, Lyngby, Denmark","2022 2nd International Conference on Computer, Control and Robotics (ICCCR)","14 Jun 2022","2022","","","179","185","Reinforcement learning systems will to a greater and greater extent make decisions that significantly impact the well-being of humans, and it is therefore essential that these systems make decisions that conform to our expectations of morally good behavior. The morally good is often defined in causal terms, as in whether one's actions have in fact caused a particular outcome, and whether the outcome could have been anticipated. We propose an online reinforcement learning method that learns a policy under the constraint that the agent should not be the cause of harm. This is accomplished by defining cause using the theory of actual causation and assigning blame to the agent when its actions are the actual cause of an undesirable outcome. We conduct experiments on a toy ethical dilemma in which a natural choice of reward function leads to clearly undesirable behavior, but our method learns a policy that avoids being the cause of harmful behavior, demonstrating the soundness of our approach. Allowing an agent to learn while observing causal moral distinctions such as blame, opens the possibility to learning policies that better conform to our moral judgments.","","978-1-6654-6674-5","10.1109/ICCCR54399.2022.9790262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9790262","Causality;Reinforcement learning;Actual Causation;Ethical reinforcement learning","Digital control;Ethics;Costs;Philosophical considerations;Toy manufacturing industry;Reinforcement learning;Forestry","causality;ethical aspects;learning (artificial intelligence);multi-agent systems","actual causation;agent;causal moral distinctions;causal terms;harmful behavior;moral judgments;moral reinforcement learning;morally good behavior;online reinforcement learning method;reward function;toy ethical dilemma","","","","23","IEEE","14 Jun 2022","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Approach for Inventory Control under Stochastic Lead Time and Demand","M. Shakya; B. -S. Lee; H. Y. Ng","School of Computer Science and Engineering Nanyang Technological University, Singapore; School of Computer Science and Engineering Nanyang Technological University, Singapore; Singapore Institute of Manufacturing Technology, Singapore","2022 IEEE Symposium Series on Computational Intelligence (SSCI)","30 Jan 2023","2022","","","760","766","In the last few years, the deep learning paradigm has experienced huge success in various machine learning research areas like computer vision, drug discoveries, natural language processing, and combinatorial optimizations. Moreover, the world has witnessed remarkable achievements when combining deep learning with reinforcement learning (now known as Deep Reinforcement Learning) in the areas like robotics, video games, business, and healthcare. One of the strongest parts of Deep Reinforcement Learning (DRL) is the ability to solve sequential decision-making problems. The inventory control problem is one such field where DRL can be applied to learn the optimal ordering policy to minimize the total inventory cost. In this paper, a linear supply chain model is considered with stochastic lead time and demand. The problem is then modeled into Markov Decision Processes (MDP). We then designed three different agents: Q-learning agent, Deep Q-network (DQN, also known as Deep Q-Learning), and (R, S) policy-based agent. The Q-learning and DQN agents were trained and evaluated. The (R, S) policy is used as a baseline as it is one of the most popular policies in business organizations. In comparison to traditional reinforcement learning (i.e Q-learning) and rule-based learning (i.e. (R, S) policy), the DQN model performs better in making the optimal ordering decision so that the total cost is minimized.","","978-1-6654-8768-9","10.1109/SSCI51031.2022.10022256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10022256","deep reinforcement learning;deep Q-learning;inventory optimization","Deep learning;Video games;Costs;Q-learning;Supply chains;Medical services;Inventory control","combinatorial mathematics;computer vision;decision making;deep learning (artificial intelligence);knowledge based systems;Markov processes;natural language processing;optimisation;production engineering computing;reinforcement learning;stock control;supply chains","business organizations;combinatorial optimizations;computer vision;deep learning paradigm;deep Q-network;deep reinforcement learning approach;DQN agents;DRL;drug discoveries;inventory control problem;linear supply chain model;machine learning research;Markov decision processes;natural language processing;optimal ordering policy;policy-based agent;Q-learning agent;rule-based learning;sequential decision-making problems;stochastic lead time;video games","","","","14","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Large-Scale Dynamic Scheduling for Flexible Job-Shop With Random Arrivals of New Jobs by Hierarchical Reinforcement Learning","K. Lei; P. Guo; Y. Wang; J. Zhang; X. Meng; L. Qian","School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China; School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China; Department of Mathematics, Auburn University at Montgomery, Montgomery, AL, USA; School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China; School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China; School of Mechanical Engineering, Southwest Jiaotong University, Chengdu, China","IEEE Transactions on Industrial Informatics","","2023","PP","99","1","12","As the intelligent manufacturing paradigm evolves, it is urgent to design a near real-time decision-making framework for handling the uncertainty and complexity of production line control. The dynamic flexible job shop scheduling problem (DFJSP) is frequently encountered in the manufacturing industry. However, it is still challenging to obtain high-quality schedules for DFJSP with dynamic job arrivals in real-time, especially facing thousands of operations from a large-scale scene with complex contexts in an assembly plant. This paper aims to propose a novel end-to-end hierarchical reinforcement learning framework for solving the large-scale DFJSP in near real-time. In the DFJSP, the processing information of newly arrived jobs is unknown in advance. Besides, two optimization tasks, including job operation selection and job-to-machine assignment, have to be handled, which means multiple actions must be controlled simultaneously. In our framework, a higher-level layer is designed to automatically divide the DFJSP into sub-problems, i.e., static FJSPs with different scales. And two lower-level layers are constructed to solve the sub-problems. In particular, one layer based on a graph neural network is in charge of sequencing job operations, and another layer based on a multi-layer perceptron is used to assign a machine to process the job operations. Numerical experiments, including offline training and online testing, are conducted on several instances with different scales. The results verify the superior performance of the proposed framework compared with existing dynamic scheduling methods, such as well-known dispatching rules and meta-heuristics.","1941-0050","","10.1109/TII.2023.3272661","National Key Research and Development Plan(grant numbers:2020YFB1712200); MOE (Ministry of Education in China) Project of Humanities and Social Sciences(grant numbers:21YJC630034); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10114974","Dynamic flexible job shop scheduling problem;graph neural network;markov decision process;real-time optimization;hierarchical reinforcement learning","Job shop scheduling;Dynamic scheduling;Real-time systems;Dispatching;Heuristic algorithms;Schedules;Production","","","","","","","IEEE","3 May 2023","","","IEEE","IEEE Early Access Articles"
"Neurodynamics Adaptive Reward and Action for Hand-to-Eye Calibration With Deep Reinforcement Learning","Z. Zheng; M. Yu; P. Guo; D. Zeng","School of Mathematics, South China University of Technology, Guangzhou, China; School of Mathematics, South China University of Technology, Guangzhou, China; School of Computational Science, Zhongkai University of Agriculture and Engineering, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China","IEEE Access","22 Jun 2023","2023","11","","60292","60304","Calibration performed by a robotic manipulator is crucial in the field of industrial intelligent production, as it ensures precise and accurate measurements. In this paper, we present a new method for addressing the hand-to-eye calibration problem using deep reinforcement learning. Our proposed algorithm utilizes an actor-critic framework and incorporates neurodynamics adaptive reward and action functions, which allows for better convergence, reduces the dependence on the initial value, and overcomes the local convergence issues of traditional deep reinforcement learning method. Additionally, we introduce a step-wise mechanism under the guidance of the attention mechanism, and zero stability to handle the complexity of the calibration task in challenging environments. A number of experiments were conducted to demonstrate the validity of the proposed algorithm. The experimental results show that our proposed algorithm can achieve a nearly 100% success rate after training phase. Additionally, we compared our proposed algorithm with other widely used methods, such as deterministic deep policy gradient (DDPG) and soft actor-critic (SAC) to further demonstrate its effectiveness.","2169-3536","","10.1109/ACCESS.2023.3287098","Fundamental Research Program 401 of Guangdong, China(grant numbers:2020B1515310023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10154063","Calibration;deep reinforcement learning;actor-critic;neurodynamics adaptive method","Calibration;Robots;Cameras;Robot vision systems;Reinforcement learning;Manipulators;Deep learning","calibration;control engineering computing;deep learning (artificial intelligence);gradient methods;industrial manipulators;production engineering computing;reinforcement learning","action functions;actor-critic framework;calibration task;DDPG;deep reinforcement learning;deterministic deep policy gradient;hand-to-eye calibration problem;industrial intelligent production;local convergence issues;neurodynamics adaptive reward;robotic manipulator;SAC;soft actor-critic;step-wise mechanism","","","","42","CCBY","16 Jun 2023","","","IEEE","IEEE Journals"
"Deep Learning with TensorFlow and Keras: Build and deploy supervised, unsupervised, deep, and reinforcement learning models","A. Kapoor; A. Gulli; S. Pal; F. Chollet",NA; NA; NA; NA,"Deep Learning with TensorFlow and Keras: Build and deploy supervised, unsupervised, deep, and reinforcement learning models","","2022","","","","","Build cutting edge machine and deep learning systems for the lab, production, and mobile devices. Purchase of the print or Kindle book includes a free eBook in PDF format.Key FeaturesUnderstand the fundamentals of deep learning and machine learning through clear explanations and extensive code samplesImplement graph neural networks, transformers using Hugging Face and TensorFlow Hub, and joint and contrastive learningLearn cutting-edge machine and deep learning techniquesBook DescriptionDeep Learning with TensorFlow and Keras teaches you neural networks and deep learning techniques using TensorFlow (TF) and Keras. You'll learn how to write deep learning applications in the most powerful, popular, and scalable machine learning stack available. TensorFlow 2.x focuses on simplicity and ease of use, with updates like eager execution, intuitive higher-level APIs based on Keras, and flexible model building on any platform. This book uses the latest TF 2.0 features and libraries to present an overview of supervised and unsupervised machine learning models and provides a comprehensive analysis of deep learning and reinforcement learning models using practical examples for the cloud, mobile, and large production environments. This book also shows you how to create neural networks with TensorFlow, runs through popular algorithms (regression, convolutional neural networks (CNNs), transformers, generative adversarial networks (GANs), recurrent neural networks (RNNs), natural language processing (NLP), and graph neural networks (GNNs)), covers working example apps, and then dives into TF in production, TF mobile, and TensorFlow with AutoML.What you will learnLearn how to use the popular GNNs with TensorFlow to carry out graph mining tasksDiscover the world of transformers, from pretraining to fine-tuning to evaluating themApply self-supervised learning to natural language processing, computer vision, and audio signal processingCombine probabilistic and deep learning models using TensorFlow ProbabilityTrain your models on the cloud and put TF to work in real environmentsBuild machine learning and deep learning systems with TensorFlow 2.x and the Keras APIWho this book is forThis hands-on machine learning book is for Python developers and data scientists who want to build machine learning and deep learning systems with TensorFlow. This book gives you the theory and practice required to use Keras, TensorFlow, and AutoML to build machine learning systems. Some machine learning knowledge would be useful. We don’t assume TF knowledge.","","9781803245713","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162596.pdf&bkn=10162595&pdfType=book","","","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Adversarial Inverse Reinforcement Learning With Self-Attention Dynamics Model","J. Sun; L. Yu; P. Dong; B. Lu; B. Zhou","Chinese University of Hong Kong, Hong Kong, China; Computer Science Department, Stanford University, Mountain View, CA, USA; State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China; Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China","IEEE Robotics and Automation Letters","9 Mar 2021","2021","6","2","1880","1886","In many real-world applications where specifying a proper reward function is difficult, it is desirable to learn policies from expert demonstrations. Adversarial Inverse Reinforcement Learning (AIRL) is one of the most common approaches for learning from demonstrations. However, due to the stochastic policy, current computation graph of AIRL is no longer end-to-end differentiable like Generative Adversarial Networks (GANs), resulting in the need for high-variance gradient estimation methods and large sample size. In this work, we propose the Model-based Adversarial Inverse Reinforcement Learning (MAIRL), an end-to-end model-based policy optimization method with self-attention. By adopting the self-attention dynamics model to make the computation graph end-to-end differentiable, MAIRL has the low variance for policy optimization. We evaluate our approach thoroughly on various control tasks. The experimental results show that our approach not only learns near-optimal rewards and policies that match expert behavior but also outperforms previous inverse reinforcement learning algorithms in real robot experiments. Code is available at https://decisionforce.github.io/MAIRL/.","2377-3766","","10.1109/LRA.2021.3061397","InnoHK CPII; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361118","Imitation learning;learning from demonstration","Computational modeling;Atmospheric modeling;Reinforcement learning;Training;Trajectory;Stochastic processes;Task analysis","gradient methods;learning (artificial intelligence);optimisation","self-attention dynamics model;AIRL;stochastic policy;current computation graph;end-to-end differentiable;Generative Adversarial Networks;high-variance gradient estimation methods;Model-based Adversarial Inverse Reinforcement Learning;end-to-end model-based policy optimization method;computation graph end-to-end","","6","","27","IEEE","23 Feb 2021","","","IEEE","IEEE Journals"
"AGV Path Planning Using Curiosity-Driven Deep Reinforcement Learning","H. Yin; Y. Lin; J. Yan; Q. Meng; K. Festl; L. Schichler; D. Watzenig","School of Electronic and Information Engineering, Tongji University, Shanghai, China; School of Electronic and Information Engineering, Tongji University, Shanghai, China; School of Electronic and Information Engineering, Tongji University, Shanghai, China; MIS Research Group, University of South-eastern Norway, Vestfold, Norway; Virtual Vehicle Research GmbH, Graz, Austria; Virtual Vehicle Research GmbH, Graz, Austria; Institute of Automation and Control, Graz University of Technology, Graz, Austria","2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)","28 Sep 2023","2023","","","1","6","In the field of intelligent manufacturing, automated guided vehicles (AGV s) are required to plan an optimal path to a destination quickly and safely in complex environments. Several research papers have shown that Deep Reinforcement Learning (DRL) can plan a reasonable path for AGVs. However, almost all studies do not consider the influence of sparse external rewards. In addition, most research still uses an unrealistic 2D grid for their path planning environments. To alleviate these problems, we propose a novel AGV path planning method and provide the virtual environments of AGV s that approximate the real physical world. Moreover, the curiosity mechanism helps to enhance the Proximal Policy Optimization (PPO) method to provide additional intrinsic rewards to AGV agents in the sparse reward scenario. As for the environments, we no longer divide them into a 2D grid. Thus, the AGV has a continuous position in our environments. In addition, dynamic obstacles are added to make our environments more realistic. Based on our experiments, we make qualitative and quantitative analyses of the performance of the proposed method and the baseline DRL method. The empirical results show that our proposed method can make AGVs plan a reasonable and safe path with considerable speed during the training process in our virtual environments.","2161-8089","979-8-3503-2069-5","10.1109/CASE56687.2023.10260579","National Natural Science Foundation of China(grant numbers:62133011,61922063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10260579","path planning;deep reinforcement learning;curiosity;AGV","Training;Deep learning;Remotely guided vehicles;Statistical analysis;Virtual environments;Reinforcement learning;Path planning","automatic guided vehicles;collision avoidance;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;multi-agent systems;optimisation;path planning;reinforcement learning","additional intrinsic rewards;AGV agents;automated guided vehicles;complex environments;curiosity-driven Deep Reinforcement;novel AGV path planning method;optimal path;path planning environments;Proximal Policy Optimization method;reasonable path;safe path;sparse external rewards;sparse reward scenario;unrealistic 2D grid;virtual environments","","","","25","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Fully Automated Design Method Based on Reinforcement Learning and Surrogate Modeling for Antenna Array Decoupling","Z. Wei; Z. Zhou; P. Wang; J. Ren; Y. Yin; G. F. Pedersen; M. Shen","Department of the Electronic Systems, Aalborg University, Aalborg, Denmark; Department of the Electronic Systems, Aalborg University, Aalborg, Denmark; Department of the Electronic Systems, Aalborg University, Aalborg, Denmark; National Key Laboratory of Antennas and Microwave Technology, Xidian University, Xi’an, China; National Key Laboratory of Antennas and Microwave Technology, Xidian University, Xi’an, China; Department of the Electronic Systems, Aalborg University, Aalborg, Denmark; Department of the Electronic Systems, Aalborg University, Aalborg, Denmark","IEEE Transactions on Antennas and Propagation","18 Jan 2023","2023","71","1","660","671","Modern electromagnetic (EM) device design generally relies on extensive iterative optimizations by designers using simulation software (e.g., CST), which is a very time-consuming and tedious process. To relieve human engineers and boost productivity, we proposed a machine learning (ML) framework to solve the problem of automated design for EM tasks. The proposed approach combines advanced reinforcement learning (RL) algorithms and deep neural networks (DNNs) in an attempt to simulate the decision-making process of human designers to realize automation learning. Specifically, the RL-based agent can interact with the EM design software without engaging human designers, allowing for automated design. Besides, the data accumulated during EM software simulation in the early design stage are reused as training data to build a DNN surrogate model to replace the time-consuming EM simulation and further accelerate the training of RL to achieve better optimization of EM design. Two types of antenna array decoupling including  $1\times $  2 and  $1\times $  4 arrays working at 3.5 GHz are used as test vehicles to validate the proposed method. The decoupling metasurfaces designed by the proposed fully automated method based on RL showed satisfactory results comparable to the results achievable by human designers. This indicates that the proposed method can be used to build powerful tools to boost the design efficiency of EM devices.","1558-2221","","10.1109/TAP.2022.3221613","China Scholar Council and; National Natural Science Foundation of China(grant numbers:61901316); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9953971","Decoupling metasurface (DCMS);deep neural networks (DNNs);design automation;reinforcement learning (RL)","Antenna arrays;Metasurfaces;Data models;Training data;Training;Estimation;Optimization","antenna arrays;electrical engineering computing;iterative methods;neural nets;optimisation;reinforcement learning","antenna array decoupling;automated design method;automation learning;decision-making process;design efficiency;DNN surrogate model;EM design software;EM software simulation;extensive iterative optimizations;frequency 3.5 GHz;fully automated method;human designers;human engineers;machine learning framework;modern electromagnetic device design;reinforcement learning algorithms;RL-based agent;simulation software","","10","","55","IEEE","16 Nov 2022","","","IEEE","IEEE Journals"
"Toward Learning Human-Like, Safe and Comfortable Car-Following Policies With a Novel Deep Reinforcement Learning Approach","M. U. Yavas; T. Kumbasar; N. K. Ure","Department of Mechatronics Engineering, Istanbul Technical University, Istanbul, Turkey; Department of Control and Automation Engineering, Istanbul Technical University, Istanbul, Turkey; Artificial Intelligence and Data Science Research Center, Istanbul Technical University, Istanbul, Turkey","IEEE Access","24 Feb 2023","2023","11","","16843","16854","In this paper, we present an advanced adaptive cruise control (ACC) concept powered by Deep Reinforcement Learning (DRL) that generates safe, human-like, and comfortable car-following policies. Unlike the current trend in developing DRL-based ACC systems, we propose defining the action space of the DRL agent with discrete actions rather than continuous ones, since human drivers never set the throttle/brake pedal level to be actuated, but rather the required change of the current pedal levels. Through this human-like throttle-brake manipulation representation, we also define explicit actions for holding (keeping the last action) and coasting (no action), which are usually omitted as actions in ACC systems. Moreover, based on the investigation of a real-world driving dataset, we cast a novel reward function that is easy to interpret and personalized. The proposed reward enforces the agent to learn stable and safe actions, while also encouraging the holding and coasting actions, just like a human driver would. The proposed discrete action DRL agent is trained with action masking, and the reward terms are completely derived from the real-world dataset collected from a human driver. We present exhaustive comparative results to show the advantages of the proposed DRL approach in both simulation and scenarios extracted from real-world driving. We clearly show that the proposed policy imitates human driving significantly better and handles complex driving situations, such as cut-ins and cut-outs, implicitly, in comparison with a DRL agent trained with a widely-used reward function proposed for ACC, a model predictive control structure, and traditional car-following approaches.","2169-3536","","10.1109/ACCESS.2023.3245831","Turkish Academy of Sciences of Turkey (TÜBA); TÜBA Outstanding Young Scientist Award Programme (GEBİP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10045659","Adaptive cruise control;reinforcement learning;deep learning;naturalistic driving;advanced driving assistance systems","Vehicles;Cruise control;Behavioral sciences;Advanced driver assistance systems;Tuning;Safety;Heuristic algorithms;Reinforcement learning;Deep learning","automobiles;control engineering computing;deep learning (artificial intelligence);optimal control;predictive control;reinforcement learning;road traffic control;traffic engineering computing;velocity control","ACC;action masking;action space;adaptive cruise control;car-following policies;coasting actions;deep reinforcement learning;discrete actions;DRL;explicit actions;human driving;model predictive control structure;reward function;reward terms;throttle-brake manipulation representation","","1","","30","CCBY","15 Feb 2023","","","IEEE","IEEE Journals"
"Cost dependent strategy for electricity markets bidding based on adaptive reinforcement learning","T. Pinto; Z. Vale; F. Rodrigues; I. Praça; H. Morais","GECAD-Knowledge Engineering and Decision-Support Research Center of the Institute of Engineering, Polytechnic of Porto (ISEP IPP), Porto, Portugal; GECAD-Knowledge Engineering and Decision-Support Research Center of the Institute of Engineering, Polytechnic of Porto (ISEP IPP), Porto, Portugal; GECAD-Knowledge Engineering and Decision-Support Research Center of the Institute of Engineering, Polytechnic of Porto (ISEP IPP), Porto, Portugal; GECAD-Knowledge Engineering and Decision-Support Research Center of the Institute of Engineering, Polytechnic of Porto (ISEP IPP), Porto, Portugal; GECAD-Knowledge Engineering and Decision-Support Research Center of the Institute of Engineering, Polytechnic of Porto (ISEP IPP), Porto, Portugal","2011 16th International Conference on Intelligent System Applications to Power Systems","17 Nov 2011","2011","","","1","6","Electricity markets are complex environments, involving a large number of different entities, playing in a dynamic scene to obtain the best advantages and profits. MASCEM is a multi-agent electricity market simulator to model market players and simulate their operation in the market. Market players are entities with specific characteristics and objectives, making their decisions and interacting with other players. MASCEM provides several dynamic strategies for agents' behavior. This paper presents a method that aims to provide market players with strategic bidding capabilities, allowing them to obtain the higher possible gains out of the market. This method uses a reinforcement learning algorithm to learn from experience how to choose the best from a set of possible bids. These bids are defined accordingly to the cost function that each producer presents.","","978-1-4577-0809-1","10.1109/ISAP.2011.6082167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6082167","Bidding Strategies;Electricity Markets;Multiagent Simulation;Reinforcement Learning;Simulated Annealing","Production;Electricity supply industry;Learning;Simulated annealing;Generators;Cost function;Adaptation models","learning (artificial intelligence);power markets","electricity markets bidding;adaptive reinforcement learning;MASCEM;multi-agent electricity market simulator;market players;strategic bidding","","2","","14","IEEE","17 Nov 2011","","","IEEE","IEEE Conferences"
"Navigation Support for an Autonomous Ferry Using Deep Reinforcement Learning in Simulated Maritime Environments","N. Smirnov; S. Tomforde","Department of Computer Science. Intelligent Systems, Christian-Albrechts-University of Kiel, Kiel, Germany; Department of Computer Science. Intelligent Systems, Christian-Albrechts-University of Kiel, Kiel, Germany","2022 IEEE Conference on Cognitive and Computational Aspects of Situation Management (CogSIMA)","22 Jul 2022","2022","","","142","149","The development of shipping is witnessing increasing automation - from existing assistance systems to fully autonomous behaviour. In this article, we present a building block on the way to a fully autonomous passenger ferry for the Kiel Fjord in Germany by presenting a simulation-based approach to situation modelling of maritime environments and the behaviour therein. We show how this can be used for a posteriori analysis of the possible behaviour of the ship. This analysis then in turn flows into the decision-making process. We also use this environment to investigate the application of Deep Reinforcement Learning techniques to optimize navigation tasks and identify challenges and limitations.","2379-1675","978-1-6654-8330-8","10.1109/CogSIMA54611.2022.9830689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9830689","situation modelling;autonomous ship;ferry;deep reinforcement learning;maritime simulation","Automation;Navigation;Computational modeling;Conferences;Decision making;Reinforcement learning;Task analysis","decision making;learning (artificial intelligence);marine safety;ships","fully autonomous passenger ferry;Kiel Fjord;simulation-based approach;situation modelling;posteriori analysis;possible behaviour;ship;decision-making process;navigation tasks;navigation support;autonomous ferry;Deep Reinforcement Learning;simulated maritime environments;increasing automation;assistance systems;fully autonomous behaviour","","2","","28","IEEE","22 Jul 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning Application Testing Method based on Multi-attribute Fusion","L. Cai; J. Wang; M. Chen; J. Wang","School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; Shanghai Key Laboratory of Computer Software Testing & Evaluating, Shanghai Development Center of Computer Software Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China","2022 9th International Conference on Dependable Systems and Their Applications (DSA)","25 Oct 2022","2022","","","24","33","Reinforcement learning has been successfully applied to assess the reliability of applications, but the existing testing methods based on reinforcement learning have the problems of invalid interactive widgets and difficult training, resulting in low testing efficiency. In order to solve these problems, this paper proposes a lightweight application automation testing method of deep reinforcement learning based on multi-attribute fusion (MARTesting). First, the invalid widgets are removed by the difference operation of the attribute sets of the current and previous state, then the attributes of all widget elements on the page are abstracted into state as the input of the neural network, and a state is accurately determined by fusing the position and text information of page elements, finally combine the novelty of the state and the execution frequency of the action as a reward function. The experimental results on six open source applications show that the MARTesting method proposed in this paper has achieved significant improvements in code coverage and branch coverage compared with existing methods.","2767-6684","978-1-6654-8877-8","10.1109/DSA56465.2022.00013","National Key R&D Program of China(grant numbers:2018YFB1403400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9914400","Deep Reinforcement Learning;Application Testing;Muti-Attribute Fusion","Software testing;Training;Recurrent neural networks;Codes;Automation;Semantics;Reinforcement learning","neural nets;program testing;public domain software;reinforcement learning","multiattribute fusion;testing methods;invalid interactive widgets;low testing efficiency;lightweight application automation testing method;deep reinforcement learning;invalid widgets;attribute sets;widget elements;page elements;open source applications;MARTesting method","","1","","30","IEEE","25 Oct 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Stream Reservation for Distributed Time Sensitive Networks","F. Xu; D. Qiu; B. Lv","College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Beijing Institute of Astronautical Systems Engineering, Beijing, China","2022 IEEE 22nd International Conference on Communication Technology (ICCT)","27 Mar 2023","2022","","","1811","1816","In order to achieve the pursuit of deterministic delay and unified network transmission in the industrial automation field, the time-sensitive networking group has developed a stream reservation mechanism to reserve the required bandwidth for time-sensitive traffic. In the distributed network configuration model, how to configure per-hop delay guarantee properly has become a key problem to restrict network performance, the reason is that an excessive loose or tight guarantee will reduce the streams that can be added to the network. Therefore, this paper proposes a stream reservation method based on reinforcement learning which can dynamically adjust the per-hop delay guarantee to improve the acceptance of streams. For this, it firstly defines the optimization problem of network configuration in the reservation process and give the per-hop delay model. Then the operation of the algorithm on bridge's data plane and control plane is simulated. We evaluate the performance of the algorithm with acceptance rate and bandwidth utilization in different scenarios, and the results demonstrate the effectiveness of the proposed algorithm.","2576-7828","978-1-6654-7067-4","10.1109/ICCT56141.2022.10073138","National Key R and D Program of China(grant numbers:2017YFB0502700); National Natural Science Foundation of China(grant numbers:61471195); Fundamental Research Funds for the Central Universities(grant numbers:NZ2016105 KFJJ20180404); National Defense Pre-Research Foundation of China(grant numbers:6140413020116HK02001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10073138","time sensitive networking;stream reservation;reinforcement learning;distributed configuration model","Automation;Heuristic algorithms;Reinforcement learning;Bandwidth;Communications technology;Delays;Optimization","reinforcement learning;telecommunication computing;telecommunication scheduling;telecommunication traffic;wireless LAN","bandwidth utilization;deterministic delay;distributed network configuration model;distributed time sensitive networks;industrial automation field;network configuration;network performance;optimization problem;per-hop delay guarantee;reinforcement learning-based stream reservation;reservation process;stream reservation mechanism;stream reservation method;time-sensitive networking group;time-sensitive traffic;unified network transmission","","","","9","IEEE","27 Mar 2023","","","IEEE","IEEE Conferences"
"Development of Visual Smooth Pursuit Model Using Inverse Reinforcement Learning For Humanoid Robots","H. U. Din; W. Muhammad; N. Siddique; M. J. Irshad; A. Asghar; M. W. Jabbar","Department of Electrical Engineering, University of Gujrat, Gujrat, Pakistan; Department of Electrical Engineering, University of Gujrat, Gujrat, Pakistan; Department of Electrical Engineering, University of Gujrat, Gujrat, Pakistan; Department of Electrical Engineering, University of Gujrat, Gujrat, Pakistan; Department of Electrical Engineering, University of Gujrat, Gujrat, Pakistan; Department of Electrical Engineering, University of Gujrat, Gujrat, Pakistan","2023 International Conference on Energy, Power, Environment, Control, and Computing (ICEPECC)","11 Aug 2023","2023","","","1","6","This Early in the $20^{\mathrm{t}\mathrm{h}}$ century, research on smooth pursuit began. Nowadays, it may be found in everything from little robots to sophisticated automation projects. There are now many study studies in this area, but they are all reward-based conventionally, which is not biologically feasible. In these techniques, the robot performs an action, and the agent determines the next course of action based on the performance and a certain kind of positive or negative reward. The reward in this thesis is derived from the sensory space rather than the action space, which enables the robot to predict the reward without any prior defined reward. PC/BC-DIM, a new Deep Inverse Reinforcement Learning (DIRL) technique, is presented. Rather than relying on previously specified rewards, PC/BC-DIM assesses the prediction error between certain inputs and determines whether or not to update the weight. It was controlled independently and successfully arrived at the target place, yielding satisfying results. The iCub humanoid robot simulator is used to evaluate the performance of the suggested system.","","978-1-6654-7601-0","10.1109/ICEPECC57281.2023.10209527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10209527","Smooth pursuit;Humanoid robots;PC/BC-DIM;Inverse Reinforcement Learning","Training;Visualization;Automation;Neurons;Humanoid robots;Reinforcement learning;Artificial neural networks","control engineering computing;deep learning (artificial intelligence);humanoid robots;mobile robots;motion control;reinforcement learning","action space;automation projects;BC-DIM;deep inverse reinforcement learning;DIRL;humanoid robots;iCub humanoid robot simulator;negative reward;PC-DIM;positive reward;prediction error;prior defined reward;sensory space;visual smooth pursuit model","","","","19","IEEE","11 Aug 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-Based Task Offloading and Resource Allocation for Industrial IoT in MEC Federation System","H. Mai Do; T. P. Tran; M. Yoo","Department of Information Communication Convergence Technology, Soongsil University, Seoul, South Korea; Department of Information Communication Convergence Technology, Soongsil University, Seoul, South Korea; School of Electronic Engineering, Soongsil University, Seoul, South Korea","IEEE Access","10 Aug 2023","2023","11","","83150","83170","The rapid growth of the Internet of Things (IoT) has resulted in the development of intelligent industrial systems known as Industrial IoT (IIoT). These systems integrate smart devices, sensors, cameras, and 5G technologies to enable automated data gathering and analysis boost production efficiency and overcome scalability issues. However, IoT devices have limited computer power, memory, and battery capacities. To address these challenges, mobile edge computing (MEC) has been introduced to IIoT systems to reduce the computational burden on the devices. While the dedicated MEC paradigm limits optimal resource utilization and load balancing, the MEC federation can potentially overcome these drawbacks. However, previous studies have relied on idealized assumptions when developing optimal models, raising concerns about their practical applicability. In this study, we investigated the joint decision offloading and resource allocation problem for MEC federation in the IIoT. Specifically, an optimization model was constructed based on all real-world factors influencing system performance. To minimize the total energy delay cost, the original problem was transformed into a Markov decision process. Considering task generation dynamics and continuity, we addressed the Markov decision process using a deep reinforcement learning method. We propose a deep deterministic policy gradient algorithm with prioritized experience replay (DDPG-PER)-based resource allocation that can handle high-dimensional continuity of action and state spaces. The simulation results indicate that the proposed approach effectively minimizes the energy-delay costs associated with tasks.","2169-3536","","10.1109/ACCESS.2023.3302518","Institute of Information and Communications Technology Planning and Evaluation (IITP) funded by the Korean Government [Ministry of Science and ICT (MSIT)], South Korea, through the Development of Candidate Element Technology for Intelligent 6G Mobile Core Network(grant numbers:2022-0-01015); MSIT under the Information Technology Research Center (ITRC) Support Program Supervised by IITP(grant numbers:IITP-2023-2021-0-02046); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10210011","MEC federation;IIoT;task offloading;resource allocation;Markov decision process;deep reinforcement learning","Industrial Internet of Things;Servers;Task analysis;Resource management;Optimization;Delays;Energy consumption;Markov processes;Reinforcement learning;Deep learning;Decision making","5G mobile communication;decision making;deep learning (artificial intelligence);edge computing;gradient methods;Internet of Things;Markov processes;mobile computing;optimisation;production engineering computing;reinforcement learning;resource allocation","automated data gathering;battery capacities;computational burden;computer power;deep deterministic policy gradient algorithm;deep reinforcement learning method;IIoT systems;Industrial IoT;intelligent industrial systems;IoT devices;joint decision offloading;load balancing;Markov decision process;MEC federation system;mobile edge computing;optimal models;optimization model;prioritized experience replay-based resource allocation;production efficiency;scalability issues;smart devices;system performance;task generation dynamics;task offloading;total energy delay cost","","","","41","CCBYNCND","7 Aug 2023","","","IEEE","IEEE Journals"
"A Proactive Cloud Application Auto-Scaler using Reinforcement Learning","A. Heimerson; J. Eker; K. -E. Årzén","Dept. of Automatic Control, Lund University; Dept. of Automatic Control, Lund University/Ericsson Research; Dept. of Automatic Control, Lund University","2022 IEEE/ACM 15th International Conference on Utility and Cloud Computing (UCC)","14 Mar 2023","2022","","","213","220","This work explores the use of reinforcement learning to design a proactive cloud resource auto-scaler that is able to predict usage across a distributed microservice application. The focus is on serving time-sensitive workloads, e.g., industrial automation, connected XRNR (eXtended Reality/Virtual Reality), etc., where each job has a deadline and there is some cost associated with missing a deadline. A simple workload model, as well as a microservice application model, is presented. A reinforcement learning agent is trained to identify workloads and predict needed utilization for identified service chains. The results are compared to standard purely reactive techniques.","","978-1-6654-6087-3","10.1109/UCC56403.2022.00040","Vinnova; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10061723","Cloud Computing;Reinforcement Learning;Auto-Scaling;Microservice;Proactive Control","Training;Cloud computing;Costs;Automation;Computational modeling;Microservice architectures;Reinforcement learning","cloud computing;learning (artificial intelligence);reinforcement learning;resource allocation;service-oriented architecture;virtual reality","connected XRNR;deadline;distributed microservice application;industrial automation;microservice application model;proactive cloud application auto-scaler;proactive cloud resource auto-scaler;reinforcement learning agent;simple workload model","","","","14","IEEE","14 Mar 2023","","","IEEE","IEEE Conferences"
"Captcha me if you can: Imitation Games with Reinforcement Learning","I. Tsingenopoulos; D. Preuveneers; L. Desmet; W. Joosen","imec-DistriNet, KU Leuven, Leuven, Belgium; imec-DistriNet, KU Leuven, Leuven, Belgium; imec-DistriNet, KU Leuven, Leuven, Belgium; imec-DistriNet, KU Leuven, Leuven, Belgium","2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P)","23 Jun 2022","2022","","","719","735","Since their inception, Captchas have been widely used as reverse Turing tests for combating bot proliferation on the web. This has resulted in an arms race between bot developers that automate Captcha solvers and Captcha services that adjust the challenges accordingly or come up with new ones altogether. Ultimately, older generations could be bypassed consistently, and thus in the third version of reCAPTCHA, Google offers zero user friction. The intent in the new system is not only to avoid interrupting user experience but to also obfuscate the nature of the challenge itself, being much less prominent than a text or image recognition task. We introduce a methodology that learns through interaction how to evade detection, while collecting and analyzing reCAPTCHA v3 scores over fifteen months and various web environments. With reinforcement learning as the backbone, we build models that can simulate human-like web browsing behaviour by using the returned score as an informative signal. Our study exposes an important vulnerability: while the score is influenced by a multitude of undisclosed factors, it is easily accessible and it enables adversaries to learn and perfect evasive models. Notably, we demonstrate that our automation models, which integrate general web browsing capabilities, transfer between websites with an evasion rate up to 99.6%.","","978-1-6654-1614-6","10.1109/EuroSP53844.2022.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797367","Reinforcement Learning;Adversarial Machine Learning;Captcha","Privacy;Automation;Friction;Weapons;Reinforcement learning;User experience;Internet","computer games;human computer interaction;image recognition;Internet;online front-ends;reinforcement learning;security of data","imitation games;reinforcement learning;reverse Turing tests;bot proliferation;Captcha services;zero user friction;user experience;Web browsing behaviour;automation models;reCAPTCHA;image recognition","","","","50","IEEE","23 Jun 2022","","","IEEE","IEEE Conferences"
"Performance Testing Using a Smart Reinforcement Learning-Driven Test Agent","M. H. Moghadam; G. Hamidi; M. Borg; M. Saadatmand; M. Bohlin; B. Lisper; P. Potena","RISE Research Institutes of Sweden, Västerås, Sweden; Mälardalen University, Västerås, Sweden; RISE Research Institutes of Sweden, Västerås, Sweden; RISE Research Institutes of Sweden, Västerås, Sweden; Mälardalen University, Västerås, Sweden; Mälardalen University, Västerås, Sweden; RISE Research Institutes of Sweden, Västerås, Sweden","2021 IEEE Congress on Evolutionary Computation (CEC)","9 Aug 2021","2021","","","2385","2394","Performance testing with the aim of generating an efficient and effective workload to identify performance issues is challenging. Many of the automated approaches mainly rely on analyzing system models, source code, or extracting the usage pattern of the system during the execution. However, such information and artifacts are not always available. Moreover, all the transactions within a generated workload do not impact the performance of the system the same way, a finely tuned workload could accomplish the test objective in an efficient way. Model-free reinforcement learning is widely used for finding the optimal behavior to accomplish an objective in many decision-making problems without relying on a model of the system. This paper proposes that if the optimal policy (way) for generating test workload to meet a test objective can be learned by a test agent, then efficient test automation would be possible without relying on system models or source code. We present a self-adaptive reinforcement learning-driven load testing agent, RELOAD, that learns the optimal policy for test workload generation and generates an effective workload efficiently to meet the test objective. Once the agent learns the optimal policy, it can reuse the learned policy in subsequent testing activities. Our experiments show that the proposed intelligent load test agent can accomplish the test objective with lower test cost compared to common load testing procedures, and results in higher test efficiency.","","978-1-7281-8393-0","10.1109/CEC45853.2021.9504763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9504763","performance testing;load testing;workload generation;reinforcement learning;autonomous testing","Analytical models;Automation;Transfer learning;Decision making;Reinforcement learning;Knowledge representation;Evolutionary computation","automatic testing;decision making;learning (artificial intelligence);program testing;source code (software)","performance testing;smart reinforcement learning-driven test agent;system models;source code;generated workload;finely tuned workload;test objective;model-free reinforcement learning;efficient test automation;self-adaptive reinforcement learning-driven load testing agent;optimal policy;test workload generation;learned policy;testing activities;intelligent load test agent;test efficiency;load testing procedures","","","","46","IEEE","9 Aug 2021","","","IEEE","IEEE Conferences"
"Reducing the Learning Time of Reinforcement Learning for the Supervisory Control of Discrete Event Systems","J. Yang; K. Tan; L. Feng; A. M. El-Sherbeeny; Z. Li","School of Electro-Mechanical Engineering, Xidian University, Xi’an, China; Department of Machine Design, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Machine Design, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Industrial Engineering, College of Engineering, King Saud University, Riyadh, Saudi Arabia; School of Electro-Mechanical Engineering, Xidian University, Xi’an, China","IEEE Access","23 Jun 2023","2023","11","","59840","59853","Reinforcement learning (RL) can obtain the supervisory controller for discrete-event systems modeled by finite automata and temporal logic. The published methods often have two limitations. First, a large number of training data are required to learn the RL controller. Second, the RL algorithms do not consider uncontrollable events, which are essential for supervisory control theory (SCT). To address the limitations, we first apply SCT to find the supervisors for the specifications modeled by automata. These supervisors remove illegal training data violating these specifications and hence reduce the exploration space of the RL algorithm. For the remaining specifications modeled by temporal logic, the RL algorithm is applied to search for the optimal control decision within the confined exploration space. Uncontrollable events are considered by the RL algorithm as uncertainties in the plant model. The proposed method can obtain a nonblocking supervisor for all specifications with less learning time than the published methods.","2169-3536","","10.1109/ACCESS.2023.3285432","Chinese Scholarship Council (CSC), KTH Centre of Excellence in Production Research (XPRES); National Key R&D Program of China(grant numbers:2018YFB1700104); National Natural Science Foundation of China(grant numbers:61873342); Science Technology Development Fund, Macau Special Administrative Region (MSAR)(grant numbers:0064/2021/A2); Scientific and Technological Innovation Strategy of Guangdong Province(grant numbers:2022A0505030025); King Saud University, Riyadh, Saudi, Arabia(grant numbers:RSP2023R133); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10149832","Discrete event system;linear temporal logic;supervisory control theory;reinforcement learning","Computational modeling;Supervisory control;Reinforcement learning;Learning automata;Computational complexity;Automata;Markov processes","discrete event systems;finite automata;learning (artificial intelligence);optimal control;reinforcement learning;temporal logic","discrete event systems;discrete-event systems;finite automata;illegal training data;optimal control decision;plant model;reinforcement learning;RL algorithm;RL controller;SCT;supervisors;supervisory control theory;supervisory controller;temporal logic;uncontrollable events","","","","46","CCBYNCND","13 Jun 2023","","","IEEE","IEEE Journals"
"Model Free Safe Control for Reinforcement Learning in a Clustered Dynamic Environment","G. Zheng; M. Yang; Y. Wu","The Robot Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Faculty of Engineering, The University of Hongkong, Hongkong, China; Department of Mechanical, Materials and Manufacturing Engineering, University of Nottingham, Nottingham, UK","2022 5th International Conference on Mechatronics, Robotics and Automation (ICMRA)","12 Jun 2023","2022","","","19","26","While reinforcement learning (RL) has been wildly used in continuous control tasks with impressive performance, there are two main challenges in RL development: continuously satisfying the safety constraints in a clustered dynamic environment and lacking the explicit analytical models of dynamic systems for typical safeguard algorithms in RL training. This paper proposed a model-free safe control strategy to safeguard the RL agent in a clustered dynamic environment. By extending and applying the adaptive momentum boundary approximating (AdamBA) method to monitor and modify the RL nominal controls in a clustered dynamic environment, experimental results have shown the better safeguard performance of the proposed algorithm than other safe RL methods. The proposed algorithm could be easily extended to other RL algorithm with black-box implicit analytical model of dynamic systems.","","978-1-6654-7265-4","10.1109/ICMRA56206.2022.10145738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10145738","safe control;clustered environment;reinforcement learning;robotics","Training;Analytical models;Heuristic algorithms;Clustering algorithms;Reinforcement learning;Approximation algorithms;Safety","reinforcement learning","AdamBA method;adaptive momentum boundary approximating method;black-box implicit analytical model;clustered dynamic environment;continuous control tasks;dynamic systems;explicit analytical models;model-free safe control;reinforcement learning;RL agent;RL development;RL nominal controls;RL training;safe RL methods","","","","9","IEEE","12 Jun 2023","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning for Strategic Bidding in Power Markets","A. C. Tellidou; A. G. Bakirtzis","Department of Electrical & Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Electrical & Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece","2006 3rd International IEEE Conference Intelligent Systems","23 Apr 2007","2006","","","408","413","In the agent-based simulation discussed in this paper, we study the dynamics of the power market, when suppliers act following a Q-learning based bidding strategy. Power suppliers aim to satisfy two objectives: the maximization of their profit and their utilization rate. To meet with success their goals, they need to acquire a complex behavior by learning through a continuous exploiting and exploring process. Reinforcement learning theory provides a formal framework, along with a family of learning methods. In this paper we use Q-learning algorithm, perhaps the most popular among temporal difference methods. Q-learning offers suppliers the ability to evaluate their actions and to retain the most profitable of them. A five bus power system is used for our case studies; our experiments are contacted with three supplier-agents in all cases but the last one where sine agents participate. The locational marginal pricing (LMP) system serves as the market clearing mechanism","1941-1294","1-4244-0195-X","10.1109/IS.2006.348454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4155461","Electricity spot markets;multi-agent modeling;Q-learning algorithm;reinforcement learning;supplier bidding strategy","Learning;Power markets;Power system modeling;Testing;Proposals;Electricity supply industry;Computational modeling;Analytical models;Power supplies;Electricity supply industry deregulation","learning (artificial intelligence);multi-agent systems;optimisation;power markets;pricing;profitability","multiagent reinforcement learning;strategic bidding;power markets;agent-based simulation;power suppliers;Q-learning algorithm;temporal difference methods;sine agents;locational marginal pricing system;market clearing mechanism;electricity spot markets;multiagent modeling;supplier bidding strategy","","13","1","11","IEEE","23 Apr 2007","","","IEEE","IEEE Conferences"
"Parallel Implementation of Reinforcement Learning Q-Learning Technique for FPGA","L. M. D. Da Silva; M. F. Torquato; M. A. C. Fernandes","Department of Computer Science and Technology, Federal Institute of Rio Grande do Norte, Santa Cruz, Brazil; College of Engineering, Swansea University, Swansea, U.K.; Department of Computer Engineering and Automation, Federal University of Rio Grande do Norte, Natal, Brazil","IEEE Access","6 Jan 2019","2019","7","","2782","2798","Q-learning is an off-policy reinforcement learning technique, which has the main advantage of obtaining an optimal policy interacting with an unknown model environment. This paper proposes a parallel fixed-point Q-learning algorithm architecture implemented on field programmable gate arrays (FPGA) focusing on optimizing the system processing time. The convergence results are presented, and the processing time and occupied area were analyzed for different states and actions sizes scenarios and various fixed-point formats. The studies concerning the accuracy of the Q-learning technique response and resolution error associated with a decrease in the number of bits were also carried out for hardware implementation. The architecture implementation details were featured. The entire project was developed using the system generator platform (Xilinx), with a Virtex-6 xc6vcx240t-1ff1156 as the target FPGA.","2169-3536","","10.1109/ACCESS.2018.2885950","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior(grant numbers:Code 001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574886","FPGA;Q-learning;reinforcement learning;reconfigurable computing","Hardware;Field programmable gate arrays;Clocks;Computer architecture;Signal processing algorithms;Power demand","","","","45","","34","OAPA","13 Dec 2018","","","IEEE","IEEE Journals"
"Flying Through a Narrow Gap Using End-to-End Deep Reinforcement Learning Augmented With Curriculum Learning and Sim2Real","C. Xiao; P. Lu; Q. He","Interdisciplinary Division of Aeronautical and Aviation Engineering, The Hong Kong Polytechnic University, Hong Kong; Interdisciplinary Division of Aeronautical and Aviation Engineering, The Hong Kong Polytechnic University, Hong Kong; School of Automation, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","2 May 2023","2023","34","5","2701","2708","Traversing through a tilted narrow gap is previously an intractable task for reinforcement learning mainly due to two challenges. First, searching feasible trajectories is not trivial because the goal behind the gap is difficult to reach. Second, the error tolerance after Sim2Real is low due to the relatively high speed in comparison to the gap’s narrow dimensions. This problem is aggravated by the intractability of collecting real-world data due to the risk of collision damage. In this brief, we propose an end-to-end reinforcement learning framework that solves this task successfully by addressing both problems. To search for dynamically feasible flight trajectories, we use a curriculum learning to guide the agent toward the sparse reward behind the obstacle. To tackle the Sim2Real problem, we propose a Sim2Real framework that can transfer control commands to a real quadrotor without using real flight data. To the best of our knowledge, our brief is the first work that accomplishes successful gap traversing task purely using deep reinforcement learning.","2162-2388","","10.1109/TNNLS.2021.3107742","Platform Technology Fund of The University of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9530254","Quadrotor;reinforcement learning","Reinforcement learning;Trajectory;Task analysis;Optimal control;Optimization;Training;Planning","autonomous aerial vehicles;collision avoidance;control engineering computing;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;reinforcement learning;trajectory control","accomplishes successful gap;collision damage;curriculum learning;deep reinforcement learning;dynamically feasible flight trajectories;end-to-end deep reinforcement;end-to-end reinforcement;error tolerance;feasible trajectories;flight data;flying;intractability;intractable task;narrow dimensions;real-world data;relatively high speed;Sim2;Sim2Real;tilted narrow gap","","7","","28","IEEE","6 Sep 2021","","","IEEE","IEEE Journals"
"Learning-Based Optimal Large-Signal Stabilization for DC/DC Boost Converters feeding CPLs via Deep Reinforcement Learning","B. Huangfu; C. Cui; C. Zhang; L. Xu","Shanghai, China; Shanghai, China; Shanghai, China; Ministry of Education, School of Automation, Key Laboratory of Measurement and Control of CSE, Southeast University, Nanjing, China","IEEE Journal of Emerging and Selected Topics in Power Electronics","","2022","PP","99","1","1","For the DC/DC boost converter feeding constant power loads (CPLs), improving its dynamic characteristics and guaranteeing the output voltage stability have aroused much attention from the power electronics society in recent years. This paper proposes an optimal output regulation strategy by fusing a robust stabilization strategy with deep reinforcement learning (DRL). Firstly, by employing the high-order sliding mode observer (HOSMO) to estimate the uncertainties of the system, then we are able to realize a fast performance recovery ability via feedforward compensation loops. Secondly, the deep deterministic policy gradient (DDPG) is trained to adaptively adjust the control coefficients. Thirdly, the stability analysis is given to guarantee the large-signal stability of the control system. In order to evaluate the strength and effectiveness of the proposed adaptive method, experiments are conducted in this paper on a DC microgrid platform.","2168-6785","","10.1109/JESTPE.2022.3189078","National Natural Science Foundation of China(grant numbers:62173221 and 62003086); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9817114","Constant power load;higher-order sliding mode observer;DC/DC boost converter;deep deterministic policy gradient;reinforcement learning","Voltage control;Microgrids;Power electronics;Control systems;PI control;Stability criteria;Heuristic algorithms","","","","3","","","IEEE","7 Jul 2022","","","IEEE","IEEE Early Access Articles"
"Mnemonic Dictionary Learning for Intrinsic Motivation in Reinforcement Learning","R. Yan; Z. Wu; Y. Zhan; P. Tao; Z. Wang; Y. Cai; J. Xing","School of Software & Microelectronics, Peking University, Beijing, China; Independent Researcher; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of the Integrated Circuits, Peking University, Beijing, China; School of the Integrated Circuits, Peking University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","2023 International Joint Conference on Neural Networks (IJCNN)","2 Aug 2023","2023","","","1","7","Reinforcement learning for hard-exploration tasks remains challenging due to the long-term dependence and sparse-and-delay rewards in complex environments. In these challenging tasks, intrinsic motivation has become a dominant paradigm to enable the agent to explore the environment when no external reward feedback is available. In this work, inspired by studies from the human memory mechanism, we present a mnemonic dictionary learning (MDL) model for intrinsic motivation in reinforcement learning. The MDL model leverages sparse dictionary learning to incremental abstract the exploration histories into a compact memory-like dictionary, providing an excellent intrinsic motivation model. This mnemonic dictionary model not only drives the agent to explore novel stats in the environments indicated by the memory reconstruction error but also helps the agent to remember the key states and structure of the environments using its learned bases and reconstruction coefficients. The proposed MDL model can serve as a generative module for existing exploration methods. Extensive experimental results on typical sparse-reward tasks demonstrate its effectiveness and applicability over several competing algorithms. We will release the source code and trained models to facilitate further studies in this research direction.","2161-4407","978-1-6654-8867-9","10.1109/IJCNN54540.2023.10191424","National Key R&D Program of China(grant numbers:2022ZD0116401,2019YFB2205401); Natural Science Foundation of China(grant numbers:62076238,62222606,61834001,62025401); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191424","Reinforcement learning;exploration;intrinsic motivation;deep neural network;sparse dictionary learning","Dictionaries;Source coding;Buildings;Neural networks;Reinforcement learning;Games;Hybrid learning","reinforcement learning","compact memory-like dictionary;complex environments;excellent intrinsic motivation model;exploration histories;exploration methods;external reward feedback;hard-exploration tasks;human memory mechanism;learned bases;long-term dependence;MDL model;memory reconstruction error;mnemonic dictionary learning model;mnemonic dictionary model;reconstruction coefficients;reinforcement learning;sparse dictionary;sparse- delay rewards;sparse-and-delay rewards;sparse-reward tasks;trained models","","","","45","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Stochastic Computation Offloading in Digital Twin Networks","Y. Dai; K. Zhang; S. Maharjan; Y. Zhang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Informatics, University of Oslo, Oslo, Norway; Department of Informatics, University of Oslo, Oslo, Norway","IEEE Transactions on Industrial Informatics","5 Apr 2021","2021","17","7","4968","4977","The rapid development of industrial Internet of Things (IIoT) requires industrial production towards digitalization to improve network efficiency. Digital Twin is a promising technology to empower the digital transformation of IIoT by creating virtual models of physical objects. However, the provision of network efficiency in IIoT is very challenging due to resource-constrained devices, stochastic tasks, and resources heterogeneity. Distributed resources in IIoT networks can be efficiently exploited through computation offloading to reduce energy consumption while enhancing data processing efficiency. In this article, we first propose a new paradigm digital twin network to build network topology and the stochastic task arrival model in IIoT systems. Then, we formulate the stochastic computation offloading and resource allocation problem to minimize the long-term energy efficiency. As the formulated problem is a stochastic programming problem, we leverage Lyapunov optimization technique to transform the original problem into a deterministic per-time slot problem. Finally, we present asynchronous actor-critic algorithm to find the optimal stochastic computation offloading policy. Illustrative results demonstrate that our proposed scheme is able to significantly outperforms the benchmarks.","1941-0050","","10.1109/TII.2020.3016320","National Natural Science Foundation of China(grant numbers:61941102); Xi'an Key Laboratory of Mobile Edge Computing and Security(grant numbers:201805052ZD3CG36); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9166745","Computation offloading;deep reinforcement learning (DRL);digital twin;industrial Internet of Things (IIoT)","Task analysis;Stochastic processes;Computational modeling;Base stations;Servers;Wireless communication;Network topology","augmented reality;deep learning (artificial intelligence);energy conservation;Internet of Things;Lyapunov methods;minimisation;power aware computing;production engineering computing;real-time systems;resource allocation;stochastic programming","asynchronous actor-critic algorithm;Lyapunov optimization technique;industrial Internet of Things;deep reinforcement learning;optimal stochastic computation offloading policy;deterministic per-time slot problem;stochastic programming problem;long-term energy efficiency;resource allocation problem;IIoT systems;stochastic task arrival model;network topology;energy consumption;IIoT networks;distributed resources;resource heterogeneity;resource-constrained devices;physical objects;virtual models;digital transformation;network efficiency;digitalization;industrial production;digital twin networks","","101","","21","IEEE","13 Aug 2020","","","IEEE","IEEE Journals"
"WaveLearner: A Knowledge-Combined Reinforcement Learning to Understand Coordinated Traffic Signal Control along Urban Arteries","T. Han; S. Lyu; T. Oguchi","Graduate School of Engineering, University of Tokyo, Tokyo, Japan; Center for Spatial Information Science, University of Tokyo, Chiba, Japan; Institute of Industry Science, University of Tokyo, Tokyo, Japan","2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)","1 Nov 2022","2022","","","1167","1174","With the development of detection and computation techniques, the use of reinforcement learning (RL) in traffic signal control problems is widely discussed. After formulating diverse isolated RL agents to control one intersection design, most existing studies tend to directly duplicate isolated agents for large-scale coordinated control problems. However, two questionable challenges are 1) the coordinated control commonly differs from isolated control owing to different objectives; 2) the coordination necessity varies under different traffic demand. Thus, a naive duplication or aggregation of isolated RL agents seems unsatisfactory. In this paper, we focus on the classical arterial control problem to investigate an appropriate coordination strategy. Inspired by green-wave control, a knowledge-combined RL controller is proposed that can predict the potential opportunity of creating non-stop traffic through an artery by matching an ego intersection's phase selection and upstream historical states. Relying on realistic detection, the potential coordination cases can be recorded and rewarded, which can enhance the controller to catch opportunities to create a green wave in further learning. A simulation experiment was conducted to systematically compare the existing coordinated RL methods. According to the results, a promised performance of the proposed method was observed under various traffic conditions.","","978-1-6654-6880-0","10.1109/ITSC55140.2022.9922269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9922269","","Training;Knowledge engineering;Industries;Sensitivity;Roads;Reinforcement learning;Robustness","learning (artificial intelligence);road traffic;road traffic control;traffic control;traffic engineering computing","classical arterial control problem;appropriate coordination strategy;green-wave control;knowledge-combined RL controller;nonstop traffic;artery;ego intersection;potential coordination cases;RL methods;traffic conditions;knowledge-combined reinforcement learning;understand coordinated traffic signal control;urban arteries;computation techniques;traffic signal control problems;diverse isolated RL agents;intersection design;large-scale coordinated control;isolated control;coordination necessity;different traffic demand;naive duplication","","","","31","IEEE","1 Nov 2022","","","IEEE","IEEE Conferences"
"QueueLearner: A Knowledge-Combined Reinforcement Learning to Understand Queuing Evolution in Isolated Traffic Signal Control","T. Han; T. Oguchi; S. Lyu","Graduate School of Engineering, University of Tokyo, Tokyo, Japan; Institute of Industry Science, University of Tokyo, Tokyo, Japan; Center for Spatial Information Science, University of Tokyo, Chiba, Japan","2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)","1 Nov 2022","2022","","","1175","1182","Reinforcement learning (RL) methods have been used in traffic signal control for decades. Traditional RL controller with model-free design treat traffic states as a Markov Process (MP) to approximate future benefit of control strategy and improve its policy with discretized action space. In such treatment, the statistical connection between traffic state and RL produced action might mismatch with theoretical understanding. The mismatching can lead to a invalid control when traffic under control differing from statistical assumption. To enhance inference processes of RL controller with traffic engineering knowledge, a knowledge-combined RL controller, QueueLearner, is proposed for the isolated intersection control problem. The estimated queue is first introduced to teach queuing evolution with realistic detection to a Deep Q-Network (DQN) controller. Furthermore, a changeable phase duration is employed to address the DQN limitation of discrete action. The proposed QueueLearner outperforms traffic engineering and RL methods in a diverse simulation experiment with the same data conditions.","","978-1-6654-6880-0","10.1109/ITSC55140.2022.9922584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9922584","","Knowledge engineering;Industries;Roads;Process control;Reinforcement learning;Aerospace electronics;Traffic control","learning (artificial intelligence);Markov processes;queueing theory;road traffic;road traffic control;traffic engineering computing","discrete action;QueueLearner;knowledge-combined reinforcement;understand queuing evolution;isolated traffic signal control;reinforcement learning methods;traditional RL controller;model-free design treat traffic states;Markov Process;approximate future benefit;control strategy;discretized action space;statistical connection;traffic state;invalid control;statistical assumption;inference processes;traffic engineering knowledge;knowledge-combined RL controller;isolated intersection control problem;Deep Q-Network controller","","","","35","IEEE","1 Nov 2022","","","IEEE","IEEE Conferences"
"Multi-Energy Scheduling of an Industrial Integrated Energy System by Reinforcement Learning-Based Differential Evolution","Z. Xu; G. Han; L. Liu; M. Martínez-García; Z. Wang","Department of Information and Communication System, Hohai University (Changzhou Campus), Changzhou, China; Department of Information and Communication System, Hohai University (Changzhou Campus), Changzhou, China; Department of Information and Communication System, Hohai University (Changzhou Campus), Changzhou, China; Department of Aeronautical and Automotive Engineering, Loughborough University, Loughborough, U.K.; College of Computer and Information, Hohai University, Nanjing, China","IEEE Transactions on Green Communications and Networking","23 Aug 2021","2021","5","3","1077","1090","The Industrial Internet of Things (IIoT) is one of the main catalysts towards the realization of the Industry 4.0 paradigm, thus it is regarded as an essential element in future industrial systems - which can assist in reducing energy consumption and in enhancing product life-cycle management. In this study, an industrial multi-energy scheduling framework (IMSF) is proposed, with the aim of optimizing the usage of renewable energy and reducing the energy costs. The proposed method addresses the management of multi-energy flows in industrial integrated energy systems - incorporating multi-energy storage, renewable energy generation, energy conversion, and energy trading in a synchronous manner. The method considers the typical energy load of the industrial users, the energy price of the national grid and the trading platform, and the trade-off between investment costs and benefits from the various sub-systems. As this results in a complex system of systems, an artificial intelligence method is proposed to treat the problem, using reinforcement learning based differential evolution (RLDE), that can determine the optimal mutation strategy and associated parameters in an adaptive way. Case studies on real-world data evidence the effectiveness of the IMSF and the RLDE algorithm in reducing the energy costs in industrial environments.","2473-2400","","10.1109/TGCN.2021.3061789","National Key Research and Development Program of China(grant numbers:2017YFE0125300); Jiangsu Key Research and Development Program(grant numbers:BE2019648); Project of Shenzhen Science and Technology Innovation Committee(grant numbers:JCYJ20190809145407809); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361643","Optimal multi-energy flow;integrated energy system;reinforcement learning based differential evolution;artificial intelligence","Thermal energy;Energy storage;Job shop scheduling;Reinforcement learning;Optimal scheduling;Renewable energy sources;Industrial Internet of Things","cost reduction;energy conservation;energy consumption;energy storage;evolutionary computation;industrial power systems;Internet of Things;investment;learning (artificial intelligence);optimisation;power engineering computing;power generation economics;power grids;power markets;pricing;product life cycle management;production engineering computing;renewable energy sources;scheduling","reinforcement learning-based differential evolution;energy consumption;energy costs reduction;multienergy flows;industrial integrated energy systems;multienergy storage;renewable energy generation;energy conversion;energy trading;typical energy load;energy price;industrial environments;multienergy scheduling;IIoT;industrial internet of things;Industry 4.0 paradigm;industrial multienergy scheduling framework;IMSF;national grid;trading platform;investment costs;artificial intelligence method;optimal mutation strategy;RLDE algorithm","","36","","33","IEEE","24 Feb 2021","","","IEEE","IEEE Journals"
"A Spectrum Resource Sharing Algorithm for IoT Networks based on Reinforcement Learning","Z. Shi; X. Xie; M. Kadoch; M. Cheriet","Key Laboratory of Intelligent Perception and Computing of Anhui Province, Anqing Normal University, Anqing, China; The School of Computer Science and Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; Department of Electrical Engineering, École de Technologie Supérieure, University of Quebec, Montreal, Canada; Department of Automated Production Engineering, École de Technologie Supérieure, University of Quebec, Montreal, Canada","2020 International Wireless Communications and Mobile Computing (IWCMC)","27 Jul 2020","2020","","","1019","1024","Internet of Things (IoT) has attracted tremendous interest since it can improve production efficiency and system intelligence significantly. However, with the explosive growth of various types of device and data flow, IoT suffers spectrum resource scarcity for wireless applications. In this paper, we propose a solution for spectrum resource sharing in the IoT network, with the objective to facilitate the limited spectrum sharing between different kinds of sensors. To overcome the challenges of unknown dynamic IoT environment, the deep Q-learning network (DQN) is adopted. BS acts as the single agent and centrally manages all spectrum resources. First, a new reward function is designed to drive the learning process, which takes into account the different communication requirements of various sensors. In addition, to improve the learning efficiency of DQN, we compress the action space. Finally, simulation results show that compared with other algorithms, the proposed algorithm can achieve good network performance.","2376-6506","978-1-7281-3129-0","10.1109/IWCMC48107.2020.9148428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9148428","Internet of Things (IoT);spectrum resource sharing;deep reinforcement learning","Resource management;Intelligent sensors;Wireless communication;Wireless sensor networks;Sensor systems and applications;Machine learning","Internet of Things;learning (artificial intelligence);neural nets;radio spectrum management","spectrum resource sharing algorithm;IoT network;reinforcement learning;production efficiency;system intelligence;data flow;spectrum resource scarcity;wireless applications;unknown dynamic IoT environment;deep Q-learning network;learning process;communication requirements;learning efficiency;good network performance","","3","","30","IEEE","27 Jul 2020","","","IEEE","IEEE Conferences"
"Model-Based Reinforcement Learning for Advanced Adaptive Cruise Control: A Hybrid Car Following Policy","U. Yavas; T. Kumbasar; N. K. Ure","Eatron Technologies, Istanbul, Turkey; Control and Automation Engineering Department, Istanbul Technical University, Turkey; Department of Aeronautical Engineering, Artificial Intelligence and Data Science Research Center, Istanbul Technical University, Turkey","2022 IEEE Intelligent Vehicles Symposium (IV)","19 Jul 2022","2022","","","1466","1472","Adaptive cruise control (ACC) is one of the frontier functionality for highly automated vehicles and has been widely studied by both academia and industry. However, previous ACC approaches are reactive and rely on precise information about the current state of a single lead vehicle. With the advancement in the field of artificial intelligence, particularly in reinforcement learning, there is a big opportunity to enhance the current functionality. This paper presents an advanced ACC concept with unique environment representation and model-based reinforcement learning (MBRL) technique which enables predictive driving. By being predictive, we refer to the capability to handle multiple lead vehicles and have internal predictions about the traffic environment which avoids reactive short-term policies. Moreover, we propose a hybrid policy that combines classical car following policies with MBRL policy to avoid accidents by monitoring the internal model of the MBRL policy. Our extensive evaluation in a realistic simulation environment shows that the proposed approach is superior to the reference model-based and model-free algorithms. The MBRL agent requires only 150k samples (approximately 50 hours driving) to converge, which is x4 more sample efficient than model-free methods.","","978-1-6654-8821-1","10.1109/IV51971.2022.9827279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9827279","","Industries;Adaptation models;Intelligent vehicles;Reinforcement learning;Predictive models;Lead;Hybrid electric vehicles","adaptive control;automobiles;control engineering computing;mobile robots;reinforcement learning;road traffic;velocity control","hybrid car following policy;artificial intelligence;ACC approaches;reference model-based algorithm;single lead vehicle;highly automated vehicles;frontier functionality;advanced adaptive cruise control;model-free algorithms;MBRL policy;reactive short-term policies;internal predictions;multiple lead vehicles;predictive driving;model-based reinforcement learning;unique environment representation","","3","","27","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Radio Resource Allocation in NOMA-based Remote State Estimation","G. Pang; W. Liu; Y. Li; B. Vucetic","School of Electrical and Information Engineering, The University of Sydney, Australia; School of Electrical and Information Engineering, The University of Sydney, Australia; School of Electrical and Information Engineering, The University of Sydney, Australia; School of Electrical and Information Engineering, The University of Sydney, Australia","GLOBECOM 2022 - 2022 IEEE Global Communications Conference","11 Jan 2023","2022","","","3059","3064","Remote state estimation, where many sensors send their measurements of distributed dynamic plants to a remote estimator over shared wireless resources, is essential for mission-critical applications of Industry 4.0. Most of the existing works on remote state estimation assumed orthogonal multiple access and the proposed dynamic radio resource allocation algorithms can only work for very small-scale settings. In this work, we consider a remote estimation system with non-orthogonal multiple access. We formulate a novel dynamic resource allocation problem for achieving the minimum overall long-term average estimation mean-square error. Both the estimation quality state and the channel quality state are taken into account for decision making at each time. The problem has a large hybrid discrete and continuous action space for joint channel assignment and power allocation. We propose a novel action-space compression method and develop an advanced deep reinforcement learning algorithm to solve the problem. Numerical results show that our algorithm solves the resource allocation problem effectively, presents much better scalability than the literature, and provides significant performance gain compared to some benchmarks.","","978-1-6654-3540-6","10.1109/GLOBECOM48099.2022.10001377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10001377","Remote state estimation;radio resource allocation;NOMA;deep reinforcement learning;task-oriented communications","Deep learning;NOMA;Wireless sensor networks;Heuristic algorithms;Reinforcement learning;Performance gain;Benchmark testing","channel allocation;channel estimation;decision making;deep learning (artificial intelligence);mean square error methods;nonorthogonal multiple access;production engineering computing;reinforcement learning;resource allocation;state estimation;wireless channels","action-space compression method;advanced deep reinforcement learning algorithm;decision making;distributed dynamic plants measurements;dynamic radio resource allocation algorithms;dynamic resource allocation problem;Industry 4.0 mission-critical applications;joint channel assignment;long-term average estimation mean-square error;NOMA-based remote state estimation;nonorthogonal multiple access;power allocation;remote estimation system;shared wireless resources","","2","","19","IEEE","11 Jan 2023","","","IEEE","IEEE Conferences"
"Supplementary Reinforcement Learning Controller Designed for Quadrotor UAVs","X. Lin; Y. Yu; C. Sun","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation, Southeast University, Nanjing, China","IEEE Access","12 Mar 2019","2019","7","","26422","26431","The control problem for quadrotor UAVs is difficult and challenging due to the complex nonlinear dynamics and ever-changing disturbances. In this paper, a supplementary controller based on reinforcement learning (RL) is proposed to improve the control performance of quadrotor UAVs. The proposed RL method is constructed by an actor-critic structure and some improved technologies, e.g., Q-learning, temporal difference, and experience replay. With the proposed method, the speed and stability of training can be improved greatly. On one hand, the supplementary controller can work together with the traditional controller online, which can guarantee the stability of the system. On the other hand, the model uncertainties and external disturbances could be restrained through online RL training. The Lyapunov theory is used to prove the convergence of the RL controller's weights theoretically. Finally, three simulations are provided to illustrate the effectiveness of the proposed controller.","2169-3536","","10.1109/ACCESS.2019.2901295","Fundamental Research Funds for the Central Universities(grant numbers:FRF-GF-17-B46); National Natural Science Foundation of China(grant numbers:61520106009,61533008,61703037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8651281","Quadrotor;UAVs;reinforcement learning;ADP;control system","Training;Performance analysis;Reinforcement learning;Nonlinear dynamical systems;Uncertainty;Convergence;Mathematical model","autonomous aerial vehicles;control engineering computing;control system synthesis;helicopters;learning (artificial intelligence);Lyapunov methods;nonlinear control systems;stability;vehicle dynamics","complex nonlinear dynamics;actor-critic structure;quadrotor UAV;ever-changing disturbances;Lyapunov theory;supplementary reinforcement learning controller;supplementary RL controller;system stability;model uncertainties;external disturbances","","21","","26","OAPA","24 Feb 2019","","","IEEE","IEEE Journals"
"An application of reinforcement learning algorithms to industrial multi-robot stations for cooperative handling operation","D. Schwung; F. Csaplar; A. Schwung; S. X. Ding","South Westfalia University of Applied Sciences, Soest, Germany; South Westfalia University of Applied Sciences, Soest, Germany; South Westfalia University of Applied Sciences, Soest, Germany; University of Duisburg-Essen, Duisburg, Germany","2017 IEEE 15th International Conference on Industrial Informatics (INDIN)","13 Nov 2017","2017","","","194","199","This paper presents a novel approach to operate industrial robots as used for manufacturing lines within a cooperative robot station. The proposed framework consists of the application of especially to the cooperative robot handling problem adjusted Reinforcement Learning (RL) algorithms. Such RL-algorithms deal with sequential decision making processes in a trial-and-error learning interaction with the environment, to finally gain an optimal team-working behavior among the robots. In particular application results to a real team-working robot station underline the effectiveness of the novel RL approach.","2378-363X","978-1-5386-0837-1","10.1109/INDIN.2017.8104770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8104770","","Service robots;Collision avoidance;Robot kinematics;Learning (artificial intelligence);Manufacturing","decision making;industrial robots;learning (artificial intelligence);multi-robot systems","industrial multirobot stations;handling operation;industrial robots;manufacturing lines;robot handling problem;Reinforcement Learning algorithms;RL-algorithms deal;sequential decision;optimal team-working behavior;team-working robot station;trial-and-error learning interaction","","12","","22","IEEE","13 Nov 2017","","","IEEE","IEEE Conferences"
"Dynamically Changing Sequencing Rules with Reinforcement Learning in a Job Shop System With Stochastic Influences","J. Heger; T. Voss","Leuphana University, GERMANY; Leuphana University, GERMANY","2020 Winter Simulation Conference (WSC)","29 Mar 2021","2020","","","1608","1618","Sequencing operations can be difficult, especially under uncertain conditions. Applying decentral sequencing rules has been a viable option; however, no rule exists that can outperform all other rules under varying system performance. For this reason, reinforcement learning (RL) is used as a hyper heuristic to select a sequencing rule based on the system status. Based on multiple training scenarios considering stochastic influences, such as varying inter arrival time or customers changing the product mix, the advantages of RL are presented. For evaluation, the trained agents are exploited in a generic manufacturing system. The best agent trained is able to dynamically adjust sequencing rules based on system performance, thereby matching and outperforming the presumed best static sequencing rules by ≈ 3%. Using the trained policy in an unknown scenario, the RL heuristic is still able to change the sequencing rule according to the system status, thereby providing robust performance.","1558-4305","978-1-7281-9499-8","10.1109/WSC48552.2020.9383903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383903","","Training;Sequential analysis;NP-hard problem;System performance;Reinforcement learning;Manufacturing systems","job shop scheduling;learning (artificial intelligence);stochastic processes","dynamically changing sequencing rules;reinforcement learning;job shop system;stochastic influences;sequencing operations;uncertain conditions;decentral sequencing rules;system performance;sequencing rule;multiple training scenarios;trained agents;generic manufacturing system;presumed best static sequencing rules;RL heuristic","","4","","26","IEEE","29 Mar 2021","","","IEEE","IEEE Conferences"
"Optimal Energy Scheduling of Flexible Industrial Prosumers via Reinforcement Learning","N. van den Bovenkamp; J. S. Giraldo; E. M. Salazar Duque; P. P. Vergara; C. Konstantinou; P. Palensky","Intelligent Electrical Power Grids - EEMCS, Delft University of Technology, Netherlands; Energy Transition Studies Group, Netherlands Organisation for Applied Scientific Research (TNO), Netherlands; Electrical Energy Systems - EES, Eindhoven University of Technology, Netherlands; Intelligent Electrical Power Grids - EEMCS, Delft University of Technology, Netherlands; CEMSE Division, King Abdullah University of Science and Technology (KAUST), Saudi Arabia; Intelligent Electrical Power Grids - EEMCS, Delft University of Technology, Netherlands","2023 IEEE Belgrade PowerTech","9 Aug 2023","2023","","","1","6","This paper introduces an energy management system (EMS) aiming to minimize electricity operating costs using reinforcement learning (RL) with a linear function approximation. The proposed EMS uses a Q-learning with tile coding (QLTC) algorithm and is compared to a deterministic mixed-integer linear programming (MILP) with perfect forecast information. The comparison is performed using a case study on an industrial manufacturing company in the Netherlands, considering measured electricity consumption, PV generation, and wholesale electricity prices during one week of operation. The results show that the proposed EMS can adjust the prosumer's power consumption considering favorable prices. The electricity costs obtained using the QLTC algorithm are 99% close to those obtained with the MILP model. Furthermore, the results demonstrate that the QLTC model can generalize on previously learned control policies even in the case of missing data and can deploy actions 80% near to the MILP's optimal solution.","","978-1-6654-8778-8","10.1109/PowerTech55446.2023.10202699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10202699","Q-learning;tile coding;energy management system;mixed-integer linear programming","Costs;Smart buildings;Q-learning;Power demand;Scalability;Mixed integer linear programming;Manufacturing","energy management systems;function approximation;integer programming;linear programming;power consumption;power engineering computing;power markets;pricing;reinforcement learning","electricity operating costs;EMS;energy management system;favorable prices;flexible industrial prosumers;industrial manufacturing company;learned control policies;linear function approximation;measured electricity consumption;MILP model;MILP's optimal solution;mixed-integer linear programming;optimal energy scheduling;perfect forecast information;prosumer;PV generation;QLTC algorithm;QLTC model;reinforcement learning;tile coding algorithm;wholesale electricity prices","","","","16","IEEE","9 Aug 2023","","","IEEE","IEEE Conferences"
"Fast A3RL: Aesthetics-Aware Adversarial Reinforcement Learning for Image Cropping","D. Li; H. Wu; J. Zhang; K. Huang","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China","IEEE Transactions on Image Processing","14 Aug 2019","2019","28","10","5105","5120","Image cropping aims at improving the quality of images by removing unwanted outer areas, which is widely used in the photography and printing industry. Most of the previous cropping methods that do not need bounding box supervision rely on the sliding window mechanism. The sliding window method results in fixed aspect ratios and limits the shape of the cropping region. Moreover, the sliding window method usually produces lots of candidates on the input image, which is very time-consuming. Motivated by these challenges, we formulate image cropping as a sequential decision-making process and propose a reinforcement learning-based framework to address this problem, namely, Fast Aesthetics-Aware Adversarial Reinforcement Learning (Fast A3RL). Particularly, the proposed method develops an aesthetics-aware reward function that is dedicated for image cropping. Similar to human's decision-making process, we use a comprehensive state representation, including both the current observation and the historical experience. We train the agent using the actor-critic architecture in an end-to-end manner. The adversarial learning process is also applied during the training stage. The proposed method is evaluated on several popular cropping datasets, in which the images are unseen during training. The experiment results show that our method achieves the state-of-the-art performance with much fewer candidate windows and much less time compared with related methods.","1941-0042","","10.1109/TIP.2019.2914360","Ministry of Science and Technology of the People's Republic of China(grant numbers:2016YFB1001004,2016YFB1001005); National Natural Science Foundation of China(grant numbers:61876181,61673375,61721004); Chinese Academy of Sciences(grant numbers:QYZDB-SSW-JSC006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8708941","Reinforcement learning;adversarial learning;image cropping","Microsoft Windows;Reinforcement learning;Computational efficiency;Training;Image processing;Shape;Decision making","crops;decision making;image processing;learning (artificial intelligence);photography;printing industry","image cropping;reinforcement learning-based framework;fast aesthetics-aware adversarial reinforcement learning;Fast A3RL;image quality;printing industry;photography;sliding window method;sequential decision-making process","","25","","59","IEEE","7 May 2019","","","IEEE","IEEE Journals"
"Toward Using Reinforcement Learning for Trigger Selection in Network Slice Mobility","R. A. Addad; D. L. C. Dutra; T. Taleb; H. Flinck","Communication and Networking Department, Electrical Engineering School, Aalto University, Espoo, Finland; Systems Engineering and Computer Science Program (PESC), UFRJ, Federal University of Rio de Janeiro, Rio de Janeiro, Brazil; Communication and Networking Department, Electrical Engineering School, Aalto University, Espoo, Finland; Nokia Bell Labs, Espoo, Finland","IEEE Journal on Selected Areas in Communications","17 Jun 2021","2021","39","7","2241","2253","Recent 5G trials have demonstrated the usefulness of the Network Slicing concept that delivers customizable services to new and under-serviced industry sectors. However, user mobility's impact on the optimal resource allocation within and between slices deserves more attention. Slices and their dedicated resources should be offered where the services are to be consumed to minimize network latency and associated overheads and costs. Different mobility patterns lead to different resource re-allocation triggers, leading eventually to slice mobility when enough resources are to be migrated. The selection of the proper triggers for resource re-allocation and related slice mobility patterns is challenging due to triggers' multiplicity and overlapping nature. In this paper, we investigate the applicability of two Deep Reinforcement Learning based algorithms for allowing a fine-grained selection of mobility triggers that may instantiate slice and resource mobility actions. While the first proposed algorithm relies on a value-based learning method, the second one exploits a hybrid approach to optimize the action selection process. We present an enhanced ETSI Network Function Virtualization edge computing architecture that incorporates the studied mechanisms to implement service and slice migration. We evaluate the proposed methods' efficiency in a simulated environment and compare their performance in terms of training stability, learning time, and scalability. Finally, we identify and quantify the applicability aspects of the respective approaches.","1558-0008","","10.1109/JSAC.2021.3078501","European Union’s Horizon 2020 ICT Cloud Computing Program under the Adaptive Edge/Cloud Compute and Network to Support Nextgen Applications (ACCORDION) Project(grant numbers:871793); European Union’s Horizon 2020 Research and Innovation Program under the Cloud for Holography and Augmented Reality (CHARITY) Project(grant numbers:101016509); Academy of Finland Project 6Genesis(grant numbers:318927); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439923","5G;network slicing;multi-access edge computing;network softwarisation;deep reinforcement learning;agent-based resource orchestration","5G mobile communication;Heuristic algorithms;Cloud computing;Reinforcement learning;Network function virtualization;Computer architecture;Service level agreements","cloud computing;learning (artificial intelligence);mobile computing;resource allocation;service industries;virtualisation","dedicated resources;different mobility patterns;different resource re-allocation triggers;proper triggers;related slice mobility patterns;Deep Reinforcement;based algorithms;fine-grained selection;mobility triggers;resource mobility actions;action selection process;slice migration;Reinforcement Learning;trigger selection;Network slice mobility;recent 5G trials;Network Slicing concept;customizable services;under-serviced industry sectors;user mobility;optimal resource allocation;enhanced ETSI network function virtualization edge computing architecture","","10","","68","IEEE","24 May 2021","","","IEEE","IEEE Journals"
"A reinforcement learning approach to tackle illegal, unreported and unregulated fishing","T. Akinbulire; H. Schwartz; R. Falcon; R. Abielmona","Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada; Research & Engineering Division, Larus Technologies Corporation, Ottawa, Canada; Research & Engineering Division, Larus Technologies Corporation, Ottawa, Canada","2017 IEEE Symposium Series on Computational Intelligence (SSCI)","8 Feb 2018","2017","","","1","8","Illegal, unreported and unregulated fishing is a worldwide problem that is causing local and global financial losses, depleting natural resources, changing our diverse ecosystem and causing undue pressure upon the fishing industry. This paper presents a Reinforcement-Learning-based approach to response generation once this type of fishing event has been detected. The Fuzzy Actor Critic Learning technique is used to train one or more pursuers to effectively catch an evader. This technique is utilized on both the pursuer and evader vessel agents in order to simulate real-world illegal and unreported fishing pursuit events. Simulations are executed along two such scenarios, namely the Automatic Identification System Gaps and the Local Fishing, Foreign Delivery ones involving both illegal and unreported fishing vessels (as evaders) and law enforcement vessels (as pursuers). Experimental results are presented and analyzed whereby the pursuers catch the evading fishing vessels within a preset capture time. To our knowledge, this is the first time any Reinforcement Learning techniques have been applied as a response to such fishing events. The proposed methodology here is generic enough that it can be easily extrapolated to other domains.","","978-1-5386-2726-6","10.1109/SSCI.2017.8285315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8285315","","Learning (artificial intelligence);Fuzzy logic;Inference algorithms;Fuzzy sets;Risk analysis;Hidden Markov models","financial management;fishing industry;fuzzy set theory;learning (artificial intelligence)","local financial losses;fuzzy actor critic learning technique;evader vessel agents;reinforcement learning;ursuer vessel agents;fishing event detection;illegal fishing pursuit events;automatic identification system gaps;foreign delivery;d law enforcement vessels;illegal fishing vessels;Local Fishing;unreported fishing pursuit events;fishing industry;global financial losses;unregulated fishing;evading fishing vessels;unreported fishing vessels","","9","","18","IEEE","8 Feb 2018","","","IEEE","IEEE Conferences"
"Edge-Based Video Surveillance With Graph-Assisted Reinforcement Learning in Smart Construction","Z. Ming; J. Chen; L. Cui; S. Yang; Y. Pan; W. Xiao; L. Zhou","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Faculty of Computer Science and Control Engineering, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Research Institute, Tsinghua University, Shenzhen, China; China Resources Construction Corporation, Shenzhen, China","IEEE Internet of Things Journal","7 Jun 2022","2022","9","12","9249","9265","The smart construction site is developing rapidly with the intelligentization of industrial management. Intelligent devices are being widely deployed in construction industry to support artificial intelligence applications. Video surveillance is a core function of smart construction, which demands both high accuracy and low latency. The challenge is that the computation and networking resources in a construction site are often limited, and the inefficient scheduling policies create congestions in the network and bring additional delay that is unbearable to realtime surveillance. Adaptive video configuration and edge computing have been proposed to improve accuracy and reduce latency with limited resources. However, optimizing the video configuration and task scheduling in edge computing involves several factors that often interfere with each other, which significantly decreases the performance of video surveillance. In this article, we present an edge-based solution of video surveillance in the smart construction site assisted by a graph neural network. It leverages the distributed computing model to realize flexible allocation of resources. A graph-assisted hierarchical reinforcement learning algorithm is developed to illustrate the feature of the mobile-edge network and optimize the scheduling policy by the Deep- $Q$  Network. We implement and test the proposed solution in the commercial residential buildings of a fortune global 500 real estate company and observe that the proposed algorithm is efficient to maintain a reliable accuracy and keep lower delay. We further conduct a case study to demonstrate the superiority of the proposed solution by comparing it with traditional mechanisms.","2327-4662","","10.1109/JIOT.2021.3090513","National Key Research and Development Program of China(grant numbers:2018YFB1800302,2018YFB1800805); National Natural Science Foundation of China(grant numbers:62002237,6190071046,61772345); Department of Science and Technology of Guangdong Province(grant numbers:2021A1515012295); Shenzhen Science and Technology Program(grant numbers:RCYX20200714114645048,JCYJ20190808142207420,GJHZ20190822095416463); Pearl River Young Scholars Funding of Shenzhen University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459774","Adaptive configuration (AC);graph neural network;reinforcement learning;smart construction;task scheduling (TS)","Streaming media;Video surveillance;Task analysis;Servers;Delays;Reinforcement learning;Internet of Things","artificial intelligence;civil engineering computing;construction industry;graph theory;mobile computing;neural nets;optimisation;reinforcement learning;video surveillance","graph-assisted hierarchical reinforcement learning algorithm;mobile-edge network;edge-based video surveillance;smart construction site;industrial management;intelligent devices;construction industry;artificial intelligence applications;networking resources;adaptive video configuration;edge computing;task scheduling;graph neural network;distributed computing model;deep-Q network","","8","","49","IEEE","18 Jun 2021","","","IEEE","IEEE Journals"
"Slabstone Installation Skill Acquisition for Dual-Arm Robot based on Reinforcement Learning","D. Liu; J. Cao; X. Lei","Autocontrol Institute, Xi’an Jiaotong University, Xi’an, P.R. China; Autocontrol Institute, Xi’an Jiaotong University, Xi’an, P.R. China; Autocontrol Institute, Xi’an Jiaotong University, Xi’an, P.R. China","2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)","20 Jan 2020","2019","","","1298","1305","Slabstone installation is so widely used in the construction industry that it is very significant to use the robot to finish this process, making the process more automatic, effective and safer. Because of the uncertain factors in environments and filling material, it is difficult to finish the final step of slabstone installation that pressing the slabstone into good contact with filling material. There is currently no mature theory and method to achieve control of the process. In this paper, we use a dual-arm robot to accomplish the final step. First, we use the spring-damping elements to model and simulate the filling material. Then, based on deep reinforcement learning, a skill acquisition method is proposed to solve the problems of uncertain factors in the final step of slabstone installation process. Finally, we use a simulation-platform to train and evaluate our algorithm. The results have shown that our method is capable and effective in accomplishing the installation process.","","978-1-7281-6321-5","10.1109/ROBIO49542.2019.8961805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8961805","slabstone installation;reinforcement learning;dual-arm robot;cooperation motion control","","backpropagation;construction industry;damping;image sensors;industrial manipulators;learning (artificial intelligence);motion control;neural nets;springs (mechanical)","dual-arm robot;filling material;deep reinforcement learning;uncertain factors;slabstone installation process;slabstone installation skill acquisition;construction industry","","1","","26","IEEE","20 Jan 2020","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning in Dynamic Industrial Context","H. Zhang; J. Li; Z. Qi; A. Aronsson; J. Bosch; H. H. Olsson","Chalmers University of Technology, Gothenburg, Sweden; Ericsson Research, Ericsson; Ericsson Research, Ericsson; Ericsson Research, Ericsson; Chalmers University of Technology, Gothenburg, Sweden; Malmö University, Malmö, Sweden","2023 IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC)","2 Aug 2023","2023","","","448","457","Deep reinforcement learning has advanced signifi-cantly in recent years, and it is now used in embedded systems in addition to simulators and games. Reinforcement Learning (RL) algorithms are currently being used to enhance device operation so that they can learn on their own and offer clients better services. It has recently been studied in a variety of industrial applications. However, reinforcement learning, especially when controlling a large number of agents in an industrial environment, has been demonstrated to be unstable and unable to adapt to realistic situations when used in a real-world setting. To address this problem, the goal of this study is to enable multiple reinforcement learning agents to independently learn control policies on their own in dynamic industrial contexts. In order to solve the problem, we propose a dynamic multi-agent reinforcement learning (dynamic multi-RL) method along with adaptive exploration (AE) and vector-based action selection (VAS) techniques for accelerating model convergence and adapting to a complex industrial environment. The proposed algorithm is tested for validation in emergency situations within the telecommunications industry. In such circumstances, three unmanned aerial vehicles (UAV-BSs) are used to provide temporary coverage to mission-critical (MC) customers in disaster zones when the original serving base station (BS) is destroyed by natural disasters. The algorithm directs the participating agents automatically to enhance service quality. Our findings demonstrate that the proposed dynamic multi-RL algorithm can proficiently manage the learning of multiple agents and adjust to dynamic industrial environments. Additionally, it enhances learning speed and improves the quality of service.","0730-3157","979-8-3503-2697-0","10.1109/COMPSAC57700.2023.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10196928","Reinforcement Learning;Multi-Agent;Machine Learning;Software Engineering;Emergency Communication Network","Training;Adaptation models;Heuristic algorithms;Mission critical systems;Reinforcement learning;Quality of service;Network architecture","autonomous aerial vehicles;control engineering computing;deep learning (artificial intelligence);disasters;emergency services;mobile communication;mobile robots;multi-agent systems;multi-robot systems;quality of service;reinforcement learning;telecommunication computing;telecommunication control;telecommunication industry","adaptive exploration;AE;base station;complex industrial environment;control policies;deep reinforcement learning;disaster zones;dynamic industrial context;dynamic industrial environments;dynamic multiagent reinforcement learning method;dynamic multiRL method;industrial applications;mission-critical customers;model convergence;multiple reinforcement learning agents;multiRL algorithm;natural disasters;quality of service;telecommunications industry;UAV-BS;unmanned aerial vehicles;VAS technique;vector-based action selection technique","","","","28","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Accelerating Training of Reinforcement Learning-Based Construction Robots in Simulation Using Demonstrations Collected in Virtual Reality","L. Huang; Z. Zou","Department of Civil Engineering, University of British Columbia, Vancouver, BC, CANADA; Department of Civil Engineering, University of British Columbia, Vancouver, BC, CANADA","2022 Winter Simulation Conference (WSC)","23 Jan 2023","2022","","","2451","2462","The application of construction robots is crucial to mitigate challenges faced by the construction industry, such as labor shortages and low productivity. Reinforcement learning (RL) enables robots to take actions based on observed states, improving flexibility over traditional robots pre-programmed to follow determined sequences of instructions. However, RL-based control is time-consuming to train, hindering the wide adoption of RL-based construction robots. This paper proposes an approach that utilizes expert demonstrations collected from virtual reality to accelerate the RL training of construction robots. For evaluation, we implement the approach for the task of window pickup and installation on a virtual construction site. In our experiment, out of 10 RL agents trained using virtual expert demonstrations, 7 agents converge to an optimal policy faster than the baseline RL agent trained without demonstrations by around 40 epochs, which proves adding expert demonstrations can effectively accelerate the training of robots learning construction tasks.","1558-4305","978-1-6654-7661-4","10.1109/WSC57314.2022.10015308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015308","","Training;Solid modeling;Service robots;Accelerated aging;Virtual reality;Robot sensing systems;Robot learning","construction industry;learning (artificial intelligence);optimisation;virtual reality","10 RL agents;accelerating training;baseline RL;construction industry;construction tasks;demonstrations collected;reinforcement learning-based construction robots;RL training;RL-based construction robots;RL-based control;traditional robots;virtual construction site;virtual expert demonstrations;virtual reality","","","","44","IEEE","23 Jan 2023","","","IEEE","IEEE Conferences"
"Energy-Saving Train Operation Synergy Based on Multi-Agent Deep Reinforcement Learning on Spark Cloud","M. Shang; Y. Zhou; Y. Mei; J. Zhao; H. Fujita","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; Zhongguancun Smart City Company, Ltd, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Faculty of Information Technology, HUTECH University, Ho Chi Minh City, Vietnam","IEEE Transactions on Vehicular Technology","13 Jan 2023","2023","72","1","214","226","With the development of urban rail transit, energy conservation has been highly concerned by the worldwide railway industry. The existing energy-saving control methods are hard to meet the requirements of large-scale multi-train intelligent cooperation. Furthermore, in order to optimize the overall energy consumption and make effective utilization of regenerative braking energy (RBE), the micro train driving strategies must be taken into account in the macro train schedule regulation. This article aims to establish a systematic optimization model to describe the train traffic environment and design a deep reinforcement learning (DRL) approach using multi-agent cooperative actor-critic (MACAC) to reschedule multiple trains for energy saving. Due to the high dimensional characteristic of control actions, the action representation technique is applied for generalization. The multiple agents are deployed on the Spark cloud platform to effectively improve the learning efficiency through multi-node parallel computing. The proposed MACAC approach on Spark cloud is capable of dealing with the intelligent learning and computing efficiency of large-scale train operation synergy, and saving energy to the maximum extent on the premise of satisfying various time constraints. The evaluation was carried out through the field data of Beijing Yizhuang subway line. The results show that the proposed MACAC approach can effectively learn to improve the energy efficiency, and the parallel computing method on Spark cloud is suitable for practical applications.","1939-9359","","10.1109/TVT.2022.3205379","Beijing Natural Science Foundation(grant numbers:L191017); National Natural Science Foundation of China(grant numbers:61673049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9882357","Energy saving;multi-agent deep reinforcement learning;regenerative braking;schedule regulation","Schedules;Regulation;Energy consumption;Reinforcement learning;Sparks;Vehicle dynamics;Resource management","deep learning (artificial intelligence);energy conservation;energy consumption;multi-agent systems;optimisation;rail traffic;railway industry;railways;regenerative braking;reinforcement learning","action representation technique;Beijing Yizhuang subway line;computing efficiency;DRL approach;energy conservation;energy consumption;energy efficiency;energy saving;energy-saving train operation synergy;existing energy-saving control methods;intelligent learning;large-scale multitrain intelligent cooperation;large-scale train operation synergy;MACAC approach;macro train schedule regulation;microtrain driving strategies;multiagent cooperative actor-critic;multiagent deep reinforcement learning;multinode parallel computing;multiple agents;parallel computing method;RBE;regenerative braking energy;Spark cloud platform;systematic optimization model;train traffic environment;urban rail transit;worldwide railway industry","","","","31","IEEE","9 Sep 2022","","","IEEE","IEEE Journals"
"Event-Triggered Constrained Optimal Control for Organic Rankine Cycle Systems via Safe Reinforcement Learning","L. Zhang; R. Lin; L. Xie; W. Dai; H. Su","State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; Ministry of Education, Engineering Research Center of Intelligent Control for Underground Space, China University of Mining and Technology, Xuzhou, China; State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","12","The organic Rankine cycle (ORC) is an effective application for converting low-grade heat sources into power and is crucial for environmentally friendly production and energy recovery. However, the inherent complexity of the mechanism, its strong and unidentified nonlinearity, and the presence of control constraints severely impair the design of its optimal controller. To solve these issues, this study provides a novel event-triggered (ET) constrained optimal control approach for the ORC systems based on a safe reinforcement learning technique to find the optimal control law. Instead of employing the usual non-quadratic integral form to solve the control-limited optimal control problems, a constraint handling strategy based on a relaxed weighted barrier function (BF) technique is proposed. By adding the BF terms to the original value function, a modified value iteration algorithm is developed to make the control input solutions that tend to violate the constraints be pushed back and maintained in their safe sets. In addition, the ET mechanism proposed in this article is critically required for the ORC systems, and it can significantly reduce the computational load. The combination of these two techniques allows the ORC systems to achieve set-point tracking control and satisfy the control restrictions. The proposed approach is conducted based on a heuristic dynamic programming framework with three neural networks (NNs) involved. The safety and convergence of the proposed approach and the stability of the closed-loop system are analyzed. Simulation results and comparisons are presented to demonstrate its effectiveness.","2162-2388","","10.1109/TNNLS.2022.3213825","National Natural Science Foundation of China(grant numbers:62073286); Science Fund for Creative Research Groups of the National Natural Science Foundation of China(grant numbers:61621002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9994656","Barrier function (BF);control constraints;event-triggered (ET) control;organic Rankine cycle (ORC);safe reinforcement learning","Waste heat;Optimal control;Heat recovery;Optimization;Mathematical models;Artificial neural networks;Transient analysis","","","","","","","IEEE","20 Dec 2022","","","IEEE","IEEE Early Access Articles"
"Risk-Aware Identification of Highly Suspected COVID-19 Cases in Social IoT: A Joint Graph Theory and Reinforcement Learning Approach","B. Wang; Y. Sun; T. Q. Duong; L. D. Nguyen; L. Hanzo","Xuzhou Engineering Research Center of Intelligent Industry Safety and Emergency Collaboration, Xuzhou, China; Xuzhou Engineering Research Center of Intelligent Industry Safety and Emergency Collaboration, Xuzhou, China; School of Electronics, Electrical Engineering, and Computer Science, Queen’s University Belfast, Belfast, U.K.; Duy Tan University, Da Nang, Vietnam; School of Electronics and Computer Science, University of Southampton, Southampton, U.K.","IEEE Access","30 Jun 2020","2020","8","","115655","115661","The recent outbreak of the coronavirus disease 2019 (COVID-19) has rapidly become a pandemic, which calls for prompt action in identifying suspected cases at an early stage through risk prediction. To suppress its further spread, we exploit the social relationships between mobile devices in the Social Internet of Things (SIoT) to help control its propagation by allocating the limited protective resources to the influential so-called high-degree individuals to stem the tide of precipitated spreading. By exploiting the so-called differential contact intensity and the infectious rate in susceptible-exposed-infected-removed (SEIR) epidemic model, the resultant optimization problem can be transformed into the minimum weight vertex cover (MWVC) problem of graph theory. To solve this problem in a high-dynamic random network topology, we propose an adaptive scheme by relying on the graph embedding technique during the state representation and reinforcement learning in the training phase. By relying on a pair of real-life datasets, the results demonstrate that our scheme can beneficially reduce the epidemiological reproduction rate of the infection. This technique has the potential of assisting in the early identification of COVID-19 cases.","2169-3536","","10.1109/ACCESS.2020.3003750","Royal Academy of Engineering (RAEng) through the RAEng Research Fellowships Schemer(grant numbers:RF14151422); Researcher Links through the Newton Fund Partnership(grant numbers:527612186); U.K. Department for Business, Energy and Industrial Strategy and delivered by the British Council. L. Hanzo would like to acknowledge the financial support of the Engineering and Physical Sciences Research Council projects EP/N004558/1, EP/P034284/1, EP/P034284/1, EP/P003990/1 (COALESCE), of the Royal Society’s Global Challenges Research Fund Grant as well as of the European Research Council’s Advanced Fellow Grant QuantCom; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9121230","Social Internet of Thing (SIoT);COVID-19;reinforcement learning;graph theory","COVID-19;Network topology;Reinforcement learning;Image edge detection;Social factors;Human factors","diseases;epidemics;graph theory;Internet of Things;learning (artificial intelligence);medical computing;optimisation","differential contact intensity;infectious rate;susceptible-exposed-infected-removed epidemic model;optimization;minimum weight vertex cover;high-dynamic random network topology;state representation;risk-aware identification;COVID-19;social IoT;graph theory;reinforcement learning;coronavirus disease 2019;risk prediction;social relationships;mobile devices;MWVC;SEIR epidemic model","","39","","18","CCBY","19 Jun 2020","","","IEEE","IEEE Journals"
"Dynamic Resource Configuration for Low-Power IoT Networks: A Multi-Objective Reinforcement Learning Method","Y. Huang; C. Hao; Y. Mao; F. Zhou","Key Laboratory of Dynamic Cognitive System of Electromagnetic Spectrum Space, Ministry of Industry and Information Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Shenzhen Station of State Radio Monitoring Center, Shenzhen, China; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Key Laboratory of Dynamic Cognitive System of Electromagnetic Spectrum Space, Ministry of Industry and Information Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Communications Letters","9 Jul 2021","2021","25","7","2285","2289","Considering grant-free transmissions in low-power IoT networks with unknown time-frequency distribution of interference, we address the problem of Dynamic Resource Configuration (DRC), which amounts to a Markov decision process. Unfortunately, off-the-shelf methods based on single-objective reinforcement learning cannot guarantee energy-efficient transmission, especially when all frequency-domain channels in a time interval are interfered. Therefore, we propose a novel DRC scheme where configuration policies are optimized with a Multi-Objective Reinforcement Learning (MORL) framework. Numerical results show that the average decision error rate achieved by the MORL-based DRC can be even less than 12% of that yielded by the conventional R-learning-based approach.","1558-2558","","10.1109/LCOMM.2021.3074756","National Natural Science Foundation (NSF) of China(grant numbers:61901216,U2001210,62071223,62031012); NSF of Jiangsu Province(grant numbers:BK20190400); open research fund of National Mobile Communications Research Laboratory, Southeast University(grant numbers:2020D08); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9410272","The IoT networks;multi-objective reinforcement learning;grant-free;spectrum sharing","Fading channels;Time-frequency analysis;Error analysis;Reinforcement learning;Interference;Markov processes;Dynamic scheduling","Internet of Things;learning (artificial intelligence);Markov processes;telecommunication computing;time-frequency analysis","dynamic resource configuration;low-power IoT networks;multiobjective reinforcement learning;grant-free transmissions;Markov decision process;off-the-shelf methods;single-objective reinforcement learning;energy-efficient transmission;frequency-domain channels;time interval;configuration policies;MORL-based DRC;conventional R-learning-based approach;DRC scheme;unknown time-frequency distribution;average decision error rate","","7","","15","IEEE","21 Apr 2021","","","IEEE","IEEE Journals"
"Reinforcement learning-based robust adaptive tracking control for multi-wheeled mobile robots synchronization with optimality","N. T. Luy; Nguyen Thien Thanh; Hoang Minh Tri","Ho Chi Minh City University of Industry, Vietnam; Ho Chi Minh City University of Technology, Vietnam; Ho Chi Minh City University of Industry, Vietnam","2013 IEEE Workshop on Robotic Intelligence in Informationally Structured Space (RiiSS)","26 Sep 2013","2013","","","74","81","This paper proposes a new method based on reinforcement learning to design robust adaptive tracking control laws with optimality for multi-wheeled mobile robots synchronization in communication graph without requiring knowledge of drift tracking terms in node dynamics. Wheeled mobile robots are controlled by integrated kinematic and dynamic laws. Actor critic structures in the control scheme for every node is proposed such as only single NN is used to reduce computational cost and storage resources, but parameters of critic and actors are updated synchronously. Novel tuning laws for the NNs are designed not only to learn online adaptive solutions of cooperative Hamilton-Jacobi-Isaacs (HJI) equation on purpose of approximating optimal cooperative tracking performance index functions and robust direct adaptive tracking control inputs as well as worst case disturbances but also to guarantee closed-loop stability in real-time. The convergence and stability of the overall system are proven by Lyapunov techniques. The simulation results on multi-wheeled mobile robots systems verify the effectiveness of the proposed controller.","","978-1-4673-5877-4","10.1109/RiiSS.2013.6607932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607932","Wheeled mobile robot system;Cooperative Hamilton-Jacobi-Isaacs equations;Robust direct adaptive control","Synchronization;Equations;Robustness;Vectors;Mobile robots;Heuristic algorithms","adaptive control;closed loop systems;computational complexity;control engineering computing;control system synthesis;cooperative systems;graph theory;learning (artificial intelligence);Lyapunov methods;mobile robots;multi-robot systems;robot dynamics;robot kinematics;robust control;stability;synchronisation","reinforcement learning;robust adaptive tracking control law design;multiwheeled mobile robots synchronization;communication graph;integrated kinematic-dynamic laws;actor critic structures;computational cost reduction;storage resource reduction;tuning laws;cooperative HamiltonJacobi-Isaacs equation;HJI equation;optimal cooperative tracking performance index functions;robust direct adaptive tracking control inputs;worst case disturbances;real-time closed-loop stability;convergence;Lyapunov techniques;NN;cooperative wheeled mobile robots","","6","","20","IEEE","26 Sep 2013","","","IEEE","IEEE Conferences"
"Trajectory Jerking Suppression for Mixed Traffic Flow at a Signalized Intersection: A Trajectory Prediction Based Deep Reinforcement Learning Method","S. Wang; Z. Wang; R. Jiang; R. Yan; L. Du","School of Traffic and Transportation, Beijing Jiaotong University, Beijing, China; Key Laboratory of Transport Industry of Big Data Application Technologies for Comprehensive Transport, Ministry of Transport, Beijing Jiaotong University, Beijing, China; Key Laboratory of Transport Industry of Big Data Application Technologies for Comprehensive Transport, Ministry of Transport, Beijing Jiaotong University, Beijing, China; School of Traffic and Transportation, Beijing Jiaotong University, Beijing, China; DiDi Global Inc., Beijing, China","IEEE Transactions on Intelligent Transportation Systems","12 Oct 2022","2022","23","10","18989","19000","Vehicles stopping at signalized intersections during a red light is one of the main causes of traffic oscillations. Recently, deep reinforcement learning (DRL) methods have been applied to connected and automated vehicles (CAVs) traffic to reduce the traffic oscillation at signalized intersections. However, these methods do not perform well for mixed traffic flow, including human vehicles (HVs) and CAVs, especially when the CAV rate is low. We found that this was because they did not take into account the HVs stopping at a red light and causing oscillations. If this oscillation is ignored during the speed regulation, CAVs may conflict with the oscillation wave in the future, forcing a sudden and significant speed reduction and triggering the so-called “trajectory jerking” phenomenon. In order to address this problem, this study proposes a trajectory prediction-based DRL method. By introducing the prediction of the downstream vehicle trajectory into the design of the reward function, the leading CAV will adjust its speed in advance to avoid the future oscillation wave caused by HV’s stopping during the red phase. Simulation tests on various penetration rates of CAVs are conducted for the mixed traffic environment to evaluate the performance of the proposed method. The results show that the proposed method has two advantages. Even with low CAV penetration, the oscillations are suppressed remarkably well. And fuel consumption is also significantly reduced. This research provides a new idea to suppress traffic oscillations in a mixed traffic environment.","1558-0016","","10.1109/TITS.2022.3152550","National Natural Science Foundation of China(grant numbers:71931002,71631002); Fundamental Research Funds for the Central Universities(grant numbers:2021YJS088); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9723012","Trajectory jerking;oscillations suppression;mixed traffic flow;connected and automated vehicles;reinforcement learning;twin delayed deep deterministic policy gradient","Trajectory;Oscillators;Training;Reinforcement learning;Predictive models;Data models;Automobiles","learning (artificial intelligence);road traffic;road traffic control;traffic information systems;transportation","automated vehicles;causing oscillations;CAV rate;connected vehicles;deep reinforcement learning methods;downstream vehicle trajectory;future oscillation wave;human vehicles;HV's stopping;HVs stopping;leading CAV;low CAV penetration;mixed traffic environment;mixed traffic flow;red light;reinforcement learning method;signalized intersection;significant speed reduction;sudden speed reduction;traffic oscillation;trajectory jerking suppression;trajectory prediction-based DRL method","","4","","36","IEEE","28 Feb 2022","","","IEEE","IEEE Journals"
"Adaptive Cooperative Distributed Compressed Sensing for Edge Devices: A Multiagent Deep Reinforcement Learning Approach","M. Sekine; S. Ikada","Innovation Promotion Center, Oki Electric Industry Co., Ltd., Saitama, Japan; Innovation Promotion Center, Oki Electric Industry Co., Ltd., Saitama, Japan","2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)","24 May 2021","2021","","","585","591","We propose a lightweight and adaptive distributed compressed sensing (DCS) with multi-sensor collaboration based on multiagent deep reinforcement learning (LADICS-MARL). To efficiently acquire data generated by sensor nodes deployed over a wide area and for long periods, we previously proposed a lightweight and adaptive compressed sensing method based on deep learning for edge devices, called LACSLE that changes the compression ratio in real-time according to the data between one sender and one receiver using pre-trained deep learning model. LADICS-MARL is an extension for multiple senders and one receiver and supports DCS through which multiple compressed data are simultaneously reconstructed. Multiple sensor nodes cooperate based on multiagent reinforcement learning to estimate the optimal compression ratio for all senders according to each corresponding data, as well as the transmitted compressed data from other sensor nodes. In addition, a gateway optimizes the combination of groups where some compressed data are reconstructed simultaneously. A performance evaluation using acceleration data from multiple sensor terminals acquired on a bridge suggests that the multiagent-based LADICS-MARL can reconstruct original data from less compressed data compared to the single-agent-based LACSLE under the threshold of reconstruct error.","","978-1-6654-0424-2","10.1109/PerComWorkshops51409.2021.9431085","Ministry of Internal Affairs and Communications(grant numbers:JPJ000595); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9431085","distributed compressed sensing;multiagent;deep reinforcement learning;sensor data;edge device","Performance evaluation;Deep learning;Three-dimensional displays;Conferences;Distributed databases;Reinforcement learning;Receivers","adaptive signal processing;compressed sensing;learning (artificial intelligence);multi-agent systems;sensor fusion;signal reconstruction","adaptive cooperative distributed compressed sensing;edge devices;multiagent deep reinforcement learning approach;lightweight distributed compressed sensing method;DCS;multisensor collaboration;pre-trained deep learning model;multiple compressed data;multiple sensor nodes;multiagent reinforcement;optimal compression ratio;transmitted compressed data;acceleration data;multiple sensor terminals;multiagent-based LADICS-MARL;single-agent-based LACSLE;performance evaluation;compressed data reconstruction;reconstruct error threshold","","3","","12","IEEE","24 May 2021","","","IEEE","IEEE Conferences"
"Combining Deep Reinforcement Learning with Search Heuristics for Solving Multi-Agent Path Finding in Segment-based Layouts","R. Reijnen; Y. Zhang; W. Nuijten; C. Senaras; M. Goldak - Altgassen","Jheronimus Academy of Data Science, ’s-Hertogenbosch, The Netherlands; Dept. of Industrial Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands; Dept. of Math. and Computer Science, Eindhoven University of Technology, Eindhoven, The Netherlands; Vanderlande Industries B.V., Veghel, The Netherlands; Vanderlande Industries B.V., Veghel, The Netherlands","2020 IEEE Symposium Series on Computational Intelligence (SSCI)","5 Jan 2021","2020","","","2647","2654","A multi-agent path finding (MAPF) problem is concerned with finding paths for multiple agents such that certain objectives, such as minimizing makespan, are reached in a conflict-free manner. In this paper, we solve a practical MAPF problem with automated guided vehicles (AGVs) for the conveying of luggage in segment-based layouts (MAPF-SL). Most existing algorithms for MAPF are mainly focused on grid environments. However, the conflict prevention problem is more challenging with segment-based layouts in which software is constrained to oversee that vehicles remain on predefined travel paths. Hence, the existing multi-agent path finding algorithms cannot be applied directly to solve MAPF-SL. In this paper, we propose an algorithm, called WHCA*S-RL, that combines Deep Reinforcement Learning (DRL) with a heuristic approach for solving MAPF-SL. DRL is used for determining travel directions while the heuristic approach oversees the planning in a segment-based layout. Our experiment results show that the proposed WHCA*S-RL approach can be successfully used for making path plans in which traffic congestion is both avoided and relieved. In this way, individual vehicles are found to reach goal destinations faster than the approach with search only.","","978-1-7281-2547-3","10.1109/SSCI47803.2020.9308584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9308584","multi-agent path finding;path planning;deep reinforcement learning;automated guided vehicles","Layout;Planning;Reinforcement learning;Heuristic algorithms;Approximation algorithms;System recovery;Software algorithms","automatic guided vehicles;mobile robots;multi-robot systems;multivariable systems;path planning;search problems","WHCA*S-RL;AGV;automated guided vehicles;search heuristics;multiagent path finding algorithms;path plans;heuristic approach;deep reinforcement learning;predefined travel paths;conflict prevention problem;MAPF-SL;segment-based layout;conflict-free manner;multiple agents","","1","","19","IEEE","5 Jan 2021","","","IEEE","IEEE Conferences"
"Interference-Aware Trajectory Design for Fair Data Collection in UAV-Assisted IoT Networks by Deep Reinforcement Learning","G. Chen; X. B. Zhai; C. Li",Nanjing University of Aeronautics and Astronautics; Nanjing University of Aeronautics and Astronautics; Sun Yat-sen University,"2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)","30 May 2022","2021","","","345","352","Unmanned Aerial Vehicle (UAV)-assisted data collection for Internet-of-Things (IoT) systems has drawn increasing attention, which is pivotal for providing seamless coverage and promoting system performance in wireless networks. In this paper, we study a UAV-aided data collection scenario where a UAV takes off from an initial location and flies to multiple abutting ground sensor nodes (SNs) arbitrarily scattered in the physical environment to collect sensor data. Specifically, the UAV synchronously communicates with SNs when passing through the collective communication area. Considering the signal-to-interference-plus-noise ratio (SINR) and fair performance among SNs, we maximize the fair throughput among all SNs and minimize energy consumption by optimizing the trajectory of the UAV and power allocation. The multi-optimization problem is modeled as a Markov Decision Process due to the system dynamic. To handle the continuous state and action space in this problem, we propose a deep reinforcement learning-based algorithm, named as multi-optimization trajectory design and power allocation (MOTDPA), which chooses the state-of-the-art methods, soft actor-critic with prioritized experience replay to find the efficient policy. Meanwhile, we use min-max state normalization (MMSN) to stabilize the training process. Simulation results demonstrate the better performance of the proposed approach than other commonly used baselines.","","978-1-6654-9457-1","10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00070","National Natural Science Foundation of China(grant numbers:61802181,61701231); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780962","UAV trajectory;data collection;SINR;fairness;deep reinforcement learning","Energy consumption;Heuristic algorithms;Wireless networks;Reinforcement learning;Interference;Data collection;Autonomous aerial vehicles","aircraft communication;autonomous aerial vehicles;decision theory;deep learning (artificial intelligence);Internet of Things;Markov processes;minimax techniques;radiofrequency interference;wireless sensor networks","prioritized experience replay;physical environment;trajectory optimization;training process;MMSN;minmax state normalization;soft actor-critic;MOTDPA;multioptimization trajectory design and power allocation;multioptimization trajectory design;Markov decision process;SINR;signal-to-interference-plus-noise ratio;system performance;multiple abutting ground sensor nodes;UAV-aided data collection scenario;wireless networks;seamless coverage;Internet-of-Things systems;unmanned aerial vehicle-assisted data collection;UAV-assisted IoT networks;fair data collection;interference-aware trajectory design;power allocation;deep reinforcement learning-based algorithm;multioptimization problem;fair throughput;fair performance;collective communication area;sensor data collection;SNs","","1","","23","IEEE","30 May 2022","","","IEEE","IEEE Conferences"
"Digital Twin of a Driver-in-the-Loop Race Car Simulation With Contextual Reinforcement Learning","S. Ju; P. van Vliet; O. Arenz; J. Peters","Dr. Ing. h.c. F. Porsche AG, Weissach, Germany; Dr. Ing. h.c. F. Porsche AG, Weissach, Germany; Intelligent Autonomous System, Computer Science Department, Technische Universität Darmstadt, Darmstadt, Germany; Intelligent Autonomous System, Computer Science Department, Technische Universität Darmstadt, Darmstadt, Germany","IEEE Robotics and Automation Letters","5 Jun 2023","2023","8","7","4107","4114","In order to facilitate rapid prototyping and testing in the advanced motorsport industry, we consider the problem of imitating and outperforming professional race car drivers based on demonstrations collected on a high-fidelity Driver-in-the-Loop (DiL) hardware simulator. We formulate a contextual reinforcement learning problem to learn a human-like and stochastic policy with domain-informed choices for states, actions, and reward functions. To leverage very limited training data and build human-like diverse behavior, we fit a probabilistic model to the expert demonstrations called the reference distribution, draw samples out of it, and use them as context for the reinforcement learning agent with context-specific states and rewards. In contrast to the non-human-like stochasticity introduced by Gaussian noise, our method contributes to a more effective exploration, better performance and a policy with human-like variance in evaluation metrics. Compared to previous work using a behavioral cloning agent, which is unable to complete competitive laps robustly, our agent outperforms the professional driver used to collect the demonstrations by around 0.4 seconds per lap on average, which is the first time known to the authors that an autonomous agent has outperformed a top-class professional race driver in a state-of-the-art, high-fidelity simulation. Being robust and sensitive to vehicle setup changes, our agent is able to predict plausible lap time and other performance metrics. Furthermore, unlike traditional lap time calculation methods, our agent indicates not only the gain in performance but also the driveability when faced with modified car balance, facilitating the digital twin of the DiL simulation.","2377-3766","","10.1109/LRA.2023.3279618","Dr. Ing. h.c. F. Porsche AG; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10132573","Reinforcement learning;imitation leaning;autonomous agent;autonomous racing","Intelligent vehicles;Behavioral sciences;Reinforcement learning;Predictive models;Digital twins;Vehicle dynamics;Automotive engineering","automobiles;automotive engineering;digital twins;learning (artificial intelligence);reinforcement learning","advanced motorsport industry;autonomous agent;behavioral cloning agent;competitive laps;context-specific states;contextual reinforcement learning problem;digital twin;DiL simulation;diverse behavior;domain-informed choices;Driver-in-the-Loop race car simulation;expert demonstrations;high-fidelity Driver-in-the-Loop hardware simulator;high-fidelity simulation;human-like policy;leverage very limited training data;modified car balance;performance metrics;plausible lap time;probabilistic model;professional driver;professional race car drivers;rapid prototyping;reinforcement learning agent;reward functions;stochastic policy;stochasticity;thereference distribution;time 0.4 s;top-class professional race driver;traditional lap time calculation methods","","","","27","IEEE","24 May 2023","","","IEEE","IEEE Journals"
"Multi-step reinforcement learning-based offloading for vehicle edge computing","S. Han; Y. Chen; G. Chen; J. Yin; H. Wang; J. Cao","School of Cyber Security, Guangdong Polytechnic Normal University, Guangzhou, China; School of Cyber Security, Guangdong Polytechnic Normal University, Guangzhou, China; School of Cyber Security, Guangdong Polytechnic Normal University, Guangzhou, China; Institute for Sustainable Industries and Liveable Cities, Victoria University, Melbourne, Australia; Institute for Sustainable Industries and Liveable Cities, Victoria University, Melbourne, Australia; Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia","2023 15th International Conference on Advanced Computational Intelligence (ICACI)","12 Jun 2023","2023","","","1","8","The Internet of Vehicles (IoV) system has recently attracted more attention. However, IoV applications require massive computations within strict time limits. Computation energy consumption is also a significant concern in IoV applications. Thus, this paper establishes a vehicle edge computing architecture by combining edge computing and IoV to improve the computation ability of IoV. To optimize the offloading computation process, we model the entire process as a Markov decision process (MDP). Computation delay, computation energy consumption and communication quality are considered in a utility function to establish a multi-objective optimization problem. A deep reinforcement learning algorithm based on a multi-step deep Q network (MSDQN) is proposed to solve the MDP without considering the complicated transmission channels. Especially, the optimal multi-step value is found via experiments. Simulation results show that the proposed offloading algorithm can significantly reduce the IoV computation delay and computation energy consumption in processing computationally intensive tasks.","","979-8-3503-2145-6","10.1109/ICACI58115.2023.10146186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10146186","internet of vehicles;edge computing;markov decision process;deep reinforcement learning","Energy consumption;Simulation;Reinforcement learning;Computer architecture;Markov processes;Delays;Task analysis;Optimization;Edge computing;Internet of Vehicles","cloud computing;deep learning (artificial intelligence);edge computing;energy consumption;Internet of Things;learning (artificial intelligence);Markov processes;mobile computing;optimisation;reinforcement learning;vehicular ad hoc networks","computation ability;computation energy consumption;computationally intensive tasks;deep reinforcement learning algorithm;edge computing;IoV applications;IoV computation delay;Markov decision process;massive computations;multiobjective optimization problem;multistep reinforcement learning-based;offloading computation process;optimal multistep value;vehicle edge;Vehicles system","","","","28","IEEE","12 Jun 2023","","","IEEE","IEEE Conferences"
"Unmanned Surface Vessel Motion Intention Recognition Based on Deep Reinforcement Learning","T. Chen; H. Li","College of Information and Communication Engineering, Harbin Engineering University Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin, China; College of Information and Communication Engineering, Harbin Engineering University Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin, China","2021 3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST)","7 Feb 2022","2021","","","104","110","The rapid development of machine learning in recent years has made it possible to solve the problem of surface ship monitoring. This paper takes intelligent unmanned surface vessel (USV) as the application object. It aims to solve the problem of marine target motion intention recognition based on deep reinforcement learning. Using python to build a highly realistic USV planning platform. Solving the confidentiality of field test data results in the system's simulation results unable to obtain effective feedback from examples. Aiming at the problem of low uniform sampling efficiency of DDPG algorithm, this paper proposes a USV motion control method based on prioritized experience playback DDPG(PER-DDPG) algorithm. This method assists USV to realize the movement intention recognition of the target in the unknown marine environment. This paper also constructs an accurate USV motion model, and carefully sets the USV's action and state space. By setting a new reward function to evaluate the output decision of PER-DDPG, thus strengthening the decision network to control the movement of USV. The experimental results show that the simulation has good realism and environmental adaptability. Compared with the DDPG algorithm, this algorithm has a higher learning efficiency and can realize the recognition of the target movement intention by the USV.","","978-1-6654-0267-5","10.1109/IAECST54258.2021.9695766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695766","Unmanned surface vessel;motion intention recognition;deep reinforcement learning;target reconnaissance","Adaptation models;Technological innovation;Target recognition;Simulation;Reinforcement learning;Reconnaissance;Planning","deep learning (artificial intelligence);feedback;intelligent robots;motion control;object tracking;Python;reinforcement learning;ships;signal sampling;state-space methods;unmanned surface vehicles","surface ship monitoring;intelligent unmanned surface vessel;marine target motion intention recognition;deep reinforcement learning;field test data;USV motion control;prioritized experience playback DDPG algorithm;movement intention recognition;unknown marine environment;USV action;learning efficiency;target movement intention;unmanned surface vessel motion intention recognition;machine learning;uniform sampling efficiency;USV planning platform;USV state space;Python;PER-DDPG algorithm;reward function;decision network;environmental adaptability","","","","12","IEEE","7 Feb 2022","","","IEEE","IEEE Conferences"
"QoS-Ensured Model Optimization for AIoT: A Multi-Scale Reinforcement Learning Approach","G. Wu; F. Zhou; Y. Qu; P. Luo; X. -Y. Li","Department of Computer Science, University of Science and Technology of China, Hefei, China; Key Laboratory of Dynamic Cognitive System of Electromagnetic Spectrum Space, Ministry of Industry and Information Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Key Laboratory of Dynamic Cognitive System of Electromagnetic Spectrum Space, Ministry of Industry and Information Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Computer Science, University of Science and Technology of China, Hefei, China; Department of Computer Science, University of Science and Technology of China, Hefei, China","IEEE Transactions on Mobile Computing","","2023","PP","99","1","18","Optimizing deep neural network (DNN) models to meet quality of service (QoS) requirements in terms of accuracy and computation is of crucial importance for realizing efficient on-device inference in resource-constrained artificial intelligence of things (AIoT). However, most existing works can hardly satisfy the aforementioned QoS requirements since the intrinsic multi-scale characteristic of DNN structures has been seldom considered. In this paper, we formulate a QoS-ensured DNN model structure optimization problem as a novel multi-scale Markov decision process (MSMDP), which can collaboratively decide the DNN structures from different scales. To efficiently solve the above problem, we propose a multi-scale reinforcement learning (MSRL) algorithm, which jointly optimizes block and channel number by interactive multi-scale decision, while ensuring QoS by QoS-based decision evaluation and policy update. Extensive experiments are conducted in both the actual AIoT scenarios and public datasets for different tasks by using different AIoT devices. The results confirm that our proposed MSRL outperforms the baseline schemes in terms of QoS satisfaction, convergence performance, and complexity. Specifically, our algorithm respectively reduces 98.6% computation and 95.7% model size at most while ensuring the QoS compared with the state-of-the-art methods.","1558-0660","","10.1109/TMC.2023.3294512","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10180105","Artificial intelligence of things;edge inference;model strucuture optimization;multi-scale reinforcement learning;QoS ensurance","Quality of service;Optimization;Task analysis;Computational modeling;Quantization (signal);Performance evaluation;Knowledge engineering","","","","","","","IEEE","12 Jul 2023","","","IEEE","IEEE Early Access Articles"
"Characterizing Mobile Money Phishing Using Reinforcement Learning","A. N. Njoya; V. L. T. Ngongag; F. Tchakounté; M. Atemkeng; C. Fachkha","Department of Mathematics and Computer Science, Faculty of Science, University of Ngaoundéré, Ngaoundere, Cameroon; Department of SFTI, School of Chemical Engineering and Mineral Industries, University of Ngaoundéré, Ngaoundere, Cameroon; Department of Mathematics and Computer Science, Faculty of Science, University of Ngaoundéré, Ngaoundere, Cameroon; Department of Mathematics, Rhodes University, Grahamstown, South Africa; College of Engineering and IT, University of Dubai, Dubai, United Arab Emirates","IEEE Access","2 Oct 2023","2023","11","","103839","103862","Mobile money helps people accumulate, send, and receive money using their mobile phones without having a bank account (i.e., in some African countries). Such technology is heavily and efficiently used in many areas where bank services are unavailable and/or in crisis (i.e., during the COVID-19 pandemic) when transportation and services are limited. However, malicious users such as scammers have leveraged social engineering techniques to abuse mobile money services through scams, and frauds, among others. Existing countermeasures, which are specific to mobile money security, mostly ignore the dynamic aspect of interactions between the malicious party and the victim. Considering the above insufficiency, this paper proposes a new approach to characterize mobile money phishing attacks based on reinforcement learning (RL) through  $Q-$ learning and Markov decision processes (MDP) and on deep reinforcement learning (DRL) through DRL algorithms, namely, Deep Sarsa, Advantage Actor-Critic (A2C), Deep Deterministic Policy Gradient (DDPG), and Deep Q-learning (DQN). In fact, the proposed approach models the optimal sequences of attacker actions to achieve their goals through reinforcement learning and deep reinforcement methods. We experiment on real attack scenarios that have been encountered at Orange and MTN telecoms. Furthermore, we compared reinforcement learning and deep reinforcement learning algorithms to each other and thereby demonstrated the difference between them. This analysis showed a better performance in learning with RL. We also deduced that  $Q-$ learning takes less execution time than CDM and therefore its learning quality is better for characterizing mobile money phishing attacks. Finally, we found that some deep reinforcement algorithms, such as Deep Sarsa and  $A2C$ , can improve the characterization of scammer-victim interactions during mobile payments.","2169-3536","","10.1109/ACCESS.2023.3317692","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256102","Deep learning;machine learning;mobile security;social engineering (security);phishing;mobile money","Phishing;Reinforcement learning;Deep learning;Mobile handsets;Fraud;Security;Social factors;Financial management","","","","","","58","CCBYNCND","20 Sep 2023","","","IEEE","IEEE Journals"
"Optimal Electric Vehicle Charging Strategy With Markov Decision Process and Reinforcement Learning Technique","T. Ding; Z. Zeng; J. Bai; B. Qin; Y. Yang; M. Shahidehpour","Department of Electrical Engineering, Xi'an Jiaotong University, Xi'an, China; Department of Electrical Engineering, Xi'an Jiaotong University, Xi'an, China; Department of Electrical Engineering, Xi'an Jiaotong University, Xi'an, China; Department of Electrical Engineering, Xi'an Jiaotong University, Xi'an, China; Department of Energy Technology, Aalborg University, Aalborg, Demark; ECE Department, Illinois Institute of Technology, Chicago, USA","IEEE Transactions on Industry Applications","18 Sep 2020","2020","56","5","5811","5823","Electric vehicles (EVs) have rapidly developed in recent years and their penetration has also significantly increased, which, however, brings new challenges to power systems. Due to their stochastic behaviors, the improper charging strategies for EVs may violate the voltage security region. To address this problem, an optimal EV charging strategy in a distribution network is proposed to maximize the profit of the distribution system operators while satisfying all the physical constraints. When dealing with the uncertainties from EVs, a Markov decision process model is built to characterize the time series of the uncertainties, and then the deep deterministic policy gradient based reinforcement learning technique is utilized to analyze the impact of uncertainties on the charging strategy. Finally, numerical results verify the effectiveness of the proposed method.","1939-9367","","10.1109/TIA.2020.2990096","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2016YFB0901900); National Natural Science Foundation of China(grant numbers:51977166,U1766215); Natural Science Foundation of Shaanxi Province(grant numbers:2020KW-022); China Postdoctoral Science Foundation(grant numbers:2017T100748); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076876","Electric vehicle (EV);Markov decision process (MDP);optimal charging strategy;reinforcement learning (RL).","Electric vehicle charging;Reinforcement learning;Distribution networks;Charging stations;Power system stability;Uncertainty;Markov processes","electric vehicle charging;gradient methods;learning (artificial intelligence);Markov processes","distribution network;distribution system operators;Markov decision process model;deep deterministic policy gradient based reinforcement learning technique;optimal electric vehicle charging strategy;power systems;stochastic behaviors;improper charging strategies;voltage security region;optimal EV charging strategy;time series","","73","","59","IEEE","23 Apr 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Optimal Schedule for a Battery Swapping Station Considering Uncertainties","Y. Gao; J. Yang; M. Yang; Z. Li","Shandong University, Jinan, China; Shandong University, Jinan, China; Department of Electrical Engineering, Shandong University, Jinan, China; Department of Electrical Engineering, Tsinghua University, Beijing, China","IEEE Transactions on Industry Applications","18 Sep 2020","2020","56","5","5775","5784","For a battery swapping station (BSS), the stochastic operation of electric buses (EBs) and the uncertainty of electricity prices cause unnecessary economic losses. To minimize the operating costs of the BSS, this article applies the deep reinforcement learning (DRL) and proposes a BSS model to determine the optimal real-time charge/discharge power of the charging piles. The predictability of bus operation and the uncertainty of price can be directly captured from historical data without any assumption in the model. Moreover, deep deterministic policy gradient (DDPG), as the DRL algorithm, is implemented in the model to simultaneously control multiple charging piles. Numerical results illustrate that the proposed approach can bring less operating cost than the existing benchmark control methods for a BSS while providing adequate batteries ready for swapping.","1939-9367","","10.1109/TIA.2020.2986412","State Grid Corporation of China(grant numbers:52060018000X); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9064930","Battery swapping station;charging/discharging schedules;deep reinforcement learning;electric bus;optimal control","Batteries;Uncertainty;Optimization;Machine learning;Indexes;Schedules;State of charge","electric vehicle charging;learning (artificial intelligence);minimisation;power engineering computing;real-time systems;secondary cells","battery swapping station;stochastic operation;electric buses;electricity prices;unnecessary economic losses;deep reinforcement learning;BSS model;bus operation;deep deterministic policy gradient;DRL algorithm;optimal schedule;operating cost minimization;optimal real-time charge-discharge power;DDPG;multiple charging pile control","","36","","31","IEEE","13 Apr 2020","","","IEEE","IEEE Journals"
"New Hybrid Deep Neural Architectural Search-Based Ensemble Reinforcement Learning Strategy for Wind Power Forecasting","S. M. J. Jalali; G. J. Osório; S. Ahmadian; M. Lotfi; V. M. A. Campos; M. Shafie-khah; A. Khosravi; J. P. S. Catalão","Institute for Intelligent Systems Research, and Innovation (IISRI), Deakin University, Waurn Ponds, VIC, Australia; REMIT, Portucalense University Infante D. Henrique, Porto, Portugal; Faculty of Information Technology, Kermanshah University of Technology, Kermanshah, Iran; Faculty of Engineering of the University of Porto, and INESC TEC, Porto, Portugal; Redes Energéticas Nacionais (REN) SGPS, S.A, Lisbon, Portugal; School of Technology, and Innovations, University of Vaasa, Vaasa, Finland; Institute for Intelligent Systems Research, and Innovation (IISRI), Deakin University, Waurn Ponds, VIC, Australia; Faculty of Engineering of the University of Porto, and INESC TEC, Porto, Portugal","IEEE Transactions on Industry Applications","19 Jan 2022","2022","58","1","15","27","Wind power instability and inconsistency involve the reliability of renewable power energy, the safety of the transmission system, the electrical grid stability and the rapid developments of energy market. The study on wind power forecasting is quite important at this stage in order to facilitate maximum wind energy growth as well as better efficiency of electrical power systems. In this work, we propose a novel hybrid data driven model based on the concepts of deep learning-based convolutional-long short term memory (CLSTM), mutual information, evolutionary algorithm, neural architectural search procedure, and ensemble-based deep reinforcement learning (RL) strategies. We name this hybrid model as DOCREL. In the first step, the mutual information extracts the most effective characteristics from raw wind power time series datasets. Second, we develop an improved version of the evolutionary whale optimization algorithm in order to effectively optimize the architecture of the deep CLSTM models by performing the neural architectural search procedure. At the end, our proposed deep RL-based ensemble algorithm integrates the optimized deep learning models to achieve the lowest possible wind power forecasting errors for two wind power datasets. In comparison with fourteen state-of-the-art deep learning models, our proposed DOCREL algorithm represents an excellent performance seasonally for two different case studies.","1939-9367","","10.1109/TIA.2021.3126272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609674","Advanced evolutionary algorithm;deep neural architectural search;ensemble reinforcement learning (RL) strategy;hybrid model;wind power forecasting","Predictive models;Forecasting;Wind power generation;Deep learning;Data models;Wind forecasting;Hybrid power systems","deep learning (artificial intelligence);evolutionary computation;load forecasting;power engineering computing;power system stability;power transmission lines;reinforcement learning;time series;wind power plants","deep CLSTM models;neural architectural search procedure;deep RL-based ensemble algorithm;optimized deep learning models;wind power forecasting errors;wind power datasets;fourteen state-of-the-art deep learning models;new hybrid deep neural architectural search-based ensemble reinforcement learning strategy;power instability;renewable power energy;transmission system;electrical grid stability;energy market;maximum wind energy growth;electrical power systems;hybrid data;mutual information;deep reinforcement learning strategies;hybrid model;raw wind power time series datasets;evolutionary whale optimization algorithm","","14","","57","IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"Adaptive Reinforcement Learning Neural Network Control for Uncertain Nonlinear System With Input Saturation","W. Bai; Q. Zhou; T. Li; H. Li","School of Automation, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; Navigation College, Dalian Maritime University, Dalian, China; School of Automation, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Cybernetics","10 Jul 2020","2020","50","8","3433","3443","In this paper, an adaptive neural network (NN) control problem is investigated for discrete-time nonlinear systems with input saturation. Radial-basis-function (RBF) NNs, including critic NNs and action NNs, are employed to approximate the utility functions and system uncertainties, respectively. In the previous works, a gradient descent scheme is applied to update weight vectors, which may lead to local optimal problem. To circumvent this problem, a multigradient recursive (MGR) reinforcement learning scheme is proposed, which utilizes both the current gradient and the past gradients. As a consequence, the MGR scheme not only eliminates the local optimal problem but also guarantees faster convergence rate than the gradient descent scheme. Moreover, the constraint of actuator input saturation is considered. The closed-loop system stability is developed by using the Lyapunov stability theory, and it is proved that all the signals in the closed-loop system are semiglobal uniformly ultimately bounded (SGUUB). Finally, the effectiveness of the proposed approach is further validated via some simulation results.","2168-2275","","10.1109/TCYB.2019.2921057","National Natural Science Foundation of China(grant numbers:61673072,61751202,61803064); Guangdong Natural Science Funds for Distinguished Young Scholar(grant numbers:2017A030306014); Innovative Research Team Program of Guangdong Province Science Foundation(grant numbers:2018B030312006); Science and Technology Innovation Funds of Dalian(grant numbers:2018J11CY022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746696","Discrete-time systems;input saturation;multigradient recursive (MGR);neural networks (NNs);reinforcement learning","Artificial neural networks;Reinforcement learning;Convergence;Adaptive systems;Optimal control;Discrete-time systems","actuators;adaptive control;closed loop systems;control nonlinearities;discrete time systems;gradient methods;learning (artificial intelligence);Lyapunov methods;neurocontrollers;nonlinear control systems;radial basis function networks;stability;uncertain systems","adaptive reinforcement learning neural network control;uncertain nonlinear system;adaptive neural network control problem;discrete-time nonlinear systems;radial-basis-function NNs;critic NNs;action NNs;utility functions;system uncertainties;gradient descent scheme;weight vectors;local optimal problem;multigradient recursive reinforcement learning scheme;MGR scheme;actuator input saturation;closed-loop system stability;Lyapunov stability theory;semiglobal uniformly ultimately bounded;SGUUB","Algorithms;Machine Learning;Neural Networks, Computer;Nonlinear Dynamics;Signal Processing, Computer-Assisted","142","","49","IEEE","26 Jun 2019","","","IEEE","IEEE Journals"
"Reinforcement Learning of Manipulation and Grasping Using Dynamical Movement Primitives for a Humanoidlike Mobile Manipulator","Z. Li; T. Zhao; F. Chen; Y. Hu; C. -Y. Su; T. Fukuda","College of Automation Science and Engineering, South China University of Technology, Guangzhou, China; College of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Department of Advanced Robotics, Istituto Italiano di Tecnologia, Genova, Italy; College of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Department of Mechanical, Industrial, and Aerospace Engineering, Concordia University, Montreal, QC, Canada; School of Mechatronic Engineering, Beijing Institute of Technology, Beijing, China","IEEE/ASME Transactions on Mechatronics","16 Feb 2018","2018","23","1","121","131","It is important for humanoid-like mobile robots to learn the complex motion sequences in human-robot environment such that the robots can adapt such motions. This paper describes a reinforcement learning (RL) strategy for manipulation and grasping of a mobile manipulator, which reduces the complexity of the visual feedback and handle varying manipulation dynamics and uncertain external perturbations. Two hierarchies plannings have been considered in the proposed strategy: 1) high-level online redundancy resolution based on the neural-dynamic optimization algorithm in operational space; and 2) low-level RL in joint space. At this level, the dynamic movement primitives have been considered to model and learn the joint trajectories, and then the RL is employed to learn the trajectories with uncertainties. Experimental results on the developed humanoidlike mobile robot demonstrate that the presented approach can suppress the uncertain external perturbations.","1941-014X","","10.1109/TMECH.2017.2717461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953692","Dynamic movement primitive (DMP);mobile manipulation;redundancy resolution;reinforcement learning (RL)","Mobile communication;Robot sensing systems;Manipulator dynamics;Trajectory;Learning (artificial intelligence)","control engineering computing;feedback;humanoid robots;human-robot interaction;learning (artificial intelligence);manipulator dynamics;mobile robots;motion control;neurocontrollers;optimisation;redundant manipulators","humanoidlike mobile manipulator;mobile robots;complex motion sequences;human-robot environment;reinforcement learning strategy;visual feedback;neural-dynamic optimization algorithm;dynamical movement primitives","","135","","37","IEEE","20 Jun 2017","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning With Visual Attention for Vehicle Classification","D. Zhao; Y. Chen; L. Lv","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Cognitive and Developmental Systems","7 Dec 2017","2017","9","4","356","367","Automatic vehicle classification is crucial to intelligent transportation system, especially for vehicle-tracking by police. Due to the complex lighting and image capture conditions, image-based vehicle classification in real-world environments is still a challenging task and the performance is far from being satisfactory. However, owing to the mechanism of visual attention, the human vision system shows remarkable capability compared with the computer vision system, especially in distinguishing nuances processing. Inspired by this mechanism, we propose a convolutional neural network (CNN) model of visual attention for image classification. A visual attention-based image processing module is used to highlight one part of an image and weaken the others, generating a focused image. Then the focused image is input into the CNN to be classified. According to the classification probability distribution, we compute the information entropy to guide a reinforcement learning agent to achieve a better policy for image classification to select the key parts of an image. Systematic experiments on a surveillance-nature dataset which contains images captured by surveillance cameras in the front view, demonstrate that the proposed model is more competitive than the large-scale CNN in vehicle classification tasks.","2379-8939","","10.1109/TCDS.2016.2614675","National Natural Science Foundation of China(grant numbers:61273136,61573353,61533017); National Key Research and Development Plan(grant numbers:2016YFB0101000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7580631","Convolutional neural network (CNN);reinforcement learning;vehicle classification;visual attention","Convolutional neural networks;Visualization;Learning (artificial intelligence);Feature extraction;Cameras;Computer architecture;Information entropy","computer vision;feature extraction;image classification;learning (artificial intelligence);object detection;traffic engineering computing","automatic vehicle classification;intelligent transportation system;vehicle-tracking;complex lighting;human vision system;computer vision system;convolutional neural network model;CNN;image classification;visual attention-based image processing module;focused image;classification probability distribution;vehicle classification tasks;deep reinforcement learning","","127","","43","IEEE","30 Sep 2016","","","IEEE","IEEE Journals"
"Attention-Aware Deep Reinforcement Learning for Video Face Recognition","Y. Rao; J. Lu; J. Zhou","State Key Lab of Intelligent Technologies and Systems, Beijing, China; State Key Lab of Intelligent Technologies and Systems, Beijing, China; State Key Lab of Intelligent Technologies and Systems, Beijing, China","2017 IEEE International Conference on Computer Vision (ICCV)","25 Dec 2017","2017","","","3951","3960","In this paper, we propose an attention-aware deep reinforcement learning (ADRL) method for video face recognition, which aims to discard the misleading and confounding frames and find the focuses of attentions in face videos for person recognition. We formulate the process of finding the attentions of videos as a Markov decision process and train the attention model through a deep reinforcement learning framework without using extra labels. Unlike existing attention models, our method takes information from both the image space and the feature space as the input to make better use of face information that is discarded in the feature learning process. Besides, our approach is attention-aware, which seeks different attentions of videos for the recognition of different pairs of videos. Our approach achieves very competitive video face recognition performance on three widely used video face datasets.","2380-7504","978-1-5386-1032-9","10.1109/ICCV.2017.424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237686","","Face recognition;Face;Manifolds;Feature extraction;Machine learning;Markov processes;Computational modeling","face recognition;learning (artificial intelligence);Markov processes;video signal processing","Markov decision process;deep reinforcement learning framework;face information;feature learning process;attention-aware deep reinforcement learning method;face videos;person recognition;video face recognition;ADRL method;video face datasets;image space;feature space","","103","","46","IEEE","25 Dec 2017","","","IEEE","IEEE Conferences"
"Optimal Synchronization Control of Multiagent Systems With Input Saturation via Off-Policy Reinforcement Learning","J. Qin; M. Li; Y. Shi; Q. Ma; W. X. Zheng","Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Mechanical Engineering, University of Victoria, Victoria, BC, Canada; Department of Automation, University of Science and Technology of China, Hefei, China; School of Computing, Engineering and Mathematics, Western Sydney University, Sydney, NSW, Australia","IEEE Transactions on Neural Networks and Learning Systems","21 Dec 2018","2019","30","1","85","96","In this paper, we aim to investigate the optimal synchronization problem for a group of generic linear systems with input saturation. To seek the optimal controller, Hamilton-Jacobi-Bellman (HJB) equations involving nonquadratic input energy terms in coupled forms are established. The solutions to these coupled HJB equations are further proven to be optimal and the induced controllers constitute interactive Nash equilibrium. Due to the difficulty to analytically solve HJB equations, especially in coupled forms, and the possible lack of model information of the systems, we apply the data-based off-policy reinforcement learning algorithm to learn the optimal control policies. A byproduct of this off-policy algorithm is shown that it is insensitive to probing noise that is exerted to the system to maintain persistence of excitation condition. In order to implement this off-policy algorithm, we employ actor and critic neural networks to approximate the controllers and the cost functions. Furthermore, the estimated control policies obtained by this presented implementation are proven to converge to the optimal ones under certain conditions. Finally, an illustrative example is provided to verify the effectiveness of the proposed algorithm.","2162-2388","","10.1109/TNNLS.2018.2832025","National Natural Science Foundation of China(grant numbers:61473269); Fok Ying-Tong Education Foundation for Young Teachers in the Higher Education Institutions of China(grant numbers:161059); Youth Innovation Promotion Association of the Chinese Academy of Sciences; Australian Research Council(grant numbers:DP120104986); 111 Project(grant numbers:B16014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8365133","Input saturation;multiagent systems;neural networks (NNs);off-policy reinforcement learning (RL);optimal synchronization control","Mathematical model;Synchronization;Approximation algorithms;Control systems;Multi-agent systems;Learning systems;Learning (artificial intelligence)","control nonlinearities;game theory;learning (artificial intelligence);linear systems;multi-agent systems;neurocontrollers;optimal control","optimal synchronization problem;generic linear systems;input saturation;optimal controller;Hamilton-Jacobi-Bellman equations;nonquadratic input energy terms;coupled forms;coupled HJB equations;induced controllers;interactive Nash equilibrium;model information;data-based off-policy reinforcement;optimal control policies;optimal synchronization control;multiagent systems;off-policy reinforcement learning;control policy estimation;actor neural network;critic neural network","","95","","29","IEEE","24 May 2018","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Automatic Exploration for Navigation in Unknown Environment","H. Li; Q. Zhang; D. Zhao","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","2 Jun 2020","2020","31","6","2064","2076","This paper investigates the automatic exploration problem under the unknown environment, which is the key point of applying the robotic system to some social tasks. The solution to this problem via stacking decision rules is impossible to cover various environments and sensor properties. Learning-based control methods are adaptive for these scenarios. However, these methods are damaged by low learning efficiency and awkward transferability from simulation to reality. In this paper, we construct a general exploration framework via decomposing the exploration process into the decision, planning, and mapping modules, which increases the modularity of the robotic system. Based on this framework, we propose a deep reinforcement learning-based decision algorithm that uses a deep neural network to learning exploration strategy from the partial map. The results show that this proposed algorithm has better learning efficiency and adaptability for unknown environments. In addition, we conduct the experiments on the physical robot, and the results suggest that the learned policy can be well transferred from simulation to the real robot.","2162-2388","","10.1109/TNNLS.2019.2927869","Beijing Science and Technology Plan(grant numbers:Z181100004618003); National Natural Science Foundation of China (NSFC)(grant numbers:61573353,61803371,61533017,61603268); Noah’s Ark Lab, Huawei Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789673","Automatic exploration;deep reinforcement learning (DRL);optimal decision;partial observation","Robot sensing systems;Navigation;Entropy;Neural networks;Task analysis;Planning","learning (artificial intelligence);mobile robots;neural nets","unknown environment;automatic exploration problem;robotic system;social tasks;decision rules;sensor properties;learning-based control methods;low learning efficiency;general exploration framework;exploration process;deep reinforcement learning-based decision algorithm;deep neural network;exploration strategy;adaptability;physical robot;learned policy","","90","","39","IEEE","6 Aug 2019","","","IEEE","IEEE Journals"
"Adaptive Multigradient Recursive Reinforcement Learning Event-Triggered Tracking Control for Multiagent Systems","H. Li; Y. Wu; M. Chen; R. Lu","School of Automation, Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, China; College of Control Science and Engineering, Bohai University, Jinzhou, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Automation, Guangdong Province Key Laboratory of Intelligent Decision and Cooperative Control, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Neural Networks and Learning Systems","5 Jan 2023","2023","34","1","144","156","This article proposes a fault-tolerant adaptive multigradient recursive reinforcement learning (RL) event-triggered tracking control scheme for strict-feedback discrete-time multiagent systems. The multigradient recursive RL algorithm is used to avoid the local optimal problem that may exist in the gradient descent scheme. Different from the existing event-triggered control results, a new lemma about the relative threshold event-triggered control strategy is proposed to handle the compensation error, which can improve the utilization of communication resources and weaken the negative impact on tracking accuracy and closed-loop system stability. To overcome the difficulty caused by sensor fault, a distributed control method is introduced by adopting the adaptive compensation technique, which can effectively decrease the number of online estimation parameters. Furthermore, by using the multigradient recursive RL algorithm with less learning parameters, the online estimation time can be effectively reduced. The stability of closed-loop multiagent systems is proved by using the Lyapunov stability theorem, and it is verified that all signals are semiglobally uniformly ultimately bounded. Finally, two simulation examples are given to show the availability of the presented control scheme.","2162-2388","","10.1109/TNNLS.2021.3090570","National Natural Science Foundation of China(grant numbers:62033003); Local Innovative and Research Teams Project of Guangdong Special Support Program(grant numbers:2019BT02X353); Innovative Research Team Program of Guangdong Province Science Foundation(grant numbers:2018B030312006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9470911","Event-triggered control;fault-tolerant control;multiagent systems;multigradient recursive reinforcement learning (RL) algorithm","Fault tolerant systems;Fault tolerance;Estimation;Adaptive control;Multi-agent systems;Closed loop systems;Artificial neural networks","adaptive control;closed loop systems;discrete time systems;distributed control;fault tolerant control;feedback;learning systems;Lyapunov methods;multi-agent systems;reinforcement learning;stability","adaptive compensation technique;closed-loop multiagent systems;closed-loop system stability;distributed control method;fault-tolerant adaptive multigradient recursive reinforcement learning event-triggered tracking control scheme;gradient descent scheme;learning parameters;Lyapunov stability theorem;multigradient recursive RL algorithm;relative threshold event-triggered control strategy;strict-feedback discrete-time multiagent systems","","66","","60","IEEE","1 Jul 2021","","","IEEE","IEEE Journals"
"CDDPG: A Deep-Reinforcement-Learning-Based Approach for Electric Vehicle Charging Control","F. Zhang; Q. Yang; D. An","School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Automation Science and Engineering and the SKLMSE Lab, MOE Key Laboratory for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, China; School of Automation Science and Engineering and the SKLMSE Lab, MOE Key Laboratory for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, China","IEEE Internet of Things Journal","18 Feb 2021","2021","8","5","3075","3087","Electric vehicle (EV) has become one of the most critical components in the smart grid with the applications of the Internet-of-Things (IoT) technologies. Real-time charging control is pivotal to ensure the efficient operation of EVs. However, the charging control performance is limited by the uncertainty of the environment. On the other hand, it is challenging to determine a charging control strategy that is able to optimize multiple objectives simultaneously. In this article, we formulate the EV charging control model as a Markov decision process (MDP) by constructing state, action, transition function, and reward. Then, we propose a deep-reinforcement-learning-based approach: charging control deep deterministic policy gradient (CDDPG) to learn the optimal charging control strategy for satisfying the user's requirement of battery energy while minimizing the user's charging expense. We utilize the long short-term memory (LSTM) network that extracts the information of previous energy price to determine the current charging control strategy. Moreover, Gaussian noise is added to the output of the actor network to prevent the agent from sticking into the nonoptimal strategy. In addition, we address the limitation of sparse rewards by using two replay buffers, of which one is used to store the rewards during the charging phase and another is used to store the rewards after charging is completed. The simulation results prove that the CDDPG-based approach outperforms the deep-$Q$ -learning-based approach (DQL) and the deep-deterministic-policy-gradient-based approach (DDPG) in satisfying the user's requirement for the battery energy and reducing the charging cost.","2327-4662","","10.1109/JIOT.2020.3015204","National Natural Science Foundation of China(grant numbers:61973247,61673315,61833015,61866022,61803295); National Postdoctoral Program for Innovative Talent(grant numbers:BX2020272); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9163332","Charging control;deep deterministic policy gradient (DDPG);electric vehicle (EV);Markov decision process (MDP)","Electric vehicle charging;Real-time systems;Vehicle dynamics;Internet of Things;Uncertainty;Batteries;Task analysis","battery powered vehicles;battery storage plants;cost reduction;electric vehicles;gradient methods;learning (artificial intelligence);Markov processes;optimisation;smart power grids","deep-reinforcement-learning-based approach;electric vehicle charging control;Internet-of-Things technologies;real-time charging control;charging control performance;EV charging control model;charging control deep deterministic policy gradient;optimal charging control strategy;user;battery energy;short-term memory network;current charging control strategy;nonoptimal strategy;charging phase;CDDPG-based approach;deep-deterministic-policy-gradient-based approach;charging cost","","56","","41","IEEE","10 Aug 2020","","","IEEE","IEEE Journals"
"Multi-agent reinforcement learning for traffic signal control","Prabuchandran K.J.; Hemanth Kumar A.N; S. Bhatnagar","Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India","17th International IEEE Conference on Intelligent Transportation Systems (ITSC)","20 Nov 2014","2014","","","2529","2534","Optimal control of traffic lights at junctions or traffic signal control (TSC) is essential for reducing the average delay experienced by the road users amidst the rapid increase in the usage of vehicles. In this paper, we formulate the TSC problem as a discounted cost Markov decision process (MDP) and apply multi-agent reinforcement learning (MARL) algorithms to obtain dynamic TSC policies. We model each traffic signal junction as an independent agent. An agent decides the signal duration of its phases in a round-robin (RR) manner using multi-agent Q-learning with either ε-greedy or UCB [3] based exploration strategies. It updates its Q-factors based on the cost feedback signal received from its neighbouring agents. This feedback signal can be easily constructed and is shown to be effective in minimizing the average delay of the vehicles in the network. We show through simulations over VISSIM that our algorithms perform significantly better than both the standard fixed signal timing (FST) algorithm and the saturation balancing (SAT) algorithm [15] over two real road networks.","2153-0017","978-1-4799-6078-1","10.1109/ITSC.2014.6958095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958095","traffic signal control;multi-agent reinforcement learning;Q-learning;UCB;VISSIM","Junctions;Roads;Delays;Heuristic algorithms;Q-factor;Vehicles","decision theory;learning (artificial intelligence);Markov processes;multi-agent systems;optimal control;road traffic control;traffic engineering computing","traffic signal control;optimal control;traffic lights;average delay reduction;TSC problem;discounted cost Markov decision process;MDP;multiagent reinforcement learning algorithms;MARL algorithms;dynamic TSC policies;traffic signal junction;independent agent;round-robin manner;RR manner;multiagent Q-learning;ε-greedy based exploration strategy;UCB based exploration strategy;cost feedback signal;neighbouring agents;average delay minimization","","50","","19","IEEE","20 Nov 2014","","","IEEE","IEEE Conferences"
"Hierarchical Deep Reinforcement Learning for Backscattering Data Collection With Multiple UAVs","Y. Zhang; Z. Mou; F. Gao; L. Xing; J. Jiang; Z. Han","Department of Automation, Institute for Artificial Intelligence, State Key Laboratory of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Department of Automation, Institute for Artificial Intelligence, State Key Laboratory of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Department of Automation, Institute for Artificial Intelligence, State Key Laboratory of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; School of Information Engineering, Henan University of Science and Technology, Luoyang, China; Shaanxi Key Laboratory of Information Communication Network and Security, Xi’an University of Posts and Telecommunications, Xi’an, China; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA","IEEE Internet of Things Journal","18 Feb 2021","2021","8","5","3786","3800","The emerging backscatter communication technology is recognized as a promising solution to the battery problem of Internet of Things (IoT) devices. For example, the wireless sensor network with backscatter communication technology can monitor the environment in remote areas without battery maintenance or replacement. Unfortunately, the transmission range of backscatter communication is limited. To tackle this challenge, we propose a multi-UAV-aided data collection scenario where the unmanned aerial vehicle (UAV) can fly close to the backscatter sensor node (BSN) to activate it and then collects the data. We aim to minimize the total flight time of the rechargeable UAVs when the collection mission is finished. During the data collection process, the UAVs can return to the charging station to recharge itself when the energy of UAV is not sufficient to complete the mission. To reduce the complexity of the task, we first use the Gaussian mixture model clustering method to divide the BSNs into multiple clusters. Then we consider the deterministic boundary and ambiguous boundary for the UAV flying regions, respectively. For the deterministic boundary scenario, we propose a single-agent deep option learning (SADOL) algorithm, where each UAV cannot fly beyond the deterministic boundary. For the ambiguous boundary scenario, we propose a multiagent deep option learning (MADOL) algorithm to enable the UAVs to cooperatively learn the ambiguous BSNs assignment. In the simulation, we compare the proposed algorithms with multiagent deep deterministic policy gradient (MADDPG), deep deterministic policy gradient (DDPG), and deep Q-network (DQN) algorithms, which proves the proposed algorithms can achieve better performance.","2327-4662","","10.1109/JIOT.2020.3024666","National Key Research and Development Program of China(grant numbers:2018AAA0102401); National Natural Science Foundation of China(grant numbers:61831013,61771274,61531011,61871321,61771185); Beijing Municipal Natural Science Foundation(grant numbers:4182030,L182042); U.S. NSF(grant numbers:EARS-1839818,CNS1717454,CNS-1731424,CNS-1702850); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200357","Charging;data collection;multiagent deep reinforcement learning (DRL);option;unmanned aerial vehicle (UAV)","Data collection;Backscatter;Unmanned aerial vehicles;Task analysis;Internet of Things;Charging stations;Trajectory","autonomous aerial vehicles;control engineering computing;data handling;deep learning (artificial intelligence);Gaussian processes;gradient methods;mixture models;multi-agent systems;neurocontrollers;pattern clustering","data collection process;UAV flying regions;single-agent deep option learning algorithm;multiagent deep option learning algorithm;multiagent deep deterministic policy gradient;deep Q-network algorithms;hierarchical deep reinforcement learning;backscattering data collection;multiple UAVs;emerging backscatter communication technology;Internet of Things devices;wireless sensor network;battery maintenance;multiUAV-aided data collection scenario;rechargeable UAVs;SADOL algorithm;MADOL algorithm;MADDPG;DDPG;DQN;Gaussian mixture model clustering method","","50","","42","IEEE","18 Sep 2020","","","IEEE","IEEE Journals"
"Action-Driven Visual Object Tracking With Deep Reinforcement Learning","S. Yun; J. Choi; Y. Yoo; K. Yun; J. Y. Choi","Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National University, Seoul, South Korea; CLOVA AI Research, Naver, Seoungnam-si, South Korea; Visual Intelligence Research Group, SW Contents Laboratory, Electronics and Telecommunications Research Institute, Daejeon, South Korea; Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National University, Seoul, South Korea","IEEE Transactions on Neural Networks and Learning Systems","16 May 2018","2018","29","6","2239","2252","In this paper, we propose an efficient visual tracker, which directly captures a bounding box containing the target object in a video by means of sequential actions learned using deep neural networks. The proposed deep neural network to control tracking actions is pretrained using various training video sequences and fine-tuned during actual tracking for online adaptation to a change of target and background. The pretraining is done by utilizing deep reinforcement learning (RL) as well as supervised learning. The use of RL enables even partially labeled data to be successfully utilized for semisupervised learning. Through the evaluation of the object tracking benchmark data set, the proposed tracker is validated to achieve a competitive performance at three times the speed of existing deep network-based trackers. The fast version of the proposed method, which operates in real time on graphics processing unit, outperforms the state-of-the-art real-time trackers with an accuracy improvement of more than 8%.","2162-2388","","10.1109/TNNLS.2018.2801826","ICT R&D program of MSIP/IITP (Development of Predictive Visual Intelligence Technology)(grant numbers:B0101-15-0552); ICT R&D program of MSIP/IITP (Development of High Performance Visual BigData Discovery Platform)(grant numbers:B0101-15-0266); SNU-Samsung Smart Campus Research Center at Seoul National University; Brain Korea 21 Plus Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8306309","Deep neural network;reinforcement learning (RL);visual tracking","Target tracking;Visualization;Training;Object tracking;Video sequences;Search problems","image sequences;learning (artificial intelligence);neural nets;object tracking","deep reinforcement learning;bounding box;deep neural network;training video sequences;RL;supervised learning;sequential actions learning;action-driven visual object tracking;fine-tuning;online adaptation;object tracking benchmark data set;deep network-based trackers;graphics processing unit","Algorithms;Computer Simulation;Deep Learning;Humans;Nonlinear Dynamics;Pattern Recognition, Automated;Reinforcement, Psychology;Video Recording;Visual Perception","49","","53","IEEE","2 Mar 2018","","","IEEE","IEEE Journals"
"Parallel reinforcement learning: a framework and case study","T. Liu; B. Tian; Y. Ai; L. Li; D. Cao; F. -Y. Wang","Institute of Automation Chinese Academy of Sciences, Beijing, Beijing, CN; Institute of Automation Chinese Academy of Sciences, Beijing, Beijing, CN; University of the Chinese Academy of Sciences, Beijing, Beijing, CN; Tsinghua University, Beijing, Beijing, CN; University of Waterloo, Waterloo, ON, CA; Institute of Automation Chinese Academy of Sciences, Beijing, Beijing, CN","IEEE/CAA Journal of Automatica Sinica","31 May 2018","2018","5","4","827","835","In this paper, a new machine learning framework is developed for complex system control, called parallel reinforcement learning. To overcome data deficiency of current data-driven algorithms, a parallel system is built to improve complex learning system by self-guidance. Based on the Markov chain (MC) theory, we combine the transfer learning, predictive learning, deep learning and reinforcement learning to tackle the data and action processes and to express the knowledge. Parallel reinforcement learning framework is formulated and several case studies for real-world problems are finally introduced.","2329-9274","","10.1109/JAS.2018.7511144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370297","","Learning (artificial intelligence);Complex systems;Power demand;Machine learning;Computational modeling;Control systems;Force","learning (artificial intelligence);Markov processes;parallel processing","transfer learning;predictive learning;deep learning;action processes;parallel reinforcement learning framework;machine learning framework;complex system control;parallel system;complex learning system;Markov chain theory;data-driven algorithms;parallel reinforcement learning;data processes","","47","","","","31 May 2018","","","IEEE","IEEE Journals"
"Plume Tracing via Model-Free Reinforcement Learning Method","H. Hu; S. Song; C. L. P. Chen","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","17 Jul 2019","2019","30","8","2515","2527","This paper studies the plume-tracing strategy for an autonomous underwater vehicle (AUV) in the deep-sea turbulent environment. The tracing problem is modeled as a partially observable Markov decision process with continuous state space and action space due to the spatio-temporal changes of environment. An long short-term memory-based reinforcement learning framework with full use of history information is proposed to generate a smooth strategy while the AUV interacting with the environment. Continuous temporal difference and deterministic policy gradient methods are employed to improve the strategy. To promote the performance of the algorithm, a supervised strategy generated by dynamic programming methods is utilized as transcendental knowledge of the agent. Historical searching trajectory's form and the exploration technology are specially designed to fit the algorithm. Simulation environments are established based on Reynolds-averaged Navier-Stokes equations and the effectiveness of the learned plume-tracing strategy is validated with simulation experiments.","2162-2388","","10.1109/TNNLS.2018.2885374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598800","Deterministic policy gradient (DPG);long short-term memory (LSTM);partially observable Markov decision process (POMDP);plume-tracing;reinforcement learning (RL);supervised learning","Chemicals;Heuristic algorithms;Dynamic programming;Markov processes;Mathematical model;Reinforcement learning;Trajectory","autonomous underwater vehicles;decision theory;dynamic programming;gradient methods;Markov processes;mobile robots;Navier-Stokes equations;search problems;supervised learning;trajectory control","autonomous underwater vehicle;deep-sea turbulent environment;partially observable Markov decision process;smooth strategy;continuous temporal difference;deterministic policy gradient methods;supervised strategy;dynamic programming methods;historical searching trajectory;model-free reinforcement learning method;plume-tracing strategy;long short-term memory-based reinforcement;AUV interaction;Reynolds-averaged Navier-Stokes equations","","39","","35","IEEE","1 Jan 2019","","","IEEE","IEEE Journals"
"Reinforcement Learning for Dynamic Resource Optimization in 5G Radio Access Network Slicing","Y. Shi; Y. E. Sagduyu; T. Erpek","Virginia Tech, Blacksburg, VA, USA; Intelligent Automation, Inc., Rockville, MD, USA; Virginia Tech, Blacksburg, VA, USA","2020 IEEE 25th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD)","30 Sep 2020","2020","","","1","6","The paper presents a reinforcement learning solution to dynamic resource allocation for 5G radio access network slicing. Available communication resources (frequency-time blocks and transmit powers) and computational resources (processor usage) are allocated to stochastic arrivals of network slice requests. Each request arrives with priority (weight), throughput, computational resource, and latency (deadline) requirements, and if feasible, it is served with available communication and computational resources allocated over its requested duration. As each decision of resource allocation makes some of the resources temporarily unavailable for future, the myopic solution that can optimize only the current resource allocation becomes ineffective for network slicing. Therefore, a Q-learning solution is presented to maximize the network utility in terms of the total weight of granted network slicing requests over a time horizon subject to communication and computational constraints. Results show that reinforcement learning provides major improvements in the 5G network utility relative to myopic, random, and first come first served solutions. While reinforcement learning sustains scalable performance as the number of served users increases, it can also be effectively used to assign resources to network slices when 5G needs to share the spectrum with incumbent users that may dynamically occupy some of the frequency-time blocks.","2378-4873","978-1-7281-6339-0","10.1109/CAMAD50429.2020.9209299","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9209299","5G security;network slicing;radio access network;network optimization;reinforcement learning","5G mobile communication;Resource management;Network slicing;Dynamic scheduling;Optimization;Throughput;Bit error rate","5G mobile communication;learning (artificial intelligence);radio access networks;resource allocation;telecommunication computing","frequency-time blocks;reinforcement learning;dynamic resource optimization;5G radio access network slicing;dynamic resource allocation;network slice requests;Q-learning solution;computational constraints","","37","","22","IEEE","30 Sep 2020","","","IEEE","IEEE Conferences"
"Reinforcement Q-Learning Algorithm for H∞ Tracking Control of Unknown Discrete-Time Linear Systems","Y. Peng; Q. Chen; W. Sun","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","14 Oct 2020","2020","50","11","4109","4122","This article addresses the online reinforcement Q-learning algorithms to design H∞ tracking controller for unknown discrete-time linear systems. An augmented system composed of the original system and the command generator is constructed, and a discounted performance function is introduced to establish a discounted game algebraic Riccati equation (GARE). The existence conditions of a solution to the GARE are proposed and a lower bound is found for the discount factor to assure the stability of the H∞ tracking control solution. The Q-function Bellman equation is then derived, based on which the reinforcement Q-learning algorithm is developed to learn the solution to H∞ tracking control problem without knowing the system dynamics. Both state-data-driven and output-data-driven reinforcement Q-learning algorithms toward finding the control policies are proposed. Unlike the value function approximation (VFA)-based approach, it is proved that the Q-learning scheme brings out no bias of solution to the Q-function Bellman equation under the probing noise satisfying the persistent excitation (PE) condition, and therefore, converges to the nominal discounted GARE solution. Moreover, the proposed output-data-driven method is more powerful than the state-data-driven method as it may not be available to completely measure the full system states in practical applications. A simulation example with a single-phase voltage-source UPS inverter is used to verify the effectiveness of the proposed Q-learning algorithms.","2168-2232","","10.1109/TSMC.2019.2957000","National Natural Science Foundation(grant numbers:61573154); Science and Technology Planning Project of Guangdong Province(grant numbers:2015A010106003,2017A010101009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8937784","Data driven;optimal control;output feedback (OPFB);policy iteration;reinforcement Q-learning","System dynamics;Linear systems;Heuristic algorithms;Generators;Games;Stability analysis;Trajectory","approximation theory;discrete time systems;function approximation;game theory;H∞ control;learning (artificial intelligence);learning systems;linear systems;nonlinear control systems;Riccati equations;stability","unknown discrete-time linear systems;online reinforcement Q-learning algorithms;augmented system;discounted game algebraic Riccati equation;Q-function Bellman equation;H∞ tracking control problem;output-data-driven reinforcement Q-learning;value function approximation;nominal discounted GARE solution;output-data-driven method;state-data-driven method","","27","","47","IEEE","20 Dec 2019","","","IEEE","IEEE Journals"
"Safety-Aware Reinforcement Learning Framework with an Actor-Critic-Barrier Structure","Y. Yang; Y. Yin; W. He; K. G. Vamvoudakis; H. Modares; D. C. Wunsch","School of Automation & Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation & Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation & Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Georgia Tech, Daniel Guggenheim School of Aerospace Engineering, GA, USA; Mechanical Engineering Department, Michigan State Univercity, East Lansing, MI, USA; Department of Electrical and Computer Engineering, Missouri University of Science & Technology, Rolla, MO, USA","2019 American Control Conference (ACC)","29 Aug 2019","2019","","","2352","2358","This paper considers the control problem with constraints on full-state and control input simultaneously. First, a novel barrier function based system transformation approach is developed to guarantee the full-state constraints. To deal with the input saturation, the hyperbolic-type penalty function is imposed on the control input. The actor-critic based reinforcement learning technique is combined with the barrier transformation to learn the optimal control policy that considers both the full-state constraints and input saturations. To illustrate the efficacy, a numeric simulation is implemented in the end.","2378-5861","978-1-5386-7926-5","10.23919/ACC.2019.8815335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8815335","reinforcement learning;full-state constraints;input saturation;safe control","","learning (artificial intelligence);learning systems;optimal control","barrier function based system transformation approach;optimal control policy;barrier transformation;actor-critic based reinforcement learning technique;hyperbolic-type penalty function;full-state constraints;control input;control problem;actor-critic-barrier structure;safety-aware reinforcement learning framework","","24","","19","","29 Aug 2019","","","IEEE","IEEE Conferences"
"A Multi-Agent Reinforcement Learning Routing Protocol for Underwater Optical Sensor Networks","X. Li; X. Hu; W. Li; H. Hu","School of Artificial Intelligence and Automation; School of Artificial Intelligence and Automation; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation","ICC 2019 - 2019 IEEE International Conference on Communications (ICC)","15 Jul 2019","2019","","","1","7","Much attention has been paid to underwater optical wireless sensor networks with the characteristics of high transmission rate and low delay for high-bandwidth underwater applications. However, several issues may take place and hinder the routing of underwater optical communication nodes due to the highly dynamic topology caused by the ocean current movement. On the purpose of addressing the problem and enhancing the robustness of dynamic network, in this paper, we propose a novel routing protocol, based on multi-agent reinforcement learning (MARL) for underwater optical sensor networks. The network is firstly modeled as a multi-agent system and the protocol based on reinforcement learning algorithm is designed to realize dynamic route selection by information interacting between adjacent nodes and maximize the network lifetime. The simulation results demonstrate that MARL has lower energy consumption and higher delivery ratio (about 95%) in a dynamic topology than the existing Q-learning, QDTR and AODV routing protocols.","1938-1883","978-1-5386-8088-9","10.1109/ICC.2019.8761441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8761441","","Routing;Routing protocols;Wireless sensor networks;Delays;Topology;Reinforcement learning;Optical sensors","learning (artificial intelligence);multi-agent systems;optical sensors;routing protocols;telecommunication computing;telecommunication network reliability;telecommunication network topology;underwater optical wireless communication;wireless sensor networks","multiagent reinforcement learning routing protocol;underwater optical wireless sensor networks;high-bandwidth underwater applications;underwater optical communication nodes;dynamic network;multiagent system;dynamic route selection;network lifetime;dynamic topology;high transmission rate characteristics;ocean current movement;low delay;adjacent nodes;network lifetime maximization;MARL;AODV routing protocols;QDTR routing protocols;Q-learning","","21","","22","IEEE","15 Jul 2019","","","IEEE","IEEE Conferences"
"Distributed Optimal Tracking Control of Discrete-Time Multiagent Systems via Event-Triggered Reinforcement Learning","Z. Peng; R. Luo; J. Hu; K. Shi; B. K. Ghosh","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; Yangtze Delta Region Institute (Huzhou), University of Electronic Science and Technology of China, Huzhou, China; Department of Mathematics and Statistics, Texas Tech University, Lubbock, TX, USA","IEEE Transactions on Circuits and Systems I: Regular Papers","29 Aug 2022","2022","69","9","3689","3700","In this paper, an event-triggered optimal tracking control of discrete-time multi-agent systems is addressed by using reinforcement learning. In contrast to traditional reinforcement learning-based methods for optimal coordination and control of multi-agent systems with a time-triggered control mechanism, an event-triggered mechanism is proposed to update the controller only when the designed events are triggered, which reduces the computational burden and transmission load. The stability analysis of the closed-loop multi-agent systems with event-triggered controller is described. Further, to implement the proposed scheme, an actor-critic neural network learning structure is proposed to approximate performance indices and to on-line learn the event-triggered optimal control. During the training process, event-triggered weight tuning law has been designed, wherein the weight parameters of the actor neural networks are adjusted only during triggering instances compared with traditional methods with fixed updating periods. Further, a convergence analysis of the actor-critic neural network is provided via Lyapunov method. Finally, two simulation examples show the effectiveness and performance of the obtained event-triggered reinforcement learning controller.","1558-0806","","10.1109/TCSI.2022.3177407","National Natural Science Foundation of China(grant numbers:62003073,71503206,61473061); China Scholarship Council(grant numbers:201906070069); China Postdoctoral Science Foundation(grant numbers:2021M700695,2020M683274,2021T140092); Department of Science and Technology of Sichuan Province(grant numbers:2020YFSY0012,21YYJC0469); Open Research Project of the State Key Laboratory of Industrial Control Technology, Zhejiang University(grant numbers:ICT2021B38); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9786773","Optimal tracking control;event-triggered mechanism;multi-agent systems;reinforcement learning;actor-critic neural networks","Multi-agent systems;Reinforcement learning;Optimal control;Stability analysis;Directed graphs;Convergence;Tuning","adaptive control;closed loop systems;continuous time systems;control system synthesis;discrete time systems;distributed control;dynamic programming;learning systems;Lyapunov methods;multi-agent systems;networked control systems;neurocontrollers;nonlinear control systems;optimal control;reinforcement learning;stability","distributed optimal tracking control;discrete-time multiagent systems;event-triggered optimal tracking control;reinforcement learning-based methods;optimal coordination;time-triggered control mechanism;transmission load;closed-loop multiagent systems;actor-critic neural network learning structure;event-triggered optimal control;event-triggered weight tuning law;actor neural networks;event-triggered reinforcement learning controller;performance indices;Lyapunov method","","20","","47","IEEE","2 Jun 2022","","","IEEE","IEEE Journals"
"Virtual-Action-Based Coordinated Reinforcement Learning for Distributed Economic Dispatch","D. Li; L. Yu; N. Li; F. Lewis","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; UTA Research Institute, The University of Texas at Arlington, Fort Worth, TX, USA","IEEE Transactions on Power Systems","19 Oct 2021","2021","36","6","5143","5152","A unified distributed reinforcement learning (RL) solution is offered for both static and dynamic economic dispatch problems (EDPs). Each agent is assigned with a fixed, discrete, virtual action set, and a projection method generates the feasible, actual actions to satisfy the constraints. A distributed algorithm, based on singularly perturbed system, solves the projection problem. A distributed form of Hysteretic Q-learning achieves coordination among agents. Therein, the Q-values are developed based on the virtual actions, while the rewards are produced by the projected actual actions. The proposed algorithm deals with continuous action space and power loads without using function approximations. Theoretical analysis and comparative simulation studies verify algorithm's convergence and optimality.","1558-0679","","10.1109/TPWRS.2021.3070161","National Natural Science Foundation of China(grant numbers:61773260,61590925); Key R&D Program of the Ministry of Science and Technology(grant numbers:2018YFB1305902); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9392288","Distributed reinforcement learning;economic dispatch;multi-agent system;singularly perturbed system","Power system dynamics;Multi-agent systems;Wind power generation;Power generation dispatch;Reinforcement learning","learning (artificial intelligence);multi-agent systems;optimisation;power generation dispatch;power generation economics;singularly perturbed systems","unified distributed reinforcement learning solution;static economic dispatch problems;dynamic economic dispatch problems;projection method;distributed algorithm;singularly perturbed system;projection problem;virtual actions;projected actual actions;continuous action space;virtual-action-based coordinated reinforcement learning;distributed economic dispatch;Hysteretic Q-learning","","18","","35","IEEE","31 Mar 2021","","","IEEE","IEEE Journals"
