@inproceedings{10.1145/3195970.3199855,
author = {Sadasivam, Shankar and Chen, Zhuo and Lee, Jinwon and Jain, Rajeev},
title = {Efficient Reinforcement Learning for Automating Human Decision-Making in SoC Design},
year = {2018},
isbn = {9781450357005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195970.3199855},
doi = {10.1145/3195970.3199855},
abstract = {The exponential growth in PVT corners due to Moore's law scaling, and the increasing demand for consumer applications and longer battery life in mobile devices, has ushered in significant cost and power-related challenges for designing and productizing mobile chips within a predictable schedule. Two main reasons for this are the reliance on human decision-making to achieve the desired performance within the target area and power budget, and significant increases in complexity of the human decision-making space. The problem is that to-date human design experience has not been replaced by design automation tools, and tasks requiring experience of past designs are still being performed manually.In this paper we investigate how machine learning may be applied to develop tools that learn from experience just like human designers, thus automating tasks that still require human intervention. The potential advantage of the machine learning approach is the ability to scale with increasing complexity and therefore hold the design-time constant with same manpower.Reinforcement Learning (RL) is a machine learning technique that allows us to mimic a human designers' ability to learn from experience and automate human decision-making, without loss in quality of the design, while making the design time independent of the complexity. In this paper we show how manual design tasks can be abstracted as RL problems. Based on the experience with applying RL to one of these problems, we show that RL can automatically achieve results similar to human designs, but in a predictable schedule. However, a major drawback is that the RL solution can require a prohibitively large number of iterations for training. If efficient training techniques can be developed for RL, it holds great promise to automate tasks requiring human experience. In this paper we present a Bayesian Optimization technique for reducing the RL training time.},
booktitle = {Proceedings of the 55th Annual Design Automation Conference},
articleno = {37},
numpages = {6},
location = {San Francisco, California},
series = {DAC '18}
}

@inproceedings{10.1145/3330089.3330134,
author = {Khelifa, Boudjemaa and Laouar, Mohamed Ridda},
title = {Multi-Agent Reinforcement Learning for Urban Projects Planning},
year = {2018},
isbn = {9781450361019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330089.3330134},
doi = {10.1145/3330089.3330134},
abstract = {Actually, planners always need more and more updated plans to satisfy eventual changes. Those plans usually bring immediate and even sustainable solutions. According to the nature of problems, and decision-makers yearnings, the allocated time to establish plans is still tight. Therefore, adequate technics and methods are suitable to tackle this problem. Recently, a primer research field called Machine learning (ML), whose technics are based on learning by studying data or by applying known rules to categorize things, to predict outcomes, to identify patterns, or to detect unexpected behaviors. Reinforcement learning (RL) is an active research field of ML, based on learning how to map situations to actions, so as to maximize a numerical reward. By employing RL methods, the aim is to provide better plans for urban projects, wherein they are modeled to form a multi-agents system, acting cooperatively and optimally.},
booktitle = {Proceedings of the 7th International Conference on Software Engineering and New Technologies},
articleno = {33},
numpages = {5},
keywords = {Multi-agent Systems, Reinforcement Learning, Urban Projects Planning, Sustainable Development},
location = {Hammamet, Tunisia},
series = {ICSENT 2018}
}

@article{10.1145/3449356,
author = {Balakrishnan, Aravind and Lee, Jaeyoung and Gaurav, Ashish and Czarnecki, Krzysztof and Sedwards, Sean},
title = {Transfer Reinforcement Learning for Autonomous Driving: From WiseMove to WiseSim},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-3301},
url = {https://doi.org/10.1145/3449356},
doi = {10.1145/3449356},
abstract = {Reinforcement learning (RL) is an attractive way to implement high-level decision-making policies for autonomous driving, but learning directly from a real vehicle or a high-fidelity simulator is variously infeasible. We therefore consider the problem of transfer reinforcement learning and study how a policy learned in a simple environment using WiseMove can be transferred to our high-fidelity simulator, WiseMove. WiseMove is a framework to study safety and other aspects of RL for autonomous driving. WiseMove accurately reproduces the dynamics and software stack of our real vehicle. We find that the accurately modelled perception errors in WiseMove contribute the most to the transfer problem. These errors, when even naively modelled in WiseMove, provide an RL policy that performs better in WiseMove than a hand-crafted rule-based policy. Applying domain randomization to the environment in WiseMove yields an even better policy. The final RL policy reduces the failures due to perception errors from 10\% to 2.75\%. We also observe that the RL policy has significantly less reliance on velocity compared to the rule-based policy, having learned that its measurement is unreliable.},
journal = {ACM Trans. Model. Comput. Simul.},
month = {jul},
articleno = {15},
numpages = {26},
keywords = {deep reinforcement learning, policy distillation, autonomous driving, Transfer reinforcement learning}
}

@inproceedings{10.1145/3310986.3311004,
author = {Yao, Xinlin and Lu, Xianghua},
title = {Optimizing Digital Coupon Assignment Using Constrained Reinforcement Learning},
year = {2019},
isbn = {9781450366120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310986.3311004},
doi = {10.1145/3310986.3311004},
abstract = {Coupon marketing is a traditional but effective way to retain customers and stimulate new purchases. Recently, digital coupons have been widely used in e-commerce and distributed to almost everyone. However, the decision on when and whom to issue the coupon is often based on managers' experience and calling for optimization and automation. Collaborated with a leading e-commerce platform, we propose an exploratory constrained reinforcement learning modeling to optimize digital coupon distribution policy under the constraint of maximum offering number. Our experimental results showed that the optimal policy could increase the cumulative total sales about 6\% comparing to the original policy of the platform. This work enriches the applications of reinforcement learning in real-world business practices and provides useful implications for future study on constrained reinforcement learning.},
booktitle = {Proceedings of the 3rd International Conference on Machine Learning and Soft Computing},
pages = {143–147},
numpages = {5},
keywords = {Constrained reinforcement learning, Digital coupon assignment, Online marketing},
location = {Da Lat, Viet Nam},
series = {ICMLSC '19}
}

@inproceedings{10.5555/3398761.3398909,
author = {Singh, Arambam James and Kumar, Akshat and Lau, Hoong Chuin},
title = {Hierarchical Multiagent Reinforcement Learning for Maritime Traffic Management},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Increasing global maritime traffic coupled with rapid digitization and automation in shipping mandate developing next generation maritime traffic management systems to mitigate congestion, increase safety of navigation, and avoid collisions in busy and geographically constrained ports (such as Singapore's). To achieve these objectives, we model the maritime traffic as a large multiagent system with individual vessels as agents, and VTS (Vessel Traffic Service) authority as a regulatory agent. We develop a hierarchical reinforcement learning approach where vessels first select a high level action based on the underlying traffic flow, and then select the low level action that determines their future speed. We exploit the nature of collective interactions among agents to develop a policy gradient approach that can scale up to large real world problems. We also develop an effective multiagent credit assignment scheme that significantly improves the convergence of policy gradient. Extensive empirical results on synthetic and real world data from one of the busiest port in the world show that our approach consistently performs significantly better than the previous best approach.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1278–1286},
numpages = {9},
keywords = {maritime traffic management, reinforcement learning, multiagent system},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/1068009.1068325,
author = {Browne, Will and Scott, Dan},
title = {An Abstraction Agorithm for Genetics-Based Reinforcement Learning},
year = {2005},
isbn = {1595930108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1068009.1068325},
doi = {10.1145/1068009.1068325},
abstract = {Abstraction is a higher order cognitive ability that facilitates the production of rules that are independent of their associations. Experience from real-world data-mining has shown the need for such higher level rules. The game of Connect 4 is both multistep and complex, so standard Q-learning and Learning Classifier Systems perform poorly. The introduction of a novel Abstraction algorithm into an LCS is shown to improve performance in the evolution of playing strategies.},
booktitle = {Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation},
pages = {1875–1882},
numpages = {8},
keywords = {abstraction, genetics-based machine learning, learning classifier systems},
location = {Washington DC, USA},
series = {GECCO '05}
}

@inproceedings{10.1145/3473682.3480262,
author = {Kuen, Jakob and Schartm\"{u}ller, Clemens and Wintersberger, Philipp},
title = {The TOR Agent:Optimizing Driver Take-Over with Reinforcement Learning},
year = {2021},
isbn = {9781450386418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473682.3480262},
doi = {10.1145/3473682.3480262},
abstract = {Various factors influence drivers’ response to Take-Over Requests in automated driving, and a wide range of designs have been proposed to improve transitions. Still, little research has investigated how systems could deliver Take-Over Requests at the best moment in time. In this paper, we sketch the idea of a reinforcement learning agent that learns to deliver Take-Over Requests at the right time so that drivers’ performance gets optimized, which could help to increase driving safety. We implemented such a system in Unity to evaluate this approach using a simple driver model. Our agent receives coordinates of the upcoming road segment and learns to deliver a Take-Over Request at an appropriate moment within a short time window. The reward function is composed to minimize the lateral deviation in the subsequent phase of manual driving. The initial results obtained are promising, and we will evaluate the concept with real human users soon.},
booktitle = {13th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {47–52},
numpages = {6},
keywords = {intelligent user interfaces, conditional automation, take-over requests, adaptive automation, automated driving},
location = {Leeds, United Kingdom},
series = {AutomotiveUI '21 Adjunct}
}

@article{10.1145/3577204,
author = {Boudi, Zakaryae and Wakrime, Abderrahim Ait and Toub, Mohamed and Haloua, Mohamed},
title = {A Deep Reinforcement Learning Framework with Formal Verification},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1145/3577204},
doi = {10.1145/3577204},
abstract = {Artificial Intelligence (AI) and data are reshaping organizations and businesses. Human Resources (HR) management and talent development make no exception, as they tend to involve more automation and growing quantities of data. Because this brings implications on workforce, career transparency, and equal opportunities, overseeing what fuels AI and analytical models, their quality standards, integrity, and correctness becomes an imperative for those aspiring to such systems. Based on an ontology transformation to B-machines, this article presents an approach to constructing a valid and error-free career agent with Deep Reinforcement Learning (DRL). In short, the agent's policy is built on a framework we called Multi State-Actor (MuStAc) using a decentralized training approach. Its purpose is to predict both relevant and valid career steps to employees, based on their profiles and company pathways (observations). Observations can comprise various data elements such as the current occupation, past experiences, performance, skills, qualifications, and so on. The policy takes in all these observations and outputs the next recommended career step, in an environment set as the combination of an HR ontology and an Event-B model, which generates action spaces with respect to formal properties. The Event-B model and formal properties are derived using OWL to B transformation.},
journal = {Form. Asp. Comput.},
month = {mar},
articleno = {5},
numpages = {17},
keywords = {Model Transformation, AI Control, Event-B, Formal Verification, Safe RL, Safe AI, Atelier B}
}

@inproceedings{10.5555/3466184.3466533,
author = {Gros, Timo P. and Gro\ss{}, Joschka and Wolf, Verena},
title = {Real-Time Decision Making for a Car Manufacturing Process Using Deep Reinforcement Learning},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Computer simulations of manufacturing processes are in widespread use for optimizing production planning and order processing. If unforeseeable events are common, real-time decisions are necessary to maximize the performance of the manufacturing process. Pre-trained AI-based decision support offers promising opportunities for such time-critical production processes. Here, we explore the effectiveness of deep reinforcement learning for real-time decision making in a car manufacturing process. We combine a simulation model of a central production part, the line buffer, with deep reinforcement learning algorithms, in particular with deep Q-Learning and Monte Carlo tree search. We simulate two different versions of the buffer, a single-agent and a multi-agent one, to generate large amounts of data and train neural networks to represent near-optimal strategies. Our results show that deep reinforcement learning performs extremely well and the resulting strategies provide near-optimal decisions in real-time, while alternative approaches are either slow or give strategies of poor quality.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3032–3044},
numpages = {13},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/3368756.3369038,
author = {Youssef, Fenjiro and Houda, Benbrahim},
title = {Deep Reinforcement Learning with External Control: Self-Driving Car Application},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3369038},
doi = {10.1145/3368756.3369038},
abstract = {A Self-driving car using an end-to-end deep reinforcement learning[1] algorithms trained on lane-keeping task performs well in circuits that don't need decision making but cannot deal with situations like choosing to turn left or right in an upcoming crossroads, deciding when to leave a traffic circle or toward which path/destination to go. In this paper we propose a new Deep Reinforcement Learning architecture that supports external command as high-level input, that we call Steered Deep Reinforcement Learning (SDRL), we apply the SDRL architecture on the Deep Deterministic Policy Gradient algorithm DDPG and use CARLA a High-fidelity realistic driving simulator as a testbed environment to train and experiment the new model, since testing in ground truth turns out to be costly and risky. The Steered DDPG (SDDPG) model performs well on the road/roundabouts and responds correctly to the external commands that allow the driving agent to take the right turns.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {58},
numpages = {7},
keywords = {self-driving car, external commands, deep learning, reinforcement learning},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@inproceedings{10.1145/3505688.3505693,
author = {Wang, Junjie and Zhang, Qichao and Zhao, Dongbin},
title = {Benchmarking Lane-Changing Decision-Making for Deep Reinforcement Learning},
year = {2022},
isbn = {9781450385855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505688.3505693},
doi = {10.1145/3505688.3505693},
abstract = {The development of autonomous driving has attracted extensive attention in recent years, and it is essential to evaluate the performance of autonomous driving. However, testing on the road is expensive and inefficient. Virtual testing is the primary way to validate and verify self-driving cars, and the basis of virtual testing is to build simulation scenarios. In this paper, we propose a training, testing, and evaluation pipeline for the lane-changing task from the perspective of deep reinforcement learning. First, we design lane change scenarios for training and testing, where the test scenarios include stochastic and deterministic parts. Then, we deploy a set of benchmarks consisting of learning and non-learning approaches. We train several state-of-the-art deep reinforcement learning methods in the designed training scenarios and provide the benchmark metrics evaluation results of the trained models in the test scenarios. The designed lane-changing scenarios and benchmarks are both opened to provide a consistent experimental environment for the lane-changing task1 .},
booktitle = {Proceedings of the 7th International Conference on Robotics and Artificial Intelligence},
pages = {26–32},
numpages = {7},
keywords = {Reinforcement learning, Lane change scenarios, Autonomous driving},
location = {Guangzhou, China},
series = {ICRAI '21}
}

@inproceedings{10.1145/1390156.1390189,
author = {Doshi, Finale and Pineau, Joelle and Roy, Nicholas},
title = {Reinforcement Learning with Limited Reinforcement: Using Bayes Risk for Active Learning in POMDPs},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390189},
doi = {10.1145/1390156.1390189},
abstract = {Partially Observable Markov Decision Processes (POMDPs) have succeeded in planning domains that require balancing actions that increase an agent's knowledge and actions that increase an agent's reward. Unfortunately, most POMDPs are defined with a large number of parameters which are difficult to specify only from domain knowledge. In this paper, we present an approximation approach that allows us to treat the POMDP model parameters as additional hidden state in a "model-uncertainty" POMDP. Coupled with model-directed queries, our planner actively learns good policies. We demonstrate our approach on several POMDP problems.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {256–263},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@article{10.1145/2134203.2134206,
author = {Kastanis, Iason and Slater, Mel},
title = {Reinforcement Learning Utilizes Proxemics: An Avatar Learns to Manipulate the Position of People in Immersive Virtual Reality},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1544-3558},
url = {https://doi.org/10.1145/2134203.2134206},
doi = {10.1145/2134203.2134206},
abstract = {A reinforcement learning (RL) method was used to train a virtual character to move participants to a specified location. The virtual environment depicted an alleyway displayed through a wide field-of-view head-tracked stereo head-mounted display. Based on proxemics theory, we predicted that when the character approached within a personal or intimate distance to the participants, they would be inclined to move backwards out of the way. We carried out a between-groups experiment with 30 female participants, with 10 assigned arbitrarily to each of the following three groups: In the Intimate condition the character could approach within 0.38m and in the Social condition no nearer than 1.2m. In the Random condition the actions of the virtual character were chosen randomly from among the same set as in the RL method, and the virtual character could approach within 0.38m. The experiment continued in each case until the participant either reached the target or 7 minutes had elapsed. The distributions of the times taken to reach the target showed significant differences between the three groups, with 9 out of 10 in the Intimate condition reaching the target significantly faster than the 6 out of 10 who reached the target in the Social condition. Only 1 out of 10 in the Random condition reached the target. The experiment is an example of applied presence theory: we rely on the many findings that people tend to respond realistically in immersive virtual environments, and use this to get people to achieve a task of which they had been unaware. This method opens up the door for many such applications where the virtual environment adapts to the responses of the human participants with the aim of achieving particular goals.},
journal = {ACM Trans. Appl. Percept.},
month = {mar},
articleno = {3},
numpages = {15},
keywords = {proxemics, avatars, Human-computer interaction, virtual characters}
}

@inproceedings{10.1145/3538969.3538994,
author = {Iliou, Christos and Kostoulas, Theodoros and Tsikrika, Theodora and Katos, Vasilios and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
title = {Web Bot Detection Evasion Using Deep Reinforcement Learning},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3538994},
doi = {10.1145/3538969.3538994},
abstract = {Web bots are vital for the web as they can be used to automate several actions, some of which would have otherwise been impossible or very time consuming. These actions can be benign, such as website testing and web indexing, or malicious, such as unauthorised content scraping, scalping, vulnerability scanning, and more. To detect malicious web bots, recent approaches examine the visitors’ fingerprint and behaviour. For the latter, several values (i.e., features) are usually extracted from visitors’ web logs and used as input to train machine learning models. In this research we show that web bots can use recent advances in machine learning, and, more specifically, Reinforcement Learning (RL), to effectively evade behaviour-based detection techniques. To evaluate these evasive bots, we examine (i) how well they can evade a pre-trained bot detection framework, (ii) how well they can still evade detection after the detection framework is re-trained on new behaviours generated from the evasive web bots, and (iii) how bots perform if re-trained again on the re-trained detection framework. We show that web bots can repeatedly evade detection and adapt to the re-trained detection framework to showcase the importance of considering such types of bots when designing web bot detection frameworks.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {15},
numpages = {10},
keywords = {advanced web bots, evasive web bots, web logs, reinforcement learning, web bot detection},
location = {Vienna, Austria},
series = {ARES '22}
}

@inproceedings{10.1145/3171221.3171289,
author = {Clark-Turner, Madison and Begum, Momotaz},
title = {Deep Reinforcement Learning of Abstract Reasoning from Demonstrations},
year = {2018},
isbn = {9781450349536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3171221.3171289},
doi = {10.1145/3171221.3171289},
abstract = {Extracting a set of generalizable rules that govern the dynamics of complex, high-level interactions between humans based only on observations is a high-level cognitive ability. Mastery of this skill marks a significant milestone in the human developmental process. A key challenge in designing such an ability in autonomous robots is discovering the relationships among discriminatory features. Identifying features in natural scenes that are representative of a particular event or interaction (i.e. »discriminatory features») and then discovering the relationships (e.g., temporal/spatial/spatio-temporal/causal) among those features in the form of generalized rules are non-trivial problems. They often appear as a »chicken-and-egg» dilemma. This paper proposes an end-to-end learning framework to tackle these two problems in the context of learning generalized, high-level rules of human interactions from structured demonstrations. We employed our proposed deep reinforcement learning framework to learn a set of rules that govern a behavioral intervention session between two agents based on observations of several instances of the session. We also tested the accuracy of our framework with human subjects in diverse situations.},
booktitle = {Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {160–168},
numpages = {9},
keywords = {abstract reasoning, learning from demonstration, deep learning},
location = {Chicago, IL, USA},
series = {HRI '18}
}

@inproceedings{10.1145/3398329.3398334,
author = {Wang, Shuaibing and Ren, Jingzheng},
title = {Playing T-Rex Rush with Deep Reinforcement Learning},
year = {2020},
isbn = {9781450377713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3398329.3398334},
doi = {10.1145/3398329.3398334},
abstract = {After AlphaGO defeated the world Go champion Li Shishi in early 2016, the entire industry was excited about it. More and more scholars realized the importance of deep reinforcement learning [1] in the field of artificial intelligence. In addition, Google has also made two important research results. The deep reinforcement learning algorithm based on the Atari video game and the Go Artificial Intelligence AlphaGo Zero, these achievements are tantamount to setting off a revolution in artificial intelligence learning algorithms.In this paper, considering the advantages of deep learning in image feature extraction, in order to achieve autonomous control of the T-Rex Rush game, on the basis of the combination of convolutional neural network and reinforcement learning, a deep neural network based on Q-Learning algorithm [2] is proposed. Experiments show that the new model can fully learn the control strategy, cope with more complex situations, and implement realtime policy control, and exceed the score of human players in the T-Rex Rush game.},
booktitle = {Proceedings of the 2020 International Conference on Computing, Networks and Internet of Things},
pages = {31–35},
numpages = {5},
location = {Sanya, China},
series = {CNIOT '20}
}

@inproceedings{10.1145/3485447.3512258,
author = {Mosallanezhad, Ahmadreza and Karami, Mansooreh and Shu, Kai and Mancenido, Michelle V. and Liu, Huan},
title = {Domain Adaptive Fake News Detection via Reinforcement Learning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512258},
doi = {10.1145/3485447.3512258},
abstract = {With social media being a major force in information consumption, accelerated propagation of fake news has presented new challenges for platforms to distinguish between legitimate and fake news. Effective fake news detection is a non-trivial task due to the diverse nature of news domains and expensive annotation costs. In this work, we address the limitations of existing automated fake news detection models by incorporating auxiliary information (e.g., user comments and user-news interactions) into a novel reinforcement learning-based model called REinforced Adaptive Learning Fake News Detection (REAL-FND). REAL-FND exploits cross-domain and within-domain knowledge that makes it robust in a target domain, despite being trained in a different source domain. Extensive experiments on real-world datasets illustrate the effectiveness of the proposed model, especially when limited labeled data is available in the target domain.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3632–3640},
numpages = {9},
keywords = {neural networks, reinforcement learning, domain adaptation, disinformation},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3407703.3407726,
author = {Zhang, Taidong and Li, Xianze and Li, Xudong and Liu, Guanghui and Tian, Miao},
title = {Reinforcement Learning Based Strategy Selection in StarCraft: Brood War},
year = {2020},
isbn = {9781450377270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407703.3407726},
doi = {10.1145/3407703.3407726},
abstract = {Artificial intelligence (AI) applied on real-time strategy (RTS) games, such as StarCraft has become a hot topic in recent years. Currently, many researches mainly focused on the aspects of applying reinforcement learning (RL) on the aspects, such as the building order, map analysis, macro- and micro-management, etc. This paper innovatively proposes a mechanism applying reinforcement learning on pre-designed game strategies, which can be considered as human experience and combinations of both the macro- and micro-management. In our work, game strategies are formed by many sub-strategy modules that consist a series of orderly actions corresponding to certain components and game stages. Further, by applying RL on the strategy modules, the proposed mechanism can effectively improve selection of the game strategies, and balance the AI decision and human experience. Our bot was tested against some bots selected from AIIDE 2017-2018. The results show that with the RL framework, our bot can improve the overall winning rate from 42.79\% to 59.36\%. Particularly, the improvements on opponent 32.06\%, 25.18\%, and 30.32\%, against the Zerg bot, Arrakhammer, the Protoss bot, Xelnaga, the Terran bot, UAlbertaBot.},
booktitle = {Proceedings of the 2020 Artificial Intelligence and Complex Systems Conference},
pages = {121–128},
numpages = {8},
keywords = {AI, Strategy Selection, Reinforcement Learning, StarCraft},
location = {Wuhan, China},
series = {AICSconf '20}
}

@inproceedings{10.1109/ICSE43902.2021.00048,
author = {Zheng, Yan and Liu, Yi and Xie, Xiaofei and Liu, Yepang and Ma, Lei and Hao, Jianye and Liu, Yang},
title = {Automatic Web Testing Using Curiosity-Driven Reinforcement Learning},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00048},
doi = {10.1109/ICSE43902.2021.00048},
abstract = {Web testing has long been recognized as a notoriously difficult task. Even nowadays, web testing still heavily relies on manual efforts while automated web testing is far from achieving human-level performance. Key challenges in web testing include dynamic content update and deep bugs hiding under complicated user interactions and specific input values, which can only be triggered by certain action sequences in the huge search space. In this paper, we propose WebExplor, an automatic end-to-end web testing framework, to achieve an adaptive exploration of web applications. WebExplor adopts curiosity-driven reinforcement learning to generate high-quality action sequences (test cases) satisfying temporal logical relations. Besides, WebExplor incrementally builds an automaton during the online testing process, which provides high-level guidance to further improve the testing efficiency. We have conducted comprehensive evaluations of WebExplor on six real-world projects, a commercial SaaS web application, and performed an in-the-wild study of the top 50 web applications in the world. The results demonstrate that in most cases WebExplor can achieve significantly higher failure detection rate, code coverage and efficiency than existing state-of-the-art web testing techniques. WebExplor also detected 12 previously unknown failures in the commercial web application, which have been confirmed and fixed by the developers. Furthermore, our in-the-wild study further uncovered 3,466 exceptions and errors.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {423–435},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3173386.3177537,
author = {Clark-Turner, Madison and Begum, Momotaz},
title = {Deep Reinforcement Learning of Abstract Reasoning from Demonstrations},
year = {2018},
isbn = {9781450356152},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173386.3177537},
doi = {10.1145/3173386.3177537},
abstract = {We designed a Deep Q-Network (DQN) that learns to perform high-level reasoning in a Learning from Demonstration (LfD) domain involving the analysis of human responses. We test our system by having a NAO humanoid robot automatically deliver a behavioral intervention designed to teach social skills to individuals with Autism Spectrum Disorder (ASD). Our model extracts relevant features from the multi-modal input of tele-operated demonstrations in order to deliver the intervention correctly to novel participants.},
booktitle = {Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {372},
numpages = {1},
keywords = {abstract reasoning, learning from demonstration, deep learning},
location = {Chicago, IL, USA},
series = {HRI '18}
}

@inproceedings{10.5555/3463952.3464010,
author = {Du, Yali and Liu, Bo and Moens, Vincent and Liu, Ziqi and Ren, Zhicheng and Wang, Jun and Chen, Xu and Zhang, Haifeng},
title = {Learning Correlated Communication Topology in Multi-Agent Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Communication improves the efficiency and convergence of multi-agent learning. Existing study of agent communication has been limited on predefined fixed connections. While an attention mechanism exists and is useful for scheduling the communication between agents, it, however, largely ignores the dynamical nature of communication and thus the correlation between agents' connections. In this work, we adopt a normalizing flow to encode correlation between agents interactions. The dynamical communication topology is directly learned by maximizing the agent rewards. In our end-to-end formulation, the communication structure is learned by considering it as a hidden dynamical variable. We realize centralized training of critics and graph reasoning policy, and decentralized execution from local observation and message that are received through the learned dynamical communication topology. Experiments on cooperative navigation in the particle world and adaptive traffic control tasks demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {456–464},
numpages = {9},
keywords = {reinforcement learning, communication topology, multi-agent systems},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.5555/3545946.3598953,
author = {Yang, Chen and Yang, Guangkai and Zhang, Junge},
title = {Learning Individual Difference Rewards in Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We investigate explicit solutions to multi-agent credit assignment problem. Specifically, we assign each agent individual difference rewards in addition to the team reward as to distinguish the contribution of different agents to the team. We present a novel reward decomposition network to estimate the influence of each agent's action on the team reward, and distribute difference rewards accordingly. Furthermore, we combine difference rewards with actor-critic framework and propose a new approach called learning individual difference rewards (LIDR). We evaluate LIDR on a set of StarCraft II micromanagement problems. Results show that LIDR significantly outperforms previous state-of-the-art methods.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2418–2420},
numpages = {3},
keywords = {multi-agent systems, reward shaping, credit assignment},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3589737.3605979,
author = {Crowder, Douglas Cale and Smith, John Darby and Cardwell, Suma George},
title = {Deep Reinforcement Learning Methods for Discovering Novel Neuromorphic Devices},
year = {2023},
isbn = {9798400701757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589737.3605979},
doi = {10.1145/3589737.3605979},
abstract = {Innovation in the field of neuromorphic computing is characterized by long periods of slow, steady growth that are punctuated by periods of rapid discovery. In order to accelerate discovery in the field of neuromorphic computing and disrupt the process of slow and steady growth, we are proposing a reinforcement learning architecture that is able to automatically construct simple circuits. Rather than providing the reinforcement learning agent with specifications for existing devices, we ask the reinforcement learning agent to provide specifications for novel devices that, if fabricated, can solve a specified problem. We show that, by slightly changing the problem statement, we can cause the reinforcement learning agent to produce different device specifications. Ultimately, we expect that by generating many possible solutions, the reinforcement learning agent will accelerate innovation by stimulating insight into potential solutions for problems.},
booktitle = {Proceedings of the 2023 International Conference on Neuromorphic Systems},
articleno = {37},
numpages = {8},
location = {Santa Fe, NM, USA},
series = {ICONS '23}
}

@inproceedings{10.1145/336595.337554,
author = {Pendrith, Mark D.},
title = {Distributed Reinforcement Learning for a Traffic Engineering Application},
year = {2000},
isbn = {1581132301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/336595.337554},
doi = {10.1145/336595.337554},
booktitle = {Proceedings of the Fourth International Conference on Autonomous Agents},
pages = {404–411},
numpages = {8},
location = {Barcelona, Spain},
series = {AGENTS '00}
}

@inproceedings{10.1145/3474085.3475395,
author = {Yan, Huanqian and Wei, Xingxing},
title = {Efficient Sparse Attacks on Videos Using Reinforcement Learning},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475395},
doi = {10.1145/3474085.3475395},
abstract = {More and more deep neural network models have been deployed in real-time video systems. However, it is proved that deep models are susceptible to the crafted adversarial examples. The adversarial examples are imperceptible and can make the normal deep models misclassify them. Although there exist a few works aiming at the adversarial examples of video recognition in the black-box attack mode, most of them need large perturbations or hundreds of thousands of queries. There are still lack of effective adversarial methods to produce adversarial videos with small perturbations and limited query numbers at the same time.In this paper, an efficient and powerful method is proposed for adversarial video attacks in the black-box attack mode. The proposed method is based on Reinforcement Learning (RL) like the previous work, i.e. using the agent in RL to adaptively find the sparse key frames to add perturbations. The key difference is that we design the new reward functions based on the loss reduction and the perturbation increment, and thus propose an efficient update mechanism to guide the agent to finish the attacks with smaller perturbations and fewer query numbers. The proposed algorithm has a new working mechanism. It is simple, efficient, and effective. Extensive experiments show our method has a good trade-off between the perturbation amplitude and the query numbers. Compared with the state-of-the-art algorithms, it has reduced 65.75\% query numbers without image quality loss in the un-targeted attacks and simultaneously reduced 22.47\% perturbations and 54.77\% query numbers in the targeted attacks.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2326–2334},
numpages = {9},
keywords = {black-box video attack, action recognition, reinforcement learning, adversarial examples},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3377930.3389842,
author = {Francon, Olivier and Gonzalez, Santiago and Hodjat, Babak and Meyerson, Elliot and Miikkulainen, Risto and Qiu, Xin and Shahrzad, Hormoz},
title = {Effective Reinforcement Learning through Evolutionary Surrogate-Assisted Prescription},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3389842},
doi = {10.1145/3377930.3389842},
abstract = {There is now significant historical data available on decision making in organizations, consisting of the decision problem, what decisions were made, and how desirable the outcomes were. Using this data, it is possible to learn a surrogate model, and with that model, evolve a decision strategy that optimizes the outcomes. This paper introduces a general such approach, called Evolutionary Surrogate-Assisted Prescription, or ESP. The surrogate is, for example, a random forest or a neural network trained with gradient descent, and the strategy is a neural network that is evolved to maximize the predictions of the surrogate model. ESP is further extended in this paper to sequential decision-making tasks, which makes it possible to evaluate the framework in reinforcement learning (RL) benchmarks. Because the majority of evaluations are done on the surrogate, ESP is more sample efficient, has lower variance, and lower regret than standard RL approaches. Surprisingly, its solutions are also better because both the surrogate and the strategy network regularize the decision making behavior. ESP thus forms a promising foundation to decision optimization in real-world problems.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {814–822},
numpages = {9},
keywords = {neural networks, reinforcement learning, surrogate-assisted evolution, genetic algorithms, decision making},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3307650.3322259,
author = {Li, Youjie and Liu, Iou-Jen and Yuan, Yifan and Chen, Deming and Schwing, Alexander and Huang, Jian},
title = {Accelerating Distributed Reinforcement Learning with In-Switch Computing},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322259},
doi = {10.1145/3307650.3322259},
abstract = {Reinforcement learning (RL) has attracted much attention recently, as new and emerging AI-based applications are demanding the capabilities to intelligently react to environment changes. Unlike distributed deep neural network (DNN) training, the distributed RL training has its unique workload characteristics - it generates orders of magnitude more iterations with much smaller sized but more frequent gradient aggregations. More specifically, our study with typical RL algorithms shows that their distributed training is latency critical and that the network communication for gradient aggregation occupies up to 83.2\% of the execution time of each training iteration.In this paper, we present iSwitch, an in-switch acceleration solution that moves the gradient aggregation from server nodes into the network switches, thus we can reduce the number of network hops for gradient aggregation. This not only reduces the end-to-end network latency for synchronous training, but also improves the convergence with faster weight updates for asynchronous training. Upon the in-switch accelerator, we further reduce the synchronization overhead by conducting on-the-fly gradient aggregation at the granularity of network packets rather than gradient vectors. Moreover, we rethink the distributed RL training algorithms and also propose a hierarchical aggregation mechanism to further increase the parallelism and scalability of the distributed RL training at rack scale.We implement iSwitch using a real-world programmable switch NetFPGA board. We extend the control and data plane of the programmable switch to support iSwitch without affecting its regular network functions. Compared with state-of-the-art distributed training approaches, iSwitch offers a system-level speedup of up to 3.66\texttimes{} for synchronous distributed training and 3.71\texttimes{} for asynchronous distributed training, while achieving better scalability.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {279–291},
numpages = {13},
keywords = {distributed machine learning, in-network computing, in-switch accelerator, reinforcement learning},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.5555/2772879.2773251,
author = {Efthymiadis, Kyriakos and Kudenko, Daniel},
title = {Knowledge Revision for Reinforcement Learning with Abstract MDPs},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reward shaping is a method often used in RL so as to provide domain knowledge to agents and thus improve learning. An unrealistic assumption however is that the provided knowledge is always correct. This assumption can lead to poor performance in terms of total reward and convergence speed in case it is not met. Previous research demonstrated the use of plan-based reward shaping with knowledge revision in a single agent scenario where agents showed that they can quickly identify and revise erroneous knowledge and thus benefit from more accurate plans. This method however has no mechanism to deal with non-deterministic scenarios and is thus limited to deterministic domains. In this paper we present a method to provide heuristic knowledge via abstract MDPs, coupled with a revision algorithm to manage the cases where the provided domain knowledge is wrong. We show empirically that our method can efficiently revise erroneous knowledge even in the cases where the environment is non-deterministic and also removes the need for some of the assumptions present in plan-based reward shaping with knowledge revision.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {763–770},
numpages = {8},
keywords = {knowledge revision, reinforcement learning, reward shaping},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@article{10.5555/3580523.3580530,
author = {Silverman, Charles and Stewart, Liam and Lopez, Christian},
title = {Reinforcement Learning to Generate 3D Shapes: Towards a Spatial Visualization VR Application},
year = {2022},
issue_date = {November 2022},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {38},
number = {3},
issn = {1937-4771},
abstract = {The objective of this work is to introduce a Reinforcement Learning (RL) approach to automatically generate 3D shapes of different complexities. This is with the goal to help tailor the spatial visualization task of a Virtual Reality (VR) application designed to help develop students' spatial skills. Spatial visualization skills are important skills needed and frequently used in the STEM fields. While VR has been used to help develop these skills, most of the existing applications do not necessarily tailor their content to students' skills level. Automatically generating 3D shapes can help VR applications tailor spatial visualization tasks to the skills level of students. The results of this work indicate that an RL agent is capable of creating an action policy that can generate 3D shapes with complexities similar to a given desired complexity provided. However, the results also show that the task of automatically generating 3D shapes that meet a given complexity is not trivial given the issues of sparsity in the reward space. Nevertheless, this work lays the foundation to leverage RL to automatically generate 3D shapes for VR applications designed to help develop students' spatial visualization skills.},
journal = {J. Comput. Sci. Coll.},
month = {nov},
pages = {61–75},
numpages = {15}
}

@article{10.1145/3511701,
author = {Biagiola, Matteo and Tonella, Paolo},
title = {Testing the Plasticity of Reinforcement Learning-Based Systems},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3511701},
doi = {10.1145/3511701},
abstract = {The dataset available for pre-release training of a machine-learning based system is often not representative of all possible execution contexts that the system will encounter in the field. Reinforcement Learning (RL) is a prominent approach among those that support continual learning, i.e., learning continually in the field, in the post-release phase. No study has so far investigated any method to test the plasticity of RL-based systems, i.e., their capability to adapt to an execution context that may deviate from the training one.We propose an approach to test the plasticity of RL-based systems. The output of our approach is a quantification of the adaptation and anti-regression capabilities of the system, obtained by computing the adaptation frontier of the system in a changed environment. We visualize such frontier as an adaptation/anti-regression heatmap in two dimensions, or as a clustered projection when more than two dimensions are involved. In this way, we provide developers with information on the amount of changes that can be accommodated by the continual learning component of the system, which is key to decide if online, in-the-field learning can be safely enabled or not.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {80},
numpages = {46},
keywords = {Software testing, empirical software engineering, reinforcement learning}
}

@article{10.5555/3122009.3176822,
author = {Mart\'{\i}nez, David and Aleny\`{a}, Guillem and Ribeiro, Tony and Inoue, Katsumi and Torras, Carme},
title = {Relational Reinforcement Learning for Planning with Exogenous Effects},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Probabilistic planners have improved recently to the point that they can solve difficult tasks with complex and expressive models. In contrast, learners cannot tackle yet the expressive models that planners do, which forces complex models to be mostly handcrafted. We propose a new learning approach that can learn relational probabilistic models with both action effects and exogenous effects. The proposed learning approach combines a multivalued variant of inductive logic programming for the generation of candidate models, with an optimization method to select the best set of planning operators to model a problem. We also show how to combine this learner with reinforcement learning algorithms to solve complete problems. Finally, experimental validation is provided that shows improvements over previous work in both simulation and a robotic task. The robotic task involves a dynamic scenario with several agents where a manipulator robot has to clear the tableware on a table. We show that the exogenous effects learned by our approach allowed the robot to clear the table in a more efficient way.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2689–2732},
numpages = {44},
keywords = {robot learning, model-based RL, active learning, probabilistic planning, learning models for planning}
}

@inproceedings{10.5555/3463952.3464024,
author = {Hammond, Lewis and Abate, Alessandro and Gutierrez, Julian and Wooldridge, Michael},
title = {Multi-Agent Reinforcement Learning with Temporal Logic Specifications},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper, we study the problem of learning to satisfy temporal logic specifications with a group of agents in an unknown environment, which may exhibit probabilistic behaviour. From a learning perspective these specifications provide a rich formal language with which to capture tasks or objectives, while from a logic and automated verification perspective the introduction of learning capabilities allows for practical applications in large, stochastic, unknown environments. The existing work in this area is, however, limited. Of the frameworks that consider full linear temporal logic or have correctness guarantees, all methods thus far consider only the case of a single temporal logic specification and a single agent. In order to overcome this limitation, we develop the first multi-agent reinforcement learning technique for temporal logic specifications, which is also novel in its ability to handle multiple specifications. We provide correctness and convergence guarantees for our main algorithm - ALMANAC (Automaton/Logic Multi-Agent Natural Actor-Critic) - even when using function approximation. Alongside our theoretical results, we further demonstrate the applicability of our technique via a set of preliminary experiments.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {583–592},
numpages = {10},
keywords = {multi-agent reinforcement learning, multi-objective reinforcement learning, automata, formal methods, temporal logic},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3319619.3326755,
author = {Hein, Daniel and Udluft, Steffen and Runkler, Thomas A.},
title = {Generating Interpretable Reinforcement Learning Policies Using Genetic Programming},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3326755},
doi = {10.1145/3319619.3326755},
abstract = {The search for interpretable reinforcement learning policies is of high academic and industrial interest. Especially for industrial systems, domain experts are more likely to deploy autonomously learned controllers if they are understandable and convenient to evaluate. Basic algebraic equations are supposed to meet these requirements, as long as they are restricted to an adequate complexity. In our recent work "Interpretable policies for reinforcement learning by genetic programming" published in Engineering Applications of Artificial Intelligence 76 (2018), we introduced the genetic programming for reinforcement learning (GPRL) approach. GPRL uses model-based batch reinforcement learning and genetic programming and autonomously learns policy equations from preexisting default state-action trajectory samples. Experiments on three reinforcement learning benchmarks demonstrate that GPRL can produce human-interpretable policies of high control performance.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {23–24},
numpages = {2},
keywords = {symbolic regression, interpretable reinforcement learning, model-based, genetic programming},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.1145/3411764.3445497,
author = {Todi, Kashyap and Bailly, Gilles and Leiva, Luis and Oulasvirta, Antti},
title = {Adapting User Interfaces with Model-Based Reinforcement Learning},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445497},
doi = {10.1145/3411764.3445497},
abstract = {Adapting an interface requires taking into account both the positive and negative effects that changes may have on the user. A carelessly picked adaptation may impose high costs to the user – for example, due to surprise or relearning effort – or “trap” the process to a suboptimal design immaturely. However, effects on users are hard to predict as they depend on factors that are latent and evolve over the course of interaction. We propose a novel approach for adaptive user interfaces that yields a conservative adaptation policy: It finds beneficial changes when there are such and avoids changes when there are none. Our model-based reinforcement learning method plans sequences of adaptations and consults predictive HCI models to estimate their effects. We present empirical and simulation results from the case of adaptive menus, showing that the method outperforms both a non-adaptive and a frequency-based policy.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {573},
numpages = {13},
keywords = {Predictive Models, Reinforcement Learning, Adaptive User Interfaces, Monte Carlo Tree Search},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.5555/3306127.3331915,
author = {Anastassacos, Nicolas and Musolesi, Mirco},
title = {Towards Decentralized Reinforcement Learning Architectures for Social Dilemmas},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent reinforcement learning has received significant interest in recent years notably due to the advancements made in deep reinforcement learning which have allowed for the developments of new architectures and learning algorithms. In this extended abstract we present our initial efforts towards the development of decentralized architectures for multi-agent systems in order to understand and model societies. More specifically, using social dilemmas as the training ground, we present a novel learning architecture, Learning through Probing (LTP), where agents utilize a probing mechanism to incorporate how their opponent's behavior changes when an agent takes an action.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1776–1777},
numpages = {2},
keywords = {multi-agent systems, reinforcement learning, cooperation},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.5555/3463952.3464046,
author = {Li, Yuyu and Ji, Jianmin},
title = {Parallel Curriculum Experience Replay in Distributed Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Distributed training architectures have been shown to be effective to improve the performance of reinforcement learning algorithms. However, their performances are still poor for problems with sparse rewards, e.g., the scoring task with or without goalkeeper for robots in RoboCup soccer. It is challenging to solve these tasks in reinforcement learning, especially for those that require combining high-level actions with flexible control. To address these challenges, we introduce a distributed training framework with parallel curriculum experience replay that can collect different experiences in parallel and then automatically identify the difficulty of these subtasks. Experiments on the domain of simulated RoboCup soccer show that, the approach is effective and outperforms existing reinforcement learning methods.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {782–789},
numpages = {8},
keywords = {curriculum learning, distributed training, reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3328526.3329634,
author = {Wright, Mason and Wang, Yongzhao and Wellman, Michael P.},
title = {Iterated Deep Reinforcement Learning in Games: History-Aware Training for Improved Stability},
year = {2019},
isbn = {9781450367929},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328526.3329634},
doi = {10.1145/3328526.3329634},
abstract = {Deep reinforcement learning (RL) is a powerful method for generating policies in complex environments, and recent breakthroughs in game-playing have leveraged deep RL as part of an iterative multiagent search process. We build on such developments and present an approach that learns progressively better mixed strategies in complex dynamic games of imperfect information, through iterated use of empirical game-theoretic analysis (EGTA) with deep RL policies. We apply the approach to a challenging cybersecurity game defined over attack graphs. Iterating deep RL with EGTA to convergence over dozens of rounds, we generate mixed strategies far stronger than earlier published heuristic strategies for this game. We further refine the strategy-exploration process, by fine-tuning in a training environment that includes out-of-equilibrium but recently seen opponents. Experiments suggest this history-aware approach yields strategies with lower regret at each stage of training.},
booktitle = {Proceedings of the 2019 ACM Conference on Economics and Computation},
pages = {617–636},
numpages = {20},
keywords = {double oracle, attack graphs, security games, multi-agent reinforcement learning, deep reinforcement learning},
location = {Phoenix, AZ, USA},
series = {EC '19}
}

@article{10.1145/2735392.2735396,
author = {Faust, Aleksandra},
title = {Reinforcement Learning and Planning for Preference Balancing Tasks},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/2735392.2735396},
doi = {10.1145/2735392.2735396},
abstract = {Many robotic motion tasks, such as UAV control, have non-linear and high-dimensional dynamics. Difficult for both human demonstration and explicit solutions, these tasks can be described with opposing preferences. This thesis develops PEARL, a real-time solution for such tasks on acceleration-controlled systems with unknown dynamics, and finds PEARL's safety conditions.},
journal = {AI Matters},
month = {mar},
pages = {8–12},
numpages = {5}
}

@inproceedings{10.5555/3242181.3242282,
author = {Lee, Kamwoo and Rucker, Mark and Scherer, William T. and Beling, Peter A. and Gerber, Matthew S. and Kang, Hyojung},
title = {Agent-Based Model Construction Using Inverse Reinforcement Learning},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {Agent-based modeling (ABM) assumes that behavioral rules affecting an agent's states and actions are known. However, discovering these rules is often challenging and requires deep insight about an agent's behaviors. Inverse reinforcement learning (IRL) can complement ABM by providing a systematic way to find behavioral rules from data. IRL frames learning behavioral rules as a problem of recovering motivations from observed behavior and generating rules consistent with these motivations. In this paper, we propose a method to construct an agent-based model directly from data using IRL. We explain each step of the proposed method and describe challenges that may occur during implementation. Our experimental results show that the proposed method can extract rules and construct an agent-based model with rich but concise behavioral rules for agents while still maintaining aggregate-level properties.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {95},
numpages = {12},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@inproceedings{10.1145/3397271.3401148,
author = {Xu, Jun and Wei, Zeng and Xia, Long and Lan, Yanyan and Yin, Dawei and Cheng, Xueqi and Wen, Ji-Rong},
title = {Reinforcement Learning to Rank with Pairwise Policy Gradient},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401148},
doi = {10.1145/3397271.3401148},
abstract = {This paper concerns reinforcement learning~(RL) of the document ranking models for information retrieval~(IR). One branch of the RL approaches to ranking formalize the process of ranking with Markov decision process~(MDP) and determine the model parameters with policy gradient. Though preliminary success has been shown, these approaches are still far from achieving their full potentials. Existing policy gradient methods directly utilize the absolute performance scores (returns) of the sampled document lists in its gradient estimations, which may cause two limitations: 1) fail to reflect the relative goodness of documents within the same query, which usually is close to the nature of IR ranking; 2) generate high variance gradient estimations, resulting in slow learning speed and low ranking accuracy. To deal with the issues, we propose a novel policy gradient algorithm in which the gradients are determined using pairwise comparisons of two document lists sampled within the same query. The algorithm, referred to as Pairwise Policy Gradient (PPG), repeatedly samples pairs of document lists, estimates the gradients with pairwise comparisons, and finally updates the model parameters. Theoretical analysis shows that PPG makes an unbiased and low variance gradient estimations. Experimental results have demonstrated performance gains over the state-of-the-art baselines in search result diversification and text retrieval.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {509–518},
numpages = {10},
keywords = {learning to rank, policy gradient, reinforcement learning},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/1390156.1390240,
author = {Melo, Francisco S. and Meyn, Sean P. and Ribeiro, M. Isabel},
title = {An Analysis of Reinforcement Learning with Function Approximation},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390240},
doi = {10.1145/1390156.1390240},
abstract = {We address the problem of computing the optimal Q-function in Markov decision problems with infinite state-space. We analyze the convergence properties of several variations of Q-learning when combined with function approximation, extending the analysis of TD-learning in (Tsitsiklis \&amp; Van Roy, 1996a) to stochastic control settings. We identify conditions under which such approximate methods converge with probability 1. We conclude with a brief discussion on the general applicability of our results and compare them with several related works.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {664–671},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@article{10.5555/3122009.3242024,
author = {Chow, Yinlam and Ghavamzadeh, Mohammad and Janson, Lucas and Pavone, Marco},
title = {Risk-Constrained Reinforcement Learning with Percentile Risk Criteria},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account risk, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs. Specifically, we first derive a formula for computing the gradient of the Lagrangian function for percentile risk-constrained MDPs. Then, we devise policy gradient and actor-critic algorithms that (1) estimate such gradient, (2) update the policy in the descent direction, and (3) update the Lagrange multiplier in the ascent direction. For these algorithms we prove convergence to locally optimal policies. Finally, we demonstrate the effectiveness of our algorithms in an optimal stopping problem and an online marketing application.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {6070–6120},
numpages = {51},
keywords = {conditional value-at-risk, actor-critic algorithms, Markov decision process, reinforcement learning, chance-constrained optimization, policy gradient algorithms}
}

@inproceedings{10.1145/3580305.3599393,
author = {Xu, Jiacheng and Chen, Chao and Zhang, Fuxiang and Yuan, Lei and Zhang, Zongzhang and Yu, Yang},
title = {Internal Logical Induction for Pixel-Symbolic Reinforcement Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599393},
doi = {10.1145/3580305.3599393},
abstract = {Reinforcement Learning (RL) has experienced rapid advancements in recent years. The widely studied RL algorithms mainly focus on a single input form, such as pixel-based image input or symbolic vector input. These two forms have different characteristics and, in many scenarios, will appear together, while few RL algorithms have studied the problems with mixed input types. Specifically, in the scenario where both pixel and symbolic inputs are available, symbolic input usually offers abstract features with specific semantics, which is more conducive to the agent's focus. Conversely, pixel input provides more comprehensive information, enabling the agent to make well-informed decisions. Tailoring the processing approach based on the properties of these two input types can contribute to solving the problem more effectively. To tackle the above issue, we propose an Internal Logical Induction (ILI) framework that integrates deep RL and rule learning into one system. ILI utilizes the deep RL algorithm to process the pixel input and the rule learning algorithm to induce propositional logic knowledge from symbolic input. To efficiently combine these two mechanisms, we further adopt a reward shaping technique by treating valuable knowledge as intrinsic rewards for the RL procedure. Experimental results demonstrate that the ILI framework outperforms baseline approaches in RL problems with pixel-symbolic input, and its inductive knowledge exhibits transferability advantages when pixel input semantics change.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2825–2837},
numpages = {13},
keywords = {reinforcement learning, rule learning},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/2739480.2754730,
author = {Cussat-Blanc, Sylvain and Harrington, Kyle},
title = {Genetically-Regulated Neuromodulation Facilitates Multi-Task Reinforcement Learning},
year = {2015},
isbn = {9781450334723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739480.2754730},
doi = {10.1145/2739480.2754730},
abstract = {In this paper, we use a gene regulatory network (GRN) to regulate a reinforcement learning controller, the State- Action-Reward-State-Action (SARSA) algorithm. The GRN serves as a neuromodulator of SARSA's learning parame- ters: learning rate, discount factor, and memory depth. We have optimized GRNs with an evolutionary algorithm to regulate these parameters on specific problems but with no knowledge of problem structure. We show that genetically- regulated neuromodulation (GRNM) performs comparably or better than SARSA with fixed parameters. We then ex- tend the GRNM SARSA algorithm to multi-task problem generalization, and show that GRNs optimized on multi- ple problem domains can generalize to previously unknown problems with no further optimization.},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {551–558},
numpages = {8},
keywords = {parameter control, neuromodulation, gene regulatory network, multi-task learning, reinforcement learning, learning: parameter learning},
location = {Madrid, Spain},
series = {GECCO '15}
}

@article{10.1145/3414472,
author = {Scurto, Hugo and Kerrebroeck, Bavo Van and Caramiaux, Baptiste and Bevilacqua, Fr\'{e}d\'{e}ric},
title = {Designing Deep Reinforcement Learning for Human Parameter Exploration},
year = {2021},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3414472},
doi = {10.1145/3414472},
abstract = {Software tools for generating digital sound often present users with high-dimensional, parametric interfaces, that may not facilitate exploration of diverse sound designs. In this article, we propose to investigate artificial agents using deep reinforcement learning to explore parameter spaces in partnership with users for sound design. We describe a series of user-centred studies to probe the creative benefits of these agents and adapting their design to exploration. Preliminary studies observing users’ exploration strategies with parametric interfaces and testing different agent exploration behaviours led to the design of a fully-functioning prototype, called Co-Explorer, that we evaluated in a workshop with professional sound designers. We found that the Co-Explorer enables a novel creative workflow centred on human–machine partnership, which has been positively received by practitioners. We also highlight varied user exploration behaviours throughout partnering with our system. Finally, we frame design guidelines for enabling such co-exploration workflow in creative digital applications.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {jan},
articleno = {1},
numpages = {35},
keywords = {audio/video, machine learning, Interaction design}
}

@article{10.5555/1953048.2021066,
author = {van Seijen, Harm and Whiteson, Shimon and van Hasselt, Hado and Wiering, Marco},
title = {Exploiting Best-Match Equations for Efficient Reinforcement Learning},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {This article presents and evaluates best-match learning, a new approach to reinforcement learning that trades off the sample efficiency of model-based methods with the space efficiency of model-free methods. Best-match learning works by approximating the solution to a set of best-match equations, which combine a sparse model with a model-free Q-value function constructed from samples not used by the model. We prove that, unlike regular sparse model-based methods, best-match learning is guaranteed to converge to the optimal Q-values in the tabular case. Empirical results demonstrate that best-match learning can substantially outperform regular sparse model-based methods, as well as several model-free methods that strive to improve the sample efficiency of temporal-difference methods. In addition, we demonstrate that best-match learning can be successfully combined with function approximation.},
journal = {J. Mach. Learn. Res.},
month = {jul},
pages = {2045–2094},
numpages = {50}
}

@inproceedings{10.1145/3447548.3467420,
author = {Li, Jiahui and Kuang, Kun and Wang, Baoxiang and Liu, Furui and Chen, Long and Wu, Fei and Xiao, Jun},
title = {Shapley Counterfactual Credits for Multi-Agent Reinforcement Learning},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467420},
doi = {10.1145/3447548.3467420},
abstract = {Centralized Training with Decentralized Execution (CTDE) has been a popular paradigm in cooperative Multi-Agent Reinforcement Learning (MARL) settings and is widely used in many real applications. One of the major challenges in the training process is credit assignment, which aims to deduce the contributions of each agent according to the global rewards. Existing credit assignment methods focus on either decomposing the joint value function into individual value functions or measuring the impact of local observations and actions on the global value function. These approaches lack a thorough consideration of the complicated interactions among multiple agents, leading to an unsuitable assignment of credit and subsequently mediocre results on MARL. We propose Shapley Counterfactual Credit Assignment, a novel method for explicit credit assignment which accounts for the coalition of agents. Specifically, Shapley Value and its desired properties are leveraged in deep MARL to credit any combinations of agents, which grants us the capability to estimate the individual credit for each agent. Despite this capability, the main technical difficulty lies in the computational complexity of Shapley Value who grows factorially as the number of agents. We instead utilize an approximation method via Monte Carlo sampling, which reduces the sample complexity while maintaining its effectiveness. We evaluate our method on StarCraft II benchmarks across different scenarios. Our method outperforms existing cooperative MARL algorithms significantly and achieves the state-of-the-art, with especially large margins on tasks with more severe difficulties.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {934–942},
numpages = {9},
keywords = {multi-agent systems, credit assignment, counterfactual thinking, reinforcement learning, shapley value},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.5555/3398761.3399006,
author = {Gupta, Shubham and Hazra, Rishi and Dukkipati, Ambedkar},
title = {Networked Multi-Agent Reinforcement Learning with Emergent Communication},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We develop a Multi-Agent Reinforcement Learning (MARL) method that finds approximately optimal policies for cooperative agents that co-exist in an environment. Central to achieving this is how the agents learn to communicate with each other. Can they together develop a language while learning to perform a common task? We formulate and study a MARL problem where cooperative agents are connected via a fixed underlying network. These agents communicate along the edges of this network by exchanging discrete symbols. However, the semantics of these symbols are not predefined and have to be learned during the training process. We propose a method for training these agents using emergent communication. We demonstrate the applicability of the proposed framework by applying it to the problem of managing traffic controllers, where we achieve state-of-the-art performance (as compared to several strong baselines) and perform a detailed analysis of the emergent communication.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1858–1860},
numpages = {3},
keywords = {emergent communication, traffic, multi-agent reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1109/ASONAM49781.2020.9381416,
author = {Yang, Zhou and Nguyen, Long and Zhu, Jiazhen and Pan, Zhenhe and Li, Jia and Jin, Fang},
title = {Coordinating Disaster Emergency Response with Heuristic Reinforcement Learning},
year = {2021},
isbn = {9781728110561},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASONAM49781.2020.9381416},
doi = {10.1109/ASONAM49781.2020.9381416},
abstract = {A crucial and time-sensitive task when any disaster occurs is to rescue victims and distribute resources to the right groups and locations. This task is challenging in populated urban areas, due to a huge burst of help requests made in a very short period. To improve the efficiency of the emergency response in the immediate aftermath of a disaster, we propose a heuristic multi-agent reinforcement learning scheduling algorithm, named as ResQ, which can effectively schedule a rapid deployment of volunteers to rescue victims in dynamic settings. The core concept is to quickly identify victims and volunteers from social network data and then schedule rescue parties with an adaptive learning algorithm. This framework performs two key functions: 1) identify trapped victims and volunteers, and 2) optimize the volunteers' rescue strategy in a complex time-sensitive environment. The proposed ResQ algorithm can speed up the training processes through a heuristic function which reduces the state-action space by identifying a set of particular actions over others. Experimental results showed that the proposed heuristic multi-agent reinforcement learning based scheduling outperforms several state-of-art methods, in terms of both reward rate and response times.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {565–572},
numpages = {8},
location = {Virtual Event, Netherlands},
series = {ASONAM '20}
}

@inproceedings{10.5555/3378680.3378879,
author = {Tabrez, Aaquib and Hayes, Bradley},
title = {Improving Human-Robot Interaction through Explainable Reinforcement Learning},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {Gathering the most informative data from humans without overloading them remains an active research area in AI, and is closely coupled with the problems of determining how and when information should be communicated to others [12]. Current decision support systems (DSS) are still overly simple and static, and cannot adapt to changing environments we expect to deploy in modern systems [3], [4], [9], [11]. They are intrinsically limited in their ability to explain rationale versus merely listing their future behaviors, limiting a human's understanding of the system [2], [7]. Most probabilistic assessments of a task are conveyed after the task/skill is attempted rather than before [10], [14], [16]. This limits failure recovery and danger avoidance mechanisms. Existing work on predicting failures relies on sensors to accurately detect explicitly annotated and learned failure modes [13]. As such, important non-obvious pieces of information for assessing appropriate trust and/or course-of-action (COA) evaluation in collaborative scenarios can go overlooked, while irrelevant information may instead be provided that increases clutter and mental workload. Understanding how AI models arrive at specific decisions is a key principle of trust [8]. Therefore, it is critically important to develop new strategies for anticipating, communicating, and explaining justifications and rationale for AI driven behaviors via contextually appropriate semantics.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {751–753},
numpages = {3},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

