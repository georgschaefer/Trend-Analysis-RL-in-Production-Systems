@article{KATIC200998,
title = {Policy Gradient Fuzzy Reinforcement Learning Control of Humanoid Walking},
journal = {IFAC Proceedings Volumes},
volume = {42},
number = {19},
pages = {98-103},
year = {2009},
note = {2nd IFAC Conference on Intelligent Control Systems and Signal Processing},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20090921-3-TR-3005.00019},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015308168},
author = {Duško M. Katić and Aleksandar D. Rodić},
abstract = {Abstract
This paper presents a novel dynamic control approach to acquire biped walking of humanoid robots focussed on policy gradient reinforcement learning with fuzzy evaluative feedback. The proposed structure of controller involves two feedback loops: conventional computed torque controller including impact-force controller and reinforcement learning computed torque controller. Reinforcement learning part includes fuzzy information about Zero-Moment Point errors. To demonstrate the effectiveness of our method, we apply it in simulation to the learning of a biped walking.}
}
@article{MUKHERJEE2021109451,
title = {Reduced-dimensional reinforcement learning control using singular perturbation approximations},
journal = {Automatica},
volume = {126},
pages = {109451},
year = {2021},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2020.109451},
url = {https://www.sciencedirect.com/science/article/pii/S000510982030649X},
author = {Sayak Mukherjee and He Bai and Aranya Chakrabortty},
keywords = {Reinforcement learning, Linear quadratic regulator, Singular perturbation, Model-free control, Model reduction},
abstract = {We present a set of model-free, reduced-dimensional reinforcement learning (RL) based optimal control designs for linear time-invariant singularly perturbed (SP) systems. We first present a state feedback and an output feedback based RL control design for a generic SP system with unknown state and input matrices. We take advantage of the underlying time-scale separation property of the plant to learn a linear quadratic regulator (LQR) for only its slow dynamics, thereby saving significant amount of learning time compared to the conventional full-dimensional RL controller. We analyze the sub-optimality of the designs using SP approximation theorems, and provide sufficient conditions for closed-loop stability. Thereafter, we extend both designs to clustered multi-agent consensus networks, where the SP property reflects through clustering. We develop both centralized and cluster-wise block-decentralized RL controllers for such networks, in reduced dimensions. We demonstrate the details of the implementation of these controllers using simulations of relevant numerical examples, and compare them with conventional RL designs to show the computational benefits of our approach.}
}
@article{GUAN2023109093,
title = {A deep reinforcement learning method for structural dominant failure modes searching based on self-play strategy},
journal = {Reliability Engineering & System Safety},
volume = {233},
pages = {109093},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109093},
url = {https://www.sciencedirect.com/science/article/pii/S095183202300008X},
author = {Xiaoshu Guan and Huabin Sun and Rongrong Hou and Yang Xu and Yuequan Bao and Hui Li},
keywords = {Structural reliability analysis, Dominant failure modes, Deep reinforcement learning, Self-play strategy, Monte Carlo tree search},
abstract = {In the research area of structural reliability analysis (SRA), the dominant failure modes (DFMs) of a structural system make significant contributions to life-span failure prediction and safety assessment. However, the high computational cost caused by the combinatorial explosion is the main problem in DFMs searching that hinders its application and further development. Recently, many successful applications have proved that the self-play deep reinforcement learning (DRL) has a strong ability to obtain action policy in the face of combinatorial explosion problems. Inspired by this, a self-play strategy is designed to optimize the DRL-based DFMs searching process and reduce the computational effort. A scoring function is designed and used as the refereeing standard of the self-play games and helps improve the efficiency of Monte Carlo tree search (MCTS) in an asynchronous training process. In comparison with the β-unzipping method and exploration-based DFMs searching method, the proposed method significantly improved training efficiency with an accuracy of over 95% and a lower requirement of the number of finite element analysis (FEA), both of which contribute to the policy learning of failure component selection. In summary, the method shows potential applications for actual structures and makes valuable contributions to the problem with high computing costs.}
}
@article{SHEN2020105920,
title = {Remote sensing image captioning via Variational Autoencoder and Reinforcement Learning},
journal = {Knowledge-Based Systems},
volume = {203},
pages = {105920},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.105920},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120302586},
author = {Xiangqing Shen and Bing Liu and Yong Zhou and Jiaqi Zhao and Mingming Liu},
keywords = {Transformer, Variational Autoencoder, Transfer learning, Remote sensing image captioning, Self-attention mechanisms, Convolutional neural network, Reinforcement learning},
abstract = {Image captioning, i.e., generating the natural semantic descriptions of given image, is an essential task for machines to understand the content of the image. Remote sensing image captioning is a part of the field. Most of the current remote sensing image captioning models suffered the overfitting problem and failed to utilize the semantic information in images. To this end, we propose a Variational Autoencoder and Reinforcement Learning based Two-stage Multi-task Learning Model (VRTMM) for the remote sensing image captioning task. In the first stage, we finetune the CNN jointly with the Variational Autoencoder. In the second stage, the Transformer generates the text description using both spatial and semantic features. Reinforcement Learning is then applied to enhance the quality of the generated sentences. Our model surpasses the previous state of the art records by a large margin on all seven scores on Remote Sensing Image Caption Dataset. The experiment result indicates our model is effective on remote sensing image captioning and achieves the new state-of-the-art result.}
}
@article{LANGER2022120020,
title = {A reinforcement learning approach to home energy management for modulating heat pumps and photovoltaic systems},
journal = {Applied Energy},
volume = {327},
pages = {120020},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120020},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922012776},
author = {Lissy Langer and Thomas Volling},
keywords = {Home energy management, Building energy management, Heat pump, Photovoltaics (PV), Reinforcement learning, Deep deterministic policy gradient (DDPG)},
abstract = {Buildings are one of the main drivers of global energy consumption and CO2 emissions. Efficient energy management systems will have to integrate renewable energy sources with heating and/or cooling to mitigate climate change. In this study, we analyze the potential of deep reinforcement learning (DRL) to control a smart home with a modulating air-to-water heat pump, a photovoltaic system, a battery energy, and a thermal storage system for floor heating and hot water supply. We transform a mixed-integer linear program (MILP) into a DRL implementation. In our numerical analysis, we compare our results based on the deep deterministic policy gradient (DDPG) algorithm to the theoretical upper bound of the model predictive control (MPC) result under full information, as well as a practice-oriented rule-based benchmark. We show that our proposed DRL implementation outperforms the rule-based approach and achieves a self-sufficiency of 75% with only limited comfort violations. Analyzing different DRL formulations, we conclude that domain knowledge is key to formalizing an efficient decision problem with stable results. Our input data and models, developed using the Julia programming language, are available open source.}
}
@article{DUGULEANA2012132,
title = {Obstacle avoidance of redundant manipulators using neural networks based reinforcement learning},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {28},
number = {2},
pages = {132-146},
year = {2012},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2011.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0736584511000962},
author = {Mihai Duguleana and Florin Grigore Barbuceanu and Ahmed Teirelbar and Gheorghe Mogan},
keywords = {Obstacle avoidance, Redundant manipulators, Neural networks, -learning, Virtual reality, CAVE},
abstract = {This paper proposes a new approach for solving the problem of obstacle avoidance during manipulation tasks performed by redundant manipulators. The developed solution is based on a double neural network that uses Q-learning reinforcement technique. Q-learning has been applied in robotics for attaining obstacle free navigation or computing path planning problems. Most studies solve inverse kinematics and obstacle avoidance problems using variations of the classical Jacobian matrix approach, or by minimizing redundancy resolution of manipulators operating in known environments. Researchers who tried to use neural networks for solving inverse kinematics often dealt with only one obstacle present in the working field. This paper focuses on calculating inverse kinematics and obstacle avoidance for complex unknown environments, with multiple obstacles in the working field. Q-learning is used together with neural networks in order to plan and execute arm movements at each time instant. The algorithm developed for general redundant kinematic link chains has been tested on the particular case of PowerCube manipulator. Before implementing the solution on the real robot, the simulation was integrated in an immersive virtual environment for better movement analysis and safer testing. The study results show that the proposed approach has a good average speed and a satisfying target reaching success rate.}
}
@article{ZHANG2023109136,
title = {Off-policy deep reinforcement learning with automatic entropy adjustment for adaptive online grid emergency control},
journal = {Electric Power Systems Research},
volume = {217},
pages = {109136},
year = {2023},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2023.109136},
url = {https://www.sciencedirect.com/science/article/pii/S0378779623000251},
author = {Ying Zhang and Meng Yue and Jianhui Wang},
keywords = {Deep reinforcement learning, Grid emergency control, Soft actor-critic, Voltage stability},
abstract = {Electric overloading conditions and contingencies put modern power systems at risk of voltage collapse and blackouts. Load shedding is crucial to maintain voltage stability for grid emergency control. However, the rule- or model-based schemes rely on accurate dynamic system models and face considerable challenges in adapting to various operating conditions and uncertain event occurrences. To address these issues, this paper proposes a novel deep reinforcement learning (DRL)-based voltage stability control algorithm with automatic entropy adjustment (AEA) for grid emergency control. Various dynamic network components for complex system operations are modeled to construct the DRL environment. An off-policy soft actor-critic architecture is developed to maximize the expected reward and policy entropy simultaneously. The AEA mechanism is proposed to facilitate the policy maximum entropy procedure, and the proposed method can automatically provide effective discrete and continuous actions against various fault scenarios. Our approach accomplishes high sampling efficiency, scalability, and auto-adaptivity of the control policies under high uncertainties. Comparative studies with the existing DRL-based control methods in IEEE benchmarks indicate salient performance improvement of the proposed method for dynamic system emergency control.}
}
@article{MHAISEN202039,
title = {To chain or not to chain: A reinforcement learning approach for blockchain-enabled IoT monitoring applications},
journal = {Future Generation Computer Systems},
volume = {111},
pages = {39-51},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19334399},
author = {Naram Mhaisen and Noora Fetais and Aiman Erbad and Amr Mohamed and Mohsen Guizani},
keywords = {Blockchain, Smart contracts, Monitoring applications, Internet of things, Reinforcement learning, Cost optimization},
abstract = {Traceability and autonomous business logic execution are highly desirable features in IoT monitoring applications. Traceability enables verifying signals’ history for security or analytical purposes. On the other hand, the autonomous execution of pre-defined rules establishes trust between parties involved in such applications and improves the efficiency of their workflow. Smart Contracts (SCs) firmly guarantee these two requirements due to the blockchain’s immutable distributed ledger and secure cryptographic consensus rules. Thus, SCs emerged as an appealing technology for monitoring applications. However, the cost of using public blockchains to harvest these guarantees can be prohibitive, especially with the considerable fluctuation of coin prices and different use case requirements. In this paper, we introduce a general SC-based IoT monitoring framework that can leverage the security features of public blockchains while minimizing the corresponding monetary cost. The framework contains a reinforcement learning agent that adapts to users’ needs and acts in real-time to smartly set the data submission rate of IoT sensors. Results based on the Ethereum protocol show significant potential cost saving depending on users’ preferences.}
}
@article{KIM2023104715,
title = {Simulating travel paths of construction site workers via deep reinforcement learning considering their spatial cognition and wayfinding behavior},
journal = {Automation in Construction},
volume = {147},
pages = {104715},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104715},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522005854},
author = {Minguk Kim and Youngjib Ham and Choongwan Koo and Tae Wan Kim},
keywords = {Construction site, Construction worker, Site layout planning, Deep reinforcement learning, Pathfinding},
abstract = {Many optimization methods for construction site layout planning (CSLP) generate the shortest path of workers to calculate traveling costs and site safety performance. However, this approach often degrades the solution's reliability because workers in real-life situations do not necessarily take the shortest path to their chosen destination. Thus, this paper proposes a novel approach for generating realistic paths that mimic their wayfinding decision-making process. This approach uses deep reinforcement learning, for which the framework to facilitate its use includes the following elements: (1) the required properties and functions for site objects; and (2) the state, action space, and reward functions intended. The similarity between the paths simulated and the real workers' trajectories has been validated better by 17.8% than the traditional A* algorithm. The proposed approach is expected to be used as an appropriate input, and thereby help improve the reliability of the solutions based on the CSLP optimization methods.}
}
@article{FANG2021107505,
title = {A3CMal: Generating adversarial samples to force targeted misclassification by reinforcement learning},
journal = {Applied Soft Computing},
volume = {109},
pages = {107505},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107505},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621004282},
author = {Zhiyang Fang and Junfeng Wang and Jiaxuan Geng and Yingjie Zhou and Xuan Kan},
keywords = {Malware classification, Adversarial samples, Deep reinforcement learning, A3C},
abstract = {Machine learning algorithms have been proved to be vulnerable to adversarial attacks. The potential adversary is able to force the model to produce deliberate errors by elaborately modifying the training samples. For malware analysis, most of the existing research on evasion attacks focuses on a detection scenario, while less attention is paid to the classification scenario which is vital to decide a suitable system response in time. To fulfill this gap, this paper tries to address the misclassification problem in malware analysis. A reinforcement learning model named A3CMal is proposed. This adversarial model aims to generate adversarial samples which can fool the target classifier. As a core component of A3CMal, the self-learning agent constantly takes optimal actions to confuse the classification by slightly modifying samples on the basis of the observed states. Extensive experiments are performed to test the validity of A3CMal. The results show that the proposed A3CMal can force the target classifier to make wrong predictions while preserving the malicious functionality of the malware. Remarkably, not only can it cause the system to indicate an incorrect classification, but also can mislead the target model to classify malware into a specific category. Furthermore, our experiments demonstrate that the PE-based classifier is vulnerable to the adversarial samples generated by A3CMal.}
}
@article{SHEN2023125350,
title = {Fast online reinforcement learning control of small lift-driven vertical axis wind turbines with an active programmable four bar linkage mechanism},
journal = {Energy},
volume = {262},
pages = {125350},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.125350},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222022320},
author = {He Shen and Alexis Ruiz and Ni Li},
keywords = {Vertical axis wind turbine, Pitch control, Four bar linkage mechanism, Reinforcement learning, Transient response},
abstract = {Maintaining high power generation for small lift-driven vertical axis wind turbines in a changing wind environment has not been well studied yet, due to the challenges inherited from the unpredictable turbulent flow-blade interaction and complex blade interferences. Herein, a fast online reinforcement learning pitch control using an active programmable four bar linkage mechanism is proposed, making it possible for turbines to quickly adapt to wind changes and maintain high power output in operation. We formulate the pitching mechanism using a drag-link configuration with a variable frame link length into an optimization problem and further solve it by the interior point algorithm under a wide range of tip speed ratios. Then, a parameter explorative policy gradient reinforcement learning method is designed for the turbine to adaptively tune the frame link length. Since the design significantly reduces the number of parameters needed to depict a whole pitch trajectory, the proposed online learning process can converge quickly, making it capable of handling complex wind conditions in an urban environment. The transient behavior overlooked in much of the literature is also studied. Comparisons to two benchmarks have demonstrated that our proposed system has a superior performance.}
}
@article{XIE2023118893,
title = {Data-driven torque and pitch control of wind turbines via reinforcement learning},
journal = {Renewable Energy},
volume = {215},
pages = {118893},
year = {2023},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2023.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S0960148123007905},
author = {Jingjie Xie and Hongyang Dong and Xiaowei Zhao},
keywords = {Wind turbine control, Reinforcement learning, Deep neural network, Model predictive control},
abstract = {This paper addresses the torque and pitch control problems of wind turbines. The main contribution of this work is the development of an innovative reinforcement learning (RL)-based control method targeting wind turbine applications. Our RL-based control framework synergistically combines the advantages of deep neural networks (DNNs) and model predictive control (MPC) technologies. The proposed control strategy is data-driven, adapting to real-time changes in system dynamics and enhancing control performance and robustness. Additionally, the incorporation of an MPC structure within our design improves learning efficiency and reduces the high computational complexity typically found in deep RL algorithms. Specifically, a DNN is designed to approximate the wind turbine dynamics based on a continuously updated dataset composed of state and action measurements taken at specified sampling intervals. The real-time control policy is generated by integrating the online trained DNN into an MPC architecture. The proposed method iteratively updates the DNN and control policy in real-time to optimize performance. As a primary result of this work, the proposed method demonstrates superior robustness and control performance compared to commonly-employed MPC and other baseline wind turbine controllers in the presence of uncertainties and unexpected actuator faults. This effectiveness is showcased through simulations with a high-fidelity wind turbine simulator.}
}
@article{CHENG2023115062,
title = {Deep reinforcement learning for cost-optimal condition-based maintenance policy of offshore wind turbine components},
journal = {Ocean Engineering},
volume = {283},
pages = {115062},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.115062},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823014464},
author = {Jianda Cheng and Yan Liu and Wei Li and Tianyun Li},
keywords = {Offshore wind turbine, Dynamic inspection interval, Adaptive repair threshold, Deep reinforcement learning, Condition-based maintenance},
abstract = {The high cost of operation and maintenance can impede the development of offshore wind turbines (OWTs). With advancements in detection technology, condition-based maintenance (CBM) has emerged as a promising approach to managing maintenance. Numerous CBM polices have been studied to achieve a cost-effective maintenance. However, little research has been conducted on discovering the cost-optimal CBM policy for OWTs. With this motivation, a deep reinforcement learning (DRL) framework for exploring the cost-optimal CBM policy is proposed in this paper. Four CBM policies (i.e. uniform or dynamic inspection interval, and fixed or adaptive repair threshold) are formulated as the Markov decision process model. Two DRL algorithms (deep Q network and proximal policy optimization) are applied to derive the dynamic inspection interval and the adaptive repair threshold. To illustrate this framework, a fatigue OWT component is used as an example. The four policies are optimized under varying conditions to find the cost-optimal CBM policy. Moreover, performance comparison of DRL algorithms is performed. The case study illustrates the advantage of DRL in deriving optimal maintenance policies for fatigue-sensitive OWT components and highlights the interrelationship between maintenance policy and economic performance. This paper concludes by discussing the effect of cost elements on the CBM policies.}
}
@article{LIU2006142,
title = {Experimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory: Part 1. Theoretical foundation},
journal = {Energy and Buildings},
volume = {38},
number = {2},
pages = {142-147},
year = {2006},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2005.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S037877880500085X},
author = {Simeng Liu and Gregor P. Henze},
keywords = {Load shifting, Thermal Energy Storage (TES), Optimal control, Learning control, Reinforcement learning},
abstract = {This paper is the first part of a two-part investigation of a novel approach to optimally control commercial building passive and active thermal storage inventory. The proposed building control approach is based on simulated reinforcement learning, which is a hybrid control scheme that combines features of model-based optimal control and model-free learning control. An experimental study was carried out to analyze the performance of a hybrid controller installed in a full-scale laboratory facility. The first part presents an overview of the project with an emphasis on the theoretical foundation. The motivation of the research will be introduced first, followed by a review of past work. A brief introduction of the theory is provided including classic reinforcement learning and its variation, so-called simulated reinforcement learning, which constitutes the basic architecture of the hybrid learning controller. A detailed discussion of the experimental results will be presented in the companion paper.}
}
@incollection{MAHADEVAN1992290,
title = {Enhancing Transfer in Reinforcement Learning by Building Stochastic Models of Robot Actions},
editor = {Derek Sleeman and Peter Edwards},
booktitle = {Machine Learning Proceedings 1992},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {290-299},
year = {1992},
isbn = {978-1-55860-247-2},
doi = {https://doi.org/10.1016/B978-1-55860-247-2.50042-5},
url = {https://www.sciencedirect.com/science/article/pii/B9781558602472500425},
author = {Sridhar Mahadevan},
abstract = {Recent work has shown that reinforcement learning is a viable method of automatically programming behavior-based robots. However, one weakness with this approach is that the learning typically does not transfer across tasks. Furthermore, there is unnecessary duplication of learning effort because information is not shared among the various behaviors. This paper describes an alternative technique based on action models that attempts to maximize transfer within and across tasks. Action models are inferred using a statistical clustering technique from instances generated by a robot exploring its task environment. Task-specific knowledge is encoded using a reward function for each subtask. A multi-step lookahead strategy using the reward functions as static evaluators is employed to select the most appropriate action. Experiments on simulated and real robots show that useful action models can be learned from a 12 by 12 scrolling certainty grid sensor. Furthermore, on the simulator these models are sufficiently rich to enable significant transfer within and across two tasks, box pushing and wall following.}
}
@article{PANG2021240,
title = {Robust Reinforcement Learning for Stochastic Linear Quadratic Control with Multiplicative Noise⁎⁎This work has been supported in part by the U.S. National Science Foundation under Grants ECCS-1501044 and EPCN-1903781.},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {7},
pages = {240-243},
year = {2021},
note = {19th IFAC Symposium on System Identification SYSID 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.365},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321011393},
author = {Bo Pang and Zhong-Ping Jiang},
keywords = {Reinforcement learning control, Stochastic optimal control problems, Data-based control, Robustness analysis, Input-to-state stability},
abstract = {This paper studies the robustness of reinforcement learning for discrete-time linear stochastic systems with multiplicative noise evolving in continuous state and action spaces. As one of the popular methods in reinforcement learning, the robustness of policy iteration is a longstanding open issue for the stochastic linear quadratic regulator (LQR) problem with multiplicative noise. A solution in the spirit of small-disturbance input-to-state stability is given, guaranteeing that the solutions of the policy iteration algorithm are bounded and enter a small neighborhood of the optimal solution, whenever the error in each iteration is bounded and small. In addition, a novel off-policy multiple-trajectory optimistic least-squares policy iteration algorithm is proposed, to learn a near-optimal solution of the stochastic LQR problem directly from online input/state data, without explicitly identifying the system matrices. The efficacy of the proposed algorithm is supported by rigorous convergence analysis and numerical results on a second-order example.}
}
@article{KATIC20081717,
title = {Dynamic Control Algorithm for Biped Walking Based on Policy Gradient Fuzzy Reinforcement Learning},
journal = {IFAC Proceedings Volumes},
volume = {41},
number = {2},
pages = {1717-1722},
year = {2008},
note = {17th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20080706-5-KR-1001.00294},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016391996},
author = {Duško M. Katić and Aleksandar D. Rodić},
abstract = {This paper presents a novel dynamic control approach to acquire biped walking of humanoid robots focussed on policy gradient reinforcement learning with fuzzy evaluative feedback. The proposed structure of controller involves two feedback loops: conventional computed torque controller including impact-force controller and reinforcement learning computed torque controller. Reinforcement learning part includes fuzzy information about Zero-Moment Point errors. To demonstrate the effectiveness of our method, we apply it in simulation to the learning of a biped walking.}
}
@article{WAWRZYNSKI2012205,
title = {Autonomous Reinforcement Learning with Experience Replay for Humanoid Gait Optimization},
journal = {Procedia Computer Science},
volume = {13},
pages = {205-211},
year = {2012},
note = {Proceedings of the International Neural Network Society Winter Conference (INNS-WC2012)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.09.130},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912007375},
author = {Paweł Wawrzyński},
keywords = {Reinforcement learning, Autonomous learning, Learning in robots},
abstract = {This paper demonstrates application of Reinforcement Learning to optimization of control of a complex system in realistic setting that requires efficiency and autonomy of the learning algorithm. Namely, Actor-Critic with experience replay (which addresses efficiency), and the Fixed Point method for step-size estimation (which addresses autonomy) is applied here to approximately optimize humanoid robot gait. With complex dynamics and tens of continuous state and action variables, humanoid gait optimization represents a challenge for analytical synthesis of control. The presented algorithm learns a nimble gait within 80minutes of training.}
}
@article{CAO2021102278,
title = {A deep reinforcement learning-based on-demand charging algorithm for wireless rechargeable sensor networks},
journal = {Ad Hoc Networks},
volume = {110},
pages = {102278},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2020.102278},
url = {https://www.sciencedirect.com/science/article/pii/S1570870520306399},
author = {Xianbo Cao and Wenzheng Xu and Xuxun Liu and Jian Peng and Tang Liu},
keywords = {Wireless rechargeable sensor networks, Time window, Mobile charging, Deep reinforcement learning technique},
abstract = {Wireless rechargeable sensor networks are widely used in many fields. However, the limited battery capacity of sensor nodes hinders its development. With the help of wireless energy transfer technology, employing a mobile charger to charge sensor nodes wirelessly has become a promising technology for prolonging the lifetime of wireless sensor networks. Considering that the energy consumption rate varies significantly among sensors, we need a better way to model the charging demand of each sensor, such that the sensors are able to be charged multiple times in one charging tour. Therefore, time window is used to represent charging demand. In order to allow the mobile charger to respond to these charging demands in time and transfer more energy to the sensors, we introduce a new metric: charging reward. This new metric enables us to measure the quality of sensor charging. And then, we study the problem of how to schedule the mobile charger to replenish the energy supply of sensors, such that the sum of charging rewards collected by mobile charger on its charging tour is maximized. The sum of the collected charging reward is subject to the energy capacity constraint on the mobile charger and the charging time windows of all sensor nodes. We first prove that this problem is NP-hard. Due to the complexity of the problem, then deep reinforcement learning technique is exploited to obtain the moving path for mobile charger. Finally, experimental simulations are conducted to evaluate the performance of the proposed charging algorithm, and the results show that the proposed scheme is very promising.}
}
@article{GONCALVES2021109623,
title = {Max-plus approximation for reinforcement learning},
journal = {Automatica},
volume = {129},
pages = {109623},
year = {2021},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2021.109623},
url = {https://www.sciencedirect.com/science/article/pii/S0005109821001436},
author = {Vinicius Mariano Gonçalves},
keywords = {Max-Plus Algebra, Reinforcement learning, Dynamic programming},
abstract = {Max-Plus Algebra has been applied in several contexts, especially in the control of discrete events systems. In this article, we discuss another application closely related to control: the use of Max-Plus algebra concepts in the context of reinforcement learning. Max-Plus Algebra and reinforcement learning are strongly linked due to the latter’s dependence on the Bellman Equation which, in some cases, is a linear Max-Plus equation. This fact motivates the application of Max-Plus algebra to approximate the value function, central to the Bellman Equation and thus also to reinforcement learning. This article proposes conditions so that this approach can be done in a simple way and following the philosophy of reinforcement learning: explore the environment, receive the rewards and use this information to improve the knowledge of the value function. The proposed conditions are related to two matrices and impose on them a relationship that is analogous to the concept of weak inverses in traditional algebra.}
}
@article{MORI2022107,
title = {Probabilistic generative modeling and reinforcement learning extract the intrinsic features of animal behavior},
journal = {Neural Networks},
volume = {145},
pages = {107-120},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003932},
author = {Keita Mori and Naohiro Yamauchi and Haoyu Wang and Ken Sato and Yu Toyoshima and Yuichi Iino},
keywords = {Probabilistic generative model, Mixture density network, Recurrent neural network, Reinforcement learning, Behavior analysis, Computational ethology},
abstract = {It is one of the ultimate goals of ethology to understand the generative process of animal behavior, and the ability to reproduce and control behavior is an important step in this field. However, it is not easy to achieve this goal in systems with complex and stochastic dynamics such as animal behavior. In this study, we have shown that MDN–RNN,a type of probabilistic deep generative model, is able to reproduce stochastic animal behavior with high accuracy by modeling the behavior of C. elegans. Furthermore, we found that the model learns different dynamics in a disentangled representation as a time-evolving Gaussian mixture. Finally, by combining the model and reinforcement learning, we were able to extract a behavioral policy of goal-directed behavior in silico, and showed that it can be used for regulating the behavior of real animals. This set of methods will be applicable not only to animal behavior but also to broader areas such as neuroscience and robotics.}
}
@article{PANG2020109035,
title = {Reinforcement learning for adaptive optimal control of continuous-time linear periodic systems},
journal = {Automatica},
volume = {118},
pages = {109035},
year = {2020},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2020.109035},
url = {https://www.sciencedirect.com/science/article/pii/S0005109820302338},
author = {Bo Pang and Zhong-Ping Jiang and Iven Mareels},
keywords = {Optimal control, Reinforcement learning (RL), Policy iteration (PI), Adaptive dynamic programming (ADP)},
abstract = {This paper studies the infinite-horizon adaptive optimal control of continuous-time linear periodic (CTLP) systems, using reinforcement learning techniques. By means of policy iteration (PI) for CTLP systems, both on-policy and off-policy adaptive dynamic programming (ADP) algorithms are derived, such that the solution of the optimal control problem can be found without the exact knowledge of the system dynamics. Starting with initial stabilizing controllers, the proposed PI-based ADP algorithms converge to the optimal solutions under mild conditions. Application to the adaptive optimal control of the lossy Mathieu equation demonstrates the efficacy of the proposed learning-based adaptive optimal control algorithm.}
}
@article{WU2023,
title = {Multi-objective reinforcement learning-based energy management for fuel cell vehicles considering lifecycle costs},
journal = {International Journal of Hydrogen Energy},
year = {2023},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2023.06.145},
url = {https://www.sciencedirect.com/science/article/pii/S0360319923030240},
author = {J.J. Wu and D.F. Song and X.M Zhang and C.S. Duan and D.P. Yang},
keywords = {Fuel cell vehicle, Energy management strategy, Multi-objective control, Lifecycle costs, Reinforcement learning},
abstract = {To balance the hydrogen consumption of fuel cell vehicle (FCV), the durability of the fuel cell (FC), and the life of the power battery (PB) to further reduce the whole lifecycle costs of FCV. A multi-objective reinforcement learning-based (MORL-based) energy management strategy (EMS) is proposed in this research. First, the composition mechanism of the FCV lifecycle costs is analyzed, and the equivalent hydrogen consumption model, FC durability degradation model, and PB life decay model are established; Then, a three-dimensional reward function is constructed by integrating the objectives of equivalent hydrogen consumption, FC durability degradation, and PB life decay. And the penalty terms coupled with the decay factors are introduced into the reward function to satisfy the mutual constraint characteristics between the PB and the FC system to ensure the stability of the MORL-based EMS; In addition, the prioritized experience replay technology is introduced into the MORL-based EMS to improve the learning efficiency and convergence of traditional deep Q network (DQN) algorithm; After that, the evaluation and target network of the embedded dueling network are introduced to solve the multi-objective overestimation problem encountered in the training process by generalizing the behavior learning in the presence of similar value behaviors; Finally, the performance of MORL-based EMS and DQN-based EMS is compared by numerical simulation under various driving cycles. The results show that the MORL-based EMS proposed in this paper has better convergence ability, adaptability, and lower lifecycle costs than the DQN-based EMS. In addition, the lifecycle costs of the MORL-based EMS can achieve a 99.2% control effect of the dynamic programming-based EMS.}
}
@article{LIU2022102694,
title = {Deep dispatching: A deep reinforcement learning approach for vehicle dispatching on online ride-hailing platform},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {161},
pages = {102694},
year = {2022},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2022.102694},
url = {https://www.sciencedirect.com/science/article/pii/S1366554522000862},
author = {Yang Liu and Fanyou Wu and Cheng Lyu and Shen Li and Jieping Ye and Xiaobo Qu},
keywords = {Vehicle dispatching, Deep reinforcement learning, Load balancing},
abstract = {The vehicle dispatching system is one of the most critical problems in online ride-hailing platforms, which requires adapting the operation and management strategy to the dynamics of demand and supply. In this paper, we propose a single-agent deep reinforcement learning approach for the vehicle dispatching problem called deep dispatching, by reallocating vacant vehicles to regions with a large demand gap in advance. The simulator and the vehicle dispatching algorithm are designed based on industrial-scale real-world data and the workflow of online ride-hailing platforms, ensuring the practical value of our approach. Besides, the vehicle dispatching problem is translated in analogy with the load balancing problem in computer networks. Inspired by the recommendation system, the problem of high concurrency of dispatching requests is addressed by sorting the actions as a recommendation list, whereby matching action with requests. Experiments demonstrate that the proposed approach is superior to existing benchmarks. It is also worth noting that the proposed approach won first place in the vehicle dispatching task of KDD Cup 2020.}
}
@article{CANDELA2023103923,
title = {Risk-aware controller for autonomous vehicles using model-based collision prediction and reinforcement learning},
journal = {Artificial Intelligence},
volume = {320},
pages = {103923},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103923},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223000693},
author = {Eduardo Candela and Olivier Doustaly and Leandro Parada and Felix Feng and Yiannis Demiris and Panagiotis Angeloudis},
keywords = {Autonomous vehicles, Decision making, Risk-aware AI, Reinforcement learning, Model-based prediction, Explainable AI},
abstract = {Autonomous Vehicles (AVs) have the potential to save millions of lives and increase the efficiency of transportation services. However, the successful deployment of AVs requires tackling multiple challenges related to modeling and certifying safety. State-of-the-art decision-making methods usually rely on end-to-end learning or imitation learning approaches, which still pose significant safety risks. Hence the necessity of risk-aware AVs that can better predict and handle dangerous situations. Furthermore, current approaches tend to lack explainability due to their reliance on end-to-end Deep Learning, where significant causal relationships are not guaranteed to be learned from data. This paper introduces a novel risk-aware framework for training AV agents using a bespoke collision prediction model and Reinforcement Learning (RL). The collision prediction model is based on Gaussian Processes and vehicle dynamics, and is used to generate the RL state vector. Using an explicit risk model increases the post-hoc explainability of the AV agent, which is vital for reaching and certifying the high safety levels required for AVs and other safety-sensitive applications. Experimental results obtained with a simulator and state-of-the-art RL algorithms show that the risk-aware RL framework decreases average collision rates by 15%, makes AVs more robust to sudden harsh braking situations, and achieves better performance in both safety and speed when compared to a standard rule-based method (the Intelligent Driver Model). Moreover, the proposed collision prediction model outperforms other models in the literature.}
}
@article{TOSIN2022117772,
title = {Identification and removal of contaminants in sEMG recordings through a methodology based on Fuzzy Inference and Actor-Critic Reinforcement learning},
journal = {Expert Systems with Applications},
volume = {206},
pages = {117772},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117772},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422010454},
author = {Maurício Cagliari Tosin and Alexandre Balbinot},
keywords = {Actor-Critic Reinforcement Learning, sEMG signal contamination, Electromyography, Fuzzy Inference System},
abstract = {Purpose
Contaminants in surface electromyography (sEMG) recordings might configure an issue if not kept at lower levels since they can impair the extraction of information. In this context, several approaches have been proposed to minimize the effects of specific contaminants. However, knowing the type of interference is important to improve the system efficiency and avoid deformation in the EMG signal by unnecessary filtering. Thereby, this paper proposes a new strategy to recognize and minimize four contaminants commonly found in sEMG recordings (motion artifact, electrocardiography, powerline interference, and additive white Gaussian noise).
Methods
An Actor-Critic Reinforcement Learning model with a Fuzzy Inference System (FIS)-based reward function (FIS-ACRL) was designed for contaminant identification and removal. The ACRL model consists of an environment (sEMG), a state (represented by a set of six handcrafted features), a set of actions (four filters/methodologies to remove each contaminant), and an actor and a critic (formed by two neural networks). A reward is assigned to the agent actions through a FIS, where the inputs are determined according to the impact of the action in the features, and the defuzzified output configures a score that is, in turn, converted to the proper reward.
Results
The ACRL model evaluation was through a supervised experiment (the reward assigning was from the correct label), achieving an overall median accuracy of 93.13% at classifying the four contaminants with Signal-to-Noise Ratio (SNR) ranging from −30 to 10 dB in steps of 10 dB. The FIS-ACRL performance assessment was through an unsupervised experiment in the same dataset. It was obtained 92.60% of median accuracy, outperforming three typical clustering algorithms (k-Means, Self-Organizing Map (SOM)-k-Means, and SOM-Ward).
Conclusion
The results validate the proposed strategy, showing that it is possible to identify the contaminant type through unsupervised and continuous learning, besides automatically executing the correct procedure to minimize it. Moreover, the nature of ACRL theory enables the continuous adaptation of the agent learning over the environment changes.}
}
@article{PETSAGKOURAKIS202011264,
title = {Constrained Reinforcement Learning for Dynamic Optimization under Uncertainty},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {11264-11270},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.361},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320306455},
author = {P. Petsagkourakis and I.O. Sandoval and E. Bradford and D. Zhang and E.A. {del Rio-Chanona}},
keywords = {Reinforcement learning, Uncertain dynamic systems, Stochastic control, Chemical process control, Adaptive control, Policy gradient},
abstract = {Dynamic real-time optimization (DRTO) is a challenging task due to the fact that optimal operating conditions must be computed in real time. The main bottleneck in the industrial application of DRTO is the presence of uncertainty. Many stochastic systems present the following obstacles: 1) plant-model mismatch, 2) process disturbances, 3) risks in violation of process constraints. To accommodate these difficulties, we present a constrained reinforcement learning (RL) based approach. RL naturally handles the process uncertainty by computing an optimal feedback policy. However, no state constraints can be introduced intuitively. To address this problem, we present a chance-constrained RL methodology. We use chance constraints to guarantee the probabilistic satisfaction of process constraints, which is accomplished by introducing backoffs, such that the optimal policy and backoffs are computed simultaneously. Backoffs are adjusted using the empirical cumulative distribution function to guarantee the satisfaction of a joint chance constraint. The advantage and performance of this strategy are illustrated through a stochastic dynamic bioprocess optimization problem, to produce sustainable high-value bioproducts.}
}
@article{FANG2022112788,
title = {Wind turbine rotor speed design optimization considering rain erosion based on deep reinforcement learning},
journal = {Renewable and Sustainable Energy Reviews},
volume = {168},
pages = {112788},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112788},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122006724},
author = {Jianhao Fang and Weifei Hu and Zhenyu Liu and Weiyi Chen and Jianrong Tan and Zhiyu Jiang and Amrit Shankar Verma},
keywords = {Wind turbine blade, Rain erosion, Rotor speed, Design optimization, Deep reinforcement learning, Deep deterministic policy gradient},
abstract = {Rain erosion is one of the most detrimental factors contributing to wind turbine blade (WTB) coating fatigue damage especially for utility-scale wind turbines (WTs). To prevent rain erosion induced WTB coating fatigue damage, this paper proposes a deep reinforcement learning (DRL)-based optimization method for finding the optimal rotor speed under different rain intensities and wind speeds. First, an efficient physics-based model for predicting WTB coating fatigue damage considering the comprehensive blade coating fatigue mechanism, rain intensity distribution, and wind speed distribution is presented. Then, a WT rotor speed design optimization problem is constructed to search for the optimal rotor speed under different rain intensity and wind speed conditions. To address the challenge of optimizing the efficiency, the original design optimization problem is converted into a DRL-based design optimization model. A hybrid reward is proposed to enhance the DRL agent trained by a deep deterministic policy gradient algorithm. Finally, the proposed DRL-based design optimization method is utilized to guide the optimal rotor speed scheduling of a 5-MW WT under given wind speed and rain intensity conditions. The results show that the proposed method could extend the predicted WTB blade coating fatigue life by 2.55 times with a minor reduction in the energy yield (0.027%) compared to the original rotor speed schedule that only considers maximum power capture. The computational time of the proposed method is reduced significantly compared to that of the traditional gradient and evolutional design optimization methods.}
}
@article{HE2023820,
title = {Reinforcement learning for multi-item retrieval in the puzzle-based storage system},
journal = {European Journal of Operational Research},
volume = {305},
number = {2},
pages = {820-837},
year = {2023},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2022.03.042},
url = {https://www.sciencedirect.com/science/article/pii/S0377221722002661},
author = {Jing He and Xinglu Liu and Qiyao Duan and Wai Kin (Victor) Chan and Mingyao Qi},
keywords = {Machine learning, Puzzle-based storage system, Deep reinforcement learning, Multi-item retrieval, Integer programming},
abstract = {Nowadays, fast delivery services have created the need for high-density warehouses. The puzzle-based storage system is a practical way to enhance the storage density, however, facing difficulties in the retrieval process. In this work, a deep reinforcement learning algorithm, specifically the Double&Dueling Deep Q Network, is developed to solve the multi-item retrieval problem in the system with general settings, where multiple desired items, escorts, and I/O points are placed randomly. Additionally, we propose a general compact integer programming model to evaluate the solution quality. Extensive numerical experiments demonstrate that the reinforcement learning approach can yield high-quality solutions and outperforms three related state-of-the-art heuristic algorithms. Furthermore, a conversion algorithm and a decomposition framework are proposed to handle simultaneous movement and large-scale instances respectively, thus improving the applicability of the PBS system.}
}
@article{MANTOUKA2022103770,
title = {Deep Reinforcement Learning for Personalized Driving Recommendations to Mitigate Aggressiveness and Riskiness: Modeling and Impact Assessment},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {142},
pages = {103770},
year = {2022},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103770},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22002029},
author = {Eleni G. Mantouka and Eleni I. Vlahogianni},
keywords = {Driving behavior, Driving recommendations, Deep reinforcement learning, DDPG, Microsimulation, Naturalistic driving, Clustering},
abstract = {Most driving recommendation and assistance systems, such as Advanced Driving Assistance Systems (ADAS), are usually designed based on the behavior of an average driver. Nevertheless, personalized driving systems that can be adapted to different driving styles and recognize individual needs and preferences, may be key to the sensitization of drivers and the adoption of safer driving habits. In this paper, an enhanced self-aware driving recommendation system is developed using a Deep Reinforcement Learning algorithm, which produces personalized driving recommendations with a view to improving driving safety, while respecting individual driving styles and preferences. The impact of applying this recommendation system is evaluated through microscopic simulation; findings revealed that, in case all drivers follow the suggestions, there is a significant improvement in road safety and some minor changes in traffic flow properties. The outputs of this work may be useful within the framework of an advanced active cruise control system, can be exploited in the development of enhanced behavioral models or even lead to the revision of policy measures that utilize driving behavior as a key controller of traffic management.}
}
@article{ZHANG2020107094,
title = {Deep reinforcement learning for condition-based maintenance planning of multi-component systems under dependent competing risks},
journal = {Reliability Engineering & System Safety},
volume = {203},
pages = {107094},
year = {2020},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2020.107094},
url = {https://www.sciencedirect.com/science/article/pii/S0951832020305950},
author = {Nailong Zhang and Wujun Si},
keywords = {Maintenance, Markov decision process, Deep Q network, Failure dependency, Cost minimization},
abstract = {Condition-Based Maintenance (CBM) planning for multi-component systems has been receiving increasing attention in recent years. Most existing research on CBM assumes that preventive maintenances should be conducted when the degradations of system components reach specific threshold levels upon inspection. However, the search of optimal maintenance threshold levels is often efficient for low-dimensional CBM but becomes challenging if the number of components gets large, especially when those components are subject to complex dependencies. To overcome the challenge, in this paper we propose a novel and flexible CBM model based on a customized deep reinforcement learning for multi-component systems with dependent competing risks. Both stochastic and economic dependencies among the components are considered. Specifically, different from the threshold-based decision making paradigm used in traditional CBM, the proposed model directly maps the multi-component degradation measurements at each inspection epoch to the maintenance decision space with a cost minimization objective, and the leverage of deep reinforcement learning enables high computational efficiencies and thus makes the proposed model suitable for both low and high dimensional CBM. Various numerical studies are conducted for model validations.}
}
@article{WANG2021107526,
title = {Deep reinforcement learning for transportation network combinatorial optimization: A survey},
journal = {Knowledge-Based Systems},
volume = {233},
pages = {107526},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107526},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007887},
author = {Qi Wang and Chunlei Tang},
keywords = {Combinatorial optimization, Deep learning, Reinforcement learning, Graph neural network},
abstract = {Traveling salesman and vehicle routing problems with their variants, as classic combinatorial optimization problems, have attracted considerable attention for decades of their theoretical and practical value. Many classic algorithms have been proposed, for example, exact algorithms, heuristic algorithms, solution solvers, etc. Still, due to their complexity, even the most advanced traditional methods require too much computational time or are not well-defined mathematically; algorithm-based decision-making is no exception. Also, these methods cannot be generalized to a larger scale or other similar problems. With the latest developments in machine and deep learning, people believe it is feasible to apply reinforcement learning and other technologies in the decision-making or heuristic for learning combinatorial optimization. In this paper, we first gave an overview on how combinate deep reinforcement learning for the NP-hard combinatorial optimization, emphasizing general optimization problems as data points and exploring the relevant distribution of data used for learning in a given task. We next reviewed state-of-the-art learning techniques related to combinational optimization problems on graphs. Then, we summarized the experimental methods of using reinforcement learning to solve combinatorial optimization problems and analyzed the performance comparison of different algorithms. Lastly, we sorted out the challenges encountered by deep reinforcement learning in solving combinatorial optimization problems and future research directions.}
}
@article{LIU20096995,
title = {Study of genetic algorithm with reinforcement learning to solve the TSP},
journal = {Expert Systems with Applications},
volume = {36},
number = {3, Part 2},
pages = {6995-7001},
year = {2009},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2008.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0957417408006064},
author = {Fei Liu and Guangzhou Zeng},
keywords = {TSP (traveling salesman problem), Genetic algorithm, Heterogeneous pairing selection, Reinforcement mutation},
abstract = {TSP (traveling salesman problem) is one of the typical NP-hard problems in combinatorial optimization problem. An improved genetic algorithm with reinforcement mutation, named RMGA, was proposed to solve the TSP in this paper. The core of RMGA lies in the use of heterogeneous pairing selection instead of random pairing selection in EAX and the construction of reinforcement mutation operator, named RL-M, by modifying the Q-learning algorithm and applying it to those individual generated from modified EAX. The experimental results on small and large size TSP instances in TSPLIB (traveling salesman problem library) have shown that RMGA could almost get optimal tour every time in reasonable time and thus outperformed the known EAX-GA and LKH in the quality of solutions and the running time.}
}
@article{WANG2023134,
title = {AOR: Adaptive opportunistic routing based on reinforcement learning for planetary surface exploration},
journal = {Computer Communications},
volume = {211},
pages = {134-146},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2023.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0140366423003183},
author = {Yijie Wang and Ziping Yu and Zhongliang Zhao and Xianbin Cao},
keywords = {Planetary surface exploration, Mobile ad hoc networks, Reinforcement learning, Opportunistic routing},
abstract = {Planetary surface exploration mobile ad-hoc networks (PSEMANET) show the characteristics of routing void caused by craters and electromagnetic interference, relatively high dynamics caused by node heterogeneity, and small node capacity caused by load constraints, which is the reason for the increase of communication delay and packet loss. Adaptive opportunistic routing based on reinforcement learning (AOR) is an opportunistic routing protocol that determines the forwarding node based on the current coordinates, queue length, and the number of neighbors. The full forwarding areas are used to avoid routing void, and the dual competition mechanism is designed to avoid the node hidden problem. The Q-value obtained by reinforcement learning is used to design the dynamic delay cost (DDC) so that nodes in the forwarding areas can compete to achieve adaptive path switching to the environment. Compared with representative proactive routing OLSR, reactive routing AODV, geographic routing GPSR and opportunistic routing BLR, AOR and AOR with memory(AOR-M) have high packet delivery ratio (PDR) and low end-to-end delay in planetary surface exploration scenarios implemented by our simulation system. And the AOR-M protocol shows the lowest expected end-to-end delay and highest channel utilization in both high and low speed scenarios. The result shows that the AOR-M provides efficient and robust routing in planetary surface exploration mobile ad-hoc networks with a complex environment, highly dynamic nodes, and performance constraints.}
}
@article{AGASUCCI2023100394,
title = {Solving the train dispatching problem via deep reinforcement learning},
journal = {Journal of Rail Transport Planning & Management},
volume = {26},
pages = {100394},
year = {2023},
issn = {2210-9706},
doi = {https://doi.org/10.1016/j.jrtpm.2023.100394},
url = {https://www.sciencedirect.com/science/article/pii/S2210970623000264},
author = {Valerio Agasucci and Giorgio Grani and Leonardo Lamorgese},
keywords = {Scheduling, Reinforcement learning, Optimization},
abstract = {Every day, railways experience disturbances and disruptions, both on the network and the fleet side, that affect the stability of rail traffic. Induced delays propagate through the network, which leads to a mismatch in demand and offer for goods and passengers, and, in turn, to a loss in service quality. In these cases, it is the duty of human traffic controllers, the so-called dispatchers, to do their best to minimize the impact on traffic. However, dispatchers inevitably have a limited depth of perception of the knock-on effect of their decisions, particularly how they affect areas of the network that are outside their direct control. In recent years, much work in Decision Science has been devoted to developing methods to solve the problem automatically and support the dispatchers in this challenging task. This paper investigates Machine Learning-based methods for tackling this problem, proposing two different Deep Q-Learning methods(Decentralized and Centralized). Numerical results show the superiority of these techniques respect to the classical linear Q-Learning based on matrices. Moreover the Centralized approach is compared with a MILP formulation showing interesting results. The experiments are inspired on data provided by a U.S. class 1 railroad.}
}
@article{HASSANPOUR2021100288,
title = {A hierarchical agent-based approach to simulate a dynamic decision-making process of evacuees using reinforcement learning},
journal = {Journal of Choice Modelling},
volume = {39},
pages = {100288},
year = {2021},
issn = {1755-5345},
doi = {https://doi.org/10.1016/j.jocm.2021.100288},
url = {https://www.sciencedirect.com/science/article/pii/S175553452100021X},
author = {Sajjad Hassanpour and Amir Abbas Rassafi and Vicente A. González and Jiamou Liu},
keywords = {Evacuation simulation, Hierarchical architecture, Agent-based models, Reinforcement learning, Discrete choice models},
abstract = {Simulation models are an undeniable tool to help researchers and designers forecast effects of definite policies regarding pedestrian social and collective movement behaviour. Considering both the environment's details and the complexity of human behaviour in choosing paths simultaneously is the main challenge in micro-simulation pedestrian dynamics models. This paper aims to present a novel comprehensive hierarchical agent-based simulation of pedestrian evacuation from a dynamic network of the environment using reinforcement learning, which is the closest to human behaviour among the other machine learning algorithms. In the approach, agents autonomously decide through a three-layer hierarchical model, including goal, node, and cell selection layers. A multinomial logit model is used to model the process of choosing the main movement direction at each time-step. The proposed model was successfully tested to simulate the pedestrian evacuation process from the Britomart Transport Centre platforms in Auckland during an abstract destructive event. Maximum evacuation flow, total evacuation time, average evacuation time, and average evacuation flow were investigated as dependent variables through different evacuation scenarios. The results from the approach can be used by designers and managers to optimise the quality of evacuation; also, the proposed model has the potential of becoming a potent tool for constructional management if coupled with other constructional tools.}
}
@article{KARA2018150,
title = {Reinforcement learning approaches for specifying ordering policies of perishable inventory systems},
journal = {Expert Systems with Applications},
volume = {91},
pages = {150-158},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.08.046},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417305900},
author = {Ahmet Kara and Ibrahim Dogan},
keywords = {Reinforcement learning, Inventory management system, Simulation-based optimization, Ordering management, Perishable item},
abstract = {In this study, we deal with the inventory management system of perishable products under the random demand and deterministic lead time in order to minimize the total cost of a retailer. We investigate two different ordering policies to emphasize the importance of the age information in the perishable inventory systems using Reinforcement Learning (RL). Stock-based policy replenishes stocks according to the stock quantities, and Age-based policy considers both inventory level and the age of the items in stock. The problem considered in this article has been modeled using Reinforcement Learning and the policies are optimized using Q-learning and Sarsa algorithms. The performance of the proposed policies compared with similar policies from the literature. The experiments demonstrate that the ordering policy which takes into account the age information appears to be an acceptable policy and learning with RL provides better results when demand has high variance and products has short lifetimes.}
}
@article{GUO2023100246,
title = {Function approximation reinforcement learning of energy management with the fuzzy REINFORCE for fuel cell hybrid electric vehicles},
journal = {Energy and AI},
volume = {13},
pages = {100246},
year = {2023},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2023.100246},
url = {https://www.sciencedirect.com/science/article/pii/S2666546823000186},
author = {Liang Guo and Zhongliang Li and Rachid Outbib and Fei Gao},
keywords = {Energy management strategy, Fuel cell hybrid electric vehicle, Reinforcement learning, Fuzzy inference system, Fuzzy policy gradient, Hardware-in-loop},
abstract = {In the paper, a novel self-learning energy management strategy (EMS) is proposed for fuel cell hybrid electric vehicles (FCHEV) to achieve the hydrogen saving and maintain the battery operation. In the EMS, it is proposed to approximate the EMS policy function with fuzzy inference system (FIS) and learn the policy parameters through policy gradient reinforcement learning (PGRL). Thus, a so-called Fuzzy REINFORCE algorithm is first proposed and studied for EMS problem in the paper. Fuzzy REINFORCE is a model-free method that the EMS agent can learn itself through interactions with environment, which makes it independent of model accuracy, prior knowledge, and expert experience. Meanwhile, to stabilize the training process, a fuzzy baseline function is adopted to approximate the value function based on FIS without affecting the policy gradient direction. Moreover, the drawbacks of traditional reinforcement learning such as high computation burden, long convergence time, can also be overcome. The effectiveness of the proposed methods were verified by Hardware-in-Loop experiments. The adaptability of the proposed method to the changes of driving conditions and system states is also verified.}
}
@article{SCHAMBERG2022102227,
title = {Continuous action deep reinforcement learning for propofol dosing during general anesthesia},
journal = {Artificial Intelligence in Medicine},
volume = {123},
pages = {102227},
year = {2022},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102227},
url = {https://www.sciencedirect.com/science/article/pii/S0933365721002207},
author = {Gabriel Schamberg and Marcus Badgeley and Benyamin Meschede-Krasa and Ohyoon Kwon and Emery N. Brown},
keywords = {Anesthesia, Reinforcement learning},
abstract = {Purpose
Anesthesiologists simultaneously manage several aspects of patient care during general anesthesia. Automating administration of hypnotic agents could enable more precise control of a patient's level of unconsciousness and enable anesthesiologists to focus on the most critical aspects of patient care. Reinforcement learning (RL) algorithms can be used to fit a mapping from patient state to a medication regimen. These algorithms can learn complex control policies that, when paired with modern techniques for promoting model interpretability, offer a promising approach for developing a clinically viable system for automated anesthestic drug delivery.
Methods
We expand on our prior work applying deep RL to automated anesthetic dosing by now using a continuous-action model based on the actor-critic RL paradigm. The proposed RL agent is composed of a policy network that maps observed anesthetic states to a continuous probability density over propofol-infusion rates and a value network that estimates the favorability of observed states. We train and test three versions of the RL agent using varied reward functions. The agent is trained using simulated pharmacokinetic/pharmacodynamic models with randomized parameters to ensure robustness to patient variability. The model is tested on simulations and retrospectively on nine general anesthesia cases collected in the operating room. We utilize Shapley additive explanations to gain an understanding of the factors with the greatest influence over the agent's decision-making.
Results
The deep RL agent significantly outperformed a proportional-integral-derivative controller (median episode median absolute performance error 1.9% ± 1.8 and 3.1% ± 1.1). The model that was rewarded for minimizing total doses performed the best across simulated patient demographics (median episode median performance error 1.1% ± 0.5). When run on real-world clinical datasets, the agent recommended doses that were consistent with those administered by the anesthesiologist.
Conclusions
The proposed approach marks the first fully continuous deep RL algorithm for automating anesthestic drug dosing. The reward function used by the RL training algorithm can be flexibly designed for desirable practices (e.g. use less anesthetic) and bolstered performances. Through careful analysis of the learned policies, techniques for interpreting dosing decisions, and testing on clinical data, we confirm that the agent's anesthetic dosing is consistent with our understanding of best-practices in anesthesia care.}
}
@article{SUN2020107255,
title = {SmartFCT: Improving power-efficiency for data center networks with deep reinforcement learning},
journal = {Computer Networks},
volume = {179},
pages = {107255},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107255},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620300384},
author = {Penghao Sun and Zehua Guo and Sen Liu and Julong Lan and Junchao Wang and Yuxiang Hu},
keywords = {Data center networks, Software-Defined networking, Power efficiency, Flow completion time, Deep reinforcement learning},
abstract = {Reducing the power consumption of Data Center Networks (DCNs) and guaranteeing the Flow Completion Time (FCT) of applications in DCNs are two major concerns for data center operators. However, existing works cannot realize the two goals together because of two issues: (1) dynamic traffic pattern in DCNs is hard to accurately model; (2) an optimal flow scheduling scheme is computationally expensive. In this paper, we propose SmartFCT, which employs the Deep Reinforcement Learning (DRL) coupled with Software-Defined Networking (SDN) to improve the power efficiency of DCNs and guarantee FCT. SmartFCT dynamically collects traffic distribution from switches to train its DRL model. The well-trained DRL agent of SmartFCT can quickly analyze the complicated traffic characteristics using neural networks and adaptively generate a action for scheduling flows and deliberately configuring margins for different links. Following the generated action, flows are consolidated into a few of active links and switches for saving power, and fine-grained margin configuration for active links avoids FCT violation of unexpected flow bursts. Simulation results show that SmartFCT can guarantee FCT and save up to 12.2% power consumption, compared with the state-of-the-art solutions.}
}
@article{JENDOUBI2022100919,
title = {Data-driven sustainable distributed energy resources’ control based on multi-agent deep reinforcement learning},
journal = {Sustainable Energy, Grids and Networks},
volume = {32},
pages = {100919},
year = {2022},
issn = {2352-4677},
doi = {https://doi.org/10.1016/j.segan.2022.100919},
url = {https://www.sciencedirect.com/science/article/pii/S2352467722001643},
author = {Imen Jendoubi and François Bouffard},
keywords = {Data-driven, Deep reinforcement learning, Eco-neighborhood, Energy storage systems, Microgrid, Multi-agent reinforcement learning, Power dispatch},
abstract = {With the ongoing energy transition, electric power and energy systems are becoming increasingly multi-dimensional and complex with higher levels of uncertainty. Obtaining an accurate system model or multiple predictions in such environments is becoming increasingly problematic. Such features are challenging aspects to the classical power system control approaches which are essentially model-based and rely on accurate predictions. Reinforcement learning is promising a data-driven alternative that can efficiently tackle the raising complexity of controlling such systems with no prior system dynamics modeling or predictions. This work proposes a multi-agent deep reinforcement learning-based control framework for optimally solving multi-dimensional power dispatch problems in systems featuring multiple uncertainties. Learned control strategies are based on centralized training decentralized execution to promote an efficient and robust coordination among different dispatchable assets with no communication burden. Experimental results of various typical power dispatch scenarios with significant integration of low-carbon generation demonstrate the effectiveness of such control strategies.}
}
@article{ALI2023108811,
title = {Efficient congestion control in communications using novel weighted ensemble deep reinforcement learning},
journal = {Computers and Electrical Engineering},
volume = {110},
pages = {108811},
year = {2023},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2023.108811},
url = {https://www.sciencedirect.com/science/article/pii/S0045790623002355},
author = {Majid Hamid Ali and Serkan Öztürk},
keywords = {Active queue management, Deep reinforcement learning, Weighted ensemble approach, Network throughput, Random early detection},
abstract = {In this paper, we introduce Deep Reinforcement Learning (DRL) for congestion control in the Transmission Control Protocol/Internet Protocol (TCP/IP) networks. We propose a weighted ensemble DRL model that combines four DRL models Deep Q-Learning (DQN), Proximal Policy Optimisation (PPO), Deep Deterministic Policy Gradient (DDPG), and Twin Delay DDPG (rlTD3). These four models are designed with varying action spaces for efficient congestion control in an Active Queue Management (AQM) system. The proposed model outperformed single DRL models and established congestion control algorithms like Random Early Detection (RED) in normal and stress testing. The proposed model improved throughput by 4% and delays by 10.52% compared to DQN and 2.92% compared to DDPG. The proposed model has shown promising results in managing congestion in dynamic network environments and handling high-traffic loads.}
}
@article{WANG2023150,
title = {Dual-attention assisted deep reinforcement learning algorithm for energy-efficient resource allocation in Industrial Internet of Things},
journal = {Future Generation Computer Systems},
volume = {142},
pages = {150-164},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22004137},
author = {Ying Wang and Fengjun Shang and Jianjun Lei and Xiangwei Zhu and Haoming Qin and Jiayu Wen},
keywords = {Industrial Internet of Things, Energy-efficient resource allocation, Deep reinforcement learning, Multi-scale convolutional attention, Multi-head self-attention},
abstract = {This paper investigates the distributed resource allocation problem for energy-efficient data forwarding in resource-constrained Industrial Internet of Things (IIoT). We formulate this problem as a decentralized partially observable Markov decision process (Dec-POMDP) and propose a novel energy-efficient resource allocation algorithm based on the dual-attention assisted deep reinforcement learning (DRL) model, namely DADR, which adopts the centralized training and distributed executed (CTDE) framework to provide the intelligent decision-making ability for resource-constrained nodes. In DADR, we design a multi-scale convolutional attention module (CAM) in actor network that can extract the local state’s feature information from different dimensions; meanwhile, we propose a novel critic network based on dual-attention module and experience reconstruction module which provides a more objective and correct state evaluation from a global perspective. Moreover, the proposed critic network can solve the partially observable and non-stationary problems in multi-agent systems and can flexibly scale without changing the model structure even in a dynamic environment. Furthermore, the cooperation between CAM and multi-head self-attention (MHSA) in DADR improves the representation learning ability of DRL and offers better optimization direction for the DRL model to maximize the energy-efficient and data transmission reliability. Simulation results demonstrate that the proposed DADR algorithm outperforms the existing resource allocation algorithms and MARL models in terms of network lifetime, transmission reliability, and network stability, respectively.}
}
@article{HAN2019113708,
title = {Energy management based on reinforcement learning with double deep Q-learning for a hybrid electric tracked vehicle},
journal = {Applied Energy},
volume = {254},
pages = {113708},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.113708},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919313959},
author = {Xuefeng Han and Hongwen He and Jingda Wu and Jiankun Peng and Yuecheng Li},
keywords = {Energy management, Reinforcement learning, Double deep Q-learning, Hybrid vehicle, Tracked-vehicle},
abstract = {An energy management strategy, based on double deep Q-learning algorithm, is proposed for a dual-motor driven hybrid electric tracked-vehicle. Typical model framework of tracked-vehicle is established where the lateral dynamic can be taken into consideration. For the propose of optimizing the fuel consumption performance, a double deep Q-learning-based control structure is put forward. Compared to conventional deep Q-learning, the proposed strategy prevents training process falling into the overoptimistic estimate of policy value and highlights its significant advantages in terms of the iterative convergence rate and optimization performance. Unique observation states are selected as input variables of reinforcement learning algorithm in view of revealing tracked-vehicles characteristic. The conventional deep Q-learning and dynamic programming are also employed and compared with the proposed strategy for different driving schedules. Simulation results demonstrate the fuel economy of proposed methodology achieves 7.1% better than that of conventional deep Q learning-based strategy and reaches 93.2% level of Dynamic programing benchmark. Moreover, the designed algorithm has a good performance in battery SOC retention with different initial values.}
}
@article{UCHIBE2021138,
title = {Forward and inverse reinforcement learning sharing network weights and hyperparameters},
journal = {Neural Networks},
volume = {144},
pages = {138-153},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003221},
author = {Eiji Uchibe and Kenji Doya},
keywords = {Reinforcement learning, Inverse reinforcement learning, Imitation learning, Entropy regularization},
abstract = {This paper proposes model-free imitation learning named Entropy-Regularized Imitation Learning (ERIL) that minimizes the reverse Kullback–Leibler (KL) divergence. ERIL combines forward and inverse reinforcement learning (RL) under the framework of an entropy-regularized Markov decision process. An inverse RL step computes the log-ratio between two distributions by evaluating two binary discriminators. The first discriminator distinguishes the state generated by the forward RL step from the expert’s state. The second discriminator, which is structured by the theory of entropy regularization, distinguishes the state–action–next-state tuples generated by the learner from the expert ones. One notable feature is that the second discriminator shares hyperparameters with the forward RL, which can be used to control the discriminator’s ability. A forward RL step minimizes the reverse KL estimated by the inverse RL step. We show that minimizing the reverse KL divergence is equivalent to finding an optimal policy. Our experimental results on MuJoCo-simulated environments and vision-based reaching tasks with a robotic arm show that ERIL is more sample-efficient than the baseline methods. We apply the method to human behaviors that perform a pole-balancing task and describe how the estimated reward functions show how every subject achieves her goal.}
}
@article{WANG2022118575,
title = {Multi-agent deep reinforcement learning for resilience-driven routing and scheduling of mobile energy storage systems},
journal = {Applied Energy},
volume = {310},
pages = {118575},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.118575},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922000563},
author = {Yi Wang and Dawei Qiu and Goran Strbac},
keywords = {Resilience, Mobile energy storage systems, Multi-agent deep reinforcement learning, Hybrid discrete-continuous action space, Linearized AC-OPF, Power and transportation networks},
abstract = {Extreme events are featured by high impact and low probability, which can cause severe damage to power systems. There has been much research focused on resilience-driven operational problems incorporating mobile energy storage systems (MESSs) routing and scheduling due to its mobility and flexibility. However, existing literature focuses on model-based optimization approaches to implement the routing process of MESSs, which can be time consuming and raise privacy issues since the requirement for global information of both power and transportation networks. Furthermore, a real-time automatic control scheme of MESSs has become a challenging task due to the system high variability. As such, this paper develops a model-free real-time multi-agent deep reinforcement learning approach featuring parameterized double deep Q-networks to reformulate the coordination effect of MESSs routing and scheduling process as a Partially Observable Markov Game, which is capable of capturing a hybrid policy including both discrete and continuous actions. A coupled transportation network and linearized AC-OPF algorithm are realized as the environment, while the internal uncertainties associated with renewable energy sources, load profiles, line outages, and traffic volumes are incorporated into the proposed data-driven approach through learning procedure. Extensive case studies including both 6-bus and 33-bus power networks are developed to evaluate the effectiveness of the proposed approach. Specifically, a detailed comparison between different multi-agent reinforcement learning and model-based optimization approaches is conducted to present the superior performance of the proposed approach.}
}
@article{HE2023109985,
title = {A multi-agent virtual market model for generalization in reinforcement learning based trading strategies},
journal = {Applied Soft Computing},
volume = {134},
pages = {109985},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.109985},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623000030},
author = {Fei-Fan He and Chiao-Ting Chen and Szu-Hao Huang},
keywords = {Multi-agent systems, Market simulation, Reinforcement learning, Trading strategy},
abstract = {Many studies have successfully used reinforcement learning (RL) to train an intelligent agent that learns profitable trading strategies from financial market data. Most of RL trading studies have simplified the effect of the actions of the trading agent on the market state. The trading agent is trained to maximize long-term profit by optimizing fixed historical data. However, such approach frequently results in the trading performance during out-of-sample validation being considerably different from that during training. In this paper, we propose a multi-agent virtual market model (MVMM) comprised of multiple generative adversarial networks (GANs) which cooperate with each other to reproduce market price changes. In addition, the action of the trading agent can be superimposed on the current state as the input of the MVMM to generate an action-dependent next state. In this research, real historical data were replaced with the simulated market data generated by the MVMM. The experimental results indicated that the trading strategy of the trained RL agent achieved a 12% higher profit and exhibited low risk of loss in the 2019 China Shanghai Shenzhen 300 stock index futures backtest.}
}
@article{YAN20231218,
title = {An online reinforcement learning approach to charging and order-dispatching optimization for an e-hailing electric vehicle fleet},
journal = {European Journal of Operational Research},
volume = {310},
number = {3},
pages = {1218-1233},
year = {2023},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2023.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0377221723002667},
author = {Pengyu Yan and Kaize Yu and Xiuli Chao and Zhibin Chen},
keywords = {Transportation, Electric vehicle, Charging and dispatching decision, Reinforcement learning, Markov decision process},
abstract = {Given the uncertainty of orders and the dynamically changing workload of charging stations, how to dispatch and charge electric vehicle (EV) fleets becomes a significant challenge facing e-hailing platforms. The common practice is to dispatch EVs to serve orders by heuristic matching methods but enable EV drivers to independently make charging decisions based on their experiences, which may compromise the platform’s performance. This study proposes a Markov decision process to jointly optimize the charging and order-dispatching schemes for an e-hailing EV fleet, which provides pick-up services for passengers only from a designated transportation hub (i.e., no pick-up from different locations). The objective is to maximize the total revenue of the fleet throughout a finite horizon. The complete state transition equations of the EV fleet are formulated to track the state-of-charge of their batteries. To learn the charging and order-dispatching policy in a dynamic stochastic environment, an online approximation algorithm is developed, which integrates the model-based reinforcement learning (RL) framework with a novel SARSA(Δ)-sample average approximation (SAA) architecture. Compared with the model-free RL algorithm and approximation dynamic programming (ADP), our algorithm explores high-quality decisions by an SAA model with empirical state transitions and exploits the best decisions so far by an SARSA(Δ) sample-trajectory updating. Computational results based on a real case show that, compared with the existing heuristic method and the ADP in the literature, the proposed approach increases the daily revenue by an average of 31.76% and 14.22%, respectively.}
}
@article{MAHADEVAN1992311,
title = {Automatic programming of behavior-based robots using reinforcement learning},
journal = {Artificial Intelligence},
volume = {55},
number = {2},
pages = {311-365},
year = {1992},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(92)90058-6},
url = {https://www.sciencedirect.com/science/article/pii/0004370292900586},
author = {Sridhar Mahadevan and Jonathan Connell},
abstract = {This paper describes a general approach for automatically programming a behavior-based robot. New behaviors are learned by trial and error using a performance feedback function as reinforcement. Two algorithms for behavior learning are described that combine Q learning, a well-known scheme for propagating reinforcement values temporally across actions, with statistical clustering and Hamming distance, two ways of propagating reinforcement values spatially across states. A real behavior-based robot called OBELIX is described that learns several component behaviors in an example task involving pushing boxes. A simulator for the box pushing task is also used to gather data on the learning techniques. A detailed experimental study using the real robot and the simulator suggests two conclusions. 1.(1) The learning techniques are able to learn the individual behaviors, sometimes outperforming a handcoded program.2.(2) Using a behavior-based architecture speeds up reinforcement learning by converting the problem of learning a complex task into that of learning a simpler set of special-purpose reactive subtasks.}
}
@article{JAAFRA201957,
title = {Reinforcement learning for neural architecture search: A review},
journal = {Image and Vision Computing},
volume = {89},
pages = {57-66},
year = {2019},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0262885619300885},
author = {Yesmina Jaafra and Jean {Luc Laurent} and Aline Deruyver and Mohamed {Saber Naceur}},
keywords = {Reinforcement learning, Convolutional neural networks, Neural Architecture Search, AutoML},
abstract = {Deep neural networks are efficient and flexible models that perform well for a variety of tasks such as image, speech recognition and natural language understanding. In particular, convolutional neural networks (CNN) generate a keen interest among researchers in computer vision and more specifically in classification tasks. CNN architecture and related hyperparameters are generally correlated to the nature of the processed task as the network extracts complex and relevant characteristics allowing the optimal convergence. Designing such architectures requires significant human expertise, substantial computation time and does not always lead to the optimal network. Reinforcement learning (RL) has been extensively used in automating CNN models design generating notable advances and interesting results in the field. This work aims at reviewing and discussing the recent progress of RL methods in Neural Architecture Search task and the current challenges that still require further consideration.}
}
@article{ITO2024124788,
title = {Optimisation of initial velocity distribution of jets for entrainment and diffusion control using deep reinforcement learning},
journal = {International Journal of Heat and Mass Transfer},
volume = {218},
pages = {124788},
year = {2024},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2023.124788},
url = {https://www.sciencedirect.com/science/article/pii/S001793102300933X},
author = {Yasumasa Ito and Yusuke Hayashi and Koji Iwano and Takahiro Katagiri},
keywords = {Jet control, Entrainment, Diffusion, Deep reinforcement learning},
abstract = {The control of the entrainment and diffusion of the heat and mass ejected along the jet flow requires the control of fluid motion. In the present study, optimal initial velocity distributions for suppressing and promoting jet entrainment and diffusion were identified using deep reinforcement learning under constant initial jet flow rate conditions. Two-dimensional jet flow simulations using OpenFOAM were combined with a deep Q-network as the learning algorithm. The spatial distribution of the initial jet (velocity and ejection angle) changed. The results show that to suppress the entrainment of the jet, an optimal initial velocity distribution shape in which the velocity in the centre is almost 0, with an inward velocity just outside of it, and a large outward velocity at the outer edge, is desirable. However, to promote entrainment, a shape with a large velocity in the centre portion and small velocity at the outer edge, with an overall inward angle, is desirable. To suppress the diffusion of the jet, an optimal initial velocity distribution shape with a large velocity in the centre portion, a small velocity at the outer edge, and an overall inward angle is desirable. Conversely, to promote diffusion, a shape with a large outward velocity at both the centre and outer edges is desirable. It is also indicated that, when the initial velocity distribution of the jet has a special shape, and the relationship between flow rate, centre velocity, and centre temperature that is normally observed in a jet flow may not hold. In other words, by changing the initial velocity distribution, the entrainment and diffusion of the jet can be separated and controlled, which is usually difficult to achieve using conventional engineering methods.}
}
@article{SONG2023120625,
title = {Learning disentangled skills for hierarchical reinforcement learning through trajectory autoencoder with weak labels},
journal = {Expert Systems with Applications},
volume = {230},
pages = {120625},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120625},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423011272},
author = {Wonil Song and Sangryul Jeon and Hyesong Choi and Kwanghoon Sohn and Dongbo Min},
keywords = {Deep reinforcement learning, Hierarchical reinforcement learning, Skill learning, Variational autoencoder, Disentangled representation, Weak label, Planning},
abstract = {Typically, hierarchical reinforcement learning (RL) requires skills that are applicable to various downstream tasks. Although several recent studies have proposed the supervised and unsupervised learning of such skills, the learned skills are often entangled, which hinders their interpretation. To alleviate this, we propose a novel method to use weak labels for learning disentangled skills from the continuous latent representations of trajectories. To this end, we extended a trajectory variational autoencoder (VAE) to impose an inductive bias using weak labels, which explicitly enforces the disentangling of the trajectory representations into factors of interest intended for the model to learn. Using the latent representations as skills, a skill-based policy network is trained to generate trajectories similar to the learned decoder of the trajectory VAE. Furthermore, using the disentangled skill, we propose a skill repetition that can expand the entire trajectories generated by the policy at test time, resulting in an effective planning strategy. Experiments were performed on several challenging navigation tasks in mazes, and the results demonstrate the effectiveness of our method at solving hierarchical RL problems even with a long horizon and sparse rewards.}
}
@article{WANG2020115036,
title = {Reinforcement learning for building controls: The opportunities and challenges},
journal = {Applied Energy},
volume = {269},
pages = {115036},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.115036},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920305481},
author = {Zhe Wang and Tianzhen Hong},
keywords = {Building controls, Reinforcement learning, Machine learning, Optimization, Building performance},
abstract = {Building controls are becoming more important and complicated due to the dynamic and stochastic energy demand, on-site intermittent energy supply, as well as energy storage, making it difficult for them to be optimized by conventional control techniques. Reinforcement Learning (RL), as an emerging control technique, has attracted growing research interest and demonstrated its potential to enhance building performance while addressing some limitations of other advanced control techniques, such as model predictive control. This study conducted a comprehensive review of existing studies that applied RL for building controls. It provided a detailed breakdown of the existing RL studies that use a specific variation of each major component of the Reinforcement Learning: algorithm, state, action, reward, and environment. We found RL for building controls is still in the research stage with limited applications (11%) in real buildings. Three significant barriers prevent the adoption of RL controllers in actual building controls: (1) the training process is time consuming and data demanding, (2) the control security and robustness need to be enhanced, and (3) the generalization capabilities of RL controllers need to be improved using approaches such as transfer learning. Future research may focus on developing RL controllers that could be used in real buildings, addressing current RL challenges, such as accelerating training and enhancing control robustness, as well as developing an open-source testbed and dataset for performance benchmarking of RL controllers.}
}
@article{YAZDJERDI201915,
title = {Reinforcement learning-based control of tumor growth under anti-angiogenic therapy},
journal = {Computer Methods and Programs in Biomedicine},
volume = {173},
pages = {15-26},
year = {2019},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S016926071831887X},
author = {Parisa Yazdjerdi and Nader Meskin and Mohammad Al-Naemi and Ala-Eddin {Al Moustafa} and Levente Kovács},
keywords = {Anti-angiogenic therapy, Angiogenesis, Drug administration control, Reinforcement learning},
abstract = {Background and objectives: In recent decades, cancer has become one of the most fatal and destructive diseases which is threatening humans life. Accordingly, different types of cancer treatment are studied with the main aim to have the best treatment with minimum side effects. Anti-angiogenic is a molecular targeted therapy which can be coupled with chemotherapy and radiotherapy. Although this method does not eliminate the whole tumor, but it can keep the tumor size in a given state by preventing the formation of new blood vessels. In this paper, a novel model-free method based on reinforcement learning (RL) framework is used to design a closed-loop control of anti-angiogenic drug dosing administration. Methods: A Q-learning algorithm is developed for the drug dosing closed-loop control. This controller is designed using two different values of the maximum drug dosage to reduce the tumor volume up to a desired value. The mathematical model of tumor growth under anti-angiogenic inhibitor is used to simulate a real patient. Results: The effectiveness of the proposed method is shown through in silico simulation and its robustness to patient parameters variation is demonstrated. It is demonstrated that the tumor reaches its minimal volume in 84 days with maximum drug inlet of 30 mg/kg/day. Also, it is shown that the designed controller is robust with respect to  ± 20% of tumor growth parameters changes. Conclusion: The proposed closed-loop reinforcement learning-based controller for cancer treatment using anti-angiogenic inhibitor provides an effective and novel result such that with a clinically valid and safe dosage of drug, the volume reduces up to 1mm3 in a reasonable short period compared to the literature.}
}
@article{CAI2023105793,
title = {Energy management in residential microgrid using model predictive control-based reinforcement learning and Shapley value},
journal = {Engineering Applications of Artificial Intelligence},
volume = {119},
pages = {105793},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105793},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622007837},
author = {Wenqi Cai and Arash Bahari Kordabad and Sébastien Gros},
keywords = {Reinforcement learning (RL), Model predictive control (MPC), Microgrid (MG), Energy management(EM), Cooperative coalition game (CCG), Shapley value, Profit distribution},
abstract = {This paper presents an Energy Management (EM) strategy for residential microgrid systems using Model Predictive Control (MPC)-based Reinforcement Learning (RL) and Shapley value. We construct a typical residential microgrid system that considers fluctuating spot-market prices, highly uncertain user demand and renewable generation, and collective peak power penalties. To optimize the benefits for all residential prosumers, the EM problem is formulated as a Cooperative Coalition Game (CCG). The objective is to first find an energy trading policy that reduces the collective economic cost (including spot-market cost and peak-power cost) of the residential coalition, and then to distribute the profits obtained through cooperation to all residents. An MPC-based RL approach, which compensates for the shortcomings of MPC and RL and benefits from the advantages of both, is proposed to reduce the monthly collective cost despite the system uncertainties. To determine the amount of monthly electricity bill each resident should pay, we transfer the cost distribution problem into a profit distribution problem. Then, the Shapley value approach is applied to equitably distribute the profits (i.e., cost savings) gained through cooperation to all residents based on the weighted average of their respective marginal contributions. Finally, simulations are performed on a three-household microgrid system located in Oslo, Norway, to validate the proposed strategy, where a real-world dataset of April 2020 is used. Simulation results show that the proposed MPC-based RL approach could effectively reduce the long-term economic cost by about 17.5%, and the Shapley value method provides a solution for allocating the collective bills fairly.}
}
@article{FAN2022130274,
title = {A deep reinforcement learning-based method for predictive management of demand response in natural gas pipeline networks},
journal = {Journal of Cleaner Production},
volume = {335},
pages = {130274},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.130274},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621044395},
author = {Lin Fan and Huai Su and Enrico Zio and Lixun Chi and Li Zhang and Jing Zhou and Zhe Liu and Jinjun Zhang},
keywords = {Demand response, Natural gas pipeline network, Reinforcement learning, Deep Q learning},
abstract = {With the increase of natural gas in the world's energy consumption, the efficient and reliable management of natural gas pipeline networks is becoming even more important than before. In recent years, Demand Response (DR) is considered an effective approach for cleaner production and economic strategy, by introducing the participation of customers (CUs). This paper proposes a novel DR method for predictive management in multi-level natural gas markets with different stakeholders. This method is able to make a better trade-off among supplier's profits, gas demand volatility and CU satisfaction. This method includes three parts: dynamic pricing model, intelligent decision making and data-driven demand forecasting. A Markov decision process-based model is developed to illustrate the process of dynamical optimizing energy prices. Then, deep learning and reinforcement learning are integrated to efficiently solve the sequential decision-making problem, based on the physics constraints of natural gas pipeline networks. Besides, to realize the function of predictive optimization, an energy demand forecasting model is developed based on the deep recurrent neural network model. The proposed dynamic pricing method is able to optimize the pricing strategies in accordance to the demand patterns, and dynamically improve the system stability and energy efficiency. Finally, we apply the developed method to a natural gas network with relatively complex topology and different CUs. The results indicate that the proposed method can achieve the targets of peak shaving and valley filling under different pricing periods. Besides, the sensitivity analysis of the critical parameters in the dynamic pricing model is analyzed in detail, which can give a solid criterion for ensuring the effectiveness of this framework.}
}
@article{ABKAR2023100475,
title = {Reinforcement learning for wind-farm flow control: Current state and future actions},
journal = {Theoretical and Applied Mechanics Letters},
pages = {100475},
year = {2023},
issn = {2095-0349},
doi = {https://doi.org/10.1016/j.taml.2023.100475},
url = {https://www.sciencedirect.com/science/article/pii/S2095034923000466},
author = {Mahdi Abkar and Navid Zehtabiyan-Rezaie and Alexandros Iosifidis},
keywords = {Wind-farm flow control, Turbine wakes, Power losses, Reinforcement learning, Machine learning},
abstract = {Wind-farm flow control stands at the forefront of grand challenges in wind-energy science. The central issue is that current algorithms are based on simplified models and, thus, fall short of capturing the complex physics of wind farms associated with the high-dimensional nature of turbulence and multiscale wind-farm-atmosphere interactions. Reinforcement learning (RL), as a subset of machine learning, has demonstrated its effectiveness in solving high-dimensional problems in various domains, and the studies performed in the last decade prove that it can be exploited in the development of the next generation of algorithms for wind-farm flow control. This review has two main objectives. Firstly, it aims to provide an up-to-date overview of works focusing on the development of wind-farm flow control schemes utilizing RL methods. By examining the latest research in this area, the review seeks to offer a comprehensive understanding of the advancements made in wind-farm flow control through the application of RL techniques. Secondly, it aims to shed light on the obstacles that researchers face when implementing wind-farm flow control based on RL. By highlighting these challenges, the review aims to identify areas requiring further exploration and potential opportunities for future research.}
}
@article{YANG2023442,
title = {Leveraging transition exploratory bonus for efficient exploration in Hard-Transiting reinforcement learning problems},
journal = {Future Generation Computer Systems},
volume = {145},
pages = {442-453},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2300136X},
author = {Shangdong Yang and Huihui Wang and Shaokang Dong and Xingguo Chen},
keywords = {Decision spatiotemporal data, Reinforcement learning, Sparse reward, Hard-Transiting},
abstract = {In reinforcement learning (RL), agents learn policies from spatiotemporal data generated by interaction with the environment. However, spatiotemporal data containing reward signals are often sparse, the agent only gets a reward signal when it reaches the goal state. This kind of problem is technically challenging since rewards are the basis of policy learning. Recently, the prior knowledge of the task structure has been used to solve such problems such as hierarchical learning and goal-conditioned learning. In this paper, we consider a common task structure called Hard-Transiting. In a Hard-Transiting task, the difficulty of moving forward of the agent increases as it approaches the goal state. We formalize the Hard-Transiting RL problem with sparse rewards in which the transition probability decreases as the agent approaches the goal state. Inspired by reward bonus in interactive spatiotemporal data, we propose two novel algorithms with efficient exploration to solve such problems. For tabular setting, we adopt Transition Exploratory Bonus (TEB) to encourage exploration in Hard-Transiting problems and propose Model-Based Interval Estimation-TEB (MBIE-TEB) in which TEB is considered in the value iteration phase of the conventional MBIE algorithm. And under the performance metric of sample complexity, we give a theoretical proof of the upper bound of MBIE-TEB. For non-tabular setting, we propose Deep Q-Network-TEB (DQN-TEB) in which TEB is used as the intrinsic motivation in DQN. We test the proposed algorithms on two numerical tasks and one large-scale task. And the experimental results demonstrate that with the transition exploratory bonus, the proposed algorithms outperform the compared algorithms.}
}
@article{LIAO2022,
title = {Modelling personalised car-following behaviour: a memory-based deep reinforcement learning approach},
journal = {Transportmetrica A Transport Science},
year = {2022},
issn = {2324-9935},
doi = {https://doi.org/10.1080/23249935.2022.2035846},
url = {https://www.sciencedirect.com/science/article/pii/S2324993522006728},
author = {Yaping Liao and Guizhen Yu and Peng Chen and Bin Zhou and Han Li},
keywords = {Car-following, autonomous driving, twin delayed deep deterministic policy gradients, long short-term memory, driving styles},
abstract = {ABSTRACT
To adapt to human-driving habits, this study develops a personalised car-following model via a memory-based deep reinforcement learning approach. Specifically, Twin Delayed Deep Deterministic Policy Gradients (TD3) is integrated with a long short-term memory (LSTM) (abbreviated as LSTM-TD3). Using the NGSIM dataset, unsupervised learning-based clustering and data feature analyses are performed. The driving characteristics related to safety, efficiency and comfort are extracted for different driving styles, i.e. aggressive, common and conservative. Then, reward functions are constructed for different driving styles by incorporating their driving characteristics. By resorting to the TD3 policy within a recurrent actor–critic framework, LSTM-TD3 optimises the car-following behaviour via trial-and-error interactions according to the reward functions. Results show that compared with LSTM-DDPG and DDPG, LSTM-TD3 reproduces personalised car-following behaviour with desirable convergence speed and reward. It reveals that LSTM-TD3 can reflect the essential difference in safety, efficiency and comfort requirements among different driving styles.}
}
@article{ESILVAVIEIRA2023100594,
title = {Towards sample efficient deep reinforcement learning in collectible card games},
journal = {Entertainment Computing},
volume = {47},
pages = {100594},
year = {2023},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100594},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000496},
author = {Ronaldo {e Silva Vieira} and Anderson Rocha Tavares and Luiz Chaimowicz},
keywords = {Reinforcement learning, Collectible card games},
abstract = {Collectible card games (CCGs) are widely-played games in which players build a deck from a set of custom cards and use it to battle each other. They are notoriously more challenging than games such as Go and Texas Hold’em Poker, the protagonists of recent breakthroughs in game-playing AI. Deep reinforcement learning approaches have recently become state-of-the-art in CCGs, although requiring huge amounts of computational power to train. In this paper, we propose a collection of deep reinforcement learning approaches to battling in CCGs that are trainable on a single desktop computer. Each approach tries different mechanisms to increase sample efficiency. We use Legends of Code and Magic, a CCG designed for AI research, as a testbed and compare our approaches to each other, considering their win rate and other metrics. Then, we discuss the position of our approaches regarding the current literature, their limitations, directions of improvement, and extension to commercial CCGs.}
}
@article{LI2023104917,
title = {Multiagent deep meta reinforcement learning for sea computing-based energy management of interconnected grids considering renewable energy sources in sustainable cities},
journal = {Sustainable Cities and Society},
volume = {99},
pages = {104917},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2023.104917},
url = {https://www.sciencedirect.com/science/article/pii/S2210670723005280},
author = {Jiawen Li and Tao Zhou},
keywords = {Sustainable city, Load frequency control, Artificial intelligence, Deep meta-reinforcement learning, Sea computing, Data-driven coordinated control},
abstract = {In a sustainable city with a large amount of renewable energy, the difficulty inherent in the coordination of the load frequency control strategies of grid units and area units lead to severe frequency fluctuations. Conventional load frequency control has difficulty overcoming the above problems due to its lack of adaptive control ability and robustness. To overcome this problem, this paper proposes a sea computing-based grid-area coordinated load frequency control (SCGAC-LFC) method, which equates each grid unit and area unit in each area as independent agents. Instead of different units relying on different strategies, all units collectively obtain the LFC policy that suits the market requirements. In its online application, no communication is needed since each unit can arrive at its own decision. In addition, this paper proposes a curriculum multiagent deep meta-actor-critic (CMA-DMAC) algorithm, which introduces meta-reinforcement learning and curriculum learning to guide the agent training to improve the robustness and quality of the obtained SCGAC-LFC strategies. Using a simulation of the four-area LFC model for the China Southern Grid (CSG), our proposed method carries significantly lower frequency error and regulation mileage payment.}
}
@article{ISLAM2023100187,
title = {Software-Defined Network-Based Proactive Routing Strategy in Smart Power Grids Using Graph Neural Network and Reinforcement Learning},
journal = {e-Prime - Advances in Electrical Engineering, Electronics and Energy},
volume = {5},
pages = {100187},
year = {2023},
issn = {2772-6711},
doi = {https://doi.org/10.1016/j.prime.2023.100187},
url = {https://www.sciencedirect.com/science/article/pii/S2772671123000827},
author = {Md Aminul Islam and Muhammad Ismail and Rachad Atat and Osman Boyaci and Susmit Shannigrahi},
keywords = {Smart power grid, Software-defined networking, Reinforcement learning, Traffic prediction, Graph neural network, Machine learning-based routing},
abstract = {Smart power grid relies on sensors and actuators to provide continuous monitoring and precise control functions. Two types of data and command packets are associated with such field devices, namely, periodic fixed scheduling (FS) and emergency-related event-driven (ED) packets, which require different levels of quality-of-service (QoS) support. However, existing routing strategies in smart power grids are not adaptive to network conditions and cannot guarantee differentiated QoS support. To overcome this limitation, we propose a software-defined network (SDN) proactive routing framework in smart grids that takes into account the current and future state of the network while making the routing decisions. The proposed framework offers the following features: (a) It sets up separate queues for ED and FS packets, with higher priority for the ED queue; (b) It predicts the future traffic condition at each switch within the network (congested or not congested) using a graph-neural-network (GNN) model that provides an accurate prediction of the traffic condition despite the sparsity of the ED events; (c) It adopts a reinforcement learning (RL) strategy that establishes an ideal route and updates the queue service rate for each switch along the route following the network’s current and predicted future congestion condition. The proposed framework is tested on the cyber layers of the IEEE 14-bus and 39-bus test systems, and compared to two benchmarks. Our results indicate the superiority of our proposed framework compared with the benchmarks.}
}
@article{MA2021108515,
title = {An intelligent scheme for congestion control: When active queue management meets deep reinforcement learning},
journal = {Computer Networks},
volume = {200},
pages = {108515},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108515},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621004485},
author = {Huihui Ma and Du Xu and Yueyue Dai and Qing Dong},
keywords = {Active queue management, Congestion control, Bufferbloat, Reinforcement learning},
abstract = {With the explosive growth of data transmission requirements, the massive bursty traffic results in more frequent and serious network congestion. To deal with the burst traffic, the buffer size of the network is designed to be much larger than required. However, increasing buffer size leads to bufferbloat problem, which exacerbates network congestion. To address this problem, Active Queue Management (AQM) is proposed to cooperate with Transmission Control Protocol (TCP) congestion control mechanism. But it is hard to model dynamic AQM/TCP system and tune the parameters of conventional AQM schemes to obtain good performance under different congestion scenarios. In this paper, we aim to study the AQM scheme from a whole new perspective by leveraging emerging Deep Reinforcement Learning (DRL). A model-free approach, DRL-AQM, enables the AQM to learn the best dropping policy as human beings learn skills. DRL-AQM can capture intricate patterns in the data traffic model after trained in a simple network scenario, and leverage this to improve performance in a great variety of scenarios. There are two phases in our scheme, offline training phase and deployment phase. Once the model is trained, it can work well without any parameter-tuning in many scenarios. Experimental results show that (1) DRL-AQM outperforms conventional AQM algorithms in a variety of complex network scenarios and it is robust and insensitive to network conditions. (2) DRL-AQM keeps persistently low buffer capacity utilization without over dropping and damaging throughput. (3) DRL-AQM adapts automatically and continuously to the dynamics of the network links.}
}
@article{SHI2023110485,
title = {Physics-informed deep reinforcement learning-based integrated two-dimensional car-following control strategy for connected automated vehicles},
journal = {Knowledge-Based Systems},
volume = {269},
pages = {110485},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110485},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123002356},
author = {Haotian Shi and Yang Zhou and Keshu Wu and Sikai Chen and Bin Ran and Qinghui Nie},
keywords = {Connected automated vehicles, Two-dimensional control, Deep reinforcement learning, Traffic oscillation dampening, Path tracking},
abstract = {Connected automated vehicles (CAVs) are broadly recognized as next-generation transformative transportation technologies having great potential to improve traffic safety, efficiency, and stability. Efficiently controlling CAVs on two-dimensional curvilinear roadways to follow preceding vehicles is denoted as the two-dimensional car-following process, which is highly critical; this process is challenging to implement owing to the complexity and varied nature of driving environments. This study proposes an innovative integrated two-dimensional control strategy for CAVs based on deep reinforcement learning (DRL), which efficiently regulates the two-dimensional car-following process of CAVs in terms of both stability-wise longitudinal control performance and accurate lateral path-tracking performance. Within the control framework, each CAV can receive the surrounding information from downstream vehicles and roadway geometry based on vehicle-to-everything (V2X) communication. To better utilize this information, we designed a physics-informed DRL state fusion approach and reward function, which efficiently embeds prior physics knowledge and borrows the merits of the equilibrium and consensus concepts from the control theory. Given the physics-informed information, the DRL-based controller outputs the integrated control instructions for both longitudinal and lateral control. For training, we constructed a roadway with a set of varying curvatures and embedded the ground-truth vehicle trajectory datasets to more effectively capture the realistic variations in the roadway geometry and driving environment. To facilitate value function approximation and enhance the policy iteration process in training, the distributed proximal policy optimization (DPPO) algorithm was applied, owing to its balanced performance. A series of simulated experiments were conducted to validate the controller’s lateral control accuracy and stability-wise oscillation dampening performance in diverse traffic scenarios, including extreme ones.}
}
@article{HAN2021133068,
title = {Control and anti-control of chaos based on the moving largest Lyapunov exponent using reinforcement learning},
journal = {Physica D: Nonlinear Phenomena},
volume = {428},
pages = {133068},
year = {2021},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2021.133068},
url = {https://www.sciencedirect.com/science/article/pii/S0167278921002256},
author = {Yanyan Han and Jianpeng Ding and Lin Du and Youming Lei},
keywords = {Chaos control, Chaos anti-control, Reinforcement learning, Moving largest Lyapunov exponent},
abstract = {In this work, we propose a method of control and anti-control of chaos based on the moving largest Lyapunov exponent using reinforcement learning. In this method, we design a reward function for the reinforcement learning according to the moving largest Lyapunov exponent, which is similar to the moving average but computes the corresponding largest Lyapunov exponent using a recently updated time series with a fixed, short length. We adopt the density peaks-based clustering algorithm to determine a linear region of the average divergence index so that we can obtain the largest Lyapunov exponent of the small data set by fitting the slope of the linear region. We show that the proposed method is fast and easy to implement through controlling and anti-controlling typical systems such as the Henon map and Lorenz system.}
}
@article{HUANG2022273,
title = {Reinforcement Learning for feedback-enabled cyber resilience},
journal = {Annual Reviews in Control},
volume = {53},
pages = {273-295},
year = {2022},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2022.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1367578822000013},
author = {Yunhan Huang and Linan Huang and Quanyan Zhu},
keywords = {Reinforcement Learning, Security, Resilience, Feedback control systems, Advanced Persistent Threats, Optimal control theory, Cyber vulnerabilities, Moving target defense, Cyber deception, Honeypots, Human inattention},
abstract = {The rapid growth in the number of devices and their connectivity has enlarged the attack surface and made cyber systems more vulnerable. As attackers become increasingly sophisticated and resourceful, mere reliance on traditional cyber protection, such as intrusion detection, firewalls, and encryption, is insufficient to secure the cyber systems. Cyber resilience provides a new security paradigm that complements inadequate protection with resilience mechanisms. A Cyber-Resilient Mechanism (CRM) adapts to the known or zero-day threats and uncertainties in real-time and strategically responds to them to maintain the critical functions of the cyber systems in the event of successful attacks. Feedback architectures play a pivotal role in enabling the online sensing, reasoning, and actuation process of the CRM. Reinforcement Learning (RL) is an important gathering of algorithms that epitomize the feedback architectures for cyber resilience. It allows the CRM to provide dynamic and sequential responses to attacks with limited or without prior knowledge of the environment and the attacker. In this work, we review the literature on RL for cyber resilience and discuss the cyber-resilient defenses against three major types of vulnerabilities, i.e., posture-related, information-related, and human-related vulnerabilities. We introduce moving target defense, defensive cyber deception, and assistive human security technologies as three application domains of CRMs to elaborate on their designs. The RL algorithms also have vulnerabilities themselves. We explain the major vulnerabilities of RL and present develop several attack models where the attacker target the information exchanged between the environment and the agent: the rewards, the state observations, and the action commands. We show that the attacker can trick the RL agent into learning a nefarious policy with minimum attacking effort. The paper introduces several defense methods to secure the RL-enabled systems from these attacks. However, there is still a lack of works that focuses on the defensive mechanisms for RL-enabled systems. Last but not least, we discuss the future challenges of RL for cyber security and resilience and emerging applications of RL-based CRMs.}
}
@article{AYDIN2000169,
title = {Dynamic job-shop scheduling using reinforcement learning agents},
journal = {Robotics and Autonomous Systems},
volume = {33},
number = {2},
pages = {169-178},
year = {2000},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(00)00087-7},
url = {https://www.sciencedirect.com/science/article/pii/S0921889000000877},
author = {M.Emin Aydin and Ercan Öztemel},
keywords = {Intelligent agents, Reinforcement learning, -III learning, Dynamic job-shop scheduling},
abstract = {Static and dynamic scheduling methods have attracted a lot of attention in recent years. Among these, dynamic scheduling techniques handle scheduling problems where the scheduler does not possess detailed information about the jobs, which may arrive at the shop at any time. In this paper, an intelligent agent based dynamic scheduling system is proposed. It consists of two independent components: the agent and the simulated environment. The agent selects the most appropriate priority rule according to the shop conditions in real time, while simulated environment performs scheduling activities using the rule selected by the agent. The agent is trained by an improved reinforcement learning algorithm through the learning stage and then it successively makes decisions to schedule the operations.}
}
@article{ERDODI2021102903,
title = {Simulating SQL injection vulnerability exploitation using Q-learning reinforcement learning agents},
journal = {Journal of Information Security and Applications},
volume = {61},
pages = {102903},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102903},
url = {https://www.sciencedirect.com/science/article/pii/S2214212621001290},
author = {László Erdődi and Åvald Åslaugson Sommervoll and Fabio Massimo Zennaro},
keywords = {SQL injection, Capture the flag, Vulnerability detection, Autonomous agents, Reinforcement learning, Q-learning},
abstract = {In this paper, we propose a formalization of the process of exploitation of SQL injection vulnerabilities. We consider a simplification of the dynamics of SQL injection attacks by casting this problem as a security capture-the-flag challenge. We model it as a Markov decision process, and we implement it as a reinforcement learning problem. We then deploy reinforcement learning agents tasked with learning an effective policy to perform SQL injection; we design our training in such a way that the agent learns not just a specific strategy to solve an individual challenge but a more generic policy that may be applied to perform SQL injection attacks against any system instantiated randomly by our problem generator. We analyze the results in terms of the quality of the learned policy and in terms of convergence time as a function of the complexity of the challenge and the learning agent’s complexity. Our work fits in the wider research on the development of intelligent agents for autonomous penetration testing and white-hat hacking, and our results aim to contribute to understanding the potential and the limits of reinforcement learning in a security environment.}
}
@article{HACHEM2021110317,
title = {Deep reinforcement learning for the control of conjugate heat transfer},
journal = {Journal of Computational Physics},
volume = {436},
pages = {110317},
year = {2021},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110317},
url = {https://www.sciencedirect.com/science/article/pii/S0021999121002126},
author = {E. Hachem and H. Ghraieb and J. Viquerat and A. Larcher and P. Meliga},
keywords = {Deep reinforcement learning, Artificial neural networks, Conjugate heat transfer, Computational fluid dynamics, Thermal control},
abstract = {This research gauges the ability of deep reinforcement learning (DRL) techniques to assist the control of conjugate heat transfer systems governed by the coupled Navier–Stokes and heat equations. It uses a novel, “degenerate” version of the proximal policy optimization (PPO) algorithm, intended for situations where the optimal policy to be learnt by a neural network does not depend on state, as is notably the case in optimization and open-loop control problems. The numerical reward fed to the neural network is computed with an in-house stabilized finite elements environment combining variational multi-scale (VMS) modeling of the governing equations, immerse volume method, and multi-component anisotropic mesh adaptation. Several test cases of natural and forced convection in two and three dimensions are used as testbed for developing the methodology. The approach successfully alleviates the natural convection induced enhancement of heat transfer in a two-dimensional, differentially heated square cavity controlled by piece-wise constant fluctuations of the sidewall temperature. It also proves capable of improving the homogeneity of temperature across the surface of two and three-dimensional hot workpieces under impingement cooling. Various cases are tackled, in which the position of multiple cold air injectors is optimized relative to a fixed workpiece position. The flexibility of the numerical framework makes it tractable to solve also the inverse problem, i.e., to optimize the workpiece position relative to a fixed injector distribution. The obtained results showcase the potential of the method for black-box optimization of practically meaningful computational fluid dynamics (CFD) conjugate heat transfer systems. More significantly, they stress how DRL can reveal unanticipated solutions or parameter relations (as the optimal workpiece position under symmetrical actuation turns to be offset from the symmetry axis), in addition to being a tool for optimizing searches in large parameter spaces.}
}
@article{TAN20114741,
title = {Stock trading with cycles: A financial application of ANFIS and reinforcement learning},
journal = {Expert Systems with Applications},
volume = {38},
number = {5},
pages = {4741-4755},
year = {2011},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2010.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S095741741000905X},
author = {Zhiyong Tan and Chai Quek and Philip Y.K. Cheng},
keywords = {Stock trading, Price cycles, Investment decisions, ANFIS, Reinforcement learning},
abstract = {Based on the principles of technical analysis, this paper proposes an artificial intelligence model, which employs the Adaptive Network Fuzzy Inference System (ANFIS) supplemented by the use of reinforcement learning (RL) as a non-arbitrage algorithmic trading system. The novel intelligent trading system is capable of identifying a change in a primary trend for trading and investment decisions. It dynamically determines the periods for momentum and moving averages using the RL paradigm and also appropriately shifting the cycle using ANFIS-RL to address the delay in the predicted cycle. This is used as a proxy to determine the best point in time to go LONG and visa versa for SHORT. When this is coupled with a group of stocks, we derive a simple form of “riding the cycles – waves”. These are the derived features of the underlying stock movement. It provides a learning framework to trade on cycles. Initial experimental results are encouraging. Firstly, the proposed framework is able to outperform DENFIS and RSPOP in terms of true error and correlation. Secondly, based on the test trading with five US stocks, the proposed trading system is able to beat the market by about 50 percentage points over a period of 13years.}
}
@article{ENDER2022114570,
title = {Reinforcement learning to reduce failures in SOT-MRAM switching},
journal = {Microelectronics Reliability},
volume = {135},
pages = {114570},
year = {2022},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2022.114570},
url = {https://www.sciencedirect.com/science/article/pii/S0026271422000944},
author = {Johannes Ender and Roberto {Lacerda de Orio} and Simone Fiorentini and Siegfried Selberherr and Wolfgang Goes and Viktor Sverdlov},
keywords = {Reinforcement learning, Spin-orbit torque memory, Magnetic field-free, Switching reliability},
abstract = {A reinforcement learning strategy is applied to find a reliable switching scheme for deterministic switching of a perpendicularly magnetized spin-orbit torque magnetoresistive memory cell. Current pulses sent along orthogonal metal wires allow the field-free reversal of the magnetization. The current pulses are optimized such that reliable switching can be achieved over a wide range of material parameters. Micromagnetic simulations confirm the reliability of the presented approach.}
}
@article{CUI2022117389,
title = {A reinforcement learning based artificial bee colony algorithm with application in robot path planning},
journal = {Expert Systems with Applications},
volume = {203},
pages = {117389},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117389},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422007333},
author = {Yibing Cui and Wei Hu and Ahmed Rahmani},
keywords = {Artificial bee colony algorithm, Reinforcement learning, Mittag-Leffler distribution, Differential search equation, Robot path planning},
abstract = {Artificial bee colony (ABC) algorithm is a popular optimization algorithm with excellent exploration ability and various applications. Nevertheless, its effectiveness is limited by the one-dimensional search strategy. Therefore, in order to improve its performance, a reinforcement learning (RL) based ABC algorithm is proposed (named ABC_RL). In ABC_RL, the number of dimensions to be updated in search equation of the employed bee phase is varied and adjusted intelligently via RL. Moreover, two improved search strategies are adopted to maintain a nice balance between diversification and intensification. The performance of ABC_RL is evaluated through a series of comparisons conducted on CEC 2017 benchmark problems. The results indicate that ABC_RL outperforms the compared ABC variants considering the solution accuracy. In addition, a robot path planning problem is concerned to further test the effectiveness of ABC_RL. And the comparison results show the advantages of ABC_RL in terms of path length and running time.}
}
@article{KINTSAKIS201994,
title = {Reinforcement Learning based scheduling in a workflow management system},
journal = {Engineering Applications of Artificial Intelligence},
volume = {81},
pages = {94-106},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619300351},
author = {Athanassios M. Kintsakis and Fotis E. Psomopoulos and Pericles A. Mitkas},
keywords = {Workflow management systems, Scheduling optimization, Machine learning, Reinforcement Learning, Neural networks},
abstract = {Any computational process from simple data analytics tasks to training a machine learning model can be described by a workflow. Many workflow management systems (WMS) exist that undertake the task of scheduling workflows across distributed computational resources. In this work, we introduce a WMS that leverages machine learning to predict workflow task runtime and the probability of failure of task assignments to execution sites. The expected runtime of workflow tasks can be used to approximate the weight of the workflow graph branches in respect to the total workflow workload and the ability to anticipate task failures can discourage task assignments that are unlikely to succeed. We demonstrate that the proposed machine learning models can lead to significantly more informed scheduling decisions that minimize task failures and utilize execution sites more efficiently, thus leading to reduced workflow runtime. Additionally, we train a modified sequence-to-sequence neural network architecture via reinforcement learning to perform scheduling decisions as part of a WMS. Our approach introduces a WMS that can drastically improve its scheduling performance by independently learning over time, without external intervention or reliance on any specific heuristic or optimization technique. Finally, we test our approach in real-world scenarios utilizing computationally demanding and data intensive workflows and evaluate its performance against existing scheduling methodologies traditionally used in WMSes. The performance evaluation outcome confirms that the proposed approach significantly outperforms the other scheduling algorithms in a consistent manner and achieves the best execution runtime with the lowest number of failed tasks and communication costs.}
}
@article{MIAO2021108327,
title = {Federated deep reinforcement learning based secure data sharing for Internet of Things},
journal = {Computer Networks},
volume = {197},
pages = {108327},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108327},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621003285},
author = {Qinyang Miao and Hui Lin and Xiaoding Wang and Mohammad Mehedi Hassan},
keywords = {Secure data sharing, Federated learning, Deep reinforcement learning, IoT},
abstract = {The increasing number of Internet of Things (IoT) devices motivate the data sharing that improves the quality of IoT services. However, data providers usually suffer from the privacy leakage caused by direct data sharing. To solve this problem, in this paper, we propose a Federated Learning based Secure data Sharing mechanism for IoT, named FL2S. Specifically, to accomplish efficient and secure data sharing, a hierarchical asynchronous federated learning (FL) framework is developed based on the sensitive task decomposition. In addition, to improve data sharing quality, the deep reinforcement learning (DRL) technology is utilized to select participants of sufficient computational capabilities and high quality datasets. By integrating task decomposition and participant selection, reliable data sharing is realized by sharing local data models instead of the source data with data privacy preserved. Experiment results show that the proposed FL2S achieves high accuracy in secure data sharing for various IoT applications.}
}
@incollection{MARTINEZ20221693,
title = {CSTR control with deep reinforcement learning},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {1693-1698},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50282-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596502827},
author = {Borja Martínez and Manuel Rodríguez and Ismael Díaz},
keywords = {Deep Reinforcement learning, process control},
abstract = {In this paper we have applied the use of the deep reinforcement learning (DRL) for process control, to explore its applicability. The main objective is to develop a controller based on the deep reinforcement learning methodology in order to keep the level and composition of a continuous stirred-tank reactor under control.}
}
@article{AHRARINOURI2022100795,
title = {Distributed reinforcement learning energy management approach in multiple residential energy hubs},
journal = {Sustainable Energy, Grids and Networks},
volume = {32},
pages = {100795},
year = {2022},
issn = {2352-4677},
doi = {https://doi.org/10.1016/j.segan.2022.100795},
url = {https://www.sciencedirect.com/science/article/pii/S235246772200100X},
author = {Mehdi Ahrarinouri and Mohammad Rastegar and Kiana Karami and Ali Reza Seifi},
keywords = {Distributed energy management, Reinforcement learning, Transferring energy, Residential buildings, Multiple energy hubs, Interconnected energy systems},
abstract = {Energy management optimization in residential buildings plays an essential role in addressing the problem of energy crisis in the world. This paper introduces a novel method to optimize the energy scheduling for multiple residential buildings in an interconnected framework through transferring energy concepts. To this end, a distributed reinforcement learning energy management (DRLEM) approach is proposed to manage the energy scheduling in multi-carrier energy buildings. In such facilities, equipped with the micro-combined heat and power (micro-CHP) and the gas boiler, the possibility of heat and electrical energy transfer among energy hubs is provided. The effectiveness of the proposed method is verified in a test residential interconnected energy hubs (EHs). Results show a noticeable improvement in energy costs while transferring energy concept is available. In a test frame consisting of three residential EHs, the proposed DRLEM approach in an interconnected mode leads to a daily cost reduction of 3.3% and wasted heat energy decrement of about 18.3% in a typical day compared to the independent mode. Furthermore, in peak tariff energy hours, EHs tend to share their produced excess energy about 23% more than low tariff energy hours to reduce overall energy price.}
}
@article{HE2023108517,
title = {Energy management optimization for connected hybrid electric vehicle using offline reinforcement learning},
journal = {Journal of Energy Storage},
volume = {72},
pages = {108517},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2023.108517},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X2301914X},
author = {Hongwen He and Zegong Niu and Yong Wang and Ruchen Huang and Yiwen Shou},
keywords = {Energy management strategy (EMS), Hybrid electric vehicle (HEV), Offline deep reinforcement learning (DRL), Jensen–Shannon divergence},
abstract = {Energy management strategy (EMS) is critical to ensure the long-term energy economy of hybrid electric vehicles. The classical deep reinforcement learning algorithms exist many issues such as the safety constraint and the simulation-to-real gap, resulting in difficulties in applications to industrial tasks. Thus, this paper proposes a novel EMS and a policy updating method based on the offline deep reinforcement learning algorithm to address the energy optimization problem. Firstly, the batch-constrained deep Q-learning algorithm is applied to provide a solution training the control strategy based on the existing datasets without interaction with the environment. Secondly, the EMS updating method is proposed to improve the adaptability under complicated driving cycles. The Jensen–Shannon divergence is introduced to determine when offline control strategy updates in real time. Finally, the optimality and effectiveness of the proposed EMS are validated, and the real-time and adaptive performance of the proposed method is also verified. The results indicate that the proposed EMS can learn a superior policy from fixed data, and the proposed updating method can utilize the real-time data to update the offline policy approaching the online deep reinforcement learning-based strategy in energy consumption.}
}
@article{XING2023108637,
title = {Real-time optimal scheduling for active distribution networks: A graph reinforcement learning method},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {145},
pages = {108637},
year = {2023},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2022.108637},
url = {https://www.sciencedirect.com/science/article/pii/S0142061522006330},
author = {Qiang Xing and Zhong Chen and Tian Zhang and Xu Li and KeHui Sun},
keywords = {Distribution network, Graph attention network, Deep reinforcement learning, Real-time optimal scheduling, Power regulation, Topology structure variation},
abstract = {To improve the economy and safety of the active distribution network (ADN) operation, this paper proposes a real-time optimal scheduling strategy based on graph reinforcement learning (GRL), achieving online collaborative optimization of controllable equipment. For realizing the perception of inborn graph-structured features of the ADN and the formulation of the real-time scheduling scheme, the active-reactive power coordination process is firstly characterized as a dynamic graph-structured network interaction. A graph attention network (GAT) block is leveraged to extract and aggregate the topology branch correlation and node power information. Then the obtained topology information embedded with node features, as intermediate latent environment states, is fed into the underlying network architecture of deep reinforcement learning (DRL). Moreover, the power regulation problem of controllable equipment is duly formulated as a finite Markov decision process (FMDP). And the complex decision-making is solved by a deep deterministic policy gradient (DDPG) algorithm. Specially, under the electrical fault scenarios with variations in topology structures and feature information, GRL helps the agent efficiently learn and capture new graph-structured knowledge. Case studies are conducted within a modified IEEE 33-bus system. Extensive experimental results verify the superiority and adaptability of our proposed methodology.}
}
@article{BELHADI2021102541,
title = {Privacy reinforcement learning for faults detection in the smart grid},
journal = {Ad Hoc Networks},
volume = {119},
pages = {102541},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102541},
url = {https://www.sciencedirect.com/science/article/pii/S1570870521000913},
author = {Asma Belhadi and Youcef Djenouri and Gautam Srivastava and Alireza Jolfaei and Jerry Chun-Wei Lin},
keywords = {Energy systems, Privacy learning, Reinforcement learning, Anomaly detection, Smart grid},
abstract = {Recent anticipated advancements in ad hoc Wireless Mesh Networks (WMN) have made them strong natural candidates for Smart Grid’s Neighborhood Area Network (NAN) and the ongoing work on Advanced Metering Infrastructure (AMI). Fault detection in these types of energy systems has recently shown lots of interest in the data science community, where anomalous behavior from energy platforms is identified. This paper develops a new framework based on privacy reinforcement learning to accurately identify anomalous patterns in a distributed and heterogeneous energy environment. The local outlier factor is first performed to derive the local simple anomalous patterns in each site of the distributed energy platform. A reinforcement privacy learning is then established using blockchain technology to merge the local anomalous patterns into global complex anomalous patterns. Besides, different optimization strategies are suggested to improve the whole outlier detection process. To demonstrate the applicability of the proposed framework, intensive experiments have been carried out on well-known CASAS (Center of Advanced Studies in Adaptive Systems) platform. Our results show that our proposed framework outperforms the baseline fault detection solutions.}
}
@article{ZHANG202215067,
title = {A double-deck deep reinforcement learning-based energy dispatch strategy for an integrated electricity and district heating system embedded with thermal inertial and operational flexibility},
journal = {Energy Reports},
volume = {8},
pages = {15067-15080},
year = {2022},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2022.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S2352484722024143},
author = {Bin Zhang and Amer M.Y.M. Ghias and Zhe Chen},
keywords = {Integrated energy systems, Renewable energy, Machine learning, Deep reinforcement learning, Energy dispatch},
abstract = {With the high penetration of wind power connected to the integrated electricity and district heating systems (IEDHSs), wind power curtailment still inevitably occurs in the traditional IEDHS dispatch. Focusing on the flexibilities of the IEDHS is considered to be a beneficial solution to further promote the integration of wind power. In the district heating network, the thermal inertia is utilized to improve such flexibility. Therefore, an IEDHS dispatch model considering the thermal inertia of district heating network and operational flexibility of generators is proposed in this paper. In addition, to avoid the tendency of traditional reinforcement learning (RL) to fall into local optimality when solving high-dimensional problems, a double-deck deep RL (D3RL) framework is proposed in this study. D3RL combines with a deep deterministic policy gradient (DDPG) agent in the upper level and a conventional optimization solver in the lower level to simplify the action and reward design. In the simulation, the proposed model considering the transmission time delay characteristics of the district heating network and the operational flexibility of generators is verified in four scheduling scenarios. Besides, the superiority of the proposed D3RL method is validated in a larger IEDHS. Numerical results show that the considered scheduling model can use the heat storage characteristics of heating pipelines, reduce operating costs, improve the operational flexibility and encourage wind power utilization. Compared with traditional RL, the proposed optimization method can improve its training speed and convergence performance.}
}
@article{XU2022373,
title = {Moving target defense of routing randomization with deep reinforcement learning against eavesdropping attack},
journal = {Digital Communications and Networks},
volume = {8},
number = {3},
pages = {373-387},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000037},
author = {Xiaoyu Xu and Hao Hu and Yuling Liu and Jinglei Tan and Hongqi Zhang and Haotian Song},
keywords = {Routing randomization, Moving target defense, Deep reinforcement learning, Deep deterministic policy gradient},
abstract = {Eavesdropping attacks have become one of the most common attacks on networks because of their easy implementation. Eavesdropping attacks not only lead to transmission data leakage but also develop into other more harmful attacks. Routing randomization is a relevant research direction for moving target defense, which has been proven to be an effective method to resist eavesdropping attacks. To counter eavesdropping attacks, in this study, we analyzed the existing routing randomization methods and found that their security and usability need to be further improved. According to the characteristics of eavesdropping attacks, which are “latent and transferable”, a routing randomization defense method based on deep reinforcement learning is proposed. The proposed method realizes routing randomization on packet-level granularity using programmable switches. To improve the security and quality of service of legitimate services in networks, we use the deep deterministic policy gradient to generate random routing schemes with support from powerful network state awareness. In-band network telemetry provides real-time, accurate, and comprehensive network state awareness for the proposed method. Various experiments show that compared with other typical routing randomization defense methods, the proposed method has obvious advantages in security and usability against eavesdropping attacks.}
}
@article{LIU202395,
title = {An effective energy management Layout-Based reinforcement learning for household demand response in digital twin simulation},
journal = {Solar Energy},
volume = {258},
pages = {95-105},
year = {2023},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2023.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X23002967},
author = {Huafeng Liu and Qine Liu and Chaoping Rao and Fei Wang and Fahad Alsokhiry and Alexey V. Shvetsov and Mohamed A. Mohamed},
keywords = {Fuzzy reasoning, Reinforcement learning, Solar-based smart home, Home energy management, Demand response, digital twin},
abstract = {With the growth in energy consumption, demand response (DR) programs in the power network have gained popularity and can be expected to become more widespread in the future. Through DR programs, users are encouraged for utilizing renewable energy and reducing their power consumption at peak times, thereby helping to balance supply and demand on the grid, as well as generating revenue from the sale of excess power. This paper presents an effective energy management layout (EML) for household DR employing Reinforcement Learning (RL) and Fuzzy Reasoning (FR). RL would be a model-free control method that consists of doing measures and assessing the outcomes as it interacts with the environments. Through direct integration of customer feedback into its control logic, the suggested method takes into account user satisfaction by utilizing FR as a reward function. Through the shift of controllable devices from peak hours, whenever energy cost is higher, to off-peak periods, whenever energy cost is low, Q-learning, an RL method according to a reward scheme, has been applied for scheduling the execution of smart home devices. With the suggested method, 14 home devices can be controlled by one agent, and many status-action pairs as well as fuzzy logic for the reward function are used to assess the actions taken for a particular status. Simulations are implemented in the digital twin environment and demonstrate that the suggested device planning method smooths the energy usage and minimizes the energy price by taking into account the consumers' satisfaction, the consumers' feedback, and their satisfaction settings. The Home EML has been presented with a consumer interface in MATLAB/Simulink for demonstrating the suggested DR approach. The simulation tools include smart devices, energy price signals, smart meters, solar photovoltaics, batteries, electric vehicle, and grid supply.}
}
@article{DONG2021106850,
title = {Self-learned suppression of roll oscillations based on model-free reinforcement learning},
journal = {Aerospace Science and Technology},
volume = {116},
pages = {106850},
year = {2021},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2021.106850},
url = {https://www.sciencedirect.com/science/article/pii/S1270963821003606},
author = {Yizhang Dong and Zhiwei Shi and Kun Chen and Zhangyi Yao},
keywords = {Reinforcement learning, Real-world, Uncommanded roll oscillation, Closed-loop blowing},
abstract = {The high-angle-of-attack uncommanded roll oscillations are dangerous and cause significant challenges in flight control. This paper focuses on investigating the feasibility and performance of an artificial-intelligence method - “model-free reinforcement learning” (MFRL) on this issue, in both simulation and experiment. In simulation, two algorithms TD3 and SAC were used to learn the policies to suppress the roll oscillations of a widely used mathematical model. The agents only utilized current states as observation vector, and achieved perfect results. In the experiments, these two algorithms were used to learn the policies to suppress the roll oscillations of a flying-wing model which utilized spanwise blowing as its roll effectors. It is worth noting that unlike in simulation, the experiments investigated the influence of the observation vectors' memory size on training results. The results show that for both algorithms, the agents cannot learn good-enough policies when their observation spaces were constructed only by the current sensor data. This phenomenon, which is a big difference between experiments and simulations of MFRL, is due to the non-Markovian characteristic of the real-world dynamics caused by inevitable latencies. However, constructing the observation space using current and past sensor data help both the TD3 and SAC agents to learn great policies to suppress the oscillations using spanwise blowing in real world. Surprisingly, the trained agents showed some counterintuitive “smart” behaviors in tests.}
}
@article{FEIRSTEIN2016113,
title = {Reinforcement Learning of Potential Fields to achieve Limit-Cycle Walking},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {14},
pages = {113-118},
year = {2016},
note = {6th IFAC Workshop on Periodic Control Systems PSYCO 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.07.994},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316312824},
author = {Denise S. Feirstein and Ivan Koryakovskiy and Jens Kober and Heike Vallery},
keywords = {Machine learning, Energy Control, Limit cycles, Walking, Robot control},
abstract = {Reinforcement learning is a powerful tool to derive controllers for systems where no models are available. Particularly policy search algorithms are suitable for complex systems, to keep learning time manageable and account for continuous state and action spaces. However, these algorithms demand more insight into the system to choose a suitable controller parameterization. This paper investigates a type of policy parameterization for impedance control that allows energy input to be implicitly bounded: Potential fields. In this work, a methodology for generating a potential field-constrained impedance controller via approximation of example trajectories, and subsequently improving the control policy using Reinforcement Learning, is presented. The potential field-const rained approximation is used as a policy parameterization for policy search reinforcement learning and is compared to its unconstrained counterpart. Simulations on a simple biped walking model show the learned controllers are able to surpass the potential field of gravity by generating a stable limit-cycle gait on flat ground for both parameterizations. The potential field-constrained controller provides safety with a known energy bound while performing equally well as the unconstrained policy.}
}
@article{XU201295,
title = {URL: A unified reinforcement learning approach for autonomic cloud management},
journal = {Journal of Parallel and Distributed Computing},
volume = {72},
number = {2},
pages = {95-105},
year = {2012},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2011.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731511001924},
author = {Cheng-Zhong Xu and Jia Rao and Xiangping Bu},
keywords = {Reinforcement learning, Cloud computing, Virtual machine autoconfiguration},
abstract = {Cloud computing is emerging as an increasingly important service-oriented computing paradigm. Management is a key to providing accurate service availability and performance data, as well as enabling real-time provisioning that automatically provides the capacity needed to meet service demands. In this paper, we present a unified reinforcement learning approach, namely URL, to automate the configuration processes of virtualized machines and appliances running in the virtual machines. The approach lends itself to the application of real-time autoconfiguration of clouds. It also makes it possible to adapt the VM resource budget and appliance parameter settings to the cloud dynamics and the changing workload to provide service quality assurance. In particular, the approach has the flexibility to make a good trade-off between system-wide utilization objectives and appliance-specific SLA optimization goals. Experimental results on Xen VMs with various workloads demonstrate the effectiveness of the approach. It can drive the system into an optimal or near-optimal configuration setting in a few trial-and-error iterations.}
}
@incollection{KIM2023,
title = {Offline reinforcement learning methods for real-world problems},
series = {Advances in Computers},
publisher = {Elsevier},
year = {2023},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2023.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065245823000372},
author = {Taewoo Kim and Ho Suk and Shiho Kim},
keywords = {Offline learning, Offline reinforcement learning, Distribution shift, Out-of-distribution error, Domain generalization},
abstract = {Offline learning, also known as batch learning or offline training, describes training machine learning models using a fixed dataset that is not updated with new data in real time. Offline Reinforcement Learning is a popular approach for training agents in real-world scenarios, such as robotics, autonomous driving, and healthcare, where direct environmental interaction or building a highly accurate simulator is not feasible. However, Offline RL experiences several challenges due to the absence of real-time feedback from the online environment. The major challenges are distribution shift, bootstrapping error, and out-of-distribution error. Various methods are used in Offline RL and can be categorized according to the type of function approximator they utilize. These methods include value-regularized, policy-constraint, model-based, and uncertainty-based methods. Additionally, several techniques exist to apply offline RL in the real world and overcome the limitations of a fixed dataset by looking beyond the offline dataset. These techniques include optimism learning and domain generalization. The field of Offline RL is rapidly evolving, and there is still much to be done to overcome its limitations and expand its capabilities. However, the potential benefits of Offline RL, such as the ability to train systems using existing data, make it a promising area for future research and development.}
}
@article{RAMANATHAN2018422,
title = {Smart controller for conical tank system using reinforcement learning algorithm},
journal = {Measurement},
volume = {116},
pages = {422-428},
year = {2018},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2017.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0263224117307091},
author = {Prabhu Ramanathan and Kamal Kant Mangla and Sitanshu Satpathy},
keywords = {Non-linear process, Machine learning, Reinforcement learning, Data acquisition, Conical tank control},
abstract = {The objective of the paper is to study the implementation of machine learning based controller to control non-linear systems. A smart controller based on reinforcement learning algorithm is proposed, and its performance is demonstrated by using it to control the level of liquid in a non-linear conical tank system. The system is represented in terms of a Markov Decision Process (MDP), and a reinforcement learning technique based on Q-learning algorithm is used to control the process. The advantage is that a standalone controller is designed on its own without prior knowledge of the environment or the system. The hardware implementation of the designed controller showed that the controller controlled the level of fluid in the conical tank efficiently, and rejected random disturbances introduced in the system. This controller provides an edge over PID, fuzzy, and other neural network based controllers, by eliminating the need for linearizing non-linear characteristics, tuning PID parameters, designing transfer functions, and developing fuzzy membership functions.}
}
@article{GUERRERO2022116037,
title = {Decision support system in health care building design based on case-based reasoning and reinforcement learning},
journal = {Expert Systems with Applications},
volume = {187},
pages = {116037},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116037},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421013828},
author = {J.I. Guerrero and G. Miró-Amarante and A. Martín},
keywords = {Decision support system, Health care facilities design, Case-based reasoning, Knowledge-based system},
abstract = {The health care building designing is a very difficult process in which there are a great quantity of parameters and variables that it is necessary to consider. The health care building should reach the population needs. Additionally, the design of this type of building and the related facilities involves a great quantity of regulations which they are adapted to the different countries. Thus, the health care facilities should be designed according to numerous and complex regulations. The checking of this regulation is very difficult and usually involves big teams of specialized engineers and architects. The proposed Case-Based Reasoning (CBR) and Reinforcement Learning can analyse the data about building design (provided in an Extended Mark-up Language or XML file, and other compatible formats), checking, and validating the regulations. This approach allows to reduce the specialized and high qualified personnel, providing a report with the checking regulations, and the traceability of warning and faults in the application of regulations.}
}
@article{LAWHEAD2019252,
title = {A bounded actor–critic reinforcement learning algorithm applied to airline revenue management},
journal = {Engineering Applications of Artificial Intelligence},
volume = {82},
pages = {252-262},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619300934},
author = {Ryan J. Lawhead and Abhijit Gosavi},
keywords = {Reinforcement learning, Actor critics, Airline revenue management},
abstract = {Reinforcement Learning (RL) is an artificial intelligence technique used to solve Markov and semi-Markov decision processes. Actor critics form a major class of RL algorithms that suffer from a critical deficiency, which is that the values of the so-called actor in these algorithms can become very large causing computer overflow. In practice, hence, one has to artificially constrain these values, via a projection, and at times further use temperature-reduction tuning parameters in the popular Boltzmann action-selection schemes to make the algorithm deliver acceptable results. This artificial bounding and temperature reduction, however, do not allow for full exploration of the state space, which often leads to sub-optimal solutions on large-scale problems. We propose a new actor–critic algorithm in which (i) the actor’s values remain bounded without any projection and (ii) no temperature-reduction tuning parameter is needed. The algorithm also represents a significant improvement over a recent version in the literature, where although the values remain bounded they usually become very large in magnitude, necessitating the use of a temperature-reduction parameter. Our new algorithm is tested on an important problem in an area of management science known as airline revenue management, where the state-space is very large. The algorithm delivers encouraging computational behavior, outperforming a well-known industrial heuristic called EMSR-b on industrial data.}
}
@article{DEFREITASCUNHA2023239,
title = {An SMDP approach for Reinforcement Learning in HPC cluster schedulers},
journal = {Future Generation Computer Systems},
volume = {139},
pages = {239-252},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003090},
author = {Renato Luiz {de Freitas Cunha} and Luiz Chaimowicz},
keywords = {Deep Reinforcement Learning, Scheduling, Semi-Markov Decision Processes, Workload traces, Machine Learning, Simulation},
abstract = {Deep reinforcement learning applied to computing systems has shown potential for improving system performance, as well as faster discovery of better allocation strategies. In this paper, we map HPC batch job scheduling to the SMDP formalism, and present an online, deep reinforcement learning-based solution that uses a modification of the Proximal Policy Optimization algorithm for minimizing job slowdown with action masking, supporting large action spaces. In our experiments, we assess the effects of noise in run time estimates in our model, evaluating how it behaves in small (64 processors) and large (16384 processors) clusters. We also show our model is robust to changes in workload and in cluster sizes, showing transfer works with changes of cluster size of up to 10×, and changes from synthetic workload generators to supercomputing workload traces. In our experiments, the proposed model outperforms learning models from the literature and classic heuristics, making it a viable modeling approach for robust, transferable, learning scheduling models.}
}
@article{HE2023104352,
title = {Toward personalized decision making for autonomous vehicles: A constrained multi-objective reinforcement learning technique},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {156},
pages = {104352},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2023.104352},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X2300342X},
author = {Xiangkun He and Chen Lv},
keywords = {Autonomous vehicle, Personalized decision making, Reinforcement learning, Multi-objective optimization},
abstract = {Reinforcement learning promises to provide a state-of-the-art solution to the decision making problem of autonomous driving. Nonetheless, numerous real-world decision making problems involve balancing multiple conflicting or competing objectives. In addition, passengers may typically prefer to explore diversified driving modes through their specific preferences (i.e., relative importance of different objectives). Taking into account these demands, traditional reinforcement learning algorithms with applications in personalized self-driving vehicles remain challenging. Consequently, here we present a novel constrained multi-objective reinforcement learning technique for personalized decision making in autonomous driving, with the goal of learning a single model for Pareto optimal policies across the space of all possible user preferences. Specifically, a nonlinear constraint incorporating a user-specified preference and a vectorized action–value function is introduced to ensure both diversity in learned decision behaviors and efficient alignment between the user-specified preference and the corresponding optimal policy. Additionally, a constrained multi-objective actor–critic approach is advanced to approximate the Pareto optimal policies for any user-specified preferences while adhering to the nonlinear constraint. Finally, the proposed personalized decision making scheme for autonomous driving is assessed in a highway on-ramp merging scenario with dynamic traffic flows. The results demonstrate the effectiveness of our method by comparing it with classical and state-of-the-art baselines.}
}
@incollection{JIANG2023297,
title = {Polymer Grade Transition Control via Reinforcement Learning Trained with a Physically Consistent Memory Sequence-to-Sequence Digital Twin},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {297-303},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50048-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152740500482},
author = {Zhen-Feng Jiang and David Shan-Hill Wong and Jia-Lin Kang and Yuan Yao and Yao-Chen Chuang},
keywords = {Reinforcement learning, Sequence-to-Sequence with Memory Layer, Model Predictive Control, Grade transition},
abstract = {In this work, a memory layer sequence-to-sequence digital twin (ML-StSDT) of a high-density polyethylene (HDPE) reactor simulated by ASPEN DynamicsTM was constructed using simulated grade transition and steady-state operating data. A reinforcement learning control (RLC) algorithm was developed by training with the ML-StSDT. The RLC was able to control both grade transition and steady-state operation of the simulated plant. The RLC performs better or equally well when compared with the direct application of ML-StSDT in nonlinear model predictive control (NLMPC) but substantially reduces the computation load. Our results demonstrate the feasibility of deep learning models serving as a digital twin for RLC training in nonlinear process control applications.}
}
@article{VAMPLEW201774,
title = {Softmax exploration strategies for multiobjective reinforcement learning},
journal = {Neurocomputing},
volume = {263},
pages = {74-86},
year = {2017},
note = {Multiobjective Reinforcement Learning: Theory and Applications},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.09.141},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217310974},
author = {Peter Vamplew and Richard Dazeley and Cameron Foale},
keywords = {Multiobjective reinforcement learning, Exploration, ϵ-greedy exploration, Optimistic initialisation, Softmax},
abstract = {Despite growing interest over recent years in applying reinforcement learning to multiobjective problems, there has been little research into the applicability and effectiveness of exploration strategies within the multiobjective context. This work considers several widely-used approaches to exploration from the single-objective reinforcement learning literature, and examines their incorporation into multiobjective Q-learning. In particular this paper proposes two novel approaches which extend the softmax operator to work with vector-valued rewards. The performance of these exploration strategies is evaluated across a set of benchmark environments. Issues arising from the multiobjective formulation of these benchmarks which impact on the performance of the exploration strategies are identified. It is shown that of the techniques considered, the combination of the novel softmax–epsilon exploration with optimistic initialisation provides the most effective trade-off between exploration and exploitation.}
}
@article{QUE2023119153,
title = {Solving 3D packing problem using Transformer network and reinforcement learning},
journal = {Expert Systems with Applications},
volume = {214},
pages = {119153},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.119153},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422021716},
author = {Quanqing Que and Fang Yang and Defu Zhang},
keywords = {3D packing problem, Deep reinforcement learning, Transformer},
abstract = {The three-dimensional packing problem (3D-PP) is a classic NP-hard problem in operations research and computer science. One of the most popular ways to solve the problem is heuristic methods with a search strategy. However, approaches based on machine learning have recently received widespread attention because of their efficiency. In this work, we propose a deep reinforcement learning (DRL) model to solve 3D-PP. Our method employs Transformer architecture as the policy network and uses Proximal Policy Optimization (PPO) to train the network. Compared with previous approaches using DRL, our method presents a novel state representation of packing environment, and introduces plane features for representing the length and width information of container. Our method achieves the new state-of-the-art results for using DRL to solve 3D-PP. The code of our method will be released to facilitate future research.}
}
@article{WEN2021107605,
title = {A multi-robot path-planning algorithm for autonomous navigation using meta-reinforcement learning based on transfer learning},
journal = {Applied Soft Computing},
volume = {110},
pages = {107605},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107605},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621005263},
author = {Shuhuan Wen and Zeteng Wen and Di Zhang and Hong Zhang and Tao Wang},
keywords = {Multi-robot system, Path planning, Deep reinforcement learning, Meta learning, Transfer learning},
abstract = {The adaptability of multi-robot systems in complex environments is a hot topic. Aiming at static and dynamic obstacles in complex environments, this paper presents dynamic proximal meta policy optimization with covariance matrix adaptation evolutionary strategies (dynamic-PMPO-CMA) to avoid obstacles and realize autonomous navigation. Firstly, we propose dynamic proximal policy optimization with covariance matrix adaptation evolutionary strategies (dynamic-PPO-CMA) based on original proximal policy optimization (PPO) to obtain a valid policy of obstacles avoidance. The simulation results show that the proposed dynamic-PPO-CMA can avoid obstacles and reach the designated target position successfully. Secondly, in order to improve the adaptability of multi-robot systems in different environments, we integrate meta-learning with dynamic-PPO-CMA to form the dynamic-PMPO-CMA algorithm. In training process, we use the proposed dynamic-PMPO-CMA to train robots to learn multi-task policy. Finally, in testing process, transfer learning is introduced to the proposed dynamic-PMPO-CMA algorithm. The trained parameters of meta policy are transferred to new environments and regarded as the initial parameters. The simulation results show that the proposed algorithm can have faster convergence rate and arrive the destination more quickly than PPO, PMPO and dynamic-PPO-CMA.}
}
@article{ZENG2022119688,
title = {Resilience enhancement of multi-agent reinforcement learning-based demand response against adversarial attacks},
journal = {Applied Energy},
volume = {324},
pages = {119688},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119688},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922009850},
author = {Lanting Zeng and Dawei Qiu and Mingyang Sun},
keywords = {Demand response, Cyber security, Resilience, Multi-agent reinforcement learning, Adversarial attacks},
abstract = {Demand response improves grid security by adjusting the flexibility of consumers meanwhile maintaining their demand–supply balance in real-time. With the large-scale deployment of distributed digital communication technologies and advanced metering infrastructures, data-driven approaches such as multi-agent reinforcement learning (MARL) are being widely employed to solve demand response problems. Nevertheless, the massive interaction of data inside and outside the demand response management system may lead to severe threats from the perspective of cyber-attacks. The cyber security requirements of MARL-based demand response problems are less discussed in the existing studies. To this end, this paper proposes a robust adversarial multi-agent reinforcement learning framework for demand response (RAMARL-DR) with an enhanced resilience against adversarial attacks. In particular, the proposed RAMARL-DR first constructs an adversary agent that aims to cause the worst-case performance via formulating an adversarial attack; and then adopts periodic alternating robust adversarial training scenarios with the optimal adversary aiming to diminish the severe impacts induced by adversarial attacks. Case studies are conducted based on an OpenAI Gym environment CityLearn, which provides a standard evaluation platform of MARL algorithms for demand response problems. Empirical results indicate that the MARL-based demand response management system is vulnerable when the adversary agent occurs, and its performance can be significantly improved after periodic alternating robust adversarial training. It can be found that the adversary agent can result in a 41.43% higher metric value of Ramping than the no adversary case, whereas the proposed RAMARL-DR can significantly enhance the system resilience with an approximately 38.85% reduction in the ramping of net demand.}
}
@article{BELHADI2021213,
title = {Reinforcement learning multi-agent system for faults diagnosis of mircoservices in industrial settings},
journal = {Computer Communications},
volume = {177},
pages = {213-219},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002644},
author = {Asma Belhadi and Youcef Djenouri and Gautam Srivastava and Jerry Chun-Wei Lin},
keywords = {Reinforcement learning, Multi-agents system, Microservices, Local outliers, Global outliers},
abstract = {This paper develops a new framework called MASAD (Multi-Agents System for Anomaly Detection), a hybrid combination of reinforcement learning, and a multi-agents system to identify abnormal behaviors of microservices in industrial environment settings. A multi-agent system is implemented using reinforcement learning, where each agent learns from the given microservice. Intelligent communication among the different agents is then established to enhance the learning of each agent by considering the experience of the agents of the other microservices of the system. The above setting not only allows to identify local anomalies but global ones from the whole microservices architecture. To show the effectiveness of the framework as proposed, we have gone through a thorough experimental analysis on two microservice architectures (NETFLIX, and LAMP). Results showed that our proposed framework can understand the behavior of microservices, and accurately simulate different interactions in the microservices. Besides, our approach outperforms baseline methods in identifying both local and global outliers.}
}
@article{VAMPLEW2021104186,
title = {Potential-based multiobjective reinforcement learning approaches to low-impact agents for AI safety},
journal = {Engineering Applications of Artificial Intelligence},
volume = {100},
pages = {104186},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104186},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621000336},
author = {Peter Vamplew and Cameron Foale and Richard Dazeley and Adam Bignold},
keywords = {Safe reinforcement learning, Multiobjective reinforcement learning, AI safety, Potential-based rewards, Low-impact agents, Reward engineering, Side-effects},
abstract = {The concept of impact-minimisation has previously been proposed as an approach to addressing the safety concerns that can arise from utility-maximising agents. An impact-minimising agent takes into account the potential impact of its actions on the state of the environment when selecting actions, so as to avoid unacceptable side-effects. This paper proposes and empirically evaluates an implementation of impact-minimisation within the framework of multiobjective reinforcement learning. The key contributions are a novel potential-based approach to specifying a measure of impact, and an examination of a variety of non-linear action-selection operators so as to achieve an acceptable trade-off between achieving the agent’s primary task and minimising environmental impact. These experiments also highlight a previously unreported issue with noisy estimates for multiobjective agents using non-linear action-selection, which has broader implications for the application of multiobjective reinforcement learning.}
}
@article{DURMAZ2022116722,
title = {Intelligent software debugging: A reinforcement learning approach for detecting the shortest crashing scenarios},
journal = {Expert Systems with Applications},
volume = {198},
pages = {116722},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116722},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422001968},
author = {Engin Durmaz and M. Borahan Tümer},
keywords = {Reinforcement learning in automated bug detection, Exploring crashes by SARSA, Exploring crashes by prioritized sweeping, Delta debugging, Detected goal catalyst},
abstract = {The Quality Assurance (QA) team verifies software for months before its release decisions. Nevertheless, some crucial bugs remain undetected in manual testing. These bugs would make the system unusable on field, thus merchant loses money then manufacturer loses its customers. Thus, automatic software testing methods have become inevitable to catch more bugs. To locate and repair bugs with an emphasis on the crash scenarios, we present in this work a reinforcement learning (RL) approach for finding and simplifying the input sequence(s) leading to a system crash or blocking, which represents the goal state of the RL problem. We aim at obtaining the shortest input sequence for the same bug so that developers would analyze agent’s actions causing crashes or freeze. We first simplify the given crash scenario using Recursive Delta Debugging (RDD), then we apply RL algorithms to explore a possibly shorter crashing sequence. We approach the exploration of crash scenarios as a RL problem where the agent first attains the goal state of crash/blocking by executing inputs, then shortens the input sequence with the help of the rewarding mechanism. We apply both model-free on-policy and model-based planning-capable RL agents to our problem. Furthermore, we present a novel RL approach, involving Detected Goal Catalyst (DGC), which reduces the time complexity by avoiding grappling with convergence via stopping learning at a small variance and attaining the shortest crash sequence with an algorithm that recursively removes the unrelated actions. Experiments show DGC significantly improves the learning performance of both SARSA and Prioritized Sweeping algorithms on obtaining the shortest path.}
}
@article{PAN2023288,
title = {Reinforcement learning for automatic quadrilateral mesh generation: A soft actor–critic approach},
journal = {Neural Networks},
volume = {157},
pages = {288-304},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S089360802200418X},
author = {Jie Pan and Jingwei Huang and Gengdong Cheng and Yong Zeng},
keywords = {Reinforcement learning, Mesh generation, Soft actor–critic, Neural networks, Computational geometry, Quadrilateral mesh},
abstract = {This paper proposes, implements, and evaluates a reinforcement learning (RL)-based computational framework for automatic mesh generation. Mesh generation plays a fundamental role in numerical simulations in the area of computer aided design and engineering (CAD/E). It is identified as one of the critical issues in the NASA CFD Vision 2030 Study. Existing mesh generation methods suffer from high computational complexity, low mesh quality in complex geometries, and speed limitations. These methods and tools, including commercial software packages, are typically semiautomatic and they need inputs or help from human experts. By formulating the mesh generation as a Markov decision process (MDP) problem, we are able to use a state-of-the-art reinforcement learning (RL) algorithm called “soft actor-critic” to automatically learn from trials the policy of actions for mesh generation. The implementation of this RL algorithm for mesh generation allows us to build a fully automatic mesh generation system without human intervention and any extra clean-up operations, which fills the gap in the existing mesh generation tools. In the experiments to compare with two representative commercial software packages, our system demonstrates promising performance with respect to scalability, generalizability, and effectiveness.}
}