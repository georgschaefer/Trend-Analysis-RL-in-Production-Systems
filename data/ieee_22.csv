"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Policy Fusion for Adaptive and Customizable Reinforcement Learning Agents","A. Sestini; A. Kuhnle; A. D. Bagdanov","Dipartimento di Ingegneria dell'Informazione, Università degli Studi di Firenze, Florence, Italy; University of Cambridge, Cambridge, United Kingdom; Dipartimento di Ingegneria dell'Informazione, Università degli Studi di Firenze, Florence, Italy","2021 IEEE Conference on Games (CoG)","7 Dec 2021","2021","","","01","08","In this article we study the problem of training intelligent agents using Reinforcement Learning for the purpose of game development. Unlike systems built to replace human players and to achieve super-human performance, our agents aim to produce meaningful interactions with the player, and at the same time demonstrate behavioral traits as desired by game designers. We show how to combine distinct behavioral policies to obtain a meaningful “fusion” policy which comprises all these behaviors. To this end, we propose four different policy fusion methods for combining pre-trained policies. We further demonstrate how these methods can be used in combination with Inverse Reinforcement Learning in order to create intelligent agents with specific behavioral styles as chosen by game designers, without having to define many and possibly poorly-designed reward functions. Experiments on two different environments indicate that entropy-weighted policy fusion significantly outperforms all others. We provide several practical examples and use-cases for how these methods are indeed useful for video game production and designers.","2325-4289","978-1-6654-3886-5","10.1109/CoG52621.2021.9618983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9618983","","Training;Deep learning;Conferences;Games;Reinforcement learning;Production;Planning","behavioural sciences;computer games;reinforcement learning","pre-trained policies;inverse reinforcement learning;behavioral styles;game designers;poorly-designed reward functions;entropy-weighted policy fusion;video game production;training intelligent agents;game development;human players;super-human performance;distinct behavioral policies;policy fusion methods;customizable reinforcement learning agents","","","","39","IEEE","7 Dec 2021","","","IEEE","IEEE Conferences"
"Mobile Robot Path Planning in Dynamic Environments Through Globally Guided Reinforcement Learning","B. Wang; Z. Liu; Q. Li; A. Prorok","Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Technology, University of Cambridge, Cambridge, U.K.; Department of Computer Science and Technology, University of Cambridge, Cambridge, U.K.; Department of Computer Science and Technology, University of Cambridge, Cambridge, U.K.","IEEE Robotics and Automation Letters","6 Oct 2020","2020","5","4","6932","6939","Path planning for mobile robots in large dynamic environments is a challenging problem, as the robots are required to efficiently reach their given goals while simultaneously avoiding potential conflicts with other robots or dynamic objects. In the presence of dynamic obstacles, traditional solutions usually employ re-planning strategies, which re-call a planning algorithm to search for an alternative path whenever the robot encounters a conflict. However, such re-planning strategies often cause unnecessary detours. To address this issue, we propose a learning-based technique that exploits environmental spatio-temporal information. Different from existing learning-based methods, we introduce a globally guided reinforcement learning approach (G2RL), which incorporates a novel reward structure that generalizes to arbitrary environments. We apply G2RL to solve the multi-robot path planning problem in a fully distributed reactive manner. We evaluate our method across different map types, obstacle densities, and the number of robots. Experimental results show that G2RL generalizes well, outperforming existing distributed methods, and performing very similarly to fully centralized state-of-the-art benchmarks.","2377-3766","","10.1109/LRA.2020.3026638","Engineering and Physical Sciences Research Council(grant numbers:EP/S015493/1); ARL DCIST(grant numbers:CRA W911NF- 17-2-0181); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9205217","Hierarchical path planning;mobile robots;reinforcement learning;scalability","Mobile robots;Path planning;Reinforcement learning;Learning (artificial intelligence);Scalability","collision avoidance;learning (artificial intelligence);mobile robots;multi-robot systems","mobile robot path planning;dynamic obstacles;re-planning strategies;planning algorithm;learning-based technique;environmental spatio-temporal information;learning-based methods;globally guided reinforcement learning approach;multirobot path;G2RL approach","","94","","26","IEEE","24 Sep 2020","","","IEEE","IEEE Journals"
"Modular Deep Reinforcement Learning for Continuous Motion Planning With Temporal Logic","M. Cai; M. Hasanbeig; S. Xiao; A. Abate; Z. Kan","Department of Mechanical Engineering, University of Iowa, Iowa City, IA, USA; Department of Computer Science, University of Oxford, Oxford, U.K.; Department of Mechanical Engineering, University of Iowa, Iowa City, IA, USA; Department of Computer Science, University of Oxford, Oxford, U.K.; Department of Automation, University of Science and Technology of China, Hefei, Anhui, China","IEEE Robotics and Automation Letters","20 Aug 2021","2021","6","4","7973","7980","This letter investigates the motion planning of autonomous dynamical systems modeled by Markov decision processes (MDP) with unknown transition probabilities over continuous state and action spaces. Linear temporal logic (LTL) is used to specify high-level tasks over infinite horizon, which can be converted into a limit deterministic generalized Büchi automaton (LDGBA) with several accepting sets. The novelty is to design an embedded product MDP (EP-MDP) between the LDGBA and the MDP by incorporating a synchronous tracking-frontier function to record unvisited accepting sets of the automaton, and to facilitate the satisfaction of the accepting conditions. The proposed LDGBA-based reward shaping and discounting schemes for the model-free reinforcement learning (RL) only depend on the EP-MDP states and can overcome the issues of sparse rewards. Rigorous analysis shows that any RL method that optimizes the expected discounted return is guaranteed to find an optimal policy whose traces maximize the satisfaction probability. A modular deep deterministic policy gradient (DDPG) is then developed to generate such policies over continuous state and action spaces. The performance of our framework is evaluated via an array of OpenAI gym environments.","2377-3766","","10.1109/LRA.2021.3101544","National Natural Science Foundation of China(grant numbers:U2013601); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506925","Deep Reinforcement Learning;Linear Temporal Logic;Motion Planning","Task analysis;Planning;Automata;Reinforcement learning;Uncertainty;Extraterrestrial measurements;Robustness","automata theory;control engineering computing;decision theory;deep learning (artificial intelligence);gradient methods;Markov processes;mobile robots;optimisation;path planning;probability;set theory;temporal logic","modular deep deterministic policy gradient;continuous state;action spaces;modular deep reinforcement learning;continuous motion planning;autonomous dynamical systems;Markov decision processes;unknown transition probabilities;linear temporal logic;high-level tasks;infinite horizon;limit deterministic generalized Büchi automaton;accepting sets;embedded product MDP;synchronous tracking-frontier function;model-free reinforcement learning;sparse rewards;satisfaction probability;EP-MDP;LTL;LDGBA-based reward shaping;LDGBA-based reward discounting;optimal policy;DDPG;OpenAI gym environments","","33","","32","IEEE","4 Aug 2021","","","IEEE","IEEE Journals"
"Closed-Loop Dynamic Control of a Soft Manipulator Using Deep Reinforcement Learning","A. Centurelli; L. Arleo; A. Rizzo; S. Tolu; C. Laschi; E. Falotico","BioRobotics Institute, Scuola Superiore Sant’Anna, Pontedera, Italy; BioRobotics Institute, Scuola Superiore Sant’Anna, Pontedera, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Turin, Italy; Department of Electrical Engineering (Automation and Control), Technical University of Denmark, Lyngby, Denmark; Department of Mechanical Engineering, National University of Singapore, Singapore; BioRobotics Institute, Scuola Superiore Sant’Anna, Pontedera, Italy","IEEE Robotics and Automation Letters","1 Mar 2022","2022","7","2","4741","4748","The focus of the research community in the soft robotic field has been on developing innovative materials, but the design of control strategies applicable to these robotic platforms is still an open challenge. This is due to their highly nonlinear dynamics which is difficult to model and the degree of stochasticity they often incorporate. Data-driven controllers based on neural networks have recently been explored as a viable solution to be employed for these manipulators. This letter presents a neural network-based closed-loop controller, trained by a deep reinforcement learning algorithm called Trust Region Policy Optimization (TRPO). The training takes place in simulation, using an approximation of the robot forward dynamic model obtained with a Long-short Term Memory (LSTM) network. The trained controller allows following different paths executed with different velocities in the workspace of the robot. The results demonstrate that the controller is effective in normal working conditions and with a payload attached to the end-effector of the manipulator.","2377-3766","","10.1109/LRA.2022.3146903","European Union’s Horizon 2020 FET-Open program(grant numbers:863212); PROBOSCIS Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9697385","Manipulator dynamics;robot control;robot learning;soft robotics","Robots;Manipulators;Manipulator dynamics;Task analysis;Predictive models;Aerospace electronics;Payloads","approximation theory;closed loop systems;control engineering computing;deep learning (artificial intelligence);manipulator dynamics;manipulator kinematics;neurocontrollers;optimisation;recurrent neural nets;reinforcement learning","closed-loop dynamic control;soft manipulator;soft robotic field;data-driven controllers;neural networks;closed-loop controller;trust region policy optimization;robot forward dynamic model;long-short term memory network;deep reinforcement learning","","15","","27","IEEE","31 Jan 2022","","","IEEE","IEEE Journals"
"Fairness Control of Traffic Light via Deep Reinforcement Learning","C. Li; X. Ma; L. Xia; Q. Zhao; J. Yang","Center for Intelligent and Networked System, Tsinghua University, Beijing, China; Center for Intelligent and Networked System, Tsinghua University, Beijing, China; Business School, Sun Yat-Sen University, Guangzhou, China; Center for Intelligent and Networked System, Tsinghua University, Beijing, China; Dept Automation, Tsinghua University, Beijing, China","2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)","8 Oct 2020","2020","","","652","658","Traffic congestion is a severe issue of a developing world. Recently, many researchers are attempting to utilize deep reinforcement learning algorithms to bring intelligence to traffic lights. To the best of our knowledge, most prior researchers only consider the average criterion of all vehicles while training. However, fairness is another important metric but ignored. In this paper, we study the fairness control of traffic light and propose a deep reinforcement learning algorithm to optimize the fairness of all drivers’ waiting time. The objective is to minimize the maximal waiting time of drivers during a light time loop, which also partly reflects the optimization of the average waiting time. We conduct experiments for a 4-lane crossroad in SUMO. Simulation results show that our algorithm can efficiently optimize the fairness criterion. Meanwhile the average criterion is further improved. We wish to shed light on complementing the entire framework of reinforcement learning with our research on fairness control.","2161-8089","978-1-7281-6904-0","10.1109/CASE48305.2020.9216899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9216899","","Optimization;Standards;Sociology;Statistics;Green products;Machine learning;Learning (artificial intelligence)","learning (artificial intelligence);traffic engineering computing","SUMO;maximal waiting time;fairness criterion;light time loop;average criterion;deep reinforcement learning algorithm;traffic congestion;traffic light;fairness control","","6","","23","IEEE","8 Oct 2020","","","IEEE","IEEE Conferences"
"Meta-Reinforcement Learning Based Resource Allocation for Dynamic V2X Communications","Y. Yuan; G. Zheng; K. -K. Wong; K. B. Letaief","Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, U.K.; Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, U.K.; Department of Electronic and Electrical Engineering, University College London, London, U.K.; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong","IEEE Transactions on Vehicular Technology","20 Sep 2021","2021","70","9","8964","8977","This paper studies the allocation of shared resources between vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) links in vehicle-to-everything (V2X) communications. In existing algorithms, dynamic vehicular environments and quantization of continuous power become the bottlenecks for providing an effective and timely resource allocation policy. In this paper, we develop two algorithms to deal with these difficulties. First, we propose a deep reinforcement learning (DRL)-based resource allocation algorithm to improve the performance of both V2I and V2V links. Specifically, the algorithm uses deep Q-network (DQN) to solve the sub-band assignment and deep deterministic policy-gradient (DDPG) to solve the continuous power allocation problem. Second, we propose a meta-based DRL algorithm to enhance the fast adaptability of the resource allocation policy in the dynamic environment. Numerical results demonstrate that the proposed DRL-based algorithm can significantly improve the performance compared to the DQN-based algorithm that quantizes continuous power. In addition, the proposed meta-based DRL algorithm can achieve the required fast adaptation in the new environment with limited experiences.","1939-9359","","10.1109/TVT.2021.3098854","Engineering and Physical Sciences Research Council(grant numbers:EP/N007840/1,EP/T015985/1); Leverhulme Trust Research(grant numbers:RPG-2017-129); Hong Kong Research Grant Council(grant numbers:16220719); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9495238","Vehicular communications;meta-learning;deep reinforcement learning;DDPG","Resource management;Vehicle-to-everything;Heuristic algorithms;Vehicle dynamics;Interference;Fading channels;Wireless communication","deep learning (artificial intelligence);gradient methods;mobile radio;resource allocation;telecommunication computing;vehicular ad hoc networks","meta-reinforcement learning;shared resources;vehicle-to-infrastructure communications;vehicle-to-vehicle communications;vehicle-to-everything communications;dynamic vehicular environments;effective resource allocation policy;timely resource allocation policy;deep Q-network;deep deterministic policy-gradient;continuous power allocation problem;meta-based DRL algorithm;dynamic environment;DRL-based algorithm;DQN-based algorithm;deep reinforcement learning;resource allocation algorithm;dynamic V2X communications;continuous power quantization;V2I links;V2V links;sub-band assignment","","27","","37","IEEE","26 Jul 2021","","","IEEE","IEEE Journals"
"Reinforcement-Learning-Enabled Partial Confident Information Coverage for IoT-Based Bridge Structural Health Monitoring","L. Yi; X. Deng; L. T. Yang; H. Wu; M. Wang; Y. Situ","School of Civil Engineering, University of South China, Hengyang, China; Hunan Province Key Laboratory for Ultra-Fast Micro/Nano Technology and Advanced Laser Manufacture, School of Electrical Engineering, University of South China, Hengyang, China; Department of Computer Science, St. Francis Xavier University, Antigonish, Canada; School of Civil Engineering, University of South China, Hengyang, China; Hunan Province Key Laboratory for Ultra-Fast Micro/Nano Technology and Advanced Laser Manufacture, School of Electrical Engineering, University of South China, Hengyang, China; Department of Bridge Supervision, Foshan Road and Bridge Supervision Station Cooperation Ltd., Foshan, China","IEEE Internet of Things Journal","18 Feb 2021","2021","8","5","3108","3119","Internet-of-Things (IoT)-based bridge structural health monitoring (BSHM) has recently attracted considerable attention from both academic and industrial communities of civil engineering and computer science. In conjunction with researchers from civil engineering and computer science, this article studied a fundamental problem motivated from practical IoT-based BSHM: how to effectively prolong network lifetime while guaranteeing desired coverage. Integrating a promising reinforcement learning model named learning automata (LA) with confident information coverage (CIC) model, this article presented an energy-efficient sensor scheduling strategy for partial CIC coverage in IoT-based BSHM system to guarantee network coverage and prolong network lifetime. The proposed scheme fully exploits cooperation among deployed nodes and alternatively schedules the wake/sleep status of nodes while satisfying network connectivity and partial coverage ratio. Especially, the proposed scheme takes full advantage of the LA model to adaptively learn the optimal sensor scheduling strategy and significantly extend network lifetime. A series of comparison simulations using real data sets collected by a practical BSHM system strongly verify the effectiveness and energy efficiency of the proposed algorithm. To the best of our knowledge, this is the first study on how to combine the reinforcement learning mechanism with partial coverage for maximizing the network lifetime of the IoT-based BSHM.","2327-4662","","10.1109/JIOT.2020.3028325","National Natural Science Foundation of China(grant numbers:61871209,61901210,61971215); Hunan Provincial Innovation Foundation for Postgraduate(grant numbers:CX20190733); Research Foundation of Education Bureau of Hunan Province(grant numbers:19B498,18B287); Hunan Province Engineering Research Center of Radioactive Control Technology in Uranium Mining and Metallurgy and Hunan Province Engineering Technology Research Center of Uranium Tailings Treatment Technology(grant numbers:2019YKZX1006); Visiting Scholar Program at St. Francis Xavier University funded by State Scholarship Fund of the China Scholarship Council(grant numbers:201908430066); Opening Project of Cooperative Innovation Center for Nuclear Fuel Cycle Technology and Equipment, University of South China(grant numbers:2019KFZ12,2019KFY24); Hunan Province Applied Basic Research Base of Photoelectric Information Technology(grant numbers:GD19K02,GD19K03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211720","Bridge structural health monitoring (BSHM);confident information coverage (CIC) model;Internet of Things (IoT);learning automata (LA);partial coverage;reinforcement learning","Bridges;Monitoring;Computer science;Sensors;Learning automata;Internet of Things;Civil engineering","bridges (structures);condition monitoring;Internet of Things;learning (artificial intelligence);scheduling;structural engineering computing;wireless sensor networks","reinforcement learning mechanism;energy efficiency;practical BSHM system;optimal sensor scheduling strategy;partial coverage ratio;satisfying network connectivity;alternatively schedules;network coverage;IoT-based BSHM system;partial CIC coverage;energy-efficient sensor scheduling strategy;confident information coverage model;learning automata;promising reinforcement learning model;prolong network lifetime;practical IoT-based BSHM;computer science;civil engineering;industrial communities;academic communities;internet-of-Things-based bridge structural health monitoring;IoT-based bridge structural health monitoring;reinforcement-learning-enabled partial confident information coverage","","23","","35","IEEE","2 Oct 2020","","","IEEE","IEEE Journals"
"Towards transferring skills to flexible surgical robots with programming by demonstration and reinforcement learning","J. Chen; H. Y. K. Lau; W. Xu; H. Ren","Department of Industrial and Manufacturing Systems Engineering, The University of Hong Kong, Hong Kong SAR; Department of Industrial and Manufacturing Systems Engineering, The University of Hong Kong, Hong Kong SAR; Department of Biomedical Engineering, National University of Singapore, Singapore; Department of Biomedical Engineering, National University of Singapore, Singapore","2016 Eighth International Conference on Advanced Computational Intelligence (ICACI)","9 Apr 2016","2016","","","378","384","Flexible manipulators such as tendon-driven serpentine manipulators perform better than traditional rigid ones in minimally invasive surgical tasks, including navigation in confined space through key-hole like incisions. However, due to the inherent nonlinearities and model uncertainties, motion control of such manipulators becomes extremely challenging. In this work, a hybrid framework combining Programming by Demonstration (PbD) and reinforcement learning is proposed to solve this problem. Gaussian Mixture Models (GMM), Gaussian Mixture Regression (GMR) and linear regression are used to learn the inverse kinematic model of the manipulator from human demonstrations. The learned model is used as nominal model to calculate the output end-effector trajectories of the manipulator. Two surgical tasks are performed to demonstrate the effectiveness of reinforcement learning: tube insertion and circle following. Gaussian noise is introduced to the standard model and the disturbed models are fed to the manipulator to calculate the actuator input with respect to the task specific end-effector trajectories. An expectation maximization (E-M) based reinforcement learning algorithm is used to update the disturbed model with returns from rollouts. Simulation results have verified that the disturbed model can be converged to the standard one and the tracking accuracy is enhanced.","","978-1-4673-7782-9","10.1109/ICACI.2016.7449855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7449855","surgical robot;programming by demonstration;reinforcement learning;inverse kinematics;policy search","Manipulators;Kinematics;Mathematical model;Learning (artificial intelligence);Hidden Markov models;Aerospace electronics;Standards","actuators;automatic programming;end effectors;expectation-maximisation algorithm;Gaussian noise;learning (artificial intelligence);manipulator kinematics;medical computing;medical robotics;mixture models;motion control;regression analysis;surgery","nominal model;output end-effector trajectories;tube insertion;circle following;Gaussian noise;standard model;disturbed models;actuator input;expectation maximization based reinforcement learning algorithm;E-M based reinforcement learning algorithm;human demonstrations;manipulator inverse kinematic model;linear regression;GMR;Gaussian mixture regression;GMM;Gaussian mixture models;PbD;hybrid framework;motion control;model uncertainties;nonlinearities;key-hole like incisions;minimally invasive surgical tasks;tendon-driven serpentine manipulators;flexible manipulators;programming by demonstration;flexible surgical robots;skills transferring","","23","","30","IEEE","9 Apr 2016","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Solving the Heterogeneous Capacitated Vehicle Routing Problem","J. Li; Y. Ma; R. Gao; Z. Cao; A. Lim; W. Song; J. Zhang","Department of Industrial Systems Engineering and Management, National University of Singapore, Singapore; Department of Industrial Systems Engineering and Management, National University of Singapore, Singapore; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Manufacturing System Division, Singapore Institute of Manufacturing Technology, Singapore; School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China; Institute of Marine Science and Technology, Shandong University, Jinan, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Cybernetics","18 Nov 2022","2022","52","12","13572","13585","Existing deep reinforcement learning (DRL)-based methods for solving the capacitated vehicle routing problem (CVRP) intrinsically cope with a homogeneous vehicle fleet, in which the fleet is assumed as repetitions of a single vehicle. Hence, their key to construct a solution solely lies in the selection of the next node (customer) to visit excluding the selection of vehicle. However, vehicles in real-world scenarios are likely to be heterogeneous with different characteristics that affect their capacity (or travel speed), rendering existing DRL methods less effective. In this article, we tackle heterogeneous CVRP (HCVRP), where vehicles are mainly characterized by different capacities. We consider both min–max and min–sum objectives for HCVRP, which aim to minimize the longest or total travel time of the vehicle(s) in the fleet. To solve those problems, we propose a DRL method based on the attention mechanism with a vehicle selection decoder accounting for the heterogeneous fleet constraint and a node selection decoder accounting for the route construction, which learns to construct a solution by automatically selecting both a vehicle and a node for this vehicle at each step. Experimental results based on randomly generated instances show that, with desirable generalization to various problem sizes, our method outperforms the state-of-the-art DRL method and most of the conventional heuristics, and also delivers competitive performance against the state-of-the-art heuristic method, that is, slack induction by string removal. In addition, the results of extended experiments demonstrate that our method is also able to solve CVRPLib instances with satisfactory performance.","2168-2275","","10.1109/TCYB.2021.3111082","National Natural Science Foundation of China(grant numbers:61803104,62102228); Young Scholar Future Plan of Shandong University(grant numbers:62420089964188); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547060","Deep reinforcement learning (DRL);heterogeneous CVRP (HCVRP);min–max objective;min–sum objective","Reinforcement learning;Vehicle routing;Deep learning;Optimization","deep learning (artificial intelligence);optimisation;search problems;vehicle routing","deep reinforcement learning-based methods;existing DRL methods;HCVRP;heterogeneous capacitated vehicle routing problem;heterogeneous CVRP;heterogeneous fleet constraint;homogeneous vehicle fleet;longest travel time;min-sum objectives;node selection decoder accounting;route construction;single vehicle;state-of-the-art DRL method;state-of-the-art heuristic method;total travel time;travel speed;vehicle selection decoder accounting","","21","","62","IEEE","23 Sep 2021","","","IEEE","IEEE Journals"
"Bayesian Reinforcement Learning and Bayesian Deep Learning for Blockchains With Mobile Edge Computing","A. Asheralieva; D. Niyato","Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Cognitive Communications and Networking","5 Mar 2021","2021","7","1","319","335","We present a novel game-theoretic, Bayesian reinforcement learning (RL) and deep learning (DL) framework to represent interactions of miners in public and consortium blockchains with mobile edge computing (MEC). Within the framework, we formulate a stochastic game played by miners under incomplete information. Each miner can offload its block operations to one of the base stations (BSs) equipped with the MEC server. The miners select their offloading BSs and block processing rates simultaneously and independently, without informing other miners about their actions. As such, no miner knows the past and current actions of others and, hence, constructs its belief about these actions. Accordingly, we devise a Bayesian RL algorithm based on the partially-observable Markov decision process for miner's decision making that allows each miner to dynamically adjust its strategy and update its beliefs through repeated interactions with each other and with the mobile environment. We also propose a novel unsupervised Bayesian deep learning algorithm where the uncertainties about unobservable states are approximated with Bayesian neural networks. We show that the proposed Bayesian RL and DL algorithms converge to the stable states where the miners' actions and beliefs form the perfect Bayesian equilibrium (PBE) and myopic PBE, respectively.","2332-7731","","10.1109/TCCN.2020.2994366","National Natural Science Foundation of China (NSFC)(grant numbers:61950410603); National Research Foundation (NRF), Singapore, under Singapore Energy Market Authority (EMA), Energy Resilience, Singapore, Singapore NRF National Satellite of Excellence, Design Science and Technology for Secure Critical Infrastructure(grant numbers:NRF2017EWT-EP003-041,NRF2015-NRF-ISF001-2277,NSoE DeST-SCI2019-0007); A*STAR-NTU-SUTD Joint Research Grant on Artificial Intelligence for the Future of Manufacturing RGANS1906, Singapore(grant numbers:WASP/NTU M4082187 (4080),MOE Tier 2 MOE2014-T2-2-015 ARC4/15,MOE Tier 1 2017-T1-002-007 RG122/17); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9092992","Bayesian methods;blockchains;deep learning;game theory;incomplete information;machine learning;mobile edge computing;partially-observable Markov decision process;reinforcement learning;resource management","Task analysis;Bayes methods;Games;Resource management;Machine learning;Protocols","Bayes methods;belief networks;decision making;learning (artificial intelligence);Markov processes;stochastic games","stochastic game;Markov decision process;deep learning framework;game-theoretic learning framework;unsupervised Bayesian deep learning algorithm;miners interactions;Bayesian neural networks;Bayesian RL algorithm;consortium blockchains;mobile edge computing;Bayesian reinforcement learning","","19","","56","IEEE","13 May 2020","","","IEEE","IEEE Journals"
"An Edge Computing Framework for Powertrain Control System Optimization of Intelligent and Connected Vehicles Based on Curiosity-Driven Deep Reinforcement Learning","B. Hu; J. Li","Ministry of Education, Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Chongqing University of Technology, Chongqing, China; Ministry of Education, Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Chongqing University of Technology, Chongqing, China","IEEE Transactions on Industrial Electronics","3 May 2021","2021","68","8","7652","7661","For the ongoing revolution in developing intelligent and connected vehicles (ICVs), there is a lack of research for powertrain control systems using the latest artificial intelligence and vehicle-to-everything technology that have already been widely adopted in the autonomous driving systems. In this context, recent development of deep reinforcement learning (DRL) and one of the latest computing frameworks are coupled to facilitate an onboard-based intelligent powertrain control. Taking the boost control of a diesel engine equipped with variable geometry turbocharger as an example, the results show that the final control behavior indicated by the cumulated rewards is improved by 50.43% and the learning efficiency is improved by 74.29% for the proposed curiosity-driven DRL algorithm, compared with the same structure DRL algorithm with classic random exploration policy. In addition, unlike most of the DRL-based powertrain optimization algorithms, which have only been applied to single-machine architecture, this work manages the proposed DRL algorithm in parallel and, more importantly, from an edge computing perspective. This, in addition to greatly speeding up the algorithm training, can also realize a good balance of control accuracy and generality depending upon the selected training scenario. Moreover, unlike most of the cloud computing frameworks, which require low network latency, the proposed architecture can achieve a similar final control performance even if good network communication is not allowed. Compared with other existing powertrain control methods, the proposed algorithm is able to approximate a global powertrain control optimization autonomously in a connected manner, making it attractive to current ICVs with advanced automated driving and traditional powertrain control.","1557-9948","","10.1109/TIE.2020.3007100","National Natural Science Foundation of China(grant numbers:51905061); Natural Science Foundation of Chongqing(grant numbers:cstc2019jcyj-msxmX0097); State Key Laboratory of Engines(grant numbers:k2019-02); Science and Technology Research Program of Chongqing Municipal Education Commission(grant numbers:KJQN201801124); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9145806","Curiosity;deep reinforcement learning (DRL);edge computing;intelligent and connected vehicle (ICV);powertrain control","Mechanical power transmission;Edge computing;Reinforcement learning;Optimization;Artificial neural networks;Vehicle-to-everything;Prediction algorithms","automobiles;cloud computing;deep learning (artificial intelligence);diesel engines;fuel systems;intelligent transportation systems;learning systems;neurocontrollers;optimisation;power transmission (mechanical)","algorithm training;similar final control performance;existing powertrain control methods;global powertrain control optimization;powertrain control system optimization;curiosity-driven deep reinforcement learning;intelligent vehicles;connected vehicles;powertrain control systems;vehicle-to-everything technology;autonomous driving systems;deep reinforcement learning;onboard-based intelligent powertrain control;boost control;variable geometry turbocharger;control behavior;learning efficiency;curiosity-driven DRL algorithm;structure DRL algorithm;classic random exploration policy;DRL-based powertrain optimization algorithms;edge computing;diesel engine;single-machine architecture;network latency;network communication;ICV","","18","","32","IEEE","21 Jul 2020","","","IEEE","IEEE Journals"
"Real-time Motion Planning for Robotic Teleoperation Using Dynamic-goal Deep Reinforcement Learning","K. Kamali; I. A. Bonev; C. Desrosiers","Department of Automated Manufacturing Engineering, École de technologie supérieure, Montreal, Canada; Department of Automated Manufacturing Engineering, École de technologie supérieure, Montreal, Canada; Department of Software and IT Engineering, École de technologie supérieure, Montreal, Canada","2020 17th Conference on Computer and Robot Vision (CRV)","5 Jun 2020","2020","","","182","189","We propose Dynamic-goal Deep Reinforcement Learning (DGDRL) method to address the problem of robot arm motion planning in telemanipulation applications. This method intuitively maps human hand motions to a robot arm in real-time, while avoiding collisions, joint limits and singularities. We further propose a novel hardware setup, based on the HTC VIVE VR system, that enables users to smoothly control the robot tool position and orientation with hand motions, while monitoring its movements in a 3D virtual reality environment. A VIVE controller captures 6D hand movements and gives them as reference trajectories to a deep neural policy network for controlling the robot’s joint movements. Our DGDRL method leverages the state-of-art Proximal Policy Optimization (PPO) algorithm for deep reinforcement learning to train the policy network with the robot joint values and reference trajectory observed at each iteration. Since training the network on a real robot is time-consuming and unsafe, we developed a simulation environment called RobotPath which provides kinematic modeling, collision analysis and a 3D VR graphical simulation of industrial robots. The deep neural network trained using RobotPath is then deployed on a physical robot (ABB IRB 120) to evaluate its performance. We show that the policies trained in the simulation environment can be successfully used for trajectory planning on a real robot. The the codes, data and video presenting our experiments are available at https://github.com/kavehkamali/ppoRobotPath.","","978-1-7281-9891-0","10.1109/CRV50864.2020.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108691","deep reinforcement learning;robot path planning;obstacle avoidance;Teleoperation","Solid modeling;Analytical models;Three-dimensional displays;Service robots;Reinforcement learning;Real-time systems;Trajectory","collision avoidance;dexterous manipulators;industrial robots;learning (artificial intelligence);manipulator dynamics;manipulator kinematics;mobile robots;motion control;neural nets;optimisation;solid modelling;telerobotics;trajectory control;virtual reality","robotic teleoperation;robot arm motion planning;human hand motions;HTC VIVE VR system;robot tool position;3D virtual reality environment;deep neural policy network;proximal policy optimization algorithm;robot joint values;3D VR graphical simulation;industrial robots;deep neural network;physical robot;trajectory planning;dynamic-goal deep reinforcement;collision avoidance;DGDRL method;6D hand movements;kinematic modeling;RobotPath;PPO algorithm","","16","","41","IEEE","5 Jun 2020","","","IEEE","IEEE Conferences"
"Shifting Deep Reinforcement Learning Algorithm Toward Training Directly in Transient Real-World Environment: A Case Study in Powertrain Control","B. Hu; J. Li","Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing, China; Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing, China","IEEE Transactions on Industrial Informatics","26 Aug 2021","2021","17","12","8198","8206","Deep reinforcement learning (DRL) excels at playing a wide variety of simulated games and allows for a generic learning process that does not consider a specific knowledge of the task. However, due to the fact that a large prohibitively number of interactions with the environment are required and that the initial policy behavior is almost random, such an algorithm cannot be trained directly in a real-world environment while satisfying given safety constraints. In this article, a control framework based on DRL that shifts toward training directly in the transient real-world environment is proposed. This research is working on the assumption that some demonstration knowledge that operates under previous controllers and an abstract of the agent environment dynamics are available. By encoding this prior knowledge into a sophisticated learning architecture, a warm-starting DRL algorithm with a safe exploration guarantee can be anticipated. Taking the boost control problem for a variable geometry turbocharger equipped diesel engine as an example, the proposed algorithm improves the initial performance by 74.6% and the learning efficiency by an order of magnitude in contrast to its vanilla counterpart. Compared with other existing DRL-based powertrain control methods, the proposed algorithm can realize the “model-free” concept in the strict sense, making it attractive for future DRL-based powertrain control algorithms to build on.","1941-0050","","10.1109/TII.2021.3063489","National Natural Science Foundation of China(grant numbers:51905061); Natural Science Foundation of Chongqing(grant numbers:cstc2019jcyj-msxmX0097); China Postdoctoral Science Foundation(grant numbers:2020M671842); Science of Technology Research Program of Chongqing Municipal Education Commission(grant numbers:KJQN201801124); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369142","Deep reinforcement learning (DRL);demonstration;powertrain control;real-world environment","Mechanical power transmission;Safety;Task analysis;Engines;Transient analysis;Training;Informatics","diesel engines;fuel systems;learning (artificial intelligence);power transmission (mechanical)","future DRL-based powertrain control algorithms;existing DRL-based powertrain control methods;learning efficiency;variable geometry turbocharger equipped diesel engine;boost control problem;DRL algorithm;sophisticated learning architecture;agent environment dynamics;previous controllers;demonstration knowledge;control framework;initial policy behavior;specific knowledge;generic learning process;simulated games;transient real-world environment;deep reinforcement learning algorithm","","15","","24","IEEE","3 Mar 2021","","","IEEE","IEEE Journals"
"A Deployment-Efficient Energy Management Strategy for Connected Hybrid Electric Vehicle Based on Offline Reinforcement Learning","B. Hu; J. Li","Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing, China; Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing, China","IEEE Transactions on Industrial Electronics","5 Apr 2022","2022","69","9","9644","9654","With the development of recent artificial intelligence technology, especially after the great success of AlphaGo, there has been a growing interest in applying reinforcement learning (RL) to solve energy management strategy (EMS) problems for hybrid electric vehicles. However, the issues of current RL algorithms including deployment inefficiency, safety constraint, and simulation-to-real gap make it inapplicable to many industrial EMS tasks. With these in mind and considering the fact that there exists many suboptimal EMS controllers which can generate plentiful amounts of interactive data containing informative behaviors, an offline RL training framework that tries to extract policies with the maximum possible utility out of the available offline data is proposed. Furthermore, with connected vehicle technology standard in many new cars, rather than bringing all the data to the storage and analytics, a scheduled training framework is put forward. This cloud-based approach not only alleviates the computational burden of edge devices, but also more importantly provides a deployment-efficient solution to EMS tasks that have to adapt to changes of driving cycle. To evaluate the effectiveness of the proposed algorithm on real controllers, a hardware-in-the-loop (HIL) test is performed and the superiority of the proposed algorithm in contrast to dynamic programming, behavior cloning, rule-based, and vanilla off-policy RL algorithms is given.","1557-9948","","10.1109/TIE.2021.3116581","National Natural Science Foundation of China(grant numbers:51905061); Natural Science Foundation of Chongqing(grant numbers:cstc2019jcyj-msxmX0097); China Postdoctoral Science Foundation(grant numbers:2020M671842); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9562166","Connected;deployment-efficient;energy management strategy (EMS);hybrid electric vehicle (HEV);offline reinforcement learning (RL)","Energy management;Hybrid electric vehicles;Training;Task analysis;Heuristic algorithms;Batteries;Safety","cloud computing;dynamic programming;hardware-in-the-loop simulation;hybrid electric vehicles;reinforcement learning;traffic engineering computing","interactive data;informative behaviors;offline RL training framework;maximum possible utility;connected vehicle technology standard;scheduled training framework;cloud-based approach;deployment-efficient solution;vanilla off-policy RL algorithms;deployment-efficient energy management strategy;connected hybrid electric vehicle;offline reinforcement learning;artificial intelligence technology;energy management strategy problems;deployment inefficiency;safety constraint;simulation-to-real gap;suboptimal EMS controllers;hardware-in-the-loop test;HIL test;dynamic programming;behavior cloning;rule-based RL algorithm","","12","","33","CCBY","6 Oct 2021","","","IEEE","IEEE Journals"
"An Adaptive Hierarchical Energy Management Strategy for Hybrid Electric Vehicles Combining Heuristic Domain Knowledge and Data-Driven Deep Reinforcement Learning","B. Hu; J. Li","Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing, China; Key Laboratory of Advanced Manufacturing Technology for Automobile Parts, Ministry of Education, Chongqing University of Technology, Chongqing, China","IEEE Transactions on Transportation Electrification","2 Aug 2022","2022","8","3","3275","3288","With the development of artificial intelligence, there has been a growing interest in machine learning-based control strategy, among which reinforcement learning (RL) has opened up a new direction in the field of hybrid electric vehicle (HEV) energy management. However, the issues of the current RL setting ranging from inappropriate battery state-of-charge (SOC) constraint to ineffective and risky exploration make it inapplicable to many industrial energy management strategy (EMS) tasks. To address this, an adaptive hierarchical EMS combining heuristic equivalent consumption minimization strategy (ECMS) knowledge and deep deterministic policy gradient (DDPG), which is a state-of-the-art data-driven RL algorithm, is proposed in this work. For comparison purposes, the proposed strategy is contrasted with dynamic programming (DP), proportion integration differentiation (PID)-based adaptive ECMS, and rule-based and standard RL-based counterparts, and the results show that the fuel consumption after SOC correction for the proposed strategy is very close to that of the DP-based control and lower than that of the other three benchmark strategies. Considering that the proposed strategy can make better use of the RL techniques while realizing an effective, efficient, and safe exploration in a data-driven manner, it may become a strong foothold for future RL-based EMS to build on, especially when the controller has to be trained directly and from scratch in a real-world environment.","2332-7782","","10.1109/TTE.2021.3132773","National Natural Science Foundation of China(grant numbers:51905061); China Postdoctoral Science Foundation(grant numbers:2020M671842); Natural Science Foundation of Chongqing(grant numbers:cstc2019jcyj-msxmX0097); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635809","Energy management strategy (EMS);heuristic knowledge;hierarchical;hybrid electric vehicle (HEV);reinforcement learning (RL)","State of charge;Energy management;Hybrid electric vehicles;Optimization;Task analysis;Environmental management;Engines","battery powered vehicles;dynamic programming;energy management systems;hybrid electric vehicles;learning (artificial intelligence)","standard RL-based counterparts;SOC correction;DP-based control;benchmark strategies;RL techniques;data-driven manner;future RL-based EMS;adaptive hierarchical energy management strategy;hybrid electric vehicles combining heuristic domain knowledge;data-driven deep reinforcement;artificial intelligence;machine learning-based control strategy;reinforcement learning;hybrid electric vehicle energy management;current RL;battery state-of-charge constraint;risky exploration;industrial energy management strategy tasks;adaptive hierarchical EMS;heuristic equivalent consumption minimization strategy knowledge;deep deterministic policy gradient;state-of-the-art data-driven RL algorithm","","11","","42","IEEE","3 Dec 2021","","","IEEE","IEEE Journals"
"A Reinforcement Learning-Based User-Assisted Caching Strategy for Dynamic Content Library in Small Cell Networks","X. Zhang; G. Zheng; S. Lambotharan; M. R. Nakhai; K. -K. Wong","School of Computer Science and Electronic Engineering, University of Essex, Colchester, U.K.; Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, U.K.; Wolfson School of Mechanical, Electrical and Manufacturing Engineering, Loughborough University, Loughborough, U.K.; Department of Engineering, King’s College London, London, U.K.; Department of Electronic and Electrical Engineering, University College London, London, U.K.","IEEE Transactions on Communications","16 Jun 2020","2020","68","6","3627","3639","This paper studies the problem of joint edge cache placement and content delivery in cache-enabled small cell networks in the presence of spatio-temporal content dynamics unknown a priori. The small base stations (SBSs) satisfy users' content requests either directly from their local caches, or by retrieving from other SBSs' caches or from the content server. In contrast to previous approaches that assume a static content library at the server, this paper considers a more realistic non-stationary content library, where new contents may emerge over time at different locations. To keep track of spatio-temporal content dynamics, we propose that the new contents cached at users can be exploited by the SBSs to timely update their flexible cache memories in addition to their routine off-peak main cache updates from the content server. To take into account the variations in traffic demands as well as the limited caching space at the SBSs, a user-assisted caching strategy is proposed based on reinforcement learning principles to progressively optimize the caching policy with the target of maximizing the weighted network utility in the long run. Simulation results verify the superior performance of the proposed caching strategy against various benchmark designs.","1558-0857","","10.1109/TCOMM.2020.2977895","UK Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/N008219/1,EP/N007840/1); Leverhulme Trust Research Project Grant(grant numbers:RPG-2017-129); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020168","Non-stationary bandit;cache placement;content delivery;time-varying popularity;dynamic content library","Libraries;Servers;Indexes;Microcell networks;Optimization;Heuristic algorithms;Gallium nitride","cache storage;learning (artificial intelligence);mobile communication;telecommunication computing","reinforcement learning-based user-assisted caching strategy;dynamic content library;joint edge cache placement;content delivery;cache-enabled small cell networks;static content library;nonstationary content library;caching space;caching policy;weighted network utility;off-peak main cache updates;flexible cache memories;spatio-temporal content dynamics","","7","","38","IEEE","2 Mar 2020","","","IEEE","IEEE Journals"
"A Multi-Agent Deep Reinforcement Learning Approach for Practical Decentralized UAV Collision Avoidance","N. Thumiger; M. Deghat","School of Mechanical and Manufacturing Engineering, University of New South Wales, Sydney, NSW, Australia; School of Mechanical and Manufacturing Engineering, University of New South Wales, Sydney, NSW, Australia","IEEE Control Systems Letters","10 Jan 2022","2022","6","","2174","2179","This letter proposes an improved deep reinforcement learning controller for the decentralized collision avoidance problem. Using a unique architecture incorporating ‘long-short term memory cells’ and a reward function inspired from gradient-based approaches, the controller outperforms existing techniques in environments with variable numbers of agents. The design of the reward function is also evaluated to critique existing deep reinforcement learning approaches and used to establish a better result. The proposed controller is subsequently tested in simulation and a real-world 3-dimensional drone environment: outperforming the predominant classical approach in the literature by a significant margin.","2475-1456","","10.1109/LCSYS.2021.3138941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9663555","Machine learning;autonomous systems;control applications","Training;Trajectory;Collision avoidance;Reinforcement learning;System recovery;Navigation;Drones","autonomous aerial vehicles;collision avoidance;decentralised control;gradient methods;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems","gradient-based approaches;reward function;deep reinforcement learning approaches;real-world 3-dimensional drone environment;predominant classical approach;multiagent deep reinforcement learning approach;practical decentralized UAV collision avoidance;improved deep reinforcement learning controller;decentralized collision avoidance problem;unique architecture incorporating;long-short term memory cells","","7","","21","IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"Multi-Robot Cooperation Strategy in Game Environment Using Deep Reinforcement Learning","H. Zhang; D. Li; Y. He","The University of Chinese Academy of Sciences, Bejing, China; Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, China; Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, China","2018 IEEE International Conference on Robotics and Biomimetics (ROBIO)","14 Mar 2019","2018","","","886","891","The multi-robot system combines the characteristics and advantages of each component robot and can break through the constraints of a single robot's capability, greatly expanding the application of the robot. However, in the game environment, multi-robot systems face the challenge of intelligent decision-making in high-dimensional complex dynamic environments. The research progress of multi-agent decision-making strategies in the game environment based on deep reinforcement learning provides a solution for solving the problems faced by multi-robot systems. To this end, based on the deep reinforcement learning method, we analyze the multi-agent collaboration strategy in the game environment and propose a learning method that can measure cooperative information between multiple agents. On this basis, we conduct a Nash equilibrium game strategy analysis on the specific multi-agent game problem-the territory defense, use deep Q learning method to learn the defender's joint defense strategy. We conducted simulation experiments and verified the effectiveness of our method. Furthermore, we conducted experiments on the actual multi-robot system platform and demonstrated the feasibility of multi-agent cooperation strategy in practical multi-robot system based on deep reinforcement learning.","","978-1-7281-0377-8","10.1109/ROBIO.2018.8665165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8665165","","Games;Reinforcement learning;Multi-robot systems;Robot kinematics;Decision making;Task analysis","decision making;game theory;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems","multirobot cooperation strategy;deep reinforcement learning method;multiagent collaboration strategy;decision-making;Nash equilibrium game strategy;deep Q-learning method;defense strategy","","4","","15","IEEE","14 Mar 2019","","","IEEE","IEEE Conferences"
"Demand and Capacity Balancing Technology Based on Multi-agent Reinforcement Learning","Y. Chen; Y. Xu; M. Hu; L. Yang","School of Aerospace, Transport and Manufacturing, Cranfield University, Bedford, United Kingdom; School of Aerospace, Transport and Manufacturing, Cranfield University, Bedford, United Kingdom; College of Civil Aviation, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Civil Aviation, Nanjing University of Aeronautics and Astronautics, Nanjing, China","2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC)","15 Nov 2021","2021","","","1","9","To effectively solve Demand and Capacity Balancing (DCB) in large-scale and high-density scenarios through the Ground Delay Program (GDP) in the pre-tactical stage, a sequential decision-making framework based on a time window is proposed. On this basis, the problem is transformed into Markov Decision Process (MDP) based on local observation, and then Multi-Agent Reinforcement Learning (MARL) method is adopted. Each flight is regarded as an independent agent to decide whether to implement GDP according to its local state observation. By designing the reward function in multiple combinations, a Mixed Competition and Cooperation (MCC) mode considering fairness is formed among agents. To improve the efficiency of MARL, we use the double Q-Learning Network (DQN), experience replay technology, adaptive ϵ-greedy strategy and Decentralized Training with Decentralized Execution (DTDE) framework. The experimental results show that the training process of the MARL method is convergent, efficient and stable. Compared with the Computer-Assisted Slot Allocation (CASA) method used in the actual operation, the number of flight delays and the average delay time is reduced by 33.7% and 36.7% respectively.","2155-7209","978-1-6654-3420-1","10.1109/DASC52595.2021.9594343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9594343","demand and capacity balancing;ground delay program;multi-agent reinforcement learning;double Q-learning network;experience replay;adaptive ϵ-greedy strategy;decentralized training with decentralized execution","Training;Uncertainty;Economic indicators;Velocity control;Decision making;Reinforcement learning;Markov processes","aerospace computing;decision making;greedy algorithms;learning (artificial intelligence);Markov processes;multi-agent systems;resource allocation","MARL method;flight delays;high-density scenarios;ground delay program;GDP;sequential decision-making framework;time window;Markov decision process;multiagent reinforcement learning method;independent agent;local state observation;reward function;demand and capacity balancing technology;DCB;pretactical stage;MDP;mixed competition and cooperation mode;MCC;double Q-learning network;DQN;replay technology;adaptive ε-greedy strategy;decentralized training with decentralized execution framework;DTDE;computer-assisted slot allocation method;CASA","","2","","18","IEEE","15 Nov 2021","","","IEEE","IEEE Conferences"
"Integrated Frameworks of Unsupervised, Supervised and Reinforcement Learning for Solving Air Traffic Flow Management Problem","C. Huang; Y. Xu","School of Aerospace, Transport and Manufacturing, Cranfield University, Bedford, UK; School of Aerospace, Transport and Manufacturing, Cranfield University, Bedford, UK","2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC)","15 Nov 2021","2021","","","1","10","This paper studies the demand-capacity balancing (DCB) problem in air traffic flow management (ATFM) with collaborative multi-agent reinforcement learning (MARL). To attempt the proper ground delay for resolving airspace hotspots, a multi-agent asynchronous advantage actor-critic (MAA3C) framework is firstly constructed with the long short-term memory network (LSTM) for the observations, in which the number of agents varies across training steps. The unsupervised learning and supervised learning are then introduced for better collaboration and learning among the agents. Experimental results demonstrate the scalability and generalization of the proposed frameworks, by means of applying the trained models to resolve different simulated and real-world DCB scenarios, with various flights number, sectors number and capacity settings.","2155-7209","978-1-6654-3420-1","10.1109/DASC52595.2021.9594397","China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9594397","DCB;Multi-agent;Reinforcement Learning;Unsupervised Learning;Supervised Learning","Training;Scalability;Supervised learning;Collaboration;Clustering algorithms;Reinforcement learning;Delays","air traffic;groupware;multi-agent systems;recurrent neural nets;reinforcement learning;supervised learning;unsupervised learning","integrated frameworks;air traffic flow management;demand-capacity balancing problem;collaborative multiagent reinforcement learning;ground delay;airspace hotspots;multiagent asynchronous advantage actor-critic framework;MAA3C;long short-term memory network;unsupervised learning;supervised learning;DCB scenarios","","1","","23","IEEE","15 Nov 2021","","","IEEE","IEEE Conferences"
"Network Maintenance Planning Via Multi-Agent Reinforcement Learning","J. Thomas; M. P. Hernández; A. K. Parlikad; R. Piechocki","Communications, Systems and Network Group, University of Bristol; Institute for Manufacturing, University of Cambridge; Institute for Manufacturing, University of Cambridge; Communications, Systems and Network Group, University of Bristol","2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","6 Jan 2022","2021","","","2289","2295","Within this work, the challenge of developing maintenance planning solutions for networked assets is considered. This is challenging due to the very nature of these systems which are often heterogeneous, distributed and have complex co-dependencies between the constituent components for effective operation. We develop a Multi-Agent Reinforcement Learning (MARL) solution for this domain and apply it to a simulated Radio Access Network (RAN) comprising of nine Base Stations (BS). Through empirical evaluation we show that our model outperforms fixed corrective and preventive maintenance policies in terms of network availability whilst generally utilizing less than or equal amounts of maintenance resource.","2577-1655","978-1-6654-4207-7","10.1109/SMC52423.2021.9659150","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9659150","","Base stations;Conferences;Reinforcement learning;Planning;Preventive maintenance;Cybernetics;Radio access networks","multi-agent systems;preventive maintenance;radio access networks;reinforcement learning;telecommunication computing;telecommunication network planning","networked assets;complex co-dependencies;constituent components;multiagent reinforcement learning solution;corrective maintenance policies;preventive maintenance policies;network availability;maintenance resource;network maintenance planning;simulated radio access network","","1","","29","IEEE","6 Jan 2022","","","IEEE","IEEE Conferences"
"Data Center HVAC Control Harnessing Flexibility Potential via Real-Time Pricing Cost Optimization Using Reinforcement Learning","M. Biemann; P. A. Gunkel; F. Scheller; L. Huang; X. Liu","Department of Technology, Management and Economics, Section of Energy Economics and Modelling, Technical University of Denmark, Lyngby, Denmark; Department of Technology, Management and Economics, Section of Energy Economics and Modelling, Technical University of Denmark, Lyngby, Denmark; Faculty of Business and Engineering, Technical University of Applied Sciences Würzburg-Schweinfurt, Würzburg, Germany; Department of Manufacturing and Civil Engineering, Norwegian University of Science and Technology, Trondheim, Norway; Department of Technology, Management and Economics, Section of Energy Economics and Modelling, Technical University of Denmark, Lyngby, Denmark","IEEE Internet of Things Journal","25 Jul 2023","2023","10","15","13876","13894","With increasing electricity prices, cost savings through load shifting are becoming increasingly important for energy end users. While dynamic pricing encourages customers to shift demand to low price periods, the nonstationary and highly volatile nature of electricity prices poses a significant challenge to energy management systems. In this article, we investigate the flexibility potential of data centers by optimizing heating, ventilation, and air conditioning systems with a general model-free reinforcement learning (RL) approach. Since the soft actor-critic algorithm with feedforward networks did not work satisfactorily in this scenario, we propose instead a parameterization with a recurrent neural network architecture to successfully handle spot-market price data. The past is encoded into a hidden state, which provides a way to learn the temporal dependencies in the observations and highly volatile rewards. The proposed method is then evaluated in experiments on a simulated data center. Considering real temperature and price signals over multiple years, the results show a cost reduction compared to a proportional, integral and derivative controller while maintaining the temperature of the data center within the desired operating ranges. In this context, this work demonstrates an innovative and applicable RL approach that incorporates complex economic objectives into agent decision-making. The proposed control method can be integrated into various Internet of Things-based smart building solutions for energy management.","2327-4662","","10.1109/JIOT.2023.3263261","Nordic5Tech Ph.D. Fellowship; Reinforcing the Health Data Infrastructure in Mobility and Assurance through Data Democratization Project funded by the Norwegian Research Council(grant numbers:288856); Flexible Energy Denmark Project funded by Innovation Fund Denmark(grant numbers:8090-00069B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10089168","Data center cooling;heating;ventilation and air-conditioning (HVAC) control;load shifting;long short term memory (LSTM);real-time pricing (RTP);reinforcement learning (RL)","Costs;Pricing;HVAC;Cooling;Predictive models;Internet of Things;Reinforcement learning","building management systems;cost reduction;decision making;energy management systems;feedforward neural nets;HVAC;Internet of Things;multi-agent systems;optimisation;power engineering computing;power markets;pricing;recurrent neural nets;reinforcement learning","agent decision-making;air conditioning systems;data center HVAC control;dynamic pricing;electricity prices;energy end users;energy management systems;feedforward networks;general model-free reinforcement learning;heating, ventilation, and air conditioning systems;highly volatile rewards;innovative RL approach;Internet of Things-based smart building solutions;load shifting;proportional, integral derivative controller;real-time pricing cost optimization;recurrent neural network architecture;soft actor-critic algorithm;spot-market price data","","","","87","IEEE","30 Mar 2023","","","IEEE","IEEE Journals"
"Tensor product model transformation based integral sliding mode control with reinforcement learning strategy","G. Guoliang; C. Zhao; D. Wang","Modern Manufacture Engineering Center, Heilongjiang University of Science and Technology, Harbin, China; Modern Manufacture Engineering Center, Heilongjiang University of Science and Technology, Harbin, China; Faculty of Electronic Information and Electronic Engineering, Dalian University of Technology, Dalian, Liaoning, CN","Proceedings of the 33rd Chinese Control Conference","15 Sep 2014","2014","","","77","82","This paper presents a new chattering elimination method, and an optimal adaptive integral sliding mode controller design based on reinforcement learning for translational oscillations by a rotational actuator (TORA) system is demonstrated. At first, we introduce the tensor product model transformation based adaptive integral sliding mode controller. Next, we utilize an adaptive boundary layer width saturation function to get better performance. Reinforcement learning algorithm is employed to find the instantaneous optimal value for the boundary layer width of saturation function appeared in the adaptive integral sliding mode controller. The proposed tensor product model transformation based adaptive integral sliding mode controller with reinforcement learning strategy is verified by TORA system whereas the agent is rewarded for lower chattering, and punished for higher chattering. Simulation results show that chattering can be reduced effectively by incorporating reinforcement learning strategy into the adaptive integral sliding mode controller.","1934-1768","978-9-8815-6387-3","10.1109/ChiCC.2014.6896599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6896599","Tensor Product Model Transformation;Reinforcement Learning;Sliding Mode Control;Integral Sliding Mode Control","Tensile stress;Learning (artificial intelligence);Adaptation models;Sliding mode control;Numerical models;Uncertainty;Nonlinear systems","actuators;adaptive control;learning (artificial intelligence);matrix multiplication;oscillations;tensors;variable structure systems","adaptive boundary layer width saturation function;TORA system;translational oscillations by a rotational actuator system;optimal adaptive integral sliding mode controller design;chattering elimination method;reinforcement learning strategy;tensor product model transformation based integral sliding mode control","","","","30","","15 Sep 2014","","","IEEE","IEEE Conferences"
"Privacy-Preserving Federated Deep Reinforcement Learning for Mobility-as-a-Service","K. -F. Chu; W. Guo","School of Aerospace, Transport and Manufacturing, Cranfield University, Milton Keynes, U.K.; School of Aerospace, Transport and Manufacturing, Cranfield University, Milton Keynes, U.K.","IEEE Transactions on Intelligent Transportation Systems","","2023","PP","99","1","15","Mobility-as-a-service (MaaS) is a new transport model that combines multiple transport modes in a single platform. Dynamic passenger behavior based on past experiences requires reinforcement-based optimization of MaaS services. Deep reinforcement learning (DRL) may improve passenger satisfaction by offering the most appropriate transport services based on individual passenger experiences and preferences. However, this produces a new privacy risk to the MaaS platform using the centralized DRL method. Information leakage will occur if the platform is not carefully designed with privacy-preserving mechanisms. In this paper, we propose a federated deep deterministic policy gradient (FDDPG) that maximizes passenger satisfaction and MaaS long-term profit while preserving privacy. We enforce an equally weighted experience sampling mechanism to prevent sampling bias such that the solution quality of FDDPG is statistically equivalent to the centralized algorithm. During the model training and inference, information is processed locally, and only the gradients are shared, which prevents information leakage to any semi-honest participants and eavesdroppers. Secure aggregation protocol in line with the dynamic property of the mobile agent is also used in the gradient sharing step to ensure that the algorithm is prevented from inference attacks. We perform experiments on New York City-based real-world and synthetic scenarios. The results show that the proposed FDDPG can improve the MaaS profit and passenger satisfaction by about 90% and 15%, respectively, and maintain stable training against agent dropout. Our approach and findings could enhance MaaS utility as well as facilitate passenger trust and participation in MaaS and other data-driven transportation systems.","1558-0016","","10.1109/TITS.2023.3317358","Engineering and Physical Sciences Research Council (EPSRC) MACRO—Mobility as a Service: MAnaging Cybersecurity Risks across Consumers, Organisations and Sectors(grant numbers:EP/V039164/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10271125","Privacy-preserving machine learning;federated reinforcement learning;mobility-as-a-service;passenger behavior;secret sharing","Mobility as a service;Servers;Training;Privacy;Inference algorithms;Reinforcement learning;Data models","","","","","","","IEEE","3 Oct 2023","","","IEEE","IEEE Early Access Articles"
"Sensor-based control, real-time motion planning, and reinforcement learning for industrial robots","T. Kroeger","Director Intelligent Process Control and Robotics Laboratory, Karlsruhe Institute of Technology, Germany","2017 IEEE/SICE International Symposium on System Integration (SII)","5 Feb 2018","2017","","","3","3","Embedding multiple sensors — force/torque, vision, and distance — in the feedback loops of motion controllers has enabled new robot applications. For instance, safe human-robot interaction and many assembly tasks that could not be automated before. As important as these real-time control features is the ability to plan robot motions deterministically and in real-time. To enable spontaneous changes from sensor-guided robot motion control (e.g., force/torque or visual servo control) to trajectory-following motion control, an algorithmic framework is explained that lets us compute robot motions deterministically within less than one millisecond. The resulting class of on-line trajectory generation algorithms serves as an intermediate layer between low-level motion control and high-level sensor-based motion planning. Online motion generation from arbitrary states is an essential feature for autonomous hybrid switched motion control systems. Building upon this framework and with the goal of significantly reducing the amount of resources needed for programing industrial and service robots, reinforcement learning offers a yet unused potential that will be introduced as well. Samples and use-cases — including manipulation and human-robot interaction tasks — will accompany the talk in order to provide a comprehensible insight into these interesting and relevant fields of robotics.","2474-2325","978-1-5386-2263-6","10.1109/SII.2017.8279182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8279182","","Robot sensing systems;Service robots;Real-time systems;Planning;Robot motion;Motion control","","","","2","","","IEEE","5 Feb 2018","","","IEEE","IEEE Conferences"
"PD-FAC: Probability Density Factorized Multi-Agent Distributional Reinforcement Learning for Multi-Robot Reliable Search","W. Sheng; H. Guo; W. -Y. Yau; Y. Zhou","School of Automation Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Institute for Infocomm Research, A*STAR, Singapore, Singapore; Institute for Infocomm Research, A*STAR, Singapore, Singapore; College of Computer Science, Sichuan University, Chengdu, Sichuan, China","IEEE Robotics and Automation Letters","18 Jul 2022","2022","7","4","8869","8876","This letter presents a new range of multi-robot search for a non-adversarial moving target problems, namely multi-robot reliable search (MuRRS). The term ‘reliability’ in MuRRS is defined as the expectation of a predefined utility function over the probability density function (PDF) of the target’s capture time. We argue that MuRRS subsumes the canonical multi-robot efficient search (MuRES) problem, which minimizes the target’s expected capture time, as its special case, and offers the end user with a wide range of objective selection options. Since state-of-the-art algorithms are usually targeting the MuRES problem, and cannot offer up-to-standard performance to the various MuRRS objectives, we, thereby, propose a probability density factorized multi-agent distributional reinforcement learning method, namely PD-FAC, as a unified solution to the MuRRS problem. PD-FAC decomposes the PDF of the multi-robot system’s overall value distribution into a set of individual value distributions and guarantees that any reliability objective defined as a function of the overall system’s value distribution can be linearly approximated by the same reliability metric defined over the agent’s individual value distribution. In this way, the individual global maximum (IGM) principle is satisfied for all the pre-defined reliability metrics. It means that when each reinforcement learning agent is executing the individual policy, which maximizes its own reliability metric, the system’s overall reliability performance is also maximized. We evaluate and compare the performance of PD-FAC with state of the arts in a range of canonical multi-robot search environments with satisfying results, and also deploy PD-FAC to a real multi-robot system for non-adversarial moving target search.","2377-3766","","10.1109/LRA.2022.3188904","Singapore Government’s Research, Innovation, and Enterprise 2020 Plan(grant numbers:A1687b0033); Agency for Science, Technology, and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9816134","Factorized multi-agent reinforcement learning;moving target search;multi-robot reliable search","Search problems;Reliability;Reinforcement learning;Measurement;Robot kinematics;Robot sensing systems;Probability density function","learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;optimisation;path planning;probability;reliability","reinforcement learning agent;reliability metric;PD-FAC;canonical multirobot search environments;multirobot system;nonadversarial moving target search;probability density factorized multiagent distributional reinforcement learning;multirobot reliable search;nonadversarial moving target problems;term reliability;probability density function;canonical multirobot efficient search problem;MuRES problem;MuRRS objectives;MuRRS problem;value distribution;individual value distributions;reliability objective;pre-defined reliability metrics","","3","","30","IEEE","6 Jul 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning Based Multi-robot Formation Control Under Separation Bearing Orientation Scheme","Z. He; L. Dong; C. Sun; J. Wang","College of Electronics and Information Engineering, Tongji University, Shanghai, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","3792","3797","The multi-robot formation is a promising technology that endows robots with greater flexibility and cooperation capability. This paper aims at solving the separation-bearing-orientation scheme (SBOS) control problem of wheeled mobile robots (WMRs) based on the hybrid architecture of the improved deep deterministic policy gradient (DDPG) algorithm and the classical nonlinear formation control method. In particular, the trick of priority experience replay (PER) is utilized to improve the efficiency and stability of the whole training process. The proposed approach regards the controller gain hyperparameters of the three degrees of freedom in the WMR as the action space and develops the effective reward function to ensure the continuity of the velocity and acceleration while achieving the task. The simulation results of the two scenarios show that the proposed approach is available in performing various WMR-SBOS formation control tasks.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327315","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327315","reinforcement learning;formation control;determinitic policy gradient;priority experience replay","Trajectory;Training;Task analysis;Mobile robots;Wheels;Kinematics;Trajectory tracking","gradient methods;learning (artificial intelligence);mobile robots;multi-robot systems;position control","action space;DDPG algorithm;WMR-SBOS formation control;separation-bearing-orientation scheme control;priority experience replay;nonlinear formation control;deep deterministic policy gradient algorithm;hybrid architecture;wheeled mobile robots;multirobot formation","","3","","14","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Improvement of End-to-end Automatic Driving Algorithm Based on Reinforcement Learning","J. Tang; L. Li; Y. Ai; B. Zhao; L. Ren; B. Tian","Jiangsu XCMG Construction, Machinery Research Institute LTD, Xuzhou, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Jiangsu XCMG Construction, Machinery Research Institute LTD, Xuzhou, China; Jiangsu XCMG Construction, Machinery Research Institute LTD, Xuzhou, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","5086","5091","The end-to-end model is an important research direction in the field of automatic driving. Reinforcement learning is a common policy for training end-to-end models. Because the Deep Deterministic Policy Gradient algorithm can solve the problem of continuous motion space, it is a good method to deal with the problem of automatic driving. However, the Deep Deterministic Policy Gradient algorithm uses the random sampling method, which makes the training of neural network slow. We propose a reinforcement learning method combining Prioritized Experience Replay and Deep Deterministic Policy Gradient, making the training convergence faster.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8997184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8997184","Reinforcement learning;automatic driving;deep deterministic policy gradient;end-to-end","Training;Acceleration;Learning (artificial intelligence);Training data;Buffer storage;Neural networks;Autonomous vehicles","control engineering computing;gradient methods;learning (artificial intelligence);neural nets;road traffic control;sampling methods;traffic engineering computing","training end-to-end models;deep deterministic policy gradient algorithm;reinforcement learning method;end-to-end automatic driving algorithm;end-to-end model;random sampling method;prioritized experience replay","","2","","22","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Optimal Sliding Mode Control of ROV Fixed Depth Attitude Based on Reinforcement Learning","W. Fule; Q. Qiuxia; Y. Baolong; S. Liangliang; L. Yupeng; G. Guanyan; X. Zupeng; S. Liang; L. Zhigang","School of Information and Control Engineering, Shenyang Jianzhu University, Shenyang, China; School of Information and Control Engineering, Shenyang Jianzhu University, Shenyang, China; School of Information and Control Engineering, Shenyang Jianzhu University, Shenyang, China; School of Information and Control Engineering, Shenyang Jianzhu University, Shenyang, China; School of Information and Control Engineering, Shenyang Jianzhu University, Shenyang, China; School of Information and Control Engineering, Shenyang Jianzhu University, Shenyang, China; School of Information and Control Engineering, Shenyang Jianzhu University, Shenyang, China; School of Information and Control Engineering, Shenyang Jianzhu University, Shenyang, China; Shenyang Institute of Automation, Chinese Academy of Science, Shenyang, CHINA","2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","10 Nov 2021","2021","","","79","84","In this paper, an integral sliding mode control algorithm based on reinforcement learning is proposed for underwater vehicle depth determination control system. Since it is difficult for nonlinear continuous systems to track time-varying trajectories, the optimal tracking problem is transformed into a nonlinear time invariant optimal control problem by introducing a new state variable. The HJB equation of nonlinear systems is solved by adaptive dynamic programming (ADP) algorithm to find an approximate optimal strategy. Combined with integral sliding mode control, an approximate optimal sliding mode controller is designed. In addition, the Lyapunov equation is used to verify that the control strategy proposed in this paper can ensure that the tracking error of the system converges to zero gradually, and the error is also verified in a small range. Finally, the effectiveness of the algorithm is verified by simulation experiments, which enhances the anti-interference and robustness of the underwater robot in the depth control direction.","2642-6633","978-1-6654-2527-8","10.1109/CYBER53097.2021.9588177","National Natural Science Foundation of China(grant numbers:61873174); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9588177","","Neural networks;Stability criteria;Reinforcement learning;Approximation algorithms;Unmanned underwater vehicles;Mathematical models;Trajectory","attitude control;autonomous underwater vehicles;continuous systems;control system synthesis;dynamic programming;intelligent robots;Lyapunov methods;mobile robots;nonlinear control systems;optimal control;reinforcement learning;robust control;time-invariant systems;time-varying systems;variable structure systems","underwater robot robustness;Lyapunov equation;adaptive dynamic programming algorithm;HJB equation;nonlinear time invariant optimal control problem;optimal tracking problem;time-varying trajectories;nonlinear continuous systems;underwater vehicle depth determination control system;integral sliding mode control algorithm;reinforcement learning;ROV fixed depth attitude;depth control direction;tracking error;approximate optimal sliding mode controller","","","","19","IEEE","10 Nov 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning based Adaptive Resource Allocation Scheme for Multi-User Augmented Reality Service","K. Lee; C. -H. Youn","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea","2022 13th International Conference on Information and Communication Technology Convergence (ICTC)","25 Nov 2022","2022","","","1989","1994","Nowadays, thanks to the rapidly evolving AR/VR industry and 5G high-speed communication technology, it is possible to provide high-quality AR services to general users. However, despite this trend, the remaining problem is that the use of existing AR devices is inconvenient for various reasons such as heat generation and lack of computational power. Overcoming this could be achieved by the scheme named computational offloading. Applying the computational offloading in the edge server environment as in the typical general user AR/VR service scenario, however, requires careful management, on both the server and the user. In this paper, we propose a novel RL based resource management agent MARCO, which can perform edge server resource scheduling in a way that increases the user service quality, fully utilizing the resources in the edge server in AR offloading scenario. From our experimental results, we concluded that with our carefully designed reward functions and the training pipeline, our MARCO can surely act as an effective resource scheduler by outperforming other baselines by a large margin.","2162-1241","978-1-6654-9939-2","10.1109/ICTC55196.2022.9952934","Institute for Information & communications Technology Promotion (IITP)(grant numbers:2020-0-00537); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9952934","Reinforcement Learning;Deep Reinforcement Learning;Resource Allocation;Optical Flow;Augmented Reality","Training;Industries;Job shop scheduling;Scheduling algorithms;Pipelines;Reinforcement learning;Market research","augmented reality;mobile computing;reinforcement learning;resource allocation;scheduling","5G high-speed communication technology;AR offloading scenario;careful management;carefully designed reward functions;computational offloading;computational power;edge server environment;edge server resource scheduling;effective resource scheduler;heat generation;high-quality AR services;management agent MARCO;multiuserr augmented reality service;rapidly evolving AR/VR industry;reinforcement learning based adaptive resource allocation scheme;typical general user;user service quality","","","","30","IEEE","25 Nov 2022","","","IEEE","IEEE Conferences"
"Towards a multi-agent non-player character road network: a Reinforcement Learning approach","S. Makri; P. Charalambous","CYENS - Centre of Excellence, Nicosia, Cyprus; CYENS - Centre of Excellence, Nicosia, Cyprus","2021 IEEE Conference on Games (CoG)","7 Dec 2021","2021","","","1","5","Creating detailed and interactive game environments is an area of great importance in the video game industry. This includes creating realistic Non-Player Characters which respond seamlessly to the players actions. Machine learning had great contributions to the area, overcoming scalability and robustness shortcomings of hand-scripted models. We introduce the early results of a reinforcement learning approach in building a simulation environment for heterogeneous, multi-agent non-player characters in a dynamic road network game scene.","2325-4289","978-1-6654-3886-5","10.1109/CoG52621.2021.9619047","European Union's Horizon 2020 Research and Innovation Programme(grant numbers:739578); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619047","reinforcement learning;multi-agent;dynamic environment;Non-Player Characters;traffic simulation","Industries;Roads;Scalability;Conferences;Buildings;Games;Reinforcement learning","computer games;learning (artificial intelligence);multi-agent systems","multiagent nonplayer character road network;reinforcement learning approach;detailed game environments;interactive game environments;video game industry;players actions;machine learning;great contributions;hand-scripted models;simulation environment;multiagent nonplayer characters;dynamic road network game scene","","","","18","IEEE","7 Dec 2021","","","IEEE","IEEE Conferences"
"Weapon Targets Assignment for Electro-optical System Countermeasures based on Multi-objective Reinforcement Learning","H. Gong; S. Zhu; K. Xu; W. Sun","School of Science, Shenyang Ligong University, Shenyang, China; School of Automation and Electrical Engineering, Shenyang Ligong University, Shenyang, China; School of Science, Shenyang Ligong University, Shenyang, China; School of Science, Shenyang Ligong University, Shenyang, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","6714","6719","A reasonable electro-optical weapon assignment scheme can improve the combat efficiency and reduce the cost. Considering the spectrum type, field angle, and effective interference distance of electro-optical weapon, an optimization model is established with the objectives of maximum interference effectiveness and minimum weapon cost. A multi-objective deep reinforcement learning algorithm based on convex envelope Q learning (CEQL-EWTA)is designed to solve the electro-optical weapon-target assignment model. The states are designed by the number and characteristics of weapons and incoming targets, the actions were designed by heuristic rules, and the reward functions were designed based on the objective functions. The importance of objective functions is converted into preferences, and Q-value learning under multiple preferences is performed by deep Q-networks. Simulation experiments show that CEQL-EWTA can effectively solve the weapon assignment problem for electro-optical countermeasures system, which has higher efficiency compared with genetic algorithms and deep Q networks.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055731","electro-optical system countermeasures;weapon target assignment;multi-objective optimization;reinforcement learning","Costs;Q-learning;Weapons;Heuristic algorithms;Interference;Linear programming;Electrooptical waveguides","deep learning (artificial intelligence);genetic algorithms;learning (artificial intelligence);optimisation;reinforcement learning;weapons","CEQL-EWTA;combat efficiency;deep Q networks;deep Q-networks;effective interference distance;electro-optical countermeasures system;electro-optical system countermeasures;electro-optical weapon-target assignment model;incoming targets;maximum interference effectiveness;minimum weapon cost;multiobjective deep reinforcement learning algorithm;multiobjective reinforcement learning;objective functions;optimization model;Q-value learning;reasonable electro-optical weapon assignment scheme;weapon assignment problem;weapon targets assignment","","","","15","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Automated Post-Breach Penetration Testing through Reinforcement Learning","S. Chaudhary; A. O’Brien; S. Xu","The Beacom College of Computer and Cyber Sciences, Dakota State University, Madison, SD; The Beacom College of Computer and Cyber Sciences, Dakota State University, Madison, SD; The Beacom College of Computer and Cyber Sciences, Dakota State University, Madison, SD","2020 IEEE Conference on Communications and Network Security (CNS)","7 Aug 2020","2020","","","1","2","Predicting cyber attacks to networks is ever present challenges in the security domain. Rapid growth of Artificial Intelligence (AI) has made this even more challenging as machine learning algorithms are now used to attack such systems while defense systems continue to protect them with traditional approaches. Penetration testing (pentest) has long been one way to prevent security breaches by mimicking black hat hackers to expose possible exploits and vulnerabilities. Using trained machine learning agents to automate this process is an important research area that still needs to be explored. The objective of this paper is to apply machine learning in the post-exploitation phase of penetration testing to assess the vulnerability of the system and hence, contribute to the automation process of penetration testing. We train the agent using reinforcement learning by providing an appropriate environment to explore a compromised network and find sensitive files. By utilizing several different network environments during training, we hope to generalize our agent as much as possible, allowing for more widespread application. Extended research may include training our agent for further lateral exploration and exploitation in the system.","","978-1-7281-4760-4","10.1109/CNS48642.2020.9162301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162301","","Learning (artificial intelligence);Penetration testing;Training;Machine learning;Computer architecture;Linux","computer crime;learning (artificial intelligence);program testing","machine learning agents;automated post-breach penetration testing;black hat hackers;security breaches;defense systems;machine learning algorithms;artificial intelligence;cyber attacks;reinforcement learning","","21","","4","IEEE","7 Aug 2020","","","IEEE","IEEE Conferences"
"The Optical RL-Gym: An open-source toolkit for applying reinforcement learning in optical networks","C. Natalino; P. Monti","Electrical Engineering Department, Chalmers University of Technology, Gothenburg, Sweden; Electrical Engineering Department, Chalmers University of Technology, Gothenburg, Sweden","2020 22nd International Conference on Transparent Optical Networks (ICTON)","22 Sep 2020","2020","","","1","5","Reinforcement Learning (RL) is leading to important breakthroughs in several areas (e.g., self-driving vehicles, robotics, and network automation). Part of its success is due to the existence of toolkits (e.g., OpenAI Gym) to implement standard RL tasks. On the one hand, they allow for the quick implementation and testing of new ideas. On the other, these toolkits ensure easy reproducibility via quick and fair benchmarking. RL is also gaining traction in the optical networks research community, showing promising results while solving several use cases. However, there are many scenarios where the benefits of RL-based solutions remain still unclear. A possible reason for this is the steep learning curve required to tailor RL-based frameworks to each specific use case. This, in turn, might delay or even prevent the development of new ideas. This paper introduces the Optical Network Reinforcement-Learning-Gym (Optical RL-Gym)11The toolkit presented in this paper is available at: https://github.com/carlosnatalino/optical-rl-gym., an open-source toolkit that can be used to apply RL to problems related to optical networks. The Optical RL-Gym follows the principles established by the OpenAI Gym, the de-facto standard for RL environments. Optical RL-Gym allows for the quick integration with existing RL agents, as well as the possibility to build upon several already available environments to implement and solve more elaborated use cases related to the optical networks research area. The capabilities and the benefits of the proposed toolkit are illustrated by using the Optical RL-Gym to solve two different service provisioning problems.","2161-2064","978-1-7281-8423-4","10.1109/ICTON51198.2020.9203239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9203239","Machine learning;autonomous network management;resource assignment;reinforcement learning environments","Optical fiber networks;Integrated optics;Quality of service;Training;WDM networks;Optical modulation","learning (artificial intelligence);public domain software;telecommunication computing","open-source toolkit;OpenAI Gym;RL agents;optical networks research area;optical RL-Gym;optical network reinforcement-learning-gym;de-facto standard","","20","","15","IEEE","22 Sep 2020","","","IEEE","IEEE Conferences"
"Decentralized Multi-Agent Deep Reinforcement Learning in Swarms of Drones for Flood Monitoring","D. Baldazo; J. Parras; S. Zazo","Information Processing and Telecommunications Center, Universidad Politécnica de Madrid, Madrid, Spain; Information Processing and Telecommunications Center, Universidad Politécnica de Madrid, Madrid, Spain; Information Processing and Telecommunications Center, Universidad Politécnica de Madrid, Madrid, Spain","2019 27th European Signal Processing Conference (EUSIPCO)","18 Nov 2019","2019","","","1","5","Multi-Agent Deep Reinforcement Learning is becoming a promising approach to the problem of coordination of swarms of drones in dynamic systems. In particular, the use of autonomous aircraft for flood monitoring is now regarded as an economically viable option and it can benefit from this kind of automation: swarms of unmanned aerial vehicles could autonomously generate nearly real-time inundation maps that could improve relief work planning. In this work, we study the use of Deep Q-Networks (DQN) as the optimization strategy for the trajectory planning that is required for monitoring floods, we train agents over simulated floods in procedurally generated terrain and demonstrate good performance with two different reward schemes.","2076-1465","978-9-0827-9703-9","10.23919/EUSIPCO.2019.8903067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8903067","navigation;reinforcement learning;swarms;decentralized control;floods","Aircraft;Training;Floods;Reinforcement learning;Atmospheric modeling;Europe;Signal processing","autonomous aerial vehicles;control engineering computing;emergency management;floods;learning (artificial intelligence);multi-agent systems;optimisation;trajectory control","relief work planning;Deep Q-Networks;flood monitoring;dynamic systems;autonomous aircraft;real-time inundation maps;decentralized multiagent deep reinforcement learning;drones swarms coordination problem;unmanned aerial vehicles swarms;optimization strategy;trajectory planning","","18","","12","","18 Nov 2019","","","IEEE","IEEE Conferences"
"Proposal of Allocating Radio Resources to Multiple Slices in 5G using Deep Reinforcement Learning","Y. Abiko; D. Mochizuki; T. Saito; D. Ikeda; T. Mizuno; H. Mineno","Graduate School of Integrated Science and Technology, Shizuoka University, Hamamatsu, Japan; Graduate School of Integrated Science and Technology, Shizuoka University, Hamamatsu, Japan; Research Laboratories, NTT DOCOMO, Inc., Yokosuka, Japan; Research Laboratories, NTT DOCOMO, Inc., Yokosuka, Japan; Faculty of Information Science, Aichi Institute of Technology, Toyota, Japan; Graduate School of Integrated Science and Technology, Shizuoka University, Hamamatsu, Japan","2019 IEEE 8th Global Conference on Consumer Electronics (GCCE)","27 Feb 2020","2019","","","1","2","Fifth-generation (5G) mobile communication is expected to provide a suitable network for all service requirements. Automation of network slicing is required to respond to the dynamically changing service requirements. This paper proposes a method to allocate the radio resources that satisfy the service requirements irrespective of the number of slices utilizing reinforcement learning. From the evaluation of the proposed method using a scenario, in which the number of slices fluctuates with the passage of time, it is clarified that this method allocates the radio resources to fulfill the requirements of the service following the change in the number of slices.","2378-8143","978-1-7281-3575-5","10.1109/GCCE46687.2019.9015369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9015369","network slicing;reinforcement learning;radio access network;5G","Bandwidth;5G mobile communication;Resource management;Network slicing;Games;Artificial intelligence;Channel allocation","5G mobile communication;learning (artificial intelligence);resource allocation;telecommunication computing","network slicing;radio resources;deep reinforcement learning;fifth-generation mobile communication;5G mobile communication","","5","","8","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning based Target Detection for Unmanned Aerial Vehicle","T. Swain; M. Rath; J. Mishra; S. Banerjee; T. Samant","School of Computer Engineering, Kalinga Institute of Industrial Technology, Bhubaneswar, India; School of Computer Applications, Kalinga Institute of Industrial Technology, Bhubaneswar, India; School of Computer Engineering, Kalinga Institute of Industrial Technology, Bhubaneswar, India; Department of Engineering Technology, BITS - Pilani, Rajasthan, India; School of Electronics Engineering, Kalinga Institute of Industrial Technology, Bhubaneswar, India","2022 IEEE India Council International Subsections Conference (INDISCON)","24 Aug 2022","2022","","","1","5","Considering the automation for delivering items using unmanned aerial vehicles has emerged with many research and studies. The research is proposed with one vital aspect to provide foremost necessary medical aids by reaching accurately to patients. This paper deals with developing an integrated camera-based positioning system with GPS for quadcopters to automate the landing process. In addition to the frontal-viewing camera, a downward-viewing camera is used for landing at the identified location based on a QR code. The efficient deep learning-based method for object detection is followed by a pixel-extraction-based method for the code recognition to identify the site accurately.","","978-1-6654-6601-1","10.1109/INDISCON54605.2022.9862891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9862891","Drone Delivery System;Object Recognition;Object Detection;ROI;QR Code Detection","Learning systems;Codes;Target recognition;Object detection;Reinforcement learning;QR codes;Cameras","autonomous aerial vehicles;cameras;feature extraction;helicopters;learning (artificial intelligence);object detection;remotely operated vehicles","landing process;frontal-viewing camera;downward-viewing camera;efficient deep learning-based method;object detection;pixel-extraction-based method;deep reinforcement;target detection;unmanned aerial vehicle;vital aspect;foremost necessary medical aids;integrated camera-based","","4","","12","IEEE","24 Aug 2022","","","IEEE","IEEE Conferences"
"Towards Continuous Cyber Testing with Reinforcement Learning for Whole Campaign Emulation","T. Cody; P. Beling; L. Freeman","National Security Institute, Virginia Tech, Arlington, VA, USA; National Security Institute, Virginia Tech, Arlington, VA, USA; National Security Institute, Virginia Tech, Arlington, VA, USA","2022 IEEE AUTOTESTCON","23 Dec 2022","2022","","","1","5","Modern automated penetration testing uses rule-based procedures and model-checking concepts to search through all possible attacks on network models and identify those that violate some correctness or security property by generating an attack graph. By generating all possible attacks, modern, top-down approaches inherently do not isolate the few attacks that matter the most. This weakness is exacerbated in future network settings like 5G and Internet of Things (IoT) settings where networks are expected to have thousands of hosts (or more) and evolve over time. This has created a perception that the attack graph concept itself is inadequate, in turn hindering the automation of cyber testing. Recent research re-positions automated attack graph generation as a best practice in cyber defense by applying deep reinforcement learning (RL). While recent research into penetration testing with RL has seen a rapid growth in interest, a clear concept of operational use has not been defined. We define and provide formalism for the concept of whole campaign emulation (WCE). We present WCE as both a challenge problem and a framework for automating cyber T&E with RL. This manuscript captures an RL-oriented perspective on the past, present, and future of attack graph generation, and serves as a primer from researchers and practitioners alike. With WCE, organizations from small businesses to nation-states can feasibly institute continuous cyber T&E with low test costs and low disruption to operations.","","978-1-7281-5400-8","10.1109/AUTOTESTCON47462.2022.9984769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9984769","penetration testing;attack graphs;emulation;network security;reinforcement learning","Deep learning;Costs;Emulation;Reinforcement learning;Organizations;Benchmark testing;Internet of Things","formal verification;graph theory;reinforcement learning;security of data","attack graph concept;campaign emulation;clear concept;continuous cyber T&E;continuous cyber testing;cyber defense;cyber testing;deep reinforcement learning;future network settings;Internet of Things settings;IoT;low test costs;model-checking concepts;modern automated penetration testing;network models;re-positions automated attack graph generation;RL-oriented perspective;rule-based procedures;security property;WCE","","2","","31","IEEE","23 Dec 2022","","","IEEE","IEEE Conferences"
"Reinforcement Learning Lifecycle for the Design of Advanced Robotic Systems","P. Kurrek; F. Zoghlami; M. Jocas; M. Stoelen; V. Salehi","Applied Sciences and Mechatronics, Munich University of Applied Sciences, Munich, Germany; Applied Sciences and Mechatronics, Munich University of Applied Sciences, Munich, Germany; Applied Sciences and Mechatronics, Munich University of Applied Sciences, Munich, Germany; School of Computing, Electronics and Mathematics, University of Plymouth, Plymouth, United Kingdom; Applied Sciences and Mechatronics, Munich University of Applied Sciences, Munich, Germany","2020 IEEE Conference on Industrial Cyberphysical Systems (ICPS)","4 Dec 2020","2020","1","","230","235","Machine learning is a recognised technology for problem solving and accelerates the automation by enabling systems to act independently. Cyber-physical systems based on machine learning enable factories to increase the skills of robotic systems. The lack of standardised tools and workflows for artificial intelligence (AI) rises the importance of research methodologies and frameworks for industrial application. First concepts have shown potential for a combination of holistic development methodologies and AI. We present a Reinforcement Learning Lifecycle (RLL) for the development of advanced robots. The autonomous software agent can furthermore lead to the automated optimisation of the systems. The virtual-based method speeds up learning processes, improve development and operation processes by the evaluation of multiple simulation environments. We show how an AI-based methodology assists the development of advanced robots along the product lifecycle. The first implementations show potential regarding the usability and results of the approach during different development processes.","","978-1-7281-6389-5","10.1109/ICPS48405.2020.9274698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274698","Industrial Robots;Simulation;Methodologies and Tools","Robots;Reinforcement learning;Task analysis;Service robots;Artificial intelligence;Trajectory;Training","learning (artificial intelligence);robots;software agents","machine learning;cyber-physical systems;standardised tools;artificial intelligence;holistic development methodologies;AI;advanced robots;automated optimisation;virtual-based method;learning processes;product lifecycle;reinforcement learning lifecycle","","1","","38","IEEE","4 Dec 2020","","","IEEE","IEEE Conferences"
"Research on interacted response technology of cyber security protection devices based on deep reinforcement learning oriented to new generation of power system","Y. Cao; X. Li; J. Liu; J. Yan; J. Zhao; H. Li","State Grid Information & Telecommunication Branch, Beijing, China; State Grid Information & Telecommunication Branch, Beijing, China; State Grid Information & Telecommunication Branch, Beijing, China; State Grid Information & Telecommunication Branch, Beijing, China; State Grid Information & Telecommunication Branch, Beijing, China; State Grid Information & Telecommunication Branch, Beijing, China","2022 2nd International Conference on Electrical Engineering and Control Science (IC2ECS)","6 Apr 2023","2022","","","377","381","The existing power system cyber security protection system lacks pertinence for new services and has a low degree of automation. It is urgent to propose a security protection framework for the characteristics of new generation of power system business, especially to improve the attack prevention capability of distributed power sources, flexible and adjustable loads, etc., ensuring the stable operation of the power grid. Aiming at the lack of efficient linkage and coordination of existing cyber security protection devices in new generation of power systems, this paper proposes an interacted response technology for security protection devices based on intelligent learning. Our scheme standardizes and correlates the various threat intelligence collected by the security protection equipment, and use the multi-agent deep reinforcement learning method to realize the issuance and optimization of automated strategies in combination with the analysis results. The interacted strategy of safety protection devices is optimized through collaborative heterogeneous reinforcement learning model. And the validity of interacted configuration is verified for correctness, completeness, redundancy and consistency.","","979-8-3503-9916-5","10.1109/IC2ECS57645.2022.10087929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10087929","Interacted response technology;Cyber security protection;Deep reinforcement learning;New generation of power system","Couplings;Deep learning;Redundancy;Reinforcement learning;Network security;Power systems;Safety","computer network security;cyber-physical systems;deep learning (artificial intelligence);learning (artificial intelligence);multi-agent systems;power engineering computing;power grids;reinforcement learning;security of data","collaborative heterogeneous reinforcement learning model;cyber security protection devices;distributed power sources;existing power system cyber security protection system;interacted response technology;multiagent deep reinforcement learning method;power grid;power system business;safety protection devices;security protection equipment;security protection framework","","","","13","IEEE","6 Apr 2023","","","IEEE","IEEE Conferences"
"Using reinforcement learning for demand response of domestic hot water buffers: A real-life demonstration","O. De Somer; A. Soares; K. Vanthournout; F. Spiessens; T. Kuijpers; K. Vossen","VITO / EnergyVille, Genk, Belgium; VITO / EnergyVille, Genk, Belgium; VITO / EnergyVille, Genk, Belgium; VITO / EnergyVille, Genk, Belgium; Enervalis, Belgium; Enervalis, Belgium","2017 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe)","18 Jan 2018","2017","","","1","7","This paper demonstrates a data-driven control approach for demand response in real-life residential buildings. The objective is to optimally schedule the heating cycles of the Domestic Hot Water (DHW) buffer to maximize the self-consumption of the local photovoltaic (PV) production. A model-based reinforcement learning technique is used to tackle the underlying sequential decision-making problem. The proposed algorithm learns the stochastic occupant behavior, predicts the PV production and takes into account the dynamics of the system. A real-life experiment with six residential buildings is performed using this algorithm. The results show that the self-consumption of the PV production is significantly increased, compared to the default thermostat control.","","978-1-5386-1953-7","10.1109/ISGTEurope.2017.8260152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8260152","Demand Response;Domestic Hot Water;Field Experiment;Reinforcement Learning","Temperature measurement;Temperature sensors;Water heating;Space heating;Heat pumps;Boilers","building management systems;decision making;demand side management;learning (artificial intelligence);photovoltaic power systems;power generation scheduling;space heating;thermostats;water supply","demand response;domestic hot water buffers;heating cycles;reinforcement learning technique;PV production;residential buildings;photovoltaic production;sequential decision-making problem;thermostat control","","27","","14","IEEE","18 Jan 2018","","","IEEE","IEEE Conferences"
"RLDRM: Closed Loop Dynamic Cache Allocation with Deep Reinforcement Learning for Network Function Virtualization","B. Li; Y. Wang; R. Wang; C. Tai; R. Iyer; Z. Zhou; A. Herdrich; T. Zhang; A. Haj-Ali; I. Stoica; K. Asanovic","Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley","2020 6th IEEE Conference on Network Softwarization (NetSoft)","12 Aug 2020","2020","","","335","343","Network function virtualization (NFV) technology attracts tremendous interests from telecommunication industry and data center operators, as it allows service providers to assign resource for Virtual Network Functions (VNFs) on demand, achieving better flexibility, programmability, and scalability. To improve server utilization, one popular practice is to deploy best effort (BE) workloads along with high priority (HP) VNFs when high priority VNF's resource usage is detected to be low. The key challenge of this deployment scheme is to dynamically balance the Service level objective (SLO) and the total cost of ownership (TCO) to optimize the data center efficiency under inherently fluctuating workloads. With the recent advancement in deep reinforcement learning, we conjecture that it has the potential to solve this challenge by adaptively adjusting resource allocation to reach the improved performance and higher server utilization. In this paper, we present a closed-loop automation system RLDRM11RLDRM: Reinforcement Learning Dynamic Resource Management to dynamically adjust Last Level Cache allocation between HP VNFs and BE workloads using deep reinforcement learning. The results demonstrate improved server utilization while maintaining required SLO for the HP VNFs.","","978-1-7281-5684-2","10.1109/NetSoft48620.2020.9165471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165471","","Resource management;Servers;Dynamic scheduling;Hardware;Network function virtualization;Machine learning;Data centers","cache storage;computer centres;learning (artificial intelligence);resource allocation;software defined networking;virtualisation","deep reinforcement learning;network function virtualization technology;telecommunication industry;data center operators;service providers;high priority VNF resource usage;deployment scheme;service level objective;data center efficiency;resource allocation;higher server utilization;closed-loop automation system RLDRM;reinforcement learning dynamic resource management;level cache allocation;HP VNF;virtual network functions;loop dynamic cache allocation;SLO;total cost of ownership;TCO","","11","","53","IEEE","12 Aug 2020","","","IEEE","IEEE Conferences"
"Coexistence Management for URLLC in Campus Networks via Deep Reinforcement Learning","B. Khodapanah; T. Hößler; B. Yuncu; A. N. Barreto; M. Simsek; G. Fettweis","Vodafone Chair Mobile Communications Systems, Technische Universität, Dresden, Germany; Vodafone Chair Mobile Communications Systems, Technische Universität, Dresden, Germany; Vodafone Chair Mobile Communications Systems, Technische Universität, Dresden, Germany; Barkhausen Institut, Dresden, Germany; International Computer Science Institute, Berkeley, USA; Vodafone Chair Mobile Communications Systems, Technische Universität, Dresden, Germany","2020 IEEE Wireless Communications and Networking Conference (WCNC)","19 Jun 2020","2020","","","1","6","Increased usage of wireless technologies in unlicensed frequency bands inevitably increases the co-channel interference. Hence, for applications such as ultra-reliable-low-latency-communications (URLLC) in factory automation, the interference should be avoided. An intelligent coexistence management entity, which dynamically distributes the time and frequency resources, has been shown to be greatly beneficial in boosting efficiency and avoiding crippling interruptions of the wireless medium. This entity also supports multi-connectivity schemes, which are crucial for industry-level reliability requirements. The proposed governing technique of the coexistence management is a deep reinforcement learning (DRL) method, which is a model-free framework and channel allocation decisions are learned merely by interactions with the environment. The simulation results have shown that the employed method can greatly increase the reliability of the wireless network, when compared with legacy methods.","1558-2612","978-1-7281-3106-1","10.1109/WCNC45663.2020.9120498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9120498","Coexistence Management;URLLC;Campus Network;Deep Reinforcement Learning;5G","Time-frequency analysis;Recurrent neural networks;Wireless networks;Simulation;Reinforcement learning;Ultra reliable low latency communication;Channel allocation","channel allocation;computer network management;computer network reliability;learning (artificial intelligence);telecommunication computing;wireless channels","URLLC;campus networks;wireless technologies;unlicensed frequency bands;co-channel interference;ultra-reliable-low-latency-communications;factory automation;intelligent coexistence management entity;frequency resources;wireless medium;multiconnectivity schemes;industry-level reliability requirements;deep reinforcement learning method;model-free framework;channel allocation decisions;crippling interruption avoidance;time resources;wireless network reliability","","5","","14","IEEE","19 Jun 2020","","","IEEE","IEEE Conferences"
"Gait synthesis of a hybrid legged robot using reinforcement learning","J. Lopes dos Santos; C. L. Nascimento","Division of Electronic Engineering, Instituto Tecnológico de Aeronáutica, Brazil; Division of Electronic Engineering, Instituto Tecnologico de Aeronautica, Sao Jose dos Campos, SP, BR","2015 Annual IEEE Systems Conference (SysCon) Proceedings","4 Jun 2015","2015","","","439","444","This article is concerned with the gait synthesis problem of a hybrid robot (in this case, a four-legged robot with free wheels on its feet) considering multiple criteria. It is assumed that the position of each leg actuator over time is described by a periodic function with parameters that are determined using the learning automata reinforcement learning algorithm. Analysis of the robot morphology is used to group similar legs and decrease the number of actuator functions that must be determined. MATLAB/Simulink/SimMechanics Toolbox are used to simulate the robot gait. The simulated robot response is evaluated by the reinforcement learning algorithm considering: 1) the robot frontal speed, 2) the “smoothness” of the robot movements, 3) the largest torque required by all leg actuators, and 4) the robot energy consumption. When the reinforcement learning algorithm converges to a good solution, it is applied to the real robot which was built using the Bioloid Comprehensive Kit, an educational robot kit manufactured by ROBOTIS. The responses of the simulated and real robot are then compared and are shown to be similar.","","978-1-4799-5927-3","10.1109/SYSCON.2015.7116790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7116790","Walking Machine;Hybrid Legged Robot;Reinforcement Learning;Learning Automata;Gait Synthesis","Legged locomotion;Actuators;Robot kinematics;Convergence;Learning (artificial intelligence)","actuators;automata theory;control engineering computing;intelligent robots;learning (artificial intelligence);legged locomotion;wheels","gait synthesis;hybrid legged robot;four-legged robot;wheels;leg actuator;periodic function;learning automata reinforcement learning algorithm;robot morphology analysis;MATLAB;Simulink;SimMechanics Toolbox;robot gait simulation;robot energy consumption;bioloid comprehensive kit;educational robot kit;ROBOTIS","","1","","13","IEEE","4 Jun 2015","","","IEEE","IEEE Conferences"
"Semiconductor Power Module Current Balancing Using Reinforcement Machine Learning","B. Westmoreland; A. V. Bilbao; S. B. Bayne","Department of Electrical and Computer Engineering, Texas Tech University, Lubbock, Texas; Department of Electrical and Computer Engineering, Texas Tech University, Lubbock, Texas; Department of Electrical and Computer Engineering, Texas Tech University, Lubbock, Texas","2021 IEEE Pulsed Power Conference (PPC)","15 Mar 2022","2021","","","1","5","In high power applications, semiconductor power modules containing paralleled MOSFETs are often used to achieve high output currents. The current distribution between devices within a module is influenced by several factors such as component layout, minor defects due to manufacturing tolerances, and general devices degradation that occurs over time. This paper describes a method of balancing the current between paralleled MOSFETs by independently modulating each device’s gate-to-source voltage and measuring the corresponding drain-to-source currents. To achieve this, a detailed simulation is created using MATLAB and Simulink. A reinforcement learning agent is implemented with the goal of adaptively balancing power module current as the components inside degrade over time.","2158-4923","978-1-6654-3347-1","10.1109/PPC40517.2021.9733124","Arm; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733124","","MOSFET;Semiconductor device measurement;Voltage measurement;Software packages;Layout;Multichip modules;Reinforcement learning","electric current control;learning (artificial intelligence);power MOSFET;semiconductor device models","general devices degradation;paralleled MOSFETs;gate-to-source voltage;drain-to-source currents;reinforcement learning agent;semiconductor power module current balancing;reinforcement machine learning;high power applications;high output currents;current distribution;component layout","","","","7","IEEE","15 Mar 2022","","","IEEE","IEEE Conferences"
"Research on Intelligent Optimization Algorithm of Arranging Based on Reinforcement Learning","F. He","Department of music, Nanjing Xiaozhuang University, Nanjing, Jiangsu, China","2022 2nd International Conference on Networking, Communications and Information Technology (NetCIT)","28 Mar 2023","2022","","","342","345","Reinforcement learning is an important machine learning method, which has been widely used in the fields of intelligent robot, economics, industrial manufacturing, game and so on. Reinforcement learning is a kind of learning from environmental state to action mapping, and it is expected that action will get the maximum cumulative reward from the environment. Reinforcement learning can be divided into two basic processes: learning process and planning process. Learning refers to the process of directly interacting with the environment, and in this process, using the obtained direct experience to update the value function to improve the strategy. Planning refers to the process of learning in the environmental model, and in this process, the simulation experience generated by the model is used to update the value function to improve the strategy. The research of intelligent algorithm is not only conducive to all aspects of the national economy, but also brings convenience to people's activities. It has certain theoretical significance and great application prospects. This paper summarizes the research trends of optimization algorithms, reviews the development status of swarm intelligence algorithms, and provides a theoretical basis for the follow-up research of swarm intelligence algorithms. This paper summarizes and reviews the relevant theoretical methods and Application Research of using deep reinforcement learning method to solve combinatorial optimization problems in recent years, summarizes and summarizes its basic principles, relevant methods and application research, and points out some problems to be solved in this direction in the future.","","978-1-6654-9273-7","10.1109/NetCIT57419.2022.00087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10079044","Strengthen learning;Intelligent optimization algorithm of arraying music;Combinatorial optimization problem","Deep learning;Training;Scheduling algorithms;Optimization methods;Reinforcement learning;Games;Scheduling","deep learning (artificial intelligence);intelligent robots;learning (artificial intelligence);optimisation;particle swarm optimisation;reinforcement learning;swarm intelligence","basic processes;deep reinforcement learning method;important machine learning method;intelligent algorithm;intelligent optimization algorithm;learning process;planning process;swarm intelligence algorithms","","","","12","IEEE","28 Mar 2023","","","IEEE","IEEE Conferences"
"Classification of nutrient deficiency in rice based on CNN model with Reinforcement Learning augmentation","C. Wang; Y. Ye; Y. Tian; Z. Yu","Rose-Hulman Institute of Technology, Terre Haute, IN, USA; University of Hong Kong, Hong Kong SAR, China; Xi’an University of Technology, Xi’an City, China; Xi’an Jiaotong-liverpool University, Suzhou City, China","2021 International Symposium on Artificial Intelligence and its Application on Media (ISAIAM)","23 Aug 2021","2021","","","107","111","Rice is an important crop for agricultural production. Because of the vulnerability of rice, it is likely to be affected by many external conditions. Particularly, rice may lack of various kinds of nutrition elements such as potassium, nitrogen, and phosphorus. For people who are not experienced in botany, it is hard for them to identify nutrition deficiency and add corresponding supplement. Symptoms of nutrient deficiencies in rice plants often are usually represented by phenotype of the leaves. Therefore, the phenotype of leaves can be examined to identify nutrition deficiency. Our goal is to utilize deep learning, specifically, convolutional neural network (CNN), and combine reinforcement learning to help people to address this problem. Experiments were conducted with a dataset containing 1,500 images of rice leaves subject to three different types of nutrition deficiency—nitrogen (N), phosphorus (P), and potassium (K) deficiencies. This research preprocesses images first, constructs different CNN architectures, trains the model, and compares the results. With the reinforcement learning augmentation, experiments indicate that Densenet-121 is the best deep CNN model among these structures we had trained to identify what nutrition does a plant’s leave lack of with a test accuracy of 97%. This study demonstrates Densenet-121 model is a good tool to diagnose nutrient deficiency in crops and attempts to incorporate reinforcement learning to augment inputs.","","978-1-6654-3260-3","10.1109/ISAIAM53259.2021.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516522","machine learning;deep learning;convolutional neural network;computer vision;reinforcement learning;crops;phenotype","Training;Reinforcement learning;Tools;Phosphorus;Agriculture;Stability analysis;Convolutional neural networks","agricultural products;botany;convolutional neural nets;crops;deep learning (artificial intelligence);image classification;nitrogen","reinforcement learning augmentation;nutrition elements;phosphorus deficiencies;rice plants;leaf phenotype;deep learning;rice leaves;nutrition deficiency;potassium deficiencies;CNN architectures;deep CNN model;Densenet-121 model;rice nutrient deficiency classification;convolutional neural network;crop nutrient deficiency diagnosis;nitrogen deficiencies;image preprocessing;N;P;K","","2","","10","IEEE","23 Aug 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning with Imitation for Cavity Filter Tuning","S. Lindståh; X. Lan","Ericsson Research, Stock-holm, Sweden; Ericsson Research, Stock-holm, Sweden","2020 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)","5 Aug 2020","2020","","","1335","1340","Cavity filters are vital components of radio base stations and networks. After production, they need tuning, which has proven to be a difficult process to do manually and even more so to automate. Previous attempts to automate this process with Reinforcement Learning have failed to reach consistent performance on anything but the simplest filter models. In this paper, we build upon these results and aim to improve them. Multiple methods are tested and evaluated, including introducing a pre-processing step, tuning hyperparameters and dividing the problem into multiple sub-tasks. In particular, by using supervised Imitation Learning as an initial phase, a semi-realistic filter model with 13 tuning screws is tuned, fulfilling both insertion loss and return loss requirements. On this problem, this algorithm has a greater efficiency than any previously published results on Reinforcement Learning for Cavity filter tuning.","2159-6255","978-1-7281-6794-7","10.1109/AIM43001.2020.9158839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9158839","","Tuning;Fasteners;Cavity resonators;Learning (artificial intelligence);Integrated circuit modeling;Scattering parameters;Convergence","cavity resonator filters;comb filters;filters;learning (artificial intelligence);tuning","Reinforcement Learning;Cavity filter tuning;Cavity filters;vital components;radio base stations;automate;simplest filter models;tuning hyperparameters;supervised Imitation Learning;semirealistic filter model;13 tuning screws","","2","","12","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"A Biologically-Inspired Cognitive Agent Model Integrating Declarative Knowledge and Reinforcement Learning","A. -H. Tan; G. -W. Ng","Nanyang Technological University, Singapore; DSO National Laboratories, Singapore","2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology","1 Nov 2010","2010","2","","248","251","The paper proposes a biologically-inspired cognitive agent model, known as FALCON-X, based on an integration of the Adaptive Control of Thought (ACT-R) architecture and a class of self-organizing neural networks called fusion Adaptive Resonance Theory (fusion ART). By replacing the production system of ACT-R by a fusion ART model, FALCON-X integrates high-level deliberative cognitive behaviors and real-time learning abilities, based on biologically plausible neural pathways. We illustrate how FALCON-X, consisting of a core inference area interacting with the associated intentional, declarative, perceptual, motor and critic memory modules, can be used to build virtual robots for battles in a simulated RoboCode domain. The performance of FALCON-X demonstrates the efficacy of the hybrid approach.","","978-1-4244-8482-9","10.1109/WI-IAT.2010.210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5616152","Cognitive Agents;Knowledge Representation;Reinforcement Learning","","cognition;learning (artificial intelligence);self-organising feature maps;software agents","biologically-inspired cognitive agent model;declarative knowledge;reinforcement learning;FALCON-X agent model;adaptive control-of-thought architecture;self-organizing neural networks;fusion adaptive resonance theory;core inference area;virtual robots;RoboCode domain;fusion architecture for learning and cognition","","2","","9","IEEE","1 Nov 2010","","","IEEE","IEEE Conferences"
"FPGA-Based Accelerator for AI-Toolbox Reinforcement Learning Library","L. Leiva; J. Torrents-Barrena; M. Vázquez","LabSET-INTIA, Universidad Nacional del Centro de la Provincia de Buenos Aires, Tandil, Argentina; Large Format Printing, HP Inc., Barcelona, Spain; LabSET-INTIA, Universidad Nacional del Centro de la Provincia de Buenos Aires, Tandil, Argentina","IEEE Embedded Systems Letters","26 May 2023","2023","15","2","113","116","In reinforcement learning (RL) an agent interacts with the environment based on sequential decisions. This agent receives a reward from the environment according to decisions and tries to maximize the reward. RL is used in several domains, such as production, autonomous driving, business management, education, games, healthcare, natural language processing, robotics, and among others. RL methodologies require processing large volumes of data and computational power. To speed up these applications, field-programmable gate array (FPGA) are widely employed in the literature. This letter proposes an accelerator for the Markov decision process (MDP) implemented in the AI-Toolbox public library using high-level synthesis tools, using the tiger-antelope problem as use case. Our approach shows an acceleration greater than  $7\times $  compared to the original version.","1943-0671","","10.1109/LES.2022.3218168","Universidad Nacional del Centro de la Provincia de Buenos Aires and Universidad FASTA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9932676","Field-programmable gate array (FPGA);high-level synthesis (HLS);reinforcement learning (RL)","Libraries;Field programmable gate arrays;Coprocessors;Sparse matrices;Hardware;Q-learning;Optimization;Embedded systems;High level synthesis;Reinforcement learning","field programmable gate arrays;Markov processes;natural language processing;reinforcement learning","AI-Toolbox public library;AI-Toolbox reinforcement learning library;autonomous driving;business management;computational power;field-programmable gate array;FPGA-based accelerator;Markov decision process;natural language processing;RL methodologies;sequential decisions","","","","21","IEEE","31 Oct 2022","","","IEEE","IEEE Journals"
"Adaptive Client Selection in Resource Constrained Federated Learning Systems: A Deep Reinforcement Learning Approach","H. Zhang; Z. Xie; R. Zarei; T. Wu; K. Chen","Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; School of Information Technology, Deakin University, Melbourne, VIC, Australia; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Mechanical Engineering and Mechanics, Ningbo University, Ningbo, China","IEEE Access","15 Jul 2021","2021","9","","98423","98432","With data increasingly collected by end devices and the number of devices is growing rapidly in which data source mainly located outside the cloud today. To guarantee data privacy and remain data on client devices, federated learning (FL) has been proposed. In FL, end devices train a local model with their data and send the model parameters rather than raw data to server for aggregating a new global model. However, due to the limited wireless bandwidth and energy of mobile devices, it is not practical for FL to perform model updating and aggregation on all participating devices in parallel. And it is difficulty for FL server to select apposite clients to take part in model training which is important to save energy and reduce latency. In this paper, we establish a novel mobile edge computing (MEC) system for FL and propose an experience-driven control algorithm that adaptively chooses client devices to participate in each round of FL. Adaptive client selection mechanism in MEC can be modeled as a Markov Decision Process in which we do not need any prior knowledge of the environment. We then propose a client selection scheme based on reinforcement learning that learns to select a subset of devices in each communication round to minimize energy consumption and training delay that encourages the increase number of client devices to participate in model updating. The experimental results show that the unit of energy required in FL can be reduced by up to 50% and training delay required can be reduced by up to 20.70% compared to the other static algorithms. Finally, we demonstrate the scalability of MEC system with different tasks and the influence of different non independent and identically distributed (non-IID) settings.","2169-3536","","10.1109/ACCESS.2021.3095915","Nature Science Foundation of China(grant numbers:U20A20121); Ningbo International Science and Technology Cooperation Program(grant numbers:2016D10008); Ningbo Key Science and Technology Plan (2025) Project(grant numbers:2019B10125,2019B10028,20201ZDYF020077); Special Research funding from the Marine Biotechnology and Marine Engineering Discipline Group, Ningbo University(grant numbers:422004582); Project of Research and Development of Intelligent Resource Allocation and Sharing Platform for Marine Electronic Information Industry(grant numbers:2017GY116); Key Science and Technology Projects of Zhejiang Province(grant numbers:2020C03064); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9478892","Client selection;federated learning;mobile edge computing;reinforcement learning","Computational modeling;Training;Data models;Servers;Adaptation models;Data privacy;Collaborative work","data privacy;learning (artificial intelligence);Markov processes;mobile computing;mobile radio","resource constrained federated learning systems;data source;data privacy;model parameters;raw data;global model;mobile devices;model updating;participating devices;FL server;apposite clients;model training;mobile edge computing system;adaptively chooses client devices;adaptive client selection mechanism;client selection scheme;reinforcement learning;energy consumption;training delay;MEC system","","17","","28","CCBY","9 Jul 2021","","","IEEE","IEEE Journals"
"A Study of Aero-Engine Control Method Based on Deep Reinforcement Learning","Q. Zheng; C. Jin; Z. Hu; H. Zhang","Jiangsu Province Key Laboratory of Aerospace Power System, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Jiangsu Province Key Laboratory of Aerospace Power System, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Jiangsu Province Key Laboratory of Aerospace Power System, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Jiangsu Province Key Laboratory of Aerospace Power System, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Access","6 May 2019","2019","7","","55285","55289","A novel aero-engine control method based on deep reinforcement learning (DRL) is proposed to improve the engine response ability. The Q-learning that is model free and can be performed online is adopted. For improving the learning capacity of DRL, the online sliding window deep neural network (OL-SW-DNN) is proposed and adopted to estimate the action value function. The OL-SW-DNN selects the nearest point data with certain length as training data and is insensitivity to the noise. Finally, the comparison simulations of the proposed method with the proportion-integration-differentiation (PID) that is the most commonly used as an engine controller algorithm in industry are conducted to verify the validity of the proposed method. The results show that, compared with the PID, the acceleration time of the proposed method decreased by 1.525 s under the premise of satisfying all engine limits.","2169-3536","","10.1109/ACCESS.2018.2883997","National Natural Science Foundation of China(grant numbers:51576096); Q. Lan and the 333 Project; Research Funds for Central Universities(grant numbers:NF2018003); Six Talents Peak Project of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704271","Aero-engine control method;response ability;deep reinforcement learning;on line;deep neural network","Engines;Artificial neural networks;Acceleration;Reinforcement learning;Control systems;Fuels","aerospace engines;aircraft control;learning (artificial intelligence);neurocontrollers;three-term control","deep reinforcement learning;DRL;engine response ability;Q-learning;online sliding window deep neural network;OL-SW-DNN;action value function;nearest point data;engine controller algorithm;aero-engine control method;proportion-integration-differentiation","","16","","19","OAPA","1 May 2019","","","IEEE","IEEE Journals"
"Resource Allocation in Vehicular Communications Using Graph and Deep Reinforcement Learning","S. Gyawali; Y. Qian; R. Q. Hu","Department of Electrical and Computer Engineering, University of Nebraska-Lincoln, Omaha, NE, USA; Department of Electrical and Computer Engineering, University of Nebraska-Lincoln, Omaha, NE, USA; Department of Electrical and Computer Engineering, Utah State University, Logan, UT, USA","2019 IEEE Global Communications Conference (GLOBECOM)","27 Feb 2020","2019","","","1","6","Cellular based vehicle-to-everything (V2X) communications have recently gained more interest from both academia and industry. However, there exist many challenges in cellular-based V2X communications in which resource allocation is one of the main challenges. In this paper, we propose a graph and deep reinforcement learning-based resource allocations in which channels for vehicular communications are assigned in a centralized manner by the base station whereas vehicular user equipment uses deep reinforcement learning for distributed power control. Graph-based channel allocation includes a weighted bipartite matching and clustering scheme and relies on strictly limited channel state information (CSI). Whereas, power selection is performed using deep reinforcement learning where each agent selects the transmission power to maximize the aggregated V2V data rate. Our proposed scheme relies on realistic channel assumption with minimum transmission overhead. In addition, we have also performed simulations and have shown that our scheme is better compared to previous schemes in terms of sum V2V and sum V2I capacity.","2576-6813","978-1-7281-0962-6","10.1109/GLOBECOM38437.2019.9013594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9013594","","Resource management;Channel allocation;Base stations;Machine learning;Clustering algorithms;Partitioning algorithms;Power control","cellular radio;channel allocation;cooperative communication;graph theory;learning (artificial intelligence);mobile radio;power control;resource allocation;vehicular ad hoc networks;wireless channels","vehicular communications;deep reinforcement learning;Cellular based vehicle-to-everything communications;resource allocation;base station;vehicular user equipment;graph-based channel allocation","","15","","15","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Traffic Optimization in Satellites Communications: A Multi-agent Reinforcement Learning Approach","Z. Qin; H. Yao; T. Mai","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China","2020 International Wireless Communications and Mobile Computing (IWCMC)","27 Jul 2020","2020","","","269","273","Past few years have witnessed the compelling applications of the satellite communications and networking in our daily life. Due to the extremely high moving speeds and limited networking resources of LEO satellites, how to optimize inter-satellite traffic has received amount of attention from both academia and industry. In this paper, we proposed a hybrid satellites network traffic control paradigm. In our architecture, the centralized platform collect the global state and the joint action from each agent during the training phase to ease the training, and during execution, the each agent can return the action to the local state through the trained policy. Besides, we adopt a multiagent actor-critic algorithms named Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments(MADDPG) to our architecture. In addition, some simulation results are presented to evaluate the correctness of our architecture and algorithm.","2376-6506","978-1-7281-3129-0","10.1109/IWCMC48107.2020.9148523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9148523","Satellites Communications;Multi-agent Reinforcement Learning;Traffic Optimization","Satellites;Training;Learning (artificial intelligence);Topology;Satellite communication;Low earth orbit satellites;Optimization","cooperative communication;learning (artificial intelligence);multi-agent systems;satellite communication;telecommunication computing;telecommunication congestion control;telecommunication traffic","mixed cooperative-competitive environments;multiagent actor-critic;hybrid satellites network traffic control paradigm;inter-satellite traffic;LEO satellites;limited networking resources;satellite communications;multiagent reinforcement learning;traffic optimization;multiagent actor-critic algorithms;trained policy","","8","","16","IEEE","27 Jul 2020","","","IEEE","IEEE Conferences"
"Reinforcement learning in the real world","A. G. Barto","Department of Computer Science, University of Massachusetts, Amherst, Canada","2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)","17 Jan 2005","2004","3","","1661 vol.3","","Summary form only given. Reinforcement learning refers to improving performance through trial-and-error experience. Although, modern computational approaches to reinforcement learning were inspired by animal learning, they have now branched out in a very several different directions. Some researchers are interested in finding high-quality approximate solutions to large-scale stochastic planning problems that are important for industry and government. Others are pursuing the goal of building intelligent, resourceful autonomous agents that, like animals, can succeed while acting in real-time in complex environments. While these goals have much in common - and they both involve the real world - they represent two very different perspectives on reinforcement learning and related methods. After first reviewing the major elements of modern reinforcement learning and its relationship to optimal control, other types of machine learning, and to neuroscience, I present several striking example applications and describe some of the latest research directed toward scaling up to ever more complex problems. I conclude by laying out a view of the future of reinforcement learning research, emphasizing that the issues that need to be addressed depend strongly on which type of reinforcement learning one has in mind.","1098-7576","0-7803-8359-1","10.1109/IJCNN.2004.1380847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1380847","","Learning;Animals;Large-scale systems;Stochastic processes;Government;Computational intelligence;Intelligent agent;Intelligent structures;Autonomous agents;Optimal control","learning (artificial intelligence);stochastic processes;optimal control","reinforcement learning;animal learning;stochastic planning problems;resourceful autonomous agents;optimal control;machine learning;neuroscience","","7","","","IEEE","17 Jan 2005","","","IEEE","IEEE Conferences"
"Proximal Policy Based Deep Reinforcement Learning Approach for Swarm Robots","Z. Tan; M. Karaköse","Erzincan Binali Yıldırım University, Erzincan, Turkey; Department of Computer Engineering, Firat University, Elazığ, Turkey","2021 Zooming Innovation in Consumer Technologies Conference (ZINC)","2 Aug 2021","2021","","","166","170","Artificial intelligence technology is becoming more active in all areas of our lives day by day. This technology affects our daily life by more developing in areas such as industry 4.0, security and education. Deep reinforcement learning is one of the most developed algorithms in the field of artificial intelligence. In this study, it is aimed that three different robots in a limited area learn to move without hitting each other, fixed obstacles and the boundaries of the field. These robots have been trained using the deep reinforcement learning approach and Proximal policy optimization (PPO) policy. Instead of uses value-based methods with the discrete action space, PPO that can easily manipulate the continuous action field and successfully determine the action of the robots has been proposed. PPO policy achieves successful results in multi-agent problems, especially with the use of the Actor-Critic network. In addition, information is given about environment control and learning approaches for swarm behavior. We propose parameter sharing and behavior-based method for this study. Finally, trained model is recorded and tested in 9 different environments where the obstacles are located differently. With our method, robots can perform their tasks in closed environments in the real world without damaging anyone or anything.","","978-1-6654-0417-4","10.1109/ZINC52049.2021.9499288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499288","deep reinforcement learning;swarm behavior;deep learning","Training;Technological innovation;Service robots;Swarm robotics;Reinforcement learning;Task analysis;Artificial intelligence","control engineering computing;deep learning (artificial intelligence);multi-agent systems;multi-robot systems;optimisation;reinforcement learning;robot programming","swarm robots;artificial intelligence technology;value-based methods;discrete action space;PPO policy;parameter sharing;behavior-based method;deep reinforcement learning;proximal policy optimization policy;swarm behavior;actor-critic network","","5","","20","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"Control of Single-Segment Continuum Robots: Reinforcement Learning vs. Neural Network based PID","S. Chattopadhyay; S. Bhattacherjee; S. Bandyopadhyay; A. Sengupta; S. Bhaumik","Electrical Engineering Department, IIEST Shibpur; IIEST Shibpur, School of Mechatronics and Robotics; Electrical Engineering Department, IIEST Shibpur; Electrical Engineering Department, IIEST Shibpur; IIEST Shibpur, School of Mechatronics and Robotics","2018 International Conference on Control, Power, Communication and Computing Technologies (ICCPCCT)","13 Dec 2018","2018","","","222","226","Continuum robots have been very popular in the recent days due to their wide spread applications in space, defence, medical, underwater, industries etc. Modelling of these types of robots is difficult due to their highly nonlinear dynamic characteristic which necessitates the need for model-less intelligent control. In this paper two intelligent model-less adaptive methods,Reinforcement Learning (RL) and Artificial Neural Network based proportional integral derivative ANN-PID control have been applied on a hardware continuum robot. Here the RL technique involves a continuous state discrete action Q learning method and the ANN-PID is implemented by a single neuron. Performance of both the methods are compared by implementing them on a hardware robot.","","978-1-5386-0796-1","10.1109/ICCPCCT.2018.8574225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574225","ANN-PID;reinforcement learning;continuum robot;machine intelligence","Hardware;Robot kinematics;Biological neural networks;Tendons;Adaptation models;Artificial neural networks","learning (artificial intelligence);mobile robots;neurocontrollers;three-term control","hardware continuum robot;RL technique;adaptive methods;ANN-PID;Q learning method;proportional integral derivative control;artificial neural network;intelligent control;nonlinear dynamic characteristic;reinforcement learning","","4","","18","IEEE","13 Dec 2018","","","IEEE","IEEE Conferences"
"REINDEAR : REINforcement learning agent for Dynamic system control in Edge-Assisted Augmented Reality service","K. Lee; C. -H. Youn","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea","2020 International Conference on Information and Communication Technology Convergence (ICTC)","21 Dec 2020","2020","","","949","954","Nowadays the industry of the Augmented/Virtual Reality is rapidly growing in various domains such as AI assistance or gaming. While the main goal of the AR service itself is to provide the user the visualized computing experience in their life, the systematic requirements include the real-time performance since the service should be reacting to the user's state which changes dynamically over time. This kind of real-time response is often very hard to achieve on typical edge devices such as mobile phone or AR goggles due to lack of computational power. To overcome this issue, many of the researches suggest server offloading technique which can provide sufficient amount of computational power in exchange of the transmission overhead. The tradeoff relationship between the computational power and transmission overhead makes the control of the offloading procedure important for the overall service quality. In this paper we propose an RL based server-client controlling scheme REINDEAR. REINDEAR is a system that conducts class-wise characteristic analysis from the experience, so that it could control the AR service tradeoff quality adaptively. From the result of the experiments, we showed that our REINDEAR system learns the underlying behavioural patterns of video objects and provides controls that suits each pattern.","2162-1233","978-1-7281-6758-9","10.1109/ICTC49870.2020.9289225","National Research Council of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9289225","Context awareness;Intelligent systems;Edge computing;Object detection;Augmented Reality","Training;Adaptation models;Analytical models;Visualization;Reinforcement learning;Real-time systems;Trajectory","augmented reality;client-server systems;learning (artificial intelligence);mobile computing;virtual reality","real-time response;typical edge devices;mobile phone;computational power;server offloading technique;transmission overhead;offloading procedure;service quality;RL based server-client;AR service;REINDEAR system;REINforcement learning agent;dynamic system control;edge-assisted Augmented Reality service;visualized computing experience;systematic requirements;real-time performance","","2","","10","IEEE","21 Dec 2020","","","IEEE","IEEE Conferences"
"Joint Optimization of Resource Allocation and Service Performance in vEPC Using Reinforcement Learning","G. Yunjie; H. Yuxiang; D. Yuehang; X. Jichao","National Digital Switching System Engineering and Technological R&D Center, Zheng Zhou, China; National Digital Switching System Engineering and Technological R&D Center, Zheng Zhou, China; National Digital Switching System Engineering and Technological R&D Center, Zheng Zhou, China; National Digital Switching System Engineering and Technological R&D Center, Zheng Zhou, China","2019 IEEE 4th International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)","30 May 2019","2019","","","306","310","Network Function Virtualization (NFV) is the transition from proprietary hardware functions to virtualized counterparts of them within the telecommunication industry. With the aim of quality of service guarantee and energy saving, telco operates need to decided when and how to scale the virtual resource with the traffic processing demand. In this paper, we proposed an auto-scaling mechanism based on reinforcement learning. First, we establish a system model for vEPC (virtualized Evolved Packed Core) to gather the state information. Second, auto-scaling mechanism based on reinforcement learning can treat procedure of scaling decision as Markov Decision Process. By simulation, our mechanism outperforms threshold based policy, and realizes the joint optimization of resource allocation and service performance in vEPC.","","978-1-7281-1410-1","10.1109/ICCCBDA.2019.8725620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8725620","network function virtualization;reinforcement learning;auto scaling;resource allocation","Reinforcement learning;Resource management;Time factors;Optimization;Load modeling;Switching systems;Quality of service","computer networks;learning (artificial intelligence);Markov processes;optimisation;quality of service;resource allocation;virtualisation","resource allocation;service performance;vEPC;reinforcement learning;proprietary hardware functions;auto-scaling mechanism;network function virtualization;quality of service guarantee;energy saving;virtualized evolved packed core;Markov decision process","","2","","15","IEEE","30 May 2019","","","IEEE","IEEE Conferences"
"Reinforcement-Learning Based Threshold Policies for Continuous Intraday Electricity Market Trading","G. Bertrand; A. Papavasiliou","CORE, Université catholique de Louvain; CORE, Université catholique de Louvain","2019 IEEE Power & Energy Society General Meeting (PESGM)","30 Jan 2020","2019","","","1","5","Continuous intraday electricity market h as become increasingly important in recent years, due to the increasing integration of renewable resources in power systems. Trading in this market is challenging due to the multistage nature of the problem, its high uncertainty, and the fact that decisions need to be made rapidly in order to lock in profitable trades. We cast the problem of trading in continuous intraday markets as a reinforcement learning problem, and tackle the problem using policy function approximation. We specifically parametrize the trading policy using price thresholds, and optimize the choice of these thresholds using the REINFORCE algorithm. We demonstrate the effectiveness of our proposed policy by showing that it outperforms the method, classically used in the industry, rolling intrinsic of 4.2% (out of sample) on the 165 last days of 2015 in the German continuous intraday market.","1944-9933","978-1-7281-1981-6","10.1109/PESGM40551.2019.8973602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8973602","","","function approximation;learning (artificial intelligence);power engineering computing;power generation economics;power markets;pricing;profitability;renewable energy sources","renewable resources;power systems;profitable trades;policy function approximation;trading policy;REINFORCE algorithm;German continuous intraday market;reinforcement-learning based threshold policies","","2","","17","IEEE","30 Jan 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Adaptive Controller of DC Electric Drive for Reduced Torque and Current Ripples","R. Anugula; S. P. Krishna Karri","Depart ment of Electrical Engineering, NIT Andhra Pradesh, Tadepalligudem, India; Depart ment of Electrical Engineering, NIT Andhra Pradesh, Tadepalligudem, India","2021 IEEE International Conference on Technology, Research, and Innovation for Betterment of Society (TRIBES)","11 Apr 2022","2021","","","1","6","Artificial Intelligence and Machine Learning-based intelligent control algorithms are replacing traditional control algorithms due to their adopting and self-learning capabilities. The DC-DC Buck converter fed DC motor has a wide variety of applications from household appliances to industry level. Conventionally, the proportional-integral controller is widely used to optimally control the speed of DC-DC Buck converter fed DC Motor but the performance deteriorates for parameters changes which lead to increase ripples in torque and current. The demanding task is to control the electric drives for mitigated ripple content in speed operation. This work proposes a new adaptive controller based on deep reinforcement learning (DRL) to mitigate those problems. The simulation results are illustrating that the Deep Deterministic Policy Gradient (DDPG) algorithm performs superior to the PI controller under parameter changes and also proving that the DDPG making the system stable where the PI fails to stabilize it for a wider range of parameter changes.","","978-1-6654-3342-6","10.1109/TRIBES52498.2021.9751630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751630","DC-DC Buck Converter;DC Motor;Deep Reinforcement Learning;Deep Deterministic Policy Gradient controller","Training;Technological innovation;Buck converters;Torque;PI control;Machine learning algorithms;Simulation","adaptive control;angular velocity control;control engineering computing;DC motor drives;DC motors;DC-DC power convertors;deep learning (artificial intelligence);electric current control;intelligent control;machine control;optimal control;power engineering computing;reinforcement learning;torque control","deep reinforcement learning;adaptive controller;DC electric drive;current ripples;DC-DC buck converter;DC motor;torque ripples;DRL;speed control;intelligent control","","1","","13","IEEE","11 Apr 2022","","","IEEE","IEEE Conferences"
"Hypergraph-Based Reinforcement Learning for Stock Portfolio Selection","X. Li; C. Cui; D. Cao; J. Du; C. Zhang","School of Computer Science and Technology, Shandong University of Finance and Economics; School of Computer Science and Technology, Shandong University of Finance and Economics; School of Computer Science and Technology, Shandong University of Finance and Economics; School of Computer Science and Technology, Shandong University of Finance and Economics; School of Computer Science and Technology, Shandong University of Finance and Economics","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","4028","4032","Stock portfolio selection is an important financial planning task that dynamically re-allocates the investments to stock assets to achieve the goals such as maximal profits and minimal risks. In this paper, we propose a hypergraph-based reinforcement learning method for stock portfolio selection, in which the fundamental issue is to learn a policy function generating appropriate trading actions given the current environments. The historical time-series patterns of stocks are firstly captured. Then, different from prior works ignoring or implicitly modeling stock pairwise correlations, we present a HyperGraph Attention Module (HGAM) in the portfolio policy learning, which utilizes the hypergraph structure to explicitly model the group-wise industry-belonging relationships among stocks. The attention mechanism is also introduced in HGAM that quantifies the importance of different neighbors regarding the target node to aggregate the information on the stock hypergraph adaptively. Extensive experiments on the real-world dataset collected from China’s A-share market demonstrate the significant superiority of our method, compared with state-of-the-art methods in portfolio selection, including both online learning-based methods and reinforcement learning-based methods. The data and codes of our work have been released at https://github.com/lixiaojieff/stock-portfolio.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9747138","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9747138","Stock portfolio selection;reinforcement learning;portfolio policy;hypergraph attention networks","Learning systems;Pairwise error probability;Costs;Conferences;Reinforcement learning;Signal processing;Task analysis","financial management;investment;learning (artificial intelligence);stock markets;time series","stock portfolio selection;important financial planning task;stock assets;hypergraph-based reinforcement learning method;implicitly modeling stock pairwise correlations;HyperGraph Attention Module;portfolio policy learning;hypergraph structure;stock hypergraph;online learning-based methods;reinforcement learning-based methods","","1","","23","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Reinforcement learning based scheduling in semiconductor final testing","Z. Zhang","Department of Industrial Engineering, Dongguan University of Technology, Dongguan, China","2010 IEEE International Conference on Industrial Engineering and Engineering Management","23 Dec 2010","2010","","","1693","1697","Semiconductor test scheduling problem is a variation of reentrant unrelated parallel machine problem considering multiple resources constraints, intricate {product, tester, kit, component} eligibility constraints, and sequence-dependant setup times, etc. A multi-step reinforcement learning (RL) algorithm called Sarsa(λ,k) is proposed and applied to deal with it. Allowing enabler reconfiguration, the capacity of the test facility is expanded and scheduling optimization is performed at the component level. In order to apply Sarsa(λ,k), the scheduling problem is transformed into an RL problem by defining state representation, constructing actions and the reward function. Experiments show that Sarsa(λ,k) outperforms the scheduling method in industry and validate the effectiveness of Sarsa(λ,k) to solve the scheduling problem.","2157-362X","978-1-4244-8503-1","10.1109/IEEM.2010.5674587","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674587","Semiconductor test;Scheduling;Reinforcement learning;Resource constraint","Argon","learning (artificial intelligence);semiconductor device testing","reinforcement learning based scheduling;semiconductor final testing;state representation;Sarsa(λ,k);resource constraint","","1","","10","IEEE","23 Dec 2010","","","IEEE","IEEE Conferences"
"Low-latency Patient Monitoring Service for Cloud Computing Based Healthcare System by Applying Reinforcement Learning","H. Xu; L. Zuo; F. Sun; M. Yang; N. Liu","University of Electronic Science and Technology of China, ChengDu, China; University of Electronic Science and Technology of China, ChengDu, China; University of Electronic Science and Technology of China, ChengDu, China; University of Electronic Science and Technology of China, ChengDu, China; University of Electronic Science and Technology of China, ChengDu, China","2022 IEEE 8th International Conference on Computer and Communications (ICCC)","20 Mar 2023","2022","","","1373","1377","Traditional healthcare systems are difficult to provide low-latency patient monitoring services for multiple patients. However, with the widespread application of novel technologies such as cloud computing, Internet of things(loT), 5G wireless communication, health industry is gradually walking into the era of smart and precise medical services with powerful healthcare cloud. Specifically, this kind of promising healthcare system which is based on cloud computing and loT, makes low-latency and multiple patients monitoring service possible, which signif- icantly improves patient's medical experience and doctor's fast access to patient's real-time data of vital signs for later precise diagnosis and analysis. Nonetheless, how to satisfy the low latency requirement while notably reducing the use of communication resource and computation resource of healthcare private cloud that has expensive cost is still a challenging problem. In this paper, we apply reinforcement learning(RL) algorithm which will learn a policy to automatically adjust transmission rate of monitoring services and computation resources of cloud servers, in conjunction with effective two-stage tandem queue system to tackle the above problem. Finally, we conduct extensive experiments to verify the effectiveness and efficiency of our proposed method in cloud computing based healthcare system.","","978-1-6654-5051-5","10.1109/ICCC56324.2022.10065744","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10065744","remote patient monitoring;healthcare system;cloud computing;internet of things;reinforcement learning;resource allocation","Wireless communication;Cloud computing;Patient monitoring;Computational modeling;Medical services;Reinforcement learning;Real-time systems","5G mobile communication;cloud computing;health care;Internet of Things;learning (artificial intelligence);patient monitoring;reinforcement learning","cloud computing;cloud servers;communication resource;computation resource;computation resources;healthcare private cloud;low latency requirement;low-latency patient monitoring service;monitoring services;multiple patients;powerful healthcare cloud;precise medical services;promising healthcare system;service possible;smart services;traditional healthcare systems","","","","20","IEEE","20 Mar 2023","","","IEEE","IEEE Conferences"
"AutoEncoder-based Safe Reinforcement Learning for Power Augmentation in a Lower-limb Exoskeleton","M. Abbasi; M. Karami; A. Koushki; G. Vossoughi","Department of Mechanical Engineering, Sharif University of Technology, Tehran, Iran; Department of Mechanical Engineering, Sharif University of Technology, Tehran, Iran; Department of Mechanical Engineering, Sharif University of Technology, Tehran, Iran; Department of Mechanical Engineering, Sharif University of Technology, Tehran, Iran","2021 9th RSI International Conference on Robotics and Mechatronics (ICRoM)","7 Jan 2022","2021","","","138","143","Power augmentation in wearable robots has recently attracted the attention of researchers in industry and academia. Human-robot synchronized control is one of the most substantial goals that needs to be achieved. The change of gait patterns between different users and cycles, and the close interaction of human and robot necessitates the estimation of human motion intention in real-time. Reinforcement learning (RL) is beneficial in such situations mainly due to its online and model-free nature. However, RL algorithms struggle in high-dimensional continuous search spaces in terms of convergence speed and safety. Therefore, this paper proposes an RL-based control strategy for power augmentation in a lower-limb exoskeleton to extract the human motion and minimize the interaction force. A deep AutoEncoder (AE) neural network is trained and employed for dimensionality reduction to improve the convergence speed and safety. Simulation and two experiments were carried out with a lower-limb exoskeleton on a healthy participant. The results indicate the efficacy of the proposed control strategy and its superior performance compared to model-based approaches while improving speed and safety.","2572-6889","978-1-6654-2094-5","10.1109/ICRoM54204.2021.9663471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9663471","Reinforcement Learning;Deep Autoencoder;Power Augmentation;Interaction Force;Lower-limb Exoskeleton","Mechatronics;Service robots;Exoskeletons;Neural networks;Reinforcement learning;Wearable robots;Real-time systems","humanoid robots;human-robot interaction;legged locomotion;motion control;neural nets;reinforcement learning;safety;wearable robots","convergence speed;safety;control strategy;power augmentation;lower-limb exoskeleton;deep AutoEncoder neural network;AutoEncoder-based safe reinforcement learning;wearable robots;human-robot synchronized control;substantial goals;gait patterns;cycles;human robot;human motion intention;model-free nature;RL algorithms;high-dimensional continuous search","","","","13","IEEE","7 Jan 2022","","","IEEE","IEEE Conferences"
"RLayout: Interior Design System Based on Reinforcement Learning","N. Wang; C. Niu; Z. Li","College of Electronics and Information Engineering, Tongji University, Shanghai, China; College of Architecture and Urban Planning, Tongji University, Shanghai, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China","2019 12th International Symposium on Computational Intelligence and Design (ISCID)","22 May 2020","2019","1","","117","120","The advanced artificial intelligence technologies have promoted the development of other industries. As a combination of AI technologies and art, intelligent design has become a research hotspot in recent years. Intelligent design uses computer to simulate human thinking activities with AI technologies, enabling the computers to undertake more complex design tasks in a better way. At the same time, reinforcement learning plays an increasingly important role in many fields, such as system control, game theory, computer network, decision making, etc. In this work, an intelligent design system based on the reinforcement learning algorithm was proposed, and corresponding reward rules were developed according to the design principles. The system can help the designer to generate the optimal scheme automatically, which can be extended and applied to find the best scheme in many problems, including interior design, daily schedule, resource arrangement, etc. Our demo shows that the system can offer excellent design scheme according to pre-set reward rules, and reduce workload of related designers.","2473-3547","978-1-7281-4653-9","10.1109/ISCID.2019.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098288","reinforcement learning;intelligent design;interior design","Learning (artificial intelligence);Layout;Machine learning;Training;Intelligent systems","computer networks;decision making;game theory;learning (artificial intelligence);telecommunication scheduling","RLayout;interior design system;artificial intelligence technologies;AI technologies;human thinking activities;complex design tasks;system control;game theory;computer network;intelligent design system;reinforcement learning algorithm;design principles;reward rules","","","","8","IEEE","22 May 2020","","","IEEE","IEEE Conferences"
"Reinforcement-Learning-Based Solutions to Power Issues in Wireless IoT System","X. He","International school, Beijing University of Posts and Telecommunications, Beijing, China","2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)","23 Apr 2021","2020","","","174","178","With the combination of Internet and Things, people's life become more efficient and convenient, which leads to the explosive growth of data and the consumption of resources as well. Hence, in an IoT (Internet of Things) system, it is an emergent problem to optimize the strategy of resource allocation, which will help to maximize the resources utilization. In the mean time, it is becoming more and more difficult to deal with the skyrocketing data by using traditional routing protocols. In recent years, deep learning also develops dramatically, which is applied in many industries and fields. This paper concentrates on summarizing reinforcement-learning-based existing solutions and proposing a comprehensive solution, which optimizes the traditional structure and strategy of wireless internet of things system.","","978-1-7281-9619-0","10.1109/ICBASE51474.2020.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403837","reinforcement learning;internet of things;Q learning;task schedule","Wireless communication;Schedules;Job shop scheduling;Power demand;Routing;Routing protocols;Internet of Things","Internet;Internet of Things;learning (artificial intelligence);resource allocation;routing protocols","resources utilization;skyrocketing data;traditional routing protocols;deep learning;reinforcement-learning-based existing solutions;comprehensive solution;traditional structure;wireless internet;reinforcement-learning-based solutions;power issues;wireless IoT system;people;Internet of Things;emergent problem;resource allocation","","","","10","IEEE","23 Apr 2021","","","IEEE","IEEE Conferences"
"Information Gain Regulation In Reinforcement Learning With The Digital Twins’ Level of Realism","G. Szabó; J. Pető; L. Németh; A. Vidács","Ericsson Research, Budapest, Hungary; HSN Lab, Budapest University of Technology and Economics, Hungary; HSN Lab, Budapest University of Technology and Economics, Hungary; HSN Lab, Budapest University of Technology and Economics, Hungary","2020 IEEE 31st Annual International Symposium on Personal, Indoor and Mobile Radio Communications","8 Oct 2020","2020","","","1","7","Digital Twin (DT) is widely used in various industrial sectors to optimize the operations and maintenance of physical assets, system and manufacturing processes. In this paper our goal is to introduce an architecture in which the radio access control happens automatically to minimize the utilized radio resources while still maximizing the production KPIs of the robot cell. To achieve this, we apply Reinforcement Learning (RL) in a simulated environment to explore the environment fast, while the DT ensures that the learned policy can be applied on the real world environment as well. We show that the application of Ultra Reliable Low Latency Communication (URLLC) connection can be reduced to approx. 30% of the total radio time while achieving real-world accurate robot control. The system in action can be seen on [1].","2166-9589","978-1-7281-4490-0","10.1109/PIMRC48278.2020.9217201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9217201","digital twin;network effect;machine learning","5G mobile communication;Service robots;Real-time systems;Manipulators;Hardware;Switches","cellular radio;learning (artificial intelligence);mobile robots;radio access networks;telecommunication computing;telecommunication control;telecommunication network reliability","robot cell;reinforcement learning;real-world accurate robot control;information gain regulation;digital twins;industrial sectors;physical assets;manufacturing processes;radio access control;radio resources;production KPI;ultra reliable low latency communication connection;URLLC connection","","1","","26","IEEE","8 Oct 2020","","","IEEE","IEEE Conferences"
" $ {H}_{ {\infty }}$  Tracking Control of Completely Unknown Continuous-Time Systems via Off-Policy Reinforcement Learning","H. Modares; F. L. Lewis; Z. -P. Jiang","University of Texas at Arlington Research Institute, Fort Worth, TX, USA; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; Department of Electrical and Computer Engineering with the Polytechnic School of Engineering, New York University, NY, USA","IEEE Transactions on Neural Networks and Learning Systems","20 May 2017","2015","26","10","2550","2562","This paper deals with the design of an H∞ tracking controller for nonlinear continuous-time systems with completely unknown dynamics. A general bounded L2-gain tracking problem with a discounted performance function is introduced for the H∞ tracking. A tracking Hamilton-Jacobi-Isaac (HJI) equation is then developed that gives a Nash equilibrium solution to the associated min-max optimization problem. A rigorous analysis of bounded L2-gain and stability of the control solution obtained by solving the tracking HJI equation is provided. An upper-bound is found for the discount factor to assure local asymptotic stability of the tracking error dynamics. An off-policy reinforcement learning algorithm is used to learn the solution to the tracking HJI equation online without requiring any knowledge of the system dynamics. Convergence of the proposed algorithm to the solution to the tracking HJI equation is shown. Simulation examples are provided to verify the effectiveness of the proposed method.","2162-2388","","10.1109/TNNLS.2015.2441749","National Science Foundation (NSF)(grant numbers:ECCS-1405173,IIS-1208623); Office of Naval Research, Arlington, VA, USA(grant numbers:N00014-13-1-0562,N000141410718); U.S. Army Research Office(grant numbers:W911NF-11-D-0001); NSF(grant numbers:ECCS-1101401,ECCS-1230040); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7132753","Bounded L₂-gain;H∞ tracking controller;reinforcement learning (RL);tracking Hamilton-Jacobi-Isaac (HJI) equation.;Bounded $L_{2}$ -gain;$H_{\infty }$ tracking controller;reinforcement learning (RL);tracking Hamilton–Jacobi–Isaac (HJI) equation","Attenuation;Mathematical model;Asymptotic stability;Heuristic algorithms;Feedforward neural networks;Optimal control;Trajectory","asymptotic stability;continuous time systems;control system synthesis;game theory;H∞ control;learning (artificial intelligence);nonlinear control systems;tracking","H∞ tracking controller design;completely unknown continuous-time systems;off-policy reinforcement learning algorithm;nonlinear continuous-time systems;completely unknown dynamics;general bounded L2-gain tracking problem;discounted performance function;tracking Hamilton-Jacobi-Isaac equation;Nash equilibrium solution;local asymptotic stability;tracking error dynamics","","342","","53","IEEE","24 Jun 2015","","","IEEE","IEEE Journals"
"Event-Driven Guaranteed Cost Control Design for Nonlinear Systems With Actuator Faults via Reinforcement Learning Algorithm","H. Zhang; Y. Liang; H. Su; C. Liu","State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; School of Information Science and Engineering, Northeastern University, Shenyang, China; School of Information Science and Engineering, Northeastern University, Shenyang, China; School of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","14 Oct 2020","2020","50","11","4135","4150","This article presents a novel event-driven guaranteed cost control method for nonlinear systems subject to actuator faults. For the purpose of handling the problem of actuator faults and obtaining the event-driven approximate optimal guaranteed cost control approach for general nonlinear dynamics, the reinforcement learning (RL) algorithm is utilized to develop a sliding-mode control (SMC) strategy. To begin with, the unknown faults can be estimated by designing a fault observer. Meanwhile, an SMC technique is presented aiming at countering the effect of abrupt faults. In addition, the optimal performance of the equivalent sliding mode dynamics is considered, then an event-driven guaranteed cost control mechanism is implemented by using RL principle. In the control process, a general cost function, which has a simpler structure, is given to reduce the computation complexity. At the same time, a modified cost function is approximated to obtain optimal guaranteed cost control by using a single critic neural network (NN). In addition, a modified weight update law for critic NN is presented to relax the persistence of excitation (PE) condition. Moreover, a newly triggering condition, which is easy to be implemented, is designed, and the critic NN update law makes sure that the system states are stable. Furthermore, in light of the Lyapunov analysis, it is demonstrated that the developed event-driven control method guarantees the uniformly ultimately bounded (UUB) property of all the signals. Finally, three simulation results are given to validate the designed control method.","2168-2232","","10.1109/TSMC.2019.2946857","National Natural Science Foundation of China(grant numbers:61433004,61627809,61621004); Liaoning Revitalization Talents Program(grant numbers:XLYC1801005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8882387","Event-driven control;fault tolerant control;guaranteed cost control;reinforcement learning (RL);sliding mode control (SMC)","Actuators;Nonlinear systems;Heuristic algorithms;Approximation algorithms;Cost function;Process control","approximation theory;computational complexity;control system synthesis;fault diagnosis;learning (artificial intelligence);Lyapunov methods;neurocontrollers;nonlinear control systems;observers;optimal control;robust control;uncertain systems;variable structure systems","computation complexity;SMC strategy;UUB property;uniformly ultimately bounded property;Lyapunov analysis;critic neural network;event-driven approximate optimal guaranteed cost control;persistence of excitation;event-driven guaranteed cost control design;developed event-driven control method;critic NN;fault observer;sliding-mode control strategy;general nonlinear dynamics;nonlinear systems;reinforcement learning;actuator faults","","70","","62","IEEE","24 Oct 2019","","","IEEE","IEEE Journals"
"Adaptive Fuzzy Fault-Tolerant Tracking Control for Partially Unknown Systems With Actuator Faults via Integral Reinforcement Learning Method","H. Zhang; K. Zhang; Y. Cai; J. Han","State Key Laboratory of Synthetical Automation for Process Industries and the School of Information Science and Engineering, Northeastern University, Shenyang, China; School of Information Science and Engineering, Northeastern University, Shenyang, China; School of Information Science and Engineering, Northeastern University, Shenyang, China; School of Mathematics and Statistics Science, Ludong University, Yantai, China","IEEE Transactions on Fuzzy Systems","4 Oct 2019","2019","27","10","1986","1998","In this paper, a fuzzy reinforcement learning (RL)-based tracking control algorithm is first proposed for partially unknown systems with actuator faults. Based on Takagi-Sugeno fuzzy model, a novel fuzzy-augmented tracking dynamic is developed and the overall fuzzy control policy with corresponding performance index is designed, where four kinds of actuator faults, including actuator loss of effectiveness and bias fault, are considered. Combining the RL technique and fuzzy-augmented model, the new fuzzy integral RL-based fault-tolerant control algorithm is designed, and it runs in real time for the system with actuator faults. The dynamic matrices can be partially unknown and the online algorithm requires less information transmissions or computational load along with the learning process. Under the overall fuzzy fault-tolerant policy, the tracking objective is achieved and the stability is proven by Lyapunov theory. Finally, the applications in the single-link robot arm system and the complex pitch-rate control problem of F-16 fighter aircraft demonstrate the effectiveness of the proposed method.","1941-0034","","10.1109/TFUZZ.2019.2893211","National Natural Science Foundation of China(grant numbers:61433004,61627809,61621004); National High Technology Research and Development Program of China(grant numbers:2012AA040104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613792","Actuator faults;fault tolerant control;fuzzy model;integral reinforcement learning (IRL);tracking control","Actuators;Fault tolerance;Fault tolerant systems;Heuristic algorithms;Reinforcement learning;Standards;System dynamics","actuators;adaptive control;aircraft control;closed loop systems;control system synthesis;fault tolerance;fuzzy control;learning (artificial intelligence);Lyapunov methods;nonlinear control systems;optimal control;performance index;stability","fuzzy-augmented model;fuzzy integral RL-based fault-tolerant control algorithm;actuator faults;fuzzy fault-tolerant policy;adaptive fuzzy fault-tolerant tracking control;partially unknown systems;integral reinforcement learning method;fuzzy reinforcement learning-based;Takagi-Sugeno fuzzy model;fuzzy control policy;fuzzy-augmented tracking dynamic;performance index;RL technique;stability;Lyapunov theory;single-link robot arm system;F-16 fighter aircraft;pitch-rate control problem","","68","","52","IEEE","16 Jan 2019","","","IEEE","IEEE Journals"
"Decentralized Tracking Optimization Control for Partially Unknown Fuzzy Interconnected Systems via Reinforcement Learning Method","K. Zhang; H. Zhang; Y. Mu; C. Liu","School of Information Science and Engineering, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; School of Information Science and Engineering, Northeastern University, Shenyang, China; School of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Transactions on Fuzzy Systems","31 Mar 2021","2021","29","4","917","926","In this article, a novel parallel tracking control optimization algorithm is first proposed for partially unknown fuzzy interconnected systems. In the existing standard optimal tracking control, the bounded or nonasymptotic stable reference trajectory will lead the feedback control not converging to zero, which causes the performance index infinite and invalid. By using the precompensation technique, in this article, the working feedback control is considered as a reconstructed dynamic with the virtual control and a new augmented fuzzy interconnected tracking system is built, thus that the performance index is valid for optimal control. Then, combining the integral reinforcement learning (RL) method and decentralized control design, the novel integral RL parallel algorithm is first developed to solve the tracking controls for interconnected systems, which relax the requirements of exact matrices information $A_i^k$ and $B_i^k$ during the solving process. Both the convergence and stability of the designed control optimization scheme are guaranteed by theorems. Finally, the new parallel tracking algorithm for interconnected systems is verified through the dual-manipulator coordination system and simulation results demonstrate the effectiveness.","1941-0034","","10.1109/TFUZZ.2020.2966418","National Natural Science Foundation of China(grant numbers:61627809,61433004,61621004); Liaoning Revitalization Talents Program(grant numbers:XLYC1801005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8957627","Fuzzy model;interconnected system;integral reinforcement learning (RL);parallel tracking control","Interconnected systems;Trajectory;Standards;Performance analysis;Optimal control;Reinforcement learning;Feedback control","control system synthesis;decentralised control;feedback;fuzzy control;interconnected systems;learning (artificial intelligence);linear quadratic control;Lyapunov methods;manipulators;matrix algebra;nonlinear control systems;optimal control;optimisation;parallel algorithms;stability;tracking","dual-manipulator coordination system;parallel tracking algorithm;designed control optimization scheme;tracking controls;novel integral RL parallel algorithm;control design;integral reinforcement learning method;optimal control;augmented fuzzy interconnected tracking system;virtual control;working feedback control;nonasymptotic stable reference trajectory;bounded reference trajectory;existing standard optimal tracking control;novel parallel tracking control optimization algorithm;partially unknown fuzzy interconnected systems;decentralized tracking optimization control","","32","","41","IEEE","13 Jan 2020","","","IEEE","IEEE Journals"
" $H_{\infty}$  Static Output-Feedback Control Design for Discrete-Time Systems Using Reinforcement Learning","A. P. Valadbeigi; A. K. Sedigh; F. L. Lewis","Department of Electrical Engineering, Islamic Azad University, Science and Research Branch, Tehran, Iran; Department of Electrical Engineering, K. N. Toosi University of Technology, Tehran, Iran; UTA Research Institute, University of Texas at Arlington, FortWorth, USA","IEEE Transactions on Neural Networks and Learning Systems","6 Feb 2020","2020","31","2","396","406","This paper provides necessary and sufficient conditions for the existence of the static output-feedback (OPFB) solution to the H∞ control problem for linear discrete-time systems. It is shown that the solution of the static OPFB H∞ control is a Nash equilibrium point. Furthermore, a Q-learning algorithm is developed to find the H∞ OPFB solution online using data measured along the system trajectories and without knowing the system matrices. This is achieved by solving a game algebraic Riccati equation online and using the measured data. A simulation example shows the effectiveness of the proposed method.","2162-2388","","10.1109/TNNLS.2019.2901889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693992","Discrete-time (DT) systems;H∞ static output feedback (OPFB);reinforcement learning (RL)","Games;Heuristic algorithms;Performance analysis;Uncertainty;Optimal control;Learning systems","control system synthesis;discrete time systems;feedback;game theory;learning (artificial intelligence);linear systems;Riccati equations","static OPFB H∞ control;H∞ static output-feedback control design;linear discrete-time systems;necessary and sufficient conditions;reinforcement learning;game algebraic Riccati equation;system trajectories;Q-learning algorithm;Nash equilibrium point","","19","","34","IEEE","19 Apr 2019","","","IEEE","IEEE Journals"
"Manipulator Control Method Based on Deep Reinforcement Learning","R. Zeng; M. Liu; J. Zhang; X. Li; Q. Zhou; Y. Jiang","Special Environment Robot Technology Key Laboratory of Sichuan Province, Southwest University of Science and Technology, Mianyang; Special Environment Robot Technology Key Laboratory of Sichuan Province, Southwest University of Science and Technology, Mianyang; Special Environment Robot Technology Key Laboratory of Sichuan Province, Southwest University of Science and Technology, Mianyang; Special Environment Robot Technology Key Laboratory of Sichuan Province, Southwest University of Science and Technology, Mianyang; Special Environment Robot Technology Key Laboratory of Sichuan Province, Southwest University of Science and Technology, Mianyang; Special Environment Robot Technology Key Laboratory of Sichuan Province, Southwest University of Science and Technology, Mianyang","2020 Chinese Control And Decision Conference (CCDC)","11 Aug 2020","2020","","","415","420","Robotic arm have transformed the manufacturing industry and have been used for scientific exploration in human inaccessible environments. The existing manipulator control methods based on deep reinforcement learning usually discretize the action space or consider the planar manipulator, which results in great limitations of the tasks that the manipulator can accomplish complete. In this paper, we propose a control method based on the Deep Deterministic Policy Gradient (DDPG) algorithm for the 6 degree-of-freedom manipulator that reach the object position in three-dimensional space. This paper designs two types of reward functions, and introduces the manipulability index into the algorithm. The manipulability index evaluates the flexibility of the robotic arm in the work space, which is referenced by the algorithm to optimize the joint pose of the robotic arm to reach the object position. By building a simulation platform to compare the algorithms based on two reward functions, the effectiveness of the DDPG algorithm is verified, and the 6 degree-of-freedom manipulator can reach the object position with more flexible posture based on the DDPG algorithm with manipulability index.","1948-9447","978-1-7281-5855-6","10.1109/CCDC49329.2020.9164440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9164440","Deep reinforcement learning;Manipulator;Reward function;Joint pose","Manipulators;Task analysis;Aerospace electronics;Indexes;Learning (artificial intelligence);Service robots","control engineering computing;learning (artificial intelligence);manipulators;neural nets;position control","deep reinforcement learning;robotic arm;manufacturing industry;deep deterministic policy gradient algorithm;6 degree-of-freedom manipulator;object position;reward functions;manipulability index;DDPG algorithm;manipulator control method","","8","","17","IEEE","11 Aug 2020","","","IEEE","IEEE Conferences"
"Hierarchical Deep Reinforcement Learning With Experience Sharing for Metaverse in Education","R. Hare; Y. Tang","Department of Electrical and Computer Engineering, Rowan University, Glassboro, NJ, USA; Institute of Smart Education, Qingdao Academy of Intelligent Industries, Qingdao, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","16 Mar 2023","2023","53","4","2047","2055","Metaverse has gained increasing interest in education, with much of literature focusing on its great potential to enhance both individual and social aspects of learning. However, little work has been done to address the systems and technologies behind providing meaningful Metaverse learning. This article proposes a technical framework to address this research gap, where a hierarchical multiagent reinforcement learning approach with experience sharing is developed to augment the intelligence of nonplayer characters in Metaverse learning for personalization. The utility and benefits of the proposed framework and methodologies are demonstrated in Gridlock, a Metaverse learning game, as well as through extensive simulations.","2168-2232","","10.1109/TSMC.2022.3227919","National Science Foundation(grant numbers:1913809); U.S. Department of Education Graduate Assistance in Areas of National Need (GAANN)(grant numbers:P200A180055); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9994611","ACP;experience sharing;metaverse learning;reinforcement learning (RL)","Metaverse;Task analysis;Education;Neural networks;Cybernetics;Games;Convergence","computer games;deep learning (artificial intelligence);learning (artificial intelligence);multi-agent systems;reinforcement learning","experience sharing;hierarchical deep reinforcement learning;hierarchical multiagent reinforcement learning approach;meaningful Metaverse learning;Metaverse learning game;technical framework","","6","","47","CCBY","20 Dec 2022","","","IEEE","IEEE Journals"
"Consensus of Nonlinear Multiagent Systems With Uncertainties Using Reinforcement Learning Based Sliding Mode Control","J. Li; L. Yuan; T. Chai; F. L. Lewis","School of Information and Control Engineering, Liaoning Petrochemical University, Fushun, China; School of Information and Control Engineering, Liaoning Petrochemical University, Fushun, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; UTA Research Institute, The University of Texas at Arlington, Arlington, TX, USA","IEEE Transactions on Circuits and Systems I: Regular Papers","24 Jan 2023","2023","70","1","424","434","This paper investigates distributed control protocols design for uncertain nonlinear multi-agent systems with the goal of achieving the optimal consensus. The critical challenges encountered when designing the optimal distributed control protocols are mainly caused by the internal coupling of agents, uncertainty and nonlinear dynamics. Communication delay among agents makes overcoming these challenges even more difficult. To this end, a novel sliding mode control design method is developed based on the sliding mode control principle and the reinforcement learning technique. The remarkable highlights of the developed method in this paper include the design of distributed sliding mode controllers and the integrated framework of sliding mode control and reinforcement learning, which bring the outcome of successfully learning the composite distributed control protocols for multi-agent systems. Thus, all agents can successfully eliminate the negative impacts brought by system uncertainties and communication delay among agents, and finally follow the leader with a nearly optimal approach. The reachability of sliding mode surfaces and the optimal consensus are rigorously proven and analyzed. Finally, simulation results illustrate the effectiveness of the developed method.","1558-0806","","10.1109/TCSI.2022.3206102","National Natural Science Foundation of China(grant numbers:62073158,61991404,61991400,61673280); Science and Technology Major Project 2020 of Liaoning Province(grant numbers:2020JH1/10100008); Open Project of Key Field Alliance of Liaoning Province(grant numbers:2019KF0306); 111 Project 2.0(grant numbers:B08015); Basic Research Project of Education Department of Liaoning Province(grant numbers:LJKZ0401); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9906584","Multi-agent systems;distributed consensus control;sliding mode control;reinforcement learning","Uncertainty;Delays;Protocols;Sliding mode control;Multi-agent systems;Robustness;Reinforcement learning","control system synthesis;delays;distributed control;multi-agent systems;nonlinear control systems;reinforcement learning;uncertain systems;variable structure systems","communication delay;composite distributed control protocols;control protocols design;distributed sliding mode controllers;nonlinear dynamics;optimal distributed control protocols;reinforcement learning based sliding mode control;sliding mode control design method;system uncertainties;uncertain nonlinear multiagent systems","","4","","36","IEEE","30 Sep 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning and Optimal Control of PMSM Speed Servo System","J. Zhao; C. Yang; W. Gao; L. Zhou","Engineering Research Center of Intelligent Control for Underground Space, Ministry of Education, Xuzhou, China; Engineering Research Center of Intelligent Control for Underground Space, Ministry of Education, Xuzhou, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; Engineering Research Center of Intelligent Control for Underground Space, Ministry of Education, Xuzhou, China","IEEE Transactions on Industrial Electronics","17 Mar 2023","2023","70","8","8305","8313","This article proposes a novel model-free optimal speed tracking control scheme for permanent magnet synchronous motors (PMSMs) through reinforcement learning (RL). To achieve the speed servo control, we formulate the linear quadratic regulator associated with the reduced-order model in the outer loop controller design. Such a model is obtained in terms of singular perturbation theory, which enables the separation of slow and fast time-scale dynamics. Moreover, we develop an off-policy RL algorithm to iteratively approximate the ideal value of solution to the linear quadratic regulator without requiring any knowledge of model parameters of the PMSM and the measurement of the load torque. Both simulation and experimental tests are carried out to justify that the proposed control scheme realizes precision speed tracking performance and shape transient response in the presence of unknown model parameters.","1557-9948","","10.1109/TIE.2022.3220886","National Natural Science Foundation of China(grant numbers:61873272,62073327,62273350); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950521","Optimal control;permanent magnet synchronous motor (PMSM);reinforcement learning (RL);singular perturbation;speed tracking","Mathematical models;Regulators;Synchronous motors;Heuristic algorithms;Torque;Stators;PI control","angular velocity control;approximation theory;control engineering computing;control system synthesis;iterative methods;linear quadratic control;machine vector control;optimal control;permanent magnet motors;perturbation theory;reduced order systems;reinforcement learning;servomechanisms;synchronous motors","iterative approximation;linear quadratic regulator;model-free optimal speed tracking control;off-policy reinforcement learning algorithm;off-policy RL algorithm;optimal control;outer loop controller design;permanent magnet synchronous motor speed servo system;PMSM speed servo system;precision speed tracking performance;reduced-order model;shape transient response;singular perturbation theory;unknown model parameters","","3","","31","IEEE","14 Nov 2022","","","IEEE","IEEE Journals"
"Event-Triggered Guarantee Cost Control for Partially Unknown Stochastic Systems via Explorized Integral Reinforcement Learning Strategy","Y. Liang; H. Zhang; J. Zhang; Z. Ming","School of Information Science and Engineering, Northeastern University, Shenyang, China; School of Information Science and Engineering, Northeastern University, Shenyang, China; School of Information Science and Engineering, Northeastern University, Shenyang, China; School of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","15","In this article, an integral reinforcement learning (IRL)-based event-triggered guarantee cost control (GCC) approach is proposed for stochastic systems which are modulated by randomly time-varying parameters. First, with the aid of the RL algorithm, the optimal GCC (OGCC) problem is converted into an optimal zero-sum game by solving a modified Hamilton–Jacobin–Isaac (HJI) equation of the auxiliary system. Moreover, in order to address the stochastic zero-sum game, we propose an on-policy IRL-based control approach involved by the multivariate probabilistic collocation method (MPCM), which can accurately predict the mean value of uncertain functions with randomly time-varying parameters. Furthermore, a novel GCC method, which combines the explorized IRL algorithm and MPCM, is designed to relax the restriction of knowing the system dynamics for the class of stochastic systems. On this foundation, for the purpose of reducing computation cost and avoiding the waste of resources, we propose an event-triggered GCC approach involved with explorized IRL and MPCM by utilizing critic-actor-disturbance neural networks (NNs). Meanwhile, the weight vectors of three NNs are updated simultaneously and aperiodically according to the designed triggering condition. The ultimate boundedness (UB) properties of the controlled systems have been proved by means of the Lyapunov theorem. Finally, the effectiveness of the developed GCC algorithms is illustrated via two simulation examples.","2162-2388","","10.1109/TNNLS.2022.3221105","National Key Research and Development Program of China(grant numbers:2018YFA0702200); National Natural Science Foundation of China(grant numbers:61433004,61627809,61621004); Liaoning Revitalization Talents Program(grant numbers:XLYC1801005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954270","Event-triggered control (ETC);guaranteed cost control;integral reinforcement learning (IRL);multivariate probabilistic collocation method (MPCM);stochastic systems;ultimate boundedness (UB)","Uncertainty;Stochastic systems;Costs;Heuristic algorithms;Stochastic processes;Optimal control;Time-varying systems","","","","2","","","IEEE","17 Nov 2022","","","IEEE","IEEE Early Access Articles"
"Sliding Mode Control Based on Reinforcement Learning for T-S Fuzzy Fractional-Order Multiagent System With Time-Varying Delays","Y. Yan; H. Zhang; J. Sun; Y. Wang","College of Information Science and Engineering, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries and the College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","12","This article researches the sliding mode control (SMC) for fuzzy fractional-order multiagent system (FOMAS) subject to time-varying delays over directed networks based on reinforcement learning (RL),  $\alpha\in(0,1).$  First, since there is information communication between an agent and another agent, a new distributed control policy  $\xi_{i}(t)$  is introduced so that the sharing of signals is implemented through RL, whose propose is to minimize the error variables with learning. Then, different from the existed papers studying normal fuzzy MASs, a new stability basis of fuzzy FOMASs with time-varying delay terms is presented to guarantee that the states of each agent eventually converge to the smallest possible domain of  $0$  using Lyapunov–Krasovskii functionals, free weight matrix, and linear matrix inequality (LMI). Furthermore, in order to provide appropriate parameters for SMC, the RL algorithm is combined with SMC strategy, and the constraints on the initial conditions of the control input  $u_i(t)$  are eliminated, so that the sliding motion satisfy the reachable condition within a finite time. Finally, to illustrate that the proposed protocol is valid, the results of the simulation and numerical examples are presented.","2162-2388","","10.1109/TNNLS.2023.3241070","National Key Research and Development (R&D) Program of China(grant numbers:2018YFA0702200); National Natural Science Foundation of China(grant numbers:61627809,62173080); Liaoning Revitalization Talents Program(grant numbers:XLYC1801005); Nature Science Foundation of Liaoning Province of China(grant numbers:2022JH25/10100008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10040984","Fractional-order multiagent system (FOMAS);Lyapunov stability;reinforcement learning (RL);sliding mode control (SMC)","Delays;Reinforcement learning;Time-varying systems;Multi-agent systems;Lyapunov methods;Learning systems;Aerospace electronics","","","","2","","","IEEE","8 Feb 2023","","","IEEE","IEEE Early Access Articles"
"Cooperative Multiagent Deep Reinforcement Learning for Computation Offloading: A Mobile Network Operator Perspective","K. Li; X. Wang; Q. He; B. Yi; A. Morichetta; M. Huang","College of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Computer Science and Engineering and the State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China; Distributed Systems Group, TU Wien, Vienna, Austria; College of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Internet of Things Journal","18 Nov 2022","2022","9","23","24161","24173","Computation offloading decisions play a crucial role in implementing mobile-edge computing (MEC) technology in the Internet of Things (IoT) services. Mobile network operators (MNOs) can employ computation offloading techniques to reduce task completion delay and improve the Quality of Service (QoS) for users by optimizing the system’s processing delay and energy consumption. However, different IoT applications (e.g., entertainment and autonomous driving) generate different delay tolerances and benefits for computational tasks from the MNO perspective. Therefore, simply minimizing the delay of all tasks does not satisfy the QoS of each user. The system architecture design should consider the significance of users and the heterogeneity of tasks. Unfortunately, rare work has been done to discuss this practical issue. In this article, from the perspective of MNO, we investigate the computation offloading optimization problem of multiuser delay-sensitive tasks. First, we propose a new optimization model, which designs different optimization objectives for the cost and revenue of tasks. Then, we transform the problem into a Markov decision processes problem, which leads to designing a multiagent iterative optimization framework. For the strategic optimization of each agent, we further propose a cooperative multiagent deep reinforcement learning (CMDRL) algorithm to optimize two different objectives at the same time. Two agents are integrated into the CMDRL framework to enable agents to collaborate and converge to the global optimum in a distributed manner. At the same time, the priority experience replay method is introduced to improve the utilization rate of effective samples and the learning efficiency of the algorithm. The experimental results show that our proposed method can effectively achieve a significantly higher profit than the alternative state-of-the-art method and exhibit a more favorable computational performance than benchmark deep reinforcement learning methods.","2327-4662","","10.1109/JIOT.2022.3189445","National Key Research and Development Program of China(grant numbers:2019YFB1802800); National Natural Science Foundation of China(grant numbers:62032013,61872073); Major International(Regional) Joint Research Project of NSFC(grant numbers:71620107003); Liaoning Revitalization Talents Program(grant numbers:XLYC1902010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9830656","Computational offloading;deep reinforcement learning (DRL);delay bounds;mobile-edge computing (MEC);task revenue","Task analysis;Delays;Optimization;Reinforcement learning;Quality of service;Vehicle dynamics;Costs","cloud computing;Internet of Things;iterative methods;learning (artificial intelligence);Markov processes;mobile computing;multi-agent systems;optimisation;quality of service","autonomous driving;benchmark deep reinforcement learning methods;computation offloading decisions;computation offloading optimization problem;computational tasks;different delay tolerances;different IoT applications;different optimization objectives;energy consumption;entertainment;favorable computational performance;learning efficiency;Markov decision;MNO perspective;mobile network operator perspective;mobile network operators;mobile-edge computing;multiagent deep reinforcement learning algorithm;multiagent iterative optimization framework;multiuser delay-sensitive tasks;optimization model;QoS;strategic optimization;system architecture design;task completion delay;Things services","","1","","34","IEEE","15 Jul 2022","","","IEEE","IEEE Journals"
"Task Computation Offloading for Multi-Access Edge Computing via Attention Communication Deep Reinforcement Learning","K. Li; X. Wang; Q. He; M. Yang; M. Huang; S. Dustdar","College of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Computer Science and Engineering and State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; Distributed Systems Group, TU Wien, Vienna, Austria","IEEE Transactions on Services Computing","8 Aug 2023","2023","16","4","2985","2999","This article investigates how to enhance the Multi-access Edge Computing (MEC) systems performance with the aid of device-to-device (D2D) communication computation offloading. By adequately exploiting a novel computation offloading mechanism based on D2D collaboration, users can efficiently share computational resources with each other. However, it is challenging to distinguish valuable information that truly promotes a collaborative decision, as worthless information can hinder collaboration among users. In addition, the transmission of large volumes of information requires high bandwidth and incurs significant latency and computational complexity, resulting in unacceptable costs. In this article, we propose an efficient D2D-assisted MEC computation offloading framework based on Attention Communication Deep Reinforcement Learning (ACDRL), which simulates the interactions between related entities, including device-to-device collaboration in the horizontal and device-to-edge offloading in the vertical. Second, we developed a distributed cooperative reinforcement learning algorithm that includes an attention mechanism that skews computational resources towards active users to avoid unnecessary resource wastage in large-scale MEC systems. Finally, to improve the effectiveness and rationality of cooperation among users, we introduce a communication channel to integrate information from all users in a communication group, thus facilitating cooperative decision-making. The proposed framework is benchmarked, and the experimental results show that the proposed framework can effectively reduce latency and provide valuable insights for practical design compared to other baseline approaches.","1939-1374","","10.1109/TSC.2022.3225473","National Key R & D Program of China(grant numbers:2022YFB4500800); National Natural Science Foundation of China(grant numbers:62032013,61872073); LiaoNing Revitalization Talents Program(grant numbers:XLYC1902010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10025797","Multi-access edge computing;reinforcement learning;task computation offloading;user cooperation","Task analysis;Device-to-device communication;Costs;Computer architecture;Collaboration;Reinforcement learning;Dynamic scheduling","5G mobile communication;cloud computing;computational complexity;decision making;deep learning (artificial intelligence);edge computing;learning (artificial intelligence);mobile computing;multi-access systems;nonorthogonal multiple access;optimisation;reinforcement learning;resource allocation;telecommunication computing","active users;Attention Communication Deep Reinforcement Learning;attention mechanism;collaborative decision;communication channel;communication group;computational complexity;device-to-device collaboration;device-to-device communication computation offloading;device-to-edge offloading;distributed cooperative reinforcement learning algorithm;large-scale MEC systems;MEC computation offloading framework;Multiaccess Edge Computing systems performance;novel computation offloading mechanism;skews computational resources;task computation offloading;worthless information","","1","","46","IEEE","25 Jan 2023","","","IEEE","IEEE Journals"
"Off-policy reinforcement learning for distributed output synchronization of linear multi-agent systems","B. Kiumarsi; F. L. Lewis","UTA Research Institute, University of Texas at Arlington, Ft. Worth, USA; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Ft. Worth, USA","2017 IEEE Symposium Series on Computational Intelligence (SSCI)","5 Feb 2018","2017","","","1","8","In this paper, off-policy reinforcement learning (RL) is used to find a model-free optimal solution to the H∞ output synchronization of heterogeneous multi-agent discrete-time systems. First, the output synchronization problem is formulated as a set of local optimal tracking problems. It is shown that optimal local synchronization control protocols can be found by solving augmented game algebraic Riccati equations (GAREs). The solutions to the GAREs require the state of the leader for all agents and the knowledge of agent dynamics. To obviate this requirement, a distributed adaptive observer is designed to estimate the leader state for all agents without requiring complete knowledge of the leader dynamics. Moreover, off-policy RL algorithm is used to learn the solution to the GAREs using only measured data and without requiring the knowledge of the agent or the leader dynamics. In the proposed approach, in contrast to other model free approaches, the disturbance input does not need to be adjusted in a specific manner. A simulation example is given to show the effectiveness of the proposed method.","","978-1-5386-2726-6","10.1109/SSCI.2017.8280830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8280830","output synchronization;heterogeneous systems;off-policy","Synchronization;Observers;Multi-agent systems;Heuristic algorithms;Learning (artificial intelligence);Mathematical model;Protocols","discrete time systems;game theory;learning systems;linear systems;multi-agent systems;observers;Riccati equations;synchronisation","linear multiagent systems;off-policy reinforcement learning;model-free optimal solution;heterogeneous multiagent discrete-time systems;output synchronization problem;local optimal tracking problems;optimal local synchronization control protocols;augmented game algebraic Riccati equations;GAREs;agent dynamics;distributed adaptive observer;leader dynamics;off-policy RL algorithm;distributed output synchronization","","","","18","IEEE","5 Feb 2018","","","IEEE","IEEE Conferences"
"Dynamic Event-Triggered-Based Integral Reinforcement Learning Algorithm for Frequency Control of Microgrid With Stochastic Uncertainty","X. Tong; D. Ma; R. Wang; X. Xie; H. Zhang","College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; Institute of Advanced Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; State Key Laboratory of Synthetical Automation for Process Industries and the College of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Transactions on Consumer Electronics","16 Aug 2023","2023","69","3","321","330","The intermittency of renewable energy and the uncertainty of load put forward higher robustness requirements for the frequency recovery of microgrid (MG). The energy storage equipment provides an idea for frequency control because of its fast response and strong controllability. In this paper, an on-line adaptive frequency control method is proposed to control the governor and energy storage to realize the frequency recovery of MG under stochastic uncertainty. First, the MG system with external disturbances is constructed as a zero-sum differential game model to obtain a robust optimal control scheme. Then, considering the system parameter uncertainty, an improved integral reinforcement learning (IRL) algorithm is designed, in which the reinforcement signal contains a non-quadratic function to solve the energy storage control constraints. Furthermore, a novel dynamic event-triggered control (DETC) is developed to reduce control update times of energy storage. The dynamic variable in DETC coupled with static trigger includes not only the past triggering information but also the disturbances. DETC has a larger trigger threshold than static event-triggered control (SETC). Meanwhile, the proposed algorithm is implemented by an action-critic network structure, in which the action network of energy storage is updated aperiodically. Finally, simulation results show that the proposed control algorithm can realize the frequency recovery of MG, and it has good robustness compared with other algorithms.","1558-4127","","10.1109/TCE.2023.3241684","National Natural Science Foundation of China(grant numbers:U22A20221,62073064); National Key Research and Development Program of China(grant numbers:2018YFA0702200); Fundamental Research Funds for the Central Universities, China(grant numbers:N2204007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035446","Microgrid;stochastic uncertainty;frequency control;integral reinforcement learning;dynamic event triggering;input constraints","Frequency control;Energy storage;Heuristic algorithms;Uncertainty;Renewable energy sources;Power system dynamics;Stochastic processes","control engineering computing;controllability;differential games;distributed power generation;energy storage;frequency control;optimal control;power system control;reinforcement learning;robust control","controllability;DETC;dynamic event-triggered control;energy storage control constraints;frequency recovery;integral reinforcement learning algorithm;MG system;microgrid;on-line adaptive frequency control;reinforcement signal;renewable energy;robust optimal control;robustness;SETC;static event-triggered control;stochastic uncertainty;system parameter uncertainty;zero-sum differential game model","","","","41","IEEE","2 Feb 2023","","","IEEE","IEEE Journals"
"Optimization control for CCS of coal-fired power unit based on reinforcement learning using process data","B. Dai; F. Wang; Y. Chang","College of Information Science and Engineering, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China","2022 34th Chinese Control and Decision Conference (CCDC)","14 Feb 2023","2022","","","230","235","Due to the large-scale grid-connection of new energy power generation, the requirements for the control system of coal-fired power units have become increasingly stringent. This paper investigates the optimization control of the coal-fired unit and proposes a reinforcement learning control framework using process data of coal-fired unit. The core of the proposed method is to employ the historical operation data to train the policy network by the supervised learning algorithm, and then the pretrained policy is further improved by the reinforcement learning via interacting with the power unit. The results of simulation experiment show that the proposed method can accelerate the training process, alleviate the computational burden compared with the standard reinforcement learning without using process data.","1948-9447","978-1-6654-7896-0","10.1109/CCDC55256.2022.10034073","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10034073","Reinforcement Learning;Process Data;Coordinated Control System;Coal-Fired Power Unit","Training;Computational modeling;Supervised learning;Process control;Optimal control;Reinforcement learning;Data models","coal;learning (artificial intelligence);optimal control;power engineering computing;power grids;reinforcement learning;supervised learning","coal-fired power unit;coal-fired unit;control system;energy power generation;historical operation data;large-scale grid-connection;optimization control;process data;reinforcement learning control framework;standard reinforcement learning;supervised learning algorithm","","","","19","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"An Energy-Efficient Communication Protocol for Power-Constrained IoT Networks: A Deep Reinforcement Learning Approach","S. A. Ullah; S. Muhammad Khalid; U. A. Korai; A. Ullah","Department of Electronic Engineering, Balochistan University of Information Technology Engineering and Management Sciences (BUITEMS), Quetta, Pakistan; Control, Automotive and Robotics Lab, National Centre of Robotics and Automation (NCRA), BUITEMS, Quetta, Pakistan; Department of Telecommunication Engineering, Mehran University of Engineering and Technology, Jamshoro, Pakistan; Control, Automotive and Robotics Lab, National Centre of Robotics and Automation (NCRA), BUITEMS, Quetta, Pakistan","2023 Global Conference on Wireless and Optical Technologies (GCWOT)","20 Mar 2023","2023","","","1","6","Power-limited devices (or sensors) constrain the deployment of modern IoT networks, such as Next-Generation Industrial IoT (NG-IIoT). These networks are envisioned as the key enablers to facilitate connectivity to billions of devices for applications like smart industries, smart healthcare, etc. This paper presents an optimal up-link communication protocol for a power-limited unlicensed sensor operating among numerous licensed sensors communicating in a time division multiple access (TDMA)-based scheme. The transmission of the power-limited sensor is ensured by employing the non-orthogonal multiple access (NOMA) technique during the time slot of a licensed sensor. To ensure energy-efficient communication, we maximize the throughput of the power-limited sensor using a deep reinforcement learning (DRL) framework recognized as a combined experience replay deep deterministic policy gradient (CER-DDPG) algorithm. Our simulation results demonstrated that the CER-DDPG-based communication protocol outperforms the benchmark schemes, such as DDPG and stochastic algorithms, in terms of throughput.","","979-8-3503-3371-8","10.1109/GCWOT57803.2023.10064678","NCR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10064678","Next-generation Industrial Internet-of-Things (IIoT);non-orthogonal multiple access (NOMA);deep reinforcement learning (DRL);combined experience replay deep deterministic policy gradient (CER-DDPG)","Optical losses;NOMA;Protocols;Simulation;Reinforcement learning;Throughput;Energy efficiency","computer network security;deep learning (artificial intelligence);gradient methods;Internet of Things;nonorthogonal multiple access;protocols;reinforcement learning;telecommunication computing;telecommunication power management;time division multiple access","CER-DDPG-based communication protocol;deep deterministic policy gradient algorithm;deep reinforcement learning framework;energy-efficient communication protocol;licensed sensor;modern IoT networks;Next-Generation Industrial IoT;NG-IIoT;nonorthogonal multiple access technique;numerous licensed sensors;power-constrained IoT networks;power-limited devices;power-limited sensor;power-limited unlicensed sensor operating;smart healthcare;time division multiple access-based scheme;time slot;up-link communication protocol","","","","17","IEEE","20 Mar 2023","","","IEEE","IEEE Conferences"
"Adaptive Suboptimal Output-Feedback Control for Linear Systems Using Integral Reinforcement Learning","L. M. Zhu; H. Modares; G. O. Peen; F. L. Lewis; B. Yue","Department of Basic, North China Institute of Science and Technology, Hebei, China; Arlington Research Institute, University of Texas, Fort Worth, TX, USA; Singapore Institute of Manufacturing Technology, Singapore; State Key Laboratory of Synthetical Process Automation, Northeastern University, Shenyang, China; Department of Mechanics, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Control Systems Technology","18 Dec 2014","2015","23","1","264","273","Reinforcement learning (RL) techniques have been successfully used to find optimal state-feedback controllers for continuous-time (CT) systems. However, in most real-world control applications, it is not practical to measure the system states and it is desirable to design output-feedback controllers. This paper develops an online learning algorithm based on the integral RL (IRL) technique to find a suboptimal output-feedback controller for partially unknown CT linear systems. The proposed IRL-based algorithm solves an IRL Bellman equation in each iteration online in real time to evaluate an output-feedback policy and updates the output-feedback gain using the information given by the evaluated policy. The knowledge of the system drift dynamics is not required by the proposed method. An adaptive observer is used to provide the knowledge of the full states for the IRL Bellman equation during learning. However, the observer is not needed after the learning process is finished. The convergence of the proposed algorithm to a suboptimal output-feedback solution and the performance of the proposed method are verified through simulation on two real-world applications, namely, the X-Y table and the F-16 aircraft.","1558-0865","","10.1109/TCST.2014.2322778","National Science Foundation(grant numbers:ECCS-1128050,IIS-1208623); Office of Naval Research(grant numbers:N00014-13-1-0562); European Office of Aerospace Research and Development, Air Force Officeof Scientific Research(grant numbers:13-3055); National Natural Science Foundation of China(grant numbers:61120106011); China Education Ministry Project 111(grant numbers:B08015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824757","Integral reinforcement learning (IRL);linear continuous-time (CT) systems;optimal control;output feedback.;Integral reinforcement learning (IRL);linear continuous-time (CT) systems;optimal control;output feedback","Equations;Heuristic algorithms;Mathematical model;Observers;Linear systems;Control systems;Convergence","adaptive control;continuous time systems;learning (artificial intelligence);learning systems;linear systems;observers;state feedback;suboptimal control","adaptive suboptimal output-feedback control;linear systems;integral reinforcement learning technique;optimal state-feedback controllers;continuous-time systems;CT systems;output-feedback controllers;online learning algorithm;IRL technique;partially unknown CT linear systems;IRL-based algorithm;IRL Bellman equation;output-feedback policy;output-feedback gain;suboptimal output-feedback solution;F-16 aircraft;X-Y table","","67","","37","IEEE","3 Jun 2014","","","IEEE","IEEE Journals"
"Hierarchical Terrain-Aware Control for Quadrupedal Locomotion by Combining Deep Reinforcement Learning and Optimal Control","Q. Yao; J. Wang; D. Wang; S. Yang; H. Zhang; Y. Wang; Z. Wu","Shenyang Institute of Automation, and Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences (CAS), Shenyang, China; University of California Santa Cruz, Santa Cruz, CA, USA; Machine Intelligence Lab (MiLAB), School of Engineering, Westlake University, Hangzhou, China; Machine Intelligence Lab (MiLAB), School of Engineering, Westlake University, Hangzhou, China; Machine Intelligence Lab (MiLAB), School of Engineering, Westlake University, Hangzhou, China; Machine Intelligence Lab (MiLAB), School of Engineering, Westlake University, Hangzhou, China; Machine Intelligence Lab (MiLAB), School of Engineering, Westlake University, Hangzhou, China","2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","16 Dec 2021","2021","","","4546","4551","Quadruped robots possess advantages on different terrains over other types of mobile robots by virtue of their flexible choices of foothold points. It is crucial to integrate terrain perception with motion planning to exploit the potential of quadruped robots. We propose a novel hierarchical terrain-aware control (HTC) framework, which leverages deep reinforcement learning (DRL) for the high-level planner and optimal control for the low-level controller. In general, traditional control methods yield better stability by using an optimization algorithm. In addition, DRL is able to offer more adaptive behavior. Our approach makes full use of the advantages of these two methods and possesses better adaptability and stability in challenging natural environments. Furthermore, the global height map of the terrain serves as visual information for the DRL, which determines the desired footholds for the robot’s leg swings and body postures. Optimal control calculates the torque of the joints on the standing legs to maintain body balance. Our method is tested on various terrains both simulated and real environments. The experimental results show that HTC can effectively enhance the adaptability of the quadruped robot by coordinating body posture.","2153-0866","978-1-6654-1714-3","10.1109/IROS51168.2021.9636738","Westlake University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636738","","Legged locomotion;Visualization;Torque;Robot kinematics;Optimal control;Reinforcement learning;Stability analysis","learning (artificial intelligence);legged locomotion;mobile robots;motion control;optimal control;path planning;robot dynamics;robots","DRL;high-level planner;optimal control;low-level controller;general control methods;traditional control methods;optimization algorithm;possesses;real environments;quadruped robot;deep reinforcement learning;different terrains;mobile robots;terrain perception;novel hierarchical terrain-aware control","","1","","24","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Adversary Agnostic Robust Deep Reinforcement Learning","X. Qu; A. Gupta; Y. -S. Ong; Z. Sun","Bytedance AI Laboratory, Speech and Audio Team, Singapore; A*STAR, Singapore Institute of Manufacturing Technology (SIMTech), Singapore; Data Science and Artificial Intelligence Research Centre, Nanyang Technological University, Singapore; A*STAR, Institute of High Performance Computing, Singapore","IEEE Transactions on Neural Networks and Learning Systems","1 Sep 2023","2023","34","9","6146","6157","Deep reinforcement learning (DRL) policies have been shown to be deceived by perturbations (e.g., random noise or intensional adversarial attacks) on state observations that appear at test time but are unknown during training. To increase the robustness of DRL policies, previous approaches assume that explicit adversarial information can be added into the training process, to achieve generalization ability on these perturbed observations as well. However, such approaches not only make robustness improvement more expensive but may also leave a model prone to other kinds of attacks in the wild. In contrast, we propose an adversary agnostic robust DRL paradigm that does not require learning from predefined adversaries. To this end, we first theoretically show that robustness could indeed be achieved independently of the adversaries based on a policy distillation (PD) setting. Motivated by this finding, we propose a new PD loss with two terms: 1) a prescription gap maximization (PGM) loss aiming to simultaneously maximize the likelihood of the action selected by the teacher policy and the entropy over the remaining actions and 2) a corresponding Jacobian regularization (JR) loss that minimizes the magnitude of gradients with respect to the input state. The theoretical analysis substantiates that our distillation loss guarantees to increase the prescription gap and hence improves the adversarial robustness. Furthermore, experiments on five Atari games firmly verify the superiority of our approach compared to the state-of-the-art baselines.","2162-2388","","10.1109/TNNLS.2021.3133537","A*STAR Cyber-Physical Production System (CPPS), towards Contextual and Intelligent Response Research Program RIE2020 IAF-PP(grant numbers:A19C1a0018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9660371","Adversarial robustness;adversary agnostic;Atari games;deep reinforcement learning (DRL)","Robustness;Training;Perturbation methods;Reinforcement learning;Jacobian matrices;Games;Entropy","deep learning (artificial intelligence);entropy;generalisation (artificial intelligence);gradient methods;maximum likelihood estimation;optimisation;random noise;reinforcement learning;security of data","action likelihood maximization;adversarial robustness;adversary agnostic robust deep reinforcement learning;adversary agnostic robust DRL paradigm;distillation loss;DRL policy robustness;explicit adversarial information;generalization ability;gradient magnitude minimization;intensional adversarial attack;Jacobian regularization loss;perturbed observations;PGM loss;policy distillation setting;prescription gap maximization loss;random noise;robustness improvement;state observation;teacher policy","","2","","33","IEEE","22 Dec 2021","","","IEEE","IEEE Journals"
"Service Chain Mapping Algorithm Based on Reinforcement Learning","W. Li; H. Wu; C. Jiang; P. Jia; N. Li; P. Lin","Communication branch, State Grid Jiangsu Electric Power Co., Ltd, Jiangsu, China; Communication branch, State Grid Jiangsu Electric Power Co., Ltd, Jiangsu, China; Communication branch, State Grid Jiangsu Electric Power Co., Ltd, Jiangsu, China; Communication branch, State Grid Jiangsu Electric Power Co., Ltd, Jiangsu, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Vectinfo Technologies Co., Ltd., Beijing, China","2020 International Wireless Communications and Mobile Computing (IWCMC)","27 Jul 2020","2020","","","800","805","Network function virtualization integrates different types of dedicated network equipment into standard industry IT server, storage and switch equipment, enabling network functions traditionally implemented using specific equipment to use software running on IT industry standard server hardware, thereby enhancing system flexibility. Using the organic combination of NFV and software-defined network technologies, an software defined Smart grid communication network can be constructed, so that the network functions of service function chaining can be implemented on general-purpose equipment, and end-to-end services are transformed into a set of sequentially connected VNFs, which can effectively deploy and manage service function chains. Service provision and server resource utilization will be affected by SFC mapping. In order to ensure the reasonable use of network resources and the QoS of SFC, the research on SFC mapping algorithms is particularly important. In this paper, we propose a service chain mapping algorithm based on reinforcement learning, which aims to learn by the system status and the feedback value given by the mapped environment and then finally determine the actual deployment location of each virtual function node in the SFC. The comparison and analysis with other algorithms show that the SFC mapping algorithm proposed in this paper can adjust the feedback value function to optimize the load balance of the system and reduce the SFC average link delay in different network topologies.","2376-6506","978-1-7281-3129-0","10.1109/IWCMC48107.2020.9148460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9148460","network function virtualization;service function chain;SFC mapping;reinforcement learning","","learning (artificial intelligence);network servers;power engineering computing;quality of service;resource allocation;smart power grids;software defined networking;telecommunication network topology;virtualisation","service chain mapping algorithm;reinforcement learning;network function virtualization;network equipment;switch equipment;software-defined network technologies;service function chaining;general-purpose equipment;end-to-end services;server resource utilization;network resources;SFC mapping algorithm;mapped environment;virtual function node;feedback value function;smart grid communication network;IT industry standard server hardware;standard industry IT server;NFV;QoS;network topology;load balancing","","4","","14","IEEE","27 Jul 2020","","","IEEE","IEEE Conferences"
"Goal-Driven Autonomous Exploration Through Deep Reinforcement Learning","R. Cimurs; I. H. Suh; J. H. Lee","Robert Bosch GmbH, Hanyang University, Seoul, South Korea; COGAPLEX, Seoul, South Korea; CLE Inc., Hanyang University, Seoul, South Korea","IEEE Robotics and Automation Letters","14 Dec 2021","2022","7","2","730","737","In this letter, we present an autonomous navigation system for goal-driven exploration of unknown environments through deep reinforcement learning (DRL). Points of interest (POI) for possible navigation directions are obtained from the environment and an optimal waypoint is selected, based on the available data. Following the waypoints, the robot is guided towards the global goal and the local optimum problem of reactive navigation is mitigated. Then, a motion policy for local navigation is learned through a DRL framework in a simulation. We develop a navigation system where this learned policy is integrated into a motion planning stack as the local navigation layer to move the robot between waypoints towards a global goal. The fully autonomous navigation is performed without any prior knowledge while a map is recorded as the robot moves through the environment. Experiments show that the proposed method has an advantage over similar exploration methods, without reliance on a map or prior information in complex static as well as dynamic environments.","2377-3766","","10.1109/LRA.2021.3133591","Ministry of Trade, Industry and Energy(grant numbers:10080638); National Research Foundation; Ministry of Science and ICT, South Korea(grant numbers:2020M3H8A1114945); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645287","AI-enabled robotics;reinforcement learning;sensor-based Control","Navigation;Robots;Robot sensing systems;Robot kinematics;Neural networks;Lasers;Task analysis","control engineering computing;deep learning (artificial intelligence);mobile robots;navigation;path planning;reinforcement learning","deep reinforcement learning;optimal waypoint;waypoints;global goal;local optimum problem;reactive navigation;motion policy;DRL framework;motion planning stack;local navigation layer;fully autonomous navigation;dynamic environments;goal-driven autonomous exploration;policy learning;points of interest;robot movement","","15","","42","IEEE","10 Dec 2021","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Dynamic Proportional-Integral (PI) Gain Auto-Tuning Method for a Robot Driver System","J. Park; H. Kim; K. Hwang; S. Lim","Graduate School of Automotive Engineering, Kookmin University, Seongbuk-gu, Seoul, South Korea; Graduate School of Automotive Engineering, Kookmin University, Seongbuk-gu, Seoul, South Korea; Electrification Energy Efficiency & Drivability Team 3, Hyundai Motor Company, Hwaseong, South Korea; Department of Automobile and IT Convergence, Kookmin University, Seongbuk-gu, Seoul, South Korea","IEEE Access","24 Mar 2022","2022","10","","31043","31057","To meet the growing trend of stringent fuel economy regulations, automakers around the world are designing modules such as engines, motors, transmissions and batteries to be as efficient as possible. In order to verify the effect of these designs on the overall fuel efficiency of the vehicle, the vehicle equipped with each module is placed on the chassis dynamometer, driven to follow the target vehicle speed, and actual fuel efficiency is measured. These tests are traditionally performed by human operators, but are now being replaced by robots (physical or software) to ensure the accuracy and reliability of test results. Although the conventionally proposed proportional integral (PI)-based controller has a simple structure and is easy to implement, it requires the process of finding the optimal gain whenever the test conditions such as vehicle or drive cycle change, which is difficult and time consuming. In this study, we propose a proportional integral controller gain adjustment algorithm using deep reinforcement learning. The reinforcement learning agent learns to dynamically modify the PI gain value of the acceleration/deceleration pedal to better follow the target vehicle in a simulation environment. The perturbation is used in each training episode to reduce the difference between the simulation and real testing environment. Upon completion of the training process, the trained agent performs an adjustment process that generates a reference gain table. We then use this reference gain table to perform a real test. The performance of the proposed system was evaluated using Hyundai Tucson HEV (NX4) on an AVL chassis dynamometer. We also compared the performance of our proposed algorithm to traditional fuzzy logic-based PI controllers. The obtained experimental results show that the proposed control system achieved a performance improvement of aounrd 46.8% compared to the conventional PI control system in terms of root mean square error.","2169-3536","","10.1109/ACCESS.2022.3159785","Hyundai Motor Group, Korea Institute of Police Technology (KIPoT); Korean Government [Korea National Police Agency (KNPA)](grant numbers:092021C26S03000); Artificial Intelligence (AI)-based Autonomous Driving Computing Module Development and Service Demonstration Program; Ministry of Trade, Industry, and Energy of Korea(grant numbers:20005673); Brain Korea 21 (BK21) Program through the National Research Foundation of Korea (NRF); Ministry of Education(grant numbers:5199990814084); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9737020","Automation;deep Q-learning;emission test;machine learning;PID control;reinforcement learning;vehicle control","Robots;Vehicles;PI control;Heuristic algorithms;Vehicle dynamics;Control systems;Dynamometers","control engineering computing;control system synthesis;dynamometers;fuel economy;fuzzy control;fuzzy logic;hybrid electric vehicles;learning (artificial intelligence);nonlinear control systems;PI control","deep reinforcement learning;dynamic proportional-integral gain auto-tuning method;robot driver system;stringent fuel economy regulations;target vehicle speed;fuel efficiency;human operators;physical;optimal gain;test conditions;drive cycle change;proportional integral controller gain adjustment algorithm;reinforcement learning agent;PI gain value;simulation environment;testing environment;training process;trained agent;adjustment process;reference gain table;AVL chassis dynamometer;traditional fuzzy logic-based PI controllers;performance improvement;conventional PI control system","","1","","39","CCBYNCND","16 Mar 2022","","","IEEE","IEEE Journals"
"Cooperative Reinforcement Learning Aided Dynamic Routing in UAV Swarm Networks","Z. Wang; H. Yao; T. Mai; Z. Xiong; F. R. Yu","State Key Lab. of Net. and Switching Tech., Beijing Univ. of Posts and Telecom., Beijing, P.R. China; State Key Lab. of Net. and Switching Tech., Beijing Univ. of Posts and Telecom., Beijing, P.R. China; State Key Lab. of Net. and Switching Tech., Beijing Univ. of Posts and Telecom., Beijing, P.R. China; Singapore Univ. of Tech. and Design, Singapore; Depart. of Sys. and Comp. Eng., Carleton Univ., Ottawa, ON, Canada","ICC 2022 - IEEE International Conference on Communications","11 Aug 2022","2022","","","1","6","The Unmanned Aerial Vehicle (UAV) swarm has attracted widespread attention from both academia and industry. It has been widely adopted in disaster recovery, military communication, agricultural production, and industrial automation. In critical situations or places where communication infrastructure is lacking, deploying a UAV swarm network is a cost-effective solution. However, considering the high speed of UAV devices, designing an effective routing mechanism has been a challenging problem. In this paper, enlightened by the recent success of multi-agent reinforcement learning, we propose a multi-agent policy gradients-based UAV routing algorithm. We adopt a centralized training and decentralized executing framework, where a centralized training platform is implemented to guide the policy updating of each UAV node. Moreover, we introduce a counterfactual baseline scheme in our algorithm to improve the convergence speed. Extensive simulation results validate the effectiveness of the proposed algorithms compared to the state-of-the-art schemes.","1938-1883","978-1-5386-8347-7","10.1109/ICC45855.2022.9838808","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9838808","UAV;dynamic routing;multi-agent system;reinforcement learning","Training;Military communication;Heuristic algorithms;Simulation;Reinforcement learning;Production;Autonomous aerial vehicles","aerospace communication;autonomous aerial vehicles;cooperative communication;gradient methods;multi-agent systems;reinforcement learning;telecommunication network routing","UAV swarm networks;unmanned aerial vehicle swarm networks;multiagent reinforcement learning;cooperative reinforcement learning aided dynamic routing;multiagent policy gradient-based UAV routing algorithm;centralized training;decentralized execution;convergence speed;counterfactual baseline scheme","","2","","16","IEEE","11 Aug 2022","","","IEEE","IEEE Conferences"
"RMRL: Robot Navigation in Crowd Environments With Risk Map-Based Deep Reinforcement Learning","H. Yang; C. Yao; C. Liu; Q. Chen","Department of Electronics and Information Engineering, Tongji University, Shanghai, China; Department of Electronics and Information Engineering, Tongji University, Shanghai, China; Department of Electronics and Information Engineering, Tongji University, Shanghai, China; Department of Electronics and Information Engineering, Tongji University, Shanghai, China","IEEE Robotics and Automation Letters","20 Oct 2023","2023","8","12","7930","7937","Achieving safe and effective navigation in crowds is a crucial yet challenging problem. Recent work has mainly encoded the pedestrian-robot state pairs, which cannot fully capture the interactions among humans. Besides, existing work attempts to achieve “hard” collision avoidance, which may leave no feasible path to the robot in human-rich scenarios. We suppose that this can be addressed by introducing the local risk map and thus incorporate the risk map into the deep reinforcement learning architecture. The proposed map structure contains the crowd interaction states and geometric information. Meanwhile, a “soft” risk mapping of pedestrians is proposed to promote the robot to generate more humanlike motion patterns, and the riskaware dynamic window is designed to enhance the robot's obstacle avoidance ability. Experiments show that our method outperforms the baseline in terms of navigation performance and social attributes. Furthermore, we successfully validate the proposed policy through real-world environments.","2377-3766","","10.1109/LRA.2023.3322093","National Natural Science Foundation of China(grant numbers:62173248,62233013,62073245); Suzhou Key Industry Technological Innovation-Core Technology R&D Program(grant numbers:SGC2021035); Special funds for Jiangsu Science and Technology Plan(grant numbers:BE2022119); Suzhou Key Research and Development Project(grant numbers:SGC2021069); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10271559","Autonomous vehicle navigation;reinforcement learning;social HRI","Robots;Pedestrians;Collision avoidance;Navigation;Trajectory;Human-robot interaction;Behavioral sciences","","","","","","34","IEEE","4 Oct 2023","","","IEEE","IEEE Journals"
"Coarse-to-Fine UAV Target Tracking With Deep Reinforcement Learning","W. Zhang; K. Song; X. Rong; Y. Li","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China","IEEE Transactions on Automation Science and Engineering","4 Oct 2019","2019","16","4","1522","1530","The aspect ratio of a target changes frequently during an unmanned aerial vehicle (UAV) tracking task, which makes the aerial tracking very challenging. Traditional trackers struggle from such a problem as they mainly focus on the scale variation issue by maintaining a certain aspect ratio. In this paper, we propose a coarse-to-fine deep scheme to address the aspect ratio variation in UAV tracking. The coarse-tracker first produces an initial estimate for the target object, then a sequence of actions are learned to fine-tune the four boundaries of the bounding box. The coarse-tracker and the fine-tracker are designed to have different action spaces and operating target. The former dominates the entire bounding box and the latter focuses on the refinement of each boundary. They are trained jointly by sharing the perception network with an end-to-end reinforcement learning architecture. Experimental results on benchmark aerial data set prove that the proposed approach outperforms existing trackers and produces significant accuracy gains in dealing with the aspect ratio variation in UAV tracking.","1558-3783","","10.1109/TASE.2018.2877499","National Key Research and Development Plan of China(grant numbers:2017YFB1300205); National Natural Science Foundation of China(grant numbers:61573222); Major Research Program of Shandong Province(grant numbers:2018CXGC1503); Shandong University(grant numbers:2016JC014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8558518","Computer vision;reinforcement learning (RL);unmanned aerial vehicle (UAV);visual tracking","Target tracking;Unmanned aerial vehicles;Computer vision;Reinforcement learning","autonomous aerial vehicles;learning (artificial intelligence);object tracking;target tracking","scale variation issue;coarse-to-fine deep scheme;aspect ratio variation;UAV tracking;coarse-tracker;target object;fine-tracker;action spaces;bounding box;end-to-end reinforcement learning architecture;benchmark aerial data;coarse-to-fine UAV target tracking;deep reinforcement learning;unmanned aerial vehicle tracking task;aerial tracking","","83","","41","IEEE","4 Dec 2018","","","IEEE","IEEE Journals"
"Real-Time Adaptive Control of a Flexible Manipulator Using Reinforcement Learning","S. K. Pradhan; B. Subudhi","Department of Electrical Engineering, National Institute of Technology, Rourkela, Orissa, India; Department of Electrical Engineering, National Institute of Technology, Rourkela, Orissa, India","IEEE Transactions on Automation Science and Engineering","3 Apr 2012","2012","9","2","237","249","This paper exploits reinforcement learning (RL) for developing real-time adaptive control of tip trajectory and deflection of a two-link flexible manipulator handling variable payloads. This proposed adaptive controller consists of a proportional derivative (PD) tracking loop and an actor-critic-based RL loop that adapts the actor and critic weights in response to payload variations while suppressing the tip deflection and tracking the desired trajectory. The actor-critic-based RL loop uses a recursive least square (RLS)-based temporal difference (TD) learning with eligibility trace and an adaptive memory to estimate the critic weights and a gradient-based estimator for estimating actor weights. Tip trajectory tracking and suppression of tip deflection performances of the proposed RL-based adaptive controller (RLAC) are compared with that of a nonlinear regression-based direct adaptive controller (DAC) and a fuzzy learning-based adaptive controller (FLAC). Simulation and experimental results envisage that the RLAC outperforms both the DAC and FLAC.","1558-3783","","10.1109/TASE.2012.2189004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165676","Adaptive control;flexible-link manipulator;reinforcement learning;tip trajectory tracking","Trajectory;Payloads;Manipulators;Vectors;Adaptive control;Real-time systems;Dynamics","adaptive control;flexible manipulators;fuzzy control;gradient methods;learning systems;least squares approximations;nonlinear control systems;PD control;position control;recursive estimation;regression analysis","real-time adaptive control;flexible manipulator;tip deflection suppression;proportional derivative tracking loop;PD tracking loop;actor-critic-based reinforcement learning loop;recursive least square-based temporal difference learning;eligibility trace;adaptive memory;payload variations;actor weights;critic weights;gradient-based estimator;tip trajectory tracking;nonlinear regression-based direct adaptive controller;fuzzy learning-based adaptive controller;distributed flexibility;complex flexible space shuttle system","","78","","12","IEEE","7 Mar 2012","","","IEEE","IEEE Journals"
"Adaptive Video Transmission Control System Based on Reinforcement Learning Approach Over Heterogeneous Networks","B. Cheng; J. Yang; S. Wang; J. Chen","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Automation Science and Engineering","17 Jul 2015","2015","12","3","1104","1113","Video may pass through various types of heterogeneous networks during the process of transmission, which has adverse impacts on the real-time video quality. Traditional methods focus on how to compress videos based on the video flow without considering the real-time network information. This paper presents an adaptive method that combines video encoding and the video transmission control system over heterogeneous networks. This method includes the following steps: first, to collect and standardize the real-time information describing the network and the video, then to assess the video quality and calculate the video coding rate based on the standardized information, and then to process the encoded compression of the video according to the calculated coding rate and transfer the compressed video. The experiments show that there is a significant improvement for the quality of real-time videos transmission without changing the existing network, particularly the core equipment. Our solution is easy to deploy and implement quickly and may help to extensively ensure video quality for normal users.","1558-3783","","10.1109/TASE.2014.2387212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7015599","Adaptive;heterogeneous networks;neural networks;reinforcement learning;video transmission control","Streaming media;Video coding;Real-time systems;Video recording;Quality assessment;Encoding;Learning (artificial intelligence)","data compression;learning (artificial intelligence);neural nets;video coding","adaptive video transmission control system;reinforcement learning approach;heterogeneous networks;video encoding;video quality;video coding rate;encoded video compression","","20","","25","IEEE","20 Jan 2015","","","IEEE","IEEE Journals"
"Scalable Autonomous Separation Assurance With Heterogeneous Multi-Agent Reinforcement Learning","M. Brittain; P. Wei","Department of Aerospace Engineering, Iowa State University, Ames, IA, USA; Department of Mechanical and Aerospace Engineering, George Washington University, Washington, DC, USA","IEEE Transactions on Automation Science and Engineering","13 Oct 2022","2022","19","4","2837","2848","In this article, a scalable autonomous separation assurance framework is proposed for high-density en route airspace sectors with heterogeneous aircraft objectives. To handle the complex dynamic decision making under uncertainty, multi-agent reinforcement learning is used in a decentralized approach with each aircraft being represented as an agent. Based on this, each agent locally solves the separation assurance problem, allowing the framework to scale to a large number of aircraft. In addition, each agent has the ability to learn the intention of the intruder aircraft, which is essential in environments with heterogeneous agents. Numerical experiments are performed in a real-time air traffic simulator. The results demonstrate that the proposed framework is able to effectively ensure the safe separation of heterogeneous agents, while also optimizing the intrinsic agent objectives in high-density en route airspace sectors. In addition, the efficiency of the proposed framework is demonstrated and shown to provide real-time decision making for separation assurance. Note to Practitioners—In commercial aviation, the workload of human air traffic controllers increases with the growth of air traffic density. Robust decision making systems to augment human air traffic controllers allows for increased air traffic without increased workload, resulting in a safer airspace environment. In addition, advanced air mobility (AAM) is concerned with low-altitude airspace operations with both human and autonomous pilots. In this environment, autonomous real-time separation assurance systems are required. Most traditional separation assurance approaches fail to handle stochastic and high-density environments, rendering them inapplicable to future high-density traditional airspace and the envisioned low-altitude AAM operations. Therefore, it is important to study decentralized approaches that can place the separation assurance problem as an intrinsic objective of each aircraft to ensure cooperation in high-density airspace. With this consideration, a scalable, decentralized autonomous separation assurance framework capable of handling heterogeneous agents is proposed in this article. This framework is able to perceive the current air traffic environment and select speed advisories to ensure safe separation requirements, while balancing intrinsic objectives such as minimizing delay. While one limitation of multi-agent reinforcement learning is the long training time, this article demonstrates how the framework also can leverage modern computing clusters to significantly reduce training time without sacrificing performance.","1558-3783","","10.1109/TASE.2022.3151607","National Science Foundation(grant numbers:1718420); NASA Iowa Space Grant(grant numbers:NNX16AL88H); NVIDIA GPU Grant Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9717997","Multi-agent reinforcement learning;separation assurance;air traffic management","Aircraft;Reinforcement learning;Decision making;Real-time systems;Multi-agent systems;Air traffic control;Scalability","air traffic;air traffic control;aircraft;decision making;learning (artificial intelligence);multi-agent systems","heterogeneous aircraft objectives;complex dynamic decision making;decentralized approach;separation assurance problem;intruder aircraft;heterogeneous agents;real-time air traffic simulator;intrinsic agent objectives;route airspace sectors;real-time decision;human air traffic controllers increases;air traffic density;increased air traffic;safer airspace environment;advanced air mobility;low-altitude airspace operations;human pilots;autonomous pilots;real-time separation assurance systems;traditional separation assurance approaches;high-density environments;future high-density traditional airspace;intrinsic objective;high-density airspace;scalable separation assurance framework;decentralized autonomous separation assurance framework;current air traffic environment;safe separation requirements;heterogeneous multiagent reinforcement learning;scalable autonomous separation assurance framework","","10","","57","IEEE","21 Feb 2022","","","IEEE","IEEE Journals"
"Direct Thermal Load Control in Active Distribution Networks Based on Deep Reinforcement Learning","Z. Chen; H. Liu; W. Wu; Y. Li; J. Jing; T. Da","Department of Electrical Engineering, Tsinghua University, Beijing, China; Department of Electrical Engineering, Tsinghua University, Beijing, China; Department of Electrical Engineering, Tsinghua University, Beijing, China; Power Automation Department, China Electric Power Research Institute, Nanjing, China; Dispatching Control Center State Grid, Jiangsu Electric Power Co., Ltd., Nanjing, China; Zhenjiang Power Supply Branch Power Dispatching Control Center State Grid, Jiangsu Electric Power Co., Ltd., Zhenjiang, China","2021 IEEE 5th Conference on Energy Internet and Energy System Integration (EI2)","25 Feb 2022","2021","","","3184","3189","With the growth of electrical power consumption in the tertiary industry, the proportion of building thermal load is increasing significantly. On the demand side, load aggregators attempt to realize peak shifting by direct active power control of heating ventilation and air-conditioning (HVAC) devices. However, the difficulty in establishing accurate physical models of HVAC equipment has limited the performance of model-based optimization methods. Recently, deep reinforcement learning (DRL) has been widely studied and trusted to achieve optimal decisions without priori knowledge on the environment. In this paper, a DRL-based building thermal load control method is proposed to provide the auxiliary service of load shifting under the premise of thermal comfort. A resistor-capacitor model is introduced to describe the thermodynamics of exterior protection structure, and the regulation behavior is formulated into a Markov decision process. Then, an augmented algorithm called improved deep deterministic policy gradient (iDDPG) is proposed with high sample efficiency. Comprehensive numerical simulations are conducted to verify the feasibility and superiority of the proposed method. The results have supported the effectiveness of iDDPG in maximizing thermal comfort, optimizing peak shifting effect and reducing network loss.","","978-1-6654-3425-6","10.1109/EI252483.2021.9713068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9713068","thermal load control;reinforcement learning;building cluster;demand-side power control","HVAC;Atmospheric modeling;Power control;Buildings;Reinforcement learning;Numerical simulation;Numerical models","building management systems;buildings (structures);energy conservation;HVAC;load regulation;Markov processes;optimisation;power consumption;power control;power distribution control;power engineering computing;reinforcement learning","resistor-capacitor model;Markov decision process;deep deterministic policy gradient;thermal comfort;peak shifting effect;reducing network loss;direct thermal load control;active distribution networks;deep reinforcement learning;electrical power consumption;tertiary industry;load aggregators;direct active power control;heating ventilation;air-conditioning;accurate physical models;HVAC equipment;model-based optimization methods;optimal decisions;priori knowledge;DRL-based;thermal load control method;load shifting","","","","15","IEEE","25 Feb 2022","","","IEEE","IEEE Conferences"
"Non-Technical Loss Detection Using Deep Reinforcement Learning for Feature Cost Efficiency and Imbalanced Dataset","J. Lee; Y. G. Sun; I. Sim; S. H. Kim; D. I. Kim; J. Y. Kim","Department of Electronic Convergence Engineering, Kwangwoon University, Seoul, South Korea; Department of Electronic Convergence Engineering, Kwangwoon University, Seoul, South Korea; Department of Electronic Convergence Engineering, Kwangwoon University, Seoul, South Korea; Department of Electronic Convergence Engineering, Kwangwoon University, Seoul, South Korea; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electronic Convergence Engineering, Kwangwoon University, Seoul, South Korea","IEEE Access","15 Mar 2022","2022","10","","27084","27095","One of the problems of the electricity grid system is electricity loss due to energy theft, which is known as non-technical loss (NTL). The sustainability and stability of the grid system are threatened by the unexpected electricity losses. Energy theft detection based on data analysis is one of the solutions to alleviate the drawbacks of NTL. The main problem of data-based NTL detection is that collected electricity usage dataset is imbalanced. In this paper, we approach the NTL detection problem using deep reinforcement learning (DRL) to solve the data imbalanced problem of NTL. The advantage of the proposed method is that the classification method is adopted to use the partial input features without pre-processing method for input feature selection. Moreover, extra pre-processing steps to balance the dataset are unnecessary to detect NTL compared to the conventional NTL detection algorithms. From the simulation results, the proposed method provides better performances compared to the conventional algorithms under various simulation environments.","2169-3536","","10.1109/ACCESS.2022.3156948","Human Resources Program in Energy Technology of the Korea Institute of Energy Technology Evaluation and Planning (KETEP), Granted Financial Resource from the Ministry of Trade, Industry &Energy, Republic of Korea(grant numbers:20194010201830); Ministry of Science and ICT (MSIT), South Korea, under the Information Technology Research Center (ITRC) Support Program Supervised by the Institute for Information & Communications Technology Promotion (IITP)(grant numbers:IITP-2022-0-01846); Kwangwoon University, in 2021; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9729226","Deep reinforcement learning;non-technical loss (NTL);energy theft;feature cost efficiency;data imbalanced problem","Neural networks;Mathematical models;Electricity supply industry;Data models;Classification algorithms;Power system stability;Reinforcement learning;Deep learning;Cost benefit analysis;Energy consumption","data analysis;data mining;deep learning (artificial intelligence);feature selection;learning (artificial intelligence);pattern classification;power engineering computing;power system protection","imbalanced dataset;electricity grid system;electricity loss;sustainability;stability;unexpected electricity losses;energy theft detection;data analysis;data-based NTL detection;collected electricity usage dataset;NTL detection problem;deep reinforcement learning;partial input features;pre-processing method;input feature selection;extra pre-processing steps;conventional NTL detection algorithms;nontechnical loss detection;feature cost efficiency","","6","","30","CCBY","4 Mar 2022","","","IEEE","IEEE Journals"
"Gaussian Processes and Reinforcement Learning for Identification and Control of an Autonomous Blimp","J. Ko; D. J. Klein; D. Fox; D. Haehnel","Department of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Department of Aeronautics & Astronautics, University of Washington, Seattle, WA, USA; Department of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Intel Research Seattle, Seattle, WA, USA","Proceedings 2007 IEEE International Conference on Robotics and Automation","21 May 2007","2007","","","742","747","Blimps are a promising platform for aerial robotics and have been studied extensively for this purpose. Unlike other aerial vehicles, blimps are relatively safe and also possess the ability to loiter for long periods. These advantages, however, have been difficult to exploit because blimp dynamics are complex and inherently non-linear. The classical approach to system modeling represents the system as an ordinary differential equation (ODE) based on Newtonian principles. A more recent modeling approach is based on representing state transitions as a Gaussian process (GP). In this paper, we present a general technique for system identification that combines these two modeling approaches into a single formulation. This is done by training a Gaussian process on the residual between the non-linear model and ground truth training data. The result is a GP-enhanced model that provides an estimate of uncertainty in addition to giving better state predictions than either ODE or GP alone. We show how the GP-enhanced model can be used in conjunction with reinforcement learning to generate a blimp controller that is superior to those learned with ODE or GP models alone.","1050-4729","1-4244-0601-3","10.1109/ROBOT.2007.363075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4209179","","Gaussian processes;Learning;Robots;Remotely operated vehicles;Vehicle safety;Nonlinear dynamical systems;Vehicle dynamics;Modeling;Differential equations;System identification","aerospace robotics;differential equations;Gaussian processes;identification;learning (artificial intelligence);mobile robots;nonlinear control systems;robot dynamics","Gaussian process;reinforcement learning;autonomous blimp control;aerial robotics;aerial vehicles;blimp dynamics;nonlinear dynamics;ordinary differential equation;Newtonian principles;system identification;uncertainty estimation","","88","","16","IEEE","21 May 2007","","","IEEE","IEEE Conferences"
