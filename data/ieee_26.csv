"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"An Optimal Vehicle Speed Planning Algorithm for Regenerative Braking at Traffic Lights Intersections based on Reinforcement Learning","Y. Zhang; H. Xie; K. Song","School of Mechanical Engineering, Tianjin University, Tianjin, China; School of Mechanical Engineering, Tianjin University, Tianjin, China; School of Mechanical Engineering, Tianjin University, Tianjin, China","2020 4th CAA International Conference on Vehicular Control and Intelligence (CVCI)","9 Feb 2021","2020","","","193","198","For electric vehicle or hybrid electric vehicles, the regenerative braking is one of the important means to realize energy saving, for which braking ahead of a traffic light intersection is a representative scenario. The uncertainty in driver behavior and future traffic flow, however, make it challenging to achieve optimal dynamic energy recovery through conventional braking operation by drivers. Therefore, in this paper, an energy recovery optimization-oriented vehicle speed planning algorithm ahead of traffic lights intersection is proposed, for autonomous vehicle or driving assistance system. First, the reward function is designed, taking the energy recovery amount, traffic efficiency and driving smoothness into consideration. Then, the information of traffic lights at intersections is obtained in advance through V2I (vehicle to infrastructure) communication. Finally, the q-table and neural network are trained in the framework of reinforcement learning, deriving optimal vehicle speed profile. Simulation results on a high-fidelity model show that the amount of recovered electrical energy using q-learning algorithm is 45.08% higher than that of uniform deceleration. The amount of electrical energy using DQN (Deep Q-network) algorithm is 2.24% higher than q-learning, showing to be a better candidate in terms of comprehensive optimality than q-learning.","","978-1-7281-8497-5","10.1109/CVCI51460.2020.9338590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9338590","Reinforcement learning;V2I;Traffic lights;Speed planning;New energy vehicle","Torque;Vehicle-to-infrastructure;Heuristic algorithms;Reinforcement learning;Planning;Optimization;Vehicles","braking;electric vehicles;hybrid electric vehicles;learning (artificial intelligence);optimisation;production engineering computing;regenerative braking;road traffic;road vehicles;traffic engineering computing","future traffic flow;optimal dynamic energy recovery;conventional braking operation;energy recovery optimization-oriented vehicle speed planning algorithm;traffic lights intersection;autonomous vehicle;energy recovery amount;traffic efficiency;driving smoothness;reinforcement learning;optimal vehicle speed profile;recovered electrical energy;q-learning algorithm;optimal vehicle speed planning algorithm;regenerative braking;electric vehicle;hybrid electric vehicles;traffic light intersection","","4","","12","IEEE","9 Feb 2021","","","IEEE","IEEE Conferences"
"Batch reinforcement learning on the industrial benchmark: First experiences","D. Hein; S. Udluft; M. Tokic; A. Hentschel; T. A. Runkler; V. Sterzing","Siemens AG, Corporate Technology, Munich, Germany; Siemens AG, Corporate Technology, Munich, Germany; Siemens AG, Corporate Technology, Munich, Germany; Siemens AG, Corporate Technology, Munich, Germany; Siemens AG, Corporate Technology, Munich, Germany; Siemens AG, Corporate Technology, Munich, Germany","2017 International Joint Conference on Neural Networks (IJCNN)","3 Jul 2017","2017","","","4214","4221","The Particle Swarm Optimization Policy (PSO-P) has been recently introduced and proven to produce remarkable results on interacting with academic reinforcement learning benchmarks in an off-policy, batch-based setting. To further investigate the properties and feasibility on real-world applications, this paper investigates PSO-P on the so-called Industrial Benchmark (IB), a novel reinforcement learning (RL) benchmark that aims at being realistic by including a variety of aspects found in industrial applications, such as continuous state and action spaces, a high dimensional, partially observable state space, delayed effects, and complex stochasticity. The experimental results of PSO-P on IB are compared to results of closed-form control policies derived from the model-based Recurrent Control Neural Network (RCNN) and the model-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not only of interest for academic benchmarks, but also for real-world industrial applications, since it also yielded the best performing policy in our IB setting. Compared to other well established RL techniques, PSO-P produced outstanding results in performance and robustness, requiring only a relatively low amount of effort in finding adequate parameters or making complex design decisions.","2161-4407","978-1-5090-6182-2","10.1109/IJCNN.2017.7966389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966389","","Training;Benchmark testing;Fatigue;Computational modeling;Data models;Markov processes;Predictive models","learning (artificial intelligence);particle swarm optimisation;production engineering computing","particle swarm optimization policy;academic reinforcement learning benchmarks;off-policy batch-based setting;PSO-P;industrial benchmark;industrial applications;IB setting;complex design decision making","","4","","27","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Resource Management for DNN Inference in IIoT","W. Zhang; D. Yang; H. Peng; W. Wu; W. Quan; H. Zhang; X. S. Shen","School of Electronic and Information Engineering, Beijing Jiaotong University, China; School of Electronic and Information Engineering, Beijing Jiaotong University, China; Department of Electrical and Computer Engineering, University of Waterloo, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Canada; School of Electronic and Information Engineering, Beijing Jiaotong University, China; School of Electronic and Information Engineering, Beijing Jiaotong University, China; Department of Electrical and Computer Engineering, University of Waterloo, Canada","GLOBECOM 2020 - 2020 IEEE Global Communications Conference","25 Jan 2021","2020","","","1","6","In this paper, we investigate the joint task assignment and resource allocation for deep neural network (DNN) inference in the device-edge-cloud based industrial Internet of things (IIoT) networks. To efficiently orchestrate the limited spectrum and computing resources in IIoT networks for massive DNN inference tasks, a resource management problem is formulated with the objective of maximizing the average inference accuracy while satisfying the quality-of-service of DNN inference tasks. Considering the strict delay requirements of inference tasks, we transform the formulated problem into a Markov decision process, and propose a deep deterministic policy gradient based learning algorithm to obtain the solution rapidly. Simulation results show that the proposed algorithm can achieve high average inference accuracy.","2576-6813","978-1-7281-8298-8","10.1109/GLOBECOM42002.2020.9322223","National Natural Science Foundation of China(grant numbers:61771040); Natural Sciences and Engineering Research Council of Canada(grant numbers:10.13039/501100000038); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9322223","DNN inference;IIoT;resource management;deep deterministic policy gradient","Task analysis;Industrial Internet of Things;Computational modeling;Biological system modeling;Delays;Resource management;Servers","cloud computing;gradient methods;industrial engineering;Internet of Things;learning (artificial intelligence);Markov processes;neural nets;production engineering computing;resource allocation","deep reinforcement;joint task assignment;resource allocation;deep neural network inference;computing resources;IIoT networks;massive DNN inference tasks;resource management problem;deep deterministic policy gradient;high average inference accuracy;device-edge-cloud based industrial Internet of Things network","","4","","17","IEEE","25 Jan 2021","","","IEEE","IEEE Conferences"
"Skill-based Multi-objective Reinforcement Learning of Industrial Robot Tasks with Planning and Knowledge Integration","M. Mayr; F. Ahmad; K. Chatzilygeroudis; L. Nardi; V. Krueger","Department of Computer Science, Faculty of Engineering (LTH), Lund University, Lund, SE, Sweden; Department of Computer Science, Faculty of Engineering (LTH), Lund University, Lund, SE, Sweden; Computer Engineering and Informatics Department (CEID), University of Patras, Greece; Department of Computer Science, Faculty of Engineering (LTH), Lund University, Lund, SE, Sweden; Department of Computer Science, Faculty of Engineering (LTH), Lund University, Lund, SE, Sweden","2022 IEEE International Conference on Robotics and Biomimetics (ROBIO)","18 Jan 2023","2022","","","1995","2002","In modern industrial settings with small batch sizes it should be easy to set up a robot system for a new task. Strategies exist, e.g. the use of skills, but when it comes to handling forces and torques, these systems often fall short. We introduce an approach that provides a combination of task-level planning with targeted learning of scenario-specific parameters for skill-based systems. We propose the following pipeline: the user provides a task goal in the planning language PDDL, then a plan (i.e., a sequence of skills) is generated and the learnable parameters of the skills are automatically identified, and, finally, an operator chooses reward functions and parameters for the learning process. Two aspects of our methodology are critical: (a) learning is tightly integrated with a knowledge framework to support symbolic planning and to provide priors for learning, (b) using multi-objective optimization. This can help to balance key performance indicators (KPIs) such as safety and task performance since they can often affect each other. We adopt a multi-objective Bayesian optimization approach and learn entirely in simulation. We demonstrate the efficacy and versatility of our approach by learning skill parameters for two different contact-rich tasks. We show their successful execution on a real 7-DOF KUKA-iiwa manipulator and outperform the manual parameterization by human robot operators.","","978-1-6654-8109-0","10.1109/ROBIO55434.2022.10011996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011996","","Service robots;Biomimetics;Pipelines;Key performance indicator;Reinforcement learning;Manipulators;Planning","Bayes methods;control engineering computing;industrial manipulators;optimisation;planning (artificial intelligence);production engineering computing;reinforcement learning","7-DOF KUKA-iiwa manipulator;contact-rich tasks;human robot operators;industrial robot tasks;key performance indicators;knowledge integration;learning skill parameters;multiobjective Bayesian optimization;PDDL;planning language;reward functions;robot system;scenario-specific parameters;skill-based multiobjective reinforcement learning;skill-based systems;symbolic planning;targeted learning;task goal;task-level planning","","3","","44","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"A Policy Gradient Based Reinforcement Learning Method for Supply Chain Management","Y. Hachaïchi; Y. Chemingui; M. Affes","Research Laboratory Smart Electricity & ICT, SE& ICT ENICarthage, Tunisia; Ecole Polytechnique de Tunisie, Universit de Carthage, Tunisia; Ecole Polytechnique de Tunisie, Universit de Carthage, Tunisia","2020 4th International Conference on Advanced Systems and Emergent Technologies (IC_ASET)","20 Jan 2021","2020","","","135","140","Technological advances of the recent decades have significantly affected the business world, the retail business in particular. Retailers need to innovate to maintain a competitive edge over competitors and ensure business sustainability. Therefore Research and Development is a crucial component of businesses growth. Intuition based approaches are replaced by supply chain computerized solutions such as inventory management, warehousing, allocation and replenishment. This paper aims at building a reinforcement learning agent capable of placing optimal orders for the sake of constructing a replenishment plan for next period. The goal is to develop a novel method of inventory replenishment. We base the developed module on the recent breakthroughs of reinforcement learning research in using deep neural networks for control. We compare the classical RL methods to the recently introduced Proximal policy optimization algorithm. As far as we know, this is the first time PPO is used in Supply Chain Management.","","978-1-7281-6356-7","10.1109/IC_ASET49463.2020.9318258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9318258","Reinforcement Learning;PPO;Deep Learning;Supply Chain Management;Replenishment","Reinforcement learning;Computational modeling;Transportation;Mathematical model;Delays;Buffer storage;Supply chains","deep learning (artificial intelligence);inventory management;optimisation;production engineering computing;retailing;supply chain management","policy gradient based reinforcement learning method;supply chain management;retailers;inventory replenishment method;classical RL methods;proximal policy optimization algorithm;retail business sustainability;research and development;deep neural networks;PPO","","3","","21","IEEE","20 Jan 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Fed-Batch Optimization with Reaction Surrogate Model","Y. Ma; Z. Wang; I. Castillo; R. Rendall; R. Bindlish; B. Ashcraft; D. Bentley; M. G. Benton; J. A. Romagnoli; L. H. Chiang","Chemical Engineering Department, Louisiana State University, Baton Rouge, LA, USA; Dow Inc., Lake Jackson, TX, USA; Dow Inc., Lake Jackson, TX, USA; Dow Inc., Lake Jackson, TX, USA; Dow Inc., Lake Jackson, TX, USA; Dow Inc., Lake Jackson, TX, USA; Dow Inc., Lake Jackson, TX, USA; Chemical Engineering Department, Louisiana State University, Baton Rouge, LA, USA; Chemical Engineering Department, Louisiana State University, Baton Rouge, LA, USA; Dow Inc., Lake Jackson, TX, USA","2021 American Control Conference (ACC)","28 Jul 2021","2021","","","2581","2586","In this paper, we implement a framework which combines Reinforcement Learning (RL) based reaction optimization with first principle model and plant historical data of the reaction system. Here we employ a Long-Short-Term-Memory (LSTM) network for reaction surrogate modeling, and Proximal Policy Optimization (PPO) algorithm for the fed-batch optimization. The proposed reaction surrogate model combines simulation data with real plant data for an accurate and computationally efficient reaction simulation. Based on the surrogate model, the RL optimization result suggests maintaining an increased temperature setpoint and high reactant feed flow to maximize the product profits. The simulation results by following the RL profile suggests an estimate of 6.4% improvement of the product profits.","2378-5861","978-1-6654-4197-1","10.23919/ACC50511.2021.9482807","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9482807","Deep Reinforcement Learning;Surrogate Modeling;fed-batch optimization;Proximal Policy Optimization;LSTM modeling","Computational modeling;Simulation;Reinforcement learning;Data models;Stability analysis;Real-time systems;Computational efficiency","batch processing (industrial);chemical engineering;learning (artificial intelligence);optimisation;production engineering computing;recurrent neural nets","LSTM;PPO;reinforcement learning-based fed-batch optimization;computationally efficient reaction simulation;accurate reaction simulation;proximal policy optimization algorithm;reaction surrogate modeling;long-short-term-memory network;plant historical data;principle model;reaction surrogate model;RL optimization result","","3","","21","","28 Jul 2021","","","IEEE","IEEE Conferences"
"Using case-based reasoning as a reinforcement learning framework for optimisation with changing criteria","Dajun Zeng; K. Sycara","The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA","Proceedings of 7th IEEE International Conference on Tools with Artificial Intelligence","6 Aug 2002","1995","","","56","62","Practical optimization problems such as job-shop scheduling often involve optimization criteria that change over time. Repair-based frameworks have been identified as flexible computational paradigms for difficult combinatorial optimization problems. Since the control problem of repair-based optimization is severe, reinforcement learning (RL) techniques can be potentially helpful. However, some of the fundamental assumptions made by traditional RL algorithms are not valid for repair-based optimization. Case-based reasoning compensates for some of the limitations of traditional RL approaches. We present a case-based reasoning RL approach, implemented in the C/sub A/B/sub I/NS system, for repair-based optimization. We chose job-shop scheduling as the testbed for our approach. Our experimental results show that C/sub A/B/sub I/NS is able to effectively solve problems with changing optimization criteria which are not known to the system and only exist implicitly in a extensional manner in the case base.","1082-3409","0-8186-7312-5","10.1109/TAI.1995.479378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=479378","","Learning;Problem-solving;Optimization methods;Robots;Processor scheduling;Testing;Artificial intelligence;Signal processing;Search methods;Design optimization","learning by example;case-based reasoning;optimisation;scheduling;production control;software agents","case-based reasoning;reinforcement learning framework;optimization problems;job-shop scheduling;optimization criteria;repair-based frameworks;flexible computational paradigms;combinatorial optimization;case-based reasoning RL approach;CABINS system","","2","","20","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Sequential Topology Attack of Supply Chain Networks Based on Reinforcement Learning","L. Zhang; J. Zhou; Y. Ma; L. Shen","School of Economics and Management, Nanjing University of Science and Technology, Nanjing, China; School of Economics and Management, Nanjing University of Science and Technology, Nanjing, China; School of Economics and Management, Nanjing University of Science and Technology, Nanjing, China; ETH Zürich, Future Resilient Systems Singapore-ETH Centre Singapore, Singapore","2022 International Conference on Cyber-Physical Social Intelligence (ICCSI)","16 Dec 2022","2022","","","744","749","The robustness of supply chain networks (SCNs) against sequential topology attacks is significant for maintaining firm relationships and activities. Although SCNs have experienced many emergencies demonstrating that mixed failures exacerbate the impact of cascading failures, existing studies of sequential attacks rarely consider the influence of mixed failure modes on cascading failures. In this paper, a reinforcement learning (RL)-based sequential attack strategy is applied to SCNs with cascading failures that consider mixed failure modes. To solve the large state space search problem in SCNs, a deep Q-network (DQN) optimization framework combining deep neural networks (DNNs) and RL is proposed to extract features of state space. Then, it is compared with the traditional random-based, degree-based, and load-based sequential attack strategies. Simulation results on Barabasi-Albert (BA), Erdos-Renyi (ER), and Watts-Strogatz (WS) networks show that the proposed RL-based sequential attack strategy outperforms three existing sequential attack strategies. It can trigger cascading failures with greater influence. This work provides insights for effectively reducing failure propagation and improving the robustness of SCNs.","","978-1-6654-9835-7","10.1109/ICCSI55536.2022.9970706","National Natural Science Foundation of China(grant numbers:71931006,72101116,71871119,72001006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9970706","supply chain networks;robustness;reinforcement learning;sequential attacks","Network topology;Simulation;Power system protection;Supply chains;Reinforcement learning;Search problems;Robustness","deep learning (artificial intelligence);graph theory;network theory (graphs);neural nets;production engineering computing;search problems;supply chain management;topology","Barabasi-Albert networks;deep neural networks;deep Q-network optimization framework;DNN;DQN;Erdos-Renyi networks;firm relationships;mixed failure modes;reinforcement learning;SCN;sequential topology attack;sequential topology attacks;supply chain networks;Watts-Strogatz networks","","2","","23","IEEE","16 Dec 2022","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning-based Joint Caching and Computing Edge Service Placement for Sensing-Data-Driven IIoT Applications","Y. Chen; Y. Sun; B. Yang; T. Taleb","School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; School of Computer and Information Engineering, Chuzhou University, Chuzhou, China; Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland","ICC 2022 - IEEE International Conference on Communications","11 Aug 2022","2022","","","4287","4292","Edge computing (EC) is a promising technology to support a variety of performance-sensitive intelligent applications, especially in the Industrial Internet of Things (IIoT). The sensing-data-driven applications whose task processing requires sensing data from various sensors are typical applications in IIoT systems. The placement of caching and computing edge service functions for such applications is vital to ensure system performance and resource utilization in EC-enabled IIoT systems. Therefore, this paper investigates the joint caching and computing edge service placement (JCCESP) for multiple sensing-data-driven IIoT applications in an EC-enabled IIoT system. The JCCESP problem is formulated as a Markov Decision Process (MDP). Then, a deep reinforcement learning (DRL)-based approach is proposed to address the challenges like limited prior knowledge and the heterogeneity of such IIoT systems. Under such an approach, the policy network of the DRL agent is constructed based on an encoder-decoder model to tackle various applications requiring different numbers of service functions. A REINFORCE-based method is further employed to train the policy network. Simulation results indicate that the performances achieved by our proposed approach can converge after training and are significantly superior to benchmarks.","1938-1883","978-1-5386-8347-7","10.1109/ICC45855.2022.9838832","National Natural Science Foundation of China; T and E; Fundamental Research Funds for the Central Universities; Chinese Government Scholarship; Academy of Finland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9838832","Edge computing;IIoT;sensing-data-driven application;joint service placement;and DRL","Training;System performance;Simulation;Reinforcement learning;Markov processes;Sensor systems and applications;Resource management","cache storage;cloud computing;data fusion;deep learning (artificial intelligence);Internet of Things;Markov processes;production engineering computing;reinforcement learning","edge computing;performance-sensitive intelligent applications;system performance;resource utilization;EC-enabled IIoT systems;Markov decision process;REINFORCE-based method;deep reinforcement learning;sensing data-driven IIoT applications;joint caching and computing edge service placement;JCCESP;encoder-decoder model;DRL;Industrial Internet of Things","","2","","13","IEEE","11 Aug 2022","","","IEEE","IEEE Conferences"
"Remaining Useful Life Estimation in Prognostics Using Deep Reinforcement Learning","Q. Hu; Y. Zhao; Y. Wang; P. Peng; L. Ren","College of Energy and Power Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Energy and Power Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Energy and Power Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Energy and Power Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Energy and Power Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Access","6 Apr 2023","2023","11","","32919","32934","In modern industrial systems, condition-based maintenance (CBM) has been wildly adopted as an efficient maintenance strategy. Prognostics, as a key enabler of CBM, involves the kernel task of estimating the remaining useful life (RUL) for engineered systems. Much research in recent years has focused on developing new machine learning (ML) based approaches for RUL estimation. A variety of ML algorithms have been employed in these approaches. However, there was no research on applying deep reinforcement learning (DRL) to RUL estimation. To fill this research gap, a novel DRL based prognostic approach is proposed for RUL estimation in this paper. In the proposed approach, the conventional RUL estimation task is first formulated into a Markov decision process (MDP) model. Then an advanced DRL algorithm is employed to learn the optimal RUL estimation policy from this MDP environment. The effectiveness and superiority of the proposed approach are demonstrated through a case study on turbofan engines in C-MAPSS dataset. Compared to other approaches, the proposed approach obtains superior performance on all four sub-datasets of C-MAPSS dataset. What is more, on the most complicated sub-datasets FD002 and FD004, the RMSE metric is improved by 14.4% and 7.81%, and the score metric is improved by 3.7% and 48.79%, respectively.","2169-3536","","10.1109/ACCESS.2023.3263196","National Science and Technology Major Project(grant numbers:J2019-I-0010-0010); Fundamental Research Funds for the Central Universities(grant numbers:NS2022027); Science Center for Gas Turbine Project(grant numbers:P2022-B-V-002-001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10087275","Condition-based maintenance;prognostics;remaining useful life estimation;Markov decision process;deep reinforcement learning","Estimation;Reinforcement learning;Maintenance engineering;Convolutional neural networks;Deep learning;Hidden Markov models;Supervised learning","condition monitoring;deep learning (artificial intelligence);jet engines;maintenance engineering;Markov processes;production engineering computing;remaining life assessment","condition-based maintenance;deep reinforcement learning;industrial systems;machine learning based approaches;Markov decision process model;MDP environment;ML algorithms;optimal RUL estimation policy;remaining useful life;turbofan engines","","1","","43","CCBYNCND","29 Mar 2023","","","IEEE","IEEE Journals"
"A Deep Reinforcement Learning Approach to Flexible Job Shop Scheduling","Z. Zeng; X. Li; C. Bai","College of Informatics, Huazhong Agricultural University, China; College of Informatics, Huazhong Agricultural University, China; College of Informatics, Huazhong Agricultural University, China","2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","18 Nov 2022","2022","","","884","890","F1exible job shop scheduling (FJSP) is one of the most important problems in the domain of machining process optimization. This paper proposes a deep reinforcement learning approach to resolve the FJSP. In the approach, the FJSP is formulated as a Markov decision process where disjunctive graph is used to represent the state, operation set and machine allocation are used as the actions, the reward function is established based on the optimization objective (i.e. makespan). To obtain the embedding representation of the disjunctive graph of the FJSP, the corresponding graph neural network (GNN) is used to extract the state features. The multi-layer perceptron (MLP) decision network and scheduling rules cooperate to achieve the selection of actions (i.e. operations and machines). The multi-threaded asynchronous advantage actor-critic (A3C) algorithm is employed to optimize the model parameters to shorten the training time. The approach has been tested on the benchmarks. The results prove that this approach is superior to scheduling rules and meta-heuristic algorithms in results and computing time respectively.","2577-1655","978-1-6654-5258-8","10.1109/SMC53654.2022.9945107","Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9945107","Deep reinforcement learning;graph isomorphic network;actor_critic algorithm;flexible job shop scheduling","Training;Job shop scheduling;Processor scheduling;Computational modeling;Heuristic algorithms;Metaheuristics;Reinforcement learning","deep learning (artificial intelligence);graph theory;job shop scheduling;Markov processes;multi-threading;multilayer perceptrons;production engineering computing;reinforcement learning","deep reinforcement learning approach;disjunctive graph;F1exible job shop scheduling;FJSP;flexible job shop scheduling;graph neural network;machine allocation;machining process optimization;Markov decision process;multithreaded asynchronous advantage actor-critic algorithm;optimization objective;reward function;scheduling rules;state features","","1","","37","IEEE","18 Nov 2022","","","IEEE","IEEE Conferences"
"Studies on the less-used actions exploration problem of a rationing algorithm based on reinforcement learning","M. Stoica; G. A. Calangiu; F. Sisak","Electrical Engineering Department, Transilvania University of Brasov, Brasov, Romania; Electrical Engineering Department, Transilvania University of Brasov, Brasov, Romania; Electrical Engineering Department, Transilvania University of Brasov, Brasov, Romania","2011 15th IEEE International Conference on Intelligent Engineering Systems","14 Jul 2011","2011","","","341","346","Programming by demonstration is an interesting subject in the field of robotics and it is developing more and more in the direction of robots for services and humanoid robots. Programming by demonstration is much less researched when it comes to industrial robots. One of the reasons is that an industrial robot has to act in a precise and certain manner. However, extending research regarding programming by demonstration to the field of industrial robots could lead to the creation of intelligent systems where the industrial robot could be programmed in an easier way. The goal of our research is to develop an intelligent system useful for industrial robot programming by demonstration. The reasoning algorithms are the mechanisms which offer flexibility to the proposed system. We have focused our research on the creation of a reasoning algorithm based on artificial neural networks. Because the results of this algorithm were not satisfying we have switched our focus to the development of a reasoning algorithm based on reinforcement learning. The algorithm is based on the idea that marks can be assigned to each possible action whenever the robot is in an unknown state. The exploration of less-used actions plays also an important role in the case the robot must to take a decision. Based on the marks and on the exploration feature of the algorithm the robot updates its behaviour. This paper presents a description and some studies on less-used actions exploration problem of the algorithm. Some chapters of the paper will deal with the problems implementing the algorithm, the conducted experiments in terms of exploration feature of the algorithm and the results obtained. The analysis of the results and the characteristics of the algorithm in terms of less-used actions exploration are also discussed in this paper.","1543-9259","978-1-4244-8956-5","10.1109/INES.2011.5954770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954770","","Training;Learning;Programming;Classification algorithms;Service robots;Software","automatic programming;industrial robots;inference mechanisms;learning (artificial intelligence);production engineering computing;robot programming","less-used actions exploration problem;rationing algorithm;reinforcement learning;programming by demonstration;intelligent system;industrial robot;robot programming;reasoning algorithms","","1","","21","IEEE","14 Jul 2011","","","IEEE","IEEE Conferences"
"Towards Online Continuous Reinforcement Learning on Industrial Internet of Things","C. Qian; W. Yu; X. Liu; D. Griffith; N. Golmie","Towson University, USA; Towson University, USA; Towson University, USA; National Institute of Standards and Technology (NIST), USA; National Institute of Standards and Technology (NIST), USA","2021 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/IOP/SCI)","18 Nov 2021","2021","","","280","287","Training machine learning models, such as reinforcement learning models, require a significant investment of time, and a trained model can only work on a specific system in a specific environment. When the application scenario of reinforcement learning changes, or the application environment changes, the reinforcement learning model needs to be retrained. Thus, it is critical to design techniques that can reduce the overhead of retraining reinforcement learning models, enabling them adapt to constantly changing environments. In this paper, toward improving the performance of learning models in dynamic Industrial Internet of Things (IIoT), we propose an online continuous reinforcement learning strategy. In our process, when the retraining condition is triggered, our online continuous learning strategy will re-engage the training process and update the well-trained model. To evaluate the performance of our proposed approach, we categorize the entire application space for applying reinforcement learning to IIoT systems into four scenarios, namely, non-continuous learning without learning model sharing, non-continuous learning with learning model sharing, continuous learning without learning model sharing, and continuous learning without learning model sharing. For each scenario, we design a Q-learning based reinforcement learning algorithm. Via extensive evaluation, our results show that the online continuous reinforcement learning approach that we propose can significantly reduce the overhead of retraining the learning model, enabling the learning algorithm to quickly adapt to a changing environment.","","978-1-6654-1236-0","10.1109/SWC50871.2021.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9604405","Industrial Internet of Things;Online continuous learning;Reinforcement learning","Training;Adaptation models;Technological innovation;Smart cities;Reinforcement learning;Internet;Industrial Internet of Things","industrial engineering;Internet of Things;production engineering computing;reinforcement learning","training machine learning models;application environment changes;noncontinuous learning;model sharing;Q-learning based reinforcement learning algorithm;online continuous reinforcement learning approach;industrial Internet of Things;IIoT;retraining condition;training process","","1","","22","IEEE","18 Nov 2021","","","IEEE","IEEE Conferences"
"Reinforcement Learning With Model-Based Assistance for Shape Control in Sendzimir Rolling Mills","J. Park; B. Kim; S. Han","Department of Convergence IT Engineering, Pohang University of Science and Technology, Pohang, South Korea; Department of Convergence IT Engineering, Pohang University of Science and Technology, Pohang, South Korea; Department of Convergence IT Engineering, Pohang University of Science and Technology, Pohang, South Korea","IEEE Transactions on Control Systems Technology","22 Jun 2023","2023","31","4","1867","1874","As one of the most popular tandem cold rolling mills, the Sendzimir rolling mill (ZRM) aims to obtain a flat steel strip shape by properly allocating the rolling pressure. To improve the performance of the ZRM, it is meaningful to adopt recently emerging deep reinforcement learning (DRL) that is powerful for difficult-to-solve and challenging problems. However, the direct application of DRL techniques may be impractical because of a serious singularity, partial observability, and even safety issues inherent in mill systems. In this brief, we propose an effective hybridization approach that integrates a model-based assistant into model-free DRL to resolve such practical issues. For the model-based assistant, a model-based optimization problem is first constructed and solved for the static part of the mill model. Then, the obtained static model-based coarse assistant, or controller, is improved by the proposed reinforcement learning, considering the remaining dynamic part of the mill model. The serious singularity can be resolved using the model-based approach, and the issue of partial observability is addressed by the long short-term memory (LSTM) state estimator in the proposed method. In simulation results, the proposed method successfully learns a highly performing policy for the ZRM, achieving a higher reward than pure model-free DRL. It is also observed that the proposed method can safely improve the shape controller of the mill system. The demonstration results strongly confirm the high applicability of DRL to other cold multiroll mills, such as four-high, six-high, and cluster mills.","1558-0865","","10.1109/TCST.2022.3227502","National Research Foundation (NRF) of Korea through the Korean Government(grant numbers:2019R1A2C2008637); Pohang Iron and Steel Company (POSCO)(grant numbers:Steel/Green Science); Electronics and Telecommunications Research Institute (ETRI) through the Korean Government(grant numbers:22ZS1220); Institute of Information and Communications Technology Planning and Evaluation (IITP) through the Ministry of Science and Information Communication Technology (MSIT), the Korean Government(grant numbers:2019-0-00762); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9991941","Actor-critic policy gradient;cold rolling mill;partially observable Markov decision process (MDP);reinforcement learning;Sendzimir rolling mill (ZRM)","Shape;Actuators;Strips;Heuristic algorithms;Steel;Observability;Optimization","cold rolling;deep learning (artificial intelligence);learning (artificial intelligence);optimisation;production engineering computing;recurrent neural nets;reinforcement learning;rolling mills;shape control;steel;strips","cluster mills;cold multiroll mills;deep reinforcement learning;flat steel strip shape;mill model;mill system;model-based approach;model-based assistance;model-based assistant;model-based optimization problem;partial observability;popular tandem cold rolling mills;pure model-free DRL;rolling pressure;Sendzimir rolling mill;serious singularity;shape control;shape controller;static model-based coarse assistant;ZRM","","1","","34","IEEE","19 Dec 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Deterministic Routing and Scheduling for Mixed-Criticality Flows","H. Yu; T. Taleb; J. Zhang","Center of Wireless Communications, The University of Oulu, Oulu, Finland; Center of Wireless Communications, The University of Oulu, Oulu, Finland; State Key Laboratory of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Industrial Informatics","13 Jul 2023","2023","19","8","8806","8816","Deterministic networking has recently drawn much attention by investigating deterministic flow scheduling. Combined with artificial intelligent (AI) technologies, it can be leveraged as a promising network technology for facilitating automated network configuration in the Industrial Internet of Things (IIoT). However, the stricter requirements of the IIoT have posed significant challenges, that is, deterministic and bounded latency for time-critical applications. This article incorporates deep reinforcement learning (DRL) in cycle specified queuing and forwarding and proposes a DRL-based deterministic flow scheduler (Deep-DFS) to solve the deterministic flow routing and scheduling problem. Novel delay aware network representations, action masking and criticality aware reward function design are proposed to make deep-DFS more scalable and efficient. Simulation experiments are conducted to evaluate the performances of deep-DFS, and the results show that deep-DFS can schedule more flows than the other benchmark methods (heuristic- and AI-based methods).","1941-0050","","10.1109/TII.2022.3222314","European Unions Horizon 2020 Research and Innovation Program(grant numbers:101016509,871793); Academy of Finland 6Genesis project(grant numbers:318927); Academy of Finland IDEA-MILL Project(grant numbers:352428); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950700","6G and artificial intelligence;deep reinforcement learning (DRL);deterministic networking (DetNet);Industrial Internet of Things (IIoT);Internet of Things (IoT);mixed-criticality flows","Routing;Job shop scheduling;Delays;Industrial Internet of Things;Time factors;Schedules;Dynamic scheduling","computer network security;deep learning (artificial intelligence);Internet of Things;production engineering computing;reinforcement learning;scheduling;telecommunication computing;telecommunication network routing","AI-based methods;artificial intelligent technologies;automated network configuration;deep reinforcement learning;deep-DFS;design function;deterministic flow routing;deterministic flow scheduling;deterministic networking;deterministic routing;DRL-based deterministic flow scheduler;IIoT;Industrial Internet of Things;mixed-criticality flows;network technology;time-critical applications","","1","","23","CCBY","15 Nov 2022","","","IEEE","IEEE Journals"
"Fixed-Wing Stalled Maneuver Control Technology Based on Deep Reinforcement Learning","W. Hu; Z. Gao; J. Quan; X. Ma; J. Xiong; W. Zhang","School of Astronautics Northwestern Polytechnical University, Xi'an, China; School of Astronautics Northwestern Polytechnical University, Xi'an, China; School of Astronautics Northwestern Polytechnical University, Xi'an, China; School of Astronautics Northwestern Polytechnical University, Xi'an, China; School of Astronautics Northwestern Polytechnical University, Xi'an, China; School of Astronautics Northwestern Polytechnical University, Xi'an, China","2022 IEEE 5th International Conference on Big Data and Artificial Intelligence (BDAI)","26 Aug 2022","2022","","","19","25","A fixed-wing flight control method based on Deep Reinforcement Learning (DRL) was proposed to solve the problem of strong coupling between the channels of a fixed-wing aircraft during actual flight control, and the strong non-linearity and uncertainty of the aerodynamic parameters of the aircraft during stalled maneuver. When designing the control system, Proximal Policy Optimization (PPO) and neural network are used to design a control method that is directly mapped from the state to the aircraft actuator. A formalized reward function is designed to simulate the control of the vehicle's stalled maneuver, taking into account the relationship between the position and attitude of the vehicle and the actual demand command. The simulation results show that the control system design based on PPO algorithm can reduce the dependence on the model, achieve the intelligent control of the aircraft.","","978-1-6654-7081-0","10.1109/BDAI56143.2022.9862674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9862674","DRL;PPO algorithm;fixed-wing;formalized reward;stalled maneuver","Training;Actuators;Uncertainty;Atmospheric modeling;Simulation;Reinforcement learning;Control systems","aerodynamics;aerospace components;aerospace control;aircraft;aircraft control;design engineering;intelligent control;learning (artificial intelligence);optimisation;production engineering computing","fixed-wing stalled maneuver control technology;actual flight control;proximal policy optimization;control system design;intelligent control;deep reinforcement learning;fixed-wing aircraft actuator;aerodynamic parameters;PPO algorithm","","","","21","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"A Deep Reinforcement Learning Approach for Resource-Constrained Project Scheduling","X. Zhao; W. Song; Q. Li; H. Shi; Z. Kang; C. Zhang","Institute of Marine Science and Technology, Shandong University, Qingdao, China; Institute of Marine Science and Technology, Shandong University, Qingdao, China; Institute of Marine Science and Technology, Shandong University, Qingdao, China; Qingdao Cigarette Factory of China Tobacco Shandong Industrial Co.,Ltd.; Qingdao Cigarette Factory of China Tobacco Shandong Industrial Co.,Ltd.; Qingdao Cigarette Factory of China Tobacco Shandong Industrial Co.,Ltd.","2022 IEEE Symposium Series on Computational Intelligence (SSCI)","30 Jan 2023","2022","","","1226","1234","The Resource-Constrained Project Schedule Problem (RCPSP) is one of the most studied Cumulative Scheduling Problems with many real-world applications. Priority rules are widely adopted in practical RCPSP solving, however traditional rules are manually designed by human experts and may perform poorly. Lately, Deep Reinforcement Learning (DRL) has been shown to be effective in learning dispatching rules for disjunctive scheduling problems. However, research on cumulative problems such as RCPSP is rather sparse. In this paper, we propose an end-to-end DRL method to train high-quality priority rules for RCPSP. Based on its graph structure, we leverage Graph Neural Network to effectively capture the complex features for the internal scheduling states. Experiments show that by training on small instances, our method can learn scheduling policy that performs well on a wide range of problem scales, which outperforms traditional manual priority rules and state-of-the-art genetic programming based hyper-heuristics.","","978-1-6654-8768-9","10.1109/SSCI51031.2022.10022122","Shandong Provincial Natural Science Foundation(grant numbers:ZR2021QF063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10022122","Resource Constrained Project Scheduling Problem;Deep Reinforcement Learning;parallel schedule generation scheme;Graph Neural Network","Deep learning;Training;Schedules;Processor scheduling;Genetic programming;Reinforcement learning;Manuals","deep learning (artificial intelligence);genetic algorithms;graph theory;production engineering computing;reinforcement learning;scheduling","cumulative scheduling problems;deep reinforcement learning approach;disjunctive scheduling problems;end-to-end DRL method;genetic programming based hyper-heuristics;graph structure;high-quality priority rules;human experts;internal scheduling states;learning dispatching rules;leverage graph neural network;manual priority rules;practical RCPSP solving;resource-constrained project schedule problem","","","","29","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Adaptive Feature Boosting for Smart Grid Intrusion Detection","C. Hu; J. Yan; X. Liu","School of Computer Science, McGill University, Montreal, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, Canada; School of Computer Science, McGill University, Montreal, Canada","IEEE Transactions on Smart Grid","21 Jun 2023","2023","14","4","3150","3163","Intrusion detection systems (IDSs) are crucial in the security monitoring for the smart grid with increasing machine-to-machine communications and cyber threats thereafter. However, the multi-sourced, correlated, and heterogeneous smart grid data pose significant challenges to the accurate attack detection by IDSs. To improve the attack detection, this paper proposes Reinforcement Learning-based Adaptive Feature Boosting, which aims to leverage a series of AutoEncoders (AEs) to capture critical features from the multi-sourced smart grid data for the classification of normal, fault, and attack events. Multiple AEs are utilized to extract representative features from different feature sets that are automatically generated through a weighted feature sampling process; each AE-extracted feature set is then applied to build a Random Forest (RF) base classifier. In the feature sampling process, Deep Deterministic Policy Gradient (DDPG) is introduced to dynamically determine the feature sampling probability based on the classification accuracy. The critical features that improve the classification accuracy are assigned larger sampling probabilities and increasingly participate in the training of next AE. The presence of critical features is increased in the event classification over the multi-sourced smart grid data. Considering potential different alarms among base classifiers, an ensemble classifier is further built to distinguish normal, fault, and attack events. Our proposed approach is evaluated on the two realistic datasets collected from Hardware-In-the-Loop (HIL) and WUSTIL-IIOT-2021 security testbeds, respectively. The evaluation on the HIL security dataset shows that our proposed approach achieves the classification accuracy with 97.28%, an effective 5.5% increase over the vanilla Adaptive Feature Boosting. Moreover, the proposed approach not only accurately and stably selects critical features on the WUSTIL-IIOT-2021 dataset based on the significant difference of feature sampling probabilities between critical and uncritical features, i.e., the probabilities greater than 0.08 and less than 0.01, but also outperforms the other best-performing approaches with the increasing Matthew Correlation Coefficient (MCC) of 8.03%.","1949-3061","","10.1109/TSG.2022.3230730","Natural Sciences and Engineering Research Council of Canada (NSERC)(grant numbers:RGPIN-2018-06724); Fonds de Recherche du Québec–Nature et Technologies (FRQNT)(grant numbers:2019-NC-254971); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9993774","Adaptive feature boosting;feature extraction;intrusion detection systems;reinforcement learning;smart grids","Feature extraction;Smart grids;Data mining;Boosting;Reinforcement learning;Phasor measurement units;Security","computer network security;deep learning (artificial intelligence);feature extraction;gradient methods;Internet of Things;learning (artificial intelligence);pattern classification;power engineering computing;probability;production engineering computing;random forests;reinforcement learning;security of data;smart power grids","accurate attack detection;base classifiers;classification accuracy;correlated, grid data;critical features;different feature sets;feature sampling probability;feature set;heterogeneous smart grid data;intrusion detection systems;larger sampling probabilities;machine-to-machine communications;multisourced smart grid data;normal fault;Random Forest base classifier;Reinforcement Learning-based Adaptive Feature Boosting;representative features;smart grid intrusion detection;uncritical features;vanilla Adaptive Feature Boosting;weighted feature sampling process","","","","47","IEEE","20 Dec 2022","","","IEEE","IEEE Journals"
"Image features for vision-based robot manipulation based on deep reinforcement learning","R. Li","Center for Advanced Systems Understandings Helmholtz-Zentrum Dresden-Rossendorf, Germany","2021 IEEE 17th International Conference on Intelligent Computer Communication and Processing (ICCP)","16 Mar 2022","2021","","","359","364","Deep Reinforcement Learning (DRL) provides a potential toolset that enables industrial robots to autonomously learn manipulation skills, but the learning efficiency (success rate within certain learning episodes) is the bottleneck. In this work, we ascertained well-designed environmental observations to be vital for improving efficiency. To determine the impacts of different observations, we conducted simulation experiments of robots grasping, and evaluated three popular categories of environment observations -positions of the Tool-Center-Point, raw images from a fixed viewpoint camera, and image features (Sobel, Laplacian, HOG, LBP). The results indicate ""image features"" proved to be superior to the others, they contribute to higher success rate and learning speed.","","978-1-6654-0976-6","10.1109/ICCP53602.2021.9733597","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733597","Machine Learning for Robot Control;Reinforcement Learning;Simulation and Animation","Laplace equations;Service robots;Conferences;Robot vision systems;Process control;Reinforcement learning;Grasping","cameras;deep learning (artificial intelligence);feature extraction;industrial manipulators;machine tools;production engineering computing;reinforcement learning;robot vision","image features;vision-based robot manipulation;deep reinforcement learning;industrial robots;manipulation skills;learning efficiency;environmental observations;robots grasping;tool-center-point;raw images;fixed viewpoint camera;Sobel;Laplacian;HOG;LBP","","","","33","IEEE","16 Mar 2022","","","IEEE","IEEE Conferences"
"Supply Chain Delay Mitigation via Supplier Risk Index Assessment and Reinforcement Learning","K. Sedamaki; A. Kattepur","Indian Institute of Technology, Tirupati, India; Ericsson Research, Bangalore, India","2022 IEEE 1st International Conference on Data, Decision and Systems (ICDDS)","9 Feb 2023","2022","","","1","6","Supply chains are vulnerable to unforeseen delays, which might adversely affect delivery performance. Quantifying the risk profiles of each supplier based on their historic delivery patterns and forecast deviations can help make superior decisions in multi-supplier scenarios. This problem has been previously approached from linear programming and qualitative assessment perspectives; however, application of machine learning and reinforcement learning-based methods are still in a nascent stage. This paper proposes a machine learning technique to classify a supplier into one of four risk indices accurately on real-world datasets from Ericsson's supply hub. A reinforcement learning agent is also trained in a custom-modeled environment to split an order among multiple suppliers while minimizing the delays. Additionally, a working web-based tool is developed to demonstrate these techniques, that may be extended to other domains.","","978-1-6654-9162-4","10.1109/ICDDS56399.2022.10037409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10037409","Supply chain;Risk Index;Machine Learning;Reinforcement Learning","Learning systems;Supply chains;Reinforcement learning;Linear programming;Delays;Indexes","linear programming;production engineering computing;reinforcement learning;risk management;supply chain management;supply chains","Ericsson supply hub;linear programming;machine learning technique;multisupplier scenarios;reinforcement learning agent;supplier risk index assessment;supply chain delay mitigation;working web-based tool","","","","19","IEEE","9 Feb 2023","","","IEEE","IEEE Conferences"
"Trustable Reinforcement Learning for Asset Integrity Management","Z. Mahmoodzadeh; A. Mosleh","University of California, Los Angeles; University of California, Los Angeles","2021 Annual Reliability and Maintainability Symposium (RAMS)","22 Nov 2021","2021","","","1","7","SUMMARY & CONCLUSIONSSignificant recent accomplishments and immense future potentials of Reinforcement Learning (RL) are behind the surge in interest in this class of decision-making algorithms within the Artificial Intelligence domain. However, RL application to real-world problems such as technical asset integrity management has remained very limited. One of the main obstacles to the real-world deployment of RL can be broadly characterized as the lack of trust. Assets operators are reluctant to deploy policies derived from RL-based software because they are not currently designed to earn the user confidence. This research is an effort to address the ""trustability"" concerns in RL for safety and asset integrity management type of problems. We propose a methodology that sets up multiple risk barriers in the RL output policy and provides an estimation of the RL policy's performance prior to deployment. Our proposed method is compatible with any RL algorithm. The methodology was tested on a scaled-down integrity management case-study in a simulated pipeline environment and successfully derived a safer and superior policy for the pipe's corrosion-related maintenance management.","2577-0993","978-1-7281-8017-5","10.1109/RAMS48097.2021.9605744","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9605744","asset integrity management;reinforcement learning;trust;internal corrosion maintenance","Pipelines;Decision making;Random access memory;Estimation;Reinforcement learning;Software;Safety","asset management;data integrity;decision making;maintenance engineering;pipelines;production engineering computing;reinforcement learning;trusted computing","technical asset integrity management;assets operators;trustability concerns;decision-making algorithms;artificial intelligence;trustable reinforcement learning;policy deployment;risk barriers;pipeline environment;pipe corrosion-related maintenance management","","","","25","USGov","22 Nov 2021","","","IEEE","IEEE Conferences"
"Simulation of the Internal Electric Fleet Dispatching Problem at a Seaport: A Reinforcement Learning Approach","M. Brunetti; G. Campuzano; M. Mes","Department of High-Tech Business and Entrepreneurship, University of Twente, Enschede, AE, The Netherlands; Department of High-Tech Business and Entrepreneurship, University of Twente, Enschede, AE, The Netherlands; Department of High-Tech Business and Entrepreneurship, University of Twente, Enschede, AE, The Netherlands","2022 Winter Simulation Conference (WSC)","23 Jan 2023","2022","","","1","12","Through discrete-event simulation, we evaluate the impact of using a fleet of electric and autonomous vehicles (EAVs) to decouple inbound trucks from the internal freight flows in a seaport located in the Netherlands. To support the operational control of EAVs, we use agent-based modeling and support the decision-making capabilities using a reinforcement learning (RL) approach. More specifically, to model the assignment of EAVs to container transport or battery charge, we introduce the Internal Electric Fleet Dispatching Problem (IEFDP). To solve the IEFDP, we propose an RL approach and benchmark its performance against four different assignment heuristics. Our results are compelling: the RL approach outperforms the benchmark heuristics, and the decoupling process significantly reduces congestion and waiting times for truck drivers as well as potentially improve the traffic's sustainability, against a slight increase in length of stay of containers at the port.","1558-4305","978-1-6654-7661-4","10.1109/WSC57314.2022.10015516","Dutch Research Council (NWO)(grant numbers:439.18.458 B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015516","","Decision making;Reinforcement learning;Seaports;Containers;Benchmark testing;Dispatching;Batteries","control engineering computing;decision making;discrete event simulation;dispatching;electric vehicles;freight handling;industrial robots;mobile robots;multi-agent systems;production engineering computing;reinforcement learning;road vehicles;sea ports","agent-based modeling;assignment heuristics;battery charge;container transport;decision-making capabilities;discrete-event simulation;EAVs;electric and autonomous vehicles;IEFDP;inbound trucks;internal electric fleet dispatching problem;internal freight flows;Netherlands;operational control;reinforcement learning;RL approach;seaport;traffic sustainability;truck driver waiting times","","","","16","IEEE","23 Jan 2023","","","IEEE","IEEE Conferences"
"Reinforcement Learning-Based Supervisor System Proposal for Fault-Tolerant Control of Direct Fired Heater","M. R. Canelón; E. C. Morles","Gerencia de Optimización Petróleos de Venezuela, San Tomé, Venezuela; Applied Research Institute The University of Illinois Urbana-Champaign, USA","2022 International Conference on Electrical, Computer and Energy Technologies (ICECET)","9 Sep 2022","2022","","","1","6","This work proposes a reinforcement learning-based supervisor system that incorporates automatic fault detection and fault-tolerant control in a fired heater plant, furnace, used to raise the temperature of crude oil for post-processing purposes. The faults considered are associated with the plant’s operating conditions, including the temperature sensor. The supervisor system contemplates supervised-trained neural networks to build a fault detector and an estimator of the controlled variable, a virtual sensor, and a reinforcement-trained neural network for the fault-tolerant controller; specifically, the Monte Carlo algorithm is implemented. Computational simulations illustrate the supervisor system’s functionalities, and a discussion of its physical implementation is presented.","","978-1-6654-7087-2","10.1109/ICECET55527.2022.9872640","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9872640","Reinforcement learning;Neural network controller;Monte Carlo tree search algorithm;Supervisory system;Fault-tolerant system;Fault detection;Direct fire heater","Temperature sensors;Training;Fault tolerance;Event detection;Soft sensors;Furnaces;Computational modeling","control engineering computing;crude oil;fault diagnosis;fault tolerant control;furnaces;heat systems;industrial plants;Monte Carlo methods;neural nets;production engineering computing;reinforcement learning;supervised learning;temperature sensors","direct fired heater;automatic fault detection;fired heater plant;supervised-trained neural networks;reinforcement-trained neural network;fault-tolerant controller;reinforcement learning-based supervisor system;furnace;crude oil temperature;temperature sensor;virtual sensor;Monte Carlo algorithm;computational simulations;supervisor system functionalities","","","","13","IEEE","9 Sep 2022","","","IEEE","IEEE Conferences"
"An improved reinforcement learning approach to solve flow job scheduling problems","D. Hu; X. Jiang; J. Wang","College of Computer Science and Technology, Qilu University of Technology (shandong academy of sciences), Jinan, China; College of Computer Science and Technology, Qilu University of Technology (shandong academy of sciences), Jinan, China; Software and Bigdata Department, Shandong College of Information Technology, Weifang, China","2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)","26 Apr 2021","2020","","","538","544","The fow shop scheduling problem is an important scheduling problems with a large number of practical applications. For the high performance computing applications, flow shop scheduling directly affects resource utilization and power dissipation.In this paper, we propose a novel algorithm TS_Qlearning algorithm that combines the tabu search(TS) method and the Qlearning algorithm to minimize the idle time. We also calculate the makespan value,which is used to comprehensively evaluate the quality of the algorithm. The comparative analysis of the results proves the superiority of the TS_Qlearning algorithm.","","978-1-7281-7649-9","10.1109/HPCC-SmartCity-DSS50907.2020.00067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407893","flowshop;Qlearning;scheduling;reinforcement learning","Job shop scheduling;Processor scheduling;High performance computing;Conferences;Reinforcement learning;Resource management","flow shop scheduling;job shop scheduling;learning (artificial intelligence);production engineering computing;search problems","power dissipation;reinforcement learning approach;flow job scheduling problems;high performance computing applications;resource utilization;TS-Qlearning algorithm;tabu search method","","","","35","IEEE","26 Apr 2021","","","IEEE","IEEE Conferences"
"An end-to-end Approach to a Reinforcement Learning in Transport Logistics","N. Ramón Gómez; M. El-Hajj","Semantics, Cybersecurity & Services, University of Twente, Enschede, Netherlands; Semantics, Cybersecurity & Services, University of Twente, Enschede, Netherlands","2023 16th International Conference on Signal Processing and Communication System (ICSPCS)","28 Sep 2023","2023","","","1","10","The use of machine learning and reinforcement learning techniques has become increasingly important in enhancing the performance of transportation in supply chains. These techniques allow for real-time adaptation to changing conditions and optimization of decision-making, resulting in more efficient and cost-effective transportation routes. By incorporating machine learning and reinforcement learning, companies can improve their overall supply chain management and competitiveness in today's fast-paced business environment. In this paper, we proposed a multi-mode transportation and route planning using Reinforcement Learning (RL) algorithm. The algorithm showed good performance in multi-modal routing and transport selection based on cost functions through the evaluation of three trained agents in 100 different environments. However, a comparison with the Dijkstra algorithm revealed sub-optimal decisions with higher costs. Further training is needed to fully define the optimal policy” with the dynamic environment being a challenge","","979-8-3503-3351-0","10.1109/ICSPCS58109.2023.10261141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261141","Machine learning;Reinforcement Learning;Supply Chain Management;Transportation;Markov Decision Process","Training;Machine learning algorithms;Costs;Supply chain management;Heuristic algorithms;Supply chains;Signal processing algorithms","decision making;logistics;production engineering computing;reinforcement learning;supply chain management;transportation","cost-effective transportation routes;decision-making;Dijkstra algorithm;dynamic environment;end-to-end approach;fast-paced business environment;machine learning;optimization;real-time adaptation;reinforcement learning algorithm;route planning;suboptimal decisions;supply chain management;transport logistics;transport selection","","","","27","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Task Scheduling via Modified Deep Reinforcement Learning for MEC-Enabled Industrial IoT","Y. Wang; H. Zhang; X. Zhou; D. Li; D. Yuan","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; Shandong Key Laboratory of Wireless Communication Technologies, Shandong University, Jinan, China; Shandong Key Laboratory of Wireless Communication Technologies, Shandong University, Jinan, China","2022 14th International Conference on Wireless Communications and Signal Processing (WCSP)","15 Feb 2023","2022","","","661","666","With the development of Industrial Internet of Things (IIoT), the ever growing mismatch between the numerous tasks generated in real industrial scenarios and the limited computing ability is enlarging the system delay. How to schedule the system tasks to enhance the system efficiency has become extremely significant. Along this line, a task scheduling scheme for MEC-enabled IIoT systems is proposed in this work to minimize the total delay of the whole system. Since there exist time overlaps between different tasks while the tasks are conducted in parallel, it is difficult to accurately model the process of the tasks in terms of delay. To solve this, a novel modeling method is proposed to transform the optimization problem of minimizing the total delay into minimizing the unprocessed data volumes. To solve the formulated problem, we formulate the process of the tasks being executed as a Markov decision process (MDP) and propose a modified deep reinforcement learning (DRL) algorithm. To evaluate the performance of our proposed task scheduling scheme, intensive experiments have been conducted. The results show that our proposed scheme achieves better performance than some existing schemes. In the end, the scalability and availability of our scheme are tested.","","978-1-6654-5085-0","10.1109/WCSP55476.2022.10039332","National Natural Science Foundation of China(grant numbers:61971270,61860206005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10039332","Industrial Internet of Things (IIoT);deep reinforcement learning (DRL);mobile edge computing (MEC)","Deep learning;Job shop scheduling;Scalability;Signal processing algorithms;Reinforcement learning;Data models;Delays","decision theory;deep learning (artificial intelligence);Internet of Things;Markov processes;mobile computing;optimisation;production engineering computing;reinforcement learning;scheduling","delay minimization;DRL algorithm;Industrial Internet of Things;industrial scenarios;Markov decision process;MDP;MEC-enabled IIoT systems;MEC-enabled Industrial IoT;mobile edge computing;modified deep reinforcement learning algorithm;optimization problem;system delay;system efficiency;system tasks;task scheduling scheme;unprocessed data volume minimization","","","","13","IEEE","15 Feb 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Data Freshness-oriented Scheduling in Industrial IoT","J. Li; J. Tang; Z. Liu","Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, China; Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, China; School of Computer Science and Electronics Engineering, University of Essex, UK","GLOBECOM 2022 - 2022 IEEE Global Communications Conference","11 Jan 2023","2022","","","6271","6276","Making a timely and precise scheduling in Industrial Internet of Things (IIoT) is fundamental and critical. Recently, Age of Incorrect Information (AoII) is proposed and utilized to measure the timeliness and accuracy of monitoring. In this work, we investigate a multi-sensor update system and leverage AoII to quantify the information freshness. Our goal is to obtain an optimal scheduling policy to minimize the system-wide cost. We first model the source statuses monitored by sensors as Markov chains and the scheduling problem as a Markov decision process (MDP). Due to the heterogeneity of source statuses in IIoT, it is prohibitive to solve the formulated MDP problem by conventional methods. To this end, we make use of a deep reinforcement learning (DRL) algorithm to solve this scheduling problem. Extensive numerical results verify the effectiveness of the adopted DRL algorithm. In addition, comparing to the conventional Age of Information (AoI) oriented method, we find that the AoII oriented method is much more effective, from the perspective of system-wide cost.","","978-1-6654-3540-6","10.1109/GLOBECOM48099.2022.10001430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10001430","Age of Incorrect Information;Deep Reinforcement Learning;Markov Decision Process;Industrial Internet of Things","Deep learning;Job shop scheduling;Costs;Simulation;Optimal scheduling;Reinforcement learning;Markov processes","decision theory;deep learning (artificial intelligence);Internet of Things;Markov processes;production engineering computing;reinforcement learning;scheduling;sensor fusion","Age of Incorrect Information;AoII;data freshness;deep reinforcement learning;DRL;IIoT;Industrial Internet of Things;Industrial IoT;information freshness;Markov chains;Markov decision process;MDP;multisensor update system;optimal scheduling policy","","","","16","IEEE","11 Jan 2023","","","IEEE","IEEE Conferences"
"Multi-hop Computational Offloading with Reinforcement Learning for Industrial IoT Networks","S. B. Roy; E. Tan; A. S. Madhukumar","Advanced Remanufacturing & Technology Centre, Agency for Science, Technology and Research, Singapore; Advanced Remanufacturing & Technology Centre, Agency for Science, Technology and Research, Singapore; School of Computer Scinece & Engineering, Nanyang Technological University, Singapore","2023 IEEE 97th Vehicular Technology Conference (VTC2023-Spring)","14 Aug 2023","2023","","","1","5","To serve advanced use-cases in industrial internet of things (IIoT) setups, communication and computation over wireless networks have faced overlapping resource management challenges. Two crucial resources in this context are radio re-sources and computational resources. The problem to achieve the ultra-low latency for mission critical applications is motivating enterprises to invest in offloading capability of computation heavy tasks while retaining the bandwidth efficiency of edge nodes. This work proposes a novel multi-hop offloading framework powered by deep reinforcement learning to aid the edge nodes in making intelligent decisions on task offloading. The proposed method is benchmarked against existing state of the art techniques to measure task completion delay and algorithmic runtime.","2577-2465","979-8-3503-1114-3","10.1109/VTC2023-Spring57618.2023.10200149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10200149","","Vehicular and wireless technologies;Runtime;Spectral efficiency;Wireless networks;Mission critical systems;Reinforcement learning;Spread spectrum communication","cloud computing;computer network security;deep learning (artificial intelligence);Internet of Things;production engineering computing;reinforcement learning;resource allocation;telecommunication scheduling","computation heavy tasks;computational resources;crucial resources;deep reinforcement learning;edge nodes;IIoT;industrial Internet of things setups;industrial IoT networks;mission critical applications;multihop computational offloading;multihop offloading framework;radio re-sources;reinforcement learning;resource management;task offloading;wireless networks","","","","13","IEEE","14 Aug 2023","","","IEEE","IEEE Conferences"
"Control of a Quadrotor With Reinforcement Learning","J. Hwangbo; I. Sa; R. Siegwart; M. Hutter","Robotic Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; Robotic Systems Lab, ETH Zurich, Zurich, Switzerland","IEEE Robotics and Automation Letters","12 Jul 2017","2017","2","4","2096","2103","In this letter, we present a method to control a quadrotor with a neural network trained using reinforcement learning techniques. With reinforcement learning, a common network can be trained to directly map state to actuator command making any predefined control structure obsolete for training. Moreover, we present a new learning algorithm that differs from the existing ones in certain aspects. Our algorithm is conservative but stable for complicated tasks. We found that it is more applicable to controlling a quadrotor than existing algorithms. We demonstrate the performance of the trained policy both in simulation and with a real quadrotor. Experiments show that our policy network can react to step response relatively accurately. With the same policy, we also demonstrate that we can stabilize the quadrotor in the air even under very harsh initialization (manually throwing it upside-down in the air with an initial velocity of 5 m/s). Computation time of evaluating the policy is only 7 μs per time step, which is two orders of magnitude less than common trajectory optimization algorithms with an approximated model.","2377-3766","","10.1109/LRA.2017.2720851","Swiss National Science Foundation; National Centre of Competence in Research Robotics(grant numbers:200021-166232); European Unions Horizon 2020 research and innovation programme(grant numbers:644227); Swiss State Secretariat for Education, Research and Innovation (SERI)(grant numbers:15.0029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961277","Aerial systems: mechanics and control;learning and adaptive systems","Trajectory;Junctions;Learning (artificial intelligence);Computational modeling;Neural networks;Robots;Optimization","aircraft control;helicopters;learning systems;neurocontrollers;stability;step response","quadrotor control;reinforcement learning;neural network;step response;stabilization","","297","","21","IEEE","28 Jun 2017","","","IEEE","IEEE Journals"
"Reinforced Imitation: Sample Efficient Deep Reinforcement Learning for Mapless Navigation by Leveraging Prior Demonstrations","M. Pfeiffer; S. Shukla; M. Turchetta; C. Cadena; A. Krause; R. Siegwart; J. Nieto","Autonomous Systems Lab, Computer Vision Lab, Learning and Adaptive Systems Group, and Max Planck ETH Center for Learning Systems, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, Computer Vision Lab, Learning and Adaptive Systems Group, and Max Planck ETH Center for Learning Systems, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, Computer Vision Lab, Learning and Adaptive Systems Group, and Max Planck ETH Center for Learning Systems, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, Computer Vision Lab, Learning and Adaptive Systems Group, and Max Planck ETH Center for Learning Systems, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, Computer Vision Lab, Learning and Adaptive Systems Group, and Max Planck ETH Center for Learning Systems, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, Computer Vision Lab, Learning and Adaptive Systems Group, and Max Planck ETH Center for Learning Systems, ETH Zurich, Zurich, Switzerland; Autonomous Systems Lab, Computer Vision Lab, Learning and Adaptive Systems Group, and Max Planck ETH Center for Learning Systems, ETH Zurich, Zurich, Switzerland","IEEE Robotics and Automation Letters","26 Sep 2018","2018","3","4","4423","4430","This letter presents a case study of a learning-based approach for target-driven mapless navigation. The underlying navigation model is an end-to-end neural network, which is trained using a combination of expert demonstrations, imitation learning (IL) and reinforcement learning (RL). While RL and IL suffer from a large sample complexity and the distribution mismatch problem, respectively, we show that leveraging prior expert demonstrations for pretraining can reduce the training time to reach at least the same level of the performance compared to plain RL by a factor of 5. We present a thorough evaluation of different combinations of expert demonstrations, different RL algorithms, and reward functions, both in simulation and on a real robotic platform. Our results show that the final model outperforms both standalone approaches in the amount of successful navigation tasks. In addition, the RL reward function can be significantly simplified when using pretraining, e.g., by using a sparse reward only. The learned navigation policy is able to generalize to unseen and real-world environments.","2377-3766","","10.1109/LRA.2018.2869644","European Union Horizon 2020 project CROWDBOT(grant numbers:779942); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8458422","Navigation;deep reinforcement learning;end-to-end planning","Navigation;Training;Robots;Task analysis;Collision avoidance;Neural networks;Planning","learning (artificial intelligence);mobile robots;navigation;neural nets","learning-based approach;sample efficient deep reinforcement learning;reinforced imitation;learned navigation policy;RL reward function;successful navigation tasks;standalone approaches;final model;plain RL;leveraging prior expert demonstrations;distribution mismatch problem;sample complexity;IL;end-to-end neural network;underlying navigation model;target-driven mapless navigation","","92","","33","IEEE","11 Sep 2018","","","IEEE","IEEE Journals"
"Multi-Agent Motion Planning for Dense and Dynamic Environments via Deep Reinforcement Learning","S. H. Semnani; H. Liu; M. Everett; A. de Ruiter; J. P. How","Aerospace Engineering Department, Ryerson University, Toronto, Canada; Faculty of Applied Science and Engineering, University of Toronto, Toronto, Canada; Aerospace Controls Laboratory, Massachusetts Institute of Technology, Cambridge, USA; Aerospace Engineering Department, Ryerson University, Toronto, Canada; Aerospace Controls Laboratory, Massachusetts Institute of Technology, Cambridge, USA","IEEE Robotics and Automation Letters","6 Mar 2020","2020","5","2","3221","3226","This letter introduces a hybrid algorithm of deep reinforcement learning (RL) and Force-based motion planning (FMP) to solve distributed motion planning problem in dense and dynamic environments. Individually, RL and FMP algorithms each have their own limitations. FMP is not able to produce time-optimal paths and existing RL solutions are not able to produce collision free paths in dense environments. Therefore, we first tried improving the performance of recent RL approaches by introducing a new reward function that not only eliminates the requirement of a pre supervised learning (SL) step but also decreases the chance of collision in crowded environments. That improved things, but there were still a lot of failure cases. So, we developed a hybrid approach to leverage the simpler FMP approach in stuck, simple and high-risk cases, and continue using RL for normal cases in which FMP can't produce optimal path. Also, we extend GA3CCADRL algorithm to 3D environment. Simulation results show that the proposed algorithm outperforms both deep RL and FMP algorithms and produces up to 50% more successful scenarios than deep RL and up to 75% less extra time to reach goal than FMP.","2377-3766","","10.1109/LRA.2020.2974695","Ontario Centers of Excellence(grant numbers:27481); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9001226","Motion planning;distributed algorithms;collision avoidance;deep learning;reinforcement learning;trajectory optimization;hybrid control","Planning;Navigation;Collision avoidance;Trajectory;Dynamics;Heuristic algorithms;Three-dimensional displays","collision avoidance;learning (artificial intelligence);mobile robots;multi-robot systems","collision free paths;time-optimal paths;dynamic environments;distributed motion planning problem;hybrid algorithm;deep reinforcement learning;multiagent motion planning;FMP algorithms;deep RL;GA3CCADRL algorithm;optimal path;pre supervised learning step","","51","","29","IEEE","18 Feb 2020","","","IEEE","IEEE Journals"
"Efficient Sampling-Based Maximum Entropy Inverse Reinforcement Learning With Application to Autonomous Driving","Z. Wu; L. Sun; W. Zhan; C. Yang; M. Tomizuka","Department of Mechanical Engineering, University of California, Berkeley, USA; Department of Mechanical Engineering, University of California, Berkeley, USA; Department of Mechanical Engineering, University of California, Berkeley, USA; Department of Computer Science, Shanghai Jiaotong University, Shanghai, China; Department of Mechanical Engineering, University of California, Berkeley, USA","IEEE Robotics and Automation Letters","17 Jul 2020","2020","5","4","5355","5362","In the past decades, we have witnessed significant progress in the domain of autonomous driving. Advanced techniques based on optimization and reinforcement learning become increasingly powerful when solving the forward problem: given designed reward/cost functions, how we should optimize them and obtain driving policies that interact with the environment safely and efficiently. Such progress has raised another equally important question: what should we optimize? Instead of manually specifying the reward functions, it is desired that we can extract what human drivers try to optimize from real traffic data and assign that to autonomous vehicles to enable more naturalistic and transparent interaction between humans and intelligent agents. To address this issue, we present an efficient sampling-based maximum-entropy inverse reinforcement learning (IRL) algorithm in this letter. Different from existing IRL algorithms, by introducing an efficient continuous-domain trajectory sampler, the proposed algorithm can directly learn the reward functions in the continuous domain while considering the uncertainties in demonstrated trajectories from human drivers. We evaluate the proposed algorithm via real-world driving data, including both non-interactive and interactive scenarios. The experimental results show that the proposed algorithm achieves more accurate prediction performance with faster convergence speed and better generalization compared to other baseline IRL algorithms.","2377-3766","","10.1109/LRA.2020.3005126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9126156","Learning from demonstration;intelligent transportation systems;inverse reinforcement learning;autonomous driving;social human-robot interaction","Trajectory;Autonomous vehicles;Reinforcement learning;Optimization;Entropy;Prediction algorithms","driver information systems;entropy;learning (artificial intelligence);mobile robots","driving policies;reward cost functions;human drivers;real-world driving data;autonomous driving;optimization;sampling-based maximum entropy inverse reinforcement learning;real traffic data;autonomous vehicles;human intelligent agents;continuous-domain trajectory sampler;noninteractive scenarios;baseline IRL algorithms","","48","","26","IEEE","25 Jun 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Safe Local Planning of a Ground Vehicle in Unknown Rough Terrain","S. Josef; A. Degani","Technion Autonomous Systems Program, Technion – Israel Institute of Technology, Haifa, Israel; Faculty of Civil and Environmental Engineering and with the Technion Autonomous Systems Program, Technion – Israel Institute of Technology, Haifa, Israel","IEEE Robotics and Automation Letters","31 Aug 2020","2020","5","4","6748","6755","Safe unmanned ground vehicle navigation in unknown rough terrain is crucial for various tasks such as exploration, search and rescue and agriculture. Offline global planning is often not possible when operating in harsh, unknown environments, and therefore, online local planning must be used. Most online rough terrain local planners require heavy computational resources, used for optimal trajectory searching and estimating vehicle orientation in positions within the range of the sensors. In this work, we present a deep reinforcement learning approach for local planning in unknown rough terrain with zero-range to local-range sensing, achieving superior results compared to potential fields or local motion planning search spaces methods. Our approach includes reward shaping which provides a dense reward signal. We incorporate self-attention modules into our deep reinforcement learning architecture in order to increase the explainability of the learnt policy. The attention modules provide insight regarding the relative importance of sensed inputs during training and planning. We extend and validate our approach in a dynamic simulation, demonstrating successful safe local planning in environments with a continuous terrain and a variety of discrete obstacles. By adding the geometric transformation between two successive timesteps and the corresponding action as inputs, our architecture is able to navigate on surfaces with different levels of friction. Reinforcement learning, autonomous vehicle navigation, motion and path planning.","2377-3766","","10.1109/LRA.2020.3011912","Technion Autonomous Systems Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9149720","Motion planning;robot learning;unmanned autonomous vehicles","Navigation;Planning;Vehicle dynamics;Robot sensing systems;Machine learning;Dynamics","collision avoidance;learning (artificial intelligence);mobile robots;motion control;navigation;optimisation;road vehicles;trajectory control","geometric transformation;discrete obstacles;dense reward signal;local motion planning search spaces method;optimal trajectory searching;safe local planning;path planning;autonomous vehicle navigation;deep reinforcement learning architecture;local-range sensing;deep reinforcement learning approach;vehicle orientation estimation;online rough terrain local planners;online local planning;harsh environments;offline global planning;safe unmanned ground vehicle navigation;unknown rough terrain","","46","","37","IEEE","27 Jul 2020","","","IEEE","IEEE Journals"
"Actor-Critic Reinforcement Learning for Control With Stability Guarantee","M. Han; L. Zhang; J. Wang; W. Pan","Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China; Department of Computer Science, University College London, London, U.K.; Department of Cognitive Robotics, Delft University of Technology, Delft, Netherlands","IEEE Robotics and Automation Letters","10 Aug 2020","2020","5","4","6217","6224","Reinforcement Learning (RL) and its integration with deep learning have achieved impressive performance in various robotic control tasks, ranging from motion planning and navigation to end-to-end visual manipulation. However, stability is not guaranteed in model-free RL by solely using data. From a control-theoretic perspective, stability is the most important property for any control system, since it is closely related to safety, robustness, and reliability of robotic systems. In this letter, we propose an actor-critic RL framework for control which can guarantee closed-loop stability by employing the classic Lyapunov's method in control theory. First of all, a data-based stability theorem is proposed for stochastic nonlinear systems modeled by Markov decision process. Then we show that the stability condition could be exploited as the critic in the actor-critic RL to learn a controller/policy. At last, the effectiveness of our approach is evaluated on several well-known 3-dimensional robot control tasks and a synthetic biology gene network tracking task in three different popular physics simulation platforms. As an empirical evaluation on the advantage of stability, we show that the learned policies can enable the systems to recover to the equilibrium or way-points when interfered by uncertainties such as system parametric variations and external disturbances to a certain extent.","2377-3766","","10.1109/LRA.2020.3011351","Harbin Institute of Technology Scholarship; Major Scientific Research Project Cultivation(grant numbers:ZDXMPY20180101); AnKobot Smart Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146733","Reinforcement learning;stability;lyapunov's method","Task analysis;Stability criteria;Asymptotic stability;Lyapunov methods;Robots;Control systems","closed loop systems;control engineering computing;learning (artificial intelligence);Lyapunov methods;Markov processes;mobile robots;nonlinear control systems;path planning;stability;stochastic systems","actor-critic reinforcement learning;deep learning;robotic control tasks;motion planning;end-to-end visual manipulation;model-free RL;control-theoretic perspective;actor-critic RL framework;closed-loop stability;classic Lyapunov's method;data-based stability theorem;stochastic nonlinear systems;Markov decision process;3-dimensional robot control tasks;synthetic biology gene network;system parametric variations;physics simulation platforms","","39","","48","IEEE","23 Jul 2020","","","IEEE","IEEE Journals"
"Decentralized Multi-Agent Pursuit Using Deep Reinforcement Learning","C. de Souza; R. Newbury; A. Cosgun; P. Castillo; B. Vidolov; D. Kulić","Université de Technologie de Compiègne, CNRS, Heudiasyc, CS, France; Department of Electrical and Computer System Engineering, Monash University, Clayton, Australia; Department of Electrical and Computer System Engineering, Monash University, Clayton, Australia; Université de Technologie de Compiègne, CNRS, Heudiasyc, CS, France; Université de Technologie de Compiègne, CNRS, Heudiasyc, CS, France; Department of Electrical and Computer System Engineering, Monash University, Clayton, Australia","IEEE Robotics and Automation Letters","12 Apr 2021","2021","6","3","4552","4559","Pursuit-evasion is the problem of capturing mobile targets with one or more pursuers. We use deep reinforcement learning for pursuing an omnidirectional target with multiple, homogeneous agents that are subject to unicycle kinematic constraints. We use shared experience to train a policy for a given number of pursuers, executed independently by each agent at run-time. The training uses curriculum learning, a sweeping-angle ordering to locally represent neighboring agents, and a reward structure that encourages a good formation and combines individual and group rewards. Simulated experiments with a reactive evader and up to eight pursuers show that our learning-based approach outperforms recent reinforcement learning techniques as well as non-holonomic adaptations of classical algorithms. The learned policy is successfully transferred to the real-world in a proof-of-concept demonstration with three motion-constrained pursuer drones.","2377-3766","","10.1109/LRA.2021.3068952","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387125","Multi-robot systems;reinforcement learning;cooperating robots","Reinforcement learning;Games;Drones;Kinematics;Task analysis;Trajectory;Training","learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems","motion-constrained pursuer drones;learned policy;recent reinforcement learning techniques;learning-based approach;simulated experiments;group rewards;combines individual;good formation;curriculum learning;kinematic constraints;homogeneous agents;multiple agents;omnidirectional target;pursuers;mobile targets;pursuit-evasion;deep reinforcement learning;decentralized multiagent pursuit","","39","","41","IEEE","25 Mar 2021","","","IEEE","IEEE Journals"
"Simulation-Based Reinforcement Learning for Real-World Autonomous Driving","B. Osiński; A. Jakubowski; P. Zięcina; P. Miłoś; C. Galias; S. Homoceanu; H. Michalewski","deepsense.ai, Warsaw, Poland; deepsense.ai, Warsaw, Poland; deepsense.ai, Warsaw, Poland; deepsense.ai, Warsaw, Poland; deepsense.ai, Warsaw, Poland; Volkswagen AG, Wolfsburg, Germany; University of Warsaw, Warsaw, Poland","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","6411","6418","We use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. The driving policy takes RGB images from a single camera and their semantic segmentation as input. We use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network.Using reinforcement learning in simulation and synthetic data is motivated by lowering costs and engineering effort.In real-world experiments we confirm that we achieved successful sim-to-real policy transfer. Based on the extensive evaluation, we analyze how design decisions about perception, control, and training impact the real-world performance.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196730","","Training;Visualization;Learning (artificial intelligence);Semantics;Robots;Image segmentation;Predictive models","image segmentation;learning (artificial intelligence);road vehicles;traffic engineering computing","simulation-based reinforcement learning;real-world autonomous driving;driving system;real-world vehicle;driving policy;RGB images;single camera;semantic segmentation;synthetic data;real-world data;segmentation network;real-world experiments;sim-to-real policy transfer;real-world performance","","37","","45","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Dynamic Cloth Manipulation with Deep Reinforcement Learning","R. Jangir; G. Alenyà; C. Torras","Institut de Robòtica i Informàtica Industrial, Barcelona, Spain; Institut de Robòtica i Informàtica Industrial, Barcelona, Spain; Institut de Robòtica i Informàtica Industrial, Barcelona, Spain","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","4630","4636","In this paper we present a Deep Reinforcement Learning approach to solve dynamic cloth manipulation tasks. Differing from the case of rigid objects, we stress that the followed trajectory (including speed and acceleration) has a decisive influence on the final state of cloth, which can greatly vary even if the positions reached by the grasped points are the same. We explore how goal positions for non-grasped points can be attained through learning adequate trajectories for the grasped points. Our approach uses few demonstrations to improve control policy learning, and a sparse reward approach to avoid engineering complex reward functions. Since perception of textiles is challenging, we also study different state representations to assess the minimum observation space required for learning to succeed. Finally, we compare different combinations of control policy encodings, demonstrations, and sparse reward learning techniques, and show that our proposed approach can learn dynamic cloth manipulation in an efficient way, i.e., using a reduced observation space, a few demonstrations, and a sparse reward.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196659","","Task analysis;Manipulator dynamics;Trajectory;Textiles;Deformable models;Dynamics","clothing;learning (artificial intelligence)","sparse reward learning techniques;deep reinforcement learning approach;dynamic cloth manipulation tasks;rigid objects;followed trajectory;grasped points;goal positions;nongrasped points;adequate trajectories;control policy learning;sparse reward approach;engineering complex reward functions;state representations;control policy encodings","","37","","36","EU","15 Sep 2020","","","IEEE","IEEE Conferences"
"Driving in Dense Traffic with Model-Free Reinforcement Learning","D. M. Saxena; S. Bae; A. Nakhaei; K. Fujimura; M. Likhachev","Carnegie Mellon University, USA; University of California, Berkeley, USA; Honda Research Institute USA, Inc; Honda Research Institute USA, Inc; Carnegie Mellon University, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","5385","5392","Traditional planning and control methods could fail to find a feasible trajectory for an autonomous vehicle to execute amongst dense traffic on roads. This is because the obstacle-free volume in spacetime is very small in these scenarios for the vehicle to drive through. However, that does not mean the task is infeasible since human drivers are known to be able to drive amongst dense traffic by leveraging the cooperativeness of other drivers to open a gap. The traditional methods fail to take into account the fact that the actions taken by an agent affect the behaviour of other vehicles on the road. In this work, we rely on the ability of deep reinforcement learning to implicitly model such interactions and learn a continuous control policy over the action space of an autonomous vehicle. The application we consider requires our agent to negotiate and open a gap in the road in order to successfully merge or change lanes. Our policy learns to repeatedly probe into the target road lane while trying to find a safe spot to move in to. We compare against two model-predictive control-based algorithms and show that our policy outperforms them in simulation. As part of this work, we introduce a benchmark for driving in dense traffic for use by the community.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197132","","Roads;Autonomous vehicles;Learning (artificial intelligence);Task analysis;Benchmark testing;Trajectory","collision avoidance;learning (artificial intelligence);predictive control;road traffic control;traffic engineering computing","dense traffic;model-free reinforcement learning;control methods;autonomous vehicle;obstacle-free volume;deep reinforcement learning;continuous control policy;target road lane;model-predictive control-based algorithms","","36","","37","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Control Synthesis from Linear Temporal Logic Specifications using Model-Free Reinforcement Learning","A. K. Bozkurt; Y. Wang; M. M. Zavlanos; M. Pajic","Duke University, Durham, NC, USA; Duke University, Durham, NC, USA; Duke University, Durham, NC, USA; Duke University, Durham, NC, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","10349","10355","We present a reinforcement learning (RL) frame-work to synthesize a control policy from a given linear temporal logic (LTL) specification in an unknown stochastic environment that can be modeled as a Markov Decision Process (MDP). Specifically, we learn a policy that maximizes the probability of satisfying the LTL formula without learning the transition probabilities. We introduce a novel rewarding and discounting mechanism based on the LTL formula such that (i) an optimal policy maximizing the total discounted reward effectively maximizes the probabilities of satisfying LTL objectives, and (ii) a model-free RL algorithm using these rewards and discount factors is guaranteed to converge to such a policy. Finally, we illustrate the applicability of our RL-based synthesis approach on two motion planning case studies.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196796","","Learning (artificial intelligence);Automata;Planning;Markov processes;Computational modeling;Task analysis","control system synthesis;decision theory;learning (artificial intelligence);Markov processes;mobile robots;path planning;probability;temporal logic","motion planning;MDP;RL-based synthesis approach;discount factors;model-free RL algorithm;total discounted reward;optimal policy;transition probabilities;LTL formula;Markov decision process;unknown stochastic environment;control policy synthesis;reinforcement learning frame-work;model-free reinforcement learning;linear temporal logic specifications;control synthesis","","36","","35","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Super-Human Performance in Gran Turismo Sport Using Deep Reinforcement Learning","F. Fuchs; Y. Song; E. Kaufmann; D. Scaramuzza; P. Dürr","Sony Europe B.V., Schlieren/Switzerland Branch, Zurich, Switzerland; Robotics and Perception Group, University of Zurich, Zurich, Switzerland; Robotics and Perception Group, University of Zurich, Zurich, Switzerland; Robotics and Perception Group, University of Zurich, Zurich, Switzerland; Sony Europe B.V., Schlieren/Switzerland Branch, Zurich, Switzerland","IEEE Robotics and Automation Letters","9 Apr 2021","2021","6","3","4257","4264","Autonomous car racing is a major challenge in robotics. It raises fundamental problems for classical approaches such as planning minimum-time trajectories under uncertain dynamics and controlling the car at the limits of its handling. Besides, the requirement of minimizing the lap time, which is a sparse objective, and the difficulty of collecting training data from human experts have also hindered researchers from directly applying learning-based approaches to solve the problem. In the present work, we propose a learning-based system for autonomous car racing by leveraging a high-fidelity physical car simulation, a course-progress proxy reward, and deep reinforcement learning. We deploy our system in Gran Turismo Sport, a world-leading car simulator known for its realistic physics simulation of different race cars and tracks, which is even used to recruit human race car drivers. Our trained policy achieves autonomous racing performance that goes beyond what had been achieved so far by the built-in AI, and at the same time, outperforms the fastest driver in a dataset of over 50,000 human players.","2377-3766","","10.1109/LRA.2021.3064284","Sony R&D Center Europe Stuttgart Laboratory 1; National Centre of Competence in Research Robotics; Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; SNSF-ERC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9372847","Autonomous agents;reinforcement learning","Autonomous agents;Neural networks;Autonomous automobiles;Reinforcement learning","","","","32","","29","IEEE","8 Mar 2021","","","IEEE","IEEE Journals"
"Robotic Grasping using Deep Reinforcement Learning","S. Joshi; S. Kumra; F. Sahin","Rochester Institute of Technology, Rochester, NY, USA; Rochester Institute of Technology, Rochester, NY, USA; Rochester Institute of Technology, Rochester, NY, USA","2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)","8 Oct 2020","2020","","","1461","1466","In this work, we present a deep reinforcement learning based method to solve the problem of robotic grasping using visio-motor feedback. The use of a deep learning based approach reduces the complexity caused by the use of hand-designed features. Our method uses an off-policy reinforcement learning framework to learn the grasping policy. We use the double deep Q-learning framework along with a novel GraspQ-Network to output grasp probabilities used to learn grasps that maximize the pick success. We propose a visual servoing mechanism that uses a multi-view camera setup that observes the scene which contains the objects of interest. We performed experiments using a Baxter Gazebo simulated environment as well as on the actual robot. The results show that our proposed method outperforms the baseline Q-learning framework and increases grasping accuracy by adapting a multi-view model in comparison to a single-view model.","2161-8089","978-1-7281-6904-0","10.1109/CASE48305.2020.9216986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9216986","","Grasping;Machine learning;Cameras;Learning (artificial intelligence);Robot vision systems;Grippers","cameras;control engineering computing;learning (artificial intelligence);manipulators;neural nets;probability;visual servoing","single-view model;Baxter Gazebo simulated environment;multiview camera setup;visual servoing mechanism;output grasp probabilities;GraspQ-Network;off-policy reinforcement learning framework;baseline Q-learning framework;actual robot;double deep Q-learning framework;grasping policy;deep learning based approach;visio-motor feedback;deep reinforcement learning;robotic grasping","","32","","20","IEEE","8 Oct 2020","","","IEEE","IEEE Conferences"
"Human-Centered Collaborative Robots With Deep Reinforcement Learning","A. Ghadirzadeh; X. Chen; W. Yin; Z. Yi; M. Björkman; D. Kragic","Division of Robotics, Perception and Learning (RPL), KTH Royal Institute of Technology, Sweden; Division of Robotics, Perception and Learning (RPL), KTH Royal Institute of Technology, Sweden; Division of Robotics, Perception and Learning (RPL), KTH Royal Institute of Technology, Sweden; Division of Robotics, Perception and Learning (RPL), KTH Royal Institute of Technology, Sweden; Division of Robotics, Perception and Learning (RPL), KTH Royal Institute of Technology, Sweden; Division of Robotics, Perception and Learning (RPL), KTH Royal Institute of Technology, Sweden","IEEE Robotics and Automation Letters","15 Jan 2021","2021","6","2","566","571","We present a reinforcement learning based framework for human-centered collaborative systems. The framework is proactive and balances the benefits of timely actions with the risk of taking improper actions by minimizing the total time spent to complete the task. The framework is learned end-to-end in an unsupervised fashion addressing the perception uncertainties and decision making in an integrated manner. The framework is shown to provide more time-efficient coordination between human and robot partners on an example task of packaging compared to alternatives for which perception and decision-making systems are learned independently, using supervised learning. Two important benefits of the proposed approach are that tedious annotation of motion data is avoided, and the learning is performed on-line.","2377-3766","","10.1109/LRA.2020.3047730","Stiftelsen för Strategisk Forskning; Knut och Alice Wallenbergs Stiftelse; European Research Council(grant numbers:884 807); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9309387","Human-Centered robotics;human-robot collaboration;reinforcement learning","Robots;Task analysis;Robot kinematics;Collaboration;Uncertainty;Planning;Reinforcement learning","control engineering computing;decision making;deep learning (artificial intelligence);human-robot interaction;supervised learning","human-centered collaborative robots;deep reinforcement learning;human-centered collaborative systems;perception uncertainties;time-efficient coordination;decision-making systems;supervised learning","","31","","31","IEEE","28 Dec 2020","","","IEEE","IEEE Journals"
"DeepRacer: Autonomous Racing Platform for Experimentation with Sim2Real Reinforcement Learning","B. Balaji; S. Mallya; S. Genc; S. Gupta; L. Dirac; V. Khare; G. Roy; T. Sun; Y. Tao; B. Townsend; E. Calleja; S. Muralidhara; D. Karuppasamy",Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services,"2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","2746","2754","DeepRacer is a platform for end-to-end experimentation with RL and can be used to systematically investigate the key challenges in developing intelligent control systems. Using the platform, we demonstrate how a 1/18th scale car can learn to drive autonomously using RL with a monocular camera. It is trained in simulation with no additional tuning in the physical world and demonstrates: 1) formulation and solution of a robust reinforcement learning algorithm, 2) narrowing the reality gap through joint perception and dynamics, 3) distributed on-demand compute architecture for training optimal policies, and 4) a robust evaluation method to identify when to stop training. It is the first successful large-scale deployment of deep reinforcement learning on a robotic control agent that uses only raw camera images as observations and a model-free learning method to perform robust path planning. We open source our code and video demo on GitHub2.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197465","","Training;Automobiles;Robots;Computational modeling;Cameras;Robustness;Navigation","intelligent robots;learning (artificial intelligence);mobile robots;path planning;robot dynamics;robot vision","reality gap;joint perception;on-demand compute architecture;training optimal policies;robust evaluation;deep reinforcement learning;robotic control agent;raw camera images;robust path planning;DeepRacer;autonomous racing platform;Sim2Real reinforcement learning;end-to-end experimentation;RL;intelligent control systems;monocular camera;physical world;robust reinforcement learning;model-free learning","","31","","91","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement Learning","P. Cai; H. Wang; H. Huang; Y. Liu; M. Liu","Hong Kong University of Science and Technology, Hong Kong; Hong Kong University of Science and Technology, Hong Kong; Hong Kong University of Science and Technology, Hong Kong; Hong Kong University of Science and Technology, Hong Kong; Hong Kong University of Science and Technology, Hong Kong","IEEE Robotics and Automation Letters","29 Jul 2021","2021","6","4","7262","7269","Autonomous car racing is a challenging task in the robotic control area. Traditional modular methods require accurate mapping, localization and planning, which makes them computationally inefficient and sensitive to environmental changes. Recently, deep-learning-based end-to-end systems have shown promising results for autonomous driving/racing. However, they are commonly implemented by supervised imitation learning (IL), which suffers from the distribution mismatch problem, or by reinforcement learning (RL), which requires a huge amount of risky interaction data. In this work, we present a general deep imitative reinforcement learning approach (DIRL), which successfully achieves agile autonomous racing using visual inputs. The driving knowledge is acquired from both IL and model-based RL, where the agent can learn from human teachers as well as perform self-improvement by safely interacting with an offline world model. We validate our algorithm both in a high-fidelity driving simulation and on a real-world 1/20-scale RC-car with limited onboard computation. The evaluation results demonstrate that our method outperforms previous IL and RL methods in terms of sample efficiency and task performance. Demonstration videos are available at https://caipeide.github.io/autorace-dirl/.","2377-3766","","10.1109/LRA.2021.3097345","Zhongshan Municipal Science and Technology Bureau Fund(grant numbers:ZSST21EG06); Collaborative Research Fund by Research Grants Council Hong Kong(grant numbers:C4063-18G); Department of Science and Technology of Guangdong Province Fund(grant numbers:GDST20EG54); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9488179","Reinforcement learning;imitation learning;model learning for control;autonomous racing;uncertainty awareness","Task analysis;Computational modeling;Automobiles;Training;Robots;Sensors;Reinforcement learning","automobiles;control engineering computing;deep learning (artificial intelligence);robot vision;traffic engineering computing","robotic control area;localization;planning;environmental changes;deep-learning-based end-to-end systems;supervised imitation learning;deep imitative reinforcement learning;agile autonomous racing;driving knowledge;model-based RL;high-fidelity driving simulation;vision-based autonomous car racing","","28","","30","IEEE","16 Jul 2021","","","IEEE","IEEE Journals"
"Model-Based Meta-Reinforcement Learning for Flight With Suspended Payloads","S. Belkhale; R. Li; G. Kahn; R. McAllister; R. Calandra; S. Levine","University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA; University of California, Berkeley, CA, USA; Facebook AI Research, Menlo Park, CA, USA; University of California, Berkeley, CA, USA","IEEE Robotics and Automation Letters","25 Feb 2021","2021","6","2","1471","1478","Transporting suspended payloads is challenging for autonomous aerial vehicles because the payload can cause significant and unpredictable changes to the robot's dynamics. These changes can lead to suboptimal flight performance or even catastrophic failure. Although adaptive control and learning-based methods can in principle adapt to changes in these hybrid robot-payload systems, rapid mid-flight adaptation to payloads that have a priori unknown physical properties remains an open problem. We propose a meta-learning approach that “learns how to learn” models of altered dynamics within seconds of post-connection flight data. Our experiments demonstrate that our online adaptation approach outperforms non-adaptive methods on a series of challenging suspended payload transportation tasks. Videos and other supplemental material are available on our website: https://sites.google.com/view/meta-rl-for-flight","2377-3766","","10.1109/LRA.2021.3057046","National Science Foundation(grant numbers:IIS-1700697,IIS-1651843); ARL DCIST CRA(grant numbers:W911NF-17-2-0181); NASA ESI; Defense Advanced Research Projects Agency; Office of Naval Research; Google; NVIDIA; Amazon Catalyst; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9345959","Machine learning for robot control;probabilistic inference;reinforcement learning","Payloads;Adaptation models;Task analysis;Robots;Heuristic algorithms;Mathematical model;Data models","adaptive control;autonomous aerial vehicles;learning (artificial intelligence);manipulator dynamics;mobile robots","model-based meta-reinforcement learning;suspended payloads;autonomous aerial vehicles;significant changes;unpredictable changes;suboptimal flight performance;catastrophic failure;adaptive control;learning-based methods;hybrid robot-payload systems;mid-flight adaptation;a priori unknown physical properties;meta-learning approach;altered dynamics;post-connection flight data;online adaptation approach;nonadaptive methods;payload transportation tasks","","28","","35","IEEE","3 Feb 2021","","","IEEE","IEEE Journals"
"“Good Robot!”: Efficient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer","A. Hundt; B. Killeen; N. Greene; H. Wu; H. Kwon; C. Paxton; G. D. Hager","The Johns Hopkins University, Baltimore, USA; The Johns Hopkins University, Baltimore, USA; The Johns Hopkins University, Baltimore, USA; The Johns Hopkins University, Baltimore, USA; The Johns Hopkins University, Baltimore, USA; NVIDIA, Seattle, USA; The Johns Hopkins University, Baltimore, USA","IEEE Robotics and Automation Letters","27 Aug 2020","2020","5","4","6724","6731","Current Reinforcement Learning (RL) algorithms struggle with long-horizon tasks where time can be wasted exploring dead ends and task progress may be easily reversed. We develop the SPOT framework, which explores within action safety zones, learns about unsafe regions without exploring them, and prioritizes experiences that reverse earlier progress to learn with remarkable efficiency. The SPOT framework successfully completes simulated trials of a variety of tasks, improving a baseline trial success rate from 13% to 100% when stacking 4 cubes, from 13% to 99% when creating rows of 4 cubes, and from 84% to 95% when clearing toys arranged in adversarial patterns. Efficiency with respect to actions per trial typically improves by 30% or more, while training takes just 1-20 k actions, depending on the task. Furthermore, we demonstrate direct sim to real transfer. We are able to create real stacks in 100% of trials with 61% efficiency and real rows in 100% of trials with 59% efficiency by directly loading the simulation-trained model on the real robot with no additional real-world fine-tuning. To our knowledge, this is the first instance of reinforcement learning with successful sim to real transfer applied to long term multi-step tasks such as block-stacking and row-making with consideration of progress reversal. Code is available at https://github.com/jhu-lcsr/good_robot.","2377-3766","","10.1109/LRA.2020.3015448","NSF NRI(grant numbers:1637949,1763705); Office of Naval Research(grant numbers:N00014-17-1-2124); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165109","Computer vision for other robotic applications;deep learning in grasping and manipulation;reinforcement learning","Task analysis;Learning (artificial intelligence);Robot sensing systems;Stacking;Training;Grasping","control engineering computing;learning (artificial intelligence);robot vision","robot;multistep visual tasks;SPOT framework;action safety zones;adversarial patterns;reinforcement learning","","27","","44","IEEE","11 Aug 2020","","","IEEE","IEEE Journals"
"Meta Reinforcement Learning for Sim-to-real Domain Adaptation","K. Arndt; M. Hazara; A. Ghadirzadeh; V. Kyrki","Aalto University, Espoo, Finland; Aalto University, Espoo, Finland; Aalto University, Espoo, Finland; Aalto University, Espoo, Finland","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","2725","2731","Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196540","","Adaptation models;Task analysis;Trajectory;Robots;Training;Learning (artificial intelligence);Heuristic algorithms","learning (artificial intelligence);medical robotics","dynamic conditions;task-specific trajectory generation model;KUKA LBR 4+ robot;sim-to-real domain transfer;robotic policy training;meta reinforcement learning","","27","","32","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Automatic Gesture Recognition in Robot-assisted Surgery with Reinforcement Learning and Tree Search","X. Gao; Y. Jin; Q. Dou; P. -A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; CUHK T Stone Robotics Institute; CUHK T Stone Robotics Institute","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","8440","8446","Automatic surgical gesture recognition is fundamental for improving intelligence in robot-assisted surgery, such as conducting complicated tasks of surgery surveillance and skill evaluation. However, current methods treat each frame individually and produce the outcomes without effective consideration on future information. In this paper, we propose a framework based on reinforcement learning and tree search for joint surgical gesture segmentation and classification. An agent is trained to segment and classify the surgical video in a human-like manner whose direct decisions are re-considered by tree search appropriately. Our proposed tree search algorithm unites the outputs from two designed neural networks, i.e., policy and value network. With the integration of complementary information from distinct models, our framework is able to achieve the better performance than baseline methods using either of the neural networks. For an overall evaluation, our developed approach consistently outperforms the existing methods on the suturing task of JIGSAWS dataset in terms of accuracy, edit score and F1 score. Our study highlights the utilization of tree search to refine actions in reinforcement learning framework for surgical robotic applications.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196674","Surgical gesture recognition;Deep reinforcement learning in robotics;Tree search;Robotic surgery","Surgery;Gesture recognition;Robots;Hidden Markov models;Learning (artificial intelligence);Feature extraction;Task analysis","gesture recognition;image classification;image segmentation;learning (artificial intelligence);medical robotics;neural nets;surgery;tree searching;video signal processing;video surveillance","joint surgical gesture segmentation;tree search algorithm;neural networks design;reinforcement learning framework;surgical robotic applications;surgical video classification;baseline methods;JIGSAWS dataset;surgery surveillance;automatic surgical gesture recognition;robot-assisted surgery","","26","","35","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Towards Practical Multi-Object Manipulation using Relational Reinforcement Learning","R. Li; A. Jabri; T. Darrell; P. Agrawal","Massachusetts Institute of Technology, USA; University of California Berkeley; University of California Berkeley; Massachusetts Institute of Technology, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","4051","4058","Learning robotic manipulation tasks using reinforcement learning with sparse rewards is currently impractical due to the outrageous data requirements. Many practical tasks require manipulation of multiple objects, and the complexity of such tasks increases with the number of objects. Learning from a curriculum of increasingly complex tasks appears to be a natural solution, but unfortunately, does not work for many scenarios. We hypothesize that the inability of the state- of-the-art algorithms to effectively utilize a task curriculum stems from the absence of inductive biases for transferring knowledge from simpler to complex tasks. We show that graph-based relational architectures overcome this limitation and enable learning of complex tasks when provided with a simple curriculum of tasks with increasing numbers of objects. We demonstrate the utility of our framework on a simulated block stacking task. Starting from scratch, our agent learns to stack six blocks into a tower. Despite using step-wise sparse rewards, our method is orders of magnitude more data- efficient and outperforms the existing state-of-the-art method that utilizes human demonstrations. Furthermore, the learned policy exhibits zero-shot generalization, successfully stacking blocks into taller towers and previously unseen configurations such as pyramids, without any further training.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197468","","Task analysis;Stacking;Robots;Learning (artificial intelligence);Poles and towers;Training;Three-dimensional displays","graph theory;learning (artificial intelligence);manipulators;mobile robots;neural nets","multiobject manipulation;relational reinforcement learning;learning robotic manipulation tasks;outrageous data requirements;task curriculum;graph-based relational architectures;simulated block stacking task;step-wise sparse rewards;zero-shot generalization","","25","","64","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Socially Compliant Robot Navigation in Crowded Environment by Human Behavior Resemblance Using Deep Reinforcement Learning","S. S. Samsani; M. S. Muhammad","Department of Artificial Intelligence, Sungkyunkwan University, Natural Sciences Campus, Suwon, South Korea; Department of Electrical and Computer Engineering, Sungkyunkwan University, Natural Sciences Campus, Suwon, South Korea","IEEE Robotics and Automation Letters","26 Apr 2021","2021","6","3","5223","5230","Social robots have evolved in diverse applications with the emergence of deep reinforcement learning methods. However, safe and secure navigation of social robots in a complex crowded environment remains a challenging task. The robot can safely navigate in a crowd only if it can predict the next action of humans, however this task becomes difficult because of the unpredictable human behavior. To address the issue of socially compliant navigation, the robot needs to learn real-time human behavior. This manuscript models Danger-Zone for the robot by considering all possible actions that humans can take at given time. The Danger Zones are formulated by considering the real time human behavior. The robot is trained to avoid these danger zones for safe and secure navigation. The proposed model is tested on the three state of art methods, Collision Avoidance with Deep Reinforcement Learning (CADRL), Long Short Term Memory Reinforcement Learning (LSTM-RL) and Social Attention with Reinforcement Learning (SARL) in multi-agent navigation. Experimental results signify that proposed model can understand human behavior and navigate in a socially compliant manner with safety as the highest priority.","2377-3766","","10.1109/LRA.2021.3071954","Institute of Information and Communications Technology Planning and Evaluation (IITP); Government of Korea(grant numbers:2019-0-00421); Artificial Intelligence Graduate School Program(grant numbers:Sungkyunkwan University); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9399789","Safety in HRI;human-aware motion planning;reinforcement learning;collision avoidance;and autonomous agents","Robots;Navigation;Real-time systems;Collision avoidance;Reinforcement learning;Geometry;Task analysis","collision avoidance;control engineering computing;deep learning (artificial intelligence);humanoid robots;human-robot interaction;mobile robots;multi-agent systems;multi-robot systems;navigation;recurrent neural nets","human behavior resemblance;social robots;safe navigation;secure navigation;complex crowded environment;unpredictable human behavior;socially compliant navigation;real-time human behavior;danger zones;long short term memory;multiagent navigation;deep reinforcement learning;socially compliant robot navigation;collision avoidance;CADRL;LSTM-RL;social attention with reinforcement learning;SARL","","24","","34","IEEE","9 Apr 2021","","","IEEE","IEEE Journals"
"Reinforcement Learned Distributed Multi-Robot Navigation With Reciprocal Velocity Obstacle Shaped Rewards","R. Han; S. Chen; S. Wang; Z. Zhang; R. Gao; Q. Hao; J. Pan","Department of Computer Science, The University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, Guangdong, China; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, Guangdong, China; Department of Computer Science, The University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, Guangdong, China; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, Guangdong, China; Department of Computer Science, The University of Hong Kong, Hong Kong","IEEE Robotics and Automation Letters","7 Apr 2022","2022","7","3","5896","5903","The challenges to solving the collision avoidance problem lie in adaptively choosing optimal robot velocities in complex scenarios full of interactive obstacles. In this letter, we propose a distributed approach for multi-robot navigation which combines the concept of reciprocal velocity obstacle (RVO) and the scheme of deep reinforcement learning (DRL) to solve the reciprocal collision avoidance problem under limited information. The novelty of this work is threefold: (1) using a set of sequential VO and RVO vectors to represent the interactive environmental states of static and dynamic obstacles, respectively; (2) developing a bidirectional recurrent module based neural network, which maps the states of a varying number of surrounding obstacles to the actions directly; (3) developing a RVO area and expected collision time based reward function to encourage reciprocal collision avoidance behaviors and trade off between collision risk and travel time. The proposed policy is trained through simulated scenarios and updated by the actor-critic based DRL algorithm. We validate the policy in complex environments with various numbers of differential drive robots and obstacles. The experiment results demonstrate that our approach outperforms the state-of-art methods and other learning based approaches in terms of the success rate, travel time, and average speed.","2377-3766","","10.1109/LRA.2022.3161699","Shenzhen Fundamental Research Program(grant numbers:JCYJ20200109141622964); HKSAR RGC GRF(grant numbers:11202119,11207818); Innovation and HKSAR Technology Commission; InnoHK Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9740403","Collision avoidance;reinforcement learning;multi-robot systems","Collision avoidance;Robots;Robot sensing systems;Robot kinematics;Navigation;Sensors;Feature extraction","collision avoidance;learning (artificial intelligence);mobile robots;multi-robot systems","reinforcement learned distributed multirobot navigation;reciprocal velocity obstacle shaped rewards;optimal robot velocities;interactive obstacles;distributed approach;deep reinforcement learning;reciprocal collision avoidance problem;sequential VO;interactive environmental states;static obstacles;dynamic obstacles;bidirectional recurrent module;RVO area;collision time based reward function;collision risk;travel time;simulated scenarios;actor-critic based DRL algorithm;complex environments;differential drive robots;learning based approaches","","23","","27","IEEE","23 Mar 2022","","","IEEE","IEEE Journals"
"Analyzing the Suitability of Cost Functions for Explaining and Imitating Human Driving Behavior based on Inverse Reinforcement Learning","M. Naumann; L. Sun; W. Zhan; M. Tomizuka","FZI Research Center for Information Technology, and Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA; Department of Mechanical Engineering, University of California, Berkeley, CA, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","5481","5487","Autonomous vehicles are sharing the road with human drivers. In order to facilitate interactive driving and cooperative behavior in dense traffic, a thorough understanding and representation of other traffic participants' behavior are necessary. Cost functions (or reward functions) have been widely used to describe the behavior of human drivers since they can not only explicitly incorporate the rationality of human drivers and the theory of mind (TOM), but also share similarity with the motion planning problem of autonomous vehicles. Hence, more human-like driving behavior and comprehensible trajectories can be generated to enable safer interaction and cooperation. However, the selection of cost functions in different driving scenarios is not trivial, and there is no systematic summary and analysis for cost function selection and learning from a variety of driving scenarios. In this work, we aim to investigate to what extent cost functions are suitable for explaining and imitating human driving behavior. Further, we focus on how cost functions differ from each other in different driving scenarios. Towards this goal, we first comprehensively review existing cost function structures in literature. Based on that, we point out required conditions for demonstrations to be suitable for inverse reinforcement learning (IRL). Finally, we use IRL to explore suitable features and learn cost function weights from human driven trajectories in three different scenarios.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196795","Automated vehicles;cost function;inverse reinforcement learning;imitation learning;cooperative motion planning","Cost function;Trajectory;Vehicles;Roads;Learning (artificial intelligence);Safety;Planning","behavioural sciences computing;driver information systems;learning (artificial intelligence);path planning;road safety;road traffic;road vehicles;vehicles","human drivers;interactive driving;cooperative behavior;dense traffic;autonomous vehicles;safer interaction;cost function selection;cost function structures;inverse reinforcement learning;human driven trajectories;human driving behavior imitation;human driving behavior explanation;theory of mind","","23","","21","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Discrete Deep Reinforcement Learning for Mapless Navigation","E. Marchesini; A. Farinelli","Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","10688","10694","Our goal is to investigate whether discrete state space algorithms are a viable solution to continuous alternatives for mapless navigation. To this end we present an approach based on Double Deep Q-Network and employ parallel asynchronous training and a multi-batch Priority Experience Replay to reduce the training time. Experiments show that our method trains faster and outperforms both the continuous Deep Deterministic Policy Gradient and Proximal Policy Optimization algorithms. Moreover, we train the models in a custom environment built on the recent Unity learning toolkit and show that they can be exported on the TurtleBot3 simulator and to the real robot without further training. Overall our optimized method is 40% faster compared to the original discrete algorithm. This setting significantly reduces the training times with respect to the continuous algorithms, maintaining a similar level of success rate hence being a viable alternative for mapless navigation.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196739","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196739","","Training;Navigation;Robot kinematics;Robot sensing systems;Optimization;Machine learning","discrete systems;gradient methods;learning (artificial intelligence);mobile robots;navigation;optimisation;state-space methods","mapless navigation;discrete state space algorithms;continuous alternatives;double deep Q-network;parallel asynchronous training;training time;proximal policy optimization algorithms;original discrete algorithm;continuous algorithms;continuous deep deterministic policy gradient;multibatch priority experience replay;discrete deep reinforcement","","23","","34","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Model-Plant Mismatch Compensation Using Reinforcement Learning","I. Koryakovskiy; M. Kudruss; H. Vallery; R. Babuška; W. Caarls","Robotics Institute, TU Delft, Delft, CD, The Netherlands; Interdisciplinary Center for Scientific Computing, Heidelberg University, Heidelberg, Germany; Robotics Institute, TU Delft, Delft, CD, The Netherlands; Robotics Institute, TU Delft, Delft, CD, The Netherlands; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil","IEEE Robotics and Automation Letters","29 Mar 2018","2018","3","3","2471","2477","Learning-based approaches are suitable for the control of systems with unknown dynamics. However, learning from scratch involves many trials with exploratory actions until a good control policy is discovered. Real robots usually cannot withstand the exploratory actions and suffer damage. This problem can be circumvented by combining learning with a model-based control. In this letter, we employ a nominal model-predictive controller that is impeded by the presence of an unknown model-plant mismatch. To compensate for the mismatch, we propose two approaches of combining reinforcement learning with the nominal controller. The first approach learns a compensatory control action that minimizes the same performance measure as is minimized by the nominal controller. The second approach learns a compensatory signal from a difference of a transition predicted by the internal model and an actual transition. We compare the approaches on a robot attached to the ground and performing a setpoint reaching task in simulations. We implement the better approach on the real robot and demonstrate successful learning results.","2377-3766","","10.1109/LRA.2018.2800106","European Project (KoroiBot)(grant numbers:FP7-ICT-2013-10/611909); ROBOTIS which provided motors for Leo; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8276249","Learning and adaptive systems;humanoid robots","Robots;Uncertainty;Adaptation models;Learning (artificial intelligence);Computational modeling;Safety;Trajectory","control engineering computing;learning (artificial intelligence);predictive control;robot dynamics;robot programming","model-plant mismatch compensation;unknown dynamics;nominal model-predictive controller;unknown model-plant mismatch;nominal controller;compensatory control action;reinforcement learning;real robots;model-based control","","21","","29","IEEE","31 Jan 2018","","","IEEE","IEEE Journals"
"Cooperative Multi-Robot Navigation in Dynamic Environment with Deep Reinforcement Learning","R. Han; S. Chen; Q. Hao","Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","448","454","The challenges of multi-robot navigation in dynamic environments lie in uncertainties in obstacle complexities, partially observation of robots, and policy implementation from simulations to the real world. This paper presents a cooperative approach to address the multi-robot navigation problem (MRNP) under dynamic environments using a deep reinforcement learning (DRL) framework, which can help multiple robots jointly achieve optimal paths despite a certain degree of obstacle complexities. The novelty of this work includes threefold: (1) developing a cooperative architecture that robots can exchange information with each other to select the optimal target locations; (2) developing a DRL based framework which can learn a navigation policy to generate the optimal paths for multiple robots; (3) developing a training mechanism based on dynamics randomization which can make the policy generalized and achieve the maximum performance in the real world. The method is tested with Gazebo simulations and 4 differential drive robots. Both simulation and experiment results validate the superior performance of the proposed method in terms of success rate and travel time when compared with the other state-of-art technologies.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197209","","Collision avoidance;Navigation;Robot sensing systems;Robot kinematics;Training;Adaptation models","control engineering computing;learning (artificial intelligence);mobile robots;multi-robot systems;navigation;path planning","optimal paths;multiple robots;dynamics randomization;differential drive robots;dynamic environment;obstacle complexities;multirobot navigation problem;deep reinforcement learning framework;optimal target locations;DRL based framework;navigation policy","","20","","24","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Reachability-Based Trajectory Safeguard (RTS): A Safe and Fast Reinforcement Learning Safety Layer for Continuous Control","Y. S. Shao; C. Chen; S. Kousik; R. Vasudevan","School of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Robotics Institute, University of Michigan, Ann Arbor, MI, USA; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA; School of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA","IEEE Robotics and Automation Letters","26 Mar 2021","2021","6","2","3663","3670","Reinforcement Learning (RL) algorithms have achieved remarkable performance in decision making and control tasks by reasoning about long-term, cumulative reward using trial and error. However, during RL training, applying this trial-and-error approach to real-world robots operating in safety critical environment may lead to collisions. To address this challenge, this letter proposes a Reachability-based Trajectory Safeguard (RTS), which leverages reachability analysis to ensure safety during training and operation. Given a known (but uncertain) model of a robot, RTS precomputes a Forward Reachable Set of the robot tracking a continuum of parameterized trajectories. At runtime, the RL agent selects from this continuum in a receding-horizon way to control the robot; the FRS is used to identify if the agent's choice is safe or not, and to adjust unsafe choices. The efficacy of this method is illustrated in static environments on three nonlinear robot models, including a 12-D quadrotor drone, in simulation and in comparison with state-of-the-art safe motion planning methods.","2377-3766","","10.1109/LRA.2021.3063989","National Science Foundation(grant numbers:1751093); Ford Motor Company; Ford-UM Alliance(grant numbers:N022977); Office of Naval Research(grant numbers:N00014-18-1-2575); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369910","Reinforcement learning;robot safety;task and motion planning","Robots;Safety;Trajectory;Planning;Computational modeling;Collision avoidance;Training","collision avoidance;decision making;learning (artificial intelligence);mobile robots;path planning;reachability analysis;safety","Reachability-based Trajectory Safeguard;RTS;continuous control;decision making;control tasks;cumulative reward;RL training;-error approach;real-world robots;safety critical environment;reachability analysis;Forward Reachable Set;parameterized trajectories;RL agent selects;nonlinear robot models","","20","","32","IEEE","4 Mar 2021","","","IEEE","IEEE Journals"
"Grasping Unknown Objects by Coupling Deep Reinforcement Learning, Generative Adversarial Networks, and Visual Servoing","O. -M. Pedersen; E. Misimi; F. Chaumette","Norwegian Univ. of Science and Technology, Trondheim, Norway; SINTEF Ocean, Trondheim, Norway; Inria, Univ. Rennes, Rennes, France","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","5655","5662","In this paper, we propose a novel approach for transferring a deep reinforcement learning (DRL) grasping agent from simulation to a real robot, without fine tuning in the real world. The approach utilises a CycleGAN to close the reality gap between the simulated and real environments, in a reverse real-to-sim manner, effectively ""tricking"" the agent into believing it is still in the simulator. Furthermore, a visual servoing (VS) grasping task is added to correct for inaccurate agent gripper pose estimations derived from deep learning. The proposed approach is evaluated by means of real grasping experiments, achieving a success rate of 83 % on previously seen objects, and the same success rate for previously unseen, semi-compliant objects. The robustness of the approach is demonstrated by comparing it with two baselines, DRL plus CycleGAN, and VS only. The results clearly show that our approach outperforms both baselines.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197196","","Grasping;Robots;Grippers;Training;Task analysis;Gallium nitride;Image segmentation","grippers;learning (artificial intelligence);neural nets;object recognition;pose estimation;robot vision;visual servoing","semicompliant objects;grasping experiments;deep learning;inaccurate agent gripper;visual servoing grasping task;CycleGAN;DRL;deep reinforcement learning grasping agent;generative adversarial networks","","20","","39","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Robust Model Predictive Shielding for Safe Reinforcement Learning with Stochastic Dynamics","S. Li; O. Bastani","GRASP Lab, University of Pennsylvania, USA; Department of Computer and Information Science, University of Pennsylvania, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","7166","7172","We propose a framework for safe reinforcement learning that can handle stochastic nonlinear dynamical systems. We focus on the setting where the nominal dynamics are known, and are subject to additive stochastic disturbances with known distribution. Our goal is to ensure the safety of a control policy trained using reinforcement learning, e.g., in a simulated environment. We build on the idea of model predictive shielding (MPS), where a backup controller is used to override the learned policy as needed to ensure safety. The key challenge is how to compute a backup policy in the context of stochastic dynamics. We propose to use a tube-based robust nonlinear model predictive controller (NMPC) as the backup controller. We estimate the tubes using sampled trajectories, leveraging ideas from statistical learning theory to obtain high-probability guarantees. We empirically demonstrate that our approach can ensure safety in stochastic systems, including cart-pole and a non-holonomic particle with random obstacles.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196867","","Safety;Robustness;Stochastic processes;Robots;Trajectory;Nonlinear dynamical systems;Heuristic algorithms","learning (artificial intelligence);mobile robots;nonlinear control systems;nonlinear dynamical systems;predictive control;probability;robust control;stochastic processes;stochastic systems","backup policy;learned policy;control policy;additive stochastic disturbances;nominal dynamics;stochastic nonlinear dynamical systems;stochastic dynamics;safe reinforcement learning;robust model predictive shielding;stochastic systems;statistical learning theory;backup controller;tube-based robust nonlinear model predictive controller","","19","","39","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Mapless Navigation among Dynamics with Social-safety-awareness: a reinforcement learning approach from 2D laser scans","J. Jin; N. M. Nguyen; N. Sakib; D. Graves; H. Yao; M. Jagersand","Department of Computing Science, University of Alberta, Edmonton, Canada; Noah’s Ark Lab, Huawei Technologies Canada, Ltd., Edmonton, Canada; Department of Computing Science, University of Alberta, Edmonton, Canada; Noah’s Ark Lab, Huawei Technologies Canada, Ltd., Edmonton, Canada; Noah’s Ark Lab, Huawei Technologies Canada, Ltd., Edmonton, Canada; Department of Computing Science, University of Alberta, Edmonton, Canada","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","6979","6985","We consider the problem of mapless collision-avoidance navigation where humans are present using 2D laser scans. Our proposed method uses ego-safety to measure collision from the robot's perspective and social-safety to measure the impact of robot's actions on surrounding pedestrians. Specifically, the social-safety part predicts the intrusion impact of the robot's action into the interaction area with surrounding humans. We train the policy using reinforcement learning on a simple simulator and directly evaluate the learned policy in Gazebo and real robot tests. Experiments show the learned policy smoothly transferred to different scenarios without any fine tuning. We observe that our method demonstrates time-efficient path planning behavior with high success rate in the mapless navigation task. Furthermore, we test our method in a navigation task among dynamic crowds, considering both low and high volume traffic. Our learned policy demonstrates cooperative behavior that actively drives our robot into traffic flows while showing respect to nearby pedestrians. Evaluation videos are at https://sites.google.com/view/ssw-batman.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197148","","Collision avoidance;Navigation;Training;Robot sensing systems;Lasers;Path planning","collision avoidance;control engineering computing;learning (artificial intelligence);mobile robots;navigation;path planning;robot dynamics;robot programming","time-efficient path planning behavior;dynamic crowds;social-safety-awareness;reinforcement learning;2D laser scans;mapless collision-avoidance navigation;ego-safety;pedestrians;robot tests","","19","","25","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning in Soft Viscoelastic Actuator of Dielectric Elastomer","L. Li; J. Li; L. Qin; J. Cao; M. S. Kankanhalli; J. Zhu","Department of Mechanical Engineering, National University of Singapore, Singapore; Graduate School of Integrative Science and Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore","IEEE Robotics and Automation Letters","5 Mar 2019","2019","4","2","2094","2100","Dielectric elastomer actuators (DEAs) have been widely employed as artificial muscles in soft robots. Due to material viscoelasticity and nonlinear electromechanical coupling, it is challenging to accurately model a viscoelastic DEA, especially when the actuator is of a complex or irregular configuration. Control of DEAs is thus challenging but significant. In this letter, we propose a model-free method for control of DEAs, based on deep reinforcement learning. We perform dynamic feedback control by considering the time-dependent behavior of DEAs. Our method is generic in that it does not require task-specific knowledge about the structure or material parameters of the DEA. The experiments show that our method is robust to achieve accurate control for the DEAs of different configurations, different prestretches, and at different times (the material property usually changes due to viscoelasticity effects). To the best of our knowledge, this letter is the first effort to explore deep reinforcement learning for control of DEAs.","2377-3766","","10.1109/LRA.2019.2898710","MOE Tier 1, Singapore(grant numbers:R-265-000-609-114); ASTAR, Singapore(grant numbers:R-265-000-629-305); National Research Foundation; Prime Ministers Office, Singapore; Strategic Capability Research Centres Funding Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8638989","Modeling, control, and learning for soft robots;model learning for control","Actuators;Reinforcement learning;Voltage control;Electrodes;Robots;Task analysis;Muscles","dielectric materials;elastomers;electric actuators;electroactive polymer actuators;feedback;learning (artificial intelligence);muscle;viscoelasticity","dielectric elastomer actuators;material viscoelasticity;viscoelastic DEA;model-free method;deep reinforcement learning;dynamic feedback control;viscoelasticity effects;soft viscoelastic actuator","","18","","36","IEEE","10 Feb 2019","","","IEEE","IEEE Journals"
"Data-Driven Reinforcement Learning for Walking Assistance Control of a Lower Limb Exoskeleton with Hemiplegic Patients","Z. Peng; R. Luo; R. Huang; J. Hu; K. Shi; H. Cheng; B. K. Ghosh","Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China; Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China; Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China; Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China; Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China; Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China; Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","9065","9071","Lower limb exoskeleton (LLE) has received considerable interests in strength augmentation, rehabilitation and walking assistance scenarios. For walking assistance, the LLE is expected to have the capability of controlling the affected leg to track the unaffected leg’s motion naturally. An important issue in this scenario is that the exoskeleton system needs to deal with unpredictable disturbance from the patient, which requires the controller of exoskeleton system to have the ability to adapt to different wearers. This paper proposes a novel Data-Driven Reinforcement Learning (DDRL) control strategy to adapt different hemiplegic patients with unpredictable disturbances. In the proposed DDRL strategy, the interaction between two lower limbs of LLE and the legs of hemiplegic patient are modeled in the context of leader-follower framework. The walking assistance control problem is transformed into a optimal control problem. Then, a policy iteration (PI) algorithm is introduced to learn optimal controller. To achieve online adaptation control for different patients, based on PI algorithm, an Actor-Critic Neural Network (ACNN) technology of the reinforcement learning (RL) is employed in the proposed DDRL. We conduct experiments both on a simulation environment and a real LLE system. Experimental results demonstrate that the proposed control strategy has strong robustness against disturbances and adaptability to different pilots.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197229","Data-driven Control;Reinforcement Learning;Leader-Follower Multi-Agent System;Lower Limb Exoskeleton;Hemiplegic Patients;Actor-Critic Neural Network","Legged locomotion;Exoskeletons;Adaptation models;Learning (artificial intelligence);Trajectory;Extremities;Optimal control","adaptive control;artificial limbs;handicapped aids;iterative methods;learning (artificial intelligence);medical robotics;neural nets;optimal control;patient rehabilitation;wearable robots","Data-driven reinforcement learning;lower limb exoskeleton;hemiplegic patient;rehabilitation scenario;affected leg;unaffected leg;exoskeleton system;DDRL strategy;optimal control;policy iteration algorithm;online adaptation control;walking assistance control;walking assistance scenario;strength augmentation scenario;Actor-Critic Neural Network;ACNN","","18","","42","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Visual Navigation in Real-World Indoor Environments Using End-to-End Deep Reinforcement Learning","J. Kulhánek; E. Derner; R. Babuška","Czech Institute of Informatics, Robotics, and Cybernetics, Czech Technical University in Prague, Prague, Czech Republic; Czech Institute of Informatics, Robotics, and Cybernetics, Czech Technical University in Prague, Prague, Czech Republic; Cognitive Robotics, Faculty of 3mE, Delft University of Technology, Delft, The Netherlands","IEEE Robotics and Automation Letters","9 Apr 2021","2021","6","3","4345","4352","Visual navigation is essential for many applications in robotics, from manipulation, through mobile robotics to automated driving. Deep reinforcement learning (DRL) provides an elegant map-free approach integrating image processing, localization, and planning in one module, which can be trained and therefore optimized for a given environment. However, to date, DRL-based visual navigation was validated exclusively in simulation, where the simulator provides information that is not available in the real world, e.g., the robot's position or segmentation masks. This precludes the use of the learned policy on a real robot. Therefore, we present a novel approach that enables a direct deployment of the trained policy on real robots. We have designed a new powerful simulator capable of domain randomization. To facilitate the training, we propose visual auxiliary tasks and a tailored reward scheme. The policy is fine-tuned on images collected from real-world environments. We have evaluated the method on a mobile robot in a real office environment. The training took approximately 30 hours on a single GPU. In 30 navigation experiments, the robot reached a 0.3-meter neighbourhood of the goal in more than 86.7% of cases. This result makes the proposed method directly applicable to tasks like mobile manipulation.","2377-3766","","10.1109/LRA.2021.3068106","European Regional Development Fund(grant numbers:CZ.02.1.01/0.0/0.0/15_003/0000470); European Union's H2020(grant numbers:871449); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384194","Deep learning methods;reinforcement learning;vision-based navigation","Navigation;Training;Deep learning;Visualization;Reinforcement learning","","","","17","","39","IEEE","23 Mar 2021","","","IEEE","IEEE Journals"
"Grasp for Stacking via Deep Reinforcement Learning","J. Zhang; W. Zhang; R. Song; L. Ma; Y. Li","School of Control Science and Engineering, Shandong University, China; School of Control Science and Engineering, Shandong University, China; School of Control Science and Engineering, Shandong University, China; Tencent AI Lab; School of Control Science and Engineering, Shandong University, China","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","2543","2549","Integrated robotic arm system should contain both grasp and place actions. However, most grasping methods focus more on how to grasp objects, while ignoring the placement of the grasped objects, which limits their applications in various industrial environments. In this research, we propose a model-free deep Q-learning method to learn the grasping-stacking strategy end-to-end from scratch. Our method maps the images to the actions of the robotic arm through two deep networks: the grasping network (GNet) using the observation of the desk and the pile to infer the gripper's position and orientation for grasping, and the stacking network (SNet) using the observation of the platform to infer the optimal location when placing the grasped object. To make a long-range planning, the two observations are integrated in the grasping for stacking network (GSN). We evaluate the proposed GSN on a grasping-stacking task in both simulated and real-world scenarios.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197508","","Grasping;Stacking;Task analysis;Feature extraction;Manipulators;Training","grippers;industrial manipulators;learning systems;neurocontrollers;optimal control;stacking","deep reinforcement learning;integrated robotic arm system;object grasping;model-free deep Q-learning method;grasping-stacking task;GSN;grasping for stacking network;industrial environments;GNet;optimal location;long-range planning","","16","","42","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"A Multi-Channel Reinforcement Learning Framework for Robotic Mirror Therapy","J. Xu; L. Xu; Y. Li; G. Cheng; J. Shi; J. Liu; S. Chen","Department of Precision Machinery and Precision Instrumentation, University of Science and Technology of China, Hefei, China; Hefei Institutes and Physical Science, Chinese Academy of Sciences, Changzhou, China; Department of Mechanical Engineering, City University of Hong Kong, Hong Kong, China; Hefei Institutes and Physical Science, Chinese Academy of Sciences, Changzhou, China; Department of Precision Machinery and Precision Instrumentation, University of Science and Technology of China, Hefei, China; Hefei Institutes and Physical Science, Chinese Academy of Sciences, Changzhou, China; Department of Precision Machinery and Precision Instrumentation, University of Science and Technology of China, Hefei, China","IEEE Robotics and Automation Letters","17 Jul 2020","2020","5","4","5385","5392","In the letter, a robotic framework is proposed for hemiparesis rehabilitation. Mirror therapy is applied to transfer therapeutic training from the patient's function limb (FL) to the impaired limb (IL). The IL mimics the action prescribed by the FL with the assistance of the wearable robot, stimulating and strengthening the injured muscles through repetitive exercise. A master-slave robotic system is presented to implement the mirror therapy. Especially, the reinforcement learning is involved in the human-robot interaction control to enhance the rehabilitation efficacy and guarantee safety. Multi-channel sensed information, including the motion trajectory, muscle activation and the user's emotion, are incorporated in the learning algorithm. The muscle activation is expressed via the skin surface electromyography (EMG) signals, and the emotion is shown as the facial expression. The reinforcement learning approach is realized by the normalized advantage functions (NAF) algorithm. Then, a lower extremity rehabilitation robot with magnetorheological (MR) actuators is specially developed. The clinical experiments are carried out using the robot to verify the performance of the framework.","2377-3766","","10.1109/LRA.2020.3007408","National Key Research and Development Plan(grant numbers:2017YFB1303200); Anhui Science and Technology Major Project(grant numbers:17030901034); Key Research and Development Plan of Jiangsu Province(grant numbers:BE2017007-1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9134883","Rehabilitation robotics;physical human-robot interaction;reinforcement learning","Reinforcement learning;Robot kinematics;Medical treatment;Impedance;Torque;Mirrors","biomechanics;electromyography;human-robot interaction;learning (artificial intelligence);magnetorheology;medical robotics;muscle;patient rehabilitation;patient treatment","multichannel sensed information;muscle activation;learning algorithm;skin surface electromyography signals;reinforcement learning approach;normalized advantage functions algorithm;lower extremity rehabilitation robot;multichannel reinforcement learning framework;robotic mirror;robotic framework;hemiparesis rehabilitation;mirror therapy;therapeutic training;impaired limb;IL mimics;wearable robot;injured muscles;repetitive exercise;master-slave robotic system;human-robot interaction control","","15","","18","IEEE","7 Jul 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Visual Navigation With Information-Theoretic Regularization","Q. Wu; K. Xu; J. Wang; M. Xu; X. Gong; D. Manocha","College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Computer Science, National University of Defense Technology, Changsha, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Zhengzhou University, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Computer Science, the University of Maryland, College Park, MD, USA","IEEE Robotics and Automation Letters","25 Jan 2021","2021","6","2","731","738","To enhance the cross-target and cross-scene generalization of target-driven visual navigation based on deep reinforcement learning (RL), we introduce an information-theoretic regularization term into the RL objective. The regularization maximizes the mutual information between navigation actions and visual observation transforms of an agent, thus promoting more informed navigation decisions. This way, the agent models the action-observation dynamics by learning a variational generative model. Based on the model, the agent generates (imagines) the next observation from its current observation and navigation target. This way, the agent learns to understand the causality between navigation actions and the changes in its observations, which allows the agent to predict the next action for navigation by comparing the current and the imagined next observations. Cross-target and cross-scene evaluations on the AI2-THOR framework show that our method attains at least 10% improvement of average success rate over some state-of-the-art models. We further evaluate our model in two real-world settings: navigation in unseen indoor scenes from a discrete Active Vision Dataset (AVD) and continuous real-world environments with a TurtleBot. We demonstrate that our navigation model is able to successfully achieve navigation tasks in these scenarios.11[Online]. Available: https://github.com/wqynew/RL-based-navigation.","2377-3766","","10.1109/LRA.2020.3048668","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2020YFB2010702,2019YFB1707504,2018AAA0102200); National Natural Science Foundation of China(grant numbers:61902419,61772267,61622212,61532003); Aeronautical Science Foundation of China(grant numbers:2019ZE052008); National Science Foundation of Jiangsu Province(grant numbers:BK20190016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9312496","Model learning for robot control;reinforcement learning;visual-based navigation","Navigation;Visualization;Task analysis;Training;Robots;Robot sensing systems;Robot kinematics","learning (artificial intelligence);mobile robots;navigation;path planning;robot vision","causality;AI2-THOR framework;AVD;TurtleBot;active vision dataset;indoor scenes;navigation target;variational generative model;action-observation dynamics;agent models;visual observation transforms;RL objective;information-theoretic regularization term;deep reinforcement learning;target-driven visual navigation;cross-scene generalization;reinforcement learning-based visual navigation;navigation actions","","14","","34","IEEE","1 Jan 2021","","","IEEE","IEEE Journals"
"Robust Model-free Reinforcement Learning with Multi-objective Bayesian Optimization","M. Turchetta; A. Krause; S. Trimpe","Learning and Adaptive Systems Group, ETH Zurich, Zürich, Switzerland; Learning and Adaptive Systems Group, ETH Zurich, Zürich, Switzerland; Intelligent Control Systems Group, Max Planck Institute for Intelligent Systems, Stuttgart, Germany","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","10702","10708","In reinforcement learning (RL), an autonomous agent learns to perform complex tasks by maximizing an exogenous reward signal while interacting with its environment. In real world applications, test conditions may differ substantially from the training scenario and, therefore, focusing on pure reward maximization during training may lead to poor results at test time. In these cases, it is important to trade-off between performance and robustness while learning a policy. While several results exist for robust, model-based RL, the model-free case has not been widely investigated. In this paper, we cast the robust, model-free RL problem as a multi-objective optimization problem. To quantify the robustness of a policy, we use delay margin and gain margin, two robustness indicators that are common in control theory. We show how these metrics can be estimated from data in the model-free setting. We use multi-objective Bayesian optimization (MOBO) to solve efficiently this expensive-to-evaluate, multi-objective optimization problem. We show the benefits of our robust formulation both in sim-to-real and pure hardware experiments to balance a Furuta pendulum.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197000","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197000","","Robustness;Optimization;Training;Control theory;Computational modeling;Learning (artificial intelligence);Bayes methods","Bayes methods;learning (artificial intelligence);neural nets;optimisation;pendulums","robust model-free reinforcement learning;multiobjective Bayesian optimization;autonomous agent;exogenous reward signal;test conditions;pure reward maximization;model-free case;robust model-free RL problem;multiobjective optimization problem;robustness indicators;robust formulation","","14","","36","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data","A. Mandlekar; F. Ramos; B. Boots; S. Savarese; L. Fei-Fei; A. Garg; D. Fox",Stanford Vision & Learning Lab; NVIDIA; NVIDIA; Stanford Vision & Learning Lab; Stanford Vision & Learning Lab; NVIDIA; NVIDIA,"2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","4414","4420","Learning from offline task demonstrations is a problem of great interest in robotics. For simple short-horizon manipulation tasks with modest variation in task instances, offline learning from a small set of demonstrations can produce controllers that successfully solve the task. However, leveraging a fixed batch of data can be problematic for larger datasets and longer-horizon tasks with greater variations. The data can exhibit substantial diversity and consist of suboptimal solution approaches. In this paper, we propose Implicit Reinforcement without Interaction at Scale (IRIS), a novel framework for learning from large-scale demonstration datasets. IRIS factorizes the control problem into a goal-conditioned low-level controller that imitates short demonstration sequences and a high-level goal selection mechanism that sets goals for the low-level and selectively combines parts of suboptimal solutions leading to more successful task completions. We evaluate IRIS across three datasets, including the RoboTurk Cans dataset collected by humans via crowdsourcing, and show that performant policies can be learned from purely offline learning. Additional results at https://sites.google.com/stanford.edu/iris/.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196935","","Task analysis;Robots;Trajectory;Iris;Learning (artificial intelligence);Iris recognition;Grasping","human-robot interaction;learning (artificial intelligence);manipulators","RoboTurk Cans dataset;offline learning;IRIS;offline robot manipulation data;offline task demonstrations;robotics;goal-conditioned low-level controller;high-level goal selection mechanism;learning control;learning from large-scale demonstration datasets;Implicit Reinforcement without Interaction at Scale;crowdsourcing","","13","","38","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Towards Closing the Sim-to-Real Gap in Collaborative Multi-Robot Deep Reinforcement Learning","W. Zhao; J. P. Queralta; L. Qingqing; T. Westerlund","Turku Intelligent Embedded and Robotic Systems Lab, University of Turku, Finland; Turku Intelligent Embedded and Robotic Systems Lab, University of Turku, Finland; Turku Intelligent Embedded and Robotic Systems Lab, University of Turku, Finland; Turku Intelligent Embedded and Robotic Systems Lab, University of Turku, Finland","2020 5th International Conference on Robotics and Automation Engineering (ICRAE)","6 Jan 2021","2020","","","7","12","Current research directions in deep reinforcement learning include bridging the simulation-reality gap, improving sample efficiency of experiences in distributed multi-agent reinforcement learning, together with the development of robust methods against adversarial agents in distributed learning, among many others. In this work, we are particularly interested in analyzing how multi-agent reinforcement learning can bridge the gap to reality in distributed multi-robot systems where the operation of the different robots is not necessarily homogeneous. These variations can happen due to sensing mismatches, inherent errors in terms of calibration of the mechanical joints, or simple differences in accuracy. While our results are simulation-based, we introduce the effect of sensing, calibration, and accuracy mismatches in distributed reinforcement learning with proximal policy optimization (PPO). We discuss on how both the different types of perturbances and how the number of agents experiencing those perturbances affect the collaborative learning effort. The simulations are carried out using a Kuka arm model in the Bullet physics engine. This is, to the best of our knowledge, the first work exploring the limitations of PPO in multi-robot systems when considering that different robots might be exposed to different environments where their sensors or actuators have induced errors. With the conclusions of this work, we set the initial point for future work on designing and developing methods to achieve robust reinforcement learning on the presence of real-world perturbances that might differ within a multi-robot system.","","978-1-7281-8981-9","10.1109/ICRAE50850.2020.9310796","Academy of Finland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9310796","Reinforcement Learning;Multi-Robot Systems;Collaborative Learning;Deep RL;Adversarial RL;Sim-to-Real","Robots;Robot sensing systems;Reinforcement learning;Task analysis;Perturbation methods;Collaborative work;Collaboration","control engineering computing;learning (artificial intelligence);multi-agent systems;multi-robot systems;robots","adversarial agents;distributed learning;distributed multirobot systems;sensing mismatches;simple differences;accuracy mismatches;distributed reinforcement;collaborative learning effort;multirobot system;designing developing methods;robust reinforcement learning;collaborative multirobot deep reinforcement learning;current research directions;simulation-reality gap;distributed multiagent reinforcement learning","","12","","22","IEEE","6 Jan 2021","","","IEEE","IEEE Conferences"
"Incorporating Multi-Context Into the Traversability Map for Urban Autonomous Driving Using Deep Inverse Reinforcement Learning","C. Jung; D. H. Shim","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Robotics and Automation Letters","8 Mar 2021","2021","6","2","1662","1669","Autonomous driving in an urban environment with surrounding agents remains challenging. One of the key challenges is to accurately predict the traversability map that probabilistically represents future trajectories considering multiple contexts: inertial, environmental, and social. To address this, various approaches have been proposed; however, they mainly focus on considering the individual context. In addition, most studies utilize expensive prior information (such as HD maps) of the driving environment, which is not a scalable approach. In this study, we extend a deep inverse reinforcement learning-based approach that can predict the traversability map while incorporating multiple contexts for autonomous driving in a dynamic environment. Instead of using expensive prior information of the driving scene, we propose a novel deep neural network to extract contextual cues from sensing data and effectively incorporate them in the output, i.e., the reward map. Based on the reward map, our method predicts the ego-centric traversability map that represents the probability distribution of the plausible and socially acceptable future trajectories. The proposed method is qualitatively and quantitatively evaluated in real-world traffic scenarios with various baselines. The experimental results show that our method improves the prediction accuracy compared to other baseline methods and can predict future trajectories similar to those followed by a human driver.","2377-3766","","10.1109/LRA.2021.3059628","SK hynix Inc.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354889","AI-based methods;autonomous vehicle navigation;intelligent transportation systems;learning from demonstration","Trajectory;Autonomous vehicles;Predictive models;Context modeling;Reinforcement learning;Vehicle dynamics;Probabilistic logic","cartography;deep learning (artificial intelligence);driver information systems;multi-agent systems;probability","agents;egocentric traversability map;multicontext incorporation;reward map;deep neural network;driving scene;dynamic environment;HD maps;deep inverse reinforcement learning;urban autonomous driving","","12","","30","IEEE","16 Feb 2021","","","IEEE","IEEE Journals"
"Total Singulation With Modular Reinforcement Learning","I. Sarantopoulos; M. Kiatos; Z. Doulgeri; S. Malassiotis","Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Information Technologies Institute (ITI) Center of Research and Technology Hellas (CERTH), Thessaloniki, Greece","IEEE Robotics and Automation Letters","6 Apr 2021","2021","6","2","4117","4124","Prehensile robotic grasping of a target object in clutter is challenging because, in such conditions, the target touches other objects, resulting to the lack of collision free grasp affordances. To address this problem, we propose a modular reinforcement learning method which uses continuous actions to totally singulate the target object from its surrounding clutter. A high level policy selects between pushing primitives, which are learned separately. Prior knowledge is effectively incorporated into learning, through action primitives and feature selection, increasing sample efficiency. Experiments demonstrate that the proposed method considerably outperforms the state-of-the-art methods in the singulation task. Furthermore, although training is performed in simulation the learned policy is robustly transferred to a real environment without a significant drop in success rate. Finally, singulation tasks in different environments are addressed by easily adding a new primitive and by retraining only the high level policy.","2377-3766","","10.1109/LRA.2021.3062295","European Community's Framework Programme Horizon 2020(grant numbers:871704); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363569","Deep learning in grasping and manipulation;perception for grasping and manipulation","Grasping;Clutter;Robots;Reinforcement learning;Visualization;Task analysis;Training","control engineering computing;grippers;learning (artificial intelligence)","collision free grasp;singulation task;feature selection;action primitives;pushing primitives;modular reinforcement learning;target object;prehensile robotic grasping","","11","","27","IEEE","25 Feb 2021","","","IEEE","IEEE Journals"
"Towards Multi-Modal Perception-Based Navigation: A Deep Reinforcement Learning Method","X. Huang; H. Deng; W. Zhang; R. Song; Y. Li","School of Control Science and Engineering, Shandong University, Jinan, Shandong, China; School of Control Science and Engineering, Shandong University, Jinan, Shandong, China; Institute of Brain and Brain-Inspired Science, Shandong University, Jinan, Shandong, China; Institute of Brain and Brain-Inspired Science, Shandong University, Jinan, Shandong, China; School of Control Science and Engineering, Shandong University, Jinan, Shandong, China","IEEE Robotics and Automation Letters","21 Apr 2021","2021","6","3","4986","4993","In this letter, we present a novel navigation system of unmanned ground vehicle (UGV) for local path planning based on deep reinforcement learning. The navigation system decouples perception from control and takes advantage of multi-modal perception for a reliable online interaction with the surrounding environment of the UGV, which enables a direct policy learning for generating flexible actions to avoid collisions with obstacles in the navigation. By replacing the raw RGB images with their semantic segmentation maps as the input and applying a multi-modal fusion scheme, our system trained only in simulation can handle real-world scenes containing dynamic obstacles such as vehicles and pedestrians. We also introduce a modal separation learning to accelerate the training and further boost the performance. Extensive experiments demonstrate that our method closes the gap between simulated and real environments, exhibiting the superiority over state-of-the-art approaches. Please refer to https://vsislab.github.io/mmpbnv1/ for the supplementary video demonstration of UGV navigation in both simulated and real-world environments.","2377-3766","","10.1109/LRA.2021.3064461","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018AAA0102504); National Natural Science Foundation of China(grant numbers:U1913204,61991411); Natural Science Foundation of Shandong Province for Distinguished Young Scholars(grant numbers:ZR2020JQ29); Young Taishan Scholars Program of Shandong Province(grant numbers:tsqn201909029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9372890","Motion and path planning;reinforcement learning;sensor fusion","Navigation;Laser radar;Reinforcement learning;Vehicle dynamics;Annotations;Angular velocity;Training","collision avoidance;image colour analysis;image representation;image segmentation;learning (artificial intelligence);mobile robots;navigation","UGV navigation;simulated environments;pedestrians;vehicles;dynamic obstacles;multimodal fusion scheme;semantic segmentation maps;RGB images;direct policy learning;online interaction;navigation system decouples perception;deep reinforcement learning;path planning;unmanned ground vehicle;multimodal perception-based navigation","","11","","38","IEEE","8 Mar 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Selective Disassembly Sequence Planning for the End-of-Life Products With Structure Uncertainty","X. Zhao; C. Li; Y. Tang; J. Cui","State Key Laboratory of Mechanical Transmissions, Chongqing University, Chongqing, China; State Key Laboratory of Mechanical Transmissions, Chongqing University, Chongqing, China; Department of Electrical and Computer Engineering, Rowan University, Glassboro, NJ, USA,; State Key Laboratory of Mechanical Transmissions, Chongqing University, Chongqing, China","IEEE Robotics and Automation Letters","20 Aug 2021","2021","6","4","7807","7814","Selective disassembly sequence planning (SDSP) is regarded as an efficient strategy to determine optimal disassembly sequences for extracting target parts (TP) from complex end-of-life (EOL) products. Previous research assumes that all EOL products have the same structure and the optimal selective disassembly sequences are given before the EOL products are removed. However, the products have different operation states during their use stage, which results in high structure uncertainty of EOL products. The structure uncertainty of EOL products often makes the predetermined selective disassembly sequences impractical for minimizing disassembly time and maximizing disassembly profit. This letter undertakes this challenge by integrated reinforcement learning (RL) to determine the optimal disassembly sequences adaptive to the structure uncertainty of the EOL products. Firstly, a multi-level selective disassembly hybrid graph model (MSDHGM) is developed to illustrate the contact, precedence, and level relationships among parts. Then, the SDSP is formulated as a finite Markov Decision Process and a deep Q-network based selective disassembly sequence planning (DQN-SDSP) is proposed. Finally, extensive comparative experiments are conducted to verify the proposed method compared with NSGA-II and ABC algorithms.","2377-3766","","10.1109/LRA.2021.3098248","National Natural Science Foundation of China(grant numbers:51975075); National Key R&D Program of China(grant numbers:2019YFB1706103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9492060","Selective disassembly sequence planning;structure uncertainty;reinforcement learning;hybrid graph model","Uncertainty;Tools;Planning;Recycling;Heuristic algorithms;Optimization;Decision making","assembling;design for disassembly;graph theory;learning (artificial intelligence);Markov processes;optimisation;profitability;recycling","reinforcement learning-based selective disassembly sequence planning;end-of-life products;optimal disassembly sequences;EOL products;optimal selective disassembly sequences;high structure uncertainty;predetermined selective disassembly sequences;disassembly time;maximizing disassembly profit;multilevel selective disassembly hybrid graph model;deep Q-network based selective disassembly sequence","","11","","21","IEEE","20 Jul 2021","","","IEEE","IEEE Journals"
"Dynamic Interaction-Aware Scene Understanding for Reinforcement Learning in Autonomous Driving","M. Huegle; G. Kalweit; M. Werling; J. Boedecker","Dept. of Computer Science, University of Freiburg, Germany; Dept. of Computer Science, University of Freiburg, Germany; BMWGroup, Unterschleissheim, Germany; Dept. of Computer Science, University of Freiburg, Germany","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","4329","4335","The common pipeline in autonomous driving systems is highly modular and includes a perception component which extracts lists of surrounding objects and passes these lists to a high-level decision component. In this case, leveraging the benefits of deep reinforcement learning for high-level decision making requires special architectures to deal with multiple variable-length sequences of different object types, such as vehicles, lanes or traffic signs. At the same time, the architecture has to be able to cover interactions between traffic participants in order to find the optimal action to be taken. In this work, we propose the novel Deep Scenes architecture, that can learn complex interaction-aware scene representations based on extensions of either 1) Deep Sets or 2) Graph Convolutional Networks. We present the Graph-Q and DeepScene-Q off-policy reinforcement learning algorithms, both outperforming state-ofthe-art methods in evaluations with the publicly available traffic simulator SUMO.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197086","","Computer architecture;Autonomous vehicles;Lenses;Learning (artificial intelligence);Decision making;Predictive models;Neural networks","convolutional neural nets;decision making;graph theory;image representation;image sequences;learning (artificial intelligence);neural net architecture;traffic engineering computing","DeepScene-Q off-policy reinforcement learning algorithms;graph-Q;graph convolutional networks;multiple variable-length sequences;novel deep scene architecture;complex interaction-aware scene representations;traffic participants;traffic signs;object types;high-level decision making;deep reinforcement learning;high-level decision component;perception component;autonomous driving systems;dynamic interaction-aware scene understanding;traffic simulator SUMO","","11","","34","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Uncertainty-Aware Contact-Safe Model-Based Reinforcement Learning","C. -Y. Kuo; A. Schaarschmidt; Y. Cui; T. Asfour; T. Matsubara","Graduate School of Science and Technology, Nara Institute of Science and Technology, Nara, Japan; Institute of Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Center for Automotive Electronics, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Institute of Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Graduate School of Science and Technology, Nara Institute of Science and Technology, Nara, Japan","IEEE Robotics and Automation Letters","5 Apr 2021","2021","6","2","3918","3925","This letter presents contact-safe Model-based Reinforcement Learning (MBRL) for robot applications that achieves contact-safe behaviors in the learning process. In typical MBRL, we cannot expect the data-driven model to generate accurate and reliable policies to the intended robotic tasks during the learning process due to sample scarcity. Operating these unreliable policies in a contact-rich environment could cause damage to the robot and its surroundings. To alleviate the risk of causing damage through unexpected intensive physical contacts, we present the contact-safe MBRL that associates the probabilistic Model Predictive Control's (pMPC) control limits with the model uncertainty so that the allowed acceleration of controlled behavior is adjusted according to learning progress. Control planning with such uncertainty-aware control limits is formulated as a deterministic MPC problem using a computation-efficient approximated GP dynamics and an approximated inference technique. Our approach's effectiveness is evaluated through bowl mixing tasks with simulated and real robots, scooping tasks with a real robot as examples of contact-rich manipulation skills.","2377-3766","","10.1109/LRA.2021.3065271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376242","Machine learning for robot control;probabilistic inference;reinforcement learning","Task analysis;Robots;Uncertainty;Predictive models;Safety;Computational modeling;Probabilistic logic","inference mechanisms;learning (artificial intelligence);predictive control;probability;robot programming","computation efficient approximated GP dynamics;contact safe MBRL;contact safe model based reinforcement learning;uncertainty aware control limits;intended robotic tasks;contact-safe behaviors;robot applications;contact rich manipulation;probabilistic model predictive control","","10","","32","IEEE","11 Mar 2021","","","IEEE","IEEE Journals"
"A Coach-Based Bayesian Reinforcement Learning Method for Snake Robot Control","Y. Jia; S. Ma","Department of Robotics, Ritsumeikan University, Shiga, Japan; Department of Robotics, Ritsumeikan University, Shiga, Japan","IEEE Robotics and Automation Letters","12 Mar 2021","2021","6","2","2319","2326","Reinforcement Learning (RL) usually needs thousands of episodes, leading its applications on physical robots expensive and challenging. Little research has been reported about snake robot control using RL due to additional difficulty of high redundancy of freedom. We propose a coach-based deep learning method for snake robot control, which can effectively save convergence time with much less episodes. The main contributions include: 1) a unified graph-based Bayesian framework integrating a coach module to guide the RL agent; 2) an explicit stochastic formulation of robot-environment interaction with uncertainty; 3) an efficient and robust training process for snake robot control to achieve both path planning and obstacle avoidance simultaneously. The performance has been demonstrated on both simulation and real-world data in comparison with state-of-the-art, showing promising results.","2377-3766","","10.1109/LRA.2021.3061372","Robotics Innovation Based on Advanced Materials; Ritsumeikan Global Innovation Research Organization, Ritsumeikan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361111","Bayesian method;motion control;reinforcement learning;snake robot","Robots;Snake robots;Uncertainty;Computational modeling;Convergence;Bayes methods;Reinforcement learning","Bayes methods;belief networks;collision avoidance;convergence;deep learning (artificial intelligence);mobile robots;motion control;stochastic processes","snake robot control;coach-based deep learning method;unified graph-based Bayesian framework;robot-environment interaction;coach-based Bayesian reinforcement Learning method;RL agent;convergence time;explicit stochastic formulation;uncertainty;path plannning;obstacle avoidance","","10","","29","IEEE","23 Feb 2021","","","IEEE","IEEE Journals"
"Integrated moment-based LGMD and deep reinforcement learning for UAV obstacle avoidance","L. He; N. Aouf; J. F. Whidborne; B. Song","School of Aeronautics, Northwestern Polytechnical University, Xi’an, China; Dept of Electrical and Electronic Engineering, City, University of London, London, UK; Centre for Aeronautics, Cranfield University, Bedfordshire, UK; School of Aeronautics, Northwestern Polytechnical University, Xi’an, China","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","7491","7497","In this paper, a bio-inspired monocular vision perception method combined with a learning-based reaction local planner for obstacle avoidance of micro UAVs is presented. The system is more computationally efficient than other vision-based perception and navigation methods such as SLAM and optical flow because it does not need to calculate accurate distances. To improve the robustness of perception against illuminance change, the input image is remapped using image moment which is independent of illuminance variation. After perception, a local planner is trained using deep reinforcement learning for mapless navigation. The proposed perception and navigation methods are evaluated in some realistic simulation environments. The result shows that this light-weight monocular perception and navigation system works well in different complex environments without accurate depth information.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197152","","Navigation;Collision avoidance;Robustness;Lighting;Robots;Optical imaging;Machine learning","autonomous aerial vehicles;collision avoidance;control engineering computing;image motion analysis;image sequences;learning (artificial intelligence);mobile robots;neural nets;object detection;robot vision;SLAM (robots);visual perception","deep reinforcement learning;UAV obstacle avoidance;learning-based reaction local planner;microUAVs;image moment;illuminance variation;mapless navigation;moment-based LGMD;bioinspired monocular vision perception method","","10","","35","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Harmonic-Based Optimal Motion Planning in Constrained Workspaces Using Reinforcement Learning","P. Rousseas; C. Bechlioulis; K. J. Kyriakopoulos","School of Mechanical Engineering, Control Systems Laboratoty, National Technical University of Athens, Athens, Greece; School of Mechanical Engineering, Control Systems Laboratoty, National Technical University of Athens, Athens, Greece; School of Mechanical Engineering, Control Systems Laboratoty, National Technical University of Athens, Athens, Greece","IEEE Robotics and Automation Letters","9 Mar 2021","2021","6","2","2005","2011","In this work, we propose a novel reinforcement learning algorithm to solve the optimal motion planning problem. Particular emphasis is given on the rigorous mathematical proof of safety, convergence as well as optimality w.r.t. to an integral quadratic cost function, while reinforcement learning is adopted to enable the cost function's approximation. Both offline and online solutions are proposed, and an implementation of the offline method is compared to a state-of-the-art RRT* approach. This novel approach inherits the strong traits from both artificial potential fields, i.e., reactivity, as well as sampling-based methods, i.e., optimality, and opens up new paths to the age-old problem of motion planning, by merging modern tools and philosophies from various corners of the field.","2377-3766","","10.1109/LRA.2021.3060711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359451","Motion and path planning;optimization and optimal control;reinforcement learning","Harmonic analysis;Safety;Planning;Cost function;Robots;Navigation;Reinforcement learning","function approximation;harmonic analysis;learning (artificial intelligence);mobile robots;optimisation;path planning;sampling methods","harmonic-based optimal motion planning;constrained workspaces;reinforcement learning algorithm;optimal motion planning problem;integral quadratic cost function;online solutions;offline method;state-of-the-art RRT* approach;sampling-based methods;cost function approximation;artificial potential fields","","10","","26","IEEE","19 Feb 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning With Evolutionary Trajectory Generator: A General Approach for Quadrupedal Locomotion","H. Shi; B. Zhou; H. Zeng; F. Wang; Y. Dong; J. Li; K. Wang; H. Tian; M. Q. . -H. Meng","Chinese University of Hong Kong, Shenzhen, China; Baidu, Beijing, China; Baidu, Beijing, China; Baidu, Beijing, China; Baidu, Beijing, China; Baidu, Beijing, China; Baidu, Beijing, China; Baidu, Beijing, China; Shenzhen Key Laboratory of Robotics Perception and Intelligence, Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China","IEEE Robotics and Automation Letters","4 Feb 2022","2022","7","2","3085","3092","Recently reinforcement learning (RL) has emerged as a promising approach for quadrupedal locomotion, which can save the manual effort in conventional approaches such as designing skill-specific controllers. However, due to the complex nonlinear dynamics in quadrupedal robots and reward sparsity, it is still difficult for RL to learn effective gaits from scratch, especially in challenging tasks such as walking over the balance beam. To alleviate such difficulty, we propose a novel RL-based approach that contains an evolutionary foot trajectory generator. Unlike prior methods that use a fixed trajectory generator, the generator continually optimizes the shape of the output trajectory for the given task, providing diversified motion priors to guide the policy learning. The policy is trained with reinforcement learning to output residual control signals that fit different gaits. We then optimize the trajectory generator and policy network alternatively to stabilize the training and share the exploratory data to improve sample efficiency. As a result, our approach can solve a range of challenging tasks in simulation by learning from scratch, including walking on a balance beam and crawling through the cave. To further verify the effectiveness of our approach, we deploy the controller learned in the simulation on a 12-DoF quadrupedal robot, and it can successfully traverse challenging scenarios with efficient gaits. We provide a video to show the learned gaits in different tasks in YouTube.11[Online]. Available: youtube.com/watch?v=hgBLR09MEOw, and code is available in Github: github.com/PaddlePaddle/PaddleRobotics","2377-3766","","10.1109/LRA.2022.3145495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693519","Legged robots;machine learning for robot control;reinforcement learning","Trajectory;Quadrupedal robots;Task analysis;Generators;Robots;Legged locomotion;Foot","control engineering computing;gait analysis;learning (artificial intelligence);legged locomotion;optimisation;stability;trajectory control","reinforcement learning;evolutionary trajectory generator;quadrupedal locomotion;manual effort;skill-specific controllers;complex nonlinear dynamics;quadrupedal robots;reward sparsity;effective gaits;balance beam;RL-based approach;evolutionary foot trajectory generator;fixed trajectory generator;output trajectory;diversified motion priors;policy learning;output residual control signals;policy network;12-DoF quadrupedal robot;efficient gaits;learned gaits","","10","","30","IEEE","26 Jan 2022","","","IEEE","IEEE Journals"
"Defensive Escort Teams for Navigation in Crowds via Multi-Agent Deep Reinforcement Learning","Y. A. Hasan; A. Garg; S. Sugaya; L. Tapia","Department of Computer Science, MSC01 11301, University of New Mexico, Albuquerque, USA; Department of Computer Science, MSC01 11301, University of New Mexico, Albuquerque, USA; Department of Computer Science, MSC01 11301, University of New Mexico, Albuquerque, USA; Department of Computer Science, MSC01 11301, University of New Mexico, Albuquerque, USA","IEEE Robotics and Automation Letters","27 Jul 2020","2020","5","4","5645","5652","Coordinated defensive escorts can aid a navigating payload by positioning themselves strategically in order to maintain the safety of the payload from obstacles. In this letter, we present a novel, end-to-end solution for coordinating an escort team for protecting high-value payloads in a space crowded with interacting obstacles. Our solution employs deep reinforcement learning in order to train a team of escorts to maintain payload safety while navigating alongside the payload. The escorts utilize a trained centralized policy in a distributed fashion (i.e., no explicit communication between the escorts), relying only on range-limited positional information of the environment. Given this observation, escorts automatically prioritize obstacles to intercept and determine where to intercept them, using their repulsive interaction force to actively manipulate the environment. When compared to a payload navigating with a state-of-art algorithm for obstacle avoidance our defensive escort team increased navigation success up to 83% over escorts in static formation, up to 69% over orbiting escorts, and up to 66% compared to an analytic method providing guarantees in crowded environments. We also show that our learned solution is robust to several adaptations in the scenario including: a changing number of escorts in the team, changing obstacle density, unexpected obstacle behavior, changes in payload conformation, and added sensor noise.","2377-3766","","10.1109/LRA.2020.3010203","National Science Foundation(grant numbers:IIS-1528047,IIS-1553266); Air Force Research Laboratory(grant numbers:FA9453-18-2-0022); Army Research Laboratory(grant numbers:W911NF-19-2-0215); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9143392","Intelligent systems;machine learning;motion planning;multi-robot systems","Payloads;Navigation;Safety;Force;Robot sensing systems;Surveillance;Games","collision avoidance;learning (artificial intelligence);mobile robots;multi-agent systems","navigation success;learned solution;payload conformation;defensive escort teams;multiagent deep reinforcement learning;coordinated defensive escorts;high-value payloads;interacting obstacles;payload safety;obstacle avoidance;payload navigation;repulsive interaction force;defensive escort team;obstacle density;unexpected obstacle behavior;sensor noise","","9","","39","IEEE","17 Jul 2020","","","IEEE","IEEE Journals"
"AirCapRL: Autonomous Aerial Human Motion Capture Using Deep Reinforcement Learning","R. Tallamraju; N. Saini; E. Bonetto; M. Pabst; Y. T. Liu; M. J. Black; A. Ahmad","Max Planck Institute for Intelligent Systems, Tübingen, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany","IEEE Robotics and Automation Letters","26 Aug 2020","2020","5","4","6678","6685","In this letter, we introduce a deep reinforcement learning (DRL) based multi-robot formation controller for the task of autonomous aerial human motion capture (MoCap). We focus on vision-based MoCap, where the objective is to estimate the trajectory of body pose, and shape of a single moving person using multiple micro aerial vehicles. State-of-the-art solutions to this problem are based on classical control methods, which depend on hand-crafted system, and observation models. Such models are difficult to derive, and generalize across different systems. Moreover, the non-linearities, and non-convexities of these models lead to sub-optimal controls. In our work, we formulate this problem as a sequential decision making task to achieve the vision-based motion capture objectives, and solve it using a deep neural network-based RL method. We leverage proximal policy optimization (PPO) to train a stochastic decentralized control policy for formation control. The neural network is trained in a parallelized setup in synthetic environments. We performed extensive simulation experiments to validate our approach. Finally, real-robot experiments demonstrate that our policies generalize to real world conditions.","2377-3766","","10.1109/LRA.2020.3013906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9158379","Reinforecment learning;aerial systems: perception and autonomy;multi-robot systems;visual tracking","Cameras;Three-dimensional displays;Shape;Trajectory;Unmanned aerial vehicles;Robot vision systems;Computational modeling","autonomous aerial vehicles;decentralised control;decision making;image motion analysis;learning (artificial intelligence);mobile robots;multi-robot systems;neurocontrollers;optimisation;pose estimation;robot vision;stochastic systems;trajectory control","proximal policy optimization;RL method;body pose;trajectory estimation;AirCapRL;multirobot formation controller;deep reinforcement learning;vision-based motion capture;stochastic decentralized control policy;deep neural network;sequential decision making;microaerial vehicles;single moving person;vision-based MoCap;autonomous aerial human motion capture","","9","","19","CCBY","4 Aug 2020","","","IEEE","IEEE Journals"
"Spatiotemporal Costmap Inference for MPC Via Deep Inverse Reinforcement Learning","K. Lee; D. Isele; E. A. Theodorou; S. Bae","Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Research Division, Honda Research Institute USA, Inc., San Jose, CA, USA; School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Research Division, Honda Research Institute USA, Inc., San Jose, CA, USA","IEEE Robotics and Automation Letters","7 Feb 2022","2022","7","2","3194","3201","It can be difficult to autonomously produce driver behavior so that it appears natural to other traffic participants. Through Inverse Reinforcement Learning (IRL), we can automate this process by learning the underlying reward function from human demonstrations. We propose a new IRL algorithm that learns a goal-conditioned spatio-temporal reward function. The resulting costmap is used by Model Predictive Controllers (MPCs) to perform a task without any hand-designing or hand-tuning of the cost function. We evaluate our proposed Goal-conditioned SpatioTemporal Zeroing Maximum Entropy Deep IRL (GSTZ)-MEDIRL framework together with MPC in the CARLA simulator for autonomous driving, lane keeping, and lane changing tasks in a challenging dense traffic highway scenario. Our proposed methods show higher success rates compared to other baseline methods including behavior cloning, state-of-the-art RL policies, and MPC with a learning-based behavior prediction model.","2377-3766","","10.1109/LRA.2022.3146635","Honda Research Institute, USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695208","Learning from demonstration;reinforcement learning;optimization and optimal control;motion and path planning;autonomous vehicle navigation","Cost function;Entropy;Autonomous vehicles;Task analysis;Road transportation;Costs;Spatiotemporal phenomena","belief networks;learning (artificial intelligence);path planning;predictive control;spatiotemporal phenomena","spatiotemporal costmap inference;MPC;Deep Inverse Reinforcement Learning;driver behavior;traffic participants;underlying reward function;human demonstrations;IRL algorithm;goal-conditioned spatio-temporal reward function;resulting costmap;Model Predictive Controllers;hand-designing;hand-tuning;cost function;SpatioTemporal Zeroing Maximum Entropy Deep IRL-MEDIRL framework;autonomous driving;lane changing tasks;challenging dense traffic highway scenario;behavior cloning;learning-based behavior prediction model","","9","","37","IEEE","27 Jan 2022","","","IEEE","IEEE Journals"
"Fast Adaptation of Deep Reinforcement Learning-Based Navigation Skills to Human Preference","J. Choi; C. Dance; J. -e. Kim; K. -s. Park; J. Han; J. Seo; M. Kim","NAVER LABS, Gyeonggi-do, South Korea; NAVER LABS Europe, Meylan, France; NAVER LABS, Gyeonggi-do, South Korea; NAVER LABS, Gyeonggi-do, South Korea; NAVER LABS, Gyeonggi-do, South Korea; NAVER LABS, Gyeonggi-do, South Korea; NAVER LABS, Gyeonggi-do, South Korea","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","3363","3370","Deep reinforcement learning (RL) is being actively studied for robot navigation due to its promise of superior performance and robustness. However, most existing deep RL navigation agents are trained using fixed parameters, such as maximum velocities and weightings of reward components. Since the optimal choice of parameters depends on the use-case, it can be difficult to deploy such existing methods in a variety of real-world service scenarios. In this paper, we propose a novel deep RL navigation method that can adapt its policy to a wide range of parameters and reward functions without expensive retraining. Additionally, we explore a Bayesian deep learning method to optimize these parameters that requires only a small amount of preference data. We empirically show that our method can learn diverse navigation skills and quickly adapt its policy to a given performance metric or to human preference. We also demonstrate our method in real-world scenarios.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197159","","Navigation;Collision avoidance;Bayes methods;Training;Robot kinematics;Machine learning","learning (artificial intelligence);mobile robots;navigation;robust control","fast adaptation;deep reinforcement learning-based navigation skills;human preference;robot navigation;robustness;maximum velocities;reward components;optimal choice;real-world service scenarios;deep RL navigation method;reward functions;Bayesian deep learning method;preference data;diverse navigation skills;deep RL navigation agents","","8","","34","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Planning-Augmented Hierarchical Reinforcement Learning","R. Gieselmann; F. T. Pokorny","RPL, EECS, KTH Royal Institute of Technology, Stockholm, Sweden; RPL, EECS, KTH Royal Institute of Technology, Stockholm, Sweden","IEEE Robotics and Automation Letters","22 Apr 2021","2021","6","3","5097","5104","Planning algorithms are powerful at solving long-horizon decision-making problems but require that environment dynamics are known. Model-free reinforcement learning has recently been merged with graph-based planning to increase the robustness of trained policies in state-space navigation problems. Recent ideas suggest to use planning in order to provide intermediate waypoints guiding the policy in long-horizon tasks. Yet, it is not always practical to describe a problem in the setting of state-to-state navigation. Often, the goal is defined by one or multiple disjoint sets of valid states or implicitly using an abstract task description. Building upon previous efforts, we introduce a novel algorithm called Planning-Augmented Hierarchical Reinforcement Learning (PAHRL) which translates the concept of hybrid planning/RL to such problems with implicitly defined goal. Using a hierarchical framework, we divide the original task, formulated as a Markov Decision Process (MDP), into a hierarchy of shorter horizon MDPs. Actor-critic agents are trained in parallel for each level of the hierarchy. During testing, a planner then determines useful subgoals on a state graph constructed at the bottom level of the hierarchy. The effectiveness of our approach is demonstrated for a set of continuous control problems in simulation including robot arm reaching tasks and the manipulation of a deformable object.","2377-3766","","10.1109/LRA.2021.3071062","Wallenberg AI, Autonomous Systems, and Software Program; Alice Wallenberg Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9395248","Machine learning for robot control;Motion and path planning;Reinforcement learning","Task analysis;Planning;Navigation;Reinforcement learning;Robustness;Search problems;Heuristic algorithms","decision making;decision theory;graph theory;learning (artificial intelligence);Markov processes;mobile robots;path planning;planning (artificial intelligence)","trained policies;state-space navigation problems;long-horizon tasks;state-to-state navigation;multiple disjoint sets;valid states;abstract task description;Planning-Augmented Hierarchical Reinforcement Learning;implicitly defined goal;state graph;continuous control problems;Planning algorithms;long-horizon decision-making problems;model-free reinforcement learning;graph-based planning","","8","","35","IEEE","5 Apr 2021","","","IEEE","IEEE Journals"
"Deep Merging: Vehicle Merging Controller Based on Deep Reinforcement Learning with Embedding Network","I. Nishitani; H. Yang; R. Guo; S. Keshavamurthy; K. Oguchi","Toyota Motor Corporation, Japan; Toyota Motor North America, USA; Toyota Motor North America, USA; Toyota Motor North America, USA; Toyota Motor North America, USA","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","216","221","Vehicles at highway merging sections must make lane changes to join the highway. This lane change can generate congestion. To reduce congestion, vehicles should merge so as not to affect traffic flow as much as possible. In our study, we propose a vehicle controller called Deep Merging that uses deep reinforcement learning to improve the merging efficiency of vehicles while considering the impact on traffic flow. The system uses the images of a merging section as input to output the target vehicle speed. Moreover, an embedding network for estimating the controlled vehicle speed is introduced to the deep reinforcement learning network architecture to improve the learning efficiency. In order to show the effectiveness of the proposed method, the merging behavior and traffic conditions in several situations are verified by experiments using a traffic simulator. Through these experiments, it is confirmed that the proposed method enables controlled vehicles to effectively merge without adversely affecting to the traffic flow.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197559","","Merging;Machine learning;Feature extraction;Acceleration;Road transportation;Vehicle dynamics;Network architecture","control engineering computing;learning (artificial intelligence);road traffic control;road vehicles;traffic engineering computing","traffic conditions;traffic flow;Deep Merging;vehicle Merging controller;embedding network;highway merging sections;lane change;vehicle controller;merging efficiency;merging section;target vehicle speed;controlled vehicle speed;deep reinforcement learning network architecture;learning efficiency;merging behavior","","8","","18","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Model-Based Reinforcement Learning for Physical Systems Without Velocity and Acceleration Measurements","A. Dalla Libera; D. Romeres; D. K. Jha; B. Yerazunis; D. Nikovski","Department of Information Engineering, University of Padova, Padova, Italy; Mitsubishi Electric Research Laboratories, Cambridge, USA; Mitsubishi Electric Research Laboratories, Cambridge, USA; Mitsubishi Electric Research Laboratories, Cambridge, USA; Mitsubishi Electric Research Laboratories, Cambridge, USA","IEEE Robotics and Automation Letters","1 Apr 2020","2020","5","2","3548","3555","In this letter, we propose a derivative-free model learning framework for Reinforcement Learning (RL) algorithms based on Gaussian Process Regression (GPR). In many mechanical systems, only positions can be measured by the sensing instruments. Then, instead of representing the system state as suggested by the physics with a collection of positions, velocities, and accelerations, we define the state as the set of past position measurements. However, the equation of motions derived by physical first principles cannot be directly applied in this framework, being functions of velocities and accelerations. For this reason, we introduce a novel derivative-free physically-inspired kernel, which can be easily combined with nonparametric derivative-free Gaussian Process models. Tests performed on two real platforms show that the considered state definition combined with the proposed model improves estimation performance and data-efficiency w.r.t. traditional models based on GPR. Finally, we validate the proposed framework by solving two RL control problems for two real robotic systems.","2377-3766","","10.1109/LRA.2020.2977255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9017932","Model learning for control;dynamics;reinforcement learning (RL)","Reinforcement learning;Gaussian processes;Kernel","Gaussian processes;learning (artificial intelligence);position measurement;regression analysis","robotic systems;derivative-free Gaussian Process models;position measurements;sensing instruments;mechanical systems;GPR;Gaussian process regression;reinforcement learning algorithms;derivative-free model learning framework;model-based reinforcement learning","","8","","28","IEEE","28 Feb 2020","","","IEEE","IEEE Journals"
"Speed Tracking Control using Online Reinforcement Learning in a Real Car","L. Puccetti; F. Köpf; C. Rathgeber; S. Hohmann","Karlsruhe Institute of Technology (KIT), Institute of Control Systems, Karlsruhe, Germany; Karlsruhe Institute of Technology (KIT), Institute of Control Systems, Karlsruhe, Germany; BMW AG Munich, Germany; Karlsruhe Institute of Technology (KIT), Institute of Control Systems, Karlsruhe, Germany","2020 6th International Conference on Control, Automation and Robotics (ICCAR)","4 Jun 2020","2020","","","392","399","Reinforcement learning has the potential to improve classical control design methods in numerous applications. However, tracking control is still a challenge. Varying the target over time can cause the learning process to fail since the agent is unable to discern between trajectory and system dynamics. Only if the control target is assumed to be constant, a value function can be constructed. To solve this problem we propose to manipulate the state-action-reward-state tuples for training to simulate a constant target within each tuple. We further demonstrate that this mechanism can be used to move exploration noise to the trajectory. We successfully apply the presented reinforcement learning algorithm to speed control with varying setpoints both to a simulation model and a real-world road vehicle.","2251-2446","978-1-7281-6139-6","10.1109/ICCAR49639.2020.9108051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108051","machine learning in control applications;vehicle control applications;engineering applications","","automobiles;control engineering computing;control system synthesis;learning (artificial intelligence);velocity control","control design methods;state-action-reward-state tuples;speed tracking control;online reinforcement learning;car","","8","","26","IEEE","4 Jun 2020","","","IEEE","IEEE Conferences"
"Transfer Reinforcement Learning Across Homotopy Classes","Z. Cao; M. Kwon; D. Sadigh","Department of Computer Science, Stanford University, Stanford, CA, USA; Department of Computer Science, Stanford University, Stanford, CA, USA; Department of Computer Science, Stanford University, Stanford, CA, USA","IEEE Robotics and Automation Letters","18 Mar 2021","2021","6","2","2706","2713","The ability for robots to transfer their learned knowledge to new tasks—where data is scarce—is a fundamental challenge for successful robot learning. While fine-tuning has been well-studied as a simple but effective transfer approach in the context of supervised learning, it is not as well-explored in the context of reinforcement learning. In this work, we study the problem of fine-tuning in transfer reinforcement learning when tasks are parameterized by their reward functions, which are known beforehand. We conjecture that fine-tuning drastically underperforms when source and target trajectories are part of different homotopy classes: We demonstrate that fine-tuning policy parameters across homotopy classes compared to fine-tuning within a homotopy class requires more interaction with the environment, and in certain cases is impossible. We propose a novel fine-tuning algorithm, Ease-In-Ease-Out fine-tuning, that consists of a relaxing stage and a curriculum learning stage to enable transfer learning across homotopy classes. Finally, we evaluate our approach on several robotics-inspired simulated environments and empirically verify that the Ease-In-Ease-Out fine-tuning method can successfully fine-tune in a sample-efficient way compared to existing baselines.","2377-3766","","10.1109/LRA.2021.3057050","National Science Foundation(grant numbers:2006388); Defense Advanced Research Projects Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9345955","Intelligent systems;learning systems;learning (artificial intelligence)","Task analysis;Trajectory;Robots;Reinforcement learning;Measurement;Transfer learning;Supervised learning","control engineering computing;learning (artificial intelligence);robot programming","ease-in-ease-out fine-tuning method;robotics-inspired simulated environments;relaxing stage;curriculum learning stage;target trajectories;reward functions;fine-tuning policy parameters;homotopy classes;supervised learning;robot learning;transfer reinforcement learning","","7","","47","IEEE","3 Feb 2021","","","IEEE","IEEE Journals"
"On Simple Reactive Neural Networks for Behaviour-Based Reinforcement Learning","A. Pore; G. Aragon-Camarasa","Computer Vision and Autonomous group, School of Computing Science, University of Glasgow, UK; Computer Vision and Autonomous group, School of Computing Science, University of Glasgow, UK","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","7477","7483","We present a behaviour-based reinforcement learning approach, inspired by Brook's subsumption architecture, in which simple fully connected networks are trained as reactive behaviours. Our working assumption is that a pick and place robotic task can be simplified by leveraging domain knowledge of a robotics developer to decompose and train reactive behaviours; namely, approach, grasp, and retract. Then the robot autonomously learns how to combine reactive behaviours via an Actor-Critic architecture. We use an Actor-Critic policy to determine the activation and inhibition mechanisms of the reactive behaviours in a particular temporal sequence. We validate our approach in a simulated robot environment where the task is about picking a block and taking it to a target position while orienting the gripper from a top grasp. The latter represents an extra degree-of-freedom of which current end-to-end reinforcement learning approaches fail to generalise. Our findings suggest that robotic learning can be more effective if each behaviour is learnt in isolation and then combined them to accomplish the task. That is, our approach learns the pick and place task in 8,000 episodes, which represents a drastic reduction in the number of training episodes required by an end-to-end approach ( 95,000 episodes) and existing state-of-the-art algorithms.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197262","","Robots;Task analysis;Training;Computer architecture;Learning (artificial intelligence);Grasping;Feature extraction","grippers;learning (artificial intelligence);neural net architecture;neurocontrollers","reactive neural networks;fully connected networks;reactive behaviours;actor-critic architecture;robot environment;end-to-end reinforcement learning;robotic learning;pick and place task;behaviour-based reinforcement learning;Brook subsumption architecture;pick and place robotic task;actor-critic policy;activation mechanisms;inhibition mechanisms;gripper;degree-of-freedom","","7","","29","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Game-Theoretic Decision-Making for Autonomous Vehicles","M. Yuan; J. Shan; K. Mi","Department of Earth and Space Science, Lassonde school of Engineering, York University, ON, Canada; Department of Earth and Space Science, Lassonde school of Engineering, York University, ON, Canada; Division of Engineering Science, University of Toronto, ON, Canada","IEEE Robotics and Automation Letters","20 Dec 2021","2022","7","2","818","825","This letter presents an approach for implementing game-theoretic decision-making in combination with deep reinforcement learning to allow vehicles to make decisions at an unsignalized intersection by use of 2D Lidar to obtain their observations of the environment. The main novelty of this work is modeling multiple vehicles in a complex interaction scenario simultaneously as decision-makers with conservative, aggressive, and adaptive driving behaviors. The game model allows anticipating reactions of additional vehicles to the movements of the ego-vehicle without using any specific coordination or vehicle-to-vehicle communication. The solution of the game is based on cognitive hierarchy reasoning and it uses a deep reinforcement learning algorithm to obtain a near-optimal policy towards a specific goal in a realistic simulator (ROS-Gazebo). The trained models have been successfully tested on the simulator after training. Experiments show that the performance of the lab cars in the real-world is consistent with it in the simulation environment, which may have great significance to improve the safety of self-driving cars, as well as may reduce their dependence on road tests.","2377-3766","","10.1109/LRA.2021.3134249","NSERC Alliance(grant numbers:ALLRP 555847-20); Mitacs Accelerate(grant numbers:IT26108); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647929","Deep reinforcement learning;cognitive hierarchy theory;LSTM network;self-driving car;decision making","Automobiles;Cognition;Vehicle dynamics;Vehicles;Reinforcement learning;Games;Adaptation models","control engineering computing;decision making;deep learning (artificial intelligence);game theory;mobile robots;reinforcement learning;road traffic;traffic engineering computing","autonomous vehicles;game-theoretic decision-making;unsignalized intersection;2D Lidar;modeling multiple vehicles;complex interaction scenario;decision-makers;conservative driving behaviors;aggressive driving behaviors;adaptive driving behaviors;game model;ego-vehicle;vehicle-to-vehicle communication;deep reinforcement learning algorithm;trained models;simulation environment;ROS-Gazebo","","7","","26","IEEE","13 Dec 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning for Adaptive Illumination with X-rays","J. -R. Betterton; D. Ratner; S. Webb; M. Kochenderfer","Department of Computer Science, Stanford University; SLAC National Accelerator Laboratory; SLAC National Accelerator Laboratory; Department of Computer Science, Stanford University","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","328","334","We propose a learning algorithm for automating image sampling in scientific applications. We consider settings where images are sampled by controlling a probe beam's scanning trajectory over the image surface. We explore alternatives to obtaining images by the standard rastering method. We formulate the scanner control problem as a reinforcement learning (RL) problem and train a policy to adaptively sample only the highest value regions of the image, choosing the acquisition time and resolution for each sample position based on an observation of previous readings. We use convolutional neural network (CNN) policies to control the scanner as a way to generalize our approach to larger samples. We show simulation results for a simple policy on both synthetic data and real world data from an archaeological application.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9196614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196614","","Apertures;Trajectory;Learning (artificial intelligence);Time measurement;Imaging;Image reconstruction;X-rays","convolutional neural nets;image resolution;image sampling;image segmentation;learning (artificial intelligence);X-ray imaging","adaptive illumination;reinforcement learning;image sampling;image surface;convolutional neural network;rastering method;X-rays","","6","","39","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"A 3D Simulation Environment and Navigation Approach for Robot Navigation via Deep Reinforcement Learning in Dense Pedestrian Environment","Q. Liu; Y. Li; L. Liu","Department of Control Science and Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Department of Control Science and Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Department of Control Science and Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China","2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)","8 Oct 2020","2020","","","1514","1519","With the rapid development of mobile robot technology, robots are playing an increasingly important role in people's daily lives. As one of the key technologies of the basic functions of mobile robots, navigation also needs to deal with new challenges. How to navigate efficiently and collision-free in complex and changeable human environments is one of the problems that need to be solved urgently. Currently, mobile robots can achieve efficient navigation in static environments. However, in the unstructured and fast-changing environments of human daily society, robots need to make more flexible navigation strategies to deal with the dynamic scenarios. This paper built a 3D simulation environment for robot navigation via deep reinforcement learning in dense pedestrian environment. We also proposed a new navigation approach via deep reinforcement learning in dense pedestrian environment. The simulation environment of this paper integrates Gazebo, ROS navigation stack, Stable baselines and Social Force Pedestrian Simulator. In order to be able to collect the rich environmental information around the robot, our simulation environment is based on the Gazebo simulation platform. In order to use the traditional path planning methods, we introduce the ROS navigation stack. In order to make it easier to call the current mainstream reinforcement learning algorithms, we introduce Stable baselines which is a set of improved implementations of reinforcement learning algorithms based on OpenAI Baselines. In order to imitate dense pedestrian scenarios realistically, we introduce the Social Force Pedestrian Simulator which is a pedestrian simulation package, whose pedestrian's movement follows the rules of Social Force Movement. Our robot navigation approach combines the global optimality of traditional global path planning and the local barrier ability of reinforcement learning. Firstly, we plan global path by using A* algorithm. Secondly, we use Soft Actor Critic (SAC) to try to follow the waypoints generated at a certain distance on the global path to make action decisions on the premise of agile obstacle avoidance. Experiments show that our simulation environment can easily set up a robot navigation environment and navigation approaches can be simulated in various dense pedestrian environments.","2161-8089","978-1-7281-6904-0","10.1109/CASE48305.2020.9217023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9217023","","Navigation;Robots;Collision avoidance;Learning (artificial intelligence);Three-dimensional displays;Solid modeling;Machine learning","collision avoidance;control engineering computing;learning (artificial intelligence);mobile robots;navigation;neural nets","3D simulation environment;deep reinforcement learning;dense pedestrian environment;mobile robot technology;changeable human environments;static environments;flexible navigation strategies;ROS navigation stack;Gazebo simulation platform;pedestrian simulation package;robot navigation approach;social force pedestrian simulator;pedestrian movement;social force movement;global path planning;local barrier ability;A* algorithm;soft actor critic;SAC;agile obstacle avoidance;global optimality","","6","","45","IEEE","8 Oct 2020","","","IEEE","IEEE Conferences"
"Excavation Reinforcement Learning Using Geometric Representation","Q. Lu; Y. Zhu; L. Zhang","Robotics and Auto-Driving Lab, Baidu Research, Sunnyvale, CA, USA; Robotics and Auto-Driving Lab, Baidu Research, Sunnyvale, CA, USA; Robotics and Auto-Driving Lab, Baidu Research, Sunnyvale, CA, USA","IEEE Robotics and Automation Letters","25 Feb 2022","2022","7","2","4472","4479","Excavation of irregular rigid objects in clutter, such as fragmented rocks and wood blocks, is very challenging due to their complex interaction dynamics and highly variable geometries. In this paper, we adopt reinforcement learning (RL) to tackle this challenge and learn policies to plan for a sequence of excavation trajectories for irregular rigid objects, given point clouds of excavation scenes.Moreover, we separately learn a compact representation of the point cloud on geometric tasks that do not require human labeling. We show that using the representation reduces training time for RL, while achieving similar asymptotic performance compare to an end-to-end RL algorithm. When using a policy trained in simulation directly on a real scene, we show that the policy trained with the representation outperforms end-to-end RL. To our best knowledge, this letter presents the first application of RL to plan a sequence of excavation trajectories of irregular rigid objects in clutter.","2377-3766","","10.1109/LRA.2022.3150511","Baidu Research USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712242","Deep learning in grasping and manipulation;manipulation planning;reinforcement learning","Excavation;Point cloud compression;Task analysis;Trajectory;Planning;Feature extraction;Reinforcement learning","clutter;excavators;feature extraction;geometry;image representation;object detection;reinforcement learning","point clouds;excavation scenes;end-to-end RL;clutter;reinforcement learning;geometric representation;interaction dynamics","","6","","39","IEEE","11 Feb 2022","","","IEEE","IEEE Journals"
"Exploiting Symmetries in Reinforcement Learning of Bimanual Robotic Tasks","F. Amadio; A. Colomé; C. Torras","Department of Information Engineering, Università degli Studi di Padova, Padova, Italy; Institut de Robòtica i Informática Industrial, CSIC-UPC, Barcelona, Spain; Institut de Robòtica i Informática Industrial, CSIC-UPC, Barcelona, Spain","IEEE Robotics and Automation Letters","24 Feb 2019","2019","4","2","1838","1845","Movement primitives (MPs) have been widely adopted for representing and learning robotic movements using reinforcement learning policy search. Probabilistic movement primitives (ProMPs) are a kind of MP based on a stochastic representation over sets of trajectories, able to capture the variability allowed while executing a movement. This approach has proved effective in learning a wide range of robotic movements, but it comes with the necessity of dealing with a high-dimensional space of parameters. This may be a critical problem when learning tasks with two robotic manipulators, and this work proposes an approach to reduce the dimension of the parameter space based on the exploitation of symmetry. A symmetrization method for ProMPs is presented and used to represent two movements, employing a single ProMP for the first arm and a symmetry surface that maps that ProMP to the second arm. This symmetric representation is then adopted in reinforcement learning of bimanual tasks (from user-provided demonstrations), using relative entropy policy search algorithm. The symmetry-based approach developed has been tested in an experiment of cloth manipulation, showing a speed increment in learning the task.","2377-3766","","10.1109/LRA.2019.2898330","Spanish State Research Agency through the María de Maeztu Seal of Excellence to IRI(grant numbers:MDM-2016-0656); European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme(grant numbers:741930); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8637816","Learning and Adaptive systems;learning from demonstration;dual arm manipulation","Task analysis;Trajectory;Covariance matrices;Eigenvalues and eigenfunctions;Manipulators;Gaussian distribution","entropy;learning (artificial intelligence);learning systems;manipulators;motion control;search problems","bimanual robotic tasks;representing learning robotic movements;reinforcement learning policy search;probabilistic movement primitives;stochastic representation;robotic manipulators;parameter space;single ProMP;symmetry surface;symmetric representation;bimanual tasks;relative entropy policy search algorithm;symmetry-based approach","","6","","18","IEEE","8 Feb 2019","","","IEEE","IEEE Journals"
"Model-Based Reinforcement Learning for Time-Optimal Velocity Control","G. Hartmann; Z. Shiller; A. Azaria","Department of Mechanical Engineering and Mechatronics, Ariel University, Ariel, Israel; Department of Mechanical Engineering and Mechatronics, Ariel University, Ariel, Israel; Department of Computer Science, Ariel University, Ariel, Israel","IEEE Robotics and Automation Letters","7 Aug 2020","2020","5","4","6185","6192","Autonomous navigation has recently gained great interest in the field of reinforcement learning. However, little attention was given to the time-optimal velocity control problem, i.e. controlling a vehicle such that it travels at the maximal speed without becoming dynamically unstable (roll-over or sliding). Time optimal velocity control can be solved numerically using existing methods that are based on optimal control and vehicle dynamics. In this letter, we develop a model-based deep reinforcement learning to generate the time-optimal velocity control. Moreover, we introduce a method that uses a numerical solution that predicts whether the vehicle may become unstable and intervenes if needed. We show that our combined model outperforms several baselines as it achieves higher velocities (with only one minute of training) and does not encounter any failures during the training process.","2377-3766","","10.1109/LRA.2020.3012128","Ministry of Science and Technology, Israel; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9149717","Autonomous vehicle navigation;reinforcement learning;motion and path planning","Vehicle dynamics;Analytical models;Learning (artificial intelligence);Safety;Velocity control;Wheels;Stability analysis","learning (artificial intelligence);mobile robots;road vehicles;time optimal control;vehicle dynamics;velocity control","time-optimal velocity control problem;model-based deep reinforcement learning;vehicle dynamics;autonomous navigation","","6","","34","IEEE","27 Jul 2020","","","IEEE","IEEE Journals"
"Autonomous Single-Image Drone Exploration With Deep Reinforcement Learning and Mixed Reality","A. Devo; J. Mao; G. Costante; G. Loianno","Department of Engineering, University of Perugia, Perugia, Italy; Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Engineering, University of Perugia, Perugia, Italy; Tandon School of Engineering, New York University, Brooklyn, NY, USA","IEEE Robotics and Automation Letters","8 Mar 2022","2022","7","2","5031","5038","Autonomous exploration is a longstanding goal of the robotics community. Aerial drone navigation has proven to be especially challenging. The stringent requirements on cost, weight, maneuverability, and power consumption do not allow exploration approaches to easily be employed or adapted to different types of environments. End-to-End Deep Reinforcement Learning (DRL) techniques based on Convolutional Networks approximators, which grant constant-time computation, predefined memory usage, and deliver high visual perception capabilities, represent a very promising alternative to current state of the art solutions relying on metric environment reconstruction. In this work, we address the autonomous exploration problem with aerial robots with a monocular camera based on DRL. Specifically, we propose a novel asymmetric actor-critic model for drone exploration that efficiently leverages ground truth information provided by the simulator environment to speed up learning and enhance final exploration performances. Furthermore, in order to reduce the sim-to-real gap for exploration, we present a novel mixed reality framework that allows an easier, smoother, and safer simulation to real-world transition. Both aspects allow to further exploit the great potential of simulation engines and contribute to reducing the risk associated with directly deploying algorithms on a physical platform with no intermediate step between the simulation and the real world. This is well-known to create several safety concerns and be dangerous when deploying aerial vehicles. Experimental results with a drone exploring multiple environments show the effectiveness of the proposed approach.","2377-3766","","10.1109/LRA.2022.3154019","Technology Innovation Institute; NSF CPS(grant numbers:CNS-2121391); Qualcomm Research, Nokia; NYU Wireless; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721080","Aerial Systems: Applications;reinforcement learning;deep learning for visual perception","Drones;Training;Task analysis;Robots;Navigation;Autonomous aerial vehicles;Mixed reality","aircraft control;approximation theory;augmented reality;autonomous aerial vehicles;control engineering computing;convolutional neural nets;deep learning (artificial intelligence);mobile robots;reinforcement learning;robot vision","mixed reality framework;convolutional networks approximators;aerial vehicles;simulation engines;simulator environment;ground truth information;actor-critic model;aerial robots;visual perception;end-to-end deep reinforcement learning;aerial drone navigation;autonomous single-image drone exploration","","6","","37","IEEE","24 Feb 2022","","","IEEE","IEEE Journals"
"Courteous Behavior of Automated Vehicles at Unsignalized Intersections Via Reinforcement Learning","S. Yan; T. Welschehold; D. Büscher; W. Burgard","Department of Computer Science, University of Freiburg, Freiburg, Germany; Department of Computer Science, University of Freiburg, Freiburg, Germany; Department of Computer Science, University of Freiburg, Freiburg, Germany; Department of Computer Science, University of Freiburg, Freiburg, Germany","IEEE Robotics and Automation Letters","2 Nov 2021","2022","7","1","191","198","The transition from today's mostly human-driven traffic to a purely automated one will be a gradual evolution, with the effect that we will likely experience mixed traffic in the near future. Connected and automated vehicles can benefit human-driven ones and the whole traffic system in different ways, for example by improving collision avoidance and reducing traffic waves. Many studies have been carried out to improve intersection management, a significant bottleneck in traffic, with intelligent traffic signals or exclusively automated vehicles. However, the problem of how to improve mixed traffic at unsignalized intersections has received less attention. In this letter, we propose a novel approach to optimizing traffic flow at intersections in mixed traffic situations using deep reinforcement learning. Our reinforcement learning agent learns a policy for a centralized controller to let connected autonomous vehicles at unsignalized intersections give up their right of way and yield to other vehicles to optimize traffic flow. We implemented our approach and tested it in the traffic simulator SUMO based on simulated and real traffic data. The experimental evaluation demonstrates that our method significantly improves traffic flow through unsignalized intersections in mixed traffic settings and also provides better performance in a wide range of traffic situations compared to the state of the art.","2377-3766","","10.1109/LRA.2021.3121807","Cooperative Interacting Automobiles; German Science Foundation DFG; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9583896","Intelligent transportation systems;reinforcement learning;deep learning methods","Reinforcement learning;Junctions;Roads;Autonomous vehicles;Trajectory;Entropy;Trajectory planning","collision avoidance;learning (artificial intelligence);road safety;road traffic;road traffic control;road vehicles;traffic engineering computing","mixed traffic situations;deep reinforcement learning;connected autonomous vehicles;unsignalized intersections;traffic simulator SUMO;traffic data;mixed traffic settings;human-driven traffic;human-driven ones;traffic system;collision avoidance;reducing traffic waves;intersection management;intelligent traffic signals;exclusively automated vehicles;optimizing traffic flow","","6","","24","IEEE","21 Oct 2021","","","IEEE","IEEE Journals"
"Fine-Grained Driving Behavior Prediction via Context-Aware Multi-Task Inverse Reinforcement Learning","K. Nishi; M. Shimosaka","University of Tokyo, Tokyo; faculty of Tokyo Institute of Technology, Tokyo, Japan","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","2281","2287","Research on advanced driver assistance systems for reducing risks to vulnerable road users (VRUs) has recently gained popularity because the traffic accident reduction rate for VRUs is still small. Dealing with unexpected VRU movements on residential roads requires proficient acceleration and deceleration. Although fine-grained prediction of driving behavior through inverse reinforcement learning (IRL) has been reported with promising results in recent years, learning of a precise model fails when driving strategies vary with contextual factors, i.e., weather, time of day, road width, and traffic direction. In this work, we propose a novel multi-task IRL approach with a multilinear reward function to incorporate contextual information into the model. This approach can provide precise long-term prediction of fine-grained driving behavior while adjusting to context. Experimental results using actual driving data over 141 km with various contexts and roads confirm the success of this approach in terms of predicting defensive driving strategy even in unknown situations.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197126","","Roads;Context modeling;Task analysis;Vehicles;Hidden Markov models;Safety;Predictive models","behavioural sciences computing;driver information systems;learning (artificial intelligence);road accidents;road safety;road traffic","unexpected VRU movements;residential roads;proficient acceleration;deceleration;road width;traffic direction;multilinear reward function;contextual information;long-term prediction;defensive driving strategy;context-aware multitask inverse reinforcement learning;advanced driver assistance systems;vulnerable road users;traffic accident reduction rate;multitask IRL approach;fine-grained driving behavior prediction;inverse reinforcement learning","","5","","33","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"Constrained-Space Optimization and Reinforcement Learning for Complex Tasks","Y. -Y. Tsai; B. Xiao; E. Johns; G. -Z. Yang","Hamlyn Centre for Robotic Surgery, SW7 2AZ London, U.K.; Hamlyn Centre for Robotic Surgery, SW7 2AZ London, U.K.; Robot Learning Lab, Imperial College London, SW7 2AZ London, U.K.; Hamlyn Centre for Robotic Surgery, SW7 2AZ London, U.K.","IEEE Robotics and Automation Letters","22 Jan 2020","2020","5","2","683","690","Learning from demonstration is increasingly used for transferring operator manipulation skills to robots. In practice, it is important to cater for limited data and imperfect human demonstrations, as well as underlying safety constraints. This article presents a constrained-space optimization and reinforcement learning scheme for managing complex tasks. Through interactions within the constrained space, the reinforcement learning agent is trained to optimize the manipulation skills according to a defined reward function. After learning, the optimal policy is derived from the well-trained reinforcement learning agent, which is then implemented to guide the robot to conduct tasks that are similar to the experts' demonstrations. The effectiveness of the proposed method is verified with a robotic suturing task, demonstrating that the learned policy outperformed the experts' demonstrations in terms of the smoothness of the joint motion and end-effector trajectories, as well as the overall task completion time.","2377-3766","","10.1109/LRA.2020.2965392","Engineering and Physical Sciences Research Council(grant numbers:EP/L020688/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954748","Learn from demonstration (LfD);medical robotics;reinforcement learning (RL);robot learning;robotic suturing","Task analysis;Optimization;Reinforcement learning;Trajectory;Robot learning;Mathematical model","end effectors;learning (artificial intelligence);medical robotics;optimisation;trajectory control","constrained-space optimization;reinforcement learning agent;robotic suturing task;safety constraints;operator manipulation skills;joint motion;end-effector trajectories;complex tasks","","5","","29","IEEE","9 Jan 2020","","","IEEE","IEEE Journals"
"Natural Walking With Musculoskeletal Models Using Deep Reinforcement Learning","J. Weng; E. Hashemi; A. Arami","Department Mechanical and Mechatronics Engineering, University of Waterloo, Waterloo, ON, Canada; Mechanical Engineering Department, University of Alberta, Edmonton, AB, Canada; Department Mechanical and Mechatronics Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Robotics and Automation Letters","8 Apr 2021","2021","6","2","4156","4162","Human gait optimality has been investigated recently, with the development of detailed musculoskeletal models, through trajectory optimization approaches or deep reinforcement learning (DRL). Trajectory optimization studies are limited by the trajectory length and can only generate open-loop solutions. While existing DRL solutions provide closed-loop control policies without trajectory length limit, they either do not evaluate the naturalness of the behaviour, or directly impose experimental tracking data. In this letter, a DRL-based approach is proposed with a nature-inspired curriculum learning (CL) scheme and a neuromechanically-inspired reward function. This approach generates close-to-natural human walking without the aid of experimental data. Our CL scheme is realized by an evolving reward function, targeting simpler behaviours such as standing and stepping first, then gradually refining the gait. The emerged gait from the closed-loop stochastic policy demonstrated a strong correlation with human gait kinematics, with Pearson correlations of 0.95 and 0.83 at the hip and knee, respectively, and higher gait symmetry than two other DRL-based control policies without CL. Our approach was also found to have efficient convergence to walking-capable policy. This approach can facilitate the development of assistive robotic systems by providing a “human” controller, and could enable decentralized adaptation between the agent and the assistive robotic devices.","2377-3766","","10.1109/LRA.2021.3067617","Natural Sciences and Engineering Research Council of Canada; New Frontiers in Research Fund(grant numbers:NFRFE-2018-01698); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382086","Gait;modeling and simulating humans;musculoskeletal model;rehabilitation robotics;reinforcement learning","Legged locomotion;Musculoskeletal system;Computational modeling;Reinforcement learning;Rehabilitation robotics","","","","4","","27","IEEE","19 Mar 2021","","","IEEE","IEEE Journals"
"Multifingered Grasping Based on Multimodal Reinforcement Learning","H. Liang; L. Cong; N. Hendrich; S. Li; F. Sun; J. Zhang","Group TAMS, Department of Informatics, Universität Hamburg, Hamburg, Germany; Group TAMS, Department of Informatics, Universität Hamburg, Hamburg, Germany; Group TAMS, Department of Informatics, Universität Hamburg, Hamburg, Germany; Group TAMS, Department of Informatics, Universität Hamburg, Hamburg, Germany; Deptartment of Computer Science and Technology, Tsinghua University, Beijing, China; Group TAMS, Department of Informatics, Universität Hamburg, Hamburg, Germany","IEEE Robotics and Automation Letters","5 Jan 2022","2022","7","2","1174","1181","In this work, we tackle the challenging problem of grasping novel objects using a high-DoF anthropomorphic hand-arm system. Combining fingertip tactile sensing, joint torques and proprioception, a multimodal agent is trained in simulation to learn the finger motions and to determine when to lift an object. Binary contact information and level-based joint torques simplify transferring the learned model to the real robot. To reduce the exploration space, we first generate postural synergies by collecting a dataset covering various grasp types and using principal component analysis. Curriculum learning is further applied to adjust and randomize the initial object pose based on the training performance. Simulation and real robot experiments with dedicated initial grasping poses show that our method outperforms two baseline models in the grasp success rate both for seen and unseen objects. This learning approach further serves as a fundamental technology for complex in-hand manipulations based on multi-sensory the system.","2377-3766","","10.1109/LRA.2021.3138545","Deutsche Forschungsgemeinschaft; National Natural Science Foundation of China(grant numbers:61621136008); China Scholarship Council; European project H2020 Ultracept(grant numbers:778602); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664242","Grasping;multifingered hands;reinforcement learning","Grasping;Robot sensing systems;Sensors;Space exploration;Principal component analysis;Wrist;Training","dexterous manipulators;mechanoception;multi-agent systems;principal component analysis;reinforcement learning;sensor fusion;tactile sensors","multifingered grasping;multimodal reinforcement learning;high-DoF anthropomorphic hand-arm system;fingertip tactile sensing;proprioception;multimodal agent;finger motions;level-based joint torques;postural synergies;grasp types;principal component analysis;curriculum learning;robot experiments;dedicated initial grasping poses;grasp success rate;learning approach;in-hand manipulations","","4","","30","CCBY","28 Dec 2021","","","IEEE","IEEE Journals"
