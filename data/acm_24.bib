@article{10.1145/3418498,
author = {Wu, Nan and Deng, Lei and Li, Guoqi and Xie, Yuan},
title = {Core Placement Optimization for Multi-Chip Many-Core Neural Network Systems with Reinforcement Learning},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/3418498},
doi = {10.1145/3418498},
abstract = {Multi-chip many-core neural network systems are capable of providing high parallelism benefited from decentralized execution, and they can be scaled to very large systems with reasonable fabrication costs. As multi-chip many-core systems scale up, communication latency related effects will take a more important portion in the system performance. While previous work mainly focuses on the core placement within a single chip, there are two principal issues still unresolved: the communication-related problems caused by the non-uniform, hierarchical on/off-chip communication capability in multi-chip systems, and the scalability of these heuristic-based approaches in a factorially growing search space. To this end, we propose a reinforcement-learning-based method to automatically optimize core placement through deep deterministic policy gradient, taking into account information of the environment by performing a series of trials (i.e., placements) and using convolutional neural networks to extract spatial features of different placements. Experimental results indicate that compared with a naive sequential placement, the proposed method achieves 1.99\texttimes{} increase in throughput and 50.5\% reduction in latency; compared with the simulated annealing, an effective technique to approximate the global optima in an extremely large search space, our method improves the throughput by 1.22\texttimes{} and reduces the latency by 18.6\%. We further demonstrate that our proposed method is capable to find optimal placements taking advantages of different communication properties caused by different system configurations, and work in a topology-agnostic manner.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {oct},
articleno = {11},
numpages = {27},
keywords = {machine learning for system, Multi-chip many-core architecture, core placement optimization, neural network accelerator}
}

@inproceedings{10.1145/3507971.3507982,
author = {Li, Qingya and Guo, Li and Dong, Chao and Mu, Xidong},
title = {3D Trajectory Design of UAV Based on Deep Reinforcement Learning in Time-Varying Scenes},
year = {2022},
isbn = {9781450385190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507971.3507982},
doi = {10.1145/3507971.3507982},
abstract = {A joint framework is proposed for the 3D trajectory design of an unmanned aerial vehicle (UAV) as an flying base station under the time-varying scenarios of users’ mobility and communication request probability changes. The problem of 3D trajectory design is formulated for maximizing the throughput during a UAV’s flying period while satisfying the rate requirement of all ground users (GUEs). Specifically, we consider that GUEs change their positions and communication request probabilities at each time slot; the UAV needs to predict these changes so that it can design its 3D trajectory in advance to achieve the optimization target. In an effort to solve this pertinent problem, an echo state network (ESN) based prediction algorithm is first proposed for predicting the positions and communication request probabilities of GUEs. Based on these predictions, a Deep Reinforcement Learning (DRL) method is then invoked for finding the optimal deployment locations of UAV in each time slots. The proposed method 1) uses ESN based predictions to represent a part of DRL agent’s state; 2) designs the action and reward for DRL agent to learn the environment and its dynamics; 3) makes optimal strategy under the guidance of a double deep Q network (DDQN). The simulation results show that the UAV can dynamically adjust its trajectory to adapt to time-varying scenarios through our proposed algorithm and throughput gains of about 10.68\% are achieved.},
booktitle = {Proceedings of the 7th International Conference on Communication and Information Processing},
pages = {56–62},
numpages = {7},
keywords = {Time-varying Scenes, echo state network (ESN), Deep Reinforcement Learning (DRL), UAV},
location = {Beijing, China},
series = {ICCIP '21}
}

@inproceedings{10.1145/2576768.2598351,
author = {Nakata, Masaya and Lanzi, Pier Luca and Kovacs, Tim and Takadama, Keiki},
title = {Complete Action Map or Best Action Map in Accuracy-Based Reinforcement Learning Classifier Systems},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598351},
doi = {10.1145/2576768.2598351},
abstract = {We study two existing Learning Classifier Systems (LCSs): XCS, which has a complete map (which covers all actions in each state), and XCSAMm, which has a best action map (which covers only the highest-return action in each state). This allows XCSAM to learn with a smaller population size limit (but larger population size) and to learn faster than XCS on well-behaved tasks. However, many tasks have dif- ficulties like noise and class imbalances. XCS and XCSAM have not been compared on such problems before. This pa- per aims to discover which kind of map is more robust to these difficulties. We apply them to a classification problem (the multiplexer problem) with class imbalance, Gaussian noise or alternating noise (where we return the reward for a different action). We also compare them on real-world data from the UCI repository without adding noise. We analyze how XCSAM focuses on the best action map and introduce a novel deletion mechanism that helps to evolve classifiers towards a best action map. Results show the best action map is more robust (has higher accuracy and sometimes learns faster) in all cases except small amounts of alternat- ing noise.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {557–564},
numpages = {8},
keywords = {complete action map, classification, best action map, XCSAM, XCS, learning classifier system},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.1145/3485447.3512215,
author = {Le, Thai and Tran-Thanh, Long and Lee, Dongwon},
title = {Socialbots on Fire: Modeling Adversarial Behaviors of Socialbots via Multi-Agent Hierarchical Reinforcement Learning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512215},
doi = {10.1145/3485447.3512215},
abstract = {Socialbots are software-driven user accounts on social platforms, acting autonomously (mimicking human behavior), with the aims to influence the opinions of other users or spread targeted misinformation for particular goals. As socialbots undermine the ecosystem of social platforms, they are often considered harmful. As such, there have been several computational efforts to auto-detect the socialbots. However, to our best knowledge, the adversarial nature of these socialbots has not yet been studied. This begs a question “can adversaries, controlling socialbots, exploit AI techniques to their advantage?” To this question, we successfully demonstrate that indeed it is possible for adversaries to exploit computational learning mechanism such as reinforcement learning (RL) to maximize the influence of socialbots while avoiding being detected. We first formulate the adversarial socialbot learning as a cooperative game between two functional hierarchical RL agents. While one agent curates a sequence of activities that can avoid the detection, the other agent aims to maximize network influence by selectively connecting with right users. Our proposed policy networks train with a vast amount of synthetic graphs and generalize better than baselines on unseen real-life graphs both in terms of maximizing network influence (up to +18\%) and sustainable stealthiness (up to +40\% undetectability) under a strong bot detector (90\% detection accuracy). During inference, the complexity of our approach scales linearly, independent of a network’s structure and the virality of news. This makes our attack very practical in a real-life setting.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {545–554},
numpages = {10},
keywords = {reinforcement learning, social bot, socialbot, adversarial},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1613/jair.1.14390,
author = {Catacora Ocana, Jim Martin and Capobianco, Roberto and Nardi, Daniele},
title = {An Overview of Environmental Features That Impact Deep Reinforcement Learning in Sparse-Reward Domains},
year = {2023},
issue_date = {May 2023},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {76},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.14390},
doi = {10.1613/jair.1.14390},
abstract = {Deep reinforcement learning has achieved impressive results in recent years; yet, it is still severely troubled by environments showcasing sparse rewards. On top of that, not all sparse-reward environments are created equal, i.e., they can differ in the presence or absence of various features, with many of them having a great impact on learning. In light of this, the present work puts together a literature compilation of such environmental features, covering particularly those that have been taken advantage of and those that continue to pose a challenge. We expect this effort to provide guidance to researchers for assessing the generality of their new proposals and to call their attention to issues that remain unresolved when dealing with sparse rewards.},
journal = {J. Artif. Int. Res.},
month = {may},
numpages = {38}
}

@inproceedings{10.1145/3545008.3545085,
author = {Nguyen, Nang Hung and Nguyen, Phi Le and Nguyen, Thuy Dung and Nguyen, Trung Thanh and Nguyen, Duc Long and Nguyen, Thanh Hung and Pham, Huy Hieu and Truong, Thao Nguyen},
title = {FedDRL: Deep Reinforcement Learning-Based Adaptive Aggregation for Non-IID Data in Federated Learning},
year = {2023},
isbn = {9781450397339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545008.3545085},
doi = {10.1145/3545008.3545085},
abstract = {The uneven distribution of local data across different edge devices (clients) results in slow model training and accuracy reduction in federated learning. Naive federated learning (FL) strategy and most alternative solutions attempted to achieve more fairness by weighted aggregating deep learning models across clients. This work introduces a novel non-IID type encountered in real-world datasets, namely cluster-skew, in which groups of clients have local data with similar distributions, causing the global model to converge to an over-fitted solution. To deal with non-IID data, particularly the cluster-skewed data, we propose FedDRL, a novel FL model that employs deep reinforcement learning to adaptively determine each client’s impact factor (which will be used as the weights in the aggregation process). Extensive experiments on a suite of federated datasets confirm that the proposed FedDRL improves favorably against FedAvg and FedProx methods, e.g., up to 4.05\% and 2.17\% on average for the CIFAR-100 dataset, respectively.},
booktitle = {Proceedings of the 51st International Conference on Parallel Processing},
articleno = {73},
numpages = {11},
keywords = {Federated Learning, Deep Reinforcement Learning, Data Heterogeneity},
location = {Bordeaux, France},
series = {ICPP '22}
}

@inproceedings{10.1145/3502223.3502224,
author = {Zhou, Xingchen and Wang, Peng and Luo, Qiqing and Pan, Zhe},
title = {Multi-Hop Knowledge Graph Reasoning Based on Hyperbolic Knowledge Graph Embedding and Reinforcement Learning},
year = {2022},
isbn = {9781450395656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502223.3502224},
doi = {10.1145/3502223.3502224},
abstract = {Some unseen facts in knowledge graphs (KGs) can be complemented by multi-hop knowledge graph reasoning. The process of multi-hop reasoning can be presented as a serialized decision problem, and then be solved with reinforcement learning (RL). However, existing RL-based reasoning methods cannot deal with the hierarchical information in KGs effectively. To solve this issue, we propose a novel RL-based multi-hop KG reasoning model Path Additional Action-space Ranking (PAAR). Our model first proposes a hyperbolic knowledge graph embedding (KGE) model which can effectively capture hierarchical information in KGs. To introduce hierarchical information into RL, the hyperbolic KGE vector is subsequently added to the state space and helps to expand the action space. In order to alleviate the reward sparsity problem in RL, our model utilizes the score function of hyperbolic KGE model as a soft reward. Finally, the metric of hyperbolic space is added to the training of RL as a penalty strategy to constrain the sufficiency of multi-hop reasoning paths. Experimental results on two real-world datasets show that our proposed model not only provides effective answers but also offers sufficient paths.},
booktitle = {Proceedings of the 10th International Joint Conference on Knowledge Graphs},
pages = {1–9},
numpages = {9},
keywords = {knowledge graph reasoning, knowledge graph embedding, reinforcement learning},
location = {Virtual Event, Thailand},
series = {IJCKG '21}
}

@inproceedings{10.1145/3426826.3426833,
author = {Cui, Yujun and Zhang, Guohua and Dong, Wei and Sun, Xinya and Yang, Weihua},
title = {Knowledge-Based Deep Reinforcement Learning for Train Automatic Stop Control of High-Speed Railway},
year = {2020},
isbn = {9781450388344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426826.3426833},
doi = {10.1145/3426826.3426833},
abstract = {Train automatic stop control (TASC) is one of the key techniques of Automatic train operation (ATO) to achieve high stopping precision. Aiming to improve accurate stopping performance, this paper proposes a novel TASC method based on double deep Q-network (DDQN) using knowledge from experienced driver to address time allocation of braking command. The knowledge is used for estimating a braking command to improve the learning efficiency, and DDQN determines the execution time of the command to avoid frequent switching of commands and ultimately reach better stopping decisions. The proposed method can achieve a probability of 100\% and significantly outperforms 3 existing methods on the stopping errors within ± 0.30 m under high disturbances in the simulation platform, which is based on actual field data from the Beijing-Shenyang high-speed railway provided by cooperative enterprise.},
booktitle = {Proceedings of the 2020 3rd International Conference on Machine Learning and Machine Intelligence},
pages = {31–36},
numpages = {6},
keywords = {Knowledge, Double deep Q-network, Deep reinforcement learning, Train automatic stop control},
location = {Hangzhou, China},
series = {MLMI '20}
}

@inproceedings{10.1145/3302505.3310069,
author = {Zhang, Chi and Kuppannagari, Sanmukh R. and Xiong, Chuanxiu and Kannan, Rajgopal and Prasanna, Viktor K.},
title = {A Cooperative Multi-Agent Deep Reinforcement Learning Framework for Real-Time Residential Load Scheduling},
year = {2019},
isbn = {9781450362832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302505.3310069},
doi = {10.1145/3302505.3310069},
abstract = {Internet-of-Things (IoT) enabled monitoring and control capabilities are enabling increasing numbers of household users with controllable loads to actively participate in smart grid energy management. Realizing an efficient real-time energy management system that takes advantage of these developments requires novel techniques for managing the increased complexity of the control action space in resolving multiple challenges such as the uncertainty in energy prices and renewable energy output along with the need to satisy physical grid constraints such as transformer capacity. Addressing these challenges, we develop a multi-household energy management framework for residential units connected to the same transformer and containing DERs such as PV, ESS and controllable loads. The goal of our framework is to schedule controllable household appliances and ESS such that the cost of procuring electricity from the utility over a horizon is minimized while physical grid constraints are satisfied at each scheduling step. Traditional energy management frameworks either perform global optimization to satisfy grid constraints but suffer from high computational complexity (for example Integer Program, Mixed Integer Programming frameworks and centralized reinforcement learning) or perform decentralized real-time energy management without satisfying global grid constraints (for example multi-agent reinforcement learning with no cooperation). In contrast, we propose a cooperative multi-agent reinforcement learning (MARL) framework that i) operates in real-time, and ii) performs explicit collaboration to satisfy global grid constraints. The novelty in our framework is two fold. Firstly, our framework trains multiple independent learners (IL) for each household in parallel using historical data and performs real-time inferencing of control actions using the most recent system state. Secondly, our framework contains a low complexity knapsack based cooperation agent which combines the outputs of ILs to minimize cost while satisfying grid constraints. Simulation results show that our cooperative MARL approach achieves significant cost improvement over centralized reinforcement learning and day-ahead planning baselines. Moreover, our approach strictly satisfies physical constraints with no apriori knowledge of system dynamics while the baseline approaches have occasional violations. We also measure the training and inference time by ranging the number of households from 1 to 25. Results show that our cooperative MARL approach scales best among various approaches.},
booktitle = {Proceedings of the International Conference on Internet of Things Design and Implementation},
pages = {59–69},
numpages = {11},
keywords = {deep reinforcement learning, smart home, real-time load scheduling, internet-of-things, multi-agent},
location = {Montreal, Quebec, Canada},
series = {IoTDI '19}
}

@inproceedings{10.1145/3534678.3539040,
author = {Zhang, Qihua and Liu, Junning and Dai, Yuzhuo and Qi, Yiyan and Yuan, Yifan and Zheng, Kunlun and Huang, Fan and Tan, Xianfeng},
title = {Multi-Task Fusion via Reinforcement Learning for Long-Term User Satisfaction in Recommender Systems},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539040},
doi = {10.1145/3534678.3539040},
abstract = {Recommender System (RS) is an important online application that affects billions of users every day. The mainstream RS ranking framework is composed of two parts: a Multi-Task Learning model (MTL) that predicts various user feedback, i.e., clicks, likes, sharings, and a Multi-Task Fusion model (MTF) that combines the multi-task outputs into one final ranking score with respect to user satisfaction. There has not been much research on the fusion model while it has great impact on the final recommendation as the last crucial process of the ranking. To optimize long-term user satisfaction rather than obtain instant returns greedily, we formulate MTF task as Markov Decision Process (MDP) within a recommendation session and propose a Batch Reinforcement Learning (RL) based Multi-Task Fusion framework (BatchRL-MTF) that includes a Batch RL framework and an online exploration. The former exploits Batch RL to learn an optimal recommendation policy from the fixed batch data offline for long-term user satisfaction, while the latter explores potential high-value actions online to break through the local optimal dilemma. With a comprehensive investigation on user behaviors, we model the user satisfaction reward with subtle heuristics from two aspects of user stickiness and user activeness. Finally, we conduct extensive experiments on a billion-sample level real-world dataset to show the effectiveness of our model. We propose a conservative offline policy estimator (Conservative-OPEstimator) to test our model offline. Furthermore, we take online experiments in a real recommendation environment to compare performance of different models. As one of few Batch RL researches applied in MTF task successfully, our model has also been deployed on a large-scale industrial short video platform, serving hundreds of millions of users.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4510–4520},
numpages = {11},
keywords = {multi-task fusion, recommender system, batch reinforcement learning, long-term user satisfaction},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1145/3447623,
author = {Chen, Jianguo and Li, Kenli and Li, Keqin and Yu, Philip S. and Zeng, Zeng},
title = {Dynamic Bicycle Dispatching of Dockless Public Bicycle-Sharing Systems Using Multi-Objective Reinforcement Learning},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3447623},
doi = {10.1145/3447623},
abstract = {As a new generation of Public Bicycle-sharing Systems (PBS), the Dockless PBS (DL-PBS) is an important application of cyber-physical systems and intelligent transportation. How to use artificial intelligence to provide efficient bicycle dispatching solutions based on dynamic bicycle rental demand is an essential issue for DL-PBS. In this article, we propose MORL-BD, a dynamic bicycle dispatching algorithm based on multi-objective reinforcement learning to provide the optimal bicycle dispatching solution for DL-PBS. We model the DL-PBS system from the perspective of cyber-physical systems and use deep learning to predict the layout of bicycle parking spots and the dynamic demand of bicycle dispatching. We define the multi-route bicycle dispatching problem as a multi-objective optimization problem by considering the optimization objectives of dispatching costs, dispatch truck's initial load, workload balance among the trucks, and the dynamic balance of bicycle supply and demand. On this basis, the collaborative multi-route bicycle dispatching problem among multiple dispatch trucks is modeled as a multi-agent and multi-objective reinforcement learning model. All dispatch paths between parking spots are defined as state spaces, and the reciprocal of dispatching costs is defined as a reward. Each dispatch truck is equipped with an agent to learn the optimal dispatch path in the dynamic DL-PBS network. We create an elite list to store the Pareto optimal solutions of bicycle dispatch paths found in each action, and finally get the Pareto frontier. Experimental results on the actual DL-PBS show that compared with existing methods, MORL-BD can find a higher quality Pareto frontier with less execution time.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {sep},
articleno = {34},
numpages = {24},
keywords = {Pareto optimality, intelligent transportation, multi-objective reinforcement learning, bicycle dispatching, Bicycle-sharing systems}
}

@inproceedings{10.5555/2187681.2187699,
author = {Dethlefs, Nina and Cuay\'{a}huitl, Heriberto},
title = {Combining Hierarchical Reinforcement Learning and Bayesian Networks for Natural Language Generation in Situated Dialogue},
year = {2011},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Language generators in situated domains face a number of content selection, utterance planning and surface realisation decisions, which can be strictly interdependent. We therefore propose to optimise these processes in a joint fashion using Hierarchical Reinforcement Learning. To this end, we induce a reward function for content selection and utterance planning from data using the PARADISE framework, and suggest a novel method for inducing a reward function for surface realisation from corpora. It is based on generation spaces represented as Bayesian Networks. Results in terms of task success and human-likeness suggest that our unified approach performs better than a baseline optimised in isolation or a greedy or random baseline. It receives human ratings close to human authors.},
booktitle = {Proceedings of the 13th European Workshop on Natural Language Generation},
pages = {110–120},
numpages = {11},
location = {Nancy, France},
series = {ENLG '11}
}

@inproceedings{10.1109/RoSE.2019.00011,
author = {Mallozzi, Piergiuseppe and Castellano, Ezequiel and Pelliccione, Patrizio and Schneider, Gerardo and Tei, Kenji},
title = {A Runtime Monitoring Framework to Enforce Invariants on Reinforcement Learning Agents Exploring Complex Environments},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/RoSE.2019.00011},
doi = {10.1109/RoSE.2019.00011},
abstract = {Without prior knowledge of the environment, a software agent can learn to achieve a goal using machine learning. Model-free Reinforcement Learning (RL) can be used to make the agent explore the environment and learn to achieve its goal by trial and error. Discovering effective policies to achieve the goal in a complex environment is a major challenge for RL. Furthermore, in safety-critical applications, such as robotics, an unsafe action may cause catastrophic consequences in the agent or in the environment. In this paper, we present an approach that uses runtime monitoring to prevent the reinforcement learning agent to perform "wrong" actions and to exploit prior knowledge to smartly explore the environment. Each monitor is defined by a property that we want to enforce to the agent and a context. The monitors are orchestrated by a meta-monitor that activates and deactivates them dynamically according to the context in which the agent is learning. We have evaluated our approach by training the agent in randomly generated learning environments. Our results show that our approach blocks the agent from performing dangerous and safety-critical actions in all the generated environments. Besides, our approach helps the agent to achieve its goal faster by providing feedback and shaping its reward during learning.},
booktitle = {Proceedings of the 2nd International Workshop on Robotics Software Engineering},
pages = {5–12},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {RoSE '19}
}

@inproceedings{10.1145/3491396.3506544,
author = {Luo, Yu-Chen and Tsai, Chun-Wei},
title = {Multi-Agent Reinforcement Learning Based on Two-Step Neighborhood Experience for Traffic Light Control},
year = {2022},
isbn = {9781450391603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491396.3506544},
doi = {10.1145/3491396.3506544},
abstract = {Several recent studies pointed out that an effective traffic signal/light control strategy will be able to mitigate the traffic congestion problem, and therefore variants of solutions have been presented for solving this optimization problem. The multi-agent reinforcement learning (MARL) is one of the promising methods because it can provide good traffic control strategies for such complex environments. However, because each agent on its intersection of most MARL-based algorithms has only partial information from the observations of its intersection, the traffic control plan based on such incomplete information of all agents may not always be useful for improving traffic of an entire city. To enhance the performance of the MARL in solving the traffic light control problem, we present an effective algorithm based on an effective communication protocol to share the information between agents of neighbor intersections to make an integrated traffic light control plan. Moreover, a two-step decision mechanism is presented in this study to further improve the performance of MARL for traffic light control. To evaluate the performance of the proposed algorithm, we compared it with several message-passing-based algorithms on the simulator of Simulation of Urban MObility (SUMO) in this study. The results show that the proposed algorithm is capable of finding better results than all the other message-passing-based algorithms compared in this study for traffic light control problems.},
booktitle = {Proceedings of the 2021 ACM International Conference on Intelligent Computing and Its Emerging Applications},
pages = {28–33},
numpages = {6},
keywords = {SUMO, Multi-agent reinforcement learning, traffic light control},
location = {Jinan, China},
series = {ACM ICEA '21}
}

@inproceedings{10.1145/3394486.3403128,
author = {Wang, Pengyang and Liu, Kunpeng and Jiang, Lu and Li, Xiaolin and Fu, Yanjie},
title = {Incremental Mobile User Profiling: Reinforcement Learning with Spatial Knowledge Graph for Modeling Event Streams},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403128},
doi = {10.1145/3394486.3403128},
abstract = {We study the integration of reinforcement learning and spatial knowledge graph for incremental mobile user profiling, which aims to map mobile users to dynamically-updated profile vectors by incremental learning from a mixed-user event stream. After exploring many profiling methods, we identify a new imitation based criteria to better evaluate and optimize profiling accuracy. Considering the objective of teaching an autonomous agent to imitate a mobile user to plan next-visit based on the user's profile, the user profile is the most accurate when the agent can perfectly mimic the activity patterns of the user. We propose to formulate the problem into a reinforcement learning task, where an agent is a next-visit planner, an action is a POI that a user will visit next, and the state of environment is a fused representation of a user and spatial entities (e.g., POIs, activity types, functional zones). An event that a user takes an action to visit a POI, will change the environment, resulting into a new state of user profiles and spatial entities, which helps the agent to predict next visit more accurately. After analyzing such interactions among events, users, and spatial entities, we identify (1)semantic connectivity among spatial entities, and, thus, introduce a spatial Knowledge Graph (KG) to characterize the semantics of user visits over connected locations, activities, and zones. Besides, we identify (2) mutual influence between users and the spatial KG, and, thus, develop a mutual-updating strategy between users and the spatial KG, mixed with temporal context, to quantify the state representation that evolves over time. Along these lines, we develop a reinforcement learning framework integrated with spatial KG. The proposed framework can achieve incremental learning in multi-user profiling given a mixed-user event stream. Finally, we apply our approach to human mobility activity prediction and present extensive experiments to demonstrate improved performances.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {853–861},
numpages = {9},
keywords = {incremental learning, reinforcement learning, mobile user profiling, spatial knowledge graph},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3194133.3194153,
author = {Moghadam, Mahshid Helali and Saadatmand, Mehrdad and Borg, Markus and Bohlin, Markus and Lisper, Bj\"{o}rn},
title = {Adaptive Runtime Response Time Control in PLC-Based Real-Time Systems Using Reinforcement Learning},
year = {2018},
isbn = {9781450357159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194133.3194153},
doi = {10.1145/3194133.3194153},
abstract = {Timing requirements such as constraints on response time are key characteristics of real-time systems and violations of these requirements might cause a total failure, particularly in hard real-time systems. Runtime monitoring of the system properties is of great importance to check the system status and mitigate such failures. Thus, a runtime control to preserve the system properties could improve the robustness of the system with respect to timing violations. Common control approaches may require a precise analytical model of the system which is difficult to be provided at design time. Reinforcement learning is a promising technique to provide adaptive model-free control when the environment is stochastic, and the control problem could be formulated as a Markov Decision Process. In this paper, we propose an adaptive runtime control using reinforcement learning for real-time programs based on Programmable Logic Controllers (PLCs), to meet the response time requirements. We demonstrate through multiple experiments that our approach could control the response time efficiently to satisfy the timing requirements.},
booktitle = {Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems},
pages = {217–223},
numpages = {7},
keywords = {adaptive response time control, PLC-based real-time programs, reinforcement learning, runtime monitoring},
location = {Gothenburg, Sweden},
series = {SEAMS '18}
}

@article{10.14778/3611540.3611629,
author = {Wang, Junxiong and Gray, Mitchell and Trummer, Immanuel and Kara, Ahmet and Olteanu, Dan},
title = {Demonstrating ADOPT: Adaptively Optimizing Attribute Orders for Worst-Case Optimal Joins via Reinforcement Learning},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611629},
doi = {10.14778/3611540.3611629},
abstract = {Performance of worst-case optimal join algorithms depends on the order in which the join attributes are processed. It is challenging to identify suitable orders prior to query execution due to the huge search space of possible orders and unreliable execution cost estimates in case of data skew or data correlation.We demonstrate ADOPT, a novel query engine that integrates adaptive query processing with a worst-case optimal join algorithm. ADOPT divides query execution into episodes, during which different attribute orders are invoked. With runtime feedback on performance of different attribute orders, ADOPT rapidly approaches near-optimal orders. Moreover, ADOPT uses a unique data structure which keeps track of the processed input data to prevent redundant work across different episodes. It selects attribute orders to try via reinforcement learning, balancing the need for exploring new orders with the desire to exploit promising orders. In experiments, ADOPT outperforms baselines, including commercial and open-source systems utilizing worst-case optimal join algorithms, particularly for complex queries that are difficult to optimize.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {4094–4097},
numpages = {4}
}

@inproceedings{10.5555/3545946.3598616,
author = {Liu, Shanqi and Hu, Yujing and Wu, Runze and Xing, Dong and Xiong, Yu and Fan, Changjie and Kuang, Kun and Liu, Yong},
title = {Adaptive Value Decomposition with Greedy Marginal Contribution Computation for Cooperative Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Real-world cooperation often requires intensive coordination among agents simultaneously. This task has been extensively studied within the framework of cooperative multi-agent reinforcement learning (MARL), and value decomposition methods are among those cutting-edge solutions. However, traditional methods that learn the value function as a monotonic mixing of per-agent utilities cannot solve the tasks with non-monotonic returns. This hinders their application in generic scenarios. Recent methods tackle this problem from the perspective of implicit credit assignment by learning value functions with complete expressiveness or using additional structures to improve cooperation. However, they are either difficult to learn due to large joint action spaces or insufficient to capture the complicated interactions among agents which are essential to solving tasks with non-monotonic returns. Moreover, applications in real-world scenarios usually require policies to be interpretable, but interpretability is limited in the implicit credit assignment methods. To address these problems, we propose a novel explicit credit assignment method to address the non-monotonic problem. Our method, Adaptive Value decomposition with Greedy Marginal contribution (AVGM), is based on an adaptive value decomposition that learns the cooperative value of a group of dynamically changing agents. We first illustrate that the proposed value decomposition can consider the complicated interactions among agents and is feasible to learn in large-scale scenarios. Then, our method uses a greedy marginal contribution computed from the value decomposition as an individual credit to incentivize agents to learn the optimal cooperative policy. We further extend the module with an action encoder to guarantee the linear time complexity for computing the greedy marginal contribution. Experimental results demonstrate that our method achieves significant performance improvements in several non-monotonic domains. Besides, we showcase that our model maintains a good sense of interpretability and rationality. This suggests our model can be applied to scenarios with more realistic demands.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {31–39},
numpages = {9},
keywords = {non-monotonic, multi-agent cooperation, credit assignment},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3373724.3373725,
author = {Hoblitzell, Andrew and Babbar-Sebens, Meghna and Mukhopadhyay, Snehasis},
title = {Non-Stationary Reinforcement-Learning Based Dimensionality Reduction for Multi-Objective Optimization of Wetland Design},
year = {2020},
isbn = {9781450372350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373724.3373725},
doi = {10.1145/3373724.3373725},
abstract = {This paper outlines a method of non-stationary reinforcement-based learning for feature selection. The method was developed for the Watershed REstoration using Spatio-Temporal Optimization of REsources (WRESTORE) system, which is a decision support system used for wetland design on the Eagle Creek Watershed, northwest of Indianapolis, Indiana. Our results show measurable impact for maximizing reward efficiently for the feature selection task. This work describes the existing WRESTORE system, provides an overview of related work in reinforcement learning and dimensionality reduction, and shows the impact of our work in the multi-objective optimization process of WRESTORE. The contribution of this work is the application of an RL-based feature selection technique in interactive optimization of watershed design.},
booktitle = {Proceedings of the 5th International Conference on Robotics and Artificial Intelligence},
pages = {82–86},
numpages = {5},
keywords = {online learning, interactive optimization, artificial neural network, user model, decision support system, feature selection, Reinforcement based learning},
location = {Singapore, Singapore},
series = {ICRAI '19}
}

@inproceedings{10.5555/3586210.3586236,
author = {Dadgostari, Faraz and Swarup, Samarth and Adams, Stephen and Beling, Peter and Mortveit, Henning S.},
title = {Identifying Correlates of Emergent Behaviors in Agent-Based Simulation Models Using Inverse Reinforcement Learning},
year = {2023},
publisher = {IEEE Press},
abstract = {In large agent-based models, it is difficult to identify the correlate system-level dynamics with individual-level attributes. In this paper, we use inverse reinforcement learning to estimate compact representations of behaviors in large-scale pandemic simulations in the form of reward functions. We illustrate the capacity and performance of these representations identifying agent-level attributes that correlate with the emerging dynamics of large-scale multi-agent systems. Our experiments use BESSIE, an ABM for COVID-like epidemic processes, where agents make sequential decisions (e.g., use PPE/refrain from activities) based on observations (e.g., number of mask wearing people) collected when visiting locations to conduct their activities. The IRL-based reformulations of simulation outputs perform significantly better in classification of agent-level attributes than direct classification of decision trajectories and are thus more capable of determining agent-level attributes with definitive role in the collective behavior of the system. We anticipate that this IRL-based approach is broadly applicable to general ABMs.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {322–333},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3366423.3380149,
author = {Sun, Yiwei and Wang, Suhang and Tang, Xianfeng and Hsieh, Tsung-Yu and Honavar, Vasant},
title = {Adversarial Attacks on Graph Neural Networks via Node Injections: A Hierarchical Reinforcement Learning Approach},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380149},
doi = {10.1145/3366423.3380149},
abstract = {Graph Neural Networks (GNN) offer the powerful approach to node classification in complex networks across many domains including social media, E-commerce, and FinTech. However, recent studies show that GNNs are vulnerable to attacks aimed at adversely impacting their node classification performance. Existing studies of adversarial attacks on GNN focus primarily on manipulating the connectivity between existing nodes, a task that requires greater effort on the part of the attacker in real-world applications. In contrast, it is much more expedient on the part of the attacker to inject adversarial nodes, e.g., fake profiles with forged links, into existing graphs so as to reduce the performance of the GNN in classifying existing nodes. Hence, we consider a novel form of node injection poisoning attacks on graph data. We model the key steps of a node injection attack, e.g., establishing links between the injected adversarial nodes and other nodes, choosing the label of an injected node, etc. by a Markov Decision Process. We propose a novel reinforcement learning method for Node Injection Poisoning Attacks (NIPA), to sequentially modify the labels and links of the injected nodes, without changing the connectivity between existing nodes. Specifically, we introduce a hierarchical Q-learning network to manipulate the labels of the adversarial nodes and their links with other nodes in the graph, and design an appropriate reward function to guide the reinforcement learning agent to reduce the node classification performance of GNN. The results of the experiments show that NIPA is consistently more effective than the baseline node injection attack methods for poisoning graph data on three benchmark datasets.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {673–683},
numpages = {11},
keywords = {Reinforcement learning;, Adversarial Attack, Graph Poisoning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3276774.3276775,
author = {Zhang, Zhiang and Lam, Khee Poh},
title = {Practical Implementation and Evaluation of Deep Reinforcement Learning Control for a Radiant Heating System},
year = {2018},
isbn = {9781450359511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3276774.3276775},
doi = {10.1145/3276774.3276775},
abstract = {Deep reinforcement learning (DRL) has become a popular optimal control method in recent years. This is mainly because DRL has the potential to solve the optimal control problems with complex process dynamics, such as the optimal control for heating, ventilation, and air-conditioning (HVAC) systems. However, DRL control for HVAC systems has not been well studied. There is limited research on the real-life implementation and evaluation of this method. This study implements and deploys a DRL control method for a radiant heating system in a real-life office building for energy efficiency. A physics-based model for the heating system is first created and then calibrated using the measured building operation data. After that, the model is used as a simulator to train the DRL agent. The trained agent is then deployed in the actual heating system, and a smartphone App is used to let the occupants submit their thermal preferences to the DRL agent. It is found the DRL control method can save 16.6\% to 18.2\% heating demand compared to the old rule-based control logic over the three-month deployment period. However, several limitations of this study are found, such as the low participation rate of the App-based thermal preference feedback system, inefficient DRL training, and the requirement for a large amount of building data.},
booktitle = {Proceedings of the 5th Conference on Systems for Built Environments},
pages = {148–157},
numpages = {10},
keywords = {energy efficiency, HVAC control, deep reinforcement learning},
location = {Shenzen, China},
series = {BuildSys '18}
}

@inproceedings{10.5555/3463952.3464087,
author = {Sengupta, Ayan and Mohammad, Yasser and Nakadai, Shinji},
title = {An Autonomous Negotiating Agent Framework with Reinforcement Learning Based Strategies and Adaptive Strategy Switching Mechanism},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Despite abundant negotiation strategies in literature, the complexity of automated negotiation forbids a single strategy from being dominant against all others in different negotiation scenarios. To overcome this, one approach is to use mixture of experts, but at the same time, one problem of this method is the selection of experts, as this approach is limited by the competency of the experts selected. Another problem with most negotiation strategies is their incapability of adapting to dynamic variation of the opponent's behaviour within a single negotiation session resulting in poor performance. This work focuses on both, solving the problem of expert selection and adapting to the opponent's behaviour with our Autonomous Negotiating Agent Framework. This framework allows real-time classification of opponent's behaviour and provides a mechanism to select, switch or combine strategies within a single negotiation session. Additionally, our framework has a reviewer component which enables self-enhancement capability by deciding to include new strategies or replace old ones with better strategies periodically. We demonstrate an instance of our framework by implementing maximum entropy reinforcement learning based strategies with a deep learning based opponent classifier. Finally, we evaluate the performance of our agent against state-of-the-art negotiators under varied negotiation scenarios.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1163–1172},
numpages = {10},
keywords = {automated negotiation, negotiation strategy, reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3427773.3427867,
author = {Naug, Avisek and Qui\~{n}ones-Grueiro, Marcos and Biswas, Gautam},
title = {Continual Adaptation in Deep Reinforcement Learning-Based Control Applied to Non-Stationary Building Environments},
year = {2020},
isbn = {9781450381932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427773.3427867},
doi = {10.1145/3427773.3427867},
abstract = {This paper develops a continual deep reinforcement relearning (RL) controller for large buildings that exhibit non-stationary behaviors. The non-stationarity in building operations caused by unexpected changes in weather patterns, occupancy, and faults, makes it imperative to develop control algorithms that adapt to the changing conditions. Given the slow time constants in building operations, we assume that the non-stationarity can be modeled as discrete transitions between stationary models of system behavior. We address the challenge of detecting transitions between stationary processes using trend analysis, and relearn control policies that accommodate the tradeoff between energy savings and comfort after such transitions occur. We demonstrate our approach on a "smart building test-bed" for developing data-driven HVAC controllers that are deployed in large buildings on our university campus.},
booktitle = {Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings \&amp; Cities},
pages = {24–28},
numpages = {5},
keywords = {trend analysis, continuous adaptation, non-stationarity, smart buildings, reinforcement learning},
location = {Virtual Event, Japan},
series = {RLEM'20}
}

@inproceedings{10.1145/3529446.3529463,
author = {Wu, Yifei and Lei, Yonglin and Zhu, Zhi and Wang, Yan},
title = {Decision Modeling and Simulation of Fighter Air-to-Ground Combat Based on Reinforcement Learning},
year = {2022},
isbn = {9781450395823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529446.3529463},
doi = {10.1145/3529446.3529463},
abstract = {With the Artificial Intelligence (AI) widely used in air combat simulation system, the decision-making system of fighter has reached a high level of complexity. Traditionally, the pure theoretical analysis and the rule-based system are not enough to represent the cognitive behavior of pilots. In order to properly specify the autonomous decision-making of fighter, hence, we proposed a unified framework which combines the combat simulation and machine learning in this paper. This framework adopts deep reinforcement learning modelling by using the supervised learning and the Deep Q-Network (DQN) methods. As a proof of concept, we built an autonomous decision-making training scenario based on the Weapon Effectiveness Simulation System (WESS). The simulation results show that the intelligent decision-making model based on the proposed framework has better combat effects than the traditional decision-making model based on knowledge engineering.},
booktitle = {Proceedings of the 4th International Conference on Image Processing and Machine Vision},
pages = {102–109},
numpages = {8},
keywords = {Combat simulation, Deep reinforcement learning, Fighter air-to-ground combat, Intelligent decision-making},
location = {Hong Kong, China},
series = {IPMV '22}
}

@inproceedings{10.1145/3508072.3508198,
author = {Tsado, Yakubu and Jogunola, Olamide and Nawaz, Raheel and Gui, Guan and Adebisi, Bamidele},
title = {Quantifying Self-Consumption and Flexibility Provision through Battery Storage, a Deep Reinforcement Learning Approach},
year = {2022},
isbn = {9781450387347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508072.3508198},
doi = {10.1145/3508072.3508198},
abstract = {The rapid uptake of photovoltaics (PV) and energy storage system (ESS) is becoming an important contributor to the smart energy system in many countries, institutions, and homes. However, there is a limited real-data evaluation and simulation of the net benefit of such installations. Utilising Manchester Metropolitan University (MMU) Birley Field campus (Living Lab) energy hub data, we quantify the self-sufficiency rate (SSR) of the campus. Then, we propose deep Q-network (DQN) with prioritised experience replay (PER) to formulate the problem of providing flexibility via an ESS as a sequential decision making problem. This aims to optimise the daily benefits of by scheduling the charging/discharging activities to fit the dynamic campus environment with stochastic consumption pattern. While the results show an SSR for the Campus, the DQN-PER-based flexibility model improves the cost benefits by 25\% over the policy the campus currently utilises. Finally, the study provides a realistic cost evaluation and estimation of future investments and prospects for the campus. The findings can also impact other institutions and their energy policy both locally and globally.},
booktitle = {The 5th International Conference on Future Networks \&amp; Distributed Systems},
pages = {633–640},
numpages = {8},
keywords = {energy storage system, flexibility provisions, prioritised experience replay, Self-sufficiency, deep reinforcement learning, deep-Q network, smart energy infrastructure},
location = {Dubai, United Arab Emirates},
series = {ICFNDS 2021}
}

@article{10.1109/TASLP.2022.3155284,
author = {Wu, Qinzhuo and Zhang, Qi and Huang, Xuanjing},
title = {Automatic Math Word Problem Generation With Topic-Expression Co-Attention Mechanism and Reinforcement Learning},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3155284},
doi = {10.1109/TASLP.2022.3155284},
abstract = {Taking several topic words and a math expression as input, the aim of math word problem generation is to generate a problem that can be answered by the given expression and related to these topic words. Considerable progress has been achieved by sequence-to-sequence neural network models in many natural language generation tasks, but these models do not effectively consider the characteristics of the math word problem generation task. They may generate problems that are unrelated to the topic words and expressions, and problems that cannot be solved. In this paper, we propose a new model, MWPGen, for automatically generating math word problems. MWPGen has a topic-expression co-attention mechanism to extract relevant information between topic words and expressions. Further, we fine-tune MWPGen with the solving result of the generated problem as the reward for reinforcement learning. MWPGen shows improved performance in popular automatic evaluation metrics and improves the solvability of generated problems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1061–1072},
numpages = {12}
}

@inproceedings{10.1145/3565472.3592966,
author = {Massimo, David and Ricci, Francesco},
title = {Combining Reinforcement Learning and Spatial Proximity Exploration for New User and New POI Recommendations},
year = {2023},
isbn = {9781450399326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565472.3592966},
doi = {10.1145/3565472.3592966},
abstract = {Tourism Recommender Systems (TRSs) are unable to properly suggest new points of interest (POIs) to new users, i.e., to solve the combined new user and new item problem. To address this limitation we introduce a Reinforcement Learning TRS, which is called QEXP, and relies on a POI visits behaviour model mined from logs of POI visits data. This data is combined with general knowledge about the spatial range of tourists’ movements in a destination to generate recommendations. QEXP can recommend new POIs possessing the features of POIs experienced by tourists in the past, while favouring the exploration of POIs in the proximity of the target tourist position. We compare QEXP with four state-of-the-art POI RSs and we show that it can successfully tame the new user and new item problems. QEXP can also mitigate the concentration and popularity biases of the compared RSs and can recommend diverse and geographically dispersed POIs.},
booktitle = {Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
pages = {164–174},
numpages = {11},
keywords = {User Preference Learning, Reinforcement Learning, Tourism Recommender Systems},
location = {Limassol, Cyprus},
series = {UMAP '23}
}

@inproceedings{10.1145/3543507.3583313,
author = {Zhou, Jiahong and Mao, Shunhui and Yang, Guoliang and Tang, Bo and Xie, Qianlong and Lin, Lebin and Wang, Xingxing and Wang, Dong},
title = {RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583313},
doi = {10.1145/3543507.3583313},
abstract = {Recommender systems aim to recommend the most suitable items to users from a large number of candidates. Their computation cost grows as the number of user requests and the complexity of services (or models) increases. Under the limitation of computation resources (CRs), how to make a trade-off between computation cost and business revenue becomes an essential question. The existing studies focus on dynamically allocating CRs in queue truncation scenarios (i.e., allocating the size of candidates), and formulate the CR allocation problem as an optimization problem with constraints. Some of them focus on single-phase CR allocation, and others focus on multi-phase CR allocation but introduce some assumptions about queue truncation scenarios. However, these assumptions do not hold in other scenarios, such as retrieval channel selection and prediction model selection. Moreover, existing studies ignore the state transition process of requests between different phases, limiting the effectiveness of their approaches. This paper proposes a Reinforcement Learning (RL) based Multi-Phase Computation Allocation approach (RL-MPCA), which aims to maximize the total business revenue under the limitation of CRs. RL-MPCA formulates the CR allocation problem as a Weakly Coupled MDP problem and solves it with an RL-based approach. Specifically, RL-MPCA designs a novel deep Q-network to adapt to various CR allocation scenarios, and calibrates the Q-value by introducing multiple adaptive Lagrange multipliers (adaptive-λ) to avoid violating the global CR constraints. Finally, experiments on the offline simulation environment and online real-world recommender system validate the effectiveness of our approach.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {3214–3224},
numpages = {11},
keywords = {Computation Resource Allocation, Deep Reinforcement Learning, Recommender System, Weakly Coupled MDP},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3580305.3599359,
author = {Hao, Qianyue and Huang, Wenzhen and Feng, Tao and Yuan, Jian and Li, Yong},
title = {GAT-MF: Graph Attention Mean Field for Very Large Scale Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599359},
doi = {10.1145/3580305.3599359},
abstract = {Recent advancements in reinforcement learning have witnessed remarkable achievements by intelligent agents ranging from game-playing to industrial applications. Of particular interest is the area of multi-agent reinforcement learning (MARL), which holds significant potential for real-world scenarios. However, typical MARL methods are limited in their ability to handle tens of agents, leaving scenarios with up to hundreds or even thousands of agents almost unexplored. The scaling up of the number of agents presents two primary challenges: (1) agent-agent interactions are crucial in multi-agent systems while the number of interactions grows quadratically with the number of agents, resulting in substantial computational complexity and difficulty in strategies-learning; (2) the strengths of interactions among agents exhibit variations both across agents and over time, making it difficult to precisely model such interactions. In this paper, we propose a novel approach named Graph Attention Mean Field (GAT-MF). By converting agent-agent interactions into interactions between each agent and a weighted mean field, we achieve a substantial reduction in computational complexity. The proposed method offers a precise modeling of interaction dynamics with mathematical proofs of its correctness. Additionally, we design a graph attention mechanism to automatically capture the diverse and time-varying strengths of interactions, ensuring an accurate representation of agent interactions. Through extensive experimentation conducted in both manual and real-world scenarios involving over 3000 agents, we validate the efficacy of our method. The results demonstrate that our method outperforms the best baseline method with a remarkable improvement of 42.7\%. Furthermore, our method saves 86.4\% training time and 19.2\% GPU memory compared to the best baseline method. For reproducibility, our source codes and data are available at https://github.com/tsinghua-fib-lab/Large-Scale-MARL-GATMF.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {685–697},
numpages = {13},
keywords = {graph attention, large-scale decision problem, multi-agent reinforcement learning, mean field},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3493700.3493723,
author = {Gupta, Abhinav and Ghosh, Supratim and Dhara, Anulekha},
title = {Deep Reinforcement Learning Algorithm for Fast Solutions to Vehicle Routing Problem with Time-Windows},
year = {2022},
isbn = {9781450385824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493700.3493723},
doi = {10.1145/3493700.3493723},
abstract = {Vehicle routing problem (VRP) is a well known NP-hard combinatorial optimization problem having several variants. In this paper, we consider VRP along with additional constraints of capacity and time-windows (CVRPTW) and aim to provide a fast and approximately optimal solutions to large-scale CVRPTW problems. We present a deep Q-network with encoder-decoder based reinforcement learning approach to solve CVRPTW. The encoder is based on the attention mechanism whereas decoder is fully connected neural network. Via numerical experiments on benchmark datasets, we show the efficacy and computational speed our approach compared to baseline heuristics, a meta-heuristic algorithm, and a multi-agent reinforcement learning (RL) based framework.},
booktitle = {5th Joint International Conference on Data Science \&amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)},
pages = {236–240},
numpages = {5},
keywords = {reinforcement learning, time-windows, capacitated vehicle routing},
location = {Bangalore, India},
series = {CODS-COMAD 2022}
}

@inproceedings{10.1145/3505688.3505698,
author = {Wu, Qiuye and Zhao, Bo and Liu, Derong},
title = {Integral Reinforcement Learning-Based Optimal Control for Nonzero-Sum Games of Multi-Player Input-Constrained Nonlinear Systems},
year = {2022},
isbn = {9781450385855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505688.3505698},
doi = {10.1145/3505688.3505698},
abstract = {This paper investigates an integral reinforcement learning (IRL)-based optimal control scheme to solve nonzero-sum games of multi-player input-constrained nonlinear systems with unknown drift dynamics. The IRL method is introduced to obviate the identification procedure of the unknown drift dynamics. In order to achieve Nash equilibrium for each player, the simplified optimal control policy for each player is obtained by solving the coupled Hamilton-Jacobi equation with the critic neural network only. Thus, the computational resource is saved and the control structure is easy to realize. The closed-loop system is guaranteed to be uniformly ultimately bounded based on the Lyapunov stability analysis. Simulation results illustrate the effectiveness of the developed control scheme.},
booktitle = {Proceedings of the 7th International Conference on Robotics and Artificial Intelligence},
pages = {59–63},
numpages = {5},
keywords = {adaptive dynamic programming, neural networks, integral reinforcement learning, input constraints, nonzero-sum games},
location = {Guangzhou, China},
series = {ICRAI '21}
}

@inproceedings{10.1145/3533271.3561743,
author = {Angiuli, Andrea and Detering, Nils and Fouque, Jean-Pierre and Lauri\`{e}re, Mathieu and Lin, Jimin},
title = {Reinforcement Learning for Intra-and-Inter-Bank Borrowing and Lending Mean Field Control Game},
year = {2022},
isbn = {9781450393768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533271.3561743},
doi = {10.1145/3533271.3561743},
abstract = {We propose a mean field control game (MFCG) model for the intra-and-inter-bank borrowing and lending problem. This framework allows to study the competitive game arising between groups of collaborative banks. The solution is provided in terms of an asymptotic Nash equilibrium between the groups in the infinite horizon. A three-timescale reinforcement learning algorithm is applied to learn the optimal borrowing and lending strategy in a data driven way when the model is unknown. An empirical numerical analysis shows the importance of the three-timescale, the impact of the exploration strategy when the model is unknown, and the convergence of the algorithm.},
booktitle = {Proceedings of the Third ACM International Conference on AI in Finance},
pages = {369–376},
numpages = {8},
keywords = {systemic risk, mean field control game, reinforcement learning},
location = {New York, NY, USA},
series = {ICAIF '22}
}

@inproceedings{10.1145/3580305.3599241,
author = {Xing, Mingzhe and Mao, Hangyu and Yin, Shenglin and Pan, Lichen and Zhang, Zhengchao and Xiao, Zhen and Long, Jieyi},
title = {A Dual-Agent Scheduler for Distributed Deep Learning Jobs on Public Cloud via Reinforcement Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599241},
doi = {10.1145/3580305.3599241},
abstract = {Public cloud GPU clusters are becoming emerging platforms for training distributed deep learning jobs. Under this training paradigm, the job scheduler is a crucial component to improve user experiences, i.e., reducing training fees and job completion time, which can also save power costs for service providers. However, the scheduling problem is known to be NP-hard. Most existing work divides it into two easier sub-tasks, i.e., ordering task and placement task, which are responsible for deciding the scheduling orders of jobs and placement orders of GPU machines, respectively. Due to the superior adaptation ability, learning-based policies can generally perform better than traditional heuristic-based methods. Nevertheless, there are still two main challenges that have not been well-solved. First, most learning-based methods only focus on ordering or placement policy independently, while ignoring their cooperation. Second, the unbalanced machine performances and resource contention impose huge overhead and uncertainty on job duration, but rarely be considered in existing work. To tackle these issues, this paper presents a dual-agent scheduler framework abstracted from the two sub-tasks to jointly learn the ordering and placement policies and make better-informed scheduling decisions. Specifically, we design an ordering agent with a scalable squeeze-and-communicate strategy for better cooperation; for the placement agent, we propose a novel Random Walk Gaussian Process to learn the performance similarities of GPU machines while being aware of the uncertain performance fluctuation. Finally, the dual-agent is jointly optimized with multi-agent reinforcement learning. Extensive experiments conducted on the real-world production cluster trace demonstrate the superiority of our model.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2776–2788},
numpages = {13},
keywords = {gaussian process, job scheduling, reinforcement learning},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/2999541.2999548,
author = {Toivonen, Tapani and Jormanainen, Ilkka},
title = {Using JS-Eden to Introduce the Concepts of Reinforcement Learning and Artificial Neural Networks},
year = {2016},
isbn = {9781450347709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2999541.2999548},
doi = {10.1145/2999541.2999548},
abstract = {Machine learning is a subfield of computer science and it is widely taught as a part of computing degrees. Machine learning is an umbrella concept for a wide range of different algorithms and approaches, and it is often considered a difficult subject to teach. Machine learning systems work usually in a black box where only inputs and outputs of algorithms are visible to the user. This might cause difficulties for a learner to interpret and comprehend how the algorithm works. In this paper, we introduce a novel approach to demonstrate and teach machine learning concepts by turning the black box into a white box with JS-Eden, which is an Empirical Modelling platform for creating open-ended and interactive open educational resources (construals) with Eden modelling language. A specifically developed JS-Eden plugin for physical computing follows white-box approach and it enables a user to import an implementation of a machine learning algorithm to the environment and interact with the algorithm in an unusual way.},
booktitle = {Proceedings of the 16th Koli Calling International Conference on Computing Education Research},
pages = {165–169},
numpages = {5},
keywords = {JS-eden, reinforcement learning, machine learning education, artificial neural network, Q-learning},
location = {Koli, Finland},
series = {Koli Calling '16}
}

@inproceedings{10.1145/3590003.3590019,
author = {Tan, Caidong},
title = {Deep Reinforcement Learning with Copy-Oriented Context Awareness and Weighted Rewards for Abstractive Summarization},
year = {2023},
isbn = {9781450399449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590003.3590019},
doi = {10.1145/3590003.3590019},
abstract = {This paper presents a deep context-aware model with a copy mechanism based on reinforcement learning for abstractive text summarization. Our model is optimized using weighted ROUGEs as global prediction-based rewards and the self-critical policy gradient training algorithm, which can reduce the inconsistency between training and testing by directly optimizing the evaluation metrics. To alleviate the lexical diversity and component diversity problems caused by global prediction rewards, we improve the richness of the multi-head self-attention mechanism to capture context through global deep context representation with copy mechanism. We conduct experiments and demonstrate that our model outperforms many existing benchmarks over the Gigaword, LCSTS, and CNN/DM datasets. The experimental results demonstrate that our model has a significant effect on improving the quality of summarization.},
booktitle = {Proceedings of the 2023 2nd Asia Conference on Algorithms, Computing and Machine Learning},
pages = {84–89},
numpages = {6},
keywords = {abstractive summarization, copy mechanism, deep context-aware, reinforcement learning},
location = {Shanghai, China},
series = {CACML '23}
}

@inproceedings{10.1145/3240508.3240622,
author = {Ouyang, Deqiang and Shao, Jie and Zhang, Yonghui and Yang, Yang and Shen, Heng Tao},
title = {Video-Based Person Re-Identification via Self-Paced Learning and Deep Reinforcement Learning Framework},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240622},
doi = {10.1145/3240508.3240622},
abstract = {Person re-identification is an important task in video surveillance, focusing on finding the same person across different cameras. However, most existing methods of video-based person re-identification still have some limitations (e.g., the lack of effective deep learning framework, the robustness of the model, and the same treatment for all video frames) which make them unable to achieve better recognition performance. In this paper, we propose a novel self-paced learning algorithm for video-based person re-identification, which could gradually learn from simple to complex samples for a mature and stable model. Self-paced learning is employed to enhance video-based person re-identification based on deep neural network, so that deep neural network and self-paced learning are unified into one frame. Then, based on the trained self-paced learning, we propose to employ deep reinforcement learning to discard misleading and confounding frames and find the most representative frames from video pairs. With the advantage of deep reinforcement learning, our method can learn strategies to select the optimal frame groups. Experiments show that the proposed framework outperforms the existing methods on the iLIDS-VID, PRID-2011 and MARS datasets.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1562–1570},
numpages = {9},
keywords = {video-based person re-identification, deep reinforcement learning, self-paced learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3500931.3500953,
author = {Dong, Andi and Wang, Chao and Tong, Pan and Yang, Dan and Yong, Cuo},
title = {Research on Tibetan Medicine Intelligent Question Answering System Integrating Confrontation Training and Reinforcement Learning},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3500953},
doi = {10.1145/3500931.3500953},
abstract = {In this study, a knowledge graph (KG) based Tibetan medicine intelligent question answering (QA) system model was proposed based on an adversarial learning generative network model, in an attempt to alleviate the scarcity of medical resources, promote the heritage and innovation of Tibetan medicine, and ease the shortage of Tibetan medical information. In this model, the simulated answers were generated via adversarial learning, and subsequently the reinforcement learning was applied for feedback-based optimization, with the ultimate aim of enhancing the accuracy rate of this model. Besides, a triple extraction method based on Tibetan features was proposed to construct a KG dialog set. Finally, this model was subjected to an experiment in Chinese and Tibetan datasets, with the results indicating that the accuracy of this intelligent QA model incorporating adversarial networks and reinforcement learning was higher than other models.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {120–125},
numpages = {6},
keywords = {Reinforcement learning, Tibetan medicine, Knowledge extraction, Knowledge graph, Generate countermeasure network},
location = {Beijing, China},
series = {ISAIMS '21}
}

@inproceedings{10.1145/3486611.3491126,
author = {Nagy, Zoltan and Park, June Young and Drgona, Jan and Quintana, Matias},
title = {The Second ACM SIGEnergy Workshop on Reinforcement Learning for Energy Management in Buildings \&amp; Cities (RLEM)},
year = {2021},
isbn = {9781450391146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486611.3491126},
doi = {10.1145/3486611.3491126},
abstract = {This is the second edition of the ACM SIGEnergy Workshop for Reinforcement Learning for Energy Management in Buildings and Cities (RLEM). RLEM brings together researchers and industry practitioners for the advancement of (deep) reinforcement learning (RL) in the built environment as it is applied for managing energy in civil infrastructure systems (energy, water, transportation).},
booktitle = {Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {307–308},
numpages = {2},
keywords = {reinforcement learning, energy},
location = {Coimbra, Portugal},
series = {BuildSys '21}
}

@inproceedings{10.1145/3533271.3561731,
author = {Murray, Phillip and Wood, Ben and Buehler, Hans and Wiese, Magnus and Pakkanen, Mikko},
title = {Deep Hedging: Continuous Reinforcement Learning for Hedging of General Portfolios across Multiple Risk Aversions},
year = {2022},
isbn = {9781450393768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533271.3561731},
doi = {10.1145/3533271.3561731},
abstract = {We present a method for finding optimal hedging policies for arbitrary initial portfolios and market states. We develop a novel actor-critic algorithm for solving general risk-averse stochastic control problems and use it to learn hedging strategies across multiple risk aversion levels simultaneously. We demonstrate the effectiveness of the approach with a numerical example in a stochastic volatility environment.},
booktitle = {Proceedings of the Third ACM International Conference on AI in Finance},
pages = {361–368},
numpages = {8},
keywords = {Transaction Costs, Risk Averse, Reinforcement Learning, Deep Hedging},
location = {New York, NY, USA},
series = {ICAIF '22}
}

@article{10.14778/3611479.3611489,
author = {Wang, Junxiong and Trummer, Immanuel and Kara, Ahmet and Olteanu, Dan},
title = {ADOPT: Adaptively Optimizing Attribute Orders for Worst-Case Optimal Join Algorithms via Reinforcement Learning},
year = {2023},
issue_date = {July 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611479.3611489},
doi = {10.14778/3611479.3611489},
abstract = {The performance of worst-case optimal join algorithms depends on the order in which the join attributes are processed. Selecting good orders before query execution is hard, due to the large space of possible orders and unreliable execution cost estimates in case of data skew or data correlation. We propose ADOPT, a query engine that combines adaptive query processing with a worst-case optimal join algorithm, which uses an order on the join attributes instead of a join order on relations. ADOPT divides query execution into episodes in which different attribute orders are tried. Based on run time feedback on attribute order performance, ADOPT converges quickly to near-optimal orders. It avoids redundant work across different orders via a novel data structure, keeping track of parts of the join input that have been successfully processed. It selects attribute orders to try via reinforcement learning, balancing the need for exploring new orders with the desire to exploit promising orders. In experiments with various data sets and queries, it outperforms baselines, including commercial and open-source systems using worst-case optimal join algorithms, whenever queries become complex and therefore difficult to optimize.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2805–2817},
numpages = {13}
}

@inproceedings{10.5555/3306127.3331808,
author = {Lee, Hyun-Rok and Lee, Taesik},
title = {Improved Cooperative Multi-Agent Reinforcement Learning Algorithm Augmented by Mixing Demonstrations from Centralized Policy},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Many decision problems for complex systems that involve multiple decision makers can be formulated as a decentralized partially observable markov decision process (dec-POMDP) problem. Due to the computational difficulty with obtaining optimal policies, recent approaches to dec-POMDP often use a multi-agent reinforcement learning (MARL) algorithm. We propose a method to improve the existing cooperative MARL algorithms by adopting an imitation learning technique. For a reference policy in the imitation learning part, we use a centralized policy from a multi-agent MDP or a multi-agent POMDP model reduced from the original dec-POMDP model. In the proposed method, during the training process, we mix demonstrations from the reference policy by using a demonstration buffer. Demonstration samples from the buffer are used in the augmented policy gradient function for policy updates. We assess the performance of the proposed method for three well-known dec-POMDP benchmark problems -- Mars rover, co-operative box pushing, and dec-tiger. Experimental results indicate that augmenting the baseline MARL algorithm by mixing the demonstrations significantly improves the quality of policy solutions. With these results, we conclude that the imitation learning can enhance MARL algorithms and that policy solutions from MMDP and MPOMDP models are a reasonable reference policy to use in the proposed algorithm.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1089–1098},
numpages = {10},
keywords = {dec-pomdp, multi-agent reinforcement learning, imitation learning, cooperative decision making problem},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3409334.3452072,
author = {Omatu, Ngozi and Phillips, Joshua L.},
title = {Benefits of Combining Dimensional Attention and Working Memory for Partially Observable Reinforcement Learning Problems},
year = {2021},
isbn = {9781450380683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409334.3452072},
doi = {10.1145/3409334.3452072},
abstract = {Neuroscience provides a rich source of inspiration for new types of algorithms and architectures to employ when building AI and the resulting biologically-plausible approaches that provide formal, testable models of brain function. The working memory toolkit (WMtk), was developed to assist the integration of an artificial neural network (ANN)-based computational neuroscience model of working memory into reinforcement learning (RL) agents, mitigating the details of ANN design and providing a simple symbolic encoding interface. While the WMtk allows RL agents to perform well in partially-observable domains, it requires prefiltering of sensory information by the programmer: a task often delegated to dimensional attention mechanisms in other cognitive architectures. To fill this gap, we develop and test a biologically-plausible dimensional attention filter for the WMtk and validate model performance using a partially-observable 1D maze task. We show that the attention filter improves learning behavior in two ways by: 1) speeding up learning in the short-term, early in training and 2) developing emergent alternative strategies which optimize performance over the long-term.},
booktitle = {Proceedings of the 2021 ACM Southeast Conference},
pages = {209–213},
numpages = {5},
keywords = {reinforcement learning, working memory, artificial neural networks, dimensional attention learning},
location = {Virtual Event, USA},
series = {ACM SE '21}
}

@inproceedings{10.1145/3493700.3493709,
author = {Shelke, Omkar and Meisheri, Hardik and Khadilkar, Harshad},
title = {Identifying Efficient Curricula for Reinforcement Learning in Complex Environments with a Fixed Computational Budget},
year = {2022},
isbn = {9781450385824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493700.3493709},
doi = {10.1145/3493700.3493709},
abstract = {Pommerman is a hybrid cooperative/adversarial multi-agent environment, with challenging characteristics in terms of partial observability, limited or no communication, sparse and delayed rewards, and restrictive inference time limits. This makes it a challenging environment for reinforcement learning (RL) approaches. In this paper, we focus on developing a curriculum for learning a robust and promising policy in a constrained computational budget of 100,000 games, starting from a fixed base policy (which itself imitates a noisy expert policy). All RL algorithms starting from the base policy use vanilla proximal-policy optimization (PPO) with the same reward function, and the only difference between their training is the mix and sequence of opponent policies. One expects that beginning training with simpler opponents and then gradually increasing the opponent difficulty will facilitate faster learning, leading to more robust policies compared against a baseline where all available opponent policies are introduced from the start. We test this hypothesis and show that within constrained computational budgets, it is in fact better to “learn in the school of hard knocks”, i.e., against all available opponent policies nearly from the start. We also include ablation studies where we study the effect of modifying the base environment properties of ammo and bomb blast strength on the agent performance.},
booktitle = {5th Joint International Conference on Data Science \&amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)},
pages = {81–89},
numpages = {9},
keywords = {Curriculum Design, Imitation Learning, Reinforcement Learning},
location = {Bangalore, India},
series = {CODS-COMAD 2022}
}

@inproceedings{10.5555/3586210.3586416,
author = {Huang, Lei and Zou, Zhengbo},
title = {Accelerating Training of Reinforcement Learning-Based Construction Robots in Simulation Using Demonstrations Collected in Virtual Reality},
year = {2023},
publisher = {IEEE Press},
abstract = {The application of construction robots is crucial to mitigate challenges faced by the construction industry, such as labor shortages and low productivity. Reinforcement learning (RL) enables robots to take actions based on observed states, improving flexibility over traditional robots pre-programmed to follow determined sequences of instructions. However, RL-based control is time-consuming to train, hindering the wide adoption of RL-based construction robots. This paper proposes an approach that utilizes expert demonstrations collected from virtual reality to accelerate the RL training of construction robots. For evaluation, we implement the approach for the task of window pickup and installation on a virtual construction site. In our experiment, out of 10 RL agents trained using virtual expert demonstrations, 7 agents converge to an optimal policy faster than the baseline RL agent trained without demonstrations by around 40 epochs, which proves adding expert demonstrations can effectively accelerate the training of robots learning construction tasks.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2451–2462},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3356470.3365529,
author = {Yang, Shaofeng and Ogawa, Yoshiki and Ikeuchi, Koji and Akiyama, Yuki and Shibasaki, Ryosuke},
title = {Firm-Level Behavior Control after Large-Scale Urban Flooding Using Multi-Agent Deep Reinforcement Learning},
year = {2019},
isbn = {9781450369565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356470.3365529},
doi = {10.1145/3356470.3365529},
abstract = {With natural disasters have become large scale, diversified, and frequent, the indirect economic damage due to interruption of supply chain tends to be large. Therefore, it is important to recover as quickly as possible for companies after disasters. In this paper, we use reinforcement learning to optimize a company's action strategy so that it can efficiently recover the inter-firm transaction network in the supply chain after large-scale urban flooding. The agent holds information on disaster and supply chains obtained from inter-firm transaction data and flood simulation analysis data, enabling us to create a simulation with detailed urban infrastructure information by using the high-dimensional data to construct detailed spatial states. The paper also proposes an action policy for companies based on multi-agent deep reinforcement learning to optimize the behavior of companies in the recovery process. This work bridges the divide between high-dimensional data set input and post-disaster behaviors, enabling an artificial agent to learn the best action to take after a disaster. Our results are as follows. Through learning, agents can recover efficiently after a disaster. Companies tend to secure alternative business partners first and then perform recovery work and business expansion.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on GeoSpatial Simulation},
pages = {24–27},
numpages = {4},
keywords = {disaster management, multi-agent, supply chain disruption, inter-firm transaction data, deep reinforcement learning},
location = {Chicago, Illinois},
series = {GeoSim '19}
}

@inproceedings{10.1145/3551661.3561363,
author = {Edvardsen, Vegard and Spreemann, Gard and Van den Abeele, Jeriek},
title = {FORLORN: A Framework for Comparing Offline Methods and Reinforcement Learning for Optimization of RAN Parameters},
year = {2022},
isbn = {9781450394819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551661.3561363},
doi = {10.1145/3551661.3561363},
abstract = {The growing complexity and capacity demands for mobile networks necessitate innovative techniques for optimizing resource usage. Meanwhile, recent breakthroughs have brought Reinforcement Learning (RL) into the domain of continuous control of real-world systems. As a step towards RL-based network control, this paper introduces a new framework for benchmarking the performance of an RL agent in network environments simulated with ns-3. Within this framework, we demonstrate that an RL agent without domain-specific knowledge can learn how to efficiently adjust Radio Access Network (RAN) parameters to match offline optimization in static scenarios, while also adapting on the fly in dynamic scenarios, in order to improve the overall user experience. Our proposed framework may serve as a foundation for further work in developing workflows for designing RL-based RAN control algorithms.},
booktitle = {Proceedings of the 18th ACM International Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {37–44},
numpages = {8},
keywords = {network optimization, reinforcement learning, network simulation},
location = {Montreal, Quebec, Canada},
series = {Q2SWinet '22}
}

@inproceedings{10.1145/3380446.3430623,
author = {Zhang, Jinwei and Sadiqbatcha, Sheriff and Gao, Yuanqi and O'Dea, Michael and Yu, Nanpeng and Tan, Sheldon X.-D.},
title = {HAT-DRL: Hotspot-Aware Task Mapping for Lifetime Improvement of Multicore System Using Deep Reinforcement Learning},
year = {2020},
isbn = {9781450375191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380446.3430623},
doi = {10.1145/3380446.3430623},
abstract = {In this work, we propose a novel learning-based task to core mapping technique to improve lifetime and reliability based on advanced deep reinforcement learning. The new method, called HAT-DRL, is based on the observation that on-chip temperature sensors may not capture the true hotspots of the chip, which can lead to sub-optimal control decisions. In the new method, we first perform data-driven learning to model the hotspot activation indicator with respect to the resource utilization of different workloads. On top of this, we propose to employ a recently proposed, highly robust, sample-efficient soft-actor-critic deep reinforcement learning algorithm, which can learn optimal maximum entropy policies to improve the long-term reliability and minimize the performance degradation from NBTI/HCI effects. Lifetime and reliability improvement is achieved by assigning a reward function, which penalizes continuously stressing the same hotspots and encourages even stressing of cores. The proposed algorithm is validated with an Intel i7-8650U four-core CPU platform executing CPU benchmark workloads for various hotspot activation profiles. Our experimental results show that HAT-DRL balances the stress between all cores and hotspots, and achieves 50\% and 160\% longer lifetime compared to non-hotspot-aware and Linux default scheduling respectively. The proposed method can also reduce the average temperature by exploiting the true-hotspot information.},
booktitle = {Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {77–82},
numpages = {6},
keywords = {reliability, lifetime, task mapping, reinforcement learning, multicore},
location = {Virtual Event, Iceland},
series = {MLCAD '20}
}

@inproceedings{10.1145/3469877.3497694,
author = {Wang, Qian and Liu, Hao and Hu, Xiaotong},
title = {Deep Reinforcement Learning and Docking Simulations for Autonomous Molecule Generation in de Novo Drug Design},
year = {2022},
isbn = {9781450386074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469877.3497694},
doi = {10.1145/3469877.3497694},
abstract = {In medicinal chemistry programs, it is key to design and make compounds that are efficacious and safe. In this study, we developed a new deep Reinforcement learning-based compounds molecular generation method. Because chemical space is impractically large, and many existing generation models generate molecules that lack effectiveness, novelty and unsatisfactory molecular properties. Our proposed method-DeepRLDS, which integrates transformer network, balanced binary tree search and docking simulation based on super large-scale supercomputing, can solve these problems well. Experiments show that more than 96 of the generated molecules are chemically valid, 99 of the generated molecules are chemically novelty, the generated molecules have satisfactory molecular properties and possess a broader chemical space distribution.},
booktitle = {ACM Multimedia Asia},
articleno = {79},
numpages = {6},
keywords = {docking simulations, Chemical space, Deep reinforcement learning, De novo drug design},
location = {Gold Coast, Australia},
series = {MMAsia '21}
}

@inproceedings{10.1145/3459955.3460601,
author = {Iiyama, Tomoshi and Kitakoshi, Daisuke and Suzuki, Masato},
title = {An Approach for Creation of Logistics Management System for Food Banks Based on Reinforcement Learning},
year = {2021},
isbn = {9781450389136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459955.3460601},
doi = {10.1145/3459955.3460601},
abstract = {Food loss has become a serious problem in recent years, especially in developed countries including Japan. Food welfare organizations called Food Banks which distribute still-edible food to needy people have been active in Japan. Here, we sought to develop a logistics management system for the food banks to improve the efficiency of their food delivery. We propose a method to optimize the food delivery schedule of food banks by means of a reinforcement learning algorithm. In our proposed algorithm, an agent aims to learn the policy for delivering food at the lowest cost. We performed computer simulations to evaluate the validity of the proposed method, and the results demonstrated that the agent could acquire the optimal policy in a small virtual environment in many cases.},
booktitle = {Proceedings of the 4th International Conference on Information Science and Systems},
pages = {60–67},
numpages = {8},
keywords = {Food Bank, Reinforcement learning, Food delivery schedule optimization},
location = {Edinburgh, United Kingdom},
series = {ICISS '21}
}

