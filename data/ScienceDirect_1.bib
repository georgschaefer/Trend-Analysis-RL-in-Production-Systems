@article{WASCHNECK20181264,
title = {Optimization of global production scheduling with deep reinforcement learning},
journal = {Procedia CIRP},
volume = {72},
pages = {1264-1269},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.212},
url = {https://www.sciencedirect.com/science/article/pii/S221282711830372X},
author = {Bernd Waschneck and André Reichstaller and Lenz Belzner and Thomas Altenmüller and Thomas Bauernhansl and Alexander Knapp and Andreas Kyek},
keywords = {Production Scheduling, Reinforcement Learning, Machine Learning in Manufacturing},
abstract = {Industrie 4.0 introduces decentralized, self-organizing and self-learning systems for production control. At the same time, new machine learning algorithms are getting increasingly powerful and solve real world problems. We apply Google DeepMind’s Deep Q Network (DQN) agent algorithm for Reinforcement Learning (RL) to production scheduling to achieve the Industrie 4.0 vision for production control. In an RL environment cooperative DQN agents, which utilize deep neural networks, are trained with user-defined objectives to optimize scheduling. We validate our system with a small factory simulation, which is modeling an abstracted frontend-of-line semiconductor production facility.}
}
@article{STEINBACHER202267,
title = {Modelling Framework for Reinforcement Learning based Scheduling Applications},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {67-72},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.369},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322016391},
author = {Lennart M. Steinbacher and Abderahim Ait-Alla and Daniel Rippel and Tim Düe and Michael Freitag},
keywords = {Reinforcement Learning, Scheduling, Multi-Agent Simulation, Automated Model Generation, Production, Framework},
abstract = {Over the last years, reinforcement learning has been extensively applied to schedule complex and dynamic systems. There are multitudes of simulation environments and algorithms, which hinder standardization and impede testing the suitability of reinforcement learning for specific scheduling applications and their easy implementation. This article proposes a framework to model production systems easily and transform them into standard industry simulation software to solve this issue. This framework contains major elements of classic production systems and references them adequately to allow effortless modelling. Furthermore, the domain models’ adjacent systems and their respective functionalities are described to facilitate reinforcement learning-based scheduling. This study demonstrates the framework's applicability using an existing dynamic scheduling problem. The experiences during modelling and training of the reinforcement learning subsequently are discussed.}
}
@article{HE2022939,
title = {Multi-objective optimization of the textile manufacturing process using deep-Q-network based multi-agent reinforcement learning},
journal = {Journal of Manufacturing Systems},
volume = {62},
pages = {939-949},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521000728},
author = {Zhenglei He and Kim Phuc Tran and Sebastien Thomassey and Xianyi Zeng and Jie Xu and Changhai Yi},
keywords = {Deep reinforcement learning, Deep Q-networks, Multi-objective, Optimization, Decision, Process, Textile Manufacturing},
abstract = {Multi-objective optimization, such as quality, productivity, and cost, of the textile manufacturing process is increasingly challenging because of the growing complexity involved in the development of textile industry in the upcoming big data era. It is hard for traditional methods to deal with high-dimension decision space in this issue, and prior experts’ knowledge is required as well as human intervention. This paper proposed a novel framework that transformed the textile process optimization problem into a stochastic game, and introduced deep Q-networks algorithm instead of current methods to approach it in a multi-agent system. The developed multi-agent reinforcement learning system applied a utilitarian selection mechanism to maximize the sum of all agents’ rewards (obeying the increasing ε-greedy policy) in each state, to avoid the interruption of multiple equilibria and achieve the correlated equilibrium optimal solutions of the textile process. The case study result reflects that the proposed MARL system can achieve the optimal solutions for the textile ozonation process, and it performs better than the traditional approaches.}
}
@article{AHN2023104399,
title = {Robotic assembly strategy via reinforcement learning based on force and visual information},
journal = {Robotics and Autonomous Systems},
volume = {164},
pages = {104399},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104399},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023000386},
author = {Kuk-Hyun Ahn and Minwoo Na and Jae-Bok Song},
keywords = {Robotic assembly, Learning-based method},
abstract = {Since assembly tasks are frequently performed in a wide range of industries, there have been many efforts to develop robotic assembly strategies. However, robotic assembly is only applicable in structured environments wherein a target object is placed in a fixed position, because the occurrence of a large error substantially degrades performance. Thus, there is still a need for a generalized assembly strategy that can cope with a large position/orientation error regardless of the shape. To this end, this study presents an assembly strategy based on both the force and visual information. Specifically, the trajectory of the robot is obtained by combining the output of two neural-network-based trajectory generators that receive the force and image information, respectively, and then a deep reinforcement learning algorithm is applied to obtain the optimal strategy. In this process, imitation learning is applied to train the force-based network using the demonstration data collected with the suggested hand-guiding method, and the probability distribution of the feature is introduced in the image-based network to enable a robot to quickly adapt to assembly parts with different shapes. The performance of the proposed assembly strategy is experimentally verified using various peg-in-hole tasks, and the results confirm that the robot can successfully accomplish an assembly task regardless of the shapes of the assembly parts, even when the initial position/orientation error is large.}
}
@article{RINCIOG20221112,
title = {Towards Standardising Reinforcement Learning Approaches for Production Scheduling Problems},
journal = {Procedia CIRP},
volume = {107},
pages = {1112-1119},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.117},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122004012},
author = {Alexandru Rinciog and Anne Meyer},
keywords = {Production Scheduling, Reinforcement Learning, Standardization, Validation},
abstract = {Recent years have seen a rise in interest in terms of using machine learning, particularly reinforcement learning (RL), for production scheduling problems of varying degrees of complexity. The general approach is to break down the scheduling problem into a Markov Decision Process (MDP), whereupon a simulation implementing the MDP is used to train an RL agent. Since existing studies rely on (sometimes) complex simulations for which the code is unavailable, the experiments presented are hard, or, in the case of stochastic environments, impossible to reproduce accurately. Furthermore, there is a vast array of RL designs to choose from. To make RL methods widely applicable in production scheduling and work out their strength for the industry, the standardisation of model descriptions - both production setup and RL design - and validation scheme are a prerequisite. Our contribution is threefold: First, we standardize the description of production setups used in RL studies based on established nomenclature. Secondly, we classify RL design choices from existing publications. Lastly, we propose recommendations for a validation scheme focusing on reproducibility and sufficient benchmarking.}
}
@article{DENG2022103748,
title = {Reinforcement learning for industrial process control: A case study in flatness control in steel industry},
journal = {Computers in Industry},
volume = {143},
pages = {103748},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103748},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522001452},
author = {Jifei Deng and Seppo Sierla and Jie Sun and Valeriy Vyatkin},
keywords = {Strip rolling, Process control, Reinforcement learning, Ensemble learning},
abstract = {Strip rolling is a typical manufacturing process, in which conventional control approaches are widely applied. Development of the control algorithms requires a mathematical expression of the process by means of the ﬁrst principles or empirical models. However, it is difficult to upgrade the conventional control approaches in response to the ever-changing requirements and environmental conditions because domain knowledge of control engineering, mechanical engineering, and material science is required. Reinforcement learning is a machine learning method that can make the agent learn from interacting with the environment, thus avoiding the need for the above mentioned mathematical expression. This paper proposes a novel approach that combines ensemble learning with reinforcement learning methods for strip rolling control. Based on the proximal policy optimization (PPO), a multi-actor PPO is proposed. Each randomly initialized actor interacts with the environment in parallel, but only the experience from the actor that obtains the highest reward is used for updating the actors. Simulation results show that the proposed method outperforms the conventional control methods and the state-of-the-art reinforcement learning methods in terms of process capability and smoothness.}
}
@article{TSAI2020501,
title = {Utilization of a reinforcement learning algorithm for the accurate alignment of a robotic arm in a complete soft fabric shoe tongues automation process},
journal = {Journal of Manufacturing Systems},
volume = {56},
pages = {501-513},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301114},
author = {Yu-Ting Tsai and Chien-Hui Lee and Tao-Ying Liu and Tien-Jan Chang and Chun-Sheng Wang and S.J. Pawar and Pei-Hsing Huang and Jin-H. Huang},
keywords = {Artificial intelligence, Shoemaking automation, Reinforcement learning, Cyber-physical system},
abstract = {As usher in Industry 4.0, there has been much interest in the development and research that combine artificial intelligence with automation. The control and operation of equipment in a traditional automated shoemaking production line require a preliminary subjective judgment of relevant manufacturing processes, to determine the exact procedure and corresponding control settings. However, with the manual control setting, it is difficult to achieve an accurate quality assessment of an automated process characterized by high uncertainty and intricacy. It is challenging to replace handicrafts and the versatility of manual product customization with automation techniques. Hence, the current study has developed an automatic production line with a cyber-physical system artificial intelligence (CPS-AI) architecture for the complete manufacturing of soft fabric shoe tongues. The Deep-Q reinforcement learning (RL) method is proposed as a means of achieving better control over the manufacturing process, while the convolutional and long short-term memory artificial neural network (CNN + LSTM) is developed to enhance action speed. This technology allows a robotic arm to learn the specific image feature points of a shoe tongue through repeated training to improve its manufacturing accuracy. For validation, different parameters of the network architecture are tested, and the test convergence accuracy was found to be as high as 95.9 %. During its actual implementation, the production line completed 509 finished products, of which 349 products were acceptable due to the anticipated measurement error. This showed that the production line system was capable of achieving optimum product accuracy and quality with respect to the performance of repeated computations, parameter updates, and action evaluations.}
}
@article{MARCHESANO202161,
title = {A Deep Reinforcement Learning approach for the throughput control of a Flow-Shop production system},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {61-66},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321007060},
author = {Maria Grazia Marchesano and Guido Guizzi and Liberatina Carmela Santillo and Silvestro Vespoli},
keywords = {Neural networks in process control, DQN, Reinforcement learning, Flow Shop, Industry 4.0},
abstract = {This paper proposes a new method for controlling a flow shop in terms of throughput and Work In Process (WIP). In order to achieve a throughput target, a Deep Q-Network (DQN) is used to define the constant WIP quantity in the system. The main contribution of this paper is the novel approach used to formulate the state, action space, and reward function. An extensive pre-experimental campaign is conducted to determine the best network structure and appropriate hyperparameter values. Finally, the system’s performance is compared to the known results of an analytical model from the literature (Practical Worst Case, PWC).}
}
@article{RAZIEI2021104296,
title = {Adaptable automation with modular deep reinforcement learning and policy transfer},
journal = {Engineering Applications of Artificial Intelligence},
volume = {103},
pages = {104296},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104296},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621001433},
author = {Zohreh Raziei and Mohsen Moghaddam},
keywords = {Deep reinforcement learning, Actor–critic architecture, Modularization, Multi-task learning, Meta learning, Meta-World},
abstract = {Future industrial automation systems are anticipated to be shaped by intelligent technologies that allow for the adaptability of machines to the variations and uncertainties in processes and work environments. This paper is motivated by the need for devising new intelligent methods that enable efficient and scalable training of collaborative robots on a variety of tasks that foster their adaptability to new tasks and environments. Recent advances in deep Reinforcement Learning (RL) provide new possibilities to realize this vision. The state-of-the-art in deep RL offers proven algorithms that enable autonomous learning and mastery of a variety of robotic manipulation tasks with minimal human intervention. However, current deep RL algorithms predominantly specialize in a narrow range of tasks, are sample inefficient, and lack sufficient stability, which hinders their adoption in real-life, industrial settings. This paper develops and tests a Hyper-Actor Soft Actor–Critic (HASAC) deep RL framework based on the notions of task modularization and transfer learning to tackle this limitation. The goal of the proposed HASAC is to enhance an agent’s adaptability to new tasks by transferring the learned policies of former tasks to the new task through a ”hyper-actor”. The HASAC framework is tested on the virtual robotic manipulation benchmark, Meta-World. Numerical experiments indicate superior performance by HASAC over state-of-the-art deep RL algorithms in terms of reward value, success rate, and task completion time.}
}
@article{MEYES2017107,
title = {Motion Planning for Industrial Robots using Reinforcement Learning},
journal = {Procedia CIRP},
volume = {63},
pages = {107-112},
year = {2017},
note = {Manufacturing Systems 4.0 – Proceedings of the 50th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.03.095},
url = {https://www.sciencedirect.com/science/article/pii/S221282711730241X},
author = {Richard Meyes and Hasan Tercan and Simon Roggendorf and Thomas Thiele and Christian Büscher and Markus Obdenbusch and Christian Brecher and Sabina Jeschke and Tobias Meisen},
keywords = {Reinforcement Learning, Cyber-Physical Production Systems (CPPS), Self-Optimization},
abstract = {A major challenge of today's production systems in the context of Industry 4.0 and Cyber-Physical Production Systems is to be flexible and adaptive whilst being robust and economically efficient. Specifically, the implementation of motion planning processes for industrial robots need to be refined concerning their variability of the motion task and the ability to adaptively deal with variations in the environment. In this paper, we propose a reinforcement learning (RL) based, cognition-enhanced six-axis industrial robot for complex motion planning along continuous trajectories as e.g. needed for welding, gluing or cutting processes in production. Our prototype demonstrator is inspired by the classic wire loop game which involves guiding a metal loop along the path of a curved wire from start to finish while avoiding any contact between the wire and the loop. Our work shows that the RL-agent is capable of learning how to control the robot to successfully play the wire loop game without the need of modeling the wire or programming the robot motion beforehand. Furthermore, the extension of the system by a visual sensor (a camera) allows the agent to sufficiently generalize the learning problem so that it can solve new or reshaped wires without the need of additional learning. We conclude that the applicability of RL for industrial robots and production systems in general provides vast and unexplored potential for processes that feature variability to some extent and thus require a general and robust approach for process automation.}
}
@article{DOLTSINIS20121628,
title = {A Model-Free Reinforcement Learning Approach Using Monte Carlo Method for Production Ramp-Up Policy Improvement - A Copy Exactly Test Case},
journal = {IFAC Proceedings Volumes},
volume = {45},
number = {6},
pages = {1628-1634},
year = {2012},
note = {14th IFAC Symposium on Information Control Problems in Manufacturing},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20120523-3-RO-2023.00288},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016333845},
author = {Stefanos C. Doltsinis and Niels Lohse},
keywords = {Learning, Monte Carlo, Manufacturing, Production, Assembly, Intelligent Manufacturing Systems},
abstract = {Production Ramp-up is a phase in the production timeline which has gained interest from the industry in the literature in order to decrease time-to-market. Intelligent systems and machine learning (ML) techniques have been applied in manufacturing lines and have demonstrated their potential to support knowledge capturing which can aid decision making. However, they mostly focus on supervised learning techniques which require prior knowledge and data pairs are classified without a systematic framework. This work approaches ramp-up as an episodic problem with a clear final target. Ramp-up is formalised as a decision process and a reinforcement learning approach is followed for deriving a policy, for a copy-exactly test case. Finally, a test case of an assembly station ramp-up by, different users is presented. A Monte Carlo approach is used to apply Reinforcement Learning (RL) and an improved policy is generated and evaluated.}
}
@article{PARK2022104107,
title = {Control automation in the heat-up mode of a nuclear power plant using reinforcement learning},
journal = {Progress in Nuclear Energy},
volume = {145},
pages = {104107},
year = {2022},
issn = {0149-1970},
doi = {https://doi.org/10.1016/j.pnucene.2021.104107},
url = {https://www.sciencedirect.com/science/article/pii/S0149197021004595},
author = {JaeKwan Park and TaekKyu Kim and SeungHwan Seong and SeoRyong Koo},
keywords = {Reinforcement learning, Deep neural network, Compact nuclear simulator, Heat up mode, Control automation},
abstract = {Next-generation nuclear instrumentation and control technology is aimed at higher levels of automation and lower operation burden. In recent years, studies have been conducted to contribute to the operation of power plants using artificial intelligence technology. This paper proposes an automatic control method for plant heat-up mode using deep reinforcement-learning technology as a basic study for plant automation. First, the existing compact nuclear simulator (CNS) is expanded to enable reinforcement learning, and key elements for reinforcement learning are designed to be suitable for the heat-up mode. A deep neural-network structure and a CNS deep reinforcement-learning mechanism are then presented for automatic control. The experimental results demonstrate that deep reinforcement-learning has the potential to perform automatic control operation.}
}
@article{GUO2023451,
title = {Research on dynamic decision-making for product assembly sequence based on Connector-Linked Model and deep reinforcement learning},
journal = {Journal of Manufacturing Systems},
volume = {71},
pages = {451-473},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001978},
author = {Kai Guo and Rui Liu and Guijiang Duan and Jiajun Liu and Pengyong Cao},
keywords = {Assembly sequence planning, Deep reinforcement learning, Combinatorial optimization, Connector-Linked Model},
abstract = {Assembly sequence planning (ASP) is a critical challenge in the manufacturing industry, involving the arrangement of components to maximize production efficiency and reduce costs. Solving the ASP problem is not only hindered by its NP-hard nature, making it difficult to obtain optimal solutions, but also requires making rapid decisions in the face of dynamic changes in resource availability during the actual assembly process. This paper proposes a Deep reinforcement learning (DRL) approach to tackle the ASP problem. DRL learns optimal assembly strategies through interactions between an intelligent agent and its environment, using a reward function as a long-term objective to guide decision-making, ultimately enabling the acquisition of near-optimal solutions. DRL exhibits high scalability and reusability, leveraging previously acquired knowledge to adapt to dynamic changes in the assembly environment. First, a connector-based graph structure is introduced to decompose assembly products into a series of connector-based assembly elements. These elements are then transformed into a matrix format for use as input to the DRL method. Second, an encoder-decoder architecture tailored to the ASP problem is developed. This architecture utilizes mask mechanism to ensure that the generated assembly sequences adhere to assembly priority constraints while adapting to dynamic changes in resource availability during assembly. Finally, policy gradient methods are employed to train the DRL model, enabling it to generate assembly sequences with lower costs and higher efficiency. Experimental results demonstrate the effectiveness of the proposed approach. Furthermore, compared to heuristic algorithms, DRL exhibits faster decision-making capabilities and better adaptability to dynamic resource changes during the assembly process.}
}
@article{OLIFF2020326,
title = {Reinforcement learning for facilitating human-robot-interaction in manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {56},
pages = {326-340},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301084},
author = {Harley Oliff and Ying Liu and Maneesh Kumar and Michael Williams and Michael Ryan},
keywords = {Intelligent manufacturing, Reinforcement learning, Human-robot interaction, Human factors, Adaptability},
abstract = {For many contemporary manufacturing processes, autonomous robotic operators have become ubiquitous. Despite this, the number of human operators within these processes remains high, and as a consequence, the number of interactions between humans and robots has increased in this context. This is a problem, as human beings introduce a source of disturbance and unpredictability into these processes in the form of performance variation. Despite the natural human aptitude for flexibility, their presence remains a source of disturbance within the system and make modelling and optimization of these systems considerably more challenging, and in many cases impossible. Improving the ability of robotic operators to adapt their behaviour to variations in human task performance is, therefore, a significant challenge to be overcome to enable many ideas in the larger intelligent manufacturing paradigm to be realised. This work presents the development of a methodology to effectively model these systems and a reinforcement learning agent capable of autonomous decision-making. This decision-making provides the robotic operators with greater adaptability, by enabling its behaviour to change based on observed information, both of its environment and human colleagues. The work extends theoretical knowledge on how learning methods can be implemented for robotic control, and how the capabilities that they enable may be leveraged to improve the interaction between robots and their human counterparts. The work further presents a novel methodology for the implementation of a reinforcement learning-based intelligent agent which enables a change in behavioural policy in robotic operators in response to performance variation in their human colleagues. The development and evaluation are supported by a generalized simulation model, which is parameterized to enable appropriate variation in human performance. The evaluation demonstrates that the reinforcement agent can effectively learn to make adjustments to its behaviour based on the knowledge extracted from observed information, and balance the task demands to optimise these adjustments.}
}
@article{ZHANG2022102227,
title = {A reinforcement learning method for human-robot collaboration in assembly tasks},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {73},
pages = {102227},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102227},
url = {https://www.sciencedirect.com/science/article/pii/S0736584521001095},
author = {Rong Zhang and Qibing Lv and Jie Li and Jinsong Bao and Tianyuan Liu and Shimin Liu},
keywords = {Human-robot collaboration, Reinforcement learning, Adaptive decision, Task allocation},
abstract = {The assembly process of high precision products involves a variety of delicate operations that are time-consuming and energy-intensive. Neither the human operators nor the robots can complete the tasks independently and efficiently. The human-robot collaboration to be applied in complex assembly operation would help reduce human workload and improve efficiency. However, human behavior can be unpredictable in assembly activities so that it is difficult for the robots to understand intentions of the human operations. Thus, the collaboration of humans and robots is challenging in industrial applications. In this regard, a human-robot collaborative reinforcement learning algorithm is proposed to optimize the task sequence allocation scheme in assembly processes. Finally, the effectiveness of the method is verified through experimental analysis of the virtual assembly of an alternator. The result shows that the proposed method had great potential in dynamic division of human-robot collaborative tasks.}
}
@article{VENUGOPAL2023564,
title = {Structural and thermal generative design using reinforcement learning-based search strategy for additive manufacturing},
journal = {Manufacturing Letters},
volume = {35},
pages = {564-575},
year = {2023},
note = {51st SME North American Manufacturing Research Conference (NAMRC 51)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2023.08.030},
url = {https://www.sciencedirect.com/science/article/pii/S2213846323000871},
author = {Vysakh Venugopal and Sam Anand},
keywords = {Generative design, Reinforcement learning, Design for additive manufacturing, Thermal topology optimization, Compliance minimization topology optimization},
abstract = {This paper proposes a reinforcement learning-based topology optimization (TO)/generative design approach using the Upper Confidence Bound (UCB) method to achieve a diverse set of optimal part designs for additive manufacturing. The UCB method has been integrated into two density-based TO problems – a compliance minimization problem and a thermal conduction problem along with Design for Additive Manufacturing (DfAM) filters to ensure improved additive manufacturability of the resultant topologies using different AM processes. The DfAM constraints are enforced by applying the support minimization filter and the thin feature minimization constraint in the TO model. The Solid Isotropic Material Penalization based TO model is perturbed to various levels of exploration and exploitation using the UCB exploration parameter to present different optimal designs. Unlike other data-reliant deep learning methods used in TO, the non-data-driven learning method proposed in this research is based on historical TO iterations integrated into a DfAM-constrained TO model, and it improves the method’s scalability to real-world and often computationally expensive part design applications. This paper fulfills a gap in computationally efficient methods for exploring generative designs of structural and thermal loaded parts with improved functional performance and additive manufacturability.}
}
@article{ZHANG202370,
title = {Counterfactual-attention multi-agent reinforcement learning for joint condition-based maintenance and production scheduling},
journal = {Journal of Manufacturing Systems},
volume = {71},
pages = {70-81},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001607},
author = {Nianmin Zhang and Yilan Shen and Ye Du and Lili Chen and Xi Zhang},
keywords = {Condition-based maintenance and production scheduling, Counterfactual attention multi-agent reinforcement learning, Hybrid flow shop},
abstract = {Maintenance and production scheduling are interactive activities that should be considered simultaneously to maintain production systems’ reliability and high production delivery rate. This study investigates a problem of joint maintenance and production scheduling in a multi-stage hybrid flow shop experiencing machine deterioration processes. To address the structural dependency among production stages and machines, a decentralized partially observable Markov decision process (Dec-POMDP) is considered. However, the large state and action space pose a challenge for existing reinforcement learning methods to provide satisfactory solutions. To overcome this problem, the study proposes a counterfactual attention multi-agent reinforcement learning (CAMARL) solution framework that comprises three functional modules: The attention mechanism module, which adaptively shrinks the dimension of the state space; the action abstraction representation module that deals with the high-dimension of the action space; and the coordination control unit, which accelerates the exploration for the optimal production policy. Numerical experiments demonstrate the effectiveness of the proposed method by comparing it with seven benchmarks under different production scenarios.}
}
@article{DENG2023110547,
title = {Mass customization with reinforcement learning: Automatic reconfiguration of a production line},
journal = {Applied Soft Computing},
volume = {145},
pages = {110547},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110547},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623005653},
author = {Jifei Deng and Seppo Sierla and Jie Sun and Valeriy Vyatkin},
keywords = {Mass customization, Strip rolling, Machine learning, Multi-agent reinforcement learning},
abstract = {This paper addresses the problem of efficient automation system configuration for mass customization in industrial manufacturing. Due to the various demands from customers, production lines need to adjust the process parameters of the machines based on specific quality parameters. Reinforcement learning, which learns from samples, can tackle the problem more efficiently than the currently used methods. Based on the proximal policy optimization and centralized training with decentralized execution, a multi-agent reinforcement learning method (MARL) is proposed to reconfigure process parameters of machines based on the changed specifications. The proposed method has the actor of each agent observing only its own state, the agents are made to collaborate by a centralized critic which observes all the states. To evaluate the method, a steel strip rolling line with six collaborating mills is studied. Simulation results show that the proposed method outperforms the existing methods and state-of-the-art multi-agent reinforcement learning methods in terms of accuracy and computing costs.}
}
@article{LEE2022116222,
title = {Deep reinforcement learning based scheduling within production plan in semiconductor fabrication},
journal = {Expert Systems with Applications},
volume = {191},
pages = {116222},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116222},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015359},
author = {Young Hoon Lee and Seunghoon Lee},
keywords = {Deep reinforcement learning, Production planning, Scheduling, Semiconductor fabrication, Intelligent manufacturing},
abstract = {In the semiconductor industry, efficient production planning and scheduling decisions are required to enhance the manufacturing productivity of a company as the system is complicated due to a re-entry characteristic and requires a long production lead time. Production planning is implemented before scheduling and is important for successful manufacturing operations. However, if scheduling at the operation level cannot execute the production plan, failures occur because of inconsistent decisions. Therefore, scheduling needs to fulfill the production plan to ensure realistic decision-making processes for the companies aiming for economic growth and global competitiveness. In this study, deep reinforcement learning (RL) is employed to deal with a scheduling process operating within the production plan. As the algorithm of the deep RL, Deep Q-network is conjugated, and a novel state, action, and reward are suggested to optimize the scheduling policy. As a result, the performance of the proposed deep RL method is in comparison with other dispatching rules, and the proposed method outperforms the other scheduling methods in diverse cases.}
}
@article{SCHMIDL2021290,
title = {Reinforcement learning for energy reduction of conveying and handling systems},
journal = {Procedia CIRP},
volume = {97},
pages = {290-295},
year = {2021},
note = {8th CIRP Conference of Assembly Technology and Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.240},
url = {https://www.sciencedirect.com/science/article/pii/S221282712031461X},
author = {Elisabeth Schmidl and Eva Fischer and Johanna Steindl and Matthias Wenk and Jörg Franke},
keywords = {Reinforcement learning, Energy reduction, Virtual commissioning},
abstract = {One important reason for wasting energy in manufacturing processes is that individual plant components often consume too much energy despite not being needed for production or nothing being produced at that moment. The implementation of standby-strategies in the PLC to switch each component into an optimal energetic state is applied seldom due to the great programming effort. This is because there are many functional dependencies between the plant components and therefor the PLC program must be adapted manually to the respective manufacturing process. The presented solution shows an intelligent system, which can adapt to plant-specific process requirements and derive decisions about the optimal energetic state of each component autonomously. In this paper, we provide an approach that uses reinforcement learning algorithms, which train on virtual plant models. The results are verified on a conveying and handling system in our learning factory.}
}
@article{GU2022108406,
title = {Using real-time manufacturing data to schedule a smart factory via reinforcement learning},
journal = {Computers & Industrial Engineering},
volume = {171},
pages = {108406},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108406},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222004466},
author = {Wenbin Gu and Yuxin Li and Dunbing Tang and Xianliang Wang and Minghai Yuan},
keywords = {Smart factory, Real-time scheduling, Genetic programming, Production state clustering, Reinforcement learning},
abstract = {Under the background of intelligent manufacturing, internet of things and other information technologies have accumulated a large amount of data for manufacturing system. However, the traditional scheduling methods often ignore the production law and knowledge hidden in the manufacturing data. Therefore, this paper proposes a cyber-physical architecture and a communication protocol for smart factory, and a multiagent-system-based dynamic scheduling mechanism is given using contract net protocol. In the dynamic scheduling mechanism, the problem formulation module and scheduling point module are designed first. Then, a genetic programming (GP) method is proposed to form sixteen high-quality rules, which constitute the scheduling rule library. Meanwhile, combining with autoencoder, self-organizing mapping neural network and k-means clustering algorithm, the state clustering module is designed to realize the efficient clustering of production attribute vector. Moreover, an improved Q-learning algorithm is used to train the GP rule selector, so that the decision-making agent can choose the appropriate GP rule according to the production state at each scheduling point. Finally, the experimental results show that the proposed method has feasibility and superiority compared with other methods in real-time scheduling, and can effectively deal with disturbance events in the manufacturing process.}
}
@article{LAMMLE20201061,
title = {Skill-based Programming of Force-controlled Assembly Tasks using Deep Reinforcement Learning},
journal = {Procedia CIRP},
volume = {93},
pages = {1061-1066},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.153},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120310301},
author = {Arik Lämmle and Thomas König and Mohamed El-Shamouty and Marco F. Huber},
keywords = {Force-controlled assembly automation, Robot programming, Deep Reinforcement Learning},
abstract = {The selection of parameters for force-controlled assembly tasks remains a time consuming process. Skill-based approaches offer a guidance in parameter space, but still require expert knowledge to tune the parameters accordingly. Recently, Deep Reinforcement Learning algorithms have been used more frequently to learn the parameters of complex assembly tasks. For the industrial application of Reinforcement Learning, it is crucial to increase the training efficiency. Most approaches are based on engineered force-controllers or contact-models and focus on one specific robot. In this paper, we propose a framework, using state-of-the-art model-free algorithms and manipulation skills to learn force and position parameters in a simulation environment.}
}
@article{ZHANG2021103471,
title = {Planning for automatic product assembly using reinforcement learning},
journal = {Computers in Industry},
volume = {130},
pages = {103471},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103471},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521000786},
author = {Heng Zhang and Qingjin Peng and Jian Zhang and Peihua Gu},
keywords = {Product assembly, Automatic assembly, Assembly planning, Reinforcement learning, Machine learning},
abstract = {Assembly connects functional modules and components of products. The efficient and accurate assembly can improve performance of the product operation and maintenance. It is therefore essential to have an effective method for product assembly. Existing methods of the mechanical product assembly use mainly manual processes that rely on experience of operators. This paper proposes a reinforcement learning method to enable an automatic operation for improved efficiency and accuracy of the mechanical product assembly. A representation of the product assembly is proposed to build a machine learning model. The automatic assembly of product operations is planned by reinforcement learning agents. Constraints of assembly operations are considered to develop searching strategies of the maximum reward for the optimal solution of assembly operations. A quantitative method is proposed to measure efficiency of assembly operations based on the operation time. The proposed method has been applied in the assembly improvement of function modules of an industrial machine.}
}
@article{EPUREANU2020421,
title = {Self-repair of smart manufacturing systems by deep reinforcement learning},
journal = {CIRP Annals},
volume = {69},
number = {1},
pages = {421-424},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620300299},
author = {Bogdan I. Epureanu and Xingyu Li and Aydin Nassehi and Yoram Koren},
keywords = {Reconfiguration, Decision making, Artificial intelligence},
abstract = {Smart manufacturing is built on complex decision-making in real-time based on data from networked machines and sensors. As a potential enabler of smart manufacturing, reconfigurability enhances adaptability to demands and enriches the utility of the collected data. This study focuses on a synergistic combination of advanced manufacturing technologies in a highly diverse market environment, to identify deficient components by inferring from changes in product quality and to sustain the operation of the manufacturing system by creating multiple-level self-repair strategies. Deep reinforcement learning is then used to select the strategy based on system status and performance.}
}
@article{XIONG2023105710,
title = {Multi-agent deep reinforcement learning for task offloading in group distributed manufacturing systems},
journal = {Engineering Applications of Artificial Intelligence},
volume = {118},
pages = {105710},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105710},
url = {https://www.sciencedirect.com/science/article/pii/S095219762200700X},
author = {Jianyu Xiong and Peng Guo and Yi Wang and Xiangyin Meng and Jian Zhang and Linmao Qian and Zhenglin Yu},
keywords = {Cloud computing, Near-real-time optimization, Group distributed manufacturing system, Task offloading, Multi-agent deep reinforcement learning},
abstract = {The rapid development of cloud computing and the Internet of Things (IoT) have facilitated near real-time optimization of the group distributed manufacturing systems. Currently, the most common technique to accomplish near-real-time optimization is cloud–edge cooperation for offloading optimization tasks. The tasks are partially offloaded to the cloud to be completed, and the remaining are kept at the edge. Due to the complexity of task offloading, such as capacity restrictions of cloud and edge computing resources, or task deadlines, unbalanced or insufficient tasks are offloaded to cloud and edge, causing time delay. To address the imbalance and insufficiency in the task offloading process, a mixed-integer programming model was developed to reduce the latency of task calculation. The task offloading problem is decomposed into two sub-problems: 1) Defining priorities for the tasks in near real-time. 2) Determining if the task is offloaded to the cloud. A multi-agent deep reinforcement learning with attention mechanism (MaDRLAM) framework is proposed to solve the two-step decision problem. The MaDRLAM framework consists of two agents, and each agent corresponds to a sub-problem. Each agent comprises an encoder and a decoder, and the two agents cooperate in devising an offloading strategy for the tasks. The Encoder and Decoder built for each agent are based on the Transformer structure. Unlike the traditional Transformer, we added the Pointer networks to the Transformer to solve the proposed decision problem. Besides, an improved multi-actor and single-critic strategy based on the REINFORCE algorithm is designed to train the proposed MaDRLAM. Finally, Extensive computational experiments are conducted on instances with a varying number of tasks, different task data sizes, and different cloud computing capacities. Computational results show that the proposed framework can find a solution with a GAP value of less than 1% within 1 s for each instance. The proposed framework is competitive in both solution accuracy and solution time compared with other offloading strategies.}
}
@article{LOFFREDO202391,
title = {Reinforcement learning for energy-efficient control of parallel and identical machines},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {44},
pages = {91-103},
year = {2023},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2023.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1755581723000706},
author = {Alberto Loffredo and Marvin Carl May and Louis Schäfer and Andrea Matta and Gisela Lanza},
keywords = {Reinforcement Learning, Energy-Efficient Control, Production Systems, Parallel Machines, Sustainability, Manufacturing Automation},
abstract = {Nowadays, the growing interest in industry for enhancing the sustainability of manufacturing processes is becoming a major trend. Energy consumption can be lowered by controlling machine states with energy-efficient control policies that switch off/on the device. Recent studies have shown that Reinforcement Learning algorithms can effectively control manufacturing systems without the requirement of prior knowledge about system parameters. This is a significant factor since full information on system dynamics is difficult to obtain in real-world applications. This work proposes a new Reinforcement Learning-based algorithm to apply energy-efficient control strategies to a single workstation consisting of identical parallel machines. The model goal is to achieve the optimum trade-off between system productivity and energy demand without relying on full knowledge of the system dynamics. Numerical experiments confirm effectiveness, applicability, and generality of the proposed approach, even when applied to a real-world industrial system from the automotive sector.}
}
@article{PIRES2023103884,
title = {Reinforcement learning based trustworthy recommendation model for digital twin-driven decision-support in manufacturing systems},
journal = {Computers in Industry},
volume = {148},
pages = {103884},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103884},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523000349},
author = {Flávia Pires and Paulo Leitão and António Paulo Moreira and Bilal Ahmad},
keywords = {Digital twin, Decision-support, Recommendation systems, Similarity measures, Trust-based model},
abstract = {Digital twin is one promising and key technology that emerged with Industry 4.0 to assist the decision-making process in multiple industries, enabling potential benefits such as reducing costs, and risk, improving efficiency, and supporting decision-making. Despite these, the decision–making approach of carrying out a what-if simulation study using digital twin models of each and every possible scenario independently is time-consuming and requires significant computational resources. The integration of recommendation systems within the digital twin-driven decision-support framework can support the decision-making process by providing targeted scenario recommendations, reducing the decision-making time and imposing decision- making efficiency. However, recommendation systems have inherent challenges, such as cold-start, data sparsity, and prediction accuracy. The integration of trust and similarity measures with recommendation systems alleviates the challenges mentioned earlier, and the integration of machine learning techniques enables better recommendations through their ability to simulate human learning. Having this in mind, this paper proposes a trust-based recommendation approach using a reinforcement learning technique combined with similarity measures, which can be integrated within a digital twin-based what-if simulation decision-support system. This approach was experimentally validated by performing accurate recommendations in an industrial case study of a battery pack assembly line. The results show improvements in the proposed model regarding the accuracy of the prediction about the user rating of the recommended scenarios over the state-of-the-art recommendation approaches, particularly in cold-start and data sparsity scenarios.}
}
@article{ESNAASHARI2021115446,
title = {Automation of software test data generation using genetic algorithm and reinforcement learning},
journal = {Expert Systems with Applications},
volume = {183},
pages = {115446},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115446},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421008605},
author = {Mehdi Esnaashari and Amir Hossein Damia},
keywords = {Software test, Structural test, Test data generation, Genetic algorithms, Reinforcement learning},
abstract = {Software testing is one of the most important methods of analyzing software quality assurance. This process is very time consuming and expensive and accounts for almost 50% of the software production cost. In addition to the cost problem, the nature of the test, which seeks errors in the program, is such that software engineers are not interested in doing the process, so we are looking to use automated methods to reduce the cost and time of the test. In the last decade, various methods have been introduced for the automatic test data generation, the purpose of which is to maximize the detection of errors by generating minimum amount of test data. The main issue in the test data generation process is to determine the input data of the program in such a way that it meets the specified test criterion. In this research, a structural method has been used in order to automate the process of test data generation considering the criterion of covering all finite paths. In structural methods, the problem is converted into a search problem and meta-heuristic algorithms are used to solve it. The proposed method in this paper is a memetic algorithm in which reinforcement learning is used as a local search method within a genetic algorithm. Experimental results have shown that this method is faster for test data generation than many existing evolutionary or meta-heuristic algorithms and can provide better coverage with fewer evaluations. Compared algorithms include: conventional genetic algorithm, a variety of improvements to the genetic algorithm, random search, particle swarm optimization, bees algorithm, ant colony optimization, simulated annealing, hill climbing, and tabu search.}
}
@article{XIA2021210,
title = {A digital twin to train deep reinforcement learning agent for smart manufacturing plants: Environment, interfaces and intelligence},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {210-230},
year = {2021},
note = {Digital Twin towards Smart Manufacturing and Industry 4.0},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301059},
author = {Kaishu Xia and Christopher Sacco and Max Kirkpatrick and Clint Saidy and Lam Nguyen and Anil Kircaliali and Ramy Harik},
keywords = {Smart manufacturing systems, Robotics, Artificial intelligence, Digital transformation, Virtual commissioning},
abstract = {Filling the gaps between virtual and physical systems will open new doors in Smart Manufacturing. This work proposes a data-driven approach to utilize digital transformation methods to automate smart manufacturing systems. This is fundamentally enabled by using a digital twin to represent manufacturing cells, simulate system behaviors, predict process faults, and adaptively control manipulated variables. First, the manufacturing cell is accommodated to environments such as computer-aided applications, industrial Product Lifecycle Management solutions, and control platforms for automation systems. Second, a network of interfaces between the environments is designed and implemented to enable communication between the digital world and physical manufacturing plant, so that near-synchronous controls can be achieved. Third, capabilities of some members in the family of Deep Reinforcement Learning (DRL) are discussed with manufacturing features within the context of Smart Manufacturing. Trained results for Deep Q Learning algorithms are finally presented in this work as a case study to incorporate DRL-based artificial intelligence to the industrial control process. As a result, developed control methodology, named Digital Engine, is expected to acquire process knowledges, schedule manufacturing tasks, identify optimal actions, and demonstrate control robustness. The authors show that integrating a smart agent into the industrial platforms further expands the usage of the system-level digital twin, where intelligent control algorithms are trained and verified upfront before deployed to the physical world for implementation. Moreover, DRL approach to automated manufacturing control problems under facile optimization environments will be a novel combination between data science and manufacturing industries.}
}
@article{HUANG202281,
title = {Graph neural network and multi-agent reinforcement learning for machine-process-system integrated control to optimize production yield},
journal = {Journal of Manufacturing Systems},
volume = {64},
pages = {81-93},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522000887},
author = {Jing Huang and Jianyu Su and Qing Chang},
keywords = {Integrated control, Manufacturing system graph, Graph neural network, Multi-agent reinforcement learning, Production yield optimization},
abstract = {In this paper, an integrated control framework is proposed for the optimization of the production yield by integrating different levels of a manufacturing system, including system, process, and machine levels. The manufacturing system is modeled as a graph by treating machines as nodes and material flows as links. The graph model enjoys high flexibility and is able to incorporate all relevant real-time information across all levels of the manufacturing system in the dynamic node feature. Since the real-time tool state is essential for decision making, Recursive Bayesian Estimation (RBE) is adopted to reduce the tool state observations through sensors and machine learning models and provide more accurate tool state estimation to be included into the graph node feature. With the graph model, Graph Neural Network (GNN) is applied to process the node features to generate node embedding that reflects both local and global information. For the integrated control purpose, each machine node is then be treated as a distributed agent in Multi-Agent Reinforcement Learning (MARL) that conditions its policy on the node embedding from GNN. State-of-the-art GNN and MARL algorithms, namely Graph Attention Network (GAT) and Value Decomposition Actor Critic (VDAC), are implemented to train learnable parameters in GNN-MARL networks to learn the optimal multi-agent policy. Extensive numerical experiments and analysis proves the effectiveness of the proposed integrated control framework.}
}
@article{LI2022351,
title = {Hybrid feedback and reinforcement learning-based control of machine cycle time for a multi-stage production system},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {351-361},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001698},
author = {Chen Li and Qing Chang},
keywords = {Machine cycle control, Production loss, Deep reinforcement learning, Markov decision process (MDP)},
abstract = {With the increasing need of flexible manufacturing systems, machine flexibility has become one of the major impact factors. An important aspect of machine flexibility is the ability to change an individual machine’s capacity (or cycle time) to improve the overall system efficiency. In this paper, a novel control method is proposed for multi-stage production systems to dynamically change the individual machines’ cycle time to improve overall system efficiency. The proposed control method integrates distributed feedback control scheme and a Reinforcement Learning (RL) control scheme based on an extended actor-critic algorithm. The feedback control will determine whether a machine is turned on or off using real-time system status, while the RL control scheme will decide how to increase or decrease a machine’s cycle time when a machine is on. An improved actor-critic RL algorithm is developed to add an auxiliary model-based path to the standard model-free RL to enhance the learning performance. To demonstrate the effectiveness of the proposed method, numerical case studies have been performed that clearly show improvements in the overall profits and energy savings compared to other methods.}
}
@article{ZHANG2022491,
title = {A graph-based reinforcement learning-enabled approach for adaptive human-robot collaborative assembly operations},
journal = {Journal of Manufacturing Systems},
volume = {63},
pages = {491-503},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522000760},
author = {Rong Zhang and Jianhao Lv and Jie Li and Jinsong Bao and Pai Zheng and Tao Peng},
keywords = {Human-robot coexisting, Part-behavior assembly and/or graph, Behavior prediction, Self-attention, Adaptive decision making, Reinforcement learning},
abstract = {In today’s prevailing manufacturing paradigm of mass personalization, neither human operators nor robots alone can perform all assembly tasks efficiently. To overcome it, human-robot collaborative assembly shows its great potentials to ensure the flexibility of human operations with high reliability of robot assistance. However, it is often challenging to achieve harmonious coexistence between humans and robots to complete the tasks safely and efficiently. In this regard, this research provides a detailed description of the human-robot coexisting environment and further introduces key issues in collaborative assembly. A part-behavior assembly and/or graph based on process requirements is proposed to represent the assembly task of complex products. Moreover, the human behavior prediction network based on self-attention can achieve higher accuracy. Combined with the robustness of Soft Actor-Critic (SAC), the collaborative system improves the self-decision ability of the robot in the dynamic scene. Finally, the effectiveness of the method is verified through experimental analysis. The results indicate that the accuracy of the proposed behavior recognition based on self-attention method is 91%. At the same time, it is proved that the reinforcement learning method is theoretically feasible to provide adaptive decision-making for robots in human-machine collaboration. In addition, the convergence speed of the reward function proves the feasibility of SAC for adaptive decision-making in a human-robot collaborative environment.}
}
@article{HALBWIDL20211221,
title = {Deep Reinforcement Learning as an Optimization Method for the Configuration of Adaptable, Cell-Oriented Assembly Systems},
journal = {Procedia CIRP},
volume = {104},
pages = {1221-1226},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.205},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011033},
author = {Christoph Halbwidl and Thomas Sobottka and Alexander Gaal and Wilfried Sihn},
keywords = {Reinforcement Learning, Modular Assembly System, Configuration, Simulation-based Optimization},
abstract = {This paper investigates the feasibility and performance of Deep Reinforcement Learning (RL) as a method for optimizing assembly cell configurations in adaptable cell-oriented assembly systems (ACAS). ACAS can be as productive as conventional assembly lines, while offering greater flexibility and resilience. However, optimizing their layout configuration and resource assignment poses a complex challenge for conventional optimization methods. A RL and simulation-based method is evaluated in an ACAS use-case setting, including a benchmark with metaheuristics. The findings show the limitations of RL for static aspects of the optimization problem, but also indicate RL’s considerable benefits for dynamic optimization tasks in ACAS.}
}
@article{YUN2023121324,
title = {Explainable multi-agent deep reinforcement learning for real-time demand response towards sustainable manufacturing},
journal = {Applied Energy},
volume = {347},
pages = {121324},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121324},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923006888},
author = {Lingxiang Yun and Di Wang and Lin Li},
keywords = {Explainable reinforcement learning, Multi-agent deep reinforcement learning, Dynamic demand response, Manufacturing energy management, Real-time control},
abstract = {The demand response (DR) plays a significant role in manufacturing system energy management and sustainable industrial development. Current literature on DR management for manufacturing systems has mostly focused on day-ahead production scheduling, whose effectiveness is limited due to the lack of flexibility to control the production line in real time. The development of reinforcement learning (RL) possesses huge potential for real-time production control to address the flexibility issue. However, since production is the top priority for any manufacturing system, a trustful and explainable RL for manufacturing system energy management that can ensure the production requirements is necessary for this application. This study proposes an explainable multi-agent deep RL method, where the analytical manufacturing system model is applied to decompose the system-level energy management objective and production requirement to the agent level. Based on the decomposed task, the agent can then form a safe action subset that is interpretable to achieve the original system-level production requirement while learning to reduce energy costs under DR. The proposed RL method, which is referred to as decomposed multi-agent deep Q-network (DMADQN), is applied to control a section of an automotive assembly line using one year of DR electricity price data to validate its performance. Results show that the proposed method ensures the achievement of the production requirement while providing better DR energy management performance in both RL training and testing phases. In addition, the proposed approach can outperform the day-ahead scheduling approach and save up to an additional 30.7% of energy costs under dynamic DR.}
}
@article{ZHAO2021107082,
title = {A cooperative water wave optimization algorithm with reinforcement learning for the distributed assembly no-idle flowshop scheduling problem},
journal = {Computers & Industrial Engineering},
volume = {153},
pages = {107082},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.107082},
url = {https://www.sciencedirect.com/science/article/pii/S036083522030752X},
author = {Fuqing Zhao and Lixin Zhang and Jie Cao and Jianxin Tang},
keywords = {Water wave optimization algorithm, Distributed assembly no-idle flow-shop scheduling problem, Reinforcement learning, Variable neighborhood search},
abstract = {The distributed assembly no-idle flow-shop scheduling problem (DANIFSP) is a novel and considerable model, which is suitable for the modern supply chains and manufacturing systems. In this study, a cooperative water wave optimization algorithm, named CWWO, is proposed to address the DANIFSP with the goal of minimizing the maximum assembly completion time. In the propagation phase, a reinforcement learning mechanism based on the framework of the VNS is designed to balance the exploration and exploitation of the CWWO algorithm. Afterwards, the path-relinking combined with the VNS method as the modified breaking operator is introduced to enhance the capability of local search. Furthermore, a multi-neighborhood perturbation strategy in the refraction phase is applied to extract knowledge information to increase the probability of escaping the local optimal. Moreover, the comprehensive experimental program is executed to calibrate the control parameters of the CWWO algorithm and illustrate the cooperative effect of the three modified operations. The performance of the CWWO algorithm is verified on the benchmark set, and the experimental results demonstrated the stability and validity of the CWWO algorithm.}
}
@article{KULMER20221065,
title = {Medium-term Capacity Management through Reinforcement Learning – Literature review and concept for an industrial pilot-application},
journal = {Procedia CIRP},
volume = {107},
pages = {1065-1070},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.109},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122003936},
author = {Florian Kulmer and Matthias Wolf and Christian Ramsauer},
keywords = {Literature Review, Capacity Management, Capacity Planning, Reinforcement Learning, Machine Learning, Decision Support System},
abstract = {Empty storage shelves and long customer lead times due to a sharp rise in market demand from industries on the one hand (e.g., pharmaceutical products). On the other hand, increasing short-term working or unemployment due to a rapid decline in demand (e.g., automotive). Current supply and demand gaps caused by the COVID-19 pandemic remind us that successful competition in volatile business environments requires rapid adjustments of production capacities. Capacity management (CM) addresses these adjustments by adapting production capacity to market demand. Operations managers of flexible manufacturing systems can adjust the capacity by using various levers (e.g., overtime, used machines, …). To guide these managers, decision support systems (DSS) exist for short-term CM (e.g., shop floor scheduling). However, due to complexity and runtime problems, the decision-making process for medium-term CM is usually carried out with low technical support. Increases in computing power and advances in algorithm performance over the past decades have enabled Machine Learning to solve ever more complex problems such as the aforementioned issues. Reinforcement Learning (RL) in particular has shown good performance in solving short-term CM problems when compared to humans or other established heuristics. In this work we review the current literature for CM using RL in flexible manufacturing systems. We identify an existing lack of knowledge within the overlap of medium-term CM and RL. However, good performance of RL in short-term CM indicates that an application in medium-term CM should be evaluated. In addition, we propose a concept of a method for medium-term CM based on RL to support operations managers in the decision-making process. The resulting DSS could have a significant impact on production performance, especially in terms of capacity adjustment speed.}
}
@article{PING2023315,
title = {Sequence generation for multi-task scheduling in cloud manufacturing with deep reinforcement learning},
journal = {Journal of Manufacturing Systems},
volume = {67},
pages = {315-337},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S027861252300033X},
author = {Yaoyao Ping and Yongkui Liu and Lin Zhang and Lihui Wang and Xun Xu},
keywords = {Cloud manufacturing, Multi-task scheduling, Sequence generation algorithm, Deep reinforcement learning, Double Deep Q-networks},
abstract = {Cloud manufacturing is a manufacturing model that aims to deliver on-demand manufacturing services to consumers. Scheduling is an important problem that needs to be addressed carefully and effectively for cloud manufacturing to achieve that aim. Cloud manufacturing allows consumers to submit their requirements to the cloud platform simultaneously and therefore requires cloud manufacturing scheduling systems to be able to handle multiple tasks effectively. It is further complicated when multiple composite tasks are submitted to the system and to be addressed. A vast majority of existing studies have proposed various algorithms, including meta-heuristics, heuristics, and reinforcement learning algorithms, to address cloud manufacturing scheduling (CMfg-Sch) problems, but only a very small fraction of them deal with scheduling of multiple composition tasks with deep reinforcement learning. In this work, we leverage DRL coupled with sequence generation for addressing CMfg-Sch problems. Different from all existing works, we first propose two sequence generation algorithms for generating scheduling sequences of multiple composite tasks prior to scheduling. Coupled with this a Deep Q-Networks (DQN) and a Double DQN-based scheduling algorithms are proposed, respectively. Performance of the proposed algorithms is compared against seven baseline algorithms using makespan, cost, and reliability as evaluation metrics. Comparison indicates that sequence generation algorithm II (SGA-II) overall has a greater advantage over algorithm I (SGA-I), especially in terms of the makespan, and the Double DQN-based scheduling algorithm outperforms the DQN-based algorithm, which in turn performs better than other baseline algorithms.}
}
@article{YOON2022119337,
title = {Automation of membrane capacitive deionization process using reinforcement learning},
journal = {Water Research},
volume = {227},
pages = {119337},
year = {2022},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2022.119337},
url = {https://www.sciencedirect.com/science/article/pii/S0043135422012829},
author = {Nakyung Yoon and Sanghun Park and Moon Son and Kyung Hwa Cho},
keywords = {Membrane capacitive deionization, Deep reinforcement learning, Automation, Optimization},
abstract = {Capacitive deionization (CDI) is an alternative desalination technology that uses electrochemical ion separation. Although several attempts have been made to maximize the energy efficiency and productivity of CDI with conventional control methods, it is difficult to optimize the CDI processes because of the complex correlation between the operational conditions and the composition of feed water. To address these challenges, we applied deep reinforcement learning (DRL) to automatically control the membrane capacitive deionization (MCDI) process, which is one of the representative CDI processes, to accomplish high energy efficiency while desalinating water. In the DRL model, the numerical model is combined as the environment that provides states according to the actions. The feed water conditions, that is, the input state of the DRL, were assumed to have a random salt concentration and constant foulant concentration. The model was constructed to minimize energy consumption and maximize desalted water volume per cycle. After training of 1,000 episodes, the DRL model achieved a 22.07% reduction in specific energy consumption (from 0.054 to 0.042 kWh m−3) and 11.60% increase in water desalted water volume per cycle (from 1.96×10−5 to 2.19×10−5 m3), achieving the desired degree of desalination, compared to the first episode. This improved performance was because the trained model selected the optimized operating conditions of current, voltage, and the number and intensity of flushing. Furthermore, it was possible to train the model depending on demand by modifying the reward function of the DRL model. The fundamental principle described in this study for applying the DRL model in MCDI operations can be the cornerstone of a fully automated water desalination process.}
}
@article{ROHLER2022459,
title = {Knowledge-based Implementation of Deep Reinforcement Learning Agents in Assembly},
journal = {Procedia CIRP},
volume = {112},
pages = {459-464},
year = {2022},
note = {15th CIRP Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.09.088},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122012513},
author = {Marcus Röhler and Johannes Schilp},
keywords = {Production, Deep Reinforcement Learning, Knowledge-based system, Case-based Reasoning, Meta-Learning},
abstract = {Robotic systems based on Deep Reinforcement Learning have shown great potential to enable assembly systems with higher flexibility and robustness. This paper presents a concept of a Case-Based Reasoning system to automate the implementation process, based on the assumption that similar assembly tasks have similar solutions as used as heuristics in the current manual procedure. For retrieving similar cases a digital description of the assembly task and a method to measure the similarity is introduced. The retrieved cases are then used to warmstart a Bayesian Hyperparameter Optimization. The approach is evaluated on two simulated robot task.}
}
@article{GOPPERT20201091,
title = {Automated scenario analysis of reinforcement learning controlled line-less assembly systems},
journal = {Procedia CIRP},
volume = {93},
pages = {1091-1096},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.116},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120307514},
author = {Amon Göppert and Jonas Rachner and Robert H. Schmitt},
keywords = {assembly planning, matrix assembly, scenario analysis, reinforcement learning, artificial intelligence},
abstract = {The concept of line-less matrix-structured assembly systems with dynamical job routes is an alternative concept to line configurations with a promising performance for complex production scenarios. Due to the dynamical environment, fast online decisions for controlling the job routes are required. A reinforcement-learning algorithm based on Monte Carlo tree search for time-efficient online decision-making is proposed. For the performance comparison of matrix assembly systems with line configurations, a seamlessly automated scenario analysis tool generates, simulates and processes the results for a large set of assembly scenarios. The results show a significant improvement in the utilization of work stations especially for a high number of variants.}
}
@article{QIN2023242,
title = {Dynamic production scheduling towards self-organizing mass personalization: A multi-agent dueling deep reinforcement learning approach},
journal = {Journal of Manufacturing Systems},
volume = {68},
pages = {242-257},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523000468},
author = {Zhaojun Qin and Dazzle Johnson and Yuqian Lu},
keywords = {Mass personalization, Self-organizing manufacturing network, Dynamic flexible job shop scheduling problem, Multi-agent production scheduling, Reinforcement learning},
abstract = {Mass personalization is rapidly approaching. In response, manufacturing systems should be capable of autonomously changing production plans, configurations and schedules under dynamic manufacturing environments for producing personalized products. Self-organizing manufacturing network is a promising paradigm for mass personalization. The backbone of a self-organizing manufacturing network is an adaptive production scheduling method to dynamically allocate and sequence manufacturing jobs under dynamic settings, such as stochastic processing time or unplanned machine breakdown. However, existing production scheduling methods (i.e., heuristic rules, meta-heuristic algorithms, and existing reinforcement learning models) fail to automatically optimize production schedules while maintaining stable manufacturing performance, under dynamic settings. In this paper, we designed a reinforcement learning-based static-training-dynamic-execution approach for dynamic job shop scheduling problems. The scheduling policies are learned from static scheduling instances by a multi-agent dueling deep reinforcement learning approach. Under this approach, we proposed new representations of observation, action, reward, and cooperation mechanisms between agents. The learned scheduling policies are then deployed to a dynamic scheduling system where stochastic processing time and unplanned machine breakdown randomly occur. Extensive simulation experiments demonstrated that our approach outperforms heuristic rules on makespan under two dynamic manufacturing settings.}
}
@article{STAMER2023405,
title = {Dynamic pricing of product and delivery time in multi-variant production using an actor critic reinforcement learning},
journal = {CIRP Annals},
volume = {72},
number = {1},
pages = {405-408},
year = {2023},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2023.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0007850623000653},
author = {Florian Stamer and Gisela Lanza},
keywords = {Adaptive manufacturing, Mass customization, Dynamic pricing},
abstract = {The profitability of manufacturers in multi-variant production is challenged by the combination of increasing customer requirements and volatile supply chains. A potential solution is dynamic pricing, where customers can select a delivery time and price based on their preferences, and demand can be balanced during peak times. This paper presents a dynamic pricing approach using an actor-critic reinforcement learning agent in combination with a production simulation model and applies it in the automation technology industry.}
}
@article{TANG20211,
title = {A Deep Reinforcement Learning Based Scheduling Policy for Reconfigurable Manufacturing Systems},
journal = {Procedia CIRP},
volume = {103},
pages = {1-7},
year = {2021},
note = {9th CIRP Global Web Conference – Sustainable, resilient, and agile manufacturing and service operations : Lessons from COVID-19},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.09.089},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121008398},
author = {Jiecheng Tang and Konstantinos Salonitis},
keywords = {Reconfigurable manufacturing system, scheduling, reinforcement learning, dueling double deep q learning, discrete event simulation},
abstract = {Reconfigurable manufacturing systems (RMS) is one of the trending paradigms toward a digitalised factory. With its rapid reconfiguring capability, finding a far-sighted scheduling policy is challenging. Reinforcement learning is well-equipped for finding highly efficient production plans that would bring near-optimal future rewards. For minimising reconfiguring actions, this paper uses a deep reinforcement learning agent to make autonomous decision with a built-in discrete event simulation model of a generic RMS. Aiming at the completion of the assigned order lists while minimising the reconfiguration actions, the agent outperforms the conventional first-in-first-out dispatching rule after self-learning.}
}
@article{WANG2022130,
title = {Dynamic scheduling of tasks in cloud manufacturing with multi-agent reinforcement learning},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {130-145},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001327},
author = {Xiaohan Wang and Lin Zhang and Yongkui Liu and Feng Li and Zhen Chen and Chun Zhao and Tian Bai},
keywords = {Cloud manufacturing, Dynamic scheduling, Multi-agent reinforcement learning, Graph convolution network, Smart manufacturing},
abstract = {Cloud manufacturing provides a cloud platform to offer on-demand services to complete consumers’ tasks, but assigning tasks to enterprises with different services requires many-to-many scheduling. The dynamic cloud environment puts forward higher requirements on scheduling algorithms’ real-time response and generalizability. Additionally, complex manufacturing tasks with flexible processing sequences also increase the decision-making difficulty. The existing approaches either have difficulty meeting the requirements of dynamics and fast-respond or struggle to effectively capture features of tasks with flexible processing sequences. To address these limitations, we develop a novel scheduling algorithm to solve a dynamic scheduling problem in the group service cloud manufacturing environment. Our proposal is formulated and trained by multi-agent reinforcement learning. The graph convolution network encodes tasks’ graph-structure features, and the recurrent neural network records each task’s processing trajectories. We independently design the action space and the reward function and train the algorithm with a mixing network under the centralized training decentralized execution architecture. Multi-agent reinforcement learning and graph convolution networks are rarely used to cloud manufacturing scheduling problems. Contrast experiments on a case study indicate that our proposal outperforms the other six multi-agent reinforcement learning-based scheduling algorithms in terms of scheduling performance and generalizability.}
}
@article{WANG2023100471,
title = {Logistics-involved task scheduling in cloud manufacturing with offline deep reinforcement learning},
journal = {Journal of Industrial Information Integration},
volume = {34},
pages = {100471},
year = {2023},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2023.100471},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X23000444},
author = {Xiaohan Wang and Lin Zhang and Yongkui Liu and Chun Zhao},
keywords = {Cloud manufacturing, Offline reinforcement learning, Scheduling problems, Decision transformer, Attention mechanism},
abstract = {As an application of industrial information integration engineering (IIIE) in manufacturing, cloud manufacturing (CMfg) integrates enterprises’ manufacturing information and provides an open and sharing platform for processing manufacturing tasks with distributed manufacturing services. Assigning tasks to manufacturing enterprises in the CMfg platform calls for effective scheduling algorithms. In recent years, deep reinforcement learning (DRL) has been widely applied to tackle cloud manufacturing scheduling problems (CMfg-SPs) because of its high generalization and fast-responding capability. However, the current DRL algorithms need to be trial-and-error through online interaction with the environment, which is costly and not allowed in the real CMfg platform. This paper proposes a novel offline DRL scheduling algorithm that alleviates the online trial-and-error issue while retaining DRL’s original advantages. First, we describe the system model of CMfg-SPs and propose the sequential Markov decision process modeling strategy, where all tasks are regarded as one agent. Then, we introduce the framework of the decision transformer (DT), which converts the online scheduling decision-making problem into an offline classification problem. Finally, we construct an attention-based model as the agent’s policy and train it offline under the DT architecture. Experimental results indicate that the proposed method consistently matches or exceeds online DRL algorithms, including deep double q-network (DDQN), deep recurrent q-network (DRQN), proximal policy optimization (PPO), and the offline learning algorithm behavior cloning (BC) in terms of scheduling performance and model generalization.}
}
@article{YU2022118291,
title = {User-guided motion planning with reinforcement learning for human-robot collaboration in smart manufacturing},
journal = {Expert Systems with Applications},
volume = {209},
pages = {118291},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118291},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422014270},
author = {Tian Yu and Qing Chang},
keywords = {Human-robot collaboration, Learning from demonstration, Motion planning, Reinforcement learning},
abstract = {In today’s manufacturing system, robots are expected to perform increasingly complex manipulation tasks in collaboration with humans. However, current industrial robots are still largely preprogrammed with very little autonomy and still required to be reprogramed by robotics experts for even slightly changed tasks. Therefore, it is highly desirable that robots can adapt to certain task changes with motion planning strategies to easily work with non-robotic experts in manufacturing environments. In this paper, we propose a user-guided motion planning algorithm in combination with reinforcement learning (RL) method to enable robots automatically generate their motion plans for new tasks by learning from a few kinesthetic human demonstrations. Features of common human demonstrated tasks in a specific application environment, e.g., desk assembly or warehouse loading/unloading are abstracted and saved in a library. The definition of semantical similarity between features in the library and features of a new task is proposed and further used to construct the reward function in RL. To achieve an adaptive motion plan facing task changes or new task requirements, features embedded in the library are mapped to appropriate task segments based on the trained motion planning policy using Q-learning. A new task can be either learned as a combination of a few features in the library or a requirement for further human demonstration if the current library is insufficient for the new task. We evaluate our approach on a 6 DOF UR5e robot on multiple tasks and scenarios and show the effectiveness of our method with respect to different scenarios.}
}
@article{DWORSCHAK2022101612,
title = {Reinforcement Learning for Engineering Design Automation},
journal = {Advanced Engineering Informatics},
volume = {52},
pages = {101612},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101612},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622000787},
author = {Fabian Dworschak and Sebastian Dietze and Maximilian Wittmann and Benjamin Schleich and Sandro Wartzack},
keywords = {Reinforcement Learning, Design Automation, Deep Q-learning},
abstract = {Reinforcement Learning has proven to be capable of solving complex tasks like playing video games, robotics control, speech or image recognition and processing. Transferring Reinforcement Learning into engineering design helps to overcome two current issues of data-driven Design Automation in engineering design. First, dealing with sparse training data resulting from differing design samples. Second, overcoming the limited number of samples in the training data as consequence of short or insufficient product history. To introduce an alternative approach for Design Automation, this contribution studies feasibility, training effort and transferability of Reinforcement Learning in engineering design. The presented method maps engineering requirements and parametric models into learning environments and provides a novel approach for design automation. In addition to that, the contribution summarises the hyperparameters, which design engineers have to set prior to training, and introduces a novel transfer learning concept for Reinforcement Learning in related design tasks. The support is probed by design tasks of performance-oriented bike parts. Case-independent indicators are presented to estimate the case-specific training effort, the effects of hyperparameter variation and the effects of transferring a pretrained agent to related design tasks. Finally, the findings are used to compare Reinforcement Learning to other data-independent Design Automation approaches to assess potential fields of application for Reinforcement Learning in engineering design.}
}
@article{LENG2021124405,
title = {A loosely-coupled deep reinforcement learning approach for order acceptance decision of mass-individualized printed circuit board manufacturing in industry 4.0},
journal = {Journal of Cleaner Production},
volume = {280},
pages = {124405},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.124405},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620344504},
author = {Jiewu Leng and Guolei Ruan and Yuan Song and Qiang Liu and Yingbin Fu and Kai Ding and Xin Chen},
keywords = {Sustainable manufacturing, Industry 4.0, Order acceptance decision, Deep learning, Reinforcement learning, Big data},
abstract = {Printed Circuit Board (PCB) manufacturing is a kind of energy-intensive and pollution-intensive industries. With the increment of individualized requirements, PCB manufacturers face massive customized orders with a variety of specifications. The individualized customization on orders results in large differentials of the profit, energy consumption, and environmental pollution. Making energy-efficient order acceptance decisions can reduce carbon consumption and improve material utilization during the whole manufacturing process. An order acceptance decision model is established based on a loosely-coupled integration of deep learning and reinforcement learning techniques. Firstly, different from a simple assumption of the linear cost function in a small-scale manufacturing system, the deep learning algorithm is presented for accurately predicting the production cost, makespan, and carbon consumption of incoming PCB orders in the large-scale manufacturing system. Secondly, these predicted cleaner production indicators are combined with original order features to perform a reinforcement learning-based order acceptance decision. The proposed loosely-coupled deep reinforcement learning approach is verified with a dataset built based on data collected from a PCB manufacturer in China. This research is expected to provide an environment-friendly order acceptance decision-making approach for sustainable manufacturing in the Industry 4.0 context.}
}
@article{PETRIK202375,
title = {Reinforcement learning and optimization based path planning for thin-walled structures in wire arc additive manufacturing},
journal = {Journal of Manufacturing Processes},
volume = {93},
pages = {75-89},
year = {2023},
issn = {1526-6125},
doi = {https://doi.org/10.1016/j.jmapro.2023.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S1526612523002219},
author = {Jan Petrik and Markus Bambach},
keywords = {Path planning, Wire arc additive manufacturing, Reinforcement learning},
abstract = {A well-designed deposition path is one of the basic prerequisites for the successful fabrication of a component by deposition based additive manufacturing processes. Three main approaches are currently used to determine the deposition path. First, these are general path templates that are applied to the entire geometry. Nevertheless, this approach suffers from poor adaptability to the geometry. Second, they are algorithms where it is necessary to divide the geometry into sub-parts, which are then filled either by general path templates or by paths derived, e.g., from the signed distance function. These often require human intervention and may fail to find a suitable deposition path. Third, there are planning strategies that deal only with a particular topologies, and are not transferable to other geometries. A developed path planning framework named RLPlanner, which makes use of reinforcement learning as well as automatized prepossessing and Sequential Least Squares Programming optimization method, addresses these drawbacks. This solution enables fully automatic deposition path planning for thin-walled structures in wire arc additive manufacturing. In addition, the framework is able to vary the welding speed with the wire feed rate and thus influence the size of the weld bead leading to better adaptability to the geometry.}
}
@article{GU2022103863,
title = {Integrated eco-driving automation of intelligent vehicles in multi-lane scenario via model-accelerated reinforcement learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {144},
pages = {103863},
year = {2022},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103863},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22002790},
author = {Ziqing Gu and Yuming Yin and Shengbo Eben Li and Jingliang Duan and Fawang Zhang and Sifa Zheng and Ruigang Yang},
keywords = {Integrated eco-driving automation, Intelligent vehicle, Reinforcement learning, Model-accelerated},
abstract = {The development of intelligent driving technologies is expected to have the potential in energy economics. Some reported studies mainly focused on the economical driving performance in cruising, following, or ramping scenarios, where longitudinal control is primarily considered. The impact of lateral decisions on economical performance is rarely discussed, especially in traffic flows. In the multi-lane scenario, the upper decision-making module could output reasonable behavior selections to avoid the limitation of single longitudinal control and further enhance the energy-saving potential in traffic flows, such as the appropriate lane-keeping or lane-changing proposal. Furthermore, designing comprehensive rules to coordinate diverse driving goals with separated decision-making and control modules is challenging. Therefore, this paper proposes an integrated decision and control framework for economical driving in the multi-lane scenario, based on the actor–critic reinforcement learning method. The proposed integrated framework contains two function layers: a static-evaluating layer and a dynamic-tracking layer. The former, i.e., the critic network, considers static information, evaluates potentially feasible lanes, and selects an advantage lane as the lane-changing proposal. The latter, i.e., the actor network, obtains dynamic traffic information and solves a constrained control problem. Finally, the solution aims to achieve obstacle avoidance and economical and stable tracking to the proposed advantage lane as far as possible. Furthermore, a model-accelerated soft actor–critic (MSAC) algorithm is developed to simultaneously solve the integrated decision and control problem. Simulation results show that the proposed learning-based integrated method could achieve economical driving and significantly outperform baselines in accumulated performance, energy efficiency, and driving comfort.}
}
@article{CHIEN2021107782,
title = {Agent-based approach integrating deep reinforcement learning and hybrid genetic algorithm for dynamic scheduling for Industry 3.5 smart production},
journal = {Computers & Industrial Engineering},
volume = {162},
pages = {107782},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107782},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221006860},
author = {Chen-Fu Chien and Yu-Bin Lan},
keywords = {Deep reinforcement learning, Dynamic scheduling, Hybrid genetic algorithm, Semiconductor manufacturing, Industry 3.5},
abstract = {Dynamic scheduling is crucial for semiconductor manufacturing as product-mix is increasing with shortening product life cycle. However, the present problem is challenging owing to complicated constraints and short time for decision making. Focusing on realistic needs, this research aims to develop a novel agent-based approach that integrates deep reinforcement learning and hybrid genetic algorithm for the unrelated parallel machine scheduling problem with sequence-dependent setup time. In particular, deep Q network (DQN), a combination of deep learning and Q learning, is employed to train a scheduling agent. A trained agent could perform job allocation tasks in short computation time for addressing the dynamic scheduling problem. Furthermore, the proposed hybrid genetic algorithm is employed to enhance searching effectiveness and efficiency during the training process. To estimate the validity, scenarios are designed to compare the developed solution with a number of dispatching rules and other knowledge-based approaches. The experimental results have shown practical viability of the developed solution. Indeed, the developed solution is implemented in a semiconductor manufacturing company.}
}
@article{OVERBECK2021170,
title = {Reinforcement Learning Based Production Control of Semi-automated Manufacturing Systems},
journal = {Procedia CIRP},
volume = {103},
pages = {170-175},
year = {2021},
note = {9th CIRP Global Web Conference – Sustainable, resilient, and agile manufacturing and service operations : Lessons from COVID-19},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.10.027},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121008684},
author = {Leonard Overbeck and Adrien Hugues and Marvin Carl May and Andreas Kuhnle and Gisela Lanza},
keywords = {Machine Learning, Reinforcement Learning, Digital Twin, Production Control, Task Allocation, Productivity},
abstract = {In an environment which is marked by an increasing speed of changes, industrial companies have to be able to quickly adapt to new market demands and innovative technologies. This leads to a need for continuous adaption of existing production systems and the optimization of their production control. To tackle this problem digitalization of production systems has become essential for new and existing systems. Digital twins based on simulations of real production systems allow the simplification of analysis processes and, thus, a better understanding of the systems, which leads to broad optimization possibilities. In parallel, machine learning methods can be integrated to process the numerical data and discover new production control strategies. In this work, these two methods are combined to derive a production control logic in a semi-automated production system based on the chaku-chaku principle. A reinforcement learning method is integrated into the digital twin to autonomously learn a superior production control logic for the distribution of tasks between the different workers on a production line. By analyzing the influence of different reward shaping and hyper-parameter optimization on the quality and stability of the results obtained, the use of a well-configured policy-based algorithm enables an efficient management of the workers and the deduction of an optimal production control logic for the production system. The algorithm manages to define a control logic that leads to an increase in productivity while having a stable task assignment so that a transfer to daily business is possible. The approach is validated in the digital twin of a real assembly line of an automotive supplier. The results obtained suggest a new approach to optimizing production control in production lines. Production control shall be centered directly on the workers’ routines and controlled by artificial intelligence infused with a global overview of the entire production system.}
}
@article{KUHNLE2019234,
title = {Design, Implementation and Evaluation of Reinforcement Learning for an Adaptive Order Dispatching in Job Shop Manufacturing Systems},
journal = {Procedia CIRP},
volume = {81},
pages = {234-239},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.041},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119303464},
author = {Andreas Kuhnle and Louis Schäfer and Nicole Stricker and Gisela Lanza},
keywords = {Reinforcement Learning, Production Scheduling, Order Dispatching, Methodical Approach},
abstract = {Modern production systems tend to have smaller batch sizes, a larger product variety and more complex material flow systems. Since a human oftentimes can no longer act in a sufficient manner as a decision maker under these circumstances, the demand for efficient and adaptive control systems is rising. This paper introduces a methodical approach as well as guideline for the design, implementation and evaluation of Reinforcement Learning (RL) algorithms for an adaptive order dispatching. Thereby, it addresses production engineers willing to apply RL. Moreover, a real-world use case shows the successful application of the method and remarkable results supporting real-time decision-making. These findings comprehensively illustrate and extend the knowledge on RL.}
}
@article{PARASCHOS2020470,
title = {Reinforcement learning for combined production-maintenance and quality control of a manufacturing system with deterioration failures},
journal = {Journal of Manufacturing Systems},
volume = {56},
pages = {470-483},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S027861252030114X},
author = {Panagiotis D. Paraschos and Georgios K. Koulinas and Dimitrios E. Koulouriotis},
keywords = {Deteriorating systems, Machine learning, Reinforcement learning, Control policies, Inventory control, Quality control},
abstract = {This paper describes and examines thoroughly a stochastic production/inventory system that produces a single type of products. During the production process, the system is affected by several deterioration failures. It is restored to its initial and previous deterioration state by repair and maintenance activities. Both maintenance and repair duration are assumed as exponential random variables. Moreover, the quality of the manufactured products is assumed to be affected by the current deterioration level of the system. The aim of this paper is to find the optimal trade-off between conflicting performance metrics for the optimization of the total expected profit of the system. To tackle such optimization problems, researchers frequently employ Dynamic Programming. This method, though, is not appropriate for the addressed problem due to complexity reasons. To this end, a Reinforcement Learning-based approach is proposed in order to obtain the optimal joint production, maintenance and product quality control policies. To the authors’ knowledge, the proposed approach is novel and there are few examples of such implementation in the academic literature.}
}
@article{LI2022104957,
title = {A flexible manufacturing assembly system with deep reinforcement learning},
journal = {Control Engineering Practice},
volume = {118},
pages = {104957},
year = {2022},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2021.104957},
url = {https://www.sciencedirect.com/science/article/pii/S0967066121002343},
author = {Junzheng Li and Dong Pang and Yu Zheng and Xinping Guan and Xinyi Le},
keywords = {Reinforcement learning, Digital twin, Flexible manufacture, Assembly line},
abstract = {Traditional assembly line requires a significant amount of designs from engineers, especially in the case of multi-species and small-lot production. Recently, intelligent algorithms based on reinforcement learning are proposed to address this issue. However, the lower success rate and safety reasons limit their industrial applications. In this article, we proposed a systematic solution, including the automatic planning of assembly motions and the monitoring system of the production lines. In the planning stage, we built the digital twin model of the assembly line, then trained a deep reinforcement learning agent to assembly the workpieces. In the production stage, the digital twin model is used to monitor the assembly lines and predict failures. To validate the system we proposed, we conducted a peg-in-hole assembly experiment, and reached a 90% success rate for a single assembly attempt. During the whole experiment, no collision happens in the real world.}
}
@article{VIHAROS2021109616,
title = {Reinforcement Learning for Statistical Process Control in Manufacturing},
journal = {Measurement},
volume = {182},
pages = {109616},
year = {2021},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2021.109616},
url = {https://www.sciencedirect.com/science/article/pii/S0263224121005881},
author = {Zsolt J. Viharos and Richárd Jakab},
keywords = {Statistical Process Control, Optimal Control, Reinforcement Learning, Production Trends, Disturbance Handling},
abstract = {The main concept of the authors is to place Reinforcement Learning (RL) into various fields of manufacturing. As one of the first implementations, RL for Statistical Process Control (SPC) in production is introduced in the paper; it is a promising approach owing to its adaptability and the continuous ability to perform. The widely used Q-Table method was applied for get more stable, predictable, and easy to overview results. Therefore, quantization of the values of the time series to stripes inside the control chart was introduced. Detailed elements of the production environment simulation are described and its interaction with the reinforcement learning agent are detailed. Beyond the working concept for adapting RL into SPC in manufacturing, some novel RL extensions are also described, like the epsilon self-control of exploration–exploitation ratio, Reusing Window (RW) and the Measurement Window (MW). In the production related transformation, the main aim of the agent is to optimize the production cost while keeping the ratio of good products on a high level as well. Finally, industrial testing and validation is described that proved the applicability of the proposed concept.}
}
@article{ZHANG2022102412,
title = {Dynamic job shop scheduling based on deep reinforcement learning for multi-agent manufacturing systems},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {78},
pages = {102412},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102412},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000977},
author = {Yi Zhang and Haihua Zhu and Dunbing Tang and Tong Zhou and Yong Gui},
keywords = {Flexible job-shop scheduling problem, Smart manufacturing, Multi-agent manufacturing system, Reinforcement learning, Proximal policy optimization},
abstract = {Personalized orders bring challenges to the production paradigm, and there is an urgent need for the dynamic responsiveness and self-adjustment ability of the workshop. Traditional dispatching rules and heuristic algorithms solve the production planning and control problems by making schedules. However, the previous methods cannot work well in a changeable workshop environment when encountering a large number of stochastic disturbances of orders and resources. Recently, the potential of artificial intelligence (AI) algorithms in solving the dynamic scheduling problem has attracted researchers' attention. Therefore, this paper presents a multi-agent manufacturing system based on deep reinforcement learning (DRL), which integrates the self-organization mechanism and self-learning strategy. Firstly, the manufacturing equipment in the workshop is constructed as an equipment agent with the support of edge computing node, and an improved contract network protocol (CNP) is applied to guide the cooperation and competition among multiple agents, so as to complete personalized orders efficiently. Secondly, a multi-layer perceptron is employed to establish the decision-making module called AI scheduler inside the equipment agent. According to the perceived workshop state information, AI scheduler intelligently generates an optimal production strategy to perform task allocation. Then, based on the collected sample trajectories of scheduling process, AI scheduler is periodically trained and updated through the proximal policy optimization (PPO) algorithm to improve its decision-making performance. Finally, in the multi-agent manufacturing system testbed, dynamic events such as stochastic job insertions and unpredictable machine failures are considered in the verification experiments. The experimental results show that the proposed method is capable of obtaining the scheduling solutions that meet various performance metrics, as well as dealing with resource or task disturbances efficiently and autonomously.}
}
@article{MATULIS2021106,
title = {A robot arm digital twin utilising reinforcement learning},
journal = {Computers & Graphics},
volume = {95},
pages = {106-114},
year = {2021},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S009784932100011X},
author = {Marius Matulis and Carlo Harvey},
keywords = {Robot arm, Reinforcement learning, Artificial intelligence, Digital twin},
abstract = {For many industry contexts, the implementation of Artificial Intelligence (AI) has contributed to what has become known as the fourth industrial revolution or “Industry 4.0” and creates an opportunity to deliver significant benefit to both businesses and their stakeholders. Robot arms are one of the most common devices utilised in manufacturing and industrial processes, used for a wide variety of automation tasks on, for example, a factory floor but the effective use of these devices requires AI to be appropriately trained. One approach to support AI training of these devices is the use of a “Digital Twin”. There are, however, a number of challenges that exist within this domain, in particular, success depends upon the ability to collect data of what are considered as observations within the environment and the application of one or many trained AI policies to the task that is to be completed. This project presents a case-study of creating and training a Robot Arm Digital Twin as an approach for AI training in a virtual space and applying this simulation learning within physical space. A virtual space, created using Unity (a contemporary Game Engine), incorporating a virtual robot arm was linked to a physical space, being a 3D printed replica of the virtual space and robot arm. These linked environments were applied to solve a task and provide training for an AI model. The contribution of this work is to provide guidance on training protocols for a digital twin together with details of the necessary architecture to support effective simulation in a virtual space through the use of Tensorflow and hyperparameter tuning. It provides an approach to addressing the mapping of learning in the virtual domain to the physical robot twin.}
}
@article{LEE2022101443,
title = {Data science and reinforcement learning for price forecasting and raw material procurement in petrochemical industry},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101443},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101443},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001956},
author = {Chia-Yen Lee and Bai-Jian Chou and Chen-Feng Huang},
keywords = {Raw material procurement, Price forecasting, Deep learning, Reinforcement learning, Digital transformation},
abstract = {Petrochemical industry is one of the major sectors contributing to the world-wide economy and the digital transformation is urgent to enhance core competence. In general, ethylene, propylene and butadiene, which are associated with synthetic chemicals, are the main raw materials of this industry with around 70–80% cost structure. In particular, butadiene is one of the key materials for producing synthetic rubber and used for several daily commodities. However, the price of butadiene fluctuates along with the demand–supply mismatch or by the international economy and political events. This study proposes two-stage data science framework to predict the weekly price of butadiene and optimize the procurement decision. The first stage suggests several the price prediction models with a comprehensive information including contract price, supply rate, demand rate, and upstream and downstream information. The second stage applies the analytic hierarchy process and reinforcement learning technique to derive an optimal policy of procurement decision and reduce the total procurement cost. An empirical study is conducted to validate the proposed framework, and the results improve the accuracy of price forecasts and the procurement cost reduction of the raw materials.}
}
@article{LU2020115473,
title = {Multi-agent deep reinforcement learning based demand response for discrete manufacturing systems energy management},
journal = {Applied Energy},
volume = {276},
pages = {115473},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.115473},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920309855},
author = {Renzhi Lu and Yi-Chang Li and Yuting Li and Junhui Jiang and Yuemin Ding},
keywords = {Artificial intelligence, Deep reinforcement learning, Demand response, Industrial energy management, Discrete manufacturing system},
abstract = {With advances in smart grid technologies, demand response has played a major role in improving the reliability of grids and reduce the cost for customers. Implementing the demand response scheme for industry is more necessary than for other sectors, because its energy consumption is often considered the largest. This paper proposes a multi-agent deep reinforcement learning based demand response scheme for energy management of discrete manufacturing systems. In this regard, the industrial manufacturing system is initially formulated as a partially-observable Markov game; then, a multi-agent deep deterministic policy gradient algorithm is adopted to obtain the optimal schedule for different machines. A typical lithium-ion battery assembly manufacturing system is used to demonstrate the effectiveness of the proposed scheme. Simulation results show that the presented demand response algorithm can minimize electricity costs and maintain production tasks, as compared to a benchmark without demand response. Moreover, the performance of the multi-agent deep reinforcement learning approach against a mathematical model method is investigated.}
}
@article{ALJAAFREH201753,
title = {Agitation and mixing processes automation using current sensing and reinforcement learning},
journal = {Journal of Food Engineering},
volume = {203},
pages = {53-57},
year = {2017},
issn = {0260-8774},
doi = {https://doi.org/10.1016/j.jfoodeng.2017.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0260877417300407},
author = {Ahmad Aljaafreh},
keywords = {Agitation, Mixing, Machine learning, Reinforcement learning},
abstract = {Many agitation and mixing processes utilize various sensors for real-time monitoring and control, which can involve complex and costly equipment. For many mixing and agitation processes, such as in dough making, as mixing energy is placed, the resistance to extension increases and then after some point it decreases again. High-quality bread is obtained by stopping mixing at or close to the maximum resistance. The change in resistance causes a change in motor torque. The torque change affects the motor’s current draw for agitation and mixing machines driven by electrical motors. The rheological characteristics of the mixed material are related to motor torque of the mixing machine. Therefore, it is related to the motor electric current where the load variation can be estimated by a low-cost current sensor. This paper outlines a novel design for an intelligent agitator/mixer process controller. The design is based on current sensing and on-line learning through reinforcement learning using operator input. The system provides a low-cost approach to automate various kinds of production equipment currently operated manually, which are common in the developing world. Additionally, the approach requires minimal modification to the equipment: it requires only a current sensor, an on/off control relay, a set of buttons for operation, and an embedded system.}
}
@article{KONISHI202216,
title = {Efficient Safe Control via Deep Reinforcement Learning and Supervisory Control – Case Study on Multi-Robot Warehouse Automation},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {28},
pages = {16-21},
year = {2022},
note = {16th IFAC Workshop on Discrete Event Systems WODES 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.318},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322023552},
author = {Masahiro Konishi and Tomotake Sasaki and Kai Cai},
keywords = {Safe Control, Supervisory Control, Deep Reinforcement Learning},
abstract = {Safe control has recently attracted much attention due to its applications in safety-critical cyber-physical systems. Supervisory control theory (SCT) is a formal control method that provides correct-by-construction safety certificates, but is computationally inefficient when the number of system components is large. On the other hand, deep reinforcement learning (DRL) provides a toolbox of efficient algorithms to compute control decisions even for very large state space, but does not always guarantee safety. In this paper, we propose to synergize SCT and DRL into a new efficient safe control approach. Specifically, we first employ DRL algorithms to efficiently compute sub-optimal solutions which may be unsafe; then we convert the obtained solutions into a standard supervisory control problem with an automaton (plant model) and a set of unsafe states (safety specification); finally we use SCT to synthesize a supervisor with a safety certificate. A case study of multi-robot warehouse logistic automation is conducted to demonstrate the efficiency of this proposed approach.}
}
@article{GEURTSEN2023170,
title = {Deep reinforcement learning for optimal planning of assembly line maintenance},
journal = {Journal of Manufacturing Systems},
volume = {69},
pages = {170-188},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523000845},
author = {M. Geurtsen and I. Adan and Z. Atan},
keywords = {Scheduling, Maintenance, Deep reinforcement learning, Simulation, Case-study, Flexibility},
abstract = {Discovering the optimal maintenance planning strategy can have a substantial impact on production efficiency, yet this aspect is often overlooked in favor of production planning. This is a missed opportunity as maintenance and production activities are deeply intertwined. Our study sheds light on the significance of maintenance planning, particularly in the dynamic setting of an assembly line. By maximizing the average production rate and incorporating flexible planning windows, buffer content, and machine production states, a unique problem is addressed in which a policy for planning maintenance on the final machine of a serial assembly line is developed. To achieve this, novel average-reward deep reinforcement learning techniques are employed and pitted against generic dispatching methods. Using a digital twin with real-world data, experiments demonstrate the immense potential of this new deep reinforcement learning technique, producing policies that outperform generic dispatching strategies and practitioner policies.}
}
@article{KARIGIANNIS2022909,
title = {Reinforcement Learning Enabled Self-Homing of Industrial Robotic Manipulators in Manufacturing},
journal = {Manufacturing Letters},
volume = {33},
pages = {909-918},
year = {2022},
note = {50th SME North American Manufacturing Research Conference (NAMRC 50,2022)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2022.07.111},
url = {https://www.sciencedirect.com/science/article/pii/S2213846322001420},
author = {John N. Karigiannis and Philippe Laurin and Shaopeng Liu and Viktor Holovashchenko and Antoine Lizotte and Vincent Roux and Philippe Boulet},
keywords = {reinforcement learning, self-homing, parallel-agent, industrial robotic manipulator},
abstract = {Industrial robotics has been playing a major role in manufacturing across all types of industries. One common task of robotic cells in manufacturing is called homing, a step that enables a robotic arm to return to its initial / home position (HPos) from anywhere in a robotic cell, without collision or experiencing robot singularities while respecting its joint limits. In almost all industrial robotic cells, an operation cycle starts from, and ends to HPos. The home position also works as a safe state for a cycle to restart when an alarm or fault occurs within the cell. When an alarm occurs, the robot configuration in the cell is unpredictable, thus challenging to bring the robot, autonomously and with safety at HPos and restart the operation. This paper presents a non-vision, reinforcement learning-based approach of a parallel-agent setting to enable self-homing capability in industrial robotic cells, eliminating the need of manual programming of robot manipulators. This approach assumes the sensing of an unknown robotic cell environment pre-encoded in the state definition so that the policies learned can be transferred without further training. The agents are trained in a simulation environment generated by the mechanical design of an actual robotic cell to increase the accuracy of mapping the real environment to the simulated one. The approach explores the impact of certain curriculum on the agent’s learning and evaluates two choices, compared to a non-curriculum baseline. A parallel-agent, multi-process training setting is employed to enhance performance in exploring the state space, where experiences are shared among the agents via shared memory. Upon deployment, all agents are involved with their respective policies in a collective manner. The approach has been demonstrated in simulated industrial robotic cells, and it has been shown that the policies derived in simulation are transferable to a corresponding real industrial robotic cells, and are generalizable to other robotic systems in manufacturing settings.}
}
@article{HUANG2022101800,
title = {Reward shaping in multiagent reinforcement learning for self-organizing systems in assembly tasks},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101800},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101800},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002580},
author = {Bingling Huang and Yan Jin},
keywords = {Multiagent systems, Assembly task, Reinforcement learning, Design, Reward shaping, Collision avoidance},
abstract = {Self-organizing systems feature flexibility and robustness for tasks that may endure changes over time. Various methods, e.g., applying task-field and social-field, have been proposed to capture the complexity of task environments so that agents can remain simple. To expand to complex task domains, the multiagent reinforcement learning (MARL) approach has been taken to train agent teams to be more capable and intelligent, permitting reduced complexity in task descriptions. MARL depends on the design of reward functions, which has been a challenging endeavour thus far. This paper investigates the impact of reward shaping in the context of an “L-shape” assembly task that involves collision avoidance. After introducing a general form of reward shaping function, various types of reward shaping fields are studied empirically with agent teams of different sizes. The experiment results have shown that reward shaping can be highly effective, and the singularities, the proper forms of the fields, and the suitable shaping field gradients are essential for successful agent team training. Furthermore, the effect of reward shaping functions depends highly on the size of agent teams.}
}
@article{LEFRANC202341,
title = {Multirobot Allocation In A Flexible Manufacturing System, Using Reinforcement Learning For Decision-Making, Case of Study},
journal = {Procedia Computer Science},
volume = {221},
pages = {41-48},
year = {2023},
note = {Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923007020},
author = {Gastón Lefranc},
keywords = {Multirobots, Allocation, FMS, Reinforcement Learning},
abstract = {This paper presents a reinforcement learning approach for decision-making in assigning multiple robots in a flexible manufacturing system. The algorithm allows for learning the optimal action policy for assigning tasks to multiple robots, outperforming other methods. The case study involved three robots and four tasks, some requiring cooperation. The study demonstrates the effectiveness of the approach in comparison to other methods and the results indicate that the reinforcement learning approach increases production efficiency and reduces task completion time compared to traditional assignment methods.}
}
@article{TONER20231019,
title = {Opportunities and challenges in applying reinforcement learning to robotic manipulation: An industrial case study},
journal = {Manufacturing Letters},
volume = {35},
pages = {1019-1030},
year = {2023},
note = {51st SME North American Manufacturing Research Conference (NAMRC 51)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2023.08.055},
url = {https://www.sciencedirect.com/science/article/pii/S2213846323001128},
author = {Tyler Toner and Miguel Saez and Dawn M. Tilbury and Kira Barton},
keywords = {Reinforcement learning, Industrial robotics, Smart manufacturing, Wire harness installation},
abstract = {As moves towards a more agile paradigm, industrial robots are expected to perform more complex tasks in less structured environments, complicating the use of traditional automation techniques and leading to growing interest in data-driven methods, like reinforcement learning (RL). In this work, we explore the process of applying RL to enable automation of a challenging industrial manipulation task. We focus on wire harness installation as a motivating example, which presents challenges for traditional automation due to the nonlinear dynamics of the deformable harness. A physical system was developed involving a three-terminal harness manipulated by a 6-DOF UR5 robot, with control enabled through a ROS interface. Modifications were made to the harness to enable simplified grasping and marker-based visual tracking. We detail the development of an RL formulation of the problem, subject to practical constraints on control and sensing motivated by the physical system. We develop a simulator and a basic scripted policy with which to safely generate a data-set of high-quality behaviors, then apply a state-of-the-art model-free offline RL algorithm, TD3 + BC, to learn a policy to serve as a safe starting point on the physical system. Despite extensive tuning, we find that the algorithm fails to achieve acceptable performance. We propose three failure modalities to explain the learning performance, related to control frequency, task symmetry arising from problem simplifications, and unexpected policy complexity, and discuss opportunities for future applications.}
}
@article{BERUVIDES2018601,
title = {Fault pattern identification in multi-stage assembly processes with non-ideal sheet-metal parts based on reinforcement learning architecture},
journal = {Procedia CIRP},
volume = {67},
pages = {601-606},
year = {2018},
note = {11th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 19-21 July 2017, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.12.268},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117312143},
author = {Gerardo Beruvides and Alberto Villalonga and Pasquale Franciosa and Darek Ceglarek and Rodolfo E. Haber},
keywords = {Reinfocemnt learning-based architecture, Default pattern identification, Artifical neural network models, Multi-stage assembly systems},
abstract = {A reinforcement learning-based architecture to address the fault detection on body in white assembly processes is introduced in this paper. During the research were addressed: (i) generation of a random defect pattern database using a multi-physics variation modeling for multi-stage assembly systems; (ii) design and implementation of a fault pattern identification reinforcement learning-based architecture, combining neural network, genetic algorithm and Q-learning algorithms; and (iii) validation based on non-ideal sheet-metal parts case study generated by the Variation Response Method toolkit. Finally, a comparative study between the different topologies is done, highlighting the influence of the Q-learning in the default identification process.}
}
@article{PRASAD2023851,
title = {Recovery systems architecture for cyber-manufacturing systems against cyber-manufacturing attacks: Reinforcement learning approach},
journal = {Manufacturing Letters},
volume = {35},
pages = {851-860},
year = {2023},
note = {51st SME North American Manufacturing Research Conference (NAMRC 51)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2023.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S221384632300038X},
author = {Romesh Prasad and Seyed Alireza {Zarrin Mehr} and Young Moon},
keywords = {Cyber-manufacturing systems, Industry 4.0, Recovery architecture, Cyber-attacks, Deep reinforcement learning, Unity3D},
abstract = {Cyber-manufacturing attacks are cyber-attacks that identify a vulnerability in cyber entities and exploit it to impact manufacturing systems. The cyber entities inside manufacturing systems consist of operational and information technology. The number of cyber-manufacturing attacks continues to rise every year by taking advantage of weaknesses in cyber entities. The widespread impact of these attacks has attracted researchers towards developing solutions against cyber-manufacturing attacks. Additionally, it is recognized that some of the cyber-manufacturing attacks cannot be prevented, and solely adopting prevention measures are not sufficient. On the other hand, detection measures do not guarantee the functioning of the cyber-manufacturing systems after the cyber-manufacturing attacks. Hence, to ensure the attacked manufacturing systems are achieving operational goals a systematic recovery measure should be implemented. To implement a successful recovery measure an architecture is required. Thus, this work proposes a four-layer recovery architecture. The architecture consists of a systems layer, attack identification layer, data auditing and detection layer, and reinforcement learning based recovery layer. This work explains the advantages of implementing a reinforcement learning based recovery layer compared to the state of the art recovery methods. The implementation of the recovery architecture is demonstrated by conducting a simulation of the conveyor system constructed inside the Unity3D physics platform.}
}
@article{JEONG20211807,
title = {A reinforcement learning model for material handling task assignment and route planning in dynamic production logistics environment},
journal = {Procedia CIRP},
volume = {104},
pages = {1807-1812},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.305},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121012038},
author = {Yongkuk Jeong and Tarun Kumar Agrawal and Erik Flores-García and Magnus Wiktorsson},
keywords = {Type your keywords here, separated by semicolons},
abstract = {The study analyzes the application of reinforcement learning (RL) for material handling tasks in Smart Production Logistics (SPL). It presents two contributions based on empirical results of a RL model in dynamic production logistics environment from the automotive industry. Firstly, an architecture integrating the use of RL in SPL. Secondly, the study defines various elements of RL (environment, value, state, reward, and policy) relevant for training and validating models in SPL. The study provides novel insight essential for manufacturing managers and extends current understanding related to research combining artificial intelligence and SPL, granting manufacturing companies a unique competitive advantage.}
}
@article{CHEN2023109053,
title = {Cloud–edge collaboration task scheduling in cloud manufacturing: An attention-based deep reinforcement learning approach},
journal = {Computers & Industrial Engineering},
volume = {177},
pages = {109053},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109053},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223000773},
author = {Zhen Chen and Lin Zhang and Xiaohan Wang and Kunyu Wang},
keywords = {Cloud manufacturing, Scheduling, Cloud–edge, DRL, Attention},
abstract = {Cloud Manufacturing (CMfg), as a service-oriented manufacturing mode, aims to provide consumers on-demand manufacturing services. The CMfg platform requires task scheduling technology to schedule manufacturing tasks efficiently, and improve resource utilization and customer satisfaction. Existing scheduling models for manufacturing tasks mainly consider maximizing the quality of service for customers but ignore the actual production execution, which will lead to low-quality execution or delayed delivery. To maximize customer satisfaction and balance production, this article studies a cloud–edge collaboration manufacturing task scheduling in CMfg (CETS). CETS refines manufacturing services deployed in the cloud to the factory process level, and schedules tasks according to the real-time production information on the edge side and manufacturing service information on the cloud side. Considering the dynamics of CETS and the complexity of state information in CETS, an attention-based deep reinforcement learning (DRL) algorithm is proposed to solve CETS. First, the CETS is mathematically represented and built as a partially observable Markov decision process. Second, on-policy maximum a posteriori policy optimization (V-MPO) with gated transformer-XL (GTrXL) named AV-MPO is developed. The effectiveness, training stability, generalizability, scalability, and robustness of AV-MPO are investigated. Rule-based algorithms and some state of art DRL algorithms, such as proximal policy optimization (PPO), soft actor-critic (SAC), and dueling deep q network (Dueling DQN), are compared with AV-MPO. The experimental results validate that AV-MPO can deal with the CETS problem more effectively.}
}
@article{SU2022116323,
title = {Deep multi-agent reinforcement learning for multi-level preventive maintenance in manufacturing systems},
journal = {Expert Systems with Applications},
volume = {192},
pages = {116323},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116323},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421016249},
author = {Jianyu Su and Jing Huang and Stephen Adams and Qing Chang and Peter A. Beling},
keywords = {Multi-level preventive maintenance, Deep multi-agent reinforcement learning, Deep reinforcement learning, Serial production line, Production loss},
abstract = {Designing preventive maintenance (PM) policies that ensure smooth and efficient production for large-scale manufacturing systems is non-trivial. Recent model-free reinforcement learning (RL) methods shed lights on how to cope with the non-linearity and stochasticity in such complex systems. However, the action space explosion impedes RL-based PM policies to be generalized to real applications. In order to obtain cost efficient PM policies for a serial production line that has multiple levels of PM actions, a novel multi-agent modeling is adopted to support adaptive learning by modeling each machine as cooperative agent. The evaluation of system-level production loss is leveraged to construct the reward function. An adaptive learning framework based on value-decomposition multi-agent actor–critic algorithm is utilized to obtain PM policies. In simulation study, the proposed framework demonstrates its effectiveness by leading other baselines on a comprehensive set of metrics whereas the centralized RL-based methods struggles to converge to stable policies. Our analysis further demonstrates that our multi-agent reinforcement learning based method learns effective PM policies without any knowledge about the environment and maintenance strategies.}
}
@article{ZHANG2022359,
title = {Reinforcement learning and digital twin-based real-time scheduling method in intelligent manufacturing systems},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {359-364},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.413},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322016986},
author = {Lixiang Zhang and Yan Yan and Yaoguang Hu and Weibo Ren},
keywords = {Real-time scheduling, reinforcement learning, digital twin, intelligent manufacturing},
abstract = {Optimization efficiency and decision-making responsiveness are two conflicting objectives to be considered in intelligent manufacturing. Therefore, we proposed a reinforcement learning and digital twin-based real-time scheduling method, called twins learning, to satisfy multiple objectives simultaneously. First, the interaction of multiple resources is constructed in a virtual twin, including physics, behaviors, and rules to support the decision-making. Then, the real-time scheduling problems are modeled as Markov Decision Process and reinforcement learning algorithms are developed to learn better scheduling policies. The case study indicates the proposed method has excellent adaptability and learning capacity in intelligent manufacturing.}
}
@article{WANG2022108371,
title = {An adaptive artificial bee colony with reinforcement learning for distributed three-stage assembly scheduling with maintenance},
journal = {Applied Soft Computing},
volume = {117},
pages = {108371},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.108371},
url = {https://www.sciencedirect.com/science/article/pii/S156849462101142X},
author = {Jing Wang and Deming Lei and Jingcao Cai},
keywords = {Assembly scheduling problem, Distributed scheduling, Artificial bee colony, Q-learning, Maintenance},
abstract = {Distributed three-stage assembly scheduling problem extensively exists in the real-life assembly production process and is seldom considered. The integration of reinforcement learning with meta-heuristic can effectively improve the performance of meta-heuristic and effectively solve the problem; however, the integration is seldom used to cope with the problem. In this study, distributed three-stage assembly scheduling problem with DPm→1 layout and maintenance at three stages is considered and a mathematical model is provided. A new artificial bee colony with Q-learning (QABC) is proposed to minimize maximum tardiness. An effective Q-learning algorithm is implemented to dynamically select search operator, which consists of 12 states based on population quality evaluation, 8 actions defined by global search and neighborhood search, a new reward and an effective action selection. Two employed bee swarms are formed, an adaptive communication and an adaptive competition process between them are adopted to intensify exploration ability and improve search efficiency. QABC and its four comparative algorithms are tested on 80 instances. The computational results demonstrate that the new strategies of QABC really improve its search performance and QABC is a competitive algorithm for the considered problem.}
}
@article{ERIKSSON2022955,
title = {Conceptual framework of scheduling applying discrete event simulation as an environment for deep reinforcement learning},
journal = {Procedia CIRP},
volume = {107},
pages = {955-960},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.091},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122003754},
author = {Kristina Eriksson and Sudha Ramasamy and Xiaoxiao Zhang and Zhiping Wang and Fredrik Danielsson},
keywords = {Reinforcement learning, Discrete event simulation, Energy optimal scheduling, Inverse scheduling, Industry 4.0},
abstract = {Increased environmental awareness is driving the manufacturing industry towards novel ways of energy reduction to become sustainable yet stay competitive. Climate and environmental challenges put high priority on incorporating aspects of sustainability into both strategic and operational levels, such as production scheduling, in the manufacturing industry. Considering energy as a parameter when planning makes an already existing highly complex task of production scheduling even more multifaceted. The focus in this study is on inverse scheduling, defined as the problem of finding the number of jobs and duration times to meet a fixed input capacity. The purpose of this study was to investigate how scheduling can be formulated, within the environment of discrete event simulation coupled with reinforcement learning, to meet production demands while simultaneously minimize makespan and reduce energy. The study applied the method of modeling a production robot cell with its uncertainties, using discrete event simulation combined with deep reinforcement learning and trained agents. The researched scheduling approach derived solutions that take into consideration the performance measures of energy use. The method was applied and tested in a simulation environment with data from a real robot production cell. The study revealed opportunities for novel approaches of studying and reducing energy in the manufacturing industry. Results demonstrated a move towards a more holistic approach for production scheduling, which includes energy usage, that can aid decision-making and facilitate increased sustainability in production. We propose a conceptual framework for scheduling for minimizing energy use applying discrete event simulation as an environment for deep reinforcement learning.}
}
@article{CHO2022104620,
title = {Reinforcement learning-based simulation and automation for tower crane 3D lift planning},
journal = {Automation in Construction},
volume = {144},
pages = {104620},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104620},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522004903},
author = {SungHwan Cho and SangUk Han},
keywords = {Tower crane, Motion planning, Path planning, 3D dynamic simulation, Reinforcement learning algorithms: on-policy and off-policy, Reward function},
abstract = {Tower crane lift planning is important to timely provide resources to workplaces. However, previous planning approaches are still impractical because the lifting time of a plan is barely considered and the lifting path is frequently non-executable by operators. This paper describes a reinforcement learning-based method that incorporates the actuator system of a tower crane into spatio-temporal lift planning in three-dimensional virtual environments wherein various strategies of algorithm types and learning rules are tested. The results show stable and practical lift planning with a failure ratio of 3%, coordination ratio of 28%, and positive evaluation of lifting procedures by expert operators. In addition, the estimated lifting time shows a correlation of 0.6857 with the actual time from field observation. Thus, the proposed approach is promising for planning feasible lifting paths and estimating reasonable lifting times, which help generate and review lifting plans given the site conditions.}
}
@article{JAENSCH2022103,
title = {Test-Driven Reward Function for Reinforcement Learning: A Contribution towards Applicable Machine Learning Algorithms for Production Systems},
journal = {Procedia CIRP},
volume = {112},
pages = {103-108},
year = {2022},
note = {15th CIRP Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.09.043},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122011970},
author = {Florian Jaensch and Karl Kübler and Elmar Schwarz and Alexander Verl},
keywords = {Virtual Commissioning Simulation, Test-Driven Development, Reinforcement Learning},
abstract = {Reinforcement Learning algorithms find more and more application in fields where complex tasks need to be solved. The automation of production systems is one of those fields. Normally, programming a control system defines the automation strategy. Previous contributions by the authors have shown that a so-called agent can learn automation strategies for production systems using a Reinforcement Learning setup. However, the development of the reward function for the agent can be challenging and needs Reinforcement Learning domain knowledge. This paper introduces a novel approach in combining Test-Driven Development with Reinforcement Learning in order to solve the problem of a suitable reward function. In the presented approach predefined test cases are used to derive rewards for the agent. The use of an automated test framework allows for continuous learning sequences until all test cases are passed. An application example of a robot cell is used to demonstrate the novel approach and verify its suitability and usability. The first application shows promising results for further examination towards more application scenarios.}
}
@article{LI201992,
title = {Robot skill acquisition in assembly process using deep reinforcement learning},
journal = {Neurocomputing},
volume = {345},
pages = {92-102},
year = {2019},
note = {Deep Learning for Intelligent Sensing, Decision-Making and Control},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.01.087},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219301316},
author = {Fengming Li and Qi Jiang and Sisi Zhang and Meng Wei and Rui Song},
keywords = {Reinforcement learning, Robot assembly, Skill acquisition, Pose adjustment},
abstract = {Uncertain factors in environments restrict the intelligence level of industrial robots. Based on deep reinforcement learning, a skill-acquisition method is used to solve the posed problems of uncertainty in a complex assembly process. Under the frame of the Markov decision process, a quaternion sequence of the assembly process is represented. The reward function uses a trained classification model, which mainly recognizes whether the assembly is successful. The proposed skill-acquisition method is designed to make robots acquire assembly skills. The input of the model is the contact state of the assembly process, and the output is the robot action. The robot can complete the assembly by self-learning with little prior knowledge. To evaluate the performance of the proposed skill-acquisition method, simulations and real-world experiments were performed in a low-voltage apparatus assembly. The assembly success rate increases with the learning time. In the case of a random initial position and orientation, the assembly success rate was greater than 80% with little prior knowledge. The results show that the robot has a capability to complex assembly through skill acquisition.}
}
@article{WANG2022452,
title = {Solving task scheduling problems in cloud manufacturing via attention mechanism and deep reinforcement learning},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {452-468},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001418},
author = {Xiaohan Wang and Lin Zhang and Yongkui Liu and Chun Zhao and Kunyu Wang},
keywords = {Scheduling problem, Deep reinforcement learning, Transformer, Multi-head attention, Cloud manufacturing},
abstract = {Cloud manufacturing (CMfg) offers a cloud platform for both consumers and providers, and allocating consumer tasks to service providers requires many-to-many scheduling. Deep reinforcement learning (DRL) has been gradually employed to solve CMfg scheduling problems and has achieved satisfactory performance in dynamic and uncertain cloud environments. Nonetheless, we need better scheduling algorithms and more accessible modeling methods to enable practical implementation. In this study, an end-to-end scheduling algorithm is proposed to address task scheduling problems in CMfg. The proposal extracts intercorrelations in enterprise–enterprise and enterprise–task with the multi-head attention mechanism and is trained by DRL. Our proposal has extremely low time-response compared to heuristic algorithms and can generate a scheduling solution in seconds. In contrast to other DRL algorithms, our proposal can achieve better scheduling performances and uses a more accessible modeling method: the objective function alone is sufficient to train the model stably, without the need for a step-based reward function. Applying multi-head attention and DRL to scheduling problems is an exploratory attempt to achieve positive results. Experimental results on a case of automobile structure part processing in CMfg indicated that our proposal showed competitive scheduling performances and running time compared to eight DRL algorithms, two heuristic algorithms, and two priority dispatching rules. Besides, the results proved that our proposal’s generalizability and scalability were better than the other eight DRL algorithms.}
}
@article{TAO2022108714,
title = {A differential evolution with reinforcement learning for multi-objective assembly line feeding problem},
journal = {Computers & Industrial Engineering},
volume = {174},
pages = {108714},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108714},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222007021},
author = {Lue Tao and Yun Dong and Weihua Chen and Yang Yang and Lijie Su and Qingxin Guo and Gongshu Wang},
keywords = {Assembly line feeding problem, Multi-objective optimization, Differential evolution algorithm, Reinforcement learning},
abstract = {This paper studies a multi-objective assembly line feeding problem (MALFP), which is a new variant of the assembly line feeding problem in automobile manufacturers. In this problem, part families are delivered through five feeding policies to minimize three objectives simultaneously. To describe the problem, a novel multi-objective mathematical model is formulated. It not only overcomes the difficulty of determining perfect weights for objectives without prior knowledge, but also complements the traditional model by considering extended decisions on receiving warehouses, an extra cost item for policy switching, and a hybrid inventory strategy. To solve the problem, an innovative multi-objective differential evolution with a reinforcement learning (RL) based operator selection mechanism (MODE-RLOSM) is proposed. By solving MALFP with MODE-RLOSM, near-optimal candidate solutions that are suitable for different working conditions are provided to managers for making trade-offs and implementations. Compared with state-of-the-art optimization algorithms as well as a practical decision tree approach, the proposed algorithm shows superiority in cost saving, solution quality, and convergence efficiency. Through ablation study, sensitivity analysis, and RL behavior analysis, we investigate components in MODE-RLOSM and verify their effectiveness and robustness. In addition to bringing significant cost savings, the obtained solution also gives us production enlightenment and thus improves the decision-making efficiency of the enterprise. In our research, we illustrate the influence of part diversity on policy selection, give managers suggestions under different objective preferences, and find it uneconomical to pursue a specific objective excessively.}
}
@article{CHEN2023101900,
title = {Reinforcement learning-based distant supervision relation extraction for fault diagnosis knowledge graph construction under industry 4.0},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101900},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101900},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000289},
author = {Chong Chen and Tao Wang and Yu Zheng and Ying Liu and Haojia Xie and Jianfeng Deng and Lianglun Cheng},
keywords = {Distant supervision relation extraction, Knowledge graph, Fault diagnosis, Reinforcement Learning},
abstract = {Fault diagnosis is the key concern in the operation and maintenance of industrial assets. A fault diagnosis knowledge graph (KG) can provide decision support to the engineers to efficiently conduct maintenance tasks. However, as a type of domain KG, it would be time-consuming to manually label the corpus collected from the multi-source including the maintenance log, handbook and article. Meanwhile, the existence of the noisy sentence in the multi-source corpus jeopardises the performance of relation extraction modelling. In order to address this issue, this paper proposes a distant supervision relation extraction (DSRE)-based approach to construct a fault diagnosis KG. In this approach, the ontology of the fault diagnosis KG is firstly designed. Subsequently, a DSRE algorithm named relation-aware-based sentence-level attention enhanced piecewise convolutional neural network with reinforcement learning strategy (PCNN-ATTRA-RL) is proposed. The algorithm can effectively lower the impact of noisy sentences and accurately label the relation of different entities when the labelled data is insufficient. In this algorithm, PCNN-ATTRA is designed as the DSRE classifier to effectively extract the relation between entity pairs. RL is conducted to remove the noisy sentence so as to further improve the performance. An experimental study based on the multi-source corpus collected from the real world reveals that the proposed approach shows merits in comparison with the state-of-the-art algorithms. Meanwhile, a fault diagnosis KG, which can greatly support the decision-making of the engineers in the fault diagnosis, is established via the proposed approach.}
}
@article{YE2023109290,
title = {Joint optimization of maintenance and quality inspection for manufacturing networks based on deep reinforcement learning},
journal = {Reliability Engineering & System Safety},
volume = {236},
pages = {109290},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109290},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023002053},
author = {Zhenggeng Ye and Zhiqiang Cai and Hui Yang and Shubin Si and Fuli Zhou},
keywords = {Manufacturing network, Reliability model, Maintenance, Quality inspection, Optimization, Deep reinforcement learning},
abstract = {Most existing studies on joint optimization of manufacturing systems (MS) focus on small-scale systems with simple structures, such as the single-machine, simple serial, or parallel MS. Simultaneously, traditional algorithms utilized in small-scale MS always show an insufficiency in solving large-scale dynamic MS with complex structures, such as manufacturing networks. Therefore, considering the effectiveness of reinforcement learning on the infinite-horizon Markov Decision Process (MDP), this paper presents a joint optimization problem of preventive maintenance and work-in-process quality inspection for manufacturing networks with reliability-quality interactions. First, dynamic reliability and quality models are proposed at the machine level to cope with complex interactions in manufacturing networks. Second, based on the MDP-based optimization model, the proposed Deep Deterministic Policy Gradient (DDPG) algorithm realizes the optimal reliability-quality joint control in manufacturing networks. Besides, it also offers a novel mixed action space containing discrete maintenance and continuous quality inspection, which could satisfy the action diversity in actual production. At last, training and experiments imply our algorithm is more adaptable to diverse manufacturing scenarios than traditional ones. Also, it is proved that more-frequent state observations for learning cannot help the constructed reinforcement learning model get a better control policy because of the information redundancy.}
}
@article{TANG20221198,
title = {Reconfigurable manufacturing system scheduling: a deep reinforcement learning approach},
journal = {Procedia CIRP},
volume = {107},
pages = {1198-1203},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.131},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122004152},
author = {Jiecheng Tang and Yousef Haddad and Konstantinos Salonitis},
keywords = {Reconfigurable Manufacturing System, Multi-agent System, Deep Reinforcement Learning, Flexible Job-shop Scheduling Problem},
abstract = {Reconfigurable Manufacturing Systems (RMS) bring new possibilities toward meeting demand fluctuations while, at the same time, challenges scheduling efficiency. This paper presents a novel approach that, for the scheduling problem of RMS on multiple products, finds a dynamic control policy via a group of deep reinforcement learning agents. These teamed agents, embedded with a shared value decomposition network, aim on minimising the make-span of a constant updating order group by guiding a group of automated guided vehicles to move modules of machine, raw materials, and finished products inside the system.}
}
@article{RAZIEI2021546,
title = {Enabling adaptable Industry 4.0 automation with a modular deep reinforcement learning framework},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {546-551},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.168},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321009381},
author = {Zohreh Raziei and Mohsen Moghaddam},
keywords = {Industry 4.0, Collaborative robots, Deep-reinforcement learning, Actor-critic method, Modularization, Transfer learning, Meta-World},
abstract = {Industry 4.0 envisions adaptable and resilient manufacturing and logistics operations capable of handling dynamic changes or deviations in operations using intelligent sensing and computation technologies. Recent advances in artificial intelligence and collaborative robotics have created unprecedented opportunities to fully automate a variety of industrial tasks such as material handling, assembly, machine tending, and inspection, among other. With the rapidly growing interest in the vision of lot-size-of-one, a fundamental and challenging question remains open: How can robots leverage the knowledge of previously-learned tasks to expedite their learning on new tasks? We tackle this problem by developing and testing a novel deep reinforcement learning framework with task modularization to enhance adaptability of collaborative robots in performing a multitude of simulated tasks. The framework is built upon the actor-critic method and the notion of task modularity, and is compared against the Soft Actor-Critic (SAC) algorithm as a baseline. Numerical experiments on the Meta-World dataset prove the ability of the proposed framework in improving the adaptability and efficiency of collaborative robots to new tasks through task modularization and transfer of policies from previously-learned task modules.}
}
@article{CHE2023121332,
title = {A deep reinforcement learning based multi-objective optimization for the scheduling of oxygen production system in integrated iron and steel plants},
journal = {Applied Energy},
volume = {345},
pages = {121332},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121332},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923006967},
author = {Gelegen Che and Yanyan Zhang and Lixin Tang and Shengnan Zhao},
keywords = {Oxygen production system, Production scheduling, Multi-objective optimization, Deep reinforcement learning, Evolutionary algorithm},
abstract = {The oxygen production system in integrated iron and steel plants is a highly energy-intensive sector producing gaseous oxygen for the manufacturing processes. This study investigates an oxygen production system with cryogenic air separation units (ASU), vaporizers, and liquefiers under frequent demand-changing scenarios and various electricity price contracts. A multi-objective optimization model is established to minimize the total operating cost to reduce energy consumption and simultaneously minimize switching times of operating modes to pursue operational stability. The proposed model not only schedules operation modes of ASUs and on/off modes of vaporizers and liquefiers but also determines the production level of these units. To solve the problem, a multi-objective evolutionary algorithm (MOEA) is proposed, in which the proximal policy optimization, one of deep reinforcement learning (DRL) methods, is incorporated to adaptively select the mating individuals and determine the crossover ratio at each evolutionary iteration. Besides, a surrogate model for optimizing the total operating cost is presented to accelerate the training process of the proposed deep reinforcement learning based multi-objective evolutionary algorithm (DRL-MOEA). The performance of the proposed algorithms is demonstrated using practical instances, and experiment results show that it saves the total operating cost up to 0.86% and decreases the operating modes switches as much as 14.41%, compared to MOEA. The model offers the on-site manager solutions to coordinate the supply and demand with a good trade-off between reducing overall energy consumption and pursuing streamlined operating conditions.}
}
@article{LIU2023102454,
title = {Scheduling of decentralized robot services in cloud manufacturing with deep reinforcement learning},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {80},
pages = {102454},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102454},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001363},
author = {Yongkui Liu and Yaoyao Ping and Lin Zhang and Lihui Wang and Xun Xu},
keywords = {Cloud manufacturing, Scheduling, Robot service, Deep reinforcement learning, Dueling DQN},
abstract = {Cloud manufacturing is a service-oriented manufacturing model that offers manufacturing resources as cloud services. Robots are an important type of manufacturing resources. In cloud manufacturng, large-scale distributed robots are encapsulated into cloud services and provided to consumers in an on-demand manner. How to effectively and efficiently manage and schedule decentralized robot services in cloud manufacturing to achieve on-demand provisioning is a challenging issue. During the past few years, Deep Reinforcement Learning (DRL) has become very popular and successfully been applied to many different areas such as games, robotics, and manufacturing. DRL also holds tremendous potential for solving scheduling issues in cloud manufacturing. To this end, this paper is devoted to exploring effective approaches for scheduling of decentralized robot manufacturing services in cloud manufacturing with DRL. Specifically, both Deep Q-Networks (DQN) and Dueling Deep Q-Networks (DDQN)-based scheduling algorithms are proposed. Performance of different algorithms, including DQN, DDQN, and other three benchmark algorithms, indicates that DDQN performs the best with respect to each indicator. Effects of different combinations of weight coefficients and influencing degrees of different indicators on the overall scheduling objective are analyzed. Results indicate that the DDQN-based scheduling algorithm is able to generate scheduling solutions efficiently.}
}
@article{ZHENG202116,
title = {Towards Self-X cognitive manufacturing network: An industrial knowledge graph-based multi-agent reinforcement learning approach},
journal = {Journal of Manufacturing Systems},
volume = {61},
pages = {16-26},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001643},
author = {Pai Zheng and Liqiao Xia and Chengxi Li and Xinyu Li and Bufan Liu},
keywords = {Industrial knowledge graph, Graph embedding, Cognitive manufacturing, Graph neural network, Reinforcement learning},
abstract = {Empowered by the advanced cognitive computing, industrial Internet-of-Things, and data analytics techniques, today’s smart manufacturing systems are ever-increasingly equipped with cognitive capabilities, towards an emerging Self-X cognitive manufacturing network with higher level of automation. Nevertheless, to our best knowledge, the readiness of ‘Self-X’ levels (e.g., self-configuration, self-optimization, and self-adjust/adaptive/healing) is still in the infant stage. To pave its way, this work stepwise introduces an industrial knowledge graph (IKG)-based multi-agent reinforcement learning (MARL) method for achieving the Self-X cognitive manufacturing network. Firstly, an IKG should be formulated based on the extracted empirical knowledge and recognized patterns in the manufacturing process, by exploiting the massive human-generated and machine-sensed multimodal data. Then, a proposed graph neural network-based embedding algorithm can be performed based on a comprehensive understanding of the established IKG, to achieve semantic-based self-configurable solution searching and task decomposition. Moreover, a MARL-enabled decentralized system is presented to self-optimize the manufacturing process, and to further complement the IKG towards Self-X cognitive manufacturing network. An illustrative example of multi-robot reaching task is conducted lastly to validate the feasibility of the proposed approach. As an explorative study, limitations and future perspectives are also highlighted to attract more open discussions and in-depth research for ever smarter manufacturing.}
}
@article{BURGGRAF20221095,
title = {Reinforcement Learning for Process Time Optimization in an Assembly Process Utilizing an Industry 4.0 Demonstration Cell},
journal = {Procedia CIRP},
volume = {107},
pages = {1095-1100},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.114},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122003985},
author = {Peter Burggräf and Fabian Steinberg and Benjamin Heinbach and Milan Bamberg},
keywords = {Reinforcement Learning, process time optimization, Production},
abstract = {The process time of a production process is an important result of planning in supply networks, which in turn is a defining parameter, significant for further organizational decisions. Optimizing it requires extensive knowledge of the underlying processes and parameters involved. It is imperative to reduce process time while also ensuring the quality of the products to stay competitive in an ever-evolving environment. This paper demonstrates a solution for a reinforcement learning (RL) application to optimize the process time of an assembly case. Using an actual industry 4.0 demonstration cell as a hands-on, model-free simulation environment, an RL Agent interacts with an OPC UA interface to gather machine sensor data and control the machine drives. Using Q-learning, an online off-policy algorithm, with a discretized action space we achieve self-optimization of the assembly case by decreasing process time while simultaneously ensuring that the quality of the products stays within tolerable parameters. Our findings demonstrate the usefulness of RL applications in process control, in this case optimizing machine parameters. As an addition, we deduce design guidelines from this model and its implementation to help reduce possible sources of error while implementing similar approaches for industrial applications.}
}
@article{APOLINARSKA2021103569,
title = {Robotic assembly of timber joints using reinforcement learning},
journal = {Automation in Construction},
volume = {125},
pages = {103569},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103569},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521000200},
author = {Aleksandra Anna Apolinarska and Matteo Pacher and Hui Li and Nicholas Cote and Rafael Pastrana and Fabio Gramazio and Matthias Kohler},
keywords = {Robotic assembly, Reinforcement learning, Timber construction, Timber joints},
abstract = {In architectural construction, automated robotic assembly is challenging due to occurring tolerances, small series production and complex contact situations, especially in assembly of elements with form-closure such as timber structures with integral joints. This paper proposes to apply Reinforcement Learning to control robot movements in contact-rich and tolerance-prone assembly tasks and presents the first successful demonstration of this approach in the context of architectural construction. Exemplified by assembly of lap joints for custom timber frames, robot movements are guided by force/torque and pose data to insert a timber element in its mating counterpart(s). Using an adapted Ape-X DDPG algorithm, the control policy is trained entirely in simulation and successfully deployed in reality. The experiments show the policy can also generalize to situations in real world not seen in training, such as tolerances and shape variations. This caters to uncertainties occurring in construction processes and facilitates fabrication of differentiated, customized designs.}
}
@article{MUELLERZHANG2021408,
title = {Integrated Planning and Scheduling for Customized Production using Digital Twins and Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {408-413},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.046},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321007631},
author = {Zai Mueller-Zhang and Pablo {Oliveira Antonino} and Thomas Kuhn},
keywords = {Digital Twin, Reinforcement Learning, Deep-Q-Network, Integrated Planning, Scheduling},
abstract = {For customized production in small lot-sizes, traditional production plants have to be reconfigured manually multiple times to be adapted to variable order changes, what significantly increases the production costs. One of the goals of Industry 4.0 is to enable flexible production, allowing for customer-specific production or even production with lot size 1 in order to react dynamically to changes in production orders. All of this with increased quality parameters such as optimized use of machines, conveyor belts and raw materials, which ultimately leads to optimized resource utilization and cost-efficiency. To address this challenge, in this paper, we present a digital twin based self-learning process planning approach using Deep-Q-Network that is capable of identifying optimized process plans and workflows for the simultaneous production of personalized products. We have evaluated our approach on a virtual aluminum cold milling factory from the SMS Group, in the context of the BaSys 4 project. The goal of the evaluation was to provide evidence that the proposed approach is able to handle large problem space effectively. Our approach ensures the efficiency of the personalized production and the adaptivity of the production system.}
}
@article{QI2022107989,
title = {A synergistic reinforcement learning-based framework design in driving automation},
journal = {Computers and Electrical Engineering},
volume = {101},
pages = {107989},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107989},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622002580},
author = {Yuqiong Qi and Yang Hu and Haibin Wu and Shen Li and Xiaochun Ye and Dongrui Fan},
keywords = {Autonomous Driving, Heterogeneous Multicore AI Accelerator, Criteria, Reinforcement Learning, Scheduling},
abstract = {Autonomous driving, which integrates artificial intelligence and the Internet of Things, has piqued the interest of both academics and industry because of its economic and societal benefits. Rigorous accuracy and latency requirements are important for autonomous driving safety. In order to achieve high computation performance in driving automation system, we propose in this paper a heterogeneous multicore AI accelerator (HMAI). At the same time, on the HMAI, how to allocate a large number of real-time tasks to different accelerators remains a notable problem that is worth considering. Theoretically, this problem is NP-complete, and always solved using heuristic-based and guided random-search-based algorithms. However, the global state of HMAI cannot be considered comprehensively in these algorithms, which usually leads to suboptimal allocations. In this paper, we propose FlexAI, a predictive and global scheduling mechanism on HMAI. Specifically, the proposed scheduling algorithm that is based upon deep reinforcement learning (RL). In order to evaluate the quality of strategies produced by RL agent and update the observation of the scheduling agent, two scheduling metrics are proposed: Global State Value (Gvalue), Matching Score (MS) which pays attention to the requirements of various tasks in driving automation system like emergency level. In the experimental, FlexAI achieves up to 80% execution time reduction and 99% resource utilization improvement compared with Min-min, ATA in heuristics, and genetic algorithms, simulated annealing in guided random-search-based algorithms, and unscheduled case.}
}
@article{QU201655,
title = {Optimized Adaptive Scheduling of a Manufacturing Process System with Multi-skill Workforce and Multiple Machine Types: An Ontology-based, Multi-agent Reinforcement Learning Approach},
journal = {Procedia CIRP},
volume = {57},
pages = {55-60},
year = {2016},
note = {Factories of the Future in the digital environment - Proceedings of the 49th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116311647},
author = {Shuhui Qu and Jie Wang and Shivani Govil and James O. Leckie},
keywords = {scheduling, workforce management, reinforcement learning, multi-agent, multi-stage, multi-product, multi-skills, real-time information, advanced manufacturing, game theory},
abstract = {The impetus for an interconnected, efficient, and adaptive manufacturing system, as advocated by the Industry 4.0 revolution, together with the latest developments in information technology, advanced manufacturing has become a prominent research topic in recent years. One critical aspect of advanced manufacturing is how to incorporate real-time demand information with a manufacturer's resource information, including workforce data and machine capacity and condition information, among others, to optimally schedule manufacturing processes with multiple objectives. In general, optimized manufacturing scheduling is a non-deterministic polynomial-time hard problem. Due to the complexity, scheduling presents a number of challenges to find the best possible solutions. This research proposes an ontology-based framework to formally represent a synchronized, station-based flow shop with a multi-skill workforce and multiple types of machines. Based on the ontology, this research develops a multi-agent reinforcement learning approach for the optimal scheduling of a manufacturing system of multi-stage processes for multiple types of products with various machines and a multi-skilled workforce. By employing a learning algorithm, this approach enables real-time cooperation between the workforce and the machines, and adaptively updates production schedules according to dynamically changing real-time events.}
}
@article{HOPKEN202368,
title = {Delivery scheduling in meat industry using reinforcement learning},
journal = {Procedia CIRP},
volume = {118},
pages = {68-73},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123002354},
author = {Alica Höpken and Hergen Pargmann and Harald Schallner and André Galczynski and Lennard Gerdes},
keywords = {Manufacturing applications of artificial neural networks, reinforcement learning, meat industry, delivery scheduling, waste reduction},
abstract = {The reduction of food waste in the meat industry is an ongoing challenge. Therefore, as part of the German research project ”REIF - Resource Efficient, Economic and Intelligent Food Chain”, optimized planning algorithms were developed using artificial intelligence algorithms based on reinforcement learning. This approach proposes a solution for two daily planning tasks: (i) rescheduling the animal deliveries and (ii) balancing the animal qualities to sales orders qualities. A German food manufacturer has validated the prototype in terms of minimizing the amount of frozen meat not assigned to suitable customer requirements. Planning results show a significant reduction in food waste.}
}
@article{ZIMMERLING2022110423,
title = {Optimisation of manufacturing process parameters for variable component geometries using reinforcement learning},
journal = {Materials & Design},
volume = {214},
pages = {110423},
year = {2022},
issn = {0264-1275},
doi = {https://doi.org/10.1016/j.matdes.2022.110423},
url = {https://www.sciencedirect.com/science/article/pii/S0264127522000442},
author = {Clemens Zimmerling and Christian Poppe and Oliver Stein and Luise Kärger},
keywords = {Manufacturing optimisation, Composite forming, Machine learning, Artificial intelligence, Design for manufacture},
abstract = {Tailoring manufacturing processes to optimum part quality often requires numerous resource-intensive trial experiments in practice. Physics-based process simulations in combination with general-purpose optimisation algorithms allow for an a priori process optimisation and help concentrate costly trials on the most promising variants. However, considerable computation times are a significant barrier, especially for iterative optimisation. Surrogate-based optimisation often helps reduce the computational effort but surrogate models are typically case-specific and cannot adapt to different manufacturing situations. Consequently, even minor problem variations e.g. geometry adaptions invalidate the surrogate and require resampling of data and retraining of the surrogate. Reinforcement Learning aims at inferring optimal actions in variable situations. In this work, it is used to train a neural network to estimate optimal process parameters (“actions”) for variable component geometries (“situations”). The use case is fabric forming in which pressure pads are positioned to optimise the material intake. After training, the network is found to give meaningful parameter estimations even for new geometries not considered during training. Thus, it extracts reusable information from generic process samples and successfully applies it to new, non-generic components. Since data is reused rather than resampled, the approach is deemed a promising option for lean part and process development.}
}
@article{YING2023110190,
title = {Minimizing makespan in two-stage assembly additive manufacturing: A reinforcement learning iterated greedy algorithm},
journal = {Applied Soft Computing},
volume = {138},
pages = {110190},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110190},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623002089},
author = {Kuo-Ching Ying and Shih-Wei Lin},
keywords = {Scheduling, Two-stage assembly scheduling problems, Additive manufacturing, Reinforcement learning algorithm, Iterated epsilon-greedy algorithm},
abstract = {Additive manufacturing (AM) is becoming increasingly important for producing mass-customized, small-quantity products with relatively low geometric constraints. Although some AM machine scheduling problems have been proposed in recent years, no research has addressed the parallel AM machine scheduling problem with an integrated assembly stage. In this study, a two-stage assembly additive manufacturing scheduling problem is considered, in which multiple parts are produced in job batches using identical parallel AM machines in the first stage and then assembled into the desired products in the second stage. Further, a mixed-integer linear programming model and an innovative reinforcement learning metaheuristic, called the iterated epsilon-greedy algorithm, are proposed to minimize the makespan of this significant scheduling extension. The computational results based on 810 test instances show that the developed approaches are highly effective, efficient, and robust in solving the addressed problem. Notably, the research results can effectively reduce the gap between the theory and practice of AM production planning by integrating the production stage with the assembly stage.}
}
@article{WOO2021104226,
title = {Automation of load balancing for Gantt planning using reinforcement learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {101},
pages = {104226},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104226},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621000737},
author = {Jong Hun Woo and Byeongseop Kim and SuHeon Ju and Young In Cho},
keywords = {Shipbuilding, Production planning, Workload balancing, Reinforcement learning, Deep neural networks},
abstract = {Typically, in the shipbuilding industry, several vessels are built concurrently, and a production plan is established through a hierarchical planning process. This process largely comprises strategic planning (long-term) and master planning (mid-term) aspects. The portion that requires the most manual work of the planner is the load balancing in the master planning stage. The load balancing of master planning is an area where optimization studies using mixed integer programming, genetic algorithms, tabu search algorithms, and others have been actively conducted in the field of operational research. However, its practical application has not been successful due to the complexity and the curse of dimensionality, which is dependent on the manual work of the planner. Therefore, a new method that can facilitate the efficient action of optimal decisions is required, replacing conventional production planning methods based on the manual work of the planner. With the advent of the 4th industrial revolution in recent years, machine learning technology based on deep neural networks has been rapidly developing and applied to a wide range of engineering problems. This study introduces a methodology that can quickly improve the load balancing problem in shipyard master planning by using a deep neural network-based reinforcement learning algorithm among various machine learning techniques. Furthermore, we aim to verify the feasibility of the developed methodology using the ship block production data of an actual shipyard.}
}
@article{YANG2022118982,
title = {Joint control of manufacturing and onsite microgrid system via novel neural-network integrated reinforcement learning algorithms},
journal = {Applied Energy},
volume = {315},
pages = {118982},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.118982},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922003919},
author = {Jiaojiao Yang and Zeyi Sun and Wenqing Hu and Louis Steinmeister},
keywords = {Microgrid, Manufacturing, Reinforcement Learning, Markov Decision Process, Temporal Difference Learning, Deterministic Policy Gradient},
abstract = {Microgrid is a promising technology of distributed energy supply system, which consists of storage devices, generation capacities including renewable sources, and controllable loads. It has been widely investigated and applied for residential and commercial end-use customers as well as critical facilities. In this paper, we propose a joint state-based dynamic control model on microgrids and manufacturing systems where optimal controls for both sides are implemented to coordinate the energy demand and supply so that the overall production cost can be minimized considering the constraint of production target. Markov Decision Process (MDP) is used to formulate the decision-making procedure. The main computing challenge to solve the formulated MDP lies in the co-existence of both discrete and continuous parts of the high-dimensional state/action space that are intertwined with constraints. A novel reinforcement learning algorithm that leverages both Temporal Difference (TD) and Deterministic Policy Gradient (DPG) algorithms is proposed to address the computation challenge. Experiments for a manufacturing system with an onsite microgrid system with renewable sources have been implemented to justify the effectiveness of the proposed method.}
}
@article{LI202375,
title = {Deep reinforcement learning in smart manufacturing: A review and prospects},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {40},
pages = {75-101},
year = {2023},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2022.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1755581722001717},
author = {Chengxi Li and Pai Zheng and Yue Yin and Baicun Wang and Lihui Wang},
keywords = {Deep reinforcement learning, Smart manufacturing, Engineering life cycle, Artificial intelligence, Review},
abstract = {To facilitate the personalized smart manufacturing paradigm with cognitive automation capabilities, Deep Reinforcement Learning (DRL) has attracted ever-increasing attention by offering an adaptive and flexible solution. DRL takes the advantages of both Deep Neural Networks (DNN) and Reinforcement Learning (RL), by embracing the power of representation learning, to make precise and fast decisions when facing dynamic and complex situations. Ever since the first paper of DRL was published in 2013, its applications have sprung up across the manufacturing field with exponential publication growth year by year. However, there still lacks any comprehensive review of the DRL in the field of smart manufacturing. To fill this gap, a systematic review process was conducted, with 261 relevant publications selected to date (20-Oct-2022), to gain a holistic understanding of the development, application, and challenges of DRL in smart manufacturing along the whole engineering lifecycle. First, the concept and development of DRL are summarized. Then, the typical DRL applications are analyzed in the four engineering lifecycle stages: design, manufacturing, distribution, and maintenance. Finally, the challenges and future directions are illustrated, especially emerging DRL-related technologies and solutions that can improve the manufacturing system’s deployment feasibility, cognitive capability, and learning efficiency, respectively. It is expected that this work can provide an insightful guide to the research of DRL in the smart manufacturing field and shed light on its future perspectives.}
}
@article{DENG2023221,
title = {Offline reinforcement learning for industrial process control: A case study from steel industry},
journal = {Information Sciences},
volume = {632},
pages = {221-231},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523003158},
author = {Jifei Deng and Seppo Sierla and Jie Sun and Valeriy Vyatkin},
keywords = {Offline reinforcement learning, Deep ensemble, Industrial process control, Steel industry, Strip rolling},
abstract = {Flatness is a crucial indicator of strip quality that presents a challenge in regulation due to the high-speed process and the nonlinear relationship between flatness and process parameters. Conventional methods for controlling flatness are based on the first principles, empirical models, and predesigned rules, which are less adaptable to changing rolling conditions. To address this limitation, this paper proposed an offline reinforcement learning (RL) based data-driven method for flatness control. Based on the data collected from a factory, the offline RL method can learn the process dynamics from data to generate a control policy. Unlike online RL methods, the proposed method does not require a simulator for training, the policy can be potentially safer and more accurate since a simulator involves simplifications that can introduce bias. To obtain a steady performance, the proposed method incorporated ensemble Q-functions into policy evaluation to address uncertainty estimation. To address distributional shifts, based on Q-values from ensemble Q-functions, behavior cloning was added to policy improvement. Simulation and comparison results showed that the proposed method outperformed the state-of-the-art offline RL methods and achieved the best performance in producing strips with lower flatness.}
}
@article{SAMSONOV2022104868,
title = {Reinforcement Learning in Manufacturing Control: Baselines, challenges and ways forward},
journal = {Engineering Applications of Artificial Intelligence},
volume = {112},
pages = {104868},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.104868},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622001130},
author = {Vladimir Samsonov and Karim {Ben Hicham} and Tobias Meisen},
keywords = {Combinatorial optimization, Reinforcement Learning, Manufacturing Control, Job shop scheduling, Production simulation, Multi-Instance Ranked Reward},
abstract = {The field of Neural Combinatorial Optimization (NCO) offers multiple learning-based approaches to solve well-known combinatorial optimization tasks such as Traveling Salesman or Knapsack problem capable of competing with classical optimization approaches in terms of both solution quality and speed. This brought the attention of the research community to the tasks of Manufacturing Control (MC) with combinatorial nature. In this paper we outline the main components of MC tasks, select the most promising application fields and analyze dedicated learning-based solutions available in the literature. We draw multiple parallels to the current state of the art in the NCO field and allocate the main research gaps and directions on the perception, cognition and interaction levels. Using a set of practical examples we implement and benchmark common design patterns for single-agent Reinforcement Learning (RL) solutions. Along with testing existing solutions, we build on the ranked reward idea (Laterre et al., 2018) and offer a novel Multi-Instance Ranked Reward (m-R2) approach tailored to MC optimization tasks. It minimizes the reward shaping effort and defines a suitable training curriculum for more stable learning by separately tracking the agent’s performance on every scheduling task and rewarding only policies contributing towards better scheduling solutions. We implement all solution design patterns as a set of interchangeable modules with a shared API, unified in a benchmarking framework with the focus on standardization of training and evaluation processes, reproducibility and simplified experiment lifecycle management. In addition to the framework, we make available our discrete-event simulation of a job shop production.}
}