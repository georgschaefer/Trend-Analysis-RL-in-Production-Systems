"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Network Intrusion Detection System Using Reinforcement Learning Techniques","M. Malik; K. S. Saini","CSE, Chandigarh University, Kharar, India; CSE, Chandigarh University, Kharar, India","2023 International Conference on Circuit Power and Computing Technologies (ICCPCT)","22 Sep 2023","2023","","","1642","1649","Developing network intrusion detection systems (IDS) presents significant challenges due to the evolving nature of threats and the diverse range of network applications. Existing IDSs often struggle to detect dynamic attack patterns and covert attacks, leading to misidentified network vulnerabilities and degraded system performance. These requirements must be met via dependable, scalable, effective, and adaptable IDS designs. Our IDS can recognise and classify complex network threats by combining the Deep Q-Network (DQN) algorithm with distributed agents and attention techniques.. Our proposed distributed multi-agent IDS architecture has many advantages for guiding an all-encompassing security approach, including scalability, fault tolerance, and multi-view analysis. We conducted experiments using industry-standard datasets including NSL-KDD and CICIDS2017 to determine how well our model performed. The results show that our IDS outperforms others in terms of accuracy, precision, recall, F1-score, and false-positive rate. Additionally, we evaluated our model's resistance to black-box adversarial attacks, which are commonly used to take advantage of flaws in machine learning. Under these difficult circumstances, our model performed quite well.We used a denoising autoencoder (DAE) for further model strengthening to improve the IDS's robustness. Lastly, we evaluated the effectiveness of our zero-day defenses, which are designed to mitigate attacks exploiting unknown vulnerabilities. Through our research, we have developed an advanced IDS solution that addresses the limitations of traditional approaches. Our model demonstrates superior performance, robustness against adversarial attacks, and effective zero-day defenses. By combining deep reinforcement learning, distributed agents, attention techniques, and other enhancements, we provide a reliable and comprehensive solution for network security.","","979-8-3503-3324-4","10.1109/ICCPCT58313.2023.10245608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10245608","NSL-KDD;intrusion detection system;deep q learning;deep reinforcement learning;reinforcement learning;deep neural network","Resistance;System performance;Scalability;Noise reduction;Neural networks;Network intrusion detection;Reinforcement learning","computer crime;computer network security;deep learning (artificial intelligence);multi-agent systems;pattern classification;reinforcement learning","attention techniques;black-box adversarial attacks;complex network threats;covert attacks;DAE;Deep Q-Network algorithm;deep reinforcement learning;denoising autoencoder;distributed agents;DQN;dynamic attack patterns;machine learning;multiagent IDS architecture;network intrusion detection systems;network security;reinforcement learning techniques;zero-day defenses","","","","39","IEEE","22 Sep 2023","","","IEEE","IEEE Conferences"
"Enhancing IoT Security and Privacy through Blockchain Technology, Reinforcement Learning, and Constitutional AI","P. Ding; M. Sun; J. Liu; L. Li; H. Zhuang; B. Lin; Y. Fan","China Mobile Information Technology Company Limited, Shenzhen, China; China Mobile Information Technology Company Limited, Shenzhen, China; China Mobile Information Technology Company Limited, Shenzhen, China; China Mobile Information Technology Company Limited, Shenzhen, China; China Mobile Information Technology Company Limited, Shenzhen, China; China Mobile Information Technology Company Limited, Shenzhen, China; China Mobile Information Technology Company Limited, Shenzhen, China","2023 International Conference on Blockchain Technology and Information Security (ICBCTIS)","28 Aug 2023","2023","","","168","173","The rapid growth of the Internet of Things (IoT) has brought significant benefits to various industries, but it also poses new challenges in terms of security and privacy protection. In this paper, we propose a novel approach to enhance IoT security and privacy by combining blockchain technology with reinforcement learning algorithms and Constitutional Artificial Intelligence (CAI). This approach enables secure data storage and access control, adaptive data provenance and integrity, and adaptive identity management in IoT networks. We introduce Reinforcement Learning from Artificial Intelligence Feedback (RLAIF) and CAI to make the system adaptive and intelligent. RLAIF allows the system to learn from AI-generated feedback signals and make adjustments to its security mechanisms, while CAI ensures that the system adheres to the principles and guidelines set forth in a constitution, prioritizing user privacy and security. Experimental results demonstrate the effectiveness of the proposed method, paving the way for more secure and privacy-preserving IoT networks.","","979-8-3503-4040-2","10.1109/ICBCTIS59921.2023.00033","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10224696","Reinforcement Learning;Blockchain;Internet of Things;IoT;Data security;Data privacy;Smart contracts;Access control;Data traceability;Reinforcement Learning from Artificial Intelligence Feedback(RLAIF);Constitutional AI(CAI)","Training;Access control;Privacy;Adaptive systems;Memory;Reinforcement learning;Blockchains","authorisation;blockchains;computer network security;data protection;Internet of Things;reinforcement learning;storage management","access control;adaptive data integrity;adaptive data provenance;adaptive identity management;AI-generated feedback signals;blockchain technology;CAI;constitutional AI;constitutional artificial intelligence;data storage security;Internet of Things privacy;Internet of Things security;IoT privacy;IoT security;privacy protection;privacy-preserving IoT network;reinforcement learning from artificial intelligence feedback;RLAIF","","","","9","IEEE","28 Aug 2023","","","IEEE","IEEE Conferences"
"Quantum Multi-Agent Reinforcement Learning for Autonomous Mobility Cooperation","S. Park; J. P. Kim; C. Park; S. Jung; J. Kim",NA; NA; NA; NA; NA,"IEEE Communications Magazine","","2023","PP","99","1","7","For Industry 4.0 Revolution, cooperative autonomous mobility systems are widely used based on multiagent reinforcement learning (MARL). However, the MARLbased algorithms suffer from huge parameter utilization and convergence difficulties with many agents. To tackle these problems, a quantum MARL (QMARL) algorithm based on the concept of actor-critic network is proposed, which is beneficial in terms of scalability, to deal with the limitations in the noisy intermediatescale quantum (NISQ) era. Additionally, our QMARL is also beneficial in terms of efficient parameter utilization and fast convergence due to quantum supremacy. Note that the reward in our QMARL is defined as task precision over computation time in multiple agents, thus, multi-agent cooperation can be realized. For further improvement, an additional technique for scalability is proposed, which is called projection value measure (PVM). Based on PVM, our proposed QMARL can achieve the highest reward, by reducing the action dimension into a logarithmicscale. Finally, we can conclude that our proposed QMARL with PVM outperforms the other algorithms in terms of efficient parameter utilization, fast convergence, and scalability.","1558-1896","","10.1109/MCOM.020.2300199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10232949","","Artificial neural networks;Quantum computing;Qubit;Training;Reinforcement learning;Machine learning algorithms;Convergence","","","","","","","IEEE","28 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Deep Reinforcement Learning for End-to-End Network Slicing: Challenges and Solutions","Q. Liu; N. Choi; T. Han","University of Nebraska-Lincoln, USA; Nokia Bell Labs, USA; New Jersey Institute of Technology, USA","IEEE Network","5 Sep 2023","2023","37","2","222","228","5G and beyond is expected to enable various emerging use cases with diverse performance requirements from vertical industries. To serve these use cases cost-effectively, network slicing plays a key role in dynamically creating virtual end-to-end networks according to specific resource demands. A network slice may have hundreds of configurable parameters over multiple technical domains that define the performance of the network slice, which makes it impossible to use traditional model-based solutions to orchestrate resources for network slices. In this article, we discuss how to design and deploy deep reinforcement learning (DRL), a model-free approach, to address the network slicing problem. First, we analyze the network slicing problem and present a standard-compliant system architecture that enables DRL-based solutions in 5G and beyond networks. Second, we provide an in-depth analysis of the challenges in designing and deploying DRL in network slicing systems. Third, we explore multiple promising techniques, that is, safety and distributed DRL, and imitation learning, for automating end-to-end network slicing.","1558-156X","","10.1109/MNET.113.2100739","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9846944","","Network slicing;Neural networks;Systems architecture;Service level agreements;Markov processes;Base stations;Servers","5G mobile communication;deep learning (artificial intelligence);reinforcement learning;resource allocation;telecommunication computing","automating end-to-end network slicing;deep reinforcement learning;network slice;network slicing problem;network slicing systems;traditional model-based solutions;virtual end-to-end networks","","","","15","IEEE","1 Aug 2022","","","IEEE","IEEE Magazines"
"Power-Aware Traffic Engineering for Data Center Networks via Deep Reinforcement Learning","M. Gao; T. Pan; E. Song; M. Yang; T. Huang; Y. Liu","State Key Laboratory of Networking and Switching Technology, BUPT, Beijing, China; State Key Laboratory of Networking and Switching Technology, BUPT, Beijing, China; State Key Laboratory of Networking and Switching Technology, BUPT, Beijing, China; State Key Laboratory of Networking and Switching Technology, BUPT, Beijing, China; State Key Laboratory of Networking and Switching Technology, BUPT, Beijing, China; State Key Laboratory of Networking and Switching Technology, BUPT, Beijing, China","GLOBECOM 2022 - 2022 IEEE Global Communications Conference","11 Jan 2023","2022","","","6055","6060","The issue of high energy consumption and low energy utilization in data center networks (DCNs) has always been the focus of attention of both academia and industry. One general solution is to select a subset of network devices that can meet the traffic transmission requirements, thereby turning off the remaining redundant devices. However, modeling the problem as integer linear programming introduces significant time overhead, while heuristic approaches often suffer from poor generalizability. In this paper, we propose GreenDCN.ai, a closed-loop control system, which utilizes In-band Network Telemetry to collect the network-wide device-internal state, and leverages a Deep Reinforcement Learning-based energy-saving algorithm to make rapid decisions to turn on or off network device ports in response to the real-time network state. The trained GreenDCN.ai can adaptively adjust its energy-saving strategy without human intervention when the DCN topology changes. Besides, based on the regularity of the DCN topology, we design two training complexity reduction methods to address the non-convergence issue under large-scale DCN topologies. Specifically, we split the large-scale DCN topology into sub-topologies for parallel training on each sub-topology without breaking the DCN topology connectivity. Evaluation on software P4 switches suggests that GreenDCN.ai can achieve stable convergence within 590 episodes, generate effective action decisions within $\boldsymbol{79}\upmu\mathrm{s}$, and save about 34% to 39% of the network energy consumption.","","978-1-6654-3540-6","10.1109/GLOBECOM48099.2022.10001013","National Key Research and Development Program of China(grant numbers:2019YFBI802600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10001013","","Training;Energy consumption;Data centers;Network topology;Green products;Reinforcement learning;Turning","closed loop systems;computer centres;deep learning (artificial intelligence);energy consumption;green computing;integer programming;linear programming;power aware computing;reinforcement learning;telecommunication network topology;telecommunication power management;telecommunication switching;telecommunication traffic","closed-loop control system;complexity reduction methods;data center networks;DCN topology;deep reinforcement learning;deep reinforcement learning-based energy-saving algorithm;energy consumption;energy utilization;greenDCN;in-band network telemetry;integer linear programming;network energy consumption;network-wide device-internal state;power-aware traffic engineering;software P4 switches;traffic transmission","","","","13","IEEE","11 Jan 2023","","","IEEE","IEEE Conferences"
"Application of Reinforcement Learning to The Orientation and Position Control of A 6 Degrees of Freedom Robotic Manipulator","F. R. Campos; A. X. Fidêncio; J. Domingues; G. Pessin; G. Freitas","Programa de Pós-Graduação em Instrumentação, Controle e Automação de Processos de Mineração, Universidade Federal de Ouro Preto e Instituto Tecnológico Vale, Ouro Preto, MG, Brazil; Faculty of Electrical Engineering and Information Technology, Ruhr-University, Bochum, Germany; Programa de Pós-Graduação em Instrumentação, Controle e Automação de Processos de Mineração, Universidade Federal de Ouro Preto e Instituto Tecnológico Vale, Ouro Preto, MG, Brazil; Programa de Pós-Graduação em Instrumentação, Controle e Automação de Processos de Mineração, Universidade Federal de Ouro Preto e Instituto Tecnológico Vale, Ouro Preto, MG, Brazil; Programa de Pós-Graduação em Instrumentação, Controle e Automação de Processos de Mineração, Universidade Federal de Ouro Preto e Instituto Tecnológico Vale, Ouro Preto, MG, Brazil","2022 Latin American Robotics Symposium (LARS), 2022 Brazilian Symposium on Robotics (SBR), and 2022 Workshop on Robotics in Education (WRE)","4 Jan 2023","2022","","","1","6","Applications with autonomous robots play an important role in the industry and in everyday life. Among them, the activities of manipulating and moving objects are highlighted by the wide variety of possible applications. These activities in static and known environments can be implemented through logic planned by the developer, but this is not feasible in dynamic environments. Machine Learning (ML) techniques such as Reinforcement Learning (RL) algorithms have sought to replace the pre-defined programming by teaching the robot how to act. This paper presents the implementation of two RL algorithms, Deep Deterministic Policy Gradient (DDPG) and Proximal Policy Optimization (PPO), for orientation and position control of a 6-degree-of-freedom (6-DoF) robotic manipulator. The results demonstrated that the DDPG have a faster learning convergence in simpler activities, but if the complexity of the problem increases, it might not obtain a satisfactory behavior. On the other hand, PPO can solve more complex problems but it limits the convergence rate to the best result in order to avoid learning instability.","2643-685X","978-1-6654-6280-8","10.1109/LARS/SBR/WRE56824.2022.9995835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9995835","Robotics;Machine Learning;Reinforcement Learning;DDPG;PPO","Service robots;Position control;Reinforcement learning;Power system stability;Manipulators;Stability analysis;Behavioral sciences","learning (artificial intelligence);manipulators;mobile robots;position control","6 degrees;6-degree-of-freedom;6-DoF;autonomous robots;DDPG;Deep Deterministic Policy Gradient;dynamic environments;faster learning convergence;freedom robotic manipulator;known environments;learning instability;moving objects;position control;PPO;pre-defined programming;Proximal Policy Optimization;Reinforcement Learning algorithms;RL algorithms;simpler activities;static environments","","","","17","IEEE","4 Jan 2023","","","IEEE","IEEE Conferences"
"Fuel-Efficient Switching Control for Platooning Systems With Deep Reinforcement Learning","T. R. Gonçalves; R. F. Cunha; V. S. Varma; S. E. Elayoubi","Laboratoire des Signaux et Systèmes, CNRS, CentraleSupélec, Université Paris-Saclay, Gif-sur-Yvette, France; Faculty of Science and Engineering, University of Groningen, Groningen, The Netherlands; CNRS, CRAN, Université de Lorraine, Nancy, France; Laboratoire des Signaux et Systèmes, CNRS, CentraleSupélec, Université Paris-Saclay, Gif-sur-Yvette, France","IEEE Transactions on Intelligent Transportation Systems","","2023","PP","99","1","11","The wide appeal of fuel-efficient transport solutions is constantly increasing due to the major impact of the transportation industry on the environment. Platooning systems represent a relatively simple approach in terms of deployment toward fuel-efficient solutions. This paper addresses the reduction of fuel consumption in platooning systems attainable by dynamically switching between two control policies: Adaptive Cruise Control (ACC) and Cooperative Adaptive Cruise Control (CACC). The switching rule is dictated by a Deep Reinforcement Learning (DRL) technique to overcome unpredictable platoon disturbances and to learn appropriate transient shift times while maximizing fuel efficiency. However, due to safety and convergence issues of DRL, our algorithm establishes transition times and minimum periods of operation of ACC and CACC controllers instead of directly controlling vehicles. Numerical experiments show that the DRL agent outperforms both static ACC and CACC versions and the threshold logic control in terms of fuel efficiency while also being robust to perturbations and satisfying safety requirements.","1558-0016","","10.1109/TITS.2023.3304977","European Research Council(grant numbers:ERC-CoG-,771687); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10229987","Vehicle platoons;deep reinforcement learning;cooperative adaptive cruise control (CACC)","Fuels;Control systems;Switches;Atmospheric modeling;Behavioral sciences;Aerodynamics;Roads","","","","","","","IEEE","24 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Digital Twins-based Multi-agent Deep Reinforcement Learning for UAV-assisted Vehicle Edge Computing","C. Hu; Q. Qi; L. Zhang; C. Liu; D. Chen; J. Liao; Z. Zhuang; J. Wang","BeiJing University of Posts and Telecommunications, BeiJing, China; BeiJing University of Posts and Telecommunications, BeiJing, China; China United Telecommunications Co. Ltd, BeiJing, China; China Mobile Research Institute, BeiJing, China; BeiJing University of Posts and Telecommunications, BeiJing, China; BeiJing University of Posts and Telecommunications, BeiJing, China; BeiJing University of Posts and Telecommunications, BeiJing, China; BeiJing University of Posts and Telecommunications, BeiJing, China","2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta)","27 Jul 2023","2022","","","1329","1336","UAV-assisted vehicle-edge-computing (VEC) has become a viable solution for a new generation of intelligent transportation systems (ITS) and has attracted widespread attention from academia and industry. Compared with fixed ground devices, UAV can provide line-of-sight (LoS) link and has good mobility, which better matches the needs of individual wireless connectivity and high mobility of vehicle. However, the mobility of UAVs leads to dynamic changes in the network topology environment and brings new challenges in the rational path planning of UAVs, which brings new problems for network autonomous decision-making to achieve network resource allocation and load balancing. Therefore, in order to solve above problems, we introduce digital twins-based multi-agent deep Q-network (DT-based MADQN). Digital twin (DT) collects network data and reconstructs the network environment and provides the basis for Deep reinforcement learning (DRL) model training. DRL model provides a network decision-making solution based on real-time network status and empirical data. The simulation results show the effectiveness of the proposed algorithm. Compared to the baseline algorithm, it reduces the average task delay by 16.4% and improves the task completion rate by 97.6%.","","979-8-3503-4655-8","10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00192","National Natural Science Foundation of China; Ministry of Education; China Postdoctoral Science Foundation; China Mobile Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10189487","UAV-assisted;Vehicle Edge Computing;Multi Agent Deep Q-network;Digital Twin","Training;Deep learning;Computational modeling;Decision making;Reinforcement learning;Real-time systems;Digital twins","autonomous aerial vehicles;decision making;deep learning (artificial intelligence);digital twins;edge computing;intelligent transportation systems;multi-agent systems;path planning;reinforcement learning;resource allocation;telecommunication computing","deep Q-network;Deep reinforcement learning model training;digital twins-based multiagent Deep reinforcement learning;DT-based MADQN;good mobility;intelligent transportation systems;line-of-sight link;load balancing;network autonomous decision-making;network data;network decision-making solution;network environment;network resource allocation;network topology environment;real-time network status;reconstructs;UAV-assisted vehicle edge computing;UAV-assisted vehicle-edge-computing","","","","22","IEEE","27 Jul 2023","","","IEEE","IEEE Conferences"
"Design and Development of Chatbot Based on Reinforcement Learning","H. M. Jadhav; A. Mulani; M. M. Jadhav","Electronics and Telecommunication Department, Marathwada Mitra Mandal's College of Engineering, Pune, India; Electronics and Telecommunication Department, SKNSCOE, Pandharpur, India; Electronics and Telecommunication Department, NBN Sinhgad School of Engineering, Pune, India","Machine Learning Algorithms for Signal and Image Processing","","2023","","","219","229","Systems consigned with machine learning (ML) as well as artificial intelligence (AI) can think, learn, and remember. Thus today, AI/ML are key technologies for communications. This chapter presents a new approach to develop a chatbot system, applying ML to address the issue of accessing and managing information efficiently for the academic and industry sectors. It is a thrust area to maximize model accuracy and convergence rate. A chatbot is designed to make a discussion between human beings and machines. To identify the sentences, the system has been programmed with expertise. Following that, a decision is taken as a response to a query. The response idea is based on matching the user's supplied sentence. In this chapter, an expert system is developed for the college inquiry desk with a chatbot using natural language processing (NLP) as well as reinforcement‐learning (RL) algorithms. The proposed approach is based on experience learning to provide academic support services with chatbots. Here, customer experience cases are added to the chatbot to improve the connection and utilization of best practices for constant information handling.","","9781119861836","10.1002/9781119861850.ch12","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9960901.pdf&bkn=9960833&pdfType=chapter","","Chatbots;Speech recognition;Tokenization;Passwords;Information retrieval;Web search;Uncertainty","","","","","","","","22 Nov 2022","","","IEEE","Wiley-IEEE Press eBook Chapters"
"An Optimization Method-Assisted Ensemble Deep Reinforcement Learning Algorithm to Solve Unit Commitment Problems","J. Qin; Y. Gao; M. Bragin; N. Yu","Department of Electrical and Computer Engineering, University of California, Riverside, Riverside, CA, USA; Department of Electrical and Computer Engineering, University of California, Riverside, Riverside, CA, USA; Department of Electrical and Computer Engineering, University of California, Riverside, Riverside, CA, USA; Department of Electrical and Computer Engineering, University of California, Riverside, Riverside, CA, USA","IEEE Access","19 Sep 2023","2023","11","","100125","100136","Unit commitment (UC) is a fundamental problem in the day-ahead electricity market, and it is critical to solve UC problems efficiently. Mathematical optimization techniques like dynamic programming, Lagrangian relaxation, and mixed-integer quadratic programming (MIQP) are commonly adopted for UC problems. However, the calculation time of these methods increases at an exponential rate with the number of generators and energy resources, which is still the main bottleneck in the industry. Recent advances in artificial intelligence have demonstrated the capability of reinforcement learning (RL) to solve UC problems. Unfortunately, the existing research on solving UC problems with RL suffers from the curse of dimensionality when the size of UC problems grows. To deal with these problems, we propose an optimization method-assisted ensemble deep reinforcement learning algorithm, where UC problems are formulated as a Markov Decision Process (MDP) and solved by multi-step deep Q-learning in an ensemble framework. The proposed algorithm establishes a candidate action set by solving tailored optimization problems to ensure relatively high performance and the satisfaction of operational constraints. Numerical studies on three test systems show that our algorithm outperforms the baseline RL algorithm in terms of computation efficiency and operation cost. By employing the output of our proposed algorithm as a warm start, the MIQP technique can achieve further reductions in operational costs. Furthermore, the proposed algorithm shows strong generalization capacity under unforeseen operational conditions.","2169-3536","","10.1109/ACCESS.2023.3313998","Office of the President, University of California(grant numbers:L22CR4556); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247003","Deep reinforcement learning;multi-step return;optimization methods;unit commitment","Optimization;Costs;Machine learning algorithms;Heuristic algorithms;Ions;Deep learning;Uncertainty;Reinforcement learning","artificial intelligence;dynamic programming;integer programming;learning (artificial intelligence);Markov processes;optimisation;power generation dispatch;power generation scheduling;power markets;quadratic programming;reinforcement learning","fundamental problem;optimization method-assisted ensemble deep reinforcement learning algorithm;optimization problems;solve unit commitment problems;UC problems","","","","42","CCBYNCND","11 Sep 2023","","","IEEE","IEEE Journals"
"Reinforcement Learning Based Approach for Virtualized Face Detection at the Edge","S. Khebbache; M. Hadji; M. -I. Khaledi","Institut de Recherche Technologique SystemX, Saclay, France; Institut de Recherche Technologique SystemX, Saclay, France; Institut de Recherche Technologique SystemX, Saclay, France","2021 IEEE 22nd International Conference on High Performance Switching and Routing (HPSR)","15 Jul 2021","2021","","","1","7","Real-time requirements in video streaming and processing are increasing and represent one of the major issues in industry 4.0 domains. In particular, Face Detection (FD) use-case has attracted the interest of industrial and academia researchers for various applications such as cyber-physical security, fault detection, predictive maintenance, etc. To ensure applications with real time performance, Edge Computing is a good approach which consists in bringing resources and intelligence closer to connected devices and hence, it can be used to cope with strong latency and throughput expectations. In this paper, we consider optimal routing, placement and scaling of virtualized face detection services at the edge. We propose an edge networking approach based on Integer Linear formulation to cope with small problem instances. A reinforcement learning solution is proposed to address larger problem sizes and scalability issues. We assess the performance of our proposed approaches through simulations and show advantages of the reinforcement learning approach to converge towards near-optimal solutions in negligible time.","2325-5609","978-1-6654-4005-9","10.1109/HPSR52026.2021.9481827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9481827","Face Detection;Optimization;Q-Learning;Edge Networking","Image edge detection;Scalability;Reinforcement learning;Switches;Streaming media;Routing;Throughput","distributed processing;face recognition;integer programming;learning (artificial intelligence);linear programming;real-time systems;video streaming","edge networking approach;optimal routing;throughput expectations;video processing;video streaming;academia researchers;near-optimal solutions;scalability issues;reinforcement learning solution;Integer Linear formulation;virtualized face detection services;Edge Computing;predictive maintenance;fault detection;cyber-physical security;industrial researchers;FD;Face Detection use-case;real-time requirements","","","","12","IEEE","15 Jul 2021","","","IEEE","IEEE Conferences"
"Combining Reinforcement Learning with a Multi-level Abstraction Method to Design a Powerful Game AI","C. Madeira; V. Corruble","Laboratoire dInformatique de Paris 6 (LIP6), Université Pierre et Marie Curie, Paris, France; Laboratoire dInformatique de Paris 6 (LIP6), Université Pierre et Marie Curie, Paris, France","2011 Brazilian Symposium on Games and Digital Entertainment","29 Nov 2012","2011","","","132","140","This paper investigates the design of a challenging Game AI for a modern strategy game, which can be seen as a large-scale multiagent simulation of an historical military confrontation. As an alternative to the typical script-based approach used in industry, we test an approach where military units and leaders, organized in a hierarchy, learn to improve their collective behavior through playing repeated games. In order to allow the application of a reinforcement learning framework at each level of this complex hierarchical decision-making structure, we propose an abstraction mechanism that adapts semi-automatically the level of detail of the state and action representations to the level of the agent. We also study specifically various reward signals as well as inter-agent communication setups and show their impact on the Game AI performance, distinctively in offensive and defensive modes. The resulting Game AI achieves very good performance when compared with the existing commercial script-based solution.","2159-6662","978-0-7695-4648-3","10.1109/SBGAMES.2011.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363226","reinforcement learning;strategic decision-making;modern strategy games;abstraction;terrain analysis;multiagent systems","Games;Learning systems;Decision making;Complexity theory;Learning;Multiagent systems","computer games;learning (artificial intelligence);multi-agent systems","reinforcement learning;multilevel abstraction method;powerful game AI;strategy game;large-scale multiagent simulation;script-based approach;hierarchical decision-making structure;interagent communication","","","","34","IEEE","29 Nov 2012","","","IEEE","IEEE Conferences"
"Flight scenario via behavior-emergence reinforcement learning for assessment of airworthiness and evaluation of aircraft","T. Yin; S. Fu","School of Aeronautics and Astronautics, Shanghai JiaoTong University, Shanghai, China; School of Aeronautics and Astronautics, Shanghai JiaoTong University, Shanghai, China","2012 IEEE Fifth International Conference on Advanced Computational Intelligence (ICACI)","18 Feb 2013","2012","","","1037","1043","Flight scenarios for assessment of airworthiness and evaluation of aircraft are imperative for the aircraft industry. A flight scenario developing method which takes both aircraft airworthiness and evaluation into consideration was proposed. Four aspects of techniques, including configuration of dynamical relations among flight environment, weather conditions and aircraft considerations, decomposition of flight crew tasks, mapping of workload functions and factors onto operational tasks, and specification of window events and data items, were provided and integrated based on methodological guide. Especially, the decomposition of flight crew tasks into operationally related single manipulations was realized via behavior-emergence reinforcement learning. Operationally related single manipulations were obtained as control policies searched from the knowledge of flight trajectories which were collected by interaction with the aircraft system. An automated planning and scheduling mechanism was adopted to facilitate recurrence of flight conditions and detection of working status of pilot, so that flight scenarios developed could be utilized in various flight tests for fulfillment of flight tasks, implementation of workload measurement, and establishment of minimum flight crew, which were useful for assessment of airworthiness, analysis of human factors, human-centered design of aircraft, virtual prototyping and engineering of aircraft.","","978-1-4673-1744-3","10.1109/ICACI.2012.6463330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6463330","","Aircraft;Learning;Meteorology;Aerospace control;Safety;Aircraft propulsion;Mathematical model","aerospace computing;aerospace safety;aircraft testing;learning (artificial intelligence)","flight scenario;behavior-emergence reinforcement learning;aircraft evaluation;aircraft airworthiness;weather condition;flight crew task;workload function mapping;flight trajectory;automated planning;scheduling mechanism;workload measurement;human factor;human-centered design;virtual prototyping;aircraft engineering","","","","9","IEEE","18 Feb 2013","","","IEEE","IEEE Conferences"
"Reinforcement learning-based IoT sensor scheduling strategy for bridge structure health monitoring","Y. Zhang; H. Wu; L. Yi; B. Luo; Y. Qiu; F. Tang","School of Civil Engineering, University of South China, China; School of Civil Engineering, University of South China, China; School of Civil Engineering, University of South China, China; School of Civil Engineering, University of South China, China; Foshan Highway and Bridge Engineering, Monitoring Station Co, China; School of Data Science and Engineering, East China Normal University, Shanghai, China","2022 IEEE International Conferences on Internet of Things (iThings) and IEEE Green Computing & Communications (GreenCom) and IEEE Cyber, Physical & Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)","4 Oct 2022","2022","","","93","100","Internet of Things (IoT) based Bridge Structural Health Monitoring (BSHM) is a hot topic in the field of civil engineering and computer science, and has been widely concerned by academia and industry. The lifetime of the sensors is much less than that of the bridge, which is one of the main technical bottlenecks in BSHM. Therefore, how to effectively improve the network lifetime is the focus of current research. Based on reinforcement learning and Fisher information matrix, this paper proposed a node sleep scheduling strategy by using the learning automata model and confident information coverage (CIC) model to learn the optimal sensor sleep scheduling strategy through cooperative sensing among nodes. Fisher information matrix, which is widely used in civil engineering, was introduced to define the node sleep scheduling problem as a multi-objective optimization problem. With information validity as the modal assurance criterion, the network performance was measured by combining energy efficiency, network coverage requirements and network connectivity. While ensuring the network connectivity and coverage requirements, the system parameter identification error is minimized and the network life is maximized. Through the simulation of jiangbei Bridge in Guangdong, China, the effectiveness, energy efficiency and applicability of the proposed scheme are verified.","","978-1-6654-5417-9","10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics55523.2022.00052","Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903107","Internet of Things (IoT);Bridge Structural Health Monitoring (BSHM);Fisher Information Matrix;Reinforcement Learning","Bridges;Adaptation models;Social computing;Job shop scheduling;Reinforcement learning;Energy efficiency;Sensors","bridges (structures);condition monitoring;Internet of Things;learning (artificial intelligence);learning automata;optimisation;scheduling;structural engineering;wireless sensor networks","network coverage requirements;network connectivity;network life;jiangbei Bridge;reinforcement learning-based IoT sensor scheduling strategy;bridge structure health monitoring;Bridge Structural Health Monitoring;BSHM;civil engineering;computer science;main technical bottlenecks;network lifetime;Fisher information matrix;node sleep scheduling strategy;learning automata model;confident information coverage model;optimal sensor sleep scheduling strategy;node sleep scheduling problem;multiobjective optimization problem;information validity;network performance","","","","21","IEEE","4 Oct 2022","","","IEEE","IEEE Conferences"
"Robust Speed Control of Ultrasonic Motors Based on Deep Reinforcement Learning of a Lyapunov Function","A. Mustafa; T. Sasamura; T. Morita","Graduate School of Frontier Sciences, The University of Tokyo, Kashiwa, Chiba, Japan; Graduate School of Frontier Sciences, The University of Tokyo, Kashiwa, Chiba, Japan; Graduate School of Frontier Sciences, The University of Tokyo, Kashiwa, Chiba, Japan","IEEE Access","5 May 2022","2022","10","","46895","46910","Speed control of ultrasonic motors (USM) needs to be precise, fast, and robust; however, this becomes a challenging task due to the nonlinear behavior of these motors including nonlinear response, pull-out phenomenon, and speed hysteresis. However, linear controllers would be suboptimal and unstable, and nonlinear controllers would require expert knowledge, expensive online calculations, or costly model estimation. In this paper, we propose a model-free nonlinear offline controller that can significantly mitigate these challenges. Using deep reinforcement learning (DRL) algorithms, a neural network speed controller was optimized. A soft actor-critic (SAC) DRL algorithm was chosen due to its sample efficiency, fast convergence, and stable learning. To ensure controller stability, a custom control Lyapunov reward function was proposed. The steady-state USM behavior was mathematically modeled for easing controller design under simulation. The SAC agent was designed and trained first in simulation and then further trained experimentally. The experimental results support that the trained controller can successfully expand speed operation range ([0, 300] rpm), plan optimal control trajectories, and stabilize performance under varying load torque and temperature drift.","2169-3536","","10.1109/ACCESS.2022.3170995","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9764695","Ultrasonic motors;speed control;deep reinforcement learning;control Lyapunov function;soft actor-critic","Hysteresis motors;Velocity control;Stators;Torque;Mathematical models;Neural networks;Acoustics","angular velocity control;control system synthesis;learning (artificial intelligence);Lyapunov methods;machine control;neural nets;neurocontrollers;nonlinear control systems;optimal control;robust control;stability;torque control;ultrasonic motors;velocity control","nonlinear response;speed hysteresis;linear controllers;nonlinear controllers;expert knowledge;expensive online calculations;costly model estimation;model-free nonlinear offline controller;deep reinforcement learning algorithms;neural network speed controller;soft actor-critic DRL algorithm;stable learning;controller stability;custom control Lyapunov reward function;steady-state USM behavior;controller design;trained controller;speed operation;plan optimal control trajectories;robust speed control;ultrasonic motors;Lyapunov function;nonlinear behavior","","2","","37","CCBY","28 Apr 2022","","","IEEE","IEEE Journals"
"Robust Decentralized H∞ Attack-Tolerant Observer-Based Team Formation Network Control of Large-Scale Quadrotor UAVs: HJIE-Reinforcement Learning-Based Deep Neural Network Method","B. -S. Chen; P. -C. Chao","Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan","IEEE Access","10 Jul 2023","2023","11","","65810","65833","In this paper, a robust decentralized  $H_{\infty} $  attack-tolerant observer-based team formation tracking control scheme is proposed for large-scale quadrotor unmanned aerial vehicle (UAV) systems under external disturbance, measurement noise, couplings from other neighboring quadrotor UAVs, and malicious attacks on actuator and sensor of the network control system (NCS) via wireless communication. First, we constructed a smoothed model of attack signals to describe their behavior. Then, by integrating the smoothed dynamic model with the system dynamic model of each quadrotor UAV, we can simultaneously estimate the attack signals and the system state of each quadrotor UAV through a traditional Luenberger observer for the efficient robust decentralized  $H_{\infty} $  attack-tolerant observer-based team formation tracking control of large-scale quadrotor UAVs. For the design of robust decentralized  $H_{\infty} $  attack-tolerant observer-based team formation tracking control of large-scale quadrotor UAVs, a very difficult independent nonlinear partial differential observer/controller-coupled Hamilton Jacobi Issac equation (HJIE) must be solved for the observer and controller design of each quadrotor UAV. Nowadays, there are no analytical and numerical methods to resolve HJIE. Thus, an HJIE-reinforcement learning-based deep neural network (DNN) is trained to directly solve the observer/controller-coupled HJIE for robust decentralized  $H_{\infty} $  attack-tolerant observer-based team formation tracking control of each quadrotor UAV. Since the system model of the quadrotor UAV and HJIE have been adopted for the HJIE-reinforcement Adam learning algorithm DNN training, compared to the traditional DNN big data-driven training schemes, we save a lot of training data and time to achieve the robust decentralized  $H_{\infty} $  attack-tolerant observer-based team formation tracking control design. As the Adam algorithm converges, we could show that the proposed HJIE-reinforcement DNN-based decentralized  $H_{\infty} $  attack-tolerant observer-based tracking control scheme can achieve the theoretical result. Finally, the simulation results are presented with a comparison to verify the effectiveness of the proposed method.","2169-3536","","10.1109/ACCESS.2023.3290306","National Science and Technology Council(grant numbers:110-2221-E-007-115-MY2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10167636","Network control system (NCS);attack-tolerant observer-based tracking control;team formation NCS of large-scale quadrotor UAVs;observer/controller-coupled HJIE;HJIE-reinforcement learning deep neural network (DNN);H∞ decentralized reference tracking control of large-scale systems","Quadrotors;Observers;Couplings;Autonomous aerial vehicles;Noise measurement;Mathematical models;Interpolation","autonomous aerial vehicles;control engineering computing;control system synthesis;deep learning (artificial intelligence);H∞ control;helicopters;networked control systems;nonlinear control systems;observers;reinforcement learning;robust control","attack signals;controller design;controller-coupled Hamilton Jacobi Issac equation;deep neural network method;HJIE-reinforcement Adam learning algorithm;HJIE-reinforcement DNN;independent nonlinear partial differential observer;large-scale quadrotor UAV;large-scale quadrotor unmanned aerial vehicle systems;neighboring quadrotor UAV;network control system;robust decentralized H∞ attack-tolerant observer;team formation network control","","","","40","CCBYNCND","28 Jun 2023","","","IEEE","IEEE Journals"
"Online Computation Offloading in NOMA-Based Multi-Access Edge Computing: A Deep Reinforcement Learning Approach","M. Nduwayezu; Q. -V. Pham; W. -J. Hwang","Department of Information and Communication System, Inje University, Gimhae, South Korea; Research Institute of Computer, Information and Communication, Pusan National University, Busan, South Korea; School of Biomedical Convergence Engineering, Pusan National University, Yangsan, South Korea","IEEE Access","4 Jun 2020","2020","8","","99098","99109","One of the missions of fifth generation (5G) wireless networks is to provide massive connectivity of the fast growing number of Internet of Things (IoT) devices. To satisfy this mission, non-orthogonal multiple access (NOMA) has been recognized as a promising solution for 5G networks to significantly improve the network capacity. Considered as a booster of IoT devices, and in parallel with the development of NOMA techniques, multi-access edge computing (MEC) is also becoming one of the key emerging technologies for 5G networks. In this paper, with an objective of maximizing the computation rate of an MEC system, we investigate the computation offloading and subcarrier allocation problem in Multi-carrier (MC) NOMA based MEC systems and address it using Deep Reinforcement Learning for Online Computation Offloading (DRLOCO-MNM) algorithm. In particular, the DRLOCO-MNM helps each of the user equipments (UEs) decides between local and remote computation modes, and also assigns the appropriate subcarrier to the UEs in the case of remote computation mode. The DRLOCO-MNM algorithm is especially advantageous over the other machine learning techniques applied on NOMA because it does not require labeled data for training or a complete definition of the channel environment. The DRLOCO-MNM also does avoid the complexity found in many optimization algorithms used to solve channel allocation in existing NOMA related studies. Numerical simulations and comparison with other algorithms show that our proposed module and its algorithm considerably improve the computation rates of MEC systems.","2169-3536","","10.1109/ACCESS.2020.2997925","Institute of Information and Communications Technology Planning and Evaluation (IITP) grant funded by the Korea Government (MSIT)(grant numbers:(2020-0-01450, Artificial Intelligence Convergence Research Center [Pusan National University])); Pusan National University Research Grant, 2020; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9102308","5G networks;deep reinforcement learning (DRL);multi access edge computing (MEC);non-orthogonal multiple access (NOMA);online computation offloading","NOMA;Resource management;Task analysis;5G mobile communication;Wireless communication;Machine learning;Edge computing","5G mobile communication;channel allocation;distributed processing;Internet of Things;learning (artificial intelligence);multi-access systems;optimisation","MEC system;fifth generation wireless networks;5G wireless networks;massive connectivity;Internet of Things devices;nonorthogonal multiple access;network capacity;IoT devices;NOMA techniques;subcarrier allocation problem;local computation modes;remote computation mode;DRLOCO-MNM algorithm;NOMA-based multiaccess edge computing;multicarrier NOMA based MEC systems;deep reinforcement learning for online computation offloading algorithm;machine learning techniques;channel environment;optimization algorithms;channel allocation;numerical simulations;user equipments","","29","","47","CCBY","27 May 2020","","","IEEE","IEEE Journals"
"Generating Cryptographic S-Boxes Using the Reinforcement Learning","G. Kim; H. Kim; Y. Heo; Y. Jeon; J. Kim","Department of Mathematics and Financial Information Security, Kookmin University, Seoul, South Korea; Department of Mathematics and Financial Information Security, Kookmin University, Seoul, South Korea; Gyeongsang National University High School (GNUHS), Jinju, South Korea; Department of Mathematics and Financial Information Security, Kookmin University, Seoul, South Korea; Department of Mathematics and Financial Information Security, Kookmin University, Seoul, South Korea","IEEE Access","14 Jun 2021","2021","9","","83092","83104","Substitution boxes (S-boxes) are essential components of many cryptographic primitives. The Dijkstra algorithm, SAT solvers, and heuristic methods have been used to find bitsliced implementations of S-boxes. However, it is difficult to apply these methods for 8-bit S-boxes because of their size. Therefore, to implement these S-boxes so that the countermeasure of side-channel attack can be applied efficiently, using structures such as Feistel, Lai-Massey, and MISTY that can be bitsliced implemented with a small number of nonlinear operations has been widely used. Since S-boxes constructed with structures consist of small S-boxes and have specific designs, there are limitations to their cryptographic security and efficiency. In this paper, we propose a new method for generating S-boxes by stacking bitwise operations from the identity function, an approach that is different from existing methods. This method can be expressed in Markov decision process, and reinforcement learning is a suitable solver for Markov decision process. Our goal is to train this method to an agent through reinforcement learning to generate S-boxes to which the masking scheme, which is a countermeasure of side-channel attack, can be efficiently applied. In particular, our method provided various S-boxes superior or comparable to existing S-boxes. We produced 8-bit S-boxes with differential uniformity 16 (resp. 32) and linearity 128 (resp. 128), generated with nine (resp. eight) nonlinear operations, for the first time. To our best knowledge, this is the first study to construct cryptographic S-Box by incorporating reinforcement learning.","2169-3536","","10.1109/ACCESS.2021.3085861","This work was supported by Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT)(grant numbers:2021-0-00540); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9446086","S-box;masking efficiency;reinforcement learning;bitsliced implementation;linearity;differential uniformity","Cryptography;Reinforcement learning;Security;Linearity;Stacking;Markov processes;Side-channel attacks","computability;cryptography;learning (artificial intelligence);Markov processes","cryptographic S-boxes;reinforcement learning;substitution boxes;8-bit S-boxes;cryptographic primitives;Dijkstra algorithm;SAT solvers;MISTY;Markov decision process;side-channel attack;differential uniformit","","2","","56","CCBY","3 Jun 2021","","","IEEE","IEEE Journals"
"Joint Optimization of Multi-UAV Target Assignment and Path Planning Based on Multi-Agent Reinforcement Learning","H. Qie; D. Shi; T. Shen; X. Xu; Y. Li; L. Wang","College of Computer, National University of Defense Technology, Changsha, China; Tianjin Artificial Intelligence Innovation Center, Tianjin, China; Artificial Intelligence Research Center, National Innovation Institute of Defense Technology, Beijing, China; Artificial Intelligence Research Center, National Innovation Institute of Defense Technology, Beijing, China; Artificial Intelligence Research Center, National Innovation Institute of Defense Technology, Beijing, China; College of Computer, National University of Defense Technology, Changsha, China","IEEE Access","16 Oct 2019","2019","7","","146264","146272","One of the major research topics in unmanned aerial vehicle (UAV) collaborative control systems is the problem of multi-UAV target assignment and path planning (MUTAPP). It is a complicated optimization problem in which target assignment and path planning are solved separately. However, recalculation of the optimal results is too slow for real-time operations in dynamic environments because of the large number of calculations required. In this paper, we propose an artificial intelligence method named simultaneous target assignment and path planning (STAPP) based on a multi-agent deep deterministic policy gradient (MADDPG) algorithm, which is a type of multi-agent reinforcement learning algorithm. In STAPP, the MUTAPP problem is first constructed as a multi-agent system. Then, the MADDPG framework is used to train the system to solve target assignment and path planning simultaneously according to a corresponding reward structure. The proposed system can deal with dynamic environments effectively as its execution only requires the locations of the UAVs, targets, and threat areas. Real-time performance can be guaranteed as the neural network used in the system is simple. In addition, we develop a technique to improve the training effect and use experiments to demonstrate the effectiveness of our method.","2169-3536","","10.1109/ACCESS.2019.2943253","National Basic Research Program of China (973 Program)(grant numbers:2017YFB1001901); National Defense Science and Technology Foundation for Young Scientists of China(grant numbers:030403); National Natural Science Foundation of China(grant numbers:11801563,91648204,61532007,61902425); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846699","Multi-UAV;target assignment and path planning;multi-agent reinforcement learning;MADDPG;dynamic environments","Path planning;Heuristic algorithms;Optimization;Training;Reinforcement learning;Task analysis;Unmanned aerial vehicles","autonomous aerial vehicles;learning (artificial intelligence);mobile robots;multi-agent systems;optimisation;path planning;remotely operated vehicles","joint optimization;multiUAV target assignment;path planning;multiagent reinforcement learning;UAV;collaborative control systems;complicated optimization problem;dynamic environments;simultaneous target assignment;multiagent deep deterministic policy gradient algorithm;multiagent system","","123","","24","CCBY","23 Sep 2019","","","IEEE","IEEE Journals"
"Deep Robust Reinforcement Learning for Practical Algorithmic Trading","Y. Li; W. Zheng; Z. Zheng","Guangdong Key Laboratory for Big Data Analysis and Simulation of Public Opinion, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory for Big Data Analysis and Simulation of Public Opinion, Sun Yat-sen University, Guangzhou, China; National Engineering Research Center of Digital Life, Sun Yat-sen University, Guangzhou, China","IEEE Access","14 Aug 2019","2019","7","","108014","108022","In algorithmic trading, feature extraction and trading strategy design are two prominent challenges to acquire long-term profits. However, the previously proposed methods rely heavily on domain knowledge to extract handcrafted features and lack an effective way to dynamically adjust the trading strategy. With the recent breakthroughs of deep reinforcement learning (DRL), sequential real-world problems can be modeled and solved with a more human-like approach. In this paper, we propose a novel trading agent, based on deep reinforcement learning, to autonomously make trading decisions and gain profits in the dynamic financial markets. We extend the value-based deep Q-network (DQN) and the asynchronous advantage actor-critic (A3C) for better adapting to the trading market. Specifically, in order to automatically extract robust market representations and resolve the financial time series dependence, we utilize the stacked denoising autoencoders (SDAEs) and the long short-term memory (LSTM) as parts of the function approximator, respectively. Furthermore, we design several elaborate mechanisms to make the trading agent more practical to the real trading environment, such as position-controlled action and n-step reward. The experimental results show that our trading agent outperforms the baselines and achieves stable risk-adjusted returns in both the stock and the futures markets.","2169-3536","","10.1109/ACCESS.2019.2932789","National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000101); National Natural Science Foundation of China(grant numbers:61722214,U1811462); Guangdong Province Universities and Colleges Pearl River Scholar Funded Scheme(grant numbers:2016); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2016ZT06D211); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8786132","Algorithmic trading;Markov decision process;deep neural network;reinforcement learning","Reinforcement learning;Time series analysis;Autoregressive processes;Heuristic algorithms;Neural networks;Biological system modeling;Markov processes","electronic trading;feature extraction;function approximation;learning (artificial intelligence);neural nets;time series","deep robust reinforcement learning;practical algorithmic trading;feature extraction;long-term profits;handcrafted features;sequential real-world problems;trading agent;deep reinforcement learning;trading decisions;gain profits;dynamic financial markets;value-based deep Q-network;trading market;robust market representations;trading environment;DRL;trading strategy design","","69","","39","CCBY","5 Aug 2019","","","IEEE","IEEE Journals"
"Collision Avoidance in Pedestrian-Rich Environments With Deep Reinforcement Learning","M. Everett; Y. F. Chen; J. P. How","Massachusetts Institute of Technology, Cambridge, MA, USA; Facebook Reality Labs, Redmond, WA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE Access","20 Jan 2021","2021","9","","10357","10377","Collision avoidance algorithms are essential for safe and efficient robot operation among pedestrians. This work proposes using deep reinforcement (RL) learning as a framework to model the complex interactions and cooperation with nearby, decision-making agents, such as pedestrians and other robots. Existing RL-based works assume homogeneity of agent properties, use specific motion models over short timescales, or lack a principled method to handle a large, possibly varying number of agents. Therefore, this work develops an algorithm that learns collision avoidance among a variety of heterogeneous, non-communicating, dynamic agents without assuming they follow any particular behavior rules. It extends our previous work by introducing a strategy using Long Short-Term Memory (LSTM) that enables the algorithm to use observations of an arbitrary number of other agents, instead of a small, fixed number of neighbors. The proposed algorithm is shown to outperform a classical collision avoidance algorithm, another deep RL-based algorithm, and scales with the number of agents better (fewer collisions, shorter time to goal) than our previously published learning-based approach. Analysis of the LSTM provides insights into how observations of nearby agents affect the hidden state and quantifies the performance impact of various agent ordering heuristics. The learned policy generalizes to several applications beyond the training scenarios: formation control (arrangement into letters), demonstrations on a fleet of four multirotors and on a fully autonomous robotic vehicle capable of traveling at human walking speed among pedestrians.","2169-3536","","10.1109/ACCESS.2021.3050338","Ford Motor Company; Amazon Web Services; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9317723","Collision avoidance;deep reinforcement learning;motion planning;multiagent systems;decentralized execution","Collision avoidance;Robots;Reinforcement learning;Vehicle dynamics;Robot sensing systems;Heuristic algorithms;Dynamics","collision avoidance;deep learning (artificial intelligence);learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;pedestrians;recurrent neural nets","pedestrian-rich environments;deep reinforcement learning;safe robot operation;decision-making agents;RL-based works;agent properties;motion models;dynamic agents;long short-term memory;collision avoidance;deep RL-based algorithm;agent ordering heuristics;policy learning;fully autonomous robotic vehicle;LSTM;multirotors;human walking speed;formation control","","63","","48","CCBY","8 Jan 2021","","","IEEE","IEEE Journals"
"A Gentle Introduction to Reinforcement Learning and its Application in Different Fields","M. Naeem; S. T. H. Rizvi; A. Coronato","Department of Engineering, Universita’ Degli Studi di Napoli Parthenope, Napoli, Italy; Department of Computer Engineering, The University of Lahore, Lahore, Pakistan; ICAR-CNR, Napoli, Italy","IEEE Access","26 Nov 2020","2020","8","","209320","209344","Due to the recent progress in Deep Neural Networks, Reinforcement Learning (RL) has become one of the most important and useful technology. It is a learning method where a software agent interacts with an unknown environment, selects actions, and progressively discovers the environment dynamics. RL has been effectively applied in many important areas of real life. This article intends to provide an in-depth introduction of the Markov Decision Process, RL and its algorithms. Moreover, we present a literature review of the application of RL to a variety of fields, including robotics and autonomous control, communication and networking, natural language processing, games and self-organized system, scheduling management and configuration of resources, and computer vision.","2169-3536","","10.1109/ACCESS.2020.3038605","Italian Government through Universita’ Degli Studi di Napoli Parthenope, Napoli (grant 33 cycle PhD Scholarship); ASMARA - Applicazioni pilotapost Direttiva 2010/65in realta portualiitaliane della Suite MIELE a supporto delle Authority per ottimiz- zazione della inteRoperabilita nellnu2019intermodalitA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9261348","Artificial intelligence;reinforcement learning;applications;healthcare;robotics;communication;natural language processing;computer vision;resource management;IoT","Mathematical model;Prediction algorithms;Heuristic algorithms;Robot kinematics;Reinforcement learning;Markov processes;Task analysis","learning (artificial intelligence);Markov processes;multi-agent systems;natural language processing;neural nets;software agents","Reinforcement Learning;Deep Neural Networks;RL;learning method;software agent;unknown environment;environment dynamics;in-depth introduction;Markov Decision Process;natural language processing;robotics;autonomous control;self-organized system;scheduling management;resource configuration;computer vision","","57","","457","CCBY","17 Nov 2020","","","IEEE","IEEE Journals"
"Coverage Path Planning for Decomposition Reconfigurable Grid-Maps Using Deep Reinforcement Learning Based Travelling Salesman Problem","P. T. Kyaw; A. Paing; T. T. Thu; R. E. Mohan; A. Vu Le; P. Veerajagadheswar","ROAR Laboratory, Engineering Product Development, Singapore University of Technology and Design, Singapore; Department of Mechatronic Engineering, Yangon Technological University, Yangon, Myanmar; Department of Mechatronic Engineering, Yangon Technological University, Yangon, Myanmar; ROAR Laboratory, Engineering Product Development, Singapore University of Technology and Design, Singapore; Optoelectronics Research Group, Faculty of Electrical and Electronics Engineering, Ton Duc Thang University, Ho Chi Minh City, Vietnam; ROAR Laboratory, Engineering Product Development, Singapore University of Technology and Design, Singapore","IEEE Access","24 Dec 2020","2020","8","","225945","225956","Optimizing the coverage path planning (CPP) in robotics has become essential to accomplish efficient coverage applications. This work presents a novel approach to solve the CPP problem in large complex environments based on the Travelling Salesman Problem (TSP) and Deep Reinforcement Learning (DRL) leveraging the grid-based maps. The proposed algorithm applies the cellular decomposition methods to decompose the environment and generate the coverage path by recursively solving each decomposed cell formulated as TSP. A solution to TSP is determined by training Recurrent Neural Network (RNN) with Long Short Term Memory (LSTM) layers using Reinforcement Learning (RL). We validated the proposed method by systematically benchmarked with other conventional methods in terms of path length, execution time, and overlapping rate under four different map layouts with various obstacle density. The results depict that the proposed method outperforms all considered parameters than the conventional schemes. Moreover, simulation experiments demonstrate that the proposed approach is scalable to the larger grid-maps and guarantees complete coverage with efficiently generated coverage paths.","2169-3536","","10.1109/ACCESS.2020.3045027","National Robotics Programme under its Robotics Enabling Capabilities and Technologies(grant numbers:192 25 00051); National Robotics Programme under its Robot Domain Specific(grant numbers:192 22 00058); Agency for Science, Technology and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9294048","Coverage path planning;cellular reconfigurable decomposition;deep reinforcement learning;recurrent neural network;travelling salesman problem","Robots;Reinforcement learning;Path planning;Recurrent neural networks;Training;Shape;Sensors","cartography;learning (artificial intelligence);mobile robots;path planning;recurrent neural nets;travelling salesman problems","coverage path planning;decomposition reconfigurable grid-maps;CPP problem;Travelling Salesman Problem;TSP;grid-based maps;cellular decomposition methods;Long Short Term Memory layers;deep reinforcement learning;recurrent neural network training;RNN training;LSTM;robotics","","40","","39","CCBY","15 Dec 2020","","","IEEE","IEEE Journals"
"An Intelligent Path Planning Scheme of Autonomous Vehicles Platoon Using Deep Reinforcement Learning on Network Edge","C. Chen; J. Jiang; N. Lv; S. Li","State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China","IEEE Access","4 Jun 2020","2020","8","","99059","99069","Recent advancements in Intelligent Transportation Systems suggest that the roads will gradually be filled with autonomous vehicles that are able to drive themselves while communicating with each other and the infrastructure. As a representative driving pattern of autonomous vehicles, the platooning technology has great potential for reducing transport costs by lowering fuel consumption and increasing traffic efficiency. In this paper, to improve the driving efficiency of autonomous vehicular platoon in terms of fuel consumption, a path planning scheme is envisioned using deep reinforcement learning on the network edge node. At first, the system model of autonomous vehicles platooning is given on the common highway. Next, a joint optimization problem is developed considering the task deadline and fuel consumption of each vehicle in the platoon. After that, a path determination strategy employing deep reinforcement learning is designed for the platoon. To make the readers readily follow, a case study is also presented with instantiated parameters. Numerical results shows that our proposed model could significantly reduce the fuel consumption of vehicle platoons while ensuring their task deadlines.","2169-3536","","10.1109/ACCESS.2020.2998015","National Basic Research Program of China (973 Program)(grant numbers:2018YFE0126000); National Natural Science Foundation of China(grant numbers:61571338,U1636209,61672131); Key Research and Development Plan of Shaanxi Province(grant numbers:2017ZDCXL-GY-05-01,2019ZDLGY13-04,2019ZDLGY13-07); Xi’an Key Laboratory of Mobile Edge Computing and Security(grant numbers:201805052-ZD3CG36); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9102259","Path planning;platooning;fuel consumption;Q-learning","Path planning;Roads;Fuels;Edge computing;Autonomous vehicles;Optimization;Machine learning","intelligent transportation systems;learning (artificial intelligence);mobile robots;optimisation;path planning;road safety;road traffic control;road vehicles","autonomous vehicles platoon;deep reinforcement learning;intelligent transportation systems;representative driving pattern;transport costs;fuel consumption;network edge node;path determination strategy;intelligent path planning scheme;joint optimization problem","","38","","36","CCBY","27 May 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning With Optimized Reward Functions for Robotic Trajectory Planning","J. Xie; Z. Shao; Y. Li; Y. Guan; J. Tan","Beijing Advanced Innovation Center for Imaging Technology, Capital Normal University, Beijing, China; Beijing Key Laboratory of Light Industrial Robot and Safety Verification, Capital Normal University, Beijing, China; Beijing Advanced Innovation Center for Imaging Technology, Capital Normal University, Beijing, China; Beijing Key Laboratory of Light Industrial Robot and Safety Verification, Capital Normal University, Beijing, China; Engineering College, University of Tennessee, Knoxville, TN, USA","IEEE Access","12 Aug 2019","2019","7","","105669","105679","To improve the efficiency of deep reinforcement learning (DRL)-based methods for robotic trajectory planning in the unstructured working environment with obstacles. Different from the traditional sparse reward function, this paper presents two brand-new dense reward functions. First, the azimuth reward function is proposed to accelerate the learning process locally with a more reasonable trajectory by modeling the position and orientation constraints, which can reduce the blindness of exploration dramatically. To further improve the efficiency, a reward function at subtask-level is proposed to provide global guidance for the agent in the DRL. The subtask-level reward function is designed under the assumption that the task can be divided into several subtasks, which reduces the invalid exploration greatly. The extensive experiments show that the proposed reward functions are able to improve the convergence rate by up to three times with the state-of-the-art DRL methods. The percentage increase in convergence means is 2.25%–13.22% and the percentage decreases with respect to standard deviation by 10.8%–74.5%.","2169-3536","","10.1109/ACCESS.2019.2932257","National Basic Research Program of China (973 Program)(grant numbers:2017YFB1303000); Beijing Municipal Commission of Education(grant numbers:KM201710028017); National Natural Science Foundation of China(grant numbers:61702348,61772351,61602326,61602324); Beijing Municipal Science and Technology Commission(grant numbers:LJ201607); Capacity Building for Sci-Tech Innovation-Fundamental Scientific Research Fund(grant numbers:025185305000); Capital Normal University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8782495","Deep reinforcement learning;robot manipulator;trajectory planning;reward function","Manipulators;Trajectory;Planning;Azimuth;Collision avoidance;Task analysis","collision avoidance;learning (artificial intelligence);mobile robots","brand-new dense reward functions;azimuth reward function;learning process;reasonable trajectory;orientation constraints;subtask-level reward function;state-of-the-art DRL methods;optimized reward functions;robotic trajectory planning;deep reinforcement learning-based methods;unstructured working environment;traditional sparse reward function","","37","","31","CCBY","31 Jul 2019","","","IEEE","IEEE Journals"
"A Reinforcement Learning-Based Resource Allocation Scheme for Cloud Robotics","H. Liu; S. Liu; K. Zheng","Intelligent Computing and Communication Lab, Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Intelligent Computing and Communication Lab, Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Intelligent Computing and Communication Lab, Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Access","9 Apr 2018","2018","6","","17215","17222","In recent years, robotic systems combined with cloud computing capability have become an emerging topic of discussion in academic fields. The concept of cloud robotics allows the system to offload computing-intensive tasks from the robots to the cloud. An appropriate resource allocation scheme is necessary for the cloud computing service platform to efficiently allocate its computing resources, when the robots send requests asking for computing service. This paper proposes a resource allocation scheme based on reinforcement learning (RL), which can make the cloud to decide whether a request should be accepted and how many resources are supposed to be allocated. The scheme realizes an autonomous management of computing resources through online learning, reduces human participation in scheme planning, and improves the overall utility of the system in the long run. Numerical results demonstrate that the proposed RL-based computing resource allocation scheme has better performances than the greedy allocation scheme.","2169-3536","","10.1109/ACCESS.2018.2814606","China Natural Science Funding(grant numbers:61671089); China Unicom Network Technology Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8314091","Cloud robotics;reinforcement learning;resource allocation","Cloud computing;Task analysis;Resource management;Robot sensing systems;Information processing;Wireless communication","","","","37","","24","OAPA","12 Mar 2018","","","IEEE","IEEE Journals"
"A Multiagent Deep Reinforcement Learning Approach for Path Planning in Autonomous Surface Vehicles: The Ypacaraí Lake Patrolling Case","S. Y. Luis; D. G. Reina; S. L. T. Marín","Department of Electronic Engineering, Technical School of Engineering of Seville, Seville, Spain; Department of Electronic Engineering, Technical School of Engineering of Seville, Seville, Spain; Department of Electronic Engineering, Technical School of Engineering of Seville, Seville, Spain","IEEE Access","29 Jan 2021","2021","9","","17084","17099","Autonomous surfaces vehicles (ASVs) excel at monitoring and measuring aquatic nutrients due to their autonomy, mobility, and relatively low cost. When planning paths for such vehicles, the task of patrolling with multiple agents is usually addressed with heuristics approaches, such as Reinforcement Learning (RL), because of the complexity and high dimensionality of the problem. Not only do efficient paths have to be designed, but addressing disturbances in movement or the battery's performance is mandatory. For this multiagent patrolling task, the proposed approach is based on a centralized Convolutional Deep Q-Network, designed with a final independent dense layer for every agent to deal with scalability, with the hypothesis/assumption that every agent has the same properties and capabilities. For this purpose, a tailored reward function is created which penalizes illegal actions (such as collisions) and rewards visiting idle cells (cells that remains unvisited for a long time). A comparison with various multiagent Reinforcement Learning (MARL) algorithms has been done (Independent Q-Learning, Dueling Q-Network and multiagent Double Deep Q-Learning) in a case-study scenario like the Ypacaraí lake in Asunción (Paraguay). The training results in multiagent policy leads to an average improvement of 15% compared to lawn mower trajectories and a 6% improvement over the IDQL for the case-study considered. When evaluating the training speed, the proposed approach runs three times faster than the independent algorithm.","2169-3536","","10.1109/ACCESS.2021.3053348","Universidad de Sevilla through the Contract “Contratos de acceso al Sistema Español de Ciencia, Tecnología e Innovación para el desarrollo del programa propio de I+D+i de la Universidad de Sevilla”; Spanish “Ministerio de Ciencia, Innovación y Universidades, Programa Estatal de I+D+i Orientada a los Retos de la Sociedad” through the Project “Despliegue Adaptativo de Vehículos no Tripulados para Gestión Ambiental en Escenarios Dinámicos”(grant numbers:RTI2018-098964-B-I00); regional government Junta de Andalucía through the Project “Despliegue Inteligente de una red de Vehículos Acuáticos no Tripulados para la monitorización de Recursos Hídricos”(grant numbers:US-1257508); regional government Junta de Andalucía through the Project “Despliegue y Control de una Red Inteligente de Vehículos Autónomos Acuáticos para la Monitorización de Recursos Hídricos Andaluces”(grant numbers:PY18-RE0009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9330612","Deep reinforcement learning;multiagent learning;monitoring;path planning;autonomous surface vehicle;patrolling","Monitoring;Scalability;Trajectory;Task analysis;Reinforcement learning;Lakes;Water quality","control engineering computing;deep learning (artificial intelligence);mobile robots;multi-agent systems;path planning;underwater vehicles;unmanned surface vehicles","multiagent deep reinforcement learning;path planning;autonomous surface vehicles;Ypacaraí lake patrolling case;multiagent patrolling task;independent Q-learning;multiagent double deep Q-learning;aquatic nutrients;centralized convolutional deep Q-network","","35","","44","CCBY","21 Jan 2021","","","IEEE","IEEE Journals"
"Multi-Robot Flocking Control Based on Deep Reinforcement Learning","P. Zhu; W. Dai; W. Yao; J. Ma; Z. Zeng; H. Lu","Robotics Research Center, College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; Robotics Research Center, College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; Robotics Research Center, College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; Robotics Research Center, College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; Robotics Research Center, College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; Robotics Research Center, College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China","IEEE Access","21 Aug 2020","2020","8","","150397","150406","In this paper, we apply deep reinforcement learning (DRL) to solve the flocking control problem of multi-robot systems in complex environments with dynamic obstacles. Starting from the traditional flocking model, we propose a DRL framework for implementing multi-robot flocking control, eliminating the tedious work of modeling and control designing. We adopt the multi-agent deep deterministic policy gradient (MADDPG) algorithm, which additionally uses the information of multiple robots in the learning process to better predict the actions that robots will take. To address the problems such as low learning efficiency and slow convergence speed of the MADDPG algorithm, this paper studies a prioritized experience replay (PER) mechanism and proposes the Prioritized Experience Replay-MADDPG (PER-MADDPG) algorithm. Based on the temporal difference (TD) error, a priority evaluation function is designed to determine which experiences are sampled preferentially from the replay buffer. In the end, the simulation results verify the effectiveness of the proposed algorithm. It has a faster convergence speed and enables the robot group to complete the flocking task in the environment with obstacles.","2169-3536","","10.1109/ACCESS.2020.3016951","National Natural Science Foundation of China(grant numbers:U1913202,U1813205); Hunan Provincial Innovation Foundation for Postgraduate(grant numbers:CX2018B010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9169650","Multi-robot;deep reinforcement learning;flocking control;PER-MADDPG","Learning (artificial intelligence);Robot kinematics;Robot sensing systems;Collision avoidance;Task analysis;Training","control engineering computing;control system synthesis;learning (artificial intelligence);mobile robots;multi-robot systems","multirobot flocking control;deep reinforcement learning;multirobot systems;multiagent deep deterministic policy gradient algorithm;MADDPG algorithm;robot group;prioritized experience replay-MADDPG;temporal difference error","","35","","34","CCBY","17 Aug 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Motion Planning for Automatic Parking System","J. Zhang; H. Chen; S. Song; F. Hu","School of Automotive Studies, Tongji University, Shanghai, China; School of Automotive Studies, Tongji University, Shanghai, China; School of Automotive Studies, Tongji University, Shanghai, China; School of Automotive Studies, Tongji University, Shanghai, China","IEEE Access","31 Aug 2020","2020","8","","154485","154501","In automatic parking motion planning, multi-objective optimization including safety, comfort, parking efficiency, and final parking performance should be considered. Most of the current research relies on the parking data from expert drivers or prior knowledge of humans. However, it is challenging to obtain a large amount of high-quality expert drivers' data. Furthermore, expert drivers' data or prior knowledge of humans does not guarantee an optimal multi-objective parking performance. In this article, we propose a model-based reinforcement learning method that learns parking policy of the data, by executing the data generation, data evaluation, and training network, iteratively. The trained network is used to guide the data generation cycle in the subsequent iteration. Based on this proposed method, we can get rid of human experience largely and learn parking strategies autonomously and quickly. The learned strategies ensure the multi-objective optimality of above requirements in the parking process. First, an environment model that approximates the actual environment is established, and the learning efficiency is accelerated through the simulated interaction between the agent and the environment model. To make the system independent of expert data or prior knowledge, a data generation algorithm combining Monte Carlo Tree Search (MCTS) and longitudinal and lateral policies is proposed. Then, to meet the multi-objective optimal demands mentioned above, a reward function is constructed to evaluate and filter the parking data. Finally, a neural network is used to learn the parking strategy from the filtered data. From the real vehicle test benchmarked with a mass-produced parking system, the proposed method is found to achieve better parking efficiency and lower requirements for start parking posture, thereby verifying the algorithm's superiority.","2169-3536","","10.1109/ACCESS.2020.3017770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9171278","Automatic parking;motion planning;reinforcement learning;Monte Carlo tree search;neural network","Planning;Reinforcement learning;Vehicles;Data models;Safety;Training;Wheels","driver information systems;learning (artificial intelligence);mobile robots;Monte Carlo methods;motion control;neural nets;optimisation;path planning;road traffic control;tree searching","training network;data generation cycle;lateral policies;parking data;filtered data;mass-produced parking system;start parking posture;reinforcement learning-based motion planning;automatic parking system;automatic parking motion planning;multiobjective optimization;high-quality expert drivers;multiobjective parking performance;model-based reinforcement learning method;parking policy;Monte Carlo tree search;MCTS;longitudinal policies;neural network","","31","","38","CCBY","19 Aug 2020","","","IEEE","IEEE Journals"
"An Adaptive Strategy Selection Method With Reinforcement Learning for Robotic Soccer Games","H. Shi; Z. Lin; K. -S. Hwang; S. Yang; J. Chen","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; National Sun Yat-sen University, Kaohsiung, Taiwan; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China","IEEE Access","7 Mar 2018","2018","6","","8376","8386","Robotic soccer games, which have become popular, require timely and precise decisionmaking in a dynamic environment. To address the problems of complexity in a critical situation, policy improvement in robotic soccer games must occur. This paper proposes an adaptive decisionmaking method that uses reinforcement learning (RL), and the decision-making system for a robotic soccer game is composed of two subsystems. The first subsystem in the architecture for the proposed method criticizes the situation, and the second subsystem implements decision-making policy. Inspired by the support vector machine (SVM), a situation classification method, which is called an improved SVM, embeds a decision tree structure and simultaneously addresses the problems of a large scale and multiple classifications. When a variety of situations that are collected in the field are classified and congregated into the tree structure, the problem of local strategy selection for each individual class of situations over time is regarded as a RL problem and is solved using a Q-learning method. The results of simulations and experiments demonstrate that the proposed method allows satisfactory decision-making.","2169-3536","","10.1109/ACCESS.2018.2808266","National Research and Development Plan of China(grant numbers:2017YFB1001900); Fund of National Ministries(grant numbers:2016ZC53022); Fundamental Research Funds for the Central Universities(grant numbers:3102017JSJ0005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8301430","Robotic soccer;support vector machines;reinforcement learning;Q learning","Games;Support vector machines;Decision making;Robot kinematics;Learning (artificial intelligence);Decision trees","decision making;decision trees;learning (artificial intelligence);mobile robots;multi-robot systems;pattern classification;support vector machines","decision-making policy;situation classification method;Q-learning method;adaptive strategy selection method;reinforcement learning;robotic soccer game;policy improvement;adaptive decision making method;support vector machine;decision tree structure;local strategy selection;RL problem;dynamic environment;SVM","","30","","40","OAPA","23 Feb 2018","","","IEEE","IEEE Journals"
"Multi-Agent Deep Reinforcement Learning for Multi-Object Tracker","M. Jiang; T. Hai; Z. Pan; H. Wang; Y. Jia; C. Deng","Jiangsu Laboratory of Lake Environment Remote Sensing Technologies, Huaiyin Institute of Technology, Huaian, China; Computer Science Department, Baoji University of Arts and Sciences, Baoji, China; Digital Media and Interaction Research Center, Hangzhou Normal University, Hangzhou, China; Jiangsu Laboratory of Lake Environment Remote Sensing Technologies, Huaiyin Institute of Technology, Huaian, China; Jiangsu Laboratory of Lake Environment Remote Sensing Technologies, Huaiyin Institute of Technology, Huaian, China; School of Physics and Electronic Information Engineering, Henan Polytechnic University, Jiaozuo, China","IEEE Access","22 Mar 2019","2019","7","","32400","32407","Multi-object tracking has been a key research subject in many computer vision applications. We propose a novel approach based on multi-agent deep reinforcement learning (MADRL) for multi-object tracking to solve the problems in the existing tracking methods, such as a varying number of targets, non-causal, and non-realtime. At first, we choose YOLO V3 to detect the objects included in each frame. Unsuitable candidates were screened out and the rest of detection results are regarded as multiple agents and forming a multi-agent system. Independent Q-Learners (IQL) is used to learn the agents’ policy, in which, each agent treats other agents as part of the environment. Then, we conducted offline learning in the training and online learning during the tracking. Our experiments demonstrate that the use of MADRL achieves better performance than the other state-of-art methods in precision, accuracy, and robustness.","2169-3536","","10.1109/ACCESS.2019.2901300","National Key Research and Development project(grant numbers:2017YFB1002803); Major Program of Natural Science Foundation of the Higher Education Institutions of Jiangsu Province(grant numbers:18KJA520002); Jiangsu Laboratory of Lake Environment Remote Sensing Technologies(grant numbers:JSLERS-2018-005); Six Talent Peaks Project in Jiangsu Province(grant numbers:2016XYDXXJS-012); Natural Science Foundation of Jiangsu Province(grant numbers:BK20171267); 333 high-level talent training project of Jiangsu province(grant numbers:BRA2018333); 533 Talents Engineering Project in Huaian(grant numbers:HAA201738); National Natural Science Foundation of China(grant numbers:61801188); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653482","Multi-object tracking;MADRL;IQL;YOLO V3","Target tracking;Reinforcement learning;Training;Object detection;Real-time systems;Neural networks","computer vision;learning (artificial intelligence);multi-agent systems;object detection;object tracking","multiobject tracker;multiobject tracking;multiagent deep reinforcement learning;multiagent system;online learning;independent Q-learners;IQL;MADRL;YOLO V3;offline learning;computer vision applications","","29","","48","OAPA","26 Feb 2019","","","IEEE","IEEE Journals"
"Rewards Prediction-Based Credit Assignment for Reinforcement Learning With Sparse Binary Rewards","M. Seo; L. F. Vecchietti; S. Lee; D. Har","The Cho Chun Shik Graduate School of Green Transportation, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; The Cho Chun Shik Graduate School of Green Transportation, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; The Cho Chun Shik Graduate School of Green Transportation, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; The Cho Chun Shik Graduate School of Green Transportation, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Access","30 Aug 2019","2019","7","","118776","118791","In reinforcement learning (RL), a reinforcement signal may be infrequent and delayed, not appearing immediately after the action that triggered the reward. To trace back what sequence of actions contributes to delayed rewards, e.g., credit assignment (CA), is one of the biggest challenges in RL. This challenge is aggravated under sparse binary rewards, especially when rewards are given only after successful completion of the task. To this end, a novel method consisting of key-action detection, among a sequence of actions to perform a task under sparse binary rewards, and CA strategy is proposed. The key-action defined as the most important action contributing to the reward is detected by a deep neural network that predicts future rewards based on the environment information. The rewards are re-assigned to the key-action and its adjacent actions, defined as adjacent-key-actions. Such re-assignment process enables increased success rate and convergence speed during training. For efficient re-assignment, two CA strategies are considered as part of proposed method. Proposed method is combined with hindsight experience replay (HER) for experiments in the OpenAI gym suite robotics environment. In the experiments, it is demonstrated that proposed method can detect key-actions and outperform the HER, increasing success rate and convergence speed, in the Fetch slide task, a type of task that is more exacting as compared to other tasks, but is addressed by few publications in the literature. From the experiments, a guideline for selecting CA strategy according to goal location is provided through goal distribution analysis with dot map.","2169-3536","","10.1109/ACCESS.2019.2936863","Institute for Information and communications Technology Promotion; Korean Government (MSIT) (Development of Robot Hand Manipulation Intelligence to Learn Methods and Procedures for Handling Various Objects with Tactile Robot Hands)(grant numbers:2018-0-00677); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809762","Credit assignment;delayed rewards;goal distribution;reinforcement learning;reward shaping","Task analysis;Training;Convergence;Manipulators;Reinforcement learning;Neural networks","learning (artificial intelligence);neural nets","reward prediction based credit assignment;CA strategy;environment information;success rate;hindsight experience replay;HER;OpenAI gym suite robotics environment;Fetch slide task;goal distribution analysis;deep neural network;re-assignment process;convergence speed;adjacent-key-actions;key-action detection;reinforcement signal;sparse binary rewards;reinforcement learning;credit assignment","","28","","26","CCBY","22 Aug 2019","","","IEEE","IEEE Journals"
"Decentralized Control of Multi-Robot System in Cooperative Object Transportation Using Deep Reinforcement Learning","L. Zhang; Y. Sun; A. Barth; O. Ma","Department of Aerospace Engineering and Engineering Mechanics, University of Cincinnati, Cincinnati, OH, USA; Department of Aerospace Engineering and Engineering Mechanics, University of Cincinnati, Cincinnati, OH, USA; Department of Aerospace Engineering and Engineering Mechanics, University of Cincinnati, Cincinnati, OH, USA; Department of Aerospace Engineering and Engineering Mechanics, University of Cincinnati, Cincinnati, OH, USA","IEEE Access","15 Oct 2020","2020","8","","184109","184119","Object transportation could be a challenging problem for a single robot due to the oversize and/or overweight issues. A multi-robot system can take the advantage of increased driving power and more flexible configuration to solve such a problem. However, an increased number of individuals also changed the dynamics of the system which makes control of a multi-robot system more complicated. Even worse, if the whole system is sitting on a centralized decision making unit, the data flow could be easily overloaded due to the upscaling of the system. In this research, we propose a decentralized control scheme on a multi-robot system with each individual equipped with a deep Q-network (DQN) controller to perform an oversized object transportation task. DQN is a deep reinforcement learning algorithm, thus does not require the knowledge of system dynamics, instead, it enables the robots to learn appropriate control strategies through trial-and-error style interactions within the task environment. Since analogous controllers are distributed on the individuals, the computational bottleneck is avoided systematically. We demonstrate such a system in a scenario of carrying an oversized rod through a doorway by a two-robot team. The presented multi-robot system learns abstract features of the task and cooperative behaviors are observed. The decentralized DQN-style controller is showing strong robustness against uncertainties. In addition, We propose a universal metric to assess the cooperation quantitatively.","2169-3536","","10.1109/ACCESS.2020.3025287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9201368","Cooperative object transportation;decentralized control;deep Q-network;multi-robot system","Robot kinematics;Task analysis;Transportation;Robot sensing systems;Multi-robot systems;Heuristic algorithms","decentralised control;decision making;learning (artificial intelligence);mobile robots;multi-robot systems;position control","decentralized DQN-style controller;two-robot team;computational bottleneck;flexible configuration;deep reinforcement learning;decentralized control;system dynamics;oversized object transportation task;deep Q-network controller;multirobot system","","25","","35","CCBY","21 Sep 2020","","","IEEE","IEEE Journals"
"Multi-AUV Collaborative Target Recognition Based on Transfer-Reinforcement Learning","L. Cai; Q. Sun; T. Xu; Y. Ma; Z. Chen","School of Artificial Intelligence, Henan Institute of Science and Technology, Xinxiang, China; School of Information Engineering, Henan Institute of Science and Technology, Xinxiang, China; School of Artificial Intelligence, Henan Institute of Science and Technology, Xinxiang, China; School of Artificial Intelligence, Henan Institute of Science and Technology, Xinxiang, China; School of Control Science and Engineering, Shandong University, Jinan, China","IEEE Access","3 Mar 2020","2020","8","","39273","39284","Due to the existence of unfavorable factors such as turbid water quality and target occlusion, it is difficult to obtain valid data of target features. Due to the repeated calculation of similar data, the real-time performance of the algorithm is poor. In view of the above problems, this paper proposes a multi-AUV collaborative target recognition method based on transfer-reinforcement learning. The features of the target information which is collected by multi-AUV are fused based on wavelet transformation and affine invariance. The similarity of features is calculated by Mahalanobis distance and the learning model is selected autonomously based on the similarity threshold. Based on the Q-learning reinforcement learning model, the target information under the interference environment is trained intensively, and the effective features are extracted and stored in the source domain, which can reduce the impact of the environmental interference on the target recognition. The feature transfer learning model based on deep confidence network transfers the feature data of the source domain to the target domain, reducing the repeated calculation of similar data, and then ensuring the real-time performance of the algorithm. Simulation experiments are conducted in the SUN dataset under five underwater environments (turbid water, target occlusion, insufficient light, complex background, and overlapping targets), and the results demonstrate that the proposed model achieves better performance.","2169-3536","","10.1109/ACCESS.2020.2976121","National Natural Science Foundation of China(grant numbers:61703143); Science and Technology Project of Henan Province(grant numbers:192102310260); Henan University(grant numbers:2017GGJS123); Science and Technology Major Special Project of Xinxiang City(grant numbers:ZD18006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9007726","Small sample;target recognition;multi-AUV collaboration;reinforcement learning;transfer learning","Target recognition;Feature extraction;Collaboration;Training;Sensors;Real-time systems;Interference","autonomous aerial vehicles;groupware;learning (artificial intelligence);neural nets;object recognition;wavelet transforms","target occlusion;overlapping targets;transfer-reinforcement learning;turbid water;target information;wavelet transformation;affine invariance;Q-learning reinforcement learning model;feature transfer learning;insufficient light;complex background;multiAUV collaborative target recognition;deep confidence network transfers;target recognition;Mahalanobis distance","","25","","35","CCBY","24 Feb 2020","","","IEEE","IEEE Journals"
"Anti-Jamming Communications in UAV Swarms: A Reinforcement Learning Approach","J. Peng; Z. Zhang; Q. Wu; B. Zhang","Artificial Intelligence Research Center, National Innovation Institute of Defense Technology, Beijing, China; College of Computer, National University of Defense Technology, Changsha, China; College of Electronic Science, National University of Defense Technology, Changsha, China; Artificial Intelligence Research Center, National Innovation Institute of Defense Technology, Beijing, China","IEEE Access","20 Dec 2019","2019","7","","180532","180543","Intelligent unmanned aerial vehicle (UAV) swarm may accomplish complex tasks through cooperation, relying on inter-UAV communications. This paper aims to improve the communication performance of intelligent UAV swarm system in the presence of jamming, by multi-parameter programming and reinforcement learning. This paper considers a communication system, where the communication between a UAV swarm and the base station is jammed by multiple interferers. Compared with the existing work, the UAVs in the system can exploit degree-of-freedom in frequency, motion and antenna spatial domain to optimize the communication quality in the receiving area. This paper proposes a modified Q-Learning algorithm based on multi-parameter programming, where a cost is introduced to strike a balance between the motion and communication performance of the UAVs. The simulation results show the effectiveness of the algorithm.","2169-3536","","10.1109/ACCESS.2019.2958328","National Natural Science Foundation of China(grant numbers:91648204,61601486); National University of Defense Technology(grant numbers:ZDYYJCYJ140601); State Key Laboratory of High Performance Computing Project Fund(grant numbers:1502-02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8928502","Intelligent UAV swarm;anti-jamming communication;multi-parameter joint programming;antenna pattern;motion cost","Jamming;Unmanned aerial vehicles;Learning (artificial intelligence);Communication systems;Programming;Relays;Receivers","autonomous aerial vehicles;control engineering computing;jamming;learning (artificial intelligence);multi-robot systems","communication quality;modified Q-learning algorithm;multiparameter programming;anti-jamming communications;reinforcement learning approach;intelligent unmanned aerial vehicle swarm;inter-UAV communications;intelligent UAV swarm system","","24","","26","CCBY","9 Dec 2019","","","IEEE","IEEE Journals"
"Real-Time Object Navigation With Deep Neural Networks and Hierarchical Reinforcement Learning","A. Staroverov; D. A. Yudin; I. Belkin; V. Adeshkin; Y. K. Solomentsev; A. I. Panov","Moscow Institute of Physics and Technology, Dolgoprudny, Russia; Moscow Institute of Physics and Technology, Dolgoprudny, Russia; Moscow Institute of Physics and Technology, Dolgoprudny, Russia; Moscow Institute of Physics and Technology, Dolgoprudny, Russia; Moscow Institute of Physics and Technology, Dolgoprudny, Russia; Moscow Institute of Physics and Technology, Dolgoprudny, Russia","IEEE Access","4 Nov 2020","2020","8","","195608","195621","In the last years, deep learning and reinforcement learning methods have significantly improved mobile robots in such fields as perception, navigation, and planning. But there are still gaps in applying these methods to real robots due to the low computational efficiency of recent neural network architectures and their poor adaptability to robotic experiments' realities. In this article, we consider an important task in mobile robotics - navigation to an object using an RGB-D camera. We develop a new neural network framework for robot control that is fast and resistant to possible noise in sensors and actuators. We propose an original integration of semantic segmentation, mapping, localization, and reinforcement learning methods to improve the effectiveness of exploring the environment, finding the desired object, and quickly navigating to it. We created a new HISNav dataset based on the Habitat virtual environment, which allowed us to use simulation experiments to pre-train the model and then upload it to a real robot. Our architecture is adapted to work in a real-time environment and fully implements modern trends in this area.","2169-3536","","10.1109/ACCESS.2020.3034524","Russian Science Foundation [Theoretical Investigation and Methodology (Sections I–V)](grant numbers:20-71-10116); Government of the Russian Federation [Experimental Evaluation (Section VI)](grant numbers:075-02-2019-967); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9241850","Indoor navigation;cognitive robotics;object segmentation;neural networks;intelligent agents;real-time systems;robot learning","Neural networks;Navigation;Real-time systems;Task analysis;Reinforcement learning;Simultaneous localization and mapping","learning (artificial intelligence);mobile robots;neural nets","deep learning;reinforcement learning methods;mobile robots;computational efficiency;neural network architectures;robotic experiments;mobile robotics;neural network framework;robot control;real-time object navigation","","24","","61","CCBY","28 Oct 2020","","","IEEE","IEEE Journals"
"MARLA-SG: Multi-Agent Reinforcement Learning Algorithm for Efficient Demand Response in Smart Grid","S. Aladdin; S. El-Tantawy; M. M. Fouda; A. S. Tag Eldien","Department of Electrical Engineering, Faculty of Engineering at Shoubra, Benha University, Cairo, Egypt; Department of Engineering Mathematics and Physics, Faculty of Engineering, Cairo University, Giza, Egypt; Department of Electrical Engineering, Faculty of Engineering at Shoubra, Benha University, Cairo, Egypt; Department of Electrical Engineering, Faculty of Engineering at Shoubra, Benha University, Cairo, Egypt","IEEE Access","2 Dec 2020","2020","8","","210626","210639","The population is sharply growing in the last decade, resulting in non-potential power requests in dense urban areas, especially with the traditional power grid where the system is not compatible with the infrequent changes. Smart grids have shown strong potential to effectively mitigate and smooth power consumption curves to avoid shortages by adjusting and forecasting the cost function in real-time in response to consumption fluctuations to achieve the desired objectives. The main challenge for the smart grid designers is to reduce the cost and Peak to Average Ratio (PAR) while maintaining the desired satisfaction level. This article presents the development and evaluation of a Multi-Agent Reinforcement Learning Algorithm for efficient demand response in Smart Grid (MARLA-SG). Also, it shows a simple and flexible way of choosing state elements to reduce the possible number of states, regardless of the device type, range of operation, and maximum allowable delay. It also produces a simple way to represent the reward function regardless of the used cost function. SARSA (State-Action-Reward-State-Action) and Q-learning schemes are used and attained PAR reduction of 9.6%, 12.16%, and an average cost reduction of 10.2%, 7.8%, respectively.","2169-3536","","10.1109/ACCESS.2020.3038863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9262925","Smart grid;demand response;reinforcement learning;Q-learning;SARSA (State Action Reward State Action)","Smart grids;Task analysis;Reinforcement learning;Load management;Peak to average power ratio;Smart meters;Delays","cost reduction;learning (artificial intelligence);multi-agent systems;power consumption;power engineering computing;smart power grids","multiagent reinforcement learning;demand response;nonpotential power requests;dense urban areas;power grid;power consumption curves;cost function;consumption fluctuations;smart grid designers;satisfaction level;MARLA-SG;State-Action-Reward-State-Action;average cost reduction","","24","","32","CCBY","18 Nov 2020","","","IEEE","IEEE Journals"
"Unexpected Collision Avoidance Driving Strategy Using Deep Reinforcement Learning","M. Kim; S. Lee; J. Lim; J. Choi; S. G. Kang","School of Mechanical Engineering, Yonsei University, Seoul, South Korea; School of Mechanical Engineering, Yonsei University, Seoul, South Korea; School of Mechanical Engineering, Yonsei University, Seoul, South Korea; School of Mechanical Engineering, Yonsei University, Seoul, South Korea; Institute of Science and Technology, Korea University, Sejong, South Korea","IEEE Access","28 Jan 2020","2020","8","","17243","17252","In this paper, we generated intelligent self-driving policies that minimize the injury severity in unexpected traffic signal violation scenarios at an intersection using the deep reinforcement learning. We provided guidance on reward engineering in terms of the multiplicity of objective function. We used a deep deterministic policy gradient method in the simulated environment to train self-driving agents. We designed two agents, one with a single-objective reward function of collision avoidance and the other with a multi-objective reward function of both collision avoidance and goal-approaching. We evaluated their performances by comparing the percentages of collision avoidance and the average injury severity against those of human drivers and an autonomous emergency braking (AEB) system. The percentage of collision avoidance of our agents were 78.89% higher than human drivers and 84.70% higher than the AEB system. The average injury severity score of our agents were only 8.92% of human drivers and 6.25% of the AEB system.","2169-3536","","10.1109/ACCESS.2020.2967509","National Research Foundation of Korea; Ministry of Science, ICT and Future Planning(grant numbers:2018R1A2B6008063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8961990","Autonomous vehicles;collision avoidance;intelligent vehicles;injury severity;multi-objective optimization;reinforcement learning","Reinforcement learning;Collision avoidance;Injuries;Vehicles;Gradient methods;Wheels;Laser radar","braking;collision avoidance;gradient methods;injuries;learning (artificial intelligence);neural nets;road accidents;road safety;road traffic;traffic engineering computing","autonomous emergency braking system;objective function multiplicity;traffic signal violation scenarios;collision avoidance driving strategy;reward engineering;self-driving policies;deep reinforcement learning;injury severity;AEB system;multiobjective reward function;single-objective reward function;deep deterministic policy gradient method","","22","","23","CCBY","17 Jan 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Target Searching in Cognitive Electronic Warfare","S. You; M. Diao; L. Gao","College of Information and Communication, Harbin Engineering University, Harbin, China; College of Information and Communication, Harbin Engineering University, Harbin, China; College of Information and Communication, Harbin Engineering University, Harbin, China","IEEE Access","1 Apr 2019","2019","7","","37432","37447","The recent appreciation of deep reinforcement learning (DRL) arises from its successes in many domains, but the applications of DRL in practical engineering are still unsatisfactory, including optimizing control strategies in cognitive electronic warfare (CEW). CEW is a massive and challenging project, and due to the sensitivity of the data sources, there are few open studies that have investigated CEW. Moreover, the spatial sparsity, continuous action, and partially observable environment that exist in CEW have greatly limited the abilities of DRL algorithms, which strongly depend on state-value and action-value functions. In this paper, we use Python to build a 3-D space game named Explorer to simulate various CEW environments in which the electronic attacker is an unmanned combat air vehicle (UCAV) and the defender is an observation station, both of which are equipped with radar as the observation sensor. In our game, the UCAV needs to accomplish the task of detecting the target as early as possible to perform follow-up tracking and guidance tasks. To allow an ""infant"" UCAV to understand what ""target searching"" is, we train the UCAV's maneuvering strategies by means of a well-designed reward shaping, a simplified constant accelerated motion control, and a deep deterministic policy gradient (DDPG) algorithm based on a generative model and variational Bayesian estimation. The experimental results show that when the operating cycle is 0.2 s, the search success rate of the trained UCAV in 10000 episodes is improved by 33.36% compared with the benchmark, and the target destruction rate is similarly improved by 57.84%.","2169-3536","","10.1109/ACCESS.2019.2905649","National Natural Science Foundation of China(grant numbers:61701134); Equipment Pre-Research Fund of China(grant numbers:61404150101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668391","Deep reinforcement learning;cognitive electronic warfare;motion planning;deep deterministic policy gradient;variational Bayesian estimation;target searching","Task analysis;Games;Planning;Reinforcement learning;Electronic warfare;Radar;Search problems","autonomous aerial vehicles;Bayes methods;electronic warfare;gradient methods;military aircraft;motion control","deep reinforcement learning;cognitive electronic warfare;practical engineering;data sources;open studies;spatial sparsity;continuous action;partially observable environment;DRL algorithms;state-value;action-value functions;3-D space game;CEW environments;electronic attacker;unmanned combat air vehicle;observation station;observation sensor;guidance tasks;infant UCAV;simplified constant accelerated motion control;deep deterministic policy gradient algorithm;search success rate;trained UCAV;target destruction rate;control strategies optimization;tracking tasks;target searching;maneuvering strategies;generative model;variational Bayesian estimation;reward shaping;Python","","22","","45","OAPA","17 Mar 2019","","","IEEE","IEEE Journals"
"A Deep Reinforcement Learning Approach for the Patrolling Problem of Water Resources Through Autonomous Surface Vehicles: The Ypacarai Lake Case","S. Y. Luis; D. G. Reina; S. L. T. Marín","Department of Electronic Engineering, Technical School of Engineering, University of Seville, Sevilla, Spain; Department of Electronic Engineering, Technical School of Engineering, University of Seville, Sevilla, Spain; Department of Electronic Engineering, Technical School of Engineering, University of Seville, Sevilla, Spain","IEEE Access","17 Nov 2020","2020","8","","204076","204093","Autonomous Surfaces Vehicles (ASV) are incredibly useful for the continuous monitoring and exploring task of water resources due to their autonomy, mobility, and relative low cost. In the path planning context, the patrolling problem is usually addressed with heuristics approaches, such as Genetic Algorithms (GA) or Reinforcement Learning (RL) because of the complexity and high dimensionality of the problem. In this paper, the patrolling problem of Ypacarai Lake (Asunción, Paraguay) has been formulated as a Markov Decision Process (MDP) for two possible cases: the homogeneous and the non-homogeneous scenarios. A tailored reward function has been designed for the non-homogeneous case. Two Deep Reinforcement Learning algorithms such as Deep Q-Learning (DQL) and Double Deep Q-Learning (DDQL) have been evaluated to solve the patrolling problem. Furthermore, due to the high number of parameters and hyperparameters involved in the algorithms, a thorough search has been conducted to find the best values for training the neural networks and the proposed reward function. According to the results, a suitable configuration of the parameters allows better results for coverage, obtaining more than the 93% of the lake surface on average. In addition, the proposed approach achieves higher sample redundancy of important zones than other common-used algorithms for non-homogeneous coverage path planning such as Policy Gradient, lawnmower algorithm or random exploration, achieving an 64% improvement of the mean time between visits.","2169-3536","","10.1109/ACCESS.2020.3036938","Universidad de Sevilla under the contract “Contratos de acceso al Sistema Español de Ciencia, Tecnología e Innovación para el desarrollo del programa propio de I+D+i de la Universidad de Sevilla,”; Spanish “Ministerio de Ciencia, innovación y Universidades, Programa Estatal de I+D+i Orientada a los Retos de la Sociedad” through the Project “Despliegue Adaptativo de Vehículos no Tripulados para Gestión Ambiental en Escenarios Dinámicos”(grant numbers:RTI2018-098964-B-I00); regional government Junta de Andalucía through the Projects “Despliegue Inteligente de una red de Vehículos Acuáticos no Tripulados para la monitorización de Recursos Hídricos”(grant numbers:US-1257508); “Despliegue y Control de una Red Inteligente de Vehículos Autónomos Acuáticos para la Monitorización de Recursos Hídricos Andaluces”(grant numbers:PY18-RE0009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252944","Deep reinforcement learning;monitoring;path planning;autonomous surface vehicle;patrolling;complete coverage","Reinforcement learning;Lakes;Markov processes;Task analysis;Vehicle dynamics;Water resources;Genetic algorithms","genetic algorithms;learning (artificial intelligence);marine vehicles;Markov processes;mobile robots;neural nets;path planning;remotely operated vehicles","deep reinforcement learning;autonomous surface vehicles;nonhomogeneous coverage path;lake surface;Double Deep Q-Learning;patrolling problem;path planning context;continuous monitoring;Ypacarai Lake case;water resources","","21","","37","CCBY","9 Nov 2020","","","IEEE","IEEE Journals"
"Imitation Reinforcement Learning-Based Remote Rotary Inverted Pendulum Control in OpenFlow Network","J. -B. Kim; H. -K. Lim; C. -M. Kim; M. -S. Kim; Y. -G. Hong; Y. -H. Han","Department of Computer Science Engineering, Korea University of Technology and Education, Cheonan, South Korea; Department of Interdisciplinary Program in Creative Engineering, Korea University of Technology and Education, Cheonan, South Korea; Advanced Technology Research Institute, Cheonan, South Korea; Department of Knowledge-Converged Super Brain Convergence Research, Electronics and Telecommunications Research Institute, Daejeon, South Korea; Department of Knowledge-Converged Super Brain Convergence Research, Electronics and Telecommunications Research Institute, Daejeon, South Korea; Department of Computer Science Engineering, Korea University of Technology and Education, Cheonan, South Korea","IEEE Access","1 Apr 2019","2019","7","","36682","36690","Rotary inverted pendulum is an unstable and highly nonlinear device and has been used as a common application model in nonlinear control engineering field. In this paper, we use a rotary inverted pendulum as a deep reinforcement learning environment. The real device is composed of a cyber environment and physical environment based on the OpenFlow network, and the MQTT protocol is used on the Ethernet connection to connect the cyber environment and the physical environment. The reinforcement learning agent is learned to control the real device located remotely from the controller, and the classical PID controller is also utilized to implement the imitation reinforcement learning and facilitate the learning process. From our CPS-based experimental system, we verify that a deep reinforcement learning agent can successfully control the real device located remotely from the agent, and our imitation learning strategy can make the learning time reduced effectively.","2169-3536","","10.1109/ACCESS.2019.2905621","National Research Foundation of Korea; Ministry of Education(grant numbers:2018R1A6A1A03025526,2016R1D1A3B03933355); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668411","Reinforcement learning;remote control;control engineering;OpenFlow;CPS","Reinforcement learning;Aerospace electronics;Mathematical model;Optical switches;Process control","control engineering computing;learning (artificial intelligence);nonlinear control systems;pendulums;telecontrol;three-term control","OpenFlow network;unstable device;highly nonlinear device;common application model;nonlinear control engineering field;deep reinforcement learning environment;cyber environment;physical environment;Ethernet connection;classical PID controller;learning process;CPS-based experimental system;deep reinforcement learning agent;imitation learning strategy;learning time;MQTT protocol;imitation reinforcement learning-based remote rotary inverted pendulum control","","20","","22","OAPA","17 Mar 2019","","","IEEE","IEEE Journals"
"A Reinforcement Learning-Based QAM/PSK Symbol Synchronizer","M. Matta; G. C. Cardarilli; L. Di Nunzio; R. Fazzolari; D. Giardino; A. Nannarelli; M. Re; S. Spanò","Department of Electronic Engineering, University of Rome Tor Vergata, Rome, Italy; Department of Electronic Engineering, University of Rome Tor Vergata, Rome, Italy; Department of Electronic Engineering, University of Rome Tor Vergata, Rome, Italy; Department of Electronic Engineering, University of Rome Tor Vergata, Rome, Italy; Department of Electronic Engineering, University of Rome Tor Vergata, Rome, Italy; Department of Applied Mathematics and Computer Science, Danmarks Tekniske Universitet, Kgs. Lyngby, Denmark; Department of Electronic Engineering, University of Rome Tor Vergata, Rome, Italy; Department of Electronic Engineering, University of Rome Tor Vergata, Rome, Italy","IEEE Access","12 Sep 2019","2019","7","","124147","124157","Machine Learning (ML) based on supervised and unsupervised learning models has been recently applied in the telecommunication field. However, such techniques rely on application-specific large datasets and the performance deteriorates if the statistics of the inference data changes over time. Reinforcement Learning (RL) is a solution to these issues because it is able to adapt its behavior to the changing statistics of the input data. In this work, we propose the design of an RL Agent able to learn the behavior of a Timing Recovery Loop (TRL) through the Q-Learning algorithm. The Agent is compatible with popular PSK and QAM formats. We validated the RL synchronizer by comparing it to the Mueller and Müller TRL in terms of Modulation Error Ratio (MER) in a noisy channel scenario. The results show a good trade-off in terms of MER performance. The RL based synchronizer loses less than 1 dB of MER with respect to the conventional one but it is able to adapt its behavior to different modulation formats without the need of any tuning for the system parameters.","2169-3536","","10.1109/ACCESS.2019.2938390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8819991","Artificial intelligence;machine learning;reinforcement learning;Q-learning;synchronization;timing recovery loop","Synchronization;Training;Reinforcement learning;Modulation;Task analysis","phase shift keying;quadrature amplitude modulation;synchronisation;telecommunication computing;unsupervised learning","inference data changes;changing statistics;RL Agent;Timing Recovery Loop;RL synchronizer;MER performance;RL based synchronizer;supervised learning models;unsupervised learning models;telecommunication field;application-specific large datasets;Müller TRL;modulation error ratio;Q-Learning algorithm;QAM formats;PSK formats","","18","","49","CCBY","29 Aug 2019","","","IEEE","IEEE Journals"
"Efficient Training Techniques for Multi-Agent Reinforcement Learning in Combat Tasks","G. Zhang; Y. Li; X. Xu; H. Dai","Computer School, National University of Defense Technology, Changsha, China; Artificial Intelligence Research Center, National Innovation Institute of Defense Technology, Beijing, China; Artificial Intelligence Research Center, National Innovation Institute of Defense Technology, Beijing, China; Artificial Intelligence Research Center, National Innovation Institute of Defense Technology, Beijing, China","IEEE Access","16 Aug 2019","2019","7","","109301","109310","Multi-agent combat scenarios often appear in many real-time strategy games. Efficient learning for such scenarios is an indispensable step towards general artificial intelligence. Multi-agent reinforcement learning (MARL) algorithms have attracted much interests, but few of them have been shown effective for such scenarios. Most of previous research is focused on revising the learning mechanism of MARL algorithms, for example, trying different types of neural networks. The study of training techniques for improving the performance of MARL algorithms has not been paid much attention. In this paper we propose three efficient training techniques for a multi-agent combat problem which is originated from an unmanned aerial vehicle (UAV) combat scenario. The first one is the scenario-transfer training, which utilizes the experience obtained in simpler combat tasks to assist the training for complex tasks. The next one is the self-play training, which can continuously improve the performance by iteratively training agents and their counterparts. Finally, we consider using combat rules to assist the training, which is named as the rule-coupled training. We combine the three training techniques with two popular multi-agent reinforcement learning methods, multi-agent deep q-learning and multi-agent deep deterministic policy gradient (proposed by Open AI in 2017), respectively. The results show that both the converging speed and the performance of the two methods are significantly improved through the three training techniques.","2169-3536","","10.1109/ACCESS.2019.2933454","National Defense Science and Technology Foundation for Young Scientists of China(grant numbers:030403); National Natural Science Foundation of China(grant numbers:11801563,91648204,61532007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789448","Scenario-transfer training;self-play training;rule-coupled training","Training;Task analysis;Reinforcement learning;Games;Green products;Neural networks;Acceleration","autonomous aerial vehicles;computer games;control engineering computing;learning (artificial intelligence);military computing;multi-agent systems","learning mechanism;MARL algorithms;efficient training techniques;multiagent combat problem;unmanned aerial vehicle combat scenario;scenario-transfer training;simpler combat tasks;self-play training;training agents;combat rules;rule-coupled training;popular multiagent reinforcement;multiagent deep q-learning;multiagent deep deterministic policy gradient;multiagent combat scenarios;real-time strategy games;general artificial intelligence;multiagent reinforcement learning algorithms","","16","","37","CCBY","6 Aug 2019","","","IEEE","IEEE Journals"
"Hierarchical Reinforcement Learning for Autonomous Decision Making and Motion Planning of Intelligent Vehicles","Y. Lu; X. Xu; X. Zhang; L. Qian; X. Zhou","College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; Unmanned System Technology Research Center, National Innovation Institute of Defense Technology, Beijing, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China","IEEE Access","1 Dec 2020","2020","8","","209776","209789","Autonomous decision making and motion planning in complex dynamic traffic environments, such as left-turn without traffic signals and multi-lane merging from side-ways, are still challenging tasks for intelligent vehicles. It is difficult to generate optimized behavior decisions while considering the motion capabilities and dynamic properties of intelligent vehicles. Aiming at the above problems, this article proposes a hierarchical reinforcement learning approach for autonomous decision making and motion planning in complex dynamic traffic scenarios. The proposed approach consists of two layers. At the higher layer, a kernel-based least-squares policy iteration algorithm with uneven sampling and pooling strategy (USP-KLSPI) is presented for solving the decision-making problems. The motion capabilities of the ego vehicle and the surrounding vehicles are evaluated with a high-fidelity dynamic model in the decision-making layer. By doing so, the consistency between the decisions generated at the higher layer and the operations in the lower planning layer can be well guaranteed. The lower layer addresses the motion-planning problem in the lateral direction using a dual heuristic programming (DHP) algorithm learned in a batch-mode manner, while the velocity profile in the longitudinal direction is inherited from the higher layer. Extensive simulations are conducted in complex traffic conditions including left-turn without traffic signals and multi-lane merging from side-ways scenarios. The results demonstrate the effectiveness and efficiency of the proposed approach in realizing optimized decision making and motion planning in complex environments.","2169-3536","","10.1109/ACCESS.2020.3034225","National Natural Science Foundation of China(grant numbers:61751311,61825305); National Key Research and Development Program of China(grant numbers:2018YFB1305105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9241055","Autonomous driving;hierarchical reinforcement learning;complex dynamic traffics;decision making;motion planning","Decision making;Planning;Vehicle dynamics;Dynamics;Reinforcement learning;Heuristic algorithms;Intelligent vehicles","decision making;dynamic programming;heuristic programming;iterative methods;learning (artificial intelligence);least squares approximations;mobile robots;path planning;road traffic;road vehicles","autonomous decision making;motion planning;intelligent vehicles;complex dynamic traffic environments;left-turn;traffic signals;multilane merging;optimized behavior decisions;motion capabilities;dynamic properties;hierarchical reinforcement learning approach;complex dynamic traffic scenarios;kernel-based least-squares policy iteration algorithm;decision-making problems;ego vehicle;high-fidelity dynamic model;decision-making layer;lower planning layer;lower layer addresses;motion-planning problem;complex traffic conditions;realizing optimized decision making","","15","","41","CCBY","27 Oct 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning for Position Control Problem of a Mobile Robot","G. Farias; G. Garcia; G. Montenegro; E. Fabregas; S. Dormido-Canto; S. Dormido","Escuela de Ingeniería Eléctrica, Pontificia Universidad Católica de Valparaíso, Valparaíso, Chile; Department of Ocean and Mechanical Engineering, Florida Atlantic University (FAU), Boca Raton, USA; Escuela de Ingeniería Eléctrica, Pontificia Universidad Católica de Valparaíso, Valparaíso, Chile; Departamento de Informática y Automática, Universidad Nacional de Educación a Distancia, Madrid, Spain; Departamento de Informática y Automática, Universidad Nacional de Educación a Distancia, Madrid, Spain; Departamento de Informática y Automática, Universidad Nacional de Educación a Distancia, Madrid, Spain","IEEE Access","27 Aug 2020","2020","8","","152941","152951","Due to the increase in complexity in autonomous vehicles, most of the existing control systems are proving to be inadequate. Reinforcement Learning is gaining traction as it is posed to overcome these difficulties in a natural way. This approach allows an agent that interacts with the environment to get rewards for appropriate actions, learning to improve its performance continuously. The article describes the design and development of an algorithm to control the position of a wheeled mobile robot using Reinforcement Learning. One main advantage of this approach concerning traditional control algorithms is that the learning process is carried out automatically with a recursive procedure forward in time. Moreover, given the fidelity of the model for the particular implementation described in this work, the whole learning process can be carried out in simulation. This fact avoids damages to the actual robot during the learning stage. It shows that the position control of the robot (or similar specific tasks) can be done without the need to know the dynamic model of the system explicitly. Its main drawback is that the learning stage can take a long time to finish and that it depends on the complexity of the task and the availability of adequate hardware resources. This work provides a comparison between the proposed approach and traditional existing control laws in simulation and real environments. The article also discusses the main effects of using different controlled variables in the performance of the developed control law.","2169-3536","","10.1109/ACCESS.2020.3018026","Chilean National Research and Development Agency (ANID) through the Project Fondo Nacional de Desarrollo Científico y Tecnológico (FONDECYT)(grant numbers:1191188); Dirección de Investigación (DI)-Pontificia Universidad Católica de Valparaíso(grant numbers:039.437); Spanish Ministry of Economy and Competitiveness through the Projects(grant numbers:RTI2018-094665-B-I00,ENE2015-64914-C3-3-R); Spanish Ministry of Science and Innovation through the Project(grant numbers:PID2019-108377RB-C32); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9171241","Mobile robot;position control;reinforcement learning","Mobile robots;Reinforcement learning;Position control;Robot kinematics;Task analysis;Navigation","learning (artificial intelligence);mobile robots;position control","position control problem;reinforcement learning;control law;controlled variables;learning process;wheeled mobile robot","","15","","39","CCBY","19 Aug 2020","","","IEEE","IEEE Journals"
"IADRL: Imitation Augmented Deep Reinforcement Learning Enabled UGV-UAV Coalition for Tasking in Complex Environments","J. Zhang; Z. Yu; S. Mao; S. C. G. Periaswamy; J. Patton; X. Xia","RFID Laboratory, Auburn University, Auburn, USA; RFID Laboratory, Auburn University, Auburn, USA; Department of Electrical and Computer Engineering, Auburn University, Auburn, USA; RFID Laboratory, Auburn University, Auburn, USA; RFID Laboratory, Auburn University, Auburn, USA; Department of Electrical and Computer Engineering, Auburn University, Auburn, USA","IEEE Access","9 Jun 2020","2020","8","","102335","102347","Recent developments in Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) have made them highly useful for various tasks. However, they both have their respective constraints that make them incapable of completing intricate tasks alone in many scenarios. For example, a UGV is unable to reach high places, while a UAV is limited by its power supply and payload capacity. In this paper, we propose an Imitation Augmented Deep Reinforcement Learning (IADRL) model that enables a UGV and UAV to form a coalition that is complementary and cooperative for completing tasks that they are incapable of achieving alone. IADRL learns the underlying complementary behaviors of UGVs and UAVs from a demonstration dataset that is collected from some simple scenarios with non-optimized strategies. Based on observations from the UGV and UAV, IADRL provides an optimized policy for the UGV-UAV coalition to work in an complementary way while minimizing the cost. We evaluate the IADRL approach in an visual game-based simulation platform, and conduct experiments that show how it effectively enables the coalition to cooperatively and cost-effectively accomplish tasks.","2169-3536","","10.1109/ACCESS.2020.2997304","NSF(grant numbers:ECCS-1923163); RFID Laboratory and the Wireless Engineering Research and Education Center (WEREC), Auburn University, Auburn, AL, USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9099309","Unmanned aerial vehicle (UAV);unmanned ground vehicle (UGV);coalition;deep reinforcement learning (DRL);imitation learning","Task analysis;Unmanned aerial vehicles;Machine learning;Navigation;Resource management;Gallium nitride;Land vehicles","aerospace robotics;autonomous aerial vehicles;learning (artificial intelligence);mobile robots;multi-robot systems;telerobotics","power supply;payload capacity;IADRL approach;imitation augmented deep reinforcement learning enabled UGV-UAV coalition;unmanned aerial vehicles;unmanned ground vehicles;nonoptimized strategies;cost minimization","","15","","39","CCBY","25 May 2020","","","IEEE","IEEE Journals"
"Optimal Path-Planning of Nonholonomic Terrain Robots for Dynamic Obstacle Avoidance Using Single-Time Velocity Estimator and Reinforcement Learning Approach","H. Taghavifar; B. Xu; L. Taghavifar; Y. Qin","School of Mechanical, Aerospace and Automotive Engineering, Coventry University, Coventry, U.K.; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; Department of Electrical and Electronics Engineering, IAU, Tehran, Iran; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China","IEEE Access","8 Nov 2019","2019","7","","159347","159356","A single-time velocity estimator-based reinforcement learning (RL) algorithm, integrated with a chaotic metaheuristic optimization technique is proposed in this article for the optimal path-planning of the nonholonomic robots considering a moving/stationary obstacle avoidance strategy. The additional contribution of the present study is by employing the Terramechanics principles to incorporate the effects of wheel sinkage into the deformable terrain on the dynamics of the robot aiming to find the optimal compensating force/torque magnitude to sustain a robust and smooth motion. The designed systematic control-oriented system incorporates a cost function of weighted components associated with the target-tracking and the obstacle avoidance. The designed velocity estimator contributes to the finite-state Markov decision process (MDP) in order to train the transition probabilities of the problem objectives. Based on the obtained results, the optimal solution for the Q-learning in terms of the adjusting factor for the minimized tracking error and obstacle collision risk propagation profiles is found at 0.22. The results further confirm the promising capacity of the proposed optimization-based RL algorithm for the collision avoidance control of the nonholonomic robots on deformable terrains.","2169-3536","","10.1109/ACCESS.2019.2950166","National Natural Science Foundation of China(grant numbers:51505031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8886591","Mechatronics;terramechanics;path-planning;artificial intelligence","Wheels;Mobile robots;Soil;Collision avoidance;Stress;Force","collision avoidance;force control;learning (artificial intelligence);Markov processes;mobile robots;motion control;optimal control;optimisation;torque control;wheels","minimized tracking error;obstacle collision risk propagation profiles;optimization-based RL algorithm;collision avoidance control;nonholonomic robots;deformable terrain;optimal path-planning;nonholonomic terrain robots;dynamic obstacle avoidance;reinforcement learning approach;single-time velocity estimator-based reinforcement learning algorithm;chaotic metaheuristic optimization technique;robust motion;smooth motion;designed systematic control-oriented system;designed velocity estimator;finite-state Markov decision process","","15","","37","CCBY","30 Oct 2019","","","IEEE","IEEE Journals"
"UGV Navigation Optimization Aided by Reinforcement Learning-Based Path Tracking","M. Wei; S. Wang; J. Zheng; D. Chen","National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China","IEEE Access","25 Oct 2018","2018","6","","57814","57825","The success of robotic, such as UGV systems, largely benefits from the fundamental capability of autonomously finding collision-free path(s) to commit mobile tasks in routinely rough and complicated environments. Optimization of navigation under such circumstance has long been an open problem: 1) to meet the critical requirements of this task typically including the shortest distance and smoothness and 2) more challengingly, to enable a general solution to track the optimal path in real-time outdoor applications. Aiming at the problem, this study develops a two-tier approach to navigation optimization in terms of path planning and tracking. First, a “rope”model has been designed to mimic the deformation of a path in axial direction under external force and the fixedness of the radial plane to contain a UGV in a collision-free space. Second, a deterministic policy gradient (DPG) algorithm has been trained efficiently on abstracted structures of an arbitrarily derived “rope”to model the controller for tracking the optimal path. The learned policy can be generalized to a variety of scenarios. Experiments have been performed over complicated environments of different types. The results indicate that: 1) the rope model helps in minimizing distance and enhancing smoothness of the path, while guarantees the clearance; 2) the DPG can be modeled quickly (in a couple of minutes on an office desktop) and the model can apply to environments of increasing complexity under the circumstance of external disturbances without the need for tuning parameters; and 3) the DPG-based controller can autonomously adjust the UGV to follow the correct path free of risks by itself.","2169-3536","","10.1109/ACCESS.2018.2872751","National Natural Science Foundation of China(grant numbers:61772380); Foundation for Innovative Research Groups of Hubei Province(grant numbers:2017CFA007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8476521","UGV navigation;reinforcement learning;deterministic policy gradient;path tracking","Robots;Navigation;Optimization;Path planning;Force;Aerospace electronics;Task analysis","collision avoidance;control engineering computing;learning (artificial intelligence);mobile robots;navigation;remotely operated vehicles","unmanned ground vehicle;path deformation;UGV navigation optimization;collision-free path;DPG-based controller;rope model;deterministic policy gradient algorithm;collision-free space;path planning;real-time outdoor applications;UGV systems;reinforcement learning-based path tracking","","14","","47","OAPA","28 Sep 2018","","","IEEE","IEEE Journals"
"A Hybrid Tracking Control Strategy for Nonholonomic Wheeled Mobile Robot Incorporating Deep Reinforcement Learning Approach","X. Gao; R. Gao; P. Liang; Q. Zhang; R. Deng; W. Zhu","School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China","IEEE Access","26 Jan 2021","2021","9","","15592","15602","Tracking control is an essential capability for nonholonomic wheeled mobile robots (NWMR) to achieve autonomous navigation. This paper presents a novel hybrid control strategy combined mode-based control and actor-critic based deep reinforcement learning method. Based on the Lyapunov method, a kinematics control law named given control is obtained with pose errors. Then, the tracking control problem is converted to a finite Markov decision process, in which the defined state contains current tracking errors, given control inputs and one-step errors. After training with deep deterministic policy gradient method, the action named acquired control inputs is capable of compensating the existing errors. Thus, the hybrid control strategy is obtained under velocity constraint, acceleration constraint and bounded uncertainty. A cumulative error is also defined as a criteria to evaluate tracking performance. The comparison results in simulation demonstrate that our proposed method have an obviously advantage on both tracking accuracy and training efficiency.","2169-3536","","10.1109/ACCESS.2021.3053396","National Key Research and Development Program of China(grant numbers:2019YFB1309600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9330502","Deep reinforcement learning;tracking control;kinematics control;hybrid control strategy;nonholonomic wheeled mobile robot","Mobile robots;Kinematics;Robots;Robot kinematics;Reinforcement learning;Uncertainty;Vehicle dynamics","adaptive control;control system synthesis;gradient methods;learning (artificial intelligence);Lyapunov methods;Markov processes;mobile robots;position control;robot kinematics;stability;tracking;uncertain systems","deep deterministic policy gradient method;existing errors;cumulative error;tracking performance;tracking accuracy;hybrid tracking control strategy;mobile robot incorporating deep reinforcement learning approach;essential capability;nonholonomic wheeled mobile robots;novel hybrid control strategy combined mode-based control;Lyapunov method;kinematics control law;pose errors;tracking control problem;finite Markov decision process;current tracking errors;given control inputs","","13","","33","CCBYNCND","21 Jan 2021","","","IEEE","IEEE Journals"
"Autonomous Control of Combat Unmanned Aerial Vehicles to Evade Surface-to-Air Missiles Using Deep Reinforcement Learning","G. T. Lee; C. O. Kim","Department of Industrial Engineering, Yonsei University, Seoul, South Korea; Department of Industrial Engineering, Yonsei University, Seoul, South Korea","IEEE Access","29 Dec 2020","2020","8","","226724","226736","This paper proposes a new reinforcement learning approach for executing combat unmanned aerial vehicle (CUAV) missions. We consider missions with the following goals: guided missile avoidance, shortest-path flight and formation flight. For reinforcement learning, the representation of the current agent state is important. We propose a novel method of using the coordinates and angle of a CUAV to effectively represent its state. Furthermore, we develop a reinforcement learning algorithm with enhanced exploration through amplification of the imitation effect (AIE). This algorithm consists of self-imitation learning and random network distillation algorithms. We assert that these two algorithms complement each other and that combining them amplifies the imitation effect for exploration. Empirical results show that the proposed AIE approach is highly effective at finding a CUAV’s shortest-flight path while avoiding enemy missiles. Test results confirm that with our method, a single CUAV reaches its target from its starting point 95% of the time and a squadron of four simultaneously operating CUAVs reaches the target 70% of the time.","2169-3536","","10.1109/ACCESS.2020.3046284","Agency for Defense Development(grant numbers:UD170043JD); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301300","Deep reinforcement learning;combat unmanned aerial vehicle;deep learning;autonomous flight management system;path planning;exploration","Missiles;Reinforcement learning;Games;Unmanned aerial vehicles;Mathematical model;Task analysis","aircraft control;aircraft navigation;autonomous aerial vehicles;learning (artificial intelligence);military aircraft;neural nets","surface-to-air missiles;deep reinforcement learning approach;combat unmanned aerial vehicle missions;guided missile avoidance;shortest-path flight;formation flight;reinforcement learning algorithm;amplification of the imitation effect;self-imitation learning;random network distillation algorithms;AIE approach;CUAV's shortest-flight path;enemy missiles;single CUAV;autonomous control;combat unmanned aerial vehicles","","13","","41","CCBY","21 Dec 2020","","","IEEE","IEEE Journals"
"Robotic Information Gathering With Reinforcement Learning Assisted by Domain Knowledge: An Application to Gas Source Localization","T. Wiedemann; C. Vlaicu; J. Josifovski; A. Viseras","Institute of Communications and Navigation, German Aerospace Center (DLR), Wessling, Germany; Institute of Communications and Navigation, German Aerospace Center (DLR), Wessling, Germany; Department of Computer Science, Technische Universität München (TUM), Munich, Germany; Institute of Communications and Navigation, German Aerospace Center (DLR), Wessling, Germany","IEEE Access","22 Jan 2021","2021","9","","13159","13172","Gas source localization tackles the problem of finding leakages of hazardous substances such as poisonous gases or radiation in the event of a disaster. In order to avoid threats for human operators, autonomous robots dispatched for localizing potential gas sources are preferable. This work investigates a Reinforcement Learning framework that allows a robotic agent to learn how to localize gas sources. We propose a solution that assists Reinforcement Learning with existing domain knowledge based on a model of the gas dispersion process. In particular, we incorporate a priori domain knowledge by designing appropriate rewards and observation inputs for the Reinforcement Learning algorithm. We show that a robot trained with our proposed method outperforms state-of-the-art gas source localization strategies, as well as robots that are trained without additional domain knowledge. Furthermore, the framework developed in this work can also be generalized to a large variety of information gathering tasks.","2169-3536","","10.1109/ACCESS.2021.3052024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326418","Gas source localization;information gathering;reinforcement learning;mobile robot;deep learning","Robots;Task analysis;Location awareness;Mathematical model;Dispersion;Robot sensing systems;Reinforcement learning","learning (artificial intelligence);mobile robots;multi-robot systems;robots","robotic information gathering;Reinforcement Learning assisted;poisonous gases;radiation;autonomous robots;potential gas sources;Reinforcement Learning framework;robotic agent;gas dispersion process;a priori domain knowledge;Reinforcement Learning algorithm;state-of-the-art gas source localization strategies;additional domain knowledge;information gathering tasks","","12","","39","CCBY","18 Jan 2021","","","IEEE","IEEE Journals"
"Ball Motion Control in the Table Tennis Robot System Using Time-Series Deep Reinforcement Learning","L. Yang; H. Zhang; X. Zhu; X. Sheng","School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Pongbot Technology Company Ltd., Shanghai, China; School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Access","20 Jul 2021","2021","9","","99816","99827","One of the biggest challenges hindering a table tennis robot to play as well as a professional player is the ball’s accurate motion control, which depends on various factors such as the incoming ball’s position, linear, spin velocity and so forth. Unfortunately, some factors are almost impossible to be directly measured in real practice, such as the ball’s spin velocity, which is difficult to be estimated from vision due to the little texture on the ball’s surface. To perform accurate motion control in table tennis, this study proposes to learn a ball stroke strategy to guarantee desirable “target landing location” and the “over-net height” which are two key indicators to evaluate the quality of a stroke. To overcome the spin velocity challenge, a deep reinforcement learning (DRL) based stroke approach is developed with the spin velocity estimation capability, through which the system can predict the relative spin velocity of the ball and stroke it back accurately by iteratively learning from the robot-environment interactions. To pre-train the DRL-based strategy effectively, this paper develops a virtual table tennis playing environment, through which various simulated data can be collected. For the real table tennis robot implementation, experimental results demonstrate the superior performance of the proposed control strategy compared to that of the traditional aerodynamics-based method with an average landing error around 80mm and the landing-within-table probability higher than 70%.","2169-3536","","10.1109/ACCESS.2021.3093340","Science and Technology Commission of Shanghai Municipality(grant numbers:18JC1410400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9467347","Ball motion control;reinforcement learning;spin velocity estimation;table tennis robot","Sports;Robots;Trajectory;Sports equipment;Motion control;Estimation;Atmospheric modeling","","","","12","","36","CCBY","29 Jun 2021","","","IEEE","IEEE Journals"
"Decision Controller for Object Tracking With Deep Reinforcement Learning","Z. Zhong; Z. Yang; W. Feng; W. Wu; Y. Hu; C. -L. Liu","University of Chinese Academy of Sciences, Beijing, China; Sensetime Research Institute, Beijing, China; Sensetime Research Institute, Beijing, China; Sensetime Research Institute, Beijing, China; Sensetime Research Institute, Beijing, China; CAS Center for Excellence in Brain Science and Intelligence Technology, University of Chinese Academy of Sciences, Beijing, China","IEEE Access","15 Mar 2019","2019","7","","28069","28079","There are many decisions which are usually made heuristically both in single object tracking (SOT) and multiple object tracking (MOT). Existing methods focus on tackling decision-making problems on special tasks in tracking without a unified framework. In this paper, we propose a decision controller (DC) which is generally applicable to both SOT and MOT tasks. The controller learns an optimal decision-making policy with a deep reinforcement learning algorithm that maximizes long term tracking performance without supervision. To prove the generalization ability of DC, we apply it to the challenging ensemble problem in SOT and tracker-detector switching problem in MOT. In the tracker ensemble experiment, our ensemble-based tracker can achieve leading performance in VOT2016 challenge and the light version can also get a state-of-the-art result at 50 FPS. In the MOT experiment, we utilize the tracker-detector switching controller to enable real-time online tracking with competitive performance and  ${10\times }$  speed up.","2169-3536","","10.1109/ACCESS.2019.2900476","National Natural Science Foundation of China(grant numbers:61721004,61633021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648377","Computer vision;deep learning;object tracking;reinforcement learning","Object tracking;Task analysis;Switches;Reinforcement learning;Target tracking;Real-time systems","learning (artificial intelligence);object detection;object tracking","single object tracking;SOT;multiple object tracking;decision-making problems;decision controller;DC;optimal decision-making policy;deep reinforcement learning algorithm;tracker ensemble experiment;MOT experiment;tracker-detector switching controller;realtime online tracking","","12","","55","OAPA","21 Feb 2019","","","IEEE","IEEE Journals"
"Optimal Tracking Current Control of Switched Reluctance Motor Drives Using Reinforcement Q-Learning Scheduling","H. Alharkan; S. Saadatmand; M. Ferdowsi; P. Shamsi","Electrical Engineering Department, Missouri University of Science and Technology, Rolla, MO, USA; Electrical Engineering Department, Missouri University of Science and Technology, Rolla, MO, USA; Electrical Engineering Department, Missouri University of Science and Technology, Rolla, MO, USA; Electrical Engineering Department, Missouri University of Science and Technology, Rolla, MO, USA","IEEE Access","20 Jan 2021","2021","9","","9926","9936","In this article, a novel Q-learning scheduling method for the current controller of a switched reluctance motor (SRM) drive is investigated. The Q-learning algorithm is a class of reinforcement learning approaches that can find the best forward-in-time solution of a linear control problem. An augmented system is constructed based on the reference current signal and the SRM model to allow for solving the algebraic Riccati equation of the current-tracking problem. This article introduces a new scheduled-Q-learning algorithm that utilizes a table of Q-cores that lies on the nonlinear surface of an SRM model without involving any information about the model parameters to track the reference current trajectory by scheduling the infinite horizon linear quadratic trackers (LQT) handled by Q-learning algorithms. Additionally, a linear interpolation algorithm is proposed to improve the transition of the LQT between trained Q-cores to ensure a smooth response as state variables evolve on the nonlinear surface of the model. Lastly, simulation and experimental results are provided to validate the effectiveness of the proposed control scheme.","2169-3536","","10.1109/ACCESS.2021.3050167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9317726","Adaptive dynamic programming;current control;least square methods;motor drive;optimal control;reinforcement learning;switched reluctance motors","Mathematical model;Adaptation models;Rotors;Predictive models;Current control;Trajectory;Inductance","control system synthesis;electric current control;infinite horizon;interpolation;learning (artificial intelligence);linear quadratic control;linear systems;machine control;nonlinear control systems;optimal control;reluctance motor drives;reluctance motors;Riccati equations","infinite horizon linear quadratic trackers;linear interpolation algorithm;nonlinear surface;control scheme;optimal tracking current control;switched reluctance motor drive;reinforcement Q-learning;Q-learning scheduling method;current controller;reinforcement learning approaches;forward-in-time solution;linear control problem;augmented system;reference current signal;SRM model;algebraic Riccati equation;current-tracking problem;scheduled-Q-learning algorithm;model parameters;reference current trajectory","","11","","38","CCBYNCND","8 Jan 2021","","","IEEE","IEEE Journals"
"A Deep Reinforcement Learning-Based Dynamic Computational Offloading Method for Cloud Robotics","M. Penmetcha; B. -C. Min","Department of Computer and Information Technology, SMART Laboratory, Purdue University, West Lafayette, IN, USA; Department of Computer and Information Technology, SMART Laboratory, Purdue University, West Lafayette, IN, USA","IEEE Access","23 Apr 2021","2021","9","","60265","60279","Robots come with a variety of computing capabilities, and running computationally-intense applications on robots is sometimes challenging on account of limited onboard computing, storage, and power capabilities. Meanwhile, cloud computing provides on-demand computing capabilities, and thus combining robots with cloud computing can overcome the resource constraints robots face. The key to effectively offloading tasks is an application solution that does not underutilize the robot's own computational capabilities and makes decisions based on crucial cost parameters such as latency and CPU availability. In this paper, we formulate the application offloading problem as a Markovian decision process and propose a deep reinforcement learning-based deep Q-network (DQN) approach. The state-space is formulated with the assumption that input data size directly impacts application execution time. The proposed algorithm is designed as a continuous task problem with discrete action space; i.e., we apply a choice of action at each time step and use the corresponding outcome to train the DQN to acquire the maximum rewards possible. To validate the proposed algorithm, we designed and implemented a robot navigation testbed. The results demonstrated that for the given state-space values, the proposed algorithm learned to take appropriate actions to reduce application latency and also learned a policy that takes actions based on input data size. Finally, we compared the proposed DQN algorithm with a long short-term memory (LSTM) algorithm in terms of accuracy. When trained and validated on the same dataset, the proposed DQN algorithm obtained at least 9 percentage points greater accuracy than the LSTM algorithm.","2169-3536","","10.1109/ACCESS.2021.3073902","Purdue University Libraries Open Access Publishing Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406800","Cloud robotics;deep reinforcement learning;deep Q-networks (DQN);AWS;neural networks;application offloading;robot navigation","Robots;Heuristic algorithms;Cloud computing;Robot sensing systems;Task analysis;Reinforcement learning;Mobile handsets","cloud computing;learning (artificial intelligence);Markov processes;mobile computing;mobile robots","deep reinforcement learning-based dynamic computational offloading method;cloud robotics;computationally-intense applications;onboard computing;power capabilities;cloud computing;on-demand computing capabilities;resource constraints robots face;effectively offloading tasks;application solution;crucial cost parameters;latency CPU availability;application offloading problem;Markovian decision process;deep reinforcement learning-based deep Q-network approach;input data size;application execution time;continuous task problem;discrete action space;robot navigation;given state-space values;DQN algorithm","","11","","78","CCBY","19 Apr 2021","","","IEEE","IEEE Journals"
"Multi-Agent Deep Reinforcement Learning for Sectional AGC Dispatch","J. Li; T. Yu; H. Zhu; F. Li; D. Lin; Z. Li","College of Electric Power, South China University of Technology, Guangzhou, China; College of Electric Power, South China University of Technology, Guangzhou, China; College of Electric Power, South China University of Technology, Guangzhou, China; College of Electric Power, South China University of Technology, Guangzhou, China; College of Electric Power, South China University of Technology, Guangzhou, China; College of Electric Power, South China University of Technology, Guangzhou, China","IEEE Access","7 Sep 2020","2020","8","","158067","158081","Aiming at the problem of coordinating system economy, security and control performance in secondary frequency regulation of the power grid, a sectional automatic generation control (AGC) dispatch framework is proposed. The dispatch of AGC is classified as three sections with the sectional dispatch method. Besides, a hierarchical multi-agent deep deterministic policy gradient (HMA-DDPG) algorithm is proposed for the framework in this paper. This algorithm, considering economy and security of the system in AGC dispatch, can ensure the control performance of AGC. Furthermore, through simulation, the control effect of the sectional dispatch method and several AGC dispatch methods on the Guangdong province power grid system and the IEEE 39 bus system is compared. The result shows that the best effect can be achieved with the sectional dispatch method.","2169-3536","","10.1109/ACCESS.2020.3019929","National Natural Science Foundation of China(grant numbers:51777078); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9178795","Automatic generation control;hierarchical multi-agent deep deterministic policy gradient;sectional AGC dispatch;reinforcement learning","Automatic generation control;Security;Power grids;Training;Phasor measurement units;Optimization;Voltage measurement","control engineering computing;frequency control;gradient methods;learning (artificial intelligence);multi-agent systems;neural nets;power engineering computing;power generation control;power generation dispatch;power grids;power system economics;power system security","multiagent deep reinforcement learning;sectional AGC dispatch;power grid security;power grid frequency regulation;AGC dispatch methods;power grid system economy;sectional automatic generation control dispatch;hierarchical multiagent deep deterministic policy gradient algorithm;HMA-DDPG algorithm","","10","","22","CCBY","27 Aug 2020","","","IEEE","IEEE Journals"
"A Data-Driven Automatic Design Method for Electric Machines Based on Reinforcement Learning and Evolutionary Optimization","T. Sato; M. Fujita","Toshiba Energy Systems & Solutions Corporation, Yokohama, Japan; Toshiba Energy Systems & Solutions Corporation, Yokohama, Japan","IEEE Access","18 May 2021","2021","9","","71284","71294","The design problems of electric machines are actually treated as a kind of mixed-integer problem, because the machine shapes are defined by some integer variables, such as number of slots, and the other variables, such as the tooth width, which are here called the fundamental and shape variables, respectively. To automatically solve these design problems, this article presents an automatic design method by combining the reinforcement learning and evolutionary optimization. In the proposed method, the decision process is modeled as a tree structure to seek for the fundamental variables, which are determined as a result of the tree search depending on the value functions of the nodes. Then, the shape variables are estimated from the function of the fundamental variables. These functions are constructed based on the design data, to generate which the reinforcement learning and evolutionary optimization are employed. As a result, the proposed method can automatically be adapted to unexperienced design problems through the data generation and function learning. The proposed method is applied to a design problem of a linear induction motor. It is shown that the machine designs with the prescribed performance for given specifications are automatically obtained. Moreover, it is also shown that the acceptable candidate designs can immediately be generated when the given specification is similar to the previously-solved problems by utilizing the design data generated by the past explorations.","2169-3536","","10.1109/ACCESS.2021.3078668","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9427216","CMA-ES;design optimization;electric machine;evolutionary algorithms;reinforcement learning;tree search","Optimization;Reactive power;Electric machines;Reinforcement learning;Design methodology;Shape;Stochastic processes","data handling;electric machines;evolutionary computation;integer programming;learning (artificial intelligence);linear induction motors;power engineering computing;tree searching","fundamental variables;design data;function learning;tree structure;decision process;data driven automatic design method;fundamental shape variables;integer variables;machine shapes;mixed integer problem;design problem;electric machines;machine designs;data generation;unexperienced design problems;evolutionary optimization;reinforcement learning","","10","","41","CCBYNCND","10 May 2021","","","IEEE","IEEE Journals"
"Batch Prioritization in Multigoal Reinforcement Learning","L. F. Vecchietti; T. Kim; K. Choi; J. Hong; D. Har","Cho Chun Shik Graduate School of Green Transportation, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Cho Chun Shik Graduate School of Green Transportation, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Cho Chun Shik Graduate School of Green Transportation, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Energy IT, Gachon University, Seongnam, South Korea; Cho Chun Shik Graduate School of Green Transportation, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Access","4 Aug 2020","2020","8","","137449","137461","In multigoal reinforcement learning, an agent interacts with an environment and learns to achieve multiple goals. The goal-conditioned policy is trained to effectively generalize its behavior for multiple goals. During training, the experiences collected by the agent are randomly sampled from a replay buffer. Because biased sampling of achieved goals affects the success rate of a given task, it should be avoided by considering the valid goal space, introduced here as the set of goals to achieve, and the current competence of the policy. To this end, a novel prioritization method for creation of batches, e.g., collections of samples, is proposed. Candidate batches are sampled and associated with costs; in each iteration the batch with the minimum cost is chosen to train the policy. The cost function is modeled by an intended goal, which is proposed as a hypothetical goal that the policy is trying to learn in each cycle, and the information of the valid goal space. The minimum cost of the batch selected for each iteration decreases throughout training as the policy learns to achieve goals near the center of the valid goal space. The proposed batch prioritization method is combined with hindsight experience replay (HER) for experiments in robotic control tasks presented in the OpenAI gym suite to demonstrate learning performance comparable to that of other state-of-the-art prioritization methods. As a result, the proposed batch prioritization method can achieve improved learning performance in 4 out of 5 tasks, particularly for harder tasks. The experimental results suggest that the proposed method for the creation of training batches, using the valid goal space information and current competence of the policy, can enhance learning performance in multigoal tasks with high-dimensional goal space.","2169-3536","","10.1109/ACCESS.2020.3012204","National Research Foundation of Korea (NRF); Ministry of Science, ICT, through the Energy Cloud Research and Development Program(grant numbers:2019M3F2A1073314); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9149884","Experience replay;batch prioritization;goal distribution;reinforcement learning;intended goal","Training;Task analysis;Learning (artificial intelligence);Robots;Erbium;Cost function;Aerospace electronics","learning (artificial intelligence)","intended goal;hypothetical goal;batch prioritization method;learning performance;training batches;multigoal tasks;high-dimensional goal space;multigoal reinforcement learning;multiple goals;goal-conditioned policy;biased sampling;prioritization method;goal space information","","10","","33","CCBY","27 Jul 2020","","","IEEE","IEEE Journals"
"Online Scheduling Optimization for DAG-Based Requests Through Reinforcement Learning in Collaboration Edge Networks","Y. Zhang; Z. Zhou; Z. Shi; L. Meng; Z. Zhang","School of Information Engineering, China University of Geosciences (Beijing), Beijing, China; School of Information Engineering, China University of Geosciences (Beijing), Beijing, China; PetroChina Research Institute of Petroleum Exploration and Development, Beijing, China; College of Science and Engineering, Ritsumeikan University, Shiga, Japan; School of Software Engineering, Beijing Jiaotong University, Beijing, China","IEEE Access","28 Apr 2020","2020","8","","72985","72996","The wide-adoption of edge computing promotes the scheduling of tasks in complex requests upon smart devices on the network edge, whereas tasks are necessary to be offloaded to the cloud when they are intensive in computational and energy resources. Traditional techniques explore mostly the scheduling of atomic tasks, whereas complex requests scheduling on edge servers is the challenge unexplored extensively. To address this challenge, this paper proposes an online task scheduling optimization for DAG-based requests at the network edge, where this scheduling procedure is modeled as Markov decision process, in which system state, request and decision space are formally specified. A temporal-difference learning based mechanism is adopted to learn an optimal tasks allocation strategy at each decision stage. Extensive experiments are conducted, and evaluation results demonstrate that our technique can effectively reduce the system's long-term average delay and energy consumption in comparison with the state-of-art's counterparts.","2169-3536","","10.1109/ACCESS.2020.2987574","National Natural Science Foundation of China(grant numbers:61772479,61662021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9064775","Online DAG-based request optimization;edge computing;temporal-difference learning;Internet of Things","Task analysis;Servers;Optimization;Collaboration;Processor scheduling;Quality of experience;Mobile handsets","learning (artificial intelligence);Markov processes;optimisation;resource allocation;scheduling","edge servers;online task scheduling optimization;DAG-based requests;network edge;scheduling procedure;Markov decision process;decision space;temporal-difference learning based mechanism;optimal task allocation strategy;online scheduling optimization;reinforcement learning;collaboration edge networks;edge computing;smart devices;computational energy resources;atomic tasks;complex request scheduling","","10","","35","CCBY","13 Apr 2020","","","IEEE","IEEE Journals"
"Efficient Novelty Search Through Deep Reinforcement Learning","L. Shi; S. Li; Q. Zheng; M. Yao; G. Pan","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China","IEEE Access","21 Jul 2020","2020","8","","128809","128818","Novelty search, which was inspired by the nature that evolves creatures with diversity, has shown great potential in solving reinforcement learning (RL) tasks with sparse and deceptive rewards. However, most of the existing novelty search methods evolve the populations through hybrization and mutation, which is inefficient in diverging populations. In this paper, we propose a method which incorporates deep RL with novelty search to improve the efficiency of diverging the populations for novelty search. We first propose a strategy that improves the novelty of individuals generated by genetic algorithm using reinforcement learning. Based on this strategy, we propose a framework that incorporates deep RL with novelty search, and then derive an algorithm to improve the search efficiency of the novelty search for continuous control tasks. Our experimental results show that our method can improve the search efficiency of novelty search and can also provide a competitive performance compared to some of the existing novelty search methods. The implementation of our method is available at: https://github.com/shilx001/NoveltySearch_Improvement.","2169-3536","","10.1109/ACCESS.2020.3008735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139203","Reinforcement learning;novelty search;evolutionary computing;deep learning","Sociology;Statistics;Task analysis;Reinforcement learning;Optimization methods;Search problems","genetic algorithms;learning (artificial intelligence);search problems","deep reinforcement learning;reinforcement learning tasks;deep RL;novelty search methods;continuous control tasks","","10","","34","CCBY","13 Jul 2020","","","IEEE","IEEE Journals"
"Collaborative Decision-Making Method for Multi-UAV Based on Multiagent Reinforcement Learning","S. Li; Y. Jia; F. Yang; Q. Qin; H. Gao; Y. Zhou","School of Aeronautic Science and Engineering, Beihang University, Beijing, China; School of Aeronautic Science and Engineering, Beihang University, Beijing, China; School of Aeronautic Science and Engineering, Beihang University, Beijing, China; School of Aeronautic Science and Engineering, Beihang University, Beijing, China; School of Aeronautic Science and Engineering, Beihang University, Beijing, China; School of Aeronautic Science and Engineering, Beihang University, Beijing, China","IEEE Access","5 Sep 2022","2022","10","","91385","91396","The collaborative mission capability of multi-UAV has received more and more attention in recent years as the research on multi-UAV theories and applications has intensified. The artificial intelligence technology integrated into the multi-UAV collaborative decision-making system can effectively improve the collaborative mission capability of multi-UAV. We propose a multi-agent reinforcement learning algorithm for multi-UAV collaborative decision-making. Our approach is based on the actor-critic algorithm, where each UAV is treated as an actor that collects data decentralized in the environment. A centralized critic provides evaluation information for each training step during the centralized training of these actors. We introduce a gate recurrent unit in the actor to enable the UAV to make reasonable decisions concerning historical decision information. Moreover, we use an attention mechanism to design the centralized critic, which can achieve better learning in a complex environment. Finally, the algorithm is trained and experimented in a multi-UAV air combat scenario developed in the collaborative decision-making environment. The experimental results show that our approach can learn collaborative decision-making strategies with excellent performance, while convergence performance is better compared to other algorithms.","2169-3536","","10.1109/ACCESS.2022.3199070","National Natural Science Foundation of China(grant numbers:61903014); Aeronautical Science Foundation of China(grant numbers:20200017051001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857929","UAV;multi-UAV;collaborative decision-making;multi-agent reinforcement learning","Collaboration;Decision making;Autonomous aerial vehicles;Prediction algorithms;Task analysis;Training data;Heuristic algorithms;Reinforcement learning;Multi-agent systems","artificial intelligence;autonomous aerial vehicles;decision making;learning (artificial intelligence);military aircraft;multi-agent systems;multi-robot systems","collaborative decision-making method;multiagent reinforcement learning;collaborative mission capability;multiUAV theories;multiUAV collaborative decision-making system;actor-critic algorithm;UAV;centralized critic;historical decision information;multiUAV air combat scenario;collaborative decision-making environment;collaborative decision-making strategies","","10","","33","CCBYNCND","16 Aug 2022","","","IEEE","IEEE Journals"
"A New Reinforcement Learning Based Adaptive Sliding Mode Control Scheme for Free-Floating Space Robotic Manipulator","Z. Xie; T. Sun; T. H. Kwan; Z. Mu; X. Wu","School of Aerospace, Mechanical and Mechatronic Engineering, The University of Sydney, Sydney, Australia; School of Aerospace, Mechanical and Mechatronic Engineering, The University of Sydney, Sydney, Australia; Department of Thermal Science and Energy Engineering, University of Science and Technology of China, Heifei, China; School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, China; School of Aerospace, Mechanical and Mechatronic Engineering, The University of Sydney, Sydney, Australia","IEEE Access","20 Jul 2020","2020","8","","127048","127064","This paper presents a new adaptive small chattering sliding mode control (SCSMC) scheme that uses reinforcement learning (RL) and time-delay estimation (TDE) for the motion control of free-floating space robotic manipulators (FSRM) subject to model uncertainty and external disturbance. The proposed sliding mode control scheme can achieve small chattering effects and improve the tracking accuracy by using a new adaptive law for the switching gain and a RL-based robust term to handle the control inputs. In SCSMC, the complicated multiple-input-multiple-output (MIMO) uncertain system of FSRM is transformed into multiple single-input-single-output (SISO) known subsystems with bounded estimation errors by the TDE technique and state feedback compensation. Subsequently, once the sliding variable is inside the designed manifold, the derivative of the switching gain for each subsystem becomes a negative hyperbolic tangent function of the associated sliding variable, which offers the ability to reduce chattering by decreasing the switching gain. Moreover, the RL based robust term for each subsystem is designed to avoid the loss of tracking accuracy caused by the aforementioned switching gain drop. The tracking errors are proven to be uniformly-ultimately-bounded (UUB) with an arbitrarily small bound by using the Lyapunov theory. The effectiveness of the proposed control scheme is verified by numerical simulations.","2169-3536","","10.1109/ACCESS.2020.3008399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138387","Reinforcement learning;sliding mode control;space robotic manipulator;time delay estimation","Switches;Sliding mode control;Manipulator dynamics;Aerospace electronics;Uncertainty","adaptive control;aerospace robotics;closed loop systems;control system synthesis;learning (artificial intelligence);Lyapunov methods;manipulators;motion control;neurocontrollers;nonlinear control systems;robust control;state feedback;uncertain systems;variable structure systems","adaptive sliding mode control scheme;free-floating space robotic manipulator;time-delay;motion control;model uncertainty;external disturbance;chattering effects;tracking accuracy;adaptive law;RL-based robust term;control inputs;multiple single-input-single-output known subsystems;bounded estimation errors;state feedback compensation;associated sliding variable;tracking errors;reinforcement learning;switching gain drop","","9","","69","CCBY","10 Jul 2020","","","IEEE","IEEE Journals"
"Sample Efficient Reinforcement Learning Method via High Efficient Episodic Memory","D. Yang; X. Qin; X. Xu; C. Li; G. Wei","CAS Key Laboratory of Wireless-Optical Communications, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Wireless-Optical Communications, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Wireless-Optical Communications, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Wireless-Optical Communications, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Wireless-Optical Communications, University of Science and Technology of China, Hefei, China","IEEE Access","21 Jul 2020","2020","8","","129274","129284","Reinforcement Learning (RL), especially Deep Reinforcement Learning (DRL), has made great progress in many areas, such as robots, video games and driving. However, sample inefficiency is a big obstacle to the widespread practical application of DRL. Inspired by the decision making in human brain, this problem can be solved by incorporating instance based learning, i.e. episodic memory. Many episodic memory based RL algorithms have emerged recently. However, these algorithms either only replace parametric DRL algorithm with episodic control or incorporate episodic memory in a single component of DRL. In contrast to preview works, this paper proposes a new sample-efficient reinforcement learning architecture which introduces a new episodic memory module and incorporates episodic thought into some key components of DRL: exploration, experience replay and loss function. Taking Deep Q-Network (DQN) algorithm for example, when combined with DQN, our algorithm is called High Efficient Episodic Memory DQN (HE-EMDQN). In HE-EMDQN, a new non-parametric episodic memory module is introduced to help calculate the loss and modify the predicted value for exploration. For the sake of accelerating the sample learning in experience replay, an auxiliary small buffer called percentile best episode replay memory is designed to compose a mixed mini-batch. We show across the testing environments that our algorithm is significantly more powerful and sample-efficient than DQN and the recent episodic memory deep q-network (EMDQN). This work provides a new perspective for other RL algorithms to improve sample efficiency by utilising episodic memory efficiently.","2169-3536","","10.1109/ACCESS.2020.3009329","National Basic Research Program of China (973 Program)(grant numbers:2018YFA0701603); Natural Science Foundation of Anhui Province(grant numbers:2008085MF213); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9141230","Episodic memory;sample efficiency;reinforcement learning;deep learning","Learning (artificial intelligence);Memory modules;Games;Decision making;Complexity theory;Machine learning;Training","decision making;interactive systems;learning (artificial intelligence);robots","parametric DRL algorithm;episodic control;sample-efficient reinforcement learning architecture;episodic thought;experience replay;nonparametric episodic memory module;sample learning;percentile best episode replay memory;sample efficiency;episodic memory deep q-network;high efficient episodic memory DQN;deep reinforcement learning","","9","","44","CCBY","15 Jul 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Tie-Line Power Adjustment Method for Power System Operation State Calculation","H. Xu; Z. Yu; Q. Zheng; J. Hou; Y. Wei; Z. Zhang","China Electric Power Research Institute, Beijing, China; China Electric Power Research Institute, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; China Electric Power Research Institute, Beijing, China; China Electric Power Research Institute, Beijing, China; State Grid Beijing Electric Power Dispatching and Control Center, Beijing, China","IEEE Access","1 Nov 2019","2019","7","","156160","156174","Operation state calculation (OSC) provides safe operating boundaries for power systems. The operators rely on the software-aid OSC results to dispatch the generators for grid control. Currently, the OSC workload has increased dramatically, as the power grid structure expands rapidly to mitigate renewable source integration. However, the OSC is processed with a lot of manual interventions in most dispatching centers, which makes the OSC error-prone and personnel-experience oriented. Therefore, it is crucial to upgrade the current OSC in an automatic mode for efficiency and quality improvements. An essential process in the OSC is the tie-line power (TP) adjustment. In this paper, a new TP adjustment method is proposed using an adaptive mapping strategy and a Markov Decision Process (MDP) formulation. Then, a model-free deep reinforcement learning (DRL) algorithm is proposed to solve the formulated MDP and learn an optimal adjustment strategy. The improvement techniques of “stepwise training” and “prioritized target replay” are included to decompose the large-scale complex problems and improve the training efficiency. Finally, five experiments are conducted on the IEEE 39-bus system and an actual 2725-bus power grid of China for the effectiveness demonstration.","2169-3536","","10.1109/ACCESS.2019.2949480","Adaptive Generation and Interactive Adjustment Technology of Power Grid Operation State Based on Online Data(grant numbers:5442XT190011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8882242","Operation state calculation;tie-line power adjustment;deep reinforcement learning;stepwise training;prioritized target replay","Load flow;Generators;Power system stability;Reinforcement learning;Power grids;Training","decision making;fuzzy control;learning (artificial intelligence);Markov processes;optimisation;power grids;power system control;power system faults;power system interconnection;power system stability","optimal adjustment strategy;IEEE 39-bus system;model-free deep reinforcement learning algorithm;Markov decision process formulation;TP adjustment method;quality improvements;current OSC;OSC error-prone;dispatching centers;renewable source integration;power grid structure;OSC workload;grid control;software-aid OSC;power systems;safe operating boundaries;power system operation state calculation;tie-line power adjustment method","","8","","39","CCBY","24 Oct 2019","","","IEEE","IEEE Journals"
"A Reinforcement Learning-Based Framework for Robot Manipulation Skill Acquisition","D. Liu; Z. Wang; B. Lu; M. Cong; H. Yu; Q. Zou","School of Mechanical Engineering, Dalian University of Technology, Dalian, China; School of Mechanical Engineering, Dalian University of Technology, Dalian, China; School of Mechanical Engineering, Dalian University of Technology, Dalian, China; School of Mechanical Engineering, Dalian University of Technology, Dalian, China; School of Mechanical Engineering, Dalian University of Technology, Dalian, China; School of Mechanical Engineering, Dalian University of Technology, Dalian, China","IEEE Access","17 Jun 2020","2020","8","","108429","108437","This paper studies robot manipulation skill acquisition based on a proposed reinforcement learning framework. Robot can learn policy autonomously by interacting with environment with a better learning efficiency. Aiming at the manipulator operation task, a reward function design method based on objects configuration matching (OCM) is proposed. It is simple and suitable for most Pick and Place skills learning. Integrating robot and object state, high-level action set and the designed reward function, the Markov model of robot manipulator is built. An improved Proximal Policy Optimize algorithm with manipulation set as the output of Actor (MAPPO) is proposed as the main structure to construct the robot reinforcement learning framework. The framework combines with the Markov model to learn and optimize the skill policy. A same simulation environment as the real robot is set up, and three robot manipulation tasks are designed to verify the effectiveness and feasibility of the reinforcement learning framework for skill acquisition.","2169-3536","","10.1109/ACCESS.2020.3001130","National Natural Science Foundation of China(grant numbers:61873045); Natural Science Foundation of Jiangsu Province(grant numbers:BK20180190); Dalian Sci & Tech Innovation Foundation Program(grant numbers:2019J12GX043); Fundamental Research Funds for the Central Universities(grant numbers:DUT19JC56); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9112186","Robot skill acquisition;reinforcement learning;reword function;MAPPO","Reinforcement learning;Robot sensing systems;Task analysis;Service robots;End effectors","learning (artificial intelligence);manipulators;Markov processes;optimisation;robots","MAPPO;robot manipulation skill acquisition;reward function;proximal policy optimize algorithm;skill policy;robot reinforcement learning framework;robot manipulator;Markov model;objects configuration matching;reward function design method;manipulator operation task","","8","","29","CCBY","9 Jun 2020","","","IEEE","IEEE Journals"
"Temporal Graph Traversals Using Reinforcement Learning With Proximal Policy Optimization","S. H. Silva; A. Alaeddini; P. Najafirad","Secure AI and Autonomy Laboratory, The University of Texas at San Antonio, San Antonio, USA; Department of Information Systems and Cyber Security, The University of Texas at San Antonio, San Antonio, USA; Secure AI and Autonomy Laboratory, The University of Texas at San Antonio, San Antonio, USA","IEEE Access","14 Apr 2020","2020","8","","63910","63922","Graphs in real-world applications are dynamic both in terms of structures and inputs. Information discovery in such networks, which present dense and deeply connected patterns locally and sparsity globally can be time consuming and computationally costly. In this paper we address the shortest path query in spatio-temporal graphs which is a fundamental graph problem with numerous applications. In spatio-temporal graphs, shortest path query classical algorithms are insufficient or even flawed because information consistency can not be guaranteed between two timestamps and path recalculation is computationally costly. In this work, we address the complexity and dynamicity of the shortest path query in spatio-temporal graphs with a simple, yet effective model based on Reinforcement Learning with Proximal Policy Optimization. Our solution simplifies the problem by decomposing the spatio-temporal graph in two components: a static and a dynamic sub-graph. The static graph, known and immutable, is efficiently solved with A* algorithm. The sub-graphs interconnecting the static graph have unknown dynamics and we address such issue by estimating the unknown dynamic portion of the graph as a Markov Chain which correlates the observations of the agents in the environment and the path to be followed. We then derive an action policy through Proximal Policy Optimization to select the local optimal actions in the Markov Process that will lead to the shortest path, given the estimated system dynamics. We evaluate the system in a simulation environment constructed in Unity3D. In partially structured and unknown environments, with variable environment parameters we've obtained an efficiency 75% greater than the comparable deterministic solution.","2169-3536","","10.1109/ACCESS.2020.2985295","National Science Foundation, USA(grant numbers:1445604); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9055369","Machine learning;graphs;Markov-chain;deep reinforcement learning;path-planning","Navigation;Reinforcement learning;Optimization;Path planning;Heuristic algorithms;Robots;Decision making","graph theory;learning (artificial intelligence);Markov processes;query processing","dynamic subgraph;shortest path query classical algorithms;spatio-temporal graph;reinforcement learning;temporal graph traversals;proximal policy optimization;static graph","","8","","66","CCBY","2 Apr 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning for Improving the Accuracy of PM2.5 Pollution Forecast Under the Neural Network Framework","S. -W. Chang; C. -L. Chang; L. -T. Li; S. -W. Liao","Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Computer Science, National Taiwan University, Taipei, Taiwan; Department of Business Administration, National Taiwan University, Taipei, Taiwan; Department of Computer Science, National Taiwan University, Taipei, Taiwan","IEEE Access","17 Jan 2020","2020","8","","9864","9874","Air pollution seriously damages human health on a large scale. Although earlier works have improved a variety of predictive models of air pollution, the ability to accurately predict air pollution indices remains elusive. Time series prediction plays an important role in many fields. Some predecessors have experimented with artificial neural networks (NNs), combining linear autoregressive integrated moving average (ARIMA) models with nonlinear NN models. The typical assumption is that time series has a long signal with no white noise. However, a real-time short signal with white noise is common. The methods in the literature also do not guarantee that the prediction error of an NN model is minimized. Therefore, we propose the use of reinforcement learning (RL) to predict future PM2.5 values. First, we use the Q-learning algorithm in RL based on its state characteristics on the NN model. Second, we select the input with different input dimensions and values of time delay, calculate the best strategy, and evaluate the computational complexity of our RL algorithm. Finally, we show that we effectively reduce the prediction error of the NN models.","2169-3536","","10.1109/ACCESS.2019.2932413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784042","Smart city;smart environments;urban computing;autoregressive integrated moving average;time series;neural network;reinforcement learning;Q-learning","Predictive models;Atmospheric modeling;Artificial neural networks;Autoregressive processes;Reinforcement learning;Data models","air pollution;learning (artificial intelligence);neural nets","Q-learning algorithm;real-time short signal;white noise;long signal;nonlinear NN models;linear autoregressive integrated moving average models;artificial neural networks;time series prediction;air pollution indices;predictive models;human health;neural network framework;PM2.5 pollution forecast;reinforcement learning;prediction error;time delay","","8","","27","CCBY","1 Aug 2019","","","IEEE","IEEE Journals"
"A PSO-Optimized Fuzzy Reinforcement Learning Method for Making the Minimally Invasive Surgical Arm Cleverer","W. Wang; C. Du; W. Wang; Z. Du","State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China","IEEE Access","19 Apr 2019","2019","7","","48655","48670","For robotic-assisted surgery, the preoperative preparation procedure within which the surgical arms need to be adjusted manually to their expected configuration can pose interlocking problems in terms of accuracy, system robustness, and human-robot interaction experience. Previous work about variable impedance/admittance control methods did improve the smoothness of this procedure while individual characteristics of different operating personnel haven't yet been adequately considered. To further improve this process on the basis of existing methods, a novel strategy based on fuzzy Sarsa(λ)-learning algorithm is both proposed and incorporated into the virtual parameter adjustment strategy to achieve physical human-robot interaction (pHRI) so that the robot can acclimatize various handling characteristics of different operators through enough online learning. To also shorten the online training cycle, reduce the undesirable subjective factors and improve the overall training performance, a particle-swarm-optimization-based (PSO-based) algorithm is as well employed for optimizing the partition of state variable space and the distribution of discrete actions. Several groups of experiments have demonstrated the validity of this scheme and the effectiveness of the PSO optimization element.","2169-3536","","10.1109/ACCESS.2019.2910016","National Natural Science Foundation of China(grant numbers:61773141); Natural Science Foundation of Heilongjiang Province(grant numbers:F2018016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8684821","Fuzzy reinforcement learning;PSO-based optimization;physical human-robot interaction;variable admittance control;surgical robot preoperative preparation","Admittance;Impedance;Manipulators;Force;Training;Heuristic algorithms","fuzzy set theory;human-robot interaction;learning (artificial intelligence);medical robotics;particle swarm optimisation;surgery","PSO optimization element;operating personnel;physical human-robot interaction;state variable space;particle-swarm-optimization-based algorithm;online training cycle;handling characteristics;virtual parameter adjustment strategy;fuzzy Sarsa(λ)-learning algorithm;variable impedance/admittance control methods;human-robot interaction experience;system robustness;interlocking problems;expected configuration;surgical arms;preoperative preparation procedure;robotic-assisted surgery;minimally invasive surgical arm cleverer;PSO-optimized fuzzy reinforcement learning method","","8","","39","OAPA","9 Apr 2019","","","IEEE","IEEE Journals"
"Walking Control of a Biped Robot on Static and Rotating Platforms Based on Hybrid Reinforcement Learning","A. Xi; C. Chen","Laboratory of Motion Generation and Analysis, Faculty of Engineering, Monash University, Clayton, Australia; Laboratory of Motion Generation and Analysis, Faculty of Engineering, Monash University, Clayton, Australia","IEEE Access","18 Aug 2020","2020","8","","148411","148424","In this paper, we proposed a novel Hybrid Reinforcement Learning framework to maintain the stability of a biped robot (NAO) while it is walking on static and dynamic platforms. The reinforcement learning framework consists of the Model-based off-line Estimator, the Actor Network Pre-training scheme, and the Mode-free on-line optimizer. We proposed the Hierarchical Gaussian Processes as the Mode-based Estimator to predict a rough model of the system and to obtain the initial control input. Then, the initial control input is employed to pre-train the Actor Network by using the initial control input. Finally, a model-free optimizer based on Deep Deterministic Policy Gradient framework is introduced to fine tune the Actor Network and to generate the best actions. The proposed reinforcement learning framework not only successfully avoids the distribution mismatch problem while combining model-based scheme with model-free structure, but also improves the sample efficiency for the on-line learning procedure. Simulation results show that the proposed Hybrid Reinforcement Learning mechanism enables the NAO robot to maintain balance while walking on static and dynamic platforms. The robustness of the learned controllers in adapting to platforms with different angles, different magnitudes, and different frequencies is tested.","2169-3536","","10.1109/ACCESS.2020.3015506","Australian Research Council(grant numbers:LP160101192); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9163340","Biped robot;reinforcement learning;Gaussian processes;deep deterministic policy gradient","Legged locomotion;Computational modeling;Task analysis;Mathematical model;Learning (artificial intelligence);Adaptation models","control engineering computing;Gaussian processes;gradient methods;humanoid robots;learning (artificial intelligence);legged locomotion;optimisation;robot dynamics;robust control","walking control;biped robot stability;hybrid reinforcement learning framework;static platforms;dynamic platforms;Mode-free on-line optimizer;hierarchical Gaussian processes;rough model;initial control input;model-free optimizer;deep deterministic policy gradient framework;model-based scheme;model-free structure;online learning procedure;NAO robot;learned controllers;model-based off-line estimator;actor network pretraining scheme;mode-based estimator;hybrid reinforcement learning mechanism","","6","","33","CCBY","10 Aug 2020","","","IEEE","IEEE Journals"
"Subgoal-Based Reward Shaping to Improve Efficiency in Reinforcement Learning","T. Okudo; S. Yamada","Department of Informatics, The Graduate University for Advanced Studies, SOKENDAI, Tokyo, Japan; Department of Informatics, The Graduate University for Advanced Studies, SOKENDAI, Tokyo, Japan","IEEE Access","15 Jul 2021","2021","9","","97557","97568","Reinforcement learning, which acquires a policy maximizing long-term rewards, has been actively studied. Unfortunately, this learning type is too slow and difficult to use in practical situations because the state-action space becomes huge in real environments. Many studies have incorporated human knowledge into reinforcement Learning. Though human knowledge on trajectories is often used, a human could be asked to control an AI agent, which can be difficult. Knowledge on subgoals may lessen this requirement because humans need only to consider a few representative states on an optimal trajectory in their minds. The essential factor for learning efficiency is rewards. Potential-based reward shaping is a basic method for enriching rewards. However, it is often difficult to incorporate subgoals for accelerating learning over potential-based reward shaping. This is because the appropriate potentials are not intuitive for humans. We extend potential-based reward shaping and propose a subgoal-based reward shaping. The method makes it easier for human trainers to share their knowledge of subgoals. To evaluate our method, we obtained a subgoal series from participants and conducted experiments in three domains, four-rooms(discrete states and discrete actions), pinball(continuous and discrete), and picking(both continuous). We compared our method with a baseline reinforcement learning algorithm and other subgoal-based methods, including random subgoal and naive subgoal-based reward shaping. As a result, we found out that our reward shaping outperformed all other methods in learning efficiency.","2169-3536","","10.1109/ACCESS.2021.3090364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459751","Reinforcement learning;subgoals as human knowledge;potential-based reward shaping;reward shaping","Trajectory;Reinforcement learning;Task analysis;Robots;Shape;Probability distribution;Optimization","learning (artificial intelligence)","learning efficiency;long-term rewards;state-action space;human knowledge;potential-based reward shaping;human trainers;subgoal series;reinforcement learning;random subgoal;subgoal-based reward shaping;discrete states;discrete actions","","6","","41","CCBY","18 Jun 2021","","","IEEE","IEEE Journals"
"Overcoming Obstacles With a Reconfigurable Robot Using Reinforcement Learning","L. Yehezkel; S. Berman; D. Zarrouk","Department of Mechanical Engineering, Ben Gurion University of the Negev, Beersheba, Israel; Department of Industrial Engineering and Management, Ben Gurion University of the Negev, Beersheba, Israel; Department of Mechanical Engineering, Ben Gurion University of the Negev, Beersheba, Israel","IEEE Access","11 Dec 2020","2020","8","","217541","217553","The reconfigurable robot RSTAR (Rising Sprawl Tuned Autonomous Robot) is a newly developed crawling robot that can reconfigure its shape and shift the location of its center of mass. These features endow the RSTAR with inherent robustness and enhanced ability to overcome obstacles and crawl on a variety of terrains for a vast range of applications. However, defining the trajectories to fully exploit the robot's capabilities is challenging, especially when complex maneuvers are required. Here, we show how reinforcement learning can be used to determine the optimal strategies to overcome three typical obstacles: squeezing through two adjacent obstacles, ducking underneath an obstacle and climbing over an obstacle. We detail the implementation of the Q learning algorithm in a simulation environment with a physical engine (UNITY™) to learn a feasible path in a minimum number of steps. The results show that the algorithm successfully charted a feasible trajectory in all cases. Comparing the trajectory found by the algorithm to trajectories devised by 12 human experts with discrete or continuous control showed that the algorithm trajectories were shorter than the expert trajectories. Experiments showing how the physical RSTAR robot can overcome different obstacles using the trajectories found in the simulation by the Q Learning algorithm are described and presented in the attached video.","2169-3536","","10.1109/ACCESS.2020.3040896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272763","Reinforcement learning;Q learning;crawling robot;reconfigurable robot;sprawl tuning","Robots;Wheels;Robot kinematics;Trajectory;Legged locomotion;Reinforcement learning;Kinematics","collision avoidance;learning (artificial intelligence);legged locomotion;mobile robots;path planning;sampling methods","rising sprawl tuned autonomous robot;Q learning algorithm;adjacent obstacles;inherent robustness;crawling robot;reconfigurable robot;reinforcement learning;physical RSTAR robot;expert trajectories;algorithm trajectories;feasible trajectory","","6","","40","CCBY","27 Nov 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning Based Fault-Tolerant Routing Algorithm for Mesh Based NoC and Its FPGA Implementation","S. Jagadheesh; P. V. Bhanu; J. Soumya; L. R. Cenkeramaddi","Department of Electrical and Electronics Engineering, Birla Institute of Technology and Science-Pilani, Hyderabad Campus, Hyderabad, Telangana, India; Department of Electrical and Electronics Engineering, Birla Institute of Technology and Science-Pilani, Hyderabad Campus, Hyderabad, Telangana, India; Department of Electrical and Electronics Engineering, Birla Institute of Technology and Science-Pilani, Hyderabad Campus, Hyderabad, Telangana, India; Department of Information and Communication Technology, University of Agder (UiA), Grimstad, Kristiansand, Norway","IEEE Access","2 May 2022","2022","10","","44724","44737","Network-on-Chip (NoC) has emerged as the most promising on-chip interconnection framework in Multi-Processor System-on-Chips (MPSoCs) due to its efficiency and scalability. In the deep sub-micron level, NoCs are vulnerable to faults, which leads to the failure of network components such as links and routers. Failures in NoC components diminish system efficiency and reliability. This paper proposes a Reinforcement Learning based Fault-Tolerant Routing (RL-FTR) algorithm to tackle the routing issues caused by link and router faults in the mesh-based NoC architecture. The efficiency of the proposed RL-FTR algorithm is examined using System-C based cycle-accurate NoC simulator. Simulations are carried out by increasing the number of links and router faults in various sizes of mesh. Followed by simulations, real-time functioning of the proposed RL-FTR algorithm is observed using the FPGA implementation. Results of the simulation and hardware shows that the proposed RL-FTR algorithm provides an optimal routing path from the source router to the destination router.","2169-3536","","10.1109/ACCESS.2022.3168992","Indo-Norwegian Collaboration in Autonomous Cyber-Physical Systems (INCAPS) of the INTPART International Partnerships for Excellent Education, Research and Innovation Program from the Research Council of Norway(grant numbers:287918); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9760423","Fault-tolerance;FPGA;network-on-chip;reinforcement learning;routing","Routing;Topology;Fault tolerant systems;Fault tolerance;Heuristic algorithms;Machine learning algorithms;Field programmable gate arrays","electronic engineering computing;fault tolerant computing;field programmable gate arrays;learning (artificial intelligence);multiprocessor interconnection networks;network routing;network-on-chip;system-on-chip","FPGA implementation;on-chip interconnection framework;deep sub-micron level;network components;mesh-based NoC architecture;RL-FTR algorithm;System-C based cycle-accurate NoC simulator;optimal routing path;source router;destination router;reinforcement learning;fault-tolerant routing algorithm;network-on-chip;multiprocessor system-on-chips;NoC components;MPSoC","","6","","30","CCBY","20 Apr 2022","","","IEEE","IEEE Journals"
"A Dynamic Bidding Strategy Based on Model-Free Reinforcement Learning in Display Advertising","M. Liu; L. Jiaxing; Z. Hu; J. Liu; X. Nie","Network and Data Security Key Laboratory of Sichuan Province, University of Electronic Science and Technology of China, Chengdu, China; Network and Data Security Key Laboratory of Sichuan Province, University of Electronic Science and Technology of China, Chengdu, China; Network and Data Security Key Laboratory of Sichuan Province, University of Electronic Science and Technology of China, Chengdu, China; Network and Data Security Key Laboratory of Sichuan Province, University of Electronic Science and Technology of China, Chengdu, China; Network and Data Security Key Laboratory of Sichuan Province, University of Electronic Science and Technology of China, Chengdu, China","IEEE Access","7 Dec 2020","2020","8","","213587","213601","Real-time bidding (RTB) is one of the most striking advances in online advertising, where the websites can sell each ad impression through a public auction, and the advertisers can participate in bidding the impression based on its estimated value. In RTB, the bidding strategy is an essential component for advertisers to maximize their revenues (e.g., clicks and conversions). However, most existing bidding strategies may not work well when the RTB environment changes dramatically between the historical and the new ad delivery periods since they regard the bidding decision as $\boldsymbol {a}$ static optimization problem and derive the bidding function only based on historical data. Thus, the latest research suggests using the reinforcement learning (RL) framework to learn the optimal bidding strategy suitable for the highly dynamic RTB environment. In this paper, we focus on using model-free reinforcement learning to optimize the bidding strategy. Specifically, we divide an ad delivery period into several time slots. The bidding agent decides each impression's bidding price depending on its estimated value and the bidding factor of its arriving time slot. Therefore, the bidding strategy is simplified to solve each time slot's optimal bidding factor, which can adapt dynamically to the RTB environment. We exploit the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm to learn each time slot's optimal bidding factor. Finally, the empirical study on a public dataset demonstrates the superior performance and high efficiency of the proposed bidding strategy compared with other state-of-the-art baselines.","2169-3536","","10.1109/ACCESS.2020.3037940","National Natural Science Foundation of China(grant numbers:61202445,61472064); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9258910","Real-time bidding;bid optimization;model-free reinforcement learning","Heuristic algorithms;Reinforcement learning;Real-time systems;Advertising;Optimization","advertising data processing;gradient methods;learning (artificial intelligence);optimisation;pricing;tendering","bidding decision;online advertising;TD3;twin delayed deep deterministic policy gradient;display advertising;real-time bidding;dynamic bidding strategy;time slot;bidding factor;bidding agent;model-free reinforcement;RTB environment;optimal bidding strategy;reinforcement learning;bidding function","","6","","31","CCBY","13 Nov 2020","","","IEEE","IEEE Journals"
"QSOD: Hybrid Policy Gradient for Deep Multi-agent Reinforcement Learning","H. M. R. U. Rehman; B. -W. On; D. D. Ningombam; S. Yi; G. S. Choi","Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea; Department of Software Convergence Engineering, Kunsan National University, Gunsan, South Korea; Planning Division, Electronics and Telecommunications Research Institute, Daejeon, South Korea; Planning Division, Electronics and Telecommunications Research Institute, Daejeon, South Korea; Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea","IEEE Access","29 Sep 2021","2021","9","","129728","129741","When individuals interact with one another to accomplish specific goals, they learn from others’ experiences to achieve the tasks at hand. The same holds for learning in virtual environments, such as video games. Deep multiagent reinforcement learning shows promising results in terms of completing many challenging tasks. To demonstrate its viability, most algorithms use value decomposition for multiple agents. To guide each agent, behavior value decomposition is utilized to decompose the combined Q-value of the agents into individual agent Q-values. A different mixing method can be utilized, using a monotonicity assumption based on value decomposition algorithms such as QMIX and QVMix. However, this method selects individual agent actions through a greedy policy. The agents, which require large numbers of training trials, are not addressed. In this paper, we propose a novel hybrid policy for the action selection of an individual agent known as Q-value Selection using Optimization and DRL (QSOD). A grey wolf optimizer (GWO) is used to determine the choice of individuals’ actions. As in GWO, there is proper attention among the agents facilitated through the agents’ coordination with one another. We used the StarCraft 2 Learning Environment to compare our proposed algorithm with the state-of-the-art algorithms QMIX and QVMix. Experimental results demonstrate that our algorithm outperforms QMIX and QVMix in all scenarios and requires fewer training trials.","2169-3536","","10.1109/ACCESS.2021.3113350","Electronics and Telecommunications Research Institute(grant numbers:19YE1410); AFOSR(grant numbers:FA2386-19-1-4020); National Research Foundation of Korea (NRF); Korean Government through the Ministry of Science and ICT (MSIT)(grant numbers:NRF-2019R1F1A1060752,NRF-2021R1A6A1A03039493); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9540595","Artificial intelligence;multiagent systems;optimization","Training;Reinforcement learning;Optimization;Mixers;Convergence;Task analysis;Neural networks","","","","5","","37","CCBY","16 Sep 2021","","","IEEE","IEEE Journals"
"Strategic Interaction Multi-Agent Deep Reinforcement Learning","W. Zhou; J. Li; Y. Chen; L. -C. Shen","College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China","IEEE Access","6 Jul 2020","2020","8","","119000","119009","Despite the proliferation of multi-agent deep reinforcement learning (MADRL), most existing typical methods do not scale well to the dynamics of agent populations. And as the population increases, the dimensional explosion of joint state-action and the complex interaction between agents make learning extremely cumbersome, which poses the scalability challenge for MADRL. This paper focuses on the scalability issue of MADRL with homogeneous agents. In a natural population, local interaction is a more feasible mode of interplay rather than global interaction. And inspired by the strategic interaction model in economics, we decompose the value function of each agent into the sum of the expected cumulative rewards of the interaction between the agent and each neighbor. This novel value function is decentralized and decomposable, which enables it to scale well to the dynamic changes in the number of large-scale agents. Hereby, the corresponding strategic interaction reinforcement learning algorithm (SIQ), is proposed to learn the optimal policy of each agent, wherein a neural network is employed to estimate the expected cumulative reward for the interaction between the agent and one of its neighbors. We test the validity of the proposed method in a mixed cooperative-competitive confrontation game through numerical experiments. Furthermore, the scalability comparison experiments illustrate that the scalability of the SIQ algorithm outperforms the independent learning and mean field reinforcement learning algorithms in multiple scenarios with different and dynamically changing numbers.","2169-3536","","10.1109/ACCESS.2020.3005734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9127971","Multi-agent deep reinforcement learning;scalability;local interaction;large scale","Scalability;Heuristic algorithms;Reinforcement learning;Training;Neural networks;Games;Sociology","learning (artificial intelligence);multi-agent systems;neural nets","strategic interaction reinforcement learning algorithm;value function;expected cumulative reward;strategic interaction model;natural population;homogeneous agents;scalability issue;agent populations;MADRL;multiagent deep reinforcement learning","","5","","35","CCBY","29 Jun 2020","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Enabled Self-Configurable Networks-on-Chip for High-Performance and Energy-Efficient Computing Systems","M. F. Reza","Department of Mathematics and Computer Science, Eastern Illinois University, Charleston, IL, USA","IEEE Access","24 Jun 2022","2022","10","","65339","65354","Network-on-Chips (NoC) has been the superior interconnect fabric for multi/many-core on-chip systems because of its scalability and parallelism. On-chip network resources can be dynamically configured to improve the energy efficiency and performance of NoC. However, large and complex design space in heterogeneous NoC architectures becomes difficult to explore within a reasonable time for optimal trade-offs of energy and performance. Furthermore, reactive resource management is not effective in preventing problems, such as thermal hotspots, from happening in adaptive systems. Therefore, we propose machine learning (ML) techniques to provide proactive solutions within an instant in NoC-based computing systems. We present a deep reinforcement learning (deep RL) technique to configure voltage/frequency levels of NoC routers and links for both high performance and energy efficiency while meeting the global energy budget constraint. Distributed RL agents technique has been proposed, where an RL agent configures a NoC router and associated links intelligently based on system utilization and application demands. Additionally, neural networks are used to approximate the actions of distributed RL agents. Simulations results for NoC sizes ranging from 16 to 256 cores under real applications and synthetic traffic show that the proposed self-configurable and scalable approach, on average, improves energy-delay product (EDP) by 30-40% (up to 80%) and by 8% (up to 17%) compared to existing non-ML and ML based solutions, respectively.","2169-3536","","10.1109/ACCESS.2022.3182500","Department of Mathematics and Computer Science at the Eastern Illinois University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9794707","Network-on-chip (NoC);multicore architecture;mancore processor;machine learning (ML);reinforcement learning (RL);distributed RL;deep reinforcement learning (Deep RL);Q-learning;neural networks (NNs);self-configurable;energy-efficiency;high-performance","System-on-chip;Reinforcement learning;Task analysis;Artificial neural networks;Optimization;Computer architecture;Voltage","deep learning (artificial intelligence);electronic engineering computing;network routing;network-on-chip;reinforcement learning","energy-efficient computing systems;interconnect fabric;energy efficiency;complex design space;heterogeneous NoC architectures;resource management;machine learning techniques;NoC-based computing systems;deep reinforcement learning technique;deep RL;global energy budget constraint;distributed RL agents technique;RL agent;neural networks;energy-delay product;self-configurable networks-on-chip;on-chip network resource systems;NoC router;ML techniques","","5","","53","CCBY","13 Jun 2022","","","IEEE","IEEE Journals"
"Exploring and Exploiting Conditioning of Reinforcement Learning Agents","A. Asadulaev; I. Kuznetsov; G. Stein; A. Filchenkov","Machine Learning Laboratory, ITMO University, Saint-Petersburg, Russia; Machine Learning Laboratory, ITMO University, Saint-Petersburg, Russia; Machine Learning Laboratory, ITMO University, Saint-Petersburg, Russia; Machine Learning Laboratory, ITMO University, Saint-Petersburg, Russia","IEEE Access","3 Dec 2020","2020","8","","211951","211960","The outcome of Jacobian singular values regularization was studied for supervised learning problems. In supervised learning settings for linear and nonlinear networks, Jacobian regularization allows for faster learning. It also was shown that Jacobian conditioning regularization can help to avoid the “mode-collapse” problem in Generative Adversarial Networks. In this paper, we try to answer the following question: Can information about policy network Jacobian conditioning help to shape a more stable and general policy of reinforcement learning agents? To answer this question, we conduct a study of Jacobian conditioning behavior during policy optimization. We analyze the behavior of the agent conditioning on different policies under the different sets of hyperparameters and study a correspondence between the conditioning and the ratio of achieved rewards. Based on these observations, we propose a conditioning regularization technique. We apply it to Trust Region Policy Optimization and Proximal Policy Optimization (PPO) algorithms and compare their performance on 8 continuous control tasks. Models with the proposed regularization outperformed other models on most of the tasks. Also, we showed that the regularization improves the agent’s generalization by comparing the PPO performance on CoinRun environments. Also, we propose an algorithm that uses the condition number of the agent to form a robust policy, which we call Jacobian Policy Optimization (JPO). It directly estimates the condition number of an agent’s Jacobian and changes the policy trend. We compare it with PPO on several continuous control tasks in PyBullet environments and the proposed algorithm provides a more stable and efficient reward growth on a range of agents.","2169-3536","","10.1109/ACCESS.2020.3037276","Russian Ministry of Science and Higher Education(grant numbers:2.8866.2017/8.9); Research and Development(grant numbers:619416); Russian Science Foundation through Deep reinforced algorithms for solving the routing problem with dynamically changing topology and graph properties(grant numbers:20-19-00700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9256259","Reinforcement learning;neural networks;policy optimization;generalization;regularization;conditioning","Jacobian matrices;Shape;Supervised learning;Reinforcement learning;Computer architecture;Approximation algorithms;Task analysis","Jacobian matrices;learning (artificial intelligence)","reinforcement learning agents;Jacobian singular values regularization;supervised learning problems;supervised learning settings;nonlinear networks;faster learning;Jacobian conditioning regularization;mode-collapse problem;generative adversarial networks;stable policy;general policy;Jacobian conditioning behavior;agent conditioning;conditioning regularization technique;trust region policy optimization;proximal policy optimization algorithms;continuous control tasks;condition number;robust policy;policy trend;policy network Jacobian conditioning;Jacobian policy optimization","","4","","31","CCBY","11 Nov 2020","","","IEEE","IEEE Journals"
"Implementation of Decentralized Reinforcement Learning-Based Multi-Quadrotor Flocking","P. Abichandani; C. Speck; D. Bucci; W. Mcintyre; D. Lobo","Department of Electrical and Computer Engineering, Newark College of Engineering (NCE), Robotics and Data Laboratory (RADLab), New Jersey Institute of Technology, Newark, NJ, USA; Lockheed Martin Advanced Technology Laboratories, Cherry Hill, NJ, USA; Lockheed Martin Advanced Technology Laboratories, Cherry Hill, NJ, USA; Department of Electrical and Computer Engineering, Newark College of Engineering (NCE), Robotics and Data Laboratory (RADLab), New Jersey Institute of Technology, Newark, NJ, USA; Department of Electrical and Computer Engineering, Newark College of Engineering (NCE), Robotics and Data Laboratory (RADLab), New Jersey Institute of Technology, Newark, NJ, USA","IEEE Access","1 Oct 2021","2021","9","","132491","132507","Enabling coordinated motion of multiple quadrotors is an active area of research in the field of small unmanned aerial vehicles (sUAVs). While there are many techniques found in the literature that address the problem, these studies are limited to simulation results and seldom account for wind disturbances. This paper presents the experimental validation of a decentralized planner based on multi-objective reinforcement learning (RL) that achieves waypoint-based flocking (separation, velocity alignment, and cohesion) for multiple quadrotors in the presence of wind gusts. The planner is learned using an object-focused, greatest mass, state-action-reward-state-action (OF-GM-SARSA) approach. The Dryden wind gust model is used to simulate wind gusts during hardware-in-the-loop (HWIL) tests. The hardware and software architecture developed for the multi-quadrotor flocking controller is described in detail. HWIL and outdoor flight tests results show that the trained RL planner can generalize the flocking behaviors learned in training to the real-world flight dynamics of the DJI M100 quadrotor in windy conditions.","2169-3536","","10.1109/ACCESS.2021.3115711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9548090","Cooperative systems;design for experiments;unmanned aerial vehicles;multi-agent systems;motion planning;supervised learning","Wind;Heuristic algorithms;Atmospheric modeling;Software algorithms;Mathematical models;Birds","aircraft control;autonomous aerial vehicles;collision avoidance;control system synthesis;learning (artificial intelligence);mobile robots;multi-robot systems;software architecture","small unmanned aerial vehicles;wind disturbances;decentralized planner;multiobjective reinforcement learning;waypoint-based flocking;Dryden wind gust model;hardware-in-the-loop tests;multiquadrotor flocking controller;outdoor flight tests results;trained RL planner;DJI M100 quadrotor;decentralized reinforcement learning-based multiquadrotor flocking;coordinated motion;state-action-reward-state-action approach","","4","","88","CCBY","24 Sep 2021","","","IEEE","IEEE Journals"
"M-A3C: A Mean-Asynchronous Advantage Actor-Critic Reinforcement Learning Method for Real-Time Gait Planning of Biped Robot","J. Leng; S. Fan; J. Tang; H. Mou; J. Xue; Q. Li","Institute of Machine Intelligence, University of Shanghai for Science and Technology, Shanghai, China; Origin Dynamic Intelligent Robot Company Ltd., Zhengzhou, China; Institute of Machine Intelligence, University of Shanghai for Science and Technology, Shanghai, China; Institute of Machine Intelligence, University of Shanghai for Science and Technology, Shanghai, China; School of Cyber Science and Engineering, Zhengzhou University, Zhengzhou, China; Institute of Machine Intelligence, University of Shanghai for Science and Technology, Shanghai, China","IEEE Access","26 Jul 2022","2022","10","","76523","76536","Bipedal walking is a challenging task for humanoid robots. In this study, we develop a lightweight reinforcement learning method for real-time gait planning of the biped robot. We regard bipedal walking as a process in which the robot constantly interacts with the environment, judges the quality of control action through the walking state, and then adjusts the control strategy. A mean-asynchronous advantage actor-critic (M-A3C) reinforcement learning algorithm is proposed to obtain the continuous state space and action space, and directly obtain the final gait of the robot without introducing the reference gait. We use multiple sub-agents of M-A3C algorithm to train multiple virtual robots independently at the same time in the physical simulation platform. Then we transfer the trained model to the walking control of the actual robot to reduce the number of training on the actual robot, improve the training speed, and ensure the acquisition of the final gait. Finally, a biped robot is designed and fabricated to verify the effectiveness of the proposed method. Various experiments show that the proposed method can achieve the biped robot’s continuous and stable gait planning.","2169-3536","","10.1109/ACCESS.2022.3176608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779214","Biped robot;actor-critic learning;machine intelligence","Legged locomotion;Robots;Mathematical models;Training;Reinforcement learning;Process control;Planning","gait analysis;humanoid robots;learning (artificial intelligence);legged locomotion;mobile robots;robot dynamics","mean-asynchronous advantage actor-critic reinforcement learning method;real-time gait planning;biped robot;bipedal walking;humanoid robots;lightweight reinforcement learning method;control action;walking state;control strategy;continuous state space;action space;final gait;reference gait;multiple virtual robots;walking control;actual robot","","4","","41","CCBYNCND","20 May 2022","","","IEEE","IEEE Journals"
"The Recurrent Reinforcement Learning Crypto Agent","G. Borrageiro; N. Firoozye; P. Barucca","Department of Computer Science, University College London, London, U.K; Department of Computer Science, University College London, London, U.K; Department of Computer Science, University College London, London, U.K","IEEE Access","18 Apr 2022","2022","10","","38590","38599","We demonstrate a novel application of online transfer learning for a digital assets trading agent. This agent uses a powerful feature space representation in the form of an echo state network, the output of which is made available to a direct, recurrent reinforcement learning agent. The agent learns to trade the XBTUSD (Bitcoin versus US Dollars) perpetual swap derivatives contract on BitMEX on an intraday basis. By learning from the multiple sources of impact on the quadratic risk-adjusted utility that it seeks to maximise, the agent avoids excessive over-trading, captures a funding profit, and can predict the market’s direction. Overall, our crypto agent realises a total return of 350%, net of transaction costs, over roughly five years, 71% of which is down to funding profit. The annualised information ratio that it achieves is 1.46.","2169-3536","","10.1109/ACCESS.2022.3166599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9755122","Online learning;transfer learning;echo state networks;recurrent reinforcement learning;financial time series","Reinforcement learning;Transfer learning;Costs;Reservoirs;Time series analysis;Cryptography;Training","cryptocurrencies;financial data processing;recurrent neural nets;reinforcement learning","recurrent reinforcement learning crypto agent;online transfer learning;digital assets trading;feature space representation;echo state network;direct reinforcement learning agent;quadratic risk-adjusted utility;over-trading;funding profit;BitMEX;XBTUSD;bitcoin;US dollars","","4","","44","CCBY","11 Apr 2022","","","IEEE","IEEE Journals"
"Obstacle Avoidance for UAS in Continuous Action Space Using Deep Reinforcement Learning","J. Hu; X. Yang; W. Wang; P. Wei; L. Ying; Y. Liu","School for Engineering of Matter, Transport and Energy, Arizona State University, Tempe, AZ, USA; Department of Aerospace Engineering, Iowa State University, Ames, IA, USA; Department of Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; Department of Mechanical and Aerospace Engineering, The George Washington University, Washington, DC, USA; Engineering and Computer Science Department, University of Michigan, Ann Arbor, Ann Arbor, MI, USA; School for Engineering of Matter, Transport and Energy, Arizona State University, Tempe, AZ, USA","IEEE Access","5 Sep 2022","2022","10","","90623","90634","Obstacle avoidance for small unmanned aircraft is vital for the safety of future urban air mobility (UAM) and Unmanned Aircraft System (UAS) Traffic Management (UTM). There are a variety of techniques for real-time robust drone guidance, but numerous of them solve in discretized airspace and control, which would require an additional path smoothing step to provide flexible commands for UAS. To deliver safe and computationally efficient guidance for UAS operations, we explore the use of a deep reinforcement learning algorithm based on Proximal Policy Optimization (PPO) to lead autonomous UAS to their destinations while bypassing obstacles through continuous control. The proposed scenario state representation and reward function can map the continuous state space to continuous control for both heading angle and speed. To verify the effectiveness of the proposed learning framework, we conducted numerical experiments with static and moving obstacles. Uncertainties associated with the environments and safety operation bounds are investigated in detail. Results show that the proposed model is able to provide accurate and robust guidance and resolve conflict with a success rate of over 99%.","2169-3536","","10.1109/ACCESS.2022.3201962","National Aeronautics and Space Administration (NASA) University Leadership Initiative Program(grant numbers:NNX17AJ86A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9868001","Continuous control;deep reinforcement learning;UAS obstacle avoidance;uncertainty","Deep learning;Reinforcement learning;Collision avoidance;Markov processes;Air traffic control;Uncertainty;Unmanned aerial vehicles","collision avoidance;learning (artificial intelligence);mobile robots;path planning","obstacle avoidance;continuous action space;future urban air mobility;Unmanned Aircraft System;real-time robust drone guidance;discretized airspace;additional path smoothing step;flexible commands;UAS operations;deep reinforcement learning algorithm;Proximal Policy Optimization;autonomous UAS;continuous control;scenario state representation;reward function;continuous state space;learning framework;numerical experiments;accurate guidance;robust guidance","","4","","58","CCBY","26 Aug 2022","","","IEEE","IEEE Journals"
"Spiking Neural Network Discovers Energy-Efficient Hexapod Motion in Deep Reinforcement Learning","K. Naya; K. Kutsuzawa; D. Owaki; M. Hayashibe","Department of Robotics, Graduate School of Engineering, Neuro-Robotics Laboratory, Tohoku University, Sendai, Japan; Department of Robotics, Graduate School of Engineering, Neuro-Robotics Laboratory, Tohoku University, Sendai, Japan; Department of Robotics, Graduate School of Engineering, Neuro-Robotics Laboratory, Tohoku University, Sendai, Japan; Department of Robotics, Graduate School of Engineering, Neuro-Robotics Laboratory, Tohoku University, Sendai, Japan","IEEE Access","12 Nov 2021","2021","9","","150345","150354","In Deep Reinforcement Learning (DRL) for robotics application, it is important to find energy-efficient motions. For this purpose, a standard method is to set an action penalty in the reward to find the optimal motion considering the energy expenditure. This method is widely used for the simplicity of implementation. However, since the reward is a linear sum, if the penalty is too large, the system will fall into local minima and no moving solution can be obtained. In contrast, if the penalty is too small, the effect may not be sufficient. Therefore, it is necessary to adjust the amount of the penalty so that the agent always moves dynamically, and the energy-saving effect is sufficient. Nevertheless, since adjusting the hyperparameters is computationally expensive, we need a learning method that is robust to the penalty setting problem. We investigated on the Spiking Neural Network (SNN), which has been attracting attention for its computational efficiency and neuromorphic architecture. We conducted gait experiments using a hexapod agent while varying the energy penalty settings in the simulation environment. By applying SNN to the conventional state-of-the-art DRL algorithms, we examined whether the agent could explore for an optimal gait with a larger penalty variation and obtain an energy-efficient gait verified with Cost of Transport (CoT), a metric of energy efficiency for gait. Soft Actor-Critic (SAC)+SNN resulted in a CoT of 1.64, Twin Delayed Deep Deterministic policy gradient (TD3)+SNN resulted in a CoT of 2.21, and Deep Deterministic policy gradient (DDPG)+SNN resulted in a CoT of 2.08 (1.91 for normal SAC, 2.38 for TD3, and 2.40 for DDPG). DRL combined with SNN succeeded in learning more energy efficient gait with lower CoT.","2169-3536","","10.1109/ACCESS.2021.3126311","JSPS Grant-in-Aid for Scientific Research on Innovative Areas Hyper-Adaptability Project(grant numbers:20H05458); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606760","Spiking neural network;deep reinforcement learning;energy efficiency;hexapod gait;spatio-temporal backpropagation","Neurons;Legged locomotion;Task analysis;Robots;Computational modeling;Torque;Mathematical models","deep learning (artificial intelligence);legged locomotion;optimisation;reinforcement learning","penalty variation;CoT;energy efficiency;Soft Actor-Critic+SNN;energy efficient gait;robotics application;energy-efficient motions;action penalty;optimal motion;energy expenditure;learning method;penalty setting problem;Spiking Neural Network;computational efficiency;hexapod agent;energy penalty settings;optimal gait;energy-efficient hexapod motion;twin delayed deep deterministic policy gradient+SNN;deep reinforcement learning","","4","","30","CCBY","8 Nov 2021","","","IEEE","IEEE Journals"
"Energy-Efficient Gait Optimization of Snake-Like Modular Robots by Using Multiobjective Reinforcement Learning and a Fuzzy Inference System","A. Singh; W. -Y. Chiu; S. H. Manoharan; A. M. Romanov","Department of Electrical Engineering, Multi-Objective Control and Reinforcement Learning (MOCaRL) Laboratory, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, Multi-Objective Control and Reinforcement Learning (MOCaRL) Laboratory, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, Multi-Objective Control and Reinforcement Learning (MOCaRL) Laboratory, National Tsing Hua University, Hsinchu, Taiwan; Institute of Artificial Intelligence, MIREA—Russian Technological University, Moscow, Russia","IEEE Access","23 Aug 2022","2022","10","","86624","86635","Snake-like modular robots (MRs) are highly flexible, but, to traverse a challenging terrain or explore a region of interest, MR needs to attain efficient locomotion depending on a tradeoff between objectives like forward velocity and power consumption of the robot. The objectives can vary with different weights depending upon the situation, reflecting relative objective importance. This study developed a multiobjective reinforcement learning algorithm based on a fuzzy inference system (FI-MORL) to select the most appropriate gait parameters of snake-like MRs according to the objective weights. The developed algorithm employs a fuzzy inference system to reduce the number of states in an environment, which results in faster learning. The proposed approach uses the previously learned experience to rapidly achieve the best objective values in response to a change in weights. While setting equal importance to the objectives, FI-MORL delivers superior performance than single-objective reinforcement learning algorithms by consuming 2% less power and gaining 2.5% higher velocity since it mitigates the effect of weight change, similar performance found comparing an actor-critic algorithm. Likewise, the proposed method outperforms by consuming 14% less power and achieving 11% higher velocity than traditional methods like proximal policy optimization, deep Q-network, and vanilla policy gradient. Even after weight change, FI-MORL achieved a 14% higher reward than the above methods. The proposed FI-MORL framework can effectively converge quicker and efficiently handle the changes in objective weights.","2169-3536","","10.1109/ACCESS.2022.3195928","Ministry of Science and Technology of Taiwan(grant numbers:MOST 110-2221-E-007-097-MY2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847243","Energy efficiency;fuzzy inference system;gait optimization;modular robot;multiobjective reinforcement learning;snake-like modular robot","Robots;Mathematical models;Reinforcement learning;Optimization;Inference algorithms;Energy efficiency;Fuzzy logic","fuzzy systems;gradient methods;mobile robots;optimisation;reinforcement learning","energy-efficient gait optimization;snake-like modular robots;fuzzy inference system;MRs;efficient locomotion;forward velocity;power consumption;multiobjective reinforcement learning algorithm;gait parameters;objective weights;objective values;single-objective reinforcement learning algorithms;actor-critic algorithm;FI-MORL framework;weight change effect;deep Q-network;proximal policy optimization;vanilla policy gradient","","4","","42","CCBY","1 Aug 2022","","","IEEE","IEEE Journals"
"Hybrid Heuristic Algorithm Based On Improved Rules & Reinforcement Learning for 2D Strip Packing Problem","K. Zhu; N. Ji; X. D. Li","School of Information and Control Engineering, Qingdao University of Technology, Qingdao, China; School of Information and Control Engineering, Qingdao University of Technology, Qingdao, China; School of Information and Control Engineering, Qingdao University of Technology, Qingdao, China","IEEE Access","29 Dec 2020","2020","8","","226784","226796","A hybrid heuristic algorithm based on improved rules and reinforcement learning is proposed to solve the 2D strip packing problem (2DSPP). Firstly, the scoring rules based on the skyline algorithm are improved by considering the “two-step” successive items with a set of width relaxation factors. The improved scoring rules can reduce space waste efficiently. Secondly, as a reinforcement learning approach, the Deep Q-Network (DQN) is established to get the initial rectangular items sequence and at the same time as an essential supplement for the placement rules. It can improve space utilization and prevent the algorithm from falling into the local optimum. Combining the new “two-step” placement rules and DQN, the heuristic algorithm based on simple random algorithm (SRA) is proposed and finally called reinforcement learning based simple random algorithm (RSRA). Experiments on eight datasets by five algorithms have been conducted for comparison. Results show the RSRA has achieved the best performance on eight datasets (C, N, CX, NT, 2sp, NP, ZDF, BWMV) and has dropped Ave. Gap% by 45.86%, 45.16%, 30.89% and 20.56% than GRASP, SRA, IA, ISH respectively. It can be concluded that the RSRA algorithm would achieve better performance than the other four algorithms on eight datasets, especially on the larger datasets.","2169-3536","","10.1109/ACCESS.2020.3045905","Shandong Provincial Natural Science Foundation of China(grant numbers:ZR2019PEE013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9298860","2D strip packing problem (2DSPP);heuristic algorithm;improved rules;deep Q-Network (DQN);reinforcement learning","Heuristic algorithms;Reinforcement learning;Strips;Optimization;Approximation algorithms;Space debris;Simulated annealing","bin packing;learning (artificial intelligence);network theory (graphs);neural nets;optimisation;randomised algorithms","hybrid heuristic algorithm;2D strip packing problem;2DSPP;skyline algorithm;width relaxation factors;space waste;DQN;initial rectangular items sequence;RSRA algorithm;reinforcement learning based simple random algorithm;deep Q-network","","3","","42","CCBY","18 Dec 2020","","","IEEE","IEEE Journals"
"Deep Neural Network-Based Surrogate Model for Optimal Component Sizing of Power Converters Using Deep Reinforcement Learning","V. -H. Bui; F. Chang; W. Su; M. Wang; Y. L. Murphey; F. L. Da Silva; C. Huang; L. Xue; R. Glatt","Department of Electrical and Computer Engineering, College of Engineering and Computer Science, University of Michigan-Dearborn, Dearborn, MI, USA; Department of Electrical and Computer Engineering, College of Engineering and Computer Science, University of Michigan-Dearborn, Dearborn, MI, USA; Department of Electrical and Computer Engineering, College of Engineering and Computer Science, University of Michigan-Dearborn, Dearborn, MI, USA; Department of Electrical and Computer Engineering, College of Engineering and Computer Science, University of Michigan-Dearborn, Dearborn, MI, USA; Department of Electrical and Computer Engineering, College of Engineering and Computer Science, University of Michigan-Dearborn, Dearborn, MI, USA; Lawrence Livermore National Laboratory (LLNL), Livermore, CA, USA; Lawrence Livermore National Laboratory (LLNL), Livermore, CA, USA; Oak Ridge National Laboratory (ORNL), Oak Ridge, TN, USA; Lawrence Livermore National Laboratory (LLNL), Livermore, CA, USA","IEEE Access","1 Aug 2022","2022","10","","78702","78712","The optimal design of power converters often requires a huge number of simulations and numeric analyses to determine the optimal parameters. This process is time-consuming and results in a high computational cost. Therefore, this paper proposes a deep reinforcement learning (DRL)-based optimization algorithm to optimize the design parameters for power converters using a deep neural network (DNN)-based surrogate model. The surrogate model of power converters can quickly estimate the power efficiency from input parameters without requiring any simulation. The proposed optimization model includes two major steps. In the first step, the surrogate model is trained offline using a large dataset. In the second step, a soft actor-critic-based optimization model interacts with the surrogate model from step 1 to determine the optimal values of design parameters in power converters. Unlike deep Q learning-based methods, the proposed method is able to handle large state and action spaces. In addition, using entropy-regularized reinforcement learning, our proposed method can accelerate and stabilize the learning process and also prevent trapping in local optima. Finally, to show the effectiveness of the proposed method, the performance of different optimization algorithms is compared, considering over ten power converter topologies.","2169-3536","","10.1109/ACCESS.2022.3194267","Advanced Research Projects Agency-Energy (ARPA-E), U.S. Department of Energy(grant numbers:DE-AR0001219); U.S. Department of Energy, Lawrence Livermore National Laboratory(grant numbers:DE-AC52-07NA27344); Lawrence Livermore National Security through LLC(grant numbers:LLNL-JRNL_831362); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9841555","Component sizing;deep reinforcement learning;deep neural networks;optimal design parameters;optimization;power converters;surrogate model","Optimization;Mathematical models;Training;Topology;Computational modeling;Switches;Semiconductor device modeling","deep learning (artificial intelligence);optimisation;power convertors;reinforcement learning","deep neural network-based surrogate model;optimal component sizing;power converters;deep reinforcement learning-based optimization algorithm;learning-based methods;entropy-regularized reinforcement learning;power converter topologies;soft actor-critic-based optimization model","","3","","27","CCBYNCND","27 Jul 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Driving Strategy for Avoidance of Chain Collisions and Its Safety Efficiency Analysis in Autonomous Vehicles","A. J. M. Muzahid; S. F. Kamarulzaman; M. A. Rahman; A. H. Alenezi","Faculty of Computing, Universiti Malaysia Pahang, Pahang, Malaysia; Faculty of Computing, Universiti Malaysia Pahang, Pahang, Malaysia; School of Engineering, Computing & Mathematical Sciences, University of Wolverhampton, Wolverhampton, U.K.; Remote Sensing Unit, Northern Border University, Arar, Saudi Arabia","IEEE Access","29 Apr 2022","2022","10","","43303","43319","Vehicle control in autonomous traffic flow is often handled using the best decision-making reinforcement learning methods. However, unexpected critical situations make the collisions more severe and, consequently, the chain collisions. In this work, we first review the leading causes of chain collisions and their subsequent chain events, which might provide an indication of how to prevent and mitigate the crash severity of chain collisions. Then, we consider the problem of chain collision avoidance as a Markov Decision Process problem in order to propose a reinforcement learning-based decision-making strategy and analyse the safety efficiency of existing methods in driving security. To address this, A reward function is being developed to deal with the challenge of multiple vehicle collision avoidance. A perception network structure based on formation and on actor-critic methodologies is employed to enhance the decision-making process. Finally, in the safety efficiency analysis phase, we investigated the safety efficiency performance of the agent vehicle in both single-agent and multi-agent autonomous driving environments. Three state-of-the-art contemporary actor-critic algorithms are used to create an extensive simulation in Unity3D. Moreover, to demonstrate the accuracy of the safety efficiency analysis, multiple training runs of the neural networks in respect of training performance, speed of training, success rate, and stability of rewards with a trade-off between exploitation and exploration during training are presented. Two aspects (single-agent and multi-agent) have assessed the efficiency of algorithms. Every aspect has been analyzed regarding the traffic flows: (1) the controlling efficiency of unexpected traffic situations by the sudden slowdown, (2) abrupt lane change, and (3) smoothly reaching the destination. All the findings of the analysis are intended to shed insight on the benefits of a greater, more reliable autonomous traffic set-up for academics and policymakers, and also to pave the way for the actual carry-out of a driver-less traffic world.","2169-3536","","10.1109/ACCESS.2022.3167812","Ministry of Higher Education of Malaysia through the Fundamental Research Grant Scheme(grant numbers:FRGS/1/2018/TK08/UMP/02/2); Deputyship for Research & Innovation, Ministry of Education in Saudi Arabia(grant numbers:IF-2020-NBU-418); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9758806","Autonomous vehicles;deep reinforcement learning method;reward function;chain collision avoidance;autonomous traffic flow;safety efficiency analysis","Safety;Collision avoidance;Long short term memory;Training;Feature extraction;Decision making;Uncertainty","collision avoidance;decision making;learning (artificial intelligence);Markov processes;multi-agent systems;road traffic;road vehicles;traffic engineering computing","deep reinforcement learning-based;chain collisions;autonomous vehicles;autonomous traffic flow;decision-making reinforcement learning methods;subsequent chain events;chain collision avoidance;Markov Decision Process problem;reinforcement learning-based decision-making strategy;multiple vehicle collision avoidance;decision-making process;safety efficiency analysis phase;safety efficiency performance;agent vehicle;single-agent;multiagent autonomous driving environments;state-of-the-art contemporary actor-critic algorithms;controlling efficiency","","3","","41","CCBY","18 Apr 2022","","","IEEE","IEEE Journals"
"Saturated Output-Feedback Hybrid Reinforcement Learning Controller for Submersible Vehicles Guaranteeing Output Constraints","O. Elhaki; K. Shojaei; D. Shanahan; A. Montazeri","Department of Electrical Engineering, Najafabad Branch, Islamic Azad University, Najafabad, Iran; Department of Electrical Engineering, Najafabad Branch, Islamic Azad University, Najafabad, Iran; Department of Engineering, Lancaster University, Lancaster, England, U.K.; Department of Engineering, Lancaster University, Lancaster, England, U.K.","IEEE Access","11 Oct 2021","2021","9","","136580","136592","In this brief, we propose a new neuro-fuzzy reinforcement learning-based control (NFRLC) structure that allows autonomous underwater vehicles (AUVs) to follow a desired trajectory in large-scale complex environments precisely. The accurate tracking control problem is solved by a unique online NFRLC method designed based on actor-critic (AC) structure. Integrating the NFRLC framework including an adaptive multilayer neural network (MNN) and interval type-2 fuzzy neural network (IT2FNN) with a high-gain observer (HGO), a robust smart observer-based system is set up to estimate the velocities of the AUVs, unknown dynamic parameters containing unmodeled dynamics, nonlinearities, uncertainties and external disturbances. By employing a saturation function in the design procedure and transforming the input limitations into input saturation nonlinearities, the risk of the actuator saturation is effectively reduced together with nonlinear input saturation compensation by the NFRLC strategy. A predefined funnel-shaped performance function is designed to attain certain prescribed output performance. Finally, stability study reveals that the entire closed-loop system signals are semi-globally uniformly ultimately bounded (SGUUB) and can provide prescribed convergence rate for the tracking errors so that the tracking errors approach to the origin evolving inside the funnel-shaped performance bound at the prescribed time.","2169-3536","","10.1109/ACCESS.2021.3113080","Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/V027379/1,EP/R02572X/1); National Centre for Nuclear Robotics (NCNR); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9539163","Saturation function;reinforcement learning;prescribed performance;high-gain observer;interval type-2 fuzzy neural networks;multilayer neural networks","Control systems;Observers;Adaptive systems;Uncertainty;Fuzzy logic;Vehicle dynamics;Reinforcement learning","adaptive control;autonomous underwater vehicles;closed loop systems;compensation;control nonlinearities;feedback;fuzzy control;fuzzy neural nets;learning (artificial intelligence);neurocontrollers;nonlinear control systems;observers;robust control;uncertain systems","submersible vehicles;output constraints;autonomous underwater vehicles;AUV trajectory;tracking control;high gain observer;unknown dynamic parameters;input saturation nonlinearities;actuator saturation;nonlinear input saturation compensation;predefined funnel-shaped performance function;neurofuzzy reinforcement learning based control;saturated output feedback hybrid reinforcement learning controller;robust smart observer;adaptive multilayer neural network;interval type-2 fuzzy neural network;closed loop system signals;semi-globally uniformly ultimately bounded","","3","","56","CCBY","15 Sep 2021","","","IEEE","IEEE Journals"
"Reinforcement Learning for Systematic FX Trading","G. Borrageiro; N. Firoozye; P. Barucca","Department of Computer Science, University College London, London, U.K.; Department of Computer Science, University College London, London, U.K.; Department of Computer Science, University College London, London, U.K.","IEEE Access","19 Jan 2022","2022","10","","5024","5036","We explore online inductive transfer learning, with a feature representation transfer from a radial basis function network formed of Gaussian mixture model hidden processing units to a direct, recurrent reinforcement learning agent. This agent is put to work in an experiment, trading the major spot market currency pairs, where we accurately account for transaction and funding costs. These sources of profit and loss, including the price trends that occur in the currency markets, are made available to the agent via a quadratic utility, who learns to target a position directly. We improve upon earlier work by targeting a risk position in an online transfer learning context. Our agent achieves an annualised portfolio information ratio of 0.52 with a compound return of 9.3%, net of execution and funding cost, over a 7-year test set; this is despite forcing the model to trade at the close of the trading day at 5 pm EST when trading costs are statistically the most expensive.","2169-3536","","10.1109/ACCESS.2021.3139510","UCL Engineering University College London; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665743","Policy gradients;recurrent reinforcement learning;online learning;transfer learning;financial time series","Transfer learning;Reinforcement learning;Costs;Time series analysis;Currencies;Task analysis;Portfolios","foreign exchange trading;Gaussian processes;investment;mixture models;pricing;radial basis function networks;reinforcement learning","trading day;trading costs;systematic FX trading;online inductive transfer learning;feature representation transfer;radial basis function network;Gaussian mixture model hidden processing units;recurrent reinforcement learning agent;funding costs;profit;price trends;currency markets;risk position;annualised portfolio information ratio;major spot market currency pairs","","3","","42","CCBY","30 Dec 2021","","","IEEE","IEEE Journals"
"Off-Policy Meta-Reinforcement Learning With Belief-Based Task Inference","T. Imagawa; T. Hiraoka; Y. Tsuruoka","National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; National Institute of Advanced Industrial Science and Technology, Tokyo, Japan","IEEE Access","12 May 2022","2022","10","","49494","49507","Meta-reinforcement learning (RL) addresses the problem of sample inefficiency in deep RL by using experience obtained in past tasks for solving a new task. However, most existing meta-RL methods require partially or fully on-policy data, which hinders the improvement of sample efficiency. To alleviate this problem, we propose a novel off-policy meta-RL method, embedding learning and uncertainty evaluation (ELUE). An ELUE agent is characterized by the learning of what we call a task embedding space, an embedding space for representing the features of tasks. The agent learns a belief model over the task embedding space and trains a belief-conditional policy and Q-function. The belief model is designed to be agnostic to the order in which task information is obtained, thereby reducing the difficulty of task embedding learning. For a new task, the ELUE agent collects data by the pretrained policy, and updates its belief on the basis of the belief model. Thanks to the belief update, the performance of the agent improves with a small amount of data. In addition, the agent updates the parameters of its policy and Q-function so that it can adjust the pretrained relationships when there are enough data. We demonstrate that ELUE outperforms state-of-the-art meta RL methods through experiments on meta-RL benchmarks.","2169-3536","","10.1109/ACCESS.2022.3170582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763505","Artificial intelligence;inference;meta learning;reinforcement learning;uncertainty","Training data;Uncertainty;Decoding;Reinforcement learning;Markov processes;Artificial intelligence;Inference algorithms","deep learning (artificial intelligence);multi-agent systems;reinforcement learning","policy meta-reinforcement learning;belief-based task inference;sample inefficiency;deep RL;on-policy data;off-policy meta-RL method;ELUE agent;task embedding space;belief-conditional policy;task information;meta-RL benchmarks;Q-function;embedding learning and uncertainty evaluation","","3","","47","CCBY","26 Apr 2022","","","IEEE","IEEE Journals"
"An Improved Anti-Jamming Method Based on Deep Reinforcement Learning and Feature Engineering","X. Chang; Y. Li; Y. Zhao; Y. Du; D. Liu","The 54th Research Institute of China Electronics Technology Group Corporation (CETC54), Shijiazhuang, China; The 54th Research Institute of China Electronics Technology Group Corporation (CETC54), Shijiazhuang, China; The 54th Research Institute of China Electronics Technology Group Corporation (CETC54), Shijiazhuang, China; The 54th Research Institute of China Electronics Technology Group Corporation (CETC54), Shijiazhuang, China; School of Economics and Management, Shijiazhuang Tiedao University, Shijiazhuang, China","IEEE Access","11 Jul 2022","2022","10","","69992","70000","To improve the performance of anti-jamming communication in dynamic and adversarial jamming environment, an improved anti-jamming method is proposed based on deep reinforcement learning and feature engineering. Different from the existing studies that use computer vision of deep learning based on the infinite state of spectrum waterfall, the proposed method relays on analyzing spectrum differences between adjacent time slots which contains information and features of jamming patterns. First, anti-jamming strategy is trained by countering the jammer which carries out a random jamming patterns switching strategy. Second, an improved state space is introduced by containing historical spectrum of communication and jamming signal between adjacent time slots, which can help an anti-jamming agent effectively extract the features of jamming patterns to reduce computational complexity. In addition, an improved reward function based on channel switch cost is improved for considering propagation characteristics which may cause communication performance lost. Taking advantage of both feature engineering and deep reinforcement learning, an improved anti-jamming method is proposed to improve reliable anti-jamming performance. Compared with the traditional CNN-based deep reinforcement learning anti-jamming method, simulation results show that the improved method can obtain better performance and lower computational complexity.","2169-3536","","10.1109/ACCESS.2022.3187030","China Postdoctoral Science Foundation(grant numbers:2021M693002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810238","Anti-jamming;communication;feature engineering;reinforcement learning (RL);deep learning (DL)","Jamming;Feature extraction;Communication channels;Switches;Time-frequency analysis;Reliability;Geometry","computational complexity;deep learning (artificial intelligence);jamming;radiowave propagation;reinforcement learning;telecommunication computing;telecommunication network reliability","improved anti-jamming method;deep reinforcement learning;feature engineering;anti-jamming communication;dynamic jamming environment;adversarial jamming environment;deep learning;adjacent time slots;anti-jamming strategy;random jamming patterns;improved state space;jamming signal;anti-jamming agent;reliable anti-jamming performance","","3","","23","CCBY","29 Jun 2022","","","IEEE","IEEE Journals"
"Affordance-Based Human–Robot Interaction With Reinforcement Learning","F. Munguia-Galeano; S. Veeramani; J. D. Hernández; Q. Wen; Z. Ji","School of Engineering, Cardiff University, Cardiff, U.K; Cooper Group, University of Liverpool, Liverpool, U.K; School of Computer Science and Informatics, Cardiff University, Cardiff, U.K; School of Engineering, Cardiff University, Cardiff, U.K; School of Engineering, Cardiff University, Cardiff, U.K","IEEE Access","3 Apr 2023","2023","11","","31282","31292","Planning precise manipulation in robotics to perform grasp and release-related operations, while interacting with humans is a challenging problem. Reinforcement learning (RL) has the potential to make robots attain this capability. In this paper, we propose an affordance-based human-robot interaction (HRI) framework, aiming to reduce the action space size that would considerably impede the exploration efficiency of the agent. The framework is based on a new algorithm called Contextual Q-learning (CQL). We first show that the proposed algorithm trains in a reduced amount of time (2.7 seconds) and reaches an 84% of success rate. This suits the robot’s learning efficiency to observe the current scenario configuration and learn to solve it. Then, we empirically validate the framework for implementation in HRI real-world scenarios. During the HRI, the robot uses semantic information from the state and the optimal policy of the last training step to search for relevant changes in the environment that may trigger the generation of a new policy.","2169-3536","","10.1109/ACCESS.2023.3262450","Consejo Nacional de Ciencia y Tecnología (CONACyT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10082924","Q-learning;robotics;affordances;robot learning;human-robot interaction","Robot learning;Human-robot interaction;Affordances;Task analysis;Robot kinematics;Planning;Service robots;Q-learning","control engineering computing;human-robot interaction;manipulators;reinforcement learning","affordance-based human-robot interaction;contextual Q-learning;CQL;HRI;learning efficiency;planning precise manipulation;reinforcement learning;release-related operations;RL;robotics","","2","","65","CCBYNCND","27 Mar 2023","","","IEEE","IEEE Journals"
"Enhanced Off-Policy Reinforcement Learning With Focused Experience Replay","S. -H. Kong; I. M. A. Nahrendra; D. -H. Paek","The Cho Chun Shik Graduate School of Green Transportation, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; The Robotics Program, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; The Cho Chun Shik Graduate School of Green Transportation, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea","IEEE Access","2 Jul 2021","2021","9","","93152","93164","Utilizing the collected experience tuples in the replay buffer (RB) is the primary way of exploiting the experiences in the off-policy reinforcement learning (RL) algorithms, and, therefore, the sampling scheme for the experience tuples in the RB can be critical for experience utilization. In this paper, it is found that a widely used sampling scheme in the off-policy RL suffers from inefficiency due to the inadequate uneven sampling of experience tuples from the RB. In fact, the conventional uniform sampling of the experience tuples in the RB causes a severely unbalanced experience utilization, since experiences stored earlier in the RB is sampled with much higher frequency especially in the early stage of learning. We mitigate this fundamental problem by employing a half-normal sampling probability window that allocates a higher sampling probability to newer experiences in the RB. In addition, we propose general and local size adjustment schemes that determine the standard deviation of the half-normal sampling window to enhance the learning speed and performance and to mitigate the temporary performance degradation during training, respectively. For performance demonstration, we apply the proposed sampling technique to the state-of-the-art off-policy RL algorithms and test for various RL benchmark tasks such as MuJoCo gym and CARLA simulator. As a result, the proposed technique shows considerable learning speed and final performance improvement, especially on the tasks with large state and action space. Furthermore, the proposed sampling technique increases the stability of the considered RL algorithms, verified with less variance of the performance results across different random seeds of network initialization.","2169-3536","","10.1109/ACCESS.2021.3085142","Institute of Information and Communications Technology Planning and Evaluation (IITP); Ministry of Science and ICT (MSIT) of Korea Government, Development of Artificial Intelligence Technology that Continuously Improves Itself as the Situation Changes in the Real World(grant numbers:2020-0-00440); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444458","Reinforcement learning;off-policy;actor-critic;experience replay;replay buffer","Erbium;Reinforcement learning;Task analysis;Buffer storage;Standards;Robots;Time-frequency analysis","buffer storage;learning (artificial intelligence);sampling methods;task analysis","experience replay;experience tuples;off-policy reinforcement learning algorithms;sampling scheme;off-policy RL;uniform sampling;higher sampling probability;adjustment schemes;half-normal sampling window;replay buffer;MuJoCo gym;CARLA simulator","","2","","36","CCBYNCND","31 May 2021","","","IEEE","IEEE Journals"
"A Deep Reinforcement Learning Framework for High-Dimensional Circuit Linearization","C. Rong; J. Paramesh; L. R. Carley","Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Transactions on Circuits and Systems II: Express Briefs","29 Aug 2022","2022","69","9","3665","3669","Despite the successes of Reinforcement Learning (RL) in recent years, tasks that require exploring over long trajectories with limited feedback and searching in high-dimensional space remain challenging. This brief proposes a deep RL framework for high-dimensional circuit linearization with an efficient exploration strategy leveraging a scaled dot-product attention scheme and search on the replay technique. As a proof of concept, a 5-bit digital-to-time converter (DTC) is built as the environment, and an RL agent learns to tune the calibration words of the delay stages to minimize the integral nonlinearity (INL) with only scalar feedback. The policy network which selects calibration words is trained by the Soft Actor-Critic (SAC) algorithm. Our results show that the proposed RL framework can reduce the INL to less than 0.5 LSB within 60, 000 trials, which is much smaller than the size of searching space.","1558-3791","","10.1109/TCSII.2022.3183156","National Science Foundation(grant numbers:CRI-1823235); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796640","Deep reinforcement learning;circuits calibration;high-dimensional searching;attention scheme","Calibration;Delays;Training;Linearity;Codes;Phase locked loops;Reinforcement learning","analogue-digital conversion;calibration;learning (artificial intelligence)","deep Reinforcement;high-dimensional circuit linearization;Reinforcement Learning;long trajectories;high-dimensional space;deep RL framework;efficient exploration strategy leveraging;scaled dot-product attention scheme;5-bit digital-to-time converter;RL agent;calibration words;searching space","","2","","23","IEEE","15 Jun 2022","","","IEEE","IEEE Journals"
"Sequential Association Rule Mining for Autonomously Extracting Hierarchical Task Structures in Reinforcement Learning","B. Ghazanfari; F. Afghah; M. E. Taylor","School of Informatics, Computing, and Cyber Security, Northern Arizona University, Flagstaff, USA; School of Informatics, Computing, and Cyber Security, Northern Arizona University, Flagstaff, USA; School of Electrical Engineering and Computer Science, Washington State University, Pullman, USA","IEEE Access","22 Jan 2020","2020","8","","11782","11799","Reinforcement learning (RL) techniques, while often powerful, can suffer from slow learning speeds, particularly in high dimensional spaces or in environments with sparse rewards. The decomposition of tasks into a hierarchical structure holds the potential to significantly speed up learning, generalization, and transfer learning. However, the current task decomposition techniques often cannot extract hierarchical task structures without relying on high-level knowledge provided by an expert (e.g., using dynamic Bayesian networks (DBNs) in factored Markov decision processes), which is not necessarily available in autonomous systems. In this paper, we propose a novel method based on Sequential Association Rule Mining that can extract Hierarchical Structure of Tasks in Reinforcement Learning (SARM-HSTRL) in an autonomous manner for both Markov decision processes (MDPs) and factored MDPs. The proposed method leverages association rule mining to discover the causal and temporal relationships among states in different trajectories and extracts a task hierarchy that captures these relationships among sub-goals as termination conditions of different sub-tasks. We prove that the extracted hierarchical policy offers a hierarchically optimal policy in MDPs and factored MDPs. It should be noted that SARM-HSTRL extracts this hierarchical optimal policy without having dynamic Bayesian networks in scenarios with a single task trajectory and also with multiple tasks' trajectories. Furthermore, we show theoretically and empirically that the extracted hierarchical task structure is consistent with trajectories and provides the most efficient, reliable, and compact structure under appropriate assumptions. The numerical results compare the performance of the proposed SARM-HSTRL method with conventional HRL algorithms in terms of the accuracy in detecting the sub-goals, the validity of the extracted hierarchies, and the speed of learning in several testbeds. The key capabilities of SARM-HSTRL including handling multiple tasks and autonomous hierarchical task extraction can lead to the application of this HRL method in reusing, transferring, and generalization of knowledge in different domains.","2169-3536","","10.1109/ACCESS.2020.2965930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8957114","Association rule mining;extracting task structure;hierarchical reinforcement learning","Task analysis;Trajectory;Reinforcement learning;Markov processes;Bayes methods;Autonomous systems;Reliability","belief networks;data mining;learning (artificial intelligence);Markov processes","sequential association rule mining;hierarchical task structures;reinforcement learning techniques;slow learning speeds;high dimensional spaces;hierarchical structure;transfer learning;current task decomposition techniques;high-level knowledge;dynamic Bayesian networks;Markov decision processes;autonomous systems;factored MDPs;task hierarchy;hierarchical optimal policy;single task trajectory;multiple tasks;SARM-HSTRL method;autonomous hierarchical task extraction;SARM-HSTRL","","2","","53","CCBY","13 Jan 2020","","","IEEE","IEEE Journals"
"Flow-Based Reinforcement Learning","D. Samarasinghe; M. Barlow; E. Lakshika","School of Engineering and IT, University of New South Wales, Canberra, ACT, Australia; School of Engineering and IT, University of New South Wales, Canberra, ACT, Australia; School of Engineering and IT, University of New South Wales, Canberra, ACT, Australia","IEEE Access","3 Oct 2022","2022","10","","102247","102265","This paper presents a novel Flow-based reinforcement learning strategy to model agent systems that can adapt to complex and dynamic problem environments by incrementally mastering their skills. It is inspired by the psychological notion of Flow that describes the optimal mental state experienced by an individual when they are fully immersed in a task and find it intrinsically rewarding to engage with. The proposed model presents an algorithm to describe the Flow experience such that agents can be trained through finer distinctions to the challenges across training time to maintain them in the Flow zone. In contrast to the traditional and incremental learning approaches that suffer from limitations associated with overfitting, the Flow-based model drives agent behaviours not simply through external goals but also through intrinsic curiosity to improve their skills and thus the performance levels. Experimental evaluations are conducted across two simulation environments on a maze navigation task and a reward collection task with comparisons against a generic reinforcement learning model and an incremental reinforcement learning model. The results reveal that these two models are prone to overfit under different design decisions and loose the ability to perform in dynamic variations of the tasks in varying degrees. Conversely, the proposed Flow-based model is capable of achieving near optimal solutions with random environmental factors, appropriately utilising the previously learned knowledge to identify robust solutions to complex problems.","2169-3536","","10.1109/ACCESS.2022.3209260","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902986","Flow;reinforcement learning;incremental learning;machine learning;artificial intelligence","Machine learning;Adaptation models;Reinforcement learning;Psychology;Learning (artificial intelligence);Complexity theory;Artificial intelligence","multi-agent systems;optimisation;psychology;reinforcement learning","maze navigation task;reward collection task;incremental reinforcement learning model;model agent systems;complex problem environments;dynamic problem environments;optimal mental state;flow-based reinforcement learning strategy;random environmental factors","","2","","41","CCBY","26 Sep 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning for Stock Prediction and High-Frequency Trading With T+1 Rules","W. Zhang; T. Yin; Y. Zhao; B. Han; H. Liu","Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Ant Group, Shanghai, MYbank, China; Ant Group, Shanghai, MYbank, China; Shanghai Jiao Tong University, Shanghai, China","IEEE Access","15 Feb 2023","2023","11","","14115","14127","The high-frequency trading framework for the price trend prediction model and trading strategy has been a popular approach for T+0 trading in the stock market. The prediction model is used to predict price trends, and the trading strategy is used to determine the price and volume of the order. Most trading strategies consist of multiple trading logic associated with certain tuning parameters. These parameters significantly affect the profitability of high-frequency trading frameworks. There are two main disadvantages of this framework: 1) the price trend prediction model can not adapt to the current market data distribution, and 2) the trading strategy can not adapt to the current market conditions automatically. Thus, the framework cannot always maintain positive revenue. To address this problem, we propose a novel dynamic parameter optimization algorithm based on reinforcement learning for stock prediction and trading, and to generate an adaptive trading framework. First, we use a rolling model training method for stock price trend prediction. Second, we regard each set of strategy parameters as action and devise an inverse reinforcement learning algorithm for the reward function to accurately estimate the reward of each action. Because of the T+1 trading rules of the Chinese stock market, we consider the constraint of limited short position in the reward function. Finally, a reward-enhanced upper confidence bound (UCB) selection algorithm is proposed to automatically optimize the parameters of the trading logic in real-time trading. The experimental results show that our method achieves competitive performance in the Chinese stock market.","2169-3536","","10.1109/ACCESS.2022.3197165","Interdisciplinary Program of Shanghai Jiao Tong University(grant numbers:ZH2018QNB12,YG2022QN011); Ant Group, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9852239","High-frequency trading;inverse reinforcement learning;multi-armed bandit learning","Predictive models;Stock markets;Reinforcement learning;Market research;Data models;Heuristic algorithms;Supervised learning","learning (artificial intelligence);pricing;profitability;reinforcement learning;stock markets","adaptive trading framework;Chinese stock market;high-frequency trading framework;multiple trading logic;price trend prediction model;price trends;real-time trading;reinforcement learning;stock prediction;stock price trend prediction;strategy parameters;T+1 trading rules;trading strategy","","2","","45","CCBY","8 Aug 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Autonomous Map-Less Navigation of a Flying Robot","O. Doukhi; D. J. Lee","Department of Mechanical Design Engineering, Jeonbuk National University, Jeonju, South Korea; Department of Mechanical Design Engineering, Jeonbuk National University, Jeonju, South Korea","IEEE Access","16 Aug 2022","2022","10","","82964","82976","Flying robots are expected to be used in many tasks, such as aerial delivery, inspection inside dangerous areas, and rescue. However, their deployment in unstructured and highly dynamic environments has been limited. This paper proposes a novel approach for enabling a micro-aerial vehicle (MAV) system equipped with a laser rangefinder and depth sensor to autonomously navigate and explore an unknown indoor or outdoor environment. We built a modular deep-Q-network architecture to fuse information from multiple sensors mounted onboard a vehicle. The developed algorithm can perform collision-free flights in the real world, while being trained entirely on a 3D simulator. The proposed method does not require prior expert demonstrations, 3D mapping, or path planning. It transforms fused sensory data into a velocity control input for a robot through an end-to-end convolutional neural network (CNN). The obtained policy was compared to a simulation using the conventional potential-field method. Our approach achieves zero- shot transfer from simulation to real-world environments that were never experienced during training by simulating realistic sensor data. Several intensive experiments were conducted to demonstrate the effectiveness of our system for safely flying in dynamic outdoor and indoor environments. The supplementary videos for the actual flight tests can be accessed at https://bit.ly/2SEw8dQ.","2169-3536","","10.1109/ACCESS.2022.3162702","Unmanned Vehicles Core Technology Research and Development Program through the National Research Foundation of Korea (NRF), Unmanned Vehicle Advanced Research Center (UVARC); Ministry of Science Information and Communications Technologies (ICT), the Republic of Korea(grant numbers:2020M3C1C1A01082375); DNA+ Drone Technology Development Program through NRF; Ministry of Science ICT(grant numbers:NRF-2020M3C1C2A01080819); Jeonbuk National University, in 2021; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9743451","Autonomous navigation;sensor-fusion;collision-free;deep Q-network;zero-shot transfer;micro aerial vehicle","Robots;Robot sensing systems;Collision avoidance;Navigation;Autonomous robots;Three-dimensional displays;Task analysis","aerospace robotics;collision avoidance;convolutional neural nets;deep learning (artificial intelligence);laser ranging;microrobots;mobile robots;object detection;robot vision;velocity control","outdoor environment;modular deep-Q-network architecture;multiple sensors;collision-free flights;prior expert demonstrations;path planning;sensory data;velocity control input;end-to-end convolutional neural network;conventional potential-field method;real-world environments;realistic sensor data;dynamic outdoor environments;indoor environments;deep reinforcement;autonomous map-less navigation;flying robot;aerial delivery;inspection;dangerous areas;unstructured environments;highly dynamic environments;microaerial vehicle system;laser rangefinder;depth sensor;unknown indoor;MAV system;CNN","","2","","54","CCBY","28 Mar 2022","","","IEEE","IEEE Journals"
"Unmanned Aerial Vehicle Swarm Cooperative Decision-Making for SEAD Mission: A Hierarchical Multiagent Reinforcement Learning Approach","L. Yue; R. Yang; J. Zuo; Y. Zhang; Q. Li; Y. Zhang","Air Traffic Control and Navigation College, Air Force Engineering University, Xi’an, China; Air Traffic Control and Navigation College, Air Force Engineering University, Xi’an, China; Air Traffic Control and Navigation College, Air Force Engineering University, Xi’an, China; Air Traffic Control and Navigation College, Air Force Engineering University, Xi’an, China; Aeronautics Engineering College, Air Force Engineering University, Xi’an, China; Xi’an Modern Control Technology Research Institute, Xi’an, China","IEEE Access","9 Sep 2022","2022","10","","92177","92191","Unmanned aerial vehicle (UAV) swarm cooperative decision-making has attracted increasing attentions because of its low-cost, reusable, and distributed characteristics. However, existing non-learning-based methods rely on small-scale, known scenarios, and cannot solve complex multi-agent cooperation problem in large-scale, uncertain scenarios. This paper proposes a hierarchical multi-agent reinforcement learning (HMARL) method to solve the heterogeneous UAV swarm cooperative decision-making problem for the typical suppression of enemy air defense (SEAD) mission, which is decoupled into two sub-problems, i.e., the higher-level target allocation (TA) sub-problem and the lower-level cooperative attacking (CA) sub-problem. A HMARL agent model, consisting of a multi-agent deep Q network (MADQN) based TA agent and multiple independent asynchronous proximal policy optimization (IAPPO) based CA agents, is established. MADQN-TA agent can dynamically adjust the TA schemes according to the relative position. To encourage exploration and promote learning efficiency, the Metropolis criterion and inter-agent information exchange techniques are introduced. IAPPO-CA agent adopts independent learning paradigm, which can easily scale with the number of agents. Comparative simulation results validate the effectiveness, robustness, and scalability of the proposed method.","2169-3536","","10.1109/ACCESS.2022.3202938","National Natural Science Foundation of China(grant numbers:62106284); Natural Science Foundation of Shaanxi Province, China(grant numbers:2021JQ-370); Young Talent Fund of Association for Science and Technology in Shaanxi, China(grant numbers:20220101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869884","UAV swarm;suppression of enemy air defense;deep reinforcement learning;multi-agent;hierarchical reinforcement learning","Decision making;Jamming;Resource management;Autonomous aerial vehicles;Reinforcement learning;Optimization;Neural networks","autonomous aerial vehicles;decision making;learning (artificial intelligence);multi-agent systems;remotely operated vehicles","unmanned aerial vehicle swarm;decision-making;SEAD mission;hierarchical multiagent reinforcement learning approach;distributed characteristics;existing nonlearning-based methods;known scenarios;complex multiagent cooperation problem;uncertain scenarios;multiagent reinforcement learning method;heterogeneous UAV swarm;enemy air defense mission;higher-level target allocation;lower-level cooperative attacking sub-problem;HMARL agent model;multiagent deep Q network;multiple independent asynchronous proximal policy optimization based CA agents;MADQN-TA agent;TA schemes;learning efficiency;inter-agent information exchange techniques;independent learning paradigm","","2","","42","CCBY","29 Aug 2022","","","IEEE","IEEE Journals"
"Deep Adversarial Reinforcement Learning With Noise Compensation by Autoencoder","K. Ohashi; K. Nakanishi; W. Sasaki; Y. Yasui; S. Ishii","Department of Systems Science, Graduate School of Informatics, Kyoto University, Kyoto, Japan; Department of Systems Science, Graduate School of Informatics, Kyoto University, Kyoto, Japan; Department of Systems Science, Graduate School of Informatics, Kyoto University, Kyoto, Japan; Honda Research and Development Innovative Research Center, Tokyo, Japan; Department of Systems Science, Graduate School of Informatics, Kyoto University, Kyoto, Japan","IEEE Access","27 Oct 2021","2021","9","","143901","143912","We present a new adversarial learning method for deep reinforcement learning (DRL). Based on this method, robust internal representation in a deep Q-network (DQN) was introduced by applying adversarial noise to disturb the DQN policy; however, it was compensated for by the autoencoder network. In particular, we proposed the use of a new type of adversarial noise: it encourages the policy to choose the worst action leading to the worst outcome at each state. When the proposed method, called deep Q-W-network regularized with an autoencoder (DQWAE), was applied to seven different games in an Atari 2600, the results were convincing. DQWAE exhibited greater robustness against the random/adversarial noise added to the input and accelerated the learning process more than the baseline DQN. When applied to a realistic automatic driving simulation, the proposed DRL method was found to be effective at rendering the acquired policy robust against random/adversarial noise.","2169-3536","","10.1109/ACCESS.2021.3121751","Grant-in-Aid for Scientific Research on Innovative Areas, JSPS KAKENHI, Japan(grant numbers:JP17H06310); Grant-in-Aid for Scientific Research B, JSPS KAKENHI, Japan(grant numbers:JP19H04180); New Energy and Industrial Technology Development Organization (NEDO), Japan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9583275","Deep reinforcement learning;adversarial learning;robustness;regularization;automatic vehicle control","Training;Perturbation methods;Reinforcement learning;Robustness;Games;Noise measurement;Adversarial machine learning","computer games;deep learning (artificial intelligence)","worst action;worst outcome;Q-W-network;DQWAE;learning process;baseline DQN;DRL method;acquired policy robust;deep adversarial reinforcement learning;noise compensation;adversarial learning method;deep reinforcement learning;robust internal representation;deep Q-network;DQN policy;autoencoder network;Atari 2600","","2","","45","CCBYNCND","21 Oct 2021","","","IEEE","IEEE Journals"
"An Enhanced Model-Free Reinforcement Learning Algorithm to Solve Nash Equilibrium for Multi-Agent Cooperative Game Systems","Y. Jiang; F. Tan","College of Information Engineering, Shanghai Maritime University, Shanghai, China; College of Information Engineering, Shanghai Maritime University, Shanghai, China","IEEE Access","23 Dec 2020","2020","8","","223743","223755","Solving the Nash equilibrium is important for multi-agent game systems, and the speed of reaching Nash equilibrium is critical for the agent to quickly make real-time decisions. A typical scheme is the model-free reinforcement learning algorithm based on policy iteration, which is slow because each iteration will be calculated from the start state to the end state. In this paper, we propose a faster scheme based on value iteration, using Q-function in an online manner to solve the Nash equilibrium of the system. Since the calculation is based on the value from the last iteration, the convergence speed of the proposed scheme is much faster than the policy iteration. The rationality and convergence of this scheme are analyzed and proved theoretically. An actor-critic network structure is used to implement this scheme through simulation. The simulation results show that the convergence speed of our proposed scheme is about 10 times faster than that of the policy iteration algorithm.","2169-3536","","10.1109/ACCESS.2020.3043806","National Natural Science Foundation of China (NSFC)(grant numbers:61673117,52071200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9290038","Nash equilibrium;multi-agent game systems;model-free reinforcement learning","Nash equilibrium;Games;Heuristic algorithms;Convergence;Mathematical model;Synchronization;Performance analysis","game theory;iterative methods;learning (artificial intelligence);multi-agent systems","policy iteration algorithm;Nash equilibrium;real-time decisions;start state;end state;value iteration;convergence speed;actor-critic network structure;enhanced model-free reinforcement learning algorithm;multiagent cooperative game systems","","2","","26","CCBY","10 Dec 2020","","","IEEE","IEEE Journals"
