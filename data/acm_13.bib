@inproceedings{10.1145/2464576.2480773,
author = {Zhang, Jin and Maringer, Dietmar},
title = {Indicator Selection for Daily Equity Trading with Recurrent Reinforcement Learning},
year = {2013},
isbn = {9781450319645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2464576.2480773},
doi = {10.1145/2464576.2480773},
abstract = {Recurrent reinforcement learning (RRL), a machine learning technique, is very successful in training high frequency trading systems. When trading analysis of RRL is done with lower frequency financial data, e.g. daily stock prices, the decrease of autocorrelation in prices may lead to a decrease in trading profit. In this paper, we propose a RRL trading system which utilizes the price information, jointly with the indicators from technical analysis, fundamental analysis and econometric analysis, to produce long/short signals for daily trading. In the proposed trading system, we use a genetic algorithm as a pre-screening tool to search suitable indicators for RRL trading. Moreover, we modify the original RRL parameter update scheme in the literature for out-of-sample trading. Empirical studies are conducted based on data sets of 238 S\&amp;P stocks. It is found that the trading performance concerning the out-of sample daily Sharpe ratios turns better: the number of companies with a positive and significant Sharpe ratio increases after feeding the selected indicators jointly with prices information into the RRL system.},
booktitle = {Proceedings of the 15th Annual Conference Companion on Genetic and Evolutionary Computation},
pages = {1757–1758},
numpages = {2},
keywords = {econometric analysis, fundamental analysis, recurrent reinforcement learning, technical analysis, genetic algorithm},
location = {Amsterdam, The Netherlands},
series = {GECCO '13 Companion}
}

@inproceedings{10.1145/3477495.3531703,
author = {Zhao, Xiangyu and Xin, Xin and Zhang, Weinan and Zhao, Li and Yin, Dawei and Yang, Grace Hui},
title = {DRL4IR: 3rd Workshop on Deep Reinforcement Learning for Information Retrieval},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531703},
doi = {10.1145/3477495.3531703},
abstract = {Information retrieval (IR) systems have become an essential component in modern society to help users find useful information, which consists of a series of processes including query expansion, item recall, item ranking and re-ranking, etc. Based on the ranked information list, users can provide their feedbacks. Such an interaction process between users and IR systems can be naturally formulated as a decision-making problem, which can be either one-step or sequential. In the last ten years, deep reinforcement learning (DRL) has become a promising direction for decision-making, since DRL utilizes the high model capacity of deep learning for complex decision-making tasks. Recently, there have been emerging research works focusing on leveraging DRL for IR tasks. However, the fundamental information theory under DRL settings, the principle of RL methods for IR tasks, or the experimental evaluation protocols of DRL-based IR systems, has not been deeply investigated.To this end, we propose the third DRL4IR workshop (https://drl4ir.github.io) at SIGIR 2022, which provides a venue for both academia researchers and industry practitioners to present the recent advances of DRL-based IR system, to foster novel research, interesting findings, and new applications of DRL for IR. In the last two years, DRL4IR organized at SIGIR'20/21 was one of the most successful workshops and attracted over 200 workshop attendees each year. In this year, we will pay more attention to fundamental research topics and recent application advances, with an expectation of over 300 workshop participants.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3488–3491},
numpages = {4},
keywords = {deep reinforcement learning, information retrieval},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3483845.3483864,
author = {Li, Yanlin and An, Shi and Zhang, Wei},
title = {Application of Reinforcement Learning in Synchrotron Power Supply Synchronization Correction},
year = {2021},
isbn = {9781450390453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483845.3483864},
doi = {10.1145/3483845.3483864},
abstract = {As a powerful tool, machine learning has promoted the development of natural science in many fields and it also has helped engineers make many remarkable achievements in industry. Recently, using machine learning has already emerge in control of heavy ion accelerators. By studying synchronization of heavy ion accelerators, a method based on reinforcement learning is proposed. It’s a method that can make automatic synchronization correction. The action of power supply is simulated to interact with agent. As a precondition of synchronization correction processing, a novel approach is proposed to identify the slower power supply. Experimental results have show that our method can automatically identify the slower power between power supplies and it can make power supply complete synchronization through interaction. Compared with the past method, our algorithm not only saves manpower but also increaing the accuracy of synchronization.},
booktitle = {Proceedings of the 2021 2nd International Conference on Control, Robotics and Intelligent System},
pages = {105–110},
numpages = {6},
keywords = {Accelerator control, Synchronization correction, Reinforcement learning},
location = {Qingdao, China},
series = {CCRIS '21}
}

@inproceedings{10.1145/3386263.3406901,
author = {Bruns, Niklas and Gro\ss{}e, Daniel and Drechsler, Rolf},
title = {Early Verification of ISA Extension Specifications Using Deep Reinforcement Learning},
year = {2020},
isbn = {9781450379441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386263.3406901},
doi = {10.1145/3386263.3406901},
abstract = {For IoT devices the demand in faster execution and at the same time lower energy consumption is a pressing problem. A very promising solution are Application-Specific Instruction-set Processors (ASIPs). They make use of custom instructions, which are added to the processor, forming the Instruction-Set Extension (ISE) of a given Instruction Set Architecture (ISA). While the selection process for the ISE is already challenging, an incorrect ISE specification leads to severe problems: errors and security vulnerabilities go undetected in the first formalization and in the worst case show up ultimately in the final implementation. In this paper, we propose an early verification approach for ISE specifications. Our novel approach is based on two ingredients: (i) Virtual Prototypes (VPs) to enable a rapid creation of an executable specification for the ISE; and (ii) Deep Reinforcement Learning (DRL) to search for ISE programs which violate the ISE specification intent. As case study we consider extensions of the RISC-V base ISA. We demonstrate the effectiveness of our approach for finding functional bugs in the executable specification of the ISE as well as specification gaps in the ISE leading to information leakage.},
booktitle = {Proceedings of the 2020 on Great Lakes Symposium on VLSI},
pages = {297–302},
numpages = {6},
keywords = {early verification, ISA extension, deep reinforcement learning},
location = {Virtual Event, China},
series = {GLSVLSI '20}
}

@inproceedings{10.1145/3430984.3431045,
author = {Koratamaddi, Prahlad and Wadhwani, Karan and Gupta, Mridul and Sanjeevi, Dr. Sriram G.},
title = {A Multi-Agent Reinforcement Learning Approach for Stock Portfolio Allocation},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3431045},
doi = {10.1145/3430984.3431045},
abstract = {Stock portfolio allocation is one of the most challenging and interesting problems of modern finance. Recently, deep reinforcement learning applications have shown promising results in automating portfolio allocation. However, most current approaches use a single agent learning model which could inadequately capture the complex dynamics arising from the interactions of many traders in today’s stock market. In this paper, we explore the applicability of multi-agent deep reinforcement learning to this problem by implementing single-agent, 2-agent, 3-agent, and 4-agent deep deterministic policy gradients (DDPG) algorithms in a competitive setting. Upon analyzing the results obtained using standardized metrics, we observe that there is a significant improvement in the performance of our learning models with the introduction of multiple agents.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science \&amp; Management of Data (8th ACM IKDD CODS \&amp; 26th COMAD)},
pages = {410},
numpages = {1},
keywords = {stock trading, portfolio allocation, reinforcement learning, deep learning, multi-agent systems},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@inproceedings{10.1145/3397271.3401225,
author = {Hong, Daocheng and Li, Yang and Dong, Qiwen},
title = {Nonintrusive-Sensing and Reinforcement-Learning Based Adaptive Personalized Music Recommendation},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401225},
doi = {10.1145/3397271.3401225},
abstract = {As a particularly prominent application of recommender systems on automated personalized service, the music recommendation has been widely used in various music network platforms, music education and music therapy. Importantly, the individual music preference for a certain moment is closely related to personal experience of the music and music literacy, as well as temporal scenario without any interruption. Therefore, this paper proposes a novel policy for music recommendation NRRS (Nonintrusive-Sensing and Reinforcement-Learning based Recommender Systems) by integrating prior research streams. Specifically, we develop a novel recommendation framework for sensing, learning and adaptation to user's current preference based on wireless sensing and reinforcement learning in real time during a listening session. The established music recommendation prototype monitors individual vital signals for listening music, and captures song characters, individual dynamic preferences, and that it can yield better listening experience for users.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1721–1724},
numpages = {4},
keywords = {nonintrusive sensing, reinforcement learning, adaptive playlist recommendation, musical preference learning},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3508230.3508255,
author = {Fyodorov, Victor and Karsakov, Andrey},
title = {Method of Graphical User Interface Adaptation Using Reinforcement Learning and Automated Testing},
year = {2022},
isbn = {9781450387354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508230.3508255},
doi = {10.1145/3508230.3508255},
abstract = {Abstract—Graphical user interface adaptation becomes an increasingly time-consuming and resource-intensive task due to modern programs complexity and a big variety of information output devices. In this paper we propose a method for adapting a graphical user interface based on a person's workflow using a specific implementation of the interface. This method makes it possible to adapt the interface to the peculiarities of the user's workflow through optimization in the navigation area between program windows.},
booktitle = {Proceedings of the 2021 5th International Conference on Natural Language Processing and Information Retrieval},
pages = {163–167},
numpages = {5},
keywords = {Graphical user interface, adaptation, user workflow, abstract user, machine learning},
location = {Sanya, China},
series = {NLPIR '21}
}

@inproceedings{10.1145/3487664.3487698,
author = {Hirchoua, Badr and Mountasser, Imadeddine and Ouhbi, Brahim and Frikh, Bouchra},
title = {Evolutionary Deep Reinforcement Learning Environment: Transfer Learning-Based Genetic Algorithm},
year = {2022},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487698},
doi = {10.1145/3487664.3487698},
abstract = {Stock markets trading has risen as a critical challenge for artificial intelligence research. Such environments require artificial agents to coordinate and transfer their best experience to other agents. However, the strongest agents have been trained using expert capabilities or employing hand-crafted experts features. Notwithstanding these improvements, no previous single system has come near to mastering the trading environment. We address the algorithmic trading challenge using an evolutive learning method: a multi-agent reinforcement learning algorithm that uses only self trades generated by different generations of agents. The evolution-based algorithm genetic works as an evolutive league that continually adapt the agent’s internal strategies and push forward the system to train the new creative behavior for the next generations. Besides, a deep recurrent neural network drives the mutation mechanism through the attention that dynamically encodes the memory mutation size. The final agent achieved encouraging performances and outperformed traditional and intelligent baselines.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {242–249},
numpages = {8},
keywords = {Deep reinforcement learning, Financial data, Financial engineering, Knowledge uncertainty, Trading system, Partially observable Markov decision process},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.5555/3437539.3437796,
author = {Kwon, Eunji and Han, Sodam and Park, Yoonho and Kim, Young Hwan and Kang, Seokhyeong},
title = {Reinforcement Learning-Based Power Management Policy for Mobile Device Systems: Late Breaking Results},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {This paper presents a power management policy that exploits reinforcement learning to increase power efficiency of mobile device systems. Our Q-learning-based policy predicts a system's characteristics and learns power management controls to adapt to the system's variations. Therefore, we can flexibly manage the system power regardless of the application scenario and can achieve lower energy per QoS compared to previous dynamic voltage/frequency scaling governors. To minimize the process overhead, we implemented our power management policy as hardware; the hardware-implemented policy reduced the average latency up to 40\texttimes{} compared to the software-implemented policy.},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {257},
numpages = {2},
location = {Virtual Event, USA},
series = {DAC '20}
}

@inproceedings{10.5555/3237383.3237846,
author = {Liebman, Elad and Zavesky, Eric and Stone, Peter},
title = {A Stitch in Time - Autonomous Model Management via Reinforcement Learning},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Concept drift - a change, either sudden or gradual, in the underlying properties of data - is one of the most prevalent challenges to maintaining high-performing learned models over time in autonomous systems. In the face of concept drift, one can hope that the old model is sufficiently representative of the new data despite the concept drift, one can discard the old data and retrain a new model with (often limited) new data, or one can use transfer learning methods to combine the old data with the new to create an updated model. Which of these three options is chosen affects not only near-term decisions, but also future needs to transfer or retrain. In this paper, we thus model response to concept drift as a sequential decision making problem and formally frame it as a Markov Decision Process. Our reinforcement learning approach to the problem shows promising results on one synthetic and two real-world datasets.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {990–998},
numpages = {9},
keywords = {reinforcement learning, model retraining, concept drift},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.5555/3306127.3331872,
author = {Bacchiani, Giulio and Molinari, Daniele and Patander, Marco},
title = {Microscopic Traffic Simulation by Cooperative Multi-Agent Deep Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Expert human drivers perform actions relying on traffic laws and their previous experience. While traffic laws are easily embedded into an artificial brain, modeling human complex behaviors which come from past experience is a more challenging task. One of these behaviors is the capability of communicating intentions and negotiating the right of way through driving actions, as when a driver is entering a crowded roundabout and observes other cars movements to guess the best time to merge in. In addition, each driver has its own unique driving style, which is conditioned by both its personal characteristics, such as age and quality of sight, and external factors, such as being late or in a bad mood. For these reasons, the interaction between different drivers is not trivial to simulate in a realistic manner. In this paper, this problem is addressed by developing a microscopic simulator using a Deep Reinforcement Learning Algorithm based on a combination of visual frames, representing the perception around the vehicle, and a vector of numerical parameters. In particular, the algorithm called Asynchronous Advantage Actor-Critic has been extended to a multi-agent scenario in which every agent needs to learn to interact with other similar agents. Moreover, the model includes a novel architecture such that the driving style of each vehicle is adjustable by tuning some of its input parameters, permitting to simulate drivers with different levels of aggressiveness and desired cruising speeds.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1547–1555},
numpages = {9},
keywords = {microscopic traffic simulation, deep reinforcement learning, agent cooperation and negotiation, multi-agent systems},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3592307.3592333,
author = {Ong, Yi Jun and Durga Kumar, Burra Venkata},
title = {Distributed Internet of Things Load Balancing Using Deep Reinforcement Learning},
year = {2023},
isbn = {9798400700002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592307.3592333},
doi = {10.1145/3592307.3592333},
abstract = {The current emerging fifth-generation (5G) system has a significant impact on the usage of Internet of Things (IoT) devices. Load balancing plays an important role in the distributed system, as it is directly associated with the performance of the whole system. Therefore, in this paper, the author recommends a load-balancing architecture for distributed IoT systems based on deep reinforcement learning (DRL), which is capable of dealing with dynamic and large-scale network situations. Specifically, the author recommends implementing a two-layer load balancing architecture. The top layer uses the long short term memory (LSTM) based Dueling Double Deep Q-Learning Network (D3QN) model for clustering the IoT devices. The bottom layer uses the plain DRL with more than one behavior policy on joint exploration. The key objective of this paper is to improve load balancing by using the technique mentioned above, as the data source and the infrastructure of the distributed IoT system can be dynamic. Experiments are conducted by using real-world datasets for evaluating the implementation. The outcome shows that the implementation of DRL on load balancing has indeed achieved a significant improvement in the performance of the distributed IoT system compared to other simplified DRL models and static clustering methods.},
booktitle = {Proceedings of the 2023 6th International Conference on Electronics, Communications and Control Engineering},
pages = {169–173},
numpages = {5},
keywords = {Two Layer Architecture, Distributed System, Long Short Term Memory, Internet of Things, Dueling Double Deep Q-Learning Network, Load Balancing},
location = {Fukuoka, Japan},
series = {ICECC '23}
}

@inproceedings{10.1145/3505170.3506721,
author = {Chen, Po-Yan and Ke, Bing-Ting and Lee, Tai-Cheng and Tsai, I-Ching and Kung, Tai-Wei and Lin, Li-Yi and Liu, En-Cheng and Chang, Yun-Chih and Li, Yih-Lang and Chao, Mango C.-T.},
title = {A Reinforcement Learning Agent for Obstacle-Avoiding Rectilinear Steiner Tree Construction},
year = {2022},
isbn = {9781450392105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505170.3506721},
doi = {10.1145/3505170.3506721},
abstract = {This paper presents a router, which tackles a classic algorithm problem in EDA, obstacle-avoiding rectilinear Steiner minimum tree (OARSMT), with the help of an agent trained by our proposed policy-based reinforcement-learning (RL) framework. The job of the policy agent is to select an optimal set of Steiner points that can lead to an optimal OARSMT based on a given layout. Our RL framework can iteratively upgrade the policy agent by applying Monte-Carlo tree search to explore and evaluate various choices of Steiner points on various unseen layouts. As a result, our policy agent can be viewed as a self-designed OARSMT algorithm that can iteratively evolves by itself. The initial version of the agent is a sequential one, which selects one Steiner point at a time. Based on the sequential agent, a concurrent agent can then be derived to predict all required Steiner points with only one model inference. The overall training time can be further reduced by applying geometrically symmetric samples for training. The experimental results on single-layer 15x15 and 30x30 layouts demonstrate that our trained concurrent agent can outperform a state-of-the-art OARSMT router on both wire length and runtime.},
booktitle = {Proceedings of the 2022 International Symposium on Physical Design},
pages = {107–115},
numpages = {9},
keywords = {reinforcement learning, obstacle-avoiding rectilinear steiner tree, monte carlo tree search, steiner point prediction},
location = {Virtual Event, Canada},
series = {ISPD '22}
}

@inproceedings{10.5555/3545946.3598869,
author = {Hayes, Conor F. and R\u{a}dulescu, Roxana and Bargiacchi, Eugenio and Kallstrom, Johan and Macfarlane, Matthew and Reymond, Mathieu and Verstraeten, Timothy and Zintgraf, Luisa M. and Dazeley, Richard and Heintz, Fredrik and Howley, Enda and Irissappane, Athirai A. and Mannion, Patrick and Now\'{e}, Ann and Ramos, Gabriel and Restelli, Marcello and Vamplew, Peter and Roijers, Diederik M.},
title = {A Brief Guide to Multi-Objective Reinforcement Learning and Planning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Real-world sequential decision-making tasks are usually complex, and require trade-offs between multiple, often conflicting, objectives. However, the majority of research in reinforcement learning (RL) and decision-theoretic planning assumes a single objective, or that multiple objectives can be handled via a predefined weighted sum over the objectives. Such approaches may oversimplify the underlying problem, and produce suboptimal results. This extended abstract outlines the limitations of using a semi-blind iterative process to solve multi-objective decision making problems. Our extended paper, serves as a guide for the application of explicitly multi-objective methods to difficult problems.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1988–1990},
numpages = {3},
keywords = {planning, multi-objective, reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3292500.3330754,
author = {Hughes, J. Weston and Chang, Keng-hao and Zhang, Ruofei},
title = {Generating Better Search Engine Text Advertisements with Deep Reinforcement Learning},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330754},
doi = {10.1145/3292500.3330754},
abstract = {Deep Reinforcement Learning has been applied in a number of fields to directly optimize non-differentiable reward functions, including in sequence to sequence settings using Self Critical Sequence Training (SCST). Previously, SCST has primarily been applied to bring conditional language models closer to the distribution of their training set, as in traditional neural machine translation and abstractive summarization. We frame the generation of search engine text ads as a sequence to sequence problem, and consider two related goals: to generate ads similar to those a human would write, and to generate ads with high click-through rates. We jointly train a model to minimize cross-entropy on an existing corpus of Landing Page/Text Ad pairs using typical sequence to sequence training techniques while also optimizing the expected click-through rate (CTR) as predicted by an existing oracle model using SCST. Through joint training we achieve a 6.7\% increase in expected CTR without a meaningful drop in ROUGE score. Human experiments demonstrate that SCST training produces significantly more attractive ads without reducing grammatical quality.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2269–2277},
numpages = {9},
keywords = {click prediction, ad creative optimization, sponsored search, abstractive summarization, seq2seq, deep reinforcement learning},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/2480362.2480511,
author = {Lin, Szu-Yin and Chao, Kuo-Ming and Lo, Chi-Chun and Godwin, Nick},
title = {Distributed Dynamic Data Driven Prediction Based on Reinforcement Learning Approach},
year = {2013},
isbn = {9781450316569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2480362.2480511},
doi = {10.1145/2480362.2480511},
abstract = {In this paper, we propose a new distributed dynamic data driven model and strategy to direct and evaluate the interlinked data sets in Dynamic Data Driven Application Systems (DDDAS). The underlying technique is the introduction of a reinforcement Q-Learning approach including search strategies to determine how to drill and drive a series of highly dependent data in order to increase prediction accuracy and efficiency. In simulation, the new model utilizes individual sensors, distributed databases, and predictors in Dynamic Data Stream Nodes with multiple dimensional variables which can be instantiated to explore the search space, so that search convergence can be improved. We show the effectiveness and applicability of using the technique in the analysis of typhoon rainfall data. The result shows that the proposed approach performed better than traditional linear regression approaches, reducing the error rate by 30.48\%.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on Applied Computing},
pages = {779–784},
numpages = {6},
keywords = {Q-learning, reinforcement learning, dynamic data driven application systems, typhoon rainfall prediction},
location = {Coimbra, Portugal},
series = {SAC '13}
}

@article{10.1145/3611679,
author = {Sandygulova, Anara and Amir, Aida and Oralbayeva, Nurziya and Telisheva, Zhansaule and Zhanatkyzy, Aida and Shakerimov, Aidar and Sarmonov, Shamil and Aimysheva, Arna},
title = {QWriter: A Reinforcement Learning-Based Robot for Early Literacy Acquisition},
year = {2023},
issue_date = {Fall 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3611679},
doi = {10.1145/3611679},
abstract = {Robot-assisted language learning produces comparable results to human tutors in a long-term study with elementary school children.},
journal = {XRDS},
month = {oct},
pages = {10–15},
numpages = {6}
}

@inproceedings{10.1145/3347450.3357654,
author = {Liu, Mengyi and Liu, Zhu},
title = {Deep Reinforcement Learning Visual-Text Attention for Multimodal Video Classification},
year = {2019},
isbn = {9781450369183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3347450.3357654},
doi = {10.1145/3347450.3357654},
abstract = {Nowadays multimedia contents including text, images, and videos have been produced and shared ubiquitously in our daily life, which has encouraged researchers to develop algorithms for multimedia search and analysis in various applications. The trend of web data becoming increasingly multimodal makes the task of multimodal classification ever more popular and pertinent. In this paper, we mainly focus on the scenario of videos for their intrinsic multimodal property, and resort to attention learning among different modalities for classification. Specifically, we formulate the multimodal attention learning as a sequential decision-making process, and propose an end-to-end, deep reinforcement learning based framework to determine the selection of modality at each time step for the final feature aggregation model. To train our policy networks, we design a supervised reward which considers the multi-label classification loss, and two unsupervised rewards which simultaneously consider inter-modality correlation for consistency and intra-modality reconstruction for representativeness. Extensive experiments have been conducted on two large-scale multimodal video datasets to evaluate the whole framework and several key components, including the parameters of policy network, the effects of different rewards, and the rationality of the learned visual-text attention. Promising results demonstrate that our approach outperforms other state-of-the-art methods of attention mechanism and multimodal fusion for video classification task.},
booktitle = {1st International Workshop on Multimodal Understanding and Learning for Embodied Applications},
pages = {13–21},
numpages = {9},
keywords = {visual-text attention, multimodal, deep reinforcement learning},
location = {Nice, France},
series = {MULEA '19}
}

@inproceedings{10.1145/3587716.3587815,
author = {Xuan, Pengyu and Chen, Aiguo and Sha, Zexin},
title = {Preserving Model Privacy for Federated Reinforcement Learning in Complementary Environments},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587716.3587815},
doi = {10.1145/3587716.3587815},
abstract = {Federated reinforcement learning (FRL) uses data from multiple partners interacting with the environment to train a global decision model while maintaining data privacy. In specific situations, it is necessary to protect not only data privacy but also model privacy. That is, our objective is to collaboratively train a global model that can be used privately and independently by the initiator. We refer to this need for model privacy preservation as model monopolization. It allows the task initiator to ensure the privacy of the model during the training process and to own the trained model alone. To the best of our knowledge, model monopolization protection in Federated Reinforcement Learning has not been studied. We propose monopolized federated reinforcement learning in complementary environments via function transformation. We also provide contribution metrics to ensure the sustainability of monopolized federated reinforcement learning and to provide a quantitative basis for task initiators to give back to task participants. The experimental results show that our method achieved model monopolization by training agents in the Grid-world.},
booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
pages = {438–443},
numpages = {6},
keywords = {model monopolization, model privacy, federated reinforcement learning},
location = {Zhuhai, China},
series = {ICMLC '23}
}

@inproceedings{10.1145/3534678.3539180,
author = {Gammelli, Daniele and Yang, Kaidi and Harrison, James and Rodrigues, Filipe and Pereira, Francisco and Pavone, Marco},
title = {Graph Meta-Reinforcement Learning for Transferable Autonomous Mobility-on-Demand},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539180},
doi = {10.1145/3534678.3539180},
abstract = {Autonomous Mobility-on-Demand (AMoD) systems represent an attractive alternative to existing transportation paradigms, currently challenged by urbanization and increasing travel needs. By centrally controlling a fleet of self-driving vehicles, these systems provide mobility service to customers and are currently starting to be deployed in a number of cities around the world. Current learning-based approaches for controlling AMoD systems are limited to the single-city scenario, whereby the service operator is allowed to take an unlimited amount of operational decisions within the same transportation system. However, real-world system operators can hardly afford to fully re-train AMoD controllers for every city they operate in, as this could result in a high number of poor-quality decisions during training, making the single-city strategy a potentially impractical solution. To address these limitations, we propose to formalize the multi-city AMoD problem through the lens of meta-reinforcement learning (meta-RL) and devise an actor-critic algorithm based on recurrent graph neural networks. In our approach, AMoD controllers are explicitly trained such that a small amount of experience within a new city will produce good system performance. Empirically, we show how control policies learned through meta-RL are able to achieve near-optimal performance on unseen cities by learning rapidly adaptable policies, thus making them more robust not only to novel environments, but also to distribution shifts common in real-world operations, such as special events, unexpected congestion, and dynamic pricing schemes.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2913–2923},
numpages = {11},
keywords = {autonomous mobility-on-demand, meta-learning, graph neural networks, reinforcement learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1145/3609110,
author = {Bakshi, Suyash and Johnsson, Lennart},
title = {Computationally Efficient DNN Mapping Search Heuristic Using Deep Reinforcement Learning},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3609110},
doi = {10.1145/3609110},
abstract = {In this work, we present a computationally efficient Reinforcement Learning mapping search heuristic for finding high quality mappings for N-dimensional convolution loops that uses a computationally inexpensive reward function based on potential data reuse of operands to guide the search process. We also present a RL state representation generalizable to N-dimensional convolution loops, and a state representation parsing strategy ensuring that only valid mappings are evaluated for quality. Our RL search heuristic is applicable to multi-core systems with a memory hierarchy. We show that our RL based search heuristic for a range of 3D convolution layers, at significantly lower computational expense than random search, generally yields mappings with lower Energy-Delay Product (EDP) for an architecture with multiple processing elements with shared memory connected to DRAM. Our evaluation results demonstrated across 19 3D convolution layers, shows that our RL method performed only an average 11.24\% of the operations of that of Timeloop’s random search for assessing same number of valid mappings. The mappings found using Timeloop had an average 12.51\% higher EDP compared to lowest EDP mapping found using our RL method. Further, the lowest EDP mappings found using our method had an average only 4.69\texttimes{} higher EDP than the theoretical lower bound EDP, with the best case being only 1.29\texttimes{} higher.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {sep},
articleno = {115},
numpages = {21},
keywords = {mapping search, Reinforcement learning, convolution, DNN}
}

@inproceedings{10.1145/3439133.3439139,
author = {Thachappully Adithya, Praveen and Muthalagu, Raja and Sadhwani, Sapna},
title = {Genesis Net: Fine Tuned Dense Net Configuration Using Reinforcement Learning},
year = {2021},
isbn = {9781450387996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439133.3439139},
doi = {10.1145/3439133.3439139},
abstract = {Designing neural networks even in the case of relatively simpler fully connected neural networks / dense networks is a time-consuming process since the architecture design is done manually based on intuition and manual tweaking. In this paper, we present “Genesis Net”, a dense net that starts off with a very basic configuration (“seed configuration”), and subsequently tweaks itself via reinforcement learning (RL) to arrive at an optimal configuration for the task at hand. Genesis Net attained a test error within 0.59\% of a similar but bigger documented baseline model. Furthermore, our model was able to achieve this using merely 10.11\% of trainable weights that the baseline model used. This significantly smaller network was found using Q-Learning combined with a dynamic action space that allowed for fine tuning the network configuration.},
booktitle = {2020 4th International Conference on Artificial Intelligence and Virtual Reality},
pages = {46–50},
numpages = {5},
keywords = {Neural Architecture Search, Neural Network Design, Q-Learning, Reinforcement Learning, Deep Learning},
location = {Kumamoto, Japan},
series = {AIVR2020}
}

@article{10.1145/3414523,
author = {Rakesh Kumar, S. and Muthuramalingam, S. and Al-Turjman, Fadi},
title = {Multimodal News Feed Evaluation System with Deep Reinforcement Learning Approaches},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3414523},
doi = {10.1145/3414523},
abstract = {Multilingual and multimodal data analysis is the emerging news feed evaluation system. News feed analysis and evaluations are interrelated processes, which are useful in understanding the news factors. The news feed evaluation system can be implemented for single or multilingual language models. Classification techniques used on multilingual news analysis require deep layered learning techniques rather than conventional approaches. In this proposed work, a hierarchical structure of deep learning algorithms is implemented for making an effective complex news evaluation system. Deep learning techniques such as the Deep Cooperative Multilingual Reinforcement Learning Model, the Multidimensional Genetic Algorithm, and the Multilingual Generative Adversarial Network are developed to evaluate a vast number of news feeds. The proposed tech-niques collaborate in a pipeline order to build a deep news feed evaluation system. The implementation details project that the newly proposed system performs 5\% to 12\% better than the other news evaluation systems.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {8},
numpages = {12},
keywords = {RL techniques, multilingual news and analysis, DL techniques, News feeds}
}

@inproceedings{10.1145/3508546.3508614,
author = {Fang, Jie and Rao, Yunqing and Guo, Xiaoqiang and Zhao, Xusheng},
title = {A Reinforcement Learning Algorithm for Two-Dimensional Irregular Packing Problems},
year = {2022},
isbn = {9781450385053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508546.3508614},
doi = {10.1145/3508546.3508614},
abstract = {The two-dimensional(2D) irregular packing problem is a classical optimization problem with NP-Hard characteristics, which has high computational complexity. The traditional heuristic algorithm and meta heuristic algorithm are usually used to solve packing problems, which has low packing efficiency and high time cost. Inspired by reinforcement learning(RL) and combined with the characteristics of 2D irregular pieces packing, a novel method to solve multi-constraint packing problem based on reinforcement learning is proposed in this paper. A reinforcement learning model based on Monte- Carlo (MC) method and a reward mechanism based on packing height return are designed. Finally, an example is used to analyze the optimization effect of the algorithm. The experimental results show that the proposed method can quickly and efficiently realize the packing of 2D irregular pieces, and a better solution can be obtained in an acceptable time.},
booktitle = {Proceedings of the 2021 4th International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {68},
numpages = {6},
keywords = {irregular pieces, two-dimensional packing optimization, machine learning, reinforcement learning, Monte-Carlo algorithm},
location = {Sanya, China},
series = {ACAI '21}
}

@inproceedings{10.1145/3386527.3406729,
author = {Ding, Xinyi and Larson, Eric C.},
title = {Automatic RNN Cell Design for Knowledge Tracing Using Reinforcement Learning},
year = {2020},
isbn = {9781450379519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386527.3406729},
doi = {10.1145/3386527.3406729},
abstract = {Empirical results have shown that deep neural networks achieve superior performance in the application of Knowledge Tracing. However, the design of recurrent cells like long short term memory (LSTM) cells or gated recurrent units (GRU) is influenced largely by applications in natural language processing. They were proposed and evaluated in the context of sequence to sequence modeling, like machine translation. Even though the LSTM cell works well for knowledge tracing, it is unknown if its architecture is ideally suited for knowledge tracing. Despite the fact that there are several recurrent neural network based architectures proposed for knowledge tracing, the methodologies rely on empirical observations and trial and error, which may not be efficient or scalable. In this study, we investigate using reinforcement learning for the automatic design of recurrent neural network cells for knowledge tracing, showing improved performance compared to the LSTM cell. We also discuss a potential method for model regularization using neural architecture search.},
booktitle = {Proceedings of the Seventh ACM Conference on Learning @ Scale},
pages = {285–288},
numpages = {4},
keywords = {recurrent neural network, neural architecture search, knowledge tracing, reinforcement learning, regularization},
location = {Virtual Event, USA},
series = {L@S '20}
}

@inproceedings{10.1145/3529836.3529857,
author = {GE, Jun and QIN, Yuanqi and Li, Yaling and Huang, yanjia and Hu, Hao},
title = {Single Stock Trading with Deep Reinforcement Learning: A Comparative Study},
year = {2022},
isbn = {9781450395700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529836.3529857},
doi = {10.1145/3529836.3529857},
abstract = {In this paper, we apply Deep Reinforcement Learning (DRL) methods to automate the trading of single stock. The A2C, PPO, DDPG, TD3 and SAC deep reinforcement learning models are built and studied comparatively. Shanghai Composite Index (SH00001) is used as the trading stock, where the stock data before the Covid-19 is used as the training set, and the data after the Covid-19 is used as the testing (trading) set to back-test the performance of these models. Experimental results show that the DDPG, TD3, and SAC models outperform the benchmark, among which the DDPG model shows the most obvious advantages in returns and risk control, achieving a cumulative return rate of 25\%, while the TD3 and SAC models achieve a cumulative return rate of 16-17\%. The A2C and PPO models have inferior performance comparing to the benchmark.},
booktitle = {Proceedings of the 2022 14th International Conference on Machine Learning and Computing},
pages = {34–43},
numpages = {10},
keywords = {Stock analysis, Neural network, Trading process},
location = {Guangzhou, China},
series = {ICMLC '22}
}

@inproceedings{10.1145/3472735.3473389,
author = {Dugan, Kevin and Harb, Maher and Rice, Daniel},
title = {A Reinforcement Learning Framework for Optimizing Throughput in DOCSIS Networks},
year = {2021},
isbn = {9781450386340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472735.3473389},
doi = {10.1145/3472735.3473389},
abstract = {The capacity in a communication network is restricted by the famous Shannon-Hartley theorem, which establishes a relationship between maximum achievable capacity, channel bandwidth, and signal-to-noise ratio of the channel. The state-of-the-art in pushing the achievable capacity close to the theoretical limit revolves around coming up with ever more efficient error correction algorithms combined with assigning the proper modulation and encoding scheme to match the conditions of the spectrum at any given point in time. In cable broadband networks, which operate under the DOCSIS protocol, a Profile Management Application (PMA) system uses telemetry collected from cable modems and cable modem termination systems (CMTSs) to dynamically assign DOCSIS profiles that constitute a combination of Forward Error Correction (FEC) configuration, a Quadrature Amplitude Modulation (QAM) level, and other protocol-based configurations. The objective behind this dynamic assignment is twofold: maximizing capacity and keeping the uncorrectable error rate at a minimal level. The current PMA implementation, adopts a rule-based approach, where pre-defined thresholds govern the decisions for adjusting the profiles. This approach, while proven to be successful, limits opportunities to fully realize optimal DOCSIS configurations to bring system performance closer to the Shannon limit. Through a reinforcement learning (RL) implementation of PMA, it is possible to substitute the pre-defined rules for a system that learns to select the optimal configuration at each decision point, based on past outcomes and potential future rewards. In this paper, we focus on designing an RL-based PMA system to manage DOCSIS 3.0 upstream configurations.},
booktitle = {Proceedings of the 4th FlexNets Workshop on Flexible Networks Artificial Intelligence Supported Network Flexibility and Agility},
pages = {50–55},
numpages = {6},
location = {Virtual Event, USA},
series = {FlexNets '21}
}

@inproceedings{10.1145/3534678.3539141,
author = {Han, Benjamin and Lee, Hyungjun and Martin, S\'{e}bastien},
title = {Real-Time Rideshare Driver Supply Values Using Online Reinforcement Learning},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539141},
doi = {10.1145/3534678.3539141},
abstract = {In this paper, we present Online Supply Values (OSV), a system for estimating the return of available rideshare drivers to match drivers to ride requests at Lyft. Because a future driver state can be accurately predicted from a request destination, it is possible to estimate the expected action value of assigning a ride request to an available driver as a Markov Decision Process using the Bellman Equation. These estimates are updated using temporal difference and are shown to adapt to changing marketplace conditions in real-time. While reinforcement learning has been studied for rideshare dispatch, fully-online approaches without offline priors or other guardrails had never been evaluated in the real world. This work presents the algorithmic changes needed to bridge this gap. OSV is now deployed globally as a core component of Lyft's dispatch matching system. Our A/B user experiments in major US cities measure a +(0.96±0.53)\% increase in the request fulfillment rate and a +(0.73±0.22)\% increase to profit per passenger session over the previous algorithm.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2968–2976},
numpages = {9},
keywords = {matching, on-policy control, online learning, multi-agent reinforcement learning, real-time, transportation, rideshare, streaming, adaptive, temporal difference, dispatch},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3580305.3599800,
author = {Nambiar, Mila and Ghosh, Supriyo and Ong, Priscilla and Chan, Yu En and Bee, Yong Mong and Krishnaswamy, Pavitra},
title = {Deep Offline Reinforcement Learning for Real-World Treatment Optimization Applications},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599800},
doi = {10.1145/3580305.3599800},
abstract = {There is increasing interest in data-driven approaches for recommending optimal treatment strategies in many chronic disease management and critical care applications. Reinforcement learning methods are well-suited to this sequential decision-making problem, but must be trained and evaluated exclusively on retrospective medical record datasets as direct online exploration is unsafe and infeasible. Despite this requirement, the vast majority of treatment optimization studies use off-policy RL methods (e.g., Double Deep Q Networks (DDQN) or its variants) that are known to perform poorly in purely offline settings. Recent advances in offline RL, such as Conservative Q-Learning (CQL), offer a suitable alternative. But there remain challenges in adapting these approaches to real-world applications where suboptimal examples dominate the retrospective dataset and strict safety constraints need to be satisfied. In this work, we introduce a practical and theoretically grounded transition sampling approach to address action imbalance during offline RL training. We perform extensive experiments on two real-world tasks for diabetes and sepsis treatment optimization to compare performance of the proposed approach against prominent off-policy and offline RL baselines (DDQN and CQL). Across a range of principled and clinically relevant metrics, we show that our proposed approach enables substantial improvements in expected health outcomes and in consistency with relevant practice and safety guidelines.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4673–4684},
numpages = {12},
keywords = {treatment optimization, sampling, type 2 diabetes treatment, sepsis treatment, offline reinforcement learning, safety constraints},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.5555/3463952.3464131,
author = {Charakorn, Rujikorn and Manoonpong, Poramate and Dilokthanakul, Nat},
title = {Learning to Cooperate with Unseen Agents Through Meta-Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Ad hoc teamwork problem describes situations where an agent has to cooperate with previously unseen agents to achieve a common goal. For an agent to be successful in these scenarios, it has to have cooperative skills. One could implement cooperative skills into an agent by using domain knowledge (e.g., goals, roles, and protocols) to design the agent's behaviours. However, in complex domains, domain knowledge might not be available. Therefore, it is interesting to explore how to directly learn cooperative skills from data. In this work, we apply meta-reinforcement learning (meta-RL) formulation in the context of ad hoc teamwork problem. Our experiments show that such a method could produce cooperative agents in two cooperative environments with different cooperative circumstances.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1478–1479},
numpages = {2},
keywords = {ad-hoc teamwork, meta-learning, cooperation, generalisation, deep reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/372202.372204,
author = {Kalles, Dimitrios and Kanellopoulos, Panagiotis},
title = {On Verifying Game Designs and Playing Strategies Using Reinforcement Learning},
year = {2001},
isbn = {1581132875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/372202.372204},
doi = {10.1145/372202.372204},
booktitle = {Proceedings of the 2001 ACM Symposium on Applied Computing},
pages = {6–11},
numpages = {6},
keywords = {design verification, reinforcement learning, games, machine learning, playability},
location = {Las Vegas, Nevada, USA},
series = {SAC '01}
}

@inproceedings{10.1145/3109859.3109914,
author = {Zhang, Yang and Zhang, Chenwei and Liu, Xiaozhong},
title = {Dynamic Scholarly Collaborator Recommendation via Competitive Multi-Agent Reinforcement Learning},
year = {2017},
isbn = {9781450346528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109859.3109914},
doi = {10.1145/3109859.3109914},
abstract = {In an interdisciplinary environment, scientific collaboration is becoming increasingly important. Helping scholars make a right choice of potential collaborators is essential in achieving scientific success. Intuitively, the generation of collaboration relationship is a dynamic process. For instance, one scholar may first choose to work with Scholar A, and then work with Scholar B after accumulating additional academic credits. To address this property, we propose a novel dynamic collaboration recommendation method by adapting the multi-agent reinforcement learning technique to the coauthor network analysis. The collaborator selection is optimized from several different scholar similarity measurements. Unlike prior studies, the proposed method characterizes scholarly competition, a.k.a. different scholars will compete for potential collaborator at each iteration. An evaluation with the ACM data shows that multi-agent reinforcement learning plus scholarly competition modeling can be significant for collaboration recommendation.},
booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
pages = {331–335},
numpages = {5},
keywords = {reinforcement learning, multi-agent, dynamic, competition, collaborator recommendation},
location = {Como, Italy},
series = {RecSys '17}
}

@inproceedings{10.1145/3417990.3421395,
author = {Barriga, Angela and Mandow, Lawrence and de la Cruz, Jos\'{e} Luis P\'{e}rez and Rutle, Adrian and Heldal, Rogardt and Iovino, Ludovico},
title = {A Comparative Study of Reinforcement Learning Techniques to Repair Models},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3421395},
doi = {10.1145/3417990.3421395},
abstract = {In model-driven software engineering, models are used in all phases of the development process. These models may get broken due to various editions during the modeling process. To repair broken models we have developed PARMOREL, an extensible framework that uses reinforcement learning techniques. So far, we have used our version of the Markov Decision Process (MDP) adapted to the model repair problem and the Q-learning algorithm. In this paper, we revisit our MDP definition, addressing its weaknesses, and proposing a new one. After comparing the results of both MDPs using Q-Learning to repair a sample model, we proceed to compare the performance of Q-Learning with other reinforcement learning algorithms using the new MDP. We compare Q-Learning with four algorithms: Q(λ), Monte Carlo, SARSA and SARSA (λ), and perform a comparative study by repairing a set of broken models. Our results indicate that the new MDP definition and the Q(λ) algorithm can repair with faster performance.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {47},
numpages = {9},
keywords = {reinforcement learning, markov decision process, model repair},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/375735.376305,
author = {Shapiro, Daniel and Langley, Pat and Shachter, Ross},
title = {Using Background Knowledge to Speed Reinforcement Learning in Physical Agents},
year = {2001},
isbn = {158113326X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375735.376305},
doi = {10.1145/375735.376305},
abstract = {This paper describes Icarus, an agent architecture that embeds a hierarchical reinforcement learning algorithm within a language for specifying agent behavior. An Icarus program expresses an approximately correct theory about how to behave with options at varying levels of detail, while the Icarus agent determines the best options by learning from experience. We describe Icarus and its learning algorithm, then report on two experiments in a vehicle control domain. The first examines the benefit of new distinctions about state, whereas the second explores the impact of added plan structure. We show that background knowledge increases learning rate and asymptotic performance, and decreases plan size by three orders of magnitude, relative to the typical formulation of the learning problem in our test domain.},
booktitle = {Proceedings of the Fifth International Conference on Autonomous Agents},
pages = {254–261},
numpages = {8},
keywords = {agent architectures, action selection and planning, hierarchical reinforcement learning, adaptation and learning},
location = {Montreal, Quebec, Canada},
series = {AGENTS '01}
}

@article{10.5555/1622407.1622409,
author = {Drummond, Chris},
title = {Accelerating Reinforcement Learning by Composing Solutions of Automatically Identified Subtasks},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {This paper discusses a system that accelerates reinforcement learning by using transfer from related tasks. Without such transfer, even if two tasks are very similar at some abstract level, an extensive re-learning effort is required. The system achieves much of its power by transferring parts of previously learned solutions rather than a single complete solution. The system exploits strong features in the multi-dimensional function produced by reinforcement learning in solving a particular task. These features are stable and easy to recognize early in the learning process. They generate a partitioning of the state space and thus the function. The partition is represented as a graph. This is used to index and compose functions stored in a case base to form a close approximation to the solution of the new task. Experiments demonstrate that function composition often produces more than an order of magnitude increase in learning rate compared to a basic reinforcement learning algorithm.},
journal = {J. Artif. Int. Res.},
month = {feb},
pages = {59–104},
numpages = {46}
}

@article{10.1145/3320283,
author = {Yang, Hsiang-Yu and Wong, Sai-Keung},
title = {Agent-Based Cooperative Animation for Box-Manipulation Using Reinforcement Learning},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3320283},
doi = {10.1145/3320283},
abstract = {This paper presents an approach to assist the generation of agent-based cooperative animation using reinforcement learning. We focus on manipulation skills for box-shaped objects, including pushing, pulling, and moving objects in a relay way. There are a learning process and an animation process. In the learning process, different kinds of agents are trained using reinforcement learning. Policies are learned to control the agents to perform specific tasks. A physics simulator is adopted to simulate the interaction among objects. In the animation process, users animate agents with the learned policies. We propose several tools to intuitively create cooperative animations. We applied our method to generate several animations in which agents work together to finish tasks. A user study indicates that by using our tools, diverse cooperative animations can be easily created.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {jun},
articleno = {2},
numpages = {18},
keywords = {agents, cooperative animation, box manipulation, virtual reality, reinforcement learning}
}

@article{10.1145/3598301,
author = {Demirel, Berken Utku and Chen, Luke and Al Faruque, Mohammad Abdullah},
title = {Data-Driven Energy-Efficient Adaptive Sampling Using Deep Reinforcement Learning},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3598301},
doi = {10.1145/3598301},
abstract = {This article presents a resource-efficient adaptive sampling methodology for classifying electrocardiogram (ECG) signals into different heart rhythms. We present our methodology in two folds: (i) the design of a novel real-time adaptive neural network architecture capable of classifying ECG signals with different sampling rates and (ii) a runtime implementation of sampling rate control using deep reinforcement learning (DRL). By using essential morphological details contained in the heartbeat waveform, the DRL agent can control the sampling rate and effectively reduce energy consumption at runtime. To evaluate our adaptive classifier, we use the MIT-BIH database and the recommendation of the AAMI to train the classifiers. The classifier is designed to recognize three major types of arrhythmias, which are supraventricular ectopic beats (SVEB), ventricular ectopic beats (VEB), and normal beats (N). The performance of the arrhythmia classification reaches an accuracy of 97.2\% for SVEB and 97.6\% for VEB beats. Moreover, the designed system is 7.3\texttimes{} more energy-efficient compared to the baseline architecture, where the adaptive sampling rate is not utilized. The proposed methodology can provide reliable and accurate real-time ECG signal analysis with performances comparable to state-of-the-art methods. Given its time-efficient, low-complexity, and low-memory-usage characteristics, the proposed methodology is also suitable for practical ECG applications, in our case for arrhythmia classification, using resource-constrained devices, especially wearable healthcare devices and implanted medical devices.},
journal = {ACM Trans. Comput. Healthcare},
month = {sep},
articleno = {19},
numpages = {19},
keywords = {energy-efficient, real-time, Adaptive sampling, wearable devices, heart rate}
}

@inproceedings{10.1145/1502650.1502700,
author = {Atrash, Amin and Pineau, Joelle},
title = {A Bayesian Reinforcement Learning Approach for Customizing Human-Robot Interfaces},
year = {2009},
isbn = {9781605581682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1502650.1502700},
doi = {10.1145/1502650.1502700},
abstract = {Personal robots are becoming increasingly prevalent, which raises a number of interesting issues regarding the design and customization of interfaces to such platforms. The particular problem addressed by this paper is the use of learning methods to improve the quality and effectiveness of human-machine interaction onboard a robotic wheelchair. In support of this, we present a method for learning and adapting probabilistic models with the aid of a human operator. We use a Bayesian reinforcement learning framework, that allows us to mix learning and execution, as well as take advantage of prior information about the world. We address the problems of learning, handling a partially observable environment, and limiting the number of action requests. We demonstrate empirical feasibility of our approach on an interface for an autonomous wheelchair.},
booktitle = {Proceedings of the 14th International Conference on Intelligent User Interfaces},
pages = {355–360},
numpages = {6},
keywords = {intelligent assistants, activity \&amp; plan recognition, intelligent interfaces for ubiquitous computing},
location = {Sanibel Island, Florida, USA},
series = {IUI '09}
}

@inproceedings{10.1145/3167486.3167574,
author = {El Fouki, Mohammed and Aknin, Noura and El. Kadiri, K. Ed},
title = {Intelligent Adapted E-Learning System Based on Deep Reinforcement Learning},
year = {2017},
isbn = {9781450353069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167486.3167574},
doi = {10.1145/3167486.3167574},
abstract = {Today, E-Learning platforms are getting more popularity and relevance between educational institutions such as open and distance universities and research institutes. But unfortunately, these platforms present yet unsolved problems, due to the absence of face-to-face interaction. Distance formation instructors have real difficulties recognizing who their students are, understanding their student's behaviors in the virtual course, what's possibilities they have to achieve the subject, what complications they find. Basically, they require having a reply which assists them to increase the learning/teaching process. So, it is necessary to develop an intelligent and adapted learning strategy based on the recommendation system that helps professors to do their work efficiently.},
booktitle = {Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems},
articleno = {85},
numpages = {6},
keywords = {Reinforcement Learning, Learning Management System (LMS), Decision Support System, Deep Neural network, Personalized learning, E-learning},
location = {Larache, Morocco},
series = {ICCWCS'17}
}

@inproceedings{10.1145/3459637.3482292,
author = {Ji, Luo and Qin, Qi and Han, Bingqing and Yang, Hongxia},
title = {Reinforcement Learning to Optimize Lifetime Value in Cold-Start Recommendation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482292},
doi = {10.1145/3459637.3482292},
abstract = {Recommender system plays a crucial role in modern E-commerce platform. Due to the lack of historical interactions between users and items, cold-start recommendation is a challenging problem. In order to alleviate the cold-start issue, most existing methods introduce content and contextual information as the auxiliary information. Nevertheless, these methods assume the recommended items behave steadily over time, while in a typical E-commerce scenario, items generally have very different performances throughout their life period. In such a situation, it would be beneficial to consider the long-term return from the item perspective, which is usually ignored in conventional methods. Reinforcement learning (RL) naturally fits such a long-term optimization problem, in which the recommender could identify high potential items, proactively allocate more user impressions to boost their growth, therefore improve the multi-period cumulative gains. Inspired by this idea, we model the process as a Partially Observable and Controllable Markov Decision Process (POC-MDP), and propose an actor-critic RL framework (RL-LTV) to incorporate the item lifetime values (LTV) into the recommendation. In RL-LTV, the critic studies historical trajectories of items and predict the future LTV of fresh item, while the actor suggests a score-based policy which maximizes the future LTV expectation. Scores suggested by the actor are then combined with classical ranking scores in a dual-rank framework, therefore the recommendation is balanced with the LTV consideration. Our method outperforms the strong live baseline with a relative improvement of 8.67\% and 18.03\% on IPV and GMV of cold-start items, on one of the largest E-commerce platform.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {782–791},
numpages = {10},
keywords = {poc-mdp, lifetime value, cold-start recommendation, actor-critic model, reinforcement learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3583131.3590388,
author = {Lim, Bryan and Flageat, Manon and Cully, Antoine},
title = {Understanding the Synergies between Quality-Diversity and Deep Reinforcement Learning},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590388},
doi = {10.1145/3583131.3590388},
abstract = {The synergies between Quality-Diversity (QD) and Deep Reinforcement Learning (RL) have led to powerful hybrid QD-RL algorithms that have shown tremendous potential, and bring the best of both fields. However, only a single deep RL algorithm (TD3) has been used in prior hybrid methods despite notable progress made by other RL algorithms. Additionally, there are fundamental differences in the optimization procedures between QD and RL which would benefit from a more principled approach. We propose Generalized Actor-Critic QD-RL, a unified modular framework for actor-critic deep RL methods in the QD-RL setting. This framework provides a path to study insights from Deep RL in the QD-RL setting, which is an important and efficient way to make progress in QD-RL. We introduce two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ) which apply recent advancements in Deep RL to the QD-RL setting, and solve the humanoid environment which was not possible using existing QD-RL algorithms. However, we also find that not all insights from Deep RL can be effectively translated to QD-RL. Critically, this work also demonstrates that the actor-critic models in QD-RL are generally insuficiently trained and performance gains can be achieved without any additional environment evaluations.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1212–1220},
numpages = {9},
keywords = {quality-diversity, neuroevolution, deep reinforcement learning},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@inproceedings{10.1109/WIIAT.2008.77,
author = {Gomes, Eduardo Rodrigues and Kowalczyk, Ryszard},
title = {Non-Symmetric Preferences in the IPA Market with Reinforcement Learning},
year = {2008},
isbn = {9780769534961},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WIIAT.2008.77},
doi = {10.1109/WIIAT.2008.77},
abstract = {Machine Learning has been proposed to support and optimize market-based resource allocation. In particular, Reinforcement Learning (RL) has been used to improve the allocation in terms of the utility received by resource requesting agents in the Iterative Price Adjustment (IPA) mechanism. In such an approach, utility functions describe the agents' preferences for resource attributes and are the basis for RL to learn demand functions that are optimized for the market. It has been shown that the reward functions based on the individual utility of the agents and the social welfare of the allocation can deliver similar social results when the market consists only of learning agents with symmetric preferences. In this paper we investigate the IPA market-based resource allocation with RL for the case of agents with non-symmetric preferences. We show through experimental investigation that the results observed above are also held in this case. In particular, we show that the individual-based reward function is able to approximate the solution to the fairest Pareto-Optimal allocation in situations where the social-based reward function fails.},
booktitle = {Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {424–430},
numpages = {7},
keywords = {Reinforcement Learning, Iterative Price Adjustment, Individual, Market-based Resource Allocation, Social Rewards},
series = {WI-IAT '08}
}

@inproceedings{10.1145/2576768.2598358,
author = {Koutn\'{\i}k, Jan and Schmidhuber, Juergen and Gomez, Faustino},
title = {Evolving Deep Unsupervised Convolutional Networks for Vision-Based Reinforcement Learning},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598358},
doi = {10.1145/2576768.2598358},
abstract = {Dealing with high-dimensional input spaces, like visual input, is a challenging task for reinforcement learning (RL). Neuroevolution (NE), used for continuous RL problems, has to either reduce the problem dimensionality by (1) compressing the representation of the neural network controllers or (2) employing a pre-processor (compressor) that transforms the high-dimensional raw inputs into low-dimensional features. In this paper, we are able to evolve extremely small recurrent neural network (RNN) controllers for a task that previously required networks with over a million weights. The high-dimensional visual input, which the controller would normally receive, is first transformed into a compact feature vector through a deep, max-pooling convolutional neural network (MPCNN). Both the MPCNN preprocessor and the RNN controller are evolved successfully to control a car in the TORCS racing simulator using only visual input. This is the first use of deep learning in the context evolutionary RL.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {541–548},
numpages = {8},
keywords = {games, deep learning, vision-based torcs, neuroevolution, reinforcement learning},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.5555/3535850.3535950,
author = {McCalmon, Joe and Le, Thai and Alqahtani, Sarra and Lee, Dongwon},
title = {CAPS: Comprehensible Abstract Policy Summaries for Explaining Reinforcement Learning Agents},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {As reinforcement learning (RL) continues to improve and be applied in situations alongside humans, the need to explain the learned behaviors of RL agents to end-users becomes more important. Strategies for explaining the reasoning behind an agent's policy, called policy-level explanations, can lead to important insights about both the task and the agent's behaviors. Following this line of research, in this work, we propose a novel approach, named as CAPS, that summarizes an agent's policy in the form of a directed graph with natural language descriptions. A decision tree based clustering method is utilized to abstract the state space of the task into fewer, condensed states which makes the policy graphs more digestible to end-users. This abstraction allows the users to control the size of the policy graph to achieve their desired balance between comprehensibility and accuracy. In addition, we develop a heuristic optimization method to find the most explainable graph policy and present it to the users. Finally, we use the user-defined predicates to enrich the abstract states with semantic meaning. We test our approach on five RL tasks, using both deterministic and stochastic policies, and show that our method is: (1) agnostic to the algorithms used to train the policies, and (2) comparable in accuracy and superior in explanation capabilities to existing baselines. Especially, when provided with our explanation graph, end-users are able to accurately interpret policies of trained RL agents 80\% of the time, compared to 10\% when provided with the next best baseline. We make our code and datasets available to ensure the reproducibility of our research findings: https://github.com/mccajl/CAPS},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {889–897},
numpages = {9},
keywords = {policy summary, reinforcement learning, explainable reinforcement learning, policy-level explanations, explainable machine learning, user study},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/1102256.1102277,
author = {Wada, Atsushi and Takadama, Keiki and Shimohara, Katsunori},
title = {Learning Classifier System Equivalent with Reinforcement Learning with Function Approximation},
year = {2005},
isbn = {9781450378000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1102256.1102277},
doi = {10.1145/1102256.1102277},
abstract = {We present an experimental comparison of the reinforcement process between Learning Classifier System (LCS) and Reinforcement Learning (RL) with function approximation (FA) method, regarding their generalization mechanisms. To validate our previous theoretical analysis that derived equivalence of reinforcement process between LCS and RL, we introduce a simple test environment named Gridworld, which can be applied to both LCS and RL with three different classes of generalization: (1) tabular representation; (2) state aggregation; and (3) linear approximation. From the simulation experiments comparing LCS with its GA-inactivated and corresponding RL method, all the cases regarding the class of generalization showed identical results with the criteria of performance and temporal difference (TD) error, thereby verifying the equivalence predicted from the theory.},
booktitle = {Proceedings of the 7th Annual Workshop on Genetic and Evolutionary Computation},
pages = {92–93},
numpages = {2},
keywords = {function approximation, genetic-based machine learning, learning classifier systems, reinforcement learning},
location = {Washington, D.C.},
series = {GECCO '05}
}

@inproceedings{10.5555/3398761.3398927,
author = {Wang, Haozhe and Zhou, Jiale and He, Xuming},
title = {Learning Context-Aware Task Reasoning for Efficient Meta Reinforcement Learning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Despite recent success of deep network-based Reinforcement Learning (RL), it remains elusive to achieve human-level efficiency in learning novel tasks. While previous efforts attempt to address this challenge using meta-learning strategies, they typically suffer from sampling inefficiency with on-policy RL algorithms or meta-overfitting with off-policy learning. In this work, we propose a novel meta-RL strategy to address those limitations. In particular, we decompose the meta-RL problem into three sub-tasks, task-exploration, task-inference and task-fulfillment, instantiated with two deep network agents and a task encoder. During meta-training, our method learns a task-conditioned actor network for task-fulfillment, an explorer network with a self-supervised reward shaping that encourages task-informative experiences in task-exploration, and a context-aware graph-based task encoder for task inference. We validate our approach with extensive experiments on several public benchmarks and the results show that our algorithm effectively performs exploration for task inference, improves sample efficiency during both training and testing, and mitigates the meta-overfitting problem.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1440–1448},
numpages = {9},
keywords = {deep reinforcement learning, multitask learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3466752.3480114,
author = {Bera, Rahul and Kanellopoulos, Konstantinos and Nori, Anant and Shahroodi, Taha and Subramoney, Sreenivas and Mutlu, Onur},
title = {Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480114},
doi = {10.1145/3466752.3480114},
abstract = {Past research has proposed numerous hardware prefetching techniques, most of which rely on exploiting one specific type of program context information (e.g., program counter, cacheline address, or delta between cacheline addresses) to predict future memory accesses. These techniques either completely neglect a prefetcher’s undesirable effects (e.g., memory bandwidth usage) on the overall system, or incorporate system-level feedback as an afterthought to a system-unaware prefetch algorithm. We show that prior prefetchers often lose their performance benefit over a wide range of workloads and system configurations due to their inherent inability to take multiple different types of program context and system-level feedback information into account while prefetching. In this paper, we make a case for designing a holistic prefetch algorithm that learns to prefetch using multiple different types of program context and system-level feedback information inherent to its design. To this end, we propose Pythia, which formulates the prefetcher as a reinforcement learning agent. For every demand request, Pythia observes multiple different types of program context information to make a prefetch decision. For every prefetch decision, Pythia receives a numerical reward that evaluates prefetch quality under the current memory bandwidth usage. Pythia uses this reward to reinforce the correlation between program context information and prefetch decision to generate highly accurate, timely, and system-aware prefetch requests in the future. Our extensive evaluations using simulation and hardware synthesis show that Pythia outperforms two state-of-the-art prefetchers (MLOP and Bingo) by 3.4\% and 3.8\% in single-core, 7.7\% and 9.6\% in twelve-core, and 16.9\% and 20.2\% in bandwidth-constrained core configurations, while incurring only 1.03\% area overhead over a desktop-class processor and no software changes in workloads. The source code of Pythia can be freely downloaded from https://github.com/CMU-SAFARI/Pythia.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1121–1137},
numpages = {17},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@inproceedings{10.5555/3398761.3398858,
author = {Ma, Jinming and Wu, Feng},
title = {Feudal Multi-Agent Deep Reinforcement Learning for Traffic Signal Control},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reinforcement learning (RL) is a promising technique for optimizing traffic signal controllers that dynamically respond to real-time traffic conditions. Recent efforts that applied Multi-Agent RL (MARL) to this problem have shown remarkable improvement over centralized RL, with the scalability to solve large problems by distributing the global control to local RL agents. Unfortunately, it is also easy to get stuck in local optima because each agent only has partial observability of the environment with limited communication. To tackle this, we borrow ideas from feudal RL and propose a novel MARL approach combining with the feudal hierarchy. Specifically, we split the traffic network into several regions, where each region is controlled by a manager agent and the agents who control the traffic signals are its workers. In our method, managers coordinate their high-level behaviors and set goals for their workers in the region, while each lower-level worker controls traffic signals to fulfill the managerial goals. By doing so, we are able to coordinate globally while retain scalability. We empirically evaluate our method both in a synthetic traffic grid and real-world traffic network using the SUMO simulator. Our experimental results show that our approach outperforms the state-of-the-art in almost all evaluation metrics commonly used for traffic signal control.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {816–824},
numpages = {9},
keywords = {multi-agent reinforcement learning, traffic signal control, deep reinforcement learning, feudal reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3411408.3411427,
author = {Spatharis, Christos and Blekas, Konstantinos and Vouros, George A.},
title = {Apprenticeship Learning of Flight Trajectories Prediction with Inverse Reinforcement Learning},
year = {2020},
isbn = {9781450388788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411408.3411427},
doi = {10.1145/3411408.3411427},
abstract = {One of the primary goals of Artificial Intelligence research is to develop machines with human-like intelligence, perception and reasoning. In this direction teaching apprentice agents by observing demonstrations delivered by experts is a framework of imitation learning that can provide improved solutions and it is possible to significantly outperform the demonstrator. Inverse reinforcement learning (IRL) is a paradigm relying on Markov Decision Processes (MDPs) that has a twofold target: to learn optimum policies of autonomous agents for solving complex tasks from successful demonstrations, and also to discover the unknown reward function that could explain the expert behavior. In this article we are addressing the trajectory prediction problem in the aviation domain by using an IRL approach. The proposed learning scheme provides an imitation process where the algorithm tries to imitate demonstrated trajectories, exploiting raw trajectory data enriched with contextual features and learn an efficient reward model that is learned during imitation and has generalization capabilities to unknown cases. We show several experimental results using real trajectory data from the Spanish FIR that confirms the effectiveness of our approach in automatically predicting trajectories.},
booktitle = {11th Hellenic Conference on Artificial Intelligence},
pages = {241–249},
numpages = {9},
keywords = {autonomous aircraft, Apprenticeship learning, trajectory prediction, inverse reinforcement learning},
location = {Athens, Greece},
series = {SETN 2020}
}

@inproceedings{10.1145/3318265.3318294,
author = {Zhou, Yinda and Liu, Weiming and Li, Bin},
title = {Two-Stage Population Based Training Method for Deep Reinforcement Learning},
year = {2019},
isbn = {9781450366380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318265.3318294},
doi = {10.1145/3318265.3318294},
abstract = {Deep reinforcement learning (DRL) methods has been widely applied on more and more challenging learning tasks, and achieved excellent performance. However, the efficiency of deep reinforcement learning is notoriously sensitive to their own hyperparameter configuration. The optimization process of deep reinforcement learning is highly dynamic and non-stationary, rather than a simple fitting process. So, its optimal hyperparameter should be adaptively adjusted according to the current learning process, rather than using a fixed set of hyperparameter configurations from beginning to end. DeepMind innovatively proposed a population based training (PBT) method for deep reinforcement learning, which achieved hyperparameter adaptation and made the model better trained. However, we assume that at the early stage when the learning model has little knowledge of the environment, frequent hyperparameter change will not be helpful for the model to learn efficiently, while learning with a reasonable fixed hyperparameter configuration will help the model obtain necessary knowledge as quick as possible, which we consider is more important for reinforcement learning at early stage. In this paper, we verified our hypothesis through experiments, and a Two-Stage Population Based Training (TS-PBT) method is proposed, which is a more efficient population based training method for deep reinforcement learning. Experiments show that at the same computational budget, our TS-PBT method makes the final performance of the model significantly better than the PBT method. TS-PBT achieved 40\%, 310\%, 2\%, 53\%, 30\% and 38\% performance improvement over PBT separately in six test environments.},
booktitle = {Proceedings of the 3rd International Conference on High Performance Compilation, Computing and Communications},
pages = {38–44},
numpages = {7},
keywords = {two-stage, PBT, deep reinforcement learning, game, hyperparameter adaptation},
location = {Xi'an, China},
series = {HP3C '19}
}

